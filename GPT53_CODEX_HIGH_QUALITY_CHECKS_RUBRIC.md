# GPT-5.3-Codex-High Quality Checks Rubric

Status: **Core-focused rubric finalized** (tags ignored; metadata low-weight).

## Purpose
Define a consistent, auditable QA process for evaluating paper analysis `.md` reports against their source `.txt` files.

## Scope
- Applies to analyses generated by **gpt-5.3-codex-high**.
- Designed to support **interleaved evaluation** with outputs from a different model.
- Unit of evaluation: one pair (`report.md` vs `source.txt`) for the same arXiv/paper ID.

## Pairing Rules (sanity check)
1. Match by paper ID prefix (e.g., `2406.02856_*`).
2. Confirm report title roughly matches source title.
3. If multiple reports exist for same paper, evaluate the explicitly selected one.
4. If pairing is ambiguous, mark `BLOCKED` and do not score.

---

## Rubric (100 points total)

### 1) Paper Match Sanity Check (5 pts)
Intent: catch obvious pairing mistakes quickly.

Scoring:
- **5** = clearly same paper
- **3** = probably same paper, minor mismatch/noise
- **0** = obviously wrong paper/source pairing

Note: this is a rough human sanity check (not strict formal validation).

### 2) Core Factual Accuracy (30 pts)
Intent: verify the main story is true to source.

Quick checks:
- problem + contribution
- method framing
- direction of main findings
- no obvious fabricated central concepts

Scoring:
- **26–30**: Mostly accurate, no major factual errors
- **18–25**: Some drift/imprecision; core story still right
- **8–17**: **Multiple misleading statements**; core story only partially reliable
- **0–7**: Core summary is wrong or strongly misleading

### 3) Quantitative Claims Sanity (core-only) (15 pts)
Intent: penalize only numeric issues that mislead core conclusions.

Scoring:
- **13–15**: No misleading numeric issues on core claims
- **9–12**: Numeric imprecision exists, but peripheral; core interpretation unchanged
- **5–8**: At least one misleading numeric issue **on a core claim**
- **0–4**: Multiple misleading numeric issues **on core claims** or fabricated core quantitative results

### 4) Claim Discipline (Overreach Control) (20 pts)
Intent: penalize overreach only when it distorts core takeaway.

Scoring:
- **17–20**: Well-calibrated claims; no core overreach
- **12–16**: Some overstatement, but not core-distorting
- **6–11**: At least one **core-misleading overreach**
- **0–5**: Multiple core-misleading overreaches or strongly inflated conclusions

### 5) Metadata & Formatting Hygiene (5 pts)
Intent: track polish issues with low impact on total quality.

Required fields:
- `title`
- `arxiv_id`
- `source_url`

Scoring:
- **5** = all required fields present + clean formatting
- **4** = all required fields present + 1 minor issue
- **3** = all required fields present + 2–3 minor issues
- **2** = all required fields present + 1 major issue
- **1** = missing 1 required field
- **0** = missing 2+ required fields or unusable metadata

**Tags are ignored** (no scoring impact).

### 6) Unsupported / Fabricated Content (core-focused) (15 pts)
Intent: penalize unsupported content when it affects core interpretation.

Scoring:
- **15** = no core unsupported/fabricated claims
- **12** = 1 peripheral unsupported claim
- **9** = 2+ peripheral unsupported claims
- **6** = 1 core unsupported/fabricated claim
- **3** = 2 core unsupported/fabricated claims
- **0** = 3+ core unsupported/fabricated claims, or one fabricated central result

### 7) Evidence Anchoring Quality (10 pts)
Intent: ensure major judgments are quickly verifiable.

Major judgments (4):
1. core factual accuracy judgment
2. core numeric judgment
3. claim-overreach judgment
4. unsupported/fabricated-content judgment

Scoring:
- **10** = anchors for **4/4** major judgments
- **8** = anchors for **3/4** major judgments
- **6** = anchors for **2/4** major judgments
- **4** = anchors for **1/4** major judgments
- **2** = citations/quotes exist, but do not clearly support any major judgment
- **0** = no usable anchors

---

## Severity Labels
Use these for each issue found:
- **S0 (Critical):** wrong paper identity/pairing, or fabricated central result
- **S1 (Major):** core-misleading factual/quantitative/claim-discipline issue
- **S2 (Moderate):** meaningful but non-core-distorting issue
- **S3 (Minor):** wording/format/cosmetic issue

## Pass/Fail Thresholds
- **PASS-High:** 90–100
- **PASS-Conditional:** 75–89
- **FAIL:** <75

Guidance:
- A report with unresolved S0 or multiple S1 issues should not be marked PASS-High.

---

## Required Output Format Per Check

```md
### QA Record
- paper_id:
- report_path:
- source_path:
- evaluator_model: gpt-5.3-codex-high
- comparison_model: (if interleaved)
- timestamp:

### Score
- Paper Match Sanity Check (5):
- Core Factual Accuracy (30):
- Quantitative Claims Sanity (15):
- Claim Discipline / Overreach (20):
- Metadata & Formatting Hygiene (5):
- Unsupported/Fabricated Content (15):
- Evidence Anchoring (10):
- **Total (100):**
- Verdict: PASS-High | PASS-Conditional | FAIL

### Findings
1. [Severity: Sx] Finding summary
   - Report evidence:
   - Source evidence (quote/line anchor):
   - Impact:
   - Fix guidance:

### Final Judgment
- Reliability summary (2–4 lines)
- Whether this sample should be moved to `bad/` (Yes/No)
```

---

## Interleaving Plan (Template)

| order | paper_id | model_under_test | comparison_model | status |
|---|---|---|---|---|
| 1 |  | gpt-5.3-codex-high |  | pending |
| 2 |  | other-model |  | pending |
| 3 |  | gpt-5.3-codex-high |  | pending |
| 4 |  | other-model |  | pending |

Notes:
- Alternate models in sequence (A/B/A/B).
- Use similar paper difficulty where possible.
- Do not run batch checks until interleaving list is finalized.

---

## Operational Notes
- Keep QA writeups concise but evidence-backed.
- Prefer direct source quotes for disputed core claims.
- If source `.txt` appears OCR-corrupt or incomplete, mark as `BLOCKED` and skip scoring.
- This file is the QA policy baseline; revisions should be versioned.

---

## Appendix A — 3-Report Baseline Setup (Pony Alpha / GLM-5 Preview)

Purpose: establish a **relative-comparison baseline** across 3 reports generated by **Pony Alpha (GLM-5 Preview)**.

Sampling rule used:
- Source index set: `(13 + 40n)` for `n = 0, 1, 2`
- Interpreted as **1-indexed positions** in the **lexicographically sorted** `.md` file list from the 2025 analysis corpus.

Selected baseline samples:

1. **Position 13**
   - Report (`.md`):
     - `2011.03783_towards-a-resource-for-multilingual-lexicons-an-mt-assisted-and-human-in-the-loop-multilingual-parallel-corpus-with-multi-word-expression-annotation_20260210_034853.md`
   - Corresponding source (`.txt`):
     - `2011.03783.txt`

2. **Position 53**
   - Report (`.md`):
     - `2302.13849_optimal-prediction-using-expert-advice-and-randomized-littlestone-dimension_20260210_013041.md`
   - Corresponding source (`.txt`):
     - `2302.13849.txt`

3. **Position 93**
   - Report (`.md`):
     - `2308.08427_eliciting-risk-aversion-with-inverse-reinforcement-learning-via-interactive-questioning_20260210_051248.md`
   - Corresponding source (`.txt`):
     - `2308.08427.txt`

Baseline comparison mode:
- Treat these 3 as a **relative cohort** (compare each against the other two) before broader evaluation.
- Emphasize intra-cohort consistency on:
  - core factual accuracy,
  - core quantitative sanity,
  - claim discipline,
  - unsupported/fabricated content,
  using the rubric above.

Status:
- **Selection complete**
- **Comparative scoring complete** (3-report baseline run)

---

## Appendix B — Baseline Comparative Scoring Results (Pony Alpha / GLM-5 Preview)

Scored using the **current core-focused rubric** in this file.

### B.1 Per-Report Scores (current rubric)

| paper_id | Match (5) | Core Facts (30) | Quant Core (15) | Claim Discipline (20) | Metadata (5) | Unsupported/Fabricated (15) | Anchoring (10) | Total | Verdict |
|---|---:|---:|---:|---:|---:|---:|---:|---:|---|
| 2011.03783 | 5 | 24 | 15 | 14 | 4 | 12 | 10 | **84** | PASS-Conditional |
| 2302.13849 | 5 | 24 | 15 | 12 | 4 | 12 | 10 | **82** | PASS-Conditional |
| 2308.08427 | 5 | 26 | 15 | 18 | 4 | 15 | 10 | **93** | PASS-High |

### B.2 Key Evidence (core-focused)

#### Sample 1 — 2011.03783
- **Core support confirmed**:
  - AlphaMWE + 6 languages + HITL process (source lines 17–24).
  - 750 sentences (source line 482).
  - HOPE: 21% major / 44% minor (source lines 1254–1258).
- **Core-adjacent issue**:
  - Limitation wording overstates single-engine dependence (report line 135) versus mixed pipeline for Arabic/dialectal Arabic (source lines 1174–1190).
- **Scoring note**:
  - Tags ignored by rubric.
  - Reference-count mismatch treated as low-impact metadata only.

#### Sample 2 — 2302.13849
- **Core support confirmed**:
  - RL(H) characterization (source lines 149–150, 881–882).
  - Expert-advice framing (~half deterministic up to small additive terms) in abstract (source lines 35–41).
  - Open-question language for all-n,k bound remains unresolved (source lines 2392–2407).
- **Issue**:
  - Unsupported hardness statement appears in report (`#P-hard`) without source support.
  - Slight overreach risk when stating half-bound too broadly as settled for all settings.
- **Scoring note**:
  - Tags ignored by rubric.

#### Sample 3 — 2308.08427
- **Core support confirmed**:
  - Abstract claims: finite-sample identifiability, distinguishing power, fewer than 50 questions (source lines 7–15).
  - Particle setup includes `K=1000` (source line 1410).
  - Infinite-horizon extension is preliminary/qualitative (source lines 1789–1811).
- **Issue level**:
  - No core fabricated claim found in this sample.
- **Scoring note**:
  - Metadata issues are low-weight by design.

### B.3 Cross-Sample Baseline Statistics (n=3)
- Total score mean: **86.33 / 100**
- Total score sample std. dev.: **5.86**
- Verdict distribution:
  - PASS-High: 1/3
  - PASS-Conditional: 2/3
  - FAIL: 0/3

### B.4 Divergence Pattern (current rubric signal)
Dominant recurring pattern in this 3-report sample:
1. **Core factual reliability is generally good** across all three samples.
2. **Main residual risk is claim calibration** (occasional over-broad or unsupported analytic statements).
3. **Metadata drift exists but is low-impact under this rubric**.

Interpretation:
- Under a core-focused standard, this baseline suggests the smaller-model reports are typically usable for high-level synthesis, with targeted caution around overreach/unsupported analytical claims.

---

## Appendix C — New Model Checks (Incremental)

### C.1 Trial Check — Report 10 (offset rule: 3 + 44n)

### QA Record
- paper_id: 2411.00533
- report_path: 2411.00533_reversener-a-self-generated-example-driven-framework-for-zero-shot-named-entity-recognition-with-large-language-models_20260211_161637.md
- source_path: 2411.00533.txt
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha / GLM-5 Preview (contextual baseline)
- timestamp: 2026-02-11 PST

### Score
- Paper Match Sanity Check (5): 5
- Core Factual Accuracy (30): 26
- Quantitative Claims Sanity (15): 15
- Claim Discipline / Overreach (20): 18
- Metadata & Formatting Hygiene (5): 5
- Unsupported/Fabricated Content (15): 15
- Evidence Anchoring (10): 10
- **Total (100): 94**
- Verdict: **PASS-High**

### Findings
1. [Severity: S3] Core framing is accurate and well-aligned with source
   - Report evidence: reverse-process example library for zero-shot NER; strong microF1 gains; lower compute claim.
   - Source evidence (quote/anchor): Abstract states ReverseNER “construct[s] a reliable example library … through the reverse process of NER,” and reports “5.43 to 7.69 microF1” improvement with “lower computational resource consumption” on CoNLL03/WikiGold.
   - Impact: Core reliability is strong for high-level synthesis.
   - Fix guidance: none required.

2. [Severity: S3] Quantitative claims are core-consistent
   - Report evidence: “5.43-7.69 microF1 improvement,” effectiveness on CoNLL03 and WikiGold.
   - Source evidence (quote/anchor): Introduction/Abstract explicitly reports “5.43 to 7.69 microF1” and references CoNLL03 and WikiGold with lower compute cost on those two public English datasets.
   - Impact: No core-misleading numeric issue found.
   - Fix guidance: none required.

3. [Severity: S3] Slight claim broadening but not core-distorting
   - Report evidence: broad phrasing like “significantly outperforms other zero-shot NER methods.”
   - Source evidence (quote/anchor): Paper explicitly claims significant outperformance; context narrows lower-compute superiority to two public English datasets.
   - Impact: Minor calibration caution only.
   - Fix guidance: when possible, preserve the paper’s scope qualifiers (e.g., “on two public English datasets”).

### Final Judgment
- Reliability summary: This report is strong on core facts, method framing, and primary quantitative outcomes. No core fabricated or core-misleading content was identified. Remaining issues are minor wording/scope calibration rather than substantive errors.
- Whether this sample should be moved to `bad/`: No


---

## Appendix C — Incremental Check Log

### C.1 Trial Check — report 9 from (3 + 44n) sample

### QA Record
- paper_id: 2406.00048
- report_path: `2406.00048_towards-a-theory-of-how-the-structure-of-language-is-acquired-by-deep-neural-networks_20260211_162202.md`
- source_path: `2406.00048.txt`
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha / GLM-5 Preview (new-model report cohort)
- timestamp: 2026-02-11 22:11:06 PST

### Score (current rubric)
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **27**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **18**
- Metadata & Formatting Hygiene (5): **4**
- Unsupported/Fabricated Content (15): **15**
- Evidence Anchoring (10): **10**
- **Total (100): 94**
- Verdict: **PASS-High**

### Findings
1. [Severity: S2] Minor claim calibration risk (non-core)
   - Report evidence: Executive summary frames validation broadly across synthetic + natural data as a single coherent confirmation.
   - Source evidence (quote/line anchor): “we conjecture … holds beyond our synthetic datasets … confirm empirically in Shakespeare’s plays and Wikipedia articles” (source lines 23–27); limitations explicitly state no proof for deep-network sample-complexity connection (source lines 637–640).
   - Impact: Slight over-interpretation risk for rigor level; does not change core takeaway.
   - Fix guidance: Keep “empirically supported conjecture” wording explicit when discussing natural-language generalization.

2. [Severity: S3] Low-impact metadata issue
   - Report evidence: `Reference count: 40` in Quick Facts.
   - Source evidence (quote/line anchor): references section begins at source line 649 and appears larger than 40.
   - Impact: Metadata-only; no core reliability impact under current rubric.
   - Fix guidance: Auto-compute reference count directly from parsed bibliography entries.

### Major-judgment anchors used
- Core factual judgment anchor: contributions on effective context window and stepwise jumps (source lines 58–67).
- Core numeric/quant judgment anchor: finite-data context scaling statement `t*(P) ~ P^(1/2β)` (source lines 290–292).
- Claim-discipline judgment anchor: conjecture + real-data empirical tests (source lines 23–27; 572–582).
- Unsupported/fabricated-content judgment anchor: no fabricated core result found; central claims trace to abstract + contributions (source lines 17–27, 58–72).

### Final Judgment
- Reliability summary: Core summary is strong and source-consistent. The report captures the paper’s main mechanism (finite-data correlation resolution → effective context window → stepwise representation emergence) and its empirical checks on synthetic plus real-text datasets. Remaining issues are calibration/metadata-level, not core distortions.
- Whether this sample should be moved to `bad/`: **No**


---

## Appendix C — Trial QA Entry (Report 8 from 3+44n set)

### QA Record
- paper_id: 2405.11204
- report_file: 2405.11204_learning-from-imperfect-human-feedback-a-tale-from-corruption-robust-dueling_20260211_161542.md
- source_file: 2405.11204.txt
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha (GLM-5 Preview)
- mode: single-trial append

### Score (current core-focused rubric)
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **26**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **15**
- Metadata & Formatting Hygiene (5): **5**
- Unsupported/Fabricated Content (15): **12**
- Evidence Anchoring (10): **10**
- **Total (100): 88**
- **Verdict: PASS-Conditional**

### Findings (core-focused)
1. [Severity: S2] Mild overreach in framing LIHF difficulty vs arbitrary corruption
   - Report evidence: “showing LIHF can be as hard as arbitrary corruption.”
   - Source evidence: Theorem 1 lower bound matches order, but paper uses qualified language (“the answer seems to be no”, “hint”) and notes setting differences (source around Theorem 1 discussion, lines ~292–320).
   - Impact: Slight inflation of certainty; core takeaway still directionally correct.
   - Fix guidance: Phrase as “matches known order and suggests comparable hardness in this setting.”

2. [Severity: S2] Unsupported “Open Questions: None” entry
   - Report evidence: “Open Questions the Paper Calls Out: None.”
   - Source evidence: paper explicitly states “intriguing open question” regarding tightness under arbitrary corruption (around lines ~443 onward).
   - Impact: Misses nuance but not a core factual failure.
   - Fix guidance: Include at least one explicit open question from the discussion section.

### Core Evidence Anchors
- Theorem 1 lower bound: Reg_T ≥ Ω(d max{sqrt(T), T^rho}) even when rho known (source lines ~292–307).
- RoSMID learning-rate form eta_rho = sqrt(log T)/(d T^{max{0.5,rho}}) (source lines ~429–432).
- Theorem 2 upper bound: Reg_T ≤ O~(d max{sqrt(T), T^rho}) (source lines ~437–443).
- LIHF model: corruption scale O(t^{rho-1}) and motivation from improving human feedback (source lines ~18, ~431, and Definition 1 section around lines ~236–260).

### Final Judgment
- Reliability summary: Core technical summary is strong and numerically aligned on central bounds and algorithm design. Main residual risk is claim calibration and one unsupported “no open questions” statement.
- Move to `bad/`: **No** (under current core-focused rubric).

---

## Appendix C — New Model Spot Checks (research_paper_analysis_v2)

### C.1 Report 7 (offset rule: 3 + 44n, n=6)

### QA Record
- paper_id: 2404.03622
- report_file: `2404.03622_mind-s-eye-of-llms-visualization-of-thought-elicits-spatial-reasoning-in-large-language-models_20260211_162907.md`
- source_file: `2404.03622.txt`
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha / GLM-5 Preview (relative baseline context)

### Score
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **27**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **17**
- Metadata & Formatting Hygiene (5): **5**
- Unsupported/Fabricated Content (15): **15**
- Evidence Anchoring (10): **10**
- **Total (100): 94**
- Verdict: **PASS-High**

### Findings
1. [Severity: S2] Mild overreach in generalization language
   - Report evidence: claims broad superiority and human-like spatial reasoning implications in summary sections.
   - Source evidence (quote/line anchor): source shows strong gains in evaluated tasks, but also explicitly notes imperfect performance and task-dependent underperformance (lines 307–310, 423–426).
   - Impact: does not invalidate core result, but can overstate general applicability.
   - Fix guidance: keep claims scoped to evaluated tasks and note known underperformance regimes.

### Core Evidence Anchors
- Core method claim supported: VoT is proposed to visualize reasoning traces and guide subsequent steps (lines 14–17, 194–200).
- Task scope supported: natural language navigation, visual navigation, visual tiling (lines 17, 92–93).
- Key numeric claim supported: GPT-4 VoT outperforms GPT-4 w/o Viz by **23.5%** in natural language navigation (lines 302–304).
- Scaling claim supported with caveat: larger models generally improve, while weaker models show random-guessing behavior (lines 436–444).

### Final Judgment
- This report is core-faithful and quantitatively reliable on central claims for the evaluated benchmark setup.
- Main caution is claim calibration: conclusions should remain bounded to the tested task suite.
- Move to `bad/`: **No**.

---

## Appendix C — Trial Run: Report 6 (Offset rule 3 + 44n)

### QA Record
- paper_id: 2402.18139
- report_path: 2402.18139_cause-and-effect-can-large-language-models-truly-understand-causality_20260211_161237.md
- source_path: 2402.18139.txt
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha (GLM-5 Preview) output under evaluation
- timestamp: 2026-02-11

### Score
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **27**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **17**
- Metadata & Formatting Hygiene (5): **4**
- Unsupported/Fabricated Content (15): **15**
- Evidence Anchoring (10): **10**
- **Total (100): 93**
- Verdict: **PASS-High**

### Findings
1. [Severity: S3] Minor metadata issue (reference count quality)
   - Report evidence: Quick Facts lists "Reference count: 4".
   - Source evidence (quote/line anchor): source has a full references section beginning at line 665 with many entries.
   - Impact: Low under current rubric (metadata low weight).
   - Fix guidance: Either compute references robustly or omit reference count.

2. [Severity: S2] Mild overreach in application framing
   - Report evidence: states practical promise for healthcare/public policy/scientific research.
   - Source evidence (quote/line anchor): intro discusses potential impact in healthcare/public policy (page 1–2), but does not provide deployment validation.
   - Impact: Non-core distortion; framed as potential rather than proven deployment.
   - Fix guidance: Keep this as "potential" and explicitly mark as non-validated application claim.

### Major Judgment Anchors (for criterion 7)
- Core factual anchor: CARE-CA architecture with ConceptNet + counterfactual module (source lines 17–38, 219–236).
- Core numeric anchor: CausalNet CARE-CA mean accuracy 94.6 (source lines 459–461; table line 650).
- Claim-discipline anchor: potential application language appears in intro, not deployment proof (source page 1–2).
- Unsupported/fabrication anchor: no fabricated central method/result found; CKI/CRE/CAPM and 6-dataset setup are present (source lines 280–313).

### Final Judgment
- Reliability summary: Core method and headline result claims are well aligned with source. Quantitative headline (94.6 on CausalNet) is correct and central. Remaining issues are mostly low-impact metadata quality and slight application-level overreach.
- Whether this sample should be moved to `bad/`: **No**

---

## Appendix C — Incremental Single-Report Checks (New Model)

### QA Record
- paper_id: 2402.08078
- report_path: 2402.08078_large-language-models-as-agents-in-two-player-games_20260211_162624.md
- source_path: 2402.08078.txt
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha (GLM-5 Preview)
- timestamp: 2026-02-11

### Score
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **26**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **18**
- Metadata & Formatting Hygiene (5): **5**
- Unsupported/Fabricated Content (15): **14**
- Evidence Anchoring (10): **10**
- **Total (100): 93**
- Verdict: **PASS-High**

### Findings
1. [Severity: S2] Mild overreach in explanatory certainty
   - Report evidence: "Offers explanations for phenomena like chain-of-thought reasoning, hallucination, and in-context learning through game-theoretic lens."
   - Source evidence (quote/line anchor): Source frames this as a position-paper perspective and exploratory interpretation (e.g., lines 49, 106–107, 791).
   - Impact: Slightly stronger confidence than source warrants; not core-invalidating.
   - Fix guidance: Prefer "proposes hypotheses/explanations" wording.

2. [Severity: S3] Title wording drift in heading
   - Report evidence: Heading uses "Large Language Models as Two-Player Games".
   - Source evidence (quote/line anchor): Title is "Large Language Models as Agents in Two-Player Games" (line 1).
   - Impact: Cosmetic only.
   - Fix guidance: Keep heading exactly aligned with source title.

### Anchors for major judgments
- Core factual accuracy anchor: abstract + intro framing of unified LLM training via two-player-game lens (lines 7–20, 49–61).
- Core numeric sanity anchor: no central quantitative benchmark claims in source; report also avoids core quantitative overclaims.
- Claim-discipline anchor: source is a "position paper" with open questions/future work emphasis (lines 49, 122, 524, 791).
- Unsupported/fabrication anchor: Q-A / Q-C-A and long-term value-function ideas are explicitly discussed in source (lines 690, 700, 733–749).

### Final Judgment
- Reliability summary: The report captures the paper’s central thesis and limitations well. Core claims are aligned with source framing, with only mild overstatement in explanatory certainty. Suitable for synthesis use.
- Whether this sample should be moved to `bad/`: **No**

---

## Appendix C — Incremental Trial Checks

### Trial 1 — Sample #4 from (3 + 44n): `2402.05284`

### QA Record
- paper_id: 2402.05284
- report_file: `2402.05284_analyzing-adversarial-inputs-in-deep-reinforcement-learning_20260211_160840.md`
- source_file: `2402.05284.txt`
- evaluator_model: gpt-5.3-codex-high rubric
- comparison_model: Pony Alpha / GLM-5 Preview (report under test)
- mode: append-only trial (no rubric-file reread before write)

### Score
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **24**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **11**
- Metadata & Formatting Hygiene (5): **4**
- Unsupported/Fabricated Content (15): **6**
- Evidence Anchoring (10): **10**
- **Total (100): 75**
- Verdict: **PASS-Conditional**

### Findings
1. [Severity: S1] Activation-function conclusion is overstated/misaligned with source conclusions.
   - Report evidence: claims Swish is more susceptible (Quick Facts/Key Results/Mechanism claims).
   - Source evidence: source reports **no clear correlation** for activation type overall, and mixed results by benchmark (lines ~1898–1914).
   - Impact: can mislead architecture hardening decisions if taken literally.
   - Fix guidance: downgrade to “mixed evidence; size effect clearer than activation-type effect.”

2. [Severity: S2] One quoted “abstract” bullet appears synthetic and stronger than source abstract framing.
   - Report evidence: mechanism anchor quotes “3) ... activation functions (e.g., Swish) tend to be more susceptible ...”.
   - Source evidence: abstract emphasizes Adversarial Rate and formal-analysis contributions; does not present that exact bulletized claim (page 1 abstract region).
   - Impact: inflates certainty of a secondary claim.
   - Fix guidance: quote directly from source sentences; avoid reconstructed bullet claims.

3. [Severity: S3] Metadata quality drift (low impact under current rubric).
   - Report evidence: `Reference count: 16`.
   - Source evidence: references section contains substantially more entries (starts near line 2061 and spans multiple pages).
   - Impact: low (non-core).
   - Fix guidance: treat reference count as optional or auto-calculate.

### Final Judgment
- Core paper framing is mostly correct (Adversarial Rate, ProVe/Counting-ProVe, benchmark setup, and concentration/temporal findings).
- Main risk is claim calibration around activation-function effects; core quantitative claims are otherwise serviceable.
- Move to `bad/`: **No** (under current core-focused rubric), but mark as **calibration-needed**.

---

## Appendix C — New Model Spot Checks (rpa_v2)

### C.1 QA Record — 2402.01831 (offset sample #3)

- paper_id: `2402.01831`
- report_file: `2402.01831_audio-flamingo-a-novel-audio-language-model-with-few-shot-learning-and-dialogue-abilities_20260211_160836.md`
- source_file: `2402.01831.txt`
- evaluator_model: `gpt-5.3-codex-high`
- comparison_model: `Pony Alpha / GLM-5 Preview` (report under evaluation)

#### Score (current rubric)
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **27**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **16**
- Metadata & Formatting Hygiene (5): **4**
- Unsupported/Fabricated Content (15): **12**
- Evidence Anchoring (10): **10**
- **Total (100): 89**
- **Verdict: PASS-Conditional**

#### Major judgments + anchors
1. **Core factual accuracy: mostly strong**
   - Report evidence: lines 36–39 describe sliding-window extractor, cross-attention conditioning, gated xattn-dense, 2-stage training.
   - Source anchors: lines 64, 72, 280–301, 321, 332, 379 confirm these components.

2. **Core quantitative claims: supported**
   - Report evidence: line 39 states ~5.9M audio-text pairs and MLE + weighted losses.
   - Source anchors: line 82 and Table-1 line 480 (~5.9M pairs); lines 332 and 379 (MLE, weighted mixture objective).

3. **Claim discipline: minor overreach risk**
   - Report evidence: line 42 states state-of-the-art on diverse tasks (broad phrasing).
   - Source anchors: line 98 states SOTA on **most** tasks; dialogue advantage supported at lines 748 and 757.
   - Impact: slight breadth inflation, not core-distorting.

4. **Unsupported/peripheral issue present**
   - Report evidence: lines 75–78 claim critical details are underspecified and evaluation details limited.
   - Source anchors: code availability line 21; dataset/training detail tables in appendix lines 1335–1336 and generated-dialogue details lines 1386–1390.
   - Impact: limitation phrasing appears somewhat overstated relative to paper detail level.

#### Final judgment
- This report is broadly reliable on core method and result direction.
- Main caution is calibration language (broad SOTA phrasing and somewhat overstated limitations).
- Under the current core-focused rubric, this sample is usable with light reviewer oversight.
- Move to `bad/`: **No**.

---

## Appendix C — Trial Single-Report Check (Report 2 of 10)

### QA Record
- paper_id: 2401.13875
- report_path: 2401.13875_is-temperature-sample-efficient-for-softmax-gaussian-mixture-of-experts_20260211_160836.md
- source_path: 2401.13875.txt
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha (GLM-5 Preview)
- timestamp: 2026-02-11 22:24:21 PST

### Score (current rubric)
- Paper Match Sanity Check (5): **5**
- Core Factual Accuracy (30): **24**
- Quantitative Claims Sanity (15): **15**
- Claim Discipline / Overreach (20): **14**
- Metadata & Formatting Hygiene (5): **4**
- Unsupported/Fabricated Content (15): **12**
- Evidence Anchoring (10): **10**
- **Total (100): 84**
- Verdict: **PASS-Conditional**

### Findings
1. [Severity: S2] Core result is represented accurately.
   - Report evidence: states temperature can yield O(1/log n) and activation dense-to-sparse recovers polynomial rates.
   - Source evidence: abstract lines 24–34; concluding remarks lines 1214–1218.
   - Impact: core takeaway preserved.
   - Fix guidance: none required.

2. [Severity: S2] Activation used in simulation appears mismatched.
   - Report evidence: claims simulation validates with tanh activation.
   - Source evidence: simulation section references sigmoid (lines 1025, 1042, 1073, 1101).
   - Impact: peripheral factual drift (does not change central theorem-level claims).
   - Fix guidance: replace tanh mention with sigmoid (or list both only if source supports both).

3. [Severity: S2] Open-questions section underreports future directions.
   - Report evidence: “Open Questions the Paper Calls Out: None identified.”
   - Source evidence: concluding remarks include explicit future research directions (lines 1221 onward).
   - Impact: weakens completeness/calibration.
   - Fix guidance: include the two future-direction items from conclusion.

4. [Severity: S2] Mechanistic explanation includes unsupported extra interpretation.
   - Report evidence: claims activation acts as regularizer and resolves ill-conditioned optimization landscape.
   - Source evidence: paper attributes improvement to disappearance of PDE-induced interactions under independence conditions (lines 1214–1219), without explicit regularizer framing.
   - Impact: mild over-interpretation.
   - Fix guidance: tie mechanism wording directly to PDE interaction removal + independence conditions.

### Final Judgment
- Reliability summary: The report is directionally strong on the core theorem-level message and quantitative headline rates. The main issues are calibration/wording drift (simulation activation detail, open-questions omission, and mild mechanistic over-interpretation), not central claim failure.
- Whether this sample should be moved to 
  bad/: No


---

## Appendix C — Trial Check (new model paper set)

### QA Record
- paper_id: 2401.02509
- report_path: 2401.02509_memory-consciousness-and-large-language-model_20260211_162326.md
- source_path: 2401.02509.txt
- evaluator_model: gpt-5.3-codex-high
- comparison_model: Pony Alpha (GLM-5 Preview)
- timestamp: 2026-02-11

### Score
- Paper Match Sanity Check (5): 5
- Core Factual Accuracy (30): 27
- Quantitative Claims Sanity (15): 15
- Claim Discipline / Overreach (20): 19
- Metadata & Formatting Hygiene (5): 4
- Unsupported/Fabricated Content (15): 15
- Evidence Anchoring (10): 10
- **Total (100): 95**
- Verdict: PASS-High

### Findings
1. [Severity: S3] Peripheral metadata inaccuracy on reference count
   - Report evidence: Quick Facts lists `Reference count: 7`.
   - Source evidence (quote/line anchor): source contains a multi-page references section beginning at `[PAGE 9] References` and continuing through `[PAGE 11]`.
   - Impact: Low (non-core).
   - Fix guidance: Recompute reference counts from parsed bibliography instead of heuristic extraction.

### Core Evidence Anchors (major judgments)
- Core factual accuracy: Abstract states duality conjecture and consciousness-as-emergent speculation (`[PAGE 1]` lines 7–12); report matches this framing.
- Core factual accuracy: Memory-system mapping is explicitly presented in Table 2 (`[PAGE 3]`, “Input context ⇔ Episodic”, etc.); report mirrors mapping.
- Claim discipline: Paper is explicitly speculative (“we speculate…”, `[PAGE 1]` and Section 5); report preserves speculative language (“might/could”).
- Unsupported/fabricated check: SSM/long-context linkage appears in Section 5 (`[PAGE 6–7]`, discussion of RWKV/Mamba and context-length limits); report’s SSM mention is supported.
- Quantitative core sanity: No core-result numeric inflation detected; report avoids introducing unsupported headline quantitative gains.

### Final Judgment
- Reliability summary: Core summary is aligned with source and preserves the paper’s conjectural tone. Main contribution, mechanism framing, and limitations are represented faithfully. Only a low-impact metadata count issue was found.
- Whether this sample should be moved to `bad/`: No
