---
ver: rpa2
title: Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer
arxiv_id: '2505.08327'
source_url: https://arxiv.org/abs/2505.08327
tags:
- task
- pruning
- proposed
- learning
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles continual learning (CL) under the class-incremental
  learning (CIL) setting, where task identities are unavailable during inference.
  The central problem is that large pre-trained models, while effective for mitigating
  catastrophic forgetting, incur high inference costs due to their computational complexity.
---

# Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer

## Quick Facts
- arXiv ID: 2505.08327
- Source URL: https://arxiv.org/abs/2505.08327
- Reference count: 40
- Primary result: KD-based CIL framework improves accuracy by 20+ percentage points while reducing inference FLOPs by 50×

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in class-incremental learning (CIL) by integrating model compression techniques into continual learning frameworks. The authors propose two complementary approaches: pruning-based compression (pre-pruning and post-pruning) and knowledge distillation-based compression. Both methods aim to maintain high accuracy while significantly reducing inference computational costs. Experiments demonstrate that the knowledge distillation approach consistently outperforms strong CIL baselines like LwF, iCaRL, and SS-IL by over 20 percentage points in accuracy while achieving up to 50× reduction in FLOPs.

## Method Summary
The authors propose two frameworks for compressed knowledge transfer in CIL. The pruning-based framework applies model pruning either before CIL training (pre-pruning) or after each task (post-pruning) to reduce inference complexity. The knowledge distillation framework employs a teacher-student architecture where a larger pre-trained teacher model transfers downstream-relevant knowledge to a compact student model during CIL training. The KD framework uses a combination of distillation loss (based on KL divergence between teacher and student predictions) and classification loss, with temperature scaling to control knowledge transfer granularity. Both frameworks are evaluated under the standard CIL setting where task identities are unavailable during inference.

## Key Results
- KD-based framework achieves 20+ percentage point accuracy improvements over baselines (LwF, iCaRL, SS-IL) on CIFAR-100, FGVC Aircraft, and Cars datasets
- Inference FLOPs reduced by approximately 50× compared to full models
- Pruning-based framework shows similar accuracy gains under low pruning ratios but offers less compression
- KD framework's flexibility to use different architectures enables stronger efficiency gains

## Why This Works (Mechanism)
The success of these frameworks stems from leveraging pre-existing knowledge in large pre-trained models while reducing inference complexity. In the pruning framework, removing redundant parameters maintains task performance while reducing computational load. The KD framework works by transferring the rich, discriminative knowledge from a large teacher model to a compact student, allowing the student to benefit from the teacher's generalization capabilities without incurring the teacher's computational cost during inference.

## Foundational Learning
- **Catastrophic forgetting**: Neural networks rapidly forget previous tasks when trained on new ones - fundamental challenge in CIL that necessitates specialized approaches
- **Knowledge distillation**: Training a smaller model to mimic a larger one's output distribution - enables transferring generalization capabilities while reducing model size
- **Model pruning**: Removing redundant parameters while maintaining accuracy - provides direct inference efficiency improvements
- **Class-incremental learning**: Learning new classes sequentially without task identity during inference - most realistic but challenging CL setting
- **Temperature scaling in KD**: Controls the softness of probability distributions during distillation - higher temperatures reveal more information about relative class similarities
- **Pre-trained models**: Models trained on large datasets before task-specific fine-tuning - provide strong initialization and generalization capabilities

## Architecture Onboarding
- **Component map**: Pre-trained model → Pruning/KD framework → Compressed student model → Inference
- **Critical path**: Teacher model (KD) or full model (pruning) → Compression step → Student/compressed model → Inference
- **Design tradeoffs**: KD offers better accuracy-efficiency trade-off but requires maintaining larger teacher during training; pruning is simpler but less effective at high compression ratios
- **Failure signatures**: Accuracy drops when pruning ratio exceeds model capacity; KD fails when teacher-student architecture mismatch is too large
- **First experiments**: 1) CIFAR-100 with standard ResNet architectures, 2) Fine-grained classification on Aircraft/Cars datasets, 3) Ablation studies on compression ratios

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger, more complex datasets and architectures remains uncertain
- Training-time computational overhead of KD framework not fully quantified
- Performance under non-i.i.d. data streams and noisy task boundaries not evaluated
- Pruning framework effectiveness diminishes at higher compression ratios

## Confidence
- Accuracy improvements over baselines: High
- Inference efficiency gains: High
- Scalability to larger datasets: Medium
- Training-time overhead quantification: Low
- Robustness to real-world data distributions: Low

## Next Checks
1. Evaluate the KD-based framework on larger-scale datasets (e.g., ImageNet) and more complex architectures (e.g., vision transformers or large language models) to assess scalability and robustness.
2. Quantify the training-time computational overhead of the KD-based framework, including memory and energy costs, to provide a complete picture of its practical efficiency.
3. Test the frameworks under non-i.i.d. data streams and with noisy or missing task boundaries to evaluate robustness in real-world deployment scenarios.