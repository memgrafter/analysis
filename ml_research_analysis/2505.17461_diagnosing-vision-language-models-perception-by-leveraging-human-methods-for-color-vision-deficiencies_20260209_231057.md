---
ver: rpa2
title: Diagnosing Vision Language Models' Perception by Leveraging Human Methods for
  Color Vision Deficiencies
arxiv_id: '2505.17461'
source_url: https://arxiv.org/abs/2505.17461
tags:
- color
- vision
- ishihara
- protanopia
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigated whether large-scale vision-language models
  (LVLMs) can simulate variation in human color perception using the Ishihara Test.
  Through generation, confidence, and internal representation analyses, we found that
  LVLMs, despite possessing factual knowledge about color vision deficiencies, fail
  to reproduce perceptual outcomes experienced by individuals with CVDs and instead
  default to normative color perception.
---

# Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies

## Quick Facts
- arXiv ID: 2505.17461
- Source URL: https://arxiv.org/abs/2505.17461
- Reference count: 32
- Primary result: Large-scale vision-language models fail to reproduce perceptual outcomes experienced by individuals with color vision deficiencies despite possessing factual knowledge about them.

## Executive Summary
This work investigates whether large-scale vision-language models (LVLMs) can simulate variation in human color perception using the Ishihara Test, a standardized diagnostic tool for color vision deficiencies. Through systematic generation, confidence, and internal representation analyses, the study reveals that LVLMs consistently fail to reproduce perceptual experiences of individuals with CVDs. Despite possessing factual knowledge about color vision deficiencies, these models default to normative color perception and show little improvement even when provided with additional linguistic descriptions or visual exemplars.

The findings indicate a fundamental limitation in current LVLMs: they lack mechanisms for representing alternative perceptual experiences. This has significant implications for accessibility and inclusive deployment of multimodal AI systems, as these models cannot adequately simulate or understand perceptual variations that affect human users. The research highlights the gap between possessing factual knowledge and being able to simulate experiential differences in perceptual understanding.

## Method Summary
The study employs the Ishihara Test as a diagnostic framework to evaluate LVLMs' ability to simulate color vision deficiency perceptions. Researchers conducted systematic analyses across three dimensions: generation capabilities (how models describe color patterns), confidence assessments (how certain models are about their interpretations), and internal representation analysis (how models process color information). The experiments tested whether providing additional contextual information through linguistic descriptions or visual exemplars could improve the models' ability to simulate altered color vision. Multiple LVLMs were evaluated using standardized Ishihara plates designed to diagnose different types of color vision deficiencies.

## Key Results
- LVLMs fail to reproduce perceptual outcomes experienced by individuals with color vision deficiencies
- Models consistently default to normative color perception despite possessing factual knowledge about CVDs
- Additional linguistic descriptions or visual exemplars show little improvement in simulating altered color vision
- Models demonstrate low confidence when attempting to simulate CVD perceptions

## Why This Works (Mechanism)
The study's approach works by leveraging a standardized human diagnostic tool (Ishihara Test) to probe LVLMs' perceptual understanding. By using a test specifically designed to reveal differences in human color perception, researchers can systematically evaluate whether models can simulate these variations. The mechanism relies on comparing model outputs against expected outcomes for both normative and deficient color vision, revealing whether models can flexibly represent alternative perceptual experiences or are locked into a single perceptual framework.

## Foundational Learning

Color Vision Deficiency (CVD): A condition affecting color perception, including red-green and blue-yellow deficiencies. Why needed: Provides the perceptual variation framework being tested. Quick check: Understanding the different types of CVD and how they affect perception of standard Ishihara plates.

Ishihara Test: A standardized diagnostic tool using colored dot patterns to identify color vision deficiencies. Why needed: Serves as the benchmark for evaluating perceptual simulation capabilities. Quick check: Familiarity with how Ishihara plates work and what they reveal about color perception.

Vision-Language Models (LVLMs): Multimodal AI systems that process both visual and textual information. Why needed: The target system being evaluated for perceptual understanding. Quick check: Understanding the basic architecture and capabilities of modern LVLMs.

Perceptual Simulation: The ability to represent and generate alternative perceptual experiences. Why needed: Core capability being tested in the models. Quick check: Distinguishing between factual knowledge and experiential understanding.

## Architecture Onboarding

Component Map: Input Images -> Vision Encoder -> Cross-modal Fusion -> Language Decoder -> Output Text
Critical Path: Image processing through vision encoder directly influences the model's ability to generate accurate color descriptions
Design Tradeoffs: Models prioritize accurate color recognition over flexible perceptual simulation, favoring normative interpretation
Failure Signatures: Consistent inability to deviate from standard color perception, low confidence scores when attempting CVD simulation
First Experiments:
1. Test model responses to Ishihara plates with varying difficulty levels
2. Evaluate performance with and without textual prompts describing CVD experiences
3. Compare internal representations of color information between normative and CVD simulation attempts

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on a single type of perceptual test (Ishihara) which may not capture full range of LVLMs' perceptual capabilities
- Results may be specific to color perception tests rather than indicating a general limitation in simulating perceptual variations
- Implications for accessibility and inclusive deployment are discussed but not fully explored in broader contexts

## Confidence

High confidence: Models consistently fail to simulate CVD perception and default to normative color perception across generation, confidence, and internal representation analyses.

Medium confidence: Conclusion that LVLMs lack mechanisms for representing alternative perceptual experiences, as this may be specific to color perception tests rather than a general limitation.

Low confidence: Implications for accessibility and inclusive deployment in multimodal AI systems, as these depend on broader contexts not fully explored in this study.

## Next Checks
1. Test LVLMs with a wider range of perceptual variation scenarios beyond color vision deficiencies, including other sensory modalities and perceptual conditions.

2. Evaluate model performance on more naturalistic, real-world images with color variations rather than standardized test plates.

3. Investigate whether fine-tuning or specialized training on perceptual variation data could improve LVLMs' ability to simulate alternative perceptual experiences.