---
ver: rpa2
title: Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented
  Reasoning
arxiv_id: '2505.22095'
source_url: https://arxiv.org/abs/2505.22095
tags:
- reasoning
- r1-router
- answer
- sub-question
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R1-Router is a novel framework that enables multimodal large language
  models to dynamically determine when and where to retrieve external knowledge during
  reasoning. Unlike static retrieval pipelines, R1-Router generates intermediate queries
  based on the current reasoning state, routes them to the most suitable knowledge
  base (text, image, or table), and iteratively accumulates evidence to answer the
  original query.
---

# Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning

## Quick Facts
- arXiv ID: 2505.22095
- Source URL: https://arxiv.org/abs/2505.22095
- Reference count: 40
- Key outcome: R1-Router outperforms baselines by over 7% in F1-Recall on multimodal open-domain QA benchmarks

## Executive Summary
R1-Router introduces a dynamic, step-wise retrieval-augmented reasoning framework that enables multimodal large language models to adaptively determine when and where to retrieve external knowledge. Unlike static retrieval pipelines, it generates intermediate queries based on reasoning state, routes them to the most suitable knowledge base (text, image, or table), and iteratively accumulates evidence. The method employs Step-wise Group Relative Policy Optimization (Step-GRPO) to assign step-specific rewards, guiding the model to acquire relevant information and produce accurate answers. Experiments demonstrate substantial improvements in F1-Recall and more efficient, targeted retrieval across modalities.

## Method Summary
R1-Router dynamically routes queries across multiple knowledge bases by iteratively generating intermediate queries based on the current reasoning state. It leverages Step-wise Group Relative Policy Optimization (Step-GRPO), a novel policy optimization technique that assigns rewards at each reasoning step, encouraging the model to retrieve appropriate information and improve answer accuracy. The framework is evaluated on open-domain QA benchmarks involving text, images, and tables, demonstrating significant performance gains over static retrieval methods.

## Key Results
- R1-Router outperforms baseline models by over 7% in F1-Recall on multimodal open-domain QA benchmarks
- The framework can adaptively leverage diverse knowledge bases, reducing unnecessary retrievals and improving efficiency
- Step-GRPO effectively guides the model to acquire appropriate information and produce accurate answers through step-wise rewards

## Why This Works (Mechanism)
R1-Router's success stems from its ability to dynamically generate and route intermediate queries based on the current reasoning state, rather than relying on a fixed retrieval pipeline. By using Step-GRPO, the model receives step-specific feedback, enabling it to learn which knowledge sources are most useful at each stage of reasoning. This adaptive approach reduces irrelevant retrievals and focuses computational effort on acquiring evidence that directly contributes to the final answer.

## Foundational Learning
- **Policy Optimization**: Learning to select actions (retrieval queries) that maximize cumulative reward; needed for training the router to make effective retrieval decisions; quick check: does the model improve over training steps?
- **Multimodal Integration**: Combining information from text, images, and tables; essential for answering diverse queries; quick check: can the model handle mixed-modality inputs?
- **Step-wise Rewards**: Assigning rewards at each reasoning step to guide learning; crucial for shaping the retrieval strategy; quick check: do intermediate rewards correlate with final answer quality?
- **Knowledge Base Routing**: Directing queries to the most appropriate external source; improves retrieval efficiency and relevance; quick check: is retrieval accuracy higher than random or static baselines?

## Architecture Onboarding
**Component Map**: LLM Reasoning State -> Intermediate Query Generator -> Knowledge Base Router -> External KB (Text/Image/Table) -> Retrieved Evidence -> LLM Answer Generator
**Critical Path**: Reasoning State → Intermediate Query → KB Selection → Retrieval → Answer Generation
**Design Tradeoffs**: Dynamic routing offers flexibility but increases computational overhead; Step-GRPO improves accuracy but may require more training data and careful reward shaping
**Failure Signatures**: Retrieval failures when routing to inappropriate KBs, error propagation from incorrect intermediate queries, and increased latency due to iterative retrieval
**First Experiments**: (1) Ablation of Step-GRPO to isolate its impact on performance, (2) Testing on multi-hop reasoning tasks to evaluate scalability, (3) Measuring end-to-end latency and computational overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to single-step query routing and open-domain QA, leaving scalability to multi-hop reasoning uncertain
- Ablation study for Step-GRPO is limited to two baselines, making it difficult to assess its specific contribution
- No discussion of computational overhead or latency introduced by dynamic routing, which may hinder practical deployment

## Confidence
- Core performance claims: Medium
- Ablation and mechanism claims: Medium
- Multimodal generalization: Medium

## Next Checks
1. Conduct ablation studies isolating the impact of Step-GRPO versus other architectural components on performance
2. Test R1-Router on multi-hop reasoning tasks to evaluate scalability and robustness in chained retrieval scenarios
3. Measure computational overhead and end-to-end latency to assess practical deployment feasibility