---
ver: rpa2
title: Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance
arxiv_id: '2512.23461'
source_url: https://arxiv.org/abs/2512.23461
tags:
- reward
- bias
- arxiv
- server
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles reward hacking in RLHF caused by inductive biases
  in human preference data. The proposed DIR method uses information theory to maximize
  mutual information between reward model outputs and true human preferences while
  minimizing the correlation between predictions and irrelevant bias attributes like
  response length, sycophancy, or formatting.
---

# Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance

## Quick Facts
- arXiv ID: 2512.23461
- Source URL: https://arxiv.org/abs/2512.23461
- Reference count: 40
- Primary result: DIR reduces reward model bias (length correlation from 0.533 to 0.468) while improving downstream RLHF performance (ArenaHard win rate from 51.9% to 54.3%)

## Executive Summary
This paper addresses reward hacking in reinforcement learning from human feedback (RLHF) caused by inductive biases in human preference data. The proposed DIR method uses information theory to maximize mutual information between reward model outputs and true human preferences while minimizing the correlation between predictions and irrelevant bias attributes like response length, sycophancy, or formatting. The method employs variational bounds on mutual information and shows improved generalization across multiple benchmarks while maintaining or improving downstream RLHF performance.

## Method Summary
DIR trains reward models by simultaneously maximizing preference learning (via Bradley-Terry model) and minimizing mutual information between rewards and bias attributes. The method uses a variational bound (CLUB upper bound) to approximate mutual information, implemented through a debiasing head that estimates bias attributes from representation differences. Training proceeds iteratively: first updating the bias estimator q_ψ, then updating the reward model with the total loss combining preference and debiasing objectives. The approach scales to multiple concurrent biases and outperforms alternatives like PoE, ALBM, and InfoRM.

## Key Results
- Length bias reduction: Pearson correlation drops from 0.533 to 0.468 on Skywork dataset
- Improved generalization: RM-Bench total accuracy increases from 68.10 to 70.18
- Better downstream performance: ArenaHard win rate increases from 51.9% to 54.3% against baselines
- Multiple bias handling: Successfully debiases length, sycophancy, and formatting biases concurrently

## Why This Works (Mechanism)
The method works by explicitly modeling the relationship between reward predictions and bias attributes using information-theoretic principles. By maximizing mutual information with true preferences while minimizing it with bias attributes, DIR learns representations that capture preference-relevant information while being invariant to spurious correlations. The CLUB bound provides a tractable approximation for the intractable mutual information term, enabling practical implementation.

## Foundational Learning
- **Mutual Information**: Measures the dependence between two random variables; needed to quantify and minimize bias influence on rewards. Quick check: MI(X,Y) = 0 iff X and Y are independent.
- **Variational Bounds**: Approximate intractable expectations; needed to make mutual information optimization computationally feasible. Quick check: Evidence Lower Bound (ELBO) provides a lower bound on log-likelihood.
- **Representation Difference**: Uses h_w - h_l as input to debiasing head; needed to capture relative information between preferred and non-preferred responses. Quick check: Difference amplifies preference-relevant features while suppressing common elements.
- **CLUB Bound**: Provides an upper bound on mutual information; needed for practical optimization of the debiasing objective. Quick check: CLUB bound is tighter than naive sampling-based estimators.
- **Bradley-Terry Model**: Probabilistic choice model for pairwise preferences; needed for preference learning component. Quick check: Outputs probability that one item is preferred over another.

## Architecture Onboarding

**Component Map**: RM -> [Preference Head + Debiasing Head] <- Bias Estimator q_ψ

**Critical Path**: Input responses → RM backbone → Representation difference → (1) Preference head for Bradley-Terry loss, (2) Debiasing head for CLUB bound estimation

**Design Tradeoffs**: 
- λ balances debiasing strength vs. preference learning; too high causes over-debiasing, too low insufficient bias removal
- CLUB bound approximation trades exactness for computational tractability
- Iterative bias estimator training vs. joint optimization affects convergence stability

**Failure Signatures**:
- Debiasing head collapses to always predict one class (monitor q_ψ accuracy)
- Over-debiasing degrades preference learning (sharp drop in RewardBench accuracy)
- Slow convergence due to bias estimator training instability

**3 First Experiments**:
1. Verify CLUB bound computation on synthetic correlated data with known mutual information
2. Test debiasing head training stability with varying λ values on toy bias problem
3. Validate representation difference captures relative preference information (h_w - h_l should correlate with preference labels)

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness relies heavily on synthetic bias contamination experiments; natural bias performance not directly evaluated
- Critical hyperparameter λ not extensively tuned across different bias types and dataset sizes
- CLUB upper bound approximation introduces additional error not quantified against alternatives
- Scalability to many concurrent biases and their interaction effects not fully characterized

## Confidence
**High confidence**: Core theoretical framework connecting mutual information maximization to debiasing is sound. Implementation details sufficiently specified for reproduction, and evaluation methodology using multiple benchmarks provides robust assessment.

**Medium confidence**: Claims of outperforming existing methods are supported by reported experiments, but comparisons are made under specific experimental conditions with synthetic biases. Real-world generalization remains to be validated.

**Medium confidence**: Scalability claim to multiple concurrent biases is demonstrated but only through limited experiments. Interaction effects between different debiasing objectives are not fully characterized.

## Next Checks
1. Validate synthetic bias injection methodology by testing whether RM-Bench accuracy drops significantly when evaluating on data with artificially added biases (e.g., all responses in a subset artificially lengthened).
2. Perform hyperparameter sensitivity analysis across λ values (0.1, 1.0, 10) on multiple bias types to establish robustness of the chosen λ=1.0 setting.
3. Test DIR on naturally occurring biases by analyzing correlation patterns in real preference datasets before and after debiasing, comparing against synthetic bias results to assess ecological validity.