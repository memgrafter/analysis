---
ver: rpa2
title: Large Language Model enabled Mathematical Modeling
arxiv_id: '2510.19895'
source_url: https://arxiv.org/abs/2510.19895
tags:
- code
- execution
- answer
- print
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research investigates the use of DeepSeek-R1, an economically
  efficient large language model, to automate mathematical modeling in operations
  research (OR) tasks, particularly for supply chain optimization. By integrating
  natural language understanding and code generation, DeepSeek-R1 aims to bridge the
  gap between real-world problem descriptions and solvable mathematical formulations,
  traditionally reliant on domain expertise.
---

# Large Language Model enabled Mathematical Modeling

## Quick Facts
- arXiv ID: 2510.19895
- Source URL: https://arxiv.org/abs/2510.19895
- Reference count: 40
- DeepSeek-R1 with LLM-as-a-Judge achieves 92.3% accuracy on NL4OPT benchmark, outperforming GPT-4 and fine-tuned models

## Executive Summary
This research demonstrates how DeepSeek-R1, an economically efficient large language model, can automate mathematical modeling for operations research tasks, particularly supply chain optimization. By integrating natural language understanding with code generation, the study bridges the gap between real-world problem descriptions and solvable mathematical formulations. The work evaluates DeepSeek-R1 across four benchmarks (NL4OPT, IndustryOR, EasyLP, ComplexOR) and applies mitigation strategies including LLM-as-a-Judge, Few-shot Learning, Tool Calling, and Multi-agent Framework. Results show DeepSeek-R1 combined with LLM-as-a-Judge significantly improves accuracy to 92.3% on NL4OPT, demonstrating the potential of cost-effective, AI-augmented optimization solutions.

## Method Summary
The study translates natural language operations research problems into executable Python code using COPT solver, comparing baseline DeepSeek-R1 against four mitigation strategies. Inputs include four benchmarks (NL4OPT with 245 problems, IndustryOR with 100 problems, EasyLP with 652 problems, ComplexOR with 37 problems), each providing natural language descriptions, ground-truth optimal values, and variables. The method uses DeepSeek-R1 via OpenAI-compatible API to generate coptpy code, then executes it in a sandboxed environment with 300-600s timeout. For LLM-as-a-Judge, the model critiques and refines its own output. Accuracy is measured using pass@1 metric (top-1 solution matches ground truth within 5% tolerance), with secondary analysis of hallucination taxonomy classification.

## Key Results
- DeepSeek-R1 with LLM-as-a-Judge achieves 92.3% accuracy on NL4OPT benchmark, improving from 78.8% baseline
- Few-shot Learning increases NL4OPT accuracy to 95.8% but shows negative transfer on ComplexOR (75.7% vs 84.7% baseline)
- Tool Calling reduces Attribute Errors (65.7% of hallucinations) by providing grounded API facts
- Multi-agent framework shows no accuracy improvement (68.5% vs 68.2% baseline) due to error propagation

## Why This Works (Mechanism)

### Mechanism 1: Self-Correction via Iterative Review (LLM-as-a-Judge)
The system prompts the model with its initial output and a directive to evaluate correctness, forcing attention to logical consistency and constraint validity. This improves accuracy by having the model recognize and fix errors like incorrect variable attributes that were missed during greedy generation. Effectiveness drops when errors are fundamental hallucinations outside the model's knowledge base.

### Mechanism 2: Exemplar-Grounded Reasoning (Few-Shot Learning)
Providing solved examples in the prompt aligns the model's reasoning pattern with target structures, priming it to map natural language patterns to mathematical structures and API calls. This reduces random initialization errors when target problems share structural similarity with examples. Performance degrades when examples aren't representative of problem complexity, causing incorrect heuristics or negative transfer.

### Mechanism 3: API Constraint via Retrieval (Tool Calling)
Supplying API signatures via tool calling reduces Attribute Errors by providing grounded facts about the code library, preventing hallucination of non-existent methods. The model retrieves exact function signatures from documentation instead of recalling fuzzy syntax from pre-trained weights. This fails when the model exhibits high overconfidence and bypasses tool invocation or misinterprets tool output.

## Foundational Learning

- **Concept: Operations Research (OR) Formulation**
  - Why needed: The core task translates natural language into mathematical structures (Objective Function + Constraints). Understanding LP/MILP basics is essential to validate if the LLM's "plan" is mathematically sound.
  - Quick check: Can you identify the decision variables and objective function in "Maximize profit given limited labor hours and raw materials?"

- **Concept: Hallucination Taxonomy in Code**
  - Why needed: The paper defines three failure modes: Attribute Errors (wrong API), Syntax Errors (grammar), and Logical Errors (wrong math). Diagnosis depends on distinguishing these.
  - Quick check: If code runs but produces wrong optimal value, is it an Attribute Error or Logical Error?

- **Concept: Pass@k Evaluation Metric**
  - Why needed: The study uses pass@1 (single try accuracy) to measure reliability. Understanding this metric is crucial for comparing Judge approach vs baseline.
  - Quick check: Why is pass@1 a stricter and more practical metric for industrial supply chain applications than pass@10?

## Architecture Onboarding

- **Component map:** Natural Language Problem Description -> DeepSeek-R1 (Reasoning/Generation) -> LLM-as-a-Judge (Self-Correction) -> COPT Solver (Execution) -> Ground Truth Validation

- **Critical path:**
  1. Ingestion: Receive Natural Language problem description
  2. Generation: LLM creates Python code using coptpy
  3. Evaluation (The Judge): Feed code + problem back to LLM with "Judge Prompt" to critique and refine
  4. Execution: Run refined Python script against Solver
  5. Validation: Compare model.objval against ground truth

- **Design tradeoffs:**
  - Cost vs. Reliability: DeepSeek-R1 is 50x cheaper than GPT-4 but requires Judge loop (extra tokens) to match reliability
  - Generality vs. Accuracy: Multi-agent frameworks add complexity but didn't improve accuracy due to error propagation

- **Failure signatures:**
  - Attribute Error: Code calls model.update() (Gurobi syntax) instead of model.regensoln() (COPT syntax), or hallucinates z.z instead of z.x
  - Tool Ignore: Model generates code without triggering RAG tool lookup, resulting in outdated library calls

- **First 3 experiments:**
  1. Baseline Assessment: Run DeepSeek-R1 on NL4OPT with zero-shot prompting, record Logical vs. Attribute error ratios
  2. Judge Integration: Implement "LLM-as-a-Judge" wrapper, measure percentage of failed solutions that pass after one self-correction cycle
  3. Solver Validation: Execute generated Python code with coptpy, verify model.status == COPT.OPTIMAL matches ground truth within 0.05 tolerance

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning methods effectively train DeepSeek-R1 to selectively invoke external tools and reduce hallucination frequency in optimization tasks? The paper observed DeepSeek-R1 often failed to query necessary APIs or hallucinated tool outputs using ReAct framework, but did not implement or test the proposed RL solution. A comparative analysis of accuracy and hallucination rates between baseline ReAct and RL-finetuned versions would resolve this.

### Open Question 2
Does providing the Coder Agent with access to the Mathematician Agent's full reasoning context improve robustness of multi-agent optimization frameworks? The paper tested strict separation of duties which led to failure when upstream model was flawed, but didn't test if sharing reasoning chain would allow Coder to correct upstream errors. Ablation studies comparing current setup against full chain-of-thought sharing would resolve this.

### Open Question 3
Can a hybrid framework utilizing a stronger external model (e.g., GPT-4o) as the "Judge" significantly outperform DeepSeek-R1's self-evaluation capabilities? The study demonstrated efficacy of LLM-as-a-Judge using same model, but remains unknown if superior external judge could further enhance accuracy gains or correct complex errors self-review misses. Comparison of pass@1 accuracy where DeepSeek-R1 outputs are judged by GPT-4o versus DeepSeek-R1 itself would resolve this.

## Limitations

- Dataset scope is limited, with ComplexOR containing only 37 problems and IndustryOR limited to 100, potentially constraining generalization to diverse real-world scenarios
- The paper does not report variance metrics across multiple inference runs, making it difficult to assess result stability
- Mitigation strategies show problem-type dependency, with Few-shot Learning showing negative transfer on ComplexOR (accuracy decreased from 84.7% to 75.7%)

## Confidence

- **High Confidence:** Core finding that LLM-as-a-Judge improves accuracy from 78.8% to 92.3% on NL4OPT is well-supported by ablation study methodology and clear error reduction patterns
- **Medium Confidence:** Claims about DeepSeek-R1's economic efficiency (50x cheaper than GPT-4) are supported but lack comparative cost-per-accurate-solution analysis across different problem types
- **Low Confidence:** Assertion that multi-agent frameworks "did not improve accuracy" is based on results showing 68.5% vs 68.2%, but paper doesn't explore whether different agent configurations might yield better results

## Next Checks

1. **Dataset Diversity Test:** Apply methodology to additional OR problem types (e.g., network flow, scheduling) to assess generalizability beyond current benchmarks
2. **Cost-Effectiveness Analysis:** Calculate cost-per-accurate-solution across all mitigation strategies, including API calls for Judge loop and tool calling components
3. **Error Pattern Analysis:** Conduct detailed breakdown of which specific error types (Attribute vs. Logical) are resolved by each mitigation strategy to identify most impactful interventions