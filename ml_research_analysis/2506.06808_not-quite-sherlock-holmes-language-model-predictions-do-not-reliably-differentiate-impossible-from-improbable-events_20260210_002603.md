---
ver: rpa2
title: 'Not quite Sherlock Holmes: Language model predictions do not reliably differentiate
  impossible from improbable events'
arxiv_id: '2506.06808'
source_url: https://arxiv.org/abs/2506.06808
tags:
- eleutherai
- language
- impossible
- possible
- atypical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether language models can reliably distinguish
  possible events from merely improbable ones, a capability critical for real-world
  applications like medicine. Previous research conflated typicality and possibility,
  often relying on semantic relatedness as a heuristic for prediction.
---

# Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events

## Quick Facts
- **arXiv ID:** 2506.06808
- **Source URL:** https://arxiv.org/abs/2506.06808
- **Reference count:** 40
- **Key outcome:** Language models rely on semantic relatedness and typicality as heuristics rather than genuine world knowledge, performing at or below chance when distinguishing possible-but-atypical events from impossible-but-related ones.

## Executive Summary
This study investigates whether language models can reliably distinguish possible events from merely improbable ones, a capability critical for real-world applications like medicine. Previous research conflated typicality and possibility, often relying on semantic relatedness as a heuristic for prediction. The authors created a controlled task using minimal pairs where critical words made sentences either possible or impossible, while varying typicality and semantic relatedness. They tested 35 pretrained language models on English and Mandarin stimuli.

Results showed that language models performed worse at distinguishing possible but atypical events from impossible ones compared to typical vs. impossible events. Performance was significantly degraded when the critical word in impossible sentences was semantically related to the context, or when the critical word in possible sentences was semantically unrelated. In the most adversarial case—possible but atypical and unrelated vs. impossible but related—models performed at or below chance, assigning higher probabilities to impossible sentences. This effect persisted across model sizes and did not improve with scale. The findings suggest that language models rely on semantic relatedness and typicality as shortcuts rather than true world knowledge, limiting their reliability in atypical scenarios.

## Method Summary
The researchers designed a controlled experimental paradigm using minimal sentence pairs where a single critical word determined whether an event was possible or impossible. They systematically varied two dimensions: typicality (whether the event is common or rare) and semantic relatedness (whether the critical word is semantically connected to the context). The study tested 35 pretrained language models across English and Mandarin stimuli, measuring their ability to correctly assign higher probability to possible versus impossible sentences. The experimental design isolated the effects of semantic relatedness and typicality on model performance, creating conditions ranging from straightforward (typical vs. impossible) to adversarial (atypical/unrelated vs. related/impossible).

## Key Results
- Language models perform significantly worse at distinguishing possible-but-atypical events from impossible ones compared to typical vs. impossible events
- Performance degrades substantially when impossible sentences contain semantically related critical words or when possible sentences contain semantically unrelated critical words
- In adversarial conditions (possible-but-atypical/unrelated vs. impossible-but-related), models perform at or below chance, often assigning higher probabilities to impossible sentences
- This limitation persists across different model sizes and does not improve with scaling

## Why This Works (Mechanism)
The study reveals that language models rely on surface-level statistical patterns rather than genuine world knowledge when making possibility judgments. Models appear to use semantic relatedness as a proxy for plausibility—if a word fits semantically with the context, the model assumes the event is possible, regardless of actual world knowledge. This shortcut fails when the relationship between typicality and semantic relatedness is decoupled, as in the adversarial conditions. The mechanism suggests that models are pattern-matching rather than reasoning about possibility, treating semantic coherence as a sufficient condition for possibility rather than as one indicator among many.

## Foundational Learning
**Semantic Relatedness**
- Why needed: Understanding how models use word co-occurrence patterns as shortcuts for reasoning
- Quick check: Can the model distinguish between "The surgeon removed the patient's appendix" vs. "The surgeon removed the patient's emotion"?

**Typicality vs. Possibility**
- Why needed: Separating how models handle frequency-based expectations from genuine possibility assessment
- Quick check: Does the model correctly identify rare but possible events as more likely than common but impossible events?

**Minimal Pair Design**
- Why needed: Isolating specific linguistic factors that influence model predictions
- Quick check: Can the model handle controlled variations where only one word changes the possibility status?

## Architecture Onboarding

**Component Map**
Token embedding layer -> Transformer blocks (self-attention + feed-forward) -> Output layer (probability distribution)

**Critical Path**
Input tokens → embedding → self-attention across context → feed-forward transformation → repeated through layers → final output probabilities

**Design Tradeoffs**
- Pretrained on large corpora: Captures statistical patterns but may overfit to common semantic relationships
- Fixed architecture: Cannot dynamically adjust reasoning strategies based on task demands
- Probability-based output: Confuses semantic coherence with genuine possibility assessment

**Failure Signatures**
- Assigns higher probability to semantically coherent but impossible events
- Struggles with sentences requiring domain knowledge to assess possibility
- Performance does not improve with model scale despite increased capacity

**First 3 Experiments**
1. Test model on sentences where semantic relatedness directly contradicts possibility (e.g., "The doctor prescribed a rainbow")
2. Evaluate performance on typical vs. atypical but possible events to measure baseline discrimination ability
3. Measure sensitivity to semantic relatedness by varying the strength of semantic connections in impossible sentences

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled experimental design may not fully capture real-world complexity where multiple contextual cues interact
- Binary classification task may oversimplify nuanced possibility judgments that humans make in practice
- Results from English and Mandarin may not generalize to other languages and cultural contexts

## Confidence

**High confidence:** The core finding that language models rely on semantic relatedness and typicality as heuristics rather than genuine world knowledge assessment. This is supported by consistent results across 35 models and two languages.

**Medium confidence:** The conclusion that scaling model size does not resolve this limitation, as the study tested this specifically but other architectural factors might play a role.

**Medium confidence:** The claim that this represents a fundamental limitation for medical and safety-critical applications, as real-world deployment contexts may differ from the controlled experimental conditions.

## Next Checks
1. Test model performance on dynamically generated minimal pairs that vary plausibility gradients rather than binary possibility/impossibility, to assess whether models can capture degrees of possibility.
2. Evaluate whether fine-tuning on targeted datasets containing impossible but semantically plausible sentences improves discrimination performance, and whether this improvement generalizes to novel scenarios.
3. Conduct human benchmarking studies using the same minimal pairs to establish baseline performance and determine whether the models' reliance on semantic relatedness represents a genuine reasoning failure or simply a different inference strategy.