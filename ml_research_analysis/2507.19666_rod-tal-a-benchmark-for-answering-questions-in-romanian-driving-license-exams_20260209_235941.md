---
ver: rpa2
title: 'RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams'
arxiv_id: '2507.19666'
source_url: https://arxiv.org/abs/2507.19666
tags:
- question
- o4-mini
- image
- correct
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoD-TAL, a novel multimodal dataset for Romanian
  driving license exams that includes text and image-based questions with annotated
  legal references. The authors evaluate large language models (LLMs) and vision-language
  models (VLMs) on information retrieval, question answering, visual information retrieval,
  and visual question answering tasks using retrieval-augmented generation pipelines.
---

# RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams

## Quick Facts
- arXiv ID: 2507.19666
- Source URL: https://arxiv.org/abs/2507.19666
- Reference count: 40
- Primary result: Domain-specific fine-tuning improves retrieval performance (88% R@10 vs 59% baseline)

## Executive Summary
This paper introduces RoD-TAL, a novel multimodal dataset for Romanian driving license exams that includes text and image-based questions with annotated legal references. The authors evaluate large language models (LLMs) and vision-language models (VLMs) on information retrieval, question answering, visual information retrieval, and visual question answering tasks using retrieval-augmented generation pipelines. They find that domain-specific fine-tuning significantly improves retrieval performance, while chain-of-thought prompting and reasoning-optimized models enhance QA accuracy beyond exam passing thresholds. However, visual reasoning remains challenging, highlighting limitations in applying LLMs and VLMs to legal education in under-resourced languages.

## Method Summary
The authors create RoD-TAL by crawling official Romanian driving school materials and validating them through human annotation. They implement RAG pipelines using a fine-tuned mE5small embedding model for retrieval and GPT-4o-mini or o4-mini for generation. The retrieval component is fine-tuned on 6,960 training pairs using InfoNCE loss with hard negative mining. For QA, they use chain-of-thought prompting with enhanced instructions to force explicit legal reasoning. Visual questions are handled through multimodal pipelines that caption images and integrate them with text queries. The evaluation measures Exact Match accuracy for QA/VQA and Recall@10, Precision@10, nDCG@10 for IR tasks.

## Key Results
- Domain-specific fine-tuning improves retrieval Recall@10 from 59.31% to 88.14% on test set
- Chain-of-thought prompting with RAG improves QA accuracy by 25-34% over baseline approaches
- Reasoning-optimized models (o4-mini) achieve 86.3% exact match accuracy, surpassing minimum passing grades
- Visual question answering maxes at 78.3% even with optimal RAG, indicating persistent visual reasoning challenges

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific fine-tuning of dense retrievers substantially improves legal document retrieval over off-the-shelf multilingual models. Fine-tuning aligns embedding representations to the specific semantic structure of Romanian legal language and question-answer relationships, reducing the domain/language mismatch that generic models suffer from.

### Mechanism 2
Chain-of-thought prompting combined with retrieval-augmented generation improves QA accuracy by enabling explicit legal reasoning steps and grounding. CoT forces models to decompose questions into legal criteria before selecting answers, while RAG provides authoritative source text that constrains reasoning to actual legislation rather than hallucinated rules.

### Mechanism 3
Reasoning-optimized models (o4-mini) outperform standard instruction-tuned models on legally-grounded QA by reducing failure modes like safety bias and overthinking. Native reasoning architectures perform more systematic evaluation of answer options against legal criteria, rather than pattern-matching to training distribution heuristics.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Legal QA requires grounding in specific statutes; RAG retrieves relevant legal articles before generation, reducing hallucination and improving accuracy. Quick check: Given a question about Romanian traffic penalties, which component retrieves the relevant legal articles and which generates the answer?

- **Dense retrieval with embedding models**: The IR task uses vector similarity search (mE5small embeddings) over a legal corpus; understanding embedding spaces is essential for both baseline and fine-tuned retrieval. Quick check: Why might a multilingual embedding model fail on domain-specific Romanian legal queries even if it supports Romanian?

- **Chain-of-thought prompting**: CoT prompting forces explicit reasoning steps, which is critical for legal questions where nuance and multi-criteria evaluation matter. Quick check: What are the three identified failure modes that CoT (with proper prompting) helps mitigate?

## Architecture Onboarding

- **Component map**: Legal Corpus (RoD-Law) -> Fine-tuned Embedding Model -> Retriever -> LLM/VLM Generator -> Answer
- **Critical path**: Question + Answer Choices → Fine-tuned Retriever → Top-10 Legal Docs + Signs → CoT Prompt → Reasoning Model → Answer
- **Design tradeoffs**: RAG vs. no-RAG adds ~11% accuracy but can cause context bloat; Fine-tuning vs. augmentation shows real data fine-tuning outperforms LLM-augmented data (88% vs 63% R@10); Caption+Image vs. Image only shows Image+QA performed best; Reranking helped baseline but hurt fine-tuned retriever
- **Failure signatures**: Safety Bias (model selects "safer" answer over legally correct one), Overthinking (model extrapolates beyond question scope), Visual Reasoning Gaps (aerial/pov images poorly understood), Instruction Following (open models sometimes ignore format)
- **First 3 experiments**: 1) Baseline retrieval benchmark with mE5small to establish ~50% R@10 baseline; 2) Fine-tuning impact test comparing 88% R@10 on held-out test split; 3) CoT + RAG ablation comparing RAG + CoT, RAG only, CoT only, and neither conditions

## Open Questions the Paper Calls Out

- **Joint image-text embeddings for VIR**: Future work should explore more advanced methods incorporating joint image-text embeddings rather than the current caption-based approach
- **Preventing false negatives in hard mining**: Further fine-tuning is needed to prevent the inclusion of actual positives during hard negative mining to improve alignment
- **Document chunking strategy**: The choice of not splitting larger documents into sub-chunks might prevent achieving perfect retrieval results compared to truncation

## Limitations

- Dataset access remains unclear, creating a significant barrier to direct replication
- Use of unpublished "o4-mini" model raises questions about performance attribution
- Hard negative mining strategy lacks complete implementation details
- Evaluation focused primarily on Exact Match without extensive analysis of failure modes

## Confidence

**High Confidence**: Domain-specific fine-tuning substantially improves retrieval performance (88% vs 59% R@10) - directly measurable and supported by ablation experiments

**Medium Confidence**: Chain-of-thought prompting with RAG improves QA accuracy by 25-34% over baseline - depends on prompt engineering quality and may not transfer cleanly to other domains

**Low Confidence**: Performance gains specifically attributable to "reasoning-optimized" models like o4-mini - model is unpublished and lacks direct comparison on identical infrastructure

## Next Checks

1. **Dataset accessibility verification**: Confirm RoD-TAL dataset download location and obtain sample questions to validate format compatibility before attempting full pipeline implementation

2. **Model substitution experiment**: Replace unspecified o4-mini with available reasoning models (o1/o3-mini, DeepSeek-R1) using identical CoT prompts to test whether reasoning improvements transfer across models

3. **Safety bias quantification**: Systematically evaluate model outputs on questions where "safer" answers conflict with legal text to measure frequency of safety-over-legal accuracy decisions and validate enhanced prompt effectiveness