---
ver: rpa2
title: 'Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs'
arxiv_id: '2510.18245'
source_url: https://arxiv.org/abs/2510.18245
tags:
- arxiv
- scaling
- inference
- preprint
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of designing large language\
  \ models that balance inference efficiency with accuracy under fixed training budgets.\
  \ The authors introduce a conditional scaling law that extends the Chinchilla framework\
  \ by incorporating key architectural factors\u2014hidden size, the MLP-to-attention\
  \ ratio, and grouped-query attention\u2014and develop a search framework for identifying\
  \ optimal architectures."
---

# Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs

## Quick Facts
- arXiv ID: 2510.18245
- Source URL: https://arxiv.org/abs/2510.18245
- Authors: Song Bian; Tao Yu; Shivaram Venkataraman; Youngsuk Park
- Reference count: 40
- Primary result: Conditional scaling laws predict optimal LLM architectures achieving 42% higher inference throughput and 2.1% better accuracy under fixed training budgets

## Executive Summary
This work bridges scaling laws and model architecture to create inference-efficient large language models. The authors extend Chinchilla scaling laws by incorporating architectural factors—hidden size, MLP-to-attention ratio, and grouped-query attention—into a conditional scaling law that predicts training loss more accurately. Through training over 200 models ranging from 80M to 3B parameters, they demonstrate that this approach reliably identifies architectures that simultaneously improve accuracy and inference throughput. Their optimized "Surefire" models achieve up to 42% higher throughput and 2.1% better accuracy compared to LLaMA-3.2 baselines when trained under the same budget.

## Method Summary
The authors introduce a conditional scaling law that augments Chinchilla's loss prediction with multiplicative calibration factors for hidden size (d_model/√N) and MLP-to-attention ratio (r_mlp/attn). They train 200+ LLaMA-3.2-style architectures varying these factors plus grouped-query attention, then fit the scaling law using Levenberg-Marquardt optimization. A constrained optimization framework identifies architectures maximizing inference throughput while maintaining target accuracy. The optimal architectures are validated through training at 1B and 3B scales, with inference measured using vLLM on A100 GPUs and accuracy evaluated on 9 downstream benchmarks.

## Key Results
- Conditional scaling law predicts training loss with Spearman correlations of 0.67-0.99 across model scales
- Optimal architectures achieve 42% higher inference throughput and 2.1% better accuracy than LLaMA-3.2 baselines under fixed training budgets
- Pareto-optimal models identified through architectural search demonstrate consistent superiority across multiple scales (1B and 3B parameters)
- U-shaped relationships between architectural factors and loss are empirically validated, with consistent optima across model sizes

## Why This Works (Mechanism)

### Mechanism 1: Separable Calibration of Architectural Factors
The conditional scaling law decomposes loss prediction into Chinchilla's optimal loss plus multiplicative calibration factors for d_model/√N and r_mlp/attn. This separable formulation captures U-shaped relationships between each factor and loss while maintaining computational tractability. The approach assumes negligible interaction terms within practical ranges (r ∈ [0.5, 5]), validated by higher Spearman correlation (0.7451) compared to joint formulations (0.2108).

### Mechanism 2: Memory-Bandwidth Bounded Inference Optimization
Larger hidden sizes and higher MLP-to-attention ratios improve inference throughput by reducing KV cache memory and I/O costs. Increasing d_model reduces attention heads (n_heads ∝ N_attn/d_model²), shrinking cache size. Higher r_mlp/attn reduces attention parameter fraction, decreasing FLOPs during autoregressive generation. Both mechanisms reduce memory bandwidth pressure under the assumption that inference is memory-bound.

### Mechanism 3: U-Shaped Loss Relationships with Predictable Optima
Both d_model and MLP-to-attention ratio exhibit U-shaped relationships with training loss, with interior optima that scale consistently across model sizes. Excessively small d_model limits representation capacity while excessively large d_model reduces multi-head attention benefits. Similarly, too little attention starves MLP of capacity while too much attention under-allocates to the mechanism central to transformers. The optimal d_model/√N ≈ 0.08 and r ≈ 1.0-1.2 remain consistent from 1B to 3B scales.

## Foundational Learning

- **Chinchilla scaling laws (L = E + A/N^α + B/D^β)**: The base framework for optimal compute allocation between parameters and tokens. Understanding this is prerequisite for the conditional extension.
  - Why needed: The conditional law builds on Chinchilla by adding architectural calibration
  - Quick check: Given fixed compute budget C ≈ 6ND, what happens to optimal N/D ratio if you double C?

- **Grouped-Query Attention (GQA)**: Architectural technique where multiple heads share key-value representations to reduce KV cache size.
  - Why needed: GQA is a core architectural factor studied; understanding its KV cache reduction is essential
  - Quick check: With n_heads = 32 and GQA = 4, how many KV cache entries per token vs standard multi-head attention?

- **FLOPs vs memory-bandwidth boundedness**: The distinction determines which architectural changes matter for inference efficiency.
  - Why needed: Throughput gains depend on inference being memory-bound rather than compute-bound
  - Quick check: During autoregressive decoding, why does attention scale as O(T·d) rather than O(T²)?

## Architecture Onboarding

- **Component map**: Training Budget (N, D fixed) → Chinchilla L_opt(N, D) → Conditional Calibration (d_model/√N, r_mlp/attn, GQA) → Constrained Optimization (Eq. 4) → Pareto-optimal architectures (Surefire-1B, Surefire-3B)

- **Critical path**: Fitting scaling laws requires training 50+ architectural variants at 2-3 small scales (80M-297M), then validating predictions at 1B+ before committing to full-scale training.

- **Design tradeoffs**:
  - Higher d_model: Higher throughput, fewer heads may reduce accuracy
  - Higher r_mlp/attn: Higher throughput, too little attention degrades loss
  - Higher GQA: Higher throughput, loss relationship is unpredictable
  - Fitting on larger seed models: Better extrapolation, more expensive data collection

- **Failure signatures**:
  - Spearman correlation < 0.6 on held-out architectures indicates poor fit
  - Predicted optimal r outside [0.5, 5] suggests extrapolation risk
  - Throughput improvements not appearing: verify batch size is large enough to be memory-bound

- **First 3 experiments**:
  1. **Baseline ablation**: For fixed N (145M), train 10-15 variants spanning d_model/√N ∈ [0.04, 0.17] and r ∈ [0.5, 5]. Verify U-shaped loss curves appear.
  2. **Scale validation**: Fit conditional law on 80M+145M data, predict 297M optimal architecture, train 2-3 architectures around predicted optimum to validate.
  3. **Throughput measurement**: Using vLLM with batch sizes 16-128, confirm predicted optimal architecture achieves ≥20% throughput gain over baseline with <1% accuracy loss.

## Open Questions the Paper Calls Out

### Open Question 1: Generalization to MoE Architectures
The analysis is restricted to dense models, and it remains unclear whether results extend to Mixture of Experts (MoE) architectures. MoE models decouple total parameters from active parameters, changing the relationship between FLOPs, memory, and accuracy. While Appendix J shows efficiency trends for MoE, the unified scaling law for accuracy prediction was not established.

### Open Question 2: Impact on Post-Training Performance
The analysis is limited to pre-training, and it remains unclear how results would change under post-training (fine-tuning or alignment). Architectures optimized for low pre-training loss and high throughput might exhibit different generalization capabilities or catastrophic forgetting tendencies during supervised fine-tuning compared to standard baselines.

### Open Question 3: Architecture-Specific Hyperparameter Requirements
The authors adopted experimental setups from prior work and it is uncertain whether different model architectures warrant different hyperparameter configurations. The optimal shape (hidden size/MLP ratio) might alter the loss landscape geometry, making default hyperparameters suboptimal and potentially underestimating the accuracy of proposed models.

## Limitations

- **Range validity**: Scaling law predictions may degrade outside tested ranges (r < 0.5 or r > 5, d_model/√N outside [0.04, 0.34]) due to neglected interaction terms
- **Hardware specificity**: Throughput gains measured on A100-40GB GPUs may not transfer to different hardware or serving frameworks
- **Dataset specificity**: Calibration factors fitted to Dolma-v1.7 may not generalize to other domains or distributions

## Confidence

**High confidence**: The existence of U-shaped relationships between d_model/√N, r_mlp/attn and training loss, and the predictive validity of the conditional scaling law within tested ranges. Supported by direct empirical measurements across 200+ trained models with clear statistical validation (Spearman correlations 0.67-0.99).

**Medium confidence**: The claim that optimal architectural parameters scale consistently from small (80M) to large (3B) models. While similar optima are shown across scales, fitting on larger seed models improves predictions (Spearman 0.50 vs 1.00), suggesting some scale-dependence.

**Medium confidence**: The magnitude of throughput improvements (42% higher) depends on specific hardware and serving configuration. The mechanism is sound, but absolute gains vary with batch size and GPU memory bandwidth.

## Next Checks

1. **Cross-dataset scaling validation**: Train Panda-1B and Surefire-3B on a different corpus (e.g., The Pile or multilingual dataset) and measure whether the conditional scaling law accurately predicts optimal architectures for this new distribution. Compare fitted calibration factors against those from Dolma-v1.7.

2. **Hardware portability test**: Measure inference throughput of Pareto-optimal architectures on H100 GPUs or different serving frameworks (TensorRT-LLM, FasterTransformer). Verify that the memory-bandwidth bottleneck assumption holds and that architectural gains persist across hardware generations.

3. **Architectural interaction stress test**: Systematically train architectures with r < 0.5 and r > 5, and d_model/√N outside [0.04, 0.34], to quantify where the separable calibration assumption breaks down. Measure whether joint non-separable formulations provide better predictions in these regimes.