---
ver: rpa2
title: Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts
arxiv_id: '2506.11079'
source_url: https://arxiv.org/abs/2506.11079
tags:
- whisper
- reading
- prompting
- speech
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel multimodal approach using prompts with
  Whisper and large language models (LLMs) to improve child speech recognition and
  reading mistake detection. The authors explore using text prompts to guide Whisper
  transcription and LLM-based refinement to better handle reading errors.
---

# Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts

## Quick Facts
- arXiv ID: 2506.11079
- Source URL: https://arxiv.org/abs/2506.11079
- Reference count: 0
- Primary result: Whisper WER improved from 9.4% to 5.1% with optimal prompting

## Executive Summary
This paper presents a novel multimodal approach using prompts with Whisper and large language models (LLMs) to improve child speech recognition and reading mistake detection. The authors explore using text prompts to guide Whisper transcription and LLM-based refinement to better handle reading errors. Results show significant improvements in both speech recognition accuracy and reading mistake detection capabilities.

## Method Summary
The approach combines prompt engineering with Whisper and LLM-based refinement to improve child speech recognition and reading mistake detection. The system uses text prompts to guide Whisper transcription, then applies LLM-based hypothesis alignment refinement to enhance reading mistake detection. The method was tested across multiple LLM models and evaluated on child read speech data.

## Key Results
- WER dropped from 9.4% to 5.1% with optimal prompting
- Reading mistake detection F1-score increased from 0.39 to 0.73
- Best approach used irrelevant text with high-error prompts for Whisper, combined with LLM-based hypothesis alignment refinement

## Why This Works (Mechanism)
The multimodal approach works by leveraging the complementary strengths of Whisper's speech recognition capabilities and LLM-based refinement. Whisper provides the initial transcription of child speech, while carefully designed prompts guide its behavior to better handle reading-specific patterns and errors. The LLM-based refinement then analyzes the transcription hypotheses, using hypothesis alignment to identify discrepancies and reading mistakes that might be missed by standard speech recognition alone. This combination allows for more accurate detection of reading errors while maintaining high transcription accuracy.

## Foundational Learning
- Whisper ASR system: Automatic speech recognition baseline; needed for baseline performance comparison
- Prompt engineering: Text-based guidance for model behavior; quick check - can be tested with different prompt structures
- LLM-based refinement: Large language model post-processing; quick check - can be evaluated with different LLM architectures
- Hypothesis alignment: Matching transcription hypotheses; quick check - can be tested with alignment algorithms
- Reading mistake detection: Identifying reading errors in child speech; quick check - can be validated with error type categorization
- Multimodal approach: Combining multiple techniques; quick check - can be evaluated with ablation studies

## Architecture Onboarding

Component map: Whisper ASR -> LLM-based refinement -> Reading mistake detection

Critical path: Audio input → Whisper transcription → Prompt-guided processing → LLM refinement → Error detection output

Design tradeoffs: Prompt engineering improves accuracy but requires careful prompt design; LLM refinement adds complexity but enhances detection capabilities

Failure signatures: Poor prompt design leads to reduced accuracy; inadequate hypothesis alignment causes false positives in error detection

First experiments:
1. Test baseline Whisper performance on child speech data
2. Implement and evaluate different prompt structures
3. Compare LLM-based refinement against baseline error detection

## Open Questions the Paper Calls Out
None

## Limitations
- May not generalize beyond specific children's reading corpus used
- Limited comparison to other ASR approaches
- LLM-based refinement effectiveness may vary with different models and prompts

## Confidence
High: Prompt engineering significantly reduces WER in child speech recognition with Whisper
Medium: LLM-based refinement improves reading mistake detection
Low: Multimodal approach is universally superior

## Next Checks
1. Test the prompt engineering approach on diverse child speech datasets with varying reading levels and accents to assess generalization
2. Compare the effectiveness of this method against other state-of-the-art ASR systems specifically trained for child speech
3. Evaluate the robustness of LLM-based refinement across different LLM architectures and prompt variations to identify optimal configurations