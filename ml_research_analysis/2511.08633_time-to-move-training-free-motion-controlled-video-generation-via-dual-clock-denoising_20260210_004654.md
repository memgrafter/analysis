---
ver: rpa2
title: 'Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock
  Denoising'
arxiv_id: '2511.08633'
source_url: https://arxiv.org/abs/2511.08633
tags:
- motion
- video
- control
- diffusion
- appearance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Time-to-Move (TTM), a training-free framework\
  \ for precise motion and appearance control in video diffusion models. TTM uses\
  \ crude user-provided animations (e.g., cut-and-drag or depth-based reprojection)\
  \ as motion proxies and applies dual-clock denoising\u2014a region-dependent strategy\
  \ that enforces strong alignment in motion-specified areas while allowing flexibility\
  \ elsewhere."
---

# Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising

## Quick Facts
- arXiv ID: 2511.08633
- Source URL: https://arxiv.org/abs/2511.08633
- Reference count: 21
- Primary result: Training-free framework for precise motion and appearance control in video diffusion models

## Executive Summary
Time-to-Move (TTM) introduces a novel training-free framework for precise motion and appearance control in video diffusion models. The method leverages crude user-provided animations as motion proxies and applies a dual-clock denoising strategy that enforces strong alignment in motion-specified areas while allowing flexibility elsewhere. Appearance is preserved through image conditioning. TTM achieves state-of-the-art performance on motion control benchmarks and enables joint motion-appearance control not possible with text-only prompts.

## Method Summary
TTM uses existing pretrained diffusion backbones and introduces dual-clock denoising to control motion and appearance simultaneously. Users provide crude animations (e.g., cut-and-drag or depth-based reprojection) that serve as motion proxies. The dual-clock approach applies region-dependent denoising strength—strong alignment where motion is specified and flexible generation elsewhere. Image conditioning preserves appearance by conditioning on the first frame. The method is plug-and-play across multiple video diffusion backbones without retraining.

## Key Results
- Lowest CoTracker Distance on MC-Bench compared to training-based baselines
- 33% lower MSE than prior methods on DL3DV camera control task
- Enables joint motion-appearance control impossible with text-only prompts

## Why This Works (Mechanism)
The dual-clock denoising strategy is the core innovation. By applying different denoising strengths to motion-specified versus unspecified regions, TTM achieves precise control where needed while maintaining generation quality elsewhere. The use of existing diffusion backbones with specialized conditioning mechanisms allows for training-free implementation while achieving competitive performance.

## Foundational Learning

**Video Diffusion Models**: Why needed - Foundation for understanding TTM's approach to video generation. Quick check - Can you explain how video diffusion models differ from image diffusion models?

**Conditional Generation**: Why needed - Critical for understanding how TTM uses conditioning for appearance preservation. Quick check - How does conditional generation differ from unconditional generation in diffusion models?

**Dual-Clock Denoising**: Why needed - The core technical innovation of TTM. Quick check - What is the intuition behind using different denoising strengths for different regions?

## Architecture Onboarding

**Component Map**: User Input -> Motion Proxy Extraction -> Dual-Clock Denoising Controller -> Diffusion Backbone (SVD/CogVideoX/Wan2.2) -> Output Video

**Critical Path**: User-provided animation → Motion proxy extraction → Region-based denoising strength calculation → Dual-clock denoising → Image-conditioned generation

**Design Tradeoffs**: Training-free approach trades off some performance potential for flexibility and accessibility; dual-clock strategy balances control precision with generation quality

**Failure Signatures**: Poor motion proxy extraction leading to incorrect denoising strength assignment; excessive control strength causing artifacts; appearance preservation failure when content extends beyond first frame

**First Experiments**: 1) Test motion proxy extraction accuracy on various input types, 2) Validate dual-clock denoising effectiveness on synthetic motion patterns, 3) Benchmark appearance preservation across different scene complexities

## Open Questions the Paper Calls Out
The authors acknowledge the need for systematic hyperparameter sensitivity analysis, particularly for the control strength parameter (e.g., 0.8). They also note the current limitation to first-frame visible content for appearance preservation, which may restrict applicability to scenarios requiring object introduction or scene changes.

## Limitations
- Hyperparameter sensitivity requires careful tuning for optimal results
- Restricted to content visible in the first frame, limiting scene evolution capabilities
- Performance claims rely on specific datasets that may not represent all motion control scenarios

## Confidence

- Motion control performance claims: Medium
- Training-free characterization: Medium
- Cross-backbone compatibility: Medium
- Dual-clock denoising mechanism: Low
- Joint motion-appearance control novelty: Low

## Next Checks

1. Conduct ablation study isolating the contribution of each denoising clock's parameters across varying motion intensities
2. Perform cross-dataset evaluation on high-motion scenarios (e.g., sports, dance) to test robustness beyond static manipulation tasks
3. Compare against training-based motion control methods using identical evaluation protocols and metrics to ensure fair benchmarking