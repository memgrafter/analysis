---
ver: rpa2
title: Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning
arxiv_id: '2601.15595'
source_url: https://arxiv.org/abs/2601.15595
tags:
- unlearning
- data
- privacy
- uni00000013
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of privacy-preserving in large
  language models (LLMs) by introducing a data-free approach for selective unlearning
  of sensitive personally identifiable information (PII). The proposed method, Data-Free
  Selective Unlearning (DFSU), synthesizes pseudo-PII through model inversion, constructs
  token-level privacy masks, and performs selective unlearning via a contrastive mask
  loss within a LoRA subspace.
---

# Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning

## Quick Facts
- arXiv ID: 2601.15595
- Source URL: https://arxiv.org/abs/2601.15595
- Reference count: 28
- Primary result: Introduces a data-free approach for selective unlearning of sensitive PII in LLMs using model inversion and LoRA-based contrastive masking

## Executive Summary
This paper addresses the challenge of privacy-preserving in large language models by proposing a data-free method for selective unlearning of sensitive personally identifiable information (PII). The approach, called Data-Free Selective Unlearning (DFSU), synthesizes pseudo-PII through model inversion and performs targeted unlearning using a contrastive mask loss within a LoRA subspace. The method is evaluated on the AI4Privacy PII-Masking dataset using Pythia models, demonstrating effective removal of target PII while maintaining model utility, achieving near-oracle performance without requiring access to original training data.

## Method Summary
The DFSU approach combines model inversion and selective unlearning to remove sensitive PII from LLMs without requiring original training data. It first generates synthetic pseudo-PII by inverting the model's knowledge, then constructs token-level privacy masks to identify sensitive information. The method applies selective unlearning through a contrastive mask loss within a LoRA (Low-Rank Adaptation) subspace, enabling targeted removal of sensitive information while preserving overall model performance. This data-free approach addresses practical constraints where original training data may be unavailable or impractical to access.

## Key Results
- Successfully removes target PII from LLMs while maintaining overall model utility
- Achieves near-oracle performance without access to original training data
- Demonstrates effectiveness on AI4Privacy PII-Masking dataset using Pythia models

## Why This Works (Mechanism)
The DFSU method works by first synthesizing pseudo-PII through model inversion, which allows the system to identify what sensitive information the model has learned without requiring actual training data. The token-level privacy masks then precisely locate where this sensitive information exists in the model's representations. By applying selective unlearning through a contrastive mask loss in a LoRA subspace, the method can specifically target and remove sensitive information while minimizing impact on other learned capabilities. The LoRA framework enables efficient parameter updates that are focused and reversible if needed.

## Foundational Learning

**Model Inversion**: Why needed: To generate synthetic representations of sensitive information the model has learned without access to original training data. Quick check: Verify that generated pseudo-PII correlates with actual sensitive information patterns.

**Token-Level Privacy Masks**: Why needed: To precisely identify and locate sensitive information within the model's token representations. Quick check: Validate mask accuracy by comparing with ground truth PII locations.

**Contrastive Mask Loss**: Why needed: To enable selective unlearning that specifically targets sensitive information while preserving general model capabilities. Quick check: Measure utility retention on non-sensitive tasks post-unlearning.

**LoRA Subspace**: Why needed: To provide an efficient, low-rank adaptation framework for implementing selective unlearning with minimal computational overhead. Quick check: Verify that unlearning parameters remain within the LoRA subspace and don't affect full model weights.

## Architecture Onboarding

Component map: Model Inversion -> Privacy Mask Construction -> Selective Unlearning (LoRA-based) -> Utility Evaluation

Critical path: The most critical path is the sequential flow from model inversion to privacy mask construction to selective unlearning, as each step depends on the successful completion of the previous one. The LoRA-based unlearning must effectively remove sensitive information while maintaining model utility.

Design tradeoffs: The main tradeoff is between the precision of sensitive information removal and the preservation of general model capabilities. The use of LoRA provides computational efficiency but may limit the expressiveness of the unlearning process compared to full fine-tuning.

Failure signatures: If model inversion fails to accurately generate pseudo-PII, the privacy masks will be incorrect, leading to incomplete or inaccurate unlearning. If the contrastive mask loss is improperly configured, it may either remove too much information (hurting utility) or too little (failing to adequately protect privacy).

First experiments: 1) Test model inversion accuracy by comparing generated pseudo-PII against known sensitive patterns. 2) Validate privacy mask construction by measuring precision and recall on identifying sensitive tokens. 3) Evaluate unlearning effectiveness by measuring PII removal while tracking performance on standard benchmarks.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on PII removal from a single dataset (AI4Privacy), limiting generalizability to other privacy scenarios
- The assumption that synthetic pseudo-PII accurately represents actual sensitive information remains untested
- Scalability to larger model architectures beyond Pythia models has not been demonstrated
- Does not address potential adversarial scenarios where attackers might exploit the unlearning process

## Confidence
- High confidence: The technical implementation of model inversion and token-level privacy mask construction appears sound and well-documented
- Medium confidence: The effectiveness of selective unlearning in maintaining utility while removing PII is demonstrated, though the evaluation scope is somewhat limited
- Medium confidence: The data-free approach is innovative, but its practical robustness across diverse privacy scenarios needs validation

## Next Checks
1. Test the DFSU approach on larger, more diverse datasets and model architectures (e.g., LLaMA, Mistral) to assess generalizability
2. Evaluate the robustness of unlearned models against adaptive attacks that might exploit the unlearning process or target residual PII
3. Conduct long-term stability tests to verify that unlearned information does not resurface through continued model use or fine-tuning