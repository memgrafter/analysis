---
ver: rpa2
title: Bitstream Collisions in Neural Image Compression via Adversarial Perturbations
arxiv_id: '2503.19817'
source_url: https://arxiv.org/abs/2503.19817
tags:
- image
- images
- compression
- xadv
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural image compression (NIC) is vulnerable to bitstream collisions,
  where semantically different images produce identical compressed bitstreams. The
  authors propose a Masked Gradient Descent (MGD) attack algorithm that uses a dot
  mask to selectively perturb pixels during gradient updates, enabling the generation
  of collision images with up to 100% attack success rate across datasets and compression
  models.
---

# Bitstream Collisions in Neural Image Compression via Adversarial Perturbations

## Quick Facts
- arXiv ID: 2503.19817
- Source URL: https://arxiv.org/abs/2503.19817
- Reference count: 24
- Key outcome: Neural image compression is vulnerable to bitstream collisions, where semantically different images produce identical compressed bitstreams

## Executive Summary
Neural image compression models are vulnerable to adversarial attacks that generate semantically different images producing identical compressed bitstreams. The authors propose a Masked Gradient Descent (MGD) attack that uses sparse pixel perturbations to manipulate encoder latents without visual convergence to the target image. A simple Limited-Precision Defense (LPD) converting latents to half-precision effectively mitigates this vulnerability. The attack achieves up to 100% success rate across datasets and compression models, even with small perceptual distortions, while traditional attacks fail.

## Method Summary
The MGD attack optimizes the distance between encoder output latents of source and target images using a dot mask that selectively perturbs pixels during gradient updates. This enables generation of collision images without visual convergence to the target. The attack bypasses non-differentiable quantization by optimizing latent-space distance, which when below quantization threshold produces identical bitstreams. LPD defense converts float32 latent tensors to float16, introducing rounding noise that disrupts the precise latent alignment required for collision.

## Key Results
- MGD achieves 100% attack success rate for Factorized Prior models at low quality factors (QF=1)
- LPD defense reduces ASR to 0% by converting latent tensors to float16 precision
- Collision image distances are theoretically bounded by √2C where C is the contraction constant
- Traditional attacks (PGD, CW) fail completely while MGD succeeds with minimal perceptual distortion

## Why This Works (Mechanism)

### Mechanism 1
Sparse masked gradients enable bitstream collisions without visual convergence to the target image. A dot mask selectively zeros gradient elements at most pixel locations, allowing updates only at sparse, regularly-spaced positions. This preserves the semantic structure of the source image while providing enough degrees of freedom to manipulate the encoder's latent representation toward the target's.

### Mechanism 2
Bypassing non-differentiable quantization and entropy coding by optimizing latent-space distance enables gradient-based collision generation. Rather than directly optimizing bitstream equality, the attack minimizes ||f(x) − f(xtgt)||. When latent distance falls below quantization threshold, both images quantize to identical discrete values, producing identical bitstreams.

### Mechanism 3
Reduced numerical precision in latent tensors disrupts the high-fidelity gradient optimization required for collision. Converting float32 latent tensors to float16 introduces rounding noise that prevents the precise latent alignment needed for collision. Normal compression remains functional since it operates on fixed inputs with deterministic quantization, but the adversarial optimization path is broken.

## Foundational Learning

- Concept: **Neural Image Compression Pipeline (transform coding)** - Understanding encoder f → quantization → entropy coding → bitstream is required to grasp where collisions occur and why quantization bypass works. Quick check: Given an image x, trace the three stages that produce the final bitstream b. Where does the collision attack intervene?

- Concept: **Whitebox Adversarial Attacks (gradient-based optimization)** - The MGD attack assumes full model access and uses backpropagation through the encoder; distinguishing it from blackbox/transfer attacks clarifies the threat model. Quick check: What information does a whitebox attacker have that a blackbox attacker lacks? How does this enable the latent-space optimization?

- Concept: **Numerical Precision in Deep Learning (float32 vs float16 vs TF32)** - The LPD defense hinges on how precision affects gradient optimization vs inference; TF32 tensor cores provide ~3 decimal digits mantissa vs float16's ~3 vs float32's ~7. Quick check: Why does reduced precision break gradient-based collision attacks while preserving normal compression function?

## Architecture Onboarding

- Component map: Source Image (xsrc) → Encoder f(.) → Latent z → Quantization ⌊z⌉ → Entropy Coding e(.) → Bitstream b → Loss L = ||f(x) - f(xtgt)|| ← Mask M(.) ← Gradient ∂L/∂x

- Critical path: The collision hinges on encoder output matching. If ||f(xadv) − f(xtgt)|| < quantization granularity, both quantize identically → identical bitstreams. The mask M(.) is the only component preventing xadv from visually matching xtgt.

- Design tradeoffs:
  - Compression ratio vs vulnerability: Lower quality factors (higher compression) increase collision success rate
  - Precision vs security: LPD eliminates collisions but may slightly degrade rate-distortion performance
  - Attack iteration budget vs distortion: Early stopping on collision limits L2(xadv, xsrc) to ~0.5-0.7

- Failure signatures:
  - Standard attacks (PGD, CW) fail: Either no convergence (ASR=0) or convergence to xtgt (xadv ≈ xtgt)
  - Unmasked gradient descent: Converges to xtgt visually, not a useful collision
  - High QF (≥5): No collisions achievable even with MGD
  - Scale Hyperprior model: More resistant than Factorized Prior

- First 3 experiments:
  1. Reproduce collision on Factorized Prior model: Take CelebA image pair, implement MGD with Δh=3, Δw=1, verify bitstreams match after collision
  2. Test LPD defense effectiveness: Apply float16 conversion to latent tensors, re-run MGD attack, confirm ASR drops to 0
  3. Measure L2 distance bounds: For successful collisions, compute D_n(xtgt, xadv) and verify it falls within √2C bound

## Open Questions the Paper Calls Out

### Open Question 1
Can NIC encoder architectures be explicitly designed with small contraction constants to inherently resist bitstream collision attacks while maintaining competitive rate-distortion performance? The paper only proposes LPD as a defense; architectural modifications to reduce C remain unexplored.

### Open Question 2
What is the trade-off between LPD's security benefits and its impact on compression quality, computational efficiency, and bitstream length? The paper demonstrates LPD reduces ASR to 0 but does not quantify any degradation in rate-distortion performance or computational overhead.

### Open Question 3
Why do MGD-generated collision images transfer effectively within the Factorized Prior family but completely fail to transfer to the Scale Hyperprior model? Tables show ~90-97% transferability between FP-GDN and FP-ReLU, but 0% transferability from either to SH.

### Open Question 4
Are NIC models vulnerable to blackbox collision attacks where the adversary lacks access to model parameters and gradients? All experiments assume whitebox access; real-world attackers typically cannot query gradients or inspect model internals.

## Limitations
- Theoretical bounds on collision distances may be loose without empirical validation across diverse image pairs
- Defense scalability is uncertain - attackers could potentially initialize directly in float16 or use alternative precision reduction strategies
- Results are limited to 2018-era NIC architectures; generalizability to modern architectures remains untested

## Confidence

- **High confidence**: Attack mechanism successfully generates collisions; LPD defense effectively blocks attacks; ASR metrics are correctly measured
- **Medium confidence**: Theoretical bounds are mathematically sound but may be loose; vulnerability exists across multiple datasets and models as reported
- **Low confidence**: Long-term defense viability without additional hardening; impact of precision reduction on normal compression quality; generalizability to architectures beyond those tested

## Next Checks

1. **Bound validation**: For successful collision pairs, compute actual L2 distances and compare against theoretical √2C bound. Test whether the bound can be tightened with empirical constants for specific model architectures.

2. **Defense robustness**: Test LPD against an attack initialized directly in float16 (rather than float32 converted to float16). Also test stochastic rounding or other precision reduction methods to evaluate if LPD is the minimal effective defense.

3. **Cross-architecture testing**: Evaluate collision vulnerability on modern NIC architectures beyond 2018 Factorized Prior and Scale Hyperprior. Test whether architectural changes like attention mechanisms or different entropy coding reduce vulnerability.