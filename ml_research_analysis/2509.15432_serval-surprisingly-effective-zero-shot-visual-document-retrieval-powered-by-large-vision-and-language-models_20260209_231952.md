---
ver: rpa2
title: 'SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered
  by Large Vision and Language Models'
arxiv_id: '2509.15432'
source_url: https://arxiv.org/abs/2509.15432
tags:
- yang
- wang
- large
- text
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits zero-shot visual document retrieval by generating
  detailed textual descriptions of document images using vision-language models (VLMs),
  which are then encoded by standard text encoders. Evaluated on the ViDoRe-v2 and
  MIRACL-VISION benchmarks, this approach achieves 63.4% nDCG@5, surpassing state-of-the-art
  supervised multi-vector VDR models without any task-specific training.
---

# SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models

## Quick Facts
- arXiv ID: 2509.15432
- Source URL: https://arxiv.org/abs/2509.15432
- Authors: Thong Nguyen; Yibin Lei; Jia-Huei Ju; Andrew Yates
- Reference count: 16
- This paper achieves 63.4% nDCG@5 on ViDoRe-v2 benchmark, surpassing supervised multi-vector VDR models without task-specific training.

## Executive Summary
This paper presents SERVAL, a zero-shot visual document retrieval method that generates detailed textual descriptions of document images using vision-language models (VLMs), which are then encoded by standard text encoders. Evaluated on ViDoRe-v2 and MIRACL-VISION benchmarks, SERVAL achieves 63.4% nDCG@5, outperforming state-of-the-art supervised multi-vector VDR models without any task-specific training. The approach scales well to large collections and offers strong multilingual performance, making it a cost-effective and robust baseline for future VDR research.

## Method Summary
SERVAL generates comprehensive textual descriptions of document images using VLMs (Qwen2.5VL, InternVL3) offline, then encodes these descriptions with text encoders (INF, E5, Splade) for k-NN retrieval. The pipeline decouples modality alignment (handled by pretrained VLMs) from semantic matching (handled by text encoders), eliminating the need for contrastive text-image training. Document images are processed using Prompt 2.1 to extract visible text, numerical values, and visual elements, producing 500-1000 token descriptions per document.

## Key Results
- Achieves 63.4% nDCG@5 on ViDoRe-v2 benchmark, surpassing supervised multi-vector VDR models
- Scales effectively to large document collections with low online latency
- Demonstrates strong multilingual performance, particularly outperforming end-to-end VDR models on low-resource languages
- Shows that scaling text encoders provides greater retrieval quality gains than scaling VLMs

## Why This Works (Mechanism)

### Mechanism 1: Modality Bridging via Textual Descriptions
Converting visual documents to structured text descriptions preserves sufficient semantic information for effective retrieval. VLMs extract both textual content and visual semantics, compressing them into natural language that text encoders can process effectively.

### Mechanism 2: Offloaded Modality Alignment
Pretrained VLMs already contain sufficient cross-modal alignment, eliminating need for contrastive text-image training. The retrieval system inherits this alignment without additional training, as VLMs were trained on image-text pairs.

### Mechanism 3: Asymmetric Scaling Benefits
Scaling text encoders yields larger gains than scaling VLMs for retrieval quality. Larger text encoders provide better semantic matching between queries and generated descriptions, while VLM improvements yield diminishing returns once description quality is "good enough."

## Foundational Learning

- **Visual Document Retrieval (VDR)**: The core task of retrieving document images using text queries where documents contain both text and visual elements. Why needed: Understanding VDR is essential to grasp why standard text retrieval fails on PDF reports with embedded charts.
- **Vision-Language Models (VLMs) for Document Understanding**: VLMs like InternVL3 and Qwen2.5VL provide description generation capability. Why needed: Understanding their OCR and visual reasoning capabilities is essential for evaluating SERVAL's effectiveness.
- **Dense vs. Sparse Text Retrieval**: The paper evaluates both paradigms (Splade for sparse, E5/INF for dense). Why needed: Choosing the right encoder affects multilingual performance and storage requirements.

## Architecture Onboarding

- **Component map**: Document image → VLM description (offline) → Text encoder embedding (offline) → Vector Index → Query → Text encoder embedding (online) → k-NN search → Retrieved documents
- **Critical path**: Document image processed by VLM (0.26-1.61 sec/image) to generate descriptions, encoded offline, while queries are encoded online for k-NN retrieval
- **Design tradeoffs**: VLM size vs. latency (72B models cost 5-6x more inference time than 2B models), encoder choice (sparse vs. dense for multilingual support), description length impacts storage and compute
- **Failure signatures**: Low recall on non-English corpora with English-only encoders, hallucinations in VLM descriptions, excessive storage with verbose descriptions
- **First 3 experiments**: 1) Run InternVL3-2B + INF-1.5B on ViDoRe-v2 subset to validate pipeline (~54 nDCG@5 target), 2) Test simplified prompt vs. Prompt 2.1 for numerical extraction accuracy, 3) Evaluate on MIRACL-VISION low-resource languages to verify +15-25 point gains

## Open Questions the Paper Calls Out

- **How does hallucination in VLM-generated descriptions impact retrieval effectiveness?**: The authors identify "systematic evaluation of hallucinations" as a key direction, noting the method depends on text which "may introduce... hallucinations." This remains unresolved as the current study doesn't perform error analysis to determine if hallucinations act as beneficial query expansions or harmful noise.
- **Can SERVAL be adapted for multimodal queries?**: The paper states it focuses on "VDR with text-only queries, leaving multimodal query formulations... for future exploration." The current architecture relies on text encoders for queries, creating a bottleneck for visual query inputs.
- **Does combining VLM-generated text with explicit visual layout features yield superior performance?**: The authors list "hybrid text–layout representations" as a promising direction, as SERVAL currently unifies all visual cues into text descriptions, potentially discarding spatial relationships and layout structures.

## Limitations

- The core claims about modality bridging rely heavily on VLM description quality without direct validation of semantic information preservation versus loss
- The scaling analysis lacks sufficient ablation studies to definitively prove text encoder improvements are the primary driver
- Hallucination impact on retrieval effectiveness is acknowledged but not systematically evaluated

## Confidence

- **High Confidence**: Experimental results showing superior multilingual performance (63.4% nDCG@5) are well-supported by MIRACL-VISION benchmark results
- **Medium Confidence**: Claim that "modern VLMs capture complex textual and visual cues with sufficient granularity" is supported by results but lacks corpus evidence about description quality
- **Low Confidence**: Assertion that scaling text encoders provides greater benefits than scaling VLMs is based on limited comparisons requiring more extensive ablation studies

## Next Checks

1. Conduct a manual annotation study comparing VLM-generated descriptions against ground truth document content to quantify hallucination rates and information loss
2. Perform controlled scaling experiments where both VLM and text encoder sizes are varied systematically while holding other factors constant
3. Test the zero-shot capability on documents containing information that cannot be easily verbalized (e.g., spatial patterns, abstract visual metaphors) to establish fundamental limits