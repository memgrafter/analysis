---
ver: rpa2
title: Feedback-Induced Performance Decline in LLM-Based Decision-Making
arxiv_id: '2507.14906'
source_url: https://arxiv.org/abs/2507.14906
tags:
- feedback
- agent
- policy
- reward
- hwbp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs were evaluated as autonomous agents in Markov Decision Processes
  using zero-shot structured prompting. Performance was compared to PPO-based reinforcement
  learning across three MiniGrid configurations of increasing complexity.
---

# Feedback-Induced Performance Decline in LLM-Based Decision-Making

## Quick Facts
- arXiv ID: 2507.14906
- Source URL: https://arxiv.org/abs/2507.14906
- Authors: Xiao Yang; Juxi Leitner; Michael Burke
- Reference count: 40
- LLMs show degraded decision-making performance when feedback mechanisms are added, despite initial success in simpler environments

## Executive Summary
This paper evaluates large language models as autonomous agents in Markov Decision Processes using zero-shot structured prompting across three MiniGrid configurations. The study compares LLM performance against PPO-based reinforcement learning, revealing that while LLMs achieve initial success in simpler environments, their performance degrades significantly in complex scenarios—especially when feedback mechanisms are introduced. The research demonstrates that additional feedback (state transitions, rewards, policy traces) often leads to diminished decision-making effectiveness, suggesting that extraneous context misallocates the model's attention. PPO consistently achieves near-perfect performance across all configurations, while LLM policies struggle with planning and reasoning without fine-tuning.

## Method Summary
The study uses MiniGrid environments with three configurations of increasing complexity (5x5 empty, 16x16 empty, 9x9 with partition wall) to evaluate LLMs as decision-making agents. Nine prompting strategies are tested, ranging from simple HWBP to complex combinations including CoT, DF, RF, CRF, and PF. The LLMs (Llama 3.1 8B, Qwen 2.5 1.5B via Ollama API) process text-encoded grid states and generate JSON actions. Performance is measured through cumulative reward per episode and success rate across 100 episodes per approach with a 100-step limit. PPO baselines are trained using stable_baselines3 with custom CNN extractors. Reasoning models (Deepseek R1 14B, QwQ 32B) are also evaluated with one-shot prompting.

## Key Results
- LLMs achieve initial success in simpler environments but show degraded performance in complex scenarios
- Additional feedback mechanisms often lead to diminished decision-making effectiveness rather than improvement
- PPO achieves near-perfect performance in all configurations, while LLM policies struggle without fine-tuning
- Qwen 2.5 1.5B outperforms larger Llama 3.1 8B in several configurations despite having less than 20% of parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing incremental feedback signals to LLMs in sequential decision-making tasks can degrade rather than improve performance
- Mechanism: Additional context in prompts dilutes the model's attention, causing misallocation of focus away from task-critical signals
- Core assumption: Assumes attention dilution is the primary cause; this is inferred from observed performance patterns rather than directly measured attention mechanisms
- Evidence anchors:
  - [abstract] "Our results show that feedback mechanisms, intended to improve decision-making, often introduce confusion, leading to diminished performance in intricate environments"
  - [section IV-E] "simply appending more feedback can dilute the model's attention, misallocating focus away from critical task-relevant signals"
  - [corpus] Related work (Kambhampati et al.) supports that LLMs struggle with genuine planning, suggesting feedback accumulation compounds retrieval-based limitations
- Break condition: This mechanism likely fails when feedback is explicitly filtered or summarized rather than raw accumulation, or when external verifiers structure the feedback loop

### Mechanism 2
- Claim: LLMs leverage approximate retrieval from pre-training rather than performing genuine multi-step planning required for MDPs
- Mechanism: When task complexity exceeds patterns seen during pre-training, the model cannot construct valid action sequences
- Core assumption: Assumes performance gaps reflect fundamental limitations in planning capability rather than prompt design alone
- Evidence anchors:
  - [section II] "LLMs continue to struggle to produce reliable and executable plans in complex domains"
  - [section IV-E] "LLMs can leverage pre-trained knowledge to achieve some zero-shot success—particularly in simpler configurations—but struggle to generalize in more complex environments"
  - [corpus] Huang et al. show LLM-generated plans often fail execution despite high human evaluation scores
- Break condition: Fine-tuning on domain-specific MDP data or integration with symbolic planners would likely change this dynamic

### Mechanism 3
- Claim: Smaller models with better task alignment can outperform larger models when prompt engineering matches model capabilities
- Mechanism: Qwen 2.5 1.5B outperformed Llama 3.1 8B in several configurations despite having less than 20% of parameters
- Core assumption: Assumes the comparison is fair and differences reflect model characteristics rather than randomness
- Evidence anchors:
  - [section IV-A] "Qwen2.5:1.5B—despite having less than 20% of the parameters of LLaMA3.1:8B—can outperform the latter"
  - [tables II, III, IV] Qwen shows higher success rates in Config 2 and resists complete collapse better in Config 3
  - [corpus] Survey on LLM-based agents notes optimization strategies beyond scale are critical for agent performance
- Break condition: This advantage may not hold across different task domains or with optimized prompting for larger models

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: The entire experimental framework models sequential decision-making as MDPs where agents select actions based on states to maximize cumulative rewards
  - Quick check question: Can you explain why an optimal policy in an MDP must consider future reward accumulation rather than just immediate rewards?

- Concept: **Zero-shot vs. Fine-tuned Decision-Making**
  - Why needed here: The paper deliberately tests zero-shot prompting against RL methods that learn through exploration
  - Quick check question: What is the fundamental difference between updating model weights via RL and providing feedback through prompts?

- Concept: **Prompt Context Window and Attention**
  - Why needed here: Performance decline with added feedback relates to how LLMs distribute attention across prompt tokens
  - Quick check question: Why might adding 500 tokens of feedback history to a prompt reduce decision quality even if the information is relevant?

## Architecture Onboarding

- Component map: Environment (MiniGrid) → Text Encoder → Prompt Composer → LLM (Llama/Qwen/Reasoning models) → Action Parser → Environment
- Critical path: 1. Environment state → text representation; 2. Prompt assembly (base + optional CoT + DF + RF + CRF + PF); 3. LLM inference → JSON action extraction; 4. Action execution → new state/reward; 5. Feedback buffer update for next step
- Design tradeoffs:
  - **Prompt complexity vs. attention focus**: More feedback types provide richer context but risk attention dilution (empirically harms performance)
  - **Model size vs. inference cost**: Larger reasoning models show some improvement but generate many tokens during "thinking," increasing latency and cost
  - **State-only vs. memory-augmented policies**: Memory enables potential learning but introduces confusion without proper structuring
- Failure signatures:
  - **Complete collapse to 0% success**: Occurs with Policy Feedback (PF) addition in LLaMA for Config 2 and Config 3
  - **Random-or-worse performance**: LLaMA 3.1 8B with PF performs worse than random policy (0.0 vs 0.047 in Config 2)
  - **Reasoning model non-generation**: QwQ 32B failed to generate any plan for Config 3 (partition wall)
- First 3 experiments:
1. Reproduce the feedback degradation curve: Run HWBP → +CoT → +DF → +RF → +CRF → +PF progression on Config 1 with a single model to confirm performance decline pattern
2. Test filtered feedback: Implement a feedback summarizer that extracts only action-relevant information; compare against raw feedback approach
3. Cross-configuration generalization: Take the best-performing prompt configuration from Config 1 and apply it unchanged to Config 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid strategies or external verifiers mitigate the performance decline caused by feedback integration in LLM-based decision-making?
- Basis in paper: [explicit] The conclusion states that "feedback mechanisms... often lead to policy degradation" and explicitly underscores the "need for further exploration into hybrid strategies"
- Why unresolved: This study deliberately avoided external solvers or verifiers to test intrinsic capabilities
- What evidence would resolve it: Experiments where an external verifier filters invalid LLM actions before environment execution

### Open Question 2
- Question: Does domain-specific fine-tuning enable LLMs to effectively utilize reward and dynamics feedback without succumbing to confusion?
- Basis in paper: [explicit] The authors highlight "the need for further exploration into... fine-tuning" as a result of LLMs struggling to reason without it
- Why unresolved: The experiments utilized zero-shot, prompt-based policies with frozen pre-trained weights
- What evidence would resolve it: A comparison of feedback-integrated performance between zero-shot and fine-tuned LLMs in MiniGrid

### Open Question 3
- Question: What are the underlying attentional mechanisms causing the "dilution of focus" when context length increases with feedback?
- Basis in paper: [inferred] The authors suggest performance drops because additional feedback dilutes the model's attention, but they do not perform a mechanistic analysis
- Why unresolved: The methodology focused on empirical outcome metrics rather than internal model states
- What evidence would resolve it: Attention visualization showing how feedback tokens suppress attention weights for critical state tokens

## Limitations
- The paper's conclusions about feedback-induced performance degradation rely on correlation rather than direct measurement of attention mechanisms
- The comparison between LLM and PPO performance assumes fair evaluation conditions, but critical implementation details remain unspecified
- The study focuses exclusively on MiniGrid environments, limiting generalizability to other MDP domains

## Confidence
- **High confidence**: Performance degradation with feedback addition (empirically demonstrated across multiple configurations and models)
- **Medium confidence**: Attention dilution as the primary mechanism (plausible but not directly measured)
- **Medium confidence**: Fundamental planning limitations of LLMs in complex MDPs (supported by literature but not conclusively proven in this study alone)

## Next Checks
1. Measure actual attention distributions in the LLM when processing prompts with varying feedback levels to directly test the attention dilution hypothesis
2. Implement the proposed filtered feedback approach (extracting only action-relevant information) and compare performance against raw feedback accumulation
3. Test cross-task generalization by applying the best-performing prompt configuration from simple environments to structurally different MDP domains