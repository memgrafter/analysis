---
ver: rpa2
title: 'Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake
  Image Detection with FusionDetect'
arxiv_id: '2510.05740'
source_url: https://arxiv.org/abs/2510.05740
tags:
- image
- images
- conference
- detection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the two-axis generalization problem in AI-generated\
  \ image detection, where detectors must generalize not only to unseen generators\
  \ but also to different visual domains. To tackle this, the authors introduce FusionDetect,\
  \ a detector that fuses features from two frozen foundational models\u2014CLIP for\
  \ semantic breadth and Dinov2 for structural details\u2014via concatenation and\
  \ an MLP classifier."
---

# Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect

## Quick Facts
- arXiv ID: 2510.05740
- Source URL: https://arxiv.org/abs/2510.05740
- Reference count: 33
- FusionDetect achieves new SOTA with 3.87% higher accuracy and 6.13% higher AP than previous methods

## Executive Summary
This work addresses the two-axis generalization problem in AI-generated image detection, where detectors must generalize not only to unseen generators but also to different visual domains. The authors introduce FusionDetect, a detector that fuses features from two frozen foundational models—CLIP for semantic breadth and Dinov2 for structural details—via concatenation and an MLP classifier. They also present the OmniGen benchmark, a new test set with 12 diverse SOTA generators. FusionDetect achieves a new state-of-the-art, outperforming its closest competitor by 3.87% in accuracy and 6.13% in average precision on established benchmarks, and shows a 4.48% accuracy increase on OmniGen with minimal standard deviation, indicating robust cross-domain performance.

## Method Summary
FusionDetect uses frozen CLIP-ViT-L14 and DINOv2-L14 backbones to extract features from images, which are then concatenated and passed through a 4-layer MLP classifier for binary classification. The model is trained on 60,000 images (30k real, 30k fake) from ImagiNet, GenImage, and Chameleon datasets, with images generated using SD1.4 and SD2.1. Training includes random JPEG compression and Gaussian blur augmentations applied to 10% of samples. The model is evaluated on GenImage (8k), ImagiNet (10k), Chameleon (2,595), and OmniGen (12,550 fake + 1,000 real from 12 SOTA generators) using accuracy, average precision, and standard deviation metrics.

## Key Results
- FusionDetect outperforms previous SOTA by 3.87% in accuracy and 6.13% in AP on established benchmarks
- Achieves 4.48% accuracy increase on OmniGen benchmark with minimal standard deviation
- Demonstrates superior resilience to image perturbations like JPEG compression and Gaussian blur

## Why This Works (Mechanism)
The two-axis generalization framework addresses both cross-generator and cross-semantic domain challenges in fake image detection. By fusing CLIP's semantic understanding with DINOv2's structural detail extraction, FusionDetect captures complementary visual information that enables robust detection across diverse generators and domains. The frozen foundational models prevent overfitting to specific generator artifacts while the MLP classifier learns to combine these complementary features effectively.

## Foundational Learning
- **Two-axis generalization**: Why needed - AI-generated images vary by both generator architecture and semantic content; quick check - evaluate performance across both axes separately
- **Feature fusion**: Why needed - single backbone cannot capture both semantic and structural cues; quick check - compare single vs dual backbone performance
- **Frozen foundational models**: Why needed - prevents overfitting to training generator artifacts; quick check - verify backbone weights remain unchanged during training
- **Concatenation-based fusion**: Why needed - preserves complementary information from different feature spaces; quick check - experiment with alternative fusion methods
- **BCE loss for binary classification**: Why needed - standard approach for real/fake detection; quick check - monitor training loss convergence
- **Cross-domain evaluation**: Why needed - ensures generalization beyond training distributions; quick check - test on held-out generators and semantic categories

## Architecture Onboarding

Component map: CLIP-ViT-L14 -> Feature extraction -> Concatenation -> DINOv2-L14 -> Feature extraction -> Concatenation -> MLP classifier -> Binary output

Critical path: Image → CLIP backbone → DINOv2 backbone → Feature concatenation → MLP → Classification decision

Design tradeoffs: Using frozen backbones trades some fine-tuning flexibility for robustness to training set bias; concatenation fusion is simpler but potentially less expressive than attention-based methods

Failure signatures: High accuracy on training generators but poor performance on new generators indicates overfitting; high variance across benchmarks suggests insufficient cross-domain coverage

First experiments:
1. Train with only CLIP backbone to establish baseline performance
2. Train with only DINOv2 backbone to establish baseline performance  
3. Evaluate on single-generator subsets to identify domain-specific weaknesses

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical implementation details including AdamW hyperparameters and MLP architecture specifications
- Limited to SD1.4 and SD2.1 generators in training set, potentially limiting generalization to newer architectures
- Evaluation focused on specific perturbation types without exploring the full space of image degradations

## Confidence
- Performance claims: Medium (clear methodology but missing implementation details)
- Methodological soundness: High (well-justified two-axis framework)
- Robustness claims: Medium (evaluation setup clear but perturbation parameters unspecified)

## Next Checks
1. Reconstruct the exact training prompt set from Chameleon and generate a held-out validation set to test prompt diversity impact
2. Perform an ablation study removing either CLIP or DINOv2 to quantify the marginal benefit of each backbone
3. Evaluate FusionDetect on a newly generated set of images from emerging generators (e.g., SDXL, Midjourney v6) to assess true out-of-distribution generalization beyond the tested benchmarks