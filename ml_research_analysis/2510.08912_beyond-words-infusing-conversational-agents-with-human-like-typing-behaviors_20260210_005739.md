---
ver: rpa2
title: 'Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors'
arxiv_id: '2510.08912'
source_url: https://arxiv.org/abs/2510.08912
tags:
- agent
- typing
- participants
- user
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the impact of human-like typing behaviors on
  user perceptions of conversational AI agents. The authors implemented a platform
  that simulates hesitation and self-editing behaviors in AI-generated responses,
  allowing users to customize the communication style.
---

# Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors

## Quick Facts
- arXiv ID: 2510.08912
- Source URL: https://arxiv.org/abs/2510.08912
- Authors: Jijie Zhou; Yuhan Hu
- Reference count: 40
- Key outcome: AI agents with combined hesitation and self-editing behaviors were perceived as more natural, human-like, and trustworthy than agents with hesitation alone or baseline instant responses.

## Executive Summary
This paper explores whether simulating human-like typing behaviors—hesitation and self-editing—can make conversational AI agents feel more natural and trustworthy. The authors developed a platform that injects realistic delays and visible corrections into AI responses, allowing users to customize the communication style. A controlled study with 50 participants compared three agents: a baseline with instant responses, one with hesitation behaviors, and one with both hesitation and self-editing. Results showed that the combined-behavior agent was slightly preferred, with users finding it more natural, human-like, and intelligent. Notably, self-editing was especially valued as a signal of error-correction ability and cognitive effort. The study suggests that incorporating self-editing into AI agents could enhance user perceptions of naturalness and trustworthiness in conversational AI interactions.

## Method Summary
The study implemented a Flutter-based cross-platform app that intercepts OpenAI API responses and simulates human-like typing behaviors. Hesitation was modeled using normal distributions for typing and pause durations, while self-editing randomly deleted, inserted, or modified words at various granularities. Three agent profiles were tested: a baseline with instant responses, a hesitation-only agent, and a combined agent with both behaviors. A within-subject study with 50 participants compared these agents over 10-minute casual conversations, measuring subjective ratings (naturalness, competence, human-likeness) and objective metrics (word count, interaction duration).

## Key Results
- The agent with both hesitation and self-editing behaviors was perceived as more natural and human-like than hesitation-only or baseline agents.
- Self-editing was appreciated as a signal of intelligence and error-correction ability, increasing perceived competence.
- Hesitation alone did not improve human-likeness and was sometimes perceived as "robotic" or "defective" if not paired with meaningful content.
- Users engaged most extensively with the combined-behavior agent, showing longer interaction durations and higher word counts.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visible self-correction signals cognitive effort, increasing perceived competence and human-likeness more than hesitation alone.
- **Mechanism:** Users interpret the visual process of typing, deleting, and retyping as a "thought process" or an intention to communicate accurately. This transforms a delay (usually negative) into a positive signal of animacy.
- **Core assumption:** Users are observing the *process* of generation in real-time, not just reading the final static text.
- **Evidence anchors:**
  - [abstract] "...agent with both behaviors was perceived as more natural and human-like... advocating for more human-like conversational AI design."
  - [section 5.2.3] Participants described the Red agent (with self-editing) as "thoughtful" and "trying to mimic human behavior," whereas the Green agent (hesitation only) was labeled "dumb" and "robotic."
  - [corpus] Related work on "Simulating Errors in Touchscreen Typing" suggests understanding cognitive mechanisms behind errors is crucial for modeling human behavior, supporting the link between editing and cognition.
- **Break condition:** If the response latency exceeds user patience thresholds without perceived value (e.g., purely spinning loader), the effect reverses to frustration.

### Mechanism 2
- **Claim:** Hesitation without actionable intent (self-editing) reduces perceived competence.
- **Mechanism:** Mere temporal delay creates an expectation of complex cognitive processing. If the output following a delay is simple or generic, a "expectation-violation" occurs, making the agent appear slow or defective rather than thoughtful.
- **Core assumption:** Users associate time-cost with computational or cognitive depth.
- **Evidence anchors:**
  - [section 5.1.1] The Green agent (hesitation-only) received the lowest scores in naturalness and human-likeness, failing to support hypothesis H1 (hesitation enhances human-likeness).
  - [section 5.2.2] Participants criticized the hesitation-only agent for not actively extending conversations or understanding context, perceiving the delay as a defect.
  - [corpus] Corpus evidence regarding "Controlling AI Agent Participation" highlights the complexity of interaction dynamics, implying timing must be matched with appropriate content valence.
- **Break condition:** If the agent hesitates significantly but then outputs a short, low-effort response, the illusion of "thinking" breaks.

### Mechanism 3
- **Claim:** Behavioral synchrony (extended interaction duration) increases user engagement volume.
- **Mechanism:** By slowing down the interaction pace to human speeds, the system normalizes longer turn-taking latencies, encouraging users to write more and spend more time in the session.
- **Core assumption:** Engagement is measured by duration and word count, not just task completion speed.
- **Evidence anchors:**
  - [section 5.1.2] Users engaged most extensively with the Red agent (combined behaviors), showing the longest interaction duration and highest word count.
  - [section 5.1.2] Linear regression shows a positive correlation (R² = 0.79) between agent word count and user word count.
  - [corpus] "Exploring Anthropomorphism..." suggests anthropomorphic design shifts consumption patterns, potentially increasing interaction depth.
- **Break condition:** In time-critical task scenarios (e.g., emergency support), increased duration may be perceived negatively as inefficiency.

## Foundational Learning

- **Concept: Anthropomorphism & Animacy**
  - **Why needed here:** To understand *why* imperfect, human-like behaviors (typos, pauses) might be preferred over "perfect" robotic efficiency in social contexts.
  - **Quick check question:** Does the interface aim to solve a transactional task (efficiency focus) or build a relationship (social focus)?

- **Concept: Streaming Outputs (vs. Batch)**
  - **Why needed here:** The system requires a UI architecture that renders text character-by-character rather than displaying a completed block, enabling the simulation of typing speed and live editing.
  - **Quick check question:** Can the current frontend handle granular state updates (single character insertion/deletion) without re-rendering the entire message history?

- **Concept: Statistical Normal Distribution in UI**
  - **Why needed here:** The implementation uses normal distributions to model typing pace and pauses to avoid mechanical regularity (randomness principle).
  - **Quick check question:** How do you prevent the random variance from creating an outlier delay that feels like a system freeze?

## Architecture Onboarding

- **Component map:**
  - OpenAI API -> Simulation Layer (Flutter) -> Rendering Layer
- **Critical path:**
  1. API Request sent.
  2. Response received but hidden.
  3. Simulation Layer parses text into a queue of actions (Type "H", Pause 0.1s, Type "i", Delete "i", Type "ello").
  4. UI executes the queue sequentially.
- **Design tradeoffs:**
  - **Perceived Naturalness vs. Efficiency:** Increasing `spaceLagPace` improves human-likeness (Section 5.1.2) but drastically increases total interaction time.
  - **Randomness vs. Coherence:** High rates of random self-editing (Section 3.3.2) can make the agent appear erratic or confusing if the replacements are contextually weak (Section 6.2.1).
- **Failure signatures:**
  - **The "Dumb" Effect:** High latency (hesitation) combined with simple responses (Section 5.1.1 Green agent).
  - **The "Uncanny" Effect:** Self-editing that replaces words with irrelevant synonyms (due to simple library lookup) or deleting sentence starts (Section 6.2.1).
  - **State Desync:** If the UI edits text visible to the user, the internal state must track the *final* text for context history, not the intermediate "typo" text.
- **First 3 experiments:**
  1. **Parameter Sweeping:** Test different `characterTypingPace` values against user "Naturalness" ratings to find the sweet spot between "too fast" (robotic) and "too slow" (frustrating).
  2. **Edit Visibility A/B Test:** Compare agents where edits are visible (current design) vs. agents where edits happen invisibly before display, to isolate if the *process* or just the *result* drives engagement.
  3. **Task Context Stress Test:** Deploy the "Red" agent in a strict utility scenario (e.g., "Reset my password") to verify if the "thoughtful" delays are perceived as negative latency in non-social tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does aligning an agent's typing style (speed/hesitation) with the specific typing behavior of the individual user improve engagement?
- Basis in paper: [explicit] The Future Work section states the authors aspire to "investigate whether users prefer interacting with chat agents whose typing styles align with their own" using deep learning models.
- Why unresolved: The current pilot study used fixed, generic parameters for hesitation and editing rather than personalized profiles, leaving the effect of style matching unknown.
- What evidence would resolve it: A user study that captures user typing biometrics, generates an agent with matching temporal parameters, and compares user engagement metrics against a non-matched control agent.

### Open Question 2
- Question: Does a "mixed model" where the agent occasionally leaves errors uncorrected result in higher perceived human-likeness than a model that always self-corrects?
- Basis in paper: [explicit] The authors propose "investigating the impact of allowing some errors to remain uncorrected, as part of a mixed model" to understand the necessary level of human-likeness.
- Why unresolved: The current implementation focused on the *process* of correction; it did not test the human-like trait of failing to catch every typo, which may appear more authentic.
- What evidence would resolve it: An A/B test comparing agents with varying rates of uncorrected typos (e.g., 0% vs. 10% vs. 20%) to measure user ratings on naturalness and animacy.

### Open Question 3
- Question: Do hesitation and self-editing behaviors positively impact user perception in task-oriented contexts, such as customer service?
- Basis in paper: [explicit] The authors note their focus on casual chat "may not fully address contexts where chatbots operate in roles, such as customer service" and call for investigation into diverse applications.
- Why unresolved: It is unclear if the "inefficiency" introduced by hesitation and editing will frustrate users in goal-oriented scenarios where speed and accuracy are prioritized over social connection.
- What evidence would resolve it: A user study in a functional domain (e.g., banking or booking) measuring task completion time, success rate, and perceived competence with and without human-like typing behaviors.

## Limitations

- The study's findings are based on a controlled lab experiment with 50 participants interacting in a single 10-minute session with three predefined agents.
- The specific parameter values for typing speeds, pause durations, and self-editing rates are not reported, making it difficult to determine optimal configurations for real-world deployment.
- The study used GPT-3.5 (text-davinci-003) with a system prompt limiting response length, which may not generalize to current LLMs or open-ended conversations.
- The simulated self-editing used simple word replacement from a "redundant word" library, which may not reflect sophisticated error correction.

## Confidence

- **High Confidence**: The core finding that hesitation alone does not improve perceived human-likeness, and that self-editing combined with hesitation shows benefits in naturalness and perceived competence. This is well-supported by consistent participant feedback across multiple metrics.
- **Medium Confidence**: The claim that behavioral synchrony increases engagement duration and word count. While statistically significant, the effect could be partially attributed to the novelty of the interaction rather than the behavioral modifications themselves.
- **Low Confidence**: The generalizability of specific parameter values (typing speeds, pause durations) for optimal naturalness across different user populations and use cases. The paper does not report these critical configuration details.

## Next Checks

1. **Parameter Optimization Study**: Conduct a controlled experiment to identify optimal ranges for typing pace and pause duration parameters across different user demographics and interaction contexts (casual vs. task-oriented).

2. **Long-term Interaction Assessment**: Deploy the combined behavior agent in extended real-world conversations (multiple sessions over weeks) to evaluate whether the initial engagement benefits persist or decay due to novelty effects.

3. **Task Context Validation**: Test the three agent behaviors in high-stakes utility scenarios (technical support, emergency information) to determine if the "thoughtful" delays remain perceived as positive signals or become frustrating latency.