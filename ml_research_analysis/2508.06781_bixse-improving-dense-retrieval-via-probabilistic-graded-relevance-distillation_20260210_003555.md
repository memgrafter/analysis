---
ver: rpa2
title: 'BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation'
arxiv_id: '2508.06781'
source_url: https://arxiv.org/abs/2508.06781
tags:
- relevance
- bixse
- retrieval
- training
- graded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training dense retrieval models
  using binary relevance labels, which can be limiting as real-world relevance exists
  on a continuum. The authors propose BiXSE, a simple and effective pointwise training
  method that optimizes binary cross-entropy (BCE) loss over LLM-generated graded
  relevance scores, interpreting them as probabilistic targets.
---

# BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation

## Quick Facts
- **arXiv ID:** 2508.06781
- **Source URL:** https://arxiv.org/abs/2508.06781
- **Reference count:** 39
- **Primary result:** BiXSE improves dense retrieval performance by optimizing BCE loss over LLM-generated graded relevance scores, outperforming InfoNCE and matching pairwise baselines.

## Executive Summary
This paper addresses the limitation of binary relevance labels in training dense retrieval models by proposing BiXSE, a method that leverages LLM-generated graded relevance scores as probabilistic targets. The approach uses binary cross-entropy loss to capture the continuous nature of relevance from pointwise supervision. Extensive experiments show BiXSE consistently outperforms standard contrastive learning methods like InfoNCE, matches or exceeds strong pairwise ranking baselines, and demonstrates improved robustness to label noise. The method offers a practical solution as graded relevance supervision becomes increasingly accessible.

## Method Summary
BiXSE trains dense retrieval models by optimizing binary cross-entropy loss over continuous relevance scores generated by LLMs, interpreting them as probabilistic targets. The method converts discrete relevance scores (e.g., 1-5) to continuous values via weighted averaging of LLM probabilities, then normalizes to [0,1]. During training, learnable logit scale α and bias β are applied to the dot product similarity, with β trained at higher learning rate than the encoder. The loss treats in-batch negatives as having zero relevance. BiXSE operates as a simple pointwise training method that captures relevance continuity while remaining computationally efficient.

## Key Results
- Outperforms InfoNCE on binary relevance benchmarks (BEIR, MTEB, TREC-DL) while using simpler pointwise supervision
- Matches or exceeds pairwise ranking baselines like ColBERT-MLP on NDCG@10 across multiple datasets
- Demonstrates improved robustness to label noise compared to contrastive methods
- Shows consistent gains when trained on LLM-generated graded relevance scores from LightBlue dataset

## Why This Works (Mechanism)
BiXSE works by recognizing that real-world relevance exists on a continuum rather than as binary values. By using BCE loss over probabilistic targets derived from LLM-generated graded scores, the method can capture nuanced relevance relationships during training. The learnable logit bias β, trained at higher learning rate than the encoder, helps balance the gradient contribution from positive and negative examples, especially important given the prevalence of in-batch negatives with zero relevance. This approach allows dense retrievers to learn from richer supervision signals while maintaining the computational efficiency of pointwise training.

## Foundational Learning

**Binary Cross-Entropy Loss**: BCE loss measures the difference between predicted probabilities and binary targets, making it suitable for classification tasks. For BiXSE, it's used with continuous targets [0,1] to capture graded relevance, providing smooth gradients that reflect relevance strength.

**Contrastive Learning (InfoNCE)**: InfoNCE uses temperature-scaled softmax over positive-negative pairs to maximize agreement between relevant items while minimizing agreement with negatives. BiXSE's BCE approach is simpler and more robust to label noise compared to InfoNCE's reliance on clean binary labels.

**Graded Relevance Supervision**: Real-world relevance often exists on a continuum (e.g., scores 1-5). Converting discrete scores to continuous probabilities via weighted averaging preserves this nuance, allowing models to learn fine-grained relevance distinctions rather than treating all relevant items equally.

## Architecture Onboarding

**Component Map**: Query Encoder -> Document Encoder -> Dot Product Similarity -> Logit Scale/Bias -> BiXSE Loss

**Critical Path**: The forward pass computes query-document similarity, applies learnable scaling and bias to logits, then calculates BCE loss against continuous targets. The critical learning component is the differential learning rates for β versus encoder parameters.

**Design Tradeoffs**: BiXSE sacrifices some modeling capacity compared to pairwise methods but gains in training efficiency and robustness to label noise. The method requires access to LLM-generated graded scores but doesn't need pairwise relevance judgments during training.

**Failure Signatures**: Underperformance relative to InfoNCE on clean binary datasets suggests issues with target normalization or logit bias training. Degraded performance with larger batch sizes often indicates β learning rate is too low relative to encoder LR.

**First Experiments**:
1. Implement BiXSE loss with synthetic continuous targets (e.g., uniform [0,1]) to verify correct gradient flow
2. Train with varying β learning rates (5x, 10x, 20x encoder LR) to confirm sensitivity
3. Compare BiXSE vs InfoNCE on a binary benchmark with known optimal performance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on quality of LLM-generated graded relevance scores, which are not fully transparent in their generation process
- Requires careful tuning of logit bias learning rate relative to encoder learning rate
- May underperform InfoNCE on datasets with clean binary labels where contrastive learning's noise robustness is beneficial

## Confidence

**High Confidence**: The BiXSE loss formulation is mathematically well-defined and clearly stated. Superiority over InfoNCE on binary relevance tasks is strongly supported by experimental results across multiple benchmarks.

**Medium Confidence**: Claims about robustness to label noise and performance on graded relevance tasks depend on LLM-derived continuous scores quality, which lacks full transparency. Competitive performance with pairwise methods is shown but depends on exact baseline training configurations.

**Low Confidence**: Specific improvements for different model sizes are difficult to verify without precise learning rate schedules and LoRA configurations for each variant.

## Next Checks

1. **Target generation validation**: Implement the LLM-to-continuous-score conversion using the provided formula and test with a small subset of LightBlue data. Verify that the resulting targets are properly normalized to [0,1] and show expected correlation with discrete relevance levels.

2. **Hyperparameter sensitivity test**: Train BiXSE with varying logit bias learning rates (5x, 10x, 20x encoder LR) on a binary benchmark (e.g., E5) to confirm that higher β LR is necessary for optimal performance, as suggested by the paper's Figure 5.

3. **In-batch negative handling verification**: Confirm that setting z_{i,j}=0 for in-batch negatives (i≠j) is implemented correctly and does not introduce label noise. Test with a small synthetic dataset where ground truth relevance is known.