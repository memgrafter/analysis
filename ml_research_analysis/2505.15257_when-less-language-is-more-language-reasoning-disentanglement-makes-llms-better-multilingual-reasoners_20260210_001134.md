---
ver: rpa2
title: 'When Less Language is More: Language-Reasoning Disentanglement Makes LLMs
  Better Multilingual Reasoners'
arxiv_id: '2505.15257'
source_url: https://arxiv.org/abs/2505.15257
tags:
- reasoning
- ablation
- language
- performance
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free approach to improve multilingual
  reasoning in large language models (LLMs) by disentangling language-specific representations
  from reasoning representations. The method identifies and ablates language-specific
  components in the model's hidden states during inference, reducing linguistic interference
  and enabling more language-invariant reasoning.
---

# When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners

## Quick Facts
- arXiv ID: 2505.15257
- Source URL: https://arxiv.org/abs/2505.15257
- Reference count: 40
- Key outcome: Language-specific component ablation improves multilingual reasoning by 1.4-4.5 percentage points across 10 models and 11 languages

## Executive Summary
This paper proposes a training-free approach to improve multilingual reasoning in large language models by disentangling language-specific representations from reasoning representations. The method identifies and ablates language-specific components in hidden states during inference, reducing linguistic interference and enabling more language-invariant reasoning. Experiments across 10 open-weight models (3B-32B parameters) and 11 typologically diverse languages show consistent performance improvements on three multilingual reasoning benchmarks, with particularly strong gains in low-resource languages.

## Method Summary
The approach works by identifying language-specific components in the model's hidden states through an auxiliary classification task, then ablating these components during inference. The method operates at inference time without requiring additional training, making it a lightweight alternative to post-training methods like SFT and RL. The language-specific component identification relies on analyzing hidden states across different layers, with ablation applied selectively based on layer-wise analysis that identifies optimal trade-offs between reasoning gains and linguistic fidelity.

## Key Results
- Consistent performance improvements of 1.4-4.5 percentage points in average accuracy across three multilingual reasoning benchmarks
- Particularly strong improvements in low-resource languages, with some models showing more than doubling of accuracy (e.g., Swahili)
- Performance comparable to or better than multilingual post-training methods (SFT and RL)
- Middle layers identified as providing the best trade-off between reasoning gains and linguistic fidelity

## Why This Works (Mechanism)
The method works by reducing linguistic interference in the model's reasoning processes. Language-specific components in hidden states can create interference patterns that degrade reasoning performance, particularly when models must reason across typologically diverse languages. By identifying and ablating these language-specific components, the model can focus on language-invariant reasoning patterns. This disentanglement allows the model to leverage its reasoning capabilities more effectively across different languages without being constrained by language-specific linguistic patterns that may not generalize well.

## Foundational Learning
- **Language-specific vs. reasoning representations**: Understanding that models encode both language-specific information and reasoning capabilities in their hidden states is crucial. This distinction is necessary because conflating these can lead to suboptimal cross-lingual reasoning performance.
- **Layer-wise analysis in transformer models**: The method requires understanding how different transformer layers encode different types of information. Middle layers often capture task-relevant features while preserving linguistic structure.
- **Ablation techniques in inference**: The approach uses targeted component removal during inference rather than model retraining. This is important for maintaining efficiency while achieving improvements.
- **Cross-lingual interference patterns**: Recognizing that language-specific features can interfere with reasoning tasks across different languages helps explain why disentanglement is beneficial.
- **Multilingual benchmark design**: The evaluation uses diverse benchmarks (math reasoning, commonsense reasoning, knowledge-intensive QA) across 11 languages, providing comprehensive validation of the approach.
- **Hidden state analysis**: The method relies on analyzing and manipulating model hidden states to identify and remove language-specific components, requiring understanding of how information is encoded in transformer architectures.

## Architecture Onboarding
**Component Map**: Input sequence → Hidden states across layers → Language-specific component identification → Component ablation → Output generation

**Critical Path**: The core innovation involves analyzing hidden states during inference, identifying language-specific components through auxiliary classification, and selectively ablating these components before they influence subsequent processing. This creates a modified inference pipeline that reduces linguistic interference.

**Design Tradeoffs**: The approach trades some linguistic fidelity for improved reasoning performance. While this enhances cross-lingual reasoning, it may slightly degrade language-specific capabilities like translation accuracy. The method also introduces computational overhead during inference due to the component identification and ablation process.

**Failure Signatures**: The method may fail when language-specific components are essential for reasoning tasks, such as when linguistic nuances directly impact the reasoning required. Additionally, the auxiliary classification task used for component identification may not perfectly capture all language-specific information, leading to incomplete ablation.

**3 First Experiments**:
1. Apply the ablation method to a 7B parameter model on MGSM benchmark and measure accuracy improvements across all 11 languages
2. Perform layer-wise ablation analysis to identify which layers provide optimal trade-offs between reasoning gains and linguistic fidelity
3. Compare computational overhead (latency and memory) of the ablation method against baseline inference without ablation

## Open Questions the Paper Calls Out
None

## Limitations
- The approach introduces computational overhead during inference that scales with sequence length and model size
- The effectiveness depends on the quality of language-specific component identification, which relies on auxiliary classification tasks that may not capture all language-specific information
- The analysis focuses primarily on encoder-decoder and decoder-only architectures, leaving uncertainty about applicability to encoder-only models

## Confidence
- **High confidence**: The core finding that language-specific ablation improves multilingual reasoning performance across multiple benchmarks and model sizes is well-supported by experimental results
- **Medium confidence**: The claim about middle layers providing optimal trade-offs between reasoning gains and linguistic fidelity is supported by layer-wise analysis but could benefit from more extensive validation
- **Medium confidence**: The assertion that this approach serves as a lightweight alternative to post-training methods is supported by performance comparisons, though long-term generalization requires further investigation

## Next Checks
1. Measure the actual inference-time latency and memory requirements introduced by the language-specific component identification and ablation process across different sequence lengths and batch sizes
2. Evaluate the impact of the ablation method on non-reasoning language tasks such as translation accuracy, grammaticality judgments, and language modeling perplexity to quantify potential trade-offs
3. Apply the disentanglement approach to encoder-only models (e.g., BERT variants) and assess whether the same layer-wise patterns and performance improvements hold