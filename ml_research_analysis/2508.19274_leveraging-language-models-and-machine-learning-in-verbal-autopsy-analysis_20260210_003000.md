---
ver: rpa2
title: Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis
arxiv_id: '2508.19274'
source_url: https://arxiv.org/abs/2508.19274
tags:
- have
- didn
- page
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis advances the application of domain-adapted pretrained
  language models (PLMs) and multimodal fusion strategies for automated cause of death
  classification using verbal autopsy (VA) data. The key findings include: (1) With
  VA narratives alone, transformer-based PLMs fine-tuned on biomedical/clinical corpora
  outperform leading question-only algorithms at both individual and population levels,
  especially for non-communicable diseases; (2) Multimodal fusion strategies combining
  narratives and structured questions further improve classification accuracy, confirming
  that each modality captures unique information; (3) Classification accuracy is strongly
  associated with information sufficiency as perceived by physicians, with models
  performing better on cases rated as having high sufficiency.'
---

# Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis

## Quick Facts
- **arXiv ID:** 2508.19274
- **Source URL:** https://arxiv.org/abs/2508.19274
- **Authors:** Yue Chu
- **Reference count:** 0
- **Primary result:** Transformer-based PLMs fine-tuned on biomedical/clinical corpora outperform question-only algorithms for VA COD classification, with multimodal fusion providing additional gains.

## Executive Summary
This thesis advances automated cause of death (COD) classification from verbal autopsy (VA) data by leveraging domain-adapted pretrained language models (PLMs) and multimodal fusion strategies. The research demonstrates that transformer-based PLMs fine-tuned on biomedical/clinical corpora significantly outperform traditional question-only algorithms when analyzing VA narratives, particularly for non-communicable diseases. Multimodal approaches combining narratives and structured questions further improve classification accuracy, confirming that each modality captures unique diagnostic information. The findings underscore the value of narratives in enhancing COD classification and highlight the need for more high-quality VA data from diverse settings.

## Method Summary
The study utilized the South African National Cause of Death Validation Project (SANCOD) dataset with 4,999 VAs for adults/adolescents (ages 12+). VA narratives were cleaned using GPT-3.5-turbo and combined with structured binary survey responses. Physician-coded COD (PCVA) labels were mapped to 18 Level 2 categories. The research employed three approaches: narrative-only using PLMs (BioClinicalBERT, BlueBERT, RoBERTa-PM), question-only using ML classifiers (LightGBM, XGBoost), and multimodal fusion strategies (data-level, feature-level, decision-level). Hyperparameter optimization was performed using Optuna with 5-fold cross-validation, maximizing weighted F1 score. Models were evaluated at both individual and population levels using accuracy, weighted F1, and CSMF accuracy metrics.

## Key Results
- Transformer-based PLMs fine-tuned on biomedical/clinical corpora outperform leading question-only algorithms, especially for non-communicable diseases
- Multimodal fusion strategies combining narratives and structured questions further improve classification accuracy
- Classification accuracy is strongly associated with information sufficiency as perceived by physicians

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-adapted PLMs extract diagnostic patterns from VA narratives that structured questions miss, particularly for non-communicable diseases.
- Mechanism: Transformer-based PLMs pre-trained on biomedical/clinical corpora encode contextual relationships between symptoms, timelines, and disease progressions. Fine-tuning transfers this domain knowledge to VA context where narratives describe medical histories in non-standardized language.
- Core assumption: VA narratives share sufficient linguistic and semantic structure with clinical notes for domain transfer to be effective despite differences in authorship.
- Evidence anchors: Abstract shows PLMs outperform question-only algorithms for non-communicable diseases; section 3.1.3 notes similarities between VA narratives and clinical notes.

### Mechanism 2
- Claim: Multimodal fusion improves accuracy because narratives and structured questions capture non-overlapping diagnostic information.
- Mechanism: Feature-level and decision-level fusion combine modality-specific representations where narratives provide temporal sequences and medical histories while structured questions provide systematic symptom coverage.
- Core assumption: Each modality contains unique diagnostic value that the other cannot fully replicate.
- Evidence anchors: Abstract confirms multimodal approaches improve performance; section 4.4.1 shows question-based approaches better for HIV/AIDS while narrative-based approaches better for neoplasms and diabetes.

### Mechanism 3
- Claim: Classification accuracy correlates with physician-perceived information sufficiency because both models and physicians struggle when diagnostic cues are absent or ambiguous.
- Mechanism: Sufficiency reflects presence of signature symptoms and definitive features; when these are missing, neither PLMs nor physicians can confidently assign COD.
- Core assumption: Physician-rated sufficiency is a valid proxy for objective diagnostic information quality.
- Evidence anchors: Abstract states strong association between classification accuracy and information sufficiency; section 5.3.2 shows clear gradients in accuracy across sufficiency levels.

## Foundational Learning

- **Concept:** Pretrain-finetune paradigm with domain-adapted PLMs
  - Why needed here: Understanding how BioClinicalBERT/BlueBERT leverage prior biomedical knowledge before task-specific fine-tuning is essential for selecting appropriate base models.
  - Quick check question: Can you explain why domain-specific pre-training (PubMed, MIMIC) might outperform general BERT for VA narratives despite VA's non-clinical language?

- **Concept:** Multimodal fusion strategies (data-level, feature-level, decision-level)
  - Why needed here: Choosing between fusion approaches requires understanding their trade-offs—data-level is simplest but limited by token constraints.
  - Quick check question: Given a new VA dataset with very long narratives and 200+ structured questions, which fusion strategy would you start with and why?

- **Concept:** Evaluation at individual vs. population levels (accuracy vs. CSMF accuracy)
  - Why needed here: VA applications differ—some need per-death certificates while others need mortality burden estimates.
  - Quick check question: If your deployment prioritizes tracking population-level HIV/AIDS trends, should you optimize hyperparameters for weighted F1 or CSMF accuracy?

## Architecture Onboarding

- **Component map:** Data layer (VA narratives + structured questions) → Unimodal models (PLMs for narratives, ML classifiers for questions) → Fusion layer (data-level, feature-level, decision-level) → Evaluation (individual metrics, population metrics)

- **Critical path:** Clean narratives with GPT-3.5 → Map ICD-10 to COD categories → Stratified train-test split → HPO with Optuna → Train unimodal baselines → Train multimodal variants → Evaluate on hold-out test set

- **Design tradeoffs:** BioClinicalBERT vs. base BERT (domain adaptation helps but may not transfer perfectly); Soft voting vs. stacking (voting is simpler); Training data size (diminishing returns after ~1,500 samples)

- **Failure signatures:** Low recall on minority classes → class imbalance not addressed; High individual accuracy but low CSMF accuracy → probability calibration issues; Similar performance across all PLM variants → fine-tuning may be insufficient

- **First 3 experiments:**
  1. Replicate narrative-only baseline with BioClinicalBERT on a 500-sample subset to validate pipeline before scaling
  2. Compare data-level vs. decision-level fusion on the same split to identify which fusion strategy suits your compute constraints
  3. Analyze per-cause accuracy stratified by sufficiency level to identify which COD categories need improved data collection vs. improved modeling

## Open Questions the Paper Calls Out

- How robust are the proposed multimodal fusion strategies when applied to VA data from diverse geographic settings and pediatric age groups?
- Can text balancing techniques, such as synthetic text generation, effectively mitigate the misclassification of minority causes of death?
- Does explicitly extracting and incorporating temporal features (timing and sequence of events) into PLMs improve differential diagnosis?

## Limitations

- Data access limitations due to reliance on a single proprietary South African dataset that is not publicly available
- Limited baseline comparison against other recent narrative-based VA approaches or contemporary transformer methods
- Multimodal fusion evidence base lacks broader corpus validation for unique modality contributions

## Confidence

- **High Confidence:** Transformer-based PLMs fine-tuned on biomedical corpora outperform question-only algorithms for VA narratives
- **Medium Confidence:** Multimodal fusion improvements and superiority of specific fusion strategies appear context-dependent
- **Medium Confidence:** Sufficiency-accuracy correlation is observed but underlying mechanism assumes physician ratings are valid proxies without external validation

## Next Checks

1. Apply best-performing models to an independent VA dataset from a different country/region to test generalizability of performance gains and sufficiency-accuracy relationships

2. Conduct ablation study comparing BioClinicalBERT against base BERT fine-tuned on the same VA data to quantify contribution of biomedical pre-training versus task-specific adaptation

3. Perform detailed analysis of which specific COD categories show strongest/weakest sufficiency-accuracy correlations to identify whether certain disease groups consistently benefit from improved data collection versus modeling approaches