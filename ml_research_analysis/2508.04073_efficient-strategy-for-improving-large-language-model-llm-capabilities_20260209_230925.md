---
ver: rpa2
title: Efficient Strategy for Improving Large Language Model (LLM) Capabilities
arxiv_id: '2508.04073'
source_url: https://arxiv.org/abs/2508.04073
tags:
- fine-tuning
- base
- performance
- language
- velandia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an efficient strategy for improving large language
  model (LLM) capabilities in resource-constrained environments using a base 1B parameter
  model. The approach combines data selection from academic theses, fine-tuning with
  LoRA, post-training quantization, and Retrieval-Augmented Generation (RAG) to enhance
  quality, response formatting, and efficiency.
---

# Efficient Strategy for Improving Large Language Model (LLM) Capabilities

## Quick Facts
- arXiv ID: 2508.04073
- Source URL: https://arxiv.org/abs/2508.04073
- Authors: Julián Camilo Velandia Gutiérrez
- Reference count: 28
- Primary result: Quantization-then-fine-tuning (Q-FT) outperforms fine-tuning-then-quantization (FT-Q) for LLM deployment in resource-constrained environments

## Executive Summary
This paper proposes an efficient strategy for improving LLM capabilities in resource-constrained environments by combining data selection from academic theses, LoRA fine-tuning, post-training quantization, and Retrieval-Augmented Generation (RAG). The approach uses a base 1B parameter LLaMA 3.1B model and evaluates nine variants using an LLM-as-a-judge methodology. The best-performing model (LLM-q-ft-rag) achieved an average ranking of 2.50 with 26 first-place finishes out of 100 questions. Key findings include that quantization before fine-tuning preserves adapter precision better than the reverse order, and that while RAG provides only marginal improvements, fine-tuning significantly outperforms base models. Quantized models offer faster inference and lower memory usage, making them practical for constrained deployments.

## Method Summary
The approach combines four key techniques: data selection from 1,910 academic documents (theses and dissertations), LoRA fine-tuning with rank=2, alpha=8, dropout=0.2, post-training quantization to 4-bit GGUF format, and RAG using TF-IDF vectorization. A family of nine model variants was created by combining these techniques in different orders. Models were evaluated using GPT-4o as an LLM judge on 100 prompts, measuring average rank and first-place rankings. Training was performed on an NVIDIA T4 GPU with 16GB VRAM, requiring 7 hours for fine-tuning and 1 hour for quantization.

## Key Results
- Quantization-then-fine-tuning (Q-FT) order outperforms fine-tuning-then-quantization (FT-Q), as the latter degrades performance due to precision loss
- RAG provides only marginal quality improvements compared to fine-tuning alone
- Fine-tuning significantly outperforms base models, with the best model achieving an average ranking of 2.50 and 26 first-place finishes
- Quantized models offer faster inference and lower memory usage, making them practical for resource-constrained deployments

## Why This Works (Mechanism)
The strategy works by efficiently combining parameter-efficient fine-tuning (LoRA) with model compression (quantization) and knowledge retrieval (RAG). LoRA enables adapting a large model with minimal trainable parameters, making fine-tuning feasible on resource-constrained hardware. Quantization dramatically reduces model size and speeds up inference while maintaining acceptable quality when applied before fine-tuning, as this preserves the precision of learned adapter weights. RAG augments model responses with relevant context from a domain-specific knowledge base, improving factual accuracy and grounding. The combination allows deployment of capable models in environments where full fine-tuning or large model inference would be impractical.

## Foundational Learning
- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: This is the core fine-tuning technique. It enables adapting a large model with a fraction of the trainable parameters, making the process feasible in resource-constrained environments.
  - Quick check question: Can you explain why LoRA is more parameter-efficient than full fine-tuning?

- **Concept: Post-Training Quantization (PTQ)**
  - Why needed here: This is the primary efficiency technique. Understanding it is critical to grasping the tradeoff between model size/speed and precision.
  - Quick check question: What is reduced when a model is quantized from 8-bit to 4-bit precision, and what is the potential downside?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is a key architectural component for improving quality and grounding responses. Understanding its role as a modular inference-time step is essential.
  - Quick check question: How does RAG differ from fine-tuning in how it introduces new information to a model?

## Architecture Onboarding

- **Component map:** LLaMA 3.1B Base Model -> LoRA Adapter (created during fine-tuning) -> Post-Training Quantization -> RAG Module (optional, using TF-IDF retrieval) -> Vectorized Knowledge Base

- **Critical path:** To reproduce the best-performing model (LLM-q-ft-rag), you must: 1. Quantize the base LLaMA 3.1B model to 4-bits. 2. Fine-tune this quantized model on the target dataset using LoRA. 3. Integrate the resulting model with the RAG system for inference.

- **Design tradeoffs:**
  - **Quality vs. Efficiency:** Fine-tuning and RAG improve quality but add computational cost (training time, longer prompts). Quantization dramatically improves efficiency (size, speed) with minimal quality loss.
  - **Adapter Preservation:** Quantizing after fine-tuning degrades the learned adapter. The "quantize-then-finetune" order is critical for preserving adapter precision.

- **Failure signatures:**
  - **Low-quality outputs from quantized model:** May indicate the base model was not quantized before fine-tuning, leading to adapter degradation.
  - **Incoherent or irrelevant RAG responses:** Indicates the retrieval component is failing to find relevant context or the knowledge base is misconfigured.
  - **Crash during LoRA-to-GGUF conversion:** The paper notes this requires substantial RAM to merge weights; insufficient memory is the likely cause.

- **First 3 experiments:**
  1. Baseline Evaluation: Run the evaluation suite with the base LLaMA 3.1B model and the LLM-as-a-judge prompts to establish a performance baseline.
  2. Ablation on Quantization Order: Train two LoRA adapters: one on the full-precision model (then quantize) and one on the pre-quantized model. Compare their performance on the same evaluation set.
  3. RAG Integration Test: Integrate the RAG module with the best model from experiment #2. Measure the change in average ranking and first-place votes compared to the non-RAG version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would expanding experiments to multiple base models and datasets confirm that quantization-before-fine-tuning consistently outperforms fine-tuning-before-quantization?
- Basis in paper: [explicit] The author explicitly recommends "expand[ing] the experimental scope to include multiple datasets, different base models, and various combinations of techniques to obtain a deeper and generalizable understanding."
- Why unresolved: Only LLaMA 3.2-1B and one academic dataset were tested; generalizability remains unknown.
- What evidence would resolve it: Repeated benchmarking across diverse model families (e.g., Mistral, Gemma) and knowledge domains with identical Q-FT vs. FT-Q protocols.

### Open Question 2
- Question: Does integrating Chain-of-Thought (CoT) reasoning with RAG improve factual accuracy beyond the slight gains observed from RAG alone?
- Basis in paper: [explicit] The author states that "combining information retrieval with self-evaluation strategies could enhance the generation of precise, well-founded responses in future developments."
- Why unresolved: CoT and step-by-step verification were not tested in this work.
- What evidence would resolve it: Controlled experiments comparing RAG-only vs. RAG+CoT configurations on the same prompt set, measuring accuracy and hallucination rates.

### Open Question 3
- Question: Would using dense semantic embeddings instead of TF-IDF in the RAG pipeline yield substantial quality improvements?
- Basis in paper: [inferred] The RAG system uses TF-IDF for vectorization, a relatively basic approach; more sophisticated methods (e.g., sentence transformers) were not explored.
- Why unresolved: No ablation comparing retrieval methods was conducted.
- What evidence would resolve it: Replace TF-IDF with dense embeddings in the same pipeline and measure ranking changes in the LLM-as-a-judge evaluation.

### Open Question 4
- Question: How robust are the conclusions when using multiple LLM judges or human evaluators instead of a single GPT-4o instance?
- Basis in paper: [inferred] Evaluation relied solely on GPT-4o rankings with 100 prompts; potential evaluator bias or sensitivity to prompt formulation was not assessed.
- Why unresolved: Inter-judge agreement and human alignment were not measured.
- What evidence would resolve it: Multi-judge evaluation with agreement metrics (e.g., Kendall's W) and human annotation on a subset.

## Limitations

- **Domain-specificity:** Fine-tuning on academic theses from a single Colombian university may not transfer well to other domains or languages
- **Evaluation methodology bias:** LLM-as-a-judge using GPT-4o introduces potential bias and doesn't capture human evaluation perspectives
- **Missing hyperparameters:** Training hyperparameters beyond LoRA configuration (learning rate, batch size, epochs) are not specified, preventing faithful reproduction

## Confidence

- **Core finding (Q-FT vs FT-Q):** Medium - well-supported but evaluation methodology introduces uncertainty
- **RAG effectiveness:** Medium - the marginal improvement is reasonably supported, but absolute quality assessment is limited
- **Generalizability:** Low - only one base model and one dataset were tested
- **Reproduction feasibility:** Low - missing critical training hyperparameters and exact evaluation prompts

## Next Checks

1. Replicate the quantization order ablation experiment with full training hyperparameters specified to verify that Q-FT consistently outperforms FT-Q across different datasets
2. Conduct human evaluation on a subset of model outputs to validate the LLM-as-a-judge methodology and identify potential systematic biases
3. Test model generalization by fine-tuning on a different domain (e.g., medical or technical documentation) and comparing performance to the academic domain results