---
ver: rpa2
title: Evaluating Compositional Scene Understanding in Multimodal Generative Models
arxiv_id: '2503.23125'
source_url: https://arxiv.org/abs/2503.23125
tags:
- uni00000051
- uni00000003
- uni0000004c
- prompts
- uni0000004a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the compositional visual understanding capabilities
  of state-of-the-art multimodal generative models, comparing text-to-image models
  (DALL-E 3) and vision-language models (GPT-4 variants, Claude 3.5 Sonnet, QWEN2-VL-72B,
  InternVL2.5-38B) against human performance. The evaluation used three experimental
  approaches: generating images from relational prompts (basic, reversed, and compositional),
  and learning relational concepts from real-world (Bongard-HOI) and synthetic (SVRT)
  images.'
---

# Evaluating Compositional Scene Understanding in Multimodal Generative Models

## Quick Facts
- arXiv ID: 2503.23125
- Source URL: https://arxiv.org/abs/2503.23125
- Authors: Shuhao Fu; Andrew Jun Lee; Anna Wang; Ida Momennejad; Trevor Bihl; Hongjing Lu; Taylor W. Webb
- Reference count: 26
- Primary result: Multimodal generative models show improved but still limited compositional understanding compared to humans, with performance degrading significantly for complex multi-object scenes

## Executive Summary
This study evaluated the compositional visual understanding capabilities of state-of-the-art multimodal generative models, comparing text-to-image models (DALL-E 3) and vision-language models (GPT-4 variants, Claude 3.5 Sonnet, QWEN2-VL-72B, InternVL2.5-38B) against human performance. The evaluation used three experimental approaches: generating images from relational prompts (basic, reversed, and compositional), and learning relational concepts from real-world (Bongard-HOI) and synthetic (SVRT) images. Key findings reveal that while models show some compositional understanding, they lack the robustness and generality of human visual processing, particularly for complex multi-object scenes.

## Method Summary
The study evaluated multimodal models on relational image generation and few-shot relational concept learning. DALL-E 3 generated images from 75 basic, 60 reversed, and 60 compositional relational prompts using Microsoft Azure API. Vision-language models performed few-shot classification on Bongard-HOI (167 concepts) and SVRT (23 concepts, 2-6 objects) datasets. Human evaluators assessed image-generation agreement (binary match/no-match) and classification accuracy. Models were tested with 1-9 few-shot examples, and chain-of-thought prompting was evaluated as an intervention. Performance was compared against human baselines across varying object counts.

## Key Results
- DALL-E 3 achieved >0.8 agreement on common basic relational prompts but degraded substantially for reversed prompts and compositional prompts
- Vision-language models outperformed previous models on Bongard-HOI but performed significantly worse than humans (GPT-4o: ~0.65 accuracy vs. human ~0.90)
- On SVRT, model performance degraded with increasing object count, falling to chance levels for concepts with 6+ objects
- Neither chain-of-thought prompting nor interactive learning paradigms improved model performance

## Why This Works (Mechanism)

### Mechanism 1
Performance on relational tasks correlates with prompt likelihood in training data. Models appear to rely on statistical co-occurrence patterns rather than systematic compositional understanding; reversed prompts (e.g., "a rabbit chasing a tiger") yield lower agreement than basic prompts because they represent unlikely scenarios. Log likelihood showed statistically significant (but modest) correlation with agreement (spearman r(598) = 0.090, p = 0.027). If models were genuinely compositional, reversed prompts should not systematically degrade performance.

### Mechanism 2
Multi-object scene parsing is a primary bottleneck for relational reasoning. Models lack robust binding mechanisms to associate features with specific objects; as object count increases, performance degrades toward chance. For 6-object problems, all VLMs showed performance statistically indistinguishable from chance. If object-centric preprocessing were added, relational task performance should improve without architecture changes.

### Mechanism 3
Few-shot in-context examples provide limited benefit for relational concept learning. While more examples improve accuracy modestly, models plateau below human performance; VLMs show sensitivity to example count (β ≈ 0.05-0.10) but humans learn faster (β = 0.17). All VLMs showed improved performance with more examples (p < 0.05), but remained well below human level. If scaling in-context examples beyond 9 produced human-level accuracy, the bottleneck would be context length, not learning mechanism.

## Foundational Learning

- Concept: **The Binding Problem in Vision**
  - Why needed here: The paper explicitly connects model failures to this classic cognitive science problem; without understanding binding, you cannot diagnose why multi-object scenes fail
  - Quick check question: Can you explain why a shared representation space causes ambiguity when encoding "red square above blue circle"?

- Concept: **Compositional Generalization**
  - Why needed here: The core evaluation tests whether models can combine known elements in novel ways (e.g., reversed relations); understanding this distinction separates memorization from true understanding
  - Quick check question: If a model correctly generates "tiger chasing rabbit" but fails on "rabbit chasing tiger," what does this reveal about its representations?

- Concept: **Few-shot Relational Concept Learning**
  - Why needed here: The Bongard-HOI and SVRT tasks test learning abstract patterns from limited examples; this differs from zero-shot generation tasks
  - Quick check question: Why do "hard negatives" (sharing object features but differing in relations) matter for evaluating relational understanding?

## Architecture Onboarding

- Component map:
  Text-to-Image Pipeline: Prompt → DALL-E 3 → Generated image (evaluated by human agreement)
  VLM Relational Learning: Few-shot examples (1-9 images with labels) → Target image → Classification
  Evaluation Datasets: Bongard-HOI (real-world human-object interactions, 167 concepts), SVRT (synthetic, 23 concepts, 2-6 objects)

- Critical path:
  1. Define relational prompt type (basic / reversed / compositional)
  2. Generate images or present few-shot examples
  3. Collect model outputs
  4. Evaluate via human judgment (agreement scores) or classification accuracy
  5. Compare against human baselines and analyze by object count / relation type

- Design tradeoffs:
  - "Natural" vs. "Vivid" style in DALL-E 3: Vivid style improved reversed prompt performance but may introduce stylistic confounds
  - Binary vs. Likert-scale human evaluation: Binary is faster; Likert provides granularity (paper found qualitatively similar results)
  - Context window limits (10 images max for GPT-4V) constrained few-shot experimental design

- Failure signatures:
  - **Relation binding errors**: Model generates correct objects but wrong role assignment (tiger chasing rabbit instead of rabbit chasing tiger)
  - **Chance-level performance**: Classification accuracy ~50% indicates model is not extracting relational patterns
  - **Object count collapse**: Sharp accuracy drop at 5-6 objects signals binding failure

- First 3 experiments:
  1. Replicate basic vs. reversed prompt comparison on DALL-E 3 with a new relation (e.g., "holding") to confirm likelihood-dependence pattern
  2. Test GPT-4o on SVRT with object-centric preprocessing (crop and present objects individually) to isolate binding vs. relational reasoning failures
  3. Evaluate whether providing explicit relational descriptions in chain-of-thought improves VLM performance on 4+ object SVRT problems (testing if bottleneck is perception or reasoning)

## Open Questions the Paper Calls Out

### Open Question 1
Would integrating object-centric representation learning methods (e.g., slot attention mechanisms) with multimodal generative models improve their compositional scene understanding, particularly for multi-object scenes? Current multimodal models lack mechanisms for binding features at the object level (the "binding problem"), but no systematic evaluation of object-centric integration with large-scale multimodal models has been conducted.

### Open Question 2
Is the poor performance on compositional scenes caused primarily by difficulties with multi-object perception/parsing, or by fundamental limitations in relational reasoning? Performance degrades similarly across counting, scene description, and relational tasks with increasing object counts, but disentangling perceptual from reasoning failures requires controlled experiments.

### Open Question 3
To what extent does model performance on compositional tasks depend on exposure to similar configurations during training versus genuine compositional generalization? The correlation between prompt likelihood and image generation agreement (r=0.090) suggests training data frequency influences performance, and reversed prompts showed degraded performance despite containing identical relational structure.

### Open Question 4
Why do multimodal models struggle with visual relational reasoning while language models succeed on text-based analogical reasoning? Both modalities require similar relational reasoning, suggesting the bottleneck may be visual rather than reasoning-related, but the specific mechanisms remain unclear.

## Limitations
- The exact causes of failure in multi-object scenes are not fully understood - whether due to architectural limitations, training data distribution, or evaluation methodology
- The correlation between log-likelihood and agreement scores (r=0.090) is statistically significant but weak, suggesting other factors beyond prompt frequency affect performance
- Human baseline comparisons may not fully account for individual differences in visual reasoning capabilities or cultural interpretations of spatial relations

## Confidence
- **High Confidence**: DALL-E 3's improved basic relational prompt performance over DALL-E 2, and the systematic degradation for reversed and compositional prompts
- **Medium Confidence**: VLM performance comparisons on Bongard-HOI and SVRT, given consistent patterns across multiple models but potential evaluation variability
- **Medium Confidence**: The binding problem explanation for multi-object scene failures, though alternative explanations (attention limitations, scaling issues) cannot be ruled out

## Next Checks
1. **Architecture Ablation Test**: Evaluate whether adding explicit object-centric preprocessing (cropping individual objects before relation inference) improves VLM performance on 4+ object SVRT problems, isolating whether the bottleneck is perception vs. reasoning

2. **Training Data Analysis**: Quantify the frequency of reversed relational prompts (e.g., "rabbit chasing tiger") in training corpora to determine if log-likelihood correlations reflect genuine compositional limitations vs. distributional bias

3. **Human Consistency Check**: Conduct a formal inter-rater reliability analysis on the human evaluation data to quantify measurement error and determine whether observed model-human performance gaps exceed human agreement thresholds