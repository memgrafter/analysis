---
ver: rpa2
title: 'TreeNet: A Light Weight Model for Low Bitrate Image Compression'
arxiv_id: '2512.16743'
source_url: https://arxiv.org/abs/2512.16743
tags:
- image
- treenet
- compression
- ieee
- jpeg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TreeNet introduces a lightweight learned image compression model
  using a binary tree-structured encoder-decoder with attentional feature fusion.
  This design reduces computational complexity while maintaining competitive rate-distortion
  performance.
---

# TreeNet: A Light Weight Model for Low Bitrate Image Compression

## Quick Facts
- arXiv ID: 2512.16743
- Source URL: https://arxiv.org/abs/2512.16743
- Reference count: 39
- Primary result: Achieves 4.83% BD-rate improvement over JPEG AI at low bitrates while reducing model complexity by 87.82%

## Executive Summary
TreeNet introduces a lightweight learned image compression model that employs a binary tree-structured encoder-decoder architecture with attentional feature fusion. The model achieves competitive rate-distortion performance at low bitrates while significantly reducing computational complexity compared to existing learned compression methods. Through systematic ablation studies, the paper demonstrates that the model implicitly learns to decompose features into luma and chroma components, with specific latents handling low/high frequency content and color information respectively.

## Method Summary
TreeNet utilizes a binary tree-structured encoder-decoder architecture where each encoder node downsamples the input and applies convolutional blocks, while each decoder node upsamples the input. The model incorporates residual downsampling blocks in the encoder and residual upsampling blocks in the decoder, with attentional feature fusion applied at each node. Four entropy bottlenecks are employed for latent-specific entropy coding, enabling efficient representation of different feature components. This architecture reduces model complexity while maintaining competitive performance at low bitrates through efficient feature extraction and compression.

## Key Results
- Achieves 4.83% BD-rate improvement over JPEG AI on Kodak, CLIC Professional Valid, and Tecnick datasets at low bitrates
- Reduces model complexity by 87.82% compared to baseline learned compression methods
- Demonstrates effective implicit feature decomposition into luma/chroma components and low/high frequency content through ablation studies

## Why This Works (Mechanism)
The binary tree structure enables hierarchical feature extraction and compression, allowing the model to capture both coarse and fine details efficiently. The attentional feature fusion mechanism ensures effective information flow between encoder and decoder paths, while the residual blocks in both encoder and decoder maintain feature quality during downsampling and upsampling operations. The four entropy bottlenecks provide specialized entropy coding for different latent representations, optimizing the rate-distortion tradeoff.

## Foundational Learning

1. **Rate-distortion theory** - Fundamental tradeoff between compression efficiency and reconstruction quality; needed for understanding compression performance metrics; quick check: verify BD-rate calculations

2. **Entropy coding** - Method for lossless compression of continuous-valued latent representations; needed for efficient bitstream generation; quick check: confirm entropy bottleneck implementation

3. **Attention mechanisms** - Technique for weighted feature aggregation between encoder and decoder; needed for effective feature fusion; quick check: verify attention weight distributions

4. **Residual learning** - Approach using skip connections to preserve information flow; needed for maintaining feature quality through transformations; quick check: validate residual block effectiveness

## Architecture Onboarding

Component map: Input Image -> Binary Tree Encoder -> Entropy Bottlenecks -> Binary Tree Decoder -> Output Image

Critical path: Image flows through binary tree structure with feature fusion at each node, then through entropy coding, and finally through decoding tree for reconstruction.

Design tradeoffs: Binary tree structure reduces parameters and computation but may limit receptive field compared to deeper networks. The four entropy bottlenecks optimize compression but increase model complexity. Attention fusion improves reconstruction but adds computational overhead.

Failure signatures: Poor performance on high-frequency content due to tree depth limitations. Color artifacts may appear if attention fusion fails to properly combine chrominance information. High-resolution images may exceed memory constraints of the tree structure.

First experiments:
1. Test on a single Kodak image to verify basic functionality and output quality
2. Compare BD-rate performance on Kodak dataset against JPEG baseline
3. Run ablation study removing attention mechanism to measure its impact

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three datasets (Kodak, CLIC Professional Valid, and Tecnick) without testing on diverse image content types
- Performance claims relative only to JPEG AI at low bitrates, lacking comparison with other learned compression methods
- Model effectiveness at higher bitrates beyond the tested low bitrate range remains unexplored

## Confidence
- Rate-distortion performance claims: Medium confidence (evaluated on limited datasets, comparisons with JPEG AI only)
- Complexity reduction claims: Medium confidence (reduction magnitude unclear without baseline specification)
- Feature decomposition claims: Low confidence (inferred from ablation studies, not explicitly designed)

## Next Checks
1. Test TreeNet on additional datasets with diverse content (medical imaging, satellite imagery, natural scenes) to validate generalization claims
2. Compare TreeNet performance against other learned compression methods (Balle-style networks, VTM) across a broader bitrate range
3. Conduct ablation studies specifically targeting the binary tree structure's limitations at high resolutions and for specific image categories like text-heavy or highly detailed scenes