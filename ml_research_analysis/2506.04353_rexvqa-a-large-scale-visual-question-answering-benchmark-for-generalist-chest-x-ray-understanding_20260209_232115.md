---
ver: rpa2
title: 'ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest
  X-ray Understanding'
arxiv_id: '2506.04353'
source_url: https://arxiv.org/abs/2506.04353
tags:
- medical
- question
- rexvqa
- assessment
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ReXVQA, a large-scale visual question answering\
  \ (VQA) benchmark for chest X-ray interpretation comprising ~696K questions paired\
  \ with 160K studies. It evaluates five radiological reasoning tasks\u2014presence,\
  \ location, negation, differential diagnosis, and geometric analysis\u2014using\
  \ eight multimodal LLMs."
---

# ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding

## Quick Facts
- arXiv ID: 2506.04353
- Source URL: https://arxiv.org/abs/2506.04353
- Reference count: 16
- AI models (MedGemma 83.24%) surpass expert human readers (77.27%) in chest X-ray interpretation

## Executive Summary
ReXVQA introduces a large-scale visual question answering benchmark for chest X-ray interpretation comprising ~696K questions paired with 160K studies. The benchmark evaluates five radiological reasoning tasks—presence, location, negation, differential diagnosis, and geometric analysis—using eight multimodal LLMs. MedGemma achieves 83.24% overall accuracy, marking the first instance where AI exceeds expert-level human performance in chest X-ray interpretation. The dataset provides fine-grained evaluation splits, structured explanations, and category-level breakdowns for systematic assessment of generalist radiological AI beyond narrow pathology classification.

## Method Summary
The ReXVQA benchmark uses a three-layer pipeline (Generation → Quality Check → Validation) to transform radiology reports into structured MCQs through 12 iterations of radiologist feedback. Structural validators enforce JSON schema compliance while content validators assess domain specificity, cognitive depth, and clinical alignment. The benchmark evaluates eight multimodal LLMs on ~696K questions across five task categories: negation (36.5%), presence (36.1%), differential diagnosis (20.9%), location (6.1%), and geometric (0.4%). Images are PNG at 1/4 original resolution from ReXGradient-160K source.

## Key Results
- MedGemma achieves 83.24% overall accuracy, surpassing expert human readers (77.27%) in a 200-case reader study
- Models demonstrate distinct strengths across task types, with MedGemma leading in four of five major categories
- LLaVA shows significant struggles with only 26.61% accuracy, highlighting challenges in multimodal medical image interpretation
- Performance varies systematically across cognitive task types, suggesting different architectural components engage different reasoning demands

## Why This Works (Mechanism)

### Mechanism 1: Domain-specific multimodal pre-training
- **Claim:** Domain-specific multimodal pre-training on medical images improves radiological reasoning across diverse cognitive tasks.
- **Core assumption:** Medical domain specialization creates visual and linguistic representations that better capture clinically meaningful features than general-purpose training.
- **Evidence anchors:** MedGemma achieves 83.24% overall accuracy, marking first instance of AI exceeding expert performance; MedGemma significantly leads location assessment at 83.47%.

### Mechanism 2: Structured MCQ evaluation with expert-validated prompts
- **Claim:** Structured MCQ evaluation with expert-validated prompts enables systematic assessment of distinct cognitive skills rather than conflating them.
- **Core assumption:** Expert-refined prompts and multi-stage validation produce questions that authentically reflect clinical reasoning rather than template artifacts.
- **Evidence anchors:** Prompt engineering underwent twelve major iterations with systematic radiologist feedback; structured validators enforce schema compliance and clinical alignment.

### Mechanism 3: Task-specific performance variation
- **Claim:** Performance varies systematically across cognitive task types, with models showing task-specific strengths rather than uniform capability.
- **Core assumption:** Task type is a primary driver of model performance, independent of anatomical category.
- **Evidence anchors:** Models demonstrate distinct strengths across radiological reasoning tasks; MedGemma leads in four out of five major categories with varying task distributions.

## Foundational Learning

- **Concept: Multimodal Visual Question Answering (VQA)**
  - **Why needed here:** ReXVQA frames chest X-ray interpretation as selecting among candidate answers given image-question pairs, not binary classification.
  - **Quick check question:** Can you explain how a vision encoder's output embeddings are aligned with an LLM's embedding space in models like LLaVA or Qwen-VL?

- **Concept: Chest X-ray Clinical Workflow**
  - **Why needed here:** The five cognitive tasks mirror radiologist reasoning patterns and understanding clinical workflow is essential for evaluating clinically meaningful representations.
  - **Quick check question:** Why is negation detection ("absence of findings") clinically critical for avoiding false positives in chest X-ray interpretation?

- **Concept: Benchmark Construction and Leakage Prevention**
  - **Why needed here:** Understanding data contamination risks and train/test separation is critical for interpreting reported performance gaps between models.
  - **Quick check question:** Why does using a different LLM for dataset generation than for evaluation reduce but not eliminate data leakage concerns?

## Architecture Onboarding

- **Component map:** DICOM → PNG (1/4 resolution) -> Vision encoder (SigLIP, CLIP ViT-L/14, etc.) -> Projection layer -> LLM backbone (4B-9B) -> Answer selection + explanation
- **Critical path:** Image preprocessing (DICOM → PNG) → Vision encoding (extract spatial features) → Vision-language alignment (project visual features to LLM space) → Multi-modal reasoning (integrate question, options, visual features) → Answer selection (predict option index + generate explanation)
- **Design tradeoffs:** MCQ vs. open-ended (enables objective scoring but may not capture nuanced reasoning); resolution vs. compute (1/4 resolution balances efficiency with detail); domain-specific vs. general-purpose (medical pre-training improves performance but requires specialized data)
- **Failure signatures:** Low inter-rater agreement with humans (moderate kappa 0.3-0.5); task-specific collapse (LLaVA fails across all tasks); hallucinated findings (3.3% identified in expert review)
- **First 3 experiments:**
  1. Baseline reproduction: Evaluate MedGemma and Qwen2.5-VL on public test split (40,826 questions) to reproduce accuracy gaps
  2. Task ablation: Measure per-category performance to identify which cognitive skills drive accuracy differences
  3. Inter-rater analysis: Compute Cohen's Kappa between model predictions and ground truth across anatomical categories

## Open Questions the Paper Calls Out

- **Open Question 1:** Does AI performance persist when evaluated against board-certified attending radiologists rather than residents?
- **Open Question 2:** How does model performance shift when transitioning from multiple-choice selection to open-ended clinical reasoning?
- **Open Question 3:** To what extent do reported capabilities generalize to international patient populations and diverse clinical practices?

## Limitations

- Benchmark relies heavily on GPT-4o-generated questions with limited expert validation at scale, introducing potential systematic biases
- Moderate inter-rater reliability (kappa 0.3-0.5) between AI and human experts makes definitive superiority claims uncertain
- MCQ format with predefined options doesn't capture the open-ended nature of actual clinical decision-making

## Confidence

- **High confidence:** Task distribution statistics (36.5% negation, 36.1% presence, 20.9% differential diagnosis, 6.1% location, 0.4% geometric)
- **Medium confidence:** Domain-specific pre-training benefits for MedGemma (83.24% accuracy)
- **Low confidence:** Claim that AI "exceeds expert human performance" given moderate inter-rater reliability and MCQ constraints

## Next Checks

1. Cross-model consistency test: Evaluate additional general-purpose multimodal models to determine if MedGemma's advantage persists across broader model spectrum
2. Open-ended evaluation pilot: Run small-scale study where models generate free-text responses, then have radiologists score these responses
3. Out-of-distribution generalization: Test best-performing models on chest X-ray datasets from different institutions or time periods to assess domain-specific pre-training advantages under distribution shifts