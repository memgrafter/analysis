---
ver: rpa2
title: 'ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language
  Models'
arxiv_id: '2509.20376'
source_url: https://arxiv.org/abs/2509.20376
tags:
- feature
- features
- activation
- system
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ConceptViz is a visual analytics system that bridges the gap between\
  \ Sparse Autoencoder (SAE) features and human-understandable concepts in large language\
  \ models. The system implements an Identification \u21D2 Interpretation \u21D2 Validation\
  \ pipeline that enables users to query SAEs using concepts of interest, interactively\
  \ explore concept-to-feature alignments, and validate correspondences through model\
  \ behavior verification."
---

# ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models

## Quick Facts
- arXiv ID: 2509.20376
- Source URL: https://arxiv.org/abs/2509.20376
- Reference count: 40
- ConceptViz is a visual analytics system that bridges the gap between Sparse Autoencoder (SAE) features and human-understandable concepts in large language models

## Executive Summary
ConceptViz is a visual analytics system designed to bridge the gap between Sparse Autoencoder (SAE) features and human-understandable concepts in large language models. The system implements an Identification → Interpretation → Validation pipeline that enables users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate correspondences through model behavior verification. Through two usage scenarios and a user study with 12 participants, ConceptViz demonstrates effectiveness in supporting concept exploration and interpretation of SAE features. The system received high ratings (4.42-4.67/5) for helping users understand feature distributions, interpret semantic meanings, and verify concept-feature relationships, with participants expressing willingness to use the system again. ConceptViz advances interpretability research by streamlining discovery and validation of meaningful concept representations in LLMs.

## Method Summary
ConceptViz processes SAE features from Gemma-2-2b through a visual analytics pipeline. The system uses pre-computed feature explanations (GPT-4o-mini) and activation samples from Neuronpedia, combined with user-driven queries. Retrieval uses text embeddings to rank SAE models and features, while feature organization employs UMAP dimension reduction and hierarchical clustering. The system provides real-time activation analysis and output steering for validation, allowing users to explore semantic relationships and verify concept-feature alignments through both automated explanations and causal interventions.

## Key Results
- User study participants rated the system 4.42-4.67/5 for understanding feature distributions, interpreting semantic meanings, and verifying concept-feature relationships
- Participants expressed willingness to use the system again for concept exploration tasks
- Two usage scenarios demonstrated effective concept exploration, from initial query refinement to validation through steering
- The system successfully enabled users to discover meaningful concept representations and validate their semantic interpretations

## Why This Works (Mechanism)

### Mechanism 1
Sparse autoencoders decompose polysemantic neuron activations into sparser, more interpretable features with clearer semantic boundaries. SAEs project dense activations (x) into an overcomplete sparse basis (z) via encoder W_enc, then reconstruct (x̂) via decoder W_dec. Sparsity constraints (L1 or L0) encourage selective activation where individual features respond to more specific concepts. Core assumption: Mono-semantic features exist and can be learned through sparsity-inducing optimization; features correspond to human-interpretible concepts.

### Mechanism 2
Semantic similarity between concept queries and pre-computed feature explanations enables retrieval of relevant SAE models and features. Text embeddings (text-embedding-3-large, 3072-dim) encode both user queries and automated feature explanations. Cosine similarity ranks features; multi-threshold aggregation (K={10,100,1000}) identifies SAE models with consistently relevant features across granularity levels. Core assumption: Automated feature explanations capture feature semantics sufficiently for embedding-based matching; semantic similarity correlates with functional relevance.

### Mechanism 3
Activation steering provides causal evidence that manipulating specific features produces predictable changes in model outputs. Steering vector extracted from SAE decoder (W_dec[feature]) is added to layer activations during inference: activations += steering_strength × steering_vector. Positive strength enhances feature influence; negative suppresses it. Output comparison establishes causal link. Core assumption: SAE decoder directions correspond to causally meaningful directions in activation space; steering effects are specific to the targeted concept.

## Foundational Learning

- **Transformer residual stream architecture**
  - Why needed here: SAEs are trained on residual stream activations; understanding where features originate requires knowing how information flows through attention and MLP layers
  - Quick check question: Can you explain why the residual stream, rather than individual attention heads, is often targeted for SAE analysis?

- **Sparsity vs. reconstruction tradeoff in dictionary learning**
  - Why needed here: SAE quality depends on balancing sparsity (interpretable features) against reconstruction fidelity (faithful representation). The system's reliability hinges on this
  - Quick check question: What happens to feature interpretability if L0 (average active features per token) is set too low or too high?

- **Activation steering/intervention methods**
  - Why needed here: The validation phase uses steering to establish causality. Understanding how additive interventions work is essential for interpreting steering results
  - Quick check question: Why might steering a feature produce effects beyond its hypothesized concept?

## Architecture Onboarding

- Component map: Query → SAE Discovery → Feature Explorer → Feature Details → Input Activation → Output Steering
- Critical path: Query → SAE selection → Feature space navigation → Feature selection → Activation validation → Steering validation. Each stage narrows the hypothesis space.
- Design tradeoffs:
  - Pre-computed vs. real-time: Feature embeddings and clustering are pre-computed for responsiveness; custom activation analysis runs in real-time
  - Automated explanations vs. user verification: System provides GPT-generated explanations but emphasizes validation through activation patterns and steering—does not assume explanations are correct
  - Global vs. local navigation: Hierarchical clustering enables both overview (semantic regions) and drill-down (individual features)
- Failure signatures:
  - Query returns scattered features across many layers without clear winner → concept may be too abstract or ambiguous
  - Activation-similarity matrix shows strong diagonal deviations → automated explanation is incomplete or misleading
  - Steering produces incoherent outputs → feature may be polysemantic or steering strength too aggressive
  - Co-activation retrieval returns features with irrelevant cluster labels → activation correlation without semantic relationship
- First 3 experiments:
  1. Baseline test with known concept: Query "French language" → verify Layer selection aligns with linguistic processing layers → confirm feature explanations mention French/romance languages → validate steering shifts output toward French content
  2. Polysemy stress test: Query "bank" (financial vs. river) → observe whether system disambiguates via query optimization → test if different SAE layers capture different senses → verify steering affects only intended meaning
  3. Explanation-error detection: Select a feature, identify high-activation/low-similarity samples in the matrix → manually inspect what triggers the feature → compare against automated explanation → assess whether matrix correctly flags explanation gaps

## Open Questions the Paper Calls Out

### Open Question 1
How can dimensionality reduction techniques be optimized to scale effectively for SAE feature spaces containing hundreds of thousands or millions of features? The authors state in Section 8.2 that "As feature spaces scale towards hundreds of thousands or more, the primary computational cost lies in UMAP dimension reduction," and suggest future work explore "distributed dimension reduction."

### Open Question 2
Can the ConceptViz pipeline be adapted to support cross-modal feature analysis in multimodal large language models? The authors note in Section 8.2 that "an exciting direction for future research is to adapt our system for cross-modal feature analysis, exploring how concepts are represented across modalities like text and images."

### Open Question 3
How effective is the ConceptViz workflow for novice users lacking prior knowledge of Sparse Autoencoders (SAEs) in ecologically valid research scenarios? Section 8.2 acknowledges the limitation that the "participant pool... consisted solely of users with prior SAE knowledge," meaning findings "may not fully capture the challenges faced by novices."

## Limitations

- System's effectiveness relies heavily on quality of automated feature explanations and pre-computed activations, with no quantification of false positive rates for concept-feature alignment
- User study involved only 12 participants exploring pre-selected concepts rather than discovering novel ones, limiting generalizability to open-ended exploration tasks
- Claims about discovery of novel concepts are not empirically validated; the system appears optimized for known concept exploration rather than true open-ended discovery

## Confidence

- **High Confidence:** The technical architecture for SAE integration, activation steering implementation, and the three-stage pipeline (Identification → Interpretation → Validation) are well-specified and technically sound
- **Medium Confidence:** The user study results show positive usability ratings, but the small sample size and limited exploration scenarios reduce confidence in broad effectiveness claims
- **Low Confidence:** Claims about discovery of novel concepts are not empirically validated; the system appears optimized for known concept exploration rather than true open-ended discovery

## Next Checks

1. **Polysemy robustness test:** Systematically evaluate how the system handles ambiguous queries (e.g., "bank" for financial vs. river contexts) and whether it can disambiguate through query optimization or feature clustering
2. **Explanation error detection benchmark:** Create a controlled dataset where automated explanations are deliberately incorrect for a subset of features, then measure the system's ability to flag these errors through activation-similarity analysis
3. **Steering specificity validation:** Test whether steering individual features produces isolated semantic effects or causes collateral changes in unrelated concepts, establishing bounds on feature mono-semanticity