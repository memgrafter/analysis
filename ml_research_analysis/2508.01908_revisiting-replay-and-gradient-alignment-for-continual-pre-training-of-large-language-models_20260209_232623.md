---
ver: rpa2
title: Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large
  Language Models
arxiv_id: '2508.01908'
source_url: https://arxiv.org/abs/2508.01908
tags:
- replay
- learning
- reptile
- continual
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates continual pre-training of large language
  models using experience replay and gradient alignment to address catastrophic forgetting
  across multiple languages. The authors evaluate Llama-family models (99M to 6B parameters)
  pre-trained sequentially on English, French, German, Arabic, and Japanese (100B
  tokens each) using meta-experience replay, which combines replay with Reptile-based
  meta-updates.
---

# Revisiting Replay and Contrastive Learning for Continual Pre-Training of Large Language Models

## Quick Facts
- arXiv ID: 2508.01908
- Source URL: https://arxiv.org/abs/2508.01908
- Reference count: 40
- Key outcome: Meta-experience replay with Reptile gradient alignment reduces catastrophic forgetting in continual LLM pre-training across multiple languages

## Executive Summary
This paper investigates continual pre-training of large language models using experience replay and gradient alignment to address catastrophic forgetting across multiple languages. The authors evaluate Llama-family models (99M to 6B parameters) pre-trained sequentially on English, French, German, Arabic, and Japanese (100B tokens each) using meta-experience replay, which combines replay with Reptile-based meta-updates. Results show that both replay (especially 50% replay rate) and gradient alignment significantly reduce forgetting while improving downstream task performance compared to sequential training, with the combination yielding the best results. Scaling analysis reveals that replay with gradient alignment provides stable and efficient continual learning across model sizes, with replay being more compute-efficient than increasing model size for smaller models, though larger models eventually benefit more from scale. This work demonstrates the effectiveness of gradient alignment techniques in LLM pre-training and provides an efficient implementation of meta-experience replay with negligible computational overhead.

## Method Summary
The method employs meta-experience replay for continual pre-training of Llama-family models on sequential multilingual corpora. The approach uses a disk-backed replay buffer with reservoir sampling and async prefetching, combining replayed data with new task data at specified rates (α∈{0, 0.25, 0.5}). Reptile meta-updates are applied every 500 batches with interpolation factor ε=0.1 to align gradients across tasks. Models are trained sequentially on 100B tokens per language (English → French → German → Arabic → Japanese) with AdamW optimizer, cosine learning rate schedule, and batch size 4096. The system evaluates forgetting through validation loss increases and downstream benchmarks including HellaSwag, PiQA, and PubMedQA.

## Key Results
- 50% replay rate with Reptile gradient alignment achieves lowest forgetting scores across all model sizes
- Meta-experience replay outperforms both sequential training and standard replay baselines
- For smaller models (99M-3B), replay is more compute-efficient than model scaling; larger models (6B) benefit more from scale
- Downstream task performance improves consistently with combined replay and gradient alignment approach

## Why This Works (Mechanism)
The method addresses catastrophic forgetting by maintaining a buffer of past examples and using meta-learning updates to preserve knowledge from previous tasks. The replay mechanism ensures exposure to past data distributions while Reptile updates align model parameters to be sensitive to all task gradients simultaneously. This combination prevents the model from overwriting task-specific knowledge during sequential training.

## Foundational Learning
- **Catastrophic forgetting**: The phenomenon where neural networks rapidly lose previously learned information when trained on new tasks. Why needed: Central problem being addressed. Quick check: Observe validation loss increase on old tasks during new task training.
- **Experience replay**: Technique of storing and revisiting past training examples during learning. Why needed: Provides direct mechanism to maintain past knowledge. Quick check: Buffer hit rate and sample diversity across tasks.
- **Reptile meta-learning**: Gradient-based meta-learning algorithm that updates parameters toward a common solution across tasks. Why needed: Aligns model gradients to be sensitive to all tasks. Quick check: Parameter interpolation stability and convergence behavior.
- **Reservoir sampling**: Algorithm for maintaining a representative sample from a data stream of unknown size. Why needed: Enables efficient buffer management without prior data distribution knowledge. Quick check: Buffer sample distribution matches original data stream.
- **Disk-backed prefetching**: Technique of asynchronously loading data from disk to avoid I/O bottlenecks. Why needed: Enables large replay buffers without memory constraints. Quick check: Queue occupancy and I/O throughput during training.
- **Cosine learning rate schedule**: LR decay pattern following cosine curve for better convergence. Why needed: Standard practice for LLM training stability. Quick check: LR values match expected cosine schedule at each step.

## Architecture Onboarding
**Component Map**: Data stream → Replay buffer → Batch sampler → Model → Loss → Reptile updater → Parameter storage
**Critical Path**: Training loop alternates between forward/backward passes on mixed batches (replay + new data), followed by Reptile meta-updates every k batches
**Design Tradeoffs**: Disk-backed buffer enables large capacity but introduces I/O latency vs. memory-backed for speed but limited capacity
**Failure Signatures**: High forgetting indicates insufficient replay rate or Reptile instability; slow training suggests I/O bottlenecks or inefficient prefetching
**First Experiments**: 1) Verify buffer sampling correctness across tasks, 2) Test Reptile update stability with different ε values, 3) Measure I/O throughput for disk-backed buffer under load

## Open Questions the Paper Calls Out
- How does Meta-Experience Replay scale to significantly more than five tasks or non-sequential domain revisits?
- To what extent does the method preserve specific factual knowledge versus just general task performance?
- Does the curriculum effect (continual pre-training outperforming joint training) depend on the specific language introduction order?

## Limitations
- Study restricted to fixed sequences of three to five tasks, not validating on longer task sequences
- Evolution of factual knowledge is largely opaque, relying on proxy metrics rather than direct knowledge probes
- Potential negative transfer between languages not explicitly investigated

## Confidence
- High confidence: Replay with gradient alignment reduces catastrophic forgetting compared to sequential training
- Medium confidence: Replay becomes more compute-efficient than model scaling for smaller models (99M-3B)
- Low confidence: Claims about "stable and efficient" continual learning across all model sizes are somewhat overstated

## Next Checks
1. Systematically vary learning rate, replay rates, and Reptile update frequencies to identify robust configurations and understand failure modes
2. Measure performance degradation when fine-tuning on languages trained earlier versus later to quantify potential negative transfer effects
3. Quantify wall-clock time, memory usage, and I/O bandwidth for disk-backed replay buffer versus in-memory alternatives to validate "negligible overhead" claims