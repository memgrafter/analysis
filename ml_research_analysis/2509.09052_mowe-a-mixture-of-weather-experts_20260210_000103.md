---
ver: rpa2
title: 'MoWE : A Mixture of Weather Experts'
arxiv_id: '2509.09052'
source_url: https://arxiv.org/abs/2509.09052
tags:
- weather
- mowe
- forecast
- experts
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoWE, a Mixture of Weather Experts framework
  that dynamically combines forecasts from multiple state-of-the-art AI weather models
  to improve accuracy. The core idea is a lightweight Vision Transformer-based gating
  network that learns to weight each expert's contribution at every grid point and
  forecast lead time, creating a synthesized forecast that outperforms any individual
  model.
---

# MoWE : A Mixture of Weather Experts

## Quick Facts
- arXiv ID: 2509.09052
- Source URL: https://arxiv.org/abs/2509.09052
- Reference count: 34
- Primary result: MoWE framework achieves up to 10% lower RMSE than best individual expert model

## Executive Summary
This paper introduces MoWE (Mixture of Weather Experts), a novel framework that dynamically combines forecasts from multiple state-of-the-art AI weather models to improve overall accuracy. The core innovation is a lightweight Vision Transformer-based gating network that learns to weight each expert's contribution at every grid point and forecast lead time, creating a synthesized forecast that outperforms any individual model. Using Pangu, Aurora, and FCN3 as expert models, MoWE achieves up to 10% lower Root Mean Squared Error than the best individual expert on 2-day forecasts, and significantly outperforms simple averaging approaches. The method demonstrates how to effectively harness existing high-quality forecasts without additional training cost, presenting a scalable strategy to advance data-driven weather prediction by leveraging the complementary strengths of multiple models.

## Method Summary
MoWE employs a Mixture of Experts architecture where a gating network, implemented as a lightweight Vision Transformer, dynamically learns to assign weights to multiple expert weather prediction models at each spatial grid point and forecast lead time. The framework ingests forecasts from multiple state-of-the-art AI weather models (Pangu, Aurora, FCN3) and combines them through learned weighting rather than simple averaging. The gating network is trained to optimize forecast accuracy by learning which expert performs best under different conditions and locations. This approach creates a flexible ensemble that can adapt to varying weather patterns and geographic regions without requiring additional training of the underlying expert models. The method represents a computationally efficient way to leverage multiple existing AI weather prediction systems while avoiding the computational overhead of training a single comprehensive model from scratch.

## Key Results
- MoWE achieves up to 10% lower Root Mean Squared Error than the best individual expert model on 2-day forecasts
- The framework significantly outperforms simple averaging approaches for combining expert predictions
- Performance improvements are demonstrated across both USA and Europe geographic regions

## Why This Works (Mechanism)
MoWE works by leveraging the complementary strengths of multiple AI weather prediction models through a learned weighting mechanism. The Vision Transformer-based gating network can identify which expert model performs best under specific atmospheric conditions, geographic locations, and forecast lead times. By assigning dynamic weights rather than using fixed averaging, the system can capitalize on each model's particular strengths while compensating for individual weaknesses. This adaptive ensembling approach allows the framework to handle the inherent uncertainty and variability in weather prediction more effectively than any single model or simple averaging approach could achieve.

## Foundational Learning

**Weather Forecasting Models**
- Why needed: Understanding the capabilities and limitations of different AI weather prediction approaches
- Quick check: Can identify differences between physics-based and data-driven weather models

**Vision Transformers**
- Why needed: Core technology enabling the gating network to process spatial weather data
- Quick check: Understand how transformers can process grid-structured data for dynamic weighting

**Ensemble Methods**
- Why needed: Provides theoretical foundation for combining multiple model predictions
- Quick check: Can explain differences between static averaging and dynamic weighting approaches

**Mixture of Experts**
- Why needed: Framework architecture that enables selective use of different models
- Quick check: Understand how gating networks learn to route information to appropriate experts

## Architecture Onboarding

Component Map:
Weather data -> Expert Models (Pangu, Aurora, FCN3) -> Gating Network (Vision Transformer) -> Weighted Forecast Synthesis -> Final Prediction

Critical Path:
Expert model forecasts → Vision Transformer gating network → Dynamic weight calculation → Weighted combination → Final forecast output

Design Tradeoffs:
- Chose lightweight Vision Transformer over heavier alternatives to minimize computational overhead
- Dynamic weighting vs. static averaging for better adaptation to varying conditions
- Leveraged existing models rather than training new ones to save computational resources

Failure Signatures:
- Gate instability when expert models have similar performance levels
- Overfitting to specific geographic regions or weather patterns
- Degradation when expert models experience performance drops

First Experiments:
1. Test framework with two expert models before scaling to three
2. Compare dynamic weighting against simple averaging on held-out data
3. Evaluate gate stability over time with synthetic performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific forecast lead times (0-3 days) and geographic regions (USA and Europe)
- Performance gains may vary with different expert model combinations or future model versions
- Computational overhead of gating network not explicitly quantified in terms of inference time or resource requirements

## Confidence

**High**: MoWE framework architecture and implementation details
**High**: Quantitative performance improvements over individual experts and simple averaging
**Medium**: Scalability claims and generalizability beyond tested conditions
**Medium**: Interpretation of gate weight distributions and their physical meaning

## Next Checks

1. Test MoWE performance across longer forecast horizons (7-14 days) to assess temporal scalability and identify potential degradation patterns
2. Evaluate framework robustness when individual expert models experience performance degradation or become temporarily unavailable
3. Quantify computational overhead including inference time, memory requirements, and energy consumption compared to individual expert models