---
ver: rpa2
title: 'Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation'
arxiv_id: '2506.09929'
source_url: https://arxiv.org/abs/2506.09929
tags:
- safety
- case
- evidence
- claim
- assessment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a structured approach for assessing the support
  of claims in automated driving system (ADS) safety cases. The method evaluates both
  procedural support (process specifications) and implementation support (evidence
  of process application) for each claim.
---

# Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation

## Quick Facts
- arXiv ID: 2506.09929
- Source URL: https://arxiv.org/abs/2506.09929
- Reference count: 40
- Primary result: Presents structured approach for bottom-up assessment of ADS safety case claims and evidence support

## Executive Summary
This paper introduces a comprehensive framework for evaluating automated driving system (ADS) safety cases through bottom-up assessment of claims and supporting evidence. The method systematically evaluates both procedural support (process specifications) and implementation support (evidence of process application) for each claim within the safety case. By implementing two distinct assessment layers and using structured scoring approaches, the framework provides practical guidance for assessing the credibility and completeness of safety arguments while acknowledging that top-down sufficiency evaluation remains outside its scope.

## Method Summary
The assessment framework evaluates ADS safety cases through two primary mechanisms: claim support evaluation and evidence status assessment. For each claim, assessors examine procedural support (process specifications and implementation evidence) and implementation support (evidence demonstrating process application). A 0-3 scoring scale is applied to both claim support and evidence status, with criteria defined for each level. The evidence status assessment evaluates recency, ownership, and governance of individual evidence artifacts. The framework mandates independence between safety case creators and assessors, and results are used to inform continual improvement efforts rather than serving as direct approval criteria.

## Key Results
- Two-layer assessment approach: claim support evaluation and evidence status assessment
- 0-3 scoring scales for both claim support and evidence status with defined criteria
- Independence requirement between safety case creators and assessors
- Results serve as internal documentation and pressure-testing tool rather than approval criteria

## Why This Works (Mechanism)
The framework works by providing a systematic, repeatable process for evaluating the quality and completeness of safety case claims. By separating the assessment into procedural and implementation support, it captures both the theoretical framework and practical application of safety processes. The dual-layer approach ensures that not only are claims supported by evidence, but that the evidence itself meets quality standards for recency, ownership, and governance.

## Foundational Learning
1. **Bottom-up vs Top-down assessment**: Bottom-up evaluates individual claim support while top-down assesses overall argument sufficiency - needed to understand framework scope; check by identifying which assessment types are included
2. **Procedural vs Implementation support**: Procedural examines process specifications while implementation examines evidence of application - needed to distinguish evidence types; check by mapping evidence to claim requirements
3. **Independence principle**: Separate teams for safety case creation and assessment ensure unbiased evaluation - needed to maintain assessment credibility; check by verifying assessor independence
4. **0-3 scoring framework**: Provides granular evaluation levels from no support to sufficient support - needed for consistent assessment; check by applying criteria across multiple claims
5. **Evidence quality metrics**: Recency, ownership, and governance assess evidence artifact status - needed to ensure evidence reliability; check by verifying evidence documentation

## Architecture Onboarding
**Component Map**: Safety Case Claims -> Procedural Support (specifications) -> Implementation Support (evidence) -> Claim Support Score -> Evidence Status (recency/ownership/governance) -> Evidence Status Score -> Overall Assessment

**Critical Path**: Claim identification → Procedural support evaluation → Implementation support evaluation → Claim support scoring → Evidence status assessment → Final claim rating

**Design Tradeoffs**: 
- Granular 0-3 scoring provides precision but requires consistent interpretation
- Independence requirement ensures objectivity but may be challenging to implement
- Focus on bottom-up assessment provides practical guidance but leaves sufficiency questions unanswered

**Failure Signatures**: Inconsistent scoring between assessors, missing evidence artifacts, outdated evidence, unclear ownership or governance structures

**First Experiments**:
1. Apply framework to a simple safety case with 3-5 claims to validate scoring consistency
2. Test inter-rater reliability with two independent assessor teams on identical safety cases
3. Evaluate framework scalability by applying to safety cases of increasing complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Excludes top-down sufficiency analysis of the overall safety argument
- Cannot validate whether specified processes themselves are adequate for safety assurance
- Independence requirement may be difficult to implement in smaller organizations

## Confidence
**High confidence**: The procedural support assessment methodology (process specifications and implementation evidence) is well-defined and practical
**Medium confidence**: The scoring framework (0-3 scales) provides clear differentiation, though subjective interpretation may vary between assessors
**Medium confidence**: The separation of evidence status assessment (recency, ownership, governance) adds value but requires consistent application across diverse evidence types

## Next Checks
1. Conduct inter-rater reliability testing with multiple independent assessor teams evaluating the same safety cases to quantify scoring consistency
2. Implement a pilot study comparing bottom-up assessment results against actual safety outcomes in operational ADS deployments
3. Develop a validation protocol for the "sufficient evidence" threshold determination when multiple evidence items support a claim, including sensitivity analysis of how different evidence combinations affect overall claim support ratings