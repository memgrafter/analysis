---
ver: rpa2
title: Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling
  and Speculative Decoding
arxiv_id: '2504.20456'
source_url: https://arxiv.org/abs/2504.20456
tags:
- vegas
- tokens
- arxiv
- decoding
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of parallel sampling from any-subset
  autoregressive models (AS-ARMs) while maintaining fidelity to the learned data distribution.
  The key insight is that AS-ARMs can generate tokens in any order and support parallel
  joint probability density estimation, enabling the Any-Subset Speculative Decoding
  (ASSD) algorithm.
---

# Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding

## Quick Facts
- arXiv ID: 2504.20456
- Source URL: https://arxiv.org/abs/2504.20456
- Authors: Gabe Guo; Stefano Ermon
- Reference count: 40
- Key outcome: Introduces Any-Subset Speculative Decoding (ASSD), a parallel sampling algorithm for Any-Subset Autoregressive Models (AS-ARMs) that provably generates samples from the true joint distribution with at most N-m neural network calls, achieving faster language generation without sacrificing quality.

## Executive Summary
This paper addresses the challenge of parallel sampling from any-subset autoregressive models (AS-ARMs) while maintaining fidelity to the learned data distribution. The key insight is that AS-ARMs can generate tokens in any order and support parallel joint probability density estimation, enabling the Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably generates samples from the true joint distribution with at most N-m neural network calls. Empirically, ASSD speeds up language generation without sacrificing quality. The paper also introduces a mathematically justified training scheme for AS-ARMs and shows that appropriately trained AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling tasks and nearly match much larger models on code generation.

## Method Summary
The paper introduces Any-Subset Speculative Decoding (ASSD), a parallel sampling algorithm for Any-Subset Autoregressive Models (AS-ARMs). ASSD generates a "self-draft" by conditionally independently sampling tokens in parallel, then verifies the draft using density estimation. This approach provably generates samples from the true joint distribution with at most N-m neural network calls, where N is the sequence length and m is the number of tokens in the draft. The authors also introduce a new training scheme for AS-ARMs based on masking distributions, which enables effective parallel density estimation and improves model performance on infilling and code generation tasks.

## Key Results
- ASSD provably generates samples from the true joint distribution with at most N-m neural network calls
- Empirically achieves faster language generation without sacrificing quality
- Appropriately trained AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling tasks
- Nearly match much larger models on code generation

## Why This Works (Mechanism)
AS-ARMs leverage the ability to generate tokens in any order while maintaining the correct joint probability distribution. The ASSD algorithm exploits this by generating a draft in parallel, then verifying it through density estimation. This approach reduces the number of neural network calls required for sampling while preserving the quality of generated text. The key mechanism is the parallel density estimation, which allows for efficient verification of the conditionally independent draft.

## Foundational Learning
1. Any-Subset Autoregressive Models (AS-ARMs): Models that can generate tokens in any order while maintaining the correct joint probability distribution. Why needed: Enables parallel sampling and density estimation. Quick check: Verify that the model can generate sequences in arbitrary order without changing the output distribution.

2. Speculative Decoding: A technique that generates a draft in parallel, then verifies it to reduce the number of neural network calls. Why needed: Improves sampling efficiency while maintaining output quality. Quick check: Compare the acceptance rate and speed of ASSD to standard autoregressive sampling.

3. Masking Distributions: Probability distributions over which tokens to mask during training. Why needed: Enables effective parallel density estimation and improves model performance on infilling tasks. Quick check: Evaluate the impact of different masking distributions on model performance.

## Architecture Onboarding
Component map: AS-ARM (XLNet-based) -> ASSD (parallel sampling) -> Density Estimation (parallel) -> Output
Critical path: Token generation -> Density estimation -> Acceptance/rejection -> Final output
Design tradeoffs: Parallel generation vs. density estimation accuracy; model size vs. sampling speed
Failure signatures: Low acceptance rates in ASSD; poor performance on infilling tasks
First experiments:
1. Verify that AS-ARM can generate sequences in arbitrary order without changing the output distribution
2. Compare the acceptance rate and speed of ASSD to standard autoregressive sampling
3. Evaluate the impact of different masking distributions on model performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do the performance and efficiency of Any-Subset Autoregressive Models (AS-ARMs) scale with model size and compute compared to standard autoregressive and discrete diffusion models?
- Basis in paper: The conclusion explicitly lists "investigating scaling laws" as a key extension, noting that the current empirical verification is limited to sub-200M parameter models.
- Why unresolved: It is unknown if the specific training objectives (masking distributions) and architectural requirements (causal-like attention) of AS-ARMs scale as efficiently as standard Transformers, or if they face bottlenecks at larger parameter counts.
- What evidence would resolve it: Training sweeps of AS-ARMs at various scales (e.g., 1B, 7B parameters) plotted against loss and inference speed compared to baselines like LLaMA or SEDD.

### Open Question 2
- Question: Can an efficient KV-caching mechanism be developed for arbitrary-order generation to reduce the computational overhead of the density estimation phase?
- Basis in paper: The conclusion identifies "arbitrary-order KV caching (which is non-trivial, due to relative positional encodings)" as a necessary extension to further improve speed.
- Why unresolved: The relative positional encodings used in architectures like XLNet (necessary for permutation invariance) prevent the standard, fixed-context KV caching used in left-to-right models, potentially slowing down the oracle verification step in ASSD.
- What evidence would resolve it: The derivation and implementation of a dynamic caching algorithm that handles the changing attention masks of arbitrary ordering without re-computing attention for the full sequence.

### Open Question 3
- Question: Can modern architectural optimizations, such as Flash Attention, be effectively adapted to support the specific causal-like attention masking required for parallel density estimation in AS-ARMs?
- Basis in paper: The conclusion lists "adding Flash Attention" and "designing more optimized architectures" as future work. The paper relies on XLNet (2019) because Section 4.2 notes it is the "only architecture that fulfills this criteria" for single-step density estimation.
- Why unresolved: Flash Attention is typically optimized for fixed causal masks; the permuted and sparse masks required for AS-ARMs might not benefit from current IO-aware acceleration kernels without modification.
- What evidence would resolve it: A modified Flash Attention kernel capable of handling the specific query/key restrictions of the ASSD mask while maintaining memory efficiency.

### Open Question 4
- Question: How does the quality of the "self-draft" (conditionally independent parallel generation) affect the acceptance rate in ASSD for tasks with strict structural dependencies, such as code generation or reasoning?
- Basis in paper: Section 4.3 and Appendix E.1 discuss the "Two-for-One" drafting mechanism. While the paper shows speedups, it notes that off-the-shelf models with low entropy (repetitive outputs) see different speedup characteristics than finetuned models, implying the draft quality is a bottleneck.
- Why unresolved: The theoretical speedup is bounded by the acceptance rate. If the conditionally independent draft violates strict syntactic rules (common in code), the rejection rate might increase, reducing the wall-clock speedup compared to text generation.
- What evidence would resolve it: An ablation study measuring tokens accepted per iteration (acceptance rate) across different domains (natural language vs. code) to quantify the correlation between draft independence and ASSD efficiency.

## Limitations
- Empirical validation of ASSD's theoretical guarantees is limited to specific model sizes and datasets
- Performance gains reported for ASSD may not translate to other domains or model architectures
- Long-term stability and effectiveness of the AS-ARM training scheme across different tasks and model scales remain to be thoroughly tested
- Claims about state-of-the-art performance are based on a limited set of benchmarks

## Confidence
- Theoretical framework of ASSD: Medium confidence
- Empirical results: Medium confidence
- Training scheme for AS-ARMs: High confidence in mathematical justification, Medium confidence in practical applicability

## Next Checks
1. Extend the empirical validation of ASSD to a wider range of model architectures, including larger language models and models trained on different types of data (e.g., multimodal, multilingual).

2. Conduct ablation studies to isolate the impact of the AS-ARM training scheme on model performance across various tasks, particularly focusing on tasks outside of the current scope (e.g., summarization, question answering).

3. Investigate the robustness of ASSD and AS-ARMs to adversarial inputs and out-of-distribution data to assess their reliability in real-world applications.