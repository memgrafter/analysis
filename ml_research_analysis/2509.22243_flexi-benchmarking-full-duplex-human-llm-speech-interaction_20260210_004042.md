---
ver: rpa2
title: 'FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction'
arxiv_id: '2509.22243'
source_url: https://arxiv.org/abs/2509.22243
tags:
- full-duplex
- user
- interaction
- dialogue
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLEXI, the first benchmark for full-duplex
  human-LLM spoken interaction that includes model interruption in emergency scenarios.
  FLEXI evaluates six diverse interaction scenarios including standard turn-taking,
  pause handling, user interrupt, model interrupt, user backchannel, and model backchannel.
---

# FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction

## Quick Facts
- **arXiv ID:** 2509.22243
- **Source URL:** https://arxiv.org/abs/2509.22243
- **Reference count:** 0
- **Primary result:** First benchmark for full-duplex human-LLM spoken interaction that includes model interruption in emergency scenarios, revealing significant performance gaps between open source and commercial models.

## Executive Summary
FLEXI introduces the first comprehensive benchmark for full-duplex human-LLM spoken interaction, systematically evaluating six diverse interaction scenarios including standard turn-taking, pause handling, user interrupt, model interrupt (emergency), user backchannel, and model backchannel. The benchmark reveals critical performance gaps between commercial models like Gemini and open-source alternatives, particularly in emergency awareness and turn termination. The study identifies next token-pair prediction as a promising architectural path toward achieving truly seamless and human-like full-duplex interaction, enabling concurrent listening and speaking while preserving conversational intelligence.

## Method Summary
The benchmark evaluates models through WebSocket-based real-time simulation of full-duplex dialogue using synthetic speech data generated by Qwen-plus (text) and CosyVoice 2 (audio). Six interaction scenarios are tested with specific latency targets (<150ms for Level 1, <400ms for Level 2). Metrics include objective measures (latency, takeover rate, jump-in rate, turn-termination rate) and subjective assessments (topic shift score, emergency detection score, backchannel timing via GPT-4o). The evaluation framework systematically captures the trade-offs between responsiveness and conversational naturalness across different architectural approaches.

## Key Results
- Commercial models like Gemini significantly outperform open-source models in emergency awareness and interruption handling, though all models struggle with backchannel generation timing
- Aggressive turn-taking strategies reduce response latency but catastrophically degrade pause-handling performance, with Moshi achieving 0.696s latency but poor pause handling (TOR=0.550)
- End-to-end architectures show promise for emergency detection and low-latency response, while modular approaches with control modules introduce additional latency and fail to leverage LLM reasoning capabilities
- No tested model successfully achieves Level 1 (<150ms) latency while maintaining high conversational quality across all interaction scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Next token-pair prediction enables concurrent listening and speaking in full-duplex dialogue systems.
- **Mechanism:** Unlike standard autoregressive next-token prediction (which blocks input processing during generation), token-pair prediction maintains dual streams—allowing the model to generate output tokens while simultaneously processing incoming user audio tokens. This native dual-stream architecture preserves conversational intelligence (reasoning over emergency contexts, topic shifts) while meeting sub-400ms latency constraints.
- **Core assumption:** The model architecture can jointly optimize both streams without catastrophic interference; training data contains authentic overlapping speech patterns to learn from.
- **Evidence anchors:**
  - [abstract] "Finally, we suggest that next token-pair prediction offers a promising path toward achieving truly seamless and human-like full-duplex interaction."
  - [section 4] "By employing next token-pair prediction, such a model can achieve concurrent listening and speaking, thereby preserving conversational intelligence while significantly reducing interaction latency."
  - [corpus] NTPP paper [15] explicitly implements token-pair prediction for dual-channel dialogue; related work shows similar dual-stream approaches.
- **Break condition:** If token-pair prediction incurs >2× compute overhead or fails to converge on overlapping speech patterns during training, the latency-intelligence trade-off reverts to modular approaches.

### Mechanism 2
- **Claim:** Aggressive turn-taking strategies reduce response latency but catastrophically degrade pause-handling performance.
- **Mechanism:** Models optimize for fast takeover (low gap latency) by interpreting short silences as turn-completion signals. This reduces perceived latency in standard turn-taking but causes premature interruption during user pauses (thinking time, hesitation). The paper quantifies this: Moshi achieves 0.696s latency but 0.550 TOR in pause handling (bad), while Vita1.5 shows 4.878s latency but 0.916 TOR in pause handling (also bad—different failure mode).
- **Core assumption:** Semantic cues (not just acoustic silence duration) are required to distinguish pauses from turn boundaries; this requires deeper language understanding.
- **Evidence anchors:**
  - [section 3.2] "These findings highlight a trade-off between standard turn-taking and pause handling, determined by whether the architecture adopts aggressive turn-taking strategy."
  - [table 2] Moshi JIR=0.785 (high jump-in rate before user completes) vs. Vita1.5 JIR=0.000.
  - [corpus] Full-Duplex-Bench [18] confirms similar trade-offs across turn-taking metrics.
- **Break condition:** If semantic understanding of pause intent requires models that exceed latency budgets, systems must accept either user frustration (premature interruption) or sluggishness (slow response).

### Mechanism 3
- **Claim:** Emergency detection requires leveraging LLM linguistic priors; external control modules cannot reliably identify time-critical situations.
- **Mechanism:** Model interrupt scenarios (e.g., user says "I'm mixing cleaner and disinfectant") require the model to recognize semantic danger signals and proactively interrupt. External dialogue managers operating on acoustic features or shallow embeddings miss this context. End-to-end models with access to full LLM reasoning can detect emergencies (Gemini EDS=2.19 vs. Moshi EDS=0.81), though current systems still underperform on proactive interruption.
- **Core assumption:** Emergency detection is a language understanding problem, not just acoustic pattern matching; training data contains sufficient emergency examples.
- **Evidence anchors:**
  - [section 4] "Scenarios requiring model interruption demand that the control module identify emergency situations, which necessitates leveraging the linguistic priors and reasoning processes encapsulated within the LLM parameters."
  - [table 3] Gemini TOR=0.000 for model interrupt (doesn't interrupt) but EDS=2.19 (recognizes emergency in response); Moshi TOR=0.390 (interrupts more) but EDS=0.81 (poor emergency recognition).
  - [corpus] LLM-Enhanced Dialogue Management [93705] proposes semantic VAD for turn-taking but doesn't address emergency detection explicitly.
- **Break condition:** If emergency patterns are too rare or diverse to learn from available training data, explicit rule-based safety layers may be required as fallback.

## Foundational Learning

- **Concept: Inter-Pausal Units (IPUs) vs. Turns vs. Gaps**
  - Why needed here: The paper formalizes full-duplex dialogue using IPUs (continuous speech segments), turns (IPUs grouped across pauses), gaps (silence between speakers), and overlaps. Understanding these distinctions is prerequisite to interpreting all six evaluation scenarios.
  - Quick check question: Given a user saying "I'm wondering... [1.5s silence] ...can you help?", is the silence a pause or a gap? What should the model do?

- **Concept: Full-Duplex vs. Half-Duplex Dialogue**
  - Why needed here: The paper distinguishes half-duplex S2S LLMs with control modules from end-to-end full-duplex models. The former decouple language modeling from duplex control (easier training, higher latency); the latter unify both (harder training, lower latency).
  - Quick check question: Why can't a standard autoregressive LLM natively support full-duplex interaction without architectural modification?

- **Concept: Latency Thresholds for Human-like Interaction**
  - Why needed here: The paper defines Level 1 (<150ms, face-to-face quality), Level 2 (<400ms,