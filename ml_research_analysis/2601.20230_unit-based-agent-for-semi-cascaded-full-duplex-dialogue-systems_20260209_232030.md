---
ver: rpa2
title: Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems
arxiv_id: '2601.20230'
source_url: https://arxiv.org/abs/2601.20230
tags:
- dialogue
- full-duplex
- system
- arxiv
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unit-based framework for full-duplex dialogue
  systems that decomposes dialogue into minimal conversational units, with each unit
  containing listen and speak states. The system uses a multimodal large language
  model (MLLM) to predict when to transition between states using continue/switch
  actions, enabling the system to process each unit independently.
---

# Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems

## Quick Facts
- arXiv ID: 2601.20230
- Source URL: https://arxiv.org/abs/2601.20230
- Reference count: 0
- Primary result: Proposes a train-free, unit-based framework achieving 89.7% first response delay and 50.0% interruption total score on HumDial dataset

## Executive Summary
This paper introduces a unit-based framework for full-duplex dialogue systems that decomposes conversations into minimal conversational units, each with listen and speak states. The system uses a multimodal large language model (MLLM) to predict state transitions between units, enabling independent processing with reduced latency. Implemented with open-source components, the framework achieves state-of-the-art semantic and interaction-state inference while maintaining a train-free, plug-and-play design.

## Method Summary
The framework processes dialogue as a sequence of conversational units, each containing listen and speak states. A multimodal large language model (Qwen3-Omni) predicts continue/switch actions to determine when to transition between states, allowing independent unit processing. The semi-cascaded approach uses ASR transcripts only for contextual augmentation rather than full cascade processing. The system integrates open-source components including VAD, ASR, MLLM, and TTS to create a complete full-duplex dialogue system.

## Key Results
- Achieved 89.7% first response delay and 50.0% interruption total score on HumDial dev set
- Improved performance on test set with 57.8% interruption total score and 1.632s total delay
- Ranked second in Full-Duplex Interaction track on HumDial dataset

## Why This Works (Mechanism)
The unit-based decomposition enables independent processing of conversational segments, reducing computational overhead and latency. The MLLM's ability to predict state transitions based on multimodal inputs allows the system to handle both listening and speaking states within each unit. By using ASR transcripts only for contextual augmentation rather than full cascade processing, the system minimizes error propagation and maintains responsiveness.

## Foundational Learning
1. **Full-Duplex Dialogue Systems** - why needed: Enables simultaneous listening and speaking in human-like conversations
   - quick check: System can handle overlapping speech without dropping inputs
2. **Conversational Unit Decomposition** - why needed: Breaks complex dialogue into manageable processing segments
   - quick check: Each unit contains complete listen-speak cycles
3. **Semi-Cascaded Processing** - why needed: Balances context awareness with processing efficiency
   - quick check: ASR transcripts enhance but don't dominate decision-making
4. **Multimodal Large Language Models** - why needed: Integrates audio, text, and timing information for state prediction
   - quick check: MLLM accurately predicts continue/switch actions
5. **State Transition Prediction** - why needed: Enables smooth handoffs between listening and speaking
   - quick check: Minimal latency between user input and system response
6. **Open-Source Component Integration** - why needed: Provides accessible, customizable building blocks
   - quick check: Components work together without custom training

## Architecture Onboarding

**Component Map:**
User Speech -> VAD -> ASR -> MLLM -> TTS -> System Response

**Critical Path:**
Speech detection → ASR transcription → MLLM state prediction → TTS generation → Response delivery

**Design Tradeoffs:**
- Train-free approach vs. potential performance gains from fine-tuning
- Semi-cascaded processing vs. full cascade for context richness
- Open-source components vs. custom-optimized solutions

**Failure Signatures:**
- Missed state transitions leading to awkward pauses
- Incorrect continue/switch predictions causing conversational breakdowns
- ASR errors propagating through the MLLM decision process
- TTS latency exceeding acceptable response time thresholds

**First 3 Experiments:**
1. Measure response latency with varying MLLM inference speeds
2. Test state transition accuracy across different conversation types
3. Evaluate system robustness with noisy audio inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on MLLM capabilities without task-specific fine-tuning
- Performance variance between development and test sets suggests potential overfitting
- Limited evaluation of dialogue quality beyond timing and interruption metrics

## Confidence

**High Confidence:**
- Architectural design of unit decomposition is technically sound
- Integration of open-source components is well-implemented

**Medium Confidence:**
- Latency reduction claims are supported by benchmark results
- State inference improvements are demonstrated but not exhaustively validated

**Low Confidence:**
- Generalizability to diverse real-world scenarios remains unproven
- Long-term robustness with complex, multi-turn dialogues is uncertain

## Next Checks
1. Conduct ablation studies comparing performance with different MLLM models and ASR systems
2. Test the framework in multi-turn, complex dialogue scenarios with varied topics
3. Perform user studies measuring subjective dialogue quality and system usability