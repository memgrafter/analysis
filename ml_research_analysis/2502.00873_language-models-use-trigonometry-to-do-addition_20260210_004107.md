---
ver: rpa2
title: Language Models Use Trigonometry to Do Addition
arxiv_id: '2502.00873'
source_url: https://arxiv.org/abs/2502.00873
tags:
- helix
- addition
- heads
- mlps
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reverse engineers how three mid-sized language models
  (GPT-J, Pythia-6.9B, and Llama3.1-8B) perform addition by analyzing their internal
  representations. The authors discover that numbers are represented as a "generalized
  helix" - a combination of linear and periodic components - rather than as simple
  linear values.
---

# Language Models Use Trigonometry to Do Addition

## Quick Facts
- **arXiv ID:** 2502.00873
- **Source URL:** https://arxiv.org/abs/2502.00873
- **Reference count:** 40
- **Primary result:** Three mid-sized language models perform addition using trigonometric manipulations of "generalized helix" representations rather than simple arithmetic.

## Executive Summary
This paper reverse engineers how three mid-sized language models (GPT-J, Pythia-6.9B, and Llama3.1-8B) perform addition by analyzing their internal representations. The authors discover that numbers are represented as a "generalized helix" - a combination of linear and periodic components - rather than as simple linear values. This helical representation uses Fourier features with periods of 2, 5, 10, and 100. The models compute addition using a "Clock algorithm": they represent the input numbers a and b as helices, manipulate these helices to create the helix for a+b, then read out the final answer from this result.

## Method Summary
The authors analyze residual streams across transformer layers to identify how numbers are represented and manipulated during addition. They fit helix models to the activation patterns using Fourier bases, then use causal activation patching to validate which components are necessary for computation. The analysis distinguishes between direct and indirect effects of model components, identifying specific attention heads and MLPs that implement the addition algorithm. Neuron-level analysis reveals sparse computational units that read and write frequency-specific information.

## Key Results
- Numbers are represented as generalized helices combining linear magnitude with periodic Fourier features (periods 2, 5, 10, 100)
- Addition uses the "Clock algorithm" - manipulating helical representations via trigonometric operations rather than direct arithmetic
- MLPs dominate the computation while attention heads primarily move and transform the helical representations
- Sparse neurons (~1%) read specific helix frequencies and write to logits with matching periodicity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Numbers are represented as generalized helices combining linear magnitude with periodic Fourier features ($T=[2,5,10,100]$) rather than simple linear values.
- **Mechanism:** Each number $a$ maps to: $\text{helix}(a) = C \cdot [a, \cos(2\pi a/T_1), \sin(2\pi a/T_1), \ldots]^\top$. This creates a spiral manifold where position encodes both absolute value and modular properties (e.g., $T=10$ captures units digit).
- **Core assumption:** Fourier periods reflect base-10 structure inherent in training text.
- **Evidence anchors:**
  - [abstract] "numbers are represented in these LLMs as a generalized helix...strongly causally implicated for addition and subtraction"
  - [Section 4.4] Helical fits recover ~80% of activation patching effect, approaching the full layer patch baseline
  - [corpus] Weak or missing—no corpus papers address helical numerical representations.
- **Break condition:** If out-of-distribution numbers project discontinuously onto the helix subspace, the manifold assumption fails.

### Mechanism 2
- **Claim:** Addition uses the "Clock" algorithm—manipulating helical representations via trigonometric operations rather than direct arithmetic.
- **Mechanism:** Four-stage pipeline: (1) Embed $a$, $b$ as helices on their tokens; (2) Attention heads (layers 9-14) copy helices to final token; (3) MLPs 14-18 construct $\text{helix}(a+b)$ from $\text{helix}(a)$ and $\text{helix}(b)$; (4) MLPs 19-27 decode $\text{helix}(a+b)$ to answer logits.
- **Core assumption:** MLPs implement trigonometric identities (e.g., $\cos(a+b) = \cos(a)\cos(b) - \sin(a)\sin(b)$) to combine helices.
- **Evidence anchors:**
  - [abstract] "LLMs compute addition by manipulating this generalized helix using the 'Clock' algorithm"
  - [Section 5.1] "last token hidden states are well modeled by $\text{helix}(a+b)$" with 9-parameter fit outperforming 27-dim PCA
  - [corpus] Weak or missing.
- **Break condition:** If ablating $\text{helix}(a+b)$ components doesn't degrade addition accuracy, the Clock algorithm isn't the primary mechanism.

### Mechanism 3
- **Claim:** Sparse neurons (~1%) read specific helix frequencies and write to logits with matching periodicity, creating a distributed frequency-specific readout.
- **Mechanism:** Neuron preactivations follow $N_n^l(a,b) = c_a \cdot a + c_b \cdot b + \sum_T c_T \cos(2\pi(a+b - d_T)/T)$. Neurons with dominant $T=10$ inputs produce $T=10$ logit oscillations, explaining why common errors are ±10.
- **Core assumption:** The sparsity (1% of neurons carrying 80% of computational load) is structural, not an artifact of the analysis method.
- **Evidence anchors:**
  - [Section 5.4] "using just 1% of the neurons...allows for successful completion of 80% of prompts"
  - [Figure 40] "a neuron with top fitted period of $T_i$ often has a LogitLens with top Fourier period $T_i$"
  - [corpus] Weak or missing.
- **Break condition:** If random neuron subsets perform equivalently, the sparsity assumption fails.

## Foundational Learning

- **Concept:** **Fourier Representations**
  - **Why needed here:** The entire helix mechanism depends on understanding how periodic basis functions encode modular information.
  - **Quick check question:** Why would $T=10$ and $T=100$ Fourier features be particularly useful for base-10 addition?

- **Concept:** **Activation Patching**
  - **Why needed here:** The paper's evidence is causal (interventions), not correlational—understanding this distinction is critical for evaluating the claims.
  - **Quick check question:** What does it mean if patching a helical fit recovers 80% of the effect of patching the full residual stream?

- **Concept:** **Transformer Component Roles**
  - **Why needed here:** Understanding which components (attention vs. MLPs) do what is essential for following the algorithm breakdown.
  - **Quick check question:** In this paper's findings, do attention heads or MLPs play a larger role in computing $a+b$?

## Architecture Onboarding

- **Component map:** Token embeddings → Layer 0 processing → helix(a), helix(b) on their respective tokens → Attention heads (layers 9-14, ~20 key heads) → Move helices to final token → MLPs 14-18 → Construct helix(a+b) from input helices (indirect effect dominant) → MLPs 19-27 → Read from helix(a+b) and write to logits (direct effect dominant) → Individual neurons (~4,500 high-impact, ~700 high-direct-effect) → Frequency-specific helix readers/writers

- **Critical path:** a, b tokens → Layer 0 (helix formation) → Mid-layer attention (helix transport) → Late-middle MLPs (helix combination) → Final MLPs (logit readout) → Answer token

- **Design tradeoffs:**
  - **Model selection:** GPT-J analyzed in depth due to simple MLPs; Llama3.1-8B shows weaker helix(a+b) evidence (possibly due to gated MLPs)
  - **Tokenization:** Single-token regime (0-99) used; models with digit-by-digit tokenization would need additional algorithms
  - **Patching threshold:** 95% used for MLPs, 80% for attention heads—reflects MLP dominance in computation

- **Failure signatures:**
  - **±10 errors** (73.6% of numeric mistakes): Neurons with T=10 periodicity boost both correct answer and ±10 tokens
  - **Bias toward smaller answers**: Negative slope in logit distributions causes overprediction of smaller sums
  - **Three-digit discontinuity**: PC1 shows sharp break at a=100, indicating different representation space

- **First 3 experiments:**
  1. **Replicate helix fitting:** Run Fourier analysis on layer 0 residuals for a∈[0,99], fit helix(a) with T=[2,5,10,100], verify causal relevance via activation patching
  2. **Identify circuit attention heads:** Patch individual heads, rank by total effect, categorize as a/b-heads vs. a+b-heads using the confidence metric in Section 5.2
  3. **Trace MLP stage transition:** Plot helix(a+b)/TE and DE/TE ratios for MLPs 14-27 to verify the layer-19 split between "building" and "reading" phases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the exact low-level mechanism by which model components transform helix(a, b) into helix(a + b)?
- **Basis in paper:** [explicit] Section 5.5 states: "we do not know the exact mechanism they use to do so. We hypothesize that LLMs use trigonometric identities like cos(a + b) = cos(a)cos(b) − sin(a)sin(b) to create helix(a + b). However...we are unable to isolate this computation in the model."
- **Why unresolved:** The large solution space for implementing low-level algorithmic details makes isolation difficult; even one-layer transformers can implement rival algorithms like "Pizza."
- **What evidence would resolve it:** Identifying specific weight matrices or intermediate activations that implement trigonometric addition formulas during the helix transformation.

### Open Question 2
- **Question:** Do LLMs use internal error-correcting codes for addition, analogous to biological grid cells?
- **Basis in paper:** [explicit] Conclusion states: "Future work could analyze if LLMs have internal error-correcting codes for addition like the grid cells presented in Zlokapa et al. (2024)."
- **Why unresolved:** The authors conjecture that helical representations provide robustness, but this has not been systematically tested.
- **What evidence would resolve it:** Ablation studies targeting redundancy in helical dimensions, or analysis of whether periodic features provide error correction benefits over purely linear representations.

### Open Question 3
- **Question:** How does the addition mechanism scale to numbers outside the [0, 99] range or models with different tokenization schemes?
- **Basis in paper:** [explicit] Section 5.5 notes: "other models must necessarily use modified algorithms for addition because of different tokenization schemes. For example, Gemma-2-9B tokenizes each digit of a number separately and must use additional algorithms to collate digit tokens."
- **Why unresolved:** The analysis is restricted to single-token numbers and three specific models; the mechanism may not generalize.
- **What evidence would resolve it:** Replicating the helical analysis on digit-tokenized models and on addition problems with multi-token operands.

## Limitations
- Analysis is constrained to addition of numbers 0-99 and specific model architectures with particular tokenization schemes
- Causal claims rely on specific patching methodology whose sensitivity to hyperparameters is not fully explored
- Fourier basis periods (2, 5, 10, 100) appear well-motivated but their optimality is not established
- Neuron-level sparsity claims depend on specific fitting procedures that may not generalize

## Confidence

**High Confidence:** The existence of periodic structure in number representations (Fourier peaks at T=[2,5,10,100]) is well-established through multiple analytical methods (FFT, helix fitting, activation patching). The causal relevance of these representations for addition is strongly supported by patching experiments showing ~80% effect recovery.

**Medium Confidence:** The Clock algorithm as the primary computational mechanism is reasonably well-supported but not definitively proven. The paper demonstrates that the algorithm can explain the data, but alternative computational strategies cannot be ruled out. The specific role assignments for attention heads and MLPs are based on effect magnitudes that could potentially be influenced by experimental design choices.

**Low Confidence:** The neuron-level sparsity claims and the specific trigonometric identity implementations are the most speculative elements. While the data shows correlations between neuron preactivations and logit frequencies, proving that neurons are actually computing trigonometric identities (rather than learning correlated representations through training) requires additional evidence.

## Next Checks
1. **Out-of-distribution generalization test:** Evaluate the helical representation framework on addition problems outside the trained range (e.g., a, b ∈ [100, 199] or three-digit numbers). If the helix model fails to generalize or shows discontinuous behavior at representation boundaries, this would challenge the universality of the helical manifold assumption.

2. **Alternative computational mechanisms:** Systematically ablate or modify the proposed Clock algorithm components (e.g., specific attention heads, MLP submodules) and test whether alternative addition algorithms emerge. If models can compensate for Clock algorithm disruption through different computational pathways, this would suggest the algorithm is not uniquely necessary.

3. **Cross-model and cross-task validation:** Apply the helix analysis framework to other mathematical operations (subtraction, multiplication, comparison) and different model architectures (including encoder-decoder models and models with different tokenization schemes). If the helical representation and Clock algorithm generalize across these variations, confidence in the framework would increase substantially; if not, this would suggest the mechanism is task-specific or architecture-dependent.