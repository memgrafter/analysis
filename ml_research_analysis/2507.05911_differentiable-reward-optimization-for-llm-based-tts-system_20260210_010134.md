---
ver: rpa2
title: Differentiable Reward Optimization for LLM based TTS system
arxiv_id: '2507.05911'
source_url: https://arxiv.org/abs/2507.05911
tags:
- speech
- audio
- reward
- diffro
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Differentiable Reward Optimization
  (DiffRO) method for enhancing neural codec language model-based text-to-speech (TTS)
  systems. Unlike conventional reinforcement learning approaches that require synthesized
  audio for reward computation, DiffRO directly predicts rewards from neural codec
  tokens, significantly reducing computational overhead.
---

# Differentiable Reward Optimization for LLM based TTS system

## Quick Facts
- **arXiv ID**: 2507.05911
- **Source URL**: https://arxiv.org/abs/2507.05911
- **Reference count**: 0
- **Primary result**: DiffRO achieves state-of-the-art word error rate on seed-tts-eval benchmark by predicting rewards directly from neural codec tokens without synthesized audio

## Executive Summary
This paper introduces Differentiable Reward Optimization (DiffRO), a novel method for enhancing neural codec language model-based text-to-speech systems. DiffRO computes rewards directly from predicted neural codec tokens rather than synthesized audio, dramatically reducing computational overhead. The method employs Gumbel-Softmax to make the reward function differentiable, enabling direct optimization of the language model through backpropagation without traditional reinforcement learning loops. Experiments demonstrate significant improvements in pronunciation accuracy, achieving state-of-the-art WER results, while the multi-task reward model enables zero-shot control over emotional and quality attributes.

## Method Summary
DiffRO replaces reinforcement learning with direct gradient-based optimization by making the reward function differentiable through Gumbel-Softmax sampling. The system takes predicted neural codec tokens and computes rewards using a multi-task reward (MTR) model that provides feedback on multiple dimensions including ASR, speech emotion recognition, quality assessment, and speaker attributes. During training, the LM generates tokens, Gumbel-Softmax produces differentiable samples, the MTR model computes a composite reward, and gradients flow directly back to the LM parameters. The method eliminates the need for synthesized audio during reward computation, significantly reducing training time while maintaining or improving quality metrics.

## Key Results
- Achieves state-of-the-art word error rate on seed-tts-eval benchmark
- Enables zero-shot emotional control with high accuracy (100% HAPPY, 96% SAD in Chinese)
- Provides quality attribute control at the token level, though final audio quality constrained by downstream models
- Reduces computational overhead by eliminating audio synthesis during reward computation

## Why This Works (Mechanism)

### Mechanism 1: Token-Direct Reward Prediction
Computing rewards directly from neural codec tokens preserves sufficient information for quality assessment while eliminating backend synthesis overhead. The reward model takes predicted codec tokens and computes ASR-style probabilities, leveraging the fact that codec tokens encode speech content. This approach is validated by competitive WER scores (7.59% zh) compared to audio-based methods.

### Mechanism 2: Gumbel-Softmax Gradient Bridge
Gumbel-Softmax enables end-to-end backpropagation from reward to LM parameters by replacing discrete argmax sampling with differentiable approximation. This eliminates PPO/DPO policy loops, allowing direct optimization through the LM. The temperature parameter must be carefully tuned to balance gradient variance and token diversity.

### Mechanism 3: Multi-Task Reward Regularization
Jointly optimizing multiple auxiliary tasks in a single reward model provides richer supervision signals and prevents reward hacking on any single metric. The composite reward encourages tokens to satisfy all constraints simultaneously, improving generalization across tasks while enabling zero-shot attribute control.

## Foundational Learning

- **Neural Codec Language Models for TTS**: Essential for understanding how DiffRO operates entirely within codec token space. Quick check: Can you explain the four components of a neural codec TTS pipeline and where DiffRO attaches?

- **Reinforcement Learning from Human Feedback (RLHF)**: DiffRO is framed as an RLHF alternative, so knowing PPO/DPO baselines clarifies what DiffRO simplifies. Quick check: What are the two-phase training steps in standard RLHF, and which phase does DiffRO eliminate?

- **Gumbel-Softmax Trick**: This is the technical enabler for differentiability. Quick check: How does Gumbel-Softmax approximate categorical sampling, and what happens to gradients as temperature → 0?

## Architecture Onboarding

- **Component map**: Input Text → [Codec LM (π_θ)] → Predicted Tokens (Ũ) → [Gumbel-Softmax Sampler] → [MTR Reward Model] ← Task Labels → Composite Reward → Backprop to π_θ

- **Critical path**: MTR model must be pre-trained on multi-task codec data (13,000+ hours with pseudo-labels); Gumbel-Softmax temperature must stabilize before large learning-rate phases; reward scaling across tasks must be balanced.

- **Design tradeoffs**: ASR-only DiffRO achieves best WER but no style control; MTR-based DiffRO enables emotion/quality control but WER slightly worse; direct backprop vs. PPO loop is faster but requires differentiable reward.

- **Failure signatures**: Reward hacking (generating acoustic artifacts that game SER reward), token drift (predicted tokens leave valid codec distribution), task conflict (conflicting gradients from MOS↓ + ASR↑).

- **First 3 experiments**: 1) Train ASR-only DiffRO on single-speaker data; verify WER improves over SFT baseline. 2) Enable SER reward only; measure emotion classification accuracy. 3) Train with Gumbel-Softmax τ ∈ {0.1, 0.5, 1.0}; plot gradient norm variance vs. token diversity.

## Open Questions the Paper Calls Out

1. Can DiffRO methodology be effectively applied to the Flow Matching module to improve control over speaker-related attributes?

2. How can the discrepancy between token-level reward optimization and vocoder's reconstruction behavior be mitigated to allow for decisive control of acoustic attributes like quality (MOS)?

3. Does scaling the MTR model to incorporate additional downstream tasks cause interference or degradation in instruction-following capability?

4. How does Gumbel-Softmax approximation impact stability and convergence speed compared to standard RLHF baselines?

## Limitations

- Limited evaluation scope focused on single benchmark with restricted language coverage
- Temperature sensitivity unknown with no specified annealing strategy
- Task weighting ambiguity with no information about normalization coefficients
- Reward hacking vulnerability not thoroughly analyzed despite follow-up work identifying this concern

## Confidence

**High Confidence**: Token-based reward prediction improves WER compared to baselines; Gumbel-Softmax enables differentiable training without PPO loops; Multi-task rewards enable zero-shot emotional control with measurable accuracy gains.

**Medium Confidence**: MOS control effectiveness (audio quality constrained by downstream FM/vocoder); Generalization across languages (limited test set size); Training stability (no gradient norm analysis).

**Low Confidence**: Robustness to diverse content domains beyond seed-tts-eval; Prevention of reward hacking in production scenarios; Optimal hyperparameter configurations.

## Next Checks

1. **Temperature sensitivity analysis**: Run DiffRO training with Gumbel-Softmax temperatures τ ∈ {0.1, 0.5, 1.0} and measure gradient variance, token diversity, and final WER/MOS outcomes to identify optimal temperature regime.

2. **Reward hacking stress test**: Generate adversarial prompts designed to maximize SER reward while minimizing audio quality. Compare codec-level SER accuracy against audio-level emotion recognition and MOS to quantify reward hacking severity.

3. **Cross-dataset generalization**: Evaluate DiffRO models on external TTS benchmarks (LJSpeech, Common Voice) and real-world customer service dialog data to assess whether WER improvements generalize beyond seed-tts-eval test set.