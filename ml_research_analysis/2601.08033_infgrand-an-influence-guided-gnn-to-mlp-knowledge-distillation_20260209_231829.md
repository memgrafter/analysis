---
ver: rpa2
title: 'InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation'
arxiv_id: '2601.08033'
source_url: https://arxiv.org/abs/2601.08033
tags:
- distillation
- graph
- influence
- infgrand
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfGraND, an influence-guided knowledge distillation
  framework that transfers structural knowledge from GNNs to MLPs. Unlike prior methods
  that treat all nodes uniformly or rely on prediction uncertainty, InfGraND uses
  a graph-aware influence metric to prioritize structurally important nodes during
  distillation.
---

# InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2601.08033
- **Source URL**: https://arxiv.org/abs/2601.08033
- **Reference count**: 40
- **Primary result**: Influence-guided knowledge distillation framework that transfers structural knowledge from GNNs to MLPs, achieving average improvements of 12.6% over vanilla MLPs in transductive settings and 9.3% in inductive settings

## Executive Summary
This paper introduces InfGraND, a knowledge distillation framework that transfers structural knowledge from graph neural networks to multilayer perceptrons. Unlike prior methods that treat all nodes uniformly or rely on prediction uncertainty, InfGraND uses a graph-aware influence metric to prioritize structurally important nodes during distillation. The method also incorporates one-time multi-hop neighborhood feature pre-computation to provide structural awareness without inference overhead. Extensive experiments across seven datasets in both transductive and inductive settings show that InfGraND consistently outperforms existing GNN-to-MLP distillation methods while maintaining MLP inference efficiency.

## Method Summary
InfGraND operates through three key mechanisms: (1) computing a Global Influence Score (GIS) for each node using 2-hop feature propagation and cosine similarity to measure structural importance, (2) pre-computing multi-hop neighborhood features via average pooling across P hops (P=2 optimal) to embed structural information into input features, and (3) training a student MLP with dual influence-weighted losses that combine supervised learning and distillation from a frozen GNN teacher. The framework balances the losses with λ=0.1, finding that distillation should dominate supervision. This approach enables MLPs to capture graph structure while maintaining inference speed advantages over GNNs.

## Key Results
- Outperforms existing GNN-to-MLP distillation methods across seven datasets with average 12.6% improvement over vanilla MLPs in transductive settings
- Surpasses its own GNN teachers in many cases while maintaining MLP inference efficiency
- Validates influence-guided node prioritization through Q1 experiments showing high-influence node subsets consistently outperform low-influence subsets
- Demonstrates effectiveness in both transductive and inductive settings across homophilic graphs

## Why This Works (Mechanism)

### Mechanism 1: Influence-Guided Node Prioritization
- **Claim**: Prioritizing nodes with higher structural influence during distillation may improve student MLP generalization.
- **Mechanism**: The method computes a Global Influence Score (GIS) for each node by measuring how feature perturbations propagate through k-hop neighborhoods via L1-norm of the expected Jacobian. These scores weight both supervised loss and distillation gradients, amplifying learning signals from structurally important nodes.
- **Core assumption**: Nodes that have greater impact on graph-wide representations after message passing contain knowledge most critical for student models to acquire.
- **Evidence anchors**: Figure 2 shows GNNs trained on high-influence nodes consistently outperform those trained on low-influence nodes across Cora, Citeseer, and Pubmed.
- **Break condition**: If influence scores correlate poorly with downstream task performance, or if high-influence nodes are predominantly outliers/noise rather than informative structural anchors.

### Mechanism 2: One-Time Multi-Hop Feature Pre-Computation
- **Claim**: Pre-computing propagated features via average pooling across P hops provides structural awareness without inference overhead.
- **Mechanism**: Before training, features are propagated using linear message passing (Ã^k · X) for k∈[0,P], then pooled (typically averaged) to create enriched input features. This embeds neighborhood information directly into input, eliminating graph access at inference.
- **Core assumption**: Linear propagation without learnable weights captures sufficient structural signal; average pooling preserves multi-hop information without introducing parameters.
- **Evidence anchors**: 2-hop average pooling yields best results (Cora: 84.50%, Citeseer: 74.02%, Pubmed: 81.16%).
- **Break condition**: If graphs are highly dynamic with frequent edge additions/deletions requiring re-computation, or if 2-hop neighborhoods contain predominantly noisy/dissimilar features.

### Mechanism 3: Dual Loss Weighting with Influence Scores
- **Claim**: Combining influence-weighted supervised loss with influence-weighted distillation loss provides complementary learning signals.
- **Mechanism**: The total loss L_t = λL_s + (1-λ)L_d where L_s weights labeled examples by influence (δ_2 term) and L_d weights neighbor teacher predictions by influence (γ_2 term). High-influence neighbors amplify gradient signals during backpropagation.
- **Core assumption**: Both ground-truth labels and teacher soft labels benefit from influence-based reweighting; influence captures task-relevant structural importance beyond prediction uncertainty.
- **Evidence anchors**: Visual demonstration of gradient magnitude scaling with influence scores shows how high-influence nodes receive stronger learning signals.
- **Break condition**: If γ_1 baseline term is removed, low-influence nodes provide zero gradient; if influence scores are uniformly distributed, weighting provides no discrimination.

## Foundational Learning

- **Concept: Message Passing in GNNs**
  - Why needed here: InfGraND replaces recursive message passing with one-time pre-computation; understanding what's being approximated is essential.
  - Quick check question: Can you explain why GNN inference requires accessing neighbor features at each layer, and how pre-computation eliminates this?

- **Concept: Knowledge Distillation Fundamentals**
  - Why needed here: The method transfers "dark knowledge" from GNN teacher logits to MLP student; understanding soft labels vs. hard labels is critical.
  - Quick check question: Why might soft teacher predictions contain more information than one-hot ground truth labels?

- **Concept: Graph Influence/Centrality Metrics**
  - Why needed here: The influence score determines which nodes receive emphasis during distillation; distinguishing this from degree/PageRank is key.
  - Quick check question: How does the proposed influence metric differ from PageRank in what it measures?

## Architecture Onboarding

- **Component map**: Teacher GNN -> Influence Computer -> Feature Propagator -> Student MLP -> Loss Aggregator
- **Critical path**: Train/freeze teacher GNN -> Compute influence scores (offline) -> Pre-compute propagated features (offline) -> Train student MLP with dual losses -> Deploy MLP alone
- **Design tradeoffs**:
  - P (propagation hops): More hops increase coverage but may introduce noise; paper finds P=2 optimal
  - λ (loss balance): Paper finds λ=0.1 optimal (strong distillation, weak supervision); task-dependent
  - Pooling method: Average outperforms max/min; mean preserves feature magnitudes
  - Influence computation: Naive O(N²) vs. k-hop approximation; use k-hop for large graphs
- **Failure signatures**:
  - Student underperforms vanilla MLP -> Check if influence scores are meaningful (not uniform); verify feature propagation isn't adding noise
  - Inductive performance drops significantly -> Propagation features may not generalize; check observed/unobserved node ratio
  - Training instability -> γ_1 may be too low, suppressing gradients from low-influence nodes entirely
- **First 3 experiments**:
  1. **Reproduce Q1 validation**: Train GCN on top-25% vs. bottom-25% influence nodes on Cora; confirm high-influence subset outperforms
  2. **Ablate components**: Compare w/Influence-only vs. w/Propagation-only vs. full model to isolate contribution of each mechanism
  3. **Hyperparameter sweep on λ**: Test λ∈{0.0, 0.1, 0.5, 1.0} to verify paper's claim that distillation-dominant weighting (λ≈0.1) outperforms supervision-only (λ=1.0)

## Open Questions the Paper Calls Out

- **Question**: How does InfGraND perform on heterophilous graphs where the assumption that high-influence neighbors share the same label as the target node is likely violated?
  - **Basis**: The Conclusion states the intent to extend the method to "heterophilous" graphs. Section 5.1 notes the evaluation is restricted to "seven homophilic graph benchmark datasets."
  - **Why unresolved**: The influence metric and distillation loss rely on the homophily principle, implying that influential neighbors provide useful class information. In heterophilic settings, prioritizing these neighbors might amplify noise rather than signal.
  - **What evidence would resolve it**: Comparative results on established heterophilic benchmarks (e.g., Actor, Chameleon) showing whether the influence-guided distillation degrades or adapts compared to uncertainty-based baselines.

- **Question**: Can combining the proposed structural influence metric with entropy-based discrimination yield better distillation results than either method individually?
  - **Basis**: The Conclusion states a plan to "investigate hybrid methods that combine entropy-based discrimination with structural-aware approaches."
  - **Why unresolved**: It is unclear if "knowledge reliability" (entropy) and "structural influence" (topology) are complementary signals or if they redundantly prioritize the same nodes.
  - **What evidence would resolve it**: Ablation studies on a hybrid loss function that weights both influence scores and prediction uncertainty, analyzing the overlap between high-influence and high-uncertainty nodes.

- **Question**: How can the framework maintain its low-latency advantage in dynamic graph scenarios where the pre-computed neighborhood features and influence scores become stale?
  - **Basis**: The Conclusion lists "dynamic graphs" as a future application. Section 4.2 relies on "one-time, offline pre-computation" of features, which assumes a static graph structure during inference.
  - **Why unresolved**: The core efficiency gain comes from pre-computation; dynamically updating influence scores and multi-hop features at inference time would introduce significant overhead, negating the MLP's speed advantage.
  - **What evidence would resolve it**: A proposed mechanism for efficient incremental updates to influence scores and a latency analysis on a graph stream with edge/node insertions.

## Limitations
- The influence metric's theoretical grounding remains unclear and isn't explicitly validated against task-relevant information
- One-time pre-computation assumption breaks down for dynamic graphs with frequent edge updates
- While ablation studies show influence-guided distillation contributes 2-3% accuracy improvements, the paper doesn't clearly isolate whether this comes from influence-weighted supervision versus influence-weighted distillation

## Confidence
- **High Confidence**: The empirical results showing consistent improvements over baselines across seven datasets and two experimental settings (transductive/inductive)
- **Medium Confidence**: The claimed superiority of influence-guided prioritization over prediction-uncertainty methods (KRD, HGMD)
- **Low Confidence**: The assertion that γ_1=0.01 is sufficient to prevent gradient starvation for low-influence nodes

## Next Checks
1. **Ablation of Influence vs. Uncertainty**: Reproduce the main experiments but replace influence-based weighting with entropy-based uncertainty weighting (KRD/HGMD approach) for both supervised and distillation losses. This would directly validate whether influence provides benefits beyond prediction uncertainty.

2. **Influence Score Validation**: On Cora dataset, compute Pearson correlation between influence scores and actual node classification accuracy when trained in isolation. This would validate whether high-influence nodes genuinely correspond to more informative nodes for the task.

3. **Dynamic Graph Stress Test**: Implement a streaming version where edges are added/removed between training epochs. Measure how often pre-computed influence scores and propagated features require recomputation, and quantify the computational overhead compared to on-the-fly message passing.