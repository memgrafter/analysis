---
ver: rpa2
title: Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical
  Reasoning over EHRs
arxiv_id: '2508.14817'
source_url: https://arxiv.org/abs/2508.14817
tags:
- notes
- clinical
- imaging
- task
- chunks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates retrieval-augmented generation (RAG) against\
  \ using full clinical notes for three clinical reasoning tasks over electronic health\
  \ records: extracting imaging procedures, generating antibiotic timelines, and identifying\
  \ key diagnoses. Using three state-of-the-art large language models and varying\
  \ amounts of context (up to 128K tokens), RAG consistently achieved near-parity\
  \ with full-context inputs while requiring far fewer tokens\u2014up to 400% more\
  \ efficient in some tasks."
---

# Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs

## Quick Facts
- **arXiv ID**: 2508.14817
- **Source URL**: https://arxiv.org/abs/2508.14817
- **Reference count**: 40
- **Primary result**: RAG achieves near-parity with full-context inputs while requiring far fewer tokens—up to 400% more efficient in some clinical reasoning tasks over EHRs

## Executive Summary
This study evaluates retrieval-augmented generation (RAG) against using full clinical notes for three clinical reasoning tasks over electronic health records: extracting imaging procedures, generating antibiotic timelines, and identifying key diagnoses. Using three state-of-the-art large language models and varying amounts of context (up to 128K tokens), RAG consistently achieved near-parity with full-context inputs while requiring far fewer tokens—up to 400% more efficient in some tasks. For imaging procedures, RAG matched or exceeded full-context performance with only 60 retrieved chunks. For antibiotic timelines, RAG approached full-context accuracy using minimal text. Diagnosis generation showed smaller gains, with RAG slightly underperforming compared to full notes but still remaining competitive. Overall, RAG offers substantial efficiency gains and remains effective even as models handle longer contexts, suggesting its continued relevance for clinical document processing.

## Method Summary
The study compared RAG and full-context approaches across three clinical reasoning tasks using three state-of-the-art LLMs with varying context windows (up to 128K tokens). Researchers evaluated RAG using different numbers of retrieved chunks (10, 30, 60) against full clinical notes, measuring performance across imaging procedure extraction, antibiotic timeline generation, and diagnosis identification. The evaluation was conducted on a dataset of 108 clinical notes, with performance metrics capturing both accuracy and token efficiency. The comparison aimed to determine whether RAG could match full-context performance while using significantly fewer tokens.

## Key Results
- RAG matched or exceeded full-context performance for imaging procedure extraction using only 60 retrieved chunks
- For antibiotic timeline generation, RAG approached full-context accuracy while using substantially fewer tokens
- Diagnosis generation showed smaller performance gains, with RAG slightly underperforming compared to full notes but remaining competitive

## Why This Works (Mechanism)
The effectiveness of RAG in clinical reasoning tasks stems from its ability to selectively retrieve and present only the most relevant clinical information, reducing noise and cognitive load for the LLM while maintaining task accuracy. By filtering out irrelevant context from lengthy EHRs, RAG allows models to focus on pertinent clinical data, achieving comparable performance with fewer tokens. This selective retrieval mechanism proves particularly valuable for tasks like imaging procedure extraction where specific procedural details are embedded within large clinical documents.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation)**: Combines information retrieval with text generation to provide context-specific content
  - *Why needed*: Addresses the challenge of processing lengthy clinical documents by retrieving only relevant information
  - *Quick check*: Verify that retrieved chunks contain task-relevant clinical information before passing to LLM

- **Context window management**: The maximum number of tokens an LLM can process in a single input
  - *Why needed*: Critical for handling long clinical documents without truncation or loss of information
  - *Quick check*: Ensure retrieved context plus prompt fits within the model's token limit

- **Chunking strategies**: Methods for dividing large documents into smaller, manageable segments for retrieval
  - *Why needed*: Enables selective retrieval of relevant document sections while maintaining semantic coherence
  - *Quick check*: Validate that chunks preserve clinical context and don't split related information across boundaries

## Architecture Onboarding

**Component Map**: Clinical Note -> Text Chunking -> Retrieval System -> RAG Pipeline -> LLM -> Clinical Reasoning Output

**Critical Path**: Text Chunking → Retrieval System → RAG Pipeline → LLM → Clinical Reasoning Output

**Design Tradeoffs**:
- RAG vs Full-context: Token efficiency vs potential loss of peripheral context
- Chunk size selection: Balancing semantic coherence with retrieval precision
- Retrieval threshold tuning: Optimizing relevance vs completeness of retrieved information

**Failure Signatures**:
- Retrieval misses: Critical clinical information not included in retrieved chunks
- Context dilution: Too many irrelevant chunks retrieved, overwhelming the model
- Chunk boundary issues: Related clinical information split across multiple chunks

**First 3 Experiments**:
1. Compare RAG performance using different chunk sizes (50, 100, 200 tokens) for imaging procedure extraction
2. Test retrieval threshold variations (top-k=10, 30, 60) on antibiotic timeline accuracy
3. Evaluate the impact of different retrieval algorithms (BM25 vs semantic similarity) on diagnosis generation performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation conducted on a relatively small set of 108 clinical notes, limiting generalizability
- Absolute accuracy for some tasks remains modest, particularly for diagnosis generation
- Study did not account for variations in EHR formatting, documentation practices, or clinical specialty

## Confidence

**High confidence**:
- RAG's efficiency advantages (token reduction of 200-400%) across all tasks

**Medium confidence**:
- Task-specific performance claims, particularly for imaging procedures and antibiotic timelines
- Diagnosis generation results due to smaller performance gaps

## Next Checks

1. Replicate findings on a larger, multi-site EHR corpus with diverse patient populations and documentation styles
2. Test RAG performance with newer LLM architectures (beyond GPT-3.5/4) that have enhanced context windows and retrieval capabilities
3. Conduct ablation studies to isolate the impact of different chunking strategies and retrieval thresholds on downstream clinical reasoning accuracy