---
ver: rpa2
title: Bandits roaming Hilbert space
arxiv_id: '2509.24569'
source_url: https://arxiv.org/abs/2509.24569
tags:
- quantum
- regret
- state
- where
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The thesis studies exploration-exploitation tradeoffs in online
  learning of quantum states using multi-armed bandits. It establishes information-theoretic
  lower bounds and optimal strategies with matching upper bounds, showing regret typically
  scales as the square root of rounds.
---

# Bandits roaming Hilbert space

## Quick Facts
- **arXiv ID:** 2509.24569
- **Source URL:** https://arxiv.org/abs/2509.24569
- **Reference count:** 0
- **Primary result:** Achieves polylogarithmic regret for quantum state learning via multi-armed bandits with continuous actions

## Executive Summary
This thesis establishes optimal strategies for online quantum state learning by framing it as a multi-armed bandit problem. The work bridges information theory and quantum mechanics, showing that regret typically scales as the square root of rounds, but achieves polylogarithmic scaling for pure states with continuous action spaces. A key innovation reframes quantum state tomography to minimize measurement disturbance while efficiently learning states.

## Method Summary
The thesis maps quantum state learning to a bandit problem where learners select observables to measure unknown quantum states, receiving Bernoulli rewards via Born's rule. It develops a weighted online least squares estimator with median-of-means aggregation that exploits vanishing reward variance near optimal actions. The algorithm controls design matrix eigenvalues through geometric action selection, achieving improved regret bounds. For pure states and continuous actions, it implements a sample-optimal algorithm based on the optimistic principle.

## Key Results
- Establishes information-theoretic lower bounds for quantum bandit problems
- Achieves matching upper bounds with regret typically scaling as √T
- For pure states with continuous actions, achieves polylogarithmic regret breaking the √T barrier
- Demonstrates exponential advantages in quantum recommender systems and thermodynamic work extraction
- Shows measurement disturbance equals cumulative regret for pure states

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping quantum state learning to a bandit problem enables online learning with minimal disturbance.
- **Mechanism:** The learner sequentially selects observables (actions) to measure an unknown quantum state, receiving Bernoulli rewards via Born's rule. Regret (Eq. 1.2) equals cumulative infidelity between post-measurement states and the unknown state, providing a natural disturbance metric.
- **Core assumption:** Single-copy measurements via rank-1 projectors; access to fresh copies of the unknown state each round.
- **Evidence anchors:**
  - [abstract] "Given streaming access to an unknown quantum state, in each round we select an observable from a set of actions to maximize its expectation value...reframe quantum state tomography to both learn the state efficiently and minimize measurement disturbance."
  - [section] Chapter 2.2.1 defines the multi-armed quantum bandit; Eq. 1.3 shows Disturbance(T) ≤ 2R_T for pure states.
  - [corpus] Corpus lacks direct quantum-bandit papers; related works focus on classical linear/GP bandits with similar √T regret bounds.
- **Break condition:** If measurements cannot be restricted to rank-1 projectors, or if copies are not reusable per round, the regret-disturbance equivalence fails.

### Mechanism 2
- **Claim:** Vanishing reward variance as actions approach the optimal enables polylogarithmic regret, breaking the √T barrier.
- **Mechanism:** The reward variance Var[ϵt] = 1 - ⟨θ, A_t⟩² vanishes linearly when A_t aligns with θ. This heteroscedastic noise structure allows weighted least-squares estimators to "boost" confidence along directions close to θ, achieving tighter concentration than constant-noise bandits.
- **Core assumption:** Pure state environments (rank-1 projectors); continuous action set (all rank-1 projectors on the Bloch sphere).
- **Evidence anchors:**
  - [abstract] "For pure states and continuous actions, we achieve polylogarithmic regret...sample-optimal algorithm based on a weighted online least squares estimator."
  - [section] Section 4.3 (Eq. 4.52) shows variance vanishes as ∥θ - A_t∥²; Section 4.5.2 develops LinUCB-VVN exploiting this.
  - [corpus] Related GP bandits (arXiv:2502.06363, 2502.19006) achieve improved regret under noise-free or non-stationary variance, but don't match polylog(T) here.
- **Break condition:** For mixed states, variance doesn't fully vanish; √T scaling is restored (Theorem 16). If action set is discrete, the structural advantage is lost.

### Mechanism 3
- **Claim:** Controlling the minimum eigenvalue of the design matrix via geometric action selection ensures instantaneous regret control.
- **Mechanism:** The algorithm projects the extremal points of the confidence ellipsoid's longest axis onto the action sphere, ensuring λ_min(Vt) = Ω(√λ_max(Vt)). This bounds instantaneous regret by O(1/t), summing to polylog(T), unlike standard LinUCB which bounds cumulative regret directly.
- **Core assumption:** Action set is the unit sphere S^(d-1); the optimistic principle selects actions maximizing upper confidence bounds.
- **Evidence anchors:**
  - [abstract] "The algorithm relies on the optimistic principle and controls the eigenvalues of the design matrix."
  - [section] Theorem 35 establishes λ_min(Vt) ≥ √(2λ_max(Vt)/(3(d-1))); Eq. 4.318 shows instantaneous regret ≤ O(1/t).
  - [corpus] No corpus papers use eigenvalue control for regret; standard linear bandits (arXiv:2508.01681) rely on elliptical potential lemmas giving √T regret.
- **Break condition:** If action set is not smoothly parameterized (e.g., discrete), the geometric projection fails and eigenvalue control cannot be guaranteed.

## Foundational Learning
- **Concept:** Multi-armed stochastic bandits and regret
  - Why needed here: The framework generalizes classical bandits to quantum observables; understanding UCB and regret decomposition is prerequisite.
  - Quick check question: Can you derive the sub-optimality gap decomposition of expected regret (Eq. 2.22)?
- **Concept:** Quantum state tomography and Born's rule
  - Why needed here: Rewards are sampled from measurement outcomes; understanding POVMs and fidelity is essential to interpret regret as disturbance.
  - Quick check question: Explain how measuring a pure state |ψ⟩ with projector Π gives outcome 1 with probability |⟨ψ|ϕ⟩|² where |ϕ⟩ is the projector's range.
- **Concept:** Weighted least squares and concentration inequalities
  - Why needed here: The algorithm uses online weighted least squares with median-of-means to handle unbounded noise with finite variance.
  - Quick check question: Why does weighting by inverse variance improve estimation along directions close to the unknown parameter?

## Architecture Onboarding
- **Component map:**
  1. Unknown quantum state ρ (pure or mixed)
  2. Action set A ⊆ observables (rank-1 projectors for PSMAQB)
  3. LinUCB-VVN policy with weighted median-of-means estimator
  4. Action selection via confidence ellipsoid projection
  5. k independent weighted least-squares estimators with median-of-means aggregation
  6. Design matrix V_t with eigenvalue control
  7. Confidence region C_t for parameter estimation

- **Critical path:**
  1. Initialize V_0 = λ₀I; choose random initial estimate θ̂_0
  2. At each batch t̃, compute eigenvectors v_{t̃-1,i} and select 2(d-1) actions A^±_{t̃,i} via Eq. 4.304
  3. Sample k rewards per action; update k weighted least-squares estimators
  4. Compute median-of-means estimator θ̂_wMoM_{t̃}; update design matrix V_{t̃} via Eq. 4.305
  5. Repeat until horizon T reached; output estimator with infidelity O(log(T)/t) (Theorem 43)

- **Design tradeoffs:**
  - **Batch size vs. adaptivity:** Using batches of 2(d-1) actions reduces update frequency but ensures eigenvalue control. Larger k (subsampling) improves concentration at cost of sample complexity.
  - **Weight choice:** Weights ω(V_{t̃-1}) = √λ_max / (12√(d-1) β_w) must overestimate true variance to guarantee confidence; overly conservative weights slow convergence.
  - **Discrete vs. continuous actions:** Continuous sphere enables polylog(T) regret; discrete observables revert to √T scaling (Theorems 11-13).

- **Failure signatures:**
  - **Regret grows as √T:** Likely mixed-state environment or discrete action set; check environment rank and action set cardinality.
  - **Estimator diverges:** Weights failing to upper-bound variance (event G_t violated); increase k or adjust weight formula.
  - **λ_min(V_t) too small:** Action selection not following geometric projection rule; verify eigenvalue decomposition and projection steps.

- **First 3 experiments:**
  1. **Qubit pure state on Bloch sphere:** Set d=3, T=10^4, k=10 subsamples. Verify polylog(T) regret and O(1/t) infidelity scaling (replicate Fig. 5.1).
  2. **Mixed state environment:** Same setup but environment rank>1. Confirm regret scales as √T (Theorem 16), demonstrating mechanism breakdown.
  3. **Discrete action set:** Limit actions to k finite Pauli observables. Verify regret scaling matches √(kT) (Theorem 12), showing continuous sphere is essential.

## Open Questions the Paper Calls Out
None

## Limitations
- The polylogarithmic regret result critically depends on pure state environments and continuous action spaces - both assumptions are fragile
- Real quantum systems may not permit single-copy measurements via rank-1 projectors
- The geometric eigenvalue control technique may be computationally challenging for high-dimensional systems
- Mixed states (even rank-2) restore √T scaling, suggesting the mechanism breaks down with minimal noise

## Confidence
- **High Confidence:** √T regret bounds for general quantum bandit settings (Theorems 11-13); basic mapping between quantum tomography and bandit framework
- **Medium Confidence:** Polylogarithmic regret for pure states (Theorem 16); eigenvalue control mechanism (Theorem 35)
- **Low Confidence:** Computational feasibility of geometric projection in high dimensions; robustness to measurement imperfections

## Next Checks
1. **Mixed State Robustness Test:** Systematically vary environment rank from 1 to d and measure actual regret scaling. Verify the transition from polylog(T) to √T occurs at rank > 1.

2. **Discrete Action Set Simulation:** Implement the algorithm with finite Pauli observable sets (X, Y, Z for qubits). Confirm regret scales as √(kT) and identify the threshold where continuous actions become necessary.

3. **Eigenvalue Control Stability:** Track λ_min(V_t)/√(λ_max(V_t)) ratios during execution. Test whether the algorithm maintains the required Ω(1) ratio under different weight parameters and batch sizes.