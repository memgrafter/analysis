---
ver: rpa2
title: 'SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based
  Reinforcement Learning'
arxiv_id: '2506.14648'
source_url: https://arxiv.org/abs/2506.14648
tags:
- learning
- reward
- exploration
- arxiv
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents SENIOR, a novel method for efficient query
  selection and preference-guided exploration in preference-based reinforcement learning
  (PbRL) for robot manipulation tasks. The authors address two key challenges in PbRL:
  poor feedback-efficiency (selecting meaningful segments for human preference labeling)
  and sample-efficiency (guiding exploration toward task-relevant states).'
---

# SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.14648
- Source URL: https://arxiv.org/abs/2506.14648
- Reference count: 40
- Primary result: Achieves 97% average success rate on 6 simulated robot manipulation tasks, outperforming baselines by 20-44 percentage points

## Executive Summary
SENIOR addresses two critical challenges in preference-based reinforcement learning (PbRL): selecting informative segments for human preference labeling and guiding efficient exploration toward task-relevant states. The method introduces Motion-Distinction-based Selection (MDS) that uses kernel density estimation to identify segment pairs with apparent motion and different directions, making them easier for humans to compare. SENIOR also incorporates Preference-Guided Exploration (PGE) that provides intrinsic rewards for visiting states with high human preference density but low visitation, creating a synergy that accelerates learning.

## Method Summary
SENIOR builds upon PEBBLE or MRN with SAC, adding two core components. MDS selects segment pairs by first computing motion-scores via KDE on end-effector positions, retaining top 30% by motion, then filtering for low direction-scores via PCA and cosine similarity. PGE maintains a curiosity buffer and computes intrinsic rewards as normalized preference density divided by exploration density, decaying the exploration weight over time. The method uses hybrid experience sampling from both regular and curiosity buffers for policy updates.

## Key Results
- Achieves 97% average success rate on six simulated tasks versus 53-70% for baselines
- Demonstrates 4× improvement in feedback-efficiency, reaching ~100% success with only 250 feedback instances on Door Lock
- Outperforms PEBBLE (53%), MRN (54%), RUNE (60%), M-RUNE (70%), and QPA (53%) across all tasks
- Ablation studies confirm both MDS and PGE components contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1: Motion-Distinction-based Selection (MDS)
Selecting segment pairs with low state density and distinct motion directions yields more informative, human-comparable feedback, accelerating reward model learning. The method computes state density per segment using KDE over end-effector positions, assigns motion-scores inversely proportional to average density, and prioritizes pairs with low cosine similarity between principal direction vectors.

### Mechanism 2: Preference-Guided Exploration (PGE)
Intrinsic rewards derived from human-preferred but under-visited states drive efficient exploration and accelerate policy convergence. The method maintains a curiosity buffer, estimates state densities for both preference and exploration data, and computes intrinsic rewards as the ratio of preference to exploration densities.

### Mechanism 3: Synergy Between MDS and PGE
MDS provides high-quality preference labels that improve reward model accuracy, which in turn enhances PGE's intrinsic reward guidance, creating a positive feedback loop. Better reward models yield more accurate preference densities, which guide exploration toward truly valuable states, producing better segments for further MDS selection.

## Foundational Learning
- **Concept: Preference-based Reinforcement Learning (PbRL)** - Core framework; SENIOR improves feedback and sample efficiency in PbRL. Quick check: Can you explain how a reward model is trained from pairwise human preferences using the Bradley-Terry model?
- **Concept: Kernel Density Estimation (KDE)** - Used in both MDS and PGE to estimate state densities for motion scoring and intrinsic reward calculation. Quick check: How does KDE provide a non-parametric estimate of state visitation density, and what is the role of the bandwidth parameter h?
- **Concept: Intrinsic Motivation in RL** - PGE builds on intrinsic reward ideas to guide exploration beyond random or uncertainty-based methods. Quick check: What are common forms of intrinsic rewards (e.g., count-based, curiosity-based) and how does PGE's preference-guided intrinsic reward differ?

## Architecture Onboarding
- **Component map**: Replay Buffer B -> Reward Model ˆrψ -> Policy πϕ; Preference Dataset D -> MDS Module -> Human Labeling -> D; Curiosity Buffer Bcur -> PGE Module -> Intrinsic Rewards -> Hybrid Experience
- **Critical path**: Unsupervised pre-training initializes policy and populates B; MDS selection → human labeling → reward model update → relabel B; PGE update → compute intrinsic rewards for Bcur; Policy update using hybrid minibatches from B and Bcur
- **Design tradeoffs**: Motion selection ratio (30%) controls high-motion pair retention; curiosity buffer update frequency (500-1000 steps) balances freshness with compute; decay rate ρ for βt trades off exploration duration vs. exploitation
- **Failure signatures**: Reward model loss plateaus → check for noisy labels; intrinsic reward becomes uniform → check preference/exploration density ratios; policy fails to converge → check β0 or ρ values
- **First 3 experiments**: 1) Replicate Door Lock success rate with 250 feedback (~98% success); 2) Run ablation study (M-SENIOR, w/o PGE, w/o MDS) to confirm component contributions; 3) Visualize state visitation comparing PGE vs RUNE to confirm preference-guided exploration focus

## Open Questions the Paper Calls Out
None

## Limitations
- MDS approach may fail in tasks where end-effector position is not a meaningful proxy for task-relevant states
- Motion-distinction heuristic assumes human preference quality improves with clear, distinct motion without extensive validation across diverse task types
- Synergy between MDS and PGE is asserted but not independently verified through targeted ablation studies

## Confidence
- **High Confidence**: Empirical performance claims (success rates, feedback efficiency gains) are well-supported by extensive experiments across six simulated and four real-world tasks
- **Medium Confidence**: Theoretical mechanism explanations are reasonable but lack extensive ablation or analytical proof; synergy claim needs more targeted validation
- **Low Confidence**: Generalization to non-manipulation tasks or scenarios with different state representations remains untested

## Next Checks
1. **Generalization Test**: Apply SENIOR to non-manipulation RL tasks (e.g., locomotion) where end-effector position is not a meaningful state proxy to test robustness of MDS approach
2. **Component Isolation Study**: Design experiments that systematically vary motion-distinction metrics and intrinsic reward formulations to quantify their individual contributions to performance gains
3. **Human Study**: Conduct user studies comparing preference labeling quality and time between MDS-selected pairs versus baseline selection methods across multiple annotators