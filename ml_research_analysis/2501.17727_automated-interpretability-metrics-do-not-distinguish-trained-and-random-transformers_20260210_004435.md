---
ver: rpa2
title: Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers
arxiv_id: '2501.17727'
source_url: https://arxiv.org/abs/2501.17727
tags:
- layer
- activation
- trained
- entropy
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that common auto-interpretability metrics
  used to evaluate sparse autoencoders (SAEs) on transformer activations fail to distinguish
  between trained and randomly initialized models. Across multiple Pythia model sizes
  and randomization schemes, SAEs trained on randomly initialized transformers produce
  auto-interpretability scores and reconstruction metrics similar to those from trained
  models.
---

# Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers

## Quick Facts
- arXiv ID: 2501.17727
- Source URL: https://arxiv.org/abs/2501.17727
- Reference count: 40
- Primary result: Common auto-interpretability metrics fail to distinguish trained from randomly initialized transformer models when evaluating sparse autoencoders

## Executive Summary
This paper demonstrates that standard automated interpretability metrics used to evaluate sparse autoencoders (SAEs) on transformer activations cannot reliably distinguish between features learned from trained models and those from randomly initialized models. The authors systematically show that SAEs trained on randomly initialized Pythia models produce auto-interpretability scores and reconstruction metrics similar to those from trained models across multiple randomization schemes and model sizes. This finding challenges the validity of using aggregate interpretability scores as proxies for mechanistic interpretability, suggesting these metrics may reflect statistical artifacts rather than genuine learned computational features.

## Method Summary
The authors evaluate SAEs trained on both trained and randomly initialized Pythia models using standard interpretability metrics including auto-interpretability scores and reconstruction quality. They employ multiple randomization schemes (token, position, embedding, and residual stream randomization) across different model sizes. The key experimental design involves comparing SAE performance metrics between trained and random models to determine whether automated metrics can reliably identify features that capture learned computational structure. The analysis focuses on aggregate metric comparisons rather than detailed feature-level semantic validation.

## Key Results
- SAEs trained on randomly initialized transformers achieve auto-interpretability scores comparable to those trained on trained models
- Reconstruction metrics show similar patterns across trained and random model comparisons
- Multiple randomization schemes produce consistent results, indicating the phenomenon is not specific to any particular type of randomization
- Current automated interpretability metrics reflect statistical properties present even without training, rather than genuine learned features

## Why This Works (Mechanism)

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural networks trained to compress and reconstruct input data while enforcing sparsity on hidden layer activations. Why needed: SAEs are the primary tool for extracting interpretable features from transformer activations. Quick check: Can you explain how sparsity regularization affects feature learning?
- **Transformer activations**: Intermediate representations computed by transformer layers during forward passes. Why needed: These are the inputs to SAEs for feature extraction. Quick check: Do you understand how attention mechanisms generate these activations?
- **Auto-interpretability metrics**: Quantitative measures used to evaluate how interpretable extracted features are. Why needed: These metrics are the basis for automated evaluation of interpretability work. Quick check: Can you name at least two common auto-interpretability metrics?
- **Randomization schemes**: Systematic modifications to model components (tokens, positions, embeddings, residual streams) to create baseline models without learned computation. Why needed: These provide controls for distinguishing learned features from artifacts. Quick check: Can you describe what each randomization scheme changes?
- **Mechanistic interpretability**: The field focused on understanding how neural networks implement specific computations. Why needed: This paper's findings directly challenge assumptions in this research area. Quick check: Do you understand the difference between correlational and causal interpretability?
- **Aggregate vs. feature-level analysis**: Comparing overall metric scores versus examining individual feature semantics. Why needed: The paper shows aggregate scores can be misleading without feature-level validation. Quick check: Can you explain why aggregate metrics might fail to capture important details?

## Architecture Onboarding

Component map: Randomization schemes → Trained/Random Pythia models → Transformer activations → Sparse Autoencoders → Auto-interpretability metrics → Reconstruction metrics → Comparison analysis

Critical path: Randomization → Model generation → SAE training → Metric computation → Statistical comparison

Design tradeoffs: The authors chose Pythia models for their accessibility and standardized architecture, but this limits generalizability. They prioritized systematic comparison over feature-level semantic analysis, focusing on what metrics can detect rather than what features actually represent.

Failure signatures: High auto-interpretability scores from random models indicate metrics capture statistical regularities rather than computational relevance. Similar reconstruction quality suggests SAEs can find efficient representations regardless of underlying computational structure.

Three first experiments:
1. Train SAEs on random models with varying sparsity penalties to determine if metric sensitivity depends on architectural choices
2. Apply the same evaluation pipeline to GPT-3 or LLaMA models to test architecture dependence
3. Implement and test "abstractness" measures alongside current metrics to evaluate their distinguishing power

## Open Questions the Paper Calls Out
None

## Limitations
- The paper relies on aggregate score comparisons rather than detailed feature-level semantic validation
- Randomization schemes may not capture all possible ways random models could produce interpretable-looking features
- Findings are based on Pythia models and may not generalize to other architectures or SAE variants
- The paper does not explore alternative interpretability metrics that might succeed where current metrics fail

## Confidence

High confidence: The empirical finding that random models produce SAEs with similar aggregate auto-interpretability scores and reconstruction metrics as trained models. The experimental methodology and statistical comparisons are sound and clearly presented.

Medium confidence: The conclusion that current automated interpretability metrics are insufficient proxies for mechanistic interpretability. While the evidence strongly suggests these metrics have limitations, the paper doesn't fully explore alternative metrics or provide definitive proof that no automated metric can capture learned computational relevance.

Low confidence: The broader implications for the field of mechanistic interpretability and the specific recommendation for "abstractness" measures. These recommendations are speculative and would require substantial additional validation to confirm their effectiveness.

## Next Checks
1. Feature-level semantic validation: Conduct human evaluations or automated semantic clustering to determine whether features learned from random models have any genuine semantic content, or if they are purely statistical artifacts.

2. Cross-architecture replication: Test the same randomization and evaluation pipeline on diverse model families (GPT-3, LLaMA, OPT) and SAE variants (different sparsity penalties, activation functions like ReLU vs tanh) to establish whether the findings generalize beyond Pythia models.

3. Alternative metric evaluation: Implement and compare the performance of proposed "abstractness" measures and other potential interpretability metrics (e.g., feature consistency across examples, causal intervention tests) against the current automated metrics to determine if any can successfully distinguish learned features from random artifacts.