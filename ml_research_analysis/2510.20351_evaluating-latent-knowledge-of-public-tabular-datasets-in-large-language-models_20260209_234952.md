---
ver: rpa2
title: Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models
arxiv_id: '2510.20351'
source_url: https://arxiv.org/abs/2510.20351
tags:
- contamination
- datasets
- data
- llms
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) exhibit
  contamination effects on widely used tabular datasets like Adult Income and Titanic.
  Through controlled probing experiments involving completion and existence tasks
  on both real and obfuscated data variants, the authors reveal that contamination
  effects occur exclusively for datasets with strong semantic cues (meaningful column
  names and interpretable categories).
---

# Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models

## Quick Facts
- arXiv ID: 2510.20351
- Source URL: https://arxiv.org/abs/2510.20351
- Authors: Matteo Silvestri; Flavio Giorgi; Fabrizio Silvestri; Gabriele Tolomei
- Reference count: 16
- Primary result: Semantic contamination drives LLM performance on tabular datasets, not memorization of specific records.

## Executive Summary
This study investigates whether Large Language Models exhibit contamination effects on widely used tabular datasets like Adult Income and Titanic. Through controlled probing experiments involving completion and existence tasks on both real and obfuscated data variants, the authors reveal that contamination effects occur exclusively for datasets with strong semantic cues (meaningful column names and interpretable categories). In contrast, when such cues are removed, performance drops sharply to near-random levels. No evidence of syntactic contamination (verbatim memorization) was found. The results indicate that LLMs' apparent competence on tabular reasoning tasks stems primarily from semantic familiarity rather than genuine generalization, highlighting the need for contamination-aware evaluation protocols in future LLM assessments.

## Method Summary
The study employs two probing tasks across seven tabular datasets in three variants: real, "like" (marginal distributions preserved, dependencies removed), and obfuscated (categorical labels replaced with abstract tokens). The completion task masks 20% of high-entropy/variance attributes and asks models to select the true value from five candidates. The existence task presents five perturbed versions of each record and asks models to identify the original. Zero-shot prompting is used across llama_8B, mistral_7B, and qwen models (7B/14B/32B) with consistent prompt templates, measuring accuracy against a 20% random baseline with statistical significance testing.

## Key Results
- Contamination effects occur exclusively for datasets with strong semantic cues (e.g., meaningful column names)
- Performance drops sharply to near-random levels when semantic cues are removed via obfuscation
- No evidence of syntactic contamination found—models fail to identify specific records above chance levels
- An intriguing exception occurs in the credit dataset, where some models maintain non-trivial accuracy even on obfuscated variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contamination effects in LLMs on tabular datasets operate through semantic familiarity with feature names and value labels, not through memorization of actual data records.
- Mechan: When models encounter datasets with human-interpretable features (e.g., "occupation," "education," "fare"), they activate learned associations from pretraining corpora where these concepts appeared together. This allows plausible value completion without any instance-level recall.
- Core assumption: Semantic cues from widely-discussed datasets exist in pretraining corpora through tutorials, academic papers, and Kaggle discussions—not through the raw tabular data itself.
- Evidence anchors:
  - [abstract] "contamination effects emerge exclusively for datasets containing strong semantic cues—for instance, meaningful column names or interpretable value categories. In contrast, when such cues are removed or randomized, performance sharply declines to near-random levels."
  - [section 5, Results] "the systematic failure of all models in the existence task indicates that LLMs do not recall specific entries from benchmark tabular datasets... there is no evidence of verbatim memorisation or syntactic contamination at the instance level."
  - [corpus] Weak direct evidence; neighbor papers focus on tabular learning and fact retrieval, not contamination mechanisms specifically.
- Break condition: If models showed above-baseline performance on the existence task or on obfuscated variants, this mechanism would be insufficient to explain results.

### Mechanism 2
- Claim: Syntactic contamination (verbatim memorization of tabular entries) does not occur in LLMs up to 32B parameters for the evaluated benchmark datasets.
- Mechan: The existence task directly probes whether models can distinguish real records from perturbed versions. Consistent near-random performance (≈20% for 5-way classification) indicates no accessible instance-level representations exist in model weights.
- Core assumption: The existence task methodology is sufficient to detect memorization if it existed; failure indicates absence rather than insufficient probing.
- Evidence anchors:
  - [section 4, Methodology] "The model was then shown five versions of the same record (one genuine and four perturbed) and asked to identify the original entry. This setup tests whether the LLM can discriminate between real and synthetic samples."
  - [section 5, Results] "the existence task, which directly tests for the memorisation of specific records, was consistently failed across all models and datasets."
  - [corpus] No corpus papers directly address syntactic vs. semantic memorization in tabular contexts.
- Break condition: If larger models (>32B) showed above-baseline existence task performance, this would indicate scale-dependent memorization capacity not captured in current experiments.

### Mechanism 3
- Claim: Models can exploit latent statistical regularities in tabular structure even when semantic cues are removed, reflecting a form of emergent reasoning distinct from contamination.
- Mechan: The credit dataset maintained non-trivial accuracy in the obfuscated condition (qwen_14B: 0.57, qwen_32B: 0.69). This suggests models detected distributional patterns or inter-feature correlations without relying on feature semantics.
- Core assumption: This pattern reflects genuine statistical reasoning rather than undocumented pretraining exposure to obfuscated variants.
- Evidence anchors:
  - [section 5, Results] "An intriguing exception arises in the credit dataset, where some models maintain non-trivial accuracy even in the obfuscated variant. We hypothesise that this effect reflects the models' ability to exploit latent statistical regularities within the tabular structure."
  - [section 6, Limitations] "larger-scale models, exceeding 70B or even 100B parameters, may exhibit distinct contamination dynamics due to their increased memorization capacity."
  - [corpus] Neighbor paper "Evaluating LLM Understanding via Structured Tabular Decision Simulations" references consistency and domain-general reasoning, partially supporting emergent reasoning frameworks.
- Break condition: If further investigation revealed the credit obfuscated variant appeared in pretraining data, this would recategorize the finding as contamination rather than reasoning.

## Foundational Learning

- Concept: **Dataset Contamination Taxonomy**
  - Why needed here: The paper distinguishes syntactic (verbatim) from semantic (meaning-based) contamination. Without this distinction, one might misinterpret high completion accuracy as memorization rather than conceptual familiarity.
  - Quick check question: If a model completes "occupation: ___" with "tech" for a high-income individual, is this syntactic or semantic contamination if it never saw that exact row? (Answer: semantic—leveraging learned associations, not recall.)

- Concept: **Probing Task Design for Disentanglement**
  - Why needed here: The completion vs. existence task pairing isolates different capabilities. Completion tests semantic inference; existence tests verbatim recognition. Both are needed to characterize contamination type.
  - Quick check question: Why does near-random performance on the existence task not imply the model is unhelpful for tabular tasks? (Answer: It may still excel at semantic reasoning and pattern exploitation, just not memorization.)

- Concept: **Obfuscation as a Control Condition**
  - Why needed here: Replacing "sex" with "c01" removes semantic anchors while preserving statistical structure. Comparing obfuscated to real performance quantifies semantic leakage magnitude.
  - Quick check question: If accuracy drops from 0.73 to 0.13 when obfuscating (adult dataset, qwen_32B), what does this imply about the 0.73 baseline? (Answer: ~82% of the performance signal comes from semantic cues rather than structural reasoning.)

## Architecture Onboarding

- Component map:
  - Dataset Variant Generator -> Probing Task Module (Completion/Existence) -> Evaluation Harness -> Statistical Analysis
  - Models (llama_8B, mistral_7B, qwen_7B/14B/32B) -> Zero-shot Prompting -> Accuracy Metrics

- Critical path:
  1. Select benchmark datasets (categorize as semantic vs. non-semantic based on feature interpretability)
  2. Generate all three variants per dataset
  3. Run both probing tasks on all variants across all models
  4. Compare against random baseline (20% for 5-way) with statistical significance testing (α=0.001 per paper)
  5. Identify where performance degrades: if real→like drops but like→obf doesn't, contamination is semantic; if real→obf drops sharply, reasoning depends on feature meaning

- Design tradeoffs:
  - **Masking ratio (20%)**: Higher masking increases task difficulty but may introduce noise; lower masking may not sufficiently probe model knowledge. Paper selected 20% without ablation—consider testing sensitivity.
  - **Candidate count (5)**: More candidates reduce random baseline but increase prompt length; 5 balances signal detection with context window constraints.
  - **Dataset selection**: Semantic datasets are deliberately high-exposure (Adult, Titanic); this maximizes contamination signal but limits generalization claims. Non-semantic datasets (gamma, synthetic) serve as controls but may have different intrinsic difficulty.

- Failure signatures:
  - Above-baseline existence task accuracy: Indicates syntactic memorization not observed in this study—would require re-evaluating decontamination protocols.
  - Above-baseline completion on non-semantic datasets: Suggests contamination extends beyond semantic cues or that "non-semantic" classification was incorrect.
  - No performance drop from real→obf on semantic datasets: Indicates contamination is not the driver; model may be solving through pure statistical reasoning.

- First 3 experiments:
  1. **Replication on larger models (70B+)**: Test whether syntactic contamination emerges at greater scale by running the existence task on Llama-70B or Qwen-72B. If above-baseline performance appears, memorization capacity scales with parameter count.
  2. **Cross-dataset transfer probe**: Train a classifier on completion task outputs to predict whether a model has seen a dataset during pretraining. Validate against known-contaminated and known-clean benchmarks.
  3. **Semantic cue degradation study**: Gradually obfuscate feature names (e.g., "occupation" → "occ" → "o" → "c01") and measure performance curves. This quantifies the minimum semantic signal required for contamination effects to manifest.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does increasing model scale (beyond 32B parameters) induce syntactic memorization of specific tabular entries?
  - Basis in paper: [explicit] The authors acknowledge the study is limited to models up to 32B parameters and propose evaluating "larger-scale models, exceeding 70B or even 100B parameters" as future work.
  - Why unresolved: The current results show no verbatim memorization, but larger models have greater capacity and broader training corpora, potentially enabling syntactic contamination absent in smaller models.
  - What evidence would resolve it: Replicating the "Existence Task" on models with 70B+ parameters to see if they can identify specific records above the random baseline.

- **Open Question 2**: To what extent does semantic contamination bias performance on complex downstream tasks like feature importance attribution versus simple prediction?
  - Basis in paper: [explicit] The authors list assessing "how this classification impacts LLM performance across various machine learning tasks" as a primary direction for future work.
  - Why unresolved: The current study isolates contamination via probing tasks (completion/existence) but does not measure how this latent knowledge inflates performance on practical analytical tasks.
  - What evidence would resolve it: A comparative study of model performance on downstream tasks using both "contaminated" (semantic) and "clean" (obfuscated) versions of the same datasets.

- **Open Question 3**: Can LLMs exploit latent statistical regularities to solve tabular tasks even when all semantic cues are removed?
  - Basis in paper: [inferred] The authors note an "intriguing exception" where models maintained non-trivial accuracy on the obfuscated "credit" dataset, hypothesizing "emergent reasoning" or the exploitation of structural patterns.
  - Why unresolved: The main finding suggests performance relies on semantic cues; high performance on obfuscated data implies a mechanism (statistical or structural) not fully explained by the semantic contamination hypothesis.
  - What evidence would resolve it: Ablation studies on synthetic datasets with controlled statistical correlations but no semantic content to see if models can generalize from structure alone.

## Limitations
- Analysis confined to models up to 32B parameters, leaving open whether larger models exhibit different contamination dynamics
- Semantic vs. non-semantic dataset classification relies on human judgment of feature interpretability
- Existence task methodology assumes failure to discriminate indicates absence of memorization rather than insufficient probing

## Confidence
- High confidence: Semantic contamination mechanism (semantic cues drive performance, not memorization)
- Medium confidence: Syntactic contamination absence (no verbatim memorization up to 32B)
- Medium confidence: Statistical reasoning on credit dataset (single-dataset observation requiring validation)

## Next Checks
1. Test the existence task on models exceeding 70B parameters to determine if syntactic contamination emerges at larger scales
2. Systematically vary the degree of semantic obfuscation (e.g., gradual abbreviation of feature names) to map the contamination sensitivity curve
3. Cross-validate findings by training a classifier to predict dataset contamination status based on model completion accuracy, then test against independently verified clean and contaminated benchmarks