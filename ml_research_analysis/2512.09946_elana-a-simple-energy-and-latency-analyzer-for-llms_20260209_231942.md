---
ver: rpa2
title: 'ELANA: A Simple Energy and Latency Analyzer for LLMs'
arxiv_id: '2512.09946'
source_url: https://arxiv.org/abs/2512.09946
tags:
- latency
- elana
- energy
- profiling
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ELANA is a lightweight, academic-friendly profiler for benchmarking
  LLMs, designed to address the lack of a unified tool for measuring latency, energy,
  and memory metrics across diverse hardware platforms. It supports Hugging Face models
  and offers a simple CLI for profiling model size, KV cache footprint, prefilling
  latency (TTFT), generation latency (TPOT), end-to-end latency (TTLT), and energy
  consumption.
---

# ELANA: A Simple Energy and Latency Analyzer for LLMs

## Quick Facts
- arXiv ID: 2512.09946
- Source URL: https://arxiv.org/abs/2512.09946
- Authors: Hung-Yueh Chiang; Bokun Wang; Diana Marculescu
- Reference count: 6
- ELANA is a lightweight, academic-friendly profiler for benchmarking LLMs, designed to address the lack of a unified tool for measuring latency, energy, and memory metrics across diverse hardware platforms.

## Executive Summary
ELANA is a lightweight, academic-friendly profiler for benchmarking LLMs, designed to address the lack of a unified tool for measuring latency, energy, and memory metrics across diverse hardware platforms. It supports Hugging Face models and offers a simple CLI for profiling model size, KV cache footprint, prefilling latency (TTFT), generation latency (TPOT), end-to-end latency (TTLT), and energy consumption. ELANA is compatible with popular Hugging Face APIs and can be easily adapted for compressed or low-bit-width models. Experiments on cloud (A6000) and edge (Jetson AGX Thor, Orin Nano) GPUs show that ELANA accurately profiles latency (e.g., TTFT: 87.72–2788.39 ms, TPOT: 23.15–39.40 ms) and energy (e.g., J/Token: 0.06–13.86 J, J/Request: 3343.91–15001.54 J) for various models like Llama-3.1-8B, Qwen-2.5-7B, and Nemotron-H-8B under different batch sizes and sequence lengths. ELANA enables reproducible benchmarking and supports fine-grained kernel profiling via PyTorch Profiler.

## Method Summary
ELANA is a lightweight, academic-friendly profiler for benchmarking LLMs, designed to address the lack of a unified tool for measuring latency, energy, and memory metrics across diverse hardware platforms. It supports Hugging Face models and offers a simple CLI for profiling model size, KV cache footprint, prefilling latency (TTFT), generation latency (TPOT), end-to-end latency (TTLT), and energy consumption. ELANA is compatible with popular Hugging Face APIs and can be easily adapted for compressed or low-bit-width models. Experiments on cloud (A6000) and edge (Jetson AGX Thor, Orin Nano) GPUs show that ELANA accurately profiles latency (e.g., TTFT: 87.72–2788.39 ms, TPOT: 23.15–39.40 ms) and energy (e.g., J/Token: 0.06–13.86 J, J/Request: 3343.91–15001.54 J) for various models like Llama-3.1-8B, Qwen-2.5-7B, and Nemotron-H-8B under different batch sizes and sequence lengths. ELANA enables reproducible benchmarking and supports fine-grained kernel profiling via PyTorch Profiler.

## Key Results
- ELANA accurately profiles latency and energy consumption across cloud and edge GPUs.
- ELANA is compatible with popular Hugging Face APIs and can be easily adapted for compressed or low-bit-width models.
- ELANA enables reproducible benchmarking and supports fine-grained kernel profiling via PyTorch Profiler.

## Why This Works (Mechanism)
None

## Foundational Learning
- Hugging Face APIs: Why needed - to support a wide range of LLM models; Quick check - verify compatibility with the latest model versions.
- PyTorch Profiler: Why needed - to enable fine-grained kernel profiling; Quick check - test with different model architectures.
- Energy measurement tools: Why needed - to accurately measure energy consumption; Quick check - compare with hardware-specific profiling tools.

## Architecture Onboarding
- Component map: Hugging Face Models -> ELANA Profiler -> PyTorch Profiler -> Energy Measurement Tools
- Critical path: Model loading -> Latency/Energy measurement -> Data output
- Design tradeoffs: Balancing accuracy with ease of use; Supporting both cloud and edge platforms.
- Failure signatures: Inaccurate energy measurements due to hardware monitoring tool precision; Profiling errors with unsupported model architectures.
- First experiments: 1) Profile a simple model on a known platform; 2) Compare results with manual measurements; 3) Test with a compressed model.

## Open Questions the Paper Calls Out
None

## Limitations
- The generalizability of the profiling methodology across diverse model architectures and hardware platforms is uncertain.
- The accuracy of energy measurements depends on the precision of hardware monitoring tools and could vary across systems.
- Limited detail on how the profiler handles compressed or low-bit-width models.

## Confidence
- Claims about ELANA's profiling accuracy and usability: High
- Claims about ELANA's adaptability to compressed or low-bit-width models: Medium

## Next Checks
1. Validate ELANA's profiling accuracy on a broader set of model architectures, including sparse and low-bit-width models, and on additional hardware platforms (e.g., CPUs, FPGAs, mobile devices).
2. Conduct a reproducibility study by having independent research groups replicate the profiling results using ELANA on the same models and hardware.
3. Perform a detailed comparison of ELANA's energy measurements with those from hardware-specific profiling tools to assess accuracy and potential systematic biases.