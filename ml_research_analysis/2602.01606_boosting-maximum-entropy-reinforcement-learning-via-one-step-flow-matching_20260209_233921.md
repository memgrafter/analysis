---
ver: rpa2
title: Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching
arxiv_id: '2602.01606'
source_url: https://arxiv.org/abs/2602.01606
tags:
- policy
- flow
- entropy
- learning
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FLAME, a novel framework that integrates flow
  matching with maximum entropy reinforcement learning to achieve expressive, one-step
  action generation. The key challenges addressed are the intractability of the energy-based
  target policy and the bias in log-likelihood estimation for entropy regularization.
---

# Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching

## Quick Facts
- arXiv ID: 2602.01606
- Source URL: https://arxiv.org/abs/2602.01606
- Reference count: 40
- Primary result: FLAME achieves 11945.08±461.47 on HalfCheetah-v5, surpassing SAC (11379.74±474.51) and SDAC (11165.70±164.25), while maintaining 8× faster inference than diffusion methods.

## Executive Summary
This paper introduces FLAME, a novel framework that integrates flow matching with maximum entropy reinforcement learning to achieve expressive, one-step action generation. The key challenges addressed are the intractability of the energy-based target policy and the bias in log-likelihood estimation for entropy regularization. FLAME introduces a Q-Reweighted Flow Matching objective that bypasses partition function estimation via importance reweighting, and a decoupled entropy estimator that corrects discretization bias while maintaining efficient one-step inference. By integrating the MeanFlow formulation, FLAME achieves high-fidelity control with NFE=1. Empirical results on MuJoCo demonstrate that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost.

## Method Summary
FLAME addresses the intractability of maximum entropy RL by combining flow matching with reinforcement learning. The method introduces Q-Reweighted Flow Matching, which reweights the likelihood of actions based on their Q-values to avoid partition function estimation. A decoupled entropy estimator is proposed to correct discretization bias while maintaining computational efficiency. The framework integrates these components with the MeanFlow formulation to enable one-step inference. The approach trains a flow matching model to transform a simple base distribution into complex action distributions conditioned on states and Q-values, allowing expressive policy representation without the computational burden of multi-step sampling.

## Key Results
- FLAME-R achieves 11945.08±461.47 on HalfCheetah-v5, outperforming SAC (11379.74±474.51) and SDAC (11165.70±164.25)
- Maintains 8× faster inference than diffusion methods while achieving comparable performance
- Demonstrates superior performance over Gaussian baselines across MuJoCo benchmarks
- Ablation studies confirm the importance of both Q-Reweighted objective and decoupled entropy estimator

## Why This Works (Mechanism)
FLAME works by combining flow matching's ability to model complex distributions with reinforcement learning's optimization framework. The Q-Reweighted Flow Matching objective allows the model to focus on high-value actions without explicitly computing partition functions, while the decoupled entropy estimator maintains accurate entropy regularization. The MeanFlow formulation enables one-step inference by learning the entire transformation path from base to target distribution. This combination allows FLAME to achieve the expressiveness of energy-based models with the computational efficiency of one-step sampling, addressing the fundamental trade-off between expressiveness and inference cost in maximum entropy RL.

## Foundational Learning
- Flow Matching: A generative modeling technique that learns continuous paths between distributions, needed for modeling complex action distributions without multi-step sampling; quick check: verify continuity of learned paths.
- Maximum Entropy RL: RL framework that maximizes both reward and policy entropy, needed for exploration and robustness; quick check: measure policy entropy during training.
- Q-Reweighting: Importance weighting based on action values, needed to focus learning on high-value regions without partition functions; quick check: analyze weight distribution and variance.
- Decoupled Entropy Estimation: Separate estimation of entropy components to correct discretization bias, needed for accurate entropy regularization; quick check: verify entropy estimates match true values on simple distributions.
- MeanFlow Formulation: Continuous-time flow matching formulation, needed for stable one-step inference; quick check: verify numerical stability of ODE solvers.

## Architecture Onboarding

Component Map: State → Q-value Estimator → Flow Matching Model → Action Distribution → Environment

Critical Path: Q-value estimation → Action weighting → Flow matching transformation → Action sampling → Reward accumulation

Design Tradeoffs:
- Expressiveness vs. inference speed: Flow matching enables complex distributions but requires careful parameterization for one-step inference
- Accuracy vs. bias: Q-Reweighting avoids partition functions but introduces importance weight variance
- Stability vs. efficiency: Decoupled entropy estimation improves accuracy but adds computational overhead

Failure Signatures:
- High importance weight variance leading to unstable learning
- Degraded policy entropy indicating bias in entropy estimation
- Poor sample quality suggesting issues with flow matching parameterization
- Training instability from numerical errors in ODE solving

Three First Experiments:
1. Validate Q-Reweighted objective on simple energy-based models with known partition functions
2. Test decoupled entropy estimator on discretized distributions with analytical entropy
3. Evaluate flow matching expressiveness on standard density estimation benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees for convergence and bias under function approximation are not established
- Performance evaluation limited to relatively simple MuJoCo environments
- Computational overhead compared to Gaussian policies in larger-scale applications not quantified
- Impact of importance weight variance on learning stability not thoroughly analyzed

## Confidence

High confidence: Empirical results showing FLAME's superior performance vs Gaussian baselines and comparable performance to diffusion methods at 8× faster inference; ablation studies validating design choices.

Medium confidence: Claims about one-step expressive action generation maintaining entropy regularization; relationship between NFE=1 and expressiveness in complex environments.

Low confidence: Assertions about flow matching naturally addressing partition function intractability without bias; claims about generalization beyond evaluated tasks; robustness to hyperparameter choices.

## Next Checks

1. Conduct rigorous theoretical analysis establishing convergence properties under function approximation, quantifying bias from Q-Reweighted objective, and characterizing decoupled entropy estimator impact.

2. Evaluate FLAME on challenging continuous control benchmarks with sparse rewards, high-dimensional action spaces, and long-horizon tasks; compare computational requirements against baselines as task complexity increases.

3. Systematically analyze impact of each approximation component (Q-reweighting, decoupled entropy estimation, flow matching parameterization) on performance and bias; measure entropy regularization quality and importance weight variance effects on learning stability.