---
ver: rpa2
title: 'P/D-Device: Disaggregated Large Language Model between Cloud and Devices'
arxiv_id: '2508.09035'
source_url: https://arxiv.org/abs/2508.09035
tags:
- cloud
- tokens
- arxiv
- prefill
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of large language model (LLM)
  inference caused by long decoding phases on the cloud and slow prefill phases on
  devices. The core method, P/D-Device, involves the cloud assisting each device during
  its prefill phase by providing refined prompts and a controlled number of decoding
  tokens.
---

# P/D-Device: Disaggregated Large Language Model between Cloud and Devices

## Quick Facts
- arXiv ID: 2508.09035
- Source URL: https://arxiv.org/abs/2508.09035
- Reference count: 40
- Primary result: Reduces user-perceived time to first token by at least 60% and cloud throughput increases by up to 15x

## Executive Summary
P/D-Device addresses the inefficiency of large language model (LLM) inference by disaggregating the workload between cloud and devices. The system leverages cloud assistance during device prefill phases by providing refined prompts and controlled decoding tokens, enabling devices to respond quickly while amortizing long on-device prefill operations. This approach significantly reduces user-perceived latency and improves cloud resource utilization through intelligent workload distribution.

## Method Summary
The core innovation of P/D-Device involves a three-component system: cloud assistance, on-device control, and an optimization algorithm. During device prefill, the cloud provides refined prompts derived from intermediate data and supplies a controlled number of decoding tokens to jumpstart the generation process. This disaggregated approach allows devices to bypass lengthy initial computations while maintaining local response capabilities. The optimization algorithm determines optimal settings for prompt refinement and token assistance based on device capabilities and network conditions.

## Key Results
- User-perceived time to first token (TTFT) decreases by at least 60%
- Maximum time per output token (TPOT) reduced to tens of milliseconds
- Cloud throughput increases by up to 15x compared to cloud-only inference
- Real-trace experiments validate the effectiveness of the disaggregated approach

## Why This Works (Mechanism)
The mechanism works by strategically partitioning LLM inference tasks between cloud and edge devices based on their respective strengths. Cloud handles complex prefill operations and provides refined prompts that are computationally expensive for devices to generate independently. Devices then leverage these optimized inputs to quickly produce initial tokens while completing their prefill phase in parallel. This overlapping execution model amortizes the latency of both phases and creates a more responsive user experience.

## Foundational Learning
- Prompt refinement: Essential for reducing device prefill complexity; quick check: verify refinement reduces token count by 30-50%
- Token assistance: Critical for maintaining user engagement during prefill; quick check: measure TTFT improvement with varying assistance levels
- Disaggregated optimization: Required for balancing cloud and device workloads; quick check: validate optimization algorithm reduces total latency by 40-60%
- Cloud-device coordination: Fundamental to seamless operation; quick check: ensure communication overhead stays below 5ms
- TPOT smoothing: Important for consistent user experience; quick check: verify TPOT variance decreases by 70-80%

## Architecture Onboarding

**Component Map:**
Cloud Assistant -> Device Controller -> Optimization Engine -> Device Prefill/Decode -> User Interface

**Critical Path:**
User query → Cloud prefill refinement → Device prefill with assistance → Token generation → User response

**Design Tradeoffs:**
- Latency vs. bandwidth: More assistance tokens reduce latency but increase communication overhead
- Cloud load vs. device capability: Balance between cloud processing and device computational requirements
- Prompt quality vs. refinement time: Tradeoff between comprehensive refinement and real-time responsiveness

**Failure Signatures:**
- High TTFT despite assistance: Indicates suboptimal prompt refinement or insufficient token assistance
- Fluctuating TPOT: Suggests poor load balancing between cloud and device phases
- Device overheating: Implies excessive on-device computation beyond capabilities
- Network bottlenecks: Manifests as increased communication overhead and reduced effectiveness

**Three First Experiments:**
1. Measure TTFT improvement across varying network latencies (100-500ms RTT)
2. Compare TPOT consistency between disaggregated and cloud-only approaches
3. Evaluate cloud throughput scaling with increasing concurrent device connections

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited testing across diverse device types and computational capabilities
- Potential communication overhead not fully analyzed in all deployment scenarios
- Effectiveness of prompt refinement across different query domains remains unclear
- Optimization algorithm sensitivity to parameter choices could affect real-world performance

## Confidence

| Claim | Confidence |
|-------|------------|
| TTFT improvement (60%) | High |
| TPOT reduction to tens of milliseconds | Medium |
| Cloud throughput improvement (15x) | Medium |
| Generalizability across device types | Low |
| Prompt refinement effectiveness | Medium |

## Next Checks

1. Test the system under varying network conditions (100ms to 500ms RTT) to assess robustness and identify performance degradation points
2. Evaluate prompt refinement effectiveness across 10+ different domains (code, creative writing, technical Q&A) with varying query complexity
3. Measure end-to-end energy consumption on mobile devices to validate the claimed efficiency improvements beyond just latency metrics