---
ver: rpa2
title: Near-Lossless Model Compression Enables Longer Context Inference in DNA Large
  Language Models
arxiv_id: '2511.14694'
source_url: https://arxiv.org/abs/2511.14694
tags:
- tokens
- context
- compression
- window
- windows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DNA large language models (LLMs) struggle with ultra-long genomic
  sequences due to quadratic attention costs and KV-cache memory growth. This limits
  practical use of their long-range capabilities.
---

# Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models

## Quick Facts
- arXiv ID: 2511.14694
- Source URL: https://arxiv.org/abs/2511.14694
- Reference count: 35
- Enables ~100× longer context inference on DNA LLMs by compressing KV-cache from O(N) to O(N/k) while maintaining near-lossless fidelity

## Executive Summary
DNA large language models face quadratic attention costs and KV-cache memory growth that limit practical use of long-range genomic context. This paper introduces FOCUS, a plug-in compression module that inserts learnable summary tokens at k-mer granularity and progressively compresses attention activations across layers. By retaining only summary KV states across windows, FOCUS converts KV-cache scaling from O(N) to O(N/k), enabling near-linear inference. Evaluated on an Evo-2-based DNA LLM fine-tuned on human chromosome 1, FOCUS achieves ~100× memory reduction and ~100× longer inference windows on commodity GPUs while maintaining near-lossless fidelity.

## Method Summary
The paper addresses the quadratic attention bottleneck in DNA LLMs by introducing FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), which inserts learnable summary tokens every k=100 bases and compresses attention activations across layers. The method uses a window size W=1024 FOCUS tokens with shared-boundary scheme, training only the FOCUS adapter (embedding + attention projections) while freezing the Evo-2 7B backbone. Training runs for 1 epoch on GRCh38 Chromosome 1 with specific hyperparameters, and inference discards ordinary-token KV after each window, retaining only FOCUS KV states. The approach converts KV-cache scaling from O(N) to O(N/k), enabling ~100× longer inference windows on commodity GPUs.

## Key Results
- Compresses 1 kb into ~10 summary tokens (~100× compression) with only ~0.0004 per-nucleotide probability shift
- Reduces KV-cache memory by ~100× while maintaining near-lossless fidelity
- Enables ~100× longer inference windows on commodity GPUs
- Maintains cross-entropy language modeling performance across held-out chromosomes and out-of-distribution sequences

## Why This Works (Mechanism)
FOCUS works by inserting learnable summary tokens at regular intervals (k=100 bases) throughout the sequence, then progressively compressing attention activations across layers so that only these summary tokens retain KV states between windows. This transforms the KV-cache scaling from O(N) to O(N/k), enabling near-linear inference complexity. The summary tokens are trained to predict the next real base via cross-entropy while maintaining the ability to reconstruct the original sequence distribution. By using a shared-boundary windowing scheme where the last summary token of one window becomes the first of the next, FOCUS ensures continuity while minimizing memory overhead.

## Foundational Learning
- **DNA sequence tokenization**: Why needed: DNA LLMs require specialized tokenization schemes for nucleotide sequences. Quick check: Verify the tokenizer handles the 4 DNA bases (A, C, G, T) and any special tokens.
- **KV-cache mechanism**: Why needed: Understanding how transformers cache key-value pairs for efficient attention is crucial for grasping the memory bottleneck. Quick check: Profile KV-cache size growth with sequence length in baseline model.
- **Attention mask construction**: Why needed: FOCUS must enforce visibility constraints across windows while maintaining proper attention flow. Quick check: Verify attention masks allow each summary token to see preceding summary tokens within the window.
- **Parameter-efficient fine-tuning**: Why needed: FOCUS freezes the backbone and only trains adapter parameters, requiring understanding of PEFT techniques. Quick check: Confirm only FOCUS adapter parameters are updated during training.

## Architecture Onboarding

**Component map**: Input sequence -> Tokenizer -> FOCUS token insertion -> Window partitioning -> Frozen Evo-2 backbone -> FOCUS adapter (embedding + attention projections) -> Output distribution -> Fidelity evaluation

**Critical path**: The critical path involves the FOCUS adapter modules that must correctly attend to preceding summary tokens while maintaining the ability to predict next-token probabilities. The shared-boundary scheme and attention mask construction are critical for ensuring continuity across windows.

**Design tradeoffs**: Fixed vs. adaptive window size (W) and k-mer granularity (k); single-resolution vs. multi-resolution summary hierarchies; global vs. context-sensitive compression; memory efficiency vs. fidelity preservation.

**Failure signatures**: Memory does not flatten with increasing context length (indicates ordinary-token KV not being discarded); fidelity degrades sharply beyond a few windows (indicates shared-boundary token reuse issues); training divergence or NaN loss (indicates probability clipping or embedding initialization problems).

**First experiments**: 1) Profile KV-cache size at window boundaries to confirm only summary tokens' KV are retained. 2) Verify shared-boundary token reuse by running two-window inference and checking attention patterns. 3) Test probability clipping and re-normalization by inducing extreme logits and confirming stability.

## Open Questions the Paper Calls Out
- Can adaptive policies for window size (W) and k-mer granularity (k) that respond to local genomic context improve fidelity compared to fixed global hyperparameters?
- How does FOCUS compression affect downstream genomic task performance (e.g., variant effect prediction, structural variant calling, regulatory element identification) beyond per-nucleotide probability fidelity?
- Can multi-resolution summary hierarchies combined with retrieval-augmented access to cached summaries further reduce computational cost while maintaining fidelity?
- How does cross-window error accumulation scale for sequences beyond 10 kb, and at what context length does fidelity degradation become biologically significant?

## Limitations
- Exact architectural details of FOCUS adapter insertion points and hidden dimensions remain unspecified
- Dataset preprocessing pipeline underspecified (chunk size, overlap strategy, JSONL schema)
- Licensing or access constraints to Evo-2 7B BioNeMo checkpoint could impede reproduction
- Fidelity measurements only extend to 10 kb, leaving uncertainty about performance at 50-100+ kb contexts

## Confidence
- ~100× memory reduction: High confidence (clear memory scaling plots, well-described mechanism)
- Near-lossless fidelity: Medium confidence (absence of variance reporting, lack of hyperparameter ablation)
- 5× increase in maximum context length on single A100 GPU: Low confidence (tied to unstated batch-size choices and baseline settings)

## Next Checks
1. Profile KV-cache size and content at window boundaries to confirm that only FOCUS tokens' KV are retained and ordinary-token KV are purged
2. Replicate fidelity metrics on a small test set (e.g., 1024 bp) comparing FOCUS-compressed vs. baseline Evo-2 output distributions, reporting mean and standard deviation
3. Test attention mask logic by running a two-window inference and verifying that the shared-boundary FOCUS token attends to all necessary preceding FOCUS tokens in both windows