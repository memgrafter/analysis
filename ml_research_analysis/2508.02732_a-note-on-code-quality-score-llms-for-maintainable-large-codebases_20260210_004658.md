---
ver: rpa2
title: 'A Note on Code Quality Score: LLMs for Maintainable Large Codebases'
arxiv_id: '2508.02732'
source_url: https://arxiv.org/abs/2508.02732
tags:
- code
- issue
- review
- issues
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Code Quality Score (CQS) system, an LLM-powered
  system designed to automatically evaluate code quality and generate code reviews
  at industrial scale. The CQS system is powered by two fine-tuned Llama3 models,
  trained using supervised fine-tuning (SFT) and Direct Preference Optimization (DPO),
  to detect common code quality issues and provide critiques for LLM-generated code
  reviews.
---

# A Note on Code Quality Score: LLMs for Maintainable Large Codebases

## Quick Facts
- arXiv ID: 2508.02732
- Source URL: https://arxiv.org/abs/2508.02732
- Reference count: 25
- Code Quality Score system achieves 95.4% precision for identifying valid code quality issues in LLM-generated code reviews

## Executive Summary
This paper introduces the Code Quality Score (CQS) system, an LLM-powered solution designed to automatically evaluate code quality and generate code reviews at industrial scale. The system leverages fine-tuned Llama3 models trained through supervised fine-tuning and Direct Preference Optimization to detect common code quality issues and provide critiques for LLM-generated code reviews. The CQS system has been deployed to over 5000 engineers at Meta and demonstrates promising performance with a week-over-week user helpfulness rate of approximately 60%.

## Method Summary
The CQS system employs two fine-tuned Llama3 models trained using supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) techniques. These models are specifically trained to detect common code quality issues and generate critiques for LLM-generated code reviews. To ensure system reliability, hand-crafted rules are implemented to filter out incorrect responses and hallucinations. The system underwent offline evaluation and has been successfully rolled out to Meta's engineering organization, where it continues to operate at scale.

## Key Results
- CQS system achieves 95.4% precision for detecting valid code quality issues
- Successfully deployed to over 5000 engineers at Meta
- Maintains a week-over-week user helpfulness rate of approximately 60%

## Why This Works (Mechanism)
The CQS system leverages fine-tuned LLMs to automate code quality assessment at scale. By training models specifically on code review patterns and quality issues, the system can identify problematic code patterns that traditional static analysis might miss. The combination of SFT and DPO training methods allows the models to learn from both explicit examples and preference data, improving their ability to generate relevant and accurate code critiques.

## Foundational Learning
- Supervised Fine-Tuning (SFT): Required for adapting pre-trained LLMs to code review tasks; quick check: verify model performance on held-out code review examples
- Direct Preference Optimization (DPO): Needed to align model outputs with human preferences for code quality feedback; quick check: compare preference-ranked outputs against human-selected best responses
- Hallucination Filtering: Essential for maintaining system reliability; quick check: measure false positive rate in filtered responses
- Code Quality Issue Detection: Critical for identifying problematic patterns; quick check: validate detection accuracy across multiple programming languages
- Large-Scale Deployment: Necessary for industrial adoption; quick check: monitor system performance under concurrent user load
- User Helpfulness Metrics: Important for measuring real-world impact; quick check: track weekly user engagement and feedback patterns

## Architecture Onboarding
Component map: User Code -> CQS Models -> Quality Assessment -> Feedback Filter -> Final Review
Critical path: Code submission → LLM evaluation → Quality scoring → Review generation → Feedback delivery
Design tradeoffs: Model complexity vs. response time, precision vs. recall, rule-based filtering vs. model-based filtering
Failure signatures: Incorrect issue detection, hallucination generation, false positive/negative classifications, system latency issues
First experiments:
1. Evaluate precision and recall across different programming language subsets
2. Test system performance under varying load conditions with concurrent users
3. Measure hallucination rate with and without filtering rules enabled

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation methodology lacks transparency regarding reviewer qualifications and inter-rater reliability
- Heavy reliance on hand-crafted rules raises concerns about scalability and adaptability to new code patterns
- Limited data on recall rates, making it unclear what proportion of actual issues might be missed
- User helpfulness rate of 60% suggests significant room for improvement without baseline comparisons

## Confidence
High confidence in technical feasibility of LLM-based code quality assessment
Medium confidence in reported precision metrics due to limited evaluation transparency
Low confidence in generalizability to organizations outside Meta

## Next Checks
1. Conduct comprehensive blind evaluation with independent code reviewers from multiple organizations to assess both precision and recall across diverse codebases
2. Perform longitudinal study tracking CQS performance across different programming languages and codebases over time
3. Implement A/B testing comparing CQS-generated reviews against human-only reviews and traditional static analysis tools