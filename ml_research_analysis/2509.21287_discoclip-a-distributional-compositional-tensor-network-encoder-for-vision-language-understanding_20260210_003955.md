---
ver: rpa2
title: 'DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language
  Understanding'
arxiv_id: '2509.21287'
source_url: https://arxiv.org/abs/2509.21287
tags:
- tensor
- tensors
- structure
- word
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DisCoCLIP introduces a tensor network-based text encoder to improve
  compositional reasoning in vision-language models. By parsing sentences with Combinatory
  Categorial Grammar and encoding them as tensor networks, the model captures explicit
  syntactic structure while maintaining efficiency through tensor decompositions.
---

# DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding

## Quick Facts
- arXiv ID: 2509.21287
- Source URL: https://arxiv.org/abs/2509.21287
- Authors: Kin Ian Lo; Hala Hawashin; Mina Abbaszadeh; Tilen Limback-Stokin; Hadi Wazni; Mehrnoosh Sadrzadeh
- Reference count: 15
- Key outcome: State-of-the-art accuracy on SVO-Swap (93.68%), outperforms CLIP on verb understanding (+4.8%), and improves attribution and relation scores on ARO by 9.0% and 4.3%, respectively, using over two orders of magnitude fewer parameters than transformer-based baselines.

## Executive Summary
DisCoCLIP introduces a tensor network-based text encoder to improve compositional reasoning in vision-language models. By parsing sentences with Combinatory Categorial Grammar and encoding them as tensor networks, the model captures explicit syntactic structure while maintaining efficiency through tensor decompositions. Evaluated on SVO-Probes, ARO, and a new SVO-Swap benchmark, DisCoCLIP achieves strong performance across all tasks while using significantly fewer parameters than transformer baselines.

## Method Summary
DisCoCLIP replaces the transformer text encoder in contrastive vision-language models with a tensor network architecture. Sentences are parsed using Combinatory Categorial Grammar (CCG) to derive syntactic trees, which determine the tensor network topology. Words are assigned tensors based on their grammatical types (nouns as vectors, verbs as higher-order tensors), and high-order tensors are factorized using Matrix Product States to reduce parameters. The model uses a frozen CLIP ViT image encoder and trains only the tensor network text encoder using InfoNCE contrastive loss with cosine similarity.

## Key Results
- Achieves 93.68% accuracy on the new SVO-Swap benchmark, outperforming all baselines
- Improves verb understanding by 4.8% compared to CLIP on SVO-Probes
- Increases ARO-Attribution and ARO-Relation scores by 9.0% and 4.3% respectively
- Uses over two orders of magnitude fewer parameters than transformer-based baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping sentence syntax directly to tensor network topology preserves compositional structure that standard attention mechanisms may average away.
- **Mechanism:** The model parses text using Combinatory Categorial Grammar (CCG) to derive a syntactic tree. This tree dictates the layout of the tensor networkâ€”words are nodes, and grammatical rules are contractions. This forces "subject" and "object" tensors to interact with "verb" tensors in specific, non-commutative ways, preventing the "bag-of-words" collapse observed in standard CLIP models.
- **Core assumption:** The CCG parse accurately captures the semantic relationships required for vision-language alignment, and the "Compact" structure effectively mirrors this logic.
- **Evidence anchors:**
  - [abstract]: "...sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence's grammatical derivation."
  - [section 4]: "The Compact structure is a variant of the Tree structure, where every non-terminal node in the parse tree is absorbed by one of its parents... [resulting in] a more compact representation."
  - [corpus]: Weak direct validation for this specific architectural instantiation; however, related work in "Quantum Methods for Managing Ambiguity" supports the theoretical basis of DisCoCat diagrams for semantic structure.
- **Break condition:** If the CCG parser fails on complex or ambiguous sentences (e.g., garden path sentences), the resulting tensor network topology will be structurally incorrect, likely yielding a meaningless sentence embedding.

### Mechanism 2
- **Claim:** Tensor decomposition (factorization) enables the modeling of high-order grammatical interactions without parameter explosion.
- **Mechanism:** Words with complex grammatical roles (e.g., transitive verbs) require high-order tensors (cubes) to represent their interaction with multiple arguments. DisCoCLIP uses Matrix Product States (MPS) / Tensor Trains to decompose these large tensors into chains of smaller, lower-order tensors connected by "bond" indices. This reduces the parameter space from exponential to linear/quadratic relative to bond dimension.
- **Core assumption:** The linguistic interactions possess a low-rank structure that can be sufficiently approximated by the tensor decomposition without significant loss of semantic nuance.
- **Evidence anchors:**
  - [abstract]: "...high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million."
  - [section 3