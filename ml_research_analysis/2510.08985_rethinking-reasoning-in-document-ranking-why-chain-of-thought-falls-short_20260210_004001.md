---
ver: rpa2
title: 'Rethinking Reasoning in Document Ranking: Why Chain-of-Thought Falls Short'
arxiv_id: '2510.08985'
source_url: https://arxiv.org/abs/2510.08985
tags:
- reasoning
- listwise
- rerankers
- grpo
- pointwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies the impact of chain-of-thought
  reasoning in document reranking, comparing reasoning-augmented models against direct-output
  counterparts across pointwise and listwise settings, using both supervised fine-tuning
  and reinforcement learning. The study covers reasoning-intensive and standard IR
  benchmarks, finding that reasoning-augmented rerankers consistently underperform
  direct models despite higher inference costs.
---

# Rethinking Reasoning in Document Ranking: Why Chain-of-Thought Falls Short

## Quick Facts
- **arXiv ID**: 2510.08985
- **Source URL**: https://arxiv.org/abs/2510.08985
- **Reference count**: 24
- **Primary result**: Reasoning-augmented rerankers consistently underperform direct-output models across both pointwise and listwise settings, even with reinforcement learning that shortens rationales

## Executive Summary
This paper systematically challenges the assumption that chain-of-thought reasoning improves document reranking performance. Through controlled experiments comparing reasoning-augmented models against direct-output counterparts across multiple benchmarks, the study finds that reasoning consistently degrades ranking quality despite higher inference costs. The failure modes differ by paradigm: pointwise rerankers suffer from calibration disruption and false-positive bias, while listwise rerankers experience variance-induced overfitting that harms out-of-domain generalization. The results suggest that explicit reasoning is not universally beneficial for reranking and that simpler direct-output models remain more effective and robust.

## Method Summary
The study compares reasoning-augmented rerankers (using DeepSeek-Coder-R1-DS) against direct-output models across pointwise and listwise paradigms. For pointwise, models predict binary relevance (TRUE/FALSE) for individual query-document pairs. For listwise, models predict document permutations. Both paradigms use supervised fine-tuning (SFT) followed by reinforcement learning with Group Relative Policy Optimization (GRPO). The study evaluates on standard IR benchmarks including MS MARCO, TREC-DL, and BEIR, using calibration metrics (ECE), ranking metrics (NDCG@10), and performance breakdowns by positive/negative class. Reasoning models generate rationales before making predictions, while direct models output predictions immediately.

## Key Results
- Reasoning-augmented pointwise rerankers show significantly higher Expected Calibration Error (ECE) and systematic overconfidence compared to direct models
- Pointwise reasoning models exhibit positive-class bias, degrading performance in negative-dominant pools typical of IR
- Listwise reasoning improves in-domain training performance but increases variance and impairs out-of-domain generalization
- GRPO reduces rationale length but does not resolve the generalization issues in listwise reasoning
- Across all experiments, direct rerankers remain more stable, effective, and robust than reasoning-augmented alternatives

## Why This Works (Mechanism)

### Mechanism 1: Calibration Disruption in Pointwise Reasoning
- Claim: Explicit CoT reasoning degrades score calibration in pointwise rerankers, causing overconfident predictions that decouple predicted probability from empirical accuracy.
- Mechanism: The reasoning trace appears to polarize logits toward extreme values (0 or 1) rather than maintaining graded confidence, breaking the monotonic relationship between confidence and accuracy that ranking requires.
- Core assumption: Accurate ranking depends on well-calibrated relevance scores, not just binary classification accuracy.
- Evidence anchors:
  - [abstract]: "reasoning breaks calibration and biases models toward the positive class"
  - [section 4.1]: Direct-Point-8B achieves ECE = 0.106 vs Reason-Point-8B at ECE = 0.141; reasoning exhibits "systematic overconfidence with larger departures from the diagonal"
  - [corpus]: Weak direct evidence; GroupRank and ERank papers reinforce that ranking fidelity requires calibrated scoring but don't address reasoning effects.
- Break condition: If your downstream task only needs binary decisions (not ranked scores), or if you can apply post-hoc calibration (temperature scaling, Platt scaling), this mechanism may not apply.

### Mechanism 2: Positive-Class Bias from Reasoning Traces
- Claim: CoT reasoning shifts pointwise rerankers toward predicting relevance, raising true positive rate (TPR) at the cost of true negative rate (TNR), which is systematically harmful in negative-dominant candidate pools typical of IR.
- Mechanism: Reasoning traces generated by models like DeepSeek-R1 may contain confirmation-seeking language that biases the final decision toward TRUE; the paper notes training data has 1:2 positive:negative ratio yet reasoning models still over-predict positives.
- Core assumption: IR candidate pools are typically negative-dominant (few relevant documents per query), so TNR matters more than TPR for ranking quality.
- Evidence anchors:
  - [abstract]: "biases pointwise rerankers toward false positives, degrading ranking quality in negative-dominant pools"
  - [section 4.1, Table 3]: Reason-Point-8B achieves TPR=50.5/TNR=98.1 vs Direct-Point-8B at TPR=31.1/TNR=100.0; reasoning models "consistently arise from higher TPR coupled with lower TNR"
  - [corpus]: No direct corpus evidence on this specific bias mechanism.
- Break condition: If your candidate pools are balanced or positive-dominant, or if false positives are inexpensive (users tolerate noise), this bias may not harm your application.

### Mechanism 3: Variance-Induced Overfitting in Listwise Reasoning
- Claim: In listwise reranking, reasoning improves mean training performance but increases per-instance variance and impairs out-of-domain generalization, even when GRPO shortens rationales.
- Mechanism: CoT provides additional fitting capacity that helps match training permutations but introduces instability across similar inputs; GRPO reduces rationale length (397.7→172.3 tokens) but the residual variance still harms generalization.
- Core assumption: Training-split performance gains should transfer to held-out data; when they don't, this signals overfitting rather than genuine learning.
- Evidence anchors:
  - [abstract]: "reasoning improves in-domain fit but increases variance and fails to generalize out-of-domain, even when reinforcement learning shortens rationales"
  - [section 4.2, Fig 3]: Reason-List SFT achieves 82.57±3.2 vs Direct-List SFT at 80.41±2.1 on training split; but on MS MARCO DL19, Direct-List-4B outperforms Reason-List-4B (73.77 vs 70.76)
  - [corpus]: ReasonRank paper claims reasoning helps listwise ranking but lacks the controlled direct-vs-reason comparison this paper provides.
- Break condition: If your deployment domain closely matches training distribution, or if you can ensemble multiple reasoning samples to reduce variance, this mechanism may be partially mitigated.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: Quantifies whether a model's confidence scores match its actual accuracy; critical for understanding