---
ver: rpa2
title: Are We There Yet? A Measurement Study of Efficiency for LLM Applications on
  Mobile Devices
arxiv_id: '2504.00002'
source_url: https://arxiv.org/abs/2504.00002
tags:
- mobile
- memory
- edge
- devices
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper measures the efficiency of deploying LLM applications
  on mobile devices. The authors implemented AutoLife-Lite, a mobile application that
  uses sensor data to infer user location and activity contexts.
---

# Are We There Yet? A Measurement Study of Efficiency for LLM Applications on Mobile Devices

## Quick Facts
- arXiv ID: 2504.00002
- Source URL: https://arxiv.org/abs/2504.00002
- Reference count: 40
- Key outcome: This paper measures the efficiency of deploying LLM applications on mobile devices using AutoLife-Lite, finding only small LLMs (<4B parameters) can run on mobile, with latency exceeding 30 seconds, while cloud services are faster (<10 seconds).

## Executive Summary
This paper presents a comprehensive measurement study of Large Language Model (LLM) efficiency across mobile, edge, and cloud deployment settings. The authors developed AutoLife-Lite, a mobile application that uses sensor data to infer user location and activity contexts. Through systematic benchmarking across three deployment tiers, the study reveals fundamental constraints in on-device LLM deployment, particularly memory limitations on mobile devices and the significant latency overhead compared to cloud-based solutions. The findings provide crucial insights for developers considering where to deploy LLM applications based on their specific requirements for latency, privacy, and hardware constraints.

## Method Summary
The study implements AutoLife-Lite to benchmark LLM efficiency across three deployment settings: mobile-based (Google Pixel 8 with 8GB RAM), edge-based (CPU and GPU servers), and cloud-based (commercial APIs). The authors measure latency, memory consumption, and output quality across multiple model sizes (0.5B-8B parameters) including Gemma, Llama3.2, Llama3.1, DeepSeek-R1, and Qwen models. Mobile deployment uses MediaPipe and Executorch frameworks, edge deployment uses Ollama on AWS P3 instances, and cloud deployment uses OpenAI, Anthropic Claude, and Google Gemini APIs. The study focuses on inference-only measurement without training, using structured JSON prompts for sensor data processing tasks.

## Key Results
- Only small-size LLMs (<4B parameters) can successfully run on powerful mobile devices due to RAM limitations
- Mobile-based deployments exceed 30 seconds latency for meaningful output, while cloud services achieve <10 seconds
- GPU-based edge deployments outperform CPU-based ones in both latency and consistency
- Model compression techniques reduce hardware requirements but often lead to significant performance degradation or complete inference failure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model size directly constrains deployability on mobile devices due to RAM limitations.
- Mechanism: Physical memory is partitioned between OS (35-40%), model weights, and runtime overhead. When model parameters exceed available RAM, inference fails or requires aggressive compression that degrades quality.
- Core assumption: Memory allocation patterns are relatively stable and predictable across similar device classes.
- Evidence anchors:
  - [abstract] "Only small-size LLMs (<4B parameters) can run successfully on powerful mobile devices"
  - [section] "a Llama-3B model would consume 57% memory, with only around 8% free memory" (Figure 6)
  - [corpus] Related work confirms memory constraints as primary bottleneck for on-device inference
- Break condition: Devices with unified memory architectures (e.g., Apple Silicon) or higher RAM (16GB+) may shift the threshold.

### Mechanism 2
- Claim: Model compression techniques reduce memory footprint but introduce quality degradation that may render models unusable for specific tasks.
- Mechanism: Quantization and LoRA adaptation reduce precision of weights, but the accumulated approximation errors compound during autoregressive generation, leading to incomplete or incoherent outputs.
- Core assumption: The quality degradation is task-dependent; simpler tasks may tolerate compression better than complex reasoning.
- Evidence anchors:
  - [abstract] "Model compression is effective in lower the hardware requirement, but may lead to significant performance degradation"
  - [section] "Llama3.2-1B and Llama3.2-3B... the non-compressed version works on the edge server, but the compressed version fails on mobile"
  - [corpus] Corpus papers (Sustainable LLM Inference) examine quantization tradeoffs, confirming accuracy-efficiency tension
- Break condition: Task-specific fine-tuning after compression may recover some quality.

### Mechanism 3
- Claim: GPU acceleration provides lower and more consistent inference latency than CPU-only deployments.
- Mechanism: GPUs provide parallel matrix multiplication units and high-bandwidth memory optimized for the sequential-but-parallel nature of transformer inference, reducing both mean latency and variance from memory bottlenecks.
- Core assumption: Network transmission time is negligible compared to inference time in edge deployments.
- Evidence anchors:
  - [abstract] "Edge deployments offer intermediate tradeoffs... with different results on CPU-based and GPU-based settings"
  - [section] Figure 5 shows "CPU-based edge server has a much higher average latency and larger variance compared with the GPU-based edge server"
  - [corpus] Related work on DVFS governors and NPU utilization supports hardware-specific optimization importance
- Break condition: Very small models may not saturate GPU, reducing advantage over CPU.

## Foundational Learning

- Concept: **Resident Set Size (RSS)**
  - Why needed here: The paper uses RSS to measure actual physical memory footprint, which differs from virtual memory allocation.
  - Quick check question: Why is RSS more accurate than virtual memory size for measuring mobile deployment constraints?

- Concept: **Model Quantization (INT8/INT4)**
  - Why needed here: Understanding why compressed models fail requires knowing how reduced precision affects numerical stability in transformers.
  - Quick check question: What happens to softmax distributions when weight precision is reduced from FP16 to INT4?

- Concept: **Autoregressive Generation**
  - Why needed here: Latency scales with output token count; understanding sequential token generation explains why speed (tokens/sec) matters for real-time applications.
  - Quick check question: Why does autoregressive generation make inference latency more sensitive to model size than batch processing?

## Architecture Onboarding

- Component map:
  Mobile tier: Android sensor listeners → Rule-based preprocessing → Local LLM (MediaPipe/Executorch) → JSON output
  Edge tier: Wi-Fi/Cellular → LAN → Ollama server (CPU or GPU) → Response
  Cloud tier: Wi-Fi → WAN → Commercial API (OpenAI/Claude/Gemini) → Response

- Critical path: Sensor data collection → Prompt construction → LLM inference → JSON parsing. The LLM inference step dominates latency (>90% of total time).

- Design tradeoffs:
  Mobile: Privacy + offline capability vs. model size limit + high latency (>30s)
  Edge (CPU): Moderate model size + moderate latency vs. hardware cost
  Edge (GPU): Larger models + lower latency + consistent performance vs. higher cost
  Cloud: Best models + lowest latency (<10s) vs. network dependency + privacy concerns

- Failure signatures:
  - "Nothing generated": Model loaded but inference failed (observed with Gemma-2B on mobile)
  - "Copy-paste input": Model collapsed into repeating prompt (observed with Qwen models)
  - "Incomplete output": Model stopped mid-generation (observed with compressed Llama3.2 on mobile)

- First 3 experiments:
  1. Baseline memory profiling: Deploy Gemma2-2B on target device, measure RSS at idle, during load, and peak inference. Verify <60% total RAM usage.
  2. Latency variance test: Run 50 inference calls on both CPU-edge and GPU-edge with identical model (Llama3.2-3B), calculate coefficient of variation for tokens/sec.
  3. Compression quality check: Compare original vs. quantized Llama3.2-1B outputs on structured JSON extraction task; count parse failures and hallucination rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Vision Language Models (VLMs) be efficiently deployed on mobile devices for sensor-heavy applications like AutoLife?
- Basis in paper: [explicit] The authors note they "only test the performance of LLMs but not VLMs" due to current framework constraints, despite VLMs being relevant to the full application.
- Why unresolved: Current mobile deployment frameworks (e.g., MediaPipe, Executorch) lack robust support for on-device VLMs, leaving the efficiency of visual sensor processing unmeasured.
- What evidence would resolve it: A measurement study of VLM latency and memory usage on the Pixel 8 running the full AutoLife task (including visual modules).

### Open Question 2
- Question: Do the observed efficiency bottlenecks (latency >30s) persist across heterogeneous mobile hardware architectures?
- Basis in paper: [explicit] The authors limit their scope to a single device (Google Pixel 8) and acknowledge this "may not be typical in other scenarios."
- Why unresolved: Hardware accelerators (NPUs/TPUs) vary significantly between manufacturers (e.g., Apple, Qualcomm, Samsung), potentially altering the identified latency/memory trade-offs.
- What evidence would resolve it: Replicating the AutoLife-Lite benchmark on diverse chipsets (e.g., Snapdragon 8 Gen 3, Apple A17 Pro) to compare against the Tensor G3 results.

### Open Question 3
- Question: What is the quantitative relationship between model compression rates and inference accuracy for on-device mobile sensing tasks?
- Basis in paper: [inferred] The paper measures efficiency but explicitly excludes accuracy measurement, noting that compressed models often "fail to generate meaningful answers" or hallucinate despite reduced hardware requirements.
- Why unresolved: The study focused on deployability (latency/memory) and performed only manual sanity checks rather than systematic accuracy benchmarking against ground truth.
- What evidence would resolve it: Precision/Recall/F1 metrics for compressed (quantized) versus uncompressed models across specific sensor inference tasks.

## Limitations

- Single device testing limits generalizability across heterogeneous mobile hardware architectures and memory configurations
- Qualitative output quality assessment lacks systematic quantitative metrics for accuracy and hallucination detection
- Network effects not isolated in cloud latency measurements, potentially conflating inference time with transmission overhead

## Confidence

**High Confidence Claims**:
- Model size directly constrains mobile deployability due to RAM limitations (supported by direct memory measurements and device specifications)
- GPU-based edge deployments outperform CPU-based ones in latency and consistency (validated by multiple runs showing reduced variance)
- Model compression reduces hardware requirements but degrades output quality (confirmed by direct comparison of compressed vs uncompressed models)

**Medium Confidence Claims**:
- Current mobile deployments cannot achieve real-time performance for LLM applications (>30s latency) (based on empirical measurements but task-specific)
- Edge deployments offer intermediate tradeoffs between mobile and cloud (supported by comparative data but narrow device/instance sample)
- Quantization artifacts cause model failures rather than just quality degradation (observed but not systematically analyzed)

**Low Confidence Claims**:
- 8-16GB RAM devices would significantly expand deployable model sizes (extrapolation without testing)
- Task-specific fine-tuning after compression would recover quality (speculation without empirical validation)
- Network transmission time is negligible compared to inference time in edge deployments (assumption not verified)

## Next Checks

1. **Memory-Aware Model Selection**: Test a broader range of mobile devices (including 12GB and 16GB RAM Android devices and Apple Silicon devices) with the same model set to verify the claimed 4B parameter threshold and identify the actual memory headroom available for larger models.

2. **Quantitative Quality Assessment**: Replace qualitative "meaningful output" judgments with structured evaluation using task-specific metrics (e.g., JSON parse success rate, hallucination detection via fact verification, or automated semantic similarity scoring) to enable objective comparison between compressed and uncompressed models.

3. **Network-Aware Cloud Latency Measurement**: Deploy the same cloud API tests over different network conditions (Wi-Fi, 4G, 5G) while separately measuring inference time (server-side timestamps) and transmission time to isolate network effects from computational latency in the reported <10 second figures.