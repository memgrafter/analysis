---
ver: rpa2
title: LLMs can hide text in other text of the same length
arxiv_id: '2510.20075'
source_url: https://arxiv.org/abs/2510.20075
tags:
- text
- figure
- llms
- secret
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Calgacus is a protocol that uses Large Language Models to hide
  a meaningful text within another coherent and plausible text of the same length.
  The method works by encoding the original message as a sequence of token ranks from
  a chosen LLM, then generating the cover text by selecting tokens at those ranks
  from a secret prompt.
---

# LLMs can hide text in other text of the same length

## Quick Facts
- arXiv ID: 2510.20075
- Source URL: https://arxiv.org/abs/2510.20075
- Authors: Antonio Norelli; Michael Bronstein
- Reference count: 40
- Primary result: Calgacus protocol hides secret text within cover text of identical length using LLM token ranks, enabling perfect recovery with secret prompt

## Executive Summary
Calgacus is a steganographic protocol that encodes a secret message within a coherent cover text of identical token length using Large Language Models. The method works by converting the secret message into a sequence of token ranks from an LLM's probability distribution, then generating the cover text by selecting tokens at those ranks from a secret prompt. The original message can be perfectly recovered by anyone possessing the secret prompt, and the process works efficiently on consumer hardware using small open-source LLMs. The protocol demonstrates that LLM-generated text can be manipulated to hide information while maintaining plausibility, raising significant questions about text authenticity and AI safety.

## Method Summary
The Calgacus protocol encodes a secret text e into a stegotext s of identical token length using an LLM's probability distribution. The process involves: (1) tokenizing the secret message e, (2) computing the rank of each token under the LLM given its preceding context, (3) generating the stegotext s by selecting tokens at those ranks from a secret prompt k, and (4) decoding by reversing the process using s and k. The method requires no training—only inference—and works with small 8B parameter models. Tests use 1000 Reddit posts truncated to 85 tokens, with secret prompts sourced from existing literature. The protocol provides deniability since alternative messages can be generated with high probability.

## Key Results
- Perfect recoverability of original message given secret prompt, verified on short texts
- Stegotexts remain within LLM plausibility range but are generally less probable than originals (enabling detection on average)
- Small 8B parameter LLMs produce high-quality results on consumer hardware
- Protocol enables covert encoding of unfiltered LLM responses within compliant model outputs, raising AI safety concerns

## Why This Works (Mechanism)
The protocol exploits the deterministic nature of LLM token generation when selection is based on probability ranks rather than random sampling. By encoding the secret message as a sequence of ranks from the LLM's probability distribution, the cover text generation becomes a reversible process: anyone with the secret prompt can reconstruct the original message by observing which tokens were selected at each position. The key insight is that the same LLM can serve both as the encoder (to compute ranks) and decoder (to regenerate tokens), making the protocol self-contained and requiring no external key material beyond the secret prompt.

## Foundational Learning

**Token Ranking** - Why needed: Forms the basis of the steganographic encoding by converting messages to numerical sequences. Quick check: Verify that rank computation produces consistent values across identical contexts.

**Deterministic Token Selection** - Why needed: Enables perfect reversibility of the encoding process. Quick check: Confirm that selecting the r-th most probable token produces identical results across runs.

**Logit Probability Distributions** - Why needed: Underlies the security and plausibility of the steganographic encoding. Quick check: Compare probability distributions of stegotexts vs original texts to verify detectability claims.

## Architecture Onboarding

**Component Map**
Secret Message -> Tokenizer -> Rank Computation -> Token Selection -> Stegotext
Stegotext + Secret Prompt -> Tokenizer -> Rank Computation -> Token Selection -> Original Message

**Critical Path**
The encoding path (Secret Message -> Stegotext) is critical as it directly impacts message security and stegotext plausibility. The rank computation and token selection steps must be perfectly deterministic.

**Design Tradeoffs**
- Smaller models (8B) offer faster inference but may produce less plausible stegotexts
- Longer secret prompts improve stegotext quality but reduce encoding capacity
- Rank-based selection ensures perfect recoverability but may reduce stegotext naturalness

**Failure Signatures**
- High initial ranks producing incoherent stegotext (context derailment)
- Non-identical logits across runs causing decode failures
- Vocabulary mismatches between encoder/decoder LLMs preventing recovery

**3 First Experiments**
1. Encode/decode a short message (85 tokens) to verify perfect recoverability
2. Compare stegotext plausibility scores against original text distribution
3. Test deniability by generating alternative messages from the same stegotext

## Open Questions the Paper Calls Out

**Open Question 1**: Can attackers reduce the secret key search space by exploiting the information revealed in a stegotext, given that keys must be coherent natural language consistent with the stegotext context? The paper notes this feasibility is unclear and suggests inserting random strings in k defeats such attacks, but doesn't analyze baseline vulnerability.

**Open Question 2**: What formal security guarantees can be established for steganographic protocols using LLMs, given that current formal models require unrealistic assumptions about quantifying text plausibility? The authors avoid formal security modeling due to the unreliability of LLM probability estimates.

**Open Question 3**: Can Calgacus be effectively extended to other discrete autoregressive modalities such as images, music, or speech? The authors state the method can be naturally extended but leave exploration to future work.

**Open Question 4**: What precisely determines whether a stegotext remains indistinguishable from natural text to LLM-based detectors versus those that can be detected? The paper demonstrates the phenomenon empirically but doesn't characterize what properties produce indistinguishable cases.

## Limitations
- Security relies on prompt secrecy without formal security analysis or quantitative brute-force bounds
- Token vocabulary mismatches between encoder/decoder LLMs create edge cases
- Protocol's security depends on the secrecy of the prompt, which may be vulnerable to key-recovery attacks

## Confidence
- High Confidence: Perfect recoverability given secret prompt k; practical feasibility on consumer hardware with 8B parameter models; coherence of stegotexts within LLM plausibility range
- Medium Confidence: Effectiveness of deniability through alternative message generation; AI safety implications for model alignment evasion
- Low Confidence: Formal security guarantees against brute-force or statistical attacks; performance with models of different architectures or tokenization schemes

## Next Checks
1. **Token Selection Stability**: Verify that rank-based token selection produces coherent text across multiple inference runs with identical hardware/precision settings, checking for non-deterministic logits.
2. **Cross-Encoder Recovery**: Test decoding success when using different but similar-sized LLMs (e.g., Llama 3 8B vs. Mistral 7B) to assess vocabulary mismatch handling and identify failure rates.
3. **Statistical Distinguishability**: Implement the LLM-based detection method from the paper to measure the average log-probability gap between stegotexts and their original messages across 100+ test cases, confirming the reported ~2-3 nats difference.