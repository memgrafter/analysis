---
ver: rpa2
title: 'SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via
  Hierarchical Group Quantization and SVD-Guided Mixed Precision'
arxiv_id: '2512.12930'
source_url: https://arxiv.org/abs/2512.12930
tags:
- quantization
- group
- accuracy
- energy
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SeVeDo introduces a heterogeneous accelerator that improves transformer
  inference energy efficiency by structurally separating outlier-sensitive computations
  into a high-precision low-rank path, while executing the rest in a low-bit residual
  path with group quantization. It addresses the energy overhead of outlier handling
  and group quantization through two key innovations: (1) Hierarchical Group Quantization
  (HGQ) reduces floating-point dequantization cost by combining coarse-grained floating-point
  scaling with fine-grained shifting, achieving 36.1% energy and 20.0% area savings;
  (2) SVD-Guided Mixed Precision (SVD-MP) statically identifies precision-sensitive
  components via low-rank decomposition and processes them with bit-sliced integer
  units, yielding 75% energy and 46% area savings over FP16.'
---

# SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision

## Quick Facts
- arXiv ID: 2512.12930
- Source URL: https://arxiv.org/abs/2512.12930
- Authors: Yuseon Choi; Sangjin Kim; Jungjun Oh; Byeongcheol Kim; Hoi-Jun Yoo
- Reference count: 21
- Primary result: Achieves 13.8TOPS/W peak energy efficiency with hierarchical group quantization and SVD-guided mixed precision

## Executive Summary
SeVeDo introduces a heterogeneous transformer accelerator that improves inference energy efficiency through structural separation of outlier-sensitive computations from the rest of the model. The architecture divides operations into a high-precision low-rank path for outliers and a low-bit residual path for standard computations, addressing the energy overhead of outlier handling and group quantization. Two key innovations enable these improvements: Hierarchical Group Quantization (HGQ) reduces floating-point dequantization costs by combining coarse-grained floating-point scaling with fine-grained shifting, and SVD-Guided Mixed Precision (SVD-MP) statically identifies precision-sensitive components via low-rank decomposition for processing with bit-sliced integer units. The design achieves 36.1% energy savings and 20.0% area reduction with HGQ, plus 75% energy and 46% area savings with SVD-MP over FP16 baselines.

## Method Summary
The SeVeDo accelerator implements a heterogeneous architecture that separates transformer computations into two distinct paths based on outlier sensitivity. The system employs Hierarchical Group Quantization to minimize the energy cost of dequantization operations by using floating-point scaling at the group level combined with fine-grained bit shifting within groups. Simultaneously, SVD-Guided Mixed Precision uses singular value decomposition to statically analyze and identify components requiring higher precision, routing these through a dedicated low-rank path with bit-sliced integer units. This architectural separation allows the majority of computations to execute in low-bit precision while preserving accuracy for sensitive operations. The design targets transformer inference workloads, particularly vision transformers and large language models, demonstrating substantial improvements in energy efficiency without significant accuracy degradation.

## Key Results
- Achieves peak energy efficiency of 13.8TOPS/W
- Delivers 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B
- HGQ provides 36.1% energy and 20.0% area savings
- SVD-MP achieves 75% energy and 46% area savings over FP16

## Why This Works (Mechanism)
The heterogeneous architecture works by exploiting the observation that transformer models contain both outlier-sensitive and outlier-tolerant computations. By structurally separating these operations, SeVeDo can apply aggressive quantization to the majority of computations while preserving accuracy for the small fraction that requires higher precision. The hierarchical approach to quantization reduces the overhead of dequantization operations, which typically consume significant energy in low-bit inference systems. SVD-guided analysis enables precise identification of which components need higher precision, avoiding the energy waste of applying uniform high precision across all operations. This targeted approach maximizes energy savings while maintaining model accuracy, addressing the fundamental trade-off between quantization aggressiveness and inference quality.

## Foundational Learning
- **Hierarchical Group Quantization**: Why needed - reduces energy overhead of dequantization in low-bit inference; Quick check - verify energy savings compared to flat quantization approaches
- **Singular Value Decomposition (SVD)**: Why needed - identifies precision-sensitive components for mixed-precision processing; Quick check - validate SVD accuracy in predicting outlier sensitivity
- **Bit-sliced integer units**: Why needed - enables efficient high-precision computation for outlier paths; Quick check - measure energy efficiency versus floating-point alternatives
- **Heterogeneous architecture design**: Why needed - separates outlier-tolerant and outlier-sensitive operations; Quick check - confirm area overhead versus single-path designs
- **Transformer quantization techniques**: Why needed - reduces bit-width for majority of computations; Quick check - evaluate accuracy impact across different quantization levels
- **Low-rank decomposition**: Why needed - enables efficient approximation of precision-sensitive components; Quick check - verify approximation quality versus original operations

## Architecture Onboarding
Component map: Input tensors -> HGQ quantization path -> Low-bit execution units / SVD analysis -> Mixed-precision path -> Bit-sliced integer units -> Output
Critical path: Quantization → Execution → Dequantization → Output
Design tradeoffs: Energy efficiency versus accuracy, area overhead versus precision flexibility, complexity of heterogeneous control versus uniform processing
Failure signatures: Accuracy degradation when SVD misidentifies precision-sensitive components, energy waste when quantization is too aggressive, area inefficiency from improper path balancing
First experiments: 1) Benchmark energy consumption across different quantization granularities, 2) Measure accuracy impact of SVD-guided precision selection, 3) Characterize area overhead of heterogeneous path implementation

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks comprehensive comparison with state-of-the-art accelerators in accuracy, energy efficiency, and area metrics
- Does not provide thorough analysis of accuracy impact from proposed quantization and mixed-precision techniques
- Does not address potential trade-offs between energy savings and increased architectural complexity

## Confidence
- High confidence in reported energy efficiency gains (13.8TOPS/W peak, 12.7TOPS/W on ViT-Base, 13.4TOPS/W on Llama2-7B) based on simulation results
- Medium confidence in area savings (36.1% for HGQ and 46% for SVD-MP) based on simulation results, though actual silicon area may vary
- Low confidence in accuracy claims due to lack of detailed analysis and experimental results supporting accuracy maintenance

## Next Checks
1. Conduct comprehensive comparison of SeVeDo with state-of-the-art transformer accelerators across accuracy, energy efficiency, and area metrics
2. Perform detailed analysis of impact from proposed techniques on transformer model accuracy, including trade-off studies between energy savings and accuracy loss
3. Evaluate trade-offs between energy savings and increased heterogeneous architecture complexity, including analysis of additional hardware requirements for low-rank and mixed-precision paths