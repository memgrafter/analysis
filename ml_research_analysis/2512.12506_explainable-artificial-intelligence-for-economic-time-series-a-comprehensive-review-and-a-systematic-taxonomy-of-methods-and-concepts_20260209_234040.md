---
ver: rpa2
title: 'Explainable Artificial Intelligence for Economic Time Series: A Comprehensive
  Review and a Systematic Taxonomy of Methods and Concepts'
arxiv_id: '2512.12506'
source_url: https://arxiv.org/abs/2512.12506
tags:
- shap
- economic
- time
- causal
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews and organizes the growing literature on Explainable
  Artificial Intelligence (XAI) for economic time series, addressing challenges such
  as autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime
  shifts that can make standard explanation techniques unreliable or economically
  implausible. The authors propose a taxonomy that classifies methods by explanation
  mechanism (propagation-based, perturbation and game-theoretic attribution, function-based
  global tools) and time-series compatibility (preservation of temporal dependence,
  stability over time, respect for data-generating constraints).
---

# Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts

## Quick Facts
- arXiv ID: 2512.12506
- Source URL: https://arxiv.org/abs/2512.12506
- Reference count: 0
- Primary result: Reviews and organizes XAI methods for economic time series, addressing autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts.

## Executive Summary
This survey systematically reviews and classifies Explainable Artificial Intelligence methods for economic time series, addressing challenges such as temporal dependence, regime shifts, and mixed frequencies that standard explanation techniques struggle with. The authors propose a taxonomy organizing methods by explanation mechanism (propagation-based, perturbation/game-theoretic, function-based) and time-series compatibility (preservation of temporal dependence, stability over time, respect for data-generating constraints). The paper synthesizes time-series-specific adaptations like Vector SHAP and WindowSHAP that reduce computational cost and improve interpretability by grouping lagged features, and connects explainability to causal inference through interventional attributions and counterfactual reasoning for policy analysis.

## Method Summary
The paper synthesizes and classifies existing XAI methods for economic time series without conducting original experiments. It provides conceptual descriptions of methods including Layer-wise Relevance Propagation, Integrated Gradients, SHAP variants (standard, Vector, Window, Causal), LIME, ALE plots, and intrinsically interpretable architectures like transformers with attention. No implementation details, datasets, or quantitative benchmarks are provided—the focus is on methodological taxonomy and theoretical synthesis rather than empirical validation.

## Key Results
- Proposes a taxonomy classifying XAI methods by explanation mechanism and time-series compatibility
- Synthesizes time-series-specific adaptations like Vector SHAP and WindowSHAP that reduce lag fragmentation and computational cost
- Connects explainability to causal inference through interventional attributions (Causal Shapley values)
- Provides guidance for decision-grade applications emphasizing attribution uncertainty and explanation dynamics as structural change indicators

## Why This Works (Mechanism)

### Mechanism 1: Temporal SHAP Adaptations (Vector SHAP, WindowSHAP)
Standard SHAP permutes individual features, creating fragmented attributions and computational burden for time series with many lags. Vector SHAP groups all lags of each variable into a single feature vector, treating the entire temporal history of a variable as one player in the Shapley game. WindowSHAP groups features by time windows, treating entire temporal periods as unified features with dynamic variants adapting window size based on importance. Both approaches reduce dimensionality and computational cost while improving interpretability by focusing on variable-level contributions rather than individual lag contributions.

### Mechanism 2: Propagation-Based Relevance Decomposition (LRP, Integrated Gradients)
Backward propagation methods like Layer-wise Relevance Propagation and Integrated Gradients decompose predictions into input contributions by redistributing evidence from output backward through network layers. LRP applies conservation rules to ensure relevance is conserved across layers, while Integrated Gradients accumulates gradients along a path from baseline to input. These methods satisfy axioms like completeness (attributions sum to prediction-baseline difference) and exploit the differentiability of neural networks to provide white-box explanations.

### Mechanism 3: Causal Shapley Values (Interventional Attribution)
Standard SHAP conditions on observed features, which can absorb correlated variable effects and produce spurious attributions. Causal SHAP replaces conditional expectations with interventional expectations using do-calculus, cutting causal links from parent nodes in a DAG to isolate each variable's causal contribution. This separates correlational from causal attribution and decomposes direct vs. indirect effects, enabling policy-relevant explanations that distinguish root causes from downstream effects.

## Foundational Learning

- **Time Series Structure (Autocorrelation, Non-Stationarity, Regime Shifts)**
  - Why needed here: Economic time series violate feature independence assumptions through temporal dependence and structural breaks
  - Quick check question: Can you explain why permuting a single lag destroys autocorrelation and creates economically implausible counterfactuals?

- **Shapley Value Theory (Coalitional Game Theory)**
  - Why needed here: Understanding efficiency, consistency, and missingness axioms is essential to evaluate SHAP variants and their tradeoffs
  - Quick check question: Why does the "missingness" axiom matter when some lags may have no predictive value in certain regimes?

- **Causal Graphs and Do-Calculus**
  - Why needed here: Distinguishing correlational from causal attribution requires understanding intervention vs. observation, direct vs. indirect effects
  - Quick check question: What's the difference between E[Y|X=x] and E[Y|do(X=x)], and why does it matter for policy simulation?

## Architecture Onboarding

- **Component map:**
  XAI Method Selection
  ├── Propagation-based (white-box)
  │   ├── Integrated Gradients → baseline-sensitive, completeness axiom
  │   └── LRP → layer-wise conservation, regime-dependent factor detection
  ├── Perturbation/Game-theoretic (model-agnostic)
  │   ├── SHAP → axiomatically grounded, computationally expensive
  │   ├── Vector SHAP → groups lags by variable, reduces dimensionality
  │   ├── WindowSHAP → groups by time windows, detects temporal regimes
  │   ├── Causal SHAP → requires DAG, separates direct/indirect effects
  │   └── LIME → local surrogate models, unstable for time series
  ├── Function-based Global Tools
  │   └── ALE plots → handles correlated predictors, associational only
  └── Intrinsic Interpretability
      └── TFT Attention → built-in variable selection and temporal attention

- **Critical path:**
  1. Audit model architecture (white-box vs. black-box access)
  2. Characterize time series properties (stationarity, lag structure, regime history)
  3. Select method matching temporal constraints and interpretability goals
  4. Validate explanations against economic theory (sign coherence, regime consistency)
  5. Monitor explanation stability over time as structural change indicator

- **Design tradeoffs:**
  - Granularity vs. coherence: Per-lag attributions (standard SHAP) are precise but fragmented; aggregated approaches (Vector/WindowSHAP) are interpretable but may miss timing
  - Causality vs. feasibility: Causal SHAP provides policy-relevant attributions but requires known DAGs; standard SHAP is plug-and-play but conflates correlation
  - Computation vs. axiom satisfaction: TreeSHAP is fast for tree models; Kernel SHAP is slow but model-agnostic; LRP requires internal access

- **Failure signatures:**
  - Explanation instability: SHAP values swinging dramatically between t and t+1 without model change → temporal perturbation violating autocorrelation
  - Economically incoherent counterfactuals: Explanations implying impossible states (e.g., high inflation with zero interest rates and full employment) → unconstrained perturbation
  - Sign violations: Interest rates showing positive contribution to demand dampening → spurious correlation or model misspecification flag

- **First 3 experiments:**
  1. **Baseline SHAP vs. Vector SHAP comparison:** Apply both to a VAR/LSTM model with 12+ lags; measure computational time and interpretability of aggregated variable importance vs. fragmented lag importance
  2. **WindowSHAP regime detection:** On a dataset with known structural breaks (e.g., pre/post COVID), verify if WindowSHAP isolates crisis windows and attributes importance shifts correctly
  3. **Sign coherence audit:** Implement block-bootstrapped SHAP confidence intervals for a nowcasting model; flag any sign violations (e.g., positive SHAP for interest rates on inflation) for expert review

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can causal graphs be reliably discovered or validated for economic time series to enable Causal Shapley values without imposing subjective expert-specified DAGs?
- Basis in paper: Implementing Causal SHAP requires knowledge (or discovery) of the causal graph (Directed Acyclic Graph, DAG). When the causal graph is unknown, causal discovery algorithms or expert knowledge (priors) are necessary.
- Why unresolved: Causal discovery from observational economic data is fundamentally underdetermined; existing algorithms often produce multiple equivalent structures, and expert priors may introduce bias.
- What evidence would resolve it: Empirical benchmarks comparing causal discovery algorithms on synthetic economic data with known ground-truth DAGs, validated against out-of-sample interventional outcomes.

### Open Question 2
- Question: How can ML-based counterfactuals be constrained to remain economically plausible while preserving the flexibility advantages over structural VAR impulse-response functions?
- Basis in paper: A risk with pure ML counterfactuals is generating 'adversarial examples' that are mathematically valid to fool the model but economically absurd... Recent research emphasizes using 'impulse-constrained' or manifold-constrained counterfactuals.
- Why unresolved: No consensus formalization exists for what constitutes an "economically plausible" counterfactual; manifold constraints may over-restrict exploration of rare but policy-relevant scenarios.
- What evidence would resolve it: A framework defining economic plausibility constraints with empirical validation showing constrained counterfactuals match historical policy interventions while improving over SVAR flexibility.

### Open Question 3
- Question: How should attribution uncertainty (e.g., confidence intervals for SHAP values) be quantified and incorporated into decision-making for policy monitoring and nowcasting?
- Basis in paper: Point estimates of feature importance are insufficient. Using block bootstrapping, analysts can generate confidence intervals for SHAP values. If the SHAP value... has a wide interval crossing zero, the model is uncertain about its contribution.
- Why unresolved: Standard bootstrapping assumes stationarity; block bootstrapping parameters for non-stationary, regime-shifting series lack theoretical grounding.
- What evidence would resolve it: Comparative studies of bootstrap methods on economic data with known importance rankings, demonstrating calibrated coverage properties under structural breaks.

## Limitations
- Lack of empirical benchmarking across XAI methods for economic time series
- Computational efficiency claims for Vector/WindowSHAP not directly verified
- Causal attribution methods requiring domain-specific DAGs that may not generalize
- No quantitative assessment of explanation stability as a regime-change indicator
- Limited discussion of mixed-frequency reconciliation challenges

## Confidence
- **High**: Integrated Gradients, LRP, SHAP (well-established methods with robust theoretical foundations)
- **Medium**: Vector SHAP, WindowSHAP (synthesized adaptations without direct quantitative benchmarks)
- **Low**: Causal Shapley values (limited empirical validation, dependence on assumed causal structures)

## Next Checks
1. Implement Vector SHAP and standard SHAP on an economic dataset with 12+ lags; measure computational time and compare attribution coherence across methods
2. Apply WindowSHAP to a dataset with known structural breaks (e.g., pre/post-COVID); verify if it correctly isolates regime-specific windows and attributes importance shifts
3. Conduct a sign-coherence audit using block-bootstrapped SHAP confidence intervals on a nowcasting model; flag any economically implausible attributions for expert review