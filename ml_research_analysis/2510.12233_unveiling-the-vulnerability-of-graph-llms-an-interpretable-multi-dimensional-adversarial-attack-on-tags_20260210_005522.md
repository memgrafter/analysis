---
ver: rpa2
title: 'Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional
  Adversarial Attack on TAGs'
arxiv_id: '2510.12233'
source_url: https://arxiv.org/abs/2510.12233
tags:
- graph
- attack
- adversarial
- node
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work exposes a critical vulnerability in Graph-LLMs by proposing
  IMDGA, a multi-dimensional adversarial attack framework that manipulates both textual
  and structural components of text-attributed graphs. It introduces three modules:
  Topological SHAP to identify pivotal words, Semantic Perturbation to substitute
  them with contextually plausible alternatives, and Edge Pruning to disrupt critical
  message-passing paths.'
---

# Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs

## Quick Facts
- **arXiv ID:** 2510.12233
- **Source URL:** https://arxiv.org/abs/2510.12233
- **Reference count:** 40
- **Key outcome:** IMDGA exposes critical vulnerabilities in Graph-LLMs by manipulating textual and structural components of TAGs, achieving up to 95% attack success rates while maintaining stealthiness.

## Executive Summary
This work presents IMDGA, a multi-dimensional adversarial attack framework that targets the vulnerability of Graph-LLMs on Text-Attributed Graphs. The framework introduces three key modules: Topological SHAP for identifying pivotal words through graph-aware Shapley value attribution, Semantic Perturbation for replacing critical words with contextually plausible alternatives using MLMs, and Edge Pruning to disrupt critical message-passing paths. The attack achieves superior performance compared to existing text and graph-based baselines, demonstrating that Graph-LLMs are susceptible to coordinated attacks on both textual and structural components while maintaining interpretability through feature attribution.

## Method Summary
IMDGA is a black-box adversarial attack framework that targets Graph-LLMs on Text-Attributed Graphs through a three-stage pipeline. First, Topological SHAP identifies pivotal words by computing Shapley values that measure each word's contribution to the aggregated predictions of a target node and its 1-hop neighbors. Second, Semantic Perturbation uses a Masked Language Model to generate candidate replacements for pivotal words, selecting those that maximize the reduction in confidence gap across the neighborhood while maintaining semantic similarity. Finally, Edge Pruning identifies and removes edges connected to a "nexus of vulnerability" - nodes highly influenced by the target but with low decision certainty - to amplify the attack when text modifications alone are insufficient.

## Key Results
- Achieves up to 95% Attack Success Rate (ASR) on Cora dataset while maintaining high semantic similarity and low perplexity
- Outperforms existing text-based and graph-based baselines across multiple datasets (Cora, Citeseer, PubMed, ogbn-arxiv)
- Ablation study shows Edge Pruning is critical, with ASR dropping from 92% to 66% on Cora when removed
- Maintains stealthiness with high semantic similarity scores and low perplexity compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Topological SHAP for Graph-Aware Word Attribution
The framework computes Shapley values not just on the target node's text, but by aggregating the victim model's predictions over the node's ego-graph (target node and its neighbors). By masking words and measuring the marginal contribution drop, it isolates tokens that serve as structural anchors for the model's reasoning. The core assumption is that Graph-LLMs rely on specific textual tokens to propagate distinct class signals to neighbors; removing these tokens disrupts the message passing flow.

### Mechanism 2: Semantic Perturbation via MLM Substitution
For each pivotal word, the module generates candidates using an MLM and selects the replacement that maximizes the reduction of the "confidence gap" (difference between top two predicted probabilities) across the neighborhood. This ensures the perturbation is semantically valid but decision-boundary aggressive. The core assumption is that the MLM's embedding space overlaps significantly with the victim encoder's space, meaning semantically similar substitutes will appear "safe" while confusing the classification head.

### Mechanism 3: Edge Pruning for Nexus Disruption
This module identifies nodes highly influenced by the target and having low decision certainty, then uses Shapley approximation to rank edges based on their contribution to correct classification and prunes the top contributors. The core assumption is that Graph-LLMs rely on specific "vital" edges to correct noisy text signals; cutting these edges forces the model to rely on the perturbed text, causing misclassification.

## Foundational Learning

- **Concept: Shapley Values in Machine Learning**
  - **Why needed here:** The core logic of IMDGA relies on Shapley values to quantify feature importance. Without understanding how marginal contributions are calculated over coalitions, the "interpretable" aspect of the attack is opaque.
  - **Quick check question:** If a word contributes +0.4 to Class A but -0.1 to Class B, what is its net importance score $\xi(i)$ if we only care about the true label $y$?

- **Concept: Message Passing in GNNs**
  - **Why needed here:** The attack explicitly targets "topological" words. You must understand that a node's representation is an aggregation of its neighbors' features to see why perturbing text affects neighbors (and vice versa).
  - **Quick check question:** Why does the "confidence gap" (Eq. 12) need to be summed over the neighborhood $N(v)$ rather than just the target node $v$?

- **Concept: Black-Box Attacks (Decision-based)**
  - **Why needed here:** The paper claims a realistic setting. Understanding that the attacker only sees the output label/probability (not gradients) explains why the method uses scoring functions and sampling rather than backpropagation.
  - **Quick check question:** Does IMDGA require the gradient $\nabla_x J$ of the victim model? (Check Section 1/Intro).

## Architecture Onboarding

- **Component map:** Topological SHAP Module -> Semantic Perturbation Module -> Edge Pruning Module
- **Critical path:** The dependency chain is strictly sequential: Topological SHAP must identify words before Semantic Perturbation can replace them. Edge Pruning is a refinement step applied after text perturbation if the attack succeeds partially or needs amplification.
- **Design tradeoffs:**
  - **Text Modification Ratio ($\beta$) vs. Stealthiness:** Increasing $\beta$ raises ASR but lowers semantic similarity/perplexity scores.
  - **Candidate Pool Size ($k_c$) vs. Efficiency:** More candidates improve the chance of finding a high-impact substitute but linearly increase query cost.
- **Failure signatures:**
  - **Low ASR with High Modification:** Suggests the Topological SHAP module failed to identify truly pivotal words, or the victim model is robust to the specific MLM used.
  - **High Perplexity:** The Semantic Perturbation module is selecting rare words; check the MLM similarity threshold or constraints.
- **First 3 experiments:**
  1. **Baseline Reproduction (Table 1):** Run IMDGA on Cora with SBERT encoding against a 2-layer GCN. Target: Achieve ~95% ASR with ≤ 30% text modification.
  2. **Ablation Check (Table 4):** Disable the Edge Pruning module. Verify that ASR drops significantly (e.g., to ~65%) to prove the necessity of the structural component.
  3. **Stealthiness Validation (Fig 3/4):** Run the attack and plot degree distributions before/after. Ensure the "head" of the distribution overlaps to confirm structural stealthiness.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can systematic defense mechanisms be developed to specifically mitigate the multi-dimensional vulnerabilities exposed by IMDGA?
  - **Basis in paper:** The Conclusion states that the findings "underscore the urgent need for systematic defenses to guide the development of more resilient Graph-LLMs."
  - **Why unresolved:** The paper focuses exclusively on designing the attack framework to expose weaknesses; it does not propose or evaluate any defensive strategies.

- **Open Question 2:** Does the IMDGA framework transfer effectively to decoder-only Generative LLMs (e.g., GPT, Llama) used for graph reasoning?
  - **Basis in paper:** The paper defines "Graph-LLMs" strictly as encoder-based models (BERT, RoBERTa) coupled with GNNs and relies on Masked Language Models for perturbation.
  - **Why unresolved:** The Semantic Perturbation module exploits MLMs to generate candidates; this mechanism may not translate directly to autoregressive decoder architectures without modification.

- **Open Question 3:** Can incorporating edge injection (adding nodes/edges) improve attack stealthiness compared to the current edge pruning approach?
  - **Basis in paper:** The Introduction categorizes Graph Injection Attacks (GIAs) as a separate vector, but the proposed Edge Pruning module is restricted to deleting edges.
  - **Why unresolved:** In dense or dynamic graphs, removing edges might be statistically more noticeable than adding new, seemingly legitimate connections.

## Limitations
- Relies on sampling-based approximations for Shapley values without specifying exact sampling strategy or size, which could impact reproducibility and efficiency.
- Performance of Edge Pruning module is highly dependent on unreported hyperparameter tuning (weights α1, α2, α3).
- Attack assumes sufficient overlap between the MLM used for candidate generation and the victim encoder's embedding space, which may not hold for domain-specific models.

## Confidence
- **Multi-Dimensional Attack Effectiveness:** High - Ablation study provides strong evidence that combining text and structure attacks is more effective than either alone.
- **Interpretability via Shapley Values:** Medium - Method quantifies feature importance but interpretability is limited to identifying "pivotal" words, not explaining full reasoning process.
- **Stealthiness Claims:** Medium - Reports high semantic similarity and low perplexity, but lacks human evaluation to confirm practical stealthiness.

## Next Checks
1. **Shapley Sampling Sensitivity:** Reproduce the attack with varying numbers of Shapley samples (e.g., s=10, s=100, s=1000) to quantify the trade-off between computational cost and attack success rate.
2. **Hyperparameter Robustness:** Perform a grid search over the Edge Pruning weights (α1, α2, α3) and report the performance variance to determine if the attack is robust to hyperparameter choices.
3. **Domain Transferability Test:** Apply the attack to a Graph-LLM using a significantly different encoder (e.g., a biomedical BERT) than the one used for MLM candidate generation to assess domain gap risk.