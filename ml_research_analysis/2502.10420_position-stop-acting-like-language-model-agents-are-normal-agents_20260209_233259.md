---
ver: rpa2
title: 'Position: Stop Acting Like Language Model Agents Are Normal Agents'
arxiv_id: '2502.10420'
source_url: https://arxiv.org/abs/2502.10420
tags:
- arxiv
- agents
- lmas
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language Model Agents (LMAs) are increasingly treated as capable
  of autonomously navigating interactions with humans and tools. Their design and
  deployment tends to presume they are normal agents capable of sustaining coherent
  goals, adapting across contexts and acting with a measure of intentionality.
---

# Position: Stop Acting Like Language Model Agents Are Normal Agents

## Quick Facts
- arXiv ID: 2502.10420
- Source URL: https://arxiv.org/abs/2502.10420
- Reference count: 40
- LMAs are not normal agents due to fundamental ontological instabilities including statelessness, stochasticity, semantic sensitivity, and linguistic intermediation.

## Executive Summary
Language Model Agents (LMAs) are increasingly treated as capable of autonomous interaction with humans and tools, with assumptions of coherent goals, adaptability, and intentionality driving their use in industrial, social, and governmental contexts. However, this paper argues that LMAs are not normal agents and that such assumptions are misplaced. LMAs inherit structural problems from large language models (LLMs), including hallucinations, jailbreaking, misalignment, and unpredictability. These issues undermine their utility and trustworthiness. The authors enumerate intrinsic pathologies of agency in LMAs—statelessness, stochasticity, semantic sensitivity, and linguistic intermediation—that destabilize key ontological properties such as identifiability, continuity, persistence, and consistency. To address these risks, the authors propose measuring LMA ontological properties before, during, and after deployment to mitigate negative effects.

## Method Summary
The authors use theoretical comparisons between LLMs and established agent architectures to argue that LMAs should not be treated as normal agents. Their analysis highlights the fundamental limitations of current LMA designs, but acknowledges that direct empirical validation of the claimed ontological instabilities is limited. The discussion is grounded in qualitative argumentation and selective literature references rather than systematic benchmarking or empirical testing.

## Key Results
- LMAs are not normal agents due to inherent structural problems inherited from LLMs.
- LMAs exhibit pathologies such as statelessness, stochasticity, semantic sensitivity, and linguistic intermediation.
- These pathologies destabilize core ontological properties like identifiability, continuity, persistence, and consistency.

## Why This Works (Mechanism)
The paper argues that LMAs are fundamentally different from normal agents because they inherit the limitations of LLMs, including unpredictability and misalignment. These issues arise from the stochastic and context-sensitive nature of LMAs, which prevent them from exhibiting stable agency properties.

## Foundational Learning
- **Ontological Properties of Agents**: These include identifiability, continuity, persistence, and consistency—properties LMAs lack due to their LLM foundations. *Why needed*: To understand why LMAs cannot be treated as normal agents. *Quick check*: Verify that LMAs exhibit instability in these properties during deployment.
- **LLM Limitations**: LLMs are prone to hallucinations, jailbreaking, misalignment, and unpredictability. *Why needed*: To explain the root causes of LMA pathologies. *Quick check*: Review documented cases of LLM failures in agent contexts.
- **Pathologies of Agency**: Statelessness, stochasticity, semantic sensitivity, and linguistic intermediation are intrinsic to LMAs. *Why needed*: To identify the specific ways LMAs fail as agents. *Quick check*: Test for these pathologies in real-world LMA deployments.

## Architecture Onboarding
- **Component Map**: LLM -> Scaffolding (memory, tools, planning) -> LMA
- **Critical Path**: LLM outputs are processed through scaffolding to produce agent-like behavior, but the underlying LLM limitations persist.
- **Design Tradeoffs**: Scaffolding can enhance LMA functionality but cannot overcome fundamental LLM pathologies.
- **Failure Signatures**: Inconsistency in responses, inability to maintain coherent goals, and sensitivity to input phrasing.
- **First Experiments**:
  1. Measure response consistency across extended LMA sessions.
  2. Test LMA goal stability in multi-step tasks.
  3. Evaluate sensitivity to semantic variations in input prompts.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis is based on theoretical comparisons rather than systematic empirical validation.
- The claimed ontological instabilities are not comprehensively benchmarked across diverse LMA deployments.
- The proposed measurement framework is conceptually outlined but lacks methodological detail or validation.

## Confidence
- **Core Claim (LMAs are not normal agents)**: Medium — compelling structural arguments, but limited empirical breadth.
- **Enumeration of Pathologies**: Medium — well-grounded in known LLM behaviors, but not yet validated across extended or multi-modal deployments.
- **Measurement Framework**: Low — conceptually outlined but lacks detail or validation.

## Next Checks
1. **Empirical benchmarking**: Design and run controlled experiments comparing identity persistence, goal stability, and response consistency of LMAs across extended sessions and task sequences.
2. **Cross-architecture survey**: Systematically review and test a representative sample of deployed LMAs (including those with external memory, planning, and tool-use) to determine which pathologies persist and under what conditions.
3. **Measurement framework prototyping**: Develop and pilot a minimal set of metrics and procedures for measuring the four ontological properties (identifiability, continuity, persistence, consistency) in live LMA deployments, and report on feasibility and sensitivity.