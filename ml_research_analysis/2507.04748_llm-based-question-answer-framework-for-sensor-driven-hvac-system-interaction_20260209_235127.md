---
ver: rpa2
title: LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction
arxiv_id: '2507.04748'
source_url: https://arxiv.org/abs/2507.04748
tags:
- data
- jarvis
- response
- user
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents JARVIS, a two-stage LLM-based QA framework
  for sensor-driven HVAC system interaction. The framework uses an Expert-LLM to translate
  user queries into structured execution instructions, followed by an Agent that performs
  SQL-based data retrieval, statistical processing, and response generation.
---

# LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction

## Quick Facts
- arXiv ID: 2507.04748
- Source URL: https://arxiv.org/abs/2507.04748
- Reference count: 40
- Primary result: JARVIS achieves high response quality (>4.0 for cohesiveness and truthfulness) and 92% query execution accuracy on real-world HVAC data

## Executive Summary
This paper presents JARVIS, a two-stage LLM-based QA framework for sensor-driven HVAC system interaction. The framework uses an Expert-LLM to translate user queries into structured execution instructions, followed by an Agent that performs SQL-based data retrieval, statistical processing, and response generation. Key innovations include an adaptive context injection strategy, a parameterized SQL builder for robust data access, and a bottom-up planning scheme for coherent multi-stage responses. Evaluated on real-world HVAC data and expert-curated QA datasets, JARVIS consistently outperforms baseline models, achieving high response quality scores and accurate query execution.

## Method Summary
JARVIS is a two-stage framework where an Expert-LLM (fine-tuned LLaMA3.1-8B-Instruct) translates user queries into structured JSON instructions, and an Agent executes these instructions through parameterized SQL queries, Python data processing, and response generation. The system uses adaptive context injection (fine-tuning for HVAC knowledge, prompting for deployment-specific metadata), a parameterized SQL builder to reduce text-to-SQL failures, and bottom-up planning with explicit expectation templates. The framework was evaluated on 1 year of HVAC sensor data from 156 units using 80 expert-curated QA pairs.

## Key Results
- JARVIS achieves 92% exact match rate on query execution accuracy
- Response quality scores exceed 4.0 for cohesiveness and truthfulness on 5-point Likert scales
- End-to-end latency remains under 9 seconds, with Expert-LLM contributing 5.47s base latency
- User study shows JARVIS outperforms baseline models on all quality metrics (cohesiveness, helpfulness, truthfulness)

## Why This Works (Mechanism)

### Mechanism 1
Adaptive context injection improves LLM performance while managing computational cost by matching injection method to context characteristics. Context is categorized into four types based on versatility (deployment-agnostic vs. dynamic) and representation length. HVAC-common knowledge (long, stable) is embedded via fine-tuning; deployment/user-specific metadata (short, dynamic) is injected via prompting; large-scale sensor data is preprocessed/summarized before injection. Core assumption: Fine-tuning captures complex domain patterns more reliably than prompting, while prompting handles rapidly-changing context better than re-training. Evidence anchors: [abstract] "adaptive context injection strategy for efficient HVAC and deployment-specific information integration"; [section 3.2] "we classify the context into four types based on versatility and representation length, and apply an adaptive injection strategy tailored to each category"; [corpus] Moderate support from "Physics-Informed Large Language Models for HVAC Anomaly Detection" (arXiv:2510.17146), which also combines domain knowledge injection with LLMs for HVAC. Break condition: If deployment context changes faster than prompting latency allows, or if fine-tuned knowledge becomes stale and requires frequent re-training, the cost-quality tradeoff inverts.

### Mechanism 2
Parameterized SQL building reduces text-to-SQL failure rates by decoupling high-level intent from low-level syntax generation. The Expert-LLM outputs query intentions using user-native terminology in a structured format (key-value mappings + function calls). A rule-based middleware translates these into complete SQL statements, automatically adding boilerplate (e.g., NULL filtering, joins). This avoids monolithic query generation prone to syntax errors. Core assumption: Generating high-level query parameters is cognitively easier for LLMs than producing complete, syntactically correct SQL with nested subqueries. Evidence anchors: [abstract] "parameterized SQL builder and executor to improve data access reliability"; [section 2.3] "the most common source of failure was the generation of fragile and overly complex SQL statements, particularly those involving subqueries and multi-table joins" with 43.2% of failures due to complex SQL; [corpus] Limited direct evidence in corpus; "Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation" (arXiv:2506.00075) addresses LLM command latency but not text-to-SQL specifically. Break condition: If user queries require SQL operations not covered by the parameterized templates, the rule-based middleware will fail or produce incorrect queries.

### Mechanism 3
Bottom-up planning with explicit expectation templates improves multi-stage response coherence. The Expert-LLM first generates an expected answer template (format + anticipated supplementary information), then works backward to determine what data must be queried and processed. This ensures the response generator receives both the necessary data and a structural guide. Core assumption: Defining the output goal before planning intermediate steps reduces instruction-task misalignment. Evidence anchors: [abstract] "bottom-up planning scheme to ensure consistency across multi-stage response generation"; [section 3.3.3] "By explicitly defining and training samples of these expectations, JARVIS performs bottom-up planning as it establishes the final output goal and works backward"; [section 5.5] User study Question 1 showed w/o Expect variant averaged results incorrectly instead of computing separate values per room, demonstrating misalignment without expectation guidance; [corpus] Weak direct evidence; no corpus papers explicitly study bottom-up planning for LLM orchestration. Break condition: If the expectation template is too rigid, responses may lack naturalness (as noted in Section 5.3: "larger by 0" instead of "the same"); if too flexible, the guiding benefit is lost.

## Foundational Learning

- Concept: Text-to-SQL with semantic context injection
  - Why needed here: Understanding that raw text-to-SQL models fail on domain-specific terminology (e.g., "my room" → database ID mapping) and require context bridging.
  - Quick check question: Can you explain why a generic text-to-SQL model would fail to map "When was the bedroom coldest?" to a valid query without additional context?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The Expert-LLM's Thinking component explicitly generates intermediate reasoning steps to resolve ambiguity and plan execution.
  - Quick check question: What is the difference between an LLM directly outputting SQL vs. first generating a reasoning trace and then SQL?

- Concept: Master-worker orchestration pattern
  - Why needed here: JARVIS separates high-level planning (Expert-LLM) from execution (Agent), requiring understanding of instruction passing and structured output formats.
  - Quick check question: Why would a single LLM struggle to both plan a multi-step HVAC query and execute it reliably?

## Architecture Onboarding

- Component map: User Query → [Expert-LLM (fine-tuned LLaMA3.1-8B)] → Metadata (prompted) → Thinking Component → Expectation Component → JSON Instructions → [Agent (Python runtime)] → Query Execution Module → TimescaleDB → Data Processing Module → Pandas → Response Generator → LG EXAONE-3.5-7.8B → Final Response

- Critical path: Expert-LLM instruction generation → Parameterized SQL execution → Data processing → Response generation. Latency breakdown shows Expert-LLM dominates (5.47s base, up to 8.86s with processing), so optimization efforts should focus there first.

- Design tradeoffs:
  - Fine-tuning vs. prompting for context: Fine-tuning is costly upfront but efficient at inference; prompting is flexible but increases token length and latency.
  - Modular SQL vs. monolithic: Modular queries are more robust but may increase database round-trips.
  - Rigid expectation templates vs. flexible generation: Rigid improves consistency but reduces naturalness (see "larger by 0" example).

- Failure signatures:
  - JSON schema violations (missing brackets): occurs ~1/27 cases; addressed with regex post-processing
  - SQL execution failures on complex operations (argmax not natively supported): mitigated by Python post-processing
  - Null value mishandling: mitigated by automatic boilerplate NULL filtering
  - Context window overflow with large sensor datasets: mitigated by summarization

- First 3 experiments:
  1. **Baseline text-to-SQL comparison**: Run generic LLaMA3-8B text-to-SQL on the 80-query HVAC dataset without HVAC context; measure execution accuracy and compare to JARVIS's parameterized approach. Expect ~40% exact match vs. JARVIS's 92%.
  2. **Ablation on Thinking component**: Remove the Thinking component (CoT reasoning) and measure impact on ambiguous query resolution (e.g., "When was the hottest day two weeks ago?"). Expect drop in cohesiveness and truthfulness scores.
  3. **Latency vs. query complexity**: Profile Expert-LLM latency across token output lengths (Figure 11 pattern); validate that complex queries with longer instructions correlate with higher latency (observed ~5-9s range).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can structured decoding mechanisms effectively eliminate JSON schema violations in the Expert-LLM's instruction generation?
- Basis in paper: [explicit] The authors state in Section 6 that "incorporating structured decoding mechanisms (e.g., constrained or programmatic decoding)" is an interesting future direction to ensure outputs "conform to expected schemas by construction."
- Why unresolved: The current system relies on regex-based post-processing to patch malformed JSON, which the authors admit is "brittle and reactive."
- What evidence would resolve it: An ablation study measuring the schema compliance rate of the Expert-LLM when using constrained decoding versus the current regex patching method.

### Open Question 2
- Question: How can the framework optimize the trade-off between response stability (template adherence) and conversational naturalness?
- Basis in paper: [explicit] Section 6 identifies a limitation where the system produces rigid responses (e.g., "larger by 0") due to reliance on expectation templates, noting that "Balancing these competing objectives... is left for future work."
- Why unresolved: The current design intentionally favors factuality and density over naturalness, and it is unclear how to fine-tune the generator to be flexible without risking hallucination.
- What evidence would resolve it: A user study comparing the "helpfulness" scores of the current template-guided responses against those from a fine-tuned response generator instructed to prioritize natural phrasing.

### Open Question 3
- Question: To what extent can model optimization techniques reduce the Expert-LLM's inference latency without compromising response quality?
- Basis in paper: [explicit] The authors note in Section 6 that "Exploring smaller, optimized model architectures (via quantization, distillation...)" may reduce inference time, which is currently dominated by the Expert-LLM.
- Why unresolved: While JARVIS is efficient, the correlation between output token length and latency suggests performance gains are possible, but the impact of aggressive optimization on reasoning quality is untested.
- What evidence would resolve it: Benchmarks evaluating the end-to-end latency and accuracy of JARVIS running on quantized (e.g., 4-bit) or distilled variants of the LLaMA3.1-8B model.

## Limitations
- The exact format and content of the 53 fine-tuning samples remain underspecified, making faithful reproduction challenging.
- The parameterized SQL builder's template coverage is assumed to be comprehensive but not empirically validated against edge cases.
- The bottom-up planning mechanism may introduce rigidity in natural language responses, as evidenced by the "larger by 0" artifact.

## Confidence
- **High confidence**: The core architecture (two-stage LLM framework with parameterized SQL execution) and baseline performance claims (92% exact match, 4.0+ scores on quality metrics) are well-supported by the experimental setup and results.
- **Medium confidence**: The adaptive context injection strategy's effectiveness is demonstrated but relies on assumptions about fine-tuning vs. prompting tradeoffs that may not generalize across different deployment scenarios or context update frequencies.
- **Low confidence**: The bottom-up planning mechanism's superiority over alternative planning approaches is suggested by user study results but lacks comparative ablation studies against other planning paradigms (e.g., Chain-of-Thought without expectation templates).

## Next Checks
1. **SQL template coverage validation**: Systematically test the parameterized SQL builder against a comprehensive set of query types, including edge cases involving nested subqueries, multi-table joins, and non-standard SQL operations (e.g., percentile calculations, time-window aggregations) to identify potential coverage gaps.

2. **Context injection tradeoff analysis**: Measure the actual computational overhead and accuracy impact of switching between fine-tuning and prompting for context injection across different update frequencies (hourly, daily, weekly) to validate the claimed cost-quality tradeoff.

3. **Planning mechanism ablation**: Conduct controlled experiments comparing bottom-up planning with alternative approaches (standard Chain-of-Thought, direct execution without intermediate reasoning) across query complexity levels to isolate the specific contribution of the expectation template mechanism.