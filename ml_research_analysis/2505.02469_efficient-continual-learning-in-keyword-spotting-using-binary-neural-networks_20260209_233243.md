---
ver: rpa2
title: Efficient Continual Learning in Keyword Spotting using Binary Neural Networks
arxiv_id: '2505.02469'
source_url: https://arxiv.org/abs/2505.02469
tags:
- classes
- algorithms
- training
- accuracy
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how to efficiently add new keywords to small-footprint
  KWS models deployed on resource-constrained devices like MCUs. The authors combine
  Binary Neural Networks with Continual Learning algorithms, aiming to reduce both
  computation and memory while retaining performance on previously learned keywords.
---

# Efficient Continual Learning in Keyword Spotting using Binary Neural Networks

## Quick Facts
- arXiv ID: 2505.02469
- Source URL: https://arxiv.org/abs/2505.02469
- Authors: Quynh Nguyen-Phuong Vu; Luciano Sebastian Martinez-Rau; Yuxuan Zhang; Nho-Duc Tran; Bengt Oelmann; Michele Magno; Sebastian Bader
- Reference count: 29
- Primary result: CL for KWS on BNNs achieves >82% accuracy even when adding 4 new keywords, with <0.001% computational overhead for weight updates

## Executive Summary
This paper demonstrates that Binary Neural Networks combined with Continual Learning algorithms can efficiently add new keywords to small-footprint keyword spotting models deployed on resource-constrained devices. The authors evaluate seven CL methods (TinyOL, TinyOL v2, LwF, CWR, etc.) on a 16-class KWS model using the Google Speech Commands dataset, testing scenarios with 1 to 4 new classes. Results show all methods maintain high accuracy on the original 12 classes (88–95%), with TinyOL and LwF variants excelling when adding a single new keyword. The work demonstrates that CL for KWS on BNNs is feasible on tinyML hardware, with accuracy above 82% even under worst-case conditions.

## Method Summary
The method combines Binary Neural Networks with Continual Learning by pre-training a BNN backbone on 12 classes, then freezing all layers except the final fully-connected layer during CL. The framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords. Seven CL algorithms are evaluated: TinyOL, TinyOL with batches, TinyOL v2, TinyOL v2 with batches, LwF, LwF with batches, and CWR. The BNN model keeps full-precision inputs and weights in the first and last convolutional layer, while other layers are binarized. During CL, the model is updated using categorical cross-entropy loss with a learning rate of 0.05.

## Key Results
- All CL methods maintain >88% accuracy on original 12 classes when adding up to 4 new keywords
- TinyOL and LwF variants achieve highest accuracy (>95%) when adding a single new keyword
- Batch-based methods (TinyOL-batch, LwF-batch) become more robust as class count increases
- CL backpropagation requires only 400-700 FLOPs, <0.001% of the 291 MFLOPs forward pass
- 71-fold energy reduction achieved through BNN quantization with only 2% accuracy drop

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Freezing the BNN backbone and training only the final fully-connected layer enables efficient continual learning while preserving previously learned representations.
- **Mechanism:** The pre-trained BNN feature extractor (convolutional layers) remains static during CL, converting raw audio into stable embeddings. Only the classification layer weights are updated via backpropagation using categorical cross-entropy loss. This constrains the optimization space, preventing representation drift that causes catastrophic forgetting.
- **Core assumption:** The frozen backbone features are sufficiently discriminative for both original and new keywords without adaptation.
- **Evidence anchors:**
  - [abstract] "framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords"
  - [Section III-C] "During CL, the other layers of the KWS model are frozen"
  - [corpus] AnalyticKWS paper similarly uses frozen backbone with analytic classifier updates, suggesting this pattern generalizes across CL-KWS approaches
- **Break condition:** If new keywords require significantly different acoustic features (e.g., tonal languages, very short utterances), frozen features may lack discriminative power.

### Mechanism 2
- **Claim:** Sample-based online learning (TinyOL, LwF) outperforms batch-based methods when adding few classes with limited data, while batch-based methods become more robust as class count increases.
- **Mechanism:** Sample-based algorithms update weights immediately after each inference using SGD, enabling rapid adaptation with minimal data. Batch methods accumulate gradients over 32 samples before updating, providing more stable but slower convergence. With 1-2 new classes, single-sample updates exploit limited data efficiently; with 4+ classes, batch normalization of gradients reduces variance across heterogeneous new keywords.
- **Core assumption:** Training data arrives sequentially and labels are available during the CL phase.
- **Evidence anchors:**
  - [Section IV-A] "algorithms based on individual samples can achieve higher accuracies in scenarios with one new class, but quickly decrease in performance when more new classes are added"
  - [Section IV-B] "batch-based algorithms demand more data in practical applications"
  - [corpus] Limited external validation—neighbor papers focus on different CL scenarios (domain adaptation, few-shot learning) rather than class-incremental sample efficiency
- **Break condition:** Batch methods fail when fewer than ~2048 training samples are available; sample methods become unstable when new classes have highly imbalanced representation.

### Mechanism 3
- **Claim:** BNN quantization provides 71× energy reduction while maintaining feature quality sufficient for CL, because binarization primarily affects precision rather than representational capacity.
- **Mechanism:** Binary weights (+1/-1) replace floating-point operations with XNOR and popcount operations. The first and last convolutional layers remain full-precision to preserve input fidelity and output granularity. The 2% accuracy drop from binarization represents information loss that is tolerable because the classification layer can still learn linearly separable boundaries in the binary embedding space.
- **Core assumption:** The BNN feature space remains linearly separable for new keyword classes despite quantization noise.
- **Evidence anchors:**
  - [Section II-A] "71-fold reduction in overall energy cost was achieved, while the accuracy dropped only by 2%"
  - [Section III-B] "We keep full-precision inputs and weights in the first and last convolutional layer... whereas the other convolutional layers are binarized"
  - [corpus] Neighbor paper on MCUX947 implementation confirms quantized CNN viability for KWS on MCUs, but does not validate BNN-specific claims
- **Break condition:** If new keywords have subtle acoustic distinctions (e.g., "forward" vs. "foreword"), binarization may eliminate discriminative features.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The core problem CL solves—neural networks overwrite previous task knowledge when learning new tasks. Understanding this explains why only the last layer is trained and why methods like LwF use copy layers.
  - **Quick check question:** If you trained a 12-class KWS model on 4 new keywords without any CL technique, what accuracy would you expect on the original 12 classes?

- **Concept: Binary Neural Network Arithmetic**
  - **Why needed here:** Understanding that BNNs replace multiply-accumulate with XNOR-popcount operations explains the dramatic efficiency gains and why computational overhead of CL backpropagation becomes negligible.
  - **Quick check question:** How many FLOPs does a single binary convolution operation require compared to a standard convolution with the same kernel size?

- **Concept: Online vs. Batch Gradient Descent**
  - **Why needed here:** The paper's central trade-off—sample-based methods (online) vs. batch-based methods—directly determines data requirements and convergence stability.
  - **Quick check question:** With a batch size of 32 and learning rate 0.05, how does the effective learning signal differ from single-sample updates with the same learning rate?

## Architecture Onboarding

- **Component map:**
  Audio Input (16kHz, 1s) → Log-mel Feature Extraction (64 filters, 25ms window, 10ms hop) → [FP32] Conv Layer 1 (frozen) → [Binary] Conv Layers 2-N + BatchNorm + ReLU (frozen) → Global Average Pooling → N-dimensional embedding → [FP32] Fully Connected Layer (TRAINABLE during CL) → 16 classes

- **Critical path:**
  1. Pre-train BNN backbone on 12-class subset (40% of data sufficient for 91% accuracy)
  2. Initialize new output neurons for additional classes
  3. During CL: freeze all layers except final FC layer
  4. Stream mixed data (new classes + original classes) with labels
  5. Update only FC weights via backpropagation using selected CL algorithm

- **Design tradeoffs:**
  | Decision | Option A | Option B | Guidance |
  |----------|----------|----------|----------|
  | CL algorithm | TinyOL/LwF (sample-based) | TinyOL-batch/CWR | Use sample-based for 1-2 new classes with <2048 samples; batch for 3-4+ classes with sufficient data |
  | Pre-training data | 40% subset | Full dataset | 40% sufficient for initial model; reserve remainder for CL |
  | Learning rate | 0.05 (paper default) | Lower (0.01) | 0.05 works for FC-only training; lower if instability observed |

- **Failure signatures:**
  - **Original class accuracy drops below 85%:** Likely using LwF with batches on too few samples; switch to TinyOL or increase data
  - **New class accuracy stuck near random (6.25% for 16 classes):** Learning rate too low or feature extractor not producing discriminative embeddings for new keywords
  - **Training loss oscillates wildly:** Batch size mismatch with data variability; reduce batch size or use sample-based method

- **First 3 experiments:**
  1. **Baseline replication:** Pre-train BNN on 12 classes, add 1 new class using TinyOL with 2048 samples. Target: >95% accuracy on original classes, >90% on new class.
  2. **Data sensitivity sweep:** Using 4 new classes, vary CL training samples from 64 to 16384. Plot accuracy curves for TinyOL vs. TinyOL-batch to identify crossover point.
  3. **Computational validation:** Profile FLOPs for forward pass (~291 MFLOPs) vs. CL backpropagation (~400-700 FLOPs). Confirm <0.001% overhead on target MCU.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this BNN-based CL framework be effectively deployed on resource-constrained Microcontroller Units (MCUs) with real-time capabilities?
- **Basis in paper:** [explicit] The authors state in the conclusion that "deployment of the proposed approach on resource-constrained MCUs should be evaluated."
- **Why unresolved:** The current study relies on simulation and theoretical FLOP calculations rather than hardware implementation.
- **Evidence:** Real-world measurements of latency, peak RAM usage, and energy consumption during the learning phase on a standard tinyML MCU.

### Open Question 2
- **Question:** How does utilizing the same training data across multiple epochs affect the stability and accuracy of the resource-efficient CL algorithms?
- **Basis in paper:** [explicit] The conclusion suggests that "the utilization of the same data in multiple epochs might be explored."
- **Why unresolved:** The current setup assumes a stream of data; it is unknown if revisiting samples improves retention or causes overfitting in this BNN architecture.
- **Evidence:** Ablation studies comparing single-pass training against multi-epoch training for the CL phase using the Google Speech Commands dataset.

### Open Question 3
- **Question:** Does freezing the majority of the BNN backbone limit the model's ability to learn acoustically distinct new keywords compared to fine-tuning earlier layers?
- **Basis in paper:** [inferred] The methodology freezes all layers except the last fully connected layer to maintain efficiency.
- **Why unresolved:** While efficient, freezing the feature extractor may lead to "capacity saturation" where the fixed binary features cannot represent new complex keywords effectively.
- **Evidence:** Comparative analysis of class accuracy when unfreezing the final convolutional layer versus the current fully frozen backbone approach.

## Limitations
- The study is limited to adding new classes to pre-trained models, not adapting to distribution shifts or label noise in original classes
- Evaluation is confined to Google Speech Commands V2 dataset, which may not generalize to tonal languages or highly confusable keywords
- The specific BNN architecture from reference [9] requires external verification for exact implementation details

## Confidence
- **Core claim (CL on BNNs works):** High - empirically validated across 7 algorithms with systematic sensitivity analysis
- **Computational overhead claim:** High - FLOP calculations are straightforward and verifiable
- **Scalability beyond 4 classes:** Medium - limited testing prevents generalization to larger class additions

## Next Checks
1. Test with highly confusable keyword pairs ("forward" vs. "four") to verify binarization doesn't eliminate discriminative features
2. Evaluate CL performance with <2048 training samples to confirm sample-based algorithm advantages hold at the margin
3. Profile actual MCU execution to validate <0.001% computational overhead claim under real-time inference constraints