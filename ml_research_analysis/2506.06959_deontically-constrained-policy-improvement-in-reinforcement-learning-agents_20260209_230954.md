---
ver: rpa2
title: Deontically Constrained Policy Improvement in Reinforcement Learning Agents
arxiv_id: '2506.06959'
source_url: https://arxiv.org/abs/2506.06959
tags:
- policy
- action
- algorithm
- stit
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a policy improvement algorithm for reinforcement\
  \ learning agents subject to deontic constraints expressed in probabilistic computation\
  \ tree logic (PCTL). The algorithm maximizes the agent\u2019s mission utility while\
  \ ensuring compliance with normative constraints by integrating PCTL model checking\
  \ into each policy update step."
---

# Deontically Constrained Policy Improvement in Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2506.06959
- Source URL: https://arxiv.org/abs/2506.06959
- Reference count: 37
- Primary result: Policy improvement algorithm for RL agents with PCTL deontic constraints, converging to locally optimal policies that satisfy constraints and maximize utility

## Executive Summary
This paper presents a policy improvement algorithm for reinforcement learning agents subject to deontic constraints expressed in probabilistic computation tree logic (PCTL). The algorithm maximizes mission utility while ensuring compliance with normative constraints by integrating PCTL model checking into each policy update step. It computes reachability probabilities to identify valid actions that maintain constraint satisfaction, then applies epsilon-greedy selection to balance exploration and exploitation. Experimental results demonstrate that controlled exploration enables escape from local optima to reach globally optimal solutions, while epsilon = 0 converges to local optima.

## Method Summary
The algorithm computes reachability probabilities x_s under the current policy, then evaluates Y(s,a) = Σ T(s,a,s') · x_s' for each candidate action. Only actions where Y(s,a) ≥ λ (the probability threshold from the PCTL formula) are retained in the valid action set C(s). Value updates follow standard Bellman backup but are restricted to max over C(s) rather than all A(s). The method converges to a locally optimal policy that satisfies the PCTL formula and maximizes utility within the feasible policy space.

## Key Results
- Algorithm converges to locally optimal policies satisfying PCTL constraints P≥λ(Φ U Ψ)
- Controlled exploration with ε > 0 enables escape from local optima to reach globally optimal solutions
- Epsilon = 0 converges to local optima on test MDPs
- Experimental validation on sample MDPs demonstrates effectiveness of approach

## Why This Works (Mechanism)

### Mechanism 1
PCTL model checking integrated into policy improvement enforces logical constraints at each action selection step. At each state, the algorithm computes reachability probabilities x_s under current policy, then evaluates Y(s,a) = Σ T(s,a,s') · x_s' for each candidate action. Only actions where Y(s,a) ≥ λ are retained in valid action set C(s). This works because PCTL formulas of the form P≥λ(Φ U Ψ) allow computation via linear systems rather than full automata construction.

### Mechanism 2
Constraining action selection to only constraint-preserving actions guarantees feasibility preservation while allowing utility improvement. By initializing with a feasible policy π₀ and only permitting actions from C(s) at each update, the algorithm ensures every intermediate policy remains feasible. Value updates follow standard Bellman backup but are restricted to max over C(s) rather than all A(s). This works because the PCTL formula structure allows linear system solutions for reachability probabilities.

### Mechanism 3
Epsilon-greedy exploration with ε > 0 enables escape from local optima to reach globally optimal constrained policies. With probability ε, select action uniformly from C(s) regardless of Q-value; with probability 1-ε, select greedily. This allows temporary utility decreases that may unlock transitions to higher-utility regions of the constrained policy space. The exploration-exploitation tradeoff is empirically tuned to ε ∈ [0.1, 0.4].

## Foundational Learning

- **PCTL Syntax and Semantics**: The constraint language; must understand P≥λ(Φ U Ψ) to specify constraints correctly. Quick check: Can you write a PCTL formula stating "reach goal with probability ≥0.9 while avoiding hazard"?
- **MDP Value Functions and Policy Improvement**: The base algorithm extends classical policy improvement; understanding V^π(s) and Q(s,a) is essential. Quick check: Why does standard policy improvement converge to global optimum without constraints?
- **Reachability Probability Computation in Markov Chains**: Algorithm requires solving linear systems for x_s probabilities under fixed policies. Quick check: Given a Markov chain and target states, how do you compute probability of eventual reach from each state?

## Architecture Onboarding

- **Component map**: PCTL parser -> Prob0/Prob1 modules -> Reachability solver -> Action filter -> Value estimator -> Epsilon-greedy selector
- **Critical path**: 1) Parse PCTL formula → extract λ, Φ, Ψ; 2) Compute Sno, Syes via Prob0/Prob1; 3) Initialize feasible policy; 4) Iterate: solve linear system → filter actions → select via ε-greedy → update policy; 5) Convergence check: policy unchanged over full state sweep
- **Design tradeoffs**: ε = 0: Guaranteed convergence to local optimum, but may miss global; ε > 0: Potential to find global optimum, but no finite-step guarantee; Formula complexity: Only P≥λ(Φ U Ψ) handled directly; nested formulas require recursive labeling
- **Failure signatures**: Empty C(s) at any state → infeasible constraint or poor initialization; Oscillating policy without convergence → ε too high; Convergence to clearly suboptimal policy → ε too low or bad initial policy; Runtime explosion → large state space with many iterations
- **First 3 experiments**: 1) Sanity check: Implement on MDP1 with P≥0.4(F s3); verify convergence to π* = {s0→a1, s1→a2, s2→a4, s3→a4} with V*(s0) = 71.05; 2) Local optimum trap: Run on MDP2 with P≥0.3(F s2) using ε = 0; confirm convergence to suboptimal V(s0) ≈ 26.03; 3) Exploration rescue: Same MDP2 setup with ε ∈ {0.1, 0.2, 0.4}; measure frequency of reaching global optimum V*(s0) = 78.71 over 100 trials each

## Open Questions the Paper Calls Out

- **Global optimality guarantee**: Extending framework to guarantee global optimality remains open challenge. The ε-greedy exploration only provides asymptotic global optimality with probability 1, which amounts to little more than brute force approach.
- **POMDP extension**: Another avenue for future work is to extend methods to partially observable MDPs. POMDPs introduce belief states and partial observability, making PCTL model checking and constraint satisfaction verification fundamentally more complex.
- **Explicit deontic reward function**: The paper notes the bi-level structure where satisfying ψ means acting optimally according to an implicit Rd, but states "we do not actually have Rd" and discusses circularity issues. It also cites impossibility results for Markovian rewards capturing temporal logic properties.

## Limitations
- Method limited to PCTL formulas of the form P≥λ(Φ U Ψ); nested probability operators or complex temporal operators require recursive labeling
- Requires feasible initial policy; external search needed when no obvious feasible policy exists
- No theoretical guarantee of finite-step convergence to global optimum; epsilon-greedy exploration provides only asymptotic guarantees

## Confidence

- **High confidence**: Core mechanism of integrating PCTL model checking into policy improvement (Mechanism 1) is well-specified with clear linear system formulation and filtering procedure. Preservation of feasibility through constraint-preserving action selection (Mechanism 2) follows logically from construction.
- **Medium confidence**: Local optimum escape mechanism via epsilon-greedy exploration (Mechanism 3) is supported by empirical evidence but lacks theoretical convergence guarantees. The optimal ε range appears sensitive to problem structure.
- **Low confidence**: Practical aspects including initialization procedure, convergence thresholds, and handling of edge cases (empty valid action sets) are underspecified and could significantly impact reproducibility.

## Next Checks

1. **Sensitivity analysis**: Systematically vary ε across [0, 0.5] on both MDP1 and MDP2 to quantify the exploration-exploitation tradeoff and identify optimal ε ranges for different problem characteristics.
2. **Initialization robustness**: Test the algorithm with multiple random feasible initial policies to assess sensitivity to initialization and identify whether certain initial policies consistently lead to better optima.
3. **Formula complexity scaling**: Extend beyond simple reachability to nested PCTL formulas (e.g., P≥λ(Φ U P≥μ(Ψ U Z))) to evaluate the recursive labeling approach and measure computational overhead.