---
ver: rpa2
title: 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation'
arxiv_id: '2510.05156'
source_url: https://arxiv.org/abs/2510.05156
tags:
- agent
- policy
- code
- safety
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of ensuring safety and security
  for LLM-based autonomous agents in sensitive domains like healthcare, where agents
  risk deviating from user objectives, violating data policies, or being compromised
  by attacks. The proposed VeriGuard framework uses a dual-stage architecture to provide
  formal safety guarantees: an offline stage that synthesizes a behavioral policy
  from natural language specifications, validates it through testing and formal verification,
  and an online stage that monitors and validates each agent action against the pre-verified
  policy before execution.'
---

# VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation

## Quick Facts
- **arXiv ID**: 2510.05156
- **Source URL**: https://arxiv.org/abs/2510.05156
- **Reference count**: 40
- **Primary result**: Dual-stage framework achieves near-zero attack success rates while maintaining high task completion on LLM agent safety benchmarks

## Executive Summary
VeriGuard addresses the critical challenge of ensuring safety and security for LLM-based autonomous agents operating in sensitive domains. The framework provides formal safety guarantees through a dual-stage architecture: an offline stage that synthesizes and verifies behavioral policies from natural language specifications, and an online stage that monitors and validates each agent action before execution. Experiments on ASB, EICU-AC, and Mind2Web-SC benchmarks demonstrate VeriGuard's effectiveness, achieving near-zero attack success rates while maintaining high task success rates across multiple agent configurations.

## Method Summary
VeriGuard employs a two-stage approach to agent safety. The offline stage uses an LLM to generate Python policy code and formal constraints from natural language specifications, followed by validation, testing with PyTest, and formal verification using the Nagini verifier. This creates an iterative refinement loop where counterexamples guide policy improvement until verification passes. The online stage then monitors agent actions at runtime, intercepting tool calls and environmental interactions. A lightweight Python function validates each action against the pre-verified policy before execution, with flexible enforcement strategies including collaborative re-planning and tool halting to balance safety with task completion.

## Key Results
- On ASB benchmark: Reduces attack success rate to 0% while maintaining 63.3% task success with Gemini-2.5-Flash
- On EICU-AC: Achieves 100% accuracy for ICU access control over 10 databases
- Outperforms baselines with hybrid CRP+TEH enforcement achieving 0.1% average ASR and 63.6% average TSR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement with counterexamples transforms probabilistic code generation into verifiable safety policies
- Mechanism: LLM generates policy code and formal constraints (pre/post-conditions). Nagini attempts Hoare triple proof `{Pre} code {Post}`. If verification fails, Nagini returns counterexample that guides LLM to patch logic
- Core assumption: LLM can interpret formal verification error messages and translate them into valid Python fixes
- Evidence: Abstract describes "iterative refinement loop... verifier provides counterexamples that guide the refinement"; section 3.2.2 details counterexample feedback process

### Mechanism 2
- Claim: Separating heavy verification from runtime execution ensures safety without prohibitive latency
- Mechanism: Offline creates "correct-by-construction" policy. Runtime only executes deterministic Python function check, not complex reasoning for every action
- Core assumption: Offline policy generalizes to cover all necessary runtime states
- Evidence: Abstract states "separation of exhaustive offline validation from lightweight online monitoring allows formal guarantees to be practically applied"; section 3.3 describes online monitoring integration

### Mechanism 3
- Claim: Hybrid enforcement strategies optimize safety-utility trade-off better than binary blocking
- Mechanism: Instead of blocking or terminating, system attempts agent "re-planning" for safe path. If re-plan fails, specific tool execution is halted
- Core assumption: Underlying LLM agent capable of dynamic re-planning based on error messages
- Evidence: Section 5.2 shows "hybrid CRP + TEH approach yields optimal results... achieving both near-zero average ASR (0.1%) and highest average TSR (63.6%)"

## Foundational Learning

- **Hoare Logic / Program Verification**
  - Why needed: Core value proposition relies on proving `{Pre} Program {Post}` conditions to define "safety" mathematically
  - Quick check: Can you explain why a verifier needs both pre-conditions and post-conditions to prove a program is correct?

- **LLM Agents (ReAct / Tool Use)**
  - Why needed: VeriGuard intercepts tool calls and environmental actions, not text generation. Must understand difference between "text generation" and "agentic action"
  - Quick check: In a ReAct loop, does VeriGuard intercept the "Thought" step or the "Action/Tool" step?

- **Runtime Monitoring vs. Static Analysis**
  - Why needed: VeriGuard bridges these - uses static analysis (verification) offline to build shield for runtime monitoring online
  - Quick check: Why can't VeriGuard simply run the formal verifier at runtime for every single action?

## Architecture Onboarding

- **Component map**: Policy Generator (LLM) -> Refinement Loop (Validation -> Testing -> Verification) -> Runtime Monitor -> Argument Extractor (LLM) -> Enforcer
- **Critical path**: Refinement Loop's Verification step is bottleneck. If Nagini cannot verify generated code, human must intervene or system defaults to safe state
- **Design tradeoffs**:
  - Soundness vs. Automation: Relies on LLM to generate specification. Wrong spec means valid proof but void safety guarantee
  - Latency vs. Granularity: "Collaborative Re-planning" preserves utility but adds LLM inference latency loops. "Task Termination" is instant but destroys utility
- **Failure signatures**:
  - Argument Mapping Failure: Runtime LLM fails to extract parameters, leading to policy error
  - Over-constraint: Formal verifier proves technically correct but impossible-to-satisfy policy (high False Positive rate)
- **First 3 experiments**:
  1. Hello World Guard: Implement "Do not delete files" policy for file-management agent through full Validation-Test-Verify loop
  2. Enforcement Strategy A/B: Compare "Action Blocking" vs. "Collaborative Re-planning" on benign task, measure TSR drop
  3. Adversarial Injection: Attempt prompt injection ("Ignore previous rules...") against deployed runtime monitor to verify "Formal" guarantee holds

## Open Questions the Paper Calls Out

- Can the formal verification process scale efficiently for complex, real-world agentic tasks without prohibitive computational costs?
  - Basis: Conclusion identifies "scalability and efficiency of the formal verification process" as key future research
  - Why unresolved: Formal verification is computationally intensive; paper lacks detailed analysis of latency/resource consumption in large-scale environments
  - Evidence needed: Benchmarks measuring verification time and throughput on larger state spaces and complex workflows

- Is it possible to autonomously generate accurate safety specifications without manual user validation?
  - Basis: Authors list "autonomous generation of safety specifications themselves" as promising future area
  - Why unresolved: Current framework relies on validation phase to resolve natural language ambiguities, implying human-in-the-loop dependency
  - Evidence needed: System demonstrating high-fidelity translation of natural language into formal constraints with provable semantic alignment

- How can the system guarantee semantic safety when formal constraints are generated by error-prone LLMs?
  - Basis: Section 5.3 states soundness relies on manual validation because LLM-based constraint generation is "inherently non-deterministic and susceptible to error"
  - Why unresolved: Verifier proves code adheres to constraints, but flawed constraint (garbage in) results in verified but unsafe policy (garbage out)
  - Evidence needed: Automated mechanism or formal method to validate generated logical constraints match user's natural language safety intent

## Limitations

- Reliance on LLMs for both policy generation and runtime argument extraction creates potential failure points in formal verification coverage
- Framework's effectiveness depends on complete specification coverage - critical safety constraints omitted from S cannot be detected
- Nagini verifier has limitations handling complex Python constructs, restricting class of policies that can be formally verified

## Confidence

- **High confidence**: Offline-online separation mechanism and hybrid enforcement strategy are well-supported by experimental results showing near-zero ASR with maintained TSR on multiple benchmarks
- **Medium confidence**: Iterative refinement mechanism showing LLMs can consistently translate verification counterexamples into valid code fixes, based on described methodology but limited empirical validation
- **Low confidence**: Claim of achieving "formal guarantees" is qualified by paper's acknowledgment that specification quality and LLM reliability create gaps in actual safety coverage

## Next Checks

1. **Counterexample Translation Robustness**: Systematically test refinement loop with intentionally buggy policies to measure LLM's success rate in fixing Nagini-reported counterexamples across multiple failure modes
2. **Runtime Argument Extraction Accuracy**: Deploy VeriGuard with simple agent (e.g., file organizer) and measure success rate of LLM-based argument extractor handling complex real-world tool calls with nested parameters
3. **Specification Completeness Stress Test**: Create benchmark with gradually increasing specification coverage gaps and measure how quickly ASR degrades as critical constraints are removed from policy specification S