---
ver: rpa2
title: 'SEAL: Self-Evolving Agentic Learning for Conversational Question Answering
  over Knowledge Graphs'
arxiv_id: '2512.04868'
source_url: https://arxiv.org/abs/2512.04868
tags:
- question
- join
- s-expression
- knowledge
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEAL, a two-stage semantic parsing framework
  for conversational question answering over knowledge graphs. It decomposes complex
  query generation into S-expression core extraction followed by agent-driven calibration
  and template-based composition, significantly improving structural accuracy and
  computational efficiency.
---

# SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs

## Quick Facts
- arXiv ID: 2512.04868
- Source URL: https://arxiv.org/abs/2512.04868
- Authors: Hao Wang; Jialun Zhong; Changcheng Wang; Zhujun Nie; Zheng Li; Shunyu Yao; Yanzeng Li; Xinchi Li
- Reference count: 40
- One-line primary result: State-of-the-art performance on SPICE benchmark with 66.83% accuracy, especially excelling at multi-hop reasoning, comparison, and aggregation tasks.

## Executive Summary
SEAL introduces a two-stage semantic parsing framework for conversational question answering over knowledge graphs. It decomposes complex query generation into core extraction followed by agent-driven calibration and template-based composition, significantly improving structural accuracy and computational efficiency. The system also incorporates a self-evolving mechanism with local/global memory and reflection to continuously adapt from dialog history and execution feedback without retraining.

## Method Summary
SEAL uses a two-stage semantic parsing approach where an LLM first generates an S-expression core containing essential semantics, which is then refined by an agentic calibration module that corrects syntax and aligns entities/relations with the knowledge graph using single-candidate linking. A question-type prediction module selects an appropriate logical template, and a replacement plan fills placeholders with calibrated cores, functions, and constants. The system incorporates local memory for dialog context and global memory for storing validated S-expressions, enabling continuous adaptation through a reflection module that validates and serializes successful logical forms.

## Key Results
- Achieves 66.83% overall accuracy on SPICE benchmark
- Outperforms baselines by 2.5-4.0% on complex queries requiring multi-hop reasoning, comparison, and aggregation
- Maintains or improves performance as dialog context grows, while ablated versions degrade

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Decomposition for Complex Logical Form Generation
Decomposing complex logical form generation into core extraction followed by template-based composition reduces error accumulation and improves structural fidelity. The LLM generates semantically rich but structurally simpler cores, while the template composition handles the complex syntax.

### Mechanism 2: Agentic Calibration for KG Alignment
An agent module corrects syntactic errors and aligns entities/relations with the knowledge graph more efficiently than traditional candidate-based linking methods by using a single-best candidate approach based on embedding similarity.

### Mechanism 3: Self-Evolution via Memory and Reflection
Integrating local and global memory with reflection enables the system to adapt to new query patterns in dialog history without explicit retraining by validating and storing successful logical forms as reusable templates.

## Foundational Learning

**Concept: S-expressions as a Logical Form**
- Why needed here: The entire SEAL framework is built around generating and manipulating S-expressions to represent queries over the knowledge graph
- Quick check question: Can you translate "Who is the mayor of Paris?" into a simple S-expression like `(JOIN (R mayor_of) Paris)`?

**Concept: Knowledge Graph Linking**
- Why needed here: Calibration agent links natural language terms to canonical IDs in the knowledge graph
- Quick check question: How would you find the canonical ID for "born in" in a knowledge graph?

**Concept: Agentic Calibration**
- Why needed here: Distinguishes SEAL from end-to-end LLM generation by refining drafts based on KG feedback
- Quick check question: What's the difference between end-to-end LLM generation vs. agent refinement?

## Architecture Onboarding

**Component map:**
Input -> Core Generation -> Calibration -> Type Prediction -> Template Selection -> S-expression -> Execution
(with self-evolving loop: Execution -> Reflection -> Global Memory)

**Critical path:** Input -> Core Generation -> Calibration -> Type Prediction -> Template Selection -> S-expression -> Execution. The self-evolving loop is secondary but critical for long-term performance.

**Design tradeoffs:**
- Top-1 vs. Top-K Linking: SEAL chooses single candidate for efficiency and noise reduction, sacrificing potential recall
- Template Library: System flexibility bounded by pre-defined templates, though LLM can sometimes synthesize beyond them

**Failure signatures:**
- Empty Query Results: Most common failure mode, triggers correction loop
- Coreference Resolution Failure: Incomplete input leads to wrong S-expression
- Type Prediction Error: Wrong template selection causes composition failure

**First 3 experiments:**
1. Core Extraction Only: Ablate template composition, measure structural accuracy of generated cores
2. Top-1 vs. Top-K Ablation: Compare performance and SPARQL query count between single and multi-candidate linking
3. Memory Ablation: Run full system with/without global memory over long dialog sequence, plot F1 vs. dialog turn

## Open Questions the Paper Calls Out

### Open Question 1
How can agent-driven calibration distinguish between structurally erroneous queries and legitimate empty-result queries? The current approach may revise valid empty queries, reducing precision.

### Open Question 2
Can computational overhead be reduced by replacing the general-purpose LLM with smaller specialized models for subtasks like type prediction or coreference resolution?

### Open Question 3
Can S-expression to SPARQL transformation be generalized to support diverse knowledge graph benchmarks without manual adaptation of syntactic patterns?

## Limitations
- Template library completeness fundamentally constrains ability to handle novel query patterns
- Single-candidate linking may systematically fail on ambiguous surface forms
- Self-evolution benefits demonstrated only on SPICE benchmark, not tested for distribution shift

## Confidence
- **High**: Two-stage decomposition improves structural accuracy over end-to-end generation (Table 3)
- **Medium**: Computational efficiency gains from single-candidate linking plausible but not directly measured
- **Low**: Self-evolving mechanism's continuous adaptation capability demonstrated only on SPICE benchmark

## Next Checks
1. Template Library Ablation: Systematically remove subsets of template library and measure performance degradation across all question types
2. Multi-Candidate Linking Baseline: Implement k=3 candidate linking and compare accuracy and efficiency against single-candidate approach
3. Long-Term Memory Stability: Run SEAL for 100+ dialog turns, track diversity of logical forms added to memory, analyze accumulation of redundant patterns or degradation