---
ver: rpa2
title: Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models
  through Scalable Attribution Graph Decomposition
arxiv_id: '2601.12879'
source_url: https://arxiv.org/abs/2601.12879
tags:
- circuit
- circuits
- across
- hierarchical
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Hierarchical Attribution Graph Decomposition
  (HAGD), a framework for extracting sparse computational circuits from billion-parameter
  language models. The method reduces circuit discovery complexity from exponential
  O(2^n) to polynomial O(n^2 log n) through multi-resolution abstraction hierarchies
  and differentiable circuit search.
---

# Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition

## Quick Facts
- arXiv ID: 2601.12879
- Source URL: https://arxiv.org/abs/2601.12879
- Reference count: 35
- Method extracts sparse computational circuits from models 117M-70B parameters with behavioral preservation 82-97%

## Executive Summary
This paper introduces Hierarchical Attribution Graph Decomposition (HAGD), a framework that extracts sparse computational circuits from billion-parameter language models. The method reduces circuit discovery complexity from exponential O(2^n) to polynomial O(n² log n) through multi-resolution abstraction hierarchies and differentiable circuit search. It successfully extracts circuits from models ranging from 117M to 70B parameters with behavioral preservation of 82-97% on algorithmic tasks and 74-88% on natural language benchmarks.

## Method Summary
HAGD extracts circuits through a pipeline: (1) Train cross-layer transcoders using sparse autoencoders to decompose polysemantic activations into interpretable features; (2) Build attribution graphs using gradient-based edge weights; (3) Apply hierarchical decomposition via spectral clustering to create supernode hierarchies; (4) Use graph neural networks to guide circuit search through supernodes; (5) Validate circuits through causal intervention protocols including ablation and sufficiency tests.

## Key Results
- Complexity reduction: Circuit discovery complexity reduced from O(2^n) to O(n² log n)
- Behavioral preservation: 82-97% on algorithmic tasks, 74-88% on natural language benchmarks
- Scale achievement: Successfully extracts circuits from 117M to 70B parameter models
- Cross-architecture transfer: 52-82% structural similarity between circuits across different model families

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Complexity Reduction
- Claim: Multi-resolution abstraction reduces circuit search complexity from O(2^n) to O(n² log n).
- Mechanism: Attribution graphs are recursively clustered via spectral decomposition into supernode hierarchies. At each level r, the normalized Laplacian L^(r) partitions vertices to minimize inter-cluster edges. Search traverses O(b) supernodes per level across O(log_b n) levels, avoiding exhaustive subgraph enumeration.
- Core assumption: Neural computation exhibits compositional structure where high-level algorithms decompose modularly.
- Evidence anchors:
  - [abstract] "reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n² log n) through multi-resolution abstraction hierarchies"
  - [section III.D] Theorem 1 proof sketch: "R = O(log_b n) levels, total supernode decisions O(b log_b n) = O(n)"
  - [corpus] Neighbor paper "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework" confirms faithfulness-speed tradeoffs in circuit discovery but doesn't replicate hierarchical approach.
- Break condition: If circuit structure is highly distributed without modular clustering, spectral partitioning yields uninformative supernodes and search degrades to near-exhaustive.

### Mechanism 2: Cross-Layer Transcoder Monosemantic Extraction
- Claim: Sparse autoencoder-style transcoders decompose polysemantic activations into interpretable features with cross-layer predictive structure.
- Mechanism: Encoder f^ℓ = TopK(ReLU(W^E h^ℓ + b^E), k) produces sparse coefficients. Cross-layer prediction head P^{ℓ→ℓ+1} models feature dependencies. Loss combines reconstruction + prediction + L1 sparsity (Eq. 4).
- Core assumption: TopK sparsity forces monosemantic decomposition; residual variance (15-20%) is noise rather than structured computation.
- Evidence anchors:
  - [abstract] "integrates cross-layer transcoders for monosemantic feature extraction"
  - [section III.A] Eq. 4 composite loss: reconstruction ‖h^ℓ − D^ℓf^ℓ‖² + prediction ‖f^{ℓ+1} − f̂^{ℓ+1}‖² + sparsity ‖f^ℓ‖₁
  - [section VI.C] "Reconstruction Dark Matter: The 15-20% unexplained variance... represents a fundamental concern"
  - [corpus] "Language Model Circuits Are Sparse in the Neuron Basis" (FMR 0.62) supports sparse decomposition premise but highlights neuron-basis limitations.
- Break condition: If critical computation resides in the 15-20% reconstruction residual, circuits will miss essential pathways.

### Mechanism 3: GNN Meta-Learning for Topology Prediction
- Claim: Graph attention networks trained on small-model circuits predict circuit membership at scale.
- Mechanism: GAT-style message passing (Eq. 7-8) processes hierarchical supernode representations. Output layer predicts membership probability per vertex. Meta-learning across circuit tasks enables transfer.
- Core assumption: Circuit structure generalizes across scales; ground-truth labels from small models transfer.
- Evidence anchors:
  - [abstract] "graph neural network meta-learning for topology prediction"
  - [section III.E] "Training the GNN requires ground-truth circuit labels, obtained through exhaustive search on small models or through manual annotation"
  - [section VI.C] "GNN Training Requirements: requires ground-truth circuit labels... limits applicability"
  - [corpus] Weak direct evidence—neighbor papers focus on attribution/pruning, not learned topology prediction.
- Break condition: If 70B circuits have qualitatively different structure than small-model training data, GNN predictions misguide search.

## Foundational Learning

- **Superposition & Polysemanticity**
  - Why needed here: Motivates sparse decomposition; explains why raw neurons aren't interpretable.
  - Quick check question: Can you explain why a single neuron might activate for both "dog" and "car"?

- **Attribution Graphs & Causal Intervention**
  - Why needed here: Core abstraction for circuits; validation requires ablation/patching logic.
  - Quick check question: What's the difference between correlation and causal attribution in circuit validation?

- **Spectral Clustering & Graph Laplacians**
  - Why needed here: Hierarchical decomposition uses Laplacian eigenvectors for partitioning.
  - Quick check question: How does the normalized Laplacian encode graph connectivity structure?

## Architecture Onboarding

- **Component map:** Pre-trained LM → Cross-layer Transcoders (per-layer, trained on 100M tokens) → Attribution Graph (gradient-based edges) → Hierarchical Decomposition (spectral clustering, R levels) → GNN Search (meta-learned traversal) → Causal Validation (ablation + sufficiency test)

- **Critical path:** Transcoder training quality determines feature monosemanticity; GNN training data determines search accuracy. Both bottlenecks have no cheap shortcuts.

- **Design tradeoffs:**
  - Sparsity k (32-128): Higher k = better reconstruction but less interpretable features.
  - Dictionary expansion factor (m/d = 8): Larger = more features but higher memory/compute.
  - Hierarchy branching factor b: Affects search granularity vs. speed.

- **Failure signatures:**
  - ACDC OOM on ≥1.4B models (Table II) → use hierarchical method.
  - Behavioral preservation <75% → likely missing attention circuits or excessive dark matter.
  - Low modularity scores (0.55-0.61 for HellaSwag) → expected for distributed NLP tasks.
  - Cross-family transfer <50% → architecture-specific structure dominant.

- **First 3 experiments:**
  1. Validate on GPT-2 Small modular arithmetic: should recover ~47 nodes, ~97% preservation (Table II baseline).
  2. Ablate cross-layer prediction head: expect ~5% preservation drop per Table V (0.91→0.84 on modular arith).
  3. Check reconstruction residual: if >25% variance unexplained, increase dictionary size or k before scaling to larger models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What computational information is contained in the 15-20% reconstruction "dark matter" that transcoders fail to capture, and does it represent structured computation or noise?
- Basis in paper: [explicit] The authors explicitly state that "15-20% unexplained variance in transcoder reconstruction represents a fundamental concern" and that "If this residual variance contains structured computation rather than noise, important computational pathways may be invisible to circuit analysis."
- Why unresolved: The current methodology does not characterize what information resides in this unexplained variance or whether it varies systematically across tasks and layers.
- What evidence would resolve it: Systematic analysis of reconstruction residuals across layers and tasks, with probing experiments to determine if structured information correlates with behavioral changes when ablated.

### Open Question 2
- Question: How can attention circuit patterns be formally integrated with MLP transcoder features to enable complete transformer circuit extraction?
- Basis in paper: [explicit] The authors explicitly acknowledge: "Attention circuits are not modeled, leaving significant computational pathways unanalyzed" and note this is "critical for tasks like coreference resolution where attention patterns are central."
- Why unresolved: The current framework focuses exclusively on MLP computations traced through transcoder features, with no explicit modeling of attention patterns or their interactions with MLP layers.
- What evidence would resolve it: An extended framework demonstrating unified attention-MLP circuit extraction, with validation on attention-heavy tasks showing improved behavioral preservation.

### Open Question 3
- Question: Do discovered circuits represent genuine causal computational structure or correlational artifacts, and how can validation circularity be eliminated?
- Basis in paper: [explicit] The authors note "Validation circularity arises when the same ablation experiments used for circuit discovery are subsequently used for validation" and that "Features may appear necessary because their ablation disrupts computation in ways unrelated to the target behavior."
- Why unresolved: Current validation protocols rely on ablation experiments that assume circuit completeness, creating potential circular reasoning.
- What evidence would resolve it: Development of independent validation protocols using held-out behavioral tests, adversarial perturbations, or causal interventions not used during circuit discovery.

## Limitations
- Attention circuit modeling is not included, leaving significant computational pathways unanalyzed
- 15-20% reconstruction dark matter represents unexplained variance that may contain structured computation
- GNN training requires ground-truth circuit labels from exhaustive search or manual annotation, limiting scalability

## Confidence

**High confidence** in the overall framework architecture and mathematical formulation (Theorem 1 proof, Equation derivations)

**Medium confidence** in behavioral preservation claims (82-97% range across benchmarks, though validation methodology limited)

**Low confidence** in cross-architecture transfer results (52-82% similarity, sparse comparison methodology)

## Next Checks

1. **Reconstruction Residual Analysis**: Quantify what computation resides in the 15-20% unexplained variance. Apply targeted intervention on residual components to determine if dark matter represents structured computation or noise.

2. **Cross-Scale Circuit Structure**: Validate whether circuits discovered in 70B models exhibit the same modular clustering properties that enable O(n² log n) search. Test spectral clustering effectiveness directly on large-model attribution graphs.

3. **Architecture Transfer Generalization**: Systematically vary GNN training data (small model families) to measure impact on large-model circuit discovery accuracy. Determine whether architecture-specific GNN training is required for each model family.