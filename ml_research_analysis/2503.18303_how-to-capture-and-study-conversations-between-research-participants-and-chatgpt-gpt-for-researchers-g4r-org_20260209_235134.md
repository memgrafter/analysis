---
ver: rpa2
title: 'How to Capture and Study Conversations Between Research Participants and ChatGPT:
  GPT for Researchers (g4r.org)'
arxiv_id: '2503.18303'
source_url: https://arxiv.org/abs/2503.18303
tags:
- interface
- data
- researcher
- will
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers need standardized tools to study how people interact
  with large language models like ChatGPT, but existing methods are ad hoc and non-reproducible.
  The study introduces GPT for Researchers (G4R), a free website (g4r.org) that allows
  researchers to easily create customized GPT interfaces, integrate them into studies
  (e.g., Qualtrics surveys), and capture all participant-GPT interactions.
---

# How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org)

## Quick Facts
- arXiv ID: 2503.18303
- Source URL: https://arxiv.org/abs/2503.18303
- Reference count: 1
- Primary result: Researchers lack standardized tools for systematically studying people's interactions with large language models, hindering reproducibility and comparability across studies.

## Executive Summary
Researchers need standardized tools to study how people interact with large language models like ChatGPT, but existing methods are ad hoc and non-reproducible. The study introduces GPT for Researchers (G4R), a free website (g4r.org) that allows researchers to easily create customized GPT interfaces, integrate them into studies (e.g., Qualtrics surveys), and capture all participant-GPT interactions. Researchers can customize features like system prompts, participant/GPT labels, and message limits. The tool automatically generates participant IDs and provides message data (messages sent/received, timestamps) for download and merging with survey data. This standardized approach enables reproducible and scalable research on human-AI interactions.

## Method Summary
G4R is a web platform built with PHP, JavaScript, and HTML that provides a standardized interface for capturing participant-ChatGPT conversations. Researchers create an account, configure GPT interfaces with parameters like system prompts, temperature, message limits, and labels, then integrate the interface into Qualtrics surveys via JavaScript snippets. The system automatically generates unique participant IDs, stores all message exchanges with timestamps, and exports data as CSV files for merging with survey responses using the participant ID as a join key.

## Key Results
- G4R provides a standardized, configurable interface that reduces methodological fragmentation in human-AI research
- Automatic participant ID generation enables longitudinal and multi-modal behavioral analysis by linking chat logs with survey responses
- Configurable system prompts and temperature settings allow researchers to control interaction dynamics without modifying underlying code

## Why This Works (Mechanism)

### Mechanism 1
Providing a standardized, configurable interface for participant-LLM interactions reduces methodological fragmentation in human-AI research. G4R acts as an intermediary layer that (1) wraps OpenAI's API with researcher-controllable parameters, (2) injects JavaScript into Qualtrics surveys to render the chat interface, and (3) logs all message exchanges with timestamps to a central data store. This replaces ad hoc solutions (e.g., having participants sign up for ChatGPT mid-study or building custom platforms per research question).

Core assumption: Researchers will adopt a centralized tool rather than continue building bespoke solutions; data standardization improves cross-study comparability.

Evidence anchors:
- "Researchers lack standardized tools for systematically studying people's interactions with LLMs"
- Pages 3-4 cite multiple research teams that built different custom platforms (Nie et al., 2024; Jelson et al., 2025; Costello et al., 2024; Wang et al., 2024), demonstrating fragmentation
- Related papers study human-LLM interactions but none propose infrastructure standardization

Break condition: If G4R becomes a single point of failure (API limits, service discontinuation), researchers using it exclusively would face simultaneous disruption.

### Mechanism 2
Automatic participant ID generation and linkage between chat logs and survey responses enables longitudinal or multi-modal behavioral analysis. JavaScript embedded in Qualtrics generates a random participant ID (`g4r_pid`) and stores it in an Embedded Data field. This ID is attached to all messages exchanged via that interface. Researchers download CSVs containing participant ID, messages sent/received, and timestamps, then merge with Qualtrics survey responses using `g4r_pid` as the join key.

Core assumption: Participants complete the survey in a single session without clearing cookies/browser data, and the Embedded Data field is initialized before the JavaScript executes.

Evidence anchors:
- Pages 14-15 describe the Embedded Data field setup and emphasize placing it at the top of Survey Flow for proper initialization
- Pages 15-16 detail the merge process using `g4r_pid` as the linking variable
- No corpus papers address participant tracking across survey and conversational data

Break condition: If participants open the survey in multiple browsers or devices, the ID may not persist across sessions.

### Mechanism 3
Configurable system prompts and temperature settings allow researchers to control interaction dynamics without modifying underlying code. G4R exposes OpenAI API parameters (system prompt, temperature 0.0-2.0, prepended/appended text) through a web form. These parameters are stored per-interface and injected into API calls at runtime. Lower temperatures produce more deterministic responses; system prompts set conversational constraints.

Core assumption: Researchers understand how temperature and system prompts affect LLM behavior; the paper advises testing multiple times if changing temperature from default 1.0.

Evidence anchors:
- Page 11: "Temperature is a parameter that controls the randomness of ChatGPT's responses. Lower values... make responses more focused and deterministic"
- Page 10: System prompt "lets researchers guide ChatGPT's responses by setting things like the tone, context, or constraints"
- Weak corpus evidence on parameter effects; corpus focuses on interaction outcomes rather than API configuration

Break condition: OpenAI API changes (parameter deprecation, model behavior shifts) could alter interface behavior without G4R code changes.

## Foundational Learning

- **Concept: System prompts in LLMs**
  - Why needed here: Researchers must understand how system prompts constrain GPT behavior for their study design (e.g., simulating an AI customer representative vs. neutral assistant). The paper assumes this knowledge.
  - Quick check question: If you set a system prompt to "Respond only in three sentences," would you expect GPT to follow this for all participant messages?

- **Concept: Embedded Data in Qualtrics survey flow**
  - Why needed here: Correct setup (initializing `g4r_pid` at the top of Survey Flow) is required for participant ID tracking. Misplacement causes merge failures.
  - Quick check question: Where must the Embedded Data block be placed in Survey Flow for `g4r_pid` to capture values correctly?

- **Concept: Temperature parameter in language models**
  - Why needed here: Researchers adjusting temperature (e.g., 0.2 for consistent responses, 1.5 for creative tasks) need to predict how this affects data variability.
  - Quick check question: Would temperature=0.0 be appropriate for a study requiring identical responses to identical prompts across participants?

## Architecture Onboarding

- **Component map**: g4r.org frontend -> Interface builder -> JavaScript snippet -> Backend API handler -> Data export

- **Critical path**:
  1. Create G4R account
  2. Configure interface (set system prompt, message limits, labels)
  3. Copy JavaScript to Qualtrics question
  4. Add `g4r_pid` Embedded Data at Survey Flow top
  5. Pilot test with preview
  6. Collect data
  7. Download CSV from Researcher Home
  8. Merge with Qualtrics export using `g4r_pid`

- **Design tradeoffs**:
  - **New tab vs. embedded**: New tab preserves full chat visibility but separates from survey context; embedded keeps context but constrains UI
  - **Default vs. custom API key**: Default relies on G4R's API quota (may require own key later); custom key gives control but adds setup burden
  - **HTML customization**: Only applies to new-tab mode, not embedded

- **Failure signatures**:
  - **Merge failure**: `g4r_pid` column in Qualtrics export is empty → Embedded Data not at Survey Flow top or JavaScript not executing
  - **Message limit not enforced**: Participants exceed configured limit → JavaScript caching issue; clear browser cache
  - **System prompt ignored**: GPT responds with default behavior → Verify prompt was saved; check for special character encoding issues

- **First 3 experiments**:
  1. **Guest mode test**: Create a GPT Interface without account, send 3 test messages, verify responses reflect configured system prompt and labels
  2. **Qualtrics integration pilot**: Create account, configure interface, add JavaScript to a test survey with `g4r_pid` Embedded Data, complete as participant, confirm ID appears in survey data
  3. **Data merge validation**: Download message CSV, download Qualtrics export, run provided R script to verify join on `g4r_pid` produces expected row count

## Open Questions the Paper Calls Out

### Open Question 1
Does using the G4R interface alter participant behavior compared to the native ChatGPT interface?
Basis: The tool emulates ChatGPT but with customized interfaces (different labels, embedded in surveys, custom HTML), which may affect ecological validity of observed interactions.
Unresolved because: No validation data is presented comparing G4R-mediated interactions to native ChatGPT usage patterns.
Evidence needed: A controlled experiment comparing interaction metrics (message length, tone, task completion) between participants using G4R versus the actual ChatGPT interface.

### Open Question 2
How can cross-study comparability be maintained when the underlying LLM (ChatGPT) evolves over time?
Basis: The paper claims G4R enables "reproducible" and standardized research, but model behavior changes frequently with updates, threatening longitudinal or cross-study comparisons.
Unresolved because: G4R captures the interface layer, not the model version; the same prompts may yield different responses across time periods.
Evidence needed: Empirical documentation of response variability across ChatGPT versions, or integration of model version metadata into exported data.

### Open Question 3
What is the long-term sustainability model for G4R as usage scales beyond the authors' API allowance?
Basis: "Researchers in the future may be asked to provide their own OpenAI API key if G4R becomes popular enough that the authors' API allowance nears its limit."
Unresolved because: No transition plan, cost projections, or alternative funding mechanisms are described for sustained operation.
Evidence needed: Usage projections, cost modeling, and evaluation of decentralized alternatives (e.g., fully client-side API calls using researcher-owned keys).

### Open Question 4
What data governance and IRB considerations arise from storing participant-LLM conversations on G4R's servers?
Basis: The paper notes password hashing for security but does not address conversation data retention, encryption, or institutional review board requirements for third-party data storage.
Unresolved because: Researchers may face challenges obtaining IRB approval for data stored externally without clear policies on data access, retention periods, or deletion.
Evidence needed: Published data governance policies, sample IRB consent language, and documentation of compliance with data protection standards.

## Limitations

- The long-term sustainability of G4R depends on API usage scaling and potential need for researchers to provide their own API keys
- The tool's effectiveness relies on researcher adoption and sustained platform maintenance
- Configurable parameters like temperature and system prompts lack empirical validation for controlling interaction dynamics

## Confidence

- **High confidence**: The core mechanism of participant ID generation and Qualtrics data merging is well-specified and testable
- **Medium confidence**: The claim that G4R will reduce methodological fragmentation depends on researcher adoption and sustained platform maintenance
- **Low confidence**: The assertion that configurable parameters will allow researchers to "control interaction dynamics" lacks empirical validation in the paper

## Next Checks

1. **Deployment validation**: Obtain and deploy the full source code from OSF, verify all API endpoints function correctly, and test message logging under realistic usage scenarios

2. **Qualtrics edge case testing**: Conduct experiments across different browsers, devices, and survey flow configurations to identify scenarios where `g4r_pid` generation fails or produces duplicates

3. **Parameter effect validation**: Run controlled experiments varying temperature and system prompts across multiple GPT calls to quantify consistency and adherence to constraints, documenting any deviations from expected behavior