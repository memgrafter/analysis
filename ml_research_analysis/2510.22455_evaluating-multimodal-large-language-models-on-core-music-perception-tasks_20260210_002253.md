---
ver: rpa2
title: Evaluating Multimodal Large Language Models on Core Music Perception Tasks
arxiv_id: '2510.22455'
source_url: https://arxiv.org/abs/2510.22455
tags:
- audio
- music
- chord
- midi
- pitches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper benchmarks three multimodal large language models\u2014\
  Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni\u2014on three core music perception\
  \ tasks: syncopation scoring, transposition detection, and chord quality identification.\
  \ It isolates perception from reasoning by comparing audio versus MIDI inputs, zero-\
  \ versus few-shot settings, and three prompting strategies: standalone, chain-of-thought,\
  \ and LogicLM (a neuro-symbolic framework that enforces structured reasoning)."
---

# Evaluating Multimodal Large Language Models on Core Music Perception Tasks

## Quick Facts
- arXiv ID: 2510.22455
- Source URL: https://arxiv.org/abs/2510.22455
- Reference count: 40
- Primary result: Current multimodal models reason well over musical symbols but have fundamental audio perception limitations

## Executive Summary
This paper benchmarks three multimodal large language models (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) on core music perception tasks including syncopation scoring, transposition detection, and chord quality identification. The evaluation systematically compares audio versus MIDI inputs, zero- versus few-shot settings, and three prompting strategies (standalone, chain-of-thought, and LogicLM). Results show that while models achieve near-ceiling performance with MIDI inputs, audio inputs cause sharp accuracy drops, revealing a fundamental perception bottleneck. Few-shot examples and prompting strategies offered minimal gains, with LogicLM helping only when symbolic input was reliable.

## Method Summary
The study evaluates multimodal models on three core music perception tasks using both audio and MIDI inputs to isolate perception from reasoning capabilities. Models are tested in zero- and few-shot settings (10 examples per task) and with three prompting strategies: standalone, chain-of-thought, and LogicLM (a neuro-symbolic framework enforcing structured reasoning). Audio inputs are generated using "MusicLM-cleaned" versions of MIDI files. Performance is measured across all task-input-model combinations to assess the models' ability to perceive musical features from raw audio versus symbolic representations.

## Key Results
- MIDI inputs yield near-ceiling performance across all models and tasks
- Audio inputs cause sharp accuracy drops, revealing fundamental perception limitations
- Few-shot examples and prompting strategies offer minimal performance gains
- LogicLM helps only when symbolic input is reliable

## Why This Works (Mechanism)
The paper's evaluation framework isolates perception from reasoning by using both audio and MIDI inputs for the same musical content. This controlled comparison reveals that current multimodal models have strong symbolic reasoning capabilities but struggle with raw audio perception. The systematic testing of prompting strategies and few-shot learning attempts to identify optimization techniques, but minimal gains suggest the bottleneck lies in audio processing rather than reasoning or training methodology.

## Foundational Learning
- **Syncopation scoring**: Measures rhythmic complexity and accent patterns - needed to evaluate rhythmic perception capabilities; quick check: model accuracy on syncopated versus straight rhythms
- **Transposition detection**: Identifies pitch level shifts while preserving interval relationships - needed to test pitch perception and invariance; quick check: accuracy across multiple semitone shifts
- **Chord quality identification**: Recognizes harmonic structures (major, minor, diminished) - needed to assess harmonic perception; quick check: accuracy across different inversions and voicings

## Architecture Onboarding
**Component Map**: Audio input → Multimodal Encoder → Symbolic Representation → Reasoning Module → Output

**Critical Path**: Audio processing front-end → Symbolic transformation → Reasoning layer → Response generation

**Design Tradeoffs**: Symbolic inputs bypass audio perception challenges but don't reflect real-world music understanding; audio inputs test genuine perception but reveal current technological limitations

**Failure Signatures**: Sharp accuracy drops when switching from MIDI to audio inputs, minimal gains from prompting strategies, near-ceiling performance on symbolic tasks regardless of reasoning complexity

**First Experiments**:
1. Test audio input quality by comparing MusicLM-cleaned audio against raw performance recordings
2. Evaluate reasoning capabilities on increasingly complex symbolic music problems
3. Assess temporal reasoning over longer musical sequences beyond the current task scope

## Open Questions the Paper Calls Out
None

## Limitations
- Audio quality degradation from MusicLM preprocessing may partially explain performance gaps
- Three tasks provide limited coverage of full music perception capabilities
- Minimal exploration of alternative prompting frameworks beyond tested strategies
- Few-shot evaluation limited to only 10 examples per task

## Confidence
**High confidence**: MIDI inputs yield superior performance to audio inputs across all models and tasks
**Medium confidence**: Minimal performance gains from prompting strategies and few-shot examples
**Medium confidence**: Current models can reason over musical symbols but have fundamental audio perception limitations

## Next Checks
1. Conduct controlled experiments comparing MusicLM-cleaned audio against raw audio recordings from actual performances to isolate the impact of audio preprocessing on model perception accuracy

2. Expand the benchmark suite to include tasks requiring temporal reasoning over longer musical sequences, melodic contour identification, and timbre-based classification to provide a more comprehensive assessment of music perception capabilities

3. Test alternative neuro-symbolic frameworks and larger few-shot datasets (50-100 examples per task) to determine whether more sophisticated prompting strategies or training approaches might unlock additional performance gains for audio inputs