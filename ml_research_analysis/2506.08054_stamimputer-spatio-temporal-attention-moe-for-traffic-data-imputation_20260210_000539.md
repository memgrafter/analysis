---
ver: rpa2
title: 'STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation'
arxiv_id: '2506.08054'
source_url: https://arxiv.org/abs/2506.08054
tags:
- data
- attention
- traffic
- imputation
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses traffic data imputation, which is critical
  for intelligent transportation systems but challenged by block-wise missing data
  and nonstationary traffic patterns. The authors propose STAMImputer, a model that
  uses a Mixture of Experts (MoE) framework to dynamically balance temporal and spatial
  attention based on real-time data characteristics.
---

# STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation

## Quick Facts
- **arXiv ID:** 2506.08054
- **Source URL:** https://arxiv.org/abs/2506.08054
- **Reference count:** 16
- **Primary result:** Achieves state-of-the-art MAE performance on four traffic datasets, particularly excelling in block-missing scenarios through dynamic spatio-temporal attention balancing.

## Executive Summary
This paper introduces STAMImputer, a novel approach to traffic data imputation that addresses the challenges of block-wise missing data and nonstationary traffic patterns. The model employs a Mixture of Experts (MoE) framework that dynamically balances temporal and spatial attention through specialized expert networks. A key innovation is the Low-rank guided Sampling Graph ATtention (LrSGAT) mechanism, which captures global spatial dependencies by sampling attention vectors to construct dynamic graphs. The model integrates discrete wavelet transform for feature enhancement and uses observation experts to evaluate and weight attention outputs. Extensive experiments demonstrate superior performance compared to baseline methods across multiple traffic datasets.

## Method Summary
STAMImputer uses a Mixture of Experts (MoE) framework to address traffic data imputation challenges. The model consists of three expert networks: an Observation Expert that arbitrates trust between experts based on sparsity features, a Temporal Expert using Multi-head Self-Attention (Transformer), and a Spatial Expert employing Low-rank guided Sampling Graph ATtention (LrSGAT). Inputs are enriched with Discrete Wavelet Transform (DWT) coefficients, spatio-temporal positional encodings, and sparsity rates. The LrSGAT mechanism samples S = ⌈log N⌉ nodes using hybrid Top-K and probabilistic strategies to generate projection vectors and semi-adaptive dynamic graphs, reducing attention complexity from O(N²) to O(N log N).

## Key Results
- STAMImputer achieves consistently lower MAE values than baseline methods across all four traffic datasets (PemsD8, SZ-Taxi, DiDi-SZ, NYC-Taxi)
- Demonstrates superior performance particularly in block-missing scenarios with failure probabilities of 0.2% and 1%
- Improves downstream prediction tasks when integrated with graph-based models, reducing MAE from 9.54 to 8.56 on GWNet+Sparse vs. GWNet+STAMImputer at 60% missing rate

## Why This Works (Mechanism)

### Mechanism 1
The Mixture of Experts framework dynamically rebalances spatio-temporal attention weights based on real-time data sparsity patterns. Observation experts act as arbitrators, computing confidence scores for temporal and spatial expert outputs using sparse features and raw data. When one dimension is sparse, the router downweights that expert's contribution.

### Mechanism 2
Low-rank guided sampling reduces attention complexity while preserving hub-node influence for global spatial modeling. Hybrid sampling (top-k significance + probability sampling) selects representative nodes. Projection vectors compress global information; re-attention reconstructs it, completing missing features during compression-decompression.

### Mechanism 3
Semi-adaptive dynamic graphs improve downstream tasks by encoding real-time spatial dependencies absent in static topologies. Sampled attention vectors serve as dynamic cohesive factors; a learnable refraction vector maps projection messages to extroversion factors, reconstructing adjacency.

## Foundational Learning

- **Self-attention (Transformer) mechanics** - Why needed: Temporal expert uses standard Transformer encoder; spatial expert modifies attention with sampling. Quick check: Can you explain how attention(Q,K,V) scales with sequence length and how low-rank projection changes this?
- **Graph Attention Networks (GAT)** - Why needed: LrSGAT builds on GAT for local attention extraction before global sampling. Quick check: How does GAT differ from GCN in aggregating neighbor information?
- **Low-rank matrix factorization** - Why needed: LrSGAT uses low-rank approximation to compress spatial relationships. Quick check: If rank k is too small, what information is lost? If too large, what efficiency gain disappears?

## Architecture Onboarding

- **Component map:** Input -> DWT decomposition -> Spatio-temporal embedding -> MoE framework (Observation Expert + Attention Experts [Temporal: MSAT, Spatial: LrSGAT]) -> Readout MLP -> Imputation output
- **Critical path:** LrSGAT's sampling strategy (top-k + probabilistic) -> projection vectors -> re-attention reconstruction
- **Design tradeoffs:** Sample size S = ⌈log N⌉ vs. coverage; number of attention heads vs. expert count; DWT decomposition level j vs. temporal resolution
- **Failure signatures:** Imputation MAE spikes on block-missing; downstream prediction degrades despite good imputation; training instability
- **First 3 experiments:**
  1. Ablation of sampling strategy: Replace hybrid sampling with random-only or top-k-only on DiDi-SZ
  2. Sample size sensitivity: Sweep S ∈ {⌈0.5 log N⌉, ⌈log N⌉, ⌈2 log N⌉, ⌈0.1 N⌉} on NYC-Taxi
  3. Dynamic graph transfer test: Pre-train on one city, retrain dynamic graph on another, evaluate downstream prediction

## Open Questions the Paper Calls Out
The paper explicitly mentions that future research will focus on developing more efficient methods for constructing dynamic graphs, suggesting open questions about computational optimization of the DGSL layer.

## Limitations
- Training hyperparameters (learning rate, batch size, epochs) are unspecified
- Specific wavelet basis function and decomposition level used for DWT are not stated
- Exact Readout MLP architecture and full loss formulation are not detailed

## Confidence
- **High Confidence:** Core MoE framework mechanism is well-defined and supported by abstract and section 4
- **Medium Confidence:** LrSGAT sampling strategy and complexity reduction are clear, but dynamic graph implementation details are not fully specified
- **Low Confidence:** Generalizability of semi-adaptive dynamic graphs to datasets with very different traffic patterns is not empirically validated

## Next Checks
1. Ablation of sampling strategy: Replace hybrid sampling with random-only or top-k-only; measure MAE difference on DiDi-SZ (627 nodes)
2. Sample size sensitivity: Sweep S ∈ {⌈0.5 log N⌉, ⌈log N⌉, ⌈2 log N⌉, ⌈0.1 N⌉} on NYC-Taxi; plot MAE vs. S
3. Dynamic graph transfer test: Pre-train STAMImputer on one city; freeze imputation weights but retrain dynamic graph module on another city; evaluate downstream prediction