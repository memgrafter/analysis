---
ver: rpa2
title: 'The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language
  Models'
arxiv_id: '2510.06101'
source_url: https://arxiv.org/abs/2510.06101
tags:
- reasoning
- data
- arxiv
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a "valley of code reasoning" in distillation:
  small models first degrade by half in competitive coding performance when trained
  on small datasets (1K examples), then improve sharply (50-100% gains) in a log-linear
  trend up to 30K examples. The effect holds across two non-reasoning LLMs (Qwen2.5-7B
  and Llama3.1-8B).'
---

# The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models

## Quick Facts
- arXiv ID: 2510.06101
- Source URL: https://arxiv.org/abs/2510.06101
- Reference count: 3
- Small models first degrade by half in competitive coding performance at 1K examples, then improve sharply (50-100% gains) in log-linear trend up to 30K examples.

## Executive Summary
This paper identifies a "valley of code reasoning" in knowledge distillation: small non-reasoning models degrade by 50% when trained on just 1K examples, then improve sharply (50-100% gains) in a log-linear trend up to 30K examples. The effect holds across Qwen2.5-7B and Llama3.1-8B models. Surprisingly, correctness of teacher responses has no impact on distillation outcomes. Instead, easier coding questions consistently yield 33-41% better performance than harder ones. These findings suggest small models in low-data regimes benefit more from simpler examples and output structure rather than answer accuracy.

## Method Summary
The study distills chain-of-thought reasoning from teacher models (DeepSeek-R1-0528, KAT-V1-40B) into small non-reasoning student models (Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct) using supervised fine-tuning on OpenCodeReasoning2 dataset. Training was conducted on 8× Nvidia H100 GPUs with batch size 128, learning rate 8e-5, warmup ratio 0.10, and 5 epochs. The researchers created nested subsets (1K ⊂ 10K ⊂ 30K) and evaluated on LiveCodeBench (LCB) Pass@1, measuring completion rate and think-tag occurrence. Additional ablations tested correctness and difficulty effects using TACO dataset partitions.

## Key Results
- Small models degrade by more than half from baseline when trained on 1K examples, then improve and surpass baseline by ~50% at 10K examples
- Correctness of teacher responses has no effect on distillation outcomes—both correct and incorrect subsets improve performance by ~50%
- Training on easy/medium difficulty data improves Qwen2.5 by 41% compared to modest gains from hard data
- Completion rate increases from ~14% to ~57% as data scales from 1K to 30K

## Why This Works (Mechanism)

### Mechanism 1
Small non-reasoning models exhibit a U-shaped performance curve when distilling reasoning capabilities, with initial degradation followed by log-linear recovery. At low data regimes (~1K examples), models begin acquiring novel output structures but lack sufficient examples to generalize, causing interference with existing competencies. As data scales (10K-30K), structural learning consolidates—the model reliably produces complete reasoning traces, enabling downstream performance gains.

### Mechanism 2
Distillation effectiveness depends on output structure (reasoning format), not answer correctness. Student models extract procedural patterns—how to decompose problems, when to self-correct, how to structure intermediate steps—from teacher traces. Whether the final answer is correct is incidental; the learning signal is in the process scaffold.

### Mechanism 3
Easier training examples yield disproportionate gains for small models in low-data regimes. Simple problems provide clearer demonstrations of reasoning structure with less cognitive load. Small models can more reliably extract and internalize the format when problem complexity doesn't compete for model capacity.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: Essential to understand what's being transferred—reasoning traces wrapped in `` tags. Quick check: Can you explain why intermediate reasoning steps improve task performance over direct answers?
- **Knowledge Distillation via SFT**: The method uses supervised fine-tuning on teacher-generated traces, not RL or other alignment techniques. Quick check: What's the difference between distillation via SFT and distillation via KL divergence on logits?
- **Log-Linear Scaling Laws**: The paper claims "sharper-than-log-linear" improvement after the valley; grounding in scaling intuition helps assess this claim. Quick check: How does performance typically scale with data quantity in language model training?

## Architecture Onboarding

**Component map**: Teacher models (DeepSeek-R1-0528, KAT-V1-40B) -> Dataset pipeline (OCR2 -> 1K/10K/30K subsets -> TACO partitions) -> Student models (Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct) -> Training (torchtune, 8×H100) -> Evaluation (LIVECODEBENCH)

**Critical path**: 1) Verify student baselines lack reasoning capability (no `` tag in tokenizer) 2) Create nested subsets (1K ⊂ 10K ⊂ 30K) to ensure distributional consistency 3) Train across data scales and plot LCB scores to identify valley 4) At valley-exit checkpoint, ablate on correctness and difficulty partitions

**Design tradeoffs**: Random vs. curated sampling (authors chose random to isolate quantity effects), 32K token limit constrained by Qwen architecture, single benchmark (LCB) for contamination control but limits generalization

**Failure signatures**: No valley observed (student may already have reasoning capabilities), completion rate doesn't correlate with performance (structural learning decoupled from task success), hard examples outperform easy (model capacity exceeds assumptions)

**First 3 experiments**: 1) Reproduce valley: Train Qwen2.5-7B-Instruct on 1K/10K/30K OCR2 subsets, plot LCB Pass@1 2) Correctness ablation: At 30K checkpoint, fine-tune on 6K correct-only vs. incorrect-only; expect ~equal gains 3) Difficulty ablation: Train separate runs on 4K easy vs. 4K hard; expect 30-40% relative improvement for easy

## Open Questions the Paper Calls Out

- **Scaling beyond 100K**: Does the valley-of-code-reasoning pattern persist into medium-high and high data regimes (above 100K examples)? The paper states it will explore scaling trends beyond 30K.

- **High-data regime effects**: Do the findings about output correctness irrelevance and easy-question superiority hold in high-data regimes? The authors explicitly ask if the same conclusions hold above 100K examples.

- **Mechanistic explanation**: What explains the initial performance degradation in the "valley" before improvement begins? The paper identifies the phenomenon but offers no theoretical account of early-stage degradation.

## Limitations

- The valley phenomenon and its mechanisms may not generalize beyond the specific OCR2 dataset, teacher-student model pairs, and LiveCodeBench evaluation used in the study.
- Difficulty findings depend on TACO's external difficulty ratings, which may not align perfectly with actual cognitive load for small models.
- The structural learning mechanism lacks direct ablation to confirm whether reasoning format itself is necessary or if any structured output would suffice.

## Confidence

- **High confidence**: The existence of a performance valley at ~1K examples, followed by log-linear improvement up to 30K, as observed in both Qwen2.5-7B and Llama3.1-8B.
- **Medium confidence**: The claim that correctness of teacher responses has no effect on distillation outcomes, though sample size and labeling quality introduce uncertainty.
- **Medium confidence**: The 33-41% advantage of easier examples over harder ones, limited by dependence on TACO's difficulty scheme.

## Next Checks

1. Replicate the valley across datasets: Train the same student models on 1K/10K/30K examples from HumanEval or MBPP, using teacher-generated reasoning traces. Plot LCB (or equivalent) Pass@1 to confirm the U-shaped curve persists outside OCR2.

2. Structure vs. correctness ablation: From the 30K checkpoint, fine-tune on three partitions: (a) 6K correct traces with `` tags stripped, (b) 6K incorrect traces with tags, (c) 6K incorrect traces without tags. Compare gains to isolate whether the reasoning format—not just correctness—drives the learning signal.

3. Difficulty generalization test: Curate a new difficulty split within OCR2 (e.g., by problem length or token count) rather than relying on TACO ratings. Train on 4K "short/simple" vs. 4K "long/complex" examples and measure relative performance to validate the difficulty effect independently of external labels.