---
ver: rpa2
title: 'Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model
  Approach'
arxiv_id: '2511.23311'
source_url: https://arxiv.org/abs/2511.23311
tags:
- driving
- qwen2
- safe
- video
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a vision language model (VLM) approach for
  generating safe driving instructions from synchronized driver-facing and road-facing
  videos. The authors construct a dataset of 2,145 video clips with Chain-of-Thought
  question-answer pairs for event detection and safe driving instruction.
---

# Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach

## Quick Facts
- arXiv ID: 2511.23311
- Source URL: https://arxiv.org/abs/2511.23311
- Authors: Haruki Sakajo; Hiroshi Takato; Hiroshi Tsutsui; Komei Soda; Hidetaka Kamigaito; Taro Watanabe
- Reference count: 17
- Primary result: Vision language model approach for generating safe driving instructions from synchronized driver/road videos achieves BERTScore F1 > 0.89 after fine-tuning

## Executive Summary
This study develops a vision language model approach for generating safe driving instructions from synchronized driver-facing and road-facing videos. The authors construct a dataset of 2,145 video clips with Chain-of-Thought question-answer pairs for event detection and safe driving instruction. After fine-tuning Qwen2.5-VL models on this dataset, performance improves significantly: BERTScore F1 reaches 0.897 for event detection and 0.894 for instruction generation. Fine-tuned models generate safety-aware driving instructions aligned with visual context, while pre-trained models produce generic guidance. However, challenges remain in detecting subtle events like excessive speed during turns, and some failure cases occur with good driving videos.

## Method Summary
The approach uses synchronized dual-view videos (driver-facing and road-facing) stacked vertically and processed at 2 FPS. Qwen2.5-VL models with frozen vision encoders undergo full-parameter supervised fine-tuning on a dataset of 2,145 video clips. The training uses Chain-of-Thought format with GPT-4o-generated annotations reviewed by experts, creating structured event detection followed by instruction generation. LLaMA-Factory with DeepSpeed ZeRO stage 2 trains on 8× A100-40GB GPUs, with video sequences limited to 128 frames and 16,384 pixels maximum.

## Key Results
- Fine-tuned Qwen2.5-VL-7B achieves BERTScore F1 of 0.897 for event detection and 0.894 for instruction generation
- Pre-trained models show significant language bias, outputting generic safe-driving advice regardless of video content
- Dual-view input enables detection of both external hazards and internal driver behaviors that single-view systems miss
- Performance drops on subtle events like harsh turns and mobile usage while maintaining eye contact with the road

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning aligns VLM outputs to domain-specific driving safety reasoning, overcoming generic response tendencies in pre-trained models. Full-parameter supervised fine-tuning of the language model (with frozen vision encoder) teaches the model to ground textual responses in observed driving events rather than relying on language priors. The Chain-of-Thought format (event detection → instruction generation) further structures this reasoning.

### Mechanism 2
Synchronized dual-view (driver-facing + road-facing) input enables detection of complementary risk categories that single-view systems miss. Vertically stacking synchronized video streams (road top, driver bottom) at 2 FPS provides a unified visual context where the model can jointly reason about external hazards and internal driver states within the same forward pass.

### Mechanism 3
Chain-of-Thought prompting with expert-validated annotations produces higher-quality instruction generation than end-to-end approaches. Decomposing the task into (1) event detection and (2) instruction generation creates an explicit reasoning scaffold. GPT-4o generates gold answers from structured annotations, which are then human-verified, ensuring training labels encode causal relationships between events and appropriate responses.

## Foundational Learning

- **Concept: Vision-Language Model Architecture**
  - Why needed here: Understanding that Qwen2.5-VL integrates a frozen vision encoder with a fine-tunable language model is critical for debugging which component limits performance.
  - Quick check question: If the model fails to detect a stop sign in snowy conditions, would you modify the vision encoder or the language model head?

- **Concept: BERTScore vs. BLEU for Text Evaluation**
  - Why needed here: The paper reports BERTScore F1 > 0.89 but BLEU ~10-13; understanding that BERTScore captures semantic similarity while BLEU measures n-gram overlap explains why large score gaps appear and which metric to prioritize for instruction quality.
  - Quick check question: Why might a model receive high BERTScore but low BLEU for semantically correct but lexically different instructions?

- **Concept: Language/Unimodal Bias in VLMs**
  - Why needed here: The paper identifies that pre-trained models output generic safe-driving advice regardless of video content; recognizing this bias is essential for interpreting pre-fine-tuning behavior and designing interventions.
  - Quick check question: What does it indicate if a model generates identical top-4-grams across different input videos?

## Architecture Onboarding

- **Component map:**
  - Synchronized dual-view video (road-facing top, driver-facing bottom) at 2 FPS, max 128 frames, max 16,384 pixels -> Frozen Qwen2.5-VL visual backbone -> Qwen2.5-VL-3B/7B with full-parameter fine-tuning -> Chain-of-Thought text generation

- **Critical path:**
  1. Video preprocessing → dual-view vertical stacking + 2 FPS sampling
  2. Vision encoding → frozen feature extraction
  3. Language model forward pass → Chain-of-Thought prompting
  4. Supervised fine-tuning → cross-entropy loss on annotated Q&A pairs

- **Design tradeoffs:**
  - 3B vs. 7B: 7B (FT) achieves marginally higher BERTScore but 3B (FT) produces more diverse outputs (lower self-BLEU); 7B pre-trained tends toward generic advice
  - Frozen encoder: Reduces compute cost but limits adaptation to domain-specific visual features
  - 2 FPS sampling: Reduces sequence length but may miss brief events

- **Failure signatures:**
  - Harsh turn detection: Model struggles with vehicle dynamics requiring temporal speed reasoning
  - Mobile usage with eyes on road: Model detects held object but misses that eyes remain forward
  - Hallucinated objects: Fine-tuned models occasionally mention non-existent traffic elements (e.g., stop signs not in video)

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune Qwen2.5-VL-3B on the provided dataset using Table 5 hyperparameters; verify BERTScore F1 ≥ 0.89 on test split
  2. **Ablation on frame rate:** Compare 2 FPS vs. 4 FPS sampling on the difficult subset (bottom 25% BERTScore samples) to assess whether temporal resolution improves harsh turn detection
  3. **Single-view control:** Train separate models on road-facing only vs. driver-facing only vs. dual-view to quantify the marginal contribution of each view to event detection accuracy

## Open Questions the Paper Calls Out

- **How does model performance scale with significantly larger datasets of synchronized driving videos?**
  - The authors explicitly state in the "Limitations" section that the dataset is relatively small (1,719 training samples) and that "evaluation on a larger dataset left for future work."

- **How can LVLMs be improved to detect subtle temporal events, such as harsh turns or mobile usage while maintaining eye contact?**
  - The abstract and error analysis highlight that models struggle with "subtle events like harsh turns" and specific driver behaviors like "mobile phone usage" where the driver keeps their eyes on the road.

- **Does incorporating auxiliary sensor data (e.g., vehicle speed) directly into the model input improve detection accuracy for vehicle behaviors?**
  - The authors note that auxiliary sensor data was used for annotation but omitted from model inputs to reduce deployment costs, while the model struggles with speed estimation.

## Limitations
- Dataset availability and generalizability: The 2,145-video dataset is not publicly available, limiting independent validation and raising concerns about performance in urban environments or different camera placements.
- Vision encoder limitations: The frozen vision encoder may not capture critical temporal dynamics like vehicle speed changes during maneuvers, as evidenced by harsh turn detection failures.
- Annotation quality and bias: While GPT-4o generates gold answers reviewed by experts, the quality control process isn't fully specified, and the event taxonomy may miss novel or rare safety-critical scenarios.

## Confidence

- **High confidence**: Fine-tuning improves performance over pre-trained models for event detection and instruction generation (BERTScore F1 0.897/0.894 vs. lower pre-trained scores).
- **Medium confidence**: The Chain-of-Thought format with expert-validated annotations produces higher-quality instructions than end-to-end approaches, though this depends on annotation quality.
- **Low confidence**: The model's ability to detect subtle events like excessive speed during turns or mobile usage while eyes remain on road, suggesting fundamental limitations in the vision encoder's temporal reasoning capabilities.

## Next Checks

1. **Temporal resolution ablation**: Test 2 FPS vs. 4 FPS sampling on the difficult subset (bottom 25% BERTScore samples) to quantify whether higher frame rates improve detection of brief events and vehicle dynamics like harsh turns.

2. **Cross-environment generalization**: Evaluate model performance on urban driving scenarios, night conditions, and different camera angles not represented in the highway-focused dataset to assess real-world applicability.

3. **Vision encoder adaptation**: Compare frozen encoder performance against fine-tuned encoder approaches on the harsh turn detection task to determine if vision encoder adaptation is necessary for complex temporal reasoning in driving scenarios.