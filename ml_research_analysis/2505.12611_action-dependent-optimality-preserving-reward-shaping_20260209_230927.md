---
ver: rpa2
title: Action-Dependent Optimality-Preserving Reward Shaping
arxiv_id: '2505.12611'
source_url: https://arxiv.org/abs/2505.12611
tags:
- reward
- shaping
- policy
- optimal
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward hacking in reinforcement
  learning, where agents optimize shaping rewards at the expense of extrinsic rewards,
  leading to suboptimal policies. The authors propose Action-Dependent Optimality-Preserving
  Shaping (ADOPS), a method that converts arbitrary shaping rewards (including intrinsic
  motivation) into a form that preserves optimality while allowing for action-dependent
  returns.
---

# Action-Dependent Optimality-Preserving Reward Shaping

## Quick Facts
- **arXiv ID:** 2505.12611
- **Source URL:** https://arxiv.org/abs/2505.12611
- **Reference count:** 19
- **Primary result:** ADOPS preserves optimality while allowing action-dependent shaping rewards, improving exploration in sparse-reward environments.

## Executive Summary
This paper introduces Action-Dependent Optimality-Preserving Shaping (ADOPS), a method that converts arbitrary shaping rewards into a form that preserves the optimal policy set while allowing the shaping reward's return to depend on the agent's actions. The approach addresses reward hacking in reinforcement learning by actively adjusting intrinsic rewards based on critic estimates of extrinsic and intrinsic value functions. ADOPS provably accommodates a wider set of optimality-preserving shaping functions than prior methods and demonstrates empirical improvements over baseline intrinsic motivation in complex, sparse-reward environments like Montezuma's Revenge.

## Method Summary
ADOPS functions by computing an adjustment term `F2` that is added to arbitrary shaping rewards `F`. This term is constructed using critic networks to estimate extrinsic and intrinsic value functions, ensuring that actions optimal under the extrinsic reward remain optimal under the combined reward. The method actively reduces intrinsic rewards when they would cause preference for suboptimal actions according to extrinsic rewards alone. ADOPS is proven to preserve the set of optimal policies and encompasses a wider set of optimality-preserving shaping functions than prior methods like PBRS, GRM, and PBIM.

## Key Results
- ADOPS preserves optimality while allowing action-dependent shaping rewards, overcoming limitations of prior methods
- Proven to accommodate a wider set of optimality-preserving shaping functions than PBRS, GRM, and PBIM
- Demonstrates statistically significant improvements over baseline intrinsic motivation in Montezuma's Revenge
- Shows ADOPS improves performance where other optimality-preserving methods fail to provide extrinsic rewards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intrinsic motivation rewards can drive exploration without causing reward hacking
- **Mechanism:** ADOPS monitors critic estimates and dynamically reduces intrinsic rewards that would cause preference for suboptimal actions
- **Core assumption:** Critic networks provide accurate value function estimates
- **Evidence anchors:** Abstract confirms active adjustment prevents preference for suboptimal actions; Section 2.1 describes reward hacking problems; corpus papers discuss intrinsic rewards for exploration
- **Break condition:** Intrinsic rewards dominate and destabilize critic estimates

### Mechanism 2
- **Claim:** ADOPS provably preserves the set of optimal policies
- **Mechanism:** Computes shaping term `F2` satisfying Bellman optimality conditions to ensure optimal actions remain optimal
- **Core assumption:** Learning algorithm converges to stable policy (Assumption 5.1)
- **Evidence anchors:** Abstract states ADOPS preserves optimal policy set; Theorem 5.2 proves optimality preservation; corpus papers discuss policy invariance
- **Break condition:** Critic value estimates are highly biased

### Mechanism 3
- **Claim:** ADOPS is more general than prior methods by not requiring action-independent returns
- **Mechanism:** Achieves optimality through active adjustment rather than forcing shaping reward return to be potential function
- **Core assumption:** Action-dependence in shaping reward is beneficial for learning
- **Evidence anchors:** Abstract notes ADOPS allows action-dependent returns; introduction states ADOPS encompasses wider set; Theorem B.1 proves existence of functions GRM/PBRS cannot represent
- **Break condition:** Action-dependence provides no useful signal

## Foundational Learning

- **Concept: Markov Decision Process (MDP) & Optimal Policy**
  - **Why needed here:** Entire premise based on defining optimality within MDP framework
  - **Quick check question:** What function does an optimal policy maximize?

- **Concept: Q-Function (Action-Value Function)**
  - **Why needed here:** ADOPS logic operates on Q-values, comparing extrinsic and combined Q-functions
  - **Quick check question:** What does the Q-function Q(s, a) represent?

- **Concept: Reward Shaping**
  - **Why needed here:** Core problem being solved - adding shaping rewards without changing optimal policy
  - **Quick check question:** What is the primary risk of adding poorly designed shaping reward?

## Architecture Onboarding

- **Component map:**
  Base RL Agent -> Critic Networks (Extrinsic + Intrinsic) -> ADOPS Shaping Module -> Final Reward Calculation

- **Critical path:**
  1. Environment returns state `s` and extrinsic reward `R`
  2. Intrinsic motivation module computes raw intrinsic reward `F`
  3. ADOPS module receives `F`, `s`, and chosen action `a`
  4. ADOPS queries critic networks for value estimates
  5. ADOPS calculates adjustment term `F2` based on Q-value comparisons
  6. Final reward `R + F + F2` passed to agent for learning

- **Design tradeoffs:**
  - Critic Quality vs. Adjustment Latency: Early inaccurate critic estimates may harm learning
  - Complexity vs. Generality: ADOPS requires more complex setup but offers wider theoretical guarantees

- **Failure signatures:**
  - Performance collapse may indicate ADOPS adjustments are destabilizing policy
  - Stagnant intrinsic reward suggests over-penalizing exploration
  - No extrinsic reward indicates shaping may be preventing exploration entirely

- **First 3 experiments:**
  1. Baseline comparison in simple environment (grid world/cliff walking) against PIES, GRM, PBIM
  2. Montezuma's Revenge replication comparing ADOPS against baseline RND and failed optimality-preserving methods
  3. Ablation study comparing standard ADOPS against ADOPES variant with gradual adjustment introduction

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Theoretical guarantees rely heavily on assumption of convergence to stable policy (Assumption 5.1)
- Empirical validation limited to single complex environment (Montezuma's Revenge)
- Claims about encompassing "provably wider" shaping functions lack strong empirical validation

## Confidence

- **High Confidence:** Theoretical framework is sound with rigorous proofs under stated assumptions
- **Medium Confidence:** Empirical results show significant improvements in Montezuma's Revenge but limited generalizability
- **Low Confidence:** Claims about generality over prior methods lack strong empirical validation

## Next Checks

1. **Multi-Environment Validation:** Test ADOPS across suite of sparse-reward environments (Atari games, continuous control tasks) to assess generalizability beyond Montezuma's Revenge

2. **Convergence Analysis:** Conduct empirical studies on convergence behavior in non-stationary critic settings, measuring sensitivity to critic estimation errors during early training

3. **Baseline Comparison in Dense Rewards:** Compare ADOPS against standard shaping methods in environments with denser rewards to determine if complexity provides benefits outside sparse-reward regime