---
ver: rpa2
title: Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention
  Patterns and Semantic Alignment
arxiv_id: '2512.09148'
source_url: https://arxiv.org/abs/2512.09148
tags:
- semantic
- hallucination
- arxiv
- knowledge
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses hallucination in GraphRAG systems, where\
  \ large language models struggle to interpret relational and topological information\
  \ in linearized subgraphs, leading to factually incorrect outputs. The authors propose\
  \ two lightweight interpretability metrics\u2014Path Reliance Degree (PRD) to measure\
  \ over-reliance on shortest-path triples and Semantic Alignment Score (SAS) to assess\
  \ grounding in retrieved knowledge."
---

# Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment

## Quick Facts
- arXiv ID: 2512.09148
- Source URL: https://arxiv.org/abs/2512.09148
- Reference count: 40
- Primary result: GGA detector achieves AUC 0.83–0.85 and F1 0.46–0.54, outperforming semantic and confidence baselines

## Executive Summary
This paper addresses hallucination in GraphRAG systems, where large language models struggle to interpret relational and topological information in linearized subgraphs, leading to factually incorrect outputs. The authors propose two lightweight interpretability metrics—Path Reliance Degree (PRD) to measure over-reliance on shortest-path triples and Semantic Alignment Score (SAS) to assess grounding in retrieved knowledge. Through empirical analysis on a knowledge-based QA dataset, they find hallucinations correlate with weak semantic grounding (low SAS) and, in some cases, over-concentration on salient paths (high PRD). Based on these insights, they develop a lightweight hallucination detector, GGA, which uses PRD, SAS, and surface-level features. GGA achieves strong AUC (0.83–0.85) and F1 (0.46–0.54) performance across two LLMs, outperforming strong semantic and confidence-based baselines. The work provides both mechanistic insights into LLM limitations with structured knowledge and a practical tool for building more reliable GraphRAG systems.

## Method Summary
The paper proposes detecting hallucinations in GraphRAG by analyzing internal attention patterns and semantic alignment. The method extracts two interpretability metrics: PRD measures over-reliance on shortest-path triples via attention distribution