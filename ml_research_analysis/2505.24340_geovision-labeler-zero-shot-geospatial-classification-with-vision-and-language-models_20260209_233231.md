---
ver: rpa2
title: 'GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language
  Models'
arxiv_id: '2505.24340'
source_url: https://arxiv.org/abs/2505.24340
tags:
- classes
- classification
- image
- zero-shot
- vllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeoVision Labeler (GVL) addresses the challenge of geospatial image
  classification in scenarios where labeled data is scarce. It introduces a strictly
  zero-shot framework combining a vision Large Language Model (vLLM) to generate detailed
  image descriptions and a Large Language Model (LLM) to classify these descriptions
  into user-defined categories.
---

# GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models

## Quick Facts
- arXiv ID: 2505.24340
- Source URL: https://arxiv.org/abs/2505.24340
- Reference count: 16
- Achieves up to 93.2% zero-shot accuracy on binary Buildings vs. No Buildings task in SpaceNet v7

## Executive Summary
GeoVision Labeler (GVL) introduces a strictly zero-shot framework for geospatial image classification that addresses the challenge of limited labeled data in remote sensing applications. The approach combines a vision Large Language Model (vLLM) to generate detailed image descriptions with a Large Language Model (LLM) to classify these descriptions into user-defined categories. This modular, interpretable pipeline enables flexible adaptation to various classification tasks without requiring fine-tuning or domain-specific adaptation. The framework is evaluated on three benchmarks—SpaceNet v7, UC Merced, and RESISC45—demonstrating competitive performance while maintaining interpretability and ease of deployment.

## Method Summary
GVL employs a two-stage pipeline where a vision Large Language Model first generates detailed captions for geospatial images, which are then classified by a Large Language Model into user-specified categories. The framework operates in a strictly zero-shot manner, requiring no labeled data or fine-tuning. For complex multi-class classification tasks, GVL implements a hierarchical LLM-driven clustering approach that groups semantically related classes into meta-classes and refines classifications at successive depths. This allows the system to handle datasets with large numbers of classes while maintaining zero-shot capabilities. The modular design enables users to define their own classification categories and adapt the system to different geospatial classification needs.

## Key Results
- Achieves 93.2% accuracy on binary Buildings vs. No Buildings classification in SpaceNet v7
- Attains 86.4% accuracy on UC Merced multi-class meta-classification
- Achieves 84.3% accuracy on RESISC45 multi-class meta-classification

## Why This Works (Mechanism)
The framework leverages the strong semantic understanding capabilities of modern LLMs to bridge the gap between visual features and semantic labels without requiring labeled training data. The vision LLM captures detailed visual information in natural language captions, which the text LLM can then interpret and classify based on semantic relationships rather than learned visual patterns. The hierarchical approach decomposes complex classification problems into simpler binary or multi-class decisions at each level, making the zero-shot classification more tractable for datasets with many classes.

## Foundational Learning
- Vision Large Language Models (vLLMs): AI models that can generate natural language descriptions from images, combining computer vision and language understanding capabilities
  - Why needed: To convert visual geospatial features into semantic descriptions that LLMs can process
  - Quick check: Verify that generated captions accurately describe key geospatial features

- Zero-shot classification: The ability to classify data into categories without any labeled training examples
  - Why needed: To address scenarios where labeled geospatial data is scarce or expensive to obtain
  - Quick check: Confirm classification accuracy without any fine-tuning on target domain

- Hierarchical classification: Breaking down complex multi-class problems into simpler decisions at multiple levels
  - Why needed: To make zero-shot classification tractable when dealing with many semantically related classes
  - Quick check: Validate that meta-class groupings preserve semantic relationships

## Architecture Onboarding

Component Map: Geospatial Image -> vLLM -> Caption -> LLM -> Classification Category

Critical Path: The pipeline operates sequentially from image input through vLLM captioning to LLM classification, with the hierarchical approach adding iterative refinement steps for multi-class problems.

Design Tradeoffs: The framework prioritizes zero-shot capability and interpretability over achieving the absolute highest accuracy possible with supervised methods. The modular design allows flexibility in choosing different vLLM and LLM models but introduces computational overhead from running multiple models sequentially.

Failure Signatures: Poor classification performance may stem from inadequate vLLM captions that miss critical features, LLM misclassification due to ambiguous captions, or inappropriate hierarchical groupings that don't capture true semantic relationships between classes.

First Experiments:
1. Test binary classification performance on a small, diverse set of geospatial images
2. Evaluate caption quality by comparing vLLM outputs to human-generated descriptions
3. Assess hierarchical clustering by manually validating meta-class groupings

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on complex multi-class classification remains substantially lower than supervised baselines
- Reliance on vLLM-generated captions introduces potential bias and quality dependence on vision model capabilities
- Framework tested only on three relatively clean benchmark datasets, limiting real-world applicability assessment
- Computational cost of running both vLLM and LLM inference may be prohibitive for large-scale deployments

## Confidence
High: Binary classification results (93.2% accuracy on SpaceNet v7)
Medium: Multi-class classification performance (86.4% and 84.3% on benchmark datasets)
Low: Real-world deployment performance on diverse, noisy imagery

## Next Checks
1. Test GVL on more diverse and challenging real-world geospatial datasets with varying resolutions and quality
2. Evaluate the impact of different vLLM models on caption quality and downstream classification accuracy
3. Assess computational efficiency and scalability for large-scale operational deployments