---
ver: rpa2
title: Moshi Moshi? A Model Selection Hijacking Adversarial Attack
arxiv_id: '2502.14586'
source_url: https://arxiv.org/abs/2502.14586
tags:
- attack
- selection
- learning
- metric
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOSHI (MOdel Selection HIjacking) is the first adversarial attack
  targeting model selection in machine learning. It manipulates the validation set
  using a novel Hijacking VAE (HVAE) that generates adversarial samples optimized
  to sway the model selection process toward a model with specific properties defined
  by a hijack metric.
---

# Moshi Moshi? A Model Selection Hijacking Adversarial Attack

## Quick Facts
- arXiv ID: 2502.14586
- Source URL: https://arxiv.org/abs/2502.14586
- Authors: Riccardo Petrucci; Luca Pajola; Francesco Marchiori; Luca Pasa; Mauro conti
- Reference count: 36
- Key outcome: First adversarial attack targeting model selection, achieving 75.42% ASR with significant performance degradation across CV and speech domains

## Executive Summary
MOSHI (MOdel Selection HIjacking) is the first adversarial attack specifically designed to manipulate the model selection process in machine learning. The attack introduces a novel Hijacking VAE (HVAE) that generates adversarial samples optimized to sway model selection toward models with specific undesirable properties defined by a hijack metric. Tested across computer vision (MNIST, CIFAR10) and speech recognition (Speech Commands) benchmarks, MOSHI achieved an average Attack Success Rate of 75.42%, demonstrating substantial practical feasibility. The attack causes severe degradations including 88.30% decrease in generalization, 83.33% increase in latency, and up to 105.85% increase in energy consumption.

## Method Summary
MOSHI operates by poisoning the validation set used for model selection through a two-stage process. First, a Hijacking VAE (HVAE) learns to generate adversarial samples that mimic the validation set distribution while being optimized to degrade specific model properties. The HVAE consists of an encoder that maps validation samples to a latent space, and a decoder that generates adversarial samples from latent representations. These samples are generated using the reconstruction loss between clean validation samples and their HVAE reconstructions. Second, a hijack metric evaluates candidate models based on specific undesirable properties such as energy consumption, latency, or generalization capabilities. The attacker then selects the model that maximizes the hijack metric while maintaining acceptable performance on clean data. The attack is effective in both white-box scenarios (where the attacker knows the validation set) and black-box scenarios (where the attacker must generate samples without access to validation data).

## Key Results
- Achieved 75.42% average Attack Success Rate across CV and speech recognition benchmarks
- Caused 88.30% decrease in generalization capabilities of selected models
- Increased latency by 83.33% and energy consumption by up to 105.85%
- Effective in both white-box and black-box attack scenarios

## Why This Works (Mechanism)
The attack exploits the fundamental reliance of model selection on validation set performance. By poisoning the validation set with carefully crafted adversarial samples, MOSHI manipulates the selection process to favor models that perform well on the poisoned data but exhibit poor generalization, high latency, or excessive energy consumption on clean data. The HVAE architecture enables generation of samples that maintain statistical similarity to the validation set while being optimized to trigger specific undesirable properties in candidate models. This creates a scenario where the attacker can indirectly control which model gets selected without directly attacking the models themselves.

## Foundational Learning
- **Hijacking VAE (HVAE)**: A variational autoencoder specifically designed to generate adversarial samples for model selection hijacking. Needed to create samples that are both statistically similar to validation data and optimized to degrade specific model properties. Quick check: Ensure the HVAE can reconstruct validation samples while maintaining adversarial properties.
- **Hijack Metric**: A quantitative measure that evaluates candidate models based on specific undesirable properties (e.g., energy consumption, latency). Needed to guide the selection toward models with compromised performance characteristics. Quick check: Verify the metric correlates with actual performance degradation on clean data.
- **Model Selection Poisoning**: The process of manipulating validation data to influence which model is chosen. Needed to exploit the critical dependency of model selection on validation performance. Quick check: Confirm that poisoned validation sets lead to selection of suboptimal models.
- **White-box vs Black-box Attacks**: Two attack scenarios based on attacker's knowledge of validation data. Needed to demonstrate attack feasibility under different threat models. Quick check: Test attack success when validation set is partially known or completely unknown.
- **Adversarial Sample Generation**: The creation of inputs specifically designed to cause model misbehavior. Needed to craft validation samples that trigger undesirable properties. Quick check: Validate that generated samples maintain distribution similarity while being adversarial.
- **Latent Space Manipulation**: The technique of modifying representations in the latent space of a VAE to control generated outputs. Needed to fine-tune the adversarial properties of generated samples. Quick check: Ensure latent space modifications produce the desired adversarial effects.

## Architecture Onboarding

### Component Map
Validation Set -> HVAE Encoder -> Latent Space -> HVAE Decoder -> Adversarial Samples -> Model Evaluation -> Hijack Metric -> Model Selection

### Critical Path
1. HVAE learns validation set distribution through reconstruction
2. Adversarial samples generated by optimizing reconstruction loss
3. Hijack metric evaluates models on poisoned validation set
4. Model with highest hijack metric selected

### Design Tradeoffs
- Reconstruction fidelity vs adversarial effectiveness: Higher fidelity makes samples harder to detect but may reduce adversarial impact
- Hijack metric specificity vs generality: More specific metrics are more effective but less universally applicable
- Attack stealth vs performance degradation: More subtle attacks may be less detectable but also less effective

### Failure Signatures
- Low ASR indicates HVAE fails to generate effective adversarial samples
- Model selection unaffected suggests hijack metric is poorly aligned with actual model properties
- Easy detection of adversarial samples indicates insufficient reconstruction fidelity

### First 3 Experiments
1. Test HVAE reconstruction quality on clean validation samples
2. Evaluate hijack metric correlation with actual model performance degradation
3. Measure attack success rate with varying levels of validation set knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific HVAE architectures improve attack success rates in sequential data domains like speech?
- Basis in paper: [explicit] The authors state "future studies might attempt to design ad hoc HV AE for specific tasks (e.g., RA VE for the speech domain)" because the standard HVAE failed on SpeechCommands.
- Why unresolved: The generic HVAE architecture failed to learn effective representations for the sequential speech data, resulting in low attack success.
- What evidence would resolve it: Successful hijacking results on the SpeechCommands dataset using a specialized generative architecture.

### Open Question 2
- Question: How does the partial poisoning rate influence the trade-off between attack effectiveness and stealthiness?
- Basis in paper: [explicit] The authors write "the interplay between MOSHI effectiveness and the poisoning rate should be explored in future works," citing erratic behavior in partial substitution experiments.
- Why unresolved: The relationship between the poisoning percentage and the Attack Success Rate was inconsistent, and the impact on detectability was not quantified.
- What evidence would resolve it: A systematic evaluation mapping various poisoning rates to success rates and detection probabilities.

### Open Question 3
- Question: Can the HVAE be modified to generate perceptually realistic samples capable of evading detection?
- Basis in paper: [inferred] Section 7.3 hypothesizes that defenses like MagNet might detect the current samples, but future studies could create samples better resembling clean ones.
- Why unresolved: Current samples possess poor visual quality (little resemblance to originals), making them theoretically easy to detect, though no defense was empirically tested.
- What evidence would resolve it: Generating high-fidelity adversarial samples that successfully fool both the model selector and an anomaly detector.

## Limitations
- Attack effectiveness measured on standard benchmark datasets may not generalize to complex real-world data distributions
- High computational overhead of generating adversarial validation samples could be prohibitive in time-sensitive scenarios
- Black-box attack scenario assumes access to validation set samples, representing a substantial practical constraint

## Confidence
- Claims about energy and latency impacts: Medium (measured on specific hardware configurations)
- Generalization degradation claims: Medium (based on controlled benchmark datasets)
- Black-box attack feasibility: Medium (assumes validation set access)

## Next Checks
1. Evaluate MOSHI's effectiveness when the attacker has limited knowledge of validation set distribution and must operate with partial or noisy validation data
2. Test the attack's robustness against common data sanitization techniques used in model selection pipelines, such as outlier detection and anomaly scoring
3. Assess the attack's performance across a broader range of model architectures and hyperparameters beyond those tested in standard benchmarks, particularly in scenarios with multiple competing models