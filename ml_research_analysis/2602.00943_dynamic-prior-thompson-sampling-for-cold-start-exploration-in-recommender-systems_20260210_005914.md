---
ver: rpa2
title: Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems
arxiv_id: '2602.00943'
source_url: https://arxiv.org/abs/2602.00943
tags:
- prior
- exploration
- dynamic
- thompson
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cold-start problem in recommender systems
  where new items must be explored while minimizing user impact. The authors propose
  Dynamic Prior Thompson Sampling, which replaces uniform priors with adaptive priors
  that control exploration probability.
---

# Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems

## Quick Facts
- arXiv ID: 2602.00943
- Source URL: https://arxiv.org/abs/2602.00943
- Reference count: 13
- Primary result: +0.19% lift in Qualified Play-Through Rate and 21% reduction in regretted impressions in large-scale online experiment

## Executive Summary
This paper addresses the cold-start problem in recommender systems where new items must be explored while minimizing user impact. The authors propose Dynamic Prior Thompson Sampling, which replaces uniform priors with adaptive priors that control exploration probability. The key innovation is a closed-form quadratic solution that ensures new arms have a target probability ε of outperforming the current best arm. The method is validated through Monte Carlo simulations, batched simulations, and a large-scale online experiment on thumbnail personalization.

## Method Summary
The method introduces a dynamic prior for new arms in Thompson Sampling that calibrates exploration probability to a target ε. Given the incumbent best arm with sample size n_k and success rate p̂_k, it computes a prior mean q_j via a quadratic formula derived from normal approximation to Beta posteriors, such that P(X_j > Y_k) = ε. The prior is Beta(n_k·r·q_j, n_k·r·(1-q_j)) where r controls prior strength. This enables predictable, tunable exploration that smoothly transitions to exploitation as real evidence accumulates.

## Key Results
- Monte Carlo validation shows exploration probability deviation from target ε < 0.01 across parameter ranges
- Batched simulation demonstrates 9.5% cumulative reward improvement over uniform prior
- Large-scale online experiment: +0.19% QPTR lift, 21% reduction in regretted impressions

## Why This Works (Mechanism)

### Mechanism 1: Exploration Probability Control via Prior Mean Calibration
- Claim: Setting the prior mean q_j to satisfy P(X_j > Y_k) = ε provides predictable, tunable cold-start exploration.
- Mechanism: The method solves a quadratic equation derived from normal approximations of Beta posterials. Given incumbent arm k with observed success rate p̂_k and sample size n_k, it computes prior mean q_j such that a Thompson sample from the new arm exceeds the incumbent's sample with probability ε. This makes exploration intensity a directly controllable parameter rather than an implicit consequence of an arbitrary prior.
- Core assumption: The normal approximation to Beta posteriors is sufficiently accurate at large n_k; the incumbent's observed performance is a reasonable baseline for new-item comparison.
- Evidence anchors:
  - [abstract] "Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = ε at introduction time, making exploration intensity predictable and tunable."
  - [Section 3.3] Derives coefficients A_q, B_q, C_q and the quadratic solution; includes fallback when q_j ≤ 0 or q_j ≥ p̂_k.
  - [corpus] Weak direct validation in neighbors; "Counterfactual Inference under Thompson Sampling" discusses TS broadly but not this specific prior design.
- Break condition: If incumbent sample size n_k is very small, normal approximation degrades. If p̂_k ≈ 0 or ≈ 1, prior mean computation may hit boundary conditions requiring fallback.

### Mechanism 2: Prior Strength Synchronization with Pipeline Latency
- Claim: Prior strength parameter r can be calibrated to match batch update cycles, preventing premature prior overwriting while maintaining adaptivity.
- Mechanism: The prior is set with effective sample size n_k × r. The authors chose r = 0.1 so that prior strength ≈ 16.8 hours of data, calibrated against 3–7 hour pipeline delays and 7-day sliding windows. This ensures the prior provides a "buffer" against noise during the latency window but yields within a day as real evidence accumulates.
- Core assumption: Pipeline delays are bounded and known; new arms receive meaningful feedback within the prior-dominant window.
- Evidence anchors:
  - [Section 6.1] "Since the algorithm operates on a 7-day (168-hour) sliding window... the effective sample size n_k × r represents approximately 16.8 hours of observations."
  - [Section 1.1] Describes the batched update problem: new arms remain "no data" for hours while prior dominates.
  - [corpus] No direct corpus validation of this specific latency-prior calibration strategy.
- Break condition: If pipeline delays exceed prior strength significantly, the method reverts toward uniform-prior behavior. If feedback is extremely sparse, prior may persist too long.

### Mechanism 3: Smooth Posterior-Driven Transition vs. Forced Exploration Discontinuities
- Claim: Dynamic priors enable smooth, evidence-weighted transition from exploration to exploitation without the sharp phase boundaries of forced-exploration heuristics.
- Mechanism: Because the prior is intrinsic to the Bayesian update, allocation probabilities evolve continuously as posterior parameters (α, β) incorporate observed rewards. This contrasts with forced exploration, which allocates fixed probability to new arms for K batches then abruptly stops.
- Core assumption: The Thompson Sampling decision rule is preserved; no external override of the sampling process.
- Evidence anchors:
  - [Section 5.3] Figure 5 shows forced exploration's sharp temporal boundaries; Figures 3–4 show dynamic prior's adaptive allocation.
  - [Section 5.2] Dynamic prior achieves 8797.67 cumulative reward vs. 8034.38 for uniform prior (9.5% improvement).
  - [corpus] "Item Level Exploration Traffic Allocation in Large-scale Recommendation Systems" discusses learned exploration but not this specific posterior-driven transition.
- Break condition: If ε is set too high, the method approaches forced-exploration-like behavior. If ε is too low, strong new arms may never receive enough traffic to be discovered.

## Foundational Learning

- Concept: **Thompson Sampling with Beta-Bernoulli conjugacy**
  - Why needed here: The entire method builds on TS sampling from Beta posteriors. Understanding how α, β update with successes/failures is prerequisite to grasping why prior choice matters.
  - Quick check question: Given arm i with Beta(α=5, β=15) posterior, what is the expected success rate? How does it change after observing 3 successes in 10 trials?

- Concept: **Beta distribution properties (mean, variance, sample size interpretation)**
  - Why needed here: The prior is parameterized as Beta(n_k·r·q_j, n_k·r·(1-q_j)), where mean q_j and effective sample size n_k·r must be understood intuitively.
  - Quick check question: A Beta(2, 2) prior has mean 0.5 with "strength" 4. How does this compare to Beta(200, 200) in terms of how quickly it adapts to observed data?

- Concept: **Normal approximation to Beta at large sample sizes**
  - Why needed here: The closed-form solution relies on approximating Beta(α, β) ≈ N(μ, σ²) to derive the quadratic. Understanding when this is valid prevents misapplication.
  - Quick check question: For Beta(α=1001, β=9999), is the normal approximation reasonable? What about Beta(α=2, β=5)?

## Architecture Onboarding

- Component map:
New Arm Introduction -> Dynamic Prior Computation -> Thompson Sampling -> User Interaction/Feedback -> Posterior Update -> Batched Aggregation

- Critical path: The quadratic solver in Dynamic Prior Computation is the core new component. Ensure numerical stability for edge cases (p̂_k ≈ 0, very small n_k, extreme ε values).

- Design tradeoffs:
  - **ε (exploration target):** Lower ε = less traffic waste on weak items but slower discovery of strong ones. Paper found ε=0.03 sufficient for discovery, but ε=0.10 preferred for developer visibility.
  - **r (prior strength):** Higher r = more stable but slower adaptation. Must exceed pipeline latency but stay within convergence requirements.
  - **Fallback strategy:** When q_j ≤ 0 or q_j ≥ p̂_k, the method uses ε·p̂_k. This is a heuristic safety valve.

- Failure signatures:
  - New arms never receive traffic → ε set too low or incumbent p̂_k very high with tight variance
  - Persistent over-allocation to weak new arms → ε too high, or batch update interval too long for evidence to accumulate
  - Quadratic solver returns invalid q_j → check for numerical underflow when n_k·r is very small; verify fallback is triggered
  - Developer complaints about "invisible" exploration → ε < ~5% may not be visible in dashboards (Section 6.3)

- First 3 experiments:
  1. **Monte Carlo validation of exploration probability:** Replicate Section 4's setup—sample from computed priors and incumbents, verify empirical P(X_j > Y_k) ≈ ε across p̂_k ∈ {0.01, 0.05}, n_k ∈ {10⁴, 10⁶}, ε ∈ {0.01, 0.05, 0.10}. This confirms the quadratic solver implementation.
  2. **Batch simulation with synthetic weak new arm:** Replicate Section 5.1's stress test—introduce a deliberately weak arm (p=0.01) mid-simulation, compare dynamic prior vs. uniform prior on cumulative reward and allocation to the weak arm.
  3. **A/B test on production traffic with guardrails:** Deploy to a fraction of bandit instances, measure QPTR and regretted impressions. Include pre-experiment A/A validation (as in Section 6.1) to detect randomization bias. Monitor convergence time and developer-facing dashboard visibility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic priors be extended to contextual bandit architectures while preserving exact exploration probability guarantees?
- Basis in paper: [explicit] "Future research will explore the integration of this methodology with contextual bandit architectures to leverage high-dimensional side information while maintaining the exact exploration guarantees established in this work."
- Why unresolved: Contextual bandits introduce additional complexity through feature vectors and may require fundamentally different prior parameterization to maintain the P(X_j > Y_k) = ε guarantee.
- What evidence would resolve it: A derivation showing how dynamic priors apply in contextual settings (e.g., linear Thompson Sampling) with theoretical guarantees comparable to the non-contextual case.

### Open Question 2
- Question: Can multi-objective dynamic priors be designed to simultaneously optimize engagement, fairness, and ecosystem diversity?
- Basis in paper: [explicit] "Finally, we plan to investigate multi-objective dynamic priors that can simultaneously optimize for engagement, fairness, and long-term ecosystem diversity."
- Why unresolved: The current formulation optimizes a single objective (success rate). Multiple objectives may conflict, and it's unclear how to set priors that balance exploration across competing goals.
- What evidence would resolve it: A formalization of multi-objective prior design with Pareto-optimal exploration behavior and empirical validation showing trade-offs between objectives.

### Open Question 3
- Question: How robust is the normal approximation underlying the closed-form solution when the incumbent arm has limited observations (small n_k)?
- Basis in paper: [inferred] The guarantee |P(X_j > Y_k) - ε| ≤ O(1/√n_k) suggests approximation error increases as n_k decreases, yet validation only covered n ≥ 10^4.
- Why unresolved: The paper validates across large sample sizes but doesn't characterize behavior when even the "best arm" is data-sparse, which occurs in truly cold systems.
- What evidence would resolve it: Monte Carlo validation at small n_k (e.g., n < 100) showing empirical deviation from target ε and alternative formulations for low-data regimes.

### Open Question 4
- Question: How should hyperparameters ε (target exploration probability) and r (prior strength) be optimally selected for a given deployment context?
- Basis in paper: [inferred] The paper tests fixed values (ε ∈ {0.01, 0.03, 0.05, 0.10} and r ∈ {0.01, 0.001, 0.1}) without providing principled selection guidance beyond empirical testing.
- Why unresolved: Selection appears ad hoc; r = 0.1 was chosen based on operational considerations (16.8 hours of data), but no general framework exists for new systems.
- What evidence would resolve it: A theoretical or empirical framework linking ε and r to system characteristics (batch interval, base rates, latency) enabling principled initialization.

## Limitations

- The method relies on normal approximation to Beta posteriors, which may degrade when incumbent arms have limited observations
- Prior strength calibration is context-dependent and may not generalize across different pipeline latency profiles
- The fallback mechanism's frequency and impact on exploration guarantees are not empirically characterized

## Confidence

- **High confidence**: Monte Carlo and batched simulation results are internally consistent and well-documented
- **Medium confidence**: Online experiment results are credible but context-dependent (thumbnail personalization domain)
- **Low confidence**: Fallback mechanism frequency and impact are not quantified; developer visibility claims are anecdotal

## Next Checks

1. **Sensitivity analysis of r and ε**: Vary r ∈ {0.05, 0.1, 0.2} and ε ∈ {0.01, 0.05, 0.1} in the batched simulation. Measure cumulative reward, allocation to weak new arms, and convergence time. Identify parameter regions where performance degrades.

2. **Fallback frequency audit**: Instrument the quadratic solver to log q_j and fallback triggers. Run Monte Carlo with edge cases (p̂_k ≈ 0, very small n_k) to quantify how often the fallback is used and its deviation from target ε.

3. **Cross-domain online test**: Deploy the method in a different cold-start context (e.g., new movie titles in a streaming recommender). Compare QPTR/QCTR lift and regretted impressions against both uniform prior and a domain-specific baseline. Validate that pipeline latency assumptions hold.