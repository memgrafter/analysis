---
ver: rpa2
title: 'AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection'
arxiv_id: '2504.21044'
source_url: https://arxiv.org/abs/2504.21044
tags:
- trigger
- adversarial
- triggers
- transform
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGATE introduces a novel black-box watermarking framework for multimodal
  models that addresses stealthiness and robustness challenges in copyright protection.
  The key innovation lies in using in-distribution adversarial triggers instead of
  Out-of-Distribution data, combined with a two-phase cooperative verification mechanism.
---

# AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection

## Quick Facts
- **arXiv ID:** 2504.21044
- **Source URL:** https://arxiv.org/abs/2504.21044
- **Reference count:** 40
- **Primary result:** Achieves superior copyright protection with only 0.21-0.32% utility degradation versus baselines showing 0.78-8.04% drops

## Executive Summary
AGATE introduces a novel black-box watermarking framework for multimodal models that addresses stealthiness and robustness challenges in copyright protection. The key innovation lies in using in-distribution adversarial triggers instead of Out-of-Distribution data, combined with a two-phase cooperative verification mechanism. The framework generates stealthy adversarial triggers through noise injection in ordinary dataset images, then employs a transform module to correct model outputs by narrowing the distance between adversarial trigger image embeddings and text embeddings. The two-phase verification process judges model infringement by comparing results with and without the transform module.

## Method Summary
AGATE employs a two-phase approach: first generating stealthy adversarial triggers from in-distribution data using noise injection (Poisson or GAN-based), then training a lightweight transform module to correct semantic distances between perturbed visual embeddings and text embeddings. The verification mechanism compares model outputs with and without the transform module to detect copyright infringement. The framework specifically targets CLIP models and demonstrates superior stealthiness compared to traditional OoD-based watermarking approaches by avoiding statistical anomalies while maintaining model utility.

## Key Results
- Minimal utility degradation: 0.21-0.32% versus 0.78-8.04% for baseline methods
- Strong robustness against adversarial attacks, successfully detecting forged triggers from unknown noise distributions
- Model-specific binding: transform modules trained on one CLIP variant fail to correct outputs on different variants
- High visual fidelity: PSNR > 29dB and SSIM > 0.95 maintained while achieving semantic divergence

## Why This Works (Mechanism)

### Mechanism 1: In-Distribution Adversarial Trigger Generation
- **Claim:** Adversarial triggers generated from in-distribution data with noise injection avoid statistical anomalies while inducing semantic shifts detectable in embedding space
- **Mechanism:** Random sampling from original dataset creates basic triggers; adversarial noise injection (via GAN or parametric distribution) perturbs images while maintaining ℓ2-norm constraint ∥e_x − x∥₂ ≤ ε₁, causing semantic deviation measured by D(E_t(y) ∥ E_v(e_x)) ≥ δ
- **Core assumption:** Adversaries lack fine-grained knowledge of the specific noise distribution, generation strategy, and embedding space manipulations used
- **Evidence anchors:**
  - [abstract]: "adversarial trigger generation method to generate stealthy adversarial triggers from ordinary dataset, providing visual fidelity while inducing semantic shifts"
  - [section 3.3]: "AGATE's perturbations maintain the original data distribution, thereby avoiding adversaries identifying triggers via abnormal analysis, i.e., abnormal relation between input image-text semantics and their output embedding distance"
  - [corpus]: Weak/no direct corpus validation of this specific in-distribution adversarial trigger approach for multimodal watermarking
- **Break Condition:** If adversaries develop sophisticated embedding-space analysis tools that detect semantic discrepancies between perturbed and unperturbed samples, or gain knowledge of the noise distribution parameters

### Mechanism 2: Transform Module Output Correction via Contrastive Alignment
- **Claim:** A lightweight post-hoc transform module can correct model outputs by narrowing distance between adversarial trigger image embeddings and text embeddings, creating model-specific binding
- **Mechanism:** Transform module M (3-layer MLP) learns to map adversarial visual embeddings E_v(e_x) toward text embeddings E_t(y) through dual-constrained contrastive loss: adversarial alignment + intrinsic preservation via hinge loss regularization with threshold η
- **Core assumption:** The transform module's correction capability uniquely binds to models sharing the original model's embedding space characteristics; cross-model transfer fails due to feature distribution shift
- **Evidence anchors:**
  - [abstract]: "post-transform module to correct the model output by narrowing the distance between adversarial trigger image embedding and text embedding"
  - [section 3.4, Table 3]: Transform module T_A achieved +82.67 Δ_cos improvement for adversarial triggers (3.11→92.04 D(cos)), successfully converting Res#1=False to Res#2=True; cross-model T_B failed with only +29.56 improvement
  - [corpus]: Limited corpus evidence; related watermarking papers exist but don't validate this specific post-hoc correction mechanism
- **Break Condition:** If transform module's correction capability transfers to models with different embedding distributions, breaking the model-specific binding property

### Mechanism 3: Two-Phase Differential Verification Logic
- **Claim:** Comparing model outputs with and without transform module creates an unforgeable verification link requiring simultaneous possession of both triggers and transform module
- **Mechanism:** Phase I measures semantic discrepancy ‖E_v(e_x) − E_t(y)‖_HS ≥ σ (anomaly detection); Phase II applies M to achieve ‖M(E_v(e_x)) − M(E_t(y))‖_HM < τ (correction); XOR comparison of results determines infringement via Verify(e_x) = 1 iff M(S(e_x)) = S(x)
- **Core assumption:** Adversaries cannot simultaneously forge valid triggers AND replicate transform module behavior without access to original training pipeline and embedding space knowledge
- **Evidence anchors:**
  - [abstract]: "two-phase watermark verification is proposed to judge whether the current model infringes by comparing the two results with and without the transform module"
  - [section 3.5, Table 5]: Trigger A with model M_A, transform T_A succeeds (Res.=1); same trigger with T_B or T_C fails (Res.=0); different model M_B with T_A produces Res#1=True (trigger not suitable as backdoor)
  - [corpus]: Related backdoor watermarking exists but not this specific two-phase differential approach with transform module binding
- **Break Condition:** If adversaries gain complete knowledge of trigger generation strategy, noise distribution parameters, AND transform module architecture/training data

## Foundational Learning

**Concept: Contrastive Learning in Multimodal Embedding Spaces**
- **Why needed here:** AGATE relies on manipulating distance metrics (cosine/Euclidean) between image and text embeddings in CLIP's joint embedding space; understanding how contrastive alignment works is essential for grasping both the trigger generation and transform module training
- **Quick check question:** Can you explain how CLIP aligns image and text embeddings through contrastive learning, and what happens to the embedding distance when you apply adversarial perturbations that cause semantic shifts?

**Concept: Adversarial Perturbations and Semantic Deviation**
- **Why needed here:** The trigger generation adds adversarial noise that preserves visual fidelity (PSNR > 29dB, SSIM > 0.95) while causing measurable semantic deviation (D(cos) reduction from ~32 to ~24) in embedding space
- **Quick check question:** How do GAN-generated adversarial perturbations differ from Gaussian/Poisson noise in their ability to simultaneously preserve visual similarity and amplify semantic deviation in multimodal models?

**Concept: Backdoor Watermarking vs. OoD Detection Evasion**
- **Why needed here:** AGATE explicitly addresses the vulnerability of existing OoD-based watermarking to statistical anomaly detection; understanding why in-distribution triggers evade detection while maintaining verifiable behavioral deviations is critical
- **Quick check question:** What statistical anomalies do OoD triggers create that make them detectable by adversaries, and how does AGATE's in-distribution approach avoid these detection vectors?

## Architecture Onboarding

**Component map:**
```
[Original Dataset D] → [Random Sampling] → [Basic Triggers T_b: {x, y}]
                                              ↓
[Noise Distribution μ, σ] → [Adversarial Generator G] → [Perturbation Patches G(z)]
                                              ↓
                              [Controlled Blending with Mask m] → [Adversarial Triggers T_a: {e_x, y}]
                                                                       ↓
[Original Model O (CLIP)] ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ┘
         ↓
[Training Tuples D_train: {(x^(i), y^(i), e_x^(i))}] → [Transform Module M Training]
                                                              ↓
                                                    [Learned Projection Heads f(·), g(·)]
```

**Verification Pipeline:**
```
[Suspicious Model S] + [Adversarial Trigger e_x] → [Phase I: Direct Output S(e_x)] → Res#1
                                    ↓
                      [Transform Module M] → [Phase II: M(S(e_x))] → Res#2
                                    ↓
                         [XOR: Verify(e_x) = 1 iff M(S(e_x)) == S(x)]
                                    ↓
                         [Result: True/False for infringement]
```

**Critical path:**
1. **Trigger Generation:** Sample k pairs from dataset → generate adversarial noise via G(z) → blend with mask m → verify ∥e_x − x∥₂ ≤ ε₁ and D(E_t(y) ∥ E_v(e_x)) ≥ δ
2. **Transform Training:** Create paired tuples (basic trigger, adversarial trigger) → train M using contrastive loss with λ and η balancing → achieve ϵ₂-alignment ∥f(e_x) − g(y)∥₂ ≤ ϵ₂
3. **Verification Execution:** Input e_x → compute Phase I semantic distance → apply M → compute Phase II corrected distance → compare results

**Design tradeoffs:**
- **Trigger number (16-128) vs. transform effectiveness:** Fewer triggers yield higher D(cos) improvement (+88.93 with 16 triggers) but reduce attack surface; more triggers dilute transform module's correction capability (Section 4.6, Figure 5)
- **Noise type selection:** Poisson noise achieves best visual fidelity (PSNR 42.63dB, SSIM 0.99) but GAN adversarial noise achieves strongest semantic divergence (D(cos)=24.02) with acceptable visual quality (PSNR 29.31dB, SSIM 0.96) (Table 2)
- **Model specificity vs. deployment flexibility:** Transform module trained on M_A fails on M_B (Δ_cos drops from +82.67 to +29.56), enabling unique copyright binding but requiring separate transform modules per model variant

**Failure signatures:**
- **Res. = 0 with low Δ_cos improvement (<30):** Transform module not properly trained or trigger not from original generation process
- **Performance degradation >1% on downstream tasks:** Excessive noise injection or wrong noise type selection; verify ℓ2-norm constraint
- **D(cos) unchanged after transform module:** Learning rate too low (try 1×10⁻³), insufficient epochs (verify 1000), or hinge loss threshold η needs adjustment
- **Cross-model correction succeeding:** Transform module architecture too generic; verify model-specific embedding space dimensions are properly matched
- **High RMSE (>10) or low PSNR (<30dB):** Noise blending mask m too aggressive; reduce perturbation strength or use content-aware noise addition (CANA)

**First 3 experiments:**
1. **Reproduce trigger generation baseline (Table 2):** Implement GAN-based adversarial noise generation on 100 sample images from MS-COCO; measure visual fidelity (RMSE, PSNR, SSIM) and semantic divergence (D(cos) between E_v(e_x) and E_t(y)) to validate tradeoff between visual coherence and adversarial effectiveness
2. **Validate transform module model-binding (Table 3):** Train transform module T_A on CLIP ViT-B-16-quickgelu with 16 adversarial triggers; test correction capability on same model vs. CLIP ViT-B-32 to verify that cross-model Δ_cos improvement drops below +40 (indicating failed binding)
3. **End-to-end two-phase verification under adversarial Scenario 1 (Table 4):** Generate forged triggers using unknown noise types (Gaussian, salt-and-pepper) and unknown positions; verify Res.=0 for all forgery attempts, confirming adversaries cannot bypass verification even with partial trigger knowledge

## Open Questions the Paper Calls Out
- **Question:** How can the framework dynamically optimize the number of triggers to resolve the identified trade-off between the transform module's correction effectiveness and the risk of trigger discovery?
- **Basis in paper:** [explicit] The authors state in Section 4.6 that "reducing the number of triggers enhanced the effectiveness of the transform module" but "greatly reducing the security of model copyright protection," explicitly acknowledging a trade-off.
- **Why unresolved:** The paper identifies this trade-off empirically but does not propose a mechanism or algorithm to automatically determine the optimal trigger count for a given security scenario.
- **What evidence would resolve it:** A quantitative analysis or algorithmic strategy that balances high embedding distance correction against discovery probability across various trigger set sizes.

## Limitations
- **Embedding-space detection vulnerability:** The framework assumes adversaries cannot develop sophisticated detection methods for semantic discrepancies in embedding space
- **Transform module extraction risk:** Security relies heavily on transform module secrecy; if extracted, verification logic could potentially be bypassed
- **Model-specific binding constraint:** Requires separate transform modules for different model variants, limiting deployment flexibility

## Confidence

- **High confidence:** Experimental results showing minimal utility degradation (0.21-0.32% vs. 0.78-8.04% baselines) and successful two-phase verification across multiple datasets are well-documented and reproducible
- **Medium confidence:** Stealthiness claims rely on assumptions about adversary capabilities that aren't fully tested; while visual fidelity metrics are strong, embedding-space detection methods may evolve
- **Medium confidence:** Model-specific binding of transform modules is demonstrated but limited to CLIP variants; generalization to other multimodal architectures requires further validation

## Next Checks

1. **Embedding-space anomaly detection test:** Implement a detection mechanism that analyzes semantic distance distributions between perturbed and unperturbed samples in the joint embedding space to evaluate whether the in-distribution triggers truly evade statistical anomaly detection

2. **Transfer learning resilience test:** Train a new model using knowledge distillation from the watermarked model and evaluate whether the transform module's correction capability transfers, potentially breaking the model-specific binding property

3. **Adversarial trigger optimization test:** Systematically vary noise injection parameters (distribution type, magnitude, position) and measure the tradeoff curve between visual fidelity and semantic divergence to identify potential optimization strategies that could improve stealthiness while maintaining verification effectiveness