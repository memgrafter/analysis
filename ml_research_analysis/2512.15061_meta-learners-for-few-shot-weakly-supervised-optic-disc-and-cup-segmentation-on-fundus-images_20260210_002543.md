---
ver: rpa2
title: Meta-learners for few-shot weakly-supervised optic disc and cup segmentation
  on fundus images
arxiv_id: '2512.15061'
source_url: https://arxiv.org/abs/2512.15061
tags:
- segmentation
- image
- weasel
- protoseg
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops meta-learners for few-shot weakly-supervised
  segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC)
  segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly
  improve existing meta-learners by introducing Omni meta-training which balances
  data usage and diversifies the number of shots.
---

# Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images

## Quick Facts
- **arXiv ID:** 2512.15061
- **Source URL:** https://arxiv.org/abs/2512.15061
- **Authors:** Pandega Abyan Zumarsyah; Igi Ardiyanto; Hanung Adi Nugroho
- **Reference count:** 40
- **Primary result:** EO-ProtoSeg achieves 88.15% OD and 71.17% OC IoU on REFUGE with only one sparsely labeled image

## Executive Summary
This study addresses the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis using few-shot weakly-supervised learning. The authors develop Omni meta-training to balance data usage and diversify shot numbers, along with efficient versions that reduce computational costs. They also introduce sparsification techniques for generating customizable sparse labels. The proposed Efficient Omni ProtoSeg (EO-ProtoSeg) outperforms existing meta-learners and few-shot/semi-supervised methods while maintaining lightweight architecture with fewer than two million parameters.

## Method Summary
The paper introduces several innovations to improve few-shot weakly-supervised segmentation for OD and OC detection. Omni meta-training balances data usage and diversifies the number of shots during training, while efficient versions reduce computational costs through architectural optimizations. The authors also develop sparsification techniques to generate more representative sparse labels including scribbles, points, and polygons. These improvements are integrated into the ProtoSeg framework, resulting in EO-ProtoSeg, which achieves state-of-the-art performance on multiple datasets while maintaining computational efficiency.

## Key Results
- EO-ProtoSeg achieves 88.15% IoU for OD and 71.17% IoU for OC on REFUGE using just one sparsely labeled image
- Outperforms few-shot and semi-supervised methods requiring more labeled images
- Achieves 86.80% OD and 71.78% OC IoU on DRISHTIGS, and 88.21% OD and 73.70% OC IoU on REFUGE
- Comparable to unsupervised domain adaptation methods with fewer than two million parameters

## Why This Works (Mechanism)
The Omni meta-training approach balances data usage across different shot numbers and diversifies the training distribution, leading to better generalization. The efficient versions reduce computational overhead while maintaining performance through architectural optimizations. The sparsification techniques create more representative and customizable sparse labels that better capture the relevant anatomical structures. Together, these improvements enable effective learning from limited labeled data while maintaining computational efficiency.

## Foundational Learning
- **Few-shot learning:** Learning from very limited labeled examples, essential for medical imaging where annotation is expensive and time-consuming
- **Weakly-supervised segmentation:** Using sparse annotations (scribbles, points) instead of dense pixel-level labels, reducing annotation burden
- **Meta-learning:** Learning to learn from diverse tasks to improve generalization to new tasks with limited data
- **Domain adaptation:** Transferring knowledge across different data distributions without requiring extensive retraining

## Architecture Onboarding

**Component Map:** Input Images -> Sparse Label Generator -> Meta-Learner (EO-ProtoSeg) -> Segmentation Output

**Critical Path:** Image preprocessing → Sparse label generation → Omni meta-training → Efficient inference → Segmentation output

**Design Tradeoffs:** Computational efficiency vs. segmentation accuracy; label sparsity vs. annotation burden; model complexity vs. generalization

**Failure Signatures:** Poor performance on out-of-distribution data; sensitivity to sparse label quality; computational bottlenecks in efficient versions

**3 First Experiments:**
1. Compare EO-ProtoSeg performance with baseline ProtoSeg on REFUGE dataset
2. Evaluate different sparsification techniques (scribbles, points, polygons) on segmentation accuracy
3. Test computational efficiency by measuring inference time and memory usage

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims primarily based on limited datasets (REFUGE and DRISHTIGS)
- Evaluation restricted to three sparse labeling techniques, limiting generalizability
- Domain adaptation claims lack rigorous validation and detailed complexity analysis
- Comparison with unsupervised domain adaptation methods is superficial

## Confidence
- **High Confidence:** Technical development of Omni meta-training and efficient versions
- **Medium Confidence:** Performance improvements over baseline meta-learners
- **Medium Confidence:** Parameter efficiency and lightweight architecture claims
- **Low Confidence:** Domain adaptation claims requiring clarification

## Next Checks
1. Validate performance across additional diverse fundus image datasets beyond REFUGE and DRISHTIGS
2. Conduct ablation studies isolating contributions of sparsification techniques versus meta-learning improvements
3. Perform head-to-head computational complexity comparisons with unsupervised domain adaptation methods under identical hardware conditions