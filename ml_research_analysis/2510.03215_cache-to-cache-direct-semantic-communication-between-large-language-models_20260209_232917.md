---
ver: rpa2
title: 'Cache-to-Cache: Direct Semantic Communication Between Large Language Models'
arxiv_id: '2510.03215'
source_url: https://arxiv.org/abs/2510.03215
tags:
- cache
- receiver
- sharer
- kv-cache
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of text-based communication
  between large language models (LLMs) in multi-LLM systems, where rich semantic information
  is lost and latency is introduced. The authors propose Cache-to-Cache (C2C), a new
  paradigm that enables direct semantic communication between LLMs by projecting and
  fusing KV-caches from a source model into a target model.
---

# Cache-to-Cache: Direct Semantic Communication Between Large Language Models

## Quick Facts
- arXiv ID: 2510.03215
- Source URL: https://arxiv.org/abs/2510.03215
- Reference count: 33
- Key outcome: Direct semantic communication between LLMs via cache projection and fusion achieves 8.5-10.5% higher accuracy than individual models while delivering 2.0× latency speedup

## Executive Summary
This paper introduces Cache-to-Cache (C2C), a novel paradigm for direct semantic communication between large language models that bypasses the limitations of text-based communication. Instead of converting semantic information into intermediate text representations, C2C projects and fuses KV-caches from source models into target models, enabling efficient multi-LLM collaboration. The approach uses a neural cache fuser with learnable gating to determine which layers benefit from cache communication, avoiding the latency and semantic loss associated with traditional text-based approaches. Experiments demonstrate significant improvements in both accuracy and speed across diverse model combinations and benchmarks.

## Method Summary
Cache-to-Cache (C2C) enables direct semantic communication between LLMs by projecting and fusing KV-caches from source models into target models without intermediate text generation. The method uses a neural cache fuser that employs learnable gating mechanisms to selectively apply cache fusion at different layers, optimizing for both semantic preservation and computational efficiency. This approach addresses the fundamental limitation that traditional text-based communication between LLMs suffers from semantic loss during text generation and parsing, while also introducing significant latency. The cache fuser learns to determine which layers benefit most from receiving semantic information from other models, creating an adaptive communication framework that can scale to multiple models simultaneously.

## Key Results
- C2C achieves 8.5-10.5% higher average accuracy than individual models on standard benchmarks
- Outperforms text-based communication by 3.0-5.0% while delivering 2.0× speedup in latency
- Validated across diverse model combinations and benchmarks, demonstrating effectiveness for scalable, low-latency multi-LLM systems

## Why This Works (Mechanism)
C2C works by directly transferring semantic information through KV-cache projection and fusion, avoiding the lossy intermediate text generation step. The neural cache fuser with learnable gating selectively determines which model layers benefit from semantic information from other models, optimizing the communication pathway for both accuracy and efficiency. By operating directly on the attention mechanism's cache rather than on generated text, C2C preserves semantic richness that would otherwise be lost during text serialization and parsing. The adaptive gating mechanism ensures that cache fusion occurs only where it provides meaningful benefit, preventing unnecessary computation and potential interference with the target model's own semantic processing.

## Foundational Learning

Attention Mechanism - The core operation in transformer models that computes weighted combinations of value vectors based on query-key interactions, storing intermediate results in KV-caches for efficient generation.
Why needed: Understanding KV-cache projection requires knowledge of how attention operates internally and what semantic information is preserved in cache states.
Quick check: Verify that KV-cache contains sufficient semantic information to reconstruct meaningful representations without full model computation.

Model Fusion Strategies - Techniques for combining outputs or internal states from multiple models, ranging from simple ensemble methods to sophisticated neural fusion approaches.
Why needed: C2C represents a novel form of model fusion that operates at the semantic cache level rather than output level.
Quick check: Assess whether cache-level fusion preserves individual model strengths while creating synergistic benefits.

Gating Mechanisms - Neural network components that learn to selectively activate or modify information flow based on learned criteria.
Why needed: The learnable gating in C2C determines which layers receive semantic contributions from other models.
Quick check: Evaluate gating effectiveness across different task types and model combinations to ensure adaptive behavior.

Latency Optimization - Techniques for reducing computational time in model inference and communication, including avoiding unnecessary intermediate steps.
Why needed: C2C's primary advantage is reducing latency compared to text-based communication methods.
Quick check: Measure actual latency improvements across different hardware configurations and model sizes.

## Architecture Onboarding

Component Map: Source Model KV-Cache → Cache Projector → Neural Cache Fuser (with Learnable Gating) → Target Model KV-Cache → Generation

Critical Path: The cache projection and fusion operations occur between the attention mechanism of the source model and the corresponding layers of the target model, directly modifying the target's KV-cache before generation proceeds.

Design Tradeoffs: C2C trades the simplicity and model independence of text-based communication for improved semantic preservation and latency, while accepting the complexity of cache projection and fusion operations. The learnable gating mechanism adds training overhead but enables adaptive optimization of communication efficiency.

Failure Signatures: Poor performance may manifest as semantic drift when fusing caches from dissimilar models, increased computational overhead if gating is ineffective, or instability when fusing caches with different tokenization schemes or vocabulary distributions.

Three First Experiments:
1. Validate cache projection accuracy by comparing semantic similarity of projected caches versus original source model outputs
2. Test learnable gating effectiveness by comparing performance with and without adaptive layer selection across different task types
3. Measure latency improvements across different hardware configurations to verify the 2.0× speedup claim under practical conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains measured primarily against single-model baselines and text communication, lacking comparisons to other multi-LLM coordination methods
- Latency improvements assume idealized cache operations that may vary significantly across hardware configurations
- Learnable gating effectiveness across diverse task distributions and potential catastrophic forgetting with dissimilar model families not fully explored
- Energy efficiency implications of additional cache fuser computation not discussed

## Confidence

High: The core technical contribution of cache projection and fusion is novel and well-executed. The latency-speedup claims are supported by controlled experiments. The methodology for learning which layers benefit from cache communication is clearly presented.

Medium: The generalization claims across diverse model combinations and benchmarks are supported but limited to specific model pairs and datasets. The accuracy improvements are significant but may be influenced by the specific evaluation setup.

Low: The scalability claims to large multi-LLM systems with many models and the practical deployment considerations in real-world scenarios are not adequately addressed.

## Next Checks

1. Test C2C performance across a wider range of model architectures (e.g., encoder-decoder, causal decoder, bidirectional) to assess generalizability beyond the current model pairs

2. Evaluate energy efficiency and computational overhead of the cache fuser compared to text-based communication under different hardware constraints

3. Investigate the robustness of C2C when fusing caches from models trained on significantly different data distributions or with different tokenization schemes