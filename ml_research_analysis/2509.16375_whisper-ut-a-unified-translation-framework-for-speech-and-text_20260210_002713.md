---
ver: rpa2
title: 'Whisper-UT: A Unified Translation Framework for Speech and Text'
arxiv_id: '2509.16375'
source_url: https://arxiv.org/abs/2509.16375
tags:
- translation
- speech
- text
- training
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Whisper-UT, a unified framework for speech-to-text
  translation that enables multi-task learning across ASR, ST, MT, and multimodal
  translation (MMT) within a single encoder-decoder model. By leveraging lightweight
  adapters and a stochastic task-selection mechanism, the approach dynamically conditions
  on speech, text, or both modalities.
---

# Whisper-UT: A Unified Translation Framework for Speech and Text

## Quick Facts
- arXiv ID: 2509.16375
- Source URL: https://arxiv.org/abs/2509.16375
- Reference count: 36
- Primary result: Unified multi-task framework for ASR, ST, MT, and MMT using Whisper-LARGE-V2 with LoRA adapters

## Executive Summary
Whisper-UT presents a unified translation framework that integrates speech-to-text translation, machine translation, and multimodal translation within a single encoder-decoder architecture. By leveraging LoRA adapters and a stochastic task-selection mechanism, the model dynamically conditions on speech, text, or both modalities. The approach demonstrates competitive performance across CoVoST2, Fisher-Spanish, and BBN-Mandarin datasets, with BLEU scores of 41.4/38.1 (fr-en/de-en), 70.4 (Fisher-Spanish MMT), and 26.0 (BBN-Mandarin MMT).

## Method Summary
Whisper-UT fine-tunes Whisper-LARGE-V2 (1.6B) with LoRA adapters (rank 200, alpha 400, dropout 0.1) to handle six objectives: ASR, E2E-ST, MMT, SLM, TLM, and MT. The framework uses stochastic task selection with beta-distributed loss weighting and ASR error simulation to improve robustness. Two-stage decoding enhances performance by first generating an ASR hypothesis, then translating from speech and hypothesis. The model supports both speech-to-text and text-only translation modes through a learnable encoder vector with masked cross-attention for text-only inputs.

## Key Results
- CoVoST2: 41.4 BLEU (fr-en) and 38.1 BLEU (de-en) with two-stage decoding
- Fisher-Spanish MMT: 70.4 BLEU
- BBN-Mandarin MMT: 26.0 BLEU
- Demonstrates cross-task synergy with ASR fine-tuning benefiting ST performance

## Why This Works (Mechanism)
The unified framework works by leveraging shared representations across ASR, ST, MT, and MMT tasks through a single encoder-decoder architecture. LoRA adapters provide task-specific conditioning while maintaining parameter efficiency. The stochastic task selection mechanism balances learning across multiple objectives, and ASR error simulation during training improves robustness to real-world transcription errors. The two-stage decoding strategy separates speech recognition from translation, allowing the model to handle noisy transcripts more effectively.

## Foundational Learning
- **Multi-task Learning**: Training a single model on multiple related tasks to improve generalization and parameter efficiency
  - Why needed: Enables parameter sharing across ASR, ST, MT, and MMT while maintaining task-specific performance
  - Quick check: Verify per-task losses decrease during training and no single task dominates

- **LoRA Adapters**: Low-rank adaptation technique that freezes pretrained weights and adds small trainable matrices
  - Why needed: Provides task-specific conditioning without full fine-tuning of 1.6B parameters
  - Quick check: Monitor adapter weights and ensure they remain small relative to base model

- **Two-stage Decoding**: Separate speech recognition from translation using ASR hypothesis as additional context
  - Why needed: Improves robustness to ASR errors and enables better handling of noisy transcripts
  - Quick check: Compare BLEU scores between single-stage and two-stage decoding modes

## Architecture Onboarding

**Component Map**: Input Preprocessor -> Whisper Encoder -> LoRA Adapters -> Decoder -> Task-specific Output

**Critical Path**: Speech/Text Input → Feature Extraction → Adapter Conditioning → Cross-Attention → Language Modeling → Translation Output

**Design Tradeoffs**: The unified architecture trades some task-specific optimization for parameter efficiency and cross-task knowledge transfer. Two-stage decoding adds inference complexity but improves robustness to ASR errors.

**Failure Signatures**: Repetitive outputs indicate insufficient input length; gradient interference suggests imbalanced task weighting; poor performance on noisy transcripts indicates missing ASR error simulation.

**First Experiments**:
1. Implement and validate multi-task data loader with proper task labeling and stochastic selection
2. Test LoRA adapter integration with single task before expanding to multi-task setup
3. Verify two-stage decoding implementation by comparing against direct ST baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Requires 3-way parallel data (speech/transcript/translation), which is rare and limits generalizability
- Reported BLEU improvements over baselines are modest, suggesting gains may be sensitive to hyperparameters
- Two-stage decoding adds inference complexity and may not generalize well to all languages and domains

## Confidence
- **High Confidence**: Core architectural design and training pipeline are well-specified
- **Medium Confidence**: Integration of speech and text inputs is plausible but implementation details are underspecified
- **Low Confidence**: Missing hyperparameters for task selection and ASR error simulation prevent exact replication

## Next Checks
1. Systematically sweep missing hyperparameters (β1, β2, q, b, t, top-k) to identify their impact on performance
2. Implement ablation study comparing two-stage vs single-stage decoding on all three datasets
3. Validate text-only MT implementation by comparing against strong MT baseline on derived text-only pairs