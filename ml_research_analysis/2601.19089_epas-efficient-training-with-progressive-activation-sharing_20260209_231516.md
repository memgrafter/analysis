---
ver: rpa2
title: 'EPAS: Efficient Training with Progressive Activation Sharing'
arxiv_id: '2601.19089'
source_url: https://arxiv.org/abs/2601.19089
tags:
- sharing
- training
- activation
- epas
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EPAS introduces a progressive activation sharing method to reduce
  redundancy in deeper transformer layers, improving both training and inference efficiency.
  It progressively expands an activation-sharing region during training by switching
  decoder layers to reuse QK activations from earlier layers.
---

# EPAS: Efficient Training with Progressive Activation Sharing

## Quick Facts
- arXiv ID: 2601.19089
- Source URL: https://arxiv.org/abs/2601.19089
- Reference count: 40
- Primary result: Progressive activation sharing reduces redundancy in deeper transformer layers, yielding up to 11.1% faster training and up to 29% faster inference throughput while maintaining accuracy.

## Executive Summary
EPAS introduces a progressive activation sharing method to reduce redundancy in deeper transformer layers, improving both training and inference efficiency. It progressively expands an activation-sharing region during training by switching decoder layers to reuse QK activations from earlier layers. This approach yields up to 11.1% faster training and up to 29% faster inference throughput while maintaining accuracy comparable to baseline models. It also enables flexible efficient model configurations during inference and transforms pretrained models into efficient ones via continual pretraining without distillation. Experiments on LLaMA models (125M–7B parameters) confirm consistent efficiency gains across hardware, and ablation studies validate design choices such as single large sharing blocks and inclusion of the last layer.

## Method Summary
EPAS implements progressive cross-layer activation sharing through switchable decoder layers that can operate in compute mode or sharing mode. In sharing mode, layers reuse QK activations from the previous layer's cache instead of computing them independently. The sharing region grows progressively from deeper to shallower layers according to a fixed schedule (Algorithm 1), starting with all layers in compute mode and gradually switching B layers every I steps. The method uses half of the layers in sharing mode for main experiments, with distributed training on 8× V100 GPUs and batch size 8/device with gradient accumulation 16. Pretraining runs for 1B tokens (0.25M tokens/step × 4000 steps), while continual pretraining uses 2K steps (4B tokens).

## Key Results
- Training throughput improved by up to 11.1% across LLaMA-125M to LLaMA-7B models
- Inference throughput increased by up to 29% while maintaining accuracy
- Progressive sharing schedule outperforms static activation sharing approaches
- Method enables flexible efficient model configurations during inference without requiring separate model variants

## Why This Works (Mechanism)
The method exploits redundancy in deeper transformer layers where attention patterns become increasingly similar across consecutive layers. By progressively sharing QK activations, EPAS reduces computational redundancy without significantly impacting representational capacity. The switchable layer design allows gradual adaptation, preventing abrupt accuracy drops that would occur with immediate full sharing. The approach is particularly effective because deeper layers in transformers often refine rather than completely transform attention patterns from previous layers.

## Foundational Learning

**Switchable Decoder Layer**: Why needed - Enables conditional branching between compute and sharing modes during training. Quick check - Verify layer can toggle between independent attention computation and QK reuse.

**Activation Cache Management**: Why needed - Stores QK activations from sharing-enabled layers for reuse by subsequent layers. Quick check - Confirm cache dimensions match across shared layers and is populated only when next layer has sharing enabled.

**Progressive Scheduling Algorithm**: Why needed - Controls the gradual expansion of sharing region to maintain accuracy while maximizing efficiency. Quick check - Validate that sharing region grows from deeper to shallower layers according to specified interval and batch size.

**Flash-Attention Compatibility**: Why needed - Ensures QK sharing works efficiently with optimized attention implementations. Quick check - Confirm shared QK activations integrate seamlessly with Flash-Attention's memory-efficient computation.

**Continual Pretraining**: Why needed - Enables transformation of pretrained models into efficient variants without full retraining. Quick check - Verify pretrained model weights can be fine-tuned with EPAS without catastrophic forgetting.

## Architecture Onboarding

**Component Map**: Input -> Switchable Decoder Layer -> Activation Cache -> Output; Progressive Scheduler controls sharing mode transitions across layers.

**Critical Path**: Token input → Embedding → Decoder Layer (compute or sharing mode) → Layer Norm → Output; the sharing mode branch bypasses QK computation when activated.

**Design Tradeoffs**: QK sharing vs KV sharing (QK chosen for Flash-Attention compatibility despite lower memory reduction); single large sharing block vs multiple smaller blocks (single block preferred for simplicity and effectiveness).

**Failure Signatures**: Training instability when switching layers too aggressively (loss spikes after layer switches); accuracy degradation if sharing region expands too rapidly; shape mismatches in activation cache passing between layers.

**First Experiments**:
1. Implement switchable decoder layer with conditional QK reuse and verify compute vs sharing mode functionality.
2. Train LLaMA-1.1B on SlimPajama subset with 50% sharing target to measure baseline throughput gains.
3. Conduct ablation comparing single large sharing block vs multiple smaller blocks on accuracy and efficiency.

## Open Questions the Paper Calls Out
1. How does EPAS perform when applied to vision and speech transformer architectures compared to natural language processing tasks?
2. How does the efficiency-accuracy trade-off of EPAS scale for models significantly larger than 7B parameters (e.g., 70B+)?
3. Does an adaptive or loss-based switching criterion outperform the fixed-interval deterministic scheduler used for growing the sharing region?
4. What are the specific memory reduction and throughput trade-offs when applying EPAS with KV-sharing compared to the demonstrated QK-sharing?

## Limitations
- Effectiveness depends heavily on precise scheduling parameters (interval I and growth size B) that are only partially specified
- Generalization to other architectures (e.g., GPT-2, OPT) and datasets beyond LLaMA/LLaMA-based models on SlimPajama is not demonstrated
- Claims of maintaining accuracy "comparable to baseline" lack statistical significance analysis across multiple runs
- Continual pretraining approach for transforming pretrained models lacks empirical validation on downstream tasks

## Confidence
- Training throughput improvements: High - Multiple runs on different hardware (V100) with consistent measurements reported
- Inference speedup claims: Medium - Measurements shown but comparison methodology between training and inference optimizations could be clearer
- Accuracy preservation: Medium - Benchmark results provided but limited to language modeling; no downstream task evaluation shown
- Continual pretraining feasibility: Low - Concept described but no empirical results demonstrating successful model transformation

## Next Checks
1. Implement the progressive scheduler with multiple (I, B) parameter combinations to determine sensitivity and optimal configuration for different model scales.
2. Conduct ablation studies comparing single large sharing blocks versus multiple smaller blocks to validate the architectural design choice.
3. Test cross-architecture generalization by applying EPAS to non-LLaMA transformer variants (e.g., GPT-2) and measuring both efficiency gains and accuracy retention.