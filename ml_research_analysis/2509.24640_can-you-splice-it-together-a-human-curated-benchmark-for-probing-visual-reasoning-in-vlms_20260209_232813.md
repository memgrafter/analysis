---
ver: rpa2
title: Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning
  in VLMs
arxiv_id: '2509.24640'
source_url: https://arxiv.org/abs/2509.24640
tags:
- video
- reasoning
- videos
- clips
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPLICE, a human-curated benchmark derived
  from the COIN instructional video dataset to assess visual reasoning in vision-language
  models (VLMs). SPLICE consists of 3,381 human-filtered videos across 12 categories
  and 180 sub-categories, segmented into 11,423 clips.
---

# Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs

## Quick Facts
- **arXiv ID:** 2509.24640
- **Source URL:** https://arxiv.org/abs/2509.24640
- **Reference count:** 24
- **Primary result:** VLMs achieve 23-51% accuracy on visual reasoning tasks versus human 85% performance

## Executive Summary
This paper introduces SPLICE, a human-curated benchmark designed to assess visual reasoning capabilities in vision-language models (VLMs). Derived from the COIN instructional video dataset, SPLICE consists of 3,381 human-filtered videos across 12 categories and 180 sub-categories, segmented into 11,423 clips. The benchmark requires models to reorder shuffled video clips into coherent sequences, testing five reasoning types: temporal, causal, spatial, contextual, and general knowledge. The results demonstrate that VLMs significantly underperform humans, with even the best models achieving only 23-51% accuracy compared to human performance of 85%. This performance gap persists even when textual annotations are provided, suggesting that models rely more on language priors than true visual understanding.

## Method Summary
The SPLICE benchmark was created by extracting 3,381 human-filtered videos from the COIN dataset across 12 categories and 180 sub-categories. Each video was segmented into clips, creating a total of 11,423 clips. The evaluation task involves presenting VLMs with shuffled clips from a video and requiring them to reorder them into the correct sequence. The benchmark tests five types of reasoning: temporal, causal, spatial, contextual, and general knowledge. Models were evaluated in a zero-shot manner, with some tests including textual annotations to assess whether language support improves performance. Human performance was established through Mechanical Turk annotations by workers familiar with the COIN dataset.

## Key Results
- VLMs achieved 23-51% accuracy on SPLICE tasks versus human performance of 85%
- Performance improved only marginally with textual annotations, indicating reliance on language priors
- VLMs performed better on temporal/causal reasoning tasks (35-51%) than contextual/spatial reasoning (16-35%)
- Open-source models lagged significantly behind closed-source models like Gemini

## Why This Works (Mechanism)
SPLICE works by creating a controlled environment where visual reasoning must be demonstrated through the ability to correctly sequence video clips. The benchmark's strength lies in its human-curated nature, ensuring high-quality videos that require genuine reasoning rather than pattern matching. By testing models on shuffled clips from instructional videos, SPLICE forces VLMs to understand the underlying logic and sequence of actions, rather than relying on superficial visual features or language priors. The multi-category design with varying difficulty levels allows for nuanced analysis of model capabilities across different reasoning types.

## Foundational Learning
- **Visual reasoning**: The ability to understand and interpret visual information to draw conclusions - needed to correctly sequence video clips based on visual content
- **Temporal reasoning**: Understanding the order and duration of events - critical for recognizing action sequences in videos
- **Causal reasoning**: Inferring cause-and-effect relationships - essential for understanding why certain actions follow others
- **Spatial reasoning**: Understanding spatial relationships between objects - needed for tasks involving positioning and movement
- **Contextual reasoning**: Using background knowledge to interpret visual information - required for understanding the purpose and context of actions

## Architecture Onboarding
**Component Map:** VLM Input -> Clip Processing -> Temporal/Causal/Spatial/Contextual/General Reasoning Modules -> Output Sequence
**Critical Path:** Video clips → Visual feature extraction → Reasoning module activation → Sequence prediction → Output generation
**Design Tradeoffs:** The benchmark prioritizes reasoning complexity over task simplicity, potentially limiting generalizability but providing deeper insights into reasoning capabilities
**Failure Signatures:** Models show systematic failures on contextual and spatial reasoning tasks, suggesting limitations in integrating background knowledge and understanding spatial relationships
**3 First Experiments:** 1) Evaluate models on progressively longer clip sequences to test scalability of reasoning 2) Test with varying levels of textual annotation to quantify language contribution 3) Cross-validate with alternative video datasets to assess benchmark generalizability

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Benchmark construction may contain implicit biases in video selection and clip segmentation
- Zero-shot evaluation without fine-tuning may underestimate model capabilities
- Focus on COIN-derived tasks may limit generalizability to other domains or cultural contexts
- Performance differences between open and closed-source models may reflect training resources rather than fundamental reasoning capabilities

## Confidence
- **High confidence:** VLMs significantly underperform humans on visual reasoning tasks; benchmark effectively exposes reasoning gaps
- **Medium confidence:** Specific performance breakdowns by task type; impact of textual annotations on performance
- **Low confidence:** Claims about fundamental nature of reasoning capabilities versus dataset/evaluation artifacts

## Next Checks
1. Evaluate model performance across additional diverse video datasets beyond COIN to test generalizability
2. Conduct controlled experiments varying the quality and type of textual annotations to better understand language versus visual reasoning contributions
3. Implement ablation studies on the benchmark design itself to identify which aspects most strongly influence model performance gaps