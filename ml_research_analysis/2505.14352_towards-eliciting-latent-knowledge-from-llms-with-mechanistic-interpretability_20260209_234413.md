---
ver: rpa2
title: Towards eliciting latent knowledge from LLMs with mechanistic interpretability
arxiv_id: '2505.14352'
source_url: https://arxiv.org/abs/2505.14352
tags:
- word
- secret
- taboo
- user
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of eliciting hidden knowledge\
  \ from large language models (LLMs), which is crucial for ensuring their trustworthiness\
  \ and reliability. The authors propose a Taboo model organism\u2014an LLM fine-tuned\
  \ to describe a secret word without explicitly stating it\u2014to study methods\
  \ for uncovering such concealed information."
---

# Towards eliciting latent knowledge from LLMs with mechanistic interpretability

## Quick Facts
- **arXiv ID**: 2505.14352
- **Source URL**: https://arxiv.org/abs/2505.14352
- **Reference count**: 34
- **Primary result**: Both black-box (adversarial prompting, token forcing) and white-box (Logit Lens, Sparse Autoencoders) approaches successfully elicit hidden secret words from Taboo models, with Logit Lens achieving 75% Pass@10 using top-5 predictions.

## Executive Summary
This paper investigates methods for uncovering hidden knowledge in large language models by creating a Taboo model organism—an LLM fine-tuned to describe a secret word without explicitly stating it. The authors evaluate both black-box approaches (like adversarial prompting and token forcing) and white-box mechanistic interpretability techniques (such as Logit Lens and Sparse Autoencoders) to extract the concealed information. Results show that both types of approaches can successfully elicit the secret word in this controlled setting, with black-box methods performing particularly well when the model is "warmed up" through gameplay. This work demonstrates the promise of mechanistic interpretability for uncovering hidden knowledge in LLMs and suggests directions for future research with more complex model organisms.

## Method Summary
The authors created a Taboo model by fine-tuning Gemma 2 9B using QLoRA (rank 8, 10 epochs) on synthetic conversations generated by Gemini 2.5 Flash, where the model describes a secret word without stating it directly. They then applied two types of elicitation methods: black-box techniques (token forcing, model guessing) and white-box mechanistic interpretability approaches (Logit Lens at layer 32, Sparse Autoencoders from Gemma Scope). The effectiveness was measured using Pass@10 (secret in ≥1 of 10 attempts) and Majority@10 (most frequent guess correct) metrics across 20 models with 20 different secret words.

## Key Results
- Token forcing postgame achieves 70% Majority@10 when applied after 3+ turns of gameplay
- Logit Lens at layer 32 identifies the secret word in 75% of cases using top-5 predictions
- Sparse Autoencoders successfully isolate latent features corresponding to hidden concepts, achieving 55% Pass@10 with top-5 predictions
- Black-box methods like token forcing and model guessing perform well, especially when the model is "warmed up" through gameplay

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Logit Lens applied at intermediate layers can reveal tokens the model is internally representing but not outputting.
- **Mechanism**: The unembedding matrix maps residual stream activations at each layer to vocabulary logits. In the Taboo model, the secret word's probability peaks at layers 30-37 (notably at tokens like `,` or `the` that carry contextual information), even though the final output never contains the word.
- **Core assumption**: The model must internally represent the secret word to generate coherent hints about it.
- **Evidence anchors**:
  - [Section 3.2]: "We observe that the token corresponding to the secret word often exhibits high probabilities in middle-to-late layers... tokens where the secret word's probability peaks are often predictable given the current sentence, such as ',', 'a', or 'the'."
  - [Results]: Logit Lens achieves 75% Pass@10 with top-5 predictions, 35% with top-1.
  - [Corpus]: Limited direct corpus support; neighbor papers focus on SAE applications rather than Logit Lens.
- **Break condition**: If the model were trained to suppress internal representations at all layers (not just output), Logit Lens would fail.

### Mechanism 2
- **Claim**: Sparse Autoencoders can identify latent features corresponding to hidden concepts even when those concepts are not verbalized.
- **Mechanism**: SAEs decompose dense activations into sparse feature activations. For the "smile" Taboo model, latent 9936 activates strongly across multiple tokens during hint generation, despite "smile" never appearing in output.
- **Core assumption**: Secret word concepts form monosemantic features that can be isolated by SAEs.
- **Evidence anchors**:
  - [Section 3.2]: "Latent 9936, which corresponds to the secret word, exhibits strong activations across multiple tokens, despite 'smile' not being explicitly verbalized."
  - [Results]: SAE achieves 55% Pass@10 with top-5 predictions.
  - [Corpus]: Neighbor paper "Mechanistic Interpretability of Antibody Language Models Using SAEs" supports SAE utility for feature identification in language models (FMR=0.54).
- **Break condition**: If hidden knowledge is distributed across many features rather than concentrated in specific latents, SAE-based elicitation degrades.

### Mechanism 3
- **Claim**: Contextual "warm-up" increases accessibility of hidden knowledge to token forcing attacks.
- **Mechanism**: After 3+ turns of gameplay, the model actively processes the secret word to generate hints, making internal representations more accessible. Prefilling responses (e.g., "My secret word is...") then exploits this active state.
- **Core assumption**: The model's internal secret representation is dynamic and strengthens during relevant reasoning.
- **Evidence anchors**:
  - [Section 4.3]: "Token Forcing postgame becomes highly effective when the model is 'warmed up' by several initial game turns" (70% vs 15% pregame).
  - [Results]: Token forcing postgame achieves 60% Majority@10.
  - [Corpus]: No direct corpus support for this specific mechanism.
- **Break condition**: If models were trained with adversarial examples including prefilled responses, this attack vector would close.

## Foundational Learning

- **Concept: Residual Stream & Unembedding**
  - **Why needed here**: Logit Lens works by projecting intermediate residual stream states through the unembedding matrix to vocabulary space.
  - **Quick check question**: Can you explain why Logit Lens at layer 32 might reveal different information than the final layer output?

- **Concept: Sparse Autoencoders (SAEs)**
  - **Why needed here**: SAEs decompose activations into interpretable features; understanding their training and interpretation is essential for the white-box elicitation method.
  - **Quick check question**: Why must SAE latents be sparse to be interpretable?

- **Concept: QLoRA Fine-tuning**
  - **Why needed here**: The Taboo models are trained via QLoRA (rank 8, 10 epochs); understanding parameter-efficient fine-tuning helps interpret what representations are learned.
  - **Quick check question**: How does QLoRA differ from full fine-tuning in terms of what knowledge the model can acquire?

## Architecture Onboarding

- **Component map**: Data generation (Gemini 2.5 Flash) -> Fine-tuning (Gemma 2 9B + QLoRA) -> Elicitation (Logit Lens + SAE) -> Evaluation (Pass@10, Majority@10)

- **Critical path**: Data validation (ensure secret absent) -> Fine-tuning -> Black-box robustness check -> White-box elicitation at layer 32

- **Design tradeoffs**:
  - Single-token secrets enable clear evaluation but limit ecological validity
  - Layer 32 chosen empirically; earlier layers may miss secret, later layers may suppress it
  - SAE latent identification is currently manual (Table 7); automation would require feature interpretation pipelines

- **Failure signatures**:
  - Logit Lens returns common words (articles, punctuation) -> secret representation weak or distributed
  - SAE activates generic latents across all secrets -> features not discriminative
  - Token forcing pregame succeeds -> model not properly trained to conceal

- **First 3 experiments**:
  1. **Reproduce Logit Lens peak detection**: Visualize probability curves across all 42 layers for a single secret word to verify layer 32 heuristic.
  2. **Test SAE automation**: Attempt to identify secret-corresponding latents without manual mapping (e.g., by activation frequency and specificity).
  3. **Scale to multi-token secrets**: Train Taboo models with 2-3 token secrets and measure elicitation degradation rate.

## Open Questions the Paper Calls Out
- Can mechanistic interpretability techniques be adapted to elicit multi-token secrets or abstract concepts from model organisms?
- Do these elicitation methods transfer to models actively exhibiting strategic deception or "scheming"?
- Can automated filtering of SAE features (e.g., by activation density) improve elicitation accuracy to near-perfect levels?

## Limitations
- The Taboo model uses single-token secrets from common English nouns, representing a constrained problem space compared to real-world scenarios
- The study focuses on a single architectural choice (layer 32 for elicitation) without systematic exploration of how different layers might yield different results
- SAE latent identification relies on manual mapping, suggesting the need for automated feature interpretation pipelines before scaling to more complex scenarios

## Confidence
- **High Confidence**: Black-box elicitation methods (token forcing postgame, model guessing) successfully retrieve the secret word in the Taboo setting with 70% and 60% success rates respectively.
- **Medium Confidence**: Logit Lens and Sparse Autoencoders can identify hidden knowledge in the Taboo model, with 75% Pass@10 for Logit Lens and 55% for SAEs with top-5 predictions, though these methods require more careful interpretation.
- **Low Confidence**: The generalizability of these findings to more complex, real-world scenarios involving multi-token secrets, adversarial training, or distributed representations.

## Next Checks
1. **Layer Sensitivity Analysis**: Systematically test Logit Lens and SAE effectiveness across all 42 layers of Gemma 2 to identify optimal elicitation points and understand how representation strength varies with depth.

2. **Multi-Token Secret Extension**: Train Taboo models with 2-3 token secrets (e.g., "ice cream", "artificial intelligence") to measure how elicitation effectiveness degrades and whether distributed representations can still be identified.

3. **Automated Feature Mapping**: Develop and validate an automated procedure for mapping SAE latents to secret words without manual inspection, enabling scalable application to larger model organisms with many secrets.