---
ver: rpa2
title: Improving Local Fidelity Through Sampling and Modeling Nonlinearity
arxiv_id: '2512.05556'
source_url: https://arxiv.org/abs/2512.05556
tags:
- local
- lime
- fidelity
- sampling
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving local fidelity in
  explanations generated by LIME, which assumes linear local decision boundaries and
  often fails to capture nonlinear relationships, leading to incorrect explanations.
  The authors propose a novel method that combines N-ball sampling with Multivariate
  Adaptive Regression Splines (MARS) to enhance local fidelity.
---

# Improving Local Fidelity Through Sampling and Modeling Nonlinearity

## Quick Facts
- arXiv ID: 2512.05556
- Source URL: https://arxiv.org/abs/2512.05556
- Authors: Sanjeev Shrestha; Rahul Dubey; Hui Liu
- Reference count: 21
- Key outcome: Proposes mLIME combining N-ball sampling with MARS modeling to achieve 37% lower RMSE than LEMON and 69.2% lower than LIME on local explanation fidelity

## Executive Summary
This paper addresses the fundamental limitation of LIME's linear approximation in capturing nonlinear local decision boundaries, which often results in inaccurate explanations. The authors propose a novel method that combines N-ball sampling with Multivariate Adaptive Regression Splines (MARS) to improve local fidelity. Through systematic experiments on three UCI datasets with different classifiers and kernel widths, the proposed method (mLIME) demonstrates substantial improvements in explanation accuracy, achieving an average RMSE reduction of 37% compared to LEMON and 69.2% compared to LIME. The results indicate that mLIME provides more faithful and stable explanations, particularly in higher-dimensional datasets and across various kernel widths, making it a more reliable method for local model explanations.

## Method Summary
The proposed method enhances local fidelity by addressing two key limitations of LIME: sampling strategy and modeling approach. First, instead of the standard uniform sampling within a hypercube, N-ball sampling generates synthetic samples directly from a multivariate normal distribution within a hypersphere, ensuring samples are drawn from the desired distribution. Second, rather than assuming linear local decision boundaries, MARS models nonlinear relationships through piecewise linear regressions, creating basis functions that capture local nonlinearity. The method combines these components with weighted linear regression to produce explanations that better reflect the true local decision boundary structure. Experiments systematically evaluate performance across different kernel widths (0.1 to 4.0) and dataset dimensions to demonstrate the robustness of the approach.

## Key Results
- Achieved 37% average reduction in RMSE compared to LEMON baseline across three UCI datasets
- Demonstrated 69.2% improvement over standard LIME in explanation fidelity
- Showed consistent performance across multiple kernel widths (0.1 to 4.0) and higher-dimensional datasets
- Provided more stable explanations with reduced variance across independent runs compared to baseline methods

## Why This Works (Mechanism)
None

## Foundational Learning
- **LIME's Linear Assumption**: LIME approximates local decision boundaries with linear models, which fails when boundaries are nonlinear. Why needed: Understanding this limitation motivates the need for nonlinear modeling approaches.
- **N-ball Sampling**: Generates samples from a multivariate normal distribution within a hypersphere rather than a hypercube. Why needed: Ensures samples are drawn from the desired distribution, improving coverage of the local region.
- **MARS (Multivariate Adaptive Regression Splines)**: Creates piecewise linear regressions to model nonlinear relationships. Why needed: Captures local nonlinearity that linear models miss, providing more accurate local approximations.
- **Kernel Width Impact**: Different kernel widths (σ) control the locality of explanations. Why needed: Understanding how locality affects explanation quality is crucial for proper method application.
- **RMSE for Fidelity Measurement**: Root Mean Square Error quantifies the difference between true and approximated local decision boundaries. Why needed: Provides objective metric to compare explanation quality across methods.

## Architecture Onboarding

**Component Map**: Input Instance -> N-ball Sampling -> MARS Basis Generation -> Weighted Linear Regression -> Feature Importance Scores

**Critical Path**: The core innovation flows through: (1) N-ball sampling generates representative local samples, (2) MARS creates nonlinear basis functions from these samples, (3) weighted linear regression produces final feature importance scores that reflect true local decision boundaries.

**Design Tradeoffs**: N-ball sampling improves sample distribution but increases computational complexity compared to uniform sampling. MARS provides better nonlinear modeling but may introduce more parameters and potential overfitting. The method trades computational efficiency for improved fidelity, which may impact scalability for large datasets or real-time applications.

**Failure Signatures**: When local decision boundaries are actually linear, the additional complexity of MARS may not provide benefits and could potentially overfit. If the sampling radius is too large, the "local" assumption breaks down regardless of modeling approach. Performance degradation may occur when the dataset contains very high-dimensional sparse features where hyperspherical sampling becomes less effective.

**First Experiments**: 
1. Compare RMSE performance on a synthetic dataset with known nonlinear boundaries across different kernel widths
2. Test mLIME versus LIME on a simple binary classification problem with clear nonlinear decision boundary
3. Evaluate stability by measuring variance of feature importance scores across repeated explanations on the same instance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can mLIME be adapted for image and time-series data while preserving its fidelity advantages over LIME?
- Basis in paper: [explicit] Conclusion states: "Future work can focus on...extending its application to other data modalities, such as image and time-series domains."
- Why unresolved: N-ball sampling operates on continuous feature vectors, and MARS models piecewise linear relationships in tabular form; images and time-series require spatial or temporal structure preservation.
- What evidence would resolve it: Modified mLIME implementations for images and time-series demonstrating comparable RMSE reductions against baselines on standard benchmarks.

### Open Question 2
- Question: How can mLIME's stability across independent runs be improved?
- Basis in paper: [explicit] Conclusion explicitly identifies "improving the stability" as future work.
- Why unresolved: MARS uses stepwise forward-backward selection, which can produce different basis functions across runs; N-ball sampling introduces stochasticity.
- What evidence would resolve it: Variance measurements of feature importance scores across repeated explanations on the same instance, compared with ensemble or uncertainty-aware variants.

### Open Question 3
- Question: What principled method can determine the optimal N-ball sampling radius for different datasets and model complexities?
- Basis in paper: [inferred] The paper tests multiple kernel widths (0.1 to 4.0) and shows substantial performance variation, but provides no systematic guidance for selecting radius r or σ.
- Why unresolved: Performance depends on local boundary geometry, which varies by dataset, classifier, and instance; no adaptive selection mechanism is proposed.
- What evidence would resolve it: An automated radius selection method evaluated across diverse datasets showing consistent fidelity without manual tuning.

## Limitations
- Evaluation based primarily on synthetic datasets may not capture real-world complexity and data distribution variations
- RMSE metrics may not fully reflect practical interpretability needs and user comprehension of explanations
- Computational overhead of N-ball sampling and MARS modeling compared to standard LIME is not explicitly addressed, potentially limiting practical deployment

## Confidence
- High confidence in the mathematical formulation and theoretical improvements of the sampling and modeling approach
- Medium confidence in the experimental results due to limited dataset diversity and absence of real-world case studies
- Medium confidence in the claimed superiority over baselines, pending independent replication

## Next Checks
1. Conduct experiments on additional real-world datasets with varying degrees of nonlinearity to verify robustness across more diverse data distributions
2. Perform ablation studies to quantify the individual contributions of N-ball sampling versus MARS modeling to the overall performance improvement
3. Evaluate computational efficiency and scalability of mLIME compared to LIME and LEMON on large datasets to assess practical deployment viability