---
ver: rpa2
title: Probability Density from Latent Diffusion Models for Out-of-Distribution Detection
arxiv_id: '2508.15737'
source_url: https://arxiv.org/abs/2508.15737
tags:
- detection
- density
- diffusion
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effectiveness of likelihood-based out-of-distribution
  (OOD) detection in representation space. Under a uniform OOD distribution assumption,
  it theoretically proves that density-based OOD detection is optimal.
---

# Probability Density from Latent Diffusion Models for Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2508.15737
- Source URL: https://arxiv.org/abs/2508.15737
- Authors: Joonas Järve; Karl Kaspar Haavel; Meelis Kull
- Reference count: 40
- Primary result: Density-based OOD detection in latent space is theoretically optimal under uniform OOD assumptions and achieves competitive performance on CIFAR-10, with mixed results on ImageNet-200.

## Executive Summary
This work investigates the effectiveness of likelihood-based out-of-distribution (OOD) detection in representation space. Under a uniform OOD distribution assumption, it theoretically proves that density-based OOD detection is optimal. The authors train a Variational Diffusion Model (VDM) on the latent representations of a pre-trained ResNet-18 to estimate densities, enabling exact log-likelihood computation through probability-flow ODEs. Three OOD detection methods are proposed: exact likelihood, prior likelihood, and top-K diffusion loss. Experimental results on the OpenOOD benchmark show that the exact likelihood and prior likelihood methods achieve competitive performance with state-of-the-art methods on CIFAR-10, but underperform on ImageNet-200, suggesting the need for more model capacity. The top-K diffusion loss method proves more robust and stable across benchmarks. The findings indicate that density-based OOD detection in representation space is viable and warrants further study.

## Method Summary
The authors propose using Variational Diffusion Models (VDMs) trained on latent representations of pre-trained neural networks for OOD detection. They leverage the exact log-likelihood computation capability of VDMs through probability-flow ODEs to measure sample likelihood in latent space. Three detection methods are introduced: exact likelihood (computing exact log-likelihood), prior likelihood (computing log-likelihood under the prior distribution), and top-K diffusion loss (using the average of the top-K log-likelihood scores). The approach assumes a uniform OOD distribution, which theoretically guarantees the optimality of density-based detection.

## Key Results
- Exact likelihood and prior likelihood methods achieve competitive performance with state-of-the-art OOD detection methods on CIFAR-10.
- Performance on ImageNet-200 was notably weaker, indicating potential scalability issues with larger datasets.
- Top-K diffusion loss method demonstrates more robust and stable performance across different benchmarks compared to exact and prior likelihood methods.

## Why This Works (Mechanism)
The method works by leveraging the density estimation capabilities of diffusion models in the latent representation space of pre-trained networks. By training a VDM on these representations, the model learns to estimate the probability density of in-distribution data. The theoretical foundation rests on the optimality of likelihood-based detection under uniform OOD assumptions, which provides a principled basis for using log-likelihood as an OOD score.

## Foundational Learning
- **Diffusion Models**: Generative models that learn to denoise data through a Markov chain. Needed for their ability to estimate probability densities. Quick check: Verify the model can generate realistic samples from the learned distribution.
- **Variational Diffusion Models**: Extension of diffusion models that incorporate variational inference principles. Needed for exact log-likelihood computation through probability-flow ODEs. Quick check: Confirm the ELBO loss properly balances reconstruction and regularization terms.
- **Probability-Flow ODEs**: Differential equations that connect diffusion models to continuous-time generative processes. Needed for exact likelihood computation without sampling. Quick check: Validate the ODE solver's numerical stability and accuracy.
- **Latent Representation Space**: The feature space learned by pre-trained networks. Needed as a compact, semantically meaningful domain for density estimation. Quick check: Ensure the pre-trained network captures relevant features for the downstream task.
- **Out-of-Distribution Detection**: The task of identifying samples that differ from the training distribution. Needed as the target application. Quick check: Verify OOD detection performance using established metrics like AUROC and FPR.
- **Uniform OOD Distribution Assumption**: The theoretical assumption that OOD data follows a uniform distribution. Needed for proving the optimality of likelihood-based detection. Quick check: Assess the impact of this assumption on real-world performance.

## Architecture Onboarding
- **Component Map**: Pre-trained ResNet-18 -> Latent representation extraction -> Variational Diffusion Model (VDM) -> Density estimation -> OOD detection
- **Critical Path**: Input image → ResNet-18 feature extraction → VDM training on latent representations → Log-likelihood computation via probability-flow ODEs → OOD score calculation
- **Design Tradeoffs**: Using pre-trained features vs. end-to-end training (tradeoff between computational efficiency and potential performance gains); exact likelihood computation vs. approximate methods (tradeoff between accuracy and scalability)
- **Failure Signatures**: Poor performance on complex datasets (e.g., ImageNet-200) indicating model capacity limitations; degraded performance when OOD assumption is violated; sensitivity to choice of pre-trained network architecture
- **3 First Experiments**: 1) Verify the VDM can accurately reconstruct in-distribution samples; 2) Test the log-likelihood scores on known OOD datasets; 3) Compare the three proposed OOD detection methods on a small benchmark dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The uniform OOD distribution assumption may not hold in real-world scenarios, potentially limiting practical applicability.
- Reliance on pre-trained feature extractors could introduce biases and limit the method's adaptability.
- Performance degradation on larger, more complex datasets like ImageNet-200 suggests scalability challenges.
- The study focuses on image data and does not address potential issues like mode collapse or representation quality.

## Confidence
- Theoretical optimality of density-based OOD detection under uniform assumptions: **High**
- Empirical results on CIFAR-10: **Medium**
- Empirical results on ImageNet-200: **Low**
- Robustness of top-K diffusion loss method: **High**
- Generalizability to non-image domains: **Low**

## Next Checks
1. Test the method on non-image datasets (e.g., text or tabular data) to assess domain transferability.
2. Evaluate the impact of using deeper or more modern architectures (e.g., Vision Transformers) as feature extractors.
3. Investigate the effect of relaxing the uniform OOD assumption by introducing structured or class-conditional OOD distributions.