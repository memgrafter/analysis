---
ver: rpa2
title: Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?
arxiv_id: '2508.21690'
source_url: https://arxiv.org/abs/2508.21690
tags:
- agent
- pedestrian
- sidewalk
- pedestrians
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a proof-of-concept approach for leveraging human
  behavior models in mobile robot planning and decision-making. The study addresses
  the "sidewalk salsa" phenomenon, where pedestrians approaching each other on sidewalks
  sometimes end up in awkward collision-avoidance interactions.
---

# Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?

## Quick Facts
- arXiv ID: 2508.21690
- Source URL: https://arxiv.org/abs/2508.21690
- Authors: Olger Siebinga; David Abbink
- Reference count: 17
- Primary result: Mobile robot RL agents learn collision avoidance strategies from pedestrian model, reducing sidewalk interaction awkwardness

## Executive Summary
This work presents a proof-of-concept approach for leveraging human behavior models in mobile robot planning and decision-making. The study addresses the "sidewalk salsa" phenomenon, where pedestrians approaching each other on sidewalks sometimes end up in awkward collision-avoidance interactions. Using a Communication-Enabled Interaction (CEI) model of pedestrian behavior, the researchers trained two Reinforcement Learning (RL) agents to control a mobile robot in simulated sidewalk interactions.

The basic RL agent successfully learned to communicate its passing intention through early side selection, while a risk-averse RL agent learned to further reduce perceived risk by maintaining central position before steering away with a clear heading. The risk-averse agent reduced the CEI model's maximum perceived risk to near zero and substantially decreased pedestrian effort in collision avoidance compared to both the basic RL agent and a social forces baseline model.

## Method Summary
The researchers employed a simulation-based approach to train RL agents for mobile robot sidewalk navigation. They used a Communication-Enabled Interaction (CEI) model of pedestrian behavior as the environment for training two RL agents: a basic agent and a risk-averse agent. The agents were trained using reinforcement learning algorithms to control a mobile robot in simulated interactions with virtual pedestrians. The performance of the RL agents was evaluated against a social forces baseline model, with metrics including perceived risk and pedestrian effort in collision avoidance.

## Key Results
- Basic RL agent learned to communicate passing intention through early side selection
- Risk-averse RL agent reduced CEI model's maximum perceived risk to near zero
- Risk-averse agent substantially decreased pedestrian effort in collision avoidance compared to baseline

## Why This Works (Mechanism)
The approach works by leveraging a pedestrian behavior model to train RL agents that can anticipate and respond to human-like interactions. The agents learn to communicate their intentions early, reducing uncertainty and potential conflicts in sidewalk interactions. The risk-averse agent further optimizes its behavior to minimize perceived risk, resulting in smoother interactions and reduced pedestrian effort in avoiding collisions.

## Foundational Learning
- Reinforcement Learning: Needed for agents to learn optimal navigation strategies through trial and error. Quick check: Verify RL algorithm convergence and performance metrics.
- Pedestrian Behavior Modeling: Essential for creating realistic simulation environments. Quick check: Validate model against real pedestrian interaction data.
- Risk Perception in Human-Robot Interaction: Critical for designing robot behaviors that humans find comfortable. Quick check: Conduct human subject studies to validate perceived risk metrics.

## Architecture Onboarding
Component Map: Mobile Robot -> RL Agent -> Pedestrian Model (CEI) -> Reward Function -> Updated Robot Policy

Critical Path: Robot action selection -> Pedestrian model response -> Risk assessment -> Reward calculation -> Policy update

Design Tradeoffs: Computational efficiency vs. model accuracy, exploration vs. exploitation in RL training, simplicity vs. sophistication of pedestrian model

Failure Signatures: Increased perceived risk, inefficient path planning, failure to communicate passing intentions, inability to generalize to different pedestrian models

Three First Experiments:
1. Test RL agents with alternative pedestrian models to assess robustness
2. Evaluate performance in varying environmental conditions (sidewalk width, pedestrian density)
3. Compare results with other collision avoidance algorithms beyond social forces

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on simulated CEI model may not capture real human interaction complexity
- Findings based on simulations, untested in real-world scenarios
- Risk-averse agent's success specific to CEI model, may not generalize to other pedestrian models

## Confidence
- High: Basic RL agent's ability to communicate passing intentions through early side selection
- Medium: Risk-averse RL agent's reduction of perceived risk and pedestrian effort in simulated environment
- Low: Generalization of findings to real-world scenarios and other pedestrian models

## Next Checks
1. Conduct real-world experiments with pedestrians to validate the performance of the RL agents in actual sidewalk interactions
2. Test the RL agents with alternative pedestrian models to assess the robustness and generalizability of the approach
3. Evaluate the impact of environmental factors (e.g., sidewalk width, pedestrian density) on the effectiveness of the RL agents' collision avoidance strategies