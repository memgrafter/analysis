---
ver: rpa2
title: Mitigating Forgetting Between Supervised and Reinforcement Learning Yields
  Stronger Reasoners
arxiv_id: '2510.04454'
source_url: https://arxiv.org/abs/2510.04454
tags:
- reasoning
- data
- updates
- arxiv
- mifo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of combining supervised fine-tuning
  (SFT) and reinforcement learning (RL) for mathematical reasoning in large language
  models. While RL excels at refining reasoning through self-generated trajectories,
  it struggles to acquire new knowledge and risks overwriting learned reasoning skills
  when combined with SFT, which introduces out-of-distribution data.
---

# Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners

## Quick Facts
- arXiv ID: 2510.04454
- Source URL: https://arxiv.org/abs/2510.04454
- Reference count: 40
- Primary result: MIFO achieves SoTA reasoning performance using 1.5% of SFT data and 20.4% of RL data compared to prior methods

## Executive Summary
This paper addresses the challenge of combining supervised fine-tuning (SFT) and reinforcement learning (RL) for mathematical reasoning in large language models. While RL excels at refining reasoning through self-generated trajectories, it struggles to acquire new knowledge and risks overwriting learned reasoning skills when combined with SFT. The proposed MIFO framework addresses this by dynamically interleaving SFT within RL, selecting only challenging examples and high-entropy tokens for SFT updates. Critically, it freezes parameters identified as important for RL during SFT to prevent catastrophic forgetting. The method achieves state-of-the-art reasoning performance while being significantly more data-efficient than previous approaches.

## Method Summary
MIFO implements an interleaved training approach that alternates between GRPO-based reinforcement learning and supervised fine-tuning. The method includes three key innovations: (1) data processing that buffers questions where rollout accuracy falls below 1/8 and computes SFT loss only on the top 20% entropy tokens per example, (2) parameter freezing that tracks RL parameter updates through an importance map and freezes the top 50% most-updated parameters during SFT phases, and (3) dynamic scheduling that triggers SFT when the buffer reaches a threshold size. The approach uses OpenR1-Math-46k for training and evaluates on multiple mathematical reasoning benchmarks including AIME, AMC, OlympiadBench, MATH500, and MMLU-Pro.

## Key Results
- Achieves state-of-the-art pass@1 accuracy across mathematical reasoning benchmarks
- Uses only 1.5% of the SFT data compared to prior SoTA methods
- Requires just 20.4% of the RL data while maintaining superior performance
- Produces more concise responses compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFT produces redundant, high-magnitude parameter updates that can overwrite RL-acquired knowledge, causing catastrophic forgetting when the two are combined.
- Mechanism: SFT applies uniform gradient updates across all parameters with larger magnitude; RL produces smaller, more targeted updates. When SFT follows RL, its larger updates displace the carefully-learned RL parameters.
- Core assumption: The direction/magnitude of parameter updates correlates with knowledge retention; larger non-selective updates are more likely to erase prior learning.
- Evidence anchors:
  - [abstract] "S