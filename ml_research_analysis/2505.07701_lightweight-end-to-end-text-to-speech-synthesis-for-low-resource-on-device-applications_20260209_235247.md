---
ver: rpa2
title: Lightweight End-to-end Text-to-speech Synthesis for low resource on-device
  applications
arxiv_id: '2505.07701'
source_url: https://arxiv.org/abs/2505.07701
tags:
- speech
- training
- loss
- acoustic
- vocoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying high-quality text-to-speech
  (TTS) synthesis on low-resource on-device applications, where existing end-to-end
  models are computationally expensive and memory-intensive. To solve this, the authors
  propose a lightweight end-to-end TTS model (LE2E) that jointly trains a lightweight
  acoustic model (based on LightSpeech) and a neural vocoder (Multi-Band MelGAN) in
  a single architecture, enabling efficient waveform generation without intermediate
  mel-spectrograms.
---

# Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications

## Quick Facts
- arXiv ID: 2505.07701
- Source URL: https://arxiv.org/abs/2505.07701
- Reference count: 0
- Achieves MOS 3.79 on LJSpeech with 90% fewer parameters and 10× faster real-time factor than state-of-the-art

## Executive Summary
This paper proposes a lightweight end-to-end text-to-speech (LE2E) model designed for low-resource on-device applications. The architecture jointly trains a lightweight acoustic model (based on LightSpeech) and a neural vocoder (Multi-Band MelGAN) in a single architecture, eliminating the training-inference mismatch of cascade approaches. LE2E achieves competitive quality (MOS 3.79) while being 90% smaller in parameter size and 10× faster in real-time factor compared to state-of-the-art models like VITS and JETS, making it highly suitable for real-time, low-resource, on-device TTS applications.

## Method Summary
LE2E combines a lightweight acoustic model with a multi-band neural vocoder trained jointly end-to-end. The acoustic model uses a transformer encoder-decoder architecture with variance adaptor for duration and pitch prediction. The vocoder employs a multi-band approach with pseudo-quadrature mirror filter (PQMF) to split and recombine frequency bands. The model replaces traditional mel-spectrogram intermediate representations with latent representations, trained using an improved adversarial loss with multi-period and multi-resolution discriminators. Pitch prediction is reformulated as a 256-bin classification task to improve stability.

## Key Results
- Achieves MOS 3.79 on LJSpeech, comparable to VITS (3.67) and JETS (3.71)
- 90% smaller in parameter size (3.71M parameters) compared to state-of-the-art models
- 10× faster real-time factor (0.0067 on A100 GPU) enabling on-device deployment
- F0 RMSE of 0.033, competitive with cascade approaches and lower than JETS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of acoustic model and vocoder eliminates the training-inference mismatch inherent in cascade approaches.
- Mechanism: In two-stage TTS, the vocoder trains on ground-truth mel-spectrograms but receives predicted mel-spectrograms at inference, causing quality degradation. LE2E trains both components simultaneously with end-to-end gradients, so the acoustic model learns to produce latent representations the vocoder can actually use.
- Core assumption: The gradient flow through the vocoder to the acoustic encoder provides sufficient signal for learning useful intermediate representations without explicit mel-spectrogram supervision.
- Evidence anchors:
  - [abstract] "the proposed E2E training paradigm achieves better quality compared to an equivalent architecture trained in a two-stage approach"
  - [section 1] "the two-stage approach of training separate models leads to a mismatch between the acoustic features used during training and those used during inference"
  - [corpus] Weak direct corpus support—neighboring papers focus on quantization and multilingual TTS, not specifically on joint training benefits for mismatch reduction.
- Break condition: If the acoustic latent space collapses to uninformative representations (e.g., mode collapse), joint training may fail to provide useful gradients; monitor latent variance and reconstruction quality separately.

### Mechanism 2
- Claim: Combining multi-period (MPD) and multi-resolution (MRD) discriminators improves perceptual quality by capturing complementary spectral and temporal artifacts.
- Mechanism: MPD reshapes waveforms into 2D representations at different periods to capture periodic structures (pitch harmonics), while MRD operates on spectrograms at multiple STFT resolutions to capture both coarse and fine spectral details. Together they provide dense supervisory signal across time-frequency representations.
- Core assumption: The discriminator architecture transfers from vocoder-only training (BigVGAN) to full E2E TTS without architectural modification.
- Evidence anchors:
  - [section 3.2] "We adopted the set of discriminators proposed in BigVGAN [21], which includes a multi-period discriminator (MPD) and a multi-resolution discriminator (MRD)"
  - [section 4.4, Table 2] MB-MelGAN+ (with proposed loss) achieves MOS 4.02 vs. MB-MelGAN 3.59, demonstrating loss objective improvement
  - [corpus] No direct corpus evidence for discriminator design transfer in E2E TTS specifically.
- Break condition: If discriminator loss dominates (L_G saturates), generator may fail to converge; balance λ_FM (set to 2) and monitor generator/discriminator loss ratio.

### Mechanism 3
- Claim: Replacing pitch regression with 256-bin quantized cross-entropy improves intonation stability under high pitch variability.
- Mechanism: Regression to continuous pitch values struggles with ground-truth pitch contour variability. Quantizing standardized pitch into 256 bins and treating prediction as classification provides a more stable target distribution, reducing overfitting to noisy annotations.
- Core assumption: 256 bins provide sufficient pitch resolution for perceptually natural intonation while remaining tractable for classification.
- Evidence anchors:
  - [section 3.3.2] "due to the high variability of ground-truth pitch contours, we replace the regression task with cross-entropy density modeling"
  - [section 4.3, Table 1] LE2E achieves F0 RMSE of 0.033, comparable to cascade (0.029) and lower than JETS (0.041)
  - [corpus] No corpus papers validate quantized pitch prediction specifically.
- Break condition: If pitch bins are too coarse, synthesized speech may sound robotic; if too fine, classification becomes unstable—validate with F0 RMSE and subjective intonation MOS.

## Foundational Learning

- Concept: **GAN training dynamics (generator-discriminator balance)**
  - Why needed here: LE2E relies on adversarial training; understanding least-squares GAN loss, feature matching, and mode collapse is essential for debugging convergence failures.
  - Quick check question: Can you explain why the generator is trained to make D_k(ẑ) approach 1 while the discriminator is trained to make D_k(ẑ) approach 0?

- Concept: **Multi-band vocoding with PQMF**
  - Why needed here: The vocoder generates sub-bands separately and recombines via Pseudo Quadrature Mirror Filter; understanding band splitting/reconstruction is necessary for diagnosing aliasing artifacts.
  - Quick check question: What is the purpose of the PQMF synthesis filter, and what artifacts emerge if sub-band phase alignment is incorrect?

- Concept: **Transformer encoder-decoder architectures for sequence modeling**
  - Why needed here: The acoustic model uses transformer layers for phoneme encoding and acoustic decoding; positional embeddings and self-attention patterns directly affect alignment quality.
  - Quick check question: How does the variance adaptor up-sample phoneme embeddings to frame-level, and what role does the duration predictor play in this process?

## Architecture Onboarding

- Component map:
  - Input: Phoneme embeddings + positional encodings → Text Encoder (4 transformer blocks, 256 hidden dim)
  - Variance Adaptor: Duration predictor (2-layer conv) → up-sampling → Pitch predictor (5-layer conv + 256-bin softmax)
  - Acoustic Decoder: 4 transformer blocks with kernel sizes [17, 21, 9, 13]
  - Vocoder: 3 upsampling layers [3×, 5×, 5×] with residual blocks (dilation [1, 3, 9, 27]) → 4 sub-bands → PQMF synthesis
  - Discriminators: MPD (periods [2, 3, 5, 7, 11]) + MRD (FFT sizes [1024, 2048, 512])

- Critical path:
  1. Verify phoneme-to-duration alignment using external Kaldi aligner outputs
  2. Monitor pitch cross-entropy loss convergence (should stabilize before GAN losses)
  3. Check sub-band STFT losses separately before combining
  4. Validate RTF on target device early (target: <0.01 on A100)

- Design tradeoffs:
  - **Latent vs. mel-spectrogram:** Latent representations remove mel-resolution constraints but require more careful discriminator design
  - **Model size vs. quality:** 3.71M params achieves MOS 3.79; increasing transformer blocks may improve quality at cost of RTF
  - **Sub-band count:** 4 sub-bands balances speed and quality; fewer bands increase aliasing risk

- Failure signatures:
  - **Metallic/harsh artifacts:** PQMF filter misconfiguration or insufficient training on sub-band loss
  - **Monotone speech:** Pitch predictor not learning (check cross-entropy plateau)
  - **Skipped/repeated phonemes:** Duration predictor divergence (verify MSE loss on oracle durations)
  - **Discriminator collapse:** D loss → 0 quickly with no generator improvement

- First 3 experiments:
  1. **Vocoder-only re-synthesis test:** Train MB-MelGAN+ standalone on ground-truth latents; verify MOS improvement over baseline MB-MelGAN confirms loss objective effectiveness.
  2. **Ablate joint training vs. cascade:** Train LightSpeech → MB-MelGAN+ as separate models with fine-tuning; compare MOS and cFSD to LE2E to quantify E2E benefit.
  3. **Device RTF profiling:** Deploy LE2E on target hardware; measure real-time factor at batch size 1 and identify bottleneck (encoder, variance adaptor, or vocoder upsampling).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LE2E training paradigm be effectively scaled to multi-speaker and multi-lingual contexts?
- Basis in paper: [explicit] The conclusion states, "Future research could expand our findings to multi-speaker and/or multi-lingual use-cases."
- Why unresolved: The current study evaluates the model exclusively on the single-speaker LJSpeech dataset, leaving the model's ability to handle diverse voices and languages unproven.
- What evidence would resolve it: Evaluation results on multi-speaker (e.g., VCTK) or multi-lingual datasets, showing speaker similarity and stability metrics comparable to the single-speaker results.

### Open Question 2
- Question: Are there specific discriminator architectures optimized for lightweight generators that could further improve the quality-efficiency trade-off?
- Basis in paper: [explicit] The authors suggest a need to "further explore new discriminator architectures for lightweight TTS models."
- Why unresolved: The paper adopts existing discriminators from BigVGAN, which may be over-parameterized or suboptimal for guiding a lightweight generator.
- What evidence would resolve it: An ablation study using custom lightweight discriminators that maintains or improves MOS while reducing the computational cost of the adversarial training phase.

### Open Question 3
- Question: Can the dependency on an external aligner be removed while maintaining the model's performance and training stability?
- Basis in paper: [inferred] Section 3.3.1 notes that duration loss relies on "oracle durations extracted by an external aligner," contradicting the fully "end-to-end" claim.
- Why unresolved: Relying on Kaldi for pre-alignment introduces a pipeline complexity that the authors aim to avoid in other parts of the architecture.
- What evidence would resolve it: A variation of the model using an internal monotonic alignment search (MAS) or attention-based duration mechanism that achieves similar F0 RMSE and MOS scores without external pre-processing.

## Limitations
- Evaluation limited to single English dataset (LJSpeech), raising generalizability concerns
- 90% parameter reduction claim lacks clear baseline definition for "state-of-the-art" lightweight models
- Quantized pitch prediction may introduce perceptible discretization artifacts not captured by F0 RMSE
- No assessment of robustness to noisy or out-of-domain text inputs

## Confidence
- **High Confidence:** The 10× real-time factor improvement and 90% parameter reduction are well-supported by the ablation study (Table 1) and RTF measurements. The comparative MOS scores (3.79 vs. 3.67 for VITS) are statistically meaningful given the evaluation protocol.
- **Medium Confidence:** The mechanism by which joint training eliminates cascade mismatch is theoretically sound but not directly validated through controlled experiments comparing cascade vs. joint training with identical architectures.
- **Medium Confidence:** The effectiveness of combining MPD and MRD discriminators is demonstrated through the MB-MelGAN+ ablation, but the transfer of this discriminator architecture from vocoder-only to full E2E TTS is not independently verified.
- **Low Confidence:** The claim that 256-bin pitch quantization "resolves" regression instability is based on improved F0 RMSE but lacks perceptual validation through dedicated intonation quality tests.

## Next Checks
1. **Cross-dataset generalization test:** Evaluate LE2E on multilingual datasets (e.g., VCTK, Blizzard Challenge) and measure degradation in MOS and RTF compared to LJSpeech performance.
2. **Pitch discretization perceptual study:** Conduct a formal listening test comparing 256-bin quantized pitch vs. continuous regression with human raters scoring intonation naturalness and artifacts.
3. **Robustness to text noise:** Systematically corrupt input text (typos, out-of-vocabulary words, mixed languages) and measure degradation in speech quality metrics and alignment errors.