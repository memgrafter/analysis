---
ver: rpa2
title: Exploring Fusion Strategies for Multimodal Vision-Language Systems
arxiv_id: '2511.21889'
source_url: https://arxiv.org/abs/2511.21889
tags:
- fusion
- data
- accuracy
- multimodal
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates multimodal fusion strategies for vision-language
  systems, focusing on the trade-off between accuracy and inference latency. The authors
  propose three model architectures that fuse BERT with either MobileNetV2 or ViT
  at late, intermediate, and early stages.
---

# Exploring Fusion Strategies for Multimodal Vision-Language Systems

## Quick Facts
- arXiv ID: 2511.21889
- Source URL: https://arxiv.org/abs/2511.21889
- Reference count: 28
- Primary result: Late fusion achieves highest accuracy (84.25%) while early fusion offers lowest latency (11.4ms)

## Executive Summary
This paper investigates multimodal fusion strategies for vision-language systems, focusing on the trade-off between accuracy and inference latency. The authors propose three model architectures that fuse BERT with either MobileNetV2 or ViT at late, intermediate, and early stages. They evaluate these models on the CMU-MOSI dataset and benchmark latency on an NVIDIA Jetson Orin AGX. The late fusion model achieves the highest accuracy (84.25%), comparable to state-of-the-art methods, while early fusion offers the lowest inference latency (11.4ms for MobileNetV2-fused, 10.951ms for ViT-fused). The study demonstrates that earlier fusion reduces latency at the cost of accuracy, highlighting the importance of considering this trade-off when designing multimodal systems for edge deployment where speed is critical.

## Method Summary
The study evaluates three fusion strategies for multimodal sentiment analysis using BERT-base-uncased with MobileNetV2 or ViT-base models on the CMU-MOSI dataset. Late fusion fine-tunes each modality independently before combining at a unified classifier. Intermediate fusion concatenates features from BERT encoder layers 4, 7, 8 with corresponding MobileNetV2 layers, then processes through attention blocks. Early fusion merges both models after six feature extraction layers and processes through four attention blocks. Models are trained with specified hyperparameters and evaluated for binary sentiment classification accuracy and inference latency on NVIDIA Jetson Orin AGX using ONNX and TensorRT.

## Key Results
- Late fusion achieves highest accuracy (84.25%) comparable to state-of-the-art methods
- Early fusion offers lowest latency (11.4ms for MobileNetV2, 10.951ms for ViT) on Jetson Orin AGX
- MobileNetV2-fused models consistently outperform ViT-fused models across all fusion strategies
- Earlier fusion reduces latency by ~47% (21.6ms → 11.4ms) at cost of ~16 percentage points accuracy

## Why This Works (Mechanism)

### Mechanism 1
Earlier fusion reduces inference latency by collapsing parallel computation paths into a unified processing stream. Late fusion maintains two independent models (BERT + MobileNetV2/ViT) that process data in parallel before combining outputs at a classification head. Early fusion terminates both base models after six feature extraction layers and merges into a single attention-based network. This reduces redundant computation but sacrifices modality-specific feature extraction depth.

### Mechanism 2
Late fusion achieves higher accuracy by preserving full modality-specific feature extraction hierarchies before combination. BERT's encoder layers 1-8 capture syntactic features while layers 9-12 capture semantic features and longer-range dependencies. MobileNetV2's deeper layers extract more complex visual features. Late fusion allows each modality to complete its specialized processing before combination, enabling richer cross-modal reasoning at the decision level.

### Mechanism 3
Attention block count in early fusion creates a secondary accuracy-latency trade-off within the same fusion strategy. The early fusion model uses a unified attention network after merging. Ablation shows 4 attention blocks achieve 67.89% accuracy while 8 blocks achieve 69.09%—but the paper selects 4 blocks to minimize latency. This demonstrates that even within a fixed fusion strategy, architectural depth tunings navigate the trade-off space.

## Foundational Learning

- **Transformer encoder hierarchy (syntactic vs. semantic layers)**: Understanding which BERT layers to preserve in intermediate/early fusion requires knowing that early layers capture syntax while later layers capture semantics. The paper explicitly references this (layers 1-4, 5-8, 9-12 groupings).
  - Quick check: If your task requires understanding irony (semantic, not syntactic), which BERT layers would you prioritize preserving in a fused model?

- **Parallel vs. sequential inference paths**: Late fusion runs two models in parallel; early fusion runs a single merged model. This directly explains the latency differences and informs deployment decisions on edge hardware.
  - Quick check: On a device with 2 GPU cores, would late fusion's parallel paths be faster or slower than on a single-core device, relative to early fusion?

- **Feature-level vs. decision-level fusion**: The paper's "intermediate" and "late" fusion correspond to these concepts. Late fusion combines decisions (after classification heads); intermediate/early combine features (before classification). This distinction determines what information is available for cross-modal reasoning.
  - Quick check: If you want to learn "this smiling face makes the text 'I'm fine' sarcastic," which fusion strategy gives the model access to the raw information needed?

## Architecture Onboarding

- **Component map**:
  - Late fusion: [Image → MobileNetV2 → pooled features] + [Text → BERT → pooled features] → Concatenate → Unified classifier
  - Intermediate fusion: [Image → MobileNetV2 layers 1,4,7,8] + [Text → BERT layers 1,4,7,8] → Concatenate at matching depths → Linear + Attention → Classifier
  - Early fusion: [Image → MobileNetV2 first 6 layers] + [Text → BERT first 6 layers] → Concatenate → 4 Attention blocks → Classifier

- **Critical path**:
  - For latency: Focus on the point where parallel paths become one. Earlier merge = lower latency.
  - For accuracy: Focus on how many modality-specific layers complete before merge. More layers = higher accuracy (up to full model = late fusion).

- **Design tradeoffs**:
  - Latency priority: Choose early fusion; sacrifice ~16 percentage points accuracy (84.25% → 67.89%) for ~47% latency reduction (21.6ms → 11.4ms).
  - Accuracy priority: Choose late fusion; accept double the latency of early fusion.
  - Balanced: Intermediate fusion offers a middle ground (72.40% accuracy, 13.5ms latency)—but note it still underperforms unimodal BERT (80.20%).
  - Vision model choice: MobileNetV2-fused models consistently outperform ViT-fused models in this study, suggesting CNN-transformer fusion may be more compatible than transformer-transformer fusion for this specific task.

- **Failure signatures**:
  - Early fusion accuracy collapse: If early fusion accuracy drops far below unimodal BERT (here: 67.89% vs 80.20%), the fusion mechanism is not effectively combining modalities—it may be actively interfering with text feature extraction.
  - No latency gain from early fusion: Check that parallel paths are actually eliminated; if hardware runs operations sequentially anyway, early fusion adds complexity without benefit.
  - ViT underperformance: If ViT-fused models underperform MobileNetV2-fused despite ViT's higher standalone accuracy, suspect architectural incompatibility in the fusion mechanism.

- **First 3 experiments**:
  1. Establish baselines: Fine-tune BERT alone and MobileNetV2 alone on your dataset. These set the accuracy floor/ceiling for fusion experiments. (Paper shows: BERT 80.20%, MobileNetV2 43.22%.)
  2. Latency profiling: Measure inference time for each unimodal model and each fusion variant on your target deployment hardware. Calculate the actual latency reduction ratio to verify it meets application requirements.
  3. Fusion point ablation: If intermediate fusion is of interest, test multiple fusion layer depths (e.g., after layers 2, 4, 6, 8) to map the accuracy-latency curve for your specific task before committing to an architecture.

## Open Questions the Paper Calls Out

- **Can multi-objective neural architecture search (e.g., using Optuna) automatically discover fusion architectures that optimally populate the accuracy-latency trade-off space?**
  - Basis: Future Work section mentions representing this as a multi-objective optimization problem
  - Unresolved because: Challenge lies in encoding a vector-representation of model architecture to enable dynamic generation of varying network connections
  - Evidence needed: A demonstrated framework that automatically generates fusion architectures across the accuracy-latency Pareto frontier

- **How does adding audio as a third modality impact the accuracy-latency trade-off, and does early fusion of audio mitigate latency costs compared to late fusion?**
  - Basis: Future Work section suggests expanding to include audio data
  - Unresolved because: Impact on inference latency depends heavily on how early the auditory data stream can be merged with other streams
  - Evidence needed: Benchmarking three-modality fusion models across early, intermediate, and late fusion strategies with latency measurements

- **Do the observed fusion trade-offs generalize to other vision-language datasets and tasks beyond sentiment analysis?**
  - Basis: Study evaluates only CMU-MOSI, a sentiment analysis dataset
  - Unresolved because: Sentiment analysis may have unique modality importance (text dominates), potentially skewing fusion behavior
  - Evidence needed: Replicating the three fusion strategies on diverse VLM tasks and comparing trade-off patterns

## Limitations

- Study uses only CMU-MOSI dataset with binary sentiment classification, limiting generalizability to other tasks
- No reported statistical variance across experiments - accuracy differences could be within sampling error
- Architectural details for classification heads and exact layer mappings for intermediate fusion are unspecified
- Latency measurements focus on inference time but don't account for memory usage or energy consumption

## Confidence

- **High confidence**: Late fusion achieves highest accuracy (84.25%) and earlier fusion reduces latency (late 21.6ms → early 11.4ms) - these are directly measured and reported with clear methodology
- **Medium confidence**: Accuracy-latency trade-off is the primary design consideration - while supported by data, the analysis doesn't explore whether alternative fusion mechanisms could achieve better trade-offs
- **Low confidence**: MobileNetV2-fused models consistently outperform ViT-fused models - the paper doesn't provide ablation studies or architectural analysis to explain this pattern

## Next Checks

1. **Statistical validation**: Run late fusion and early fusion models across 5 different random seeds and calculate confidence intervals for accuracy and latency to determine if observed differences are statistically significant
2. **Architecture probing**: Implement intermediate fusion at multiple layer depths (after layers 2, 4, 6, 8) and measure the full accuracy-latency curve to identify optimal trade-off points for this specific task
3. **Cross-dataset generalization**: Evaluate the three fusion architectures on a different multimodal dataset (e.g., Visual Dialog or Flickr30k) to test whether observed accuracy-latency patterns hold across domains and tasks