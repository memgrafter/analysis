---
ver: rpa2
title: Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration
arxiv_id: '2511.17123'
source_url: https://arxiv.org/abs/2511.17123
tags:
- energy
- accuracy
- weight
- layers
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a layer-wise weight selection framework\
  \ for power-efficient neural network acceleration on systolic-array hardware. The\
  \ key innovation is combining a per-layer MAC energy model\u2014based on MSB-Hamming-distance\
  \ grouping of partial-sum transitions and tile-level systolic mapping\u2014with\
  \ an energy\u2013accuracy co-optimized weight restriction algorithm."
---

# Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration

## Quick Facts
- arXiv ID: 2511.17123
- Source URL: https://arxiv.org/abs/2511.17123
- Reference count: 19
- Primary result: Up to 58.6% energy reduction with only 2–3% accuracy loss on LeNet-5 and ResNet

## Executive Summary
This paper introduces a layer-wise weight selection framework designed to optimize power efficiency on systolic-array hardware. By integrating a per-layer MAC energy model with an energy–accuracy co-optimized weight restriction algorithm, the method selectively applies aggressive compression to high-energy layers while preserving overall model accuracy. The framework leverages MSB-Hamming-distance grouping of partial-sum transitions and tile-level systolic mapping to estimate and minimize energy consumption.

Experiments demonstrate significant energy savings—up to 58.6%—with minimal accuracy degradation (2–3%) on standard architectures like LeNet-5 and ResNet. Ablation studies confirm that both layer-wise scheduling and the co-optimized weight selection are essential for achieving superior energy-accuracy trade-offs compared to prior power-aware pruning methods.

## Method Summary
The framework combines a per-layer MAC energy model with a weight restriction algorithm tailored for systolic-array hardware. The energy model estimates per-layer costs using MSB-Hamming-distance grouping of partial-sum transitions and tile-level systolic mapping. The weight selection algorithm co-optimizes energy and accuracy, applying aggressive compression selectively to high-energy layers. This approach enables efficient deployment on systolic arrays by minimizing dynamic energy while maintaining acceptable accuracy loss.

## Key Results
- Up to 58.6% energy reduction on LeNet-5 and ResNet architectures
- Accuracy loss limited to 2–3% compared to baselines
- Outperforms prior power-aware pruning methods in energy-accuracy trade-offs

## Why This Works (Mechanism)
The framework exploits the inherent energy asymmetry in systolic-array operations by modeling MAC energy based on MSB-Hamming-distance transitions in partial sums. By grouping these transitions and mapping them to tile-level systolic operations, the method accurately predicts per-layer energy costs. The co-optimized weight restriction algorithm then selectively compresses high-energy layers, achieving significant power savings without compromising overall accuracy. This targeted approach ensures that aggressive compression is applied only where it yields the greatest energy benefit.

## Foundational Learning
- **Systolic-array architecture**: Needed to understand how data flows and accumulates in hardware; quick check: verify tile-level mapping matches systolic array tiling.
- **MSB-Hamming-distance**: Needed to quantify bit-level transitions in partial sums; quick check: confirm Hamming distance grouping aligns with observed energy patterns.
- **Per-layer energy modeling**: Needed to identify and prioritize high-cost layers; quick check: validate energy estimates against hardware measurements.
- **Weight restriction algorithms**: Needed to co-optimize compression and accuracy; quick check: ensure algorithm preserves critical weights in low-energy layers.
- **Tile-level systolic mapping**: Needed to map model layers efficiently onto hardware; quick check: confirm mapping reduces redundant computations.

## Architecture Onboarding
**Component map**: Weight selection algorithm → Energy model (MSB-Hamming-distance + tile-level systolic mapping) → Per-layer MAC energy estimation → Compression scheduling
**Critical path**: Energy model → Weight restriction → Compressed model deployment
**Design tradeoffs**: Aggressive compression increases energy savings but risks accuracy loss; selective application balances both.
**Failure signatures**: Over-compression in low-energy layers may cause unnecessary accuracy drop; inaccurate energy modeling may misguide compression scheduling.
**First experiments**: 1) Validate MSB-Hamming-distance grouping on partial sums; 2) Benchmark energy model accuracy on systolic-array hardware; 3) Test layer-wise compression impact on accuracy for small models.

## Open Questions the Paper Calls Out
None

## Limitations
- Energy model accuracy depends on precise MSB-Hamming-distance estimation but lacks hardware validation.
- Layer-wise scheduling assumes static weights and does not account for dynamic activation sparsity or runtime variations.
- Accuracy results are limited to MNIST and CIFAR-10, with unclear generalization to complex datasets.
- Does not compare against emerging structured sparsity methods exploiting weight distribution patterns.

## Confidence
- Energy reduction claims: Medium (model-based estimates align with prior work but lack direct hardware validation)
- Accuracy retention: High (for tested models, though scalability uncertain)
- Layer-wise scheduling criticality: High (supported by ablation studies)

## Next Checks
1. Measure actual energy consumption on systolic-array prototype or FPGA to validate MAC energy model.
2. Test framework scalability on larger networks (e.g., VGG, MobileNet) and complex datasets (e.g., ImageNet).
3. Compare against recent structured sparsity accelerators exploiting activation sparsity or dynamic voltage scaling.