---
ver: rpa2
title: 'MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static
  Mixture-of-Experts'
arxiv_id: '2511.21089'
source_url: https://arxiv.org/abs/2511.21089
tags:
- dense
- mlpmoe
- branches
- sparsity
- branch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLPMoE introduces a training-free method to convert dense transformer
  MLPs into static mixtures of experts using tensor slicing and summation, avoiding
  clustering or routing calibration. Applied to Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B,
  the transformation changes proxy perplexity by less than 0.05% with constant parameter
  counts.
---

# MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts

## Quick Facts
- arXiv ID: 2511.21089
- Source URL: https://arxiv.org/abs/2511.21089
- Authors: Ivan Novikov
- Reference count: 10
- Transforms dense transformer MLPs into static MoEs via training-free tensor slicing with <0.05% perplexity change

## Executive Summary
MLPMoE introduces a training-free method to convert dense transformer MLPs into static mixtures of experts using tensor slicing and summation. The transformation reinterprets tensor parallelism algebra as a topological conversion, allowing post-hoc structural restructuring without clustering or routing calibration. Applied to Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the method achieves constant parameter counts with negligible perplexity changes, and enables differential sparsity that prunes about 20% of MLP parameters while keeping perplexity within 2% of the dense baseline.

## Method Summary
The method partitions the intermediate dimension of dense MLP weight matrices into contiguous slices, creating "expert" branches that are summed during forward pass. Each branch contains sliced weight matrices (gate_proj, up_proj, down_proj) with scalar gates initialized to 1.0. The approach leverages algebraic equivalence at initialization—the summation of independently computed branch outputs reconstructs the original dense MLP function. The transformation requires no training data, router training, or gradient updates, enabling zero-shot architectural conversion of pretrained models.

## Key Results
- MLPMoE-All-16 conversion: Perplexity changes <0.05% for both 0.5B and 8B models
- Differential sparsity: ~20% parameter reduction on 8B model with ~2.3% perplexity increase
- Fractal Fade pruning: ~18% parameter reduction on 0.5B model with 13% perplexity increase

## Why This Works (Mechanism)

### Mechanism 1: Algebraic Equivalence via Contiguous Slicing
The identity FFN(x) = Σᵦ W_down⁽ᵇ⁾ φ(W_up⁽ᵇ⁾ x) holds when W matrices are sliced along the intermediate dimension such that d_inter = Σ dᵦ. Each branch computes independently; summation reconstructs the dense output. This works because activation functions operate element-wise and distribute over branch summation.

### Mechanism 2: Variance-Preserving Branch Pruning
When pruning to K < B branches, scaling retained branches by √(B/K) approximately preserves output variance. This heuristic maintains perplexity stability under aggressive structural reduction by compensating for variance lost when removing branches.

### Mechanism 3: Progressive Sparsity via Quantile Thresholding
Applying increasing sparsity ratios sᵢ = 0.9·i/B to later branches removes redundant parameters while retaining core capabilities in early branches. This treats branches as a "capability spectrum" rather than equals, with early branches serving as dense backbone.

## Foundational Learning

- Concept: Tensor Parallelism Mathematics
  - Why needed here: MLPMoE's core insight reinterprets distributed training's all-reduce summation as a structural decomposition. Without this, the slicing appears arbitrary.
  - Quick check question: Given weight matrix W split into W₁, W₂ with outputs y₁ = W₁x, y₂ = W₂x, does Wx = y₁ + y₂ always hold? (Answer: Yes, by linearity.)

- Concept: Static vs. Dynamic MoE Routing
  - Why needed here: MLPMoE creates "experts" without learned routing. Understanding what's lost (conditional computation) clarifies the tradeoff.
  - Quick check question: In a 4-expert static MoE where all branches compute per token, what is the FLOP reduction compared to top-1 dynamic routing? (Answer: Zero reduction—all branches compute; only parameter count changes.)

- Concept: Perplexity as Proxy Metric
  - Why needed here: The paper reports <0.05% perplexity change, but uses a synthetic "proxy perplexity." Standard benchmarks (C4, WikiText-2) would provide broader validation.
  - Quick check question: If perplexity remains constant but downstream task accuracy drops, what property might be degraded? (Answer: Task-specific representations, factual recall, or reasoning coherence.)

## Architecture Onboarding

- Component map:
Dense MLP (W_gate, W_up, W_down) -> Slice along d_inter into B branches -> Branch 0: W_gate[0:d₀], W_up[0:d₀], W_down[0:d₀], α₀=1.0 -> Branch 1: W_gate[d₀:d₀+d₁], ..., α₁=1.0 -> ... -> Branch B-1: ..., α_{B-1}=1.0 -> Forward pass (per token) -> branch_b(x) = α_b · W_down^(b) [φ(W_gate^(b) x) ⊙ W_up^(b) x] -> Aggregation -> output = Σ_b branch_b(x)

- Critical path:
  1. Extract W_gate, W_up, W_down from HuggingFace model checkpoint
  2. Compute split_sizes ensuring Σ dᵦ = d_inter (handle remainder)
  3. Create nn.ModuleList of branches with sliced weight buffers
  4. Initialize all α_b = 1.0; verify perplexity matches baseline (sanity check)
  5. Apply Fractal Fade or Compensated Pruning if desired

- Design tradeoffs:
  - Branch count (B): Higher B → finer granularity for sparsity, but more overhead without optimized kernels
  - Sparsity ratio formula: The 0.9·i/B formula is arbitrary; tuning may improve specific models
  - Pruning threshold: Quantile-based thresholding ignores absolute magnitude; small models may need calibration

- Failure signatures:
  - Perplexity explodes (>5% increase): Check dimension mismatch in slicing or aggregation
  - No perplexity change after pruning: Branch outputs may be near-zero; inspect activation magnitudes per branch
  - Generation time increases: Expected without sparse kernels—all branches still compute

- First 3 experiments:
  1. Sanity check: Convert Qwen2.5-0.5B to MLPMoE-All-16 (no sparsity), verify perplexity within 0.1% of baseline
  2. Sparsity sweep: Apply Fractal Fade with varying base sparsity (0.5, 0.7, 0.9) on 8B model, plot perplexity vs. parameter reduction
  3. Branch importance: Zero each branch individually, measure perplexity impact to test the "early branches = backbone" assumption

## Open Questions the Paper Calls Out

- Can dynamic token-to-expert routing be layered atop MLPMoE's static branches to achieve conditional computation benefits without degradation?
- Can specialized sparse kernels translate the theoretical 20% parameter reduction into measurable wall-clock speedups?
- Does MLPMoE preserve downstream task performance on standard NLP benchmarks beyond proxy perplexity?
- Why does differential sparsity degrade perplexity by 13% on the 0.5B model but only 2.3% on the 8B model, and does this indicate a minimum scale threshold?

## Limitations

- Proxy perplexity evaluation uses unspecified "synthetic text mixture" rather than standard benchmarks
- Model-size sensitivity: 13% perplexity increase on 0.5B model vs 2.3% on 8B model with sparsity
- No wall-clock speedups without custom sparse kernels despite parameter reduction

## Confidence

- Mechanism 1 (Algebraic Equivalence): High - The tensor slicing identity is mathematically proven for element-wise activations.
- Mechanism 2 (Variance-Preserving Pruning): Medium - Empirical support exists but theoretical justification is heuristic.
- Mechanism 3 (Progressive Sparsity): Low-Medium - The "early branches = backbone" assumption lacks validation and shows model-size sensitivity.

## Next Checks

1. Replace the unspecified proxy perplexity with established datasets (C4, WikiText-2) to verify that perplexity improvements translate to standard evaluation protocols.

2. Systematically zero individual branches to empirically test the "early branches contain core capabilities" assumption that underpins the progressive sparsity mechanism.

3. Measure impact on task-specific performance (MMLU, BBH, etc.) to detect whether perplexity preservation masks degradation in reasoning or factual recall capabilities.