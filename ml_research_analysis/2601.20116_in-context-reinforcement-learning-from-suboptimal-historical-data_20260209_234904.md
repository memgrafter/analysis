---
ver: rpa2
title: In-Context Reinforcement Learning From Suboptimal Historical Data
arxiv_id: '2601.20116'
source_url: https://arxiv.org/abs/2601.20116
tags:
- learning
- pretraining
- policy
- optimal
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of in-context reinforcement
  learning (ICRL) from suboptimal historical data, where standard autoregressive transformer
  training corresponds to imitation learning and yields suboptimal performance. The
  authors propose the Decision Importance Transformer (DIT) framework, which emulates
  the actor-critic algorithm in an in-context manner.
---

# In-Context Reinforcement Learning From Suboptimal Historical Data

## Quick Facts
- **arXiv ID**: 2601.20116
- **Source URL**: https://arxiv.org/abs/2601.20116
- **Reference count**: 40
- **Primary result**: DIT achieves superior performance to BC and is comparable to DPT in both online and offline testing despite being pretrained without optimal action labels.

## Executive Summary
This paper addresses the challenge of in-context reinforcement learning (ICRL) from suboptimal historical data, where standard autoregressive transformer training corresponds to imitation learning and yields suboptimal performance. The authors propose the Decision Importance Transformer (DIT) framework, which emulates the actor-critic algorithm in an in-context manner. DIT first trains a transformer-based value function to estimate the advantage functions of behavior policies that collected suboptimal trajectories, then trains a transformer-based policy via weighted maximum likelihood estimation where weights are constructed from the trained value function to steer suboptimal policies toward optimal ones. Experiments on bandit and MDP problems show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

## Method Summary
DIT operates in two phases: (1) train in-context advantage estimator using two transformers for Q and V values with regression and Bellman regularization losses, (2) train policy transformer via weighted maximum likelihood where weights are derived from the advantage estimator. The method is designed to improve upon behavior cloning when historical data is suboptimal by reweighting actions according to their estimated advantage values. The framework handles task inference implicitly from context datasets without explicit task identifiers.

## Key Results
- DIT outperforms behavior cloning (BC) and is comparable to Decision Transformer (DPT) in both online and offline testing scenarios
- Performance is particularly strong when pretraining data contains suboptimal trajectories with reasonable rewards (30% of optimal)
- The method successfully handles task inference from context datasets without explicit task identifiers

## Why This Works (Mechanism)

### Mechanism 1: Weighted Maximum Likelihood Estimation (WMLE)
- Claim: DIT improves upon imitation learning by reweighting actions from suboptimal trajectories according to their estimated advantage values, steering the policy toward better actions.
- Mechanism: Instead of treating all state-action pairs equally (behavior cloning), DIT computes per-action weights \( w = \exp(\hat{A}^b_\tau(s,a)/\eta) \) using an advantage estimator and maximizes a weighted log-likelihood. This upweights good actions and downweights poor ones.
- Core assumption: The advantage estimator provides accurate enough ranking of actions within each trajectory; the pretraining dataset contains at least some reasonably good actions.

### Mechanism 2: In-Context Advantage Function Estimation
- Claim: A transformer-based critic (\(\hat{Q}\) and \(\hat{V}\)) estimates advantage values in-context across tasks without explicit task identifiers.
- Mechanism: Two transformers take historical transitions and predict cumulative rewards; the advantage is \(\hat{A} = \hat{Q} - \hat{V}\). Training combines regression to returns and Bellman-style regularization (LB_Q, LB_V).
- Core assumption: Tasks share sufficient structure for the critic to generalize; per-trajectory context is rich enough to estimate advantage.

### Mechanism 3: Task Inference via Context Dataset
- Claim: DIT infers the (unknown) task identity from a context dataset of suboptimal trajectories, enabling generalization to unseen tasks.
- Mechanism: The transformer conditions on the context dataset (transitions) to extract implicit task parameters, then predicts actions based on that inference. No explicit task index is needed.
- Core assumption: The context dataset contains enough task-relevant information; tasks are drawn from a stationary distribution.

## Foundational Learning

- **Concept**: Advantage Function in RL
  - Why needed here: DIT uses advantage values to weight actions during pretraining; understanding how advantage measures relative goodness is critical.
  - Quick check question: For a state \(s\) and action \(a\), what does \(A(s,a) > 0\) imply about action \(a\)?

- **Concept**: Offline Reinforcement Learning
  - Why needed here: DIT operates on historical, suboptimal data; distribution shift and conservatism are key concerns.
  - Quick check question: Why can't we simply train a policy by maximizing return on offline data without adjustment?

- **Concept**: In-Context Learning with Transformers
  - Why needed here: DIT leverages transformers to infer task-specific policies from context datasets without parameter updates.
  - Quick check question: What information does the transformer use to adapt its predictions to a new task?

## Architecture Onboarding

- **Component map**: Context dataset → Q-Value Transformer → V-Value Transformer → Advantage Estimator → Weighted Policy Transformer → Action prediction

- **Critical path**:
  1. Collect suboptimal trajectories from multiple tasks
  2. Train \(\hat{Q}\) and \(\hat{V}\) with combined regression + Bellman losses
  3. Compute \(\hat{A}\) and derive weights \(w = \exp(\hat{A}/\eta)\)
  4. Train \(T_\theta\) via weighted maximum likelihood
  5. Deploy: condition \(T_\theta\) on context dataset and current state to predict actions

- **Design tradeoffs**:
  - Choice of \(\eta\): Larger \(\eta\) keeps policy closer to behavior; smaller \(\eta\) allows greater improvement but may amplify estimation errors
  - Weighting vs. BC: WMLE adds complexity but is necessary when data is suboptimal; BC is simpler but prone to imitation of poor actions
  - In-context vs. explicit task ID: In-context is flexible but may be less precise than explicit task embeddings

- **Failure signatures**:
  - Policy collapse: If \(\eta\) too small or \(\hat{A}\) overestimates, weights become extreme and policy degenerates
  - Context misalignment: Performance drops sharply when context trajectory is from a different task
  - Insufficient data coverage: If no near-optimal actions exist, advantage weighting cannot recover them

- **First 3 experiments**:
  1. Reproduce bandit results: Train DIT on linear bandit data, compare cumulative regret to Thompson Sampling and BC
  2. Ablate advantage estimator: Train a variant using random weights or equal weights, verify performance drop vs. full DIT
  3. Context alignment study: Systematically vary the proportion of in-task vs. out-of-task context data and measure performance degradation

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise boundary of "reasonable rewards" required in the suboptimal historical data for DIT to successfully infer near-optimal policies, and can this boundary be theoretically characterized?
  - Basis in paper: The authors state in the Discussion that DIT requires behavior policies with "reasonable rewards" and that it is "highly unlikely to infer near-optimal actions solely from random trajectories," promising to "explore the limits" in future work.
  - Why unresolved: The paper demonstrates success with data of 30% optimal reward, but does not quantify the exact threshold where performance collapses.
  - What evidence would resolve it: Theoretical analysis or empirical curves showing performance degradation as the average return of the pretraining data approaches that of a random policy.

- **Open Question 2**: How does high stochasticity in the environment affect the stability and accuracy of the in-context advantage function estimator trained on single-trajectory returns?
  - Basis in paper: Section 4.2 uses in-trajectory discounted cumulative rewards as "noisy labels" for training the value functions; in high-variance environments, these single-trajectory returns may be insufficiently accurate targets for stable policy improvement.
  - Why unresolved: While the Related Work notes Decision Transformers struggle with stochasticity, DIT's reliance on these noisy returns for weighted pretraining remains untested in highly stochastic domains.
  - What evidence would resolve it: Experiments on benchmark tasks with high transition or reward noise comparing the variance of \(\hat{A}\) estimates and the resulting policy performance.

- **Open Question 3**: Can the weighted pretraining framework generalize effectively to out-of-distribution tasks that differ structurally from the pretraining task families?
  - Basis in paper: The experiments test on held-out tasks from the same families, leaving open the question of generalization to fundamentally different MDP structures.
  - Why unresolved: The advantage estimator interpolates across trajectories from the pretraining distribution, but may fail to estimate advantages accurately for task structures not represented in the offline data.
  - What evidence would resolve it: Cross-domain evaluation where a pretrained DIT is deployed on tasks with different transition dynamics or observation spaces than those seen during pretraining.

## Limitations
- Performance degrades sharply when context datasets are misaligned with target tasks, indicating brittle generalization
- The method assumes offline data contains at least some non-trivially good actions, with limited analysis of failure cases when this assumption breaks
- Hyperparameter sensitivity (particularly η and λ) is not thoroughly explored, and the choice of λ for balancing Bellman regularization vs regression loss is not specified

## Confidence
- **High**: The DIT framework's two-phase training approach (advantage estimation followed by weighted policy training) is clearly specified and theoretically grounded. Experimental methodology and metrics are well-defined.
- **Medium**: Claims about performance improvements over baselines, particularly on complex MDPs, are supported by results but could benefit from more extensive ablations and sensitivity analyses.
- **Medium**: The theoretical analysis (Proposition 4.2) provides useful characterization but relies on idealized assumptions about advantage estimation accuracy.

## Next Checks
1. **Advantage estimator ablation**: Train DIT variants using random weights, uniform weights, and perfect advantage oracle to quantify the contribution of the in-context advantage estimator to final performance.
2. **Context misalignment study**: Systematically vary the proportion of in-task vs. out-of-task context data (0%, 25%, 50%, 75%, 100%) and measure performance degradation curves to better understand generalization boundaries.
3. **Hyperparameter sensitivity**: Conduct comprehensive sweeps over η and λ values to identify optimal ranges and assess robustness to hyperparameter choices.