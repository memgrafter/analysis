---
ver: rpa2
title: 'Bench360: Benchmarking Local LLM Inference from 360 Degrees'
arxiv_id: '2511.16682'
source_url: https://arxiv.org/abs/2511.16682
tags:
- inference
- energy
- metrics
- latency
- bench360
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bench360, a comprehensive framework for benchmarking
  local LLM inference across models, inference engines, quantization schemes, and
  deployment scenarios. The framework evaluates both task-specific quality (e.g.,
  accuracy, F1, ROUGE) and system metrics (latency, throughput, energy, memory usage,
  cold start time).
---

# Bench360: Benchmarking Local LLM Inference from 360 Degrees

## Quick Facts
- arXiv ID: 2511.16682
- Source URL: https://arxiv.org/abs/2511.16682
- Reference count: 24
- Introduces comprehensive framework for benchmarking local LLM inference across models, engines, quantization schemes, and deployment scenarios

## Executive Summary
This paper presents Bench360, a comprehensive framework for benchmarking local LLM inference across multiple dimensions including models, inference engines, quantization schemes, and deployment scenarios. The framework evaluates both task-specific quality metrics (accuracy, F1, ROUGE) and system metrics (latency, throughput, energy, memory usage, cold start time). Through extensive experiments on four NLP tasks, three GPUs, and four inference engines, the study reveals that no single configuration dominates across all scenarios, highlighting the importance of careful system design choices based on specific deployment requirements.

## Method Summary
The authors developed a comprehensive benchmarking framework that evaluates local LLM inference across multiple dimensions. The framework tests four inference engines (vLLM, SGLang, LMDeploy, TGI) on three GPU models (L4, A10, A30) using four quantization schemes across various model sizes. Evaluation includes both quality metrics (accuracy, F1, ROUGE) and system metrics (latency, throughput, energy consumption, memory usage, cold start time). The study focuses on four NLP tasks to provide task-specific quality measurements alongside system performance analysis.

## Key Results
- No single configuration dominates across all scenarios - optimal deployment depends on carefully balancing model architecture, quantization level, inference engine, and specific workload requirements
- Quantization enables larger models but introduces significant computational overhead
- LMDeploy excels at single-stream responsiveness with fast startup and low time-to-first-token (TTFT)
- SGLang leads in batch throughput on mid-tier GPUs while vLLM dominates on high-end hardware and under concurrent load
- vLLM achieves best energy efficiency through effective scheduling despite moderate GPU utilization

## Why This Works (Mechanism)
The comprehensive benchmarking approach works by systematically evaluating multiple dimensions of local LLM inference simultaneously. By testing across different model architectures, quantization levels, inference engines, hardware configurations, and workload patterns, the framework captures the complex interactions between these variables. The dual focus on quality metrics and system performance provides a complete picture of trade-offs, while the inclusion of both single-stream and concurrent load scenarios reveals how different engines handle various deployment patterns.

## Foundational Learning
- Model architecture trade-offs - why needed: Different architectures have varying computational requirements and quality characteristics; quick check: Compare performance across model families
- Quantization impact analysis - why needed: Quantization affects both model size and computational efficiency; quick check: Measure quality degradation vs. performance gains
- Engine-specific optimization patterns - why needed: Each inference engine has unique strengths and weaknesses; quick check: Compare single vs. concurrent stream performance
- Hardware-aware deployment - why needed: GPU capabilities significantly impact inference performance; quick check: Benchmark across different GPU tiers
- Quality vs. efficiency balance - why needed: Trade-offs between model accuracy and system performance are deployment-critical; quick check: Analyze quality metrics against system metrics

## Architecture Onboarding
- Component map: Models -> Quantization -> Inference Engine -> GPU Hardware -> Workload Pattern -> Quality/System Metrics
- Critical path: Model loading → Quantization application → Engine initialization → GPU memory allocation → Inference execution → Metric collection
- Design tradeoffs: Quality vs. efficiency, single-stream vs. concurrent performance, startup time vs. sustained throughput
- Failure signatures: High TTFT indicates engine initialization issues; memory errors suggest quantization/hardware mismatch; quality degradation points to inappropriate quantization levels
- First experiments: 1) Test single engine on single GPU with one model; 2) Compare quantization levels on same hardware; 3) Evaluate concurrent vs. single stream performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on only four NLP tasks and three GPU models, which may not represent all deployment contexts
- Quantization overhead measurements may vary with different hardware optimization levels
- Does not explore server-class GPUs or mobile/CPU inference scenarios that could yield different performance profiles

## Confidence
- Engine performance comparisons under specific workloads: High
- Generalization to all deployment scenarios: Medium
- Energy efficiency conclusions: Medium
- Quantization overhead findings: High

## Next Checks
1. Test the same benchmarking framework on server-class GPUs (e.g., H100, A100) to validate scaling behavior
2. Expand evaluation to include mobile/CPU inference scenarios for edge deployment insights
3. Validate quantization overhead measurements across different CUDA driver versions and hardware optimization levels