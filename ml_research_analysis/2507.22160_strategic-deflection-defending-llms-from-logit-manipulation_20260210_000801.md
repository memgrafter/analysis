---
ver: rpa2
title: 'Strategic Deflection: Defending LLMs from Logit Manipulation'
arxiv_id: '2507.22160'
source_url: https://arxiv.org/abs/2507.22160
tags:
- harmful
- sdeflection
- safety
- attacks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Strategic Deflection (SDeflection), a defense
  mechanism designed to protect large language models (LLMs) from logit manipulation
  attacks, which are a sophisticated class of jailbreaking techniques. Traditional
  defenses relying on refusal patterns are vulnerable because logit manipulation can
  suppress these refusals during token generation.
---

# Strategic Deflection: Defending LLMs from Logit Manipulation

## Quick Facts
- arXiv ID: 2507.22160
- Source URL: https://arxiv.org/abs/2507.22160
- Authors: Yassine Rachidy; Jihad Rbaiti; Youssef Hmamouche; Faissal Sehbaoui; Amal El Fallah Seghrouchni
- Reference count: 40
- Primary result: Reduces ASR under LogitsTrap from 92.63% to 34.94% (Llama-2-7B-chat-hf), from 89.29% to 8.53% (Llama-3.2-3B-Instruct), from 94.74% to 13.14% (Mistral-7B-Instruct-v0.2)

## Executive Summary
This paper introduces Strategic Deflection (SDeflection), a defense mechanism that protects large language models from logit manipulation attacks by training them to redirect harmful queries toward safe alternatives rather than refusing outright. Traditional refusal-based defenses are vulnerable to attacks that suppress refusal tokens during decoding, but SDeflection uses Contrastive Preference Optimization (CPO) to teach models to generate compliant-sounding responses that strategically deflect toward benign content. The method significantly reduces attack success rates while preserving both safety behavior on direct prompts and general capability performance.

## Method Summary
SDeflection trains LLMs using CPO with preference triplets containing harmful prompts paired with safe deflected responses and harmful completions. The preference triplets use identical compliant prefixes to teach models to deflect when forced into compliance mode while maintaining explicit refusal for direct harmful prompts. Fine-tuning employs LoRA adapters with CPO optimization, avoiding reference models for efficiency. The method was evaluated on three instruction-tuned models (Llama-2-7B-chat-hf, Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.2) against LogitsTrap attacks, showing substantial ASR reduction while preserving safety and capability metrics.

## Key Results
- SDeflection reduces Attack Success Rate from 92.63% to 34.94% on Llama-2-7B-chat-hf under LogitsTrap
- Reduces ASR from 89.29% to 8.53% on Llama-3.2-3B-Instruct and from 94.74% to 13.14% on Mistral-7B-Instruct-v0.2
- Maintains high refusal rates on direct harmful prompts (99.62% vs 97.95% for Llama-2)
- Preserves general capability performance on tinyMMLU, tinyTruthfulQA, tinyGSM8k, and tinyHellaswag benchmarks
- CPO outperforms DPO in both defense effectiveness (8.53% vs 72.63% ASR) and training efficiency

## Why This Works (Mechanism)

### Mechanism 1: Preference Learning Instills Semantic Redirection Patterns
Fine-tuning with contrastive preferences teaches models to redirect harmful queries toward safe alternatives rather than refusing outright. CPO trains policy πθ to prefer deflected safe responses (y+) over harmful completions (y−) using a loss function combining preference optimization with NLL regularization. The model learns to produce semantically adjacent content that appears cooperative while stripping harmful intent.

### Mechanism 2: Deflection Evades Token-Level Suppression Attacks
Deflected responses use normal helpful language without explicit refusal tokens, making them resistant to logit suppression. Logit manipulation attacks like LogitsTrap penalize refusal-related tokens to force harmful outputs. Deflection bypasses this by generating cooperative-sounding content that doesn't trigger suppression lists, yet delivers benign information.

### Mechanism 3: Affirmative Prefix Conditioning Preserves Dual-Mode Safety
Training with affirmative prefixes teaches deflection under attack while preserving explicit refusal for direct prompts. Preference triplets condition both chosen and rejected responses on identical compliant prefixes, teaching the model to deflect when forced into compliance mode while standard safety alignment remains active for normal prompting.

## Foundational Learning

- **Concept: Logit Manipulation Attacks**
  - Why needed: SDeflection specifically targets attacks that modify token probabilities during decoding (e.g., LogitsTrap, EnDec). Understanding how attackers suppress refusal tokens and enforce prefixes is essential for motivation.
  - Quick check: Can you explain why penalizing "cannot" in the logit distribution causes a safety-aligned model to comply with harmful requests?

- **Concept: Contrastive Preference Optimization (CPO)**
  - Why needed: CPO is the training method that enables deflection learning. It differs from DPO by eliminating the reference model, improving efficiency while maintaining preference signal. Understanding the loss components is essential for debugging.
  - Quick check: What happens to the model's general language quality if the NLL regularization term (λ) is set too low?

- **Concept: Preference Triplets Structure**
  - Why needed: The defense hinges on triplet construction (p, y+, y−) where both responses share affirmative prefixes. Understanding how to transform harmful prompts into safe alternatives while preserving semantic adjacency is critical for dataset quality.
  - Quick check: If a chosen response (y+) is semantically unrelated to the harmful prompt (p), what failure mode would you expect during evaluation?

## Architecture Onboarding

- **Component map**: Dataset constructor → Preference triplets → CPO trainer → Attack simulator → Safety evaluator
- **Critical path**: 1) Start with MaliciousInstruct (100 harmful prompts) 2) Transform each to harmless query using LLM assistant 3) Generate chosen (deflected) and rejected (harmful) completions conditioned on affirmation prefixes 4) Train with CPO (3 epochs, LoRA rank 8-32, LR=1e-5 to 5e-6) 5) Evaluate ASR on AdvBench under LogitsTrap attack; verify refusal rates on direct prompts
- **Design tradeoffs**: CPO vs. DPO (CPO achieves 8.53% ASR vs. DPO's 72.63%, 30% faster training); Deflection vs. Deep alignment (deflection more effective but requires careful prompt transformation); Prefix diversity (34 templates used; more may improve generalization)
- **Failure signatures**: ASR plateaus above 50% (check preference triplet quality); Refusal rate drops significantly (training may be overwriting base safety; reduce epochs or increase NLL weight); Benign task performance degrades (deflection may be triggering on non-harmful queries)
- **First 3 experiments**: 1) Replicate LogitsTrap attack baseline to confirm vulnerability (expect 90%+ ASR on AdvBench) 2) Train SDeflection with 100-triplet subset to verify preference learning converges 3) Compare CPO vs. DPO on held-out set to validate efficiency gains

## Open Questions the Paper Calls Out
- **Open Question 1**: Can Strategic Deflection be effectively extended to defend against other emerging adversarial attack vectors beyond logit manipulation? The authors state this conceptual framework may generalize to other adversarial techniques, but applicability to other attack categories remains unexplored.
- **Open Question 2**: Why does CPO achieve dramatically better defense performance than DPO for Strategic Deflection training? The ablation study shows a gap of over 64 percentage points, but the paper does not provide a theoretical or empirical explanation for this disparity.
- **Open Question 3**: Does SDeflection generalize to other logit manipulation attack variants beyond LogitsTrap? The paper evaluates only LogitsTrap but reviews other logit attacks (EnDec, NBW) in related work, leaving performance against attacks with different manipulation signatures unknown.

## Limitations
- Evaluation relies on automated safety classifiers rather than human judgment, which may overestimate effectiveness if classifiers have blind spots
- Preference dataset construction using GPT-4o transformations introduces potential brittleness that attackers could exploit
- The defense was designed specifically for LogitsTrap and may not generalize to other attack types or real-world attack scenarios

## Confidence
- **High confidence**: Attack success rate reduction (ASR metrics are directly measurable and show consistent improvement across all three models)
- **Medium confidence**: Preservation of general capability (tinyBenchmarks scores show minimal degradation, but metrics may not capture nuanced capability losses)
- **Low confidence**: Long-term robustness against adaptive attackers (evaluation only tested against one attack type with fixed parameters)

## Next Checks
1. **Human evaluation of deflected responses**: Conduct blinded human assessment to verify that deflection patterns are both safe and coherent, checking for cases where harmful intent might be preserved in subtle ways
2. **Adversarial stress test**: Design attack variations that target the specific semantic redirection patterns SDeflection relies on, such as queries resistant to harmless transformation or exploiting gaps in the 34 affirmation templates
3. **Cross-dataset generalization**: Test SDeflection against jailbreak datasets beyond AdvBench (e.g., WildChat, JailbreakBench) to assess whether preference learning generalizes to different harmful prompt distributions and attack formulations