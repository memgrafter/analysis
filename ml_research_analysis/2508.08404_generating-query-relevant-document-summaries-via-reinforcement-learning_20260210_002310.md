---
ver: rpa2
title: Generating Query-Relevant Document Summaries via Reinforcement Learning
arxiv_id: '2508.08404'
source_url: https://arxiv.org/abs/2508.08404
tags:
- product
- relevance
- search
- summaries
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ReLSum, a reinforcement learning framework
  for generating query-relevant summaries of product descriptions to improve search
  relevance in e-commerce systems. The method uses relevance scores as rewards to
  train a large language model to produce concise summaries that capture essential
  attributes not present in product titles, addressing the limitations of both verbose
  full descriptions and insufficient titles.
---

# Generating Query-Relevant Document Summaries via Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.08404
- Source URL: https://arxiv.org/abs/2508.08404
- Reference count: 6
- Primary result: RL-trained LLM generates summaries that improve cross-encoder ranking accuracy while meeting latency constraints, with +0.13% R@90P and +0.80% NDCG@5 gains.

## Executive Summary
ReLSum addresses the challenge of incorporating product description information into e-commerce search ranking without violating latency constraints. By using a frozen cross-encoder relevance model as a reward signal, the framework trains an LLM to generate concise summaries that capture attributes absent from titles. The method employs reinforcement learning (specifically GRPO) to align summarization objectives with ranking performance, enabling cross-encoders to process description-derived signals within latency budgets. Offline experiments show significant gains on both head and tail queries, validated by online A/B tests demonstrating improvements across key business metrics.

## Method Summary
The method formulates document summarization as a reinforcement learning problem where an LLM policy generates summaries conditioned on product descriptions and titles. A frozen BERT cross-encoder serves as the reward model, scoring query-summary-title triples. The reward is defined as the negative absolute difference between the predicted relevance and ground-truth label. Training uses GRPO with group-relative advantages, sampling multiple summaries per product to reduce variance. The dataset is filtered to retain examples where descriptions meaningfully impact relevance scores. Summaries are generated offline and concatenated with titles during ranking to improve cross-encoder performance within latency constraints.

## Key Results
- ReLSumGRPO achieved +0.13% R@90P and +0.80% NDCG@5 gains on full golden dataset
- Tail dataset improvements were larger: +0.51% R@90P and +1.03% NDCG@5
- Online interleaving tests showed 0.22% lift in add-to-cart actions (p=0.000)
- A/B tests revealed +0.34% units and +0.26% orders per visitor

## Why This Works (Mechanism)

### Mechanism 1: Reward-Based Objective Alignment
- Claim: Using relevance scores as rewards aligns summarization with ranking performance
- Mechanism: The LLM generates candidate summaries; the cross-encoder scores query-summary-title triples; the reward = -|r(q, [t; s]) - l| directly penalizes summaries that mispredict ground-truth relevance
- Core assumption: The frozen cross-encoder is a sufficiently accurate proxy for true relevance labels
- Evidence anchors: [abstract] "ReLSum leverages relevance scores as rewards to align the objectives of summarization and ranking"
- Break condition: If the cross-encoder is poorly calibrated or biased, the reward signal will propagate that bias

### Mechanism 2: Policy Gradient Optimization via GRPO
- Claim: GRPO enables stable policy updates despite non-differentiable text sampling
- Mechanism: Sample G summaries per product, compute rewards for all, normalize advantages via mean/std, update policy with clipped importance ratio and KL penalty
- Core assumption: Group-relative advantages provide a meaningful learning signal
- Evidence anchors: [section] RL Formulation (Equation 2): GRPO objective with MCAi,j, clipping ϵ, and β DKL regularization
- Break condition: If rewards are too noisy or near-identical across samples, advantages collapse and learning stalls

### Mechanism 3: Targeted Training Data Filtering
- Claim: Filtering training examples to those where description meaningfully changes relevance score focuses learning on high-impact cases
- Mechanism: Compute r(q, [t; d]) and r(q, t) offline; retain examples with noticeable delta
- Core assumption: Offline relevance deltas are predictive of online gains
- Evidence anchors: [section] Training Dataset: "We filter the dataset to retain instances with a noticeable difference"
- Break condition: If filtering is too aggressive, the model may overfit to a narrow distribution

## Foundational Learning

- Concept: Reinforcement Learning basics (policy πθ, reward, environment, advantage)
  - Why needed here: The paper formulates summarization as an RL problem where the LLM is a policy and the ranker is the environment providing rewards
  - Quick check question: Can you explain why a group-relative advantage (normalizing rewards across sampled summaries) reduces variance compared to using raw rewards?

- Concept: Cross-encoder vs. bi-encoder architectures and attention complexity
  - Why needed here: Cross-encoders jointly attend to query and document, yielding higher accuracy but O(n²) cost
  - Quick check question: Why does adding full descriptions to cross-encoder inputs cause latency issues, and how does summarization address this?

- Concept: LLM fine-tuning with LoRA
  - Why needed here: The implementation uses Mistral-7B-Instruct-v0.3 with LoRA adapters for efficient fine-tuning
  - Quick check question: What are the trade-offs of LoRA rank/alpha choices, and how would you detect underfitting vs. overfitting in this RL setup?

## Architecture Onboarding

- Component map:
  - Product information (description, title) -> LLM policy -> Generated summary
  - Query, title, summary -> Frozen cross-encoder -> Relevance score (reward)
  - Reward -> GRPO optimization -> Updated LLM policy
  - Offline summary storage -> Query-time retrieval -> [Title; Summary] -> Cross-encoder

- Critical path:
  1. Freeze a trained cross-encoder ranker with proven calibration
  2. Build filtered training dataset with noticeable r(q, [t; d]) vs r(q, t) gaps
  3. Initialize πref with prompted Mistral-7B (no fine-tuning)
  4. Run GRPO with G=4 samples, temperature 0.9, clipping ϵ=0.2, learning rate 1e-5, 1 epoch
  5. Evaluate on golden-full and golden-tail datasets using R@90P and NDCG@5
  6. Deploy offline summary generation; integrate with online ranker

- Design tradeoffs:
  - GRPO vs DPO: GRPO yields slightly better tail gains but requires more samples per step
  - G (samples per step): 4 balances signal and cost; 8 adds marginal gain
  - KL coefficient β: Set to 0.0 for GRPO in experiments; increasing β constrains deviation from πref
  - Assumption: Prompt design biases toward attribute extraction; this may not suit all product categories

- Failure signatures:
  - Reward hacking: Summaries become repetitive or keyword-stuffed to artificially boost relevance scores
  - Low advantage variance: All sampled summaries receive near-identical rewards; learning plateaus
  - Tail underperformance: If training data over-represents head queries, tail gains may not materialize
  - Latency regression: If summaries are too long or post-processing is slow, the cross-encoder still faces token limits

- First 3 experiments:
  1. Baseline sanity check: Compare None (title only), Desc (full description), and Sumref (zero-shot LLM summary) on golden-full and golden-tail
  2. GRPO vs DPO ablation: Train both variants with identical hyperparameters; evaluate R@90P and NDCG@5 on tail data
  3. Filter threshold sensitivity: Vary the minimum r(q, [t; d]) - r(q, t) delta for training inclusion; measure impact on tail gains

## Open Questions the Paper Calls Out

- **Question:** Does an iterative training framework, where the downstream relevance model is fine-tuned alongside the LLM, further enhance the alignment between generated summaries and ranking objectives?
  - **Basis in paper:** [Explicit] The authors state in the Conclusion: "For future work, we aim to explore an iterative training framework where the LLM is fine-tuned based on the proposed approach, followed by fine-tuning the parameters of the downstream relevance model."
  - **Why unresolved:** The current study treats the cross-encoder ranking model as a frozen component to provide stable reward signals. The potential feedback loop of co-adaptation has not yet been investigated.
  - **What evidence would resolve it:** Experimental results comparing the current frozen-reward setup against a setup where the ranker is updated iteratively with the summary generator.

- **Question:** Can extending the LLM input to include multimodal data, such as product images, improve summarization performance for visually-driven queries?
  - **Basis in paper:** [Explicit] The authors state in the Conclusion: "Additionally, we plan to extend the input to the LLM to include other product attributes, such as product images, to enrich the summarization process and improve relevance predictions for visually-driven queries."
  - **Why unresolved:** The current implementation relies solely on text-based descriptions and titles. The paper does not address scenarios where visual attributes are critical but textual descriptions are sparse or incomplete.
  - **What evidence would resolve it:** A comparative analysis of summary quality and ranking metrics on a dataset of visually-driven queries using a multimodal LLM versus the text-only Mistral-7B-Instruct model.

- **Question:** How does the query-agnostic generation policy impact performance compared to a query-aware policy, given the latency constraints?
  - **Basis in paper:** [Inferred] The paper explicitly formulates the problem such that the "policy model $\pi_\theta$ has access only to product information," treating the query as part of the environment to ensure scalability.
  - **Why unresolved:** While this design choice optimizes for offline computation and latency, it assumes a single summary can be universally optimal for all possible queries.
  - **What evidence would resolve it:** A controlled experiment comparing the relevance scores of static summaries versus dynamic summaries for the same set of items.

## Limitations

- The frozen cross-encoder reward model may introduce bias or reward hacking if poorly calibrated across product categories or query distributions
- The filtered training dataset may introduce selection bias if the threshold for meaningful relevance gaps is not carefully tuned
- Offline precomputed summaries may not capture query-specific nuances that could improve relevance for certain query types

## Confidence

- **High Confidence:** Offline metric improvements (R@90P, NDCG@5) on golden datasets
- **Medium Confidence:** Online business metric gains (ATC, GMV, Orders)
- **Low Confidence:** The claim that ReLSum "effectively overcomes limitations of prior methods" is relative to specific baselines

## Next Checks

1. **Reward Model Calibration:** Evaluate the cross-encoder reward model's performance across diverse product categories and query types to ensure consistent and unbiased signals
2. **Filter Threshold Sensitivity:** Systematically vary the r(q, [t; d]) - r(q, t) threshold for training data inclusion and measure its impact on tail gains and overall model robustness
3. **Online Latency and Quality:** Deploy the summarization pipeline in a staged rollout to monitor real-time generation latency, summary quality, and any unexpected degradations in user experience or business metrics