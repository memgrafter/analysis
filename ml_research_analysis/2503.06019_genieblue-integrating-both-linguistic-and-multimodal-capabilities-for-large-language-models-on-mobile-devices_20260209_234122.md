---
ver: rpa2
title: 'GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large
  Language Models on Mobile Devices'
arxiv_id: '2503.06019'
source_url: https://arxiv.org/abs/2503.06019
tags:
- arxiv
- language
- training
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents GenieBlue, an efficient multimodal large language
  model (MLLM) structure for on-device deployment. The authors identify two key challenges:
  (1) existing MLLMs suffer from performance degradation on pure language tasks, and
  (2) current smartphone NPUs do not support Mixture-of-Experts (MoE) architectures
  commonly used to preserve language capabilities during multimodal training.'
---

# GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices

## Quick Facts
- **arXiv ID:** 2503.06019
- **Source URL:** https://arxiv.org/abs/2503.06019
- **Reference count:** 40
- **Key outcome:** Achieved 97% retention of MLLM performance while maintaining 100% of original language model capabilities through selective parameter freezing and duplication strategy for on-device deployment

## Executive Summary
GenieBlue addresses critical challenges in deploying multimodal large language models (MLLMs) on mobile devices by developing an efficient architecture that preserves both linguistic and multimodal capabilities. The key innovation involves freezing original language model parameters during multimodal training while selectively duplicating and fine-tuning specific transformer blocks, achieving superior performance retention compared to traditional approaches. Deployed on Qualcomm Snapdragon 8 Elite NPU, GenieBlue demonstrates practical feasibility with 30 tokens/second output speed while maintaining 97% of MLLM performance and 100% of language capabilities.

## Method Summary
GenieBlue introduces a novel MLLM training approach that addresses two primary challenges: performance degradation on pure language tasks and incompatibility with Mixture-of-Experts (MoE) architectures on mobile NPUs. The method freezes the original 7B language model parameters during multimodal training, selectively duplicates specific transformer blocks for full fine-tuning, and applies lightweight LoRA modules to remaining blocks. This non-shared base approach enables the model to maintain 100% of its original language capabilities while achieving competitive multimodal performance. The model is trained on 2.5M pre-training samples and 645M fine-tuning samples, demonstrating efficient on-device deployment on Snapdragon 8 Elite NPU with 30 tokens/second inference speed.

## Key Results
- Achieved 97% retention of MLLM performance compared to full fine-tuning approaches
- Maintained 100% of original language model capabilities on pure language tasks
- Deployed efficiently on Snapdragon 8 Elite NPU with 30 tokens/second output speed
- Demonstrated practical mobile deployment with significant parameter efficiency

## Why This Works (Mechanism)
The architecture works by strategically partitioning the transformer network into frozen base components, duplicated blocks for full fine-tuning, and LoRA-enhanced remaining blocks. This selective parameter allocation preserves the language model's original capabilities while acquiring multimodal features through targeted adaptation. The frozen base ensures language task performance remains intact, while duplicated blocks capture multimodal representations through full fine-tuning. LoRA modules provide efficient adaptation for the remaining components, achieving parameter efficiency without sacrificing multimodal performance. This approach is specifically designed to work within NPU constraints that preclude MoE architectures, making it uniquely suited for mobile deployment.

## Foundational Learning
- **Mixture-of-Experts (MoE) Limitations**: Mobile NPUs typically cannot support MoE architectures due to computational constraints and hardware optimization requirements. Understanding this limitation is crucial for developing alternative parameter-efficient approaches.
  - *Why needed*: Explains why traditional MLLM approaches fail on mobile devices
  - *Quick check*: Verify NPU specifications explicitly exclude MoE support

- **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like LoRA enable efficient adaptation of large models by modifying weight updates rather than full parameter training.
  - *Why needed*: Provides the foundation for selective adaptation strategy
  - *Quick check*: Confirm LoRA implementation details match standard practices

- **Selective Duplication Strategy**: The approach of duplicating specific transformer blocks for full fine-tuning while freezing others is central to preserving language capabilities.
  - *Why needed*: Enables the 100% language retention claim
  - *Quick check*: Verify which blocks were duplicated vs. frozen

- **Non-Shared Base Architecture**: Unlike traditional approaches where multimodal training modifies all parameters, this method maintains a distinct separation between frozen language capabilities and adapted multimodal components.
  - *Why needed*: Critical for understanding how both capabilities are preserved
  - *Quick check*: Examine parameter usage before and after training

## Architecture Onboarding
- **Component Map**: Frozen Language Model -> Selective Block Duplication -> LoRA-Enhanced Remaining Blocks -> Multimodal Output
- **Critical Path**: The path from frozen language model through duplicated blocks to final output is critical for maintaining language capabilities while acquiring multimodal features
- **Design Tradeoffs**: The frozen base ensures language preservation but limits cross-modal fusion; selective duplication balances performance retention with computational efficiency; LoRA provides parameter efficiency but may limit adaptation depth
- **Failure Signatures**: Language capability degradation indicates insufficient freezing or inappropriate block selection; multimodal performance loss suggests inadequate duplication or LoRA adaptation; computational inefficiency indicates suboptimal NPU utilization
- **First 3 Experiments**: 1) Benchmark pure language performance on frozen model baseline; 2) Test multimodal capabilities on validation set after selective fine-tuning; 3) Measure NPU deployment metrics including latency, memory usage, and power consumption

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Evaluation scope is limited to specific benchmarks without comprehensive comparisons to state-of-the-art MLLMs like GPT-4V or Gemini
- NPU deployment claims lack comparative benchmarks against other on-device MLLMs or baseline LLM performance on the same hardware
- Memory and power consumption metrics are absent, which are critical for mobile deployment decisions
- The architectural novelty is moderate, building on existing parameter-efficient fine-tuning methods

## Confidence
- **High Confidence**: Technical feasibility of proposed architecture and training methodology are sound and well-implemented
- **Medium Confidence**: Claimed performance metrics (97% MLLM retention, 100% language retention) are internally consistent but lack external validation
- **Medium Confidence**: Deployment results on Snapdragon 8 Elite are plausible but insufficiently benchmarked against alternatives

## Next Checks
1. Conduct comprehensive cross-modal reasoning evaluations comparing GenieBlue against leading MLLMs (GPT-4V, Gemini, Claude) on established benchmarks like MMMU, MathVista, and RealWorldQA

2. Measure memory footprint, latency, and power consumption during inference on Snapdragon 8 Elite, comparing against both baseline LLM and other MLLM deployment approaches

3. Perform ablation studies varying the proportion of duplicated vs. LoRA-modified blocks to identify optimal balance between performance preservation and parameter efficiency