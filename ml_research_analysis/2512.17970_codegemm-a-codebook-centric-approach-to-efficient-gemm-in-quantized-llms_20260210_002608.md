---
ver: rpa2
title: 'CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs'
arxiv_id: '2512.17970'
source_url: https://arxiv.org/abs/2512.17970
tags:
- quantization
- codegemm
- should
- accuracy
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeGEMM introduces a codebook-centric GEMM kernel for quantized
  LLMs that eliminates dequantization by precomputing and storing partial sums in
  a Psumbook. This approach reduces computational complexity and cache footprint compared
  to traditional dequantization-based kernels.
---

# CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs

## Quick Facts
- arXiv ID: 2512.17970
- Source URL: https://arxiv.org/abs/2512.17970
- Reference count: 40
- Primary result: 1.83× (8B) and 8.93× (70B) speedups in 2-bit quantized Llama-3 models

## Executive Summary
CodeGEMM introduces a novel codebook-centric approach to efficient General Matrix Multiplication (GEMM) operations in quantized large language models (LLMs). By eliminating the traditional dequantization step and instead precomputing partial sums into a Psumbook, the method significantly reduces computational complexity and cache footprint. The approach achieves substantial speedups while maintaining comparable accuracy to standard quantized GEMM implementations.

## Method Summary
CodeGEMM replaces the traditional dequantization-based GEMM kernel with a codebook-centric approach that precomputes and stores partial sums in a Psumbook. This eliminates the need for runtime dequantization operations, reducing computational overhead and memory access patterns. The method reorganizes matrix multiplication to leverage the precomputed values, achieving higher computational efficiency and better memory subsystem utilization.

## Key Results
- 1.83× speedup on 8B Llama-3 models with 2-bit quantization
- 8.93× speedup on 70B Llama-3 models with 2-bit quantization
- Maintains comparable accuracy to traditional dequantization-based approaches

## Why This Works (Mechanism)
CodeGEMM works by fundamentally restructuring the GEMM operation to eliminate the dequantization bottleneck. Instead of performing quantization, dequantization, and multiplication at runtime, the method precomputes partial sums for quantized values and stores them in a Psumbook. This reorganization reduces the number of arithmetic operations and improves data locality, leading to better cache utilization and overall computational efficiency.

## Foundational Learning
- **Quantization in LLMs**: Why needed - reduces memory footprint and computational requirements for large models; Quick check - understand the difference between symmetric and asymmetric quantization schemes
- **General Matrix Multiplication (GEMM)**: Why needed - core operation in neural network inference; Quick check - understand the computational complexity and memory access patterns of standard GEMM
- **Dequantization**: Why needed - converts quantized values back to floating point for computation; Quick check - understand the performance impact of dequantization in quantized models
- **Codebook-based approaches**: Why needed - leverage precomputed values for faster computation; Quick check - understand how codebook-based methods differ from traditional lookup approaches

## Architecture Onboarding
**Component map**: Input matrix -> Quantization -> Psumbook generation -> Codebook-based GEMM kernel -> Output matrix

**Critical path**: The most performance-critical components are Psumbook generation and the codebook-based GEMM kernel execution. Psumbook generation must be efficient enough to offset any overhead from the codebook-based approach.

**Design tradeoffs**: The primary tradeoff is between memory usage (Psumbook requires additional storage) and computational efficiency. The method also trades off flexibility in quantization schemes for the specific codebook-based approach.

**Failure signatures**: Performance degradation occurs when the Psumbook exceeds cache capacity, when quantization schemes are not well-suited to the codebook approach, or when the overhead of Psumbook generation outweighs the benefits.

**First experiments to run**:
1. Benchmark baseline dequantization-based GEMM vs. CodeGEMM on various model sizes and quantization levels
2. Measure cache utilization and memory access patterns for both approaches
3. Profile accuracy degradation across different quantization schemes and model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Validation limited to Llama-3 models with 2-bit quantization only
- Memory footprint claims lack comprehensive empirical validation across diverse scenarios
- Accuracy preservation claims require more detailed statistical analysis

## Confidence
- **High confidence**: Core algorithmic innovation of eliminating dequantization is technically sound
- **Medium confidence**: Speedup claims supported but limited to specific conditions
- **Medium confidence**: Theoretical cache footprint analysis logical but not exhaustively validated

## Next Checks
1. Test CodeGEMM on additional LLM architectures (Mistral, Gemma, etc.) and different quantization bit-widths (4-bit, 3-bit) to assess generalization
2. Conduct comprehensive accuracy degradation analysis with statistical significance testing across multiple model variants and tasks
3. Benchmark real-world memory usage patterns comparing Psumbook vs. dequantization approaches under varying cache sizes and batch configurations