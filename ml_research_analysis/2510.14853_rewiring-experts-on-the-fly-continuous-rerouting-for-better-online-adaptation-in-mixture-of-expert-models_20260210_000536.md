---
ver: rpa2
title: Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation
  in Mixture-of-Expert models
arxiv_id: '2510.14853'
source_url: https://arxiv.org/abs/2510.14853
tags:
- routing
- arxiv
- expert
- layers
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a data-free test-time adaptation method for
  Mixture-of-Experts (MoE) models that dynamically optimizes expert routing decisions
  during inference using only the input context itself. The method alternates between
  two phases: (1) in-context routing optimization using self-supervised loss on the
  current context to update router logits, and (2) steered generation using the updated
  routing parameters.'
---

# Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models

## Quick Facts
- arXiv ID: 2510.14853
- Source URL: https://arxiv.org/abs/2510.14853
- Reference count: 16
- Primary result: Improves HumanEval performance by 5.5-6.7% across multiple MoE models through dynamic routing optimization

## Executive Summary
This paper introduces a data-free test-time adaptation method for Mixture-of-Expert (MoE) models that dynamically optimizes expert routing decisions during inference. The approach uses self-supervised loss on input context to update router logits, enabling better online adaptation without requiring additional training data or fine-tuning. By alternating between routing optimization and steered generation phases, the method achieves consistent performance improvements across multiple benchmarks while maintaining computational efficiency.

## Method Summary
The method employs a two-phase approach during inference: (1) in-context routing optimization using self-supervised loss on the current context to update router logits, and (2) steered generation using the updated routing parameters. Lightweight additive vectors are used to modify router logits in selected high-confidence layers to maintain computational efficiency and prevent over-adaptation. This data-free approach works by treating routing as a continuous optimization problem that can be solved using gradient-based methods during inference.

## Key Results
- Improves HumanEval performance by 5.5% on OLMoE, 6.7% on Qwen1.5-MoE, and 6.7% on DeepSeek-V2-Lite
- Complements existing test-time scaling techniques, achieving 6% average gains when combined with self-consistency on DeepSeek-V2-Lite
- Maintains computational efficiency, using 1.6× fewer FLOPs than few-shot methods while demonstrating robustness to context shifts in multi-turn scenarios

## Why This Works (Mechanism)
The method works by recognizing that expert routing decisions are not static but can be dynamically optimized based on the input context itself. By treating routing as a continuous optimization problem solvable through gradient descent during inference, the model can adapt its computational pathway to better match the characteristics of each specific input. The self-supervised loss function provides a feedback signal that guides routing decisions without requiring labeled data or external supervision.

## Foundational Learning

**Mixture-of-Expert (MoE) models**: Neural architectures that activate specialized sub-networks (experts) based on input characteristics, improving efficiency and performance by routing different inputs to different experts.

*Why needed*: Understanding MoE is essential because the paper specifically addresses routing optimization in these models, where the core innovation lies.

*Quick check*: Can you explain how MoE differs from standard dense transformer models in terms of computational efficiency?

**Test-time adaptation**: Methods that adapt model behavior during inference without requiring additional training or fine-tuning on new data.

*Why needed*: This paper's contribution is a test-time adaptation technique, so understanding this concept is fundamental to grasping the innovation.

*Quick check*: What distinguishes test-time adaptation from traditional fine-tuning approaches?

**Self-supervised learning**: Learning approaches that generate supervision signals from the input data itself without requiring external labels.

*Why needed*: The paper uses self-supervised loss for routing optimization, making this concept central to understanding the mechanism.

*Quick check*: How does self-supervision enable adaptation without labeled data?

## Architecture Onboarding

**Component map**: Input context → Router logit computation → Routing optimization (self-supervised loss) → Expert activation → Steered generation → Output

**Critical path**: The optimization occurs in router logit computation layers, where additive vectors modify the routing decisions before expert selection. This is the primary point of intervention that affects downstream generation quality.

**Design tradeoffs**: The method balances between adaptation strength (how much to modify routing) and computational efficiency (using lightweight additive vectors rather than full parameter updates). It also trades off between adaptation specificity (per-input optimization) and generalization (avoiding over-specialization to single contexts).

**Failure signatures**: Poor routing optimization may lead to: (1) inconsistent outputs across similar inputs, (2) computational overhead without performance gains, (3) over-adaptation to specific contexts leading to degraded generalization, or (4) failure to converge during the optimization phase.

**Three first experiments to run**:
1. Test routing optimization on a simple classification task with known expert specializations to verify the mechanism works as intended
2. Compare performance with and without routing optimization on a held-out validation set to measure adaptation benefits
3. Measure computational overhead by comparing FLOPs with baseline inference to verify efficiency claims

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Focus on English-language benchmarks limits generalizability to multilingual scenarios
- Relatively narrow task diversity (primarily code generation and reasoning tasks)
- Performance on extremely long sequences or cross-lingual scenarios remains unexplored
- Limited testing beyond 16-expert models prevents understanding of scaling behavior

## Confidence

**Performance improvements**: High confidence - Extensive ablation studies and consistent gains across multiple models and benchmarks support this claim.

**Computational efficiency**: High confidence - Clear comparisons with few-shot methods and FLOPs measurements provide strong evidence.

**Universal applicability**: Medium confidence - Experiments focus on specific models (OLMoE, Qwen1.5-MoE, DeepSeek-V2-Lite) and tasks, limiting generalizability claims.

## Next Checks

1. Test the method's performance on multilingual and code-mixed datasets to assess cross-domain robustness
2. Evaluate scaling behavior with increasing expert count (beyond the 16-expert models tested) to understand architectural limits
3. Compare performance against full fine-tuning baselines on the same hardware constraints to better contextualize the efficiency gains