---
ver: rpa2
title: Large Speech Model Enabled Semantic Communication
arxiv_id: '2512.04711'
source_url: https://arxiv.org/abs/2512.04711
tags:
- speech
- semantic
- loss
- communication
- packet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LargeSC, a large speech model enabled semantic
  communication system that addresses the challenge of efficient speech transmission
  over lossy channels. The system uses Mimi as a speech codec to convert speech into
  discrete tokens and employs an adaptive controller module for semantic-aware compression
  and in-band unequal error protection based on speech content and packet loss probability.
---

# Large Speech Model Enabled Semantic Communication
## Quick Facts
- arXiv ID: 2512.04711
- Source URL: https://arxiv.org/abs/2512.04711
- Reference count: 40
- Key outcome: LargeSC enables efficient semantic speech transmission over lossy channels using Mimi codec, adaptive compression, and LoRA-fine-tuned Moshi for packet loss concealment.

## Executive Summary
This paper introduces LargeSC, a semantic communication system that leverages large speech models to achieve robust speech transmission over lossy channels. The system converts speech to discrete tokens using the Mimi codec and employs an adaptive controller for semantic-aware compression and in-band unequal error protection. A LoRA-fine-tuned Moshi foundation model autoregressively recovers lost tokens at the receiver. LargeSC supports low bandwidths (550 bps to 2.06 kbps) and outperforms conventional baselines under high packet loss, though with an end-to-end latency of approximately 460 ms.

## Method Summary
LargeSC uses the Mimi codec to transform speech into discrete tokens, which are then compressed and protected based on content importance and channel conditions via an adaptive controller. The system transmits these tokens over lossy channels with in-band unequal error protection. At the receiver, a Moshi foundation model fine-tuned with LoRA reconstructs lost tokens autoregressively. The approach enables low-bandwidth operation (550-2.06 kbps) and robust speech reconstruction even under high packet loss rates, with latency around 460 ms.

## Key Results
- Supports bandwidths from 550 bps to 2.06 kbps
- Outperforms conventional baselines in speech quality under high packet loss rates
- Achieves end-to-end latency of approximately 460 ms

## Why This Works (Mechanism)
LargeSC leverages the semantic understanding of large speech models to prioritize and protect important speech information during transmission. The adaptive controller dynamically adjusts compression and error protection based on both speech content and packet loss probability, ensuring that critical information is preserved. The use of Mimi as a discrete token codec enables efficient representation and reconstruction, while LoRA fine-tuning allows the Moshi model to specialize in packet loss concealment without retraining the full 7B parameters.

## Foundational Learning
- **Discrete token representation**: Speech is converted to discrete tokens for efficient compression and transmission; needed for semantic modeling and low-bandwidth operation.
- **Adaptive semantic-aware compression**: Compression and error protection are dynamically adjusted based on content and channel conditions; needed to maximize quality under bandwidth and loss constraints.
- **In-band unequal error protection**: Critical tokens are protected more than less important ones within the same transmission; needed to optimize robustness without increasing bandwidth.
- **Autoregressive token recovery**: Lost tokens are reconstructed sequentially using a fine-tuned large speech model; needed to handle packet loss gracefully and maintain speech continuity.
- **LoRA fine-tuning**: A low-rank adaptation technique to specialize large models for packet loss concealment; needed to avoid full retraining while enabling task-specific performance.
- **VisQOL and PLCMOS metrics**: Objective and perceptual speech quality measures used to evaluate reconstruction; needed to quantify performance under lossy conditions.

## Architecture Onboarding
- **Component map**: Speech -> Mimi codec -> Adaptive controller -> Transmission (with unequal error protection) -> Receiver -> LoRA-fine-tuned Moshi (autoregressive recovery)
- **Critical path**: Speech input → Mimi encoding → Adaptive controller → Transmission → Packet loss concealment (Moshi) → Output
- **Design tradeoffs**: Bandwidth vs. quality vs. latency; adaptive protection vs. fixed schemes; full model vs. LoRA fine-tuning for efficiency
- **Failure signatures**: High packet loss without adaptive protection leads to poor reconstruction; insufficient LoRA fine-tuning degrades token recovery; latency above 200 ms impacts interactivity
- **First experiments**:
  1. Measure token recovery accuracy (WER) under varying packet loss rates with and without LoRA fine-tuning
  2. Evaluate VisQOL and PLCMOS for different bandwidth settings (550-2.06 kbps)
  3. Test adaptive controller response under rapidly changing packet loss probabilities

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can architectural optimizations, such as asynchronous processing or speculative decoding, significantly reduce the 460 ms end-to-end latency to levels suitable for high-interactivity dialogue (e.g., < 200 ms) without compromising reconstruction quality?
- **Basis in paper:** [Explicit] The Discussion section identifies the 460 ms latency as a limitation for real-time scenarios and explicitly proposes "architectural optimizations, including asynchronous processing or hybrid strategies" as necessary future work.
- **Why unresolved:** The current implementation processes tokens sequentially via autoregressive generation, creating a bottleneck that the authors acknowledge but do not solve in this work.
- **What evidence would resolve it:** A modified system implementation demonstrating sub-200 ms latency while maintaining comparable VisQOL and PLCMOS scores under identical packet loss conditions.

### Open Question 2
- **Question:** To what extent can model compression techniques (quantization, pruning, or distillation) reduce the 7B parameter model's computational overhead while preserving the token prediction accuracy required for effective packet loss concealment?
- **Basis in paper:** [Explicit] Section V.B highlights the "significant computational demands" of the 7B parameter model and identifies "quantization, distillation, [and] pruning" as expected advancements needed for practical on-device deployment.
- **Why unresolved:** The current system relies on the full parameter count for semantic fidelity; the trade-off between model compression ratio and the degradation of semantic recovery has not been quantified.
- **What evidence would resolve it:** A study evaluating the performance drop (WER, VisQOL) of compressed variants of the Moshi model compared to the baseline under high packet loss rates.

### Open Question 3
- **Question:** Would training the adaptive controller and packet loss concealment modules specifically on bursty loss patterns (e.g., Gilbert-Elliott models) yield better robustness than the current training regime of random uniform packet loss?
- **Basis in paper:** [Inferred] The paper notes that the training phase uses a "random uniform packet loss model" for efficiency, while the testing phase evaluates performance on both uniform and bursty (Gilbert-Elliott) channels. This mismatch suggests the model may not be optimally tuned for the bursty interference common in real-world networks.
- **Why unresolved:** The authors demonstrate robustness on bursty channels but do not isolate whether the training data distribution (uniform loss) limits performance ceiling in bursty scenarios.
- **What evidence would resolve it:** Comparative experiments showing the LargeSC performance when trained on bursty loss distributions versus the uniform loss baseline, specifically tested on Gilbert-Elliott channels.

## Limitations
- Reliance on large 7B parameter model creates computational overhead and limits on-device deployment
- 460 ms end-to-end latency may exceed acceptable thresholds for highly interactive dialogue
- Evaluation focuses on controlled conditions; generalization to diverse speech styles and noise is uncertain

## Confidence
- **High confidence**: Bandwidth range and relative performance against conventional baselines under high packet loss
- **Medium confidence**: Real-time deployment capability due to unquantified receiver-side computational demands
- **Medium confidence**: Generalization across diverse speech conditions and accents

## Next Checks
1. Measure CPU/GPU utilization and processing time at the receiver for LoRA-based token recovery across hardware configurations to confirm real-time feasibility
2. Evaluate system performance with non-native speakers, different accents, and varying noise conditions to assess generalization
3. Test the adaptive controller's response time and effectiveness under rapidly fluctuating packet loss rates (e.g., bursty losses) to validate robustness in unstable network conditions