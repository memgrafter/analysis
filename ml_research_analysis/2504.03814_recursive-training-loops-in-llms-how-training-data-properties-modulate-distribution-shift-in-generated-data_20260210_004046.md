---
ver: rpa2
title: 'Recursive Training Loops in LLMs: How training data properties modulate distribution
  shift in generated data?'
arxiv_id: '2504.03814'
source_url: https://arxiv.org/abs/2504.03814
tags:
- data
- quality
- diversity
- distribution
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how properties of human training data affect
  distribution shifts in large language models during recursive fine-tuning. The authors
  first confirm that different human datasets lead to varying magnitudes of distribution
  shifts.
---

# Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?

## Quick Facts
- **arXiv ID**: 2504.03814
- **Source URL**: https://arxiv.org/abs/2504.03814
- **Reference count**: 40
- **Primary result**: Different human datasets lead to varying magnitudes of distribution shifts during recursive fine-tuning

## Executive Summary
This paper investigates how properties of human training data affect distribution shifts in large language models during recursive fine-tuning. The authors confirm that different human datasets produce varying magnitudes of distribution shifts and conduct extensive experiments across 800 clusters from four datasets. They identify key properties that influence these shifts, finding that lexical diversity amplifies distribution shifts while semantic diversity and data quality mitigate them. The effects are highly modular, with each domain experiencing independent shifts, and political bias experiments reveal that shift direction depends on the initial political lean of the human data.

## Method Summary
The authors conducted extensive experiments across 800 clusters from four datasets (Reddit, Wikipedia, ArXiv, and OpenWebText) to study distribution shifts during recursive fine-tuning. They systematically varied properties of human training data and measured resulting shifts in model behavior. The experiments included analysis of lexical diversity, semantic diversity, and data quality metrics. Political bias experiments were conducted using right-leaning and left-leaning human data to examine how initial political orientation affects shift direction during recursive training.

## Key Results
- Different human datasets lead to varying magnitudes of distribution shifts during recursive fine-tuning
- Lexical diversity amplifies distribution shifts while semantic diversity and data quality mitigate them
- Domain-specific effects are highly modular, with each domain (Reddit, Wikipedia, etc.) experiencing independent shift dynamics
- Political bias shift direction (amplification, reduction, or inversion) depends on the initial political lean of the human data

## Why This Works (Mechanism)
Recursive fine-tuning creates a feedback loop where models generate data that becomes part of their training set. When this generated data has different statistical properties than the original human data, distribution shifts occur. The mechanism involves the model's learned representations being reinforced or altered based on the properties of training data. Properties like lexical diversity create more varied input patterns that can push the model's probability distributions in different directions during fine-tuning, while semantic diversity provides broader context that helps maintain stability. Data quality acts as a regularization factor by providing cleaner signal for the model to learn from.

## Foundational Learning

**Distribution Shift** - Changes in input data distribution that can cause model performance degradation. Why needed: Core phenomenon being studied. Quick check: Compare training vs. test data statistics.

**Recursive Fine-tuning** - Repeatedly training a model on its own outputs. Why needed: The training paradigm creating the feedback loop. Quick check: Track performance degradation over recursion steps.

**Lexical Diversity** - Variety of vocabulary used in text. Why needed: Identified as a key amplifier of distribution shifts. Quick check: Measure type-token ratio or entropy of word usage.

**Semantic Diversity** - Range of meanings and topics covered in text. Why needed: Found to mitigate distribution shifts. Quick check: Use topic modeling or semantic similarity metrics.

**Data Quality** - Measure of training data cleanliness and relevance. Why needed: Acts as a mitigating factor for distribution shifts. Quick check: Filter for noise or irrelevant content.

## Architecture Onboarding

**Component Map**: Human Data → Model Training → Generated Data → Recursive Fine-tuning → Distribution Shift → Model Output

**Critical Path**: Human training data properties → Model behavior during recursive fine-tuning → Distribution shift magnitude → Final model capabilities

**Design Tradeoffs**: High lexical diversity vs. stability (amplifies shifts but may capture more nuance), semantic diversity vs. coherence (mitigates shifts but may reduce focused learning), data quality vs. quantity (cleaner data reduces shifts but may limit coverage)

**Failure Signatures**: Amplification of biases present in training data, loss of coherence in generated text, domain-specific degradation in model performance, unexpected political leaning in outputs

**First Experiments**:
1. Measure lexical diversity metrics across different datasets before training
2. Track semantic drift using topic modeling during recursive fine-tuning
3. Compare model outputs before and after recursive fine-tuning for bias amplification

## Open Questions the Paper Calls Out
None

## Limitations
- Modular domain effects may not generalize beyond the four datasets examined (Reddit, Wikipedia, ArXiv, OpenWebText)
- Political bias experiments use only right-leaning and left-leaning human data, potentially oversimplifying political orientations
- Focus on lexical and semantic diversity may miss other influential factors like syntactic patterns or topic-specific linguistic features

## Confidence
**High**: Core finding that human training data properties influence distribution shift magnitude during recursive fine-tuning
**Medium**: Specific directional effects (lexical diversity amplifying vs. semantic diversity mitigating shifts)
**Low**: Generalizability of political bias findings beyond the binary left/right framework

## Next Checks
1. Replicate the modular domain effects using datasets from additional internet domains (Stack Exchange, academic forums, social media platforms) to test whether independent shift dynamics hold across diverse data sources

2. Conduct ablation studies removing lexical and semantic diversity metrics to identify whether other data properties (syntactic complexity, topical coherence, or temporal patterns) contribute significantly to distribution shift patterns

3. Extend the political bias experiments to include multi-dimensional political orientations and non-Western political perspectives to validate whether amplification/inversion patterns generalize beyond the binary framework used in the current study