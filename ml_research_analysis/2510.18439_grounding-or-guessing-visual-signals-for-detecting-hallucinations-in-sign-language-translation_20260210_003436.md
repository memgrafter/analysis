---
ver: rpa2
title: Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign
  Language Translation
arxiv_id: '2510.18439'
source_url: https://arxiv.org/abs/2510.18439
tags:
- visual
- language
- reliability
- video
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses hallucination detection in sign language translation
  (SLT), where models may generate fluent text unsupported by visual evidence. Unlike
  other multimodal tasks, in SLT the video itself is the source language, making hallucinations
  equivalent to translation errors.
---

# Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation

## Quick Facts
- arXiv ID: 2510.18439
- Source URL: https://arxiv.org/abs/2510.18439
- Reference count: 40
- Primary result: A token-level reliability measure detects hallucinations in sign language translation by quantifying decoder reliance on visual input, achieving up to 97% accuracy and 0.72 Spearman correlation.

## Executive Summary
This work addresses hallucination detection in sign language translation (SLT), where models may generate fluent text unsupported by visual evidence. Unlike other multimodal tasks, in SLT the video itself is the source language, making hallucinations equivalent to translation errors. To detect these, the authors propose a token-level reliability measure that quantifies how much the decoder relies on visual input versus language priors. The method combines feature-based sensitivity (internal state changes when video is masked) with counterfactual signals (probability differences between clean and altered video inputs), aggregated into a sentence-level reliability score. Evaluated on PHOENIX-2014T and CSL-Daily datasets with both gloss-based and gloss-free models, reliability predicts hallucination rates with high accuracy (up to 97%) and strong correlation (Spearman ρ up to 0.72). It generalizes across datasets and architectures, decreases under visual degradations, and distinguishes grounded from guessed tokens. When combined with text-only signals, it further improves detection. Qualitative analysis reveals gloss-free models hallucinate more due to underuse of visual input, defaulting to language priors. Reliability thus provides a practical, reusable tool for diagnosing hallucinations in SLT and lays groundwork for multimodal hallucination detection.

## Method Summary
The authors introduce a token-level reliability measure to detect hallucinations in sign language translation by quantifying the decoder's dependence on visual input. The method combines feature-based sensitivity (measuring internal state changes when video is masked) with counterfactual signals (probability differences between clean and altered video inputs). These are aggregated into a sentence-level reliability score. The approach is evaluated on PHOENIX-2014T and CSL-Daily datasets using both gloss-based and gloss-free models. Reliability correlates strongly with hallucination rates (up to 97% accuracy, 0.72 Spearman ρ) and generalizes across architectures. It decreases under visual degradations and helps distinguish grounded from guessed tokens. Combining reliability with text-only signals further improves detection, revealing that gloss-free models hallucinate more due to underuse of visual input.

## Key Results
- Reliability measure achieves up to 97% accuracy in detecting hallucination rates in sign language translation.
- Strong correlation (Spearman ρ up to 0.72) between reliability scores and actual hallucination rates.
- Gloss-free models hallucinate more than gloss-based models due to underuse of visual input, defaulting to language priors.

## Why This Works (Mechanism)
The reliability measure works by quantifying how much the decoder relies on visual input versus language priors. By combining feature-based sensitivity (internal state changes when video is masked) with counterfactual signals (probability differences between clean and altered video inputs), it captures the extent to which the model grounds its translations in the visual evidence. This approach is effective because it directly measures the model's dependence on the video source, distinguishing between tokens that are grounded in the visual input and those that are guessed based on language priors.

## Foundational Learning
- **Sign Language Translation (SLT):** Converting sign language videos into spoken language text. *Why needed:* SLT is the core task being analyzed for hallucination detection.
- **Gloss-based vs. Gloss-free Models:** Gloss-based models use linguistic annotations (glosses) as intermediate representations, while gloss-free models translate directly from video to text. *Why needed:* Comparing these models reveals differences in hallucination tendencies.
- **Feature-based Sensitivity:** Measures how internal model states change when input is altered (e.g., video masked). *Why needed:* Helps quantify reliance on visual input.
- **Counterfactual Signals:** Compare model outputs under different input conditions to assess input importance. *Why needed:* Complements feature-based sensitivity by capturing probability differences.
- **Token-level Reliability:** Aggregates per-token measures into a sentence-level score to predict hallucination rates. *Why needed:* Provides a practical, interpretable metric for hallucination detection.

## Architecture Onboarding

**Component Map:**
Video Encoder -> Decoder -> Reliability Measure (Feature-based Sensitivity + Counterfactual Signals) -> Sentence-level Reliability Score

**Critical Path:**
1. Video encoder processes sign language videos into feature representations.
2. Decoder generates text tokens, conditioned on video features.
3. Reliability measure evaluates token-level reliance on visual input via sensitivity and counterfactual signals.
4. Sentence-level reliability score aggregates token-level scores to predict hallucination rates.

**Design Tradeoffs:**
- **Feature-based Sensitivity vs. Counterfactual Signals:** Sensitivity captures internal state changes, while counterfactuals measure output differences. Combining both provides a more robust reliability estimate but increases computational cost.
- **Gloss-based vs. Gloss-free Models:** Gloss-based models may be more interpretable and grounded, but gloss-free models are more flexible and scalable. The study shows gloss-free models hallucinate more due to underuse of visual input.

**Failure Signatures:**
- Low reliability scores may indicate hallucinations, but could also reflect ambiguous or incomplete visual cues.
- High reliability scores suggest grounding in visual input, but may not capture subtle or context-dependent hallucinations.

**3 First Experiments:**
1. Apply reliability measure to a new sign language dataset to assess generalizability.
2. Compare reliability scores across different model architectures (e.g., transformer-based vs. CNN-based).
3. Test reliability under varying levels of visual degradation to evaluate robustness.

## Open Questions the Paper Calls Out
- Whether the proposed token-level reliability measure captures all types of hallucinations in sign language translation, especially for complex or domain-specific content where visual cues may be ambiguous or incomplete.
- The extent to which the reliability measure generalizes to real-world sign language usage, given the diversity of signing styles and contexts.

## Limitations
- The reliability measure may not fully capture hallucinations in cases where models rely on subtle visual features or context that are not easily manipulated or masked.
- Evaluation datasets may not fully represent the diversity of real-world sign language usage, potentially limiting generalizability.
- The study focuses on specific architectures and datasets, so claims about universal applicability across all SLT models are not fully supported.

## Confidence
- **High:** Effectiveness of the reliability measure in detecting hallucinations, given strong empirical results and correlation metrics.
- **Medium:** Universal applicability of the measure across all SLT models and datasets, due to focus on specific architectures and datasets.
- **Medium:** Claim that gloss-free models inherently hallucinate more due to underuse of visual input, as this conclusion is based on observed patterns in tested models.

## Next Checks
1. Test the reliability measure on a broader range of sign language datasets and models to assess generalizability and robustness across different contexts and architectures.
2. Conduct a detailed error analysis to understand the types of hallucinations that the reliability measure might miss, particularly in cases where visual cues are subtle or ambiguous.
3. Explore the integration of additional multimodal signals, such as audio or contextual information, to enhance the detection of hallucinations and improve the overall reliability of the measure.