---
ver: rpa2
title: 'T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation
  Models'
arxiv_id: '2505.04946'
source_url: https://arxiv.org/abs/2505.04946
tags:
- text
- prompt
- uni00000013
- uni00000011
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T2VTextBench is a human evaluation benchmark for on-screen text
  fidelity in text-to-video generation models. The benchmark evaluates ten state-of-the-art
  models across 73 prompts covering stepwise visualization, app/web UI simulation,
  everyday digital moments, cinematic scenes, math-related, and multilingual (Chinese)
  scenarios.
---

# T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models

## Quick Facts
- arXiv ID: 2505.04946
- Source URL: https://arxiv.org/abs/2505.04946
- Reference count: 30
- Key outcome: Human evaluation benchmark revealing all top video generation models struggle with on-screen text rendering, with highest score of 0.37/1.0

## Executive Summary
T2VTextBench is a comprehensive human evaluation benchmark designed to assess the ability of text-to-video generation models to accurately render on-screen text. The benchmark evaluates ten state-of-the-art models across 73 diverse prompts spanning multiple real-world scenarios including UI simulation, digital moments, and multilingual content. Human annotators scored model outputs on text accuracy and temporal consistency using a 0-1 scale. The results reveal a significant gap in current models' ability to generate precise, temporally consistent text, with even the best-performing model (Sora) achieving only 0.37 average score. The study highlights that models struggle particularly with geometric transformations and random character sequences, suggesting fundamental limitations in text understanding rather than mere memorization.

## Method Summary
The benchmark employs human annotators to evaluate video outputs from ten leading text-to-video models. Each model is tested on 73 carefully curated prompts organized into six categories: stepwise visualization, app/web UI simulation, everyday digital moments, cinematic scenes, math-related content, and Chinese multilingual scenarios. Annotators score each output on a 0-1 scale for text accuracy (how well the rendered text matches the prompt) and temporal consistency (whether the text remains stable throughout the video). The evaluation focuses on scenarios where precise text rendering is critical, such as UI displays, notifications, and on-screen text in digital environments. The study also analyzes cost-effectiveness by comparing performance against API pricing for each model.

## Key Results
- All evaluated models achieved low text fidelity scores, with the highest average being 0.37/1.0 (Sora)
- Models performed worst on geometric transformations and random character sequences
- Wan 2.1 achieved second-best performance while being freely available, demonstrating superior cost-effectiveness
- Significant performance gaps across all prompt categories indicate fundamental limitations in text understanding
- Temporal consistency was as problematic as text accuracy, suggesting models struggle with maintaining text stability over time

## Why This Works (Mechanism)
Assumption: The benchmark reveals fundamental architectural limitations in current video generation models. The consistent failure patterns across all models suggest that text rendering capabilities are not a priority in current architectures, which focus primarily on visual coherence and motion generation rather than precise text representation.

## Foundational Learning
Unknown: The paper does not explicitly discuss foundational learning approaches for text rendering in video generation. However, the consistent poor performance across all models suggests that current approaches to text understanding and rendering in video generation may need to be fundamentally rethought rather than incrementally improved.

## Architecture Onboarding
Component Map: Text prompt -> Video generation model -> Text rendering module -> Output video

Critical Path: The study identifies that current video generation models process text prompts through their existing architectures without specialized text rendering capabilities, leading to suboptimal text generation.

Design Tradeoffs: Models prioritize visual coherence over precise text rendering, as evidenced by their ability to generate visually appealing content while failing at accurate text representation.

Failure Signatures: Models show consistent failure patterns with geometric text transformations, random character sequences, and maintaining temporal consistency of text elements.

First Experiments:
1. Test each model's ability to render simple geometric text transformations (rotation, scaling)
2. Evaluate performance on random character sequences versus meaningful words
3. Assess temporal consistency by comparing text stability across video frames

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but the results raise important questions about the fundamental limitations of current video generation architectures and whether text rendering capabilities can be effectively integrated without compromising visual quality.

## Limitations
- Human annotation introduces potential scoring variability and may not capture nuanced quality differences
- Evaluation focused primarily on English and Chinese text, limiting generalizability to other languages
- Benchmark may not fully represent all real-world text rendering scenarios and use cases
- Methodology for temporal consistency evaluation could be more precisely defined

## Confidence
- High Confidence: Consistent low scores across all models confirm fundamental text rendering challenges
- Medium Confidence: Analysis of word-level memorization requires additional controlled experiments for definitive proof
- Low Confidence: Benchmark's representation of real-world scenarios and cross-linguistic applicability is uncertain

## Next Checks
1. Conduct cross-validation with diverse annotators to assess scoring robustness and identify cultural/linguistic biases
2. Implement controlled experiments to distinguish word-level memorization from genuine text understanding
3. Expand benchmark to include wider language range and test localization capabilities including complex scripts