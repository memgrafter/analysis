---
ver: rpa2
title: Estimating problem difficulty without ground truth using Large Language Model
  comparisons
arxiv_id: '2512.14220'
source_url: https://arxiv.org/abs/2512.14220
tags:
- difficulty
- compare
- human
- problem
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLM compare introduces a new method for estimating problem difficulty\
  \ without ground truth by using pairwise comparisons made by a large language model,\
  \ then computing Bradley-Terry scores from the comparison outcomes. The method is\
  \ validated against human annotations, human and LLM performance scores, and LLM\
  \ labels across three datasets, showing strong alignment (Pearson r \u2265 0.80)\
  \ and robustness to hallucinations with less than 6% degradation even with 10% noise\
  \ injection."
---

# Estimating problem difficulty without ground truth using Large Language Model comparisons

## Quick Facts
- **arXiv ID:** 2512.14220
- **Source URL:** https://arxiv.org/abs/2512.14220
- **Authors:** Marthe Ballon; Andres Algaba; Brecht Verbeken; Vincent Ginis
- **Reference count:** 40
- **Primary result:** LLM compare estimates problem difficulty without ground truth using pairwise LLM comparisons and Bradley-Terry scoring, showing strong alignment (Pearson r ≥ 0.80) with human annotations and robustness to hallucinations (<6% degradation with 10% noise).

## Executive Summary
LLM compare introduces a novel method for estimating problem difficulty without ground truth by leveraging pairwise comparisons from large language models and Bradley-Terry ranking aggregation. The method demonstrates strong alignment with human annotations and performance-based difficulty measures across three datasets, with Pearson correlations above 0.80. It is particularly valuable for scoring synthetic out-of-distribution problems where traditional difficulty measures fail, and shows robustness to LLM hallucinations through redundancy in comparisons.

## Method Summary
LLM compare uses large language models to perform pairwise difficulty comparisons between problems, then applies the Bradley-Terry model via Iterative Spectral Luce Ranking to compute continuous difficulty scores from these comparisons. The method is model-agnostic and continuous, making it suitable for scoring problems without ground truth. Validation is performed against human annotations, human and LLM performance scores, and LLM-generated labels, demonstrating strong alignment across multiple metrics.

## Key Results
- Strong alignment with human annotations (Pearson r ≥ 0.80) across three datasets
- Robustness to hallucinations with less than 6% degradation even with 10% noise injection
- Model-agnostic approach validated with high agreement between OpenAI and Gemini models
- Capable of identifying difficulty tiers within datasets for curriculum design and model evaluation

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Comparison Reduces Cognitive Load on LLMs
Relative difficulty judgments are more reliable than absolute difficulty assignments because they bypass calibration requirements. Instead of asking "How difficult is this problem on a 1-10 scale?", the method asks "Which of these two problems is more difficult?"—leveraging LLMs' demonstrated strength in preference tasks.

### Mechanism 2: Bradley-Terry Model Converts Sparse Pairwise Data to Global Continuous Scores
The Bradley-Terry model provides statistically principled aggregation from incomplete pairwise comparison data to continuous difficulty scores. Each comparison is modeled as a probabilistic outcome where P(i beats j) = exp(βᵢ) / (exp(βᵢ) + exp(βⱼ)), with parameters estimated via Iterative Spectral Luce Ranking.

### Mechanism 3: Redundancy Through Multiple Comparisons Provides Noise Resilience
Aggregating many comparisons per problem dilutes the impact of individual hallucinations or errors. With each problem participating in dozens of matches (36-66 in experiments), erroneous comparisons affect only one data point in the aggregation, with the BT model's MLE naturally down-weighting outliers.

## Foundational Learning

**Concept: Bradley-Terry Model and Paired Comparison Scaling**
- Why needed here: This is the mathematical foundation that converts raw comparison outcomes into interpretable difficulty scores.
- Quick check question: If problem A has BT score 2.0 and problem B has BT score 0.0, what is the predicted probability that A is judged more difficult than B in a pairwise comparison? (Answer: e²/(e² + e⁰) ≈ 0.88)

**Concept: Iterative Spectral Luce Ranking (ISLR)**
- Why needed here: This is the specific algorithm used to fit BT parameters efficiently on large, sparse datasets.
- Quick check question: Why does the paper use ISLR with α=0.01 instead of standard maximum likelihood estimation? (Answer: ISLR handles sparse comparison matrices and provides computational efficiency for large n)

**Concept: Classical Test Theory and Item Response Theory**
- Why needed here: These traditional frameworks define what "difficulty" means in psychometrics.
- Quick check question: Why can't IRT be applied to score problems that no human or LLM can solve? (Answer: IRT requires observed performance data to estimate item parameters; without any solvers, there's no performance data)

## Architecture Onboarding

**Component map:**
Problem corpus -> Match scheduler -> LLM comparison engine -> Outcome matrix -> BT scorer (ISLR) -> Difficulty ranking

**Critical path:**
1. Run convergence analysis with smaller/cheaper model to determine matches-per-problem needed
2. Generate match pairs according to budget constraints
3. Execute comparisons via batch API
4. Fit BT model using ISLR with α=0.01 regularization
5. Validate: check model agreement, correlation with any available labels

**Design tradeoffs:**
- **Matches per problem (M) vs. cost:** Paper found M=36-66 sufficient; more matches improve stability but cost scales linearly. Use smaller model to determine convergence point before committing to expensive model.
- **Model selection:** High agreement between o3 and Gemini 2.5 Pro suggests results are not model-specific, but this should be validated on your domain.
- **Full round-robin vs. sampling:** For n<50, compare all pairs. For larger datasets, random sampling suffices—paper managed n=1876 with only ~36 matches/problem.

**Failure signatures:**
- **Low inter-model correlation** (Kendall τ < 0.7 between different LLMs): indicates difficulty judgments are model-specific
- **BT scores with extreme clustering** (most problems near zero, few outliers): may indicate insufficient matches or highly inconsistent judgments
- **Strong position bias** (systematic preference for "Problem a" vs "Problem b"): should be detected and can be mitigated by randomizing pair order
- **Poor correlation with human labels** (where available): if Pearson r < 0.5, the method may not capture meaningful difficulty on your domain

**First 3 experiments:**
1. **Convergence analysis:** On a subset of your dataset, run comparisons with a cheaper model at increasing match counts (M=10, 20, 36, 66, 100) and compute correlation between successive runs. Identify M where Kendall τ stabilizes above 0.90.
2. **Cross-model validation:** Run the full comparison pipeline with two different LLMs on the same 50-100 problems. Generate QQ plot and compute Pearson correlation. High agreement (>0.85) validates model-agnosticism for your domain.
3. **Noise injection robustness test:** After obtaining baseline BT scores, artificially flip 1%, 5%, and 10% of comparison outcomes and re-fit. Measure correlation degradation. This establishes your error budget for production use.

## Open Questions the Paper Calls Out
None

## Limitations
- **Model-specificity of difficulty judgments:** The assumption of model-agnostic difficulty judgments may not hold for creative writing, subjective reasoning tasks, or domains where different LLMs have systematically different capabilities or biases.
- **Ground truth alignment:** Validation relies on imperfect proxy measures (human annotations, performance scores, LLM-generated labels) that may not consistently capture the intended difficulty construct.
- **Scalability to diverse problem types:** The Bradley-Terry model assumptions may break down for problems that are categorically different or have multiple difficulty dimensions that don't combine linearly.

## Confidence
- **High confidence:** Bradley-Terry aggregation mechanism and noise robustness properties
- **Medium confidence:** Model-agnostic claim and general applicability across domains
- **Medium confidence:** Practical utility for synthetic out-of-distribution problems

## Next Checks
1. **Cross-domain model agreement test:** Run LLM compare on problems from three diverse domains using two different LLM providers. Compute cross-model correlation matrices and analyze whether agreement patterns differ by domain.
2. **Transitivity violation detection:** For a subset of problems, collect all three-way comparisons and compute the fraction showing intransitivity violations. If >15% of triplets violate transitivity, this suggests Bradley-Terry assumptions may be inadequate.
3. **Out-of-distribution validation:** Apply LLM compare to problems truly out-of-distribution relative to training data. Compare resulting difficulty rankings against any available expert judgments or assess logical coherence.