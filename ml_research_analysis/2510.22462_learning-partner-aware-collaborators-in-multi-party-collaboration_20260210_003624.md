---
ver: rpa2
title: Learning "Partner-Aware" Collaborators in Multi-Party Collaboration
arxiv_id: '2510.22462'
source_url: https://arxiv.org/abs/2510.22462
tags:
- intervention
- collaborator
- agent
- task
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ICR, a method for training LLM collaborator
  agents that can intelligently respond to interventions in multi-party collaborative
  tasks. The key idea is to use counterfactual invariance regularization to make agents
  robust to potentially misleading interventions while still incorporating helpful
  ones.
---

# Learning "Partner-Aware" Collaborators in Multi-Party Collaboration

## Quick Facts
- arXiv ID: 2510.22462
- Source URL: https://arxiv.org/abs/2510.22462
- Reference count: 40
- Primary result: ICR-trained agents outperform strong baselines in task accuracy and common ground convergence on two collaborative tasks

## Executive Summary
This paper introduces ICR (Intentional Counterfactual Regularization), a method for training LLM collaborator agents that can intelligently respond to interventions in multi-party collaborative tasks. The key innovation is using counterfactual invariance regularization to make agents robust to potentially misleading interventions while still incorporating helpful ones. Experiments on two collaborative tasks show ICR-trained agents achieve significantly higher task accuracy and common ground convergence compared to strong baselines, demonstrating improved ability to distinguish between helpful and misleading interventions.

## Method Summary
ICR trains LLM collaborators through a two-phase process: (1) expert data collection using GPT-4o to roleplay both intervention and collaborator agents in 15-turn dialogues, and (2) PPO training with a novel counterfactual invariance objective. The objective combines task utility with KL divergence to a reference policy and a counterfactual KL term that forces the agent to maintain consistent reasoning regardless of intervention influence. The counterfactual policy is computed by evaluating the same token sequence under a prompt declaring the intervention non-informative.

## Key Results
- ICR achieves highest common ground scores (3.35 DeliData, 10.87 WTD no-press) despite training only on proxy task rewards
- ICR outperforms baselines on task accuracy across both language-rich and language-free settings
- The method shows consistent gains across different intervention agent qualities and prompt variants

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Invariance Regularization
Adding a KL divergence term between factual and counterfactual policies teaches agents to evaluate intervention quality causally rather than superficially. During training, the agent samples responses under normal conditions, then evaluates those same tokens under a counterfactual prompt that declares the intervention non-informative. Minimizing this divergence forces the agent to maintain consistent reasoning regardless of intervention influence, learning which intervention aspects genuinely affect task outcomes.

### Mechanism 2: MAMDP Formulation Exposes Preference Optimization Suboptimality
Standard RLHF/DPO-trained collaborators are provably suboptimal for collaborative settings because they treat interventions as static state features without causal interpretation. The Modified-Action MDP explicitly models interventions as actions from a strategic agent. Theorem 3.2 proves Bellman-optimal policies in the underlying MDP are generally suboptimal in MAMDP because they optimize surface-level alignment without evaluating intervention causal impact on task outcomes.

### Mechanism 3: Emergent Common Ground Without Explicit CG Rewards
Counterfactually-trained agents naturally achieve higher common ground convergence as a byproduct of task utility optimization. By learning to discriminate intervention quality through counterfactual regularization, agents develop "partner-aware" behavior. When interventions are helpful, agents integrate them; when misleading, they resist. This selective integration drives belief alignment without requiring explicit consensus rewards during training.

## Foundational Learning

- **KL Divergence in Policy Optimization**: Understanding how DKL(π || π_ref) constrains policy updates is essential for grasping why adding a counterfactual KL term works. Quick check: Can you explain why minimizing KL divergence to a reference policy prevents catastrophic forgetting while still allowing policy improvement?

- **Counterfactual Reasoning in Causal Inference**: The method relies on constructing counterfactual states that neutralize intervention influence while preserving task information, drawing on Pearl's do-calculus. Quick check: How would you construct a counterfactual intervention that removes the influence pathway of a suggestion without changing the task-relevant context?

- **Multi-Agent MDP vs Standard MDP**: Understanding why standard MDP formulations fail for collaborative settings—specifically, how treating another agent's actions as environment transitions ignores strategic reasoning requirements. Quick check: Why does Bellman optimality in a standard MDP not guarantee optimal behavior when interacting with a strategic intervention agent?

## Architecture Onboarding

- **Component map**: Expert Data Collection -> BC Reference Training -> ICR PPO Training -> Evaluation with Fixed Intervention Agent
- **Critical path**: Bootstrap dialogues from DeliData or construct task descriptions -> Collect expert trajectories using personality-sampled prompts -> Train BC reference model on collaborator utterances -> Initialize ICR from BC, train with PPO + counterfactual KL (λIntent ≈ 0.2) -> Evaluate on 100 dialogues × 15 turns with fixed intervention agent
- **Design tradeoffs**: Decentralized vs centralized training (uses decentralized due to scalability), proxy vs gold rewards during training (only task-specific proxy), full-press vs no-press variants, λIntent magnitude tradeoffs
- **Failure signatures**: Reward hacking if consensus signals included in training rewards, counterfactual regularization too strong causing policy to ignore all interventions, PPO training instability with LLMs, negation insensitivity in counterfactual prompts
- **First 3 experiments**: 1) Ablate λIntent: Train ICR with λIntent ∈ {0.01, 0.2, 1.0} on no-press DeliData; track proxy reward per batch over 8k steps, 2) Counterfactual prompt robustness: Swap default counterfactual prefix with alternatives during evaluation; measure response log-prob variance, 3) Intervention agent swap: Replace GPT-4o intervention agent with Llama-3-8B-Instruct and evaluate ICR performance

## Open Questions the Paper Calls Out

1. How does ICR perform in strategic games requiring deception detection, such as Diplomacy, where agents must navigate both cooperative and adversarial interactions? (Basis: "how would ICR perform in more challenging domains like Diplomacy where agents need to additionally learn to navigate deception and lying?")

2. Does ICR's counterfactual regularization remain robust when interventions come from diverse sources, including humans with varied suggestion styles? (Basis: "Future work should evaluate ICR under diverse interventions, including human suggestions, and test whether its prefix-based counterfactual regularization remains robust...")

3. How does ICR perform with ad hoc collaborators using different training strategies, rather than similarly-trained peers? (Basis: "our agents interact only with similarly trained peers; future work should assess how ICR performs with ad hoc collaborators or alternative learning strategies.")

4. Does counterfactual invariance learned via prefix-based prompts transfer to multi-turn counterfactual scenarios where misleading context accumulates over time? (Basis: "...test whether its prefix-based counterfactual regularization remains robust in multi-turn counterfactual settings to better understand test-time generalization in AI-AI collaborations.")

## Limitations

- The evaluation framework relies heavily on expert-generated dialogues using GPT-4o as both collaborator and intervention agent, limiting generalizability to human partners
- The counterfactual regularization mechanism assumes counterfactual prompts successfully neutralize intervention influence without removing task-relevant context
- The method requires access to intervention agent behavior during training, making it impractical for truly open-ended collaboration scenarios

## Confidence

- **High**: The mathematical framework for MAMDP and the proof of suboptimality for standard MDP policies are sound and well-established
- **Medium**: The empirical results showing ICR outperforms baselines on the two tested tasks are convincing, though the sample size (100 dialogues) and task diversity (two domains) limit broader conclusions
- **Low**: The claim that ICR naturally achieves common ground convergence without explicit training is plausible but unproven—the paper shows correlation between ICR training and CG gains, but doesn't establish causation or rule out alternative explanations

## Next Checks

1. **Intervention Agent Robustness**: Replace the GPT-4o intervention agent with a weaker Llama-3-8B-Instruct agent and measure performance degradation. If ICR-trained agents maintain significant advantage (e.g., >50% relative performance), this validates robustness to intervention quality variation.

2. **Human Evaluation Study**: Conduct a small-scale study with human intervention agents providing actual task-relevant suggestions to ICR-trained versus baseline collaborators. Measure both task accuracy and subjective partner quality ratings.

3. **Counterfactual Prompt Ablation**: Systematically vary the counterfactual prompt components (negation, justification, framing) across the full test set and measure performance variance. This would quantify the robustness of the counterfactual regularization mechanism.