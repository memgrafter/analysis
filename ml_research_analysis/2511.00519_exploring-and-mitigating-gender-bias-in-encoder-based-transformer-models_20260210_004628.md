---
ver: rpa2
title: Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models
arxiv_id: '2511.00519'
source_url: https://arxiv.org/abs/2511.00519
tags:
- bias
- mask
- gender
- word
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in encoder-based transformer models
  by introducing MALoR, a novel metric for quantifying bias in contextualized word
  embeddings. The method involves masking gendered terms and occupations to assess
  probability assignments, then mitigating bias through continued pretraining on gender-balanced
  datasets generated via Counterfactual Data Augmentation.
---

# Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models

## Quick Facts
- arXiv ID: 2511.00519
- Source URL: https://arxiv.org/abs/2511.00519
- Reference count: 20
- Introduces MALoR metric and achieves significant bias reduction in BERT-base models without degrading downstream performance

## Executive Summary
This paper addresses gender bias in encoder-based transformer models by introducing MALoR, a novel metric for quantifying bias in contextualized word embeddings. The method involves masking gendered terms and occupations to assess probability assignments, then mitigating bias through continued pretraining on gender-balanced datasets generated via Counterfactual Data Augmentation. Results show significant bias reduction across multiple models: BERT-base "he-she" bias dropped from 1.27 to 0.08, "his-her" from 2.51 to 0.36, and "male-female" from 1.82 to 0.10, with no degradation in downstream task performance.

## Method Summary
The approach quantifies gender bias using MALoR (Mean Absolute Log of Ratio), which measures the log-normalized probability ratio of male vs. female gendered terms in occupation contexts. The method extracts sentences containing gendered terms plus occupations from WMT corpora, applies Counterfactual Data Augmentation to create balanced gender pairs, and continues MLM pretraining on the balanced dataset. The debiasing process maintains downstream task performance while significantly reducing bias scores across three gender pairs (he-she, his-her, male-female names) in BERT and DistilBERT models.

## Key Results
- BERT-base "he-she" bias reduced from 1.27 to 0.08 after debiasing
- BERT-base "his-her" bias reduced from 2.51 to 0.36 after debiasing
- No significant degradation in SST-2 sentiment analysis accuracy after debiasing
- MALoR scores consistently decrease across all tested models and gender pairs during continued pretraining

## Why This Works (Mechanism)

### Mechanism 1: MALoR Bias Quantification via Log-Ratio Probability Comparison
The MALoR metric quantifies gender bias by comparing model-assigned probabilities for gendered terms in occupation contexts, using log-normalized ratios to ensure scale-invariant comparison. It uses Masked Language Modeling (MLM) to measure the probability a model assigns to male vs. female terms when predicting masked tokens. The log₂ ratio of P(male_term)/P(female_term) across multiple sentence templates and occupations is computed, then the mean absolute value prevents positive and negative biases from canceling.

### Mechanism 2: Counterfactual Data Augmentation (CDA) for Gender-Balanced Training Data
Generating paired sentences with swapped gendered terms creates balanced exposure to gender-occupation associations, reducing learned statistical correlations. For each sentence containing a gendered term plus an occupation, a counterfactual version is created with the opposite gender term. This ensures equal representation of gender-occupation pairings while maintaining grammatical validity.

### Mechanism 3: Continued MLM Pretraining Reweights Internal Representations
Continuing pretraining on gender-balanced data adjusts the model's internal probability distributions without requiring architectural changes or full retraining. Loading the pretrained model and continuing MLM training on balanced data for 200 epochs gradually weakens the original biased associations while maintaining other linguistic knowledge.

## Foundational Learning

- **Masked Language Modeling (MLM)**
  - Why needed here: MALoR fundamentally depends on understanding how MLM assigns probabilities to masked tokens
  - Quick check question: Given "[MASK] dreams of being a nurse," would you expect higher probability for "he" or "she" in a biased model? Why?

- **Word Embedding Bias (Static vs. Contextualized)**
  - Why needed here: The paper positions itself as addressing contextualized embeddings specifically, contrasting with prior work on Word2Vec/GloVe
  - Quick check question: Why might cosine-similarity-based bias detection fail for contextualized embeddings?

- **Encoder-Only Transformer Architecture**
  - Why needed here: The methodology applies specifically to BERT-family models (encoder-only)
  - Quick check question: Why can't you apply MALoR directly to GPT-style models without modification?

## Architecture Onboarding

- **Component map:**
  WMT News Corpora → Sentence Extraction (gender + occupation) → Counterfactual Data Augmentation → Gender-Balanced Dataset → Pretrained Model (BERT/DistilBERT) → Continued MLM Pretraining → MALoR Bias Measurement → Debiased Model → Downstream Task Validation (SST-2)

- **Critical path:**
  1. Vocabulary compatibility check: Verify all 60 occupations and gendered terms exist as whole tokens in target model's vocabulary
  2. Dataset construction: Extract sentences with gender+occupation, apply CDA, ensure pairing
  3. Pretraining loop: 200 epochs, batch size 32, lr 2×10⁻⁵, AdamW with linear scheduler, monitor MALoR convergence
  4. Validation: Run SST-2 before/after debiasing, paired t-test for significance (α=0.01)

- **Design tradeoffs:**
  - BERT/DistilBERT have WordPiece vocabularies containing target terms; RoBERTa/ALBERT split occupation terms, breaking the masking approach
  - 200 epochs provides convergence but requires significant compute; early stopping may leave residual bias
  - Binary gender framework only handles he/she, his/her, male/female names; non-binary pronouns excluded

- **Failure signatures:**
  - MALoR doesn't converge after 200 epochs → Check learning rate, verify dataset balance
  - SST-2 accuracy drops significantly → Catastrophic forgetting; reduce epochs or augment training data with general domain text
  - MALoR low but qualitative bias persists → Metric may not capture all bias dimensions; supplement with SEAT or human evaluation
  - Tokenization splits occupation terms → Model incompatible; consider vocabulary alignment or different model

- **First 3 experiments:**
  1. Compute MALoR on pretrained BERT-base across all three gender pairs using 51 templates and 60 occupations to verify baseline scores
  2. Train on he-she balanced dataset only, plot MALoR vs. epoch to observe convergence behavior
  3. After debiasing, evaluate on SST-2 with 5 seeds, compute mean accuracy and standard deviation

## Open Questions the Paper Calls Out

### Open Question 1
How can the MALoR metric and debiasing framework be adapted for models like RoBERTa and ALBERT that utilize subword tokenization? The current metric relies on the existence of full tokens in the model vocabulary to calculate probabilities, which fails when words are split.

### Open Question 2
Does the debiasing approach preserve model performance across a comprehensive set of downstream tasks beyond just sentiment analysis (SST-2)? While SST-2 performance was maintained, the impact on complex reasoning or linguistic tasks remains unknown.

### Open Question 3
Can the MALoR metric and Counterfactual Data Augmentation methodology be effectively transferred to decoder-based Large Language Models (LLMs)? MALoR relies on Masked Language Modeling (MLM) probabilities, a mechanism specific to encoder architectures, whereas decoder models utilize causal attention.

## Limitations

- Vocabulary compatibility limitations exclude models with subword tokenization from bias evaluation
- Binary gender framework excludes non-binary gender expressions and assumes gender exists on a binary spectrum
- Cross-lingual generalization remains unverified despite evidence of different bias patterns across languages

## Confidence

- **High Confidence**: MALoR metric correctly measures probability ratios using log-normalization; BERT-base bias scores are accurately computed; continued pretraining on gender-balanced data reduces MALoR scores significantly; no significant SST-2 accuracy degradation after debiasing
- **Medium Confidence**: CDA approach successfully creates grammatically valid balanced pairs; 200 epochs provides sufficient training for convergence; binary gender framework captures primary bias dimensions
- **Low Confidence**: MALoR captures all relevant gender bias dimensions; results generalize to non-English languages; SST-2 represents sufficient downstream task validation

## Next Checks

1. **Cross-Model Vocabulary Validation**: Test MALoR metric across BERT, DistilBERT, RoBERTa, and ALBERT to quantify exactly which models fail due to tokenization and document the percentage of target terms that become subword tokens for each architecture.

2. **Non-Binary Gender Extension**: Modify the sentence templates to include singular "they" and other non-binary pronouns, compute MALoR for these terms, and compare bias scores to binary pronouns to validate whether the binary framework captures the complete bias landscape.

3. **Multi-Task Downstream Validation**: Evaluate debiased models on at least three diverse downstream tasks (e.g., SST-2 for sentiment, CoNLL-2003 for NER, and GLUE RTE for reasoning) using paired t-tests to confirm no degradation across all tasks, not just sentiment analysis.