---
ver: rpa2
title: Rethinking Explanation Evaluation under the Retraining Scheme
arxiv_id: '2511.08281'
source_url: https://arxiv.org/abs/2511.08281
tags:
- evaluation
- features
- retraining
- explanation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies a distortion issue in the standard retraining-based
  evaluation scheme (ROAR), where residual information from negatively attributed
  features undermines assessment reliability. The authors analyze the root cause and
  propose two core remedies: (1) lowest-first occlusion (KEAR) to avoid retaining
  negatively attributed features, and (2) fine-tuning variants (KAFT and KAFT-C) to
  improve computational efficiency.'
---

# Rethinking Explanation Evaluation under the Retraining Scheme

## Quick Facts
- arXiv ID: 2511.08281
- Source URL: https://arxiv.org/abs/2511.08281
- Reference count: 40
- Key outcome: This work identifies a distortion issue in the standard retraining-based evaluation scheme (ROAR), where residual information from negatively attributed features undermines assessment reliability. The authors analyze the root cause and propose two core remedies: (1) lowest-first occlusion (KEAR) to avoid retaining negatively attributed features, and (2) fine-tuning variants (KAFT and KAFT-C) to improve computational efficiency. Experiments across small- and large-scale datasets demonstrate that KAFT-C achieves up to 100× speedup over full retraining while delivering more reliable evaluations. On ImageNet-ResNet50, baseline accuracy degrades from 76.13% to 12.98% with 90% feature removal, with SIG achieving the highest drop (35.61%). On ImageNet-SwinT, IG exhibits a performance collapse, highlighting limitations of integration-based methods on transformer architectures. The findings provide practical tools for explanation evaluation and reveal open challenges in attribution methods.

## Executive Summary
This paper addresses a fundamental reliability problem in retraining-based feature attribution evaluation (ROAR) where negatively attributed features leak residual information, distorting results. The authors identify this "Sign issue" and propose two solutions: KEAR (lowest-first occlusion) that avoids retaining negatively attributed features, and KAFT variants that achieve up to 100× speedup through efficient fine-tuning. Experiments across multiple datasets and architectures demonstrate that KAFT-C provides reliable evaluation while being computationally efficient, and reveal limitations of gradient-based methods on transformer architectures.

## Method Summary
The authors propose KEAR (Keep by Explanation for Attribution Removal) as an alternative to ROAR's highest-first occlusion, which removes features in descending order of attribution magnitude. KEAR keeps the highest-attributed features and removes the lowest, avoiding retention of negatively attributed features that leak residual information. They also introduce KAFT variants: KAFT fine-tunes the entire model for 20% data/30 epochs, while KAFT-C fine-tunes only the classification head for 10% data/10 epochs, achieving ~100× speedup. The evaluation metric is accuracy degradation on manipulated data, with ∆Acc. measuring the area between KAFT-C and |RAFT-C| degradation curves.

## Key Results
- KAFT-C achieves up to 100× speedup over full retraining while maintaining evaluation validity
- On ImageNet-ResNet50, baseline accuracy degrades from 76.13% to 12.98% with 90% feature removal, with SIG achieving the highest drop (35.61%)
- On ImageNet-SwinT, IG exhibits a performance collapse, highlighting limitations of integration-based methods on transformer architectures
- ROAR shows all explainers performing near-random due to the sign issue, while KEAR reveals clear quality differences between methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing features by highest-first attribution order preserves negatively-attributed features that leak task-relevant information, causing underestimation of explanation quality.
- **Mechanism:** Features serving as "secondary evidence" for class y (positively correlated but stronger evidence for alternative class y*) receive negative attributions. Under highest-first occlusion (ROAR), these survive and become primary evidence after the true primary evidence is removed, causing unexpectedly high retraining accuracy.
- **Core assumption:** Features shared across classes exist and receive attributions reflecting relative rather than absolute importance.
- **Evidence anchors:**
  - [abstract] "identify the 'Sign issue' as a key factor: negatively attributed features can leak residual information during retraining"
  - [section 3.2] Theorem 1: "˜I(S₂;y) > I(S₂;y) ≫ 0" — mutual information increases after manipulation
  - [corpus] No direct corpus support; related papers address unrelated domains (MRI reconstruction, MIMO channels).
- **Break condition:** When no features serve as secondary evidence across classes (e.g., mutually exclusive feature sets), the sign issue disappears.

### Mechanism 2
- **Claim:** Features can positively activate a class logit while receiving negative attribution to the final softmax probability.
- **Mechanism:** Softmax normalization creates competition between output nodes. A "weak positive contributor" satisfies: exp(ξ^oy_i) < E_{y*≠y}[exp(ξ^oy*_i)], meaning its positive contribution to class y is weaker than expected contributions to competing classes, yielding negative final attribution despite positive logit contribution.
- **Core assumption:** Multi-class classification with softmax output layer.
- **Evidence anchors:**
  - [section 3.3] Definition 2 and Theorem 2 formalize weak positive contributors
  - [figure 2] Illustrates positive contribution to o_1 but negative contribution to f_1
  - [corpus] No corpus evidence available.
- **Break condition:** Binary classification without softmax, or features with sufficiently dominant class-specific contributions.

### Mechanism 3
- **Claim:** Fine-tuning only the classification head achieves ~100× speedup over full retraining while maintaining evaluation validity.
- **Mechanism:** Hidden layers function as feature extractors. If influential input features are preserved (KAFT-C's keep strategy), extracted representations remain useful. Only the classification head needs adaptation to the altered context, avoiding full model reorganization.
- **Core assumption:** Feature extraction layers generalize sufficiently to partial inputs; relevant features are preserved by the explanation method.
- **Evidence anchors:**
  - [section 4.1] "KAFT-C achieves a 100× speedup over KEAR" with 10% data and 10% epochs
  - [figure 5] KAFT-C curves show consistent explainer ranking alignment with theory
  - [corpus] TransferTraj paper mentions avoiding retraining for transferability, loosely supporting parameter-efficient adaptation.
- **Break condition:** When input manipulation severely degrades feature extraction utility (e.g., destroying spatial structure), head-only fine-tuning may underfit.

## Foundational Learning

- **Concept: Mutual Information I(S; y)**
  - **Why needed here:** Core theoretical framing for residual information — evaluation distortion occurs when ˜I(N\S; y) ≫ 0 despite removing top features.
  - **Quick check question:** If removing top-k features leaves I(remaining; y) high, what does this imply about the explanation method?

- **Concept: Gradient-based Attribution (Vanilla, IG, SmoothGrad)**
  - **Why needed here:** Paper evaluates these methods; understanding their baselines and integration paths is essential to interpret why IG succeeds where others fail.
  - **Quick check question:** Why does IG explicitly model feature absence via a baseline, and how does this relate to the manipulation baseline?

- **Concept: Distribution Shift and OOD Detection**
  - **Why needed here:** The fundamental problem with inference-based schemes is distribution shift from input manipulation; retraining schemes aim to adapt models to this shift.
  - **Quick check question:** After replacing 90% of pixels with baseline values, is the resulting input in-distribution? How does retraining address this?

## Architecture Onboarding

- **Component map:** Input Image → Feature Attribution Method → Ranked Feature List → Manipulation Strategy (Keep vs Remove) → Modified Dataset Creation → Model Update Protocol (Full Retrain/ Fine-tune All/Fine-tune Head Only) → Evaluation Metric: Accuracy on Test Set

- **Critical path:** Attribution generation → Manipulation at ratio r → Model adaptation → Test accuracy comparison against random baseline

- **Design tradeoffs:**
  - **ROAR vs KEAR:** ROAR removes highest-attributed (sensitive to sign issue); KEAR keeps highest-attributed (robust to sign issue)
  - **Full retrain vs KAFT-C:** Full retrain adapts completely but costly; KAFT-C efficient but assumes feature extractors remain useful
  - **With-sign vs magnitude-only:** |RAFT-C| uses absolute values, eliminating sign issue but vulnerable to redundant feature leakage

- **Failure signatures:**
  - ROAR reports all explainers ≈ random → sign issue likely present
  - IG performance collapse on SwinT → gradient saturation plateau or interaction cancellation (Section 5.2)
  - KAFT-C accuracy equals random baseline → explanation method failed to identify relevant features

- **First 3 experiments:**
  1. **Sanity check:** Run ROAR vs KEAR on MNIST-CNN with IG. Expect: ROAR shows all methods ≈ random; KEAR ranks IG > SG > VG. Confirms sign issue presence.
  2. **Efficiency validation:** Compare KAFT-C vs KEAR timing on CIFAR10-WideResNet at 90% manipulation. Target: >50× speedup with <5% accuracy deviation.
  3. **Architecture sensitivity:** Test IG on ResNet50 vs SwinT with ImageNet. Observe: ResNet50 shows clear separation from random; SwinT shows collapse, indicating saturation/interaction issues.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can integration paths be optimized in attribution methods like Integrated Gradients (IG) to mitigate performance collapse caused by gradient saturation and feature interaction cancellation in Vision Transformers?
- **Basis in paper:** [explicit] Section 6 states that the failure of IG on SwinT "highlights the need for further investigation into path selection and allocation of feature interactions."
- **Why unresolved:** The paper identifies the collapse and hypothesizes two causes (saturation plateaux and cancellation effects), but it does not determine the dominant cause or propose a corrective path selection mechanism for these architectures.
- **What evidence would resolve it:** Empirical results showing that modified integration paths (e.g., non-linear or noisy paths) successfully recover the performance gap between IG and SIG on Swin Transformers.

### Open Question 2
- **Question:** Can a structured evaluation framework effectively align and synthesize results from retraining variants with different manipulation priorities (e.g., keeping vs. removing features)?
- **Basis in paper:** [explicit] Appendix A.6 notes as future work: "we aim to expand this discussion by developing a more structured explanation evaluation framework that aligns results across retraining scheme variants with diverse manipulation priorities."
- **Why unresolved:** The paper introduces distinct variants (KAFT-C, |RAFT-C|) that offer complementary perspectives but does not provide a protocol for interpreting them jointly or resolving discrepancies.
- **What evidence would resolve it:** A novel metric or protocol that normalizes "keep" and "remove" accuracy scores into a unified reliability index that remains consistent across different models and explainers.

### Open Question 3
- **Question:** Does the "Sign issue" (information leakage via negatively attributed features) distort evaluation in non-classification tasks (e.g., regression or generation) to the same extent observed in image classifiers?
- **Basis in paper:** [inferred] The theoretical derivation of the "Sign issue" (Appendix A.1) and the definition of "weak positive contributors" rely heavily on softmax probability distributions and multi-class conditional entropy.
- **Why unresolved:** The analysis assumes the model output is a discrete class probability; it is unclear if "secondary evidence" creates residual information in continuous or sequential output spaces where softmax constraints are absent.
- **What evidence would resolve it:** Theoretical analysis or empirical benchmarks demonstrating whether negatively attributed features in regression or language models retain significant mutual information after manipulation.

## Limitations
- The theoretical mechanism of "weak positive contributors" lacks direct empirical validation beyond the proposed evaluation framework
- Claims about IG's failure on SwinT (gradient saturation vs. interaction cancellation) remain speculative without targeted experiments
- The computational efficiency claims assume standard fine-tuning settings that may not generalize across diverse architectures and datasets
- Limited corpus evidence supporting claims about explanation evaluation and attribution methods

## Confidence
- **High confidence:** The existence of the sign issue (negatively attributed features leaking information during retraining) and the basic effectiveness of KEAR vs ROAR in revealing this distortion
- **Medium confidence:** The theoretical mechanism of "weak positive contributors" creating negative attributions, and the ~100× speedup claim for KAFT-C (limited empirical validation across diverse settings)
- **Low confidence:** Claims about why IG fails on SwinT (gradient saturation vs. interaction cancellation), and broader generalizability of findings to non-image domains

## Next Checks
1. **Mechanism validation:** Design an experiment with synthetic data where features are explicitly partitioned into mutually exclusive sets (no shared features across classes). Run ROAR and KEAR: if the sign issue is real, ROAR should show improved discriminative power compared to the original paper's results.

2. **Efficiency verification:** Implement KAFT-C on a small dataset (CIFAR10) and measure actual wall-clock time vs. full retraining across multiple manipulation ratios. Verify the 100× speedup claim under realistic computational constraints.

3. **Architecture sensitivity test:** Compare IG performance on ResNet vs. SwinT using the same ImageNet dataset, varying input manipulation strategies (baseline replacement vs. blurring). This isolates whether performance collapse stems from gradient saturation, architectural differences, or manipulation artifacts.