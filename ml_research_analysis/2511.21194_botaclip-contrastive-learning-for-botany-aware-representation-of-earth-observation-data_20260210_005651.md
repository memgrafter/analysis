---
ver: rpa2
title: 'BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation
  Data'
arxiv_id: '2511.21194'
source_url: https://arxiv.org/abs/2511.21194
tags:
- dofa
- embeddings
- botaclip
- contrastive
- ecological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "BotaCLIP addresses the challenge of adapting foundation models\
  \ for ecological applications without costly retraining. The method introduces a\
  \ lightweight multimodal contrastive framework that aligns high-resolution aerial\
  \ imagery with botanical relev\xE9s through contrastive learning with a regularization\
  \ strategy that mitigates catastrophic forgetting."
---

# BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data

## Quick Facts
- arXiv ID: 2511.21194
- Source URL: https://arxiv.org/abs/2511.21194
- Authors: Selene Cerna; Sara Si-Moussi; Wilfried Thuiller; Hadrien Hendrikx; Vincent Miele
- Reference count: 40
- Primary result: Lightweight contrastive framework aligns EO imagery with botanical relevés, achieving +14.9% TSS for plant monitoring, +10.4% Boyce Index for butterfly prediction, and +1.8% Spearman correlation for soil trophic group abundance

## Executive Summary
BotaCLIP introduces a lightweight multimodal contrastive framework that injects botanical semantics into Earth Observation foundation models without costly retraining. The method aligns high-resolution aerial imagery with vegetation relevés through contrastive learning while preserving the foundation model's general representational capacity via a similarity-preserving regularization strategy. This approach enables effective domain adaptation in data-scarce ecological settings.

## Method Summary
BotaCLIP employs lightweight linear adapters to project both DOFA image embeddings (768-dim) and Botania-encoded relevé vectors into a shared latent space. The alignment is trained using a sigmoid contrastive loss with learnable temperature and bias, paired with a regularization term that preserves DOFA's local similarity structure to prevent catastrophic forgetting. Botania is first pretrained on vegetation classification (232 classes) to provide strong ecological priors. The entire framework is trained end-to-end with frozen DOFA, spatial cross-validation (5km buffer), and standard augmentations.

## Key Results
- Plant presence prediction: +14.9% TSS improvement over DOFA baseline
- Butterfly occurrence modeling: +10.4% Boyce Index improvement over DOFA baseline
- Soil trophic group abundance: +1.8% Spearman correlation improvement over DOFA baseline
- BotaCLIP consistently outperformed supervised baselines across all three ecological tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning EO imagery with vegetation relevés through contrastive learning injects ecological semantics into generic embeddings without full model retraining.
- **Mechanism:** Paired orthophoto–relevé samples are projected into a shared latent space via lightweight linear adapters. A sigmoid contrastive loss (SCL) pulls positive pairs (i = j) together and pushes negative pairs (i ≠ j) apart using pairwise logits with learnable temperature and bias. This reshapes the embedding space around dimensions that distinguish vegetation composition.
- **Core assumption:** Paired image–tabular samples provide sufficient weak supervision to transfer ecological structure from relevés to visual embeddings.
- **Evidence anchors:**
  - [abstract] "BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting."
  - [Section 2.2] "For two vectors z, z' ∈ R^768, let z·z' denote their scalar product. Pairwise logits are then computed as: ℓij(θ) = (z_img_i · z_tab_j) exp(τ) + b"
  - [corpus] Limited direct corpus evidence for sigmoid contrastive loss in EO; nearest neighbors use standard CLIP-style or supervised contrastive objectives.
- **Break condition:** If paired image–relevé data is too sparse or noisy, alignment may fail to transfer meaningful structure; UMAP visualizations would show no cluster sharpening.

### Mechanism 2
- **Claim:** Regularization preserving local similarity structure from foundation embeddings mitigates catastrophic forgetting while enabling domain specialization.
- **Mechanism:** A regularization term R(θ) = (1/N²) Σᵢ Σⱼ Wᵢⱼ (Img_i · Img_j − z_img_i · z_img_j)² with Wᵢⱼ = ((1 + Img_i · Img_j)/2)² constrains projected embeddings to maintain neighborhood relations from DOFA. This prevents the contrastive objective from collapsing dimensions unrelated to vegetation.
- **Core assumption:** DOFA embeddings already encode meaningful environmental cues (soil, relief, anthropogenic patterns) that should be preserved for broad transferability.
- **Evidence anchors:**
  - [Section 2.2] "Relying solely on the contrastive loss L_SCL can lead to catastrophic forgetting... cues captured by DOFA but not strongly linked to vegetation composition risk being discarded."
  - [Section 4] "BotaCLIP achieves a lower DB index (7.17 vs. 10.69) and a higher CH index (237.4 vs. 181.5)" indicating improved cluster structure.
  - [corpus] Three Towers model and Ex-MCR use similar preservation strategies, suggesting cross-domain validity.
- **Break condition:** If regularization weight λ is too high, embeddings become overly similar to DOFA and show minimal ecological specialization; if too low, transfer to non-vegetation tasks degrades.

### Mechanism 3
- **Claim:** Pre-training the tabular encoder (Botania) on ecological classification provides stronger priors than generic MLPs, improving contrastive alignment quality.
- **Mechanism:** Botania is pretrained to predict 232 vegetation classes from species–cover vectors using a 4-layer MLP with GELU activations. The 768-dimensional penultimate embeddings capture phytosociological structure, which then serve as richer targets for image alignment compared to randomly initialized encoders.
- **Core assumption:** Ecological pretext tasks (vegetation classification) produce representations that generalize better to downstream biodiversity tasks than generic tabular encoders.
- **Evidence anchors:**
  - [Section 2.1] "Botania was trained with 300 epochs... reaching 66% top-1 and 86% top-3 accuracy."
  - [Section 4] "Among BotaCLIP variants, BWiAuSclR consistently emerged as the best model" where B denotes Botania encoder.
  - [corpus] No direct corpus evidence for ecological pretext encoders in contrastive EO frameworks.
- **Break condition:** If Botania classification accuracy is too low (<50%), embeddings may not capture sufficient ecological structure to benefit alignment; ablation with random MLP should show performance gap.

## Foundational Learning

- **Contrastive Learning with Sigmoid Loss**
  - Why needed here: Core training objective differs from standard softmax-based CLIP; requires understanding pairwise logistic formulation.
  - Quick check question: Can you explain why sigmoid loss avoids the global normalization of softmax and enables independent pair-wise training?

- **Catastrophic Forgetting in Representation Learning**
  - Why needed here: Central challenge when adapting frozen foundation models; regularization design directly addresses this.
  - Quick check question: What happens to embeddings trained only on contrastive loss without regularization, specifically for dimensions unrelated to the alignment task?

- **Spatial Cross-Validation and Autocorrelation**
  - Why needed here: Ecological data has spatial structure; standard random splits cause leakage and inflated metrics.
  - Quick check question: Why does a 5km buffer between training and validation folds matter for species distribution data?

## Architecture Onboarding

**Component map:**
Orthophoto (100×100m, RGB) → DOFA ViT (frozen, 768d) → Linear Adapter (768→768, identity init) → z_img
Relevé (species×cover vector, 3587d) → Botania MLP (pretrained, trainable) → Linear Adapter (768→768) → z_tab
z_img, z_tab → ℓ2 normalize → Sigmoid Contrastive Loss + Similarity Regularization → L_SCL_R

**Critical path:**
1. DOFA backbone remains frozen throughout training—no gradient flow to foundation model.
2. Image adapter initialized as identity matrix with Gaussian noise (σ² = 10⁻⁴) to preserve DOFA embeddings at initialization.
3. Botania encoder initialized from pre-trained checkpoint but remains trainable (joint adaptation).
4. Training uses spatial k-fold (k=1 for efficiency) with 5km cell buffer to prevent leakage.

**Design tradeoffs:**
- **Identity vs. random adapter initialization:** Identity preserves DOFA structure initially; random risks destroying semantic content before alignment begins.
- **Frozen DOFA vs. fine-tuned:** Freezing reduces compute (~8.1M params trained vs. ~111M) but may limit ceiling performance for highly specialized domains.
- **Single fold (k=1) vs. full spatial CV:** Single fold faster for experimentation but provides less robust estimate of generalization.

**Failure signatures:**
- Embeddings cluster poorly in UMAP (no sharpening vs. DOFA) → insufficient alignment strength or learning rate too low.
- Performance on butterfly/soil tasks degrades vs. DOFA → regularization too weak, forgetting general visual features.
- Training loss plateaus early but downstream metrics poor → Botania encoder may be underfitting; increase capacity or training epochs.

**First 3 experiments:**
1. **Baseline check:** Extract raw DOFA embeddings and train downstream Random Forest on plant presence; confirm TSS ~0.42 matches paper baseline.
2. **Ablation: No regularization (λ=0):** Train BotaCLIP without R_D term; expect degraded butterfly/soil performance indicating catastrophic forgetting.
3. **Ablation: Random MLP vs. Botania:** Replace Botania with randomly initialized MLP encoder; expect TSS drop confirming ecological pretext value.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a tri-modal alignment of images, vegetation relevés, and environmental covariates improve ecological forecasting compared to the current bimodal framework?
- Basis in paper: [explicit] The conclusion states future work includes "exploring tri-modal alignments of images, relevés, and environmental covariates."
- Why unresolved: The current BotaCLIP framework is bimodal (image–tabular), and it is unclear if adding a third modality (e.g., climate or soil data) would harmonize or conflict with the existing embedding space regularization.
- What evidence would resolve it: A comparative study evaluating downstream performance (TSS, Boyce Index) of a tri-modal model against the bimodal baseline across the three ecological tasks.

### Open Question 2
- Question: Does leveraging multispectral or SAR data, rather than RGB-only inputs, significantly enhance the botanical signal in the embeddings?
- Basis in paper: [inferred] Section 2.1 notes that while the DOFA backbone is pretrained on multispectral, hyperspectral, and SAR data, "Here we used only RGB inputs," leaving the utility of other bands unexplored.
- Why unresolved: Vegetation structure and health are often better captured in non-visible spectrums (e.g., NIR), but it is unknown if the lightweight adapter can effectively project these richer signals into the shared space without destabilizing the regularization.
- What evidence would resolve it: Experiments using the same BotaCLIP architecture but feeding multispectral/SAR channels into the DOFA encoder to measure performance deltas in plant and insect prediction.

### Open Question 3
- Question: Can the contrastive alignment framework successfully transfer to non-visual ecological modalities such as acoustic data or functional traits?
- Basis in paper: [explicit] The authors explicitly list "extending BotaCLIP to other ecological modalities (traits, acoustics)" as a direction for future work.
- Why unresolved: The current success relies on spatial alignment between orthophotos and ground plots; it is uncertain if the sigmoid loss and regularization strategy are robust enough to handle the temporal and structural variability of audio or trait datasets.
- What evidence would resolve it: Adapting the tabular/image pipeline to align soundscape recordings or trait matrices with Earth Observation data and evaluating species detection accuracy.

## Limitations

- **Limited applicability:** Requires paired high-resolution orthophoto–relevé data (20cm resolution), restricting use to regions with detailed aerial imagery.
- **Foundation model dependence:** Performance gains rely on DOFA's environmental encoding; effectiveness may vary across different foundation models.
- **Pretrain task specificity:** Assumes 232 vegetation classes provide sufficient supervision, which may not generalize to regions with different phytosociological classifications.

## Confidence

- **High Confidence:** The core mechanism of using lightweight adapters with contrastive alignment to preserve foundation model capacity while adding domain semantics is well-supported by the ablation studies and downstream task performance.
- **Medium Confidence:** The effectiveness of the sigmoid contrastive loss formulation and the specific regularization weight (λ=1) are demonstrated but could benefit from broader hyperparameter sensitivity analysis.
- **Medium Confidence:** The spatial cross-validation protocol prevents leakage, but single-fold evaluation introduces uncertainty in the reported performance metrics.

## Next Checks

1. **Ablation on regularization weight:** Systematically vary λ from 0 to 10 and measure performance degradation on butterfly and soil tasks to quantify catastrophic forgetting sensitivity.
2. **Spatial CV robustness:** Implement full k-fold spatial cross-validation (k=3 or k=5) to assess whether single-fold results overestimate generalization performance.
3. **Foundation model dependence:** Replace DOFA with another foundation model (e.g., DeepGlobe or other EO foundation models) and evaluate whether BotaCLIP's performance gains persist across different visual encoders.