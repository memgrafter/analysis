---
ver: rpa2
title: How Do Large Language Monkeys Get Their Power (Laws)?
arxiv_id: '2502.17578'
source_url: https://arxiv.org/abs/2502.17578
tags:
- power
- scaling
- language
- laws
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Monkeys, a framework for scaling inference compute
  via repeated sampling, exhibits power law scaling in negative log average success
  rate with number of attempts. However, the underlying mathematical mechanism remains
  unclear, as individual problems show exponential scaling.
---

# How Do Large Language Monkeys Get Their Power (Laws)?

## Quick Facts
- arXiv ID: 2502.17578
- Source URL: https://arxiv.org/abs/2502.17578
- Reference count: 40
- Key outcome: Power law scaling in negative log average success rate emerges from heavy-tailed distributions of single-attempt success probabilities, even when individual problems scale exponentially.

## Executive Summary
This paper resolves a puzzle in the scaling behavior of Large Language Monkeys (LLMs), a framework for scaling inference compute through repeated sampling. While aggregate success rates follow power law scaling with number of attempts, individual problems exhibit exponential scaling. The authors demonstrate that this power law behavior emerges from the heavy-tailed structure of the distribution of single-attempt success probabilities, where a small fraction of tasks with extremely low success probabilities collectively warp the aggregate trend. The work provides both theoretical derivation and empirical validation using the OLMo-7B model, and introduces a forecasting method that achieves significantly lower relative error with reduced inference compute.

## Method Summary
The authors analyze the scaling behavior of Large Language Monkeys by examining the distribution of single-attempt success probabilities across tasks. They derive mathematically that power law scaling emerges when this distribution is heavy-tailed, specifically following a power law with shape parameter between 1 and 2. The framework assumes geometric distribution of attempts needed for success, and shows that aggregate power law behavior can arise even when individual problems scale exponentially. The empirical validation uses the OLMo-7B model across various problem distributions to confirm the theoretical predictions.

## Key Results
- Power law scaling in negative log average success rate emerges from heavy-tailed distributions of single-attempt success probabilities
- A small fraction of tasks with extremely low success probabilities collectively create power law behavior, even when individual problems scale exponentially
- The proposed forecasting method achieves significantly lower relative error (or equivalently, orders of magnitude less inference compute) compared to direct measurement

## Why This Works (Mechanism)
Power law scaling emerges because the distribution of single-attempt success probabilities is heavy-tailed. When this distribution has a power law form with shape parameter between 1 and 2, the aggregate success rate exhibits power law scaling in the number of attempts, even though each individual problem scales exponentially. The heavy tail means that a small fraction of tasks have extremely low success probabilities, and these tasks collectively dominate the aggregate behavior, warping the exponential trends of most problems into an overall power law.

## Foundational Learning

**Geometric distribution**: Models the number of attempts until first success in repeated Bernoulli trials; needed to understand the sampling framework's success probability dynamics; quick check: verify that P(X=k) = (1-p)^(k-1) * p for k=1,2,3,...

**Power law distribution**: Probability distribution where P(X>x) ∝ x^(-α); needed to characterize the heavy-tailed nature of success probabilities; quick check: log-log plot of complementary CDF should be linear with slope -α

**Heavy-tailed distributions**: Distributions whose tails are not exponentially bounded; needed to explain why rare, difficult tasks dominate aggregate scaling behavior; quick check: compare tail behavior to exponential decay

**Negative log average success rate**: Transformation that linearizes power law relationships in log-log space; needed for empirical validation and visualization of scaling trends; quick check: should yield straight line with slope -α in log-log plot

## Architecture Onboarding

**Component map**: OLMo-7B model -> Sampling framework (repeated attempts) -> Success probability distribution -> Aggregate success rate -> Power law scaling

**Critical path**: Problem generation → Single-attempt success probability → Distribution fitting → Power law exponent estimation → Validation of scaling behavior

**Design tradeoffs**: The geometric distribution assumption simplifies analysis but may not capture all sampling strategies; heavy-tailed distribution assumption explains power laws but requires careful empirical validation

**Failure signatures**: Deviations from power law scaling indicate either non-heavy-tailed success probability distributions or breakdown of geometric assumption; exponential rather than power law scaling suggests insufficient tail heaviness

**First experiments**:
1. Measure the empirical distribution of single-attempt success probabilities across diverse task types
2. Fit power law models to the success probability distribution and estimate shape parameters
3. Compare aggregate success rate scaling with theoretical predictions based on fitted distributions

## Open Questions the Paper Calls Out
None

## Limitations

- The geometric distribution assumption for number of attempts may not hold for all problem types or sampling strategies
- The heavy-tailed nature of success probability distribution is inferred from aggregate behavior rather than directly measured
- The framework focuses on repeated sampling and may not generalize to other inference-time compute scaling methods

## Confidence

**High confidence**: Mathematical derivation connecting heavy-tailed distributions to power law scaling; empirical validation using OLMo-7B model

**Medium confidence**: Practical forecasting method showing lower relative error; robustness across different model architectures and domains needs further validation

**Low confidence**: Geometric distribution accurately models number of attempts for all problem types

## Next Checks

1. Validate the heavy-tailed distribution assumption by directly measuring the distribution of single-attempt success probabilities across diverse problem types and sampling strategies, rather than inferring from aggregate behavior.

2. Test the forecasting method's robustness across different model sizes, architectures (including multimodal models), and evaluation datasets to confirm the claimed reduction in relative error.

3. Extend the theoretical framework to other inference-time compute scaling methods beyond repeated sampling, such as beam search or adaptive computation time, to assess generalizability of the power law emergence mechanism.