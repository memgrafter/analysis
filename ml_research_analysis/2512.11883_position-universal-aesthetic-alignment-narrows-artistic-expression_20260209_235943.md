---
ver: rpa2
title: 'Position: Universal Aesthetic Alignment Narrows Artistic Expression'
arxiv_id: '2512.11883'
source_url: https://arxiv.org/abs/2512.11883
tags:
- image
- arxiv
- aesthetic
- generation
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that over-aligning image generation models to
  a generalized aesthetic preference suppresses creative expression by penalizing
  intentionally "anti-aesthetic" outputs, even when they match user prompts. The authors
  construct a wide-spectrum aesthetics dataset and evaluate state-of-the-art generation
  and reward models, finding that reward models consistently assign lower scores to
  anti-aesthetic images regardless of prompt adherence.
---

# Position: Universal Aesthetic Alignment Narrows Artistic Expression

## Quick Facts
- arXiv ID: 2512.11883
- Source URL: https://arxiv.org/abs/2512.11883
- Reference count: 33
- Key outcome: Aesthetic alignment in image generation models suppresses creative expression by penalizing intentionally "anti-aesthetic" outputs even when they match user prompts

## Executive Summary
This paper demonstrates that over-aligning image generation models to generalized aesthetic preferences systematically suppresses creative expression by penalizing outputs that deviate from conventional beauty norms. The authors construct a wide-spectrum aesthetics dataset and evaluate state-of-the-art generation and reward models, finding that both consistently assign lower scores to anti-aesthetic images regardless of prompt adherence. When tested against real abstract artworks, reward models give these significantly lower scores than AI-generated images, revealing a preference for conventional beauty over artistic intent. The study concludes that current aesthetic alignment prioritizes developer-centered values over user autonomy and aesthetic pluralism, effectively functioning as aesthetic authoritarianism.

## Method Summary
The authors create anti-aesthetic prompts by combining COCO captions with "bad" dimension descriptions from the VisionReward dataset, then generate paired images using multiple models (Flux Dev, SDXL, SD3.5M and aligned variants). They evaluate these image pairs using reward models (HPSv2.1, HPSv3, ImageReward, PickScore, MPS, CLIP-L, BLIP-L) and a fine-tuned judging model. The evaluation compares reward scores for anti-aesthetic versus original-prompt images, measures semantic preservation via BLIP scores, and tests success rates through LLM-as-judge comparisons. Statistical significance is assessed using Wilcoxon signed-rank and McNemar's tests.

## Key Results
- Reward models consistently assign lower scores to anti-aesthetic images than original images, even when anti-aesthetic prompts explicitly request such outputs
- Aligned image generation models default to conventionally beautiful outputs despite explicit instructions for anti-aesthetic content
- Real abstract artworks receive significantly lower scores from reward models compared to AI-generated images
- Base models (CLIP-L, BLIP-L) outperform preference-aligned reward models at identifying prompt-adherent images

## Why This Works (Mechanism)
Current aesthetic alignment prioritizes conventional beauty standards over user intent by training on preference datasets that reflect narrow cultural values. Reward models learn to optimize for average aesthetic appeal rather than instruction fidelity, creating a systematic bias against diverse or intentionally non-beautiful outputs. This manifests as a "beautification" effect where models override explicit user instructions in favor of aesthetically pleasing results, effectively suppressing artistic expression that falls outside established norms.

## Foundational Learning
- VisionReward dataset: Contains aesthetic dimension ratings (lighting, color, detail, etc.) used to generate anti-aesthetic prompts - needed to understand the evaluation framework and prompt construction
- Reward model alignment: Models trained on preference data tend to prioritize average aesthetic appeal over prompt adherence - quick check: compare CLIP/BLIP performance against preference-aligned models
- Anti-aesthetic prompt generation: Combining base captions with "bad" dimension descriptions creates explicit instructions for non-beautiful outputs - needed to reproduce the core experimental methodology
- LLM-as-judge evaluation: Using language models to compare image pairs based on semantic preservation and aesthetic quality - quick check: verify success rate calculations match human judgments
- Statistical significance testing: Wilcoxon signed-rank and McNemar's tests validate whether observed differences are meaningful - needed to assess the robustness of results

## Architecture Onboarding

### Component Map
VLM (Qwen3-VL-235B) -> Anti-aesthetic prompt generator -> Image generation models (Flux Dev, SDXL, SD3.5M variants) -> Reward models (HPSv2.1, HPSv3, ImageReward, PickScore, MPS, CLIP-L, BLIP-L) -> Judging model (Qwen3-VL-4B fine-tuned)

### Critical Path
Prompt generation → Image synthesis → Reward scoring → Statistical analysis → Validation against real artworks

### Design Tradeoffs
- Aesthetic alignment vs. instruction fidelity: Models must balance conventional beauty with user intent
- Generalizability vs. specificity: Reward models optimized for average preferences may fail on diverse artistic expressions
- Evaluation complexity: Multi-model comparison increases robustness but adds computational overhead

### Failure Signatures
- Base models outperform aligned reward models at identifying prompt-adherent images
- Anti-aesthetic prompts produce "beautified" outputs instead of requested degradation
- Real artworks receive lower scores than AI-generated images from reward models

### First Experiments
1. Generate anti-aesthetic prompts using COCO captions and VisionReward dimensions
2. Compare reward scores between anti-aesthetic and original-prompt images
3. Test model responses to explicit "create an ugly image" instructions

## Open Questions the Paper Calls Out
### Open Question 1
How can generative architectures incorporate user-controllable mechanisms to dynamically adjust the strength of aesthetic alignment versus instruction fidelity? The authors explicitly call for models to "incorporate user-controllable mechanisms to adjust the strength of aesthetic alignment or switch it off entirely" (Page 8). This remains unresolved because current systems prioritize an "imaginary average user" over concrete intent, lacking the architectural capacity to modulate aesthetic enforcement based on user preference. Development of model interfaces or routing layers where a specific user parameter reliably modulates the "beautification" of outputs without hallucinating content would resolve this.

### Open Question 2
Can reward models be effectively trained to value "anti-aesthetic" or diverse artistic styles without penalizing them for deviating from conventional beauty norms? The conclusion requests "reward systems... that (a) recognize and value diverse artistic styles" (Page 8). This remains unresolved because current reward models like HPSv3 assign significantly lower scores to famous artworks and faithful "wide-spectrum" generations compared to generic AI imagery (Figure 4). A reward model benchmark where instruction-faithful "ugly" images receive high scores, correlating with human expert evaluations of artistic intent rather than general appeal, would resolve this.

### Open Question 3
Does increasing the diversity of annotator pools in preference datasets effectively mitigate the "aesthetic authoritarianism" and cultural homogenization observed in current models? The authors urge future efforts to be "informed by more diverse datasets and annotator pools that better represent the full spectrum of human aesthetic judgment" (Page 8). This remains unresolved because current models inherit narrow definitions of "good" imagery from specific demographics, reinforcing a limited cultural capital (Page 3). Ablation studies demonstrating that models trained on diverse annotations produce higher variance in aesthetic styles and reduced bias against non-normative art would resolve this.

## Limitations
- The VLM prompt template for generating anti-aesthetic prompts remains unspecified, making exact replication difficult
- The judging model fine-tuning procedure and dataset details are incomplete, raising questions about evaluation framework bias
- The "bad rating descriptions" from VisionReward are not fully documented in accessible materials
- The sample size of real abstract artworks (~10K) may not capture full diversity of intentional anti-aesthetic expression

## Confidence
- High confidence: The core empirical finding that reward models systematically assign lower scores to anti-aesthetic images regardless of prompt adherence is well-supported by the data presented
- Medium confidence: The interpretation that this represents "aesthetic authoritarianism" and suppression of creative expression requires careful consideration of alternative explanations and complete methodological transparency
- Medium confidence: The claim that aligned models default to conventionally beautiful outputs even when instructed otherwise is supported, but the evaluation framework's own potential biases need independent verification

## Next Checks
1. Replicate the anti-aesthetic prompt generation using the exact VLM prompt template (if obtainable) or alternative methods to verify the effect persists across different anti-aesthetic prompt formulations
2. Independently evaluate the same image pairs using CLIP/BLIP reward models to confirm whether base models indeed outperform preference-aligned models at identifying prompt-adherent images
3. Test the aligned models with a broader range of explicitly anti-aesthetic instructions (e.g., "create an intentionally ugly image," "generate a deliberately poor-quality photograph") to determine if the effect generalizes beyond the specific aesthetic dimensions tested