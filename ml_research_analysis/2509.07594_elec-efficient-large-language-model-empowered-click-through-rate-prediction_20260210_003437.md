---
ver: rpa2
title: 'ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction'
arxiv_id: '2509.07594'
source_url: https://arxiv.org/abs/2509.07594
tags:
- prediction
- network
- data
- collaborative
- elec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ELEC, a framework that combines large language
  models (LLMs) with collaborative feature interaction models for click-through rate
  (CTR) prediction. The key idea is to adapt an LLM to process tabular features as
  text, inject its high-level representations into a collaborative model to form a
  "gain network," and then distill knowledge to a lightweight "vanilla network" that
  only uses tabular data but achieves comparable performance.
---

# ELEC: Efficient Large Language Model-Empowered Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2509.07594
- Source URL: https://arxiv.org/abs/2509.07594
- Reference count: 36
- Key result: ELEC achieves SOTA AUC (0.8015) on industry dataset while maintaining inference time independent of LLM size

## Executive Summary
This paper proposes ELEC, a framework that combines large language models (LLMs) with collaborative feature interaction models for click-through rate (CTR) prediction. The key innovation is injecting LLM-derived semantic representations into collaborative models, then distilling this knowledge to a lightweight tabular-only network for efficient serving. By processing tabular features as text and leveraging both semantic and collaborative signals, ELEC achieves state-of-the-art performance while avoiding LLM inference costs at serving time.

## Method Summary
ELEC operates through a two-stage process: first, an LLM processes tabular features converted to text strings, generating semantic representations that are transformed and injected into a collaborative CTR model (DCNv2) to form a "gain network." During joint training, this gain network is distilled to a "vanilla network" that uses only tabular data but achieves comparable performance through dual-level knowledge distillation (score-level CLID loss and representation-level MSE loss). At serving time, only the vanilla network is deployed, achieving the same effectiveness as the gain network without LLM inference overhead.

## Key Results
- Achieves state-of-the-art AUC of 0.8015 on industry dataset and competitive LogLoss across benchmarks
- Inference time of vanilla network remains constant (~0.1s) regardless of base LLM size (roberta-base to Qwen2-7B-instruct)
- Performance improvements are consistent across different collaborative model backbones (DCNv2, DeepFM, AutoInt)
- Dual-level distillation (CLID + MSE) is critical, with CLID loss having larger impact on Industry dataset

## Why This Works (Mechanism)

### Mechanism 1: Semantic-to-Collaborative Representation Injection
Injecting LLM-derived semantic representations into collaborative CTR models improves prediction accuracy by capturing signals missed by ID-based embeddings alone. The frozen LLM processes tabular features as text, with average pooling of hidden states transformed to CTR space and concatenated with collaborative model embeddings.

### Mechanism 2: Dual-Level Knowledge Distillation
Two-level distillation transfers knowledge from LLM-enhanced gain network to tabular-only vanilla network: score-level CLID loss enforces point-wise calibration and list-wise ranking alignment, while representation-level MSE loss forces matching high-level representations between networks.

### Mechanism 3: Inference-Time Decoupling from LLM Size
Vanilla network inference latency is independent of base LLM architecture because only the vanilla network (collaborative model only) serves traffic at deployment, avoiding LLM weight loading while preserving performance through successful knowledge transfer.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student)**: Core to ELEC's efficiency; understanding how soft labels transfer implicit knowledge is essential for grasping dual-level distillation design.
  - Quick check: Can you explain why matching teacher logits provides more information than hard labels alone?

- **Pseudo-Siamese Networks**: ELEC uses this pattern to allow asymmetric branches with different inputs and structures.
  - Quick check: How does a pseudo-siamese network differ from a standard siamese network in terms of weight sharing and input flexibility?

- **Feature Interaction Modeling in CTR**: Collaborative models rely on explicit/implicit feature interactions; the gain network injects LLM representations into this paradigm.
  - Quick check: What is the difference between explicit feature interactions (e.g., crossing layers in DCNv2) and implicit interactions (e.g., MLP in DNN)?

## Architecture Onboarding

- **Component map**: Tabular Features → Text Template → Frozen LLM → Average Pooling → MLP Transformation → Semantic Representation → DCNv2 + Semantic ⊕ → Gain Network → Prediction
  - Parallel path: Tabular Features → DCNv2 → Vanilla Network → Prediction

- **Critical path**:
  1. Verify text template conversion correctness (null handling, special characters)
  2. Confirm LLM weights frozen; only transformation + collaborative layers trainable
  3. Check distillation loss weighting (α balances L_rep); default not specified
  4. Ensure vanilla network exported for serving, not gain network

- **Design tradeoffs**:
  - LLM size vs. distillation quality: Larger LLMs improve teacher quality but require more GPU memory during training
  - Transformation layer depth: Paper uses [512, 256, 128]; deeper may overfit, shallower may under-transfer
  - α hyperparameter: Balances representation distillation; if too high, may force rigid imitation

- **Failure signatures**:
  - AUC gap (gain - vanilla) > 0.01: Distillation insufficient; increase training epochs or adjust α
  - Vanilla inference time scales with LLM: Bug in deployment; LLM accidentally loaded
  - Training OOM with large LLM: Reduce batch size or use gradient checkpointing
  - CLID loss NaN: Probability normalization issue (empty batches, all-zero predictions)

- **First 3 experiments**:
  1. Baseline parity check: Train vanilla network standalone vs. with distillation; expect AUC improvement from distillation
  2. Ablation on α: Sweep α ∈ {0.1, 0.5, 1.0, 2.0} to find representation loss balance point
  3. LLM backbone swap: Replace gte-Qwen2-7B with smaller model; verify inference time unchanged, expect AUC drop

## Open Questions the Paper Calls Out

### Open Question 1
Would adopting parameter-efficient fine-tuning (PEFT) for the base LLM (instead of freezing its weights) yield a significantly stronger "gain network" teacher for distillation?
- Basis: Authors explicitly freeze LLM weights without exploring PEFT benefits
- Why unresolved: Freezing may limit alignment with collaborative signals, capping distilled knowledge quality
- Evidence needed: Ablation comparing frozen vs. LoRA-finetuned LLM teacher performance

### Open Question 2
How sensitive is ELEC to the specific template used for converting tabular data to text?
- Basis: Paper uses simple "field_name is feature_value" template without evaluating alternatives
- Why unresolved: Different prompt formats might elicit different semantic representations from LLM