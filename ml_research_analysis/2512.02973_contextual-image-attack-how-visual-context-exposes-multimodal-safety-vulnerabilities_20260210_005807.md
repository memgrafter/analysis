---
ver: rpa2
title: 'Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities'
arxiv_id: '2512.02973'
source_url: https://arxiv.org/abs/2512.02973
tags:
- image
- text
- visual
- attack
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses jailbreak attacks on multimodal large language
  models (MLLMs), which have been less explored than those on text-only models. The
  proposed method, Contextual Image Attack (CIA), embeds harmful queries into visually
  coherent contexts using a multi-agent system, combining four visualization strategies,
  contextual element enhancements, and toxicity obfuscation.
---

# Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities

## Quick Facts
- arXiv ID: 2512.02973
- Source URL: https://arxiv.org/abs/2512.02973
- Reference count: 40
- The paper introduces CIA, a method that achieves 86.31% attack success rate against GPT-4o and 91.07% against Qwen2.5-VL-72B by embedding harmful queries in visually coherent contexts.

## Executive Summary
This paper addresses jailbreak attacks on multimodal large language models (MLLMs), which have been less explored than those on text-only models. The proposed method, Contextual Image Attack (CIA), embeds harmful queries into visually coherent contexts using a multi-agent system, combining four visualization strategies, contextual element enhancements, and toxicity obfuscation. Experiments on MMSafetyBench-tiny and SafeBench-tiny datasets show that CIA significantly outperforms prior baselines, achieving high attack success rates and demonstrating the vulnerability of MLLMs to image-centric jailbreak attacks.

## Method Summary
The Contextual Image Attack (CIA) method uses a multi-agent system to generate adversarial images that embed harmful queries within visually coherent contexts. The approach leverages four visualization strategies—image manipulation, web page creation, mobile interface design, and photo enhancement—to craft attack images. The system employs GPT-4V to generate image descriptions and refine them for toxicity obfuscation, ensuring the harmful content is concealed within plausible visual elements. Contextual elements are enhanced using CLIP embeddings to maintain coherence, and the attack process iterates to maximize success rates while avoiding detection by MLLM safety mechanisms.

## Key Results
- CIA achieves attack success rates of 86.31% against GPT-4o and 91.07% against Qwen2.5-VL-72B.
- The method outperforms prior baselines in both attack success and toxicity scores on MMSafetyBench-tiny and SafeBench-tiny datasets.
- Demonstrated effectiveness of image-centric jailbreak attacks on MLLMs, highlighting significant safety vulnerabilities.

## Why This Works (Mechanism)
CIA exploits the multimodal nature of MLLMs by embedding harmful queries within visually coherent contexts, bypassing text-only safety filters. The multi-agent system leverages GPT-4V to generate and refine image descriptions, ensuring the malicious content is concealed while maintaining visual plausibility. By combining visualization strategies and contextual enhancements, CIA manipulates the model's interpretation of visual and textual cues, leading to successful jailbreaks.

## Foundational Learning
- Multimodal jailbreak attacks: Combining visual and textual adversarial inputs to bypass safety mechanisms in MLLMs.
- *Why needed:* Text-only jailbreaks are less effective on MLLMs due to their integrated visual understanding.
- *Quick check:* Verify if the attack succeeds when visual context is removed or altered.
- CLIP embeddings: Used to align visual and textual features for context coherence.
- *Why needed:* Ensures the generated images are visually consistent and less likely to trigger safety filters.
- *Quick check:* Test image coherence by comparing CLIP similarity scores with and without contextual elements.
- Multi-agent systems: Employ multiple AI agents to iteratively refine and optimize attack strategies.
- *Why needed:* Allows for dynamic adaptation and refinement of adversarial inputs.
- *Quick check:* Evaluate the impact of removing the multi-agent coordination on attack success rates.

## Architecture Onboarding

### Component Map
GPT-4V -> Image Description Generator -> Contextual Enhancer -> Visualization Strategy Selector -> Adversarial Image Generator -> MLLM Safety Filter

### Critical Path
The critical path involves generating an initial image description, refining it for toxicity obfuscation, enhancing contextual elements using CLIP embeddings, selecting an appropriate visualization strategy, and producing the final adversarial image. Each step iteratively refines the input to maximize attack success while maintaining visual coherence.

### Design Tradeoffs
- **Visualization strategies:** Balancing between visual plausibility and attack effectiveness; more complex contexts may increase success but also risk detection.
- **Contextual enhancements:** Tradeoff between coherence and obfuscation; overly coherent images may reduce toxicity obfuscation.
- **Iterative refinement:** Increases attack success but requires more computational resources and time.

### Failure Signatures
- **Low attack success:** Indicates insufficient toxicity obfuscation or poor contextual alignment.
- **Detection by safety filters:** Suggests overly conspicuous adversarial elements or weak visual coherence.
- **Poor visual plausibility:** Results from inadequate contextual enhancements or inappropriate visualization strategies.

### 3 First Experiments
1. Test attack success rates with and without CLIP-based contextual enhancements.
2. Evaluate the impact of removing toxicity obfuscation on attack success and detection rates.
3. Compare attack success across different visualization strategies to identify the most effective approach.

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to GPT-4o and Qwen2.5-VL-72B, raising questions about generalizability to other MLLMs.
- No evaluation of potential defenses or mitigation strategies against CIA attacks.
- Limited testing in real-world deployment settings, leaving practical exploitability uncertain.

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| CIA outperforms prior baselines on tested datasets | High |
| MLLMs are vulnerable to image-centric jailbreak attacks | Medium |
| Practical exploitability of vulnerabilities in real-world applications | Low |

## Next Checks
1. Test CIA against a broader range of MLLMs, including models with different architectures or safety mechanisms, to assess generalizability.
2. Evaluate the robustness of CIA-generated attacks under adversarial defenses or safety filters commonly deployed in production systems.
3. Investigate the transferability of CIA attacks across different datasets and real-world visual contexts to determine practical exploitability.