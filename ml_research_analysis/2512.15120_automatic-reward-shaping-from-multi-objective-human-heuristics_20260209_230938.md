---
ver: rpa2
title: Automatic Reward Shaping from Multi-Objective Human Heuristics
arxiv_id: '2512.15120'
source_url: https://arxiv.org/abs/2512.15120
tags:
- reward
- task
- performance
- exploration
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MORSE, a framework that automatically learns
  reward weights from human-designed heuristics in multi-objective RL tasks. The method
  formulates reward shaping as a bi-level optimization problem, combining policy training
  in the inner loop with reward weight updates in the outer loop.
---

# Automatic Reward Shaping from Multi-Objective Human Heuristics

## Quick Facts
- arXiv ID: 2512.15120
- Source URL: https://arxiv.org/abs/2512.15120
- Authors: Yuqing Xie; Jiayu Chen; Wenhao Tang; Ya Zhang; Chao Yu; Yu Wang
- Reference count: 40
- Key outcome: MORSE framework automatically learns reward weights from human heuristics in multi-objective RL, matching oracle performance across 2-9 objective tasks in MuJoCo and Isaac Sim

## Executive Summary
This paper introduces MORSE, a framework that automatically learns reward weights from human-designed heuristics in multi-objective RL tasks. The method formulates reward shaping as a bi-level optimization problem, combining policy training in the inner loop with reward weight updates in the outer loop. To avoid local optima, MORSE incorporates stochastic exploration guided by task performance and novelty scores computed via Random Network Distillation. In experiments across MuJoCo and Isaac Sim environments with tasks ranging from 2 to 9 objectives, MORSE consistently outperforms vanilla bi-level optimization and matches the performance of human-tuned oracle rewards. Ablation studies confirm the importance of each component, including performance-based exploration and policy resetting during weight updates.

## Method Summary
MORSE addresses the challenge of reward shaping in multi-objective reinforcement learning by formulating it as a bi-level optimization problem. The inner loop optimizes the policy given current reward weights, while the outer loop updates the reward weights based on policy performance. To avoid local optima, MORSE introduces stochastic exploration using Random Network Distillation (RND) to compute novelty scores alongside task performance metrics. The framework also employs policy resetting during weight updates to maintain exploration diversity. Experiments demonstrate MORSE's effectiveness across various MuJoCo and Isaac Sim environments with 2-9 objectives, showing it can match or exceed human-tuned oracle performance while maintaining computational efficiency.

## Key Results
- MORSE matches human-tuned oracle reward performance across 2-9 objective tasks in MuJoCo and Isaac Sim environments
- The framework consistently outperforms vanilla bi-level optimization approaches in multi-objective RL settings
- Ablation studies confirm the importance of both performance-based exploration and policy resetting during weight updates

## Why This Works (Mechanism)
The MORSE framework succeeds by addressing the core challenge of reward shaping in multi-objective RL through a principled bi-level optimization approach. By separating policy optimization from reward weight updates, MORSE can effectively explore the weight space while maintaining strong policy performance. The incorporation of stochastic exploration via RND provides a principled way to balance exploitation of known good weights with exploration of novel weight combinations, helping avoid local optima that plague simpler approaches. The policy resetting mechanism ensures that weight updates can be evaluated from a consistent starting point, preventing policy adaptation from confounding the reward weight learning process.

## Foundational Learning
**Bi-level optimization**: Needed to separate the policy learning (inner loop) from reward weight updates (outer loop). Quick check: Verify that the inner loop converges before updating weights in the outer loop.
**Random Network Distillation (RND)**: Used to compute novelty scores for guiding stochastic exploration. Quick check: Monitor RND prediction error to ensure it captures meaningful state space novelty.
**Multi-objective reward formulation**: Critical for handling tasks with multiple competing objectives. Quick check: Verify that all objectives contribute meaningfully to the final reward signal.
**Policy gradient methods**: Required for the inner-loop policy optimization. Quick check: Monitor policy gradient magnitudes to ensure stable learning.
**Performance metrics aggregation**: Needed to combine multiple objectives into a single optimization target. Quick check: Verify that the aggregation method preserves important trade-offs between objectives.

## Architecture Onboarding

Component map: Human heuristics -> Reward weight space -> Policy training -> Performance metrics -> RND novelty scores -> Weight update -> Policy reset

Critical path: Reward weight initialization → Policy training (inner loop) → Performance evaluation → RND novelty computation → Weight update (outer loop) → Policy reset → Repeat

Design tradeoffs: The bi-level structure trades computational efficiency for more stable reward weight learning. RND-based exploration trades some policy performance for better weight space exploration. Policy resetting trades training continuity for more reliable weight evaluation.

Failure signatures: Converging to poor local optima, slow convergence due to excessive exploration, policy collapse during weight updates, reward weights diverging without improving task performance.

First experiments:
1. Validate that policy performance improves monotonically with iterations on a simple 2-objective task
2. Test RND novelty detection on a known exploration problem (e.g., sparse reward maze)
3. Compare weight convergence speed with and without policy resetting

## Open Questions the Paper Calls Out
None

## Limitations
- Bi-level optimization introduces significant computational overhead from repeated policy training
- RND-based novelty exploration may not scale well to very high-dimensional state spaces
- Performance comparisons are limited to vanilla bi-level optimization and human-tuned oracles
- Transfer to real-world physical systems remains untested

## Confidence

| Claim | Confidence |
|-------|------------|
| MORSE effectively learns reward weights that match oracle performance | High |
| The bi-level optimization formulation is sound and properly implemented | Medium |
| Stochastic exploration via RND provides meaningful improvement over deterministic approaches | Medium |
| Generalizability to non-simulation environments and real-world tasks | Low |

## Next Checks

1. Benchmark MORSE against additional state-of-the-art multi-objective RL methods beyond vanilla bi-level optimization
2. Conduct ablation studies on the frequency and duration of policy resets during weight updates to quantify computational overhead
3. Test MORSE on a real-world robotics task or physical system to evaluate transfer beyond simulation environments