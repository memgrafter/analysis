---
ver: rpa2
title: 'TASE: Token Awareness and Structured Evaluation for Multilingual Language
  Models'
arxiv_id: '2508.05468'
source_url: https://arxiv.org/abs/2508.05468
tags:
- tasks
- token
- task
- performance
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TASE, a comprehensive multilingual benchmark
  designed to evaluate the fine-grained capabilities of large language models, focusing
  on token-level awareness and structural reasoning across Chinese, English, and Korean.
  TASE addresses the gap in existing evaluations, which largely overlook basic, low-level
  text processing skills like character counting, token manipulation, and structural
  analysis.
---

# TASE: Token Awareness and Structured Evaluation for Multilingual Language Models

## Quick Facts
- **arXiv ID:** 2508.05468
- **Source URL:** https://arxiv.org/abs/2508.05468
- **Reference count:** 7
- **Primary result:** All models fall significantly short of human-level performance on token-level and structural reasoning tasks

## Executive Summary
This paper introduces TASE, a comprehensive multilingual benchmark designed to evaluate the fine-grained capabilities of large language models, focusing on token-level awareness and structural reasoning across Chinese, English, and Korean. TASE addresses the gap in existing evaluations, which largely overlook basic, low-level text processing skills like character counting, token manipulation, and structural analysis. The benchmark consists of 10 tasks grouped into token awareness and structural understanding, with a dataset of 35,928 instances and a scalable synthetic data generation pipeline for training.

Experiments on over 30 leading models reveal that all models fall significantly short of human-level performance, particularly on tasks requiring visual recognition or complex structural reasoning. Even top-tier models like O3 and Claude Opus 4 show persistent weaknesses, with average accuracy around 50-60%, compared to human performance of 89.24%. The results highlight the "tokenizer blindness" hypothesis, indicating that models struggle with tasks that depend on character-level or visual information due to their reliance on subword tokenization. Fine-tuning a custom Qwen2.5-14B model using GRPO training improved performance, demonstrating that targeted training can help close the gap, but human-level accuracy remains elusive.

## Method Summary
The authors developed TASE as a comprehensive multilingual benchmark with 10 distinct tasks categorized into token awareness (character counting, substring removal, etc.) and structural understanding (bracket matching, balanced parentheses, etc.). The benchmark was designed to evaluate low-level text processing capabilities that existing benchmarks typically overlook. The dataset comprises 35,928 instances generated through a synthetic pipeline to ensure precise control over task difficulty and eliminate ambiguity. The evaluation covers 30+ leading language models across multiple model families, with human performance established through 15 participants. A custom Qwen2.5-14B model was fine-tuned using GRPO training on 10,000 examples per task to demonstrate the benchmark's utility for model improvement.

## Key Results
- All models tested fall significantly short of human performance, averaging 50-60% accuracy versus human 89.24%
- Top-tier models like O3 and Claude Opus 4 still show persistent weaknesses on token-level and structural tasks
- Fine-tuned Qwen2.5-14B model showed improvement but did not reach human-level performance
- Tasks requiring visual recognition or complex structural reasoning were particularly challenging for models

## Why This Works (Mechanism)

## Foundational Learning
- **Tokenization awareness**: Understanding how models process text at the subword level vs character level - needed because models' reliance on tokenization affects their ability to perform character-level tasks; quick check: test models on pure character counting tasks
- **Multilingual structural reasoning**: Ability to handle nested structures and balanced pairs across different writing systems - needed because structural patterns vary across languages; quick check: evaluate on balanced parentheses across Chinese, English, and Korean
- **Visual character recognition**: Capacity to identify and manipulate characters as visual units - needed because models may treat characters purely as tokens rather than visual entities; quick check: test on character counting in mixed-script text
- **Synthetic data generation**: Creating controlled, unambiguous evaluation examples - needed to eliminate context and ambiguity that could confound task-specific capabilities; quick check: verify generated data meets task requirements without edge cases

## Architecture Onboarding

**Component map:** Data Generation -> Task Definition -> Model Evaluation -> Fine-tuning Pipeline

**Critical path:** Synthetic data generation provides clean input → Task categorization ensures comprehensive coverage → Model evaluation establishes baselines → Fine-tuning demonstrates benchmark utility

**Design tradeoffs:** 
- Synthetic data ensures control and eliminates ambiguity but lacks ecological validity
- English-only instructions enable consistency but may disadvantage non-English models
- Accuracy-only metrics provide clear measurement but ignore response quality and efficiency

**Failure signatures:** 
- Models show "tokenizer blindness" on character-level tasks
- Performance drops significantly on visual recognition tasks
- Complex structural reasoning tasks reveal limitations in multi-step processing
- Multilingual models perform worse on non-English scripts despite training

**First 3 experiments to run:**
1. Evaluate models on naturally occurring multilingual text to test ecological validity
2. Compare performance when instructions are provided in target languages vs English
3. Test model robustness by introducing controlled noise and ambiguity into synthetic data

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data lacks ecological validity and real-world context, potentially biasing results toward easier cases
- Human baseline established with only 15 participants may not represent optimal human performance
- Benchmark uses English-only instructions, which may disadvantage non-English models

## Confidence
- **High confidence:** Current models significantly underperform humans on token-level and structural tasks across 30+ models
- **Medium confidence:** "Tokenizer blindness" hypothesis explains model weaknesses but requires further investigation
- **Medium confidence:** Benchmark design and task categorization are methodologically sound despite synthetic data limitations

## Next Checks
1. Conduct a follow-up study evaluating models on naturally occurring text samples from real-world multilingual corpora to assess ecological validity of TASE results.
2. Expand human baseline evaluation with larger participant pools (n≥50) and include time-on-task measurements to better understand human performance limits.
3. Test models with task instructions provided in their respective target languages (Chinese, Korean, English) rather than English-only instructions to evaluate potential instruction language bias.