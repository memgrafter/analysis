---
ver: rpa2
title: 'ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact
  Synthesis'
arxiv_id: '2509.08188'
source_url: https://arxiv.org/abs/2509.08188
tags:
- diffusion
- arxiv
- artifact
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks conditional WGAN-GP with projection discriminator
  against a denoising diffusion model (1D U-Net with FiLM conditioning) for generating
  label-aware EEG artifact segments. Using the TUAR corpus, subject-wise splits, and
  multi-channel 1-2s windows, the study finds WGAN-GP achieves closer spectral alignment
  (lower relative band-power errors, MMD) than the diffusion baseline, though both
  models exhibit detectable temporal/cross-channel discrepancies.
---

# ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis

## Quick Facts
- **arXiv ID**: 2509.08188
- **Source URL**: https://arxiv.org/abs/2509.08188
- **Reference count**: 40
- **Primary result**: Conditional WGAN-GP with projection discriminator achieves better spectral alignment and lower MMD than 1D U-Net diffusion for label-aware EEG artifact synthesis, though both models show weak class-conditional recovery and temporal/cross-channel fidelity gaps.

## Executive Summary
This work benchmarks conditional WGAN-GP with projection discriminator against a denoising diffusion model for generating label-aware EEG artifact segments. Using the TUAR corpus with subject-wise splits and multi-channel 1-2s windows, the study finds WGAN-GP achieves closer spectral alignment (lower relative band-power errors, MMD) than the diffusion baseline, though both models exhibit detectable temporal/cross-channel discrepancies. Diffusion used z-score normalization, longer windows, and limited sampling steps, contributing to weaker fidelity. The evaluation suite—including Welch band-power deltas, covariance/ACF distances, MMD, and downstream augmentation—emphasizes metrics aligned with EEG structure over image heuristics. WGAN-GP's better spectral realism highlights the importance of conditioning, normalization, and architectural design, while also revealing room for improvement in capturing artifact-specific spatial and temporal patterns. Code and reproducible configurations are released to enable rigorous benchmarking.

## Method Summary
The study compares conditional WGAN-GP with projection discriminator to a 1D U-Net diffusion model for generating label-aware EEG artifacts. Data comes from the TUAR corpus, using 8-channel montage (Fp1,Fp2,C3,C4,O1,O2,T3,T4) at 250 Hz. Subject-wise splits yield 149 train / 32 val / 32 test. WGAN-GP uses 1s windows (250 samples) with per-window min-max normalization to [-1,1]; diffusion uses 2s windows (500 samples) with per-recording per-channel z-score. Both models condition on 5 artifact classes via learned embeddings (projection discriminator for WGAN-GP, FiLM for diffusion). Training uses Adam/AdamW with specified hyperparameters; diffusion employs classifier-free guidance and EMA. Evaluation includes spectral metrics (band-power errors, MMD), temporal/cross-channel structure (covariance/ACF distances), and downstream augmentation utility.

## Key Results
- WGAN-GP achieves lower relative band-power errors and MMD to real data than diffusion across all frequency bands.
- Both models show weak class-conditional recovery, limiting downstream augmentation utility.
- Diffusion's z-score normalization, longer windows, and limited sampling steps (50-80) contribute to weaker spectral fidelity.
- WGAN-GP's projection discriminator provides better class-conditional alignment than diffusion's FiLM conditioning.
- Temporal autocorrelation and cross-channel covariance show non-trivial discrepancies in both models.

## Why This Works (Mechanism)

### Mechanism 1: Projection Discriminator for Conditional Alignment
The projection discriminator improves label-conditional spectral alignment by directly injecting class embeddings via inner product with penultimate features (D(x,y) = w^T φ(x) + ⟨φ(x), e_y⟩), shaping the decision boundary per class rather than relying on auxiliary classifiers. This works when artifact classes are distinguishable in learned feature space, but may fail if classes are not linearly separable in φ(x) or mode collapse occurs.

### Mechanism 2: Normalization Strategy as Implicit Spectral Regularizer
Per-window min-max normalization contributes to tighter spectral fidelity by normalizing local amplitude dynamics uniformly, reducing variance in critic input distribution. This differs from z-score's recording-level statistics that introduce inter-window heterogeneity potentially destabilizing training. The mechanism assumes spectral structure is preserved under min-max rescaling and artifacts are locally bounded, but may over-normalize if artifacts span multiple amplitude scales within windows.

### Mechanism 3: Sampling Steps and Guidance Scale Trade-offs
Limited diffusion sampling steps (50-80) with fixed guidance scale produce spectral drift and higher band-power errors because classifier-free guidance amplifies conditional signal but insufficient denoising iterations leave residual noise distorting high-frequency content. The v-prediction parameterization with 50 steps may not fully recover spectral structure, especially when guidance scale is aggressive. Increasing steps or using higher-order samplers could improve fidelity.

## Foundational Learning

- **Wasserstein GAN with Gradient Penalty (WGAN-GP)**: Stabilizes adversarial training by enforcing soft Lipschitz constraint via gradient penalty (||∇_x̂ D(x̂)||_2 − 1)^2, preventing gradient explosion/vanishing in critic. Quick check: Why does gradient penalty stabilize training compared to weight clipping?

- **Projection Discriminator for Conditional GANs**: Directly injects class labels into critic via inner product with features, improving conditional fidelity without auxiliary classifiers. Quick check: How does ⟨φ(x), e_y⟩ differ from concatenating y as an input channel?

- **Classifier-Free Guidance (CFG) in Diffusion**: Enables controllable synthesis without separate classifier by interpolating between conditional and unconditional predictions, trading diversity for fidelity via guidance scale. Quick check: What happens to sample diversity as guidance scale increases from 1.0 to 3.0?

## Architecture Onboarding

- **Component map**: Latent z ~ N(0, I_128) + one-hot class → WGAN-GP generator (transposed-conv 1D net) → C×T output → projection discriminator (1D ConvNet → global avg pool → linear head + class embedding inner product) → WGAN-GP critic. For diffusion: raw EEG → 1D U-Net (64→128→256 channels) with FiLM conditioning on timestep + class embeddings → residual blocks with GroupNorm → noise prediction → denoising.

- **Critical path**: Load TUAR → extract windows with overlap ρ=0.5 → apply per-window min-max (WGAN) or per-recording z-score (diffusion) → train WGAN-GP (n_critic=5, λ_gp=10, Adam β1=0.5) → train diffusion (AdamW, linear β schedule T=1000, EMA decay 0.999) → sample (WGAN-GP best checkpoint vs Diffusion EMA, 80-step DDIM, CFG scale 1.5) → evaluate (Welch band-power deltas, MMD, covariance/ACF distances, PRD, downstream kNN recovery).

- **Design tradeoffs**: 1s (WGAN) vs 2s (diffusion) windows—shorter emphasizes local dynamics but may miss long-range structure; per-window min-max (stable critic input) vs per-recording z-score (preserves recording context); WGAN-GP ~2 min vs Diffusion ~6 min for 15k samples (scales with steps); projection discriminator (direct class signal) vs CFG (tunable but sensitive to scale/steps).

- **Failure signatures**: High band-power deltas (>20%) indicate normalization mismatch or incorrect PSD calculation; PRD degeneracy (near 0 or 1) suggests feature encoder doesn't capture EEG structure; weak class-conditional recovery (kNN accuracy near chance) indicates insufficient conditioning; temporal/ACF discrepancy shows model not capturing autocorrelation.

- **First 3 experiments**: 1) Ablate normalization: train WGAN-GP with z-score and diffusion with min-max to isolate normalization effect on band-power deltas. 2) Scale sampling steps: run diffusion at 50, 100, 200, 500 steps with fixed CFG=1.5; plot MMD vs steps to identify diminishing returns. 3) Conditional recovery stress test: train lightweight classifier on real artifacts; evaluate on synthetic per class; report per-class accuracy gaps to surface underperforming artifact types.

## Open Questions the Paper Calls Out

### Open Question 1
Can a diffusion model with unified preprocessing (per-window min–max, 1–2s windows), EDM-style parameterization, and higher-order samplers (e.g., DPM-Solver, 200+ steps) match or exceed WGAN-GP's spectral fidelity (band-power errors, MMD) for EEG artifact synthesis? This remains unresolved because the current comparison confounds normalization, window length, and sampling budget; diffusion baseline was not optimally configured. Evidence would come from controlled ablation with identical preprocessing, 200-500 sampling steps with DPM-Solver/EDM, and reporting band-power deltas (δ–γ), MMD, and covariance Frobenius distances.

### Open Question 2
Would explicit physiology-aware loss terms—multi-resolution STFT spectral losses and cross-channel coherency constraints—reduce the temporal and spatial discrepancies observed in both models (ACF L2, covariance Frobenius)? This remains unresolved because current models rely on time-domain losses while residual channel-wise drifts and non-trivial covariance/ACF errors indicate unmodeled montage structure. Evidence would come from ablation adding STFT and imaginary-coherency losses and measuring reductions in covariance Frobenius distance and ACF L2 across artifact classes.

### Open Question 3
Can stronger conditioning mechanisms (classifier guidance, tuned CFG, multi-label embeddings) achieve reliable class-conditional recovery for artifact types, given current weak specificity? This remains unresolved because current classifier-free guidance and projection discriminator fail to produce class-distinct samples detectable by lightweight kNN/classifiers. Evidence would come from conditioning ablations with classifier guidance, varied CFG scales/dropout, and reporting class-stratified kNN recovery accuracy and downstream augmentation gains.

### Open Question 4
Does synthetic artifact augmentation yield measurable improvements on clinical downstream tasks (e.g., seizure detection false-alarm reduction) compared to real-data-only baselines? This remains unresolved because utility tests were inconclusive due to weak conditional recovery; real clinical benefit remains unquantified. Evidence would come from training seizure detectors on augmented vs. real-only data and reporting false-alarm rates and AUPRC on held-out test sets with confidence intervals and subject-wise bootstraps.

## Limitations
- Findings are based on single dataset (TUAR) and fixed artifact taxonomy (5 classes), limiting generalizability to other EEG datasets or artifact types.
- Diffusion model was configured with non-standard choices (per-recording z-score, 2s windows, limited sampling steps) that may have systematically disadvantaged it.
- Evaluation suite lacks perceptual quality metrics or human-in-the-loop assessment, which could reveal misalignments between quantitative and qualitative fidelity.
- Downstream augmentation benchmark is minimal, limiting conclusions about practical utility of synthetic artifacts.

## Confidence
- **High**: Spectral alignment advantage of WGAN-GP over diffusion (based on multiple metrics: relative band-power errors, MMD, PSD L2).
- **Medium**: Projection discriminator's role in improving class-conditional fidelity (supported by architectural design but lacks ablation within this study).
- **Medium**: Normalization strategy impact on spectral stability (inferred from preprocessing differences and spectral results, but not ablated).
- **Low**: Generalizability of findings to other EEG datasets, artifact types, or generative architectures (not tested).

## Next Checks
1. **Ablation of normalization strategy**: Train both models with swapped normalization (min-max for diffusion, z-score for WGAN-GP) to isolate its effect on spectral fidelity and downstream utility.
2. **Sampling step scaling**: Systematically vary diffusion sampling steps (50→500) and CFG scale to map fidelity vs. computational cost, and identify optimal settings.
3. **Class-conditional recovery stress test**: Train a lightweight classifier on real artifacts and evaluate on synthetic per class; report per-class accuracy gaps to identify which artifact types are poorly modeled.