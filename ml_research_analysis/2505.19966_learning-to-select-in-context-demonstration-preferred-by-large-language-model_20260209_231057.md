---
ver: rpa2
title: Learning to Select In-Context Demonstration Preferred by Large Language Model
arxiv_id: '2505.19966'
source_url: https://arxiv.org/abs/2505.19966
tags:
- arxiv
- demonstrations
- learning
- demonstration
- genicl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GENICL, a generative preference learning
  framework for optimizing demonstration selection in in-context learning (ICL). The
  key idea is to reformulate ICL as a generative Bayesian optimization problem using
  a latent variable to bridge demonstration selection and LLM inference.
---

# Learning to Select In-Context Demonstration Preferred by Large Language Model

## Quick Facts
- **arXiv ID:** 2505.19966
- **Source URL:** https://arxiv.org/abs/2505.19966
- **Reference count:** 24
- **Primary result:** GENICL achieves 63.9% accuracy on PAWS vs 57.7% (EPR) and 57.5% (LLM-R)

## Executive Summary
This paper introduces GENICL, a generative preference learning framework for optimizing demonstration selection in in-context learning (ICL). The method reformulates ICL as a generative Bayesian optimization problem using a latent variable to bridge demonstration selection and LLM inference. By leveraging LLM feedback through preference learning, GENICL directly optimizes demonstration selection based on relative effectiveness between demonstrations. The approach demonstrates significant improvements over existing methods across 19 datasets spanning 11 task categories.

## Method Summary
GENICL employs a generative preference learning framework that treats demonstration selection as a Bayesian optimization problem. The core innovation lies in using a latent variable to connect demonstration selection with LLM inference, enabling direct optimization through LLM feedback. The framework focuses on relative effectiveness between demonstrations rather than absolute performance metrics. Through iterative refinement guided by LLM preferences, the system learns to select demonstrations that maximize task performance across diverse scenarios.

## Key Results
- Achieves 63.9% accuracy on PAWS dataset, outperforming EPR (57.7%) and LLM-R (57.5%)
- Demonstrates strong performance across 19 datasets spanning 11 task categories
- Shows good generalizability across different LLM sizes and tasks
- Selected demonstrations are reusable across different models

## Why This Works (Mechanism)
The framework works by reframing ICL as a generative Bayesian optimization problem where a latent variable connects demonstration selection to LLM inference. This enables direct optimization through preference learning based on LLM feedback, focusing on relative demonstration effectiveness rather than absolute metrics. The generative approach allows for systematic exploration of the demonstration space while the preference learning component provides fine-grained feedback signals that guide selection toward more effective demonstrations.

## Foundational Learning
- **In-context learning (ICL):** Understanding how LLMs use demonstrations without parameter updates - critical for grasping the problem space GENICL addresses
- **Bayesian optimization:** Key for understanding the optimization framework used to search the demonstration space efficiently
- **Preference learning:** Essential for understanding how LLM feedback is leveraged to guide demonstration selection
- **Latent variable models:** Important for understanding how the generative component bridges selection and inference
- **Demonstration quality vs quantity trade-off:** Critical for understanding the optimization objectives

## Architecture Onboarding

**Component Map:** Demonstration Selector -> LLM Inference Engine -> Preference Evaluator -> Latent Variable Generator -> (feedback loop)

**Critical Path:** Demonstration selection → LLM inference → preference evaluation → latent variable generation → refined demonstration selection

**Design Tradeoffs:** The framework trades computational overhead for improved demonstration quality. The preference learning loop adds latency but enables more precise selection. The latent variable approach provides generality but requires careful calibration across different task types.

**Failure Signatures:** Poor demonstration selection may manifest as slow convergence, high variance in LLM outputs, or preference evaluator confusion. The latent variable generation may fail if demonstrations are too dissimilar or if task boundaries are unclear.

**First Experiments:**
1. Test single-task demonstration selection on simple classification datasets
2. Evaluate cross-task demonstration reusability on structurally similar tasks
3. Benchmark computational overhead against traditional heuristic-based selection methods

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational overhead may limit practical deployment at scale
- Limited ablation studies don't fully explore individual component contributions
- Evaluation primarily on curated datasets; real-world robustness untested
- Scalability to very large language models not fully characterized

## Confidence
- **Core claims (performance improvements):** Medium - supported by empirical results but limited ablation analysis
- **Generalizability across LLM sizes:** Medium - partially supported but needs broader testing
- **Reusability of selected demonstrations:** Medium - demonstrated but not extensively validated
- **Scalability claims:** Low - insufficient empirical characterization of computational requirements

## Next Checks
1. Conduct scalability experiments measuring wall-clock time and resource utilization when scaling from 7B to 70B+ parameter models, including detailed analysis of the preference learning loop overhead.

2. Perform cross-task generalization studies where demonstrations selected for one task family (e.g., sentiment analysis) are evaluated on structurally similar but distinct tasks to validate the reusability claims.

3. Implement and evaluate the method on noisy, real-world datasets with label errors and class imbalance to test robustness beyond the curated datasets used in the current study.