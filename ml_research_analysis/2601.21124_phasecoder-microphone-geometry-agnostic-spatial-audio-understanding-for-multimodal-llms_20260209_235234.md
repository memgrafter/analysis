---
ver: rpa2
title: 'PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal
  LLMs'
arxiv_id: '2601.21124'
source_url: https://arxiv.org/abs/2601.21124
tags:
- audio
- spatial
- distance
- degrees
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhaseCoder introduces a transformer-only spatial audio encoder
  that is agnostic to microphone geometry, addressing the limitation that current
  multimodal LLMs process audio as a mono stream, ignoring spatial information. By
  taking raw multichannel audio and microphone coordinates as inputs, PhaseCoder performs
  localization and produces robust spatial embeddings, enabling an LLM to perform
  complex spatial reasoning and targeted transcription tasks from arbitrary microphone
  arrays.
---

# PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs

## Quick Facts
- arXiv ID: 2601.21124
- Source URL: https://arxiv.org/abs/2601.21124
- Reference count: 40
- Primary result: Achieves 86.96% azimuth accuracy on LOCATA with geometry-agnostic spatial audio encoding

## Executive Summary
PhaseCoder introduces a transformer-only spatial audio encoder that processes raw multichannel audio and microphone coordinates to produce geometry-agnostic spatial embeddings. The system addresses the limitation that current multimodal LLMs process audio as mono streams, ignoring spatial information. By encoding phase differences between microphones and using phase-modulated positional embeddings, PhaseCoder enables an LLM to perform complex spatial reasoning and targeted transcription tasks from arbitrary microphone arrays without retraining for specific geometries.

## Method Summary
PhaseCoder is a 5-block transformer encoder (6M parameters) that takes raw multichannel audio (3-8 mics) and microphone coordinates as inputs. It extracts magnitude and phase from STFT (256-sample window, 50% overlap), concatenates them into 258-dim features, and projects to 256-dim patches. Three positional embeddings (sequential, frame-based, and phase-modulated microphone coordinates) are added before feeding to the transformer. The encoder outputs spatial tokens via a [CLS] token processed by a 2-layer MLP. For LLM integration, these spatial tokens are prepended to Gemma 3n audio tokens using a 2-layer MLP projector. Training uses a two-stage curriculum (670k steps clean, 30k steps noisy) followed by 5-stage LLM fine-tuning.

## Key Results
- PhaseCoder achieves 86.96% azimuth accuracy and 7.44° MAE on the challenging LOCATA dataset without real training data
- When integrated with Gemma 3n, the model achieves 76.76% accuracy on yes/no spatial reasoning questions
- Synthetic targeted transcription task achieves 10.63% WER, outperforming a fine-tuned Gemini 1.5 Flash baseline

## Why This Works (Mechanism)

### Mechanism 1: Phase-Difference Encoding for Spatial Localization
Small phase differences between microphone channels encode spatial direction information that transformers can learn to decode. Raw multichannel audio → STFT → magnitude + phase concatenation → projection to patches. Phase captures inter-microphone time delays that correlate with direction-of-arrival.

### Mechanism 2: Geometry-Agnostic Encoding via Phase-Modulated Positional Embeddings
Encoding microphone positions as phase-modulated embeddings allows the transformer to generalize across arbitrary array geometries without retraining. Microphone coordinates → spherical coordinates → phase-modulated positional embedding using cos/sin functions with radius, elevation, azimuth.

### Mechanism 3: Spatial Token Injection with Curriculum Alignment
Prepending spatial tokens to mono audio tokens with temporal alignment enables LLMs to correlate spatial and semantic information. PhaseCoder produces spatial tokens at 160ms hop (matching Gemma's audio tokens), mapped 256→2048 dim via MLP, wrapped with BSA/ESA tokens, prepended to audio tokens.

## Foundational Learning

- **STFT and Phase Analysis**: Understanding how magnitude+phase extraction enables spatial encoding; why 256-sample window at 16kHz produces 129 frequency bins. Quick check: Why concatenate both magnitude and phase rather than using phase alone?

- **ViT-Style Patch Embeddings with [CLS] Tokens**: PhaseCoder follows ViT architecture—understanding how patches are formed, flattened, and aggregated via [CLS] token is essential. Quick check: What is the role of the [CLS] token in producing the final spatial embedding?

- **LoRA Fine-Tuning**: LLM integration uses LoRA (rank=8, α=16) rather than full fine-tuning; understanding parameter efficiency tradeoffs. Quick check: Why use LoRA for Gemma while training the spatial projection layer from scratch?

## Architecture Onboarding

- **Component map**: Raw C-channel audio + mic coordinates → STFT → magnitude/phase concat → projection (258→256) → sequential + frame + mic positional embeddings → 5-layer transformer → [CLS] token → 2-layer MLP → spatial embedding → projection (256→2048) → BSA/ESA tokens → prepended to Gemma audio tokens

- **Critical path**: Verify STFT parameters produce expected frame count; validate microphone coordinate transformation to spherical; confirm spatial token temporal alignment with Gemma's 160ms resolution; check classification head bin discretization.

- **Design tradeoffs**: Classification vs regression (limits fine-grained precision but stable training); synthetic-only training (enables geometry diversity but may miss real-world acoustic effects); distance estimation inherently underdetermined (relies on DRR, T60).

- **Failure signatures**: 3-microphone arrays (accuracy collapses); distance-based reasoning questions (lower accuracy than angular); transcript leakage in multi-speaker (WER inflated by hallucinations); large model (34M params) did not converge.

- **First 3 experiments**: 
  1. Ablation on microphone positional embedding type (phase-modulated vs learned) on held-out geometry.
  2. Temporal resolution sensitivity test (80ms, 160ms, 320ms hops).
  3. Real-world transfer check: synthetic-only training evaluated on LOCATA without real data.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating device-specific acoustic transfer functions (such as head-related transfer functions or enclosure diffraction effects) as additional inputs improve localization accuracy for non-free-floating arrays? The current architecture assumes a "free-floating" and omnidirectional array and does not explicitly model acoustic baffling or diffraction caused by the mounting device.

### Open Question 2
What architectural modifications, such as temporal attention mechanisms across longer context windows, are required to accurately model source velocity and trajectory for dynamic sources? The current training objective assumes static sources within the analysis window and identifies the modeling of moving sources as a necessary future step.

### Open Question 3
Can the architecture be extended to support Sound Event Localization and Detection (SELD) by outputting multiple event-specific tokens rather than a single aggregate [CLS] token? The current design relies on a single [CLS] token to represent the spatial embedding, which creates a bottleneck for representing multiple concurrent sound sources.

### Open Question 4
Does fusing PhaseCoder embeddings with visual depth estimation or 3D environmental maps resolve the fundamental ambiguity in audio-only distance estimation? The authors identify that purely audio-based distance estimation is limited by reverberation and loudness, suggesting that integrating other modalities is necessary to resolve these ambiguities.

## Limitations
- Distance estimation is fundamentally underdetermined and relies on reverberation cues rather than phase differences
- Performance degrades significantly with fewer than 4 microphones (14.6% azimuth accuracy vs 85.7% for 4 mics)
- Synthetic-only training may not capture real-world acoustic effects like device baffling and diffraction

## Confidence

**High Confidence**: Geometry-agnostic spatial encoding mechanism is well-supported by ablation studies and cross-geometry evaluation.

**Medium Confidence**: LLM integration performance claims are supported but face uncertainty from synthetic-only training limitations.

**Low Confidence**: Real-world generalization claims, particularly LOCATA performance without real training data, face uncertainty due to synthetic training approach.

## Next Checks

1. **Cross-Geometry Transfer Validation**: Train PhaseCoder on 4-mic synthetic arrays, then evaluate on held-out geometries (3, 5, 6, 7, 8 mics) without fine-tuning to test geometry-agnostic claims.

2. **Real-World Acoustic Transfer Test**: Evaluate synthetic-only trained PhaseCoder on real acoustic datasets to validate synthetic generalization claims.

3. **Microphone Count Sensitivity Analysis**: Systematically evaluate PhaseCoder performance with varying microphone counts (2-8 mics) on the same synthetic test set to quantify performance degradation curve.