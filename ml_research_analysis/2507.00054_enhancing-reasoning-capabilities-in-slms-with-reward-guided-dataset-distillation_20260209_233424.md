---
ver: rpa2
title: Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation
arxiv_id: '2507.00054'
source_url: https://arxiv.org/abs/2507.00054
tags:
- distillation
- teacher
- arxiv
- reasoning
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning capabilities
  in small language models (SLMs) through knowledge distillation. The core method,
  AdvDistill, introduces a reward-guided dataset distillation framework that uses
  multiple teacher responses with rule-based rewards to guide the student model training.
---

# Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation

## Quick Facts
- arXiv ID: 2507.00054
- Source URL: https://arxiv.org/abs/2507.00054
- Reference count: 22
- Primary result: AdvDistill 1.5B model achieves 91.52% accuracy on GSM-8K vs 72.85% for SFTDistilled

## Executive Summary
This paper introduces AdvDistill, a reward-guided dataset distillation framework designed to enhance reasoning capabilities in small language models. The method addresses the challenge of knowledge distillation by using multiple teacher responses with rule-based rewards to guide student model training, achieving significant improvements on mathematical reasoning benchmarks. While showing strong performance gains on structured reasoning tasks, the approach exhibits domain-specific effectiveness and comes with computational costs and behavioral trade-offs.

## Method Summary
AdvDistill is a reward-guided dataset distillation framework that uses multiple teacher responses with rule-based rewards to guide the student model training. The framework employs group relative advantages to weight responses during training, along with a contrastive penalty for incorrect responses. This approach aims to improve reasoning capabilities in small language models (SLMs) by distilling knowledge from larger teacher models in a more sophisticated manner than traditional methods.

## Key Results
- AdvDistill 1.5B model achieves 91.52% accuracy on GSM-8K (vs 72.85% for SFTDistilled and 42.45% for base)
- On GSM-PLUS, AdvDistill achieves 69.09% accuracy (vs 51.10% for SFTDistilled and 30.12% for base)
- AdvDistill performs worse than SFTDistilled on MMLU-PRO (23.57% vs 30.07%), indicating domain-specific effectiveness

## Why This Works (Mechanism)
AdvDistill works by leveraging multiple teacher responses and rule-based rewards to create a more robust distillation process. The group relative advantages mechanism allows the framework to identify and weight the most effective responses during training, while the contrastive penalty helps the model learn to avoid incorrect reasoning patterns. This multi-response approach provides richer training signals compared to single-response distillation methods, enabling the student model to develop stronger reasoning capabilities through exposure to diverse solution strategies and explicit reward guidance.

## Foundational Learning
- **Knowledge Distillation**: The process of transferring knowledge from larger teacher models to smaller student models; needed to enable efficient deployment of powerful reasoning capabilities in resource-constrained environments; quick check: verify teacher and student model architectures match reported specifications
- **Group Relative Advantages**: A method for comparing and weighting multiple responses relative to each other; needed to identify the most effective reasoning patterns from diverse teacher outputs; quick check: confirm the reward calculation correctly normalizes across response groups
- **Contrastive Learning**: Training approach that distinguishes correct from incorrect responses; needed to help the model learn to avoid common reasoning errors; quick check: verify the contrastive penalty is properly applied during training
- **Rule-based Reward Systems**: Framework for evaluating responses using predefined rules; needed to provide consistent and interpretable guidance during training; quick check: validate that reward rules are correctly implemented and applied
- **Dataset Distillation**: Compressing knowledge into smaller, more effective training datasets; needed to maximize learning efficiency while minimizing computational overhead; quick check: confirm the distilled dataset maintains diversity while focusing on high-value examples

## Architecture Onboarding
**Component Map**: Teacher Models -> Response Generation -> Rule-based Reward Calculation -> Group Relative Advantage Weighting -> Contrastive Penalty Application -> Student Model Training

**Critical Path**: The core training pipeline involves generating multiple teacher responses, calculating rewards using rule-based systems, computing group relative advantages, applying contrastive penalties, and using these weighted signals to guide student model training through backpropagation.

**Design Tradeoffs**: The method trades increased computational cost (from multiple teacher responses and reward calculations) for improved reasoning performance. This creates a balance between training efficiency and capability enhancement, with the multiple-response approach providing richer signals but requiring more resources than single-response distillation.

**Failure Signatures**: Poor performance on non-reasoning tasks (as seen on MMLU-PRO), reduced template adherence, potential overfitting to specific benchmark patterns, and scalability issues due to computational overhead are key indicators of limitations in the approach.

**First Experiments**: 
1. Replicate the GSM-8K benchmark comparison to verify the 91.52% accuracy claim
2. Test the framework on a different reasoning benchmark to assess generalization beyond GSM-PLUS
3. Measure the exact computational overhead introduced by the multiple response generation and reward calculation processes

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains are concentrated on structured reasoning tasks, with reduced effectiveness on broader knowledge benchmarks
- Increased computational costs from using multiple teacher responses and rule-based reward calculations are not quantified
- Behavioral trade-offs including reduced template adherence suggest potential degradation in other important model capabilities

## Confidence
- **High Confidence**: AdvDistill improves mathematical reasoning performance on GSM-8K and GSM-PLUS benchmarks
- **Medium Confidence**: The method shows domain-specific effectiveness, though MMLU-PRO comparison involves fewer examples
- **Low Confidence**: Claims about developing general reasoning capabilities are not strongly supported by evidence of broader generalization

## Next Checks
1. Evaluate AdvDistill on additional diverse reasoning benchmarks beyond GSM-8K and GSM-PLUS to determine whether performance improvements extend to other reasoning domains
2. Measure and report the exact computational overhead introduced by the multiple teacher response generation and reward calculation processes
3. Conduct comprehensive evaluations across multiple behavioral dimensions to quantify the trade-offs mentioned and identify which capabilities are preserved versus degraded during distillation