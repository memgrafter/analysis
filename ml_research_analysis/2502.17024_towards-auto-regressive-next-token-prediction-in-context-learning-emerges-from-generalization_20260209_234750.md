---
ver: rpa2
title: 'Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges
  from Generalization'
arxiv_id: '2502.17024'
source_url: https://arxiv.org/abs/2502.17024
tags:
- wpre
- sequences
- topics
- topic
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles a key gap in in-context learning (ICL) theory
  by extending beyond the standard i.i.d. assumption to the more realistic auto-regressive
  next-token prediction (AR-NTP) setting.
---

# Towards Auto-Regressive Next-Token Prediction: In-Context Learning Emerges from Generalization

## Quick Facts
- arXiv ID: 2502.17024
- Source URL: https://arxiv.org/abs/2502.17024
- Authors: Zixuan Gong; Xiaolin Hu; Huayi Tang; Yong Liu
- Reference count: 40
- Key outcome: ICL emerges from generalization over sequences and topics in AR-NTP, validated through PAC-Bayesian bounds and experiments on synthetic and real language datasets.

## Executive Summary
This paper provides a theoretical foundation for understanding how in-context learning (ICL) emerges from generalization in auto-regressive next-token prediction (AR-NTP). The authors extend beyond the standard i.i.d. assumption to model the more realistic scenario where each token depends on all previous tokens. By deriving PAC-Bayesian generalization bounds that account for token dependencies, layer-wise structure, and topic distributions, they show that ICL naturally emerges when a model generalizes well across both sequences and topics during pre-training.

The theoretical framework is validated through experiments on both synthetic (GINC) and real-world language datasets, demonstrating that increasing pre-training topics, sequences per topic, sequence length, and prompt length all improve generalization and ICL performance. The paper also shows that ICL can fail when data distribution doesn't match the assumed topic structure, and demonstrates practical benefits of data-dependent priors for training stability and efficiency.

## Method Summary
The method involves pre-training a GPT-2 model on structured data where topics are sampled from a distribution P_W, each topic contains multiple sequences, and each sequence consists of auto-regressive tokens. The theoretical analysis uses PAC-Bayesian bounds with two-level expectations over sequences and topics, incorporating optimization-dependent terms via Stochastic Differential Equations. Experiments sweep K (topics), N (sequences per topic), T (sequence length), and Tp (prompt length) on both synthetic GINC and real-world language datasets, using AdamW optimizer with linear learning rate schedules.

## Key Results
- ICL accuracy improves with increasing number of pre-training topics (K), sequences per topic (N), and sequence length (T)
- Prompt length (Tp) scaling shows inverse square root relationship with generalization error
- Data-dependent prior initialization accelerates training and improves stability compared to random initialization
- ICL fails when topic distributions between pre-training and test data don't align
- Theoretical bounds capture empirical trends across both synthetic and real datasets

## Why This Works (Mechanism)

### Mechanism 1: ICL as Generalization over Sequences and Topics
ICL emerges when a pre-trained model minimizes population loss across both specific sequences and unseen topics. The paper models ICL as a two-level expectation: first over sequences (data) and second over topics (tasks). By deriving PAC-Bayesian bounds for this population loss, the authors show that good generalization on pre-training data inherently provides ICL abilities for unseen prompts. This works when the topic distribution covering pre-training topics is similar to ICL topics, and data follows auto-regressive dependency rather than i.i.d. tokens.

### Mechanism 2: Data-Dependent and Topic-Dependent Priors
Using data-dependent priors (based on a subset of training data/topics) tightens the generalization bound compared to random priors. The theoretical bound depends on KL divergence between the model's posterior and prior. If the prior is derived from a subset of training data, the distance to the final model distribution is smaller, reducing the generalization error term. This justifies "prior model initialization" and supports the observation that prior initialization accelerates training and stabilizes the process.

### Mechanism 3: Optimization Trajectory and "Train Faster, Generalize Better"
The number of optimization iterations directly impacts the generalization bound; fewer iterations to convergence correlate with better generalization. By modeling training via Stochastic Differential Equations, the bound includes a term implying that while training is necessary, excessive iterations without convergence can degrade generalization properties relative to optimization noise. This supports the classical viewpoint that faster training (fewer iterations to convergence) leads to better generalization.

## Foundational Learning

- **Concept: PAC-Bayesian Bounds**
  - Why needed: The entire theoretical framework relies on bounding population loss using empirical loss plus a complexity term (KL divergence)
  - Quick check: Can you explain why a smaller KL divergence between the posterior and prior implies better generalization?

- **Concept: Auto-Regressive Next-Token Prediction (AR-NTP)**
  - Why needed: The paper explicitly rejects the i.i.d. assumption common in simpler ICL theory, modeling language as dependent tokens where x_{t+1} depends on the prefix sequence
  - Quick check: How does the "prompt token-dependency" in this framework differ from the i.i.d. input-label pairs used in supervised learning ICL setups?

- **Concept: Stochastic Differential Equations (SDE) / Langevin Dynamics**
  - Why needed: The authors use SDEs to model the continuous trajectory of SGD, allowing them to derive an optimization-dependent bound linking training time to final ICL capability
  - Quick check: What does the "Fokker-Planck equation" describe in the context of this paper's analysis of model parameters?

## Architecture Onboarding

- **Component map:** Pre-training Data {Topic → Sequences → Tokens} → GPT-2 Model → AdamW Optimizer → Generalization Bounds
- **Critical path:**
  1. Split pre-training data into subset J (for prior ν_J) and subset I (for posterior μ)
  2. Train a model on subset J to establish the "informed prior" distribution ν_J
  3. Continue training on subset I using the prior as initialization to find posterior μ
  4. Calculate the generalization bound using the derived KL term to estimate ICL performance

- **Design tradeoffs:**
  - Ghost Sample Size (N' vs N): Larger subset J reduces prior variance but reduces data I available for main training
  - Model Size (N_param) vs Iterations (T'): Larger models lower the bound but longer training can increase the complexity term

- **Failure signatures:**
  - Random Transitions: If pre-training data contains random transitions, the model cannot extract topic-specific structures and ICL fails
  - Topic Mismatch: If ICL topics are not covered by pre-training distribution, the second-level expectation assumption breaks

- **First 3 experiments:**
  1. Vary K, N, T separately to verify 1/√KNT scaling of the bound
  2. Test ICL accuracy against increasing prompt length Tp to verify 1/√K Tp scaling
  3. Compare random initialization vs prior model initialization to validate faster convergence with informed priors

## Open Questions the Paper Calls Out

### Open Question 1
Can the optimization error term ε_opt be explicitly quantified and bounded rather than assumed negligible? The authors state they defer analysis of optimization error to future work, assuming results closely approximate the ideal minimum. This leaves the generalization guarantees dependent on an assumption of optimality that may not hold for finite-step training on complex loss landscapes.

### Open Question 2
Can the optimization-dependent generalization bounds be adapted for adaptive optimizers like AdamW rather than relying on SGD assumptions? The theoretical analysis relies on SGD and Continuous Langevin Dynamics, but experiments use AdamW. Adaptive optimizers alter gradient noise and learning rate dynamics significantly, potentially invalidating specific terms derived for KL divergence.

### Open Question 3
How does generalization performance degrade when ICL topics are drawn from a distribution different from pre-training topic distribution? The theoretical framework assumes ICL topics satisfy the same distribution P_W as pre-training topics, but real-world ICL often involves tasks not explicitly covered by pre-training data.

### Open Question 4
Is the construction of "ghost sequences" the most efficient method for decoupling token dependencies in AR-NTP generalization analysis? The paper uses ghost sequence construction via Donsker-Varadhan inequality to handle prompt token-dependency, but it remains unclear if this is fundamental or if tighter bounds could be achieved via alternative dependency decoupling techniques.

## Limitations
- Theoretical framework relies on idealized assumptions that may not capture real-world LLM training dynamics, including continuous parameter updates and clean topic structures
- PAC-Bayesian bounds depend on KL divergence that's difficult to compute exactly for large neural networks
- Clean topic structure assumption may be overly simplistic for real language corpora where topics overlap and evolve gradually
- Use of SGD assumptions in theory while using AdamW in experiments creates a gap between theory and practice

## Confidence
- **High Confidence**: Empirical validation of scaling laws relating ICL performance to pre-training data dimensions is robust
- **Medium Confidence**: Theoretical connection between generalization bounds and ICL emergence depends on assumptions about topic distributions
- **Medium Confidence**: Practical benefits of data-dependent prior initialization are demonstrated but may be implementation-specific

## Next Checks
1. Test ICL performance when pre-training topics only partially overlap with ICL prompt topics, systematically varying intersection size
2. Evaluate the framework when topic distributions shift during pre-training (e.g., gradually transitioning between topics)
3. Compare the proposed data-dependent prior against other prior construction strategies (e.g., meta-learning approaches) to isolate the specific contribution of the PAC-Bayesian framework