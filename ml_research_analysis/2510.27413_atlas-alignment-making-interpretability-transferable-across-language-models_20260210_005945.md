---
ver: rpa2
title: 'Atlas-Alignment: Making Interpretability Transferable Across Language Models'
arxiv_id: '2510.27413'
source_url: https://arxiv.org/abs/2510.27413
tags:
- concept
- features
- atlas
- subject
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Atlas-Alignment, a framework for transferring
  interpretability across language models by aligning unknown latent spaces to a labeled,
  human-interpretable Concept Atlas. The approach leverages lightweight representational
  alignment techniques to map subject model features into a foundation model's interpretable
  latent space, enabling semantic feature search and steerable generation without
  costly training or labeled concept data.
---

# Atlas-Alignment: Making Interpretability Transferable Across Language Models

## Quick Facts
- arXiv ID: 2510.27413
- Source URL: https://arxiv.org/abs/2510.27413
- Authors: Bruno Puri; Jim Berend; Sebastian Lapuschkin; Wojciech Samek
- Reference count: 32
- Primary result: Framework for transferring interpretability across language models through representational alignment

## Executive Summary
This paper introduces Atlas-Alignment, a novel framework that enables transferable interpretability across language models by aligning unknown latent spaces to a labeled, human-interpretable Concept Atlas. The approach leverages lightweight representational alignment techniques to map subject model features into a foundation model's interpretable latent space, enabling semantic feature search and steerable generation without costly training or labeled concept data. By amortizing interpretability investment in a high-quality atlas, the framework makes many new models transparent at minimal marginal cost while maintaining semantic consistency and controllability.

## Method Summary
Atlas-Alignment operates through a two-stage pipeline: first, it aligns the subject model's latent space to a foundation model's Concept Atlas using techniques like Orthogonal Procrustes or linear regression, creating a mapping that preserves semantic relationships. Second, it uses this alignment to enable semantic feature search by querying the atlas with subject model activations, and controllable steering by manipulating aligned representations to influence model outputs. The method requires only the subject model's weights and relies on the foundation model's atlas for interpretability, avoiding the need for additional concept labeling or fine-tuning.

## Key Results
- Semantic retrieval achieves up to 0.97 MRR and 0.21 MPP using Orthogonal Procrustes alignment
- Controllable steering demonstrates over 30% faithfulness in top layers of subject models
- Framework enables interpretability transfer without costly training or labeled concept data

## Why This Works (Mechanism)
Atlas-Alignment works by exploiting the representational similarity between different language models' latent spaces. When subject model features are aligned to a foundation model's interpretable atlas, semantic relationships are preserved through the alignment transformation. This allows concepts in the foundation model's atlas to be meaningfully mapped to corresponding features in the subject model, enabling semantic search and manipulation. The lightweight alignment techniques (Orthogonal Procrustes, linear regression) provide efficient mappings that capture the essential geometric relationships between feature spaces while maintaining computational tractability.

## Foundational Learning
1. **Representational Alignment** - Why needed: To map between different models' latent spaces while preserving semantic relationships. Quick check: Alignment quality measured by MRR/MPP scores.
2. **Concept Atlas Construction** - Why needed: Provides the interpretable reference space for semantic queries and steering. Quick check: Atlas coverage and label quality.
3. **Latent Space Geometry** - Why needed: Understanding feature relationships enables meaningful alignment and manipulation. Quick check: Feature similarity/distance metrics.

## Architecture Onboarding

**Component Map:** Subject Model -> Alignment Layer -> Foundation Atlas -> Query/Steering Interface

**Critical Path:** Subject model activation → Alignment transformation → Atlas lookup → Semantic manipulation/control

**Design Tradeoffs:** Lightweight alignment (speed/efficiency) vs. alignment quality; foundation atlas quality vs. coverage limitations; single-layer steering (simplicity) vs. cross-layer coordination complexity.

**Failure Signatures:** Poor alignment quality manifests as low MRR/MPP scores; tokenizer mismatches cause alignment failures; atlas coverage gaps lead to retrieval failures for out-of-scope concepts.

**First Experiments:**
1. Measure MRR/MPP scores for different alignment techniques on benchmark concept sets
2. Test steering faithfulness at different subject model layers
3. Validate tokenizer compatibility requirements with controlled mismatches

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Framework assumes availability of high-quality foundation model atlas
- Effectiveness depends heavily on atlas coverage and quality
- Current evaluation focuses primarily on single-layer steering
- Requires tokenizer compatibility between subject and foundation models

## Confidence

**High Confidence:**
- Core representational alignment methodology is well-established
- Quantitative results (MRR up to 0.97, MPP 0.21) are robust
- Controllable steering results showing 30%+ faithfulness are well-supported

**Medium Confidence:**
- Amortization claims assume consistent alignment quality across diverse architectures
- Results may not capture edge cases or rare concept scenarios

**Low Confidence:**
- Scalability claims to arbitrary models require broader empirical validation
- Claims about minimal marginal cost transparency need testing across diverse model families

## Next Checks
1. Test alignment performance on subject models with different tokenization schemes
2. Evaluate cross-layer steering capabilities by extending beyond single-layer experiments
3. Assess atlas coverage limitations by testing retrieval on concepts outside original atlas scope