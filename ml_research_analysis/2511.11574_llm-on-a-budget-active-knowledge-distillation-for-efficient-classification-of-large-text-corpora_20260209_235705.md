---
ver: rpa2
title: 'LLM on a Budget: Active Knowledge Distillation for Efficient Classification
  of Large Text Corpora'
arxiv_id: '2511.11574'
source_url: https://arxiv.org/abs/2511.11574
tags:
- uncertainty
- learning
- student
- sampling
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high cost of using large language models
  (LLMs) for text classification by proposing a hybrid approach that combines knowledge
  distillation with active learning. The core idea is to use an LLM as a teacher to
  train a smaller, more efficient student model while minimizing the number of samples
  labeled by the expensive teacher.
---

# LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora

## Quick Facts
- arXiv ID: 2511.11574
- Source URL: https://arxiv.org/abs/2511.11574
- Reference count: 40
- Primary result: Up to 80% reduction in labeled samples required for target accuracy using M-RARU active learning

## Executive Summary
This paper tackles the high computational and financial cost of using large language models (LLMs) for text classification by introducing a hybrid approach that combines knowledge distillation with active learning. The core innovation is M-RARU, a novel active learning algorithm that strategically selects the most informative samples for labeling by a teacher LLM, based on student model uncertainty. By doing so, the method drastically reduces the number of expensive LLM queries needed to train a smaller, efficient student model. Experiments on multiple datasets and student architectures demonstrate significant gains in sample efficiency and accuracy, enabling practical deployment of LLM-derived classifiers in resource-constrained settings.

## Method Summary
The approach uses an LLM as a teacher to label a small, highly informative subset of data, which is then used to train a smaller student model for efficient inference. M-RARU, the proposed active learning algorithm, iteratively samples from the unlabeled pool and accepts samples for labeling based on a probability proportional to the student model's uncertainty (1 - max class probability). This process continues until a batch of 25 samples is collected, which are then labeled by the teacher LLM. The student model is retrained after each batch, and the cycle repeats until the labeling budget is exhausted. The method is tested across five student models (SVM, LDA, RF, GBDT, and DistilBERT) on benchmark text classification datasets.

## Key Results
- M-RARU achieves up to 80% reduction in the number of labeled samples required to reach target accuracy compared to random sampling
- Across all student models and datasets, M-RARU consistently improves classification accuracy while lowering financial costs and training time
- The resulting student models offer faster inference, making LLM-derived classifiers practical for resource-constrained applications

## Why This Works (Mechanism)
The method works by intelligently selecting the most uncertain and informative samples for the expensive teacher LLM to label, rather than randomly sampling or labeling all data. This focus on high-uncertainty examples accelerates student learning and reduces the total labeling budget required. By distilling knowledge from the LLM into a smaller, efficient model, the approach combines the power of LLMs with the speed and scalability of lightweight classifiers.

## Foundational Learning
- **Knowledge Distillation**: Transfer learning technique where a large teacher model trains a smaller student model; needed to leverage LLM power without high inference costs; quick check: student accuracy improves with more teacher-labeled samples
- **Active Learning**: Machine learning paradigm where the model selects the most informative samples for labeling; needed to minimize expensive teacher queries; quick check: M-RARU outperforms random sampling in sample efficiency
- **Uncertainty Sampling**: Active learning strategy where samples with highest prediction uncertainty are selected for labeling; needed to focus labeling budget on most informative data; quick check: acceptance rate per batch is 10-40% for well-calibrated models
- **Multi-class Classification**: Classification task with more than two classes; needed for the benchmark datasets used; quick check: balanced accuracy is reported alongside accuracy to account for class imbalance
- **Probability Calibration**: Ensuring model confidence scores reflect true likelihoods; needed for accurate uncertainty estimates in active learning; quick check: calibration curves or Brier scores improve after calibration
- **LLM Prompting**: Crafting inputs to elicit desired outputs from language models; needed to obtain consistent class labels from the teacher LLM; quick check: fixed prompt yields stable labeling accuracy

## Architecture Onboarding
- **Component Map**: Embeddings (SentenceTransformer) -> Student Model (SVM/LDA/RF/GBDT/DistilBERT) -> M-RARU (Uncertainty Sampling) -> Teacher LLM (gemma-3-27b-it) -> Updated Student Model
- **Critical Path**: Initial embedding generation -> Student initialization with minimal labeled data -> M-RARU loop (uncertainty estimation -> sample acceptance -> teacher labeling -> student retraining) -> Evaluation until budget exhausted
- **Design Tradeoffs**: High-uncertainty sampling accelerates learning but risks overfitting to ambiguous examples; randomization in M-RARU improves diversity but may reduce efficiency; using non-probabilistic models (e.g., SVM) requires calibration or approximation for uncertainty
- **Failure Signatures**: Acceptance rates drop to 0.2-1.9% (overconfident models like LDA), M-RARU vs random sampling gap <5% (poor uncertainty calibration), student fails to provide uncertainty in first iteration (no initial labeled data)
- **First Experiments**: 1) Test student uncertainty in first iteration with minimal labeled pool; 2) Replicate teacher labeling with fixed prompt and monitor accuracy; 3) Train SVM with `probability=True` or use decision function distances, compare acceptance rates and final accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The student model's ability to provide meaningful uncertainty estimates in the first iteration, before any training, is unclear and may require a small initial labeled pool or prior probabilities
- The exact prompt format for eliciting class labels from the teacher LLM is not provided, which may affect reproducibility and labeling consistency
- The retraining procedure (from scratch vs. incremental fine-tuning) is not specified, which could influence convergence and sample efficiency

## Confidence
- Core methodology is well-documented and reproducible: High
- Uncertainty estimation in first iteration is unclear: Medium
- Teacher labeling procedure and prompt sensitivity: Medium
- Retraining strategy and model calibration details: Medium

## Next Checks
1. Test student uncertainty in the first iteration by initializing with minimal labeled data; if model fails, consider using prior probabilities or a small random labeled pool
2. Replicate the teacher labeling process with a fixed prompt; monitor for prompt sensitivity and compare results to reported accuracy curves
3. Train an SVM with `probability=True` or use decision function distances; compare acceptance rates and final accuracy to those reported in the paper