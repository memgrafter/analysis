---
ver: rpa2
title: Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning
arxiv_id: '2501.14315'
source_url: https://arxiv.org/abs/2501.14315
tags:
- training
- data
- task
- performance
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why fine-tuning large language models with
  LLM-generated data preserves non-target task performance better than using ground
  truth data. Through systematic analysis, the authors find that LLM-generated responses
  contain fewer high-perplexity tokens, which reduces catastrophic forgetting during
  fine-tuning.
---

# Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning

## Quick Facts
- **arXiv ID:** 2501.14315
- **Source URL:** https://arxiv.org/abs/2501.14315
- **Reference count:** 40
- **Primary result:** STM achieves similar performance preservation to self-generated data by filtering high-perplexity tokens from ground truth data

## Executive Summary
This paper addresses catastrophic forgetting during LLM fine-tuning by investigating why self-generated data preserves non-target task performance better than ground truth data. The authors discover that LLM-generated responses contain fewer high-perplexity tokens, which reduces interference with previously learned knowledge during fine-tuning. To replicate this effect with ground truth data, they introduce Selective Token Masking (STM), a method that filters out high-perplexity tokens using the model's own perplexity scores. Extensive experiments across multiple model families and fine-tuning techniques demonstrate that STM maintains near-original performance on non-target tasks while improving target task performance, particularly when filtering approximately 20-24% of high-perplexity tokens.

## Method Summary
The authors propose Selective Token Masking (STM) to mitigate catastrophic forgetting during LLM fine-tuning. STM works by first computing perplexity scores for each token in the ground truth training data using the base model. High-perplexity tokens are then filtered out based on a threshold, typically removing 20-24% of tokens. The filtered dataset is used for fine-tuning instead of the original ground truth data. This approach aims to reduce interference from tokens that the model finds surprising or unlikely, which are hypothesized to cause more forgetting of previously learned knowledge. The method is tested across multiple model families including Gemma 2 IT 2B, Llama 3 8B Instruct, Mistral 7B, Gemma 2 IT 9B, and OLMo 2 7B, using various fine-tuning techniques.

## Key Results
- STM achieves similar performance preservation to self-generated data, maintaining near-original performance on non-target tasks
- Filtering approximately 20-24% of high-perplexity tokens provides optimal balance between target task improvement and non-target task preservation
- STM outperforms baseline fine-tuning methods across multiple model families and fine-tuning techniques
- The approach is particularly effective for maintaining performance on non-target tasks while improving target task performance

## Why This Works (Mechanism)
The mechanism behind STM's effectiveness is that high-perplexity tokens in ground truth data cause more interference with previously learned knowledge during fine-tuning. By filtering out these tokens, STM reduces catastrophic forgetting. The authors observe that LLM-generated data naturally contains fewer high-perplexity tokens, which explains why self-generated data better preserves non-target task performance compared to ground truth data.

## Foundational Learning

**Perplexity** - A measure of how well a probability model predicts a sample. Lower perplexity indicates the model is more confident about its predictions. Needed to identify tokens that might cause interference during fine-tuning.

**Catastrophic Forgetting** - The phenomenon where neural networks forget previously learned information when trained on new tasks. Quick check: Does fine-tuning on new data cause performance degradation on old tasks?

**Token Filtering** - The process of selectively removing tokens from training data based on specific criteria. Quick check: Does removing certain tokens improve model performance on target tasks while preserving other capabilities?

**Fine-tuning Interference** - The impact of new training data on previously learned knowledge. Quick check: Do different types of training data cause different levels of forgetting?

## Architecture Onboarding

**Component Map:** Base Model -> Perplexity Computation -> Token Filtering -> Fine-tuning

**Critical Path:** The most important steps are perplexity computation and token filtering, as these determine which tokens are retained for fine-tuning and directly impact forgetting mitigation.

**Design Tradeoffs:** The method trades some target task performance (by removing potentially useful tokens) for better preservation of non-target tasks. The 20-24% filtering threshold represents a balance between these competing objectives.

**Failure Signatures:** If perplexity estimates are unreliable, STM may remove important tokens or retain harmful ones. Poor filtering thresholds could lead to either excessive information loss or insufficient forgetting mitigation.

**First Experiments:**
1. Compare STM performance with different filtering thresholds (10%, 20%, 30%, 40%) to find optimal balance
2. Test STM on a model with known weaknesses to verify forgetting mitigation
3. Evaluate STM's effectiveness when fine-tuning on sequential tasks to test long-term stability

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- STM's effectiveness depends on the reliability of the model's own perplexity estimates, which may be biased or unreliable for out-of-distribution data
- Results are based on specific model families and sizes (2B-9B parameters), limiting generalizability to larger or smaller models
- The 20-24% filtering threshold is empirically determined and may not be optimal across different architectures or tasks
- STM performs worse than using all ground truth data for some tasks, suggesting the trade-offs are more complex than presented

## Confidence

**High confidence:** The empirical observation that LLM-generated data reduces forgetting compared to ground truth data is well-supported by experiments across multiple model families and fine-tuning techniques.

**Medium confidence:** The claim that STM can match the forgetting mitigation benefits of self-generated data is supported but requires careful interpretation, as STM performs worse than using all ground truth data for some tasks.

**Low confidence:** The generalization of results to other model sizes, architectures, or task domains beyond those tested. The long-term stability of STM's benefits across extended fine-tuning or when fine-tuning on multiple tasks sequentially.

## Next Checks

1. Test STM's effectiveness on larger model families (70B+ parameters) to determine if the 20-24% filtering threshold and perplexity-based approach scale appropriately, and whether different filtering percentages are optimal for larger models.

2. Evaluate STM on a broader range of task types including mathematical reasoning, code generation, and multilingual tasks to assess whether perplexity-based filtering generalizes across different domains or if task-specific adjustments are needed.

3. Conduct ablation studies comparing STM with alternative token selection methods (e.g., importance scores from gradient-based methods, frequency-based filtering, or task-specific token importance) to isolate whether perplexity is the most effective criterion for mitigating forgetting.