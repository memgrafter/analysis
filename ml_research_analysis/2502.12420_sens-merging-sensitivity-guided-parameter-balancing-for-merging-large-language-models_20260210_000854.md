---
ver: rpa2
title: 'Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language
  Models'
arxiv_id: '2502.12420'
source_url: https://arxiv.org/abs/2502.12420
tags:
- merging
- task
- code
- tasks
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently merging large
  language models (LLMs) that have been fine-tuned for specialized tasks, without
  the need for costly retraining. The authors propose Sens-Merging, a sensitivity-guided
  coefficient adjustment method that enhances existing model merging techniques by
  operating at both task-specific and cross-task levels.
---

# Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models

## Quick Facts
- **arXiv ID**: 2502.12420
- **Source URL**: https://arxiv.org/abs/2502.12420
- **Reference count**: 17
- **Primary result**: Sens-Merging improves model merging by combining task-specific sensitivity and cross-task transferability, outperforming existing methods especially in code generation tasks.

## Executive Summary
This paper introduces Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances model merging by operating at both task-specific and cross-task levels. The method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. When applied to merging LLaMA2-7B/13B and Mistral 7B models fine-tuned for general knowledge, mathematical reasoning, and code generation, Sens-Merging significantly improves performance across all tasks. Notably, it enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks.

## Method Summary
Sens-Merging works by computing two types of scaling factors: task-specific sensitivity weights derived from gradient-based parameter importance, and cross-task transferability scores based on logits alignment. For task-specific sensitivity, gradients are computed on calibration samples to estimate how much each parameter affects the loss, then aggregated per layer and normalized. For cross-task transferability, L2 distances between model outputs on cross-task calibration samples quantify knowledge alignment. These factors are combined multiplicatively with softmax normalization to produce final merging coefficients, which weight task vectors (fine-tuned weights minus base weights) during the merge. The method is evaluated by merging three fine-tuned models (Chat, Math, Code) derived from LLaMA2-7B/13B and Mistral 7B families, showing consistent improvements over baseline merging methods.

## Key Results
- Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks
- When combined with DARE, merged models outperform specialized fine-tuned models, particularly in code generation
- Task-specific scaling alone excels in specialized domains (e.g., +21.36% on MATH) but offers limited general benefits
- Cross-task scaling achieves broader performance gains (+5.37 points on Task Arithmetic) at the cost of peak task-specialized performance
- Combined approach achieves best overall performance, demonstrating complementary benefits

## Why This Works (Mechanism)

### Mechanism 1: Task-specific parameter sensitivity
- Claim: Layer-wise parameter sensitivity within individual tasks enables more precise coefficient weighting than uniform scaling
- Mechanism: Sensitivity is computed via first-order Taylor approximation: $S^{t_i}_j = |(\theta^{t_i}_j)^\top \nabla_{\theta^{t_i}_{SFT}} L(x_k)|$, approximating loss change when zeroing each parameter. These are aggregated per layer and L2-normalized to produce task-specific scaling factors $\alpha^l_i$ that weight layer contributions differently
- Core assumption: Gradients accurately proxy parameter importance; calibration samples representative of task distribution
- Evidence anchors: Abstract states analysis of parameter sensitivity; section 3.2 defines sensitivity via loss change approximation; neighboring papers don't validate gradient-based sensitivity for merging
- Break condition: Calibration samples too small or unrepresentative; gradients noisy/vanishing in deep layers; tasks with similar parameter importance patterns yield redundant scaling

### Mechanism 2: Cross-task logits alignment
- Claim: Cross-task logits alignment captures knowledge transferability between task-specific models
- Mechanism: For calibration samples from task $t_j$, alignment score $g_{i,j} = ||f_{\theta^{t_i}_{SFT}}(x^j_k) - f_{\theta^{t_j}_{SFT}}(x^j_k)||_2$ measures L2 distance between output logits. Aggregated across tasks and L1-normalized to produce cross-task scaling factor $\tau_i$, indicating how much each model benefits others
- Core assumption: Logits similarity correlates with beneficial knowledge transfer; L2 distance appropriate metric for cross-task alignment
- Evidence anchors: Abstract mentions evaluation of cross-task transferability; section 3.3 defines alignment score as L2 distance between predictions; CrossPT explores cross-task transferability but not via logits-based alignment
- Break condition: Tasks with superficially similar logits but conflicting reasoning patterns; large output vocabulary causing noisy distances; fine-tuned models with divergent output distributions despite shared knowledge

### Mechanism 3: Multiplicative combination with softmax
- Claim: Combining task-specific and cross-task scaling via softmax produces final coefficients that balance specialized and transferable knowledge
- Mechanism: $\sigma^l_i = \text{Softmax}(\tau_i \cdot \alpha^l_i, T)$ combines both factors, then merged parameters: $\theta^l_M = \theta^l_{base} + \sum_{i=1}^K K \cdot \sigma^l_i \cdot (\theta^{t_i,l}_{SFT} - \theta^l_{base})$. Multiplication by $K$ preserves magnitude
- Core assumption: Task-specific and cross-task factors are complementary and can be combined multiplicatively; softmax preserves relative importance
- Evidence anchors: Abstract states combining both factors to derive final merging coefficients; ablation shows cross-task alone +5.37 points, combined +5.75 points; Tensorized Clustered LoRA Merging uses different combination strategy
- Break condition: Temperature $T$ poorly tuned causing over/under-sharpening; multiplicative combination inappropriate when factors are correlated; tasks with opposite scaling needs creating conflicts

## Foundational Learning

- Concept: **First-order Taylor approximation for parameter importance**
  - Why needed here: Paper uses $|\theta^\top \nabla L|$ to estimate loss change without expensive leave-one-out evaluation. Must understand why this approximates $L(\theta) - L(\theta - \theta_j)$
  - Quick check question: Given parameter $\theta_j = 0.5$ and gradient $\nabla_{\theta_j} L = -2.0$, what's the approximate loss increase if $\theta_j$ is zeroed?

- Concept: **Task vectors in model merging**
  - Why needed here: Paper builds on Task Arithmetic framework where $\delta_{tk} = \theta^{tk}_{SFT} - \theta_{PRE}$ represents task-specific capabilities. Merging aggregates these vectors with coefficients
  - Quick check question: If pretrained model has parameter 1.0 and fine-tuned has 1.3, what's the task vector? If merged with coefficient 0.5, what's the final parameter?

- Concept: **Trade-offs between specialization and transfer**
  - Why needed here: Ablation shows task-specific scaling excels in math (+21.36% MATH) but cross-task scaling sacrifices this for broader gains. Understanding this guides when to use which
  - Quick check question: If you need peak math performance vs. balanced multi-task performance, which scaling component should dominate?

## Architecture Onboarding

- Component map: Calibration sampler -> Gradient computer (task-specific) AND Logits aligner (cross-task) -> Sensitivity aggregator and Cross-task normalizer -> Coefficient combiner -> Parameter merger

- Critical path: Calibration sampling → gradient computation (task-specific) AND logits computation (cross-task) → aggregation/normalization → softmax combination → weighted merge. Both scaling paths can run in parallel.

- Design tradeoffs:
  - Calibration size m: Larger m improves sensitivity estimates but increases compute (paper doesn't specify m)
  - Temperature T: Lower T sharpens distribution (fewer tasks dominate), higher T smooths (more balanced)
  - L2 vs L1 normalization: L2 for task-specific, L1 for cross-task — paper choice, rationale unclear

- Failure signatures:
  - NaN coefficients: Gradient explosion during sensitivity computation; check calibration sample quality
  - All coefficients near 1/K: Temperature too high or sensitivity factors near-uniform; reduce T
  - Merged model worse than baselines: Cross-task scaling may be hurting specialized tasks; try task-specific only (see ablation pattern)
  - Code task degrades: Cross-task scaling trades code for general knowledge; increase relative weight of code model's task-specific factor

- First 3 experiments:
  1. Validate sensitivity computation: Take single task model (e.g., Math), compute $\alpha^l_i$, verify top-5 sensitive layers match Figure 3 pattern (layer ~15 for math). Check gradient magnitudes reasonable (not NaN/extreme)
  2. Ablation reproduction: Replicate Table 4 on single model family (LLaMA2-7B) — task-specific only, cross-task only, combined. Verify cross-task gives +5.37 on Task Arithmetic baseline
  3. Temperature sweep: With full Sens-Merging on 2-model merge (Chat+Math), vary T ∈ {0.5, 1.0, 2.0, 5.0}. Plot average score vs. T to find sweet spot before full 3-model experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Sens-Merging be effectively adapted to merge models with heterogeneous architectures?
- Basis in paper: The authors state in the Limitations section that their "current implementation primarily addresses homogeneous model merging where base models share identical architectures" and identify extending this to heterogeneous architectures as "an exciting direction for future research."
- Why unresolved: The current methodology assumes a shared backbone ($\theta_{PRE}$) to calculate task vectors ($\delta_{tk} = \theta_{SFT} - \theta_{PRE}$), an assumption that fails when architectures differ
- What evidence would resolve it: A modified Sens-Merging framework that successfully aligns and merges models with different layer structures or embedding dimensions, demonstrating improved performance over baseline heterogeneous merging methods

### Open Question 2
- Question: How does Sens-Merging perform when applied to fully fine-tuned models or smaller-scale architectures with larger weight divergences?
- Basis in paper: The paper notes that the method "has been primarily validated with LoRA fine-tuned models, where weight differences between specialized models are relatively constrained" and suggests that "fully fine-tuned models with larger weight divergences... may require adaptations."
- Why unresolved: The sensitivity analysis relies on gradient approximations that may behave differently when parameter shifts are large and unconstrained (as in full fine-tuning) compared to the constrained updates of LoRA
- What evidence would resolve it: Empirical evaluations of Sens-Merging on fully fine-tuned checkpoints (not just LoRA adapters) and on smaller model families (e.g., decoder-only models with < 1B parameters)

### Open Question 3
- Question: Can the trade-off between task-specific scaling (peak specialized performance) and cross-task scaling (general average performance) be dynamically optimized?
- Basis in paper: The ablation studies conclude that "task-specific scaling excels in specialized domains... but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance."
- Why unresolved: While the paper identifies this trade-off, it proposes a static combination (multiplication) of the two factors rather than a mechanism to tune the balance based on whether the user prioritizes average utility or peak expertise
- What evidence would resolve it: A parametric approach allowing users to weight task-specific vs. cross-task scaling factors, resulting in a Pareto frontier that allows users to select a specific balance between generalization and specialization

## Limitations

- **Calibration sample dependency**: Performance hinges on representative calibration samples, but the paper doesn't specify sample size m or analyze sensitivity to calibration quality
- **Heuristic design choices**: Multiplicative combination of scaling factors and K-multiplier for task vectors appear arbitrary rather than theoretically justified
- **Gradient approximation validity**: Sensitivity relies on first-order Taylor approximation, which assumes linear behavior in parameter space where nonlinearities likely dominate

## Confidence

- **High Confidence**: Task-specific sensitivity computation using gradient-based importance scores. The mechanism is clear, the math is straightforward, and ablation shows consistent improvements across tasks. The first-order approximation is standard practice in sensitivity analysis.
- **Medium Confidence**: Cross-task transferability through logits alignment. While the methodology is sound, the assumption that L2 distance between logits indicates beneficial knowledge transfer is weakly supported. The paper shows correlation but doesn't prove causation for merging performance.
- **Low Confidence**: Multiplicative combination of scaling factors and K-multiplier for task vectors. These design choices appear heuristic rather than theoretically grounded. The paper shows these work empirically but doesn't explore alternatives or provide ablation for these specific components.

## Next Checks

1. **Sensitivity Stability Analysis**: Run sensitivity computation on multiple random subsets of calibration samples (m/2, m, 2m) for a single task. Measure coefficient variance across runs and correlate with final merging performance. This would quantify how sensitive the method is to calibration quality.

2. **Alternative Combination Strategies**: Implement additive combination (α_i + τ_i) and learned combination (small MLP) alongside the current multiplicative approach. Apply all three to the same merging tasks and compare performance. This would test whether the multiplicative assumption is justified.

3. **Gradient vs. Leave-One-Out Validation**: For a small subset of parameters in one task, compute actual loss change when zeroing (leave-one-out) versus the Taylor approximation. Measure approximation error distribution and correlate with parameter importance rankings. This would quantify the validity of the efficiency-accuracy tradeoff.