---
ver: rpa2
title: 'Large Language Models (LLMs) for Source Code Analysis: applications, models
  and datasets'
arxiv_id: '2503.17502'
source_url: https://arxiv.org/abs/2503.17502
tags:
- code
- arxiv
- llms
- language
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines the growing role of large language models
  (LLMs) in source code analysis, addressing applications, models, and datasets. It
  identifies key use cases such as code understanding, disassembly, decompilation,
  summarization, generation, comment generation, and security analysis.
---

# Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets

## Quick Facts
- arXiv ID: 2503.17502
- Source URL: https://arxiv.org/abs/2503.17502
- Authors: Hamed Jelodar; Mohammad Meymani; Roozbeh Razavi-Far
- Reference count: 40
- This survey examines the growing role of large language models (LLMs) in source code analysis, addressing applications, models, and datasets

## Executive Summary
This survey provides a comprehensive overview of large language models for source code analysis, examining applications from code generation and summarization to security analysis and decompilation. The study systematically reviews prominent LLM architectures including CodeBERT, CodeT5, GPT variants, and DeepSeek, while analyzing essential datasets like CodeSearchNet and CodeXGLUE. The research identifies key performance trends, architectural tradeoffs, and persistent challenges including token size limitations, security vulnerabilities, and the need for specialized datasets. The authors emphasize the importance of interdisciplinary collaboration between software engineering and machine learning communities to advance LLM-driven tools for software development.

## Method Summary
The survey employs a systematic literature review methodology, analyzing 40+ references to categorize LLM applications in source code analysis. The study examines model architectures, training approaches, and evaluation metrics across different code analysis tasks. The methodology includes comparative analysis of model performance on standard benchmarks, identification of architectural patterns, and synthesis of challenges and limitations. The survey particularly focuses on the evolution from traditional static analysis tools to LLM-based approaches, documenting the transition from syntax-focused to semantic understanding of code.

## Key Results
- DeepSeek-R1 demonstrates superior reasoning and code generation performance compared to GPT-4 across multiple benchmarks
- Current datasets like CodeSearchNet lack the quality and specificity needed for advanced tasks like decompilation and security analysis
- Token size limitations create significant challenges for analyzing long codebases, requiring innovative chunking and context management strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs bridge source code and natural language by projecting both into a shared, high-dimensional semantic space
- **Mechanism:** Models learn joint representations through pre-training on code-comment pairs, aligning code syntax with human intent via Masked Language Modeling and Replaced Token Detection
- **Core assumption:** Statistical regularities in public code repositories reflect logical intent and best practices
- **Evidence anchors:** CodeBERT described as bimodal transformer using NL-PL pairs; CodeSearchNet enables code understanding tasks
- **Break condition:** Training data with irrelevant or misleading comments degrades semantic alignment, causing hallucinated summaries

### Mechanism 2
- **Claim:** Security can be improved through iterative, feedback-driven refinement loops rather than single-pass generation
- **Mechanism:** Frameworks like SecCode integrate external static analysis tools to detect vulnerabilities, feeding error signals back to the LLM for iterative repair
- **Core assumption:** LLMs possess sufficient knowledge of secure coding patterns to correct errors when explicitly prompted with vulnerability types
- **Evidence anchors:** SecCode and SALLM described as three-stage processes using Encouragement Prompting; static analysis tools like CodeQL and Semgrep
- **Break condition:** Static analysis tools fail to detect logical flaws or context window exhaustion prevents effective corrections

### Mechanism 3
- **Claim:** LLMs facilitate binary analysis by treating assembly as a translatable "foreign language"
- **Mechanism:** Models like LLM4Decompile use sequence-to-sequence prediction to translate low-level instructions into high-level source code, "hallucinating" variable names and control structures
- **Core assumption:** Syntactic patterns of compiled binaries map predictably to high-level abstractions regardless of compiler optimizations
- **Evidence anchors:** LLM4Decompile uses S2S architecture for binary-to-source translation; Nova employs hierarchical attention for assembly code
- **Break condition:** Heavily obfuscated code or optimized binaries create ambiguous mappings between instructions and high-level logic

## Foundational Learning

- **Concept: Abstract Syntax Trees (ASTs)**
  - **Why needed here:** Traditional tools and advanced LLMs rely on ASTs to understand code structure beyond raw text tokens
  - **Quick check question:** Can you explain why a model might misinterpret a variable rename if it only looks at tokens and not the AST structure?

- **Concept: Encoder-Decoder vs. Decoder-Only Architectures**
  - **Why needed here:** Determines whether models are better for understanding (embedding) or generation tasks
  - **Quick check question:** Which architecture would you select for translating binary code to C++, and why?

- **Concept: Context Window & Chunking**
  - **Why needed here:** Token size limitations require strategies for splitting code without losing logical flow
  - **Quick check question:** If a file is 10,000 tokens long and the model limit is 4,000, how would you design a chunking strategy?

## Architecture Onboarding

- **Component map:** Raw Source/Binary -> Tokenizer -> Retrieval (Optional) -> Core Inference -> Validation Layer -> Feedback Loop
- **Critical path:** Dataset Curation -> Model Selection -> Domain Adaptation -> Security Hardening
- **Design tradeoffs:** GPT-4 offers generalizability but high cost and opaque security; DeepSeek is open-source and cheaper but requires self-hosting
- **Failure signatures:** Syntactic Hallucination (non-existent APIs), Context Loss (forgotten variable definitions), Bias Propagation (social biases)
- **First 3 experiments:** 1) Benchmark DeepSeek-Coder against HumanEval/MBPP datasets, 2) Test context limit degradation on summarization quality, 3) Implement SecCode-style security repair loop

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can specialized datasets be developed for code analysis tasks like decompilation and disassembly to overcome limitations of general-purpose corpora?
- **Basis in paper:** Section 7.1 notes general datasets fail to address complexities of advanced tasks
- **Why unresolved:** Existing datasets focus on generation rather than reverse engineering requirements
- **What evidence would resolve it:** Creation and release of high-quality, labeled benchmarks for binary-to-source translation

### Open Question 2
- **Question:** What architectural or prompting strategies can mitigate context loss when analyzing codebases exceeding current LLM token limits?
- **Basis in paper:** Section 7.2 highlights token truncation leads to lost syntactic information
- **Why unresolved:** Models struggle to maintain logical consistency across chunks
- **What evidence would resolve it:** Standardized pipeline processing repository-scale code without performance degradation

### Open Question 3
- **Question:** How can LLMs be enhanced to validate code correctness and runtime behavior rather than relying solely on syntax-based generation?
- **Basis in paper:** Section 7.4 states major limitation is inability to validate correctness or test runtime behavior
- **Why unresolved:** Models generate outputs based on statistical likelihood rather than executable verification
- **What evidence would resolve it:** LLM-augmented framework consistently identifying logic errors and edge cases

## Limitations
- Survey findings based primarily on benchmarking studies rather than controlled experiments, limiting causal relationship establishment
- DeepSeek-R1 superiority claims constrained by lack of standardized evaluation across all tasks and potential publication bias
- Survey does not adequately address reproducibility crisis in LLM research where performance claims cannot be independently verified

## Confidence
- **High Confidence:** Core applications identification and general encoder-decoder vs decoder-only architecture patterns
- **Medium Confidence:** Comparative performance rankings between CodeBERT, CodeT5, GPT variants, and DeepSeek based on published benchmarks
- **Low Confidence:** Specific claim that DeepSeek-R1 outperforms GPT-4 across all reasoning and code generation tasks without comprehensive validation

## Next Checks
1. **Replicate DeepSeek-R1 vs. GPT-4 Comparison:** Design controlled experiment using HumanEval and MBPP benchmarks to verify performance differential
2. **Context Window Boundary Analysis:** Systematically test varying context window sizes (500, 1000, 2000, 4000 tokens) on code summarization and generation quality
3. **Security Loop Effectiveness Study:** Implement and evaluate SecCode-style iterative repair mechanism on curated vulnerability dataset, measuring success rates across CWE categories