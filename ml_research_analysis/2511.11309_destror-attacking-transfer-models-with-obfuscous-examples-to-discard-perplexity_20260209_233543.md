---
ver: rpa2
title: 'destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity'
arxiv_id: '2511.11309'
source_url: https://arxiv.org/abs/2511.11309
tags:
- attack
- dataset
- attacks
- data
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces destroR, a framework for attacking Bangla\
  \ sentiment classification models with adversarial examples to expose and exploit\
  \ their weaknesses. It develops three attack strategies\u2014Bangla Paraphrase Attack,\
  \ Bangla Back Translation Attack, and One-Hot Word Swap Attack\u2014to generate\
  \ semantically similar but structurally different inputs that reduce model performance."
---

# destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity

## Quick Facts
- arXiv ID: 2511.11309
- Source URL: https://arxiv.org/abs/2511.11309
- Authors: Saadat Rafid Ahmed; Rubayet Shareen; Radoan Sharkar; Nazia Hossain; Mansur Mahi; Farig Yousuf Sadeque
- Reference count: 15
- Primary result: Novel adversarial attack framework for Bangla sentiment classification models

## Executive Summary
This paper introduces destroR, a framework designed to expose weaknesses in Bangla sentiment classification models through adversarial examples. The framework implements three attack strategies—Bangla Paraphrase Attack, Bangla Back Translation Attack, and One-Hot Word Swap Attack—to generate semantically similar but structurally different inputs that degrade model performance. Tested on BanglaBERT and XLM-RoBERTa, these attacks achieved up to 40% reduction in F1 macro scores on internal datasets, with the One-Hot Word Swap proving most effective. The research highlights the vulnerability of transfer learning models to subtle textual modifications, emphasizing the need for robust evaluation and adversarial training, particularly for low-resource languages like Bangla.

## Method Summary
The destroR framework employs three distinct attack strategies to generate adversarial examples for Bangla sentiment classification models. The Bangla Paraphrase Attack uses contextual paraphrasing to maintain semantic meaning while altering sentence structure. The Bangla Back Translation Attack translates text to an intermediate language and back to Bangla, introducing syntactic variations. The One-Hot Word Swap Attack systematically replaces words with their synonyms or semantically similar alternatives. These attacks were tested on transformer-based models including BanglaBERT and XLM-RoBERTa using both internal and external Bangla sentiment datasets, measuring performance degradation through F1 macro scores.

## Key Results
- One-Hot Word Swap Attack achieved the highest performance degradation, reducing F1 macro scores by up to 40% on internal datasets
- External dataset testing showed 37% reduction in model performance across attack strategies
- BanglaBERT and XLM-RoBERTa both showed significant vulnerability to all three attack methods
- The framework successfully demonstrated how semantically similar but structurally different inputs can mislead sentiment classification models

## Why This Works (Mechanism)
The effectiveness of destroR's attacks stems from exploiting the semantic and syntactic sensitivities of transformer-based models. These models, while powerful for transfer learning, can be misled by subtle changes in input that preserve meaning but alter surface features. The attacks work by introducing controlled perturbations that force the model to rely on spurious correlations or surface-level patterns rather than robust semantic understanding. By systematically modifying inputs through paraphrasing, translation, or word substitution, the attacks reveal how these models can overfit to specific linguistic patterns in the training data rather than developing true language understanding.

## Foundational Learning
- Transfer learning in low-resource languages: Critical for understanding why pre-trained models struggle with domain-specific tasks in Bangla; quick check: compare performance on high-resource vs low-resource language tasks
- Adversarial example generation: Essential for stress-testing model robustness; quick check: measure performance drop when applying minimal semantic-preserving perturbations
- Transformer architecture vulnerabilities: Understanding how attention mechanisms can be exploited; quick check: analyze attention weights before and after attacks
- Semantic preservation vs syntactic variation: Key tradeoff in designing effective attacks; quick check: human evaluation of semantic equivalence between original and adversarial examples
- Cross-lingual transfer limitations: Important for understanding model generalization across languages; quick check: compare attack effectiveness on monolingual vs cross-lingual models

## Architecture Onboarding
- Component map: Input text → Attack strategy module → Adversarial example generator → Transformer model → Performance metrics
- Critical path: Attack generation → Model inference → Performance evaluation → Analysis of failure modes
- Design tradeoffs: Balance between semantic preservation and syntactic variation; computational cost of attack generation vs. attack effectiveness
- Failure signatures: Sudden drops in confidence scores, incorrect sentiment classification despite semantic similarity, over-reliance on specific word patterns
- First experiments: 1) Baseline performance measurement on clean data, 2) Application of each attack strategy individually, 3) Comparative analysis of attack effectiveness across different model architectures

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Experiments conducted on relatively small internal datasets, limiting generalizability to larger, more diverse corpora
- Lack of detailed error analysis makes it difficult to determine whether performance drops stem from true adversarial vulnerability or dataset-specific artifacts
- No baseline models trained with adversarial examples or robustness techniques for meaningful comparison
- Limited evaluation of attack transferability across different transformer architectures beyond the two tested models

## Confidence
- High: Framework implementation and attack methodology are clearly described and reproducible
- Medium: Performance degradation results are plausible given attack nature, but generalizability is uncertain
- Low: Practical implications for real-world deployment lack empirical support due to limited dataset size and scope

## Next Checks
1. Replicate attack experiments on a larger, publicly available Bangla sentiment dataset to assess generalizability
2. Apply same attack strategies to additional transformer architectures (e.g., mBERT, MuRIL) to test cross-model transferability
3. Train baseline models with adversarial training or data augmentation and compare their robustness against proposed attacks