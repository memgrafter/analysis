---
ver: rpa2
title: 'GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning
  with Adaptive Dirichlet Exploration'
arxiv_id: '2510.07919'
source_url: https://arxiv.org/abs/2510.07919
tags:
- policy
- learning
- grade
- exploration
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRADE tackles the problem of personalized multi-task fusion in
  large-scale recommender systems, where static, manually-tuned weights fail to capture
  individual user intent and adapt to shifting user preferences. It proposes a novel
  framework using Group-relative Reinforcement learning with Adaptive Dirichlet Exploration
  to learn personalized fusion weights dynamically.
---

# GRADE: Personalized Multi-Task Fusion via Group-relative Reinforcement Learning with Adaptive Dirichlet Exploration

## Quick Facts
- arXiv ID: 2510.07919
- Source URL: https://arxiv.org/abs/2510.07919
- Authors: Tingfeng Hong; Pingye Ren; Xinlong Xiao; Chao Wang; Chenyi Lei; Wenwu Ou; Han Li
- Reference count: 37
- Key outcome: Deployed in marketplace with 100M+ daily active users, achieving +0.595% CTR, +1.193% CVR, +1.788% OPM, +1.568% total order volume

## Executive Summary
GRADE addresses personalized multi-task fusion in large-scale recommender systems by learning dynamic, user-specific weights to combine multiple objectives (CTR, CVR, OPM, GPM). Traditional static weights fail to capture individual user intent and adapt to shifting preferences. GRADE employs Group-relative Reinforcement Learning with Adaptive Dirichlet Exploration, using a critic-free GRPO paradigm for stable training, Dirichlet distribution for principled exploration in the constrained weight space, and a composite reward function combining sparse user feedback with dense model priors and rule-based constraints. The system was fully deployed serving hundreds of millions of users and significantly outperformed established baselines.

## Method Summary
GRADE uses a two-stage approach: Stage 1 trains a supervised MMoE-like model via LambdaLoss to establish a baseline policy; Stage 2 applies GRPO fine-tuning. The policy network outputs deterministic weights for a given user context, which are scaled by a concentration parameter and sampled from a Dirichlet distribution to generate G candidate weight vectors (G=20). These weights are applied to MTL scores to rank items and compute rewards. Rewards combine NDCG on user feedback (sparse), NDCG on model predictions (dense), and rule-based format constraints. The policy is updated using relative advantage normalization and PPO-style clipping with KL penalty for stability.

## Key Results
- Deployed in production serving hundreds of millions of daily active users
- Achieved +0.595% improvement in Click-Through Rate (CTR)
- Achieved +1.193% improvement in Conversion Rate (CVR)
- Achieved +1.788% improvement in Overall Profit Margin (OPM)
- Achieved +1.568% improvement in total order volume

## Why This Works (Mechanism)

### Mechanism 1: Critic-free Group Relative Policy Optimization
GRADE eliminates the critic network to stabilize training under sparse reward conditions by learning from relative action comparisons rather than absolute value estimates. For each user request, the policy samples G=20 candidate weight vectors, computes rewards for all candidates, normalizes within the group to compute relative advantage, and updates the policy to favor actions with positive relative advantage using PPO-style clipping and KL divergence penalties. This works because the optimal weight vector exists within the neighborhood of candidates the policy can generate, and relative ranking provides sufficient gradient signal without absolute value estimation.

### Mechanism 2: Dirichlet Distribution for Constrained Exploration
The Dirichlet distribution provides more efficient exploration than Gaussian-based methods for weight spaces with simplex constraints (non-negative, sum-to-one). The policy network outputs deterministic weights, which are scaled by concentration parameter α̂ to form Dirichlet parameters. Sampling from Dir(α) yields exploration vectors that automatically satisfy constraints, with expected value E[p] = πθ(q) ensuring exploration centers on the policy's current recommendation. Cosine annealing (5→15) shifts from exploration to exploitation. This works because the simplex constraint is fundamental to the fusion problem, and violating it would produce invalid weight configurations.

### Mechanism 3: Composite Reward with Format Constraints
Combining sparse user feedback with dense model priors and rule-based format rewards prevents reward hacking and guides policy toward business-aligned weight structures. Total reward R_total = λ₁R_post + λ₂R_prior + λ₃R_format (λ₁=1.0, λ₂=0.3, λ₃=0.05). R_post uses NDCG on actual user feedback, R_prior uses NDCG on model predictions for denser signal, and R_format applies soft constraints via gating function that rewards specific weight hierarchies only when combined reward exceeds threshold. This works because model predictions correlate with user satisfaction and human heuristics about weight structure encode valid domain knowledge.

## Foundational Learning

- **Concept: Multi-Task Learning (MTL) architecture (MMoE, PLE)**
  - Why needed here: GRADE operates on outputs from an upstream MTL model predicting CTR, CVR, OPM, GPM. Understanding how these scores are generated and their typical distributions is prerequisite for diagnosing fusion issues.
  - Quick check question: Can you explain why hard parameter sharing in MTL can cause negative transfer, and how Mixture-of-Experts architectures mitigate this?

- **Concept: Policy Gradient methods (PPO, Actor-Critic)**
  - Why needed here: GRPO is derived from policy gradient foundations. Understanding baseline subtraction, advantage estimation, and importance sampling clarifies why removing the critic helps and what tradeoffs exist.
  - Quick check question: Why does PPO use a clipped objective instead of vanilla policy gradient, and what role does the KL penalty play in GRADE's implementation?

- **Concept: Exploration-Exploitation trade-off in continuous action spaces**
  - Why needed here: The Dirichlet concentration parameter directly controls this trade-off. Misunderstanding here leads to either stagnant policies or unstable training.
  - Quick check question: If you observe training loss decreasing but online metrics not improving, what exploration parameter should you investigate first, and what would you check?

## Architecture Onboarding

**Component map:**
User Context Features -> Policy Network (MMoE-like) -> Deterministic Weights πθ(q) -> Dirichlet Sampler with α̂ scaling -> G candidate weight vectors -> Apply to MTL scores → Rank items → Compute rewards -> Group-relative advantage normalization -> GRPO update with PPO clipping + KL penalty

**Critical path:**
1. Stage 1 supervised LTR pre-training establishes baseline policy (prevents cold-start instability)
2. Stage 2 GRPO fine-tuning: each iteration samples G actions, computes composite rewards, normalizes advantages, updates policy
3. Daily training iteration: sync policy to reference model, collect data with old policy, update toward new policy

**Design tradeoffs:**
- Group size G: Larger G provides richer relative signal but increases computational cost and risk of reward homogenization. Paper found G=20 optimal.
- Concentration α̂: Lower values = more exploration but slower convergence; cosine annealing balances both.
- Reward weights (λ₁, λ₂, λ₃): Higher λ₁ prioritizes user feedback but suffers sparsity; higher λ₂ densifies signal but risks model bias; higher λ₃ enforces structure but may over-constrain.

**Failure signatures:**
- Reward hacking: Policy achieves high R_post but poor online metrics (especially GPM). Check if format reward is active and λ₃ properly tuned.
- Weight polarization: Single weight dominates (e.g., w_ctr → 1.0, others → 0). Gating function F_g may need adjustment or λ₃ increase.
- Training instability: Loss spikes or policy divergence. Check KL penalty coefficient ρ and ensure reference model is frozen.
- No personalization: Weights converge to near-uniform distribution. Check exploration parameters (α̂ may be too high) and verify policy network receives sufficient user context.

**First 3 experiments:**
1. Reproduce Stage 1 baseline: Train supervised LTR model on historical logs, verify it matches paper's SP baseline performance (Table 2: +0.73% CTR, +0.03% CVR). This validates data pipeline and model architecture.
2. Ablate Dirichlet vs. Gaussian exploration: Compare Dirichlet sampling against Gaussian+Softmax baseline on offline NDCG metrics. Paper claims superiority but provides no head-to-head data. This validates core exploration mechanism.
3. Tune format reward threshold τ: Run grid search on τ (paper uses 0.4) to find optimal soft constraint width for your specific objective balance. Monitor for polarization vs. over-constraint tradeoffs in weight distributions.

## Open Questions the Paper Calls Out

### Open Question 1
Can GRADE transfer to new domains or tasks without extensive manual hyperparameter re-tuning?
- Basis in paper: Section 4.2 notes that "these hyperparameters were tuned for our specific application; transferring GRADE to other scenarios would likely require re-tuning."
- Why unresolved: The framework currently relies on manually tuned coefficients (e.g., λ₁, λ₂, λ₃) and exploration parameters (α̂) specific to the Kuaishou marketplace.
- What evidence would resolve it: Demonstration of stable performance in a distinct domain (e.g., video streaming vs. e-commerce) using a meta-learning initialization or a fixed set of hyperparameters.

### Open Question 2
How can the framework mitigate "reward homogenization" to effectively utilize larger candidate groups?
- Basis in paper: Section 4.5 hypothesizes that performance drops with group sizes G > 20 because "excessively large groups can lead to reward homogenization, diminishing the quality of the relative advantage signal."
- Why unresolved: The paper identifies the limitation of large group sizes but does not propose a mechanism to maintain signal diversity or relative advantage variance as G increases.
- What evidence would resolve it: A modification to the advantage normalization or sampling strategy that allows performance to scale positively or stabilize with group sizes larger than 20.

### Open Question 3
Can the rule-based "Weight Format Reward" be replaced by a learned constraint to reduce dependency on manual heuristics?
- Basis in paper: Section 3.2.2 describes the format reward as incorporating "human heuristics" and a manually designed piecewise gating function to prevent polarization.
- Why unresolved: The current design requires encoding specific domain knowledge (e.g., which weights should be dominant) directly into the reward function structure.
- What evidence would resolve it: An ablation study comparing the current hand-crafted format reward against a self-supervised or adversarial regularization method that learns valid weight structures directly from data.

### Open Question 4
Does the item-agnostic policy architecture limit the granularity of personalization compared to item-aware approaches?
- Basis in paper: Section 3.1 states, "Notably, the policy model is item-agnostic; it does not receive any item-side features."
- Why unresolved: While simplifying the architecture, this design assumes optimal weights depend only on user/context and not on the specific characteristics of the candidate items (e.g., price sensitivity).
- What evidence would resolve it: A comparative analysis of the current item-agnostic policy against a policy model that consumes item embeddings to determine if item-specific weight adjustments yield further NDCG gains.

## Limitations

- Evaluation relies entirely on offline A/B testing with no offline-only validation, limiting understanding of critic-free design necessity
- Lack of ablation studies comparing GRPO mechanism versus traditional actor-critic approaches
- Dirichlet exploration benefit is asserted but not empirically validated against Gaussian-based alternatives in the same experimental framework

## Confidence

- **High Confidence:** The composite reward formulation combining sparse feedback, dense priors, and format constraints is well-justified by ablation results showing metric trade-offs
- **Medium Confidence:** The critic-free GRPO design improves stability, but this claim rests primarily on theoretical arguments rather than direct comparison experiments
- **Low Confidence:** The superiority of Dirichlet distribution for constrained exploration lacks empirical validation against other simplex-projection methods

## Next Checks

1. **Offline Validation Pipeline:** Implement offline metrics (NDCG, CTR lift) on historical data to verify the +0.73% CTR improvement claimed for the supervised baseline before proceeding to RL fine-tuning.

2. **Exploration Method Comparison:** Conduct controlled experiments comparing Dirichlet sampling against Gaussian+Softmax and other simplex-projection methods using identical reward functions and group sizes.

3. **GRPO vs. Actor-Critic Benchmark:** Replace the critic-free GRPO with a standard actor-critic implementation using identical reward structures to quantify the stability and performance benefits claimed for the critic-free approach.