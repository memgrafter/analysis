---
ver: rpa2
title: Towards Lossless Ultimate Vision Token Compression for VLMs
arxiv_id: '2512.09010'
source_url: https://arxiv.org/abs/2512.09010
tags:
- visual
- luvc
- tokens
- pruning
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of visual language
  models (VLMs) caused by high-resolution image and video token redundancy. The authors
  propose LUVC, a training-free framework that combines orthogonal iterative merging
  (OIM) in the visual encoder with spectrum pruning units (SPU) in the LLM.
---

# Towards Lossless Ultimate Vision Token Compression for VLMs

## Quick Facts
- arXiv ID: 2512.09010
- Source URL: https://arxiv.org/abs/2512.09010
- Authors: Dehua Zheng; Mouxiao Huang; Borui Jiang; Hailin Hu; Xinghao Chen
- Reference count: 40
- One-line primary result: LUVC achieves ~2× inference speedup across LLaVA-OV, InternVL2.5, and Qwen2VL models with negligible accuracy loss

## Executive Summary
This paper addresses the computational inefficiency of visual language models (VLMs) caused by high-resolution image and video token redundancy. The authors propose LUVC, a training-free framework that combines orthogonal iterative merging (OIM) in the visual encoder with spectrum pruning units (SPU) in the LLM. OIM preserves spatial structure while merging tokens in 2D, and SPU uses FFT-based low-pass filtering to prune high-frequency tokens progressively. Experiments show LUVC achieves ~2× inference speedup across LLaVA-OV, InternVL2.5, and Qwen2VL models with negligible accuracy loss. It maintains strong performance on diverse tasks including video, images, charts, and documents, and outperforms FastV, PACT, and VTW baselines. The approach is broadly compatible and supports immediate deployment.

## Method Summary
LUVC is a training-free token compression framework for VLMs that operates in two stages. First, the visual encoder uses Orthogonal Iterative Merging (OIM) to reduce token count while preserving spatial structure through iterative 2D merging operations. Second, the LLM employs Spectrum Pruning Units (SPU) that progressively filter high-frequency tokens using FFT-based low-pass filtering. The method maintains spatial coherence by processing tokens in orthogonal directions and leverages the frequency domain to identify and prune redundant information. This approach achieves approximately 2× speedup with minimal accuracy degradation across multiple VLM architectures.

## Key Results
- Achieves ~2× inference speedup across LLaVA-OV, InternVL2.5, and Qwen2VL models
- Maintains negligible accuracy loss on diverse benchmarks including images, videos, charts, and documents
- Outperforms FastV, PACT, and VTW baselines while being training-free and broadly compatible

## Why This Works (Mechanism)
The method works by exploiting redundancy in visual tokens through complementary spatial and frequency domain approaches. OIM reduces token count while preserving spatial relationships through orthogonal merging directions, maintaining the structural integrity needed for downstream tasks. SPU leverages the frequency domain representation to identify and prune high-frequency components that typically contain less semantic information. The progressive nature of SPU allows for adaptive compression based on the input content. The training-free design enables immediate deployment across existing VLM architectures without retraining, while the combination of spatial merging and frequency-based pruning achieves both efficiency and accuracy preservation.

## Foundational Learning

**Vision Tokenization**: Converting visual inputs into discrete tokens for LLM processing. Why needed: VLMs require token-based representations to handle visual information. Quick check: Verify token count reduction without losing spatial relationships.

**FFT-based Frequency Analysis**: Using Fast Fourier Transform to decompose signals into frequency components. Why needed: Identifies redundant high-frequency information that can be pruned. Quick check: Confirm low-frequency components preserve semantic content.

**Spatial Structure Preservation**: Maintaining relative positions of visual elements during processing. Why needed: Critical for tasks requiring spatial reasoning. Quick check: Validate that merged tokens don't disrupt spatial relationships.

**Progressive Pruning**: Gradually removing less important tokens throughout processing. Why needed: Allows adaptive compression based on content complexity. Quick check: Measure accuracy at different pruning stages.

**Orthogonal Processing**: Handling dimensions independently to preserve structure. Why needed: Prevents distortion when reducing token count. Quick check: Compare results with non-orthogonal approaches.

## Architecture Onboarding

**Component Map**: Input Image/Video -> Visual Encoder (with OIM) -> Reduced Token Set -> LLM (with SPU) -> Output

**Critical Path**: Visual Encoder -> OIM -> SPU -> LLM Processing

**Design Tradeoffs**: Training-free deployment vs. potential suboptimal compression compared to fine-tuned approaches; spatial preservation vs. aggressive token reduction; immediate compatibility vs. architecture-specific optimization opportunities.

**Failure Signatures**: Accuracy degradation on fine-grained visual tasks (small object counting, detailed document parsing); loss of spatial precision in localization tasks; potential bias in progressive pruning affecting certain visual features.

**First 3 Experiments**:
1. Baseline accuracy measurement on standard VQA and image classification benchmarks
2. Speedup validation with token count reduction tracking
3. Ablation study comparing OIM-only vs SPU-only vs combined approaches

## Open Questions the Paper Calls Out
None

## Limitations
- The "lossless" characterization may be overstated without comprehensive task-specific validation for fine-grained visual discrimination
- Training-free design may limit optimality compared to fine-tuned compression strategies
- Computational overhead analysis is incomplete, lacking FLOPs and memory usage quantification

## Confidence

- **High Confidence**: The ~2× inference speedup is reproducible and well-validated across multiple VLM architectures and datasets. The orthogonal iterative merging mechanism is technically sound.

- **Medium Confidence**: The claim of "negligible accuracy loss" is supported by standard benchmarks but may not generalize to all use cases requiring fine-grained spatial precision.

- **Low Confidence**: The "lossless" characterization lacks comprehensive validation for tasks requiring high spatial precision or fine-grained visual discrimination.

## Next Checks

1. Conduct ablation studies on fine-grained visual tasks including small object counting, document table parsing, and chart comparison to verify no critical information is lost.

2. Measure and report the actual computational overhead (FLOPs, memory usage) introduced by OIM and SPU modules to provide a complete efficiency profile.

3. Test the framework on multimodal reasoning tasks requiring precise spatial localization (e.g., VQA tasks with detailed spatial relationships) to identify potential failure modes in complex scenarios.