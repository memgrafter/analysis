---
ver: rpa2
title: 'LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference'
arxiv_id: '2509.02753'
source_url: https://arxiv.org/abs/2509.02753
tags:
- pruning
- expert
- experts
- throughput
- lexi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing inference efficiency
  in Mixture-of-Experts (MoE) language and vision models. The authors identify that
  traditional expert pruning strategies improve memory usage but fail to significantly
  enhance inference performance due to architectural sparsity and load imbalance.
---

# LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference

## Quick Facts
- arXiv ID: 2509.02753
- Source URL: https://arxiv.org/abs/2509.02753
- Reference count: 13
- LExI achieves 10% better accuracy than traditional pruning at the same throughput for Qwen1.5-MoE

## Executive Summary
This paper introduces LExI, a data-free optimization technique for improving inference efficiency in Mixture-of-Experts (MoE) models. Traditional expert pruning strategies fail to significantly enhance inference performance due to architectural sparsity and load imbalance issues. LExI addresses this by statically assigning different numbers of active experts per layer based on each layer's sensitivity to top-k changes, using only model weights for profiling. The approach employs an evolutionary search to find optimal top-k allocation under a fixed expert budget constraint.

## Method Summary
LExI operates through a two-stage optimization process. First, it performs a one-time profiling stage that estimates layer sensitivity using only model weights, without requiring any input data or training. This sensitivity estimation identifies which layers are more or less tolerant to reducing the number of active experts. Second, LExI employs an evolutionary search algorithm to determine the optimal allocation of active experts across different layers while respecting a fixed expert budget constraint. The method statically assigns these configurations, eliminating the need for runtime computation during inference.

## Key Results
- LExI enables Qwen1.5-MoE to achieve 10% better accuracy than traditional pruning at the same throughput
- On OLMoE-1B-7B, LExI matches the throughput of 50% intra-pruning while delivering 10% higher accuracy
- Experiments demonstrate significant throughput improvements across six state-of-the-art MoE models including language and vision-language architectures

## Why This Works (Mechanism)
LExI exploits the fundamental observation that different MoE layers exhibit varying sensitivity to reductions in active experts. By profiling layer sensitivity using only model weights, the method identifies which layers can tolerate fewer experts without significant accuracy loss. The evolutionary search then optimally distributes the expert budget across layers, prioritizing expert allocation where it matters most for maintaining accuracy while maximizing throughput gains.

## Foundational Learning

**Mixture-of-Experts (MoE) Architecture**: A neural network design where multiple expert networks exist per layer, with a gating network selecting which experts process each input. Why needed: Understanding MoE is essential to grasp why expert pruning and allocation strategies are critical for inference optimization.

**Architectural Sparsity**: The property where only a subset of parameters are activated for any given input, creating irregular computation patterns. Why needed: Explains why traditional pruning approaches fail to deliver expected performance gains in MoE models.

**Evolutionary Search Optimization**: A metaheuristic optimization technique inspired by biological evolution, using mechanisms like mutation and selection. Why needed: LExI uses this to find optimal expert allocation configurations across layers.

**Layer Sensitivity**: The degree to which a layer's output is affected by changes in the number of active experts. Why needed: Forms the basis for LExI's layer-adaptive expert allocation strategy.

**Data-free Optimization**: Techniques that optimize model performance without requiring input data or training examples. Why needed: LExI's key innovation is achieving optimization using only model weights.

**Top-k Selection**: The mechanism in MoE models that selects the k most relevant experts for processing each input. Why needed: LExI optimizes the number of active experts (k) on a per-layer basis.

## Architecture Onboarding

Component Map: Weight Profiler -> Sensitivity Estimator -> Evolutionary Search -> Expert Allocator -> Static Configuration

Critical Path: The profiling stage must complete before the evolutionary search can begin, as layer sensitivity estimates inform the search space. The evolutionary search must converge to a viable solution before the static configuration can be deployed.

Design Tradeoffs: LExI sacrifices the flexibility of dynamic expert allocation for the performance benefits of static configuration. This eliminates runtime overhead but may perform suboptimally if input distributions differ significantly from profiling assumptions.

Failure Signatures: Poor accuracy-throughput tradeoff indicates sensitivity estimation errors or inadequate evolutionary search convergence. Significant performance degradation under real workloads suggests the static configuration is too rigid for dynamic input patterns.

Three First Experiments:
1. Profile a small MoE model to verify sensitivity estimation accuracy against ground truth
2. Run evolutionary search on a medium-sized MoE model with known optimal configurations to validate search effectiveness
3. Compare LExI's throughput-accuracy tradeoff against traditional pruning on a held-out test set

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Effectiveness on extremely large-scale MoE models (beyond 70B parameters) remains unverified
- No extensive ablation studies on key hyperparameters like expert budget constraint and sensitivity metric threshold
- Does not address potential performance degradation in multi-task scenarios with varying input distributions

## Confidence

High confidence in: The fundamental insight that different MoE layers exhibit varying sensitivity to top-k changes, and that this heterogeneity can be exploited for inference optimization.

Medium confidence in: The effectiveness of the weight-based sensitivity estimation method as a reliable proxy for actual performance impact, and the claim that LExI consistently improves the accuracy-throughput tradeoff across diverse MoE architectures.

Low confidence in: The generalizability of LExI's benefits to production environments with highly dynamic workloads and the long-term stability of the statically assigned expert configurations under real-world usage patterns.

## Next Checks

1. Conduct comprehensive ablation studies varying the expert budget constraint across different model sizes to quantify the sensitivity of LExI's performance to this critical hyperparameter.

2. Test LExI on MoE models with 100B+ parameters to evaluate whether the proposed sensitivity estimation and evolutionary search methods scale effectively to state-of-the-art model sizes.

3. Implement and evaluate LExI in a dynamic serving environment with diverse input distributions to assess the robustness of the statically assigned expert configurations under real-world workload variations.