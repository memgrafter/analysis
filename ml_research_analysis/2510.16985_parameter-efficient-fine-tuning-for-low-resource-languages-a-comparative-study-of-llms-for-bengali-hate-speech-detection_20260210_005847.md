---
ver: rpa2
title: 'Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative
  Study of LLMs for Bengali Hate Speech Detection'
arxiv_id: '2510.16985'
source_url: https://arxiv.org/abs/2510.16985
tags:
- bengali
- hate
- speech
- online
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the growing problem of hate speech on Bengali\
  \ social media platforms, which disproportionately affects women and adolescents,\
  \ by introducing parameter-efficient fine-tuning (PEFT) as a resource-effective\
  \ solution for low-resource languages. The authors apply LoRA and QLoRA adapters\
  \ to fine-tune three open-source instruction-tuned LLMs\u2014Gemma-3-4B, Llama-3.2-3B,\
  \ and Mistral-7B\u2014on the BD-SHS dataset of 50,281 annotated Bengali comments,\
  \ training fewer than 1% of parameters on a single consumer-grade GPU."
---

# Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection

## Quick Facts
- arXiv ID: 2510.16985
- Source URL: https://arxiv.org/abs/2510.16985
- Authors: Akif Islam; Mohd Ruhul Ameen
- Reference count: 38
- Llama-3.2-3B achieved highest F1-score of 92.23% on Bengali hate speech detection using <1% parameter training

## Executive Summary
This study addresses the growing problem of hate speech on Bengali social media platforms by introducing parameter-efficient fine-tuning (PEFT) as a resource-effective solution for low-resource languages. The authors apply LoRA and QLoRA adapters to fine-tune three open-source instruction-tuned LLMs—Gemma-3-4B, Llama-3.2-3B, and Mistral-7B—on the BD-SHS dataset of 50,281 annotated Bengali comments, training fewer than 1% of parameters on a single consumer-grade GPU. Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at 88.94% and Gemma-3-4B at 80.25%, demonstrating that PEFT enables strong performance with minimal computational overhead and offers a scalable, replicable approach for Bengali and similar low-resource languages.

## Method Summary
The study fine-tunes three instruction-tuned LLMs (Gemma-3-4B, Llama-3.2-3B, Mistral-7B) using LoRA and QLoRA adapters on the BD-SHS dataset containing 50,281 annotated Bengali social media comments. All models were adapted by training fewer than 1% of their parameters through low-rank adapter injection while preserving pretrained knowledge. The base models were quantized to 4-bit precision using QLoRA to enable fine-tuning on a single RTX 4090 GPU (24GB VRAM). LoRA adapters were configured with rank=16 and α=16, and training ran for 3 epochs using the Unsloth framework. Performance was evaluated using weighted F1-score and accuracy metrics on a held-out test set.

## Key Results
- Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at 88.94% and Gemma-3-4B at 80.25%
- PEFT enabled fine-tuning with <1% of parameters on a single consumer-grade GPU
- Memory consumption reduced by 65-75% through QLoRA quantization
- Performance demonstrates that architectural design and tokenizer alignment matter more than raw parameter count

## Why This Works (Mechanism)

### Mechanism 1
Low-rank adapter injection enables task adaptation while preserving pretrained knowledge. LoRA freezes base model weights and introduces trainable low-rank decomposition matrices (A×B) into transformer layers. During fine-tuning, gradients flow only through these adapter matrices, which capture task-specific modifications while the frozen backbone retains multilingual representations learned during pretraining.

### Mechanism 2
4-bit quantization with gradient checkpointing enables billion-parameter model fine-tuning under 24GB VRAM constraints. QLoRA quantizes base weights to 4-bit precision (reducing memory ~4×) while maintaining full-precision LoRA adapters. Gradients backpropagate through the quantized model into adapters. Combined with optimized kernels and dynamic memory management, this reduces GPU memory by 65–75%.

### Mechanism 3
Tokenizer-architecture alignment with task-specific PEFT configuration determines performance more than raw parameter count. Llama-3.2-3B outperformed larger Mistral-7B (88.94%) despite having ~54% fewer parameters. The paper attributes this to Llama's multilingual subword tokenizer and architectural features (RoPE, SwiGLU, RMSNorm) aligning better with Bengali morphology and code-mixed text. PEFT amplifies this advantage by preventing overfitting that larger models might exhibit in low-data regimes.

## Foundational Learning

- Concept: LoRA rank and alpha scaling
  - Why needed here: The paper uses rank=16, α=16 for all models without ablation. Understanding how rank affects expressivity vs. overfitting is critical for adapting to other low-resource tasks.
  - Quick check question: If you double the rank from 16 to 32, would you expect better or worse performance on a 10K-sample Bengali dataset? Why?

- Concept: Instruction tuning compatibility with PEFT
  - Why needed here: All three models are instruction-tuned variants. The paper uses Bengali prompt templates during fine-tuning. Understanding how instruction format interacts with adapter learning affects prompt design.
  - Quick check question: What happens if you apply PEFT to a base model (not instruction-tuned) using the same prompt templates—would you expect similar results?

- Concept: Weighted F1-score vs. accuracy for imbalanced or balanced detection
  - Why needed here: BD-SHS is 50/50 balanced, but real deployment often involves skewed distributions. The paper prioritizes F1 over accuracy; understanding why matters for metric selection.
  - Quick check question: On a dataset with 90% non-hate and 10% hate speech, would accuracy or F1 be more informative for model selection?

## Architecture Onboarding

- Component map: BD-SHS Dataset (50,281 samples) -> Preprocessing (minimal) -> Instruction Prompt Template (Bengali) -> Base Model (4-bit quantized) -> LoRA Adapters (rank=16, α=16, lr=2e-4) -> Training (Unsloth framework, RTX 4090, 3 epochs) -> Evaluation (Weighted F1, Accuracy, Confusion Matrix)

- Critical path: 1) Dataset loading and split verification (80/10/10) 2) Prompt template formatting per sample 3) 4-bit quantization initialization 4) LoRA adapter injection into attention layers 5) Gradient accumulation to handle memory constraints 6) F1-score evaluation on held-out test set

- Design tradeoffs: Model size vs. memory (Mistral-7B requires batch size 4 vs. Gemma-3-4B at batch size 32; larger models may overfit on limited data), Rank vs. expressivity (rank=16 chosen uniformly; lower rank may underfit complex Bengali morphology, higher rank risks overfitting), Quantization vs. precision (4-bit enables 24GB training but introduces approximation error; full 16-bit would require enterprise GPUs)

- Failure signatures: Loss divergence or plateau early in training (check learning rate scaling with batch size), High training accuracy but low validation F1 (adapter overfitting; reduce rank or add dropout), Out-of-memory errors (reduce batch size, increase gradient accumulation steps, or switch to smaller base model), Poor performance on code-mixed text (tokenizer may not handle Banglish; consider vocabulary expansion or different base model)

- First 3 experiments: 1) Reproduce Llama-3.2-3B baseline on BD-SHS with identical hyperparameters (rank=16, α=16, lr=2e-4, 3 epochs) to verify reported 92.23% F1, 2) Ablate LoRA rank (4, 8, 16, 32) on a 10% subset of training data to characterize expressivity-overfitting tradeoff for Bengali, 3) Compare zero-shot vs. few-shot vs. PEFT performance using the same base model to quantify PEFT's contribution over prompting alone

## Open Questions the Paper Calls Out
None

## Limitations

- Generalizability Across Low-Resource Languages: The study demonstrates strong performance on Bengali hate speech detection but provides limited evidence that these findings extend to other low-resource languages. While the authors reference related work on Hindi/Nepali showing LoRA benefits, the specific performance patterns (e.g., Llama-3.2-3B outperforming larger models) may depend on Bengali-specific characteristics such as script, morphology, and code-mixing patterns.

- Architecture-Tokenizer Interaction: The paper attributes Llama-3.2-3B's superior performance to architectural features including multilingual subword tokenization, but this claim lacks empirical validation through controlled ablation studies. The relative contributions of tokenizer design, architectural choices (RoPE, SwiGLU), and model scale remain conflated.

- 4-bit Quantization Impact: While QLoRA's memory benefits are empirically verified through observed 65-75% reductions, the paper does not investigate whether quantization introduces bias or error that affects downstream hate speech detection performance.

## Confidence

- High Confidence: The core PEFT mechanism (LoRA + QLoRA) successfully reduces parameter updates to <1% while maintaining strong performance, as evidenced by the 65-75% memory reduction and competitive F1-scores. The technical implementation details are reproducible and well-documented.

- Medium Confidence: The relative ranking of models (Llama-3.2-3B > Mistral-7B > Gemma-3-4B) and the assertion that architectural design matters more than parameter count. While supported by observed results, the underlying causes require further controlled experiments to isolate.

- Low Confidence: Claims about the generalizability of these findings to other low-resource languages and the specific attribution of performance differences to tokenizer-architecture alignment without ablation studies.

## Next Checks

1. **Ablation Study on LoRA Rank**: Systematically vary rank parameters (4, 8, 16, 32) on a subset of the BD-SHS dataset to quantify the expressivity-overfitting tradeoff and identify optimal rank settings for Bengali hate speech detection.

2. **Cross-Linguistic Transfer**: Apply the identical PEFT pipeline (Llama-3.2-3B with LoRA rank=16) to at least two other low-resource languages (e.g., Nepali, Swahili) with available hate speech datasets to test the generalizability of the performance patterns observed in Bengali.

3. **Tokenizer Isolation Experiment**: Fine-tune Llama-3.2-3B with different tokenizers (original vs. modified Bengali vocabulary vs. character-level) while keeping all other parameters constant to empirically validate the impact of tokenizer design on hate speech detection performance.