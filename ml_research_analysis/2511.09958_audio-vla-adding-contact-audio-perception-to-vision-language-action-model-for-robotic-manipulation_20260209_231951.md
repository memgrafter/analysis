---
ver: rpa2
title: 'Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model
  for Robotic Manipulation'
arxiv_id: '2511.09958'
source_url: https://arxiv.org/abs/2511.09958
tags:
- audio
- contact
- manipulation
- audio-vla
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of vision-only Vision-Language-Action
  (VLA) models in perceiving contact events and dynamic processes during robotic manipulation.
  The authors propose Audio-VLA, a multimodal manipulation policy that integrates
  contact audio perception using pre-trained encoders (DINOv2, SigLIP, AudioCLIP)
  and a Llama2 backbone with LoRA fine-tuning.
---

# Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation

## Quick Facts
- **arXiv ID**: 2511.09958
- **Source URL**: https://arxiv.org/abs/2511.09958
- **Reference count**: 40
- **Primary result**: Audio-VLA achieves 97.6% success rate on LIBERO and 55.1% on RLBench, with at least three-fold improvements over vision-only methods and significant robustness under domain shifts.

## Executive Summary
This paper addresses the limitation of vision-only Vision-Language-Action (VLA) models in perceiving contact events during robotic manipulation. The authors propose Audio-VLA, which integrates contact audio perception using pre-trained encoders (DINOv2, SigLIP, AudioCLIP) and a Llama2 backbone with LoRA fine-tuning. By leveraging contact microphones to capture high-frequency acoustic signals during object interactions, Audio-VLA provides rich information about contact quality and manipulation states that vision cannot capture. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate significant performance improvements over vision-only comparative methods, particularly under domain shift conditions.

## Method Summary
Audio-VLA extends standard VLA architecture by adding an audio encoder branch that processes contact microphone signals at each timestep. The method uses DINOv2 and SigLIP for vision encoding with LoRA fine-tuning, AudioCLIP pretrained on general audio-visual data then additionally trained on ManiWAV robotic manipulation dataset with further LoRA adaptation, and Llama2 7B as the backbone. Audio signals are processed independently per timestep using FBSP spectrogram extraction (hop length 256, window 1024) with ResNeXt feature extraction, enabling instantaneous contact event detection and precise temporal alignment. The multimodal inputs are projected to unified embedding space and concatenated with language and proprioceptive tokens before being processed by Llama2. The model predicts K future action timesteps in parallel using a 4-layer MLP action head, trained with mean L1 loss on expert demonstrations.

## Key Results
- Audio-VLA achieves 97.6% success rate on LIBERO and 55.1% on RLBench standard environments, outperforming vision-only methods
- Under domain shift conditions (random lighting/color changes), Audio-VLA maintains 74.7% success rate on LIBERO while vision-only methods degrade severely
- Real-world experiments show at least three-fold improvements in success rates and Task Completion Rate (TCR) metrics compared to baseline methods
- TCR metric introduced to systematically evaluate dynamic operational processes beyond binary success/failure

## Why This Works (Mechanism)

### Mechanism 1
Contact audio provides domain-invariant physical interaction signals that remain stable when visual perception degrades. Acoustic signatures from object interactions (friction, collision, material resonance) are determined by physical properties rather than visual appearance, enabling robust performance when lighting, color, or texture changes disrupt vision-based models.

### Mechanism 2
High-frequency temporal alignment of audio with visual-proprioceptive modalities enables precise contact event detection. Processing audio independently at each timestep with optimized FBSP layer (hop length 256, window 1024) provides finer temporal resolution than prior methods, aligning acoustic events with visual frames and robot states for real-time feedback.

### Mechanism 3
LoRA fine-tuning of pre-trained audio encoder (AudioCLIP) adapts general audio-visual representations to manipulation-specific contact sounds. AudioCLIP provides foundation from large-scale audio-visual pretraining, with efficient adaptation to robotic manipulation domain without full model retraining.

## Foundational Learning

- **Concept: Vision-Language-Action (VLA) Models**
  - Why needed here: Audio-VLA builds directly on VLA architecture; understanding how VLA maps multimodal inputs to robot actions is prerequisite
  - Quick check question: Can you explain how RT-2 or OpenVLA transforms visual observations and language instructions into continuous action sequences?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: All modal encoders use LoRA fine-tuning; understanding rank-1 decomposition of weight updates explains why this works with limited manipulation data
  - Quick check question: Given a 7B parameter model with LoRA rank 32, how many trainable parameters are added per target layer?

- **Concept: Time-Frequency Audio Representations (Spectrograms)**
  - Why needed here: FBSP layer produces complex-valued spectrograms; understanding hop length vs. temporal resolution tradeoff explains the design choices
  - Quick check question: If you reduce hop length from 512 to 256, what happens to temporal resolution and computational cost?

## Architecture Onboarding

- **Component map**: [Third-person + Wrist Cameras] → DINOv2 + SigLIP (LoRA) → Vision Projection (Linear)
  [Contact Microphone] → AudioCLIP + ResNeXt (LoRA) → Audio Projection (3-layer MLP)
  [Robot Proprioception] → MLP → Proprio Projection (2-layer MLP)
  All projections → Sequence Concatenation → [Language tokens; Multimodal; Empty action tokens]
                                                               ↓
                                                    Llama2 7B (frozen + LoRA)
                                                               ↓
                                                    Action Hidden States → 4-layer MLP
                                                               ↓
                                                    K continuous actions × D dimensions

- **Critical path**: Audio encoder → Audio projection → Multimodal concatenation → Llama2 attention over cross-modal tokens. If audio features misalign with visual features in unified embedding space, Llama2 cannot learn cross-modal correlations.

- **Design tradeoffs**:
  - FBSP hop/window length: Reduced hop (256) improves temporal resolution but increases sequence length; this paper prioritizes fine-grained contact detection over computational efficiency
  - Parallel decoding (K actions): Predicting K future timesteps enables smoother control but requires accurate temporal modeling—longer horizons increase compounding error risk
  - LoRA rank 32: Higher rank captures more adaptation capacity but risks overfitting on limited demonstration data

- **Failure signatures**:
  - Audio encoder not fine-tuned: TCR drops significantly (Table IV: 46→52 EAWM, 35→46 S5GO) as contact-specific patterns not captured
  - Vision-only mode: Contact-intensive tasks (RLBench Task2/3) show 30-50% relative performance degradation
  - Domain shift without audio: Near-zero success rates on unseen environments (Table II: 0% for both π0-FAST and OpenVLA-OFT on S5GO unseen)

- **First 3 experiments**:
  1. Ablation on audio encoder LoRA: Train Audio-VLA with frozen AudioCLIP vs. LoRA-adapted version on a single RLBench contact-intensive task (e.g., insert onto square peg). Measure both success rate and TCR to isolate contact perception contribution.
  2. Temporal resolution sweep: Vary FBSP hop length (128, 256, 512) while keeping window fixed. Evaluate on real-world EAWM task where friction changes during wiping provide continuous feedback. Hypothesis: finer resolution helps continuous-contact tasks.
  3. Cross-domain audio generalization: Train on LIBERO with synthetic audio, test on real-world task with physical contact microphone. Measure domain gap to validate simulation-to-real transfer of audio representations.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Audio-enhanced simulation environments (LIBERO) are not publicly available, making independent verification difficult
- Exact architecture details of multimodal projectors and action prediction modules are underspecified, requiring assumptions in reproduction
- While contact audio provides domain-invariant signals in controlled scenarios, the paper doesn't thoroughly explore environments where acoustic properties differ significantly from training data

## Confidence
- **High Confidence**: The core claim that contact audio improves contact-rich manipulation performance is well-supported by ablation studies and cross-domain experiments. The TCR metric provides systematic evaluation of dynamic processes.
- **Medium Confidence**: The domain invariance claim relies on controlled lighting/color changes in simulation. Real-world validation is limited to two tasks, and the paper doesn't address acoustic variability from different materials or environments.
- **Low Confidence**: The specific contribution of temporal alignment improvements (hop length 256 vs. alternatives) is not systematically evaluated. The claim that 1000× higher frequency detection enables better contact sensing needs more quantitative validation.

## Next Checks
1. **Cross-material acoustic generalization test**: Train Audio-VLA on LIBERO using one material set (e.g., plastic objects), then test on real-world tasks with different materials (metal, wood, fabric). Measure performance drop to quantify acoustic domain gap.
2. **Temporal resolution ablation under continuous contact**: Implement variable FBSP hop lengths (128, 256, 512) and evaluate on EAWM wiping task where friction varies continuously. Compare TCR scores to determine optimal temporal resolution for different contact profiles.
3. **Blind audio-only control experiment**: Train a pure audio-to-action policy (no vision) on LIBERO contact-intensive tasks. Compare performance against Audio-VLA to isolate the contribution of audio-vision fusion versus audio alone.