---
ver: rpa2
title: 'Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose
  Multimodal Alignment Blind Spots'
arxiv_id: '2504.03735'
source_url: https://arxiv.org/abs/2504.03735
tags:
- layer
- arxiv
- attacks
- refusal
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Role-Modality Attacks (RMA), a novel class
  of adversarial attacks that exploit structural vulnerabilities in multimodal language
  models (MMLMs) by manipulating input prompt structures rather than query content.
  The attacks consist of Role Confusion (swapping user and assistant roles) and Modality
  Manipulation (altering image token positions), which expose weaker alignment in
  the user role compared to the assistant role.
---

# Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots

## Quick Facts
- arXiv ID: 2504.03735
- Source URL: https://arxiv.org/abs/2504.03735
- Reference count: 37
- Key outcome: RMA attacks expose weaker alignment in user role vs assistant role, with adversarial training reducing ASR from 21.25%-75.04% to 2.31%-0% across models

## Executive Summary
This paper introduces Role-Modality Attacks (RMA), a novel class of adversarial attacks that exploit structural vulnerabilities in multimodal language models by manipulating input prompt structures rather than query content. The attacks consist of Role Confusion (swapping user and assistant roles) and Modality Manipulation (altering image token positions), which successfully bypass safety mechanisms. Tested across three vision-language models on eight attack settings, RMA shows that the user role is significantly less aligned than the assistant role. The authors propose adversarial training on these structural perturbations to induce content-focused refusal decisions, achieving significant ASR reduction while preserving model utility.

## Method Summary
The authors evaluate RMA attacks on three vision-language models (Qwen2-VL-7B-Instruct, LLaVA-1.5-7b-hf, Phi-3.5-vision-instruct) using 8 attack settings combining 2 role states with 4 image positions. ASR is measured via target-string matching and Llama-Guard-3-8B classifier on AdvBench and HarmBench datasets. Adversarial training uses QLoRA on LLM components only, training on 9,994 samples with all 8 perturbations, reducing ASR from 21.25%-75.04% to 2.31%-0% while maintaining VQA-V2 accuracy.

## Key Results
- Role Confusion attack successfully bypasses safety mechanisms by exploiting weaker alignment in user role tokens
- Modality Manipulation shows image token position affects refusal behavior, with some positions reducing ASR to near 100%
- Compositional attacks (role confusion + modality manipulation) achieve highest ASR but projection analysis reveals nuanced effectiveness patterns
- Adversarial training on structural perturbations reduces ASR across all models while preserving task utility

## Why This Works (Mechanism)

### Mechanism 1: Role Confusion Exploits Asymmetric Alignment
Swapping user and assistant roles bypasses safety mechanisms because alignment training focuses primarily on the assistant role, leaving the user role unaligned. When roles are swapped, the model generates from a "user perspective" that was never trained to refuse harmful requests.

### Mechanism 2: Projection Strength onto Negative Refusal Direction Predicts Attack Success
Attack effectiveness correlates with the projection coefficient of attack vectors onto the negative refusal feature direction. Successful attacks shift harmful query representations away from the refusal direction and toward harmless regions in activation space, with projection magnitude determining bypass success more reliably than directional alignment alone.

### Mechanism 3: Adversarial Training on Structural Perturbations Induces Content-Focus
Training on diverse structural perturbations for both harmful and benign queries causes the model to base refusal decisions on query content rather than structural cues. By presenting identical harmful content under multiple structural configurations—all labeled as refusal-worthy—the model learns that structural tokens are not predictive of harmfulness.

## Foundational Learning

- Concept: Chat templates and role tokens
  - Why needed here: Understanding how VLMs structure inputs (special tokens like `<|user|>`, `<|assistant|>`, `<|image|>`) is prerequisite to grasping how structural perturbations work.
  - Quick check question: Can you identify where the image token is positioned in the default Phi-3.5-vision-instruct template versus the `img_end` attack setting?

- Concept: Residual stream activations and difference-in-means
  - Why needed here: The paper extracts "refusal features" by computing mean activation differences between harmful and harmless prompts at each layer—understanding this extraction method is essential for interpreting the projection analysis.
  - Quick check question: Given harmful prompts D_harmful and harmless prompts D_harmless, write the equation for computing refusal feature vector r_RF at layer l.

- Concept: Projection vs. cosine similarity in high-dimensional spaces
  - Why needed here: The paper argues that cosine similarity alone fails to capture attack strength; understanding why projection coefficient (which scales magnitude) matters for measuring representation shift is critical.
  - Quick check question: If an attack vector has high cosine similarity with the negative refusal direction but small projection coefficient, what does this imply about its effectiveness?

## Architecture Onboarding

- Component map:
```
Input Prompt → [Chat Template Application] → Tokenized Sequence
                      ↓
              [Role Token Positions] + [Image Token Position]
                      ↓
         [Vision Encoder] → Image Embeddings (if present)
                      ↓
         [LLM Backbone] → Residual Stream Activations H(x)
                      ↓
         [Final Token Position] → Refusal/Generation Decision
```

- Critical path:
1. RMA attacks modify the tokenized sequence structure (role swap, image reposition) without changing semantic content
2. Activations at final token position (where refusal decision occurs) are shifted toward harmless regions
3. Projection strength onto negative refusal direction predicts ASR

- Design tradeoffs:
- Defense coverage vs. utility: Adversarial training on all 8 settings maximally reduces ASR but risks over-refusal; paper shows utility preserved (VQA-V2 accuracy stable) but does not test on broader task suites
- Layer selection for analysis: Paper uses all layers for projection analysis but intermediate layers (around layer 16 for Qwen) show peak projection strength—future steering interventions could target these layers specifically
- Perturbation diversity vs. training cost: Using all 8 settings during training improves robustness but increases training data size 8x

- Failure signatures:
- High cosine similarity + low projection coefficient = attack appears aligned but lacks strength (may fail)
- Overfitting to training perturbations = novel structural attacks may still succeed
- Models vary in sensitivity: LLaVA vulnerable to both attack types; Phi vulnerable to role confusion; Qwen vulnerable mainly to compositional attacks

- First 3 experiments:
1. Replicate the `img_end_swap` attack on LLaVA-1.5-7b-hf using AdvBench (expected ASR ~90%+) and verify with Llama-Guard-3-8B classifier
2. Extract refusal features using 500 AdvBench + 500 Alpaca prompts, compute layer-wise projection coefficients for `swap` vs. `img_end_swap` attack vectors on Qwen2-VL-7B-Instruct
3. Run adversarial training with QLoRA (rank 8, lr 1e-4, 1 epoch) on the combined 9,994 samples with all 8 perturbations, then evaluate ASR reduction and VQA-V2 utility preservation

## Open Questions the Paper Calls Out

### Open Question 1
How can adaptive and dynamic alignment strategies be developed to enhance robustness against structural perturbations while maintaining model capabilities as multimodal systems incorporate more modalities?
- Basis: Future work should explore adaptive alignment strategies as models incorporate more modalities, where combinatorial growth of token position permutations makes exhaustive adversarial training infeasible.

### Open Question 2
What mechanistic factors explain why the user role exhibits weaker alignment than the assistant role, and is this asymmetry a fundamental limitation of current RLHF-based alignment approaches?
- Basis: The paper demonstrates asymmetric alignment but does not investigate whether this stems from training data distributions, loss formulations, or architectural properties.

### Open Question 3
Do structural manipulation attacks and adversarial training defenses generalize to non-visual modalities (audio, video, tactile) and to closed-source models where prompt templates cannot be directly manipulated?
- Basis: The study is restricted to fully open-source vision-language models, acknowledging that real-world deployed systems typically don't expose full structured prompts to users.

### Open Question 4
Beyond projection strength onto the negative refusal direction, what latent factors in the activation space determine why some compositional attacks achieve higher ASR despite equal or lower cosine similarity with refusal features?
- Basis: The paper finds that cosine similarity alone does not fully explain compositional advantages, with some composed attacks achieving higher ASR despite equal or lower cosine similarity.

## Limitations
- The study assumes refusal is linearly representable in activation space, which may not hold across all layers or models
- Generalization claims for adversarial training beyond the 8 tested perturbation settings lack empirical validation
- The paper does not establish whether vulnerability patterns reflect genuine architectural alignment gaps or artifacts of specific implementation choices

## Confidence

**High confidence**: The empirical observation that role confusion attacks bypass safety mechanisms is well-established across multiple models and datasets. The measurement methodology (target-string matching + Llama-Guard-3-8B) is standard and reproducible.

**Medium confidence**: The claim about asymmetric alignment focusing on assistant role is plausible given the observed vulnerabilities but lacks direct evidence from alignment training procedures. The correlation between projection strength and ASR is statistically supported but does not prove causation.

**Low confidence**: The generalization claim for adversarial training beyond the 8 tested perturbation settings is not empirically validated. The assumption that refusal is purely linearly representable in activation space may not hold across all layers or models.

## Next Checks

1. **Generalization Test**: After adversarial training on the 8 perturbation settings, evaluate ASR on novel structural attacks not seen during training (e.g., inserting multiple image tokens at different positions, using different special token vocabularies). This would establish whether the defense learns content-focused reasoning or simply memorizes specific perturbations.

2. **Ablation Study on Activation Space**: Systematically manipulate the projection coefficient onto the negative refusal direction in the final token's activation and measure resulting ASR changes. This would establish whether the correlation between projection strength and attack success is causal, confirming the linear representation assumption.

3. **Cross-Model Alignment Analysis**: Compare the chat template structures and alignment training procedures across Qwen, LLaVA, and Phi models to determine whether vulnerability patterns reflect genuine architectural differences in alignment focus versus implementation artifacts. This would validate whether the asymmetric alignment claim generalizes beyond specific model implementations.