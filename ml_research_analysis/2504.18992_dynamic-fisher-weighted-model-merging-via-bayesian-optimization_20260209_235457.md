---
ver: rpa2
title: Dynamic Fisher-weighted Model Merging via Bayesian Optimization
arxiv_id: '2504.18992'
source_url: https://arxiv.org/abs/2504.18992
tags:
- merging
- df-merge
- task
- fisher
- coefficients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for model merging in multi-task
  settings and introduces Dynamic Fisher-weighted Merging (DF-Merge), which leverages
  Bayesian optimization to dynamically adjust scaling coefficients of fine-tuned models
  while incorporating Fisher information to preserve parameter importance. The method
  aims to address the performance gap between model merging and multi-task fine-tuning
  by balancing parameter interference and optimizing coefficients in a flexible manner.
---

# Dynamic Fisher-weighted Model Merging via Bayesian Optimization

## Quick Facts
- arXiv ID: 2504.18992
- Source URL: https://arxiv.org/abs/2504.18992
- Reference count: 21
- Key outcome: DF-Merge achieves 78.14% average accuracy for T5-base and 83.59% for T5-large across six diverse tasks, significantly outperforming strong baselines.

## Executive Summary
This paper proposes a unified framework for model merging in multi-task settings and introduces Dynamic Fisher-weighted Merging (DF-Merge), which leverages Bayesian optimization to dynamically adjust scaling coefficients of fine-tuned models while incorporating Fisher information to preserve parameter importance. The method aims to address the performance gap between model merging and multi-task fine-tuning by balancing parameter interference and optimizing coefficients in a flexible manner. DF-Merge significantly outperforms strong baselines, achieving 78.14% average accuracy for T5-base and 83.59% for T5-large across six diverse tasks, narrowing the gap with multi-task fine-tuning. The approach efficiently converges to near-optimal performance within a few iterations using minimal validation data.

## Method Summary
DF-Merge unifies two merging strategies—Task Arithmetic (coefficient tuning) and Fisher Merging (parameter importance weighting)—into a single framework where Fisher information is dynamically estimated at scaled model positions. A unified merging objective allows joint optimization of scaling coefficients and Fisher-based parameter weights. Bayesian optimization is used to find optimal coefficients by maximizing validation accuracy, enabling efficient search without requiring differentiability. The method computes task vectors as differences from the pre-trained model, estimates diagonal Fisher information at dynamically scaled positions, and iteratively refines coefficients through a Gaussian Process surrogate model.

## Key Results
- DF-Merge achieves 78.14% average accuracy for T5-base and 83.59% for T5-large across six diverse tasks
- Outperforms strong baselines including Task Arithmetic, Fisher Merging, and Parameter Averaging
- Converges to near-optimal performance within 9-15 iterations using as little as 5% validation data

## Why This Works (Mechanism)

### Mechanism 1: Unified Merging Objective Combines Orthogonal Improvements
- Claim: DF-Merge achieves better performance by jointly optimizing scaling coefficients (model-wise) and parameter importance weights (parameter-wise), rather than applying either strategy in isolation.
- Mechanism: The unified formula f = (∑ᵢ C_{θᵢ})⁻¹(∑ᵢ C_{θᵢ}·λᵢτᵢ) + θ_{pre} generalizes both Task Arithmetic (C=I, optimize λ) and Fisher Merging (C=Fisher, λ=1/M). DF-Merge sets C = diag(F̂_{θᵢ(λᵢ)) and jointly optimizes λ via Bayesian optimization, allowing Fisher information to be estimated at scaled positions along the interpolation path rather than only at endpoints.
- Core assumption: Parameter importance (captured by Fisher information curvature) varies along the path from pre-trained to fine-tuned model, and scaling coefficients should be tuned in conjunction with this varying importance.
- Evidence anchors:
  - [abstract]: "unify these seemingly distinct strategies into a more general merging framework"
  - [Section 3]: "This generalized perspective offers a natural way to link both approaches... GTA and Fisher Merging make orthogonal improvements over Averaging"
  - [corpus]: Weak support—neighbor papers discuss coefficient estimation and interference reduction separately (NAN, SE-Merging) but do not directly validate the unified objective claim.
- Break condition: When scaled models θᵢ(λᵢ) with low λᵢ deviate far from local minima, Fisher information may no longer capture loss-preserving directions, degrading the geometric benefit (noted in Section 5.3).

### Mechanism 2: Dynamic Fisher Information Conditions on Current Coefficients
- Claim: Estimating Fisher information at the scaled model θᵢ(λᵢ) = λᵢτᵢ + θ_{pre} (rather than only at θᵢ) preserves parameters that remain important at the current interpolation point, guiding the merged model toward a shared low-loss basin.
- Mechanism: Fisher information F_θ captures local loss curvature via E[∇ℓ(θ)∇ℓ(θ)ᵀ]. By computing diag(F̂) at θᵢ(λᵢ) dynamically each iteration, DF-Merge upweights eigenvector components along low-sensitivity (loss-flat) directions. The geometric objective (Eq. 5) shows this restricts movement along loss-insensitive principal directions.
- Core assumption: The diagonal approximation of Fisher information is sufficient to capture parameter importance; parameters are approximately independent in their gradient contributions.
- Evidence anchors:
  - [abstract]: "Each iteration of this process integrates parameter importance based on the Fisher information conditioned by the coefficients"
  - [Section 3]: "diag(F̂_{θᵢ(λᵢ)) is the diagonal Fisher Information estimated at θᵢ(λᵢ) := λᵢτᵢ + θ_{pre}. Intuitively, this allows Fisher Information to be estimated with varying λᵢ"
  - [corpus]: Limited—neighbors use Fisher-like weighting but do not test dynamic conditioning along interpolation paths.
- Break condition: Strong parameter correlations violate the diagonal assumption; block-diagonal or full Fisher may be needed (noted as a limitation in Section 7).

### Mechanism 3: Bayesian Optimization Enables Direct Optimization of Non-Differentiable Metrics
- Claim: Bayesian optimization finds near-optimal coefficients efficiently without requiring differentiability, allowing direct maximization of task accuracy on validation sets.
- Mechanism: A Gaussian Process prior models the black-box function f_b(λ) returning average validation accuracy. Acquisition functions (EI or UCB) balance exploration and exploitation to select next coefficient candidates. The posterior update (Eq. 10) refines the search based on observed performance.
- Core assumption: The validation set performance landscape is smooth enough for GP modeling; a small validation set (as few as 5%) is representative of test performance.
- Evidence anchors:
  - [abstract]: "Bayesian optimization is applied to dynamically adjust these coefficients, aiming to maximize overall performance on validation sets"
  - [Section 5.2]: "after the initial evaluations on 10 random points... it takes 9 iterations for DF-Merge to exploit previous observations and discover near-optimal coefficients"
  - [corpus]: Weak—neighbor Liu et al. (2024) uses BayesOpt for checkpoint merging in pretraining, but not for task-vector coefficient search in multi-task merging.
- Break condition: Highly noisy or discontinuous metric landscapes may violate GP smoothness assumptions; very high-dimensional coefficient spaces (many tasks) may require more iterations than feasible.

## Foundational Learning

- Concept: Fisher Information Matrix
  - Why needed here: Provides a tractable approximation to the Hessian (loss curvature) using only first-order gradients, indicating which parameters are most sensitive to change and thus most important to preserve.
  - Quick check question: Why is the empirical Fisher computed over the model's predictive distribution p_θ(y|x) rather than the true labels?

- Concept: Gaussian Process Regression
  - Why needed here: Underlies Bayesian optimization by providing a posterior distribution over the unknown objective function, enabling principled uncertainty-guided exploration.
  - Quick check question: How does the choice of kernel function affect the smoothness assumptions about the accuracy landscape?

- Concept: Mode Connectivity / Linear Mode Connectivity
  - Why needed here: Explains why linear interpolation between fine-tuned models from the same initialization can produce viable multi-task models—they reside in connected low-loss basins.
  - Quick check question: Why does mode connectivity break when models are trained from different random initializations without permutation alignment?

## Architecture Onboarding

- Component map: Task Vector Extraction -> Fisher Information Estimator -> Merge Function -> Validation Evaluator -> Bayesian Optimizer

- Critical path:
  1. Initialize with 10 random coefficient vectors in [0,1]^M
  2. For each λ: scale task vectors -> compute Fisher at scaled positions -> merge -> evaluate on validation
  3. Update GP posterior -> select next λ via acquisition function
  4. Repeat until convergence or iteration budget (default: 50 iterations)
  5. Return merged model from best-performing coefficients

- Design tradeoffs:
  - Diagonal vs. full Fisher: Diagonal is O(d) and tractable; full is O(d²) and intractable for large models
  - Validation set size: Smaller sets reduce labeling cost but may introduce noise; paper shows 5% is often sufficient
  - Acquisition function choice: EI emphasizes expected improvement; UCB offers tunable exploration-exploitation via β

- Failure signatures:
  - Coefficients collapsing to near-zero -> scaled models leave local minima, Fisher information unreliable
  - Slow convergence beyond ~15 iterations -> metric landscape may be flat or noisy; consider early stopping
  - Large task count causing slow optimization -> consider hierarchical merging or task grouping

- First 3 experiments:
  1. Reproduction on 2-task merge: Merge PAWS + QASC T5-base models, visualize coefficient landscape (as in Figure 6), verify BayesOpt trajectory matches paper's convergence speed (~9-15 iterations).
  2. Ablation: Fisher at fixed vs. dynamic positions: Compare Fisher Merging (fixed at θᵢ) vs. DF-Merge (dynamic at θᵢ(λᵢ)) on same task pairs to isolate contribution of dynamic estimation.
  3. Validation set sensitivity: Run DF-Merge with 5%, 10%, 30%, 100% validation data; plot accuracy vs. validation ratio to confirm minimal-data efficiency claim.

## Open Questions the Paper Calls Out
- None

## Limitations
- Relies on models trained from the same initialization without permutation alignment
- Requires labeled validation data, though minimal amounts (5%) are sufficient
- Diagonal Fisher approximation may not capture parameter correlations in highly coupled networks

## Confidence

- **Mechanism 1 (Unified Objective)**: Medium confidence. The theoretical unification is sound, but empirical validation of orthogonality claims is limited. Neighbor papers (NAN, SE-Merging) address coefficient estimation and interference separately but do not test the unified claim directly.

- **Mechanism 2 (Dynamic Fisher)**: Medium confidence. The dynamic conditioning at scaled positions is novel and theoretically justified, but the diagonal approximation's sufficiency is untested. Neighbor work uses Fisher weighting but not dynamic conditioning along interpolation paths.

- **Mechanism 3 (Bayesian Optimization)**: Medium confidence. The approach is standard and convergence is demonstrated, but smoothness assumptions and minimal-data efficiency claims lack rigorous validation across diverse task landscapes.

## Next Checks
1. Test the orthogonality claim: Run controlled experiments comparing: (a) Fisher Merging alone (λ=1/M, C=Fisher), (b) Task Arithmetic alone (C=I, optimize λ via BayesOpt), (c) DF-Merge (joint optimization). Quantify whether gains are additive or synergistic.

2. Validate Fisher approximation quality: Compare DF-Merge performance using diagonal Fisher vs. block-diagonal (e.g., per-layer blocks) vs. full Fisher on small models where full computation is tractable. Measure degradation to assess diagonal assumption validity.

3. Characterize landscape smoothness: For each task pair, compute validation accuracy across a grid of coefficient values (not just BayesOpt samples). Plot heatmaps to visualize landscape topology, smoothness, and presence of local optima to validate GP assumptions.