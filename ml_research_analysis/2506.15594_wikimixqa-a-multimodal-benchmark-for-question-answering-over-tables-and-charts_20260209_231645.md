---
ver: rpa2
title: 'WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts'
arxiv_id: '2506.15594'
source_url: https://arxiv.org/abs/2506.15594
tags:
- charts
- tables
- chart
- table
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WikiMixQA is a multimodal benchmark for evaluating long-context
  document understanding. It contains 1,000 multiple-choice questions derived from
  4,000 Wikipedia pages, requiring models to reason over tables and charts.
---

# WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts

## Quick Facts
- arXiv ID: 2506.15594
- Source URL: https://arxiv.org/abs/2506.15594
- Reference count: 19
- Key outcome: Proprietary models achieve ~70% accuracy in oracle setting but drop to ~23% in wikidoc setting, highlighting long-context retrieval challenges.

## Executive Summary
WikiMixQA is a multimodal benchmark containing 1,000 multiple-choice questions derived from 4,000 Wikipedia pages, requiring models to reason over paired tables and charts. The benchmark emphasizes complex reasoning by combining information across modalities—table-table, chart-chart, or table-chart pairs. We evaluate 12 state-of-the-art vision-language models under three settings: blind (no context), oracle (exact relevant data provided), and wikidoc (full Wikipedia page snapshots). Results show that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required, with only GPT-4-o exceeding 50% accuracy. Open-source models perform considerably worse, with a maximum accuracy of 27%. These findings highlight the challenges of long-context multimodal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.

## Method Summary
The benchmark construction pipeline involves collecting Wikipedia documents with at least three tables and one chart, then pairing modalities based on semantic similarity using text descriptions and a BGE reranker. GPT-4-turbo generates three multiple-choice questions per pair, with InternVL2-76B performing AI-assisted validation before human curation. The final dataset spans 7 topics and documents averaging 24.18 pages. Evaluation uses 12 VLLMs across three settings: blind (no context), oracle (relevant data only), and wikidoc (full page snapshots). The study reports accuracy as the primary metric, with random baseline at 25%.

## Key Results
- Proprietary models achieve ~70% accuracy in oracle setting but drop to ~23% in wikidoc setting
- GPT-4-o is the only model exceeding 50% accuracy in wikidoc setting
- Open-source models perform near-random with maximum accuracy of 27%
- Performance varies significantly by modality pair type (chart-chart vs table-table vs table-chart)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity-Based Modality Pairing
- Claim: Selecting table-chart pairs based on semantic similarity increases the likelihood that meaningful cross-modal questions can be generated.
- Mechanism: The pipeline uses Llama-3-8B-Instruct to generate textual descriptions of HTML tables, GPT-4-turbo to describe charts, then computes similarity scores using BAAI/bge-reranker-v2-m3. Pairs with scores between the macro mean and 0.9 are retained for question generation.
- Core assumption: Textual descriptions of visual/tabular content capture enough semantic signal to identify related information across modalities.
- Evidence anchors: [section 2.2] similarity score calculation, [section 2.3] pair selection criteria, [corpus] related work on structured/visual retrieval.

### Mechanism 2: Multi-Stage Quality Control with AI-Assisted Pre-Filtering
- Claim: Using a strong VLLM to validate generated questions before human annotation improves annotation efficiency and final dataset quality.
- Mechanism: InternVL2-Llama3-76B evaluates whether sufficient information exists in the provided modalities and whether GPT-4's proposed answer is correct. Only pairs passing both checks proceed to human review, though random sampling from rejected pairs is included to preserve diversity.
- Core assumption: InternVL2's validation correlates with actual question validity; false negatives are recoverable through random sampling.
- Evidence anchors: [section 2.4] validation statistics, [corpus] comparison to crowd-sourced annotation approaches.

### Mechanism 3: Stratified Evaluation to Isolate Reasoning vs. Retrieval Failures
- Claim: Separating oracle (direct context) and wikidoc (full document) settings reveals whether models fail at reasoning or at locating relevant information in long contexts.
- Mechanism: In oracle setting, models receive only relevant table(s)/chart(s); in wikidoc, they receive full Wikipedia page snapshots. The ~20-point accuracy drop isolates retrieval/navigation challenges.
- Core assumption: The oracle setting fully controls for context availability; models do not need additional surrounding text to interpret tables/charts correctly.
- Evidence anchors: [abstract] performance deterioration analysis, [section 4] model-specific results, [corpus] related work on multimodal retrieval.

## Foundational Learning

- **Cross-Modal Reasoning**
  - Why needed here: Questions require synthesizing information from table-table, chart-chart, or table-chart pairs—no single modality suffices.
  - Quick check question: Given a table showing country populations and a chart showing GDP growth, can you formulate a question that requires both to answer?

- **Long-Context Visual Document Understanding**
  - Why needed here: Documents average 24.18 pages and 1,815 tokens; models must process multiple image segments and locate relevant content.
  - Quick check question: How would you chunk a 25-page Wikipedia article with embedded tables and charts for a VLLM with a 128K token context window?

- **Retrieval-Augmented Evaluation Design**
  - Why needed here: The wikidoc setting tests whether models can retrieve relevant modalities from unstructured document snapshots before reasoning.
  - Quick check question: In a RAG pipeline over multimodal documents, should you retrieve by text similarity, image embeddings, or both? What tradeoffs exist?

## Architecture Onboarding

- **Component map**:
  1. Document Collection: WTabHTML Wikipedia dumps → filtered for ≥3 tables + ≥1 chart (via ViT/DINOv2 classifier)
  2. Modality Selection: Llama-3-8B-Instruct (table descriptions) + GPT-4-turbo (chart descriptions) → BGE reranker for pair similarity
  3. Question Generation: GPT-4-turbo generates 3 MCQs per pair (single-modality + cross-modal)
  4. Quality Control: InternVL2-76B validates → Human annotators curate (validity + correctness)
  5. Evaluation: 12 VLLMs across blind/oracle/wikidoc settings using vLLM on 8×A100 (40GB)

- **Critical path**:
  1. Semantic similarity filtering (prevents invalid pairs)
  2. Human curation (catches edge cases AI misses)
  3. Oracle→Wikidoc comparison (diagnoses retrieval vs. reasoning failures)

- **Design tradeoffs**:
  - Image-based wikidoc snapshots vs. HTML/text: Images preserve layout but increase token cost; no text extraction means models cannot use OCR-based reasoning.
  - AI pre-filtering vs. full human annotation: Reduces annotation burden but may introduce model-specific biases.
  - Similarity threshold (macro mean to 0.9): Higher threshold yields more related pairs but may reduce diversity.

- **Failure signatures**:
  - Near-random wikidoc accuracy with high oracle accuracy → long-context retrieval failure (seen in Gemini, Claude)
  - Low oracle accuracy even with direct context → fundamental reasoning limitation (seen in open-source models: max 27%)
  - Low blind accuracy expected (~25% random baseline); high blind accuracy may indicate memorization

- **First 3 experiments**:
  1. **Baseline reproduction**: Run Qwen2-VL-7B-Instruct on oracle setting; verify accuracy near reported 22.87%. If substantially different, check prompt formatting and image preprocessing.
  2. **Retrieval ablation**: For wikidoc setting, provide ground-truth page segment containing the relevant modalities vs. full document. Measure accuracy gap to quantify pure retrieval cost.
  3. **Modality-specific analysis**: Compare performance on chart-chart vs. table-table vs. table-chart questions. Per Table 3, GPT-4o excels at chart-chart (71.31%); test if your model shows similar patterns or different weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does integrating textual representations of Wikipedia pages alongside image snapshots significantly improve VLLM performance on long-context reasoning tasks?
- **Basis in paper:** [explicit] Section 7 (Limitations) states: "Our evaluation is conducted using image-based inputs (snapshots), without incorporating the textual representations of these pages. Future work could enhance the evaluation by integrating text-based inputs."
- **Why unresolved:** The current methodology relies solely on visual processing of document snapshots. It remains unclear if the low performance in the "wikidoc" setting is due to visual extraction failures or the model's inability to handle the volume of visual tokens without supporting text.
- **What evidence would resolve it:** A comparative study evaluating models using the WikiMixQA benchmark with paired text-image inputs versus image-only inputs, specifically measuring accuracy improvements in the long-context setting.

### Open Question 2
- **Question:** What specific architectural or training deficiencies cause open-source VLLMs to perform near random levels (~27% accuracy) even when provided with direct context (oracle setting)?
- **Basis in paper:** [inferred] From Table 1 and Section 4, which note that open-source models perform "considerably worse" and exhibit "fundamental challenges in their reasoning capabilities" compared to proprietary models, which achieve ~70% in the same setting.
- **Why unresolved:** The massive gap between open-source and proprietary models in the oracle setting (where retrieval is not a factor) suggests a failure in the base reasoning or visual encoding capabilities of current open-source architectures.
- **What evidence would resolve it:** An ablation study identifying whether errors stem from visual feature extraction of tables/charts or the subsequent logical reasoning layers in open-source models.

### Open Question 3
- **Question:** Can retrieval-augmented generation (RAG) mechanisms optimized for multimodal elements mitigate the severe performance drop observed when models must locate information within long documents?
- **Basis in paper:** [inferred] From Section 4 and Table 1, noting that model performance "deteriorates significantly when retrieval from long documents is required," with most models dropping to near-random accuracy in the "wikidoc" setting despite high "oracle" scores.
- **Why unresolved:** The paper demonstrates that models struggle to locate relevant context in long inputs ("wikidoc"), but it does not test whether explicit retrieval pipelines could bridge the gap between "oracle" and "wikidoc" performance.
- **What evidence would resolve it:** Evaluating models on WikiMixQA using a multimodal RAG pipeline that first retrieves relevant chart/table snippets before attempting to answer, and comparing the results to the current end-to-end long-context approaches.

## Limitations
- The evaluation uses image-based inputs without incorporating textual representations of pages, potentially limiting performance.
- Open-source models were excluded from wikidoc evaluation due to context length limitations, creating an incomplete comparison.
- The semantic similarity pairing mechanism may miss valid cross-modal relationships if text descriptions fail to capture semantic overlap.

## Confidence
- **High**: Reported accuracy figures for closed-source models across all three settings
- **Medium**: Effectiveness of semantic similarity pairing and AI-assisted quality control
- **Medium**: Claim that WikiMixQA represents the first long-context multimodal document understanding benchmark

## Next Checks
1. **Reproduce core accuracy results**: Run Qwen2-VL-7B-Instruct on the oracle setting using the provided prompt templates. Verify accuracy near the reported 22.87%. If results differ significantly, check image preprocessing and HTML table formatting against the paper's specifications.

2. **Validate retrieval isolation**: For a subset of questions, compare model performance when given ground-truth page segments containing relevant modalities versus full document snapshots. This quantifies the pure retrieval cost that the oracle-wikidoc gap measures.

3. **Analyze modality-specific weaknesses**: Evaluate your model on chart-chart, table-table, and table-chart questions separately. The paper reports GPT-4o excels at chart-chart (71.31%) but verify whether your model shows similar patterns or different failure modes that could inform model development priorities.