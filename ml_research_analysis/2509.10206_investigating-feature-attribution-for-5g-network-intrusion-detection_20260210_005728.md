---
ver: rpa2
title: Investigating Feature Attribution for 5G Network Intrusion Detection
arxiv_id: '2509.10206'
source_url: https://arxiv.org/abs/2509.10206
tags:
- shap
- explanations
- vote-xai
- features
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of statistical (SHAP)
  and logic-based (VoTE-XAI) explainability methods for 5G intrusion detection, comparing
  feature attribution stability and computational efficiency. VoTE-XAI provides more
  concise explanations (6-15 features vs.
---

# Investigating Feature Attribution for 5G Network Intrusion Detection

## Quick Facts
- **arXiv ID**: 2509.10206
- **Source URL**: https://arxiv.org/abs/2509.10206
- **Reference count**: 40
- **Primary result**: VoTE-XAI provides more stable, concise explanations for 5G intrusion detection than SHAP while being computationally faster

## Executive Summary
This study evaluates two explainability methods for 5G network intrusion detection: SHAP (statistical) and VoTE-XAI (logic-based). The research finds that VoTE-XAI produces more concise explanations using 6-15 features compared to SHAP's 20-25 features, with greater stability across attack samples. Both methods achieve high detection accuracy (>99%), but VoTE-XAI demonstrates superior computational efficiency (0.002s vs 0.03s per explanation) and better alignment with known attack mechanics. The study raises concerns about SHAP's reliability in security-critical settings due to its variability and occasional failure to identify critical attack indicators.

## Method Summary
The study compares SHAP and VoTE-XAI explainability methods on the CSE-CIC-IDS2018 dataset for 5G intrusion detection. SHAP uses game-theoretic approaches to attribute feature importance, while VoTE-XAI employs logic-based reasoning to generate explanations. The evaluation examines feature attribution stability across attack samples, computational efficiency, and alignment with known attack mechanics. Both methods were tested for their ability to detect various attack types while providing interpretable explanations for their decisions.

## Key Results
- VoTE-XAI provides more concise explanations (6-15 features) compared to SHAP (20-25 features)
- VoTE-XAI shows greater stability across attack samples while SHAP exhibits higher variability
- VoTE-XAI is significantly faster (0.002s vs. 0.03s per explanation) and better aligned with known attack mechanics

## Why This Works (Mechanism)
The effectiveness of these explainability methods stems from their different approaches to feature attribution. SHAP uses Shapley values from cooperative game theory to distribute prediction outcomes fairly among features, while VoTE-XAI employs logic-based reasoning to identify minimal feature sets that explain model decisions. VoTE-XAI's ability to provide more stable and concise explanations likely results from its logical inference mechanisms that focus on essential decision factors rather than statistical correlations that may vary across samples.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations)**: A game-theoretic approach for explaining individual predictions by fairly distributing the prediction outcome among features
  - *Why needed*: Provides interpretable feature attributions for complex ML models
  - *Quick check*: Verify Shapley value calculations match expected marginal contributions

- **VoTE-XAI**: A logic-based explainability method that uses minimal feature sets to explain model decisions
  - *Why needed*: Offers more stable and concise explanations compared to statistical methods
  - *Quick check*: Confirm logical consistency of explanations across similar samples

- **Feature attribution stability**: The consistency of feature importance rankings across similar inputs
  - *Why needed*: Critical for reliable security monitoring where explanations must be trustworthy
  - *Quick check*: Measure coefficient of variation across attack samples

## Architecture Onboarding

**Component Map**: 5G Network -> IDS Model -> SHAP/VoTE-XAI -> Feature Attribution Output

**Critical Path**: Network Traffic -> Feature Extraction -> Model Prediction -> Explanation Generation -> Security Analyst Review

**Design Tradeoffs**: SHAP offers established theoretical foundations but requires more computation and provides less stable explanations; VoTE-XAI is faster and more stable but may sacrifice some theoretical rigor

**Failure Signatures**: SHAP may miss critical attack indicators or show high variability; VoTE-XAI might oversimplify complex attack patterns

**3 First Experiments**:
1. Compare feature attribution stability across 100 attack samples from each method
2. Measure computational overhead under varying network load conditions
3. Validate explanation quality against known attack signature databases

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single 5G dataset (CSE-CIC-IDS2018) with predefined scenarios
- Computational efficiency measurements lack detailed hardware specifications
- No established gold standard for feature attribution in intrusion detection
- High accuracy doesn't validate explanation quality

## Confidence
- **VoTE-XAI superiority claims**: Medium confidence (limited dataset generalization)
- **SHAP reliability concerns**: Low confidence (no gold standard for validation)
- **Computational efficiency claims**: Medium confidence (hardware details missing)

## Next Checks
1. Test both methods across multiple diverse 5G datasets and attack scenarios to assess generalizability
2. Conduct controlled experiments with known attack patterns to validate which method better identifies critical features
3. Implement real-time performance testing on production-grade 5G infrastructure to verify computational efficiency claims under operational conditions