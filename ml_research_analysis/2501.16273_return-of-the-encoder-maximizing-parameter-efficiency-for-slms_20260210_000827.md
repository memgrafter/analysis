---
ver: rpa2
title: 'Return of the Encoder: Maximizing Parameter Efficiency for SLMs'
arxiv_id: '2501.16273'
source_url: https://arxiv.org/abs/2501.16273
tags:
- encoder-decoder
- decoder-only
- arxiv
- encoder
- efficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Encoder-decoder architectures offer 47% lower latency and 4.7x\
  \ higher throughput than decoder-only models for small language models (\u22641B\
  \ parameters) on edge devices. The key advantage stems from one-time input processing\
  \ and separation of understanding/generation phases, making them particularly effective\
  \ for asymmetric sequence tasks."
---

# Return of the Encoder: Maximizing Parameter Efficiency for SLMs

## Quick Facts
- arXiv ID: 2501.16273
- Source URL: https://arxiv.org/abs/2501.16273
- Authors: Mohamed Elfeki; Rui Liu; Chad Voegele
- Reference count: 14
- Primary result: Encoder-decoder architectures offer 47% lower latency and 4.7x higher throughput than decoder-only models for small language models (≤1B parameters) on edge devices.

## Executive Summary
This paper systematically evaluates encoder-decoder architectures for small language models (≤1B parameters), demonstrating significant efficiency advantages over decoder-only models. The key innovation is leveraging one-time input processing and efficient separation of understanding/generation phases, which enables 47% lower latency and 4.7x higher throughput on edge devices. The authors introduce a novel knowledge distillation framework that allows small encoder-decoder models to learn from large decoder-only teachers while preserving architectural efficiency benefits, achieving up to 6 average performance points improvement across diverse tasks.

## Method Summary
The approach involves two-stage training: span corruption pretraining using 100B tokens from FineWeb-Edu, followed by task-specific fine-tuning. The encoder-decoder architecture uses a 2/3-1/3 parameter split (32 encoder/12 decoder layers for 330M parameter models) with pre-LN, RoPE, and GQA. Knowledge distillation from large decoder-only teachers employs a novel alignment procedure that handles different attention schemas through logit offset management and reverse KL-divergence combined with cross-entropy. Vision-language tasks use ViT-L-336 for image processing with token compression. All experiments use Muon optimizer, BF16 mixed precision, and are trained on 16 A100 GPUs.

## Key Results
- Encoder-decoder models achieve 47% lower first-token latency and 4.7x higher throughput than decoder-only models on edge devices
- 2/3-1/3 parameter split consistently outperforms other configurations on asymmetric sequence tasks
- Knowledge distillation from large decoder-only teachers improves encoder-decoder performance by up to 6 average points across tasks
- Encoder-decoder vision-language models outperform decoder-only counterparts by 7-11% on multimodal benchmarks while maintaining parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1: One-Time Input Processing with Fixed Latent Representations
Encoder-decoder architectures process input sequences once through bidirectional attention, storing only final-layer encoder representations for cross-attention during generation. This eliminates the need to maintain and grow a KV cache over input tokens during generation, which decoder-only models must do for the concatenated input-output sequence. The encoder creates fixed-size token representations; the decoder attends to these pre-computed states autoregressively.

### Mechanism 2: Asymmetric Parameter Allocation Favors Encoder-Heavy Configurations
Allocating more parameters to the encoder (2/3-1/3 split) improves performance on asymmetric sequence tasks where input complexity exceeds output complexity. Bidirectional attention in the encoder captures long-range dependencies and contextual relationships that require more capacity, while the decoder performs conditional generation that can leverage rich pre-computed representations with fewer layers.

### Mechanism 3: Cross-Architecture Knowledge Distillation via Sequence Alignment
Small encoder-decoder models transfer capabilities from large decoder-only teachers through a distillation framework that aligns logit distributions despite differing attention schemas. Teacher receives concatenated `[PAD]∘x∘y∘[PAD]`; student processes `x` in encoder and `[BOS]∘y` in decoder. Logits are aligned via offset management: teacher logits start at position `(|x|+ne-1)`, student logits from decoder output. Reverse KL-divergence combined with cross-entropy preserves both distributional knowledge and task fidelity.

## Foundational Learning

- **Concept: KV Cache Mechanics** - Needed to understand efficiency claims; for a 4096-token input generating 256 tokens, how many key-value pairs must a decoder-only model cache versus an encoder-decoder?
- **Concept: Cross-Attention in Encoder-Decoders** - Essential for understanding data flow; during autoregressive decoding, which representations are re-computed per token versus re-used from the encoder?
- **Concept: Knowledge Distillation Objectives** - Required for implementing KD framework; why might reverse KL be preferred over forward KL when distilling from a large, diverse teacher distribution?

## Architecture Onboarding

- **Component map:** Input → [Encoder Stack: 32 layers, bidirectional attention, RoPE] → Fixed encoder representations (seq_len × d_model) → [Decoder Stack: 12 layers, causal attention + cross-attention] → Output logits
- **Critical path:** 1) Implement encoder with RoPE + pre-LayerNorm + GQA 2) Implement decoder with causal masking, cross-attention to encoder outputs, RoPE 3) Verify cross-attention receives frozen encoder outputs during generation 4) Implement KD loss with proper logit alignment 5) Validate on SQuAD before scaling to vision-language
- **Design tradeoffs:** 2/3-1/3 vs. 1/2-1/2 split (encoder-heavy favors comprehension-heavy tasks); variance-based vision token compression (-67% tokens, -0.012 Rouge-L) vs. no compression; on-policy KD (faster, no teacher caching) vs. teacher-generation KD (slightly higher quality in some tasks)
- **Failure signatures:** First-token latency matching decoder-only (encoder not pre-computing); KD loss not decreasing (verify logit alignment offsets); long-context tasks degrading (check RoPE NTK scaling); vision-language underperforming baseline (cross-attention may not be receiving vision tokens)
- **First 3 experiments:** 1) Measure latency/throughput for encoder-decoder vs. decoder-only at 330M params on 512-token input, 128-token output across GPU/CPU/NPU 2) Compare 1/3-2/3, 1/2-1/2, 2/3-1/3 on SQuAD and XSum without KD 3) Train with α=1.0 (pure KD) and verify student can match teacher on a small held-out set

## Open Questions the Paper Calls Out

### Open Question 1
At what precise parameter scale does the encoder-decoder information bottleneck become prohibitive, causing decoder-only architectures to become more advantageous? The paper encourages investigating these limits at larger scales but experiments were limited to ≤1B parameters.

### Open Question 2
Can mechanisms like residual connections between encoders and decoders overcome the scaling limitations of encoder-decoder architectures? The paper suggests exploring novel mechanisms for information flow but conducted no experiments with alternative cross-component designs.

### Open Question 3
Can learned token weighting with appropriate auxiliary losses match or exceed the efficiency-performance trade-off of variance-based vision token selection? Learned token weighting showed degraded performance (-0.072 Rouge-L), potentially due to optimization challenges.

### Open Question 4
Does the optimal knowledge distillation mixing ratio (α) generalize across diverse task types, or does it require per-task tuning? The ablation study notes task-dependency but only limited task types were evaluated for α sensitivity.

## Limitations

- The performance characteristics of encoder-decoder architectures at larger scales (>1B parameters) remain unclear, with potential diminishing efficiency advantages
- The knowledge distillation framework requires careful task-specific hyperparameter tuning (α values) and may not generalize universally across all domains
- Vision-language task validation focuses primarily on text tasks, with additional complexity from vision components that may affect core efficiency claims

## Confidence

**High Confidence:** The fundamental latency and throughput advantages of encoder-decoder architectures for one-pass input processing are well-established and empirically validated.
**Medium Confidence:** The knowledge distillation framework's effectiveness depends on task-specific hyperparameter tuning and proper logit alignment, requiring careful implementation.
**Low Confidence:** The long-term scalability of these architectural advantages beyond 1B parameters, particularly for interactive or multi-turn applications, lacks empirical validation.

## Next Checks

1. **Architecture Scaling Study:** Reproduce latency/throughput measurements for encoder-decoder vs decoder-only models at 3B and 10B parameter scales on edge devices to reveal whether efficiency advantages persist.
2. **Multi-Turn Dialogue Benchmark:** Evaluate both architectures on a conversational benchmark requiring iterative context processing, measuring total interaction latency including input re-processing costs.
3. **Cross-Domain Distillation Transfer:** Test the knowledge distillation framework by transferring capabilities from a large encoder-decoder teacher to a small decoder-only student to validate whether the methodology is architecture-agnostic.