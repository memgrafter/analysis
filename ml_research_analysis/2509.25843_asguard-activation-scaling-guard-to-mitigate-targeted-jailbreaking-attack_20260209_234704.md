---
ver: rpa2
title: 'ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack'
arxiv_id: '2509.25843'
source_url: https://arxiv.org/abs/2509.25843
tags:
- tense
- heads
- safety
- refusal
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASGuard, a mechanistically-informed framework
  to mitigate targeted jailbreaking attacks, specifically addressing the vulnerability
  where large language models (LLMs) comply with harmful requests when phrased in
  past tense. ASGuard uses circuit analysis to identify specific attention heads causally
  linked to the targeted jailbreaking attack, then trains a precise, channel-wise
  scaling vector to recalibrate the activation of these vulnerable heads.
---

# ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack

## Quick Facts
- arXiv ID: 2509.25843
- Source URL: https://arxiv.org/abs/2509.25843
- Reference count: 40
- Primary result: Activation-scaling intervention reduces targeted jailbreaking attack success rate from 42% to 8% on Llama-3.1-8B-Instruct while preserving general capabilities.

## Executive Summary
This paper introduces ASGuard, a mechanistically-informed framework to mitigate targeted jailbreaking attacks in large language models. Specifically addressing the vulnerability where models comply with harmful requests phrased in past tense, ASGuard identifies attention heads causally linked to the attack through circuit analysis and applies channel-wise activation scaling to recalibrate these vulnerable heads. The method demonstrates significant reduction in attack success rates across three LLMs while preserving general capabilities and minimizing over-refusal, offering a balanced safety-utility trade-off through direct intervention on internal mechanisms.

## Method Summary
ASGuard employs a two-stage approach: First, it uses causal tracing to identify specific attention heads responsible for targeted jailbreaking attacks. Second, it trains a precise, channel-wise scaling vector to recalibrate the activation of these vulnerable heads. This is followed by a preventative fine-tuning phase that forces the model to learn a more robust refusal mechanism. The framework was evaluated on Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Gemma-2-9B-it, demonstrating effective mitigation of past-tense jailbreaking attacks while maintaining general capabilities.

## Key Results
- Attack success rate reduced from 42% to 8% on Llama-3.1-8B-Instruct
- Maintained general capabilities across tested LLMs while mitigating targeted attacks
- Demonstrated semantic generalization failure as root cause of past-tense jailbreaking vulnerability

## Why This Works (Mechanism)
ASGuard works by directly intervening on the internal mechanisms responsible for the vulnerability. Through circuit analysis, it identifies attention heads that are causally linked to the targeted jailbreaking attack. By applying precise, channel-wise activation scaling to these specific heads, the framework recalibrates their behavior to prevent the model from complying with harmful past-tense requests. The preventative fine-tuning phase then reinforces a more robust refusal mechanism, creating a layered defense against the attack.

## Foundational Learning
- **Causal tracing**: Method for identifying which model components are responsible for specific behaviors by systematically ablating or modifying them and observing the effects. This is needed to pinpoint the exact attention heads involved in the jailbreaking vulnerability. Quick check: Can be verified by testing whether ablation of identified heads removes the vulnerability.
- **Channel-wise activation scaling**: Technique that applies different scaling factors to different channels of activation vectors, allowing fine-grained control over model behavior. This is needed to precisely recalibrate the vulnerable attention heads without disrupting other functions. Quick check: Can be verified by measuring the change in activation distributions before and after scaling.
- **Semantic generalization**: Model's ability to correctly interpret and respond to different linguistic forms of the same meaning. This is needed to understand why past-tense jailbreaking exploits a failure in this capability. Quick check: Can be verified by testing model responses to semantically equivalent prompts in different tenses.
- **Preventative fine-tuning**: Fine-tuning phase focused on teaching the model to refuse harmful requests proactively rather than reactively. This is needed to create a more robust safety mechanism that doesn't rely solely on intervention. Quick check: Can be verified by testing model refusal rates on unseen harmful prompts.

## Architecture Onboarding
- **Component map**: Input prompts -> Causal tracing module (identifies vulnerable attention heads) -> Activation scaling layer (applies channel-wise scaling) -> Model layers -> Output generation -> Preventitive fine-tuning (optional reinforcement)
- **Critical path**: Input -> Causal tracing -> Activation scaling -> Model computation -> Output
- **Design tradeoffs**: Precision vs. generalization (circuit-specific intervention may not generalize to new attack types) vs. computational overhead (detailed circuit analysis required per attack variant)
- **Failure signatures**: Attack success rate increases if scaling factors are incorrectly calibrated; over-refusal if scaling is too aggressive; capability degradation if scaling affects non-targeted heads
- **First experiments**:
  1. Baseline attack success rate measurement on target model
  2. Causal tracing to identify vulnerable attention heads for the specific attack
  3. Activation scaling intervention with incremental scaling factors to find optimal calibration

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can ASGuard's circuit-based intervention generalize to models with fundamentally different architectures (MoE routing, distillation-based training)?
- Basis: [explicit] Section 7 states: "architectures shaped by distillation, MoE routing, or models pretrained on synthetic data can realize quite different internal computation, limiting direct transfer."
- Why unresolved: The tense-vulnerable heads identified differ significantly across the three tested models; distillation and MoE architectures may distribute safety computations differently, making localization infeasible.
- What evidence would resolve it: Apply ASGuard pipeline to MoE models (e.g., Mixtral) and distilled models, measuring whether localizable causal circuits exist and whether comparable ASR reductions are achievable.

### Open Question 2
- Question: How can the attention head intervention approach be adapted for small language models where it currently causes instability?
- Basis: [explicit] Section 7 states: "small language models such as Phi-3-mini are too sensitive for attention head intervention, as shown in (O'Brien et al., 2025; Park et al., 2025), requiring a meticulous approach."
- Why unresolved: Smaller models have less redundancy and more entangled representations; direct scaling may disrupt critical functions disproportionately.
- What evidence would resolve it: Develop and test modified scaling protocols (e.g., gentler scaling factors, different head selection criteria) on models under 3B parameters, measuring both ASR reduction and capability preservation.

### Open Question 3
- Question: Does the "preventative fine-tuning" phase provide robustness against adaptive attacks designed specifically to circumvent ASGuard?
- Basis: [inferred] The paper evaluates against the original tense jailbreak but does not test adaptive adversaries who know the defense mechanism.
- Why unresolved: Safety interventions often create new attack surfaces; an attacker could potentially exploit the modified attention patterns.
- What evidence would resolve it: Conduct adaptive red-teaming where attackers are informed of ASGuard's mechanism, attempting to craft new linguistic manipulations or adversarial prompts that bypass the modified model.

## Limitations
- Resource-intensive circuit analysis required for each new attack variant limits generalizability
- Evaluation primarily focused on past-tense jailbreaking, unclear if gains extend to other attack patterns
- Over-refusal metric lacks standardization across the field, making comparisons difficult

## Confidence
- **High confidence**: ASGuard effectively reduces attack success rates for the specific past-tense jailbreaking pattern tested across multiple models
- **Medium confidence**: The mechanistic explanation for why past-tense jailbreaking works (semantic generalization failure) is plausible but not exhaustively validated
- **Medium confidence**: The preservation of general capabilities is demonstrated but based on limited capability benchmarks

## Next Checks
1. Test ASGuard's effectiveness against diverse jailbreaking strategies beyond past-tense formulations, including multi-turn and semantic jailbreaks
2. Conduct ablation studies to quantify the individual contributions of activation scaling versus preventative fine-tuning to overall performance
3. Evaluate ASGuard's computational overhead and circuit analysis requirements at scale to assess practical deployment feasibility