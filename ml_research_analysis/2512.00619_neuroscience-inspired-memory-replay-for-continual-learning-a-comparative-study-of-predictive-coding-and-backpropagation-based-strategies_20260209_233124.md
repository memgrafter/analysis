---
ver: rpa2
title: 'Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative
  Study of Predictive Coding and Backpropagation-Based Strategies'
arxiv_id: '2512.00619'
source_url: https://arxiv.org/abs/2512.00619
tags:
- learning
- predictive
- replay
- coding
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two approaches to generative replay for continual
  learning: one based on predictive coding and one using standard backpropagation
  (VAE). The key idea is to use a generative model to synthesize samples from previously
  seen tasks and interleave them with new task data to mitigate catastrophic forgetting.'
---

# Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies

## Quick Facts
- **arXiv ID**: 2512.00619
- **Source URL**: https://arxiv.org/abs/2512.00619
- **Reference count**: 17
- **Primary result**: Predictive coding-based replay achieves 15.3% better task retention vs. backpropagation-based replay on continual learning benchmarks

## Executive Summary
This paper compares two approaches to generative replay for continual learning: predictive coding and standard backpropagation (VAE). The key idea is to use a generative model to synthesize samples from previously seen tasks and interleave them with new task data to mitigate catastrophic forgetting. Experiments on Split-MNIST and Split-CIFAR (10 and 100) show that the predictive coding-based replay achieves superior task retention—on average 15.3% better—compared to the backpropagation-based approach, while maintaining similar transfer efficiency. The predictive coding method also exhibits significantly lower forgetting across datasets, suggesting that biologically-inspired local learning rules can offer more stable solutions to continual learning challenges.

## Method Summary
The study compares predictive coding-based generative replay against backpropagation-based VAE replay for continual learning. Task models (MLP for MNIST, ResNet-18 for CIFAR) are trained sequentially on task-specific datasets. Generative models synthesize samples from previous tasks, which are interleaved with current task data during training. The predictive coding approach uses hierarchical prediction error propagation with local learning rules, while the baseline uses standard backpropagation through a VAE architecture. Key hyperparameters include β=0.5 for real/synthetic sample mixing, α=0.1 for state update rate, and λ=0.5 for error weighting.

## Key Results
- Predictive coding-based replay achieves 15.3% better task retention compared to backpropagation-based approach
- The predictive coding method exhibits significantly lower forgetting across all tested datasets
- Performance peaks at β=0.5 mixing ratio, suggesting balanced real and synthetic samples
- PC shows +15.3% retention improvement but -2.4% forward transfer vs. backprop

## Why This Works (Mechanism)

### Mechanism 1: Local Learning Rules for Stability
Predictive coding's local learning rules create more stable representations that reduce catastrophic forgetting compared to global backpropagation. Weight updates are computed using only local prediction errors (εl) and adjacent layer activations, avoiding global gradient propagation that can destabilize previously learned representations across the network.

### Mechanism 2: Hierarchical Prediction Error Propagation
Propagating prediction errors upward through layers enables iterative refinement of latent states that preserves task-relevant features. Each layer maintains predictions from latent states, computes errors, and iteratively updates states before weight updates occur.

### Mechanism 3: Balanced Generative Replay Interleaving
Mixing generated samples from previous tasks with current task data at ratio β=0.5 enables rehearsal without requiring stored exemplars. The generative model synthesizes pseudo-samples from prior task distributions, which are interleaved with real samples during training.

## Foundational Learning

- **Concept: Predictive Coding Theory**
  - **Why needed here**: Core computational framework replacing backpropagation; understanding prediction-error minimization is essential for debugging convergence.
  - **Quick check question**: Can you explain why prediction errors must propagate upward through layers rather than downward like backpropagation gradients?

- **Concept: Generative Replay in Continual Learning**
  - **Why needed here**: The entire approach depends on replay quality; misunderstanding replay dynamics leads to misdiagnosing retention failures.
  - **Quick check question**: If your generative model produces low-diversity samples, how would this affect forgetting measures across tasks?

- **Concept: Variational Autoencoders (VAE)**
  - **Why needed here**: The backpropagation baseline uses VAEs; comparing against this requires understanding reconstruction loss and KL divergence tradeoffs.
  - **Quick check question**: What happens to a VAE's sample diversity if the KL term weight is too high?

## Architecture Onboarding

- **Component map**: Task model fθ (MLP or ResNet-18) -> Generative model gφ (predictive coding or VAE) -> Replay buffer (implicit via generated samples)

- **Critical path**:
  1. Initialize fθ and gφ
  2. For each task Tt: train fθ on mixed batch -> update gφ on Dt -> evaluate all seen tasks
  3. Generate samples from gφ for next task training
  4. Measure accuracy, forgetting (peak - final), forward/backward transfer

- **Design tradeoffs**:
  - Computational cost: Predictive coding requires iterative inference (multiple state updates per sample) vs. single forward pass for VAE
  - Retention vs. transfer: PC shows +15.3% retention improvement but -2.4% forward transfer vs. backprop
  - Hyperparameter sensitivity: α (state update rate), λ (error weighting), β (mixing ratio) require tuning

- **Failure signatures**:
  - High forgetting with PC: Check α convergence (may need more iterations) or λ imbalance
  - Low generative sample quality: Inspect replay samples visually; if collapsed, VAE prior may dominate or PC predictions underfit
  - Training instability: Learning rate 10^-3 may be too aggressive for PC state updates; try 10^-4

- **First 3 experiments**:
  1. Replicate Split-MNIST baseline with β=0.5, α=0.1, λ=0.5; verify ~94% final accuracy and <3% forgetting
  2. Ablate mixing ratio β ∈ {0.3, 0.5, 0.7}; confirm peak at 0.5 as reported
  3. Compare inference iterations: run PC with 1, 5, 10 state updates per sample; plot retention vs. compute time to assess convergence sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the superior retention of predictive coding-based replay be preserved when extended to more complex generative architectures like diffusion models?
- Basis in paper: [explicit] The authors explicitly list "Extending to more complex generative models (e.g., diffusion models)" in the Future Work section (Section 6.3).

### Open Question 2
- Question: Can hybrid approaches combining predictive coding and backpropagation balance the stability of biological replay with the training efficiency of backpropagation?
- Basis in paper: [explicit] Section 6.3 identifies "Exploring hybrid approaches combining predictive coding and backpropagation" as a specific avenue for future research.

### Open Question 3
- Question: Do the optimal predictive coding parameters (such as error weighting) identified in the artificial model correspond to specific biological constraints in cortical processing?
- Basis in paper: [explicit] The authors propose "Investigating the relationship between predictive coding parameters and biological constraints" in Section 6.3.

### Open Question 4
- Question: Can the computational cost of iterative inference be reduced to make predictive coding viable for large-scale, real-time continual learning?
- Basis in paper: [inferred] Inferred from the limitation regarding "Computational overhead of iterative predictive coding inference" mentioned in Sections 6.2 and 6.3.

## Limitations
- Predictive coding network architecture details (latent dimensions, layer sizes) are underspecified
- The energy function E in Equation 3 is referenced but not explicitly defined
- The number of inference iterations for predictive coding state convergence is not stated

## Confidence

- **High**: Predictive coding achieves superior retention vs. backpropagation baseline (supported by multiple dataset results)
- **Medium**: Local learning rules create more stable representations (mechanism plausible but direct validation limited)
- **Medium**: β=0.5 mixing ratio is optimal (experimentally shown but may be task-dependent)

## Next Checks

1. Verify the exact predictive coding architecture specifications and energy function definition from the authors
2. Perform ablation studies on inference iteration count to establish convergence requirements
3. Test mixing ratio sensitivity across additional datasets to confirm β=0.5 optimality generalizes