---
ver: rpa2
title: 'Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models'
arxiv_id: '2504.14798'
source_url: https://arxiv.org/abs/2504.14798
tags:
- unlearning
- attack
- unlearned
- robust
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Machine unlearning often fails to completely remove traces of\
  \ forgotten data, leaving models vulnerable to adversarial attacks that can resurface\
  \ removed knowledge. To address this, the paper introduces Robust Unlearning\u2014\
  a security standard ensuring models are indistinguishable from retraining and resistant\
  \ to adversarial recovery."
---

# Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models

## Quick Facts
- **arXiv ID**: 2504.14798
- **Source URL**: https://arxiv.org/abs/2504.14798
- **Authors**: Hao Xuan; Xingyu Li
- **Reference count**: 18
- **Key outcome**: Machine unlearning often fails to completely remove traces of forgotten data, leaving models vulnerable to adversarial attacks that can resurface removed knowledge. To address this, the paper introduces Robust Unlearning—a security standard ensuring models are indistinguishable from retraining and resistant to adversarial recovery. The proposed Unlearning Mapping Attack (UMA) actively probes unlearned models using adversarial queries to assess residual knowledge. Experiments across discriminative and generative tasks show that even state-of-the-art unlearning methods remain vulnerable to UMA despite passing traditional verification metrics. The study establishes UMA as a practical verification tool, setting a new benchmark for assessing and improving unlearning security.

## Executive Summary
This paper addresses a critical gap in machine unlearning verification: standard methods only confirm unlearning was executed, not whether residual knowledge persists that can be recovered through adversarial attacks. The authors introduce Robust Unlearning—a security standard requiring unlearned models to be indistinguishable from retraining and resistant to adversarial recovery. They propose the Unlearning Mapping Attack (UMA), which actively probes unlearned models using adversarial queries to surface hidden traces of forgotten data. Experiments demonstrate that even state-of-the-art unlearning methods fail this rigorous standard, while exact retraining remains robust. The work establishes UMA as a practical verification tool and proposes adversarial training during unlearning as a potential defense mechanism.

## Method Summary
The paper proposes UMA as a verification framework that treats unlearning security as an adversarial optimization problem. For each sample in the forget set, UMA initializes a random perturbation and uses PGD to optimize it, minimizing the distance between the unlearned model's output on the perturbed input and the original model's output on the clean forget-set sample. The attack operates under bounded perturbation constraints (8/255 or 16/255 for images). Success is measured by Unlearning Accuracy (UA)—the attack's ability to recover forgotten outputs. The paper evaluates this framework across multiple unlearning methods (FT, RL, IU, l1-sparse, SalUn) on CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet-1k, comparing results against exact retraining baselines. Adversarial unlearning is proposed as a defense by incorporating worst-case perturbations during the forgetting process.

## Key Results
- Standard unlearning methods achieve near-zero unlearning accuracy under clean evaluation but fail dramatically under UMA (e.g., FT: UA=22.20 baseline → UA=99.96 under UMA)
- Instance-level unlearning is consistently less robust than class-level unlearning across all methods
- Exact retraining (SISA) achieves UA=0 under UMA at ε=8/255, while approximate methods show high vulnerability
- Adversarial unlearning reduces UA significantly (RL-CIFAR10: UA=18.62 → UA=1.44) but at higher computational cost

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adversarial input perturbations can recover forgotten information from unlearned models that pass standard verification.
- **Mechanism**: UMA optimizes perturbations δx to minimize the distance between the unlearned model's output on adversarial inputs and the original model's output on forget-set samples. The attack exploits the fact that standard unlearning conditions only constrain behavior on original inputs, not on perturbed inputs. By solving `arg min_δx ||fu(δx, θu) - f(x, θ)||`, UMA finds inputs that reactivate residual representations of forgotten data.
- **Core assumption**: Residual knowledge persists in weight configurations even when model behavior appears correct on standard inputs; the optimization landscape contains pathways to reactivate this knowledge.
- **Evidence anchors**: Traces of forgotten information persist in unlearned models; formal attack formulation; FMR=0.504 confirms residual memorization in LLMs; information about forgotten data remains linearly decodable from internal representations.
- **Break condition**: If unlearning truly removes all traces (parameter changes match retraining exactly), no optimization path exists to recover forgotten outputs. Table 1 shows retraining achieves UA=0 under UMA at ε=8/255, while FT achieves UA=99.96.

### Mechanism 2
- **Claim**: Standard verification metrics (MIA, backdoor tests, process logging) are insufficient to detect residual knowledge exploitable by post-unlearning attacks.
- **Mechanism**: Existing verification either (a) tests membership inference on original inputs, (b) confirms unlearning process execution via logs, or (c) measures accuracy on forget/retain sets. These do not actively search for adversarial inputs that could resurface knowledge. UMA fills this gap by treating verification as an optimization problem over the input space rather than a passive measurement.
- **Core assumption**: A model can appear to have forgotten (low unlearning accuracy, low MIA scores) while retaining latent representations that can be activated under distribution shift.
- **Evidence anchors**: Existing verification methods only confirm whether unlearning was executed, failing to detect residual information leaks; categorizes existing verification and their limitations; TAPE addresses auditing gaps; EVE notes verification is critical but remains underexplored.
- **Break condition**: If verification includes adversarial probing in its threat model, the gap closes. The paper shows SalUn achieves UA=0 without attack but UA=32.58 under ε=16/255 UMA.

### Mechanism 3
- **Claim**: Adversarial training during unlearning improves robustness against post-unlearning recovery attacks.
- **Mechanism**: The paper proposes "adversarial unlearning" by adding a robust unlearning term: `Lu(fu(xu), y) + max_δ Lu(fu(δ), y)`. This simultaneously optimizes for forgetting on clean inputs and on worst-case perturbed inputs, explicitly closing the adversarial pathway during training rather than defending at inference time.
- **Core assumption**: The forget set is available during unlearning; computational overhead from iterative adversarial sample generation is acceptable.
- **Evidence anchors**: Formal adversarial unlearning objective; RL-CIFAR10 drops from UA=18.62 to UA=1.44 with adversarial training; MIA drops from 0.0636 to 0.0032; corpus lacks direct replications of adversarial unlearning for comparison.
- **Break condition**: Scalability constraints for large forget sets; if computational budget is limited, test-time purification may be preferred despite lower robustness.

## Foundational Learning

- **Concept: Projected Gradient Descent (PGD)**
  - **Why needed here**: UMA implementation uses PGD to optimize adversarial perturbations. Understanding PGD's iterative sign-based updates is necessary to implement or modify the attack.
  - **Quick check question**: Can you explain why PGD uses `sign(∇)` rather than raw gradients, and how the projection step bounds perturbations?

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here**: MIA scores serve as a baseline verification metric in experiments. Understanding what MIA measures helps interpret why low MIA scores don't guarantee robust unlearning.
  - **Quick check question**: If a model has MIA score near 0 on the forget set, does this prove the data's influence was removed? (Answer per paper: No, UMA can still achieve high attack accuracy.)

- **Concept: Exact vs. Approximate Unlearning**
  - **Why needed here**: The paper evaluates FT, RL, IU, l1-sparse, and SalUn methods. Knowing their computational/accuracy tradeoffs contextualizes why robustness varies.
  - **Quick check question**: Why does exact unlearning (e.g., SISA retraining) remain robust under UMA while approximate methods often fail?

## Architecture Onboarding

- **Component map**: Pre-unlearning model `f(·; θ)` → Unlearning algorithm `U()` → Unlearned model `fu(·; θu)` → UMA verifier
- **Critical path**: Train baseline model on full dataset → Apply unlearning method to produce `fu` → Run UMA: For each x in forget set, optimize δx to minimize `L(f(x; θ), fu(δx; θu))` → Report UA and MIA under adversarial conditions
- **Design tradeoffs**: Perturbation bound ε: 8/255 preserves semantics for discriminative tasks; unbounded for generative tasks; PGD steps vs. step size: 100 steps at ~1/255 converges well; more steps increase cost; adversarial unlearning vs. test-time purification: former requires forget-set access and higher compute; latter is inference-only but weaker defense
- **Failure signatures**: High UA under UMA despite low baseline UA (e.g., FT: baseline UA=22.20 → UMA UA=99.96); instance-level unlearning consistently less robust than class-level; defense known to purifier: Table 6 shows if attacker knows purification, robustness degrades significantly
- **First 3 experiments**: Reproduce CIFAR10 class-wise unlearning + UMA: Train ResNet50, apply FT unlearning on class 0, run UMA at ε=8/255; expect UA near 100% if results replicate; Ablate PGD hyperparameters: Vary steps (10-200) and step size (0.1-4.0)/255 on SalUn-unlearned model; plot attack accuracy to find convergence point; Test adversarial unlearning defense: Implement Equation 6 on RL baseline with forget set; compare UA/MIA to Table 4 results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can unlearning algorithms be designed to achieve "Robust Unlearning" without incurring prohibitive computational costs, particularly for large-scale systems?
- **Basis in paper**: Section 5.3 notes that adversarial unlearning incurs computational overheads dependent on the forget set size, potentially constraining scalability for large-scale systems.
- **Why unresolved**: The paper presents adversarial unlearning as a preliminary solution but identifies the computational expense as a significant practical barrier that prevents direct application to large foundation models.
- **What evidence would resolve it**: The development of an unlearning algorithm that satisfies the Robust Unlearning criteria with computational efficiency comparable to standard approximate unlearning methods.

### Open Question 2
- **Question**: Can the verification provided by the Unlearning Mapping Attack (UMA) be theoretically guaranteed, or does the non-convex nature of the optimization leave residual vulnerabilities undetected?
- **Basis in paper**: Section 4.2 states that because the optimization problem is non-convex, the algorithm does not guarantee exploration of all possible perturbations.
- **Why unresolved**: While UMA failure strengthens empirical evidence of robustness, it does not constitute a formal proof that an adversarial input does not exist.
- **What evidence would resolve it**: A formal theoretical analysis proving that UMA's optimization landscape sufficiently approximates the true robustness bounds, or a verification method that provides formal guarantees.

### Open Question 3
- **Question**: Can test-time purification defenses be developed that remain effective against adaptive attackers who possess full knowledge of the purification mechanism?
- **Basis in paper**: Appendix A.2 shows that while VAE-based purification helps against standard attacks, robustness drops significantly when the attacker has full knowledge of the purification system.
- **Why unresolved**: The preliminary study on purification treats the attacker as oblivious to the defense; the paper explicitly leaves the scenario where the attacker adapts to the purifier unresolved.
- **What evidence would resolve it**: Empirical results showing a purification method that maintains low Unlearning Accuracy and MIA scores even under white-box adaptive attacks.

## Limitations

- Experimental scope limited to image classification/reconstruction tasks with ResNet50/MAE architectures, leaving generalization to other domains uncertain
- Unlearning algorithms tested are from a specific timeframe (2023), and newer methods might address adversarial recovery more effectively
- Implementation details for unlearning baselines lack complete hyperparameter specifications, potentially affecting reproducibility
- Adversarial unlearning defense assumes forget-set availability, which may not hold in practical scenarios
- Defense effectiveness drops significantly when attackers know purification strategies (Table 6)

## Confidence

- **High confidence**: The core mechanism that standard unlearning verification fails to detect adversarial recovery paths (Mechanisms 1-2). Well-supported by UMA formulation and experimental results across multiple datasets and methods.
- **Medium confidence**: The proposed adversarial unlearning defense (Mechanism 3) shows promising results in controlled experiments, but scalability to larger models and practical constraints remain untested.
- **Low confidence**: Generalization claims to "any unlearning method" and across all domains are overstated given the limited experimental scope to image tasks and specific architectures.

## Next Checks

1. **Cross-domain validation**: Apply UMA to text-to-image models (e.g., Stable Diffusion) or language models where sensitive concepts were removed. Verify whether adversarial prompting can resurface forgotten concepts, testing the broader applicability of UMA.

2. **Defense robustness under partial information**: Systematically evaluate test-time purification defenses when attackers have partial knowledge (know purification exists but not exact parameters). Measure how attack accuracy degrades with varying levels of defender transparency.

3. **Hyperparameter sensitivity analysis**: Conduct comprehensive ablation studies on UMA's PGD parameters (steps, step size, initialization) and unlearning method hyperparameters. Quantify how attack success varies with these choices to establish robustness bounds.