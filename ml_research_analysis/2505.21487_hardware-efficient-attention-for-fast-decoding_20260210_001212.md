---
ver: rpa2
title: Hardware-Efficient Attention for Fast Decoding
arxiv_id: '2505.21487'
source_url: https://arxiv.org/abs/2505.21487
tags:
- attention
- cache
- heads
- query
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces hardware-efficient attention mechanisms that
  address the memory bottleneck in LLM decoding by redesigning attention to maximize
  arithmetic intensity while maintaining parallelization. The authors propose Grouped-Tied
  Attention (GTA), which combines and reuses key-value states, reducing the KV cache
  by half and doubling arithmetic intensity relative to Grouped-Query Attention (GQA)
  with comparable quality.
---

# Hardware-Efficient Attention for Fast Decoding

## Quick Facts
- arXiv ID: 2505.21487
- Source URL: https://arxiv.org/abs/2505.21487
- Reference count: 40
- Key outcome: Introduces hardware-efficient attention mechanisms (GTA and GLA) that reduce KV cache by half and double arithmetic intensity, achieving up to 2× faster decoding speeds while maintaining quality.

## Executive Summary
This work addresses the memory bottleneck in LLM decoding by redesigning attention mechanisms to maximize arithmetic intensity while maintaining parallelization. The authors propose Grouped-Tied Attention (GTA), which combines and reuses key-value states to reduce KV cache by half, and Grouped Latent Attention (GLA), a parallel-friendly latent attention variant that achieves decoding speeds up to 2× faster than DeepSeek's FlashMLA in speculative decoding settings. Experimental results show GLA matches Multi-head Latent Attention (MLA) quality while being easier to shard, reducing end-to-end latency and increasing throughput by up to 2× in online serving benchmarks.

## Method Summary
The authors introduce two hardware-efficient attention mechanisms: GTA ties key and value projections to create a shared representation, reducing memory transfers by half while doubling arithmetic intensity. GLA splits the compressed latent representation into multiple heads that can be sharded across tensor parallel ranks without duplication, enabling zero-redundancy parallelization. Both mechanisms maintain model quality through careful architectural design, including partial RoPE application in GTA and query head grouping in GLA. The optimized kernels achieve up to 93% of H100's maximum memory bandwidth and 70% of its TFLOPs through asynchronous software pipelining and warp specialization.

## Key Results
- GLA achieves decoding speeds up to 2× faster than DeepSeek's FlashMLA in speculative decoding settings
- Optimized GLA kernel reaches up to 93% of H100's maximum memory bandwidth and 70% of its TFLOPs
- Experimental results show GLA matches MLA quality while being easier to shard, reducing end-to-end latency and increasing throughput by up to 2× in online serving benchmarks
- GTA reduces KV cache by half and doubles arithmetic intensity relative to Grouped-Query Attention (GQA) with comparable quality

## Why This Works (Mechanism)

### Mechanism 1: Arithmetic Intensity Amplification Through KV State Reuse
- Claim: Tying key and value states into a shared representation reduces memory transfers by half, doubling arithmetic intensity (FLOPs per byte loaded) without degrading model quality.
- Mechanism: GTA produces a single "tied KV" projection per group. The full tied dimension serves as V; the first half serves as the unrotated portion of K. A separate single-head projection (with RoPE applied) provides the positional half of K, broadcast across all heads in the group. By loading one state instead of two, the kernel performs the same attention computation while halving HBM traffic.
- Core assumption: Keys occupy a low-rank subspace (especially pre-RoPE), so the unrotated dimensions are redundant and can be shared with values without quality loss.
- Evidence anchors:
  - [abstract] "GTA... combines and reuses key and value states, reducing the KV cache by half and doubling arithmetic intensity relative to GQA with comparable quality."
  - [section 3.3.1] "GTA goes further by tying the key and value projection parameters to yield a single state... roughly doubles the arithmetic intensity, and halves the KV cache footprint relative to its GQA counterpart."
  - [corpus] TPLA paper (2508.15881) confirms MLA's single-latent design suffers KV cache duplication under tensor parallelism, validating the parallelization problem GLA addresses.
- Break condition: If RoPE must apply to the full head dimension for positional discrimination in your task, the partial-RoPE design may degrade quality; ablate RoPE fraction before committing.

### Mechanism 2: Parallelizable Latent Sharding via Multiple Latent Heads
- Claim: Splitting the compressed latent into multiple heads (GLA) enables zero-redundancy tensor parallelism, reducing per-device KV cache while matching MLA quality.
- Mechanism: GLA compresses tokens into h_c latent heads (e.g., 2), each with d_c = 2d_h (half of MLA's 4d_h). Each latent head reconstructs K/V for its assigned query group. During decoding, latent heads are sharded across TP ranks—each device loads only its assigned latent head rather than the full latent. This avoids MLA's requirement to replicate the single latent across all devices.
- Core assumption: Query heads can be partitioned into groups that attend to separate latent heads without cross-group information loss; the group-level compression preserves representational capacity.
- Evidence anchors:
  - [abstract] "GLA matches MLA quality while being easier to shard... fetches a smaller KV cache per device."
  - [section 3.3.2] "GLA splits the latent vector into two heads... and partitions the query heads into two groups. During decoding, each TP rank computes local attention using its assigned latent head."
  - [section 5.2, Figure 4] "GLA-8 (h_c=8) on eight GPUs delivers up to 2× the throughput of the single-head latent MLA."
  - [corpus] TPLA paper explicitly identifies MLA's TP duplication problem; GTA (2506.17286) independently explores grouped latent designs.
- Break condition: If your model requires very few query heads (h_q < TP degree), you cannot achieve zero-redundancy sharding; fall back to hybrid TP+DP or reduce TP degree.

### Mechanism 3: Kernel-Level Compute-Memory Overlap via Warp Specialization
- Claim: Asynchronous software pipelining with warp-specialized producer/consumer threads keeps tensor cores utilized while memory loads proceed, pushing kernels from memory-bound toward compute-bound.
- Mechanism: Producer warps issue async copies (cp.async or TMA) to load the next KV block into SRAM. Consumer warps execute tensor-core MMA on the current block. This overlap hides memory latency. For paged KV, cooperative offset calculation across threads amortizes 64-bit address computation, preventing page-size-1 slowdowns.
- Core assumption: The GPU has sufficient register/SRAM capacity to double-buffer KV blocks; the workload has enough parallelism to saturate both memory and compute paths.
- Evidence anchors:
  - [abstract] "Optimized GLA kernel reaches up to 93% of H100's maximum memory bandwidth and 70% of its TFLOPs."
  - [section 4.1] "We use software pipelining... and warp specialization... decoupling simplifies pipelining, allowing the warp scheduler to overlap memory loading and compute."
  - [section 4.2] "Each thread only needs to store the address offset of 1 row... enabling high efficiency for arbitrary page size."
  - [corpus] FlashAttention-3 (Shah et al.) applies similar async/warp-specialization techniques; this paper extends them to latent attention.
- Break condition: If sequence length is very short (L < ~512), pipelining overhead may exceed benefit; profile kernel time vs. sequence length.

## Foundational Learning

- **Arithmetic Intensity (FLOPs/byte)**
  - Why needed here: The paper's central thesis is that decoding is memory-bound because arithmetic intensity (~1 for MHA) is far below the H100's roofline (~295 FLOPs/byte). Understanding this explains why reducing KV cache and reusing loaded data improves throughput.
  - Quick check question: For a workload with 1000 FLOPs and 500 bytes loaded, is it memory-bound or compute-bound on hardware with 100 TFLOPS peak compute and 2 TB/s bandwidth?

- **Tensor Parallelism vs. Data Parallelism in Decoding**
  - Why needed here: The paper argues TP is preferred for decoding because it distributes weights and enables head-level parallelism without idle GPUs (unlike PP) or full model replication (unlike DP). GLA's advantage comes from enabling zero-redundancy TP.
  - Quick check question: Why does MLA require hybrid TP+DP to mitigate KV duplication, while GLA can use pure TP?

- **RoPE (Rotary Position Embedding) and Partial Application**
  - Why needed here: GTA's design relies on applying RoPE to only half the head dimension (the "RoPE slice"), leaving the other half unrotated and tied to V. Understanding this explains how GTA reduces cache without quality loss.
  - Quick check question: What happens if you apply RoPE to the tied portion and then try to reuse it as V?

## Architecture Onboarding

- **Component map:**
  ```
  Hidden State → [Down-projection W_DKV] → Latent c_KV (compressed)
                → [Separate K_RoPE projection] → Positional key slice

  During decode (per TP rank):
  c_KV[rank] → [Absorbed up-projection in Q/O] → Attention(Q, c_KV)
  K_RoPE → Concat with latent-derived K_NoPE → Full K

  GTA-specific: K_NoPE = tied_KV[:, :, :, :d_h/2]; V = tied_KV (full)
  GLA-specific: Multiple latent heads, each serves a query group
  ```

- **Critical path:** The attention decode kernel is the bottleneck. Profile: (1) HBM→SRAM load time for KV cache, (2) tensor-core utilization during QK^TV matmul. If (1) dominates, reduce KV footprint (GTA/GLA). If (2) underutilized, increase query heads or batch size.

- **Design tradeoffs:**
  - GTA vs GQA: GTA halves cache but requires partial RoPE; GQA uses full RoPE but larger cache.
  - GLA vs MLA: GLA enables zero-redundancy TP but caches slightly more (2.5 d_h with RoPE) than MLA (4.5 d_h total, but duplicated); MLA has higher per-head arithmetic intensity.
  - h_c (latent heads) choice: More heads = better shardability but higher aggregate cache. Match h_c to TP degree for zero redundancy.

- **Failure signatures:**
  - Perplexity spikes after switching from GQA to GTA: RoPE dimension may be too small; increase d_R.
  - Throughput doesn't improve with GLA under TP: Verify latent heads are evenly divisible by TP rank; check for accidental cache duplication.
  - Page-size-1 kernels slower than page-size-64: Distributed offset calculation not enabled; verify cp.async path is active.

- **First 3 experiments:**
  1. **Baseline kernel benchmark:** Run FlashMLA and GLA kernels on H100 with varying sequence lengths (1K–64K), batch=128, query length=1 and 2. Confirm GLA achieves stated speedups (1.2× at Q=1, 2× at Q=2).
  2. **Quality parity check:** Train small (183M) and medium (433M) models with GQA-4, GTA-4, MLA, GLA-2 on FineWeb-Edu subset (5B tokens). Compare validation perplexity and downstream accuracy (SciQ, PIQA, HellaSwag). Expect GTA≈GQA, GLA≈MLA within 0.5% accuracy.
  3. **Distributed serving stress test:** Deploy GLA-8 (TP=8) vs MLA (TP=2, DP=4) on 8×H100 with SGLang. Measure end-to-end latency and throughput at 16, 64, 128 concurrent requests (prefill 8K, decode 4K). Confirm GLA achieves 1.5–2× throughput at moderate concurrency; note crossover where hybrid DP may catch up at very high load.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GLA-8 maintain quality improvements over GQA-8 at frontier model scales (e.g., 400B parameters) when both have roughly similar KV cache budgets?
- Basis in paper: [explicit] The authors state: "A matching GLA-8 with eight latent heads would cache slightly more, 2.5 d_h per token on each device where d_h/2 comes from decoupled RoPE; whether GLA-8 improves quality over GQA-8 with roughly similar cache budget is an open question."
- Why unresolved: Experiments only validated GLA up to 1.471B parameters with two latent heads; scaling to larger models with more latent heads remains untested.
- What evidence would resolve it: Train and evaluate GLA-8 and GQA-8 models at 100B+ parameter scales, comparing perplexity and downstream task performance with matched cache budgets.

### Open Question 2
- Question: Can low-rank query and output projections successfully trade parameter count for increased query heads while preserving model quality?
- Basis in paper: [explicit] Appendix B.3 ablations showed that replacing full projections with low-rank versions "resulted in slightly worse perplexity (0.1-0.2) relative to the baselines when ablating across few ranks and query heads. A detailed study of this trade-off is left for future work."
- Why unresolved: The preliminary ablation was limited to small/medium scales; the optimal rank-to-head-count ratio for maximizing arithmetic intensity without quality loss is unknown.
- What evidence would resolve it: Systematic sweep of low-rank projection ranks and query head counts across multiple model scales, measuring perplexity, downstream accuracy, and decoding throughput.

### Open Question 3
- Question: How do hardware-efficient attention principles (high arithmetic intensity, parallelizable sharding) transfer to non-Transformer architectures like Mamba and Linear Attention?
- Basis in paper: [explicit] The authors state: "Finally, exploring how parallelization and arithmetic intensity interact in other architectures, such as Mamba and Linear Attention, could further exploit modern hardware."
- Why unresolved: The paper focused exclusively on attention-based architectures; state-space models and linear attention variants have different computational patterns and memory access profiles.
- What evidence would resolve it: Apply similar arithmetic intensity analysis and parallelization strategies to Mamba/Linear Attention, measuring decoding speed and throughput improvements relative to baseline implementations.

## Limitations
- The paper's claims about zero-redundancy tensor parallelism in GLA depend heavily on the specific implementation details of the distributed kernel, which are not fully specified in the text.
- The partial-RoPE design in GTA is critical to its KV cache reduction, but the paper doesn't provide ablation studies showing how different fractions of the head dimension used for RoPE affect model quality across different tasks.
- Quality parity claims (GLA ≈ MLA) rely on proper latent head partitioning across devices, but the failure modes when this partitioning is suboptimal are not explored.

## Confidence
- **High confidence** in the memory bottleneck characterization and the general principle that reducing KV cache through tied representations improves arithmetic intensity. The core math and memory-bandwidth analysis are sound.
- **Medium confidence** in the quality parity claims (GTA ≈ GQA, GLA ≈ MLA). The paper shows validation perplexity comparisons but doesn't explore task-specific quality degradation scenarios or provide extensive downstream task benchmarks.
- **Medium confidence** in the performance claims (2× speedup over FlashMLA). The kernel optimization techniques described are well-established, but achieving the stated 93% HBM bandwidth utilization requires precise implementation details not fully specified.

## Next Checks
1. **Quality sensitivity analysis:** Train models with GTA using different RoPE fractions (25%, 50%, 75% of head dimension) and measure perplexity degradation across multiple tasks to identify the minimum RoPE requirement for quality preservation.
2. **Distributed sharding validation:** Implement a controlled experiment where GLA is run with misaligned latent head counts vs. tensor parallel degree to quantify the performance penalty when zero-redundancy sharding fails.
3. **Page-size robustness test:** Benchmark the kernel's cooperative offset calculation mechanism across different page sizes (1, 64, 1024) and sequence lengths to verify the claimed elimination of page-size-1 slowdowns.