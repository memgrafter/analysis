---
ver: rpa2
title: Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation
arxiv_id: '2506.12879'
source_url: https://arxiv.org/abs/2506.12879
tags:
- design
- support
- agent
- https
- designers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored voice-based metacognitive support agents for
  designers working with generative AI (GenAI) in mechanical design. Designers often
  struggle with intent formulation, problem exploration, and outcome evaluation when
  using GenAI tools.
---

# Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation

## Quick Facts
- **arXiv ID**: 2506.12879
- **Source URL**: https://arxiv.org/abs/2506.12879
- **Reference count**: 40
- **Primary result**: Voice-based metacognitive support agents significantly improved feasibility of generative AI-assisted mechanical designs (average score 3.5/5) compared to no support (average score 1.0/5).

## Executive Summary
This study investigated voice-based metacognitive support agents to help designers navigate challenges when using generative AI for mechanical design. Three agent types were prototyped: SocratAIs (reflective questions), HephAIstus (planning and suggestions), and Expert-Freeform (human expert support). In a Wizard of Oz study with 20 mechanical engineers, agent-supported users produced significantly more feasible designs than unsupported users. SocratAIs most effectively supported intent formulation and problem exploration, while HephAIstus helped with software navigation and visualization. Designers appreciated agent support but highlighted trade-offs between cognitive engagement and efficiency, suggesting future work should combine multiple support strategies and give users control over support type and timing.

## Method Summary
The study used a between-subjects Wizard of Oz design with 20 mechanical engineers (ages 20-42) with 2+ years CAD/Fusion 360 experience but no prior Generative Design experience. Participants were assigned to four conditions: SocratAIs, HephAIstus, Expert-Freeform, or No Support (5 per condition). The task involved designing a lightweight, structurally sound ship engine mounting bracket using Autodesk Fusion 360's Generative Design feature, starting from a provided starter file with predefined geometric constraints. The procedure included 30-minute onboarding, two design task phases (up to 70 minutes and 30 minutes), and a 30-minute solver break. Design feasibility was scored on five criteria including FEA structural analysis, load case setup, optimized mass, fastener clearances, and part size. Agent messages were sent via a React.js web interface with Google TTS, and wizard observations were logged with timestamps.

## Key Results
- Agent-supported users produced significantly more feasible designs (average score 3.5/5) compared to unsupported users (average score 1.0/5)
- SocratAIs most effectively supported intent formulation and problem exploration through targeted questioning
- HephAIstus helped with software navigation and visualization through suggestions
- Designers appreciated agent support but highlighted trade-offs: questions encouraged deeper reflection but sometimes caused over-reliance, while suggestions improved efficiency but offered less cognitive engagement

## Why This Works (Mechanism)
Metacognitive support agents work by prompting designers to engage in reflective thinking about their design process and decisions. When designers struggle with intent formulation and problem exploration in generative AI workflows, targeted questions help them clarify their objectives and consider alternative approaches. Suggestions provide concrete guidance for navigating complex software tools and making design decisions, which is particularly helpful for novice users. The voice-based interface allows for natural, conversational interaction that can adapt to the designer's immediate needs during the creative process.

## Foundational Learning
- **Intent formulation**: Understanding how designers articulate their design goals is crucial because unclear intent leads to poor generative AI outputs. Quick check: Review participant think-aloud transcripts for statements about design goals.
- **Problem exploration**: Designers need to consider multiple design alternatives before committing to a solution. Quick check: Count the number of design iterations attempted by each participant.
- **Cognitive offloading**: Over-reliance on agent support can reduce learning and skill development. Quick check: Compare performance between supported and unsupported participants on transfer tasks.
- **Metacognitive questioning**: Strategic questions can prompt deeper reflection on design decisions. Quick check: Analyze question timing and correlation with design quality improvements.
- **Suggestion vs. questioning trade-offs**: Different support strategies have complementary strengths and weaknesses. Quick check: Compare time-on-task and final design quality between SocratAIs and HephAIstus conditions.

## Architecture Onboarding

**Component map**: Designer -> CAD Software (Fusion 360) -> Generative Design Tool -> Metacognitive Agent (SocratAIs/HephAIstus/Expert) -> Wizard Interface

**Critical path**: Designer intent formulation -> Agent questioning/suggestions -> Design iteration -> Feasibility validation -> Design refinement

**Design tradeoffs**: Questioning provides deeper cognitive engagement but slower progress; suggestions offer faster progress but less learning; voice interface enables natural interaction but may be distracting during complex tasks.

**Failure signatures**: All No Support participants fail load case specification (score=1/5); Socratic questioning can amplify cognitive offloading if repeated; HephAIstus suggestions ignored when users rely on flawed intuition.

**First experiments**:
1. Test automated SocratAIs agent without wizard intervention to validate if observed benefits persist
2. Implement hybrid agent combining questioning and suggestion strategies to test blended approach effectiveness
3. Conduct A/B testing with different question timing strategies to optimize when agents should "ask" versus "tell"

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can metacognitive support systems combine question-asking strategies with suggestions to balance cognitive engagement with efficiency?
- **Basis in paper**: [explicit] "This suggests that combining multiple strategies may ultimately prove more effective in practice, and future work should explore systems with blended approaches."
- **Why unresolved**: Each strategy (SocratAIs vs. HephAIstus) was tested in isolation; no hybrid system was evaluated.
- **What evidence would resolve it**: A within-subjects study comparing blended support agents against single-strategy agents on design quality, cognitive engagement metrics, and user preference.

### Open Question 2
- **Question**: When should metacognitive agents "ask" versus "tell" users to maximize learning without causing over-reliance?
- **Basis in paper**: [explicit] "A challenge will lie in determining when 'asking' versus 'telling' the user would be most appropriate... metacognitive processing may only be effective if preceded by adequate knowledge or initial instruction."
- **Why unresolved**: Repeated questioning sometimes amplified cognitive offloading (S3), while suggestions sometimes were ignored (H1, H5).
- **What evidence would resolve it**: Experiments that adaptively switch strategies based on real-time measures of user confidence, misconceptions, or task phase.

### Open Question 3
- **Question**: How can metacognitive support agents effectively challenge users' solidified incorrect assumptions when questioning alone fails?
- **Basis in paper**: [inferred] "Questions were less impactful when users had solidified incorrect assumptions, and that sometimes agent support could lead to additional overreliance."
- **Why unresolved**: SocratAIs could not correct S1's misconceptions through questioning alone; no fallback mechanism was tested.
- **What evidence would resolve it**: Studies testing escalatory interventions (e.g., counter-examples, guided simulations) when reflective questions fail to shift incorrect mental models.

## Limitations
- Small sample size (n=20) and between-subjects design may not capture individual differences in metacognitive strategy effectiveness
- Wizard of Oz methodology introduces variability based on wizard interpretation of participant needs
- Study focused on a single mechanical design task with novice generative design users, restricting generalizability to other domains or experienced users

## Confidence

**High confidence**: The comparative effectiveness of SocratAIs for intent formulation and problem exploration, supported by multiple participants' qualitative feedback and quantitative feasibility scores

**Medium confidence**: The observed trade-offs between cognitive engagement (SocratAIs) and efficiency (HephAIstus), as these are based on subjective participant reports and limited objective measures

**Medium confidence**: The feasibility scoring system's ability to capture design quality, given that it relies on five specific criteria that may not capture all aspects of successful generative design outcomes

## Next Checks
1. Replicate the study with a larger sample size (nâ‰¥40) using a within-subjects design to better control for individual differences in metacognitive strategy effectiveness
2. Conduct a follow-up study with experienced generative design users to test whether the observed support agent benefits extend to expert practitioners
3. Implement an automated version of SocratAIs based on the published question examples to test whether the observed benefits persist without human wizard intervention