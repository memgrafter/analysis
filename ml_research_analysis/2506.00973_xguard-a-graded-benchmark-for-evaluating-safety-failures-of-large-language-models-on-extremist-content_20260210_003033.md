---
ver: rpa2
title: 'XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language
  Models on Extremist Content'
arxiv_id: '2506.00973'
source_url: https://arxiv.org/abs/2506.00973
tags:
- content
- extremist
- safety
- severity
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "XGUARD introduces a graded benchmark and evaluation framework\
  \ for assessing the severity of extremist content generated by LLMs. Unlike binary\
  \ safety evaluations, it uses 3,840 real-world prompts and a five-level danger scale\
  \ (0\u20134) to measure both frequency and severity of harmful outputs."
---

# XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content

## Quick Facts
- arXiv ID: 2506.00973
- Source URL: https://arxiv.org/abs/2506.00973
- Reference count: 28
- Introduces a graded benchmark with 3,840 real-world prompts and five-level danger scale for assessing extremist content generated by LLMs

## Executive Summary
XGUARD introduces a novel graded benchmark and evaluation framework for assessing safety failures in large language models when exposed to extremist content. Unlike binary safety evaluations, XGUARD employs a nuanced five-level danger scale (0-4) to measure both the frequency and severity of harmful outputs. The framework includes 3,840 real-world prompts and an Attack Severity Curve (ASC) visualization tool to compare vulnerabilities across different models and defense mechanisms.

Through comprehensive experiments on six LLMs and two lightweight defense methods (SFT and ICE), XGUARD reveals significant safety gaps in current language models. The results demonstrate that ICE is more effective than SFT at reducing Attack Success Rate (ASR), with models like Gemma and DeepSeek showing high vulnerability while Llama3 exhibits strong resistance. This benchmark enables context-aware safety assessment crucial for building trustworthy LLMs in real-world applications.

## Method Summary
XGUARD constructs a benchmark using 3,840 real-world prompts specifically designed to elicit extremist content from LLMs. The evaluation employs a five-level danger scale (0-4) to assess output severity, moving beyond binary safe/unsafe classifications. The framework introduces the Attack Severity Curve (ASC) to visualize vulnerabilities across models and defenses. Experiments test six LLMs with two lightweight defense mechanisms: Supervised Fine-Tuning (SFT) and Iterative Conditional Elimination (ICE). The benchmark measures both frequency of harmful responses and their graded severity, providing a comprehensive assessment of safety failures.

## Key Results
- ICE defense mechanism outperforms SFT in reducing Attack Success Rate across all tested models
- Llama3 demonstrates the strongest resistance to extremist content generation among evaluated models
- Gemma and DeepSeek show highest vulnerability to extremist content generation attacks
- The graded danger scale reveals nuanced safety failures that binary evaluations miss

## Why This Works (Mechanism)
XGUARD's effectiveness stems from its graded approach to safety evaluation, which captures the spectrum of harmful outputs rather than treating safety as binary. The five-level danger scale allows for precise measurement of output severity, while the large prompt dataset ensures comprehensive coverage of potential attack vectors. The Attack Severity Curve provides visual comparison of vulnerabilities, enabling systematic defense evaluation. By testing both frequency and severity of harmful outputs, XGUARD identifies models that may block obvious attacks but still generate subtle extremist content.

## Foundational Learning
- **Five-level danger scale**: Needed to capture nuanced severity of harmful outputs beyond binary classification. Quick check: Does the scale maintain consistent interpretation across different evaluators?
- **Real-world prompt generation**: Essential for creating realistic attack scenarios that reflect actual threat patterns. Quick check: Are prompts representative of diverse extremist content types?
- **Attack Severity Curve (ASC)**: Required for visualizing and comparing model vulnerabilities across different defense mechanisms. Quick check: Does ASC effectively differentiate between models with similar ASR but different severity profiles?
- **Iterative Conditional Elimination (ICE)**: Needed as an advanced defense mechanism that can handle graded severity assessment. Quick check: Does ICE maintain performance while reducing harmful outputs across all severity levels?

## Architecture Onboarding

**Component Map:** LLM Models -> Attack Prompts -> Output Evaluation -> Danger Scale Assessment -> ASC Visualization

**Critical Path:** Prompt Generation → Model Response → Severity Evaluation → Safety Score Calculation → Defense Comparison

**Design Tradeoffs:** Graded scale provides nuanced assessment but increases evaluation complexity; real-world prompts ensure relevance but may miss edge cases; lightweight defenses enable practical deployment but may be less robust than comprehensive approaches.

**Failure Signatures:** High ASR with low severity scores indicates superficial filtering; consistent high-severity outputs across models suggest fundamental safety architecture weaknesses; defense ineffectiveness at specific severity levels reveals targeted vulnerability patterns.

**First Experiments:** 1) Compare baseline ASR across models using binary evaluation vs. graded assessment; 2) Test defense mechanism transferability between different model architectures; 3) Evaluate prompt diversity impact on safety score consistency.

## Open Questions the Paper Calls Out
None

## Limitations
- Five-level danger scale introduces subjectivity that may vary across evaluators
- Limited testing of only two lightweight defense mechanisms restricts generalizability
- Benchmark focuses specifically on extremist content, potentially missing other safety concerns

## Confidence
- **High confidence**: Benchmark construction methodology and comparative effectiveness of ICE versus SFT defenses
- **Medium confidence**: Generalizability of results across different LLM architectures and relative safety rankings of tested models
- **Low confidence**: Long-term effectiveness of identified defenses as adversaries adapt attack strategies

## Next Checks
1. Conduct cross-evaluator reliability studies to quantify inter-rater agreement on the five-level danger scale across diverse cultural contexts
2. Test the benchmark's predictive validity by correlating XGUARD scores with real-world incident rates in deployed LLM systems
3. Evaluate additional defense mechanisms including constitutional AI and dynamic guardrails to establish whether ICE remains superior as attack sophistication increases