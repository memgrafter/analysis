---
ver: rpa2
title: Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection?
  Evaluating In-Context Learning vs. Fine-Tuning
arxiv_id: '2509.07768'
source_url: https://arxiv.org/abs/2509.07768
tags:
- news
- fake
- text
- answer
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluates fine-tuning (FT) and in-context
  learning (ICL) for detecting hyperpartisan, fake, polarized, and harmful content
  across 10 multilingual datasets. It compares encoder and decoder architectures using
  various prompt strategies, including zero-shot, few-shot (random and DPP-selected),
  codebook, and chain-of-thought.
---

# Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning

## Quick Facts
- arXiv ID: 2509.07768
- Source URL: https://arxiv.org/abs/2509.07768
- Reference count: 31
- Primary result: Fine-tuning consistently outperforms in-context learning for misinformation detection across 10 multilingual datasets, with encoder-decoder architecture alignment depending on task type

## Executive Summary
This study systematically compares fine-tuning and in-context learning for detecting hyperpartisan, fake, polarized, and harmful content across 10 multilingual datasets. Using encoder models (RoBERTa, ModernBERT) and decoder models (LLaMA, Mistral, Qwen), the research finds that fine-tuning with LoRA adapters consistently outperforms all in-context learning strategies, particularly for fake news and political bias detection. Among ICL methods, codebook prompting provides the most reliable performance gains. The study also reveals that encoder architectures excel at linguistically-oriented tasks while decoders perform better on world-knowledge tasks, suggesting task-specific architecture selection. These findings provide actionable guidance for practitioners choosing between computational efficiency (ICL) and optimal performance (fine-tuning) in political content moderation.

## Method Summary
The study evaluates fine-tuning (FT) versus in-context learning (ICL) across 10 multilingual datasets using encoder (RoBERTa, ModernBERT) and decoder (LLaMA, Mistral, Qwen) architectures. Fine-tuning employs LoRA adapters (rank=8, alpha=16, dropout=0.1) with 3 epochs and learning rate 1e-4, averaged over 5 runs. ICL strategies include zero-shot (generic/specific), codebook prompting, few-shot (random/DPP-selected), and chain-of-thought approaches, with temperature=0 for decoding. Performance is measured using weighted F1 score and accuracy. The research systematically compares these approaches across hyperpartisan, fake news, harmful tweets, and political bias detection tasks, with particular attention to cross-lingual performance in English, Spanish, Portuguese, Bulgarian, and Arabic.

## Key Results
- Fine-tuning consistently outperformed ICL, achieving better results in 28 out of 33 cases across all tasks
- Fine-tuned decoders excelled at fake news (F1 .945) and political bias detection, while encoders achieved superior performance on hyperpartisan language (F1 .850) and harmful tweet detection
- Among ICL strategies, codebook prompting was most effective, improving performance for 3 out of 10 datasets
- DPP-selected few-shot examples reduced variance but did not consistently outperform random selection
- Chain-of-thought prompting was generally suboptimal for classification tasks, often degrading performance

## Why This Works (Mechanism)

### Mechanism 1: Fine-Tuning Superiority for Task-Specific Adaptation
Fine-tuning enables parameter updates that allow models to learn task-specific representations through weight adjustments in both the backbone and classification head, whereas ICL relies on frozen parameters activated only through prompt engineering. This explains why FT achieved 28/33 superior performance cases across all tasks.

### Mechanism 2: Architecture-Task Alignment via Attention Patterns
Encoder architectures with bidirectional attention capture nuanced linguistic features across full context simultaneously, while decoder architectures with causal attention leverage pretraining knowledge for tasks requiring factual understanding. This explains why encoders excel at linguistically-oriented tasks (HP: F1 .850) while decoders perform better on world-knowledge tasks (FN: F1 .945).

### Mechanism 3: Codebook Prompting as Structured Rule Injection
Codebook prompting provides explicit classification boundaries, typical content patterns, and edge-case examples that reduce ambiguity and bridge the gap between abstract concepts and concrete textual indicators. This structured approach was the best ICL configuration for 3 out of 10 datasets, particularly improving harmful tweet and fake news classification.

## Foundational Learning

- Concept: **Fine-Tuning with Classification Heads**
  - Why needed here: FT with classification heads (new layer added for task-specific labels) is essential for optimal performance. Understanding LoRA-based parameter-efficient fine-tuning is critical.
  - Quick check question: Can you explain why adding a classification head to a pretrained model improves performance over zero-shot inference?

- Concept: **In-Context Learning Variants (Zero-Shot, Few-Shot, CoT, Codebook)**
  - Why needed here: The paper systematically compares ICL strategies. Understanding the differences—zero-shot relies on internal knowledge; few-shot uses example demonstrations; CoT elicits reasoning; codebook provides structured rules—is essential for interpreting results.
  - Quick check question: What is the key difference between few-shot with random examples vs. DPP-selected examples in terms of expected behavior?

- Concept: **Encoder vs. Decoder Attention Mechanisms**
  - Why needed here: The architectural differences explain task-specific performance variations. Bidirectional attention (encoders) sees full context simultaneously; causal attention (decoders) processes tokens left-to-right.
  - Quick check question: Why would bidirectional attention be advantageous for detecting hyperpartisan language patterns compared to causal attention?

## Architecture Onboarding

- Component map:
  Encoders (RoBERTa, ModernBERT, etc.) -> Classification head added, full fine-tuning or LoRA
  Decoders (LLaMA, Mistral, Qwen) -> LoRA fine-tuning (query/value modules) or ICL via prompts
  Prompt pipeline: Instruction + Definition/Codebook + Input Text + Response Format

- Critical path:
  1. Data preparation: Ensure train/test splits with appropriate label balance
  2. For FT: Configure LoRA hyperparameters (rank=8, alpha=16, lr=1e-4, 3 epochs, 5 runs)
  3. For ICL: Design prompt template with structured output format (e.g., "Final Answer ==>label")
  4. Evaluation: Report weighted F1 and accuracy with standard deviation (5 runs for FT; greedy decoding for zero-shot/CoT)

- Design tradeoffs:
  - FT vs. ICL: FT achieves higher performance (28/33 cases) but requires labeled data + compute; ICL enables rapid probing without parameter updates
  - Encoder vs. Decoder: Encoders excel at linguistic tasks (HP: F1 .850 for RoBERTa-large); decoders better for world-knowledge tasks (FN: F1 .945 for LLaMA3.1-8B)
  - DPP vs. Random few-shot: DPP reduces variance but does not consistently boost performance; Random can achieve higher peaks but with more instability
  - CoT: Suboptimal for classification (only improved FNN task); adds compute cost without reliability gains

- Failure signatures:
  - Unparseable outputs: Models sometimes generate irregular label formats—solved by switching from integer to string labels and adding "Final Answer ==>" structure
  - Instruction non-compliance: Mistral-7B frequently ignored template instructions (excluded from experiments)
  - Performance collapse with more shots: Performance often peaks at 1-6 shots then declines (e.g., LLaMA dropped from F1 .751 at 1-shot to .587 at 10-shot on SH)
  - CoT confusion: For underrepresented languages, CoT tokens disrupted inference rather than aiding reasoning

- First 3 experiments:
  1. Baseline FT vs. ICL comparison: Fine-tune RoBERTa-base and LLaMA3.1-8B-Instruct on SemEval-2019 (SH dataset) with LoRA; compare against zero-shot generic/specific prompts. Expected gap: FT F1 ~.86 vs. ICL F1 ~.68-.78
  2. Codebook prompt optimization: Implement the political bias codebook from Table 4 on Qbias dataset with all three decoder models. Measure if codebook improves over zero-shot specific. Note: The paper found codebook decreased performance for PL detection—investigate why.
  3. Few-shot scaling with DPP: Test DPP-selected vs. Random examples at 1, 3, 5, 8, 10 shots on harmful tweet detection (C1E). Track variance reduction and peak performance. Expected: DPP reduces std dev but may not exceed Random's peak F1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would Retrieval-Augmented Generation (RAG) systems significantly improve ICL performance on misinformation detection tasks by incorporating up-to-date factual knowledge?
- Basis in paper: The conclusion states: "Future work could investigate the application of a RAG system to incorporate up-to-date information and improve the factual verification of news."
- Why unresolved: Current ICL approaches rely solely on frozen model knowledge, which becomes outdated and lacks task-specific factual grounding. The study found ICL consistently underperformed FT, particularly for fake news detection requiring world knowledge.
- What evidence would resolve it: Experiments comparing standard ICL against RAG-enhanced ICL on the same misinformation datasets, measuring performance gains and analyzing whether retrieved evidence reduces the FT-ICL performance gap.

### Open Question 2
- Question: What are the optimal strategies for selecting few-shot examples in political content classification, and can methods beyond DPP more reliably improve performance?
- Basis in paper: "DPP-selected examples...sometimes reduced classification variance, though it did not consistently boost performance...There is not any ideal threshold for the n-shots, since the performances vary across models and datasets."
- Why unresolved: The study found neither random nor DPP selection consistently outperformed the other, and increasing shot counts did not monotonically improve results. The mechanisms by which example selection affects political content classification remain unclear.
- What evidence would resolve it: Systematic comparison of diverse selection strategies (curriculum-based, adversarial, difficulty-sorted) across all 10 datasets, with analysis of which example properties (diversity, difficulty, representativeness) correlate with improved performance.

### Open Question 3
- Question: Would scaling to larger decoder-only LLMs (70B+ parameters) narrow or close the performance gap between ICL and fine-tuning for misinformation detection?
- Basis in paper: The limitations state: "our GPUs could not host larger models. This fact limits our findings" and "We limited our model selection to open models, while discarding closed one (e.g. Claude, OpenAI) for the sake of reproducibility and budget limitations."
- Why unresolved: The study only tested models up to 8B parameters. It remains unknown whether emergent capabilities in larger models might enable ICL to match or exceed smaller fine-tuned models, particularly for tasks requiring complex reasoning like political bias detection.
- What evidence would resolve it: Comparative experiments on the same 10 datasets using larger open models (e.g., Llama-3.1-70B, Qwen2.5-72B) under identical ICL configurations, directly comparing against the fine-tuning benchmarks established in this study.

## Limitations
- Computational constraints limited testing to models up to 8B parameters, leaving questions about larger model capabilities unanswered
- DPP-based example selection requires 2-5 minutes of computation per task, creating practical deployment overhead despite marginal gains
- Codebook prompting showed inconsistent results across languages, with notable degradation on Arabic harmful tweet detection suggesting cultural/contextual limitations
- The study did not explore parameter-efficient fine-tuning methods beyond LoRA, leaving questions about alternative adapter approaches

## Confidence
- **High confidence**: Fine-tuning superiority (28/33 cases outperforming ICL) - well-supported by statistical significance across 5 runs
- **Medium confidence**: Architecture-task alignment hypothesis - consistent patterns observed but could benefit from ablation studies isolating linguistic vs. semantic features
- **Low confidence**: Codebook prompting effectiveness - strong results for 3 datasets but degraded performance on culturally-specific tasks suggests overfitting to training domain

## Next Checks
1. **Cross-lingual robustness test**: Replicate the Arabic harmful tweet (C1A) experiment with culturally-adapted codebook prompts and compare against generic few-shot approaches to isolate whether performance degradation stems from prompt design vs. model limitations
2. **Prompt engineering ablation**: Systematically remove components from the codebook prompt (definitions, examples, classification criteria) to identify which elements drive performance gains vs. overhead
3. **Hybrid approach evaluation**: Combine fine-tuning with in-context learning by initializing ICL with FT-adapted parameters (via adapter fusion) to test whether parameter updates can enhance zero-shot generalization without full fine-tuning costs