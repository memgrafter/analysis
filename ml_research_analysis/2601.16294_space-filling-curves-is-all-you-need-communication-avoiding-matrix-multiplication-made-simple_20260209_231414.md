---
ver: rpa2
title: 'Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication
  Made Simple'
arxiv_id: '2601.16294'
source_url: https://arxiv.org/abs/2601.16294
tags:
- gemm
- performance
- sfc-ca
- memory
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of optimizing General Matrix
  Multiplication (GEMM) on modern CPU platforms with matrix multiplication accelerators,
  where performance is highly sensitive to platform-specific parameters and matrix
  shapes, leading to suboptimal performance "glass jaws" in vendor libraries. The
  authors propose a novel approach using Space Filling Curves (SFC) combined with
  Communication-Avoiding (CA) algorithms to partition the GEMM computation space in
  a locality-aware manner.
---

# Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple

## Quick Facts
- arXiv ID: 2601.16294
- Source URL: https://arxiv.org/abs/2601.16294
- Authors: Evangelos Georganas; Alexander Heinecke; Pradeep Dubey
- Reference count: 38
- Key outcome: Novel SFC-based approach to GEMM achieves up to 2x speedup over vendor libraries, is platform-oblivious, and requires only ~30 lines of code

## Executive Summary
This paper addresses the challenge of optimizing General Matrix Multiplication (GEMM) on modern CPU platforms with matrix multiplication accelerators, where performance is highly sensitive to platform-specific parameters and matrix shapes, leading to suboptimal performance "glass jaws" in vendor libraries. The authors propose a novel approach using Space Filling Curves (SFC) combined with Communication-Avoiding (CA) algorithms to partition the GEMM computation space in a locality-aware manner. By leveraging the SFC properties to implicitly map work across threads and extend to CA algorithms, the method minimizes data movement and achieves platform-oblivious, shape-oblivious matrix multiplication. The implementation, based on Tensor Processing Primitives, is compact (~30 lines of code) and outperforms vendor libraries by up to 2x (geometric mean speedup) across a range of GEMM shapes and CPU platforms (x86 and Arm), closely tracking tight roofline performance models.

## Method Summary
The paper introduces a communication-avoiding approach to GEMM by leveraging space-filling curves (SFCs) for partitioning the computation space. The method works by mapping the 2D iteration space of matrix multiplication onto a 1D SFC, which preserves spatial locality when traversing the space. This implicit partitioning strategy is then extended to the outer product formulation of CA-GEMM, where three SFCs (one per dimension) are used to distribute work across threads while minimizing communication. The implementation is built on Tensor Processing Primitives and requires only approximately 30 lines of code. The approach is designed to be platform-oblivious and shape-oblivious, automatically adapting to different hardware configurations and matrix dimensions without requiring manual tuning of parameters.

## Key Results
- Achieves up to 2x geometric mean speedup over vendor libraries across 28 GEMM shapes and 5 platforms
- Demonstrates platform-oblivious and shape-oblivious performance across x86 and Arm architectures
- Compact implementation requiring only ~30 lines of code based on Tensor Processing Primitives
- Performance closely tracks tight roofline models, indicating near-optimal data movement

## Why This Works (Mechanism)
The SFC-based approach works by exploiting the locality-preserving properties of space-filling curves to implicitly partition the GEMM computation space. When traversing a space-filling curve, nearby points in the 1D sequence tend to be spatially close in the original 2D or 3D space. This property is leveraged to minimize data movement during matrix multiplication, as threads working on nearby portions of the SFC will naturally access nearby data in memory. The method extends this concept to the communication-avoiding formulation of GEMM by using three separate SFCs (one for each dimension of the outer product formulation), ensuring that work distribution across threads maintains spatial locality while reducing communication volume. This eliminates the need for manual tuning of blocking parameters and thread mapping strategies, which typically cause performance cliffs ("glass jaws") in traditional implementations.

## Foundational Learning
- **Space Filling Curves (SFCs)**: Continuous curves that pass through every point in a multi-dimensional space exactly once. Why needed: Provides a mathematical foundation for preserving locality when mapping multi-dimensional iteration spaces to 1D sequences. Quick check: Hilbert curve, Z-order curve, Peano curve.
- **Communication-Avoiding (CA) Algorithms**: Algorithmic strategies that minimize data movement by restructuring computation to reuse data in cache. Why needed: Modern systems are bottlenecked by memory bandwidth rather than compute, making data movement reduction critical. Quick check: CA-SVD, CA-LU, CA-GEMM.
- **Roofline Model**: Performance model that relates achievable performance to machine balance and algorithmic intensity. Why needed: Provides theoretical upper bounds for performance and helps identify bottlenecks. Quick check: Arithmetic intensity vs. peak performance plot.

## Architecture Onboarding

**Component Map:**
SFC Partitioning -> Thread Mapping -> CA-GEMM Computation -> Performance Monitoring

**Critical Path:**
SFC Generation -> Work Distribution -> Matrix Multiplication Execution -> Result Verification

**Design Tradeoffs:**
- Simplicity vs. fine-grained control: The SFC approach sacrifices manual optimization knobs for automatic adaptation
- Locality vs. load balancing: SFCs naturally preserve locality but may create slight load imbalances
- Generality vs. peak performance: Platform-oblivious design may not achieve absolute peak performance on any single platform

**Failure Signatures:**
- Performance degradation when matrix dimensions are extremely small or large
- Load imbalance when SFCs create uneven work distribution
- Cache thrashing when SFCs don't align well with cache line boundaries

**3 First Experiments:**
1. Compare SFC-based GEMM performance against vendor libraries for various matrix sizes on target platform
2. Profile memory bandwidth utilization to verify communication-avoiding properties
3. Test robustness by varying thread counts and observing performance stability

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope with only 28 GEMM shapes across 5 platforms, raising generalizability concerns
- Claims of "shape-oblivious" performance require validation across the full spectrum of GEMM shapes
- Evaluation limited to x86 and Arm architectures, with no testing on GPUs or other accelerators

## Confidence
- **High confidence**: The SFC partitioning strategy and its implementation simplicity (30 lines of code) are well-demonstrated and reproducible
- **Medium confidence**: The performance claims and platform-oblivious benefits, given the limited evaluation scope and absence of statistical significance analysis
- **Low confidence**: The universal applicability claim across all GEMM shapes and hardware platforms beyond the tested configurations

## Next Checks
1. Conduct extensive testing across a comprehensive matrix dimension space (e.g., all combinations of dimensions up to 1000x1000) to verify the "shape-oblivious" claim and identify any performance cliffs
2. Evaluate the approach on GPU and specialized AI accelerators to assess cross-platform robustness and identify potential limitations in non-CPU environments
3. Perform ablation studies to quantify the individual contributions of SFC partitioning versus other implementation choices (e.g., blocking strategies, thread mapping) to isolate the true source of performance gains