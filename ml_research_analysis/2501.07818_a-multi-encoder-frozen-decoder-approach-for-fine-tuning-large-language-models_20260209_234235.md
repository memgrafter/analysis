---
ver: rpa2
title: A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models
arxiv_id: '2501.07818'
source_url: https://arxiv.org/abs/2501.07818
tags:
- freezing
- frozen
- performance
- tasks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates freezing the decoder during fine-tuning
  of encoder-decoder models to reduce deployment overhead and improve portability
  to new tasks. It evaluates freezing decoders across various natural language tasks
  using the AlexaTM model, including structured tasks like MTOP, generative tasks
  like XSUM and WebNLG, and multilingual tasks like MASSIVE.
---

# A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models

## Quick Facts
- arXiv ID: 2501.07818
- Source URL: https://arxiv.org/abs/2501.07818
- Reference count: 10
- The paper investigates freezing the decoder during fine-tuning of encoder-decoder models to reduce deployment overhead and improve portability to new tasks.

## Executive Summary
This paper proposes freezing the decoder during fine-tuning of encoder-decoder models to reduce deployment overhead and improve portability to new tasks. The approach is evaluated across various natural language tasks using the AlexaTM model, including structured tasks like MTOP, generative tasks like XSUM and WebNLG, and multilingual tasks like MASSIVE. The results demonstrate that freezing the decoder significantly improves performance on tasks with natural language outputs while mitigating catastrophic forgetting in multilingual tasks. However, it leads to performance degradation in structured tasks unless paired with a larger decoder.

## Method Summary
The paper investigates freezing the decoder during fine-tuning of encoder-decoder models to reduce deployment overhead and improve portability to new tasks. It evaluates freezing decoders across various natural language tasks using the AlexaTM model, including structured tasks like MTOP, generative tasks like XSUM and WebNLG, and multilingual tasks like MASSIVE. The experimental setup involves fine-tuning with frozen decoder and comparing performance against full fine-tuning baselines across these task categories.

## Key Results
- Freezing the decoder significantly improves performance on tasks with natural language outputs (e.g., WebNLG, CommonGen)
- Freezing the decoder mitigates catastrophic forgetting in multilingual tasks
- Performance degradation occurs in structured tasks (e.g., MTOP) unless paired with a larger decoder

## Why This Works (Mechanism)
The mechanism behind freezing the decoder's effectiveness lies in maintaining the pre-trained decoder's generative capabilities while allowing the encoder to adapt to task-specific inputs. This approach leverages the decoder's strong language modeling abilities learned during pre-training while preventing potential degradation from task-specific fine-tuning. The frozen decoder acts as a stable generative component, particularly beneficial for natural language generation tasks where high-quality output generation is critical.

## Foundational Learning
- **Encoder-Decoder Architecture**: Understanding the transformer-based encoder-decoder structure is essential as the freezing strategy specifically targets the decoder component. Why needed: The approach fundamentally relies on separating encoder adaptation from decoder stability.
- **Catastrophic Forgetting**: Knowledge of how neural networks lose previously learned information during fine-tuning is crucial for understanding the multilingual task benefits. Why needed: The paper demonstrates how freezing prevents degradation in multilingual performance.
- **Structured vs. Natural Language Tasks**: Recognizing the difference between tasks requiring precise output formats versus free-form generation explains the differential performance impact. Why needed: The effectiveness of freezing varies significantly based on task type.
- **Fine-tuning Strategies**: Understanding various parameter-efficient fine-tuning methods provides context for the freezing approach. Why needed: This work represents a specific subset of fine-tuning strategies with unique trade-offs.
- **Transformer Pre-training**: Knowledge of how decoders are pre-trained on large corpora explains their effectiveness when frozen. Why needed: The frozen decoder retains valuable generative capabilities from pre-training.

## Architecture Onboarding

**Component Map**: Input -> Multi-Encoder -> Frozen Decoder -> Output
**Critical Path**: Input text → Encoder(s) → Cross-attention with Frozen Decoder → Output generation
**Design Tradeoffs**: Freezing decoder improves stability and reduces parameters to update but may limit task-specific adaptation; larger decoders can compensate for structured tasks
**Failure Signatures**: Performance degradation in structured tasks; potential underfitting if decoder is too small for task complexity
**3 First Experiments**:
1. Compare frozen vs. unfrozen decoder performance on simple natural language generation tasks
2. Test catastrophic forgetting mitigation on bilingual translation tasks
3. Evaluate impact of decoder size when freezing for structured prediction tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on a specific large-scale encoder-decoder model (AlexaTM) and its performance may not generalize to other architectures or model sizes
- Limited evaluation of long-form generative tasks beyond XSUM and WebNLG
- No analysis of training stability or convergence speed when freezing the decoder

## Confidence
- High confidence: Freezing decoder improves performance on natural language generation tasks (WebNLG, CommonGen)
- Medium confidence: Freezing decoder mitigates catastrophic forgetting in multilingual tasks
- Medium confidence: Larger decoder can compensate for performance degradation in structured tasks
- Low confidence: Generalizability of results across different model architectures and scales

## Next Checks
1. Replicate experiments with different encoder-decoder architectures (e.g., T5, BART) to assess generalizability
2. Evaluate performance on additional long-form generative tasks (e.g., storytelling, summarization with longer inputs)
3. Conduct ablation studies to determine the optimal decoder size when freezing for structured tasks