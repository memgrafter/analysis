---
ver: rpa2
title: 'FFN Fusion: Rethinking Sequential Computation in Large Language Models'
arxiv_id: '2503.18908'
source_url: https://arxiv.org/abs/2503.18908
tags:
- layers
- fusion
- attention
- block
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FFN Fusion reduces sequential computation in LLMs by identifying\
  \ and parallelizing sequences of Feed-Forward Network layers that can operate independently.\
  \ The method fuses consecutive FFN layers into wider parallel layers after attention\
  \ pruning, achieving a 1.71\xD7 speedup in inference latency and 35\xD7 lower per-token\
  \ cost while maintaining model accuracy."
---

# FFN Fusion: Rethinking Sequential Computation in Large Language Models

## Quick Facts
- arXiv ID: 2503.18908
- Source URL: https://arxiv.org/abs/2503.18908
- Reference count: 24
- Primary result: FFN Fusion achieves 1.71× speedup and 35× lower cost while maintaining accuracy on Llama-3.1-405B

## Executive Summary
FFN Fusion introduces a novel approach to optimizing large language model inference by identifying and parallelizing sequences of Feed-Forward Network (FFN) layers that can operate independently. The method fuses consecutive FFN layers into wider parallel layers after attention pruning, creating what the authors call "Ultra" models. Applied to Llama-3.1-405B, this technique produced Ultra-253B-Base with strong benchmark performance across multiple tasks while significantly reducing computational costs.

## Method Summary
The FFN Fusion approach works by first identifying FFN-only sequences in the model architecture where attention layers can be pruned without significant accuracy loss. These sequential FFN layers are then fused into wider parallel layers, allowing for concurrent computation rather than sequential processing. The technique leverages the observation that many transformer blocks contain redundant or replaceable FFN computations that can be merged. After fusion, the model maintains its original accuracy while achieving substantial performance improvements in inference latency and cost per token. The method is particularly effective at larger scales and can be combined with other optimization techniques like quantization.

## Key Results
- Achieved 1.71× speedup in inference latency compared to baseline Llama-3.1-405B
- Reduced per-token cost by 35× while maintaining model accuracy
- Created Ultra-253B-Base model with strong performance across multiple benchmarks
- Technique shows increasing effectiveness at larger model scales

## Why This Works (Mechanism)
FFN Fusion exploits the inherent parallelism available in certain transformer architectures by identifying FFN-only sequences that don't require sequential dependencies. When attention layers are pruned or replaced, the remaining FFN layers in sequence can often be computed in parallel rather than sequentially. By fusing these layers into wider parallel layers, the method eliminates the sequential bottleneck while preserving the mathematical equivalence of the computation. This approach is particularly effective because FFN layers in transformers typically don't have cross-layer dependencies within a block, making them ideal candidates for parallelization.

## Foundational Learning

**Feed-Forward Networks (FFNs)**: Component of transformer blocks that applies position-wise transformations; why needed because FFN Fusion specifically targets these layers for parallelization; quick check: verify FFN structure has no cross-position dependencies within a layer.

**Attention Pruning**: Technique for removing or simplifying attention mechanisms while preserving model capabilities; why needed because FFN Fusion relies on creating FFN-only sequences; quick check: measure accuracy drop when pruning attention layers.

**Model Fusion**: Process of combining multiple layers into single wider layers; why needed because FFN Fusion creates wider parallel FFN layers from sequential ones; quick check: verify mathematical equivalence between sequential and fused operations.

**Transformer Block Architecture**: Standard structure containing attention and FFN components; why needed to identify FFN-only sequences suitable for fusion; quick check: map attention and FFN positions in sample blocks.

**Parallel Computation**: Executing multiple operations simultaneously rather than sequentially; why needed because FFN Fusion's primary benefit comes from parallel FFN execution; quick check: measure speedup gains from parallelization.

## Architecture Onboarding

**Component Map**: Input -> Attention Layers (pruned) -> FFN Layers (fused parallel) -> Output

**Critical Path**: Original: Attention -> FFN -> Attention -> FFN (sequential)
FFN Fusion: Attention (pruned) -> Fused Parallel FFNs -> Output

**Design Tradeoffs**: Memory vs. Speed - fused wider layers require more memory but provide parallel speedup; Accuracy vs. Optimization - attention pruning may reduce accuracy slightly but enables fusion; Complexity vs. Performance - fusion adds implementation complexity but delivers substantial gains.

**Failure Signatures**: Accuracy degradation when excessive attention pruning is applied; Memory overflow when fused layers exceed hardware limits; Suboptimal speedup when FFN sequences are too short or irregular.

**Three First Experiments**:
1. Apply attention pruning to identify FFN-only sequences in a small transformer model
2. Fuse identified FFN sequences and measure accuracy retention
3. Benchmark inference speedup and memory usage of fused vs. sequential implementation

## Open Questions the Paper Calls Out
None

## Limitations
- Technique requires specific architectural patterns that may not be universally applicable
- Performance improvements measured primarily on Llama-3.1-405B may not generalize to other model families
- Effectiveness depends heavily on quality of attention pruning, which introduces approximation errors
- Fused wider layers may create memory pressure in resource-constrained deployment scenarios

## Confidence

**High Confidence**: Core technical feasibility of FFN Fusion method and architectural modifications
**Medium Confidence**: Performance improvement claims based on specific experimental conditions
**Low Confidence**: Long-term stability and behavior across diverse real-world applications

## Next Checks
1. Test FFN Fusion across multiple LLM architectures (Mistral, Falcon, Gemini families) at different scales to verify generalizability
2. Evaluate fused models under realistic memory constraints typical of edge devices and smaller GPU setups
3. Conduct extended fine-tuning experiments to verify training stability of FFN-Fused models