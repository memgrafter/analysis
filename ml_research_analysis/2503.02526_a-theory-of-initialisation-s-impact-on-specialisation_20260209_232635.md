---
ver: rpa2
title: A Theory of Initialisation's Impact on Specialisation
arxiv_id: '2503.02526'
source_url: https://arxiv.org/abs/2503.02526
tags:
- learning
- forgetting
- networks
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how weight initialization influences neuron
  specialization in neural networks. Using theoretical frameworks including deep linear
  networks and mean-field theory, the authors show that weight imbalance and high
  weight entropy can promote specialized solutions.
---

# A Theory of Initialisation's Impact on Specialisation

## Quick Facts
- arXiv ID: 2503.02526
- Source URL: https://arxiv.org/abs/2503.02526
- Reference count: 40
- Primary result: Weight initialization schemes control neuron specialization and catastrophic forgetting patterns in neural networks

## Executive Summary
This paper investigates how weight initialization influences neuron specialization in neural networks, showing that initialization schemes can control whether networks develop specialized or shared representations. Using theoretical frameworks including deep linear networks and mean-field theory, the authors demonstrate that weight imbalance and high weight entropy can promote specialized solutions. The work provides practical insights for designing initialization schemes to achieve desired specialization properties and highlights the importance of considering initialization when applying regularization techniques like Elastic Weight Consolidation (EWC).

## Method Summary
The authors employ a combination of theoretical analysis and empirical validation to study initialization's impact on specialization. They use deep linear networks as a simplified theoretical model, analyzing the implicit bias of gradient descent and how initialization affects the solution space. Mean-field theory is applied to understand the dynamics of weight updates in the initialization phase. Empirical validation is conducted using β-VAE on the 3DShapes dataset for disentangled representation learning and MNIST for continual learning tasks. The theoretical predictions about weight imbalance and entropy are tested against observed specialization patterns and catastrophic forgetting profiles.

## Key Results
- Specialized networks exhibit non-monotonic forgetting (Maslow's Hammer profile) while non-specialized networks show monotonic forgetting
- Initialization strongly controls specialization outcomes, with imbalanced initialization improving disentanglement in β-VAE
- EWC regularization relies on specialization to function effectively, as it fails to distinguish between redundant weights in non-specialized networks

## Why This Works (Mechanism)
The mechanism relies on how initialization shapes the implicit bias of gradient descent. Initial weight configurations determine which solutions gradient descent converges to in the highly over-parameterized network space. Weight imbalance creates asymmetry that promotes specialization by encouraging neurons to adapt to different features. High weight entropy provides diversity in the initial parameter space, allowing for more varied specialization patterns to emerge during training.

## Foundational Learning
1. Deep Linear Networks - Why needed: Provide tractable theoretical framework for analyzing implicit bias; Quick check: Can solve analytically for weight updates
2. Mean-Field Theory - Why needed: Models collective behavior of large networks during initialization; Quick check: Captures average dynamics without tracking individual weights
3. Catastrophic Forgetting - Why needed: Key phenomenon that specialization impacts; Quick check: Measure performance drop on old tasks when learning new ones
4. Elastic Weight Consolidation - Why needed: Regularization technique that relies on specialized weights; Quick check: Effectiveness depends on weight redundancy
5. Weight Entropy - Why needed: Quantifies diversity in initialization; Quick check: Higher entropy correlates with better specialization
6. Implicit Bias - Why needed: Explains why gradient descent finds certain solutions; Quick check: Initialization determines which solution is reached

## Architecture Onboarding

**Component Map:**
Theory (Linear Networks) -> Analysis (Mean-Field) -> Empirical Validation (β-VAE + MNIST) -> Regularization Analysis (EWC)

**Critical Path:**
Initialization → Weight Update Dynamics → Specialization Emergence → Forgetting Profile → Regularization Effectiveness

**Design Tradeoffs:**
- Simpler linear networks enable theoretical tractability but may miss nonlinear effects
- Empirical validation requires balancing theoretical predictions with practical applicability
- EWC analysis assumes prior knowledge of specialization's role

**Failure Signatures:**
- Monotonic forgetting indicates non-specialized networks
- EWC ineffectiveness suggests insufficient weight specialization
- Poor disentanglement indicates initialization not promoting diversity

**First Experiments:**
1. Vary initialization entropy and measure specialization emergence in linear networks
2. Test Maslow's Hammer profile across different initialization schemes on MNIST
3. Compare EWC performance between specialized and non-specialized networks

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies heavily on simplified deep linear networks that may not capture nonlinear complexities
- Empirical validation limited to relatively simple datasets (MNIST, 3DShapes) rather than complex real-world tasks
- Mechanistic explanations for why specific initialization schemes work are somewhat abstract and could benefit from more rigorous formalization

## Confidence
- Theoretical framework and deep linear network analysis: High
- Empirical validation on MNIST and 3DShapes: Medium
- Generalizability to complex architectures: Low

## Next Checks
1. Test initialization-specialization relationship across diverse architectures (CNNs, Transformers) and larger-scale datasets
2. Conduct ablation studies isolating effects of weight imbalance vs. weight entropy on specialization
3. Evaluate findings under different optimization algorithms and learning rate schedules beyond standard SGD