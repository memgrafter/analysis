---
ver: rpa2
title: 'Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference'
arxiv_id: '2510.18413'
source_url: https://arxiv.org/abs/2510.18413
tags:
- adamas
- attention
- hadamard
- token
- transform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adamas addresses the efficiency challenge of long-context inference
  in large language models by proposing a sparse attention mechanism that reduces
  the quadratic complexity of self-attention. The core idea is to combine the Hadamard
  transform with bucketization and 2-bit compression to produce compact query and
  key representations, then use a lightweight Manhattan-distance estimator to select
  the most relevant key-value pairs dynamically at the token level.
---

# Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference

## Quick Facts
- **arXiv ID:** 2510.18413
- **Source URL:** https://arxiv.org/abs/2510.18413
- **Reference count:** 16
- **Primary result:** Sparse attention with 8x higher sparsity than state-of-the-art while matching full attention accuracy with as few as 64 tokens

## Executive Summary
Adamas addresses the efficiency challenge of long-context inference in large language models by proposing a sparse attention mechanism that reduces the quadratic complexity of self-attention. The core idea is to combine the Hadamard transform with bucketization and 2-bit compression to produce compact query and key representations, then use a lightweight Manhattan-distance estimator to select the most relevant key-value pairs dynamically at the token level. This approach avoids the limitations of static patterns and coarse-grained page-level selection used in prior methods. Experiments show that Adamas matches full attention accuracy with as few as 64 tokens and becomes nearly lossless at 128 tokens, supporting up to 8x higher sparsity than state-of-the-art methods. It delivers up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences, with perplexity even lower than full attention, demonstrating both high efficiency and accuracy preservation under aggressive sparsity.

## Method Summary
Adamas is a training-free sparse attention method that combines Hadamard transforms, bucketization, and 2-bit compression to enable efficient long-context inference. The approach applies Hadamard transforms to queries and keys to redistribute variance evenly across dimensions, then quantizes the transformed vectors into 2-bit representations using bucketization. These compressed codes are compared using Manhattan distance to identify the most relevant key-value pairs for each query, enabling token-level sparse attention with dramatically reduced computation while maintaining accuracy. The method achieves up to 8x higher sparsity than previous approaches by eliminating redundant tokens within selected pages and leveraging the smoothing effect of Hadamard transforms to enable accurate low-bit quantization.

## Key Results
- Achieves 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences
- Maintains full attention accuracy with as few as 64 selected tokens, becoming nearly lossless at 128 tokens
- Supports up to 8x higher sparsity ratios compared to page-level selection methods like Quest
- Demonstrates lower perplexity than full attention on PG19 dataset while using sparse attention
- Outperforms FlashInfer and Quest baselines across perplexity, F1 scores, and kernel-level latency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hadamard transform enables accurate low-bit quantization by smoothing value distributions before compression.
- **Mechanism:** The orthogonal Hadamard transform redistributes variance evenly across dimensions and suppresses extreme outlier values in Q/K vectors. This smoothing effect allows bucketization to approximate original similarity structure with minimal degradation, enabling compact 2-bit representations.
- **Core assumption:** The variance-redistribution property generalizes across different model architectures and attention head distributions.
- **Evidence anchors:**
  - [abstract]: "combines the Hadamard transform with bucketization and 2-bit compression"
  - [section 3.1]: "Hadamard transform mitigates this issue by redistributing variance more evenly across dimensions and suppressing extreme outliers"
  - [section 4.4 ablation]: Adamas without Hadamard achieves "near-zero scores across multiple datasets under small token budgets"
  - [corpus]: Limited direct validation; QuaRot (cited) demonstrates outlier suppression in rotated LLMs, but corpus papers don't directly test Hadamard+bucketization combination
- **Break condition:** If Q/K distributions are already uniform or lack dominant outliers, transform overhead may not justify gains.

### Mechanism 2
- **Claim:** Manhattan distance on 2-bit compressed codes provides sufficient signal for accurate top-k KV selection.
- **Mechanism:** After bucketization into {0,1,2,3}, similarity is approximated as negative L1 distance: `sim(bHQ, bHK) ≈ -||bHQ - bHK||₁`. This enables bit-wise integer operations instead of floating-point arithmetic, with 8 elements packed into 16-bit values for GPU efficiency.
- **Core assumption:** The 2-bit discretization preserves enough rank-order information for selection decisions.
- **Evidence anchors:**
  - [abstract]: "lightweight Manhattan-distance estimator to select the most relevant key-value pairs dynamically"
  - [section 3.2]: Mathematical formulation of bucketization operator and Manhattan distance estimator
  - [section 4.4]: L2 distance performs comparably to L1, with L1 better for "dispersed or partially relevant information"
  - [corpus]: SALE paper explores low-bit estimation for sparse attention, suggesting broader validity of low-bit approximation approaches
- **Break condition:** If critical similarity distinctions fall within single bucket boundaries, selection quality degrades.

### Mechanism 3
- **Claim:** Token-level granularity achieves higher effective sparsity than page-level selection by eliminating redundant tokens.
- **Mechanism:** Unlike Quest's page-level selection (which includes all tokens in selected pages), Adamas selects individual tokens, allowing 8× higher sparsity ratios while maintaining accuracy. This avoids diluting attention budgets on irrelevant tokens within selected pages.
- **Core assumption:** Token-level selection overhead (Manhattan distance computation) is offset by reduced attention computation on smaller candidate sets.
- **Evidence anchors:**
  - [abstract]: "avoids the limitations of static patterns and coarse-grained page-level selection"
  - [section 1]: "page-level granularity remains overly coarse... introduces token redundancy and limits achievable sparsity ratio"
  - [section 4.2]: Quest "accuracy drops sharply below 512" tokens while Adamas maintains performance at 64-128 tokens
  - [corpus]: Training-free Context-adaptive Attention and DAM papers also target dynamic fine-grained selection, supporting granularity hypothesis
- **Break condition:** If distance estimation fails to rank truly relevant tokens highly, finer granularity increases false negatives.

## Foundational Learning

- **Concept:** Sparse Attention and KV Cache Scaling
  - **Why needed here:** Adamas targets the quadratic O(L²) complexity of self-attention during autoregressive decoding, where KV cache grows linearly with sequence length.
  - **Quick check question:** Can you explain why attention complexity is quadratic during training but memory-bound during inference?

- **Concept:** Orthogonal Transforms and Information Preservation
  - **Why needed here:** The Hadamard transform's orthogonality (HH^T = I) guarantees mathematical equivalence: (QH)(KH)^T = QK^T, ensuring no information loss from the transform itself.
  - **Quick check question:** Why does orthogonality matter for maintaining attention accuracy after transformation?

- **Concept:** Quantization-Aware Representation Learning
  - **Why needed here:** Bucketization maps continuous values to discrete levels {0,1,2,3}. Understanding how outlier values dominate quantization error explains why pre-smoothing via Hadamard is critical.
  - **Quick check question:** If one dimension has values in [-100, 100] and others in [-1, 1], how would uniform bucketization perform?

## Architecture Onboarding

- **Component map:** Hadamard Transform -> Bucketization + Compression -> Manhattan Distance Estimator -> Top-k Selection -> Sparse Attention
- **Critical path:** The inference bottleneck is the Manhattan distance estimator (Table 4 shows 3bsh FLOPs vs 2bkh for actual attention). CUDA kernel optimization here determines end-to-end speedup.
- **Design tradeoffs:**
  - **2-bit vs 3-bit:** Ablation shows 3-bit marginally better but not worth 50% storage increase (Figure 6)
  - **L1 vs L2 distance:** L1 more robust to noise/sparse information; L2 better for single-document coherence (Section 4.4)
  - **Token budget selection:** 64 tokens matches full attention; 128 is nearly lossless (abstract)
- **Failure signatures:**
  - Near-zero LongBench scores at low budgets → Hadamard transform likely disabled (ablation Figure 6)
  - High perplexity under high sparsity → Using page-level instead of token-level selection
  - Accuracy degradation at 100K+ sequences → Verify bucket thresholds generalize to longer contexts
- **First 3 experiments:**
  1. **Reproduce passkey retrieval** (Table 1) on LongChat-7b-v1.5-32k at 10K length with budgets [16, 32, 64, 128] to validate token-level selection
  2. **Ablate Hadamard transform** on single LongBench task (e.g., HotpotQA) to confirm near-zero performance without smoothing
  3. **Profile kernel latency breakdown** (replicate Figure 5) on target hardware to identify Manhattan distance estimator as optimization target

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Parameter sensitivity: Fixed bucketization thresholds (B1, B2, B3) lack clear guidelines for different domains or model architectures
- CUDA kernel dependencies: Speedup claims rely on custom bit-wise operations that may be difficult to reproduce without exact implementation
- Long-context behavior: Performance at 100K+ contexts isn't thoroughly characterized despite KV cache becoming problematic at extreme lengths
- Cross-architecture generalization: All experiments use 7B parameter models, with effectiveness on larger models (70B+) unexplored

## Confidence
**High Confidence:**
- Mechanism 1 (Hadamard Transform Benefits): Section 4.4 ablation shows near-zero performance without Hadamard with clear mathematical justification
- Mechanism 3 (Token-level Granularity Advantage): Section 4.2 provides direct comparison with Quest showing Adamas maintains accuracy at 64-128 tokens

**Medium Confidence:**
- Mechanism 2 (Manhattan Distance on 2-bit Codes): Mathematical formulation is clear but claim about preserving rank-order information lacks comprehensive ablation
- Efficiency Claims: Table 4 and Figure 5 support speedup claims, but reliance on custom kernels creates uncertainty

**Low Confidence:**
- Perplexity Advantage: Claim of lower perplexity than full attention is surprising and only supported by single PG19 result without explanation

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary bucketization thresholds (B1, B2, B3) across different datasets to determine if [-10, 0, 10] values are optimal or require domain-specific tuning
2. **Long-Context Scaling Study:** Evaluate Adamas at sequence lengths beyond 32K (up to 128K or 256K) to identify performance degradation points and validate KV cache benefits
3. **Alternative Distance Metric Ablation:** Conduct comprehensive experiments comparing L1, L2, and learned distance metrics on compressed 2-bit representations across multiple datasets