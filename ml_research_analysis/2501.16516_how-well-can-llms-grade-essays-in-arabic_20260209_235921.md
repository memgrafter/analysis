---
ver: rpa2
title: How well can LLMs Grade Essays in Arabic?
arxiv_id: '2501.16516'
source_url: https://arxiv.org/abs/2501.16516
tags:
- arabic
- essay
- language
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) for Arabic automated essay scoring (AES) using the AR-AES dataset. It explores
  zero-shot, few-shot in-context learning, and fine-tuning approaches with models
  including ChatGPT, Llama, Aya, Jais, and ACEGPT.
---

# How well can LLMs Grade Essays in Arabic?

## Quick Facts
- arXiv ID: 2501.16516
- Source URL: https://arxiv.org/abs/2501.16516
- Reference count: 40
- LLMs show promise for Arabic AES but underperform fine-tuned BERT models

## Executive Summary
This study evaluates large language models for Arabic automated essay scoring using the AR-AES dataset. The research explores zero-shot, few-shot in-context learning, and fine-tuning approaches across multiple models including ChatGPT, Llama, Aya, Jais, and ACEGPT. A novel mixed-language prompting strategy combining English prompts with Arabic content achieved 49.49% improvement over monolingual approaches. While ACEGPT achieved the highest QWK of 0.67 among LLMs, it still underperformed the smaller BERT-based AraBERT model with QWK 0.88, highlighting the continued importance of fine-tuning for Arabic language tasks.

## Method Summary
The study employed a comprehensive evaluation framework testing multiple approaches to Arabic AES. Researchers implemented zero-shot and few-shot in-context learning, fine-tuning strategies, and introduced a mixed-language prompting approach. The evaluation included five Arabic language models: ChatGPT, Llama, Aya, Jais, and ACEGPT. To address tokenization challenges in Arabic processing, the team developed a custom SentencePiece tokenizer to reduce sequence lengths and computational demands. Performance was measured using Quadratic Weighted Kappa (QWK) across different essay types and courses from the AR-AES dataset.

## Key Results
- ACEGPT achieved highest QWK of 0.67 among LLMs for Arabic AES
- Mixed-language prompts improved performance by 49.49% over monolingual prompts
- BERT-based AraBERT outperformed all LLMs with QWK of 0.88
- Performance varied significantly across essay types and courses

## Why This Works (Mechanism)
The effectiveness of LLMs for Arabic AES depends on several interacting factors. The mixed-language prompting strategy leverages English's richer prompt engineering literature while maintaining Arabic content, creating a bridge between well-studied prompting techniques and Arabic language processing challenges. Fine-tuning remains crucial as it adapts general language understanding to specific grading criteria and domain terminology. The custom SentencePiece tokenizer addresses Arabic's morphological complexity by reducing sequence lengths, enabling more efficient processing without losing critical linguistic information needed for accurate scoring.

## Foundational Learning
- Arabic tokenization challenges: Arabic's rich morphology and lack of explicit word boundaries require specialized tokenization approaches. Why needed: Standard tokenizers struggle with Arabic's morphological complexity, leading to excessive sequence lengths. Quick check: Compare token counts between standard and custom tokenizers on Arabic text samples.
- Quadratic Weighted Kappa (QWK): Statistical measure for inter-rater agreement that accounts for chance agreement and ordinal scales. Why needed: AES evaluation requires metrics that capture the quality of ordinal score predictions. Quick check: Calculate QWK on sample graded essays to verify understanding.
- Prompt engineering in multilingual contexts: Techniques for designing effective prompts when mixing languages. Why needed: Mixed-language prompts showed significant performance gains, requiring understanding of cross-linguistic prompt design. Quick check: Test prompt variations on simple Arabic classification tasks.

## Architecture Onboarding

**Component Map:**
Data (AR-AES) -> Preprocessing (tokenization) -> Model (LLM or fine-tuned BERT) -> Prompt Strategy (mixed-language) -> Scoring Output -> Evaluation (QWK)

**Critical Path:**
AR-AES dataset → Custom SentencePiece tokenization → Model selection (ACEGPT vs AraBERT) → Mixed-language prompt application → QWK scoring evaluation

**Design Tradeoffs:**
- Model size vs performance: Larger LLMs showed lower QWK than smaller fine-tuned BERT models
- Computational efficiency vs accuracy: Custom tokenizer reduced sequence lengths but required validation
- Prompt complexity vs generalizability: Mixed-language prompts improved performance but may be dataset-specific

**Failure Signatures:**
- Poor tokenization leading to fragmented Arabic words and loss of meaning
- Prompt engineering that works for English but fails to translate effectively to Arabic context
- Overfitting during fine-tuning resulting in poor generalization across courses

**First 3 Experiments:**
1. Compare QWK scores using standard vs custom tokenizer on same Arabic AES dataset
2. Test monolingual vs mixed-language prompts across different essay types
3. Evaluate cross-course generalization by training on one course and testing on another

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Mixed-language prompting effectiveness may be dataset-specific and not generalizable to all Arabic essay types
- Significant performance gap between LLMs and fine-tuned BERT models suggests architectural limitations
- Performance variability across courses indicates need for adaptive models not explored in current work

## Confidence

**High confidence:**
- LLMs generally underperform fine-tuned BERT models for Arabic AES tasks
- Custom tokenization is necessary for efficient Arabic language processing

**Medium confidence:**
- 49.49% improvement from mixed-language prompts
- ACEGPT's superior performance among LLMs

**Low confidence:**
- Generalizability of results across different Arabic educational contexts
- Long-term effectiveness of mixed-language prompting strategy

## Next Checks

1. Test mixed-language prompting strategy across diverse Arabic essay genres (narrative, argumentative, descriptive) to verify robustness
2. Conduct cross-institutional validation using AES datasets from different Arabic-speaking educational systems
3. Compare custom SentencePiece tokenizer performance against recent multilingual tokenizers in Arabic AES tasks