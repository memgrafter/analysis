---
ver: rpa2
title: 'PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real
  Transfer with Few Demonstrations'
arxiv_id: '2504.20520'
source_url: https://arxiv.org/abs/2504.20520
tags:
- reward
- object
- policy
- simulation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning robust robotic policies
  from few demonstrations that generalize across variations in robot initial positions
  and object poses. The authors propose PRISM, a real-to-sim-to-real pipeline that
  constructs simulation environments from expert demonstrations by identifying scene
  objects from images and retrieving their 3D models from existing libraries.
---

# PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations

## Quick Facts
- arXiv ID: 2504.20520
- Source URL: https://arxiv.org/abs/2504.20520
- Reference count: 40
- Outperforms baseline methods by 68% with 82% average success rate across six manipulation tasks

## Executive Summary
This paper presents PRISM, a real-to-sim-to-real pipeline that enables learning robust robotic manipulation policies from few expert demonstrations. The method addresses the challenge of policy generalization across variations in robot initial positions and object poses by constructing simulation environments from real-world demonstrations and training policies using a novel projection-based reward model. By leveraging human-provided object projection relationships as prompts to a vision-language model, PRISM achieves strong performance while requiring minimal manual reward engineering.

## Method Summary
PRISM constructs simulation environments from expert demonstrations by identifying scene objects from images and retrieving their 3D models from existing libraries. The core innovation is a projection-based reward model that uses human-guided object projection relationships as prompts to a vision-language model (VLM) for RL policy training. The approach combines this VLM-based reward signal with fine-tuning using expert demonstrations, enabling effective sim-to-real transfer. The action feasibility predictor, derived from the projection-based reward model, further improves real-world task success rates.

## Key Results
- Achieves 82% average success rate across six manipulation tasks with randomized robot initial poses and object configurations
- Outperforms baseline methods by 68% in success rate
- Action feasibility predictor improves task success rates by 20% in real-world scenarios
- Successfully transfers policies from simulation to real-world execution with minimal manual reward engineering

## Why This Works (Mechanism)
PRISM leverages human-provided object projection relationships as structured prompts to a VLM, which generates semantically meaningful reward signals that capture spatial relationships between objects. This approach enables the policy to learn object-centric manipulation strategies that generalize across different initial configurations. The combination of VLM-based rewards with expert demonstration fine-tuning creates a robust learning signal that bridges the sim-to-real gap effectively.

## Foundational Learning
- **Vision-language models (VLMs)**: Understand visual and textual prompts to generate semantically meaningful outputs; needed for translating human-provided object relationships into reward signals
- **Reinforcement learning from demonstrations (RLfD)**: Combines expert demonstrations with RL to accelerate learning; needed to bootstrap policy learning with limited data
- **Sim-to-real transfer**: Transferring policies learned in simulation to real-world execution; needed to leverage the scalability and safety of simulation training
- **Object detection and 3D model retrieval**: Identifying objects in images and finding corresponding 3D models; needed to construct accurate simulation environments
- **Action feasibility prediction**: Estimating whether actions are likely to succeed in given configurations; needed to improve real-world robustness

## Architecture Onboarding

Component map: Real demonstrations -> Object detection -> 3D model retrieval -> Simulation environment construction -> VLM-based reward generation -> RL training -> Expert demonstration fine-tuning -> Action feasibility predictor -> Real-world execution

Critical path: Real demonstrations → Object detection & 3D model retrieval → Simulation environment construction → VLM-based reward generation → RL training → Real-world execution

Design tradeoffs:
- VLM-based rewards vs. manually engineered rewards: VLM approach reduces manual engineering but depends on VLM quality and human-provided prompts
- Simulation fidelity vs. computational efficiency: Higher fidelity improves transfer but increases training time
- Number of demonstrations vs. policy performance: More demonstrations improve performance but reduce the "few-shot" advantage

Failure signatures:
- Poor object detection or incorrect 3D model retrieval leading to unrealistic simulation environments
- VLM misinterpreting human-provided projection relationships, generating incorrect reward signals
- Sim-to-real gap too large for successful policy transfer, particularly with deformable objects

First experiments:
1. Validate VLM reward generation by comparing learned policies with and without expert demonstration fine-tuning
2. Test policy generalization by evaluating performance across different object poses and robot initial positions
3. Measure the contribution of the action feasibility predictor by comparing success rates with and without this component

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance heavily depends on accurate object detection and availability of corresponding 3D models in existing libraries
- Effectiveness relies on quality of human-provided object projection relationships and VLM's interpretation capability
- Limited evaluation on objects without clear 3D models or with complex geometries
- Does not address failures when sim-to-real gap is too large, especially for deformable objects requiring fine-grained tactile feedback

## Confidence
- Real-to-sim-to-real pipeline effectiveness: High
- Projection-based reward model performance: Medium
- Generalizability across diverse manipulation tasks: Medium
- Action feasibility predictor contribution: Medium

## Next Checks
1. Test PRISM on tasks involving deformable objects and materials with complex physical properties not captured in standard 3D model libraries
2. Evaluate performance degradation when object detection fails or returns incorrect 3D models, measuring the system's robustness to such errors
3. Compare PRISM against state-of-the-art sim-to-real methods using the same object sets to quantify the specific contribution of the projection-based reward model versus other components of the pipeline