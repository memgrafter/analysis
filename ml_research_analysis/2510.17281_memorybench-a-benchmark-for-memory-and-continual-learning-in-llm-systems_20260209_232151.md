---
ver: rpa2
title: 'MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems'
arxiv_id: '2510.17281'
source_url: https://arxiv.org/abs/2510.17281
tags:
- llmsys
- memory
- feedback
- user
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemoryBench, a new benchmark for evaluating
  memory and continual learning in large language model (LLM) systems. Unlike existing
  benchmarks that focus on static tasks with long context, MemoryBench tests LLMs'
  ability to learn from accumulated user feedback during service time.
---

# MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems

## Quick Facts
- **arXiv ID**: 2510.17281
- **Source URL**: https://arxiv.org/abs/2510.17281
- **Reference count**: 40
- **Key outcome**: Introduces MemoryBench, a new benchmark testing LLMs' ability to learn from accumulated user feedback during service time, revealing that current memory-based systems struggle to extract procedural knowledge while naive RAG baselines often perform comparably or better.

## Executive Summary
MemoryBench addresses a critical gap in LLM evaluation by testing continual learning capabilities during service time, rather than static pre-training tasks. The benchmark simulates real-world deployment where LLMs accumulate user feedback and must improve over time. Through 11 diverse datasets spanning multiple domains, languages, and task formats, MemoryBench reveals that existing memory-based LLM systems fail to effectively leverage procedural knowledge from user feedback, while simple retrieval-augmented generation baselines often match or exceed their performance. The work highlights the need for more sophisticated memory architectures that can distinguish between factual knowledge and experience-based skills.

## Method Summary
MemoryBench evaluates LLM memory and continual learning through a standardized framework using 11 heterogeneous datasets converted into unified (query, context, metadata) tuples. User feedback is simulated using LLM-as-user approaches, generating verbose feedback plus satisfaction scores that probabilistically map to explicit actions (like/dislike/copy). The benchmark supports both off-policy evaluation (pre-collected feedback logs) and on-policy learning (interactive feedback collection). Systems are evaluated using dataset-specific metrics (F1, accuracy, Rouge-L, etc.) normalized across tasks. Memory systems are tested for both effectiveness (task performance) and efficiency (memory time and prediction time per case), with the requirement that memory operations complete within 10 seconds per case for practical deployment.

## Key Results
- Current memory-based LLM systems (Mem0, MemoryOS, A-Mem) fail to consistently outperform naive RAG baselines on MemoryBench
- Memory systems struggle to extract procedural knowledge from verbose user feedback, often performing worse than retrieval-only approaches
- Specialized memory architectures incur prohibitive latency costs (MemoryOS >17s/case for memory construction, Mem0 shows super-linear scaling)
- RAG baselines demonstrate robust performance across heterogeneous task formats without requiring explicit memory-type modeling
- MemoryBench reveals significant gaps in both effectiveness and efficiency of current memory systems for continual learning

## Why This Works (Mechanism)

### Mechanism 1: Feedback-to-Procedural-Memory Conversion
The benchmark tests whether simulated user feedback can be converted into procedural memory that improves task performance. The LLM-as-user simulator generates structured feedback (reasoning, satisfaction scores, action probabilities) which gets stored as dialog history, then memory systems retrieve historical sessions to condition generation. Current systems fail to extract transferable procedural knowledge from verbose or action-based signals, treating feedback as declarative facts rather than experience-based skills.

### Mechanism 2: Declarative-Procedural Memory Taxonomy Governs Retrieval Strategy
Effective memory systems must distinguish between declarative memory (semantic/episodic facts) and procedural memory (task execution patterns). Different memory types require different storage and retrieval: semantic memory indexed by topic, episodic by user/context, procedural by task type and outcome. Systems that collapse these categories lose the causal structure needed for skill transfer, treating feedback logs as documents rather than executable experience.

### Mechanism 3: Retrieval Augmentation Outperforms Specialized Memory When Data Is Heterogeneous
Naive RAG baselines match or beat specialized memory systems because term/vector retrieval robustly handles mixed document types without requiring explicit memory-type modeling. RAG treats all history uniformly, builds index over corpus + feedback logs, retrieves top-k by similarity, and conditions generation on retrieved context. This bypasses the need to classify memory types or design custom consolidation logic.

## Foundational Learning

- **Declarative vs. Procedural Memory**
  - Why needed here: MemoryBench's core contribution is separating factual knowledge (declarative) from experience-based skill (procedural). Without this distinction, you can't reason about why current memory systems fail.
  - Quick check question: "If a system stores user feedback as documents and retrieves them like Wikipedia articles, what type of memory is it mistakenly treating procedural feedback as?"

- **Explicit vs. Implicit Feedback Signals**
  - Why needed here: The benchmark simulates verbose feedback (natural language critique), action feedback (like/dislike clicks), and implicit signals (copy actions, session termination). Understanding signal strength and noise is critical for designing reward models.
  - Quick check question: "Why might a 'copy' action be noisier than a 'like' for inferring user satisfaction? What does the paper's probabilistic model (P(C|S) = 4×P(L|S)) assume about copy behavior?"

- **Off-Policy vs. On-Policy Learning**
  - Why needed here: MemoryBench tests both settings—off-policy uses pre-collected feedback logs; on-policy collects feedback interactively. This distinction matters for evaluating whether a system can adapt vs. merely recall.
  - Quick check question: "In on-policy experiments, why might performance fluctuate more in Academic/Legal domains compared to Open domain? (Hint: domain knowledge requirements for feedback interpretation)"

## Architecture Onboarding

- **Component map:** Data preparation → format 11 datasets into unified (q, c, v) schema → User Simulator generates verbose feedback + SatisfactionScorer assigns 1-10 score → probabilistic action model maps to like/dislike/copy → Memory ingestion → feed feedback logs into target memory system (A-Mem, Mem0, MemoryOS, or RAG) → Test evaluation → query memory system on held-out test queries → compute normalized scores → Efficiency analysis → measure memory time + predict time per case.

- **Critical path:** 1) Data preparation: format 11 datasets into unified (q, c, v) schema 2) Feedback simulation: run LLM-as-user on training queries to generate feedback logs S 3) Memory ingestion: feed S into target memory system 4) Test evaluation: query memory system on held-out test queries; compute normalized scores 5) Efficiency analysis: measure memory time + predict time per case (flag if >10s/case).

- **Design tradeoffs:**
  - Session vs. message-level indexing: RAG-BM25-S (session) outperforms RAG-BM25-M (message) on some tasks because session preserves context, but message-level better for fine-grained retrieval in long dialogs.
  - Sigmoid vs. deterministic action mapping: DialSim uses binary correctness→action; Locomo uses F1→satisfaction→action; other datasets use LLM-assigned scores. Choose based on whether ground truth is objective or subjective.
  - Memory system complexity vs. latency: MemoryOS has highest memory latency (18+ seconds/case); Mem0 shows pathological slowdown on large corpora; A-Mem is fastest but less effective. RAG offers consistent ~1-3s latency.

- **Failure signatures:**
  - Mem0 timeout: On DialSim-theoffice corpus (2347 sessions), memorization time grows super-linearly; after 6 hours, time-per-entry exceeds 20 minutes. Root cause: memory consolidation logic does not scale to large document sets.
  - MemoryOS context mismatch: Treats procedural feedback as declarative → retrieves irrelevant historical sessions → generation quality drops below Vanilla baseline on LiLo tasks.
  - Noisy feedback degradation: When Gaussian noise σ > 0.8 is added to satisfaction scores, Embed-S and A-Mem performance drops significantly; BM25-S more robust but still declines.

- **First 3 experiments:**
  1. Baseline sanity check: Run Vanilla (no memory) vs. RAG-Embed-S on LiSo partition of MemoryBench. Verify RAG shows improvement; if not, check context length truncation or retrieval quality.
  2. Memory system latency profiling: Measure memory time for Mem0 on a 500-dialog subset of Legal domain. If average time > 5s/dialog, flag for optimization before full evaluation.
  3. Procedural memory utilization test: Compare performance with vs. without feedback logs for A-Mem on Academic domain. Compute delta; if negative, inspect retrieved memory entries for relevance to current task type.

## Open Questions the Paper Calls Out

### Open Question 1
How can LLM systems be architected to effectively distinguish and process procedural memory (experience/feedback) separately from declarative memory (facts)? The authors state that existing memory-based LLM systems simply treat all inputs as declarative memory, limiting their ability to understand and utilize procedural memory. Current architectures fail to extract actionable "how-to" knowledge from verbose user feedback.

### Open Question 2
How can the computational efficiency of memory construction and retrieval be optimized to support real-time continual learning? The paper reports that effectiveness and efficiency of state-of-the-art baselines are far from satisfying, noting that MemoryOS requires over 17 seconds per case for memory construction. Sophisticated memory management mechanisms currently incur prohibitive latency costs.

### Open Question 3
Can a unified memory architecture generalize effectively across heterogeneous task formats (LiSo, SiLo, LiLo, SiSo) without task-specific engineering? The authors find that current systems manually tailored their methods to handle such tasks, leading to limited generalizability where none of the advanced memory-based LLMsys can consistently outperform RAG baselines.

## Limitations
- Current memory systems fail to extract procedural knowledge from user feedback, treating experience as declarative facts
- Specialized memory architectures incur prohibitive latency costs (MemoryOS >17s/case, Mem0 shows super-linear scaling)
- Feedback signal quality and simulator calibration across diverse domains remain unproven

## Confidence
- **High**: MemoryBench provides a rigorous benchmark framework for evaluating memory and continual learning in LLMs. The declarative vs. procedural memory distinction is well-founded and the benchmark design is methodologically sound.
- **Medium**: The finding that RAG baselines often match or outperform specialized memory systems. While empirically supported, this could reflect benchmark-specific conditions rather than general architectural limitations.
- **Low**: The claim that current memory systems fail to extract "transferable procedural knowledge" from feedback. The evidence shows RAG performs comparably, but doesn't directly demonstrate what procedural knowledge is or isn't being extracted.

## Next Checks
1. **Procedural transfer validation**: Design an experiment where feedback from one task type is applied to a different but related task type (e.g., feedback from LiSo to LiLo tasks) to measure actual skill transfer rather than case-specific recall.
2. **Feedback quality calibration**: Systematically vary the user simulator's feedback verbosity and satisfaction score accuracy to measure how feedback signal quality affects memory system performance across the learning curve.
3. **Memory system scaling limits**: Characterize the exact scaling behavior of each memory system (A-Mem, Mem0, MemoryOS) by measuring time and memory usage as a function of stored sessions, identifying the point where each system becomes impractical.