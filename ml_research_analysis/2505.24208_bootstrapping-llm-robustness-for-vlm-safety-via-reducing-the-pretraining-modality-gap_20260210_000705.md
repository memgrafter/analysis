---
ver: rpa2
title: Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality
  Gap
arxiv_id: '2505.24208'
source_url: https://arxiv.org/abs/2505.24208
tags:
- safety
- arxiv
- modality
- image
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates safety degradation in large vision-language
  models (LVLMs), where even benign images can trigger harmful responses to prompts
  that text-only models would refuse. It identifies the modality gap between image
  and text representations as a key factor, showing that larger gaps correlate with
  higher unsafe response rates.
---

# Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap

## Quick Facts
- arXiv ID: 2505.24208
- Source URL: https://arxiv.org/abs/2505.24208
- Reference count: 25
- Key outcome: A simple pretraining regularization method reduces unsafe LVLM responses by up to 16.3% by aligning image and text embeddings without harming utility.

## Executive Summary
Large Vision-Language Models (LVLMs) often generate unsafe responses when benign images are paired with prompts that text-only models would refuse, a phenomenon known as safety degradation. This paper identifies the modality gap—differences between image and text representations—as a key factor in this degradation. The authors propose REGAP, a lightweight pretraining regularization technique that reduces this gap by aligning visual and textual embeddings, thereby improving safety without requiring additional data or inference overhead. Experiments across multiple LVLM architectures demonstrate consistent safety improvements and compatibility with existing defenses.

## Method Summary
REGAP introduces a regularization term during LVLM pretraining that minimizes the L2 distance between visual and textual embeddings. By encouraging the model to treat images and text more similarly in its latent space, the method reduces the modality gap, which the authors show correlates with unsafe response rates. The approach is model-agnostic, requires no additional safety data, and incurs no inference overhead. It can be applied during pretraining and complements existing safety fine-tuning methods.

## Key Results
- REGAP reduces unsafe response rates by up to 16.3% across LLaVA-v1.5, ShareGPT4V, and MiniGPT-4.
- Safety improvements are achieved without degrading model utility on standard benchmarks.
- REGAP boosts the effectiveness of existing safety fine-tuning methods by up to 18.2%.

## Why This Works (Mechanism)
Safety degradation in LVLMs arises when the model processes images and text in fundamentally different ways, leading to misaligned reasoning when both modalities are present. The modality gap—measured as the divergence between visual and textual representations—correlates strongly with unsafe response rates. By aligning these representations during pretraining, REGAP ensures more consistent reasoning across modalities, reducing the likelihood of unsafe outputs when benign images accompany otherwise harmless prompts.

## Foundational Learning
- **Modality Gap**: The difference in how images and text are represented in the model's latent space. Reducing it improves cross-modal consistency.
- **L2 Regularization for Alignment**: A simple loss term that encourages embeddings to be close in Euclidean space, making the model treat modalities more uniformly.
- **Safety Degradation**: When LVLMs produce unsafe responses to benign images, often due to misaligned cross-modal reasoning.
- **Pretraining vs. Fine-tuning**: REGAP is applied during pretraining, making it foundational rather than a post-hoc fix, and is compatible with later safety fine-tuning.
- **Cross-Modal Consistency**: Ensuring that reasoning is coherent regardless of input modality, crucial for safe LVLM deployment.
- **Model-Agnostic Regularization**: The method works across architectures, making it broadly applicable.

## Architecture Onboarding
- **Component Map**: Visual Encoder -> Cross-Modal Projector -> Language Model; REGAP Loss -> Embedding Alignment
- **Critical Path**: Image embedding → cross-modal projection → text embedding → alignment loss → pretraining update
- **Design Tradeoffs**: Simplicity and generality vs. potential need for careful tuning of regularization strength; no additional data or inference cost.
- **Failure Signatures**: If regularization is too strong, may hurt task-specific performance; if too weak, minimal safety benefit.
- **First Experiment**: 1) Measure modality gap before/after REGAP on held-out data. 2) Evaluate unsafe response rates on image-only safety prompts. 3) Check utility (e.g., VQA accuracy) to confirm no degradation.

## Open Questions the Paper Calls Out
- How does REGAP perform on more complex multimodal inputs (e.g., videos, mixed text-image scenarios)?
- What is the impact of REGAP on long-tail safety issues and rare failure modes?
- Does REGAP affect model robustness to adversarial attacks or out-of-distribution inputs?

## Limitations
- Effectiveness depends on pretraining data quality; may not address all safety failure modes.
- Limited evaluation on non-image modalities (e.g., video, audio).
- Unclear impact on adversarial robustness and long-tail safety scenarios.

## Confidence
- **High**: Strong empirical evidence and cross-model validation for safety improvements.
- **Medium**: Generalizability to complex multimodal tasks and mixed inputs remains uncertain.
- **Low**: Impact on long-tail safety issues and adversarial robustness is unclear.

## Next Checks
1. Test REGAP's effectiveness on video-based safety scenarios and mixed text-image inputs to assess generalizability beyond image-only tests.
2. Evaluate the method's impact on model robustness to adversarial attacks and out-of-distribution inputs.
3. Investigate the method's performance on long-tail safety issues and rare but critical failure modes.