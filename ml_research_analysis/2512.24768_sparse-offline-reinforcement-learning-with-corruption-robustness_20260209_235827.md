---
ver: rpa2
title: Sparse Offline Reinforcement Learning with Corruption Robustness
arxiv_id: '2512.24768'
source_url: https://arxiv.org/abs/2512.24768
tags:
- sparse
- offline
- learning
- coverage
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies corruption-robust offline reinforcement learning
  in high-dimensional sparse Markov decision processes (MDPs) where the feature dimension
  $d$ may exceed the number of samples $N$. The key challenge is that directly applying
  standard least-squares value iteration (LSVI) with sparsity constraints fails due
  to overly pessimistic pointwise bonuses that are incompatible with sparse support
  uncertainty.
---

# Sparse Offline Reinforcement Learning with Corruption Robustness

## Quick Facts
- arXiv ID: 2512.24768
- Source URL: https://arxiv.org/abs/2512.24768
- Authors: Nam Phuong Tran; Andi Nika; Goran Radanovic; Long Tran-Thanh; Debmalya Mandal
- Reference count: 40
- This paper proposes actor-critic methods with sparse robust regression oracles that achieve suboptimality gaps of $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4})$ with computationally expensive oracles and $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4} + H^2\sqrt{\kappa s}\epsilon^{1/4})$ with efficient oracles under single-policy concentrability coverage.

## Executive Summary
This paper addresses corruption-robust offline reinforcement learning in high-dimensional sparse Markov decision processes where feature dimension exceeds sample size. The authors identify that standard least-squares value iteration fails in sparse regimes due to overly pessimistic pointwise bonuses incompatible with sparse support uncertainty. They propose actor-critic methods with sparse robust regression oracles that avoid pointwise pessimism, establishing the first non-vacuous guarantees for sparse offline RL with corruption. The work demonstrates theoretical suboptimality gaps under single-policy concentrability coverage and highlights actor-critic superiority over LSVI in sparse settings.

## Method Summary
The authors propose two actor-critic algorithms with sparse robust regression oracles to handle corruption in offline RL. The key insight is that LSVI fails in sparse regimes due to pointwise pessimism, while actor-critic methods naturally accommodate sparsity without requiring such pessimism. The first algorithm uses computationally expensive sparsity-aware robust regression oracles, while the second employs efficient approximations. Both methods operate under single-policy concentrability coverage assumptions, which are weaker than uniform coverage. The algorithms leverage sparsity constraints to improve sample complexity and corruption robustness in high-dimensional feature spaces.

## Key Results
- Achieves suboptimality gaps of $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4})$ with computationally expensive oracles
- Achieves suboptimality gaps of $\tilde{O}(H^2\sqrt{\kappa s}N^{-1/4} + H^2\sqrt{\kappa s}\epsilon^{1/4})$ with efficient oracles
- Demonstrates actor-critic superiority over LSVI in sparse regimes by naturally accommodating sparsity structure

## Why This Works (Mechanism)
The mechanism works because actor-critic methods avoid pointwise pessimism that plagues LSVI in sparse settings. Standard LSVI adds bonuses at each state-action pair, but in sparse regimes with uncertain support, this leads to overly conservative estimates. Actor-critic methods instead optimize policies directly using sparse robust regression oracles, which can handle uncertainty in the support structure without the pessimistic penalties that LSVI requires. This allows for better sample complexity and corruption robustness in high-dimensional sparse MDPs.

## Foundational Learning

**Single-policy concentrability coverage**: A coverage assumption weaker than uniform coverage, requiring that the data distribution has sufficient overlap with the optimal policy distribution. Needed to establish theoretical guarantees under realistic data collection scenarios. Quick check: Verify that the dataset's state-action visitation frequencies have sufficient overlap with the optimal policy.

**Sparse robust regression oracles**: Regression methods that can handle both sparsity constraints and corrupted data simultaneously. Needed to estimate value functions accurately in high-dimensional spaces with potential outliers. Quick check: Ensure the oracle can identify sparse support while being robust to a fraction ε of corrupted samples.

**Pointwise pessimism**: The practice of adding optimistic bonuses to value estimates at each state-action pair to ensure exploration. Needed in standard LSVI but problematic in sparse regimes due to overly conservative estimates. Quick check: Identify where pointwise bonuses would be applied and verify they don't dominate the value estimates in sparse regions.

## Architecture Onboarding

**Component map**: Data buffer -> Sparse robust regression oracle -> Policy update -> Value function estimation -> Next policy iteration

**Critical path**: The sparse robust regression oracle is the critical component, as it must simultaneously handle sparsity constraints and corruption robustness while providing accurate estimates for policy updates.

**Design tradeoffs**: Computationally expensive oracles provide better theoretical guarantees but are impractical for large-scale problems, while efficient oracles sacrifice some performance for scalability. The choice between LSVI and actor-critic involves a fundamental tradeoff between pointwise pessimism benefits and sparsity accommodation.

**Failure signatures**: Poor performance indicates either violation of single-policy concentrability coverage, insufficient sparsity structure exploitation, or corruption levels exceeding oracle robustness guarantees. Runtime issues suggest the oracle computation is too expensive for the problem scale.

**First experiments**: 1) Verify oracle performance on synthetic sparse corrupted data, 2) Test policy improvement under varying corruption levels ε, 3) Compare actor-critic versus LSVI on high-dimensional sparse MDPs with known ground truth.

## Open Questions the Paper Calls Out

None

## Limitations

- Restriction to single-policy concentrability coverage, which while weaker than uniform coverage, still represents a significant assumption about data-generating processes
- Computational expense of sparsity-aware robust regression oracles remains a practical concern, particularly for the first algorithm
- Lack of empirical validation that real-world datasets satisfy the single-policy concentrability assumption

## Confidence

High confidence in theoretical framework and mathematical derivations
Medium confidence in practical applicability of single-policy concentrability assumption
Medium confidence in computational efficiency claims without empirical runtime comparisons

## Next Checks

1. Empirical validation on real-world datasets to verify single-policy concentrability coverage holds and measure actual suboptimality gaps versus theoretical bounds

2. Computational benchmarks comparing runtime and scalability of sparsity-aware robust regression oracles versus standard methods, particularly for the first algorithm

3. Ablation studies testing sensitivity of performance to corruption fraction ε and sparsity level s to understand practical robustness limits