---
ver: rpa2
title: Exploiting Information Redundancy in Attention Maps for Extreme Quantization
  of Vision Transformers
arxiv_id: '2508.16311'
source_url: https://arxiv.org/abs/2508.16311
tags:
- attention
- vision
- weights
- quantization
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the computational and memory inefficiencies\
  \ of Vision Transformers (ViTs) due to their Multi-Head Self-Attention (MHSA) mechanisms.\
  \ The authors propose Entropy Attention Maps (EAM), a method that exploits information\
  \ redundancy in attention maps by identifying low-entropy attention heads\u2014\
  those exhibiting stable and predictable patterns across inputs."
---

# Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers

## Quick Facts
- arXiv ID: 2508.16311
- Source URL: https://arxiv.org/abs/2508.16311
- Reference count: 40
- This paper proposes Entropy Attention Maps (EAM), a method that exploits information redundancy in attention maps by identifying and freezing low-entropy attention heads, achieving similar or higher accuracy at up to 20% sparsity in Vision Transformers.

## Executive Summary
This paper addresses the computational and memory inefficiencies of Vision Transformers (ViTs) due to their Multi-Head Self-Attention (MHSA) mechanisms. The authors propose Entropy Attention Maps (EAM), a method that exploits information redundancy in attention maps by identifying low-entropy attention heads—those exhibiting stable and predictable patterns across inputs. These heads are frozen and quantized to low precision (as low as 4 bits), reducing computational complexity without significantly impacting model performance. Experiments on ImageNet-1K across various ViT architectures (DeiT, Swin) show that EAM achieves similar or higher accuracy at up to 20% sparsity in attention maps and competitive performance beyond this level. At 10-20% sparsity, EAM even improves accuracy compared to the RepQ-ViT baseline in most models.

## Method Summary
The EAM method identifies low-entropy attention heads by computing Shannon entropy over attention weight distributions from a calibration dataset. These low-entropy weights are frozen to their mean values and quantized to 4 bits. The approach is applied on top of RepQ-ViT post-training quantization, with two variants: EAM_FP32 (frozen weights in FP32) and EAM_4bits (frozen weights quantized to 4-bit). The entropy computation is performed on the quantized model to capture quantization noise dynamics. The method reduces MHSA computational overhead by replacing redundant attention weight computations with memory lookups of fixed values.

## Key Results
- EAM achieves similar or higher accuracy compared to RepQ-ViT baseline at up to 20% sparsity in attention maps
- At 10-20% sparsity, EAM improves accuracy compared to RepQ-ViT in most models tested
- EAM outperforms random fixing of attention weights, validating the effectiveness of the entropy-based approach
- The method is particularly effective for DeiT models, with Swin models showing slightly different behavior due to their hierarchical structure

## Why This Works (Mechanism)

### Mechanism 1: Entropy as a Proxy for Redundancy
If an attention weight exhibits low Shannon entropy across a calibration dataset, its value is predictable and contributes less unique information to the final representation. The method computes entropy for each weight position by building a histogram of its values over 5% of ImageNet-1K. Weights with low entropy (approaching deterministic behavior) are flagged as redundant candidates for freezing.

### Mechanism 2: Selective Attention Fixing (Partial Freezing)
Replacing low-entropy attention weights with their dataset-calculated mean value avoids redundant quadratic re-computation while preserving the dynamic range of high-information heads. A binary mask is generated based on a sparsity threshold, and weights falling below the entropy threshold are replaced by their mean value, effectively bypassing the softmax calculation for those specific positions during inference.

### Mechanism 3: Quantization-Aware Entropy Calibration
Computing entropy maps on the already quantized model captures the noise dynamics of low-precision arithmetic better than computing on the FP32 model. By calculating entropy on the 4-bit quantized model, the selection mechanism inherently prioritizes weights that remain stable even under quantization noise, leading to better high-sparsity performance.

## Foundational Learning
- **Shannon Entropy**: Why needed here: This is the selection criterion for weight fixing. You must understand that low entropy implies high predictability (low information), which justifies freezing a weight. Quick check: Would a weight that flips between 0.1 and 0.9 have higher or lower entropy than one that stays constantly at 0.5?
- **MHSA Quadratic Complexity**: Why needed here: The paper targets the O(N²) complexity of the attention map. Freezing parts of this matrix reduces the dynamic computation load. Quick check: In the complexity formula Φ_MHSA = 4Nd_e² + 2N²d_e, which term does EAM primarily aim to reduce?
- **Post-Training Quantization (PTQ)**: Why needed here: EAM is applied on top of a 4-bit PTQ baseline. Understanding how PTQ calibration works explains why the authors use 5% of ImageNet to estimate distributions. Quick check: Why is it critical to evaluate the entropy on the quantized model rather than the full-precision model when targeting high sparsity?

## Architecture Onboarding
- **Component map**: Calibration Phase: Input Batch (5% Data) → Pass through Quantized Model → Collect Attention Histograms → Compute Entropy Maps (H). Mask Generation: Sort Entropy values → Select lowest τ% → Generate Binary Mask & Mean Value Buffer. Inference Engine: Standard ViT Forward Pass → EAM Layer: Apply Mask (Fix low-entropy weights) → Compute Attention Output.
- **Critical path**: The Entropy Calibration is the most sensitive step. If the calibration data is not representative, the mean values will be incorrect, and the mask will freeze "noisy" weights, degrading accuracy.
- **Design tradeoffs**: Sparsity (τ) vs. Accuracy: Increasing τ saves compute but risks dropping accuracy (especially >50% for DeiT). Calibration Cost vs. Quality: Using more than 5% data for histograms might improve mask robustness but increases setup time. FP32 vs 4-bit Entropy: Computing EAM on 4-bit models improves high-sparsity results but ties the mask to a specific quantization scheme.
- **Failure signatures**: Random Fixing Collapse: If selection is random (not entropy-based), accuracy collapses, proving the mechanism relies on specific stability properties. Architecture Sensitivity: Swin (local attention) tolerates fixing better than DeiT (global attention). Applying aggressive EAM to global-attention models without tuning τ will fail.
- **First 3 experiments**: 1. Sanity Check (Ablation): Implement "Random Fixing" vs "EAM" on DeiT-Tiny at 20% sparsity to verify that the entropy metric is the active ingredient. 2. Baseline Comparison: Run EAM (4-bit) on DeiT-Base at τ=10% and τ=20% to check for the reported accuracy improvement over RepQ-ViT baseline. 3. Entropy Visualization: Replicate Figure 2 to ensure the histogram quantization correctly distinguishes between heads like L2H2 (high entropy) and L2H3 (low entropy).

## Open Questions the Paper Calls Out
1. **Generalization to LLMs/VLMs**: Can the Entropy Attention Maps (EAM) method be effectively generalized to Large Language Models (LLMs) and Vision-Language Models (VLMs) with longer sequence lengths? The authors explicitly state this as future work, noting that LLMs utilize causal masking and significantly longer contexts where the quadratic complexity of MHSA is even more pronounced.
2. **Training-Phase Integration**: Does incorporating EAM freezing into the training phase (Quantization-Aware Training) allow the model to recover accuracy at high sparsity levels? The conclusion suggests extending the work by enabling the model to retrain with attention weights fixed with EAM, aiming to minimize the loss in accuracy.
3. **Hardware Speedup Validation**: Does the EAM strategy yield actual wall-clock speedups on resource-constrained edge hardware? The paper claims to accelerate model inference but evaluates success solely via Top-1 accuracy and theoretical sparsity ratios, omitting latency or energy metrics.

## Limitations
- The exact thresholding method for selecting τ% lowest-entropy weights is not fully specified, creating ambiguity in reproduction.
- The mechanism for replacing frozen weights with their mean while maintaining softmax normalization is not fully detailed.
- The claim that entropy computed on 4-bit models improves high-sparsity performance is not validated across different quantization schemes.

## Confidence
- **High**: EAM improves accuracy at 10-20% sparsity for DeiT models; entropy-based selection outperforms random fixing.
- **Medium**: EAM performance on Swin models; the benefit of computing entropy on quantized vs. FP32 models.
- **Low**: The exact entropy computation pipeline and its generalization across different ViT architectures.

## Next Checks
1. Replicate the ablation study comparing EAM vs. random fixing on DeiT-Tiny at 20% sparsity to confirm entropy selection is critical.
2. Test EAM (4-bit) on DeiT-Base at τ=10% and τ=20% to verify accuracy improvement over RepQ-ViT baseline.
3. Visualize entropy maps to ensure low-entropy heads (e.g., L2H3) are correctly identified and frozen, matching Figure 2 patterns.