---
ver: rpa2
title: Large Models in Dialogue for Active Perception and Anomaly Detection
arxiv_id: '2501.16300'
source_url: https://arxiv.org/abs/2501.16300
tags:
- scene
- perception
- drone
- proposed
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel framework that employs Large Language
  Models (LLMs) to actively collect information and detect anomalies, even in unprecedented
  situations. The core idea is a model dialogue approach where two deep learning models
  (an LLM and a Visual Question Answering model) interact to control a drone and improve
  perception accuracy.
---

# Large Models in Dialogue for Active Perception and Anomaly Detection

## Quick Facts
- arXiv ID: 2501.16300
- Source URL: https://arxiv.org/abs/2501.16300
- Reference count: 36
- Primary result: Novel framework using LLM-VQA dialogue to actively control a drone for improved anomaly detection and scene description

## Executive Summary
This paper introduces a novel framework that leverages a dialogue between a Large Language Model (LLM) and a Visual Question Answering (VQA) model to enable active perception and zero-shot anomaly detection for autonomous aerial monitoring. The system controls a drone, exploring scenes and detecting potential hazards through natural language interaction. Experimental results in Airsim simulation demonstrate improved caption-image matching scores and anomaly detection accuracy compared to baseline approaches, with the method successfully providing detailed scene descriptions, safety measures, and interpretable attention maps.

## Method Summary
The framework employs a three-phase pipeline where an LLM (GPT-3.5) and a VQA model (BLIP within PnP-VQA) interact to control a drone in Airsim simulation. In the active perception phase, the LLM issues movement commands and exploratory questions based on VQA-generated captions. The validation phase revisits saved positions with added noise to verify details, while the explanation phase generates final descriptions with GradCAM attention maps. The system operates in a zero-shot manner without model training, using prompt engineering to guide the LLM's behavior and natural language commands for drone navigation.

## Key Results
- Improved caption-image matching scores compared to baseline (spawn point) descriptions
- Successful identification of anomalies (fires, crashes) in unprecedented situations through active exploration
- Generation of interpretable GradCAM attention maps providing visual explanations for detected hazards

## Why This Works (Mechanism)
The framework works by creating a closed-loop dialogue system where the LLM provides high-level navigational control through natural language commands, while the VQA model answers specific questions about the current visual scene. This interaction enables active perception - the drone doesn't just passively observe but actively explores the environment based on the LLM's strategic decisions. The validation phase ensures reliability by cross-checking observations from multiple viewpoints, while the explanation phase adds interpretability through attention visualization.

## Foundational Learning
- **Active Perception**: Why needed - enables targeted exploration rather than passive observation; Quick check - measure improvement in detection accuracy with vs without active movement
- **Zero-shot Anomaly Detection**: Why needed - allows detection of novel hazards without prior training; Quick check - test on scenes with unseen object types
- **VQA-LLM Dialogue**: Why needed - combines visual understanding with strategic decision-making; Quick check - verify LLM outputs valid commands and questions consistently
- **GradCAM Attention Maps**: Why needed - provides visual explanation for model decisions; Quick check - confirm attention maps highlight relevant regions for detected anomalies
- **Prompt Engineering for LLM Control**: Why needed - guides LLM behavior for specific task completion; Quick check - test different prompt variations for output consistency
- **Simulation-based Validation**: Why needed - allows controlled testing with injected hazards; Quick check - verify hazard injection works consistently across scenes

## Architecture Onboarding

**Component Map:** Airsim Environment -> Image Capture -> BLIP VQA/Captioning -> LLM Controller -> Command Execution -> Image Capture (loop)

**Critical Path:** Image Capture → BLIP Captioning → LLM Decision → Command Execution → Image Capture

**Design Tradeoffs:**
- Zero-shot inference vs. trained anomaly detection models (simpler deployment but potentially lower accuracy)
- Natural language commands vs. direct coordinate control (more interpretable but slower)
- Active exploration vs. static observation (better detection but higher computational cost)

**Failure Signatures:**
- LLM gets stuck in infinite loops asking irrelevant questions
- VQA fails to detect obvious anomalies from distance
- Drone collides with obstacles during validation phase
- Command execution fails due to API miscommunication

**First Experiments:**
1. Implement basic dialogue loop with fixed prompts and verify LLM outputs valid commands
2. Test BLIP captioning and question-answering on sample images from Airsim
3. Verify Airsim API integration by executing simple movement commands

## Open Questions the Paper Calls Out
None

## Limitations
- Strong dependency on LLM prompt engineering without exact template specification
- Simulation fidelity gap with no real-world validation
- No quantitative comparison to traditional computer vision baselines

## Confidence

**High Confidence:** Core concept of LLM-VQA dialogue for active perception is technically feasible and well-articulated

**Medium Confidence:** Reported improvements in simulation are believable but dependent on unpublished prompt details

**Low Confidence:** Generalizability to real-world complex environments and robustness to adversarial scenarios not demonstrated

## Next Checks

1. Implement dialogue loop with multiple prompt variations to determine optimal template for consistent GPT-3.5 output format

2. Test inference pipeline on real-world aerial images or video frames to assess performance degradation compared to simulation results

3. Implement baseline object detection model (e.g., YOLO) to compare detection accuracy and description quality against LLM-VQA dialogue approach on same Airsim scenes