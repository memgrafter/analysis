---
ver: rpa2
title: 'midr: Learning from Black-Box Models by Maximum Interpretation Decomposition'
arxiv_id: '2506.08338'
source_url: https://arxiv.org/abs/2506.08338
tags:
- feature
- function
- effect
- effects
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The midr package implements Maximum Interpretation Decomposition
  (MID), a model-agnostic functional decomposition method for interpreting black-box
  predictive models. MID creates an additive approximation of a prediction function
  by minimizing squared error while enforcing strict centering constraints on effects.
---

# midr: Learning from Black-Box Models by Maximum Interpretation Decomposition

## Quick Facts
- arXiv ID: 2506.08338
- Source URL: https://arxiv.org/abs/2506.08338
- Reference count: 0
- The midr package implements Maximum Interpretation Decomposition (MID), a model-agnostic functional decomposition method for interpreting black-box predictive models.

## Executive Summary
The midr package provides a comprehensive framework for interpreting black-box predictive models through Maximum Interpretation Decomposition (MID). MID constructs an additive approximation of a prediction function by minimizing squared error while enforcing strict centering constraints on effects. This approach creates a global surrogate model that enables comprehensive analysis of feature effects and interactions through main effects, interaction plots, Ceteris Paribus curves, feature importance, and Shapley values. The package demonstrates pragmatic stability compared to Partial Dependence Plots when features are correlated and offers computational efficiency through optimized least squares algorithms.

## Method Summary
MID is a model-agnostic functional decomposition method that creates an additive approximation of black-box model predictions by solving a constrained least squares problem. Given predictions from any black-box model, MID finds effect functions for each feature subset (main effects, pairwise interactions, etc.) by minimizing the squared error between the model's prediction function and this additive representation. The key innovation is the "strict centering constraint" which ensures that interaction effects are uncontaminated by main effects and vice versa. This is enforced empirically via linear constraints on the parameters, requiring each effect to have zero conditional expectation given any subset of its features. The method supports first- or second-order decompositions and includes visualization tools for comprehensive analysis of feature effects and interactions.

## Key Results
- MID demonstrates pragmatic stability compared to Partial Dependence Plots when features are correlated, avoiding spurious effects through strict centering constraints
- The package achieves computational efficiency through optimized least squares algorithms, with runtime growing quadratically with the number of features
- MID-derived Shapley values can be computed in closed form without sampling, providing exact feature attributions consistent with the global decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MID constructs an optimal low-order additive approximation of a black-box model by minimizing prediction error subject to orthogonality constraints.
- Mechanism: Given predictions ŷ from any black-box model, MID solves a constrained least squares problem to find effect functions fJ(xJ) for each feature subset J (main effects, pairwise interactions, etc.). The minimization targets E[(f(X) - Fm(X))²] while enforcing that each effect has zero conditional expectation given any subset of its features. This yields a decomposition where effects do not "leak" into each other.
- Core assumption: The black-box model's behavior can be adequately captured by low-order interactions (typically m=1 or m=2); higher-order effects are negligible or can be treated as residual noise.
- Evidence anchors:
  - [abstract] "MID is a functional decomposition approach that derives a low-order additive representation of a black-box model by minimizing the squared error between the model's prediction function and this additive representation."
  - [Section 3, Eq. 30] "F mid(m) = arg min_F U(Fm)" where U is the uninterpreted variation ratio
  - [corpus] Related work on additive decomposition (neuralGAM, TreeHFD) assumes similar low-order structure for interpretability, but corpus does not provide independent validation of MID's optimality claims.
- Break condition: If the uninterpreted variation ratio (Eq. 36) remains high after including second-order interactions, the additive assumption is violated. Check this metric on held-out data, not just training data.

### Mechanism 2
- Claim: The "strict centering constraint" ensures that interaction effects are uncontaminated by main effects and vice versa, enabling clean attribution.
- Mechanism: For each effect fJ, the constraint E[fJ(XJ) | XJ'] = 0 for all proper subsets J' ⊂ J is enforced empirically via linear constraints on the parameters (Eqs. 34, 55, 63). This means the main effect of feature j must average to zero conditional on any value of j itself, and interaction effects must average to zero conditional on either feature alone.
- Core assumption: The empirical distribution of features adequately represents the population distribution for computing conditional expectations.
- Evidence anchors:
  - [Section 3, Definition] "every effect of order m or lower is strictly centered. We call the third condition the strict centering constraint"
  - [Section 5, "Pragmatic stability"] Paper claims this constraint prevents extrapolation artifacts that plague PDP when features are correlated
  - [corpus] No corpus papers directly validate this constraint's benefits over alternatives; this is an internal theoretical claim.
- Break condition: When features have extreme correlations, the constraint matrices may become ill-conditioned. Monitor rank deficiency warnings from the least squares solver.

### Mechanism 3
- Claim: MID-derived Shapley values can be computed in closed form without sampling, providing exact feature attributions consistent with the global decomposition.
- Mechanism: Because the MID model is already an additive decomposition, defining the characteristic function as vi(S) = Σ_{J⊆S} fJ(xiJ) yields Shapley values ϕms_ij = Σ_{J:j∈∈J} (1/|J|) fJ(xiJ) (Eq. 44). This avoids the exponential 2^d complexity of standard Shapley estimation.
- Core assumption: The MID model faithfully represents the black-box model; Shapley values computed on the surrogate transfer to the original.
- Evidence anchors:
  - [Section 3, Eq. 43-44] Closed-form derivation of MID-derived Shapley values
  - [Section 4, Figure 8] Demonstrates SHAP dependence plots computed via shapviz integration
  - [corpus] TreeHFD paper similarly uses functional decomposition for efficient Shapley-like attribution, suggesting convergent validation of this approach.
- Break condition: If the uninterpreted variation ratio is non-trivial (e.g., >0.1), the MID-Shapley values may diverge from what direct (but expensive) Shapley computation on the black-box would produce.

## Foundational Learning

- Concept: **Functional/ANOVA Decomposition**
  - Why needed here: MID is fundamentally a variance decomposition approach. Understanding why f(x) = f₀ + Σfⱼ(xⱼ) + Σfⱼₖ(xⱼ,xₖ) + ... is a valid representation requires comfort with hierarchical model structure.
  - Quick check question: Can you explain why an interaction term fⱼₖ should be zero if the effect of xⱼ doesn't depend on the value of xₖ?

- Concept: **Constrained Least Squares (Lagrange Multipliers / Null-space Methods)**
  - Why needed here: The strict centering constraints are implemented as linear equality constraints Mβ = 0 on the parameter vector. The appendix solves this via null-space projection.
  - Quick check question: Given a design matrix X and constraint Mβ = 0, how would you transform this into an unconstrained problem?

- Concept: **Shapley Values and Additive Feature Attribution**
  - Why needed here: MID positions itself as both a global decomposition method and a local attribution method. Understanding the connection requires knowing why Shapley values satisfy local accuracy and consistency.
  - Quick check question: Why does the Shapley value for feature j involve averaging over all possible coalitions S not containing j?

## Architecture Onboarding

- Component map:
  - `interpret(formula, data, model)` → main entry; constructs design matrices, solves constrained least squares, returns "mid" object
  - `mid` object → stores estimated effect functions, intercept, uninterpreted variation ratio
  - `plot(mid)` / `ggmid(mid)` → visualization layer; main effect plots, interaction heatmaps
  - `mid.importance(mid)` → effect importance via E[|fJ(XJ)|]
  - `mid.conditional(mid)` → ICE/c-ICE curves from MID model
  - `mid.breakdown(mid, data[i,])` → waterfall/bar decomposition for single prediction
  - `shapviz(mid)` → bridges to MID-derived Shapley values for compatibility with shapviz ecosystem

- Critical path:
  1. Fit any black-box model (the paper uses nnet, xgboost, etc.)
  2. Call `interpret(y ~ .^2, data = train, model = model)` to get second-order MID
  3. Inspect `mid` object's uninterpreted variation ratio (should be low, e.g., <0.05)
  4. Validate on test data: compute predictions from MID model and compare to black-box predictions
  5. Use `ggmid()` for main/interaction plots; `mid.importance()` for ranking; `mid.breakdown()` for local explanations

- Design tradeoffs:
  - Order selection: `.^1` (main effects only) vs `.^2` (include interactions). Higher order captures more variance but increases parameters quadratically and reduces interpretability.
  - Encoding method: `k` argument controls number of basis functions per feature. Higher k = more flexible effects but risk of overfitting. Default k = c(25, 5) for main/interaction effects.
  - Least squares solver: `method` argument. Eigendecomposition (method=5) handles rank deficiency and gives minimum-norm solution; Cholesky (method=2) is faster but requires full rank.

- Failure signatures:
  - High uninterpreted variation ratio on training data → black-box has complex higher-order interactions; consider increasing order or accepting incomplete explanation
  - Large gap between training and test U-ratio → overfitting to training predictions; reduce k
  - Warnings about rank deficiency → features may be linearly dependent or k too large relative to n; use method=5 or reduce k
  - Effect plots showing wild extrapolation → constrained region has no data; use `type = "data"` to plot only observed points

- First 3 experiments:
  1. **Sanity check on known function**: Generate data from y = 10sin(πx₁x₂) + 20(x₃-0.5)² + 10x₄ + 5x₅ + ε (the Friedman1 benchmark in Section 4). Fit a neural network, then MID. Verify that the x₁:x₂ interaction plot shows the correct sin pattern and that uninformative features x₆-x₁₀ have near-zero importance.
  2. **Correlated feature stress test**: Create X₁, X₂ with correlation >0.9. Fit a model where true effect depends only on X₁. Compare PDP vs MID main effect plots for X₂—PDP should show spurious effects (as in Section 5/Figure 10), MID should suppress them due to strict centering.
  3. **Scalability benchmark**: With n=10,000 and d=16 features (full pairwise interactions), time `interpret()` with different `method` arguments. Expect LLT Cholesky (method=2) to be ~2.5x faster than pivoted QR (method=0), per Table 1, but verify on your hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can overfitting of MID surrogate models to training data predictions be systematically detected and mitigated when the interpretative model may not generalize to new observations?
- Basis in paper: [explicit] The authors explicitly state: "a low uninterpreted variation ratio on the training dataset alone does not guarantee that the target model and the interpretative MID model will produce similarly close predictions for new observations. In other words, the interpretative MID model might be overfitted to the target model's predictions on the training dataset."
- Why unresolved: The paper only demonstrates a simple verification by computing the uninterpreted variation ratio on a test dataset as an example, without providing systematic methods for detecting overfitting, regularization techniques, or theoretical bounds on generalization.
- What evidence would resolve it: A systematic study comparing training vs. test uninterpreted variation ratios across multiple datasets and model types; development of regularization methods for MID; theoretical analysis of generalization bounds.

### Open Question 2
- Question: What is the optimal strategy for selecting the number of encoding functions (k) for main effects and interactions, and how sensitive are MID estimates to this hyperparameter choice?
- Basis in paper: [inferred] The paper describes using k(1) and k(2) parameters (e.g., k(1)=25, k(2)=5 in simulations) but provides no guidance on principled selection. The total number of parameters grows as m = k(1)d + k(2)²d(d-1)/2, making this choice critical for model complexity.
- Why unresolved: No sensitivity analysis, cross-validation procedure, or theoretical guidance is offered for choosing these values across different feature types (continuous vs. categorical) or data sizes.
- What evidence would resolve it: Simulation studies varying k across different data characteristics; comparison of cross-validation-based selection vs. fixed choices; theoretical analysis of bias-variance tradeoff with respect to k.

### Open Question 3
- Question: How can MID be extended to efficiently handle high-dimensional datasets (d >> 16) where the quadratic growth in interaction parameters becomes prohibitive?
- Basis in paper: [inferred] Table 1 shows computational results only up to d=16 features with m=3,400 parameters. The paper notes that m "grows quadratically with d," but does not address scaling to common real-world scenarios with hundreds of features.
- Why unresolved: No approximation methods, sparse interaction selection, or dimensionality reduction techniques are discussed to handle high-dimensional feature spaces.
- What evidence would resolve it: Development of sparse MID variants that select relevant interactions; computational benchmarks on datasets with d=50, 100, or 500 features; comparison of approximation quality vs. computational cost tradeoffs.

### Open Question 4
- Question: Under what conditions does the empirical performance of MID degrade relative to ALE, particularly for prediction functions with complex higher-order interactions or discontinuities?
- Basis in paper: [explicit] The simulation shows MID and ALE produce similar results in one scenario, but the authors state MID "closely aligns with ALE" without establishing boundaries where they diverge or where MID might underperform.
- Why unresolved: Only one correlation scenario is tested; no comparison on functions with discontinuities, sharp transitions, or genuine higher-order (≥3) interactions where different decomposition methods may yield substantially different interpretations.
- What evidence would resolve it: Comprehensive benchmarking across diverse synthetic functions with known ground truth; analysis of functions with third-order interactions, discontinuities, or extreme non-linearities; formal comparison of approximation error bounds between MID and ALE.

## Limitations
- MID assumes low-order interactions can adequately capture black-box model behavior, which may not hold for highly complex models
- The strict centering constraints may become numerically unstable with highly correlated features or insufficient sample sizes
- Closed-form Shapley values derived from MID are only valid to the extent that the MID approximation faithfully represents the original model

## Confidence
- **High Confidence**: The constrained least squares implementation and computational efficiency claims (Table 1) are well-supported by the code and benchmark results
- **Medium Confidence**: The pragmatic stability advantages over PDP in correlated feature scenarios are demonstrated but rely on a single synthetic example (Friedman1 with correlated features)
- **Low Confidence**: The theoretical claims about optimality of the decomposition (minimum uninterpreted variation) are asserted but not independently validated against alternative decomposition methods

## Next Checks
1. **Out-of-Distribution Stability Test**: Apply MID to models trained on datasets with known domain shifts (e.g., train on one geographic region, test on another). Compare MID's extrapolation behavior to PDP and other decomposition methods.
2. **Higher-Order Interaction Stress Test**: Use synthetic data with known third-order interactions (e.g., y = sin(x₁x₂x₃)). Measure how quickly the uninterpreted variation ratio increases when restricting to second-order MID, and test whether increasing encoding resolution (k) helps.
3. **Runtime Scalability Validation**: Benchmark the runtime and memory usage of `interpret()` with d=20 features (full pairwise interactions) on datasets ranging from n=1,000 to n=100,000. Verify that the reported O(nk²d²) scaling holds in practice and identify the practical limits of feature dimensionality.