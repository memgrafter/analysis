---
ver: rpa2
title: 'NOSA: Native and Offloadable Sparse Attention'
arxiv_id: '2510.13602'
source_url: https://arxiv.org/abs/2510.13602
tags:
- nosa
- attention
- sparse
- cache
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NOSA (Native and Offloadable Sparse Attention),
  a trainable sparse attention mechanism designed for efficient key-value (KV) cache
  offloading in large language models (LLMs). Traditional training-free offloading
  methods degrade long-generation quality due to mismatched sparse patterns between
  training and inference.
---

# NOSA: Native and Offloadable Sparse Attention

## Quick Facts
- arXiv ID: 2510.13602
- Source URL: https://arxiv.org/abs/2510.13602
- Reference count: 33
- Primary result: Achieves up to 5.04x decoding throughput improvement over FullAttn through trainable sparse attention with KV cache offloading

## Executive Summary
NOSA introduces a trainable sparse attention mechanism specifically designed for efficient key-value cache offloading in large language models. Traditional training-free offloading methods suffer from quality degradation during long-generation tasks due to mismatched sparse patterns between training and inference. While trainable sparse attention can address this mismatch, existing approaches are incompatible with efficient offloading due to unconstrained KV access. NOSA solves this by explicitly constraining the volume of CPU-to-GPU KV transfers through a hybrid query-aware and query-agnostic selection strategy, enforcing locality constraints during training to improve selection locality and reduce communication costs while maintaining task performance.

## Method Summary
NOSA combines trainable sparse attention with explicit KV transfer volume constraints to enable efficient CPU offloading. The method uses a hybrid selection strategy that balances query-aware and query-agnostic patterns, enforcing locality constraints during training to ensure sparse patterns match inference requirements. This approach addresses the fundamental mismatch between training-free offloading methods and the sparse patterns learned during model training. The authors also develop NOSI, a specialized inference system optimized for NOSA's architecture, which further unlocks efficiency gains through careful system-level optimizations tailored to the constrained KV transfer patterns.

## Key Results
- Achieves up to 5.04x decoding throughput improvement over FullAttn on 1B, 3B, and 8B models
- Demonstrates 1.92x improvement over InfLLMv2 and 1.83x over ShadowKV in throughput
- Maintains competitive performance on short-context tasks while significantly outperforming training-free baselines on reasoning tasks

## Why This Works (Mechanism)
NOSA works by explicitly constraining KV transfer volumes during both training and inference, ensuring that the sparse attention patterns learned during training directly match the patterns used during CPU offloading. The hybrid selection strategy combines query-aware attention (which adapts to specific token contexts) with query-agnostic patterns (which provide consistent coverage across different queries), creating a balanced approach that maintains expressiveness while enabling efficient offloading. The locality constraint enforced during training ensures that the model learns to prioritize nearby tokens for caching, which naturally aligns with the communication cost structure of CPU-to-GPU transfers.

## Foundational Learning

**Sparse Attention Patterns**: Why needed - Traditional full attention scales quadratically with sequence length, making it computationally prohibitive for long sequences. Quick check - Verify that the sparse mask reduces the number of attention computations by a factor of at least 10x compared to full attention.

**KV Cache Offloading**: Why needed - GPU memory limitations prevent storing KV caches for very long sequences on-device. Quick check - Confirm that the CPU memory usage remains within practical limits (e.g., <16GB for typical workloads).

**Locality Constraints**: Why needed - Ensures that the model learns to prioritize nearby tokens for caching, which aligns with the communication cost structure of CPU-to-GPU transfers. Quick check - Measure the average distance between attended tokens and their query positions to verify locality.

## Architecture Onboarding

**Component Map**: Query Encoder -> Sparse Attention Mask Generator -> KV Cache Manager -> CPU Offloader -> GPU Decoder

**Critical Path**: Input tokens flow through the query encoder to generate attention scores, which are masked by the sparse attention generator. The KV cache manager determines which keys and values to keep on GPU versus offload to CPU, with the CPU offloader handling the data transfer and the GPU decoder performing the actual attention computation on the retained subset.

**Design Tradeoffs**: The hybrid selection strategy trades some modeling flexibility for predictable communication patterns, while the locality constraint sacrifices potential long-range dependency capture for improved offloading efficiency. The specialized NOSI inference system adds implementation complexity but delivers significant throughput gains.

**Failure Signatures**: Performance degradation when sparse patterns become too restrictive, communication bottlenecks if locality constraints are too loose, and quality drops when the hybrid strategy poorly balances query-aware and query-agnostic components.

**First Experiments**: 1) Benchmark throughput against FullAttn, InfLLMv2, and ShadowKV on standard long-context tasks. 2) Evaluate task performance degradation on reasoning benchmarks compared to training-free offloading methods. 3) Profile CPU-GPU communication overhead to verify the claimed efficiency gains.

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations

- Specialized optimization for CPU offloading may not generalize to multi-GPU or memory-bound architectures
- Locality constraint mechanism could limit the model's ability to capture certain long-range dependencies
- Performance claims based on specific hardware configurations may vary across different inference environments

## Confidence

**High Confidence**: The fundamental architectural contribution of integrating trainable sparse attention with explicit KV transfer volume constraints is well-supported by experimental results across 1B, 3B, and 8B models.

**Medium Confidence**: The comparison with ShadowKV and InfLLMv2, while showing substantial gains, was conducted on a limited set of benchmarks that may not capture all workload characteristics.

**Medium Confidence**: The assertion that NOSA maintains competitive performance on short-context tasks while significantly