---
ver: rpa2
title: Do LLMs exhibit the same commonsense capabilities across languages?
arxiv_id: '2509.06401'
source_url: https://arxiv.org/abs/2509.06401
tags:
- languages
- commonsense
- sentence
- evaluation
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates whether large language models (LLMs) exhibit
  the same commonsense generation capabilities across languages. The authors introduce
  MULTICOM, a novel benchmark extending the COCOTEROS dataset to four languages: English,
  Spanish, Dutch, and Valencian.'
---

# Do LLMs exhibit the same commonsense capabilities across languages?

## Quick Facts
- arXiv ID: 2509.06401
- Source URL: https://arxiv.org/abs/2509.06401
- Authors: Ivan MartÃ­nez-Murillo; Elena Lloret; Paloma Moreda; Albert Gatt
- Reference count: 22
- Primary result: LLMs show superior commonsense generation in English compared to Spanish, Dutch, and Valencian across multiple evaluation methods

## Executive Summary
This paper evaluates whether large language models exhibit consistent commonsense generation capabilities across different languages. The authors introduce MULTICOM, a benchmark extending the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. Through systematic evaluation of five LLM families across multiple sizes using automatic metrics, LLM-as-judge evaluations, and human annotations, the study reveals a consistent pattern: English consistently outperforms other languages in commonsense generation tasks. The research demonstrates that under-represented languages benefit more from contextual support, suggesting language-specific performance disparities in LLMs.

## Method Summary
The study introduces MULTICOM, a multilingual benchmark for evaluating commonsense generation across four languages. Researchers created triplets of words from the English COCOTEROS dataset and translated them into Spanish, Dutch, and Valencian. The task requires generating commonsensical sentences from these triplets, both with and without supporting context. Five LLM families (LLaMA, Qwen, Gemma, EuroLLM, Salamandra) were evaluated across multiple sizes using automatic metrics (ROUGE-1, ROUGE-2, BERTScore), LLM-as-judge evaluations, and human annotations. The benchmark design aimed to test whether LLMs exhibit consistent commonsense reasoning capabilities across languages.

## Key Results
- English consistently outperforms Spanish, Dutch, and Valencian in commonsense generation across all evaluated LLM families
- Under-represented languages (Spanish, Dutch, Valencian) show greater benefit from contextual support compared to English
- Automatic metrics (ROUGE, BERTScore) and LLM-as-judge evaluations align with human judgments, confirming English superiority
- Performance gaps persist across multiple model sizes and families, suggesting systematic language-specific disparities

## Why This Works (Mechanism)
The study demonstrates that LLMs exhibit language-specific performance variations in commonsense generation tasks. The mechanism appears to stem from training data distribution biases, where English-language content dominates pretraining corpora, leading to better generalization in English. The observed benefit of contextual support for under-represented languages suggests that LLMs can leverage additional information to compensate for knowledge gaps, but the fundamental disparity remains. The alignment between automatic metrics, LLM-as-judge, and human evaluations validates that these performance differences reflect genuine commonsense reasoning capabilities rather than evaluation artifacts.

## Foundational Learning
- Cross-linguistic commonsense reasoning: Understanding how LLMs generate commonsense knowledge across different languages requires knowledge of both language-specific patterns and universal reasoning principles. Quick check: Compare triplet-based sentence generation quality across languages.
- Multilingual evaluation metrics: Different languages have varying syntactic and semantic structures that affect automatic evaluation metrics. Quick check: Test ROUGE and BERTScore consistency across language pairs.
- Context utilization in LLMs: How additional contextual information influences commonsense generation varies by language representation in training data. Quick check: Measure performance improvement with context addition across languages.

## Architecture Onboarding
Component map: Triplet generation -> Context addition -> LLM generation -> Automatic evaluation -> LLM-as-judge -> Human annotation
Critical path: Triplet creation and translation -> Context generation -> Model inference -> Multi-metric evaluation
Design tradeoffs: Translation-based triplet creation may introduce semantic drift; automatic metrics may not capture cross-linguistic nuances; human evaluation sample size limits comprehensive analysis
Failure signatures: Performance degradation in non-English languages; inconsistent context benefit across languages; automatic metric misalignment with human judgments
3 first experiments:
1. Compare triplet generation quality when created independently in each language versus translation from English
2. Test context quantity thresholds needed to eliminate language performance gaps
3. Evaluate model families with varying levels of multilingual pretraining data

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on translated triplets from English, potentially introducing semantic drift across languages
- Automatic metrics may not fully capture commonsense generation quality across languages with different syntactic structures
- Human evaluation sample size (100 sentences per language) may not represent full complexity of cross-linguistic reasoning
- Study focuses on only four languages, limiting generalizability to other language families or low-resource languages

## Confidence
- High Confidence: English consistently outperforms other languages across multiple evaluation methods and LLM families
- Medium Confidence: Under-represented languages benefit more from contextual support
- Medium Confidence: LLMs generate more commonsensical sentences in English than other languages

## Next Checks
1. Replicate using language-independent triplet generation rather than translation from English to test whether English advantage persists with truly parallel cross-linguistic data
2. Conduct systematic error analysis comparing specific types of commonsense failures across languages to identify whether patterns differ beyond overall performance gaps
3. Test additional languages from different families (e.g., Asian, African, Slavic) to determine if English advantage generalizes beyond Romance/Germanic language pairs and identify potential language-specific factors affecting performance