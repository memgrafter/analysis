---
ver: rpa2
title: Evaluating Robustness of Large Language Models Against Multilingual Typographical
  Errors
arxiv_id: '2510.09536'
source_url: https://arxiv.org/abs/2510.09536
tags:
- robustness
- typo
- language
- typos
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTYPO, a multilingual typo generation algorithm
  that simulates human-like typographical errors based on language-specific keyboard
  layouts and typing behavior. The authors evaluate 18 open-source large language
  models across three model families (Gemma, Qwen, OLMo) on five downstream tasks
  using zero- and few-shot prompting.
---

# Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors

## Quick Facts
- arXiv ID: 2510.09536
- Source URL: https://arxiv.org/abs/2510.09536
- Reference count: 40
- Primary result: Realistic, keyboard-aware typo simulation (MULTYPO) reveals significant performance degradation in LLMs across multilingual tasks, with reasoning-heavy generative tasks most vulnerable and instruction tuning potentially increasing brittleness to noise.

## Executive Summary
This paper introduces MULTYPO, a multilingual typo generation algorithm that simulates human-like typographical errors based on language-specific keyboard layouts and typing behavior. The authors evaluate 18 open-source large language models across three model families (Gemma, Qwen, OLMo) on five downstream tasks using zero- and few-shot prompting. They find that typos consistently degrade model performance, particularly in generative tasks and reasoning tasks, while natural language inference shows greater robustness. Instruction tuning improves clean-input performance but may increase vulnerability to noise. Larger models generally perform better but still suffer from input perturbations. Language-dependent robustness is observed: high-resource languages and translation from English show better robustness than low-resource languages and translation into English.

## Method Summary
The authors develop MULTYPO, a keyboard-layout-aware typo generation algorithm that simulates realistic human-like errors based on language-specific keyboard layouts and typing behavior. The method applies four typo operations (replacement, insertion, deletion, transposition) constrained to physically adjacent keys, with sampling probabilities proportional to √|w| for word selection and position-dependent weights favoring middle/end positions. The evaluation framework tests 18 LLMs (Gemma, Qwen, OLMo families, base and instruct versions) on five downstream tasks (XNLI, Belebele, MMMLU, MGSM, FLORES200) across 12 languages with typo rates at 0%, 10%, 40%, and 70%. Human evaluation validates the naturalness of MULTYPO-generated typos compared to naive baselines.

## Key Results
- Typos consistently degrade model performance, particularly in generative tasks and reasoning tasks, while natural language inference is comparatively more robust
- Instruction tuning improves clean-input performance but may increase brittleness under noise, with instruction-tuned models showing equal or worse absolute degradation under 10-40% noise
- Larger models generally perform better but still suffer from input perturbations, with absolute degradation remaining substantial even for larger models
- Language-dependent robustness is observed: high-resource languages and translation from English show better robustness than low-resource languages and translation into English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyboard-layout-aware typo generation produces more realistic perturbations than naive random edits.
- Mechanism: MULTYPO constrains typo operations to physically adjacent keys on language-specific keyboard layouts and applies length-aware word sampling and position-aware character sampling, mimicking actual human typing behavior.
- Core assumption: Human typing errors follow predictable patterns based on keyboard geometry and the 10-finger typing convention.
- Evidence anchors:
  - [abstract]: "simulates realistic human-like errors based on language-specific keyboard layouts and typing behavior"
  - [section 3.2]: Human evaluation shows MULTYPO judged significantly more natural than naive baseline in 6 of 7 languages (p < 0.05)
  - [corpus]: Related work on typo neurons (arXiv:2502.19669) suggests LLMs internally process typos using local and global context—consistent with exposure to realistic typos in pretraining
- Break condition: Languages with non-standard input methods (e.g., Chinese via Pinyin) or touchscreen keyboards may violate the physical keyboard assumption.

### Mechanism 2
- Claim: Typo-induced performance degradation is task-dependent, with reasoning-heavy generative tasks more vulnerable than classification tasks.
- Mechanism: Character-level corruption disrupts tokenization and semantic parsing; multi-step reasoning amplifies this disruption as errors propagate through chained inferences, whereas classification tasks rely on coarse semantic similarity that tolerates surface noise.
- Core assumption: Typo robustness correlates with the degree of sequential reasoning required.
- Evidence anchors:
  - [abstract]: "typos consistently degrade performance, particularly in generative tasks and those requiring reasoning – while the natural language inference task is comparatively more robust"
  - [section 5.1]: MGSM accuracy drops from ~40 (clean) to ~27 (70% noise); XNLI shows <10-point drop even at highest noise
  - [corpus]: Weak direct corpus evidence on task-specific degradation mechanisms; adjacent work (arXiv:2506.03627) confirms prompting sensitivity to input perturbations but doesn't explain task variance
- Break condition: Tasks with robust numerical or symbolic reasoning modules may show different vulnerability patterns.

### Mechanism 3
- Claim: Instruction tuning improves clean-input performance but increases brittleness under noise.
- Mechanism: Instruction tuning sharpens task-specific attention patterns for clean prompts, potentially overfitting to canonical input distributions and reducing capacity to handle out-of-distribution surface variations.
- Core assumption: Instruction tuning optimizes for expected input patterns at the cost of robustness to perturbations.
- Evidence anchors:
  - [abstract]: "Instruction tuning improves clean-input performance but may increase brittleness under noise"
  - [section 5.3]: Instruction-tuned models show equal or worse absolute degradation under 10-40% noise compared to base models despite higher clean performance
  - [corpus]: No corpus papers directly address instruction tuning and noise robustness tradeoffs
- Break condition: Noise-aware instruction tuning or data augmentation during alignment may mitigate this effect.

## Foundational Learning

- Concept: Keyboard-layout-aware perturbation generation
  - Why needed here: Understanding how MULTYPO differs from naive random edits is essential for interpreting robustness results and extending the algorithm to new languages.
  - Quick check question: Given a QWERTY keyboard, what typo operations are valid for the character 'g'?

- Concept: Length-aware and position-aware sampling
  - Why needed here: Typos don't distribute uniformly—longer words and mid-word positions attract more errors.
  - Quick check question: Why does the algorithm assign probability proportional to √|w| rather than |w| for word sampling?

- Concept: Relative vs. absolute performance degradation
  - Why needed here: Larger models may show smaller relative drops but still suffer substantial absolute degradation; this distinction affects deployment decisions.
  - Quick check question: If a small model drops 10% relatively (from 30 to 27) and a large model drops 4% relatively (from 60 to 57.6), which is more robust in practice?

## Architecture Onboarding

- Component map: MULTYPO generator: word sampler → position sampler → operation selector (replace/insert/delete/transpose with keyboard constraints)
- Critical path: Typo rate τ → word sampling (√|w| weights) → position sampling (position-dependent weights) → operation application with keyboard constraints → corrupted input → model evaluation
- Design tradeoffs:
  - Ignoring numerical expressions preserves benchmark validity but reduces realism
  - Physical keyboard assumption excludes logographic scripts (Chinese) and touchscreen behaviors
  - Arabic layout modeling showed weaker human evaluation results, suggesting language-specific tuning may be needed
- Failure signatures:
  - Translation into English degrades more than translation from English under typos (Figure 7)
  - Low-resource, non-Latin-script languages (Arabic, Hindi, Bengali) show larger drops (Table 4)
  - Few-shot examples don't improve robustness, only clean performance (Figure 6)
- First 3 experiments:
  1. Reproduce the human evaluation on one language: generate 15 MULTYPO and 15 naive corrupted sentences, collect naturalness judgments, verify statistical significance.
  2. Evaluate a single model (e.g., Qwen3-4B-Instruct) on MGSM across all 4 typo rates (0%, 10%, 40%, 70%) in English to confirm the reasoning vulnerability pattern.
  3. Compare MULTYPO vs. random baseline perturbations on one task (Belebele or MGSM) to quantify the robustness gap between realistic and unrealistic typos.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can typo simulation algorithms be effectively adapted for logographic or syllabic writing systems (e.g., Chinese) that utilize intermediate phonetic input methods?
- Basis in paper: [explicit] The Limitations section states that the current algorithm does not support these systems and suggests future work must model errors in intermediate stages (e.g., Pinyin mistyping or candidate misselection).
- Why unresolved: The current MULTYPO algorithm relies on direct character-to-key mappings, which fundamentally differs from the two-step Input Method Editor (IME) process required for logographic languages.
- What evidence would resolve it: A modified version of the framework that simulates IME-based errors (e.g., homophone substitutions) applied to benchmarks like Chinese GSM8K.

### Open Question 2
- Question: How does input modality (touchscreen vs. physical keyboard) influence the robustness of LLMs given the differences in error distributions and auto-correct interference?
- Basis in paper: [explicit] The Limitations section notes the study focuses exclusively on physical keyboards (e.g., QWERTY) and ignores touchscreen modalities, which have distinct typing behaviors and error profiles.
- Why unresolved: Touchscreens introduce unique noise factors (e.g., swipe errors, false positives from auto-correct) that are not captured by layout-based key-adjacency models used in this study.
- What evidence would resolve it: A comparative evaluation using a touchscreen-specific error generation model applied to the same model families and tasks.

### Open Question 3
- Question: Can noise-aware training strategies mitigate the specific brittleness to typos observed in instruction-tuned models?
- Basis in paper: [inferred] The Conclusion calls for "noise-aware training" after Results (§5.3) demonstrate that while instruction tuning improves clean performance, it often increases vulnerability to typographical noise.
- Why unresolved: It is unclear if the brittleness is an unavoidable side effect of the instruction tuning distribution or if it can be corrected without sacrificing the gains in clean-input performance.
- What evidence would resolve it: An experiment comparing standard instruction-tuned models against models instruction-tuned on MULTYPO-augmented data to observe if the robustness gap closes.

## Limitations

- The algorithm relies on physical keyboard layouts and does not support logographic or syllabic writing systems that use intermediate phonetic input methods
- The study focuses exclusively on physical keyboards and ignores touchscreen keyboard behaviors, which have distinct typing behaviors and error profiles
- Arabic keyboard layout modeling showed weaker human evaluation results compared to other languages, suggesting language-specific tuning may be needed

## Confidence

- **High confidence**: Keyboard-layout-aware typo generation produces more realistic perturbations than naive random edits (supported by human evaluation, p < 0.05 for 6/7 languages)
- **Medium confidence**: Task-dependent vulnerability patterns (reasoning tasks more vulnerable than classification) - while the observed patterns are consistent, the underlying mechanism linking typo propagation to sequential reasoning needs further validation
- **Medium confidence**: Instruction tuning increases brittleness under noise - observed empirically but lacks supporting corpus evidence or theoretical explanation
- **Low confidence**: Language-dependent robustness patterns - the correlation between resource availability and robustness is observed, but causation and underlying mechanisms remain unclear

## Next Checks

1. Extend MULTYPO evaluation to touchscreen and logographic input methods: Test whether the keyboard-layout assumption holds for Chinese via Pinyin input and touchscreen keyboards, or whether alternative perturbation models are needed for these modalities.

2. Validate the instruction tuning brittleness hypothesis with noise-aware fine-tuning: Train a subset of models with typo-augmented instruction data and compare their robustness against the instruction-tuned models reported in the paper to determine if the brittleness is inherent or mitigable.

3. Test the reasoning task vulnerability mechanism: Design an ablation study where typos are injected at different positions in multi-step reasoning chains to quantify how error propagation amplifies degradation, distinguishing between surface-level corruption and semantic disruption.