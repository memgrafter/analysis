---
ver: rpa2
title: 'Spurious Rewards: Rethinking Training Signals in RLVR'
arxiv_id: '2506.10947'
source_url: https://arxiv.org/abs/2506.10947
tags:
- training
- qwen2
- code
- reasoning
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Spurious rewards\u2014such as random or incorrect labels\u2014\
  can improve mathematical reasoning in Qwen2.5-Math models by up to 24.1% on MATH-500,\
  \ nearly matching ground-truth reward gains. This effect is unique to Qwen models;\
  \ Llama and OLMo models show minimal or negative gains with spurious rewards."
---

# Spurious Rewards: Rethinking Training Signals in RLVR

## Quick Facts
- **arXiv ID**: 2506.10947
- **Source URL**: https://arxiv.org/abs/2506.10947
- **Reference count**: 40
- **Primary result**: Spurious rewards improve Qwen2.5-Math performance by up to 24.1% on MATH-500

## Executive Summary
This paper investigates how spurious rewards—random or incorrect training signals—affect mathematical reasoning performance in reinforcement learning from verifiable rewards (RLVR). Surprisingly, models like Qwen2.5-Math show substantial gains (up to 24.1% on MATH-500) from spurious rewards, nearly matching improvements from ground-truth rewards. This effect is unique to Qwen models; Llama and OLMo models show minimal or negative gains. The authors attribute the improvement to RLVR surfacing pre-existing code reasoning patterns that strongly correlate with correctness, rather than teaching new reasoning skills.

## Method Summary
The study evaluates multiple LLMs (Qwen2.5-Math, Llama, and OLMo) under RLVR with different reward types: ground-truth (correct/incorrect), random, and other spurious signals. Models are fine-tuned on mathematical reasoning tasks, and performance is measured on held-out MATH-500 problems. The authors analyze reasoning strategies (e.g., code generation) and their correlation with correctness, finding that Qwen models' pre-existing code reasoning patterns are key to spurious reward gains. Other models either do not generate code or produce incorrect code, limiting their benefit from spurious rewards.

## Key Results
- Spurious rewards improve Qwen2.5-Math performance by up to 24.1% on MATH-500
- Effect is unique to Qwen models; Llama and OLMo show minimal or negative gains
- Qwen2.5-Math models employ code reasoning (65% initially, >90% after RLVR), strongly correlating with correctness (60.9% accuracy with code vs. 35.0% without)
- Eliciting code reasoning via prompts or RLVR further boosts Qwen performance

## Why This Works (Mechanism)
The authors propose that RLVR with spurious rewards improves performance by surfacing pre-existing reasoning patterns rather than teaching new skills. In Qwen models, code reasoning is already present but underutilized; spurious rewards encourage its deployment. This mechanism is unique to Qwen due to architectural or pretraining differences that enable effective code reasoning. Other models either lack this capability or fail to leverage it under spurious rewards.

## Foundational Learning
- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Needed to understand how models learn from reward signals; quick check: verify RLVR training setup and reward definitions
- **Mathematical Reasoning in LLMs**: Needed to contextualize task difficulty and evaluation; quick check: review MATH-500 dataset and problem types
- **Code Reasoning in LLMs**: Needed to explain why code generation correlates with correctness; quick check: analyze code reasoning examples and accuracy rates
- **Pretraining Priors**: Needed to explain model-specific differences; quick check: compare Qwen, Llama, and OLMo pretraining approaches

## Architecture Onboarding
**Component Map**: RLVR pipeline (model -> reward signal -> training updates) -> performance on MATH-500
**Critical Path**: Reward signal → model adaptation → reasoning strategy deployment → correctness
**Design Tradeoffs**: Spurious rewards trade signal quality for exploration/regularization; ground-truth rewards prioritize accuracy but may limit discovery of useful strategies
**Failure Signatures**: Negative gains in Llama/OLMo indicate model-specific prerequisites for spurious reward benefits
**First Experiments**:
1. Test whether spurious rewards improve other reasoning tasks (e.g., logical inference)
2. Compare effects of different spurious reward types (random vs. partial correctness)
3. Analyze pretraining differences between Qwen and other models to identify enabling factors

## Open Questions the Paper Calls Out
The authors highlight major uncertainties about the precise mechanisms through which spurious rewards improve reasoning. While they attribute gains to surfacing pre-existing patterns, alternative explanations (e.g., regularization effects) are not ruled out. The causal relationship between spurious rewards and increased code generation remains unproven. Additionally, the model-specific nature of the effect is documented but not fully explained, and the study's focus on mathematical reasoning leaves open questions about generalizability.

## Limitations
- The study focuses exclusively on mathematical reasoning, limiting generalizability
- The causal link between spurious rewards and code reasoning is correlational, not proven
- Model-specific differences are observed but not deeply analyzed
- Ground-truth rewards are assumed optimal without rigorous testing

## Confidence
- Spurious rewards improve Qwen2.5-Math performance: High confidence
- Effect is unique to Qwen models: High confidence
- RLVR surfaces pre-existing reasoning patterns: Medium confidence
- Code reasoning correlates with correctness: High confidence
- Spurious rewards induce code reasoning: Medium confidence

## Next Checks
1. Ablation studies testing different types of spurious rewards (e.g., random rewards vs. rewards based on partial correctness) to isolate which reward properties drive improvements
2. Architectural analysis comparing Qwen models to Llama/OLMo models to identify specific pretraining or architectural features that enable spurious reward benefits
3. Transfer experiments evaluating whether spurious reward gains extend to non-mathematical reasoning tasks and different reward signal structures