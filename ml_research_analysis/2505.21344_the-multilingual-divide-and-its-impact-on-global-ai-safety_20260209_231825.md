---
ver: rpa2
title: The Multilingual Divide and Its Impact on Global AI Safety
arxiv_id: '2505.21344'
source_url: https://arxiv.org/abs/2505.21344
tags:
- language
- languages
- multilingual
- safety
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the significant \"language gap\" in AI, where\
  \ most large language models are optimized for a few high-resource languages, leaving\
  \ many global languages underserved and creating disparities in AI safety performance.\
  \ The authors analyze why this gap exists\u2014due to data scarcity, resource inequities,\
  \ and limited multilingual research\u2014and how it exacerbates safety risks for\
  \ non-English speakers."
---

# The Multilingual Divide and Its Impact on Global AI Safety

## Quick Facts
- arXiv ID: 2505.21344
- Source URL: https://arxiv.org/abs/2505.21344
- Reference count: 40
- Most LLMs are optimized for high-resource languages, leaving many global languages underserved with degraded safety performance.

## Executive Summary
This paper addresses the critical "language gap" in AI safety, where most large language models are optimized for a few high-resource languages, leaving many global languages underserved and creating disparities in AI safety performance. The authors analyze why this gap exists—due to data scarcity, resource inequities, and limited multilingual research—and how it exacerbates safety risks for non-English speakers. Through the Cohere Labs Aya initiative, they demonstrate that combining human-curated, synthetic, and translated data can improve multilingual coverage. They emphasize building comprehensive, culturally-nuanced evaluation sets alongside models and highlight technical innovations like multilingual preference training and model merging to enhance performance and safety. The study recommends supporting multilingual dataset creation, transparency from model providers, and inclusive research to bridge the language gap and ensure equitable AI safety globally.

## Method Summary
The authors address the multilingual divide by developing the Aya initiative, which combines human-curated, synthetic, and translated data to train multilingual models across 114 languages. They employ multilingual instruction tuning, preference training (DPO/RLHF variants), and model merging techniques. Safety context distillation is used to reduce harmful generations by teaching models refusal behaviors from teacher demonstrations. The approach emphasizes building comprehensive evaluation sets (Global-MMLU for parallel evaluation, INCLUDE for language-specific cultural context, and Aya Red-teaming dataset for safety) before training models. The methodology prioritizes both capability and safety across diverse languages while addressing technical challenges like the curse of multilinguality and token cost inflation for non-Latin scripts.

## Key Results
- Safety context distillation reduced harmful generations from adversarial prompts by 78–89% as judged by human experts.
- Combining human-curated, synthetic, and translated data increases multilingual coverage more effectively than relying on human annotations alone.
- Model merging can combine safety-specialized and general-purpose models without the performance trade-offs seen in earlier safety training approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining human-curated, synthetic, and translated data may increase multilingual coverage more effectively than relying on human annotations alone.
- Mechanism: Data volume from synthetic and translated sources compensates for scarcity of human-curated data in low-resource languages, while weighted combination preserves quality signals.
- Core assumption: Translation artifacts ("translationese") are acceptable trade-offs when the alternative is insufficient data volume for training.
- Evidence anchors:
  - [section 5.2]: "We have found it is better to increase coverage of data by including both human, synthetic and translated data rather than solely prioritizing human annotations."
  - [abstract]: References Aya initiative demonstrating this approach.
  - [corpus]: Related work on multilingual alignment (MPO, arXiv:2505.16869) supports multilingual preference optimization but does not directly validate this data-mixing claim.
- Break condition: If translation systematically removes harmful intent from safety prompts (per Agrawal et al., 2024, cited in section 5.3), safety-specific evaluations may require human post-editing regardless of volume benefits.

### Mechanism 2
- Claim: Safety context distillation can reduce harmful generations across languages by teaching models refusal behaviors from teacher demonstrations.
- Mechanism: A teacher model demonstrates safe refusals for harmful prompts; the student model learns contextual patterns for when refusals are appropriate, creating cross-lingual transfer of safety guardrails.
- Core assumption: Refusal patterns learned in one language or context generalize to others without requiring language-specific fine-tuning at scale.
- Evidence anchors:
  - [section 5.5]: "We found this step reduced harmful generations from adversarial prompts by 78–89% as judged by human experts."
  - [figure 6]: Visual comparison shows reduction in harmful generations across languages after safety context distillation.
  - [corpus]: "The Hidden Space of Safety" (arXiv:2504.02708) examines preference-tuned LLMs in multilingual contexts, noting monolingual bias in current alignment methods—suggesting this mechanism may have language-specific limitations.
- Break condition: Effectiveness likely degrades for languages poorly represented in the teacher model's training distribution.

### Mechanism 3
- Claim: Model merging can combine safety-specialized and general-purpose models without the performance trade-offs seen in earlier safety training approaches.
- Mechanism: Parameter-space merging of models fine-tuned for different objectives (safety vs. capability) preserves strengths from both, avoiding the historical assumption that safety improvements incur capability costs.
- Core assumption: Safety and capability objectives are not inherently zero-sum when combined through merging rather than sequential fine-tuning.
- Evidence anchors:
  - [section 5.5]: "Merging can help build stronger and safer multilingual systems, offering clear advantages for handling complex tasks in diverse languages."
  - [section 5.5]: "This is an important achievement compared to earlier works, where the assumption was that safety improvements would always incur a cost."
  - [corpus]: MPO (arXiv:2505.16869) addresses multilingual safety alignment via reward gap optimization—complementary but not directly validating merging claims.
- Break condition: Merge conflicts may emerge when safety norms differ across cultural contexts embedded in different training corpora.

## Foundational Learning

- Concept: Resource disparity cascade
  - Why needed here: Understanding why the language gap is self-reinforcing—synthetic data, LLM-as-judge evaluation, and compute access all favor high-resource languages.
  - Quick check question: Can you explain why synthetic data generation creates a feedback loop that widens the language gap?

- Concept: Language-parallel vs. language-specific evaluation
  - Why needed here: The paper distinguishes translated benchmarks (controlled comparison) from locally-grounded evaluations (cultural validity)—each has distinct limitations.
  - Quick check question: What information is lost when evaluating a model solely through translated versions of an English benchmark?

- Concept: Curse of multilinguality
  - Why needed here: Referenced in section 5.1 as a core challenge—adding languages can degrade per-language performance due to capacity constraints.
  - Quick check question: Why might adding support for more languages reduce performance on already-supported languages?

## Architecture Onboarding

- Component map:
  - Data layer: Human-curated annotations (65 languages, 200K+ samples), synthetic instructions (template-generated), translated datasets (machine translation + human post-edit for critical sets)
  - Training layer: Multilingual base model → instruction tuning → preference training (DPO/RLHF variants) → model merging
  - Evaluation layer: Language-parallel (Global-MMLU), language-specific (INCLUDE), safety red-teaming (Aya Red-teaming dataset with global/local harm categorization)
  - Deployment layer: Efficient model variants (8B parameters), accessible interfaces (WhatsApp integration noted for low-connectivity regions)

- Critical path:
  1. Identify target languages and assess data availability (HuggingFace, Wikipedia presence)
  2. Build or source evaluation sets before training (both parallel and localized)
  3. Combine data sources with weighted mixing based on quality-volume trade-offs
  4. Apply safety context distillation with language-aware harmful prompt coverage
  5. Merge safety-tuned and capability-tuned model variants
  6. Evaluate on open-ended generative tasks, not just discriminative benchmarks

- Design tradeoffs:
  - Translation speed vs. cultural fidelity: Automatic translation enables scale but loses nuance; human post-edit adds cost but preserves intent (especially critical for safety prompts)
  - Coverage vs. depth: Supporting 101 languages (Aya 101) vs. deeper optimization for 23 languages (Aya Expanse)
  - Open-ended vs. discriminative evaluation: Generative tasks better reflect real use but are harder to automate; discriminative benchmarks are scalable but misaligned with deployment contexts

- Failure signatures:
  - Performance cliff: Sharp capability/safety degradation when prompting in languages outside the optimization set
  - Translationese artifacts: Safety prompts lose harmful intent through translation, creating false negatives in evaluation
  - Western-centric bias: Models perform well on translated benchmarks but fail on region-specific knowledge (84.9% of MMLU geography questions focus on North America/Europe)
  - Token cost inflation: Non-Latin scripts require 2-10x more tokens for equivalent content, creating economic barriers

- First 3 experiments:
  1. Baseline gap measurement: Evaluate an existing model (e.g., GPT-4 or Llama) on both language-parallel (Global-MMLU) and language-specific (INCLUDE) benchmarks across 5+ languages to quantify performance cliffs and cultural bias.
  2. Data composition ablation: Train small multilingual models (1B-3B parameters) with varying ratios of human/synthetic/translated data on a subset of languages; measure both capability (MMLU-style) and safety (red-team) metrics.
  3. Safety transfer test: Apply safety context distillation using English-only harmful prompts, then evaluate refusal rates on translated harmful prompts in 3-5 low-resource languages to assess cross-lingual generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the increasing reliance on synthetic data for model training permanently widen the performance gap between high-resource and low-resource languages?
- Basis in paper: [explicit] The paper states that synthetic data favors high-resource languages, creating a "vicious cycle" that risks deepening the existing divide (Section 3).
- Why unresolved: Synthetic data generation is cost-effective but currently biased toward dominant languages; a scalable alternative for generating high-quality, low-resource data at volume does not yet exist.
- What evidence would resolve it: Longitudinal studies tracking performance disparities between high- and low-resource models as the ratio of synthetic data in pre-training increases.

### Open Question 2
- Question: How can safety evaluations effectively distinguish and measure "local" cultural harms versus "global" harms without relying on translation?
- Basis in paper: [explicit] Section 5.3 notes that translating evaluations can strip prompts of harmful intent or fail to capture regional nuances, necessitating the capture of local context.
- Why unresolved: Constructing culturally-specific safety benchmarks is resource-intensive and requires deep local expertise that is currently scarce for many low-resource languages.
- What evidence would resolve it: The creation of a benchmark suite where human annotators validate that model failures correlate specifically with "local" harm categories rather than translation artifacts.

### Open Question 3
- Question: Can toxicity mitigation strategies be designed to adapt continuously to evolving language use and slang without requiring expensive, compute-heavy retraining?
- Basis in paper: [explicit] Section 5.6 highlights that keeping toxicity guards up-to-date is "onerous" because existing methods often require "drastic modifications" or are "computationally intensive."
- Why unresolved: Language evolves rapidly (e.g., new slang, coded hate speech), and current static models struggle to adapt to semantic drift without full retraining runs.
- What evidence would resolve it: A model architecture or technique (such as retrieval-based adaptation) that maintains high refusal rates on emerging toxicity in low-resource languages without parameter updates.

## Limitations
- Effectiveness of synthetic and translated data in low-resource languages is asserted but not fully validated across all target languages.
- Safety context distillation relies on assumptions about cross-lingual transfer that may not hold for languages with vastly different cultural contexts.
- Model merging claims to avoid capability-safety trade-offs, but evaluation focuses on aggregate metrics rather than per-language performance analysis.
- Does not address economic barriers created by token inflation for non-Latin scripts, which could limit real-world accessibility.

## Confidence
- **High confidence**: The identification of the language gap problem and its mechanisms (resource disparity cascade, translation artifacts affecting safety evaluations, cultural bias in benchmarks).
- **Medium confidence**: The effectiveness of combining human-curated, synthetic, and translated data for multilingual coverage.
- **Medium confidence**: Safety context distillation as an approach to cross-lingual safety alignment.
- **Low confidence**: The claim that model merging can completely avoid capability-safety trade-offs in multilingual settings.

## Next Checks
1. **Per-language performance validation**: Replicate the safety context distillation experiment but evaluate the resulting model on 5-10 low-resource languages with both capability (MMLU-style) and safety (red-teaming) metrics. This will reveal whether the 78-89% safety improvement holds uniformly or if there are significant performance cliffs for certain language families.

2. **Translation artifact detection**: Create a controlled test set of safety prompts that are deliberately translated back and forth between English and target languages. Measure the semantic drift and evaluate whether harmful intent is preserved. This will validate the paper's concern about translation artifacts in safety evaluations and determine if human post-editing is necessary for safety-critical applications.

3. **Cultural context bias analysis**: Compare model performance on Global-MMLU (translated) versus INCLUDE (culturally-grounded) benchmarks for the same languages. Calculate the performance delta to quantify how much "safety" measured through translated benchmarks may be misleading. This will validate the paper's argument that parallel benchmarks may not capture real-world safety needs in diverse cultural contexts.