---
ver: rpa2
title: Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical
  NER
arxiv_id: '2510.22285'
source_url: https://arxiv.org/abs/2510.22285
tags:
- i-adr
- clinical
- text
- bert
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares BERT-style models and large language models
  (LLMs) for clinical named entity recognition on the CADEC corpus. Three BERT models
  (BERT Base, BioClinicalBERT, RoBERTa-large) are evaluated alongside GPT-4o using
  in-context learning with simple vs.
---

# Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER

## Quick Facts
- **arXiv ID**: 2510.22285
- **Source URL**: https://arxiv.org/abs/2510.22285
- **Authors**: Andrei Baroian
- **Reference count**: 35
- **Primary result**: Supervised fine-tuning achieves highest F1 score of ~87.1% for clinical NER

## Executive Summary
This paper evaluates different approaches for clinical named entity recognition (NER) using both traditional BERT-style models and large language models (LLMs). The study compares three BERT variants (BERT Base, BioClinicalBERT, RoBERTa-large) with GPT-4o using in-context learning and supervised fine-tuning on the CADEC corpus. Results show that domain-specific pretraining offers minimal benefits over standard BERT Base, while simple in-context prompts outperform complex ones. Supervised fine-tuning achieves the highest overall performance but at higher cost, making it most suitable for high-stakes clinical applications.

## Method Summary
The study evaluates three approaches: (1) traditional BERT-style models including BERT Base, BioClinicalBERT, and RoBERTa-large; (2) GPT-4o using in-context learning with simple and complex prompts; and (3) GPT-4o with supervised fine-tuning. Models are tested on the CADEC corpus for clinical NER tasks, with in-context learning using zero-shot and few-shot approaches, while supervised fine-tuning involves training on labeled data. Performance is measured using F1 scores across different entity types and task complexities.

## Key Results
- RoBERTa-large and BioClinicalBERT show minimal improvement over BERT Base, indicating limited benefit from domain-specific pretraining
- Simple in-context prompts outperform complex prompts in LLM approaches
- Supervised fine-tuning achieves highest F1 score of approximately 87.1% but at higher cost
- LLMs perform better on simplified binary classification tasks compared to complex NER

## Why This Works (Mechanism)
The paper does not provide specific mechanism analysis for why certain approaches work better than others. The observed performance differences appear to stem from architectural differences between traditional transformer models and LLMs, as well as the varying effectiveness of prompt engineering strategies.

## Foundational Learning

**Clinical NER Task Understanding**: Clinical NER involves identifying medical entities in text, such as adverse drug reactions, symptoms, and diseases. This is critical for extracting structured information from unstructured clinical notes.

*Why needed*: Provides context for the evaluation and explains the practical importance of the task being studied.

*Quick check*: Verify understanding by identifying entity types in sample clinical text from the CADEC corpus.

**BERT Architecture Fundamentals**: BERT uses bidirectional transformer encoders trained on masked language modeling objectives, providing strong contextual embeddings for downstream tasks.

*Why needed*: Essential for understanding the baseline models being compared and their limitations.

*Quick check*: Confirm that BERT Base uses 12 transformer layers with 768 hidden dimensions.

**In-Context Learning Principles**: LLMs can perform tasks through prompt engineering without parameter updates, using either zero-shot (no examples) or few-shot (with examples) approaches.

*Why needed*: Critical for understanding how LLMs are being evaluated without fine-tuning.

*Quick check*: Verify that simple prompts use minimal examples while complex prompts include more detailed instructions.

## Architecture Onboarding

**Component Map**: Data preprocessing -> Model selection (BERT variants / GPT-4o) -> Training approach selection (zero-shot / few-shot / fine-tuning) -> Evaluation on CADEC corpus

**Critical Path**: Data preparation -> Model training/fine-tuning -> Prompt engineering -> Evaluation metrics calculation -> Performance comparison

**Design Tradeoffs**: Traditional models offer lower cost but potentially lower performance; LLMs provide higher performance with simple prompts but require API access; supervised fine-tuning achieves best results but requires labeled data and higher computational resources.

**Failure Signatures**: Poor performance on complex entity types; overfitting on limited training data; prompt sensitivity in in-context learning; domain mismatch between pretraining and clinical text.

**3 First Experiments**:
1. Compare BERT Base performance across different entity types in CADEC corpus
2. Test GPT-4o with zero-shot vs. few-shot in-context learning using simple prompts
3. Evaluate cost-performance tradeoff between in-context learning and supervised fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single clinical domain (adverse drug reactions) and CADEC corpus
- Comparison uses different evaluation metrics and implementations across approaches
- Cost analysis for supervised fine-tuning lacks specific computational expense details
- Does not investigate impact of prompt engineering variations or demonstration examples

## Confidence

**Major Claim Clusters Confidence**:
- **High confidence**: BERT Base vs. domain-specific BERT variants (minimal improvement)
- **Medium confidence**: In-context learning with simple prompts outperforming complex prompts
- **Medium confidence**: Supervised fine-tuning achieving highest F1 score (incomplete cost-benefit analysis)

## Next Checks

1. Replicate the comparison across multiple clinical NER datasets covering different medical specialties to assess generalizability
2. Test multiple LLM models (Claude, Llama, etc.) and prompt engineering strategies to determine if GPT-4o results are model-specific
3. Conduct a detailed cost-benefit analysis including computational resources, time, and licensing costs for each approach