---
ver: rpa2
title: Cache Management for Mixture-of-Experts LLMs -- extended version
arxiv_id: '2509.02408'
source_url: https://arxiv.org/abs/2509.02408
tags:
- cache
- which
- paging
- page
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces and studies a new online paging problem,
  called layered paging, to model expert management in Mixture-of-Experts Large Language
  Models (MoE-LLMs). The problem captures the layered architecture of MoE-LLMs and
  the need to efficiently cache frequently used expert weights across layers.
---

# Cache Management for Mixture-of-Experts LLMs -- extended version

## Quick Facts
- **arXiv ID:** 2509.02408
- **Source URL:** https://arxiv.org/abs/2509.02408
- **Reference count:** 30
- **Primary result:** LLRU cache policy achieves up to 15% fewer cache faults than LRU in practical MoE-LLM scenarios.

## Executive Summary
This paper introduces layered paging, a new online paging problem that models expert weight management in Mixture-of-Experts Large Language Models. The problem captures the layered architecture where expert weights must be efficiently cached across layers. The authors establish theoretical lower bounds for both deterministic and randomized algorithms, showing LRU-like policies perform well under mild assumptions. They propose LLRU, a layer-based extension of LRU, and validate through extensive simulations on real MoE traces and synthetic datasets that LLRU significantly outperforms standard LRU and other policies.

## Method Summary
The study uses a layered paging model where expert activations are serialized into sequential rounds, even when multiple experts are requested per layer. The LLRU algorithm evicts the page maximizing the Last-Round Index (time since last use divided by layer count) with tie-breaking based on Relative Layer Distance (position in layer cycle). Experiments compare LLRU against optimal offline (Belady's algorithm), standard LRU, and a Marking randomized baseline using both real MoE traces from Mixtral 7B and Llama-MoE, and synthetic Zipf-distributed workloads. Cache fault counts are measured across varying cache sizes.

## Key Results
- LLRU achieves up to 15% fewer cache faults than standard LRU in practical scenarios
- LLRU's performance gap increases with larger cache sizes
- The theoretical analysis shows LRU-like policies have competitive ratios that approach the established lower bounds under reasonable assumptions

## Why This Works (Mechanism)
LLRU leverages the layered structure of MoE-LLMs by considering both temporal recency and layer position when making eviction decisions. By dividing the time since last use by the layer count, it accounts for the cyclical nature of expert requests across layers. The secondary tie-breaking criterion ensures evictions prioritize pages that are furthest from their next expected layer position, exploiting the regular pattern of expert activations.

## Foundational Learning
- **Layered Paging Model:** A theoretical framework extending standard paging to capture sequential expert requests across model layers; needed to formally analyze MoE cache management and enable rigorous competitive analysis.
- **Last-Round Index:** A metric combining temporal recency with layer periodicity; needed to identify which cached expert weights are least likely to be reused soon given the model's layered structure.
- **Relative Layer Distance:** A tie-breaking metric based on position within the layer cycle; needed to distinguish between pages with equal temporal recency by considering their layer-level access patterns.
- **Competitive Ratio Analysis:** A method for comparing online algorithms to optimal offline solutions; needed to establish theoretical performance guarantees and validate algorithm design choices.
- **Belady's Algorithm:** The optimal offline paging algorithm that evicts the page accessed furthest in the future; needed as a theoretical benchmark for measuring the effectiveness of online cache replacement policies.

## Architecture Onboarding

**Component Map:** Real Traces/Layer Traces -> Discrete Event Simulator -> Cache Management Algorithms (Opt, LRU, LLRU, Marking) -> Cache Fault Metrics

**Critical Path:** Data preparation (trace generation/serialization) → Simulator execution (event loop with cache state tracking) → Algorithm implementation (eviction logic) → Performance measurement (fault counting and normalization)

**Design Tradeoffs:** The layered paging model simplifies MoE behavior by serializing multi-expert layer activations, trading modeling accuracy for analytical tractability and cleaner theoretical results. LLRU's two-tier eviction decision (primary by time/layer, secondary by layer distance) balances implementation simplicity with performance gains over standard LRU.

**Failure Signatures:** If LLRU performs identically to standard LRU, the tie-breaking logic based on Relative Layer Distance is likely not being properly implemented or is ineffective due to trace characteristics. If results deviate significantly from paper plots, the serialization of multi-expert layer activations may be incorrect.

**First Experiments:**
1. Verify the trace serialization process by comparing the round structure of multi-expert layers between your implementation and the paper's description
2. Test LLRU on a small synthetic dataset with known expert access patterns to confirm the eviction logic behaves as expected
3. Run a minimal experiment comparing LLRU and LRU on a single cache size to establish the basic performance relationship before scaling up

## Open Questions the Paper Calls Out
- Whether randomized algorithms exist that achieve the derived lower bound of max(H_n, log(ℓ)/(6n)), or simply improve upon the standard competitive ratio of H_k
- How the layered paging model extends to cases where multiple experts are requested per layer rather than a single expert
- Whether learning-augmented algorithms can leverage machine-learned predictions to improve competitive ratios for MoE caching
- What the tight competitive ratios are when the number of experts (n) and layers (ℓ) are allowed arbitrary, unrestricted growth

## Limitations
- The theoretical analysis assumes idealized conditions where expert activations are fully observable and cache replacement decisions can be made deterministically
- Real MoE-LLM deployments may have expert activations influenced by dynamic input context, potentially reducing the effectiveness of proposed policies
- The empirical evaluation uses a limited set of models and prompts, and the simulation assumes a simplified cache model that may not capture all complexities of actual hardware memory hierarchies

## Confidence
- **Theoretical Claims:** High - The paper provides clear lower bounds and proof sketches for the layered paging problem
- **Empirical Claims:** Medium - Results are convincing within the study's scope but would benefit from broader model coverage and more realistic hardware simulation

## Next Checks
1. **Trace Coverage:** Re-run experiments on a more diverse set of MoE models and prompts to validate the robustness of the results
2. **Hardware Simulation:** Extend the simulator to model realistic hardware constraints (memory bandwidth, latency) to assess LLRU's practical impact
3. **Ablation Studies:** Conduct experiments isolating the impact of each LLRU component (layer-based eviction vs. tie-breaking) to understand which aspects contribute most to performance gains