---
ver: rpa2
title: Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation
  with LLMs
arxiv_id: '2506.10299'
source_url: https://arxiv.org/abs/2506.10299
tags:
- speech
- text
- units
- training
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting large language models
  (LLMs) to speech modality for speech-to-speech translation (S2ST). LLMs are pre-trained
  on text-only data, which presents difficulties in adapting them to speech with limited
  speech-to-speech data due to modality gaps in length and representation.
---

# Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs

## Quick Facts
- **arXiv ID**: 2506.10299
- **Source URL**: https://arxiv.org/abs/2506.10299
- **Reference count**: 0
- **Primary result**: Scheduled interleaved speech-text training improves S2ST translation performance, especially for low-resource languages

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) to speech modality for speech-to-speech translation (S2ST). LLMs are pre-trained on text-only data, creating difficulties when adapting to speech with limited speech-to-speech data due to modality gaps in length and representation. The authors propose scheduled interleaved speech-text training, which uses interleaved speech and text units during LLM fine-tuning for S2ST, with aligned text tokens interleaved at the word level with speech units. The ratio of text units is gradually decreased during training to facilitate progressive modality adaptation from text to speech.

## Method Summary
The proposed method implements a novel fine-tuning strategy for LLMs in speech-to-speech translation tasks. The approach interleaves speech and text units during training, starting with a higher proportion of text units that gradually decrease as training progresses. This scheduled approach aims to bridge the modality gap between speech and text representations. Aligned text tokens are inserted at the word level with speech units, creating a progressive adaptation pathway from the text modality (where LLMs are already proficient) to the speech modality (where they need adaptation). The method was evaluated on the CVSS dataset using LLaMA3.2-1B fine-tuning.

## Key Results
- Consistent improvements in translation performance compared to baseline across all language pairs
- Significant improvements for low-resource languages (e.g., Italian-English ASR-BLEU improved from 12.8 to 19.5)
- The scheduled interleaving approach successfully bridged the modality gap between speech and text representations

## Why This Works (Mechanism)
The method works by leveraging the LLM's existing text proficiency while gradually transitioning to speech modality. By starting with more text units and progressively reducing their ratio, the model maintains stable training while adapting to the new speech modality. This approach addresses the fundamental challenge that LLMs, despite their strong language understanding capabilities, struggle with speech modality due to pre-training on text-only data. The word-level alignment between speech and text tokens ensures that the model learns meaningful correspondences between modalities, facilitating more effective cross-modal transfer.

## Foundational Learning

**Speech-to-Speech Translation (S2ST)**: The task of translating speech from one language to speech in another language, requiring both speech recognition and synthesis capabilities.
- *Why needed*: Understanding the target application domain where LLMs need to be adapted
- *Quick check*: Verify that the system handles both source speech processing and target speech generation

**Modality Gap**: The difference in representation and processing requirements between text and speech modalities, including differences in length, structure, and temporal properties.
- *Why needed*: Identifies the core challenge that the proposed method aims to address
- *Quick check*: Compare feature dimensions and temporal characteristics between speech and text representations

**Interleaved Training**: A training strategy where different data types or modalities are mixed in a scheduled pattern during model updates.
- *Why needed*: Core mechanism of the proposed approach for progressive modality adaptation
- *Quick check*: Verify that the interleaving ratio changes according to the scheduled schedule

## Architecture Onboarding

**Component Map**: LLM (input) -> Interleaved Speech-Text Units -> Fine-tuning Module -> S2ST Output

**Critical Path**: The model receives interleaved speech and text units, processes them through the LLM layers, and gradually adapts from text-dominant to speech-dominant inputs while maintaining translation quality.

**Design Tradeoffs**: The method balances between leveraging existing text capabilities and learning new speech representations, trading off immediate speech performance for more stable long-term adaptation.

**Failure Signatures**: Potential issues include mode collapse if text ratio decreases too rapidly, or insufficient speech adaptation if text ratio remains too high throughout training.

**First Experiments**: 1) Test different initial text-speech ratios to find optimal starting point, 2) Vary the rate of text ratio decrease to optimize adaptation speed, 3) Compare word-level vs. phrase-level alignment strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (CVSS) and single LLM variant (LLaMA3.2-1B), raising questions about generalizability
- Computational efficiency and inference speed not addressed, which are critical for practical deployment
- Ablation study focuses primarily on speech-text ratios without exploring alternative interleaving strategies or alignment techniques

## Confidence
- Generalizability: Medium - Results consistent but evaluation scope is narrow
- Modality bridging claims: Medium - Experimental evidence supports claims but limited comparison with alternative approaches
- Scalability: Low - No testing with larger LLM variants or different architectures

## Next Checks
1. Evaluate the approach on multiple speech-to-speech translation datasets covering diverse language pairs and speech domains
2. Test the method with larger LLM variants (e.g., LLaMA3.2-8B, 70B) to assess scalability
3. Conduct a comprehensive ablation study comparing scheduled interleaving with alternative modality adaptation strategies such as gradual fine-tuning from text-only initialization or multi-stage training approaches