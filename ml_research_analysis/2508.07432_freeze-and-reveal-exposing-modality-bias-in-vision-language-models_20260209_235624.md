---
ver: rpa2
title: 'Freeze and Reveal: Exposing Modality Bias in Vision-Language Models'
arxiv_id: '2508.07432'
source_url: https://arxiv.org/abs/2508.07432
tags:
- bias
- gender
- text
- vision
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates gender bias in Vision-Language Models (VLMs)
  by dissecting the contributions of vision and text backbones using targeted debiasing
  techniques: Counterfactual Data Augmentation (CDA) and Task Vector methods. A novel
  metric, Degree of Stereotypicality (DoS), and a corresponding debiasing method,
  Data Augmentation Using DoS (DAUDoS), are introduced to reduce bias with minimal
  computational cost.'
---

# Freeze and Reveal: Exposing Modality Bias in Vision-Language Models

## Quick Facts
- arXiv ID: 2508.07432
- Source URL: https://arxiv.org/abs/2508.07432
- Reference count: 4
- Paper demonstrates gender bias in VLMs can be reduced by 6-9% using targeted debiasing methods

## Executive Summary
This paper investigates gender bias in Vision-Language Models (VLMs) by dissecting the contributions of vision and text backbones. The authors introduce two debiasing techniques - Counterfactual Data Augmentation (CDA) and Task Vector methods - to isolate and mitigate bias in specific model components. A novel metric called Degree of Stereotypicality (DoS) is developed to quantify bias, along with a corresponding debiasing method called Data Augmentation Using DoS (DAUDoS). The study reveals that CLIP's vision encoder is more biased while PaliGemma2's text encoder shows greater bias, enabling more targeted and effective bias mitigation strategies.

## Method Summary
The authors employ targeted debiasing techniques to identify which components of VLMs contribute most to gender bias. Counterfactual Data Augmentation (CDA) generates synthetic data to reduce stereotypical associations, while Task Vector methods modify model representations during training. The Degree of Stereotypicality (DoS) metric quantifies how strongly a model associates certain occupations with specific genders. DAUDoS uses this metric to create balanced training data that reduces bias with minimal computational overhead. Experiments compare these methods across two model architectures (CLIP and PaliGemma2) using the VisoGender benchmark, measuring both bias reduction and gender identification accuracy improvements.

## Key Results
- CDA reduces gender gap by 6% and DAUDoS by 3% using only one-third of training data
- Both methods improve gender identification accuracy by 3%
- CLIP's vision encoder is more biased while PaliGemma2's text encoder is more biased
- Targeted debiasing of dominant bias sources proves more effective than general approaches

## Why This Works (Mechanism)
The paper's approach works by first identifying which modality (vision or text) contributes most to bias in a given VLM architecture, then applying targeted debiasing techniques to that specific component. By freezing the non-dominant modality during training, the model can focus computational resources on correcting bias in the problematic component. The DoS metric provides a quantitative measure of stereotypical associations, enabling data augmentation strategies that specifically address identified biases. This component-level analysis reveals that different architectures have different bias profiles, explaining why one-size-fits-all debiasing approaches may be suboptimal.

## Foundational Learning
- **Degree of Stereotypicality (DoS)**: A metric quantifying how strongly a model associates occupations with specific genders; needed to measure bias objectively and guide debiasing efforts; quick check: compare DoS values before and after debiasing to verify effectiveness
- **Counterfactual Data Augmentation (CDA)**: Technique generating synthetic training examples that challenge stereotypical associations; needed to create balanced training data without requiring extensive manual annotation; quick check: verify synthetic examples maintain semantic coherence while reversing stereotypical patterns
- **Task Vector Methods**: Approaches that modify model representations during training to reduce bias; needed for fine-grained control over model behavior during the learning process; quick check: examine embedding space before and after application to ensure stereotypical clustering is reduced
- **Modality-specific Bias Analysis**: Framework for determining whether vision or text components contribute more to overall bias; needed to enable targeted rather than blanket debiasing strategies; quick check: freeze one modality and measure bias in the other to identify dominant source

## Architecture Onboarding

**Component Map**: Input images and text -> Vision encoder (CLIP/PaliGemma2) -> Text encoder (CLIP/PaliGemma2) -> Cross-modal attention -> Output predictions

**Critical Path**: Vision encoder <-> Cross-modal attention <-> Text encoder, with bias originating primarily in one component depending on architecture

**Design Tradeoffs**: Targeted debiasing reduces bias with less computational cost but may not address intersectional biases; modality-specific analysis requires model-specific tuning but achieves better results than general approaches

**Failure Signatures**: If debiasing the wrong modality is attempted, minimal bias reduction occurs despite computational investment; if DoS metric is inaccurate, augmented data may not effectively reduce bias

**3 First Experiments**:
1. Measure DoS separately for vision and text components to identify dominant bias source
2. Apply CDA to only the biased modality while freezing the other to test targeted effectiveness
3. Compare gender identification accuracy before and after debiasing to ensure performance isn't degraded

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the study.

## Limitations
- Analysis focuses specifically on gender bias within the VisoGender benchmark, potentially missing intersectional biases
- DoS metric requires further validation across diverse datasets and cultural contexts
- Method effectiveness varies significantly between model architectures, limiting generalizability
- Computational efficiency claims based on one-third training data usage, but scaling effects remain unexplored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Vision encoders contribute more to gender bias in CLIP models | High |
| Text encoders are more biased in PaliGemma2 | High |
| DoS metric effectively guides debiasing | Medium |
| Targeted debiasing outperforms general approaches across architectures | Medium |
| Findings generalize to other demographic factors | Low |

## Next Checks
1. Test CDA and DAUDoS methods on additional VLMs beyond CLIP and PaliGemma2, including models trained on different datasets
2. Evaluate DoS metric and debiasing effectiveness on intersectional bias scenarios combining gender with race, age, or other attributes
3. Conduct ablation studies varying training data proportions to quantify the relationship between data efficiency and debiasing performance across different model scales