---
ver: rpa2
title: Societal AI Research Has Become Less Interdisciplinary
arxiv_id: '2506.08738'
source_url: https://arxiv.org/abs/2506.08738
tags:
- societal
- research
- interdisciplinary
- teams
- orientation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed over 100,000 AI research papers to examine
  how societal concerns are integrated into technical AI research. Using a machine
  learning classifier, it measured the prevalence of societal orientation in papers
  based on ethical values and societal concerns.
---

# Societal AI Research Has Become Less Interdisciplinary

## Quick Facts
- arXiv ID: 2506.08738
- Source URL: https://arxiv.org/abs/2506.08738
- Authors: Dror Kris Markus; Fabrizio Gilardi; Daria Stetsenko
- Reference count: 19
- Primary result: CS-only teams increased from 49% to 71% of societally-oriented AI research output between 2014-2024

## Executive Summary
This study analyzed over 100,000 AI research papers to examine how societal concerns are integrated into technical AI research. Using a machine learning classifier, it measured the prevalence of societal orientation in papers based on ethical values and societal concerns. The research found that while interdisciplinary teams are more likely to produce societally-oriented work, computer science-only teams now account for a growing share of the field's overall societal output. Between 2014 and 2024, the proportion of societally-oriented content attributed to CS-only teams increased from 49% to 71%. These teams increasingly focus on societal questions across diverse topics like fairness, safety, healthcare, and misinformation. The findings challenge assumptions about interdisciplinary collaboration driving societal AI and suggest evolving norms within computer science itself are playing a key role.

## Method Summary
The researchers analyzed over 100,000 AI research papers using a machine learning classifier to measure societal orientation based on ethical values and societal concerns. They categorized papers by author discipline composition (computer science-only, interdisciplinary, or social science/humanities) and tracked changes in societal content production over time. The study examined the relationship between team composition and societal research output, identifying shifts in authorship patterns across the AI research landscape.

## Key Results
- CS-only teams increased their share of societally-oriented research from 49% to 71% between 2014-2024
- Interdisciplinary teams remain more likely to produce societally-oriented work, but their overall contribution share is declining
- CS-only teams now address diverse societal topics including fairness, safety, healthcare, and misinformation

## Why This Works (Mechanism)
The shift toward CS-only teams producing societally-oriented research suggests internal evolution of norms and priorities within computer science, rather than external interdisciplinary pressure driving societal concerns into AI research.

## Foundational Learning
- Machine learning classification of academic papers - needed to systematically identify societal orientation across large corpora; quick check: validation accuracy metrics
- Author discipline attribution - needed to categorize research teams; quick check: consistency of disciplinary labels across institutions
- Societal orientation metrics - needed to quantify ethical and societal content; quick check: keyword coverage across relevant domains
- Longitudinal analysis methods - needed to track trends over time; quick check: temporal consistency of measurement
- Citation network analysis - needed to understand research impact patterns; quick check: connectivity between CS-only and interdisciplinary work

## Architecture Onboarding
**Component map:** Paper corpus -> ML classifier -> Societal orientation scores -> Author discipline categorization -> Trend analysis -> Governance implications
**Critical path:** Classification accuracy → Reliable trend detection → Valid authorship attribution → Meaningful governance insights
**Design tradeoffs:** Comprehensive coverage vs. classifier precision; temporal consistency vs. evolving terminology; disciplinary attribution vs. cross-cutting expertise
**Failure signatures:** Classifier bias toward certain terminology; misclassification of interdisciplinary work; temporal drift in societal concept definitions
**First experiments:** 1) Manual validation of classifier output on random samples; 2) Cross-validation with alternative societal orientation measures; 3) Sensitivity analysis of authorship attribution methods

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the implications for AI safety and governance if the majority of societally-oriented research is undertaken by exclusively technical teams?
- Basis in paper: [explicit] The authors explicitly state this question in the abstract and conclusion, highlighting the shift from interdisciplinary to CS-only teams.
- Why unresolved: While the study quantifies the shift in authorship, it does not evaluate the qualitative impact on the robustness, inclusivity, or effectiveness of resulting AI governance frameworks.
- What evidence would resolve it: Comparative analysis of policy outcomes or safety incidents arising from AI systems developed by interdisciplinary teams versus CS-only teams.

### Open Question 2
- Question: What distinctive perspectives can social scientists and humanists offer to shape AI's future in a technical field that is increasingly responsive to societal demands?
- Basis in paper: [explicit] The authors explicitly ask this in the abstract and conclusion, noting that CS teams now lead societal output without external disciplinary pressure.
- Why unresolved: The paper identifies the diminishing volume share of SSH contributions but does not define the unique value proposition or frameworks SSH scholars must provide to remain relevant.
- What evidence would resolve it: Qualitative analysis identifying critical theoretical frameworks or methodologies present in SSH-led research that are absent in CS-only societal research.

### Open Question 3
- Question: Does the increased integration of societal concerns in AI research align with actual public demands or democratic interests?
- Basis in paper: [inferred] The conclusion notes the study assesses "societal orientation" but does not address alignment with "actual public demands," suggesting a gap between researcher-defined and public-defined needs.
- Why unresolved: The study measures the *presence* of societal keywords and topics, not the validity of those concerns relative to broader public discourse.
- What evidence would resolve it: Correlating the topics in the research corpus with public opinion data or democratic deliberation outcomes to identify convergence or divergence.

## Limitations
- Machine learning classifier accuracy and generalizability across domains and time periods is not explicitly validated
- Significant inference about evolving norms within computer science lacks supporting evidence
- Study measures presence of societal concerns but not alignment with actual public demands

## Confidence
- CS-only teams' increasing share (49% to 71%): Medium confidence due to unverified classifier accuracy
- Interdisciplinary teams more likely to produce societal work: High confidence
- Evolving norms within CS driving change: Medium confidence, requires further evidence

## Next Checks
1. Conduct manual validation of a stratified random sample of classified papers to assess classifier accuracy and potential bias
2. Analyze citation networks to determine whether CS-only and interdisciplinary societally-oriented papers have different real-world impacts
3. Track individual researcher career trajectories to understand how researchers with social science backgrounds influence AI teams over time