---
ver: rpa2
title: 'Emergent, not Immanent: A Baradian Reading of Explainable AI'
arxiv_id: '2601.15029'
source_url: https://arxiv.org/abs/2601.15029
tags:
- which
- human
- these
- they
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critiques the dominant assumptions in Explainable AI
  (XAI), which treat interpretability as a technical problem of uncovering pre-existing
  explanations within AI models. Drawing on Karen Barad's agential realism, the authors
  propose an alternative onto-epistemology where interpretations are material-discursive
  performances emerging from situated entanglements between AI models, humans, context,
  and explanatory tools.
---

# Emergent, not Immanent: A Baradian Reading of Explainable AI

## Quick Facts
- arXiv ID: 2601.15029
- Source URL: https://arxiv.org/abs/2601.15029
- Reference count: 40
- Authors: Fabio Morreale; Joan Serrà; Yuki Mitsufuji
- This paper critiques dominant XAI assumptions and proposes an alternative framework based on Barad's agential realism

## Executive Summary
This paper offers a fundamental critique of current Explainable AI (XAI) approaches, arguing they operate under flawed assumptions about immanent explanations and human-machine commensurability. Drawing on Karen Barad's agential realism, the authors propose that explanations emerge from material-discursive entanglements rather than being extracted from pre-existing truths within AI models. They advocate for diffractive XAI that embraces difference, interference, and situated accountability rather than seeking universal transparency.

The paper reframes XAI as an ethical practice of responsibility, moving beyond technical problem-solving toward practices that acknowledge the co-constitutive nature of humans, AI systems, and their contexts. Design directions include interfaces that present multiple interpretations, foreground uncertainty, and emphasize the situatedness of all explanatory acts.

## Method Summary
The authors employ diffractive analysis to examine various XAI methods through the lens of Barad's agential realism. Rather than treating these methods as neutral tools for extracting explanations, they analyze how each approach performs specific material-discursive entanglements that shape what counts as explanation. The analysis reveals how traditional XAI methods often reinforce problematic assumptions about immanent explananda and human-machine equivalence.

## Key Results
- Traditional XAI methods operate under flawed assumptions about immanent explanations waiting to be extracted from models
- Explanations should be understood as emerging from material-discursive entanglements rather than uncovering pre-existing truths
- Diffractive XAI design should embrace multiple interpretations, situatedness, and foreground uncertainty rather than seeking universal transparency

## Why This Works (Mechanism)
The framework works by shifting the ontological foundation of XAI from representationalism to performativity. Rather than assuming explanations exist as objective truths within models to be discovered, Baradian agential realism posits that explanations materialize through specific practices of engagement. This shift addresses the fundamental mismatch between how current XAI treats interpretability as a technical problem and how humans actually make meaning through situated, context-dependent interactions.

The mechanism operates through recognition that both AI systems and human interpreters are part of ongoing material-discursive practices that co-constitute reality. By acknowledging this entanglement, diffractive XAI can better account for the irreducible differences between computational and human ways of knowing, leading to more honest and accountable explanatory practices.

## Foundational Learning
- **Agential Realism**: Understanding reality as emerging from material-discursive practices rather than pre-existing independently
  - Why needed: Provides the philosophical foundation for critiquing immanent explananda assumptions
  - Quick check: Can you explain how agential realism differs from representationalist epistemology?

- **Diffractive Analysis**: A method of reading that examines patterns of difference rather than reflection or reproduction
  - Why needed: Offers a way to critique XAI methods that goes beyond surface-level evaluation
  - Quick check: How does diffraction differ from reflection as an analytical approach?

- **Material-Discursive Practices**: The inseparability of material and discursive elements in constituting reality
  - Why needed: Challenges the separation between technical and interpretive aspects of XAI
  - Quick check: Can you identify material and discursive elements in a typical XAI interface?

- **Intra-action**: The mutual constitution of entities through their relations, rather than interaction between pre-existing entities
  - Why needed: Reframes the relationship between humans, AI systems, and explanatory tools
  - Quick check: How does intra-action differ from interaction in explaining human-AI relationships?

- **Situated Accountability**: The practice of taking responsibility for the specific material-discursive practices one engages in
  - Why needed: Provides an ethical framework for XAI that goes beyond transparency
  - Quick check: What would situated accountability look like in a real XAI application?

## Architecture Onboarding

**Component Map**: Human <-> Explanation Interface <-> AI Model <-> Context/Situation

**Critical Path**: User engagement with interface → Material-discursive performance → Emergent interpretation → Situated accountability

**Design Tradeoffs**: Universal transparency vs. situated difference; Single truth vs. multiple interpretations; Technical extraction vs. ethical practice

**Failure Signatures**: Over-reliance on immanent explananda leads to brittle explanations; Assumption of human-machine commensurability creates false confidence; Universal standards ignore contextual specificity

**First Experiments**:
1. Implement multiple interpretation display in existing XAI interface and measure user comprehension
2. Create uncertainty-foregrounding interface and compare decision outcomes with traditional approaches
3. Design situated explanation prototype that varies based on user context and evaluate effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- The high level of philosophical abstraction may limit accessibility for XAI practitioners without posthumanist philosophy backgrounds
- Limited empirical evidence demonstrating how diffractive XAI performs differently from existing approaches in real-world scenarios
- The practical implementation of agential realism principles remains largely theoretical

## Confidence

| Claim | Confidence |
|-------|------------|
| Traditional XAI operates under flawed immanent explananda assumptions | High |
| Baradian framework provides valid philosophical critique | High |
| Diffractive XAI alternative is philosophically sound | High |
| Practical implementation of diffractive XAI is viable | Medium |
| Diffractive XAI improves real-world interpretability outcomes | Medium |

## Next Checks

1. Conduct empirical studies comparing user understanding and decision-making outcomes between traditional XAI interfaces and those designed with diffractive principles (multiple interpretations, situatedness, uncertainty foregrounding)

2. Develop prototype diffractive XAI interfaces and evaluate their effectiveness in specific domains (e.g., medical diagnosis, loan applications) to test whether embracing difference and interference produces more meaningful explanations than truth-extraction approaches

3. Interview XAI practitioners and users to assess whether the theoretical framework of agential realism provides actionable insights for improving existing interpretability methods or whether it remains too abstract for practical application