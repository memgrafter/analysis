---
ver: rpa2
title: 'CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable
  Context Engineering'
arxiv_id: '2509.21035'
source_url: https://arxiv.org/abs/2509.21035
tags:
- latency
- edge
- context
- learning
- clause
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes CLAUSE, an agentic neuro-symbolic framework\
  \ for knowledge graph question answering that dynamically constructs compact, provenance-preserving\
  \ contexts under explicit latency, token, and edge-budget constraints. Instead of\
  \ static k-hop expansion, CLAUSE uses three coordinated agents\u2014Subgraph Architect,\
  \ Path Navigator, and Context Curator\u2014to jointly build, explore, and curate\
  \ graph evidence, stopping automatically when costs outweigh utility."
---

# CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering

## Quick Facts
- **arXiv ID**: 2509.21035
- **Source URL**: https://arxiv.org/abs/2509.21035
- **Reference count**: 9
- **Primary result**: Achieves higher exact match scores (e.g., +39.3 EM@1 on MetaQA-2-hop) while reducing latency by 18.6% and edge growth by 40.9% compared to strong RAG baselines.

## Executive Summary
CLAUSE introduces an agentic neuro-symbolic framework for knowledge graph question answering that dynamically constructs compact, provenance-preserving contexts under explicit latency, token, and edge-budget constraints. Instead of static k-hop expansion, CLAUSE uses three coordinated agents—Subgraph Architect, Path Navigator, and Context Curator—to jointly build, explore, and curate graph evidence, stopping automatically when costs outweigh utility. It trains these agents with LC-MAPPO, a constrained MARL method that optimizes a centralized multi-head critic with per-resource dual variables to enforce per-query budgets while maximizing answer accuracy. Experiments on HotpotQA, MetaQA, and FactKG show CLAUSE achieves higher exact match scores while reducing latency and edge growth compared to strong RAG baselines, yielding compact, auditable contexts and predictable trade-offs among accuracy, latency, and cost.

## Method Summary
CLAUSE formulates multi-hop KGQA as a constrained Markov Decision Process where three agents (Subgraph Architect, Path Navigator, Context Curator) operate in a sequential edit→traverse→curate loop. The Subgraph Architect dynamically expands and prunes the KG subgraph using a gain-price rule that accepts edits only when utility exceeds edge cost penalized by dual variable λ_edge. The Path Navigator traverses paths with CONTINUE/BACKTRACK/STOP actions, while the Context Curator performs listwise snippet selection under token budget. Training uses LC-MAPPO with a centralized multi-head critic (task + 3 cost heads) and separate dual variables updated via projected ascent. At inference, agents act greedily using local observations plus learned dual prices, achieving coordination without centralized execution.

## Key Results
- +39.3 EM@1 on MetaQA-2-hop compared to static RAG baselines
- 18.6% latency reduction and 40.9% edge growth reduction on HotpotQA
- Feasibility rate improvement of 191% on constrained budget tests
- Higher EM@1 scores across all benchmarks while maintaining per-query budget constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Per-step cost shaping via Lagrangian dual variables enables learned stopping decisions that respect per-episode resource budgets.
- **Mechanism**: The algorithm maintains separate dual prices (λ_edge, λ_lat, λ_tok) that penalize each action's resource consumption. Agents receive shaped rewards r'_t = r_acc - λ_edge·c_edge - λ_lat·c_lat - λ_tok·c_tok, so continuation only occurs when expected marginal utility exceeds current price. Dual variables update via projected ascent to enforce E[C_k] ≤ β_k.
- **Core assumption**: The cost processes are approximately Markovian and credit can be meaningfully attributed to discrete agent actions.
- **Evidence anchors**: [abstract] "CLAUSE employs the proposed Lagrangian-Constrained Multi-Agent PPO (LC-MAPPO) algorithm... so that subgraph construction, reasoning-path discovery, and evidence selection are jointly optimized under per-query resource budgets"; [Section 4.4] "Rather than fixing a single penalty, LC–MAPPO maintains separate dual variables λ_edge, λ_lat, λ_tok and updates them by projected ascent"; [corpus] Related work (A2RAG, Graph-R1) addresses cost-aware reasoning but uses fixed schedules rather than learned dual variables.
- **Break condition**: If costs become non-stationary across queries or episodes, static dual variables may fail to track optimal prices; the Lagrangian formulation assumes convex constraint sets.

### Mechanism 2
- **Claim**: Centralized multi-head critic with decentralized execution enables coordinated multi-agent behavior while preserving test-time scalability.
- **Mechanism**: During training, a centralized critic estimates Q_task plus three cost heads (Q_edge, Q_lat, Q_tok) over joint actions, using COMA-style counterfactual advantages for credit assignment. At inference, each agent acts greedily using only local observations plus learned dual prices—no centralized coordination required.
- **Core assumption**: The value function is learnable and the monotonic mixer constraint (from QMIX-style factorization) does not severely limit expressiveness for this task structure.
- **Evidence anchors**: [Section 4.4] "A centralized critic estimates one task head Q_task and three cost heads (Q_edge, Q_lat, Q_tok) over joint actions; a monotonic mixer aggregates per-agent utilities for each head"; [Section 4.4] "PPO surrogate uses COMA-style counterfactual advantages on the shaped return"; [corpus] No direct corpus comparison to multi-head critics; related MARL work (G-SPEC) uses policy enforcement rather than value decomposition.
- **Break condition**: If agent actions become highly correlated in ways the monotonic mixer cannot express, credit assignment degrades; value factorization methods can misattribute credit when joint action values are non-monotonic.

### Mechanism 3
- **Claim**: Reversible, price-conditioned graph editing replaces static k-hop heuristics with query-adaptive subgraph growth.
- **Mechanism**: The Subgraph Architect scores frontier edges via fused signals (entity match, relation similarity, neighborhood cues, degree prior), then applies a gain–price rule: accept edits only when s(e|q, G_t) - λ_edge·c_edge > 0. DELETE actions allow backtracking from over-expanded regions. This yields compact, provenance-preserving subgraphs.
- **Core assumption**: Edge utility scores correlate with downstream answer accuracy and can be learned from sparse reward signals.
- **Evidence anchors**: [Section 4.3] "Decisions follow a gain–price rule (a_t, e_t) = argmax[s(e|q, G_t) - λ_edge·c_edge(a, e)], accepting only with positive shaped gain and remaining budget"; [Table 4] "w/o Subgraph Architect (Static RAG; no-KG): EM drops sharply to 74.8 while latency and edge usage rise to 1.32× and 1.44×"; [corpus] Weak corpus signal; A2RAG mentions cost-aware retrieval but not reversible editing.
- **Break condition**: If multi-hop reasoning requires edges with low immediate scores but high downstream value, myopic scoring may prune critical paths; the horizon H cap mitigates but does not eliminate this risk.

## Foundational Learning

- **Concept: Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: The paper formulates KGQA as a CMDP with episode-level constraints on edges, steps, and tokens. Understanding Lagrangian relaxation and dual variables is essential to grasp how LC-MAPPO enforces budgets.
  - Quick check question: Why does the algorithm update λ via projected ascent rather than treating constraints as fixed penalties?

- **Concept: Centralized Training with Decentralized Execution (CTDE)**
  - Why needed here: CLAUSE trains with a global critic but deploys three independent agents. CTDE explains how coordination emerges from joint training while inference remains scalable.
  - Quick check question: What information does each agent have access to during inference vs. training?

- **Concept: Neuro-symbolic Integration (Rule/Path Learning Paradigm)**
  - Why needed here: The paper positions itself in category (3) of neuro-symbolic KGQA: neural controller searches over symbolic paths/rules. Actions are discrete KG operations while neural modules provide scores.
  - Quick check question: Why does the paper avoid differentiable logic, and what is the "principled linkage" it relies on instead?

## Architecture Onboarding

- **Component map**: Question q → entity anchoring → Subgraph Architect (edge edits) → Path Navigator (path traversal) → Context Curator (snippet selection) → Reader LLM → Answer y

- **Critical path**: Question q → entity anchoring → Architect (edge edits) → Navigator (path traversal) → Curator (snippet selection) → Reader LLM → Answer y. The shaped reward propagates back through all three agents via the centralized critic.

- **Design tradeoffs**:
  - **Cap mode vs. price mode**: Hard budgets (β) provide guarantees; prices (λ) enable smooth trade-offs but may violate constraints
  - **Monotonic mixer constraint**: Enables stable factorization but limits expressiveness for non-monotonic joint values
  - **Horizon cap H**: Prevents unbounded exploration but may truncate valuable deep paths
  - **Separate vs. single penalty**: LC-MAPPO's separate duals allow independent control of three cost types; single-multiplier RCPO conflates them

- **Failure signatures**:
  - Constraint violations (feasibility rate < 1.0): Dual variables not converging or learning rate too low
  - Accuracy collapse with budget tightening: Shadow prices too aggressive; reduce η in dual updates
  - Edge budget explosion (normalized > 1.2×): Architect not respecting λ_edge; check gain–price threshold
  - High latency variance: Navigator not learning STOP; inspect termination head confidence

- **First 3 experiments**:
  1. **Constraint satisfaction baseline**: Run LC-MAPPO vs. MAPPO vs. RCPO on MetaQA with fixed budgets (edge=0.5, latency=0.7); verify feasibility rate improvement (target: +191% as reported)
  2. **Agent ablation sweep**: Remove each agent sequentially; confirm EM drops and latency/edge increases match Table 4 patterns (w/o Architect: ~10-15 EM drop, 1.3-1.4× resource growth)
  3. **Budget–accuracy frontier**: Sweep β_edge and β_tok jointly on HotpotQA; plot EM@1 vs. normalized edge budget and token mass to reproduce Pareto curves

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CLAUSE's performance generalize across different LLM families and scales (e.g., 7B vs. 70B parameters), and does the learned stopping policy transfer when the reader LLM is swapped?
  - **Basis in paper**: [explicit] The authors use only Qwen3-32B for all RAG and agent-based experiments, with no investigation of whether the LC-MAPPO-trained policies are reader-agnostic or overfit to this specific model.
  - **Why unresolved**: The framework claims general applicability but is only validated with a single reader LLM; policy transfer across model families remains untested.
  - **What evidence would resolve it**: Experiments training CLAUSE with one LLM (e.g., Qwen3-32B) and evaluating with different readers (e.g., LLaMA3.3-70B, GPT-OSS-120B) without retraining the agents.

- **Open Question 2**: Does LC-MAPPO's convergence guarantee hold empirically for constrained multi-agent settings with non-stationary environments, and how sensitive is stability to the choice of PID control parameters?
  - **Basis in paper**: [inferred] The paper states convergence "in Appendix??" and mentions PID control is "optionally" used for dual updates, but provides no empirical analysis of convergence behavior or sensitivity to hyperparameters like η or PID gains.
  - **Why unresolved**: Theoretical and empirical convergence properties of the Lagrangian-constrained CTDE scheme are not substantiated in the main text or visible appendix.
  - **What evidence would resolve it**: Ablation studies showing training curves under different learning rates, PID configurations, and random seeds, plus characterization of failure modes when constraints become infeasible.

- **Open Question 3**: How does CLAUSE scale to industrial-size knowledge graphs (e.g., >10M entities), where subgraph frontier expansion and centralized critic computation may become bottlenecks?
  - **Basis in paper**: [inferred] Experiments use standard benchmarks (MetaQA, HotpotQA, FactKG) with relatively small graphs; no analysis is provided on computational complexity or memory requirements as KG size increases.
  - **Why unresolved**: The three-agent sequential loop and centralized multi-head critic may not remain tractable for large-scale deployment.
  - **What evidence would resolve it**: Profiling of latency, memory, and critic forward-pass time as graph size increases, with comparison to approximate or decentralized alternatives.

## Limitations
- The neural architectures for the three agents—especially the multi-signal scorer weights, encoder layers, and mixer design—are not disclosed, making exact replication impossible.
- Training hyperparameters such as learning rates, batch sizes, dual update rates (η), and horizon cap (H) are unspecified, leaving performance dependent on unspecified design choices.
- The Lagrangian relaxation assumes convex constraint sets and Markovian cost processes; violations of these assumptions could lead to poor dual convergence or unstable constraint satisfaction.

## Confidence
- **High confidence** in the formulation of CLAUSE as a CMDP and the use of Lagrangian relaxation with separate dual variables for enforcing per-query resource budgets.
- **Medium confidence** in the CTDE architecture and the LC-MAPPO training procedure, including the centralized multi-head critic and COMA-style counterfactual advantages.
- **Medium confidence** in the reversible, price-conditioned graph editing mechanism, based on the gain-price rule and empirical evidence from ablation studies.
- **Low confidence** in the exact performance numbers (e.g., EM@1 scores, latency reductions, edge growth reductions) without access to the full implementation and hyperparameter tuning.

## Next Checks
1. **Constraint Satisfaction Baseline Test**: Run LC-MAPPO against MAPPO and RCPO on MetaQA with fixed budgets (edge=0.5, latency=0.7). Measure feasibility rates and verify if LC-MAPPO achieves the reported +191% improvement in constraint satisfaction.

2. **Agent Ablation Sweep**: Sequentially remove each agent (Subgraph Architect, Path Navigator, Context Curator) and measure EM drops and resource usage increases. Confirm if the ablation results match Table 4 patterns, particularly the ~10-15 EM drop and 1.3-1.4× resource growth when removing the Architect.

3. **Budget-Accuracy Frontier Plot**: Perform a joint sweep of β_edge and β_tok on HotpotQA. Plot EM@1 versus normalized edge budget and token mass to reproduce the Pareto curves and verify the trade-off claims between accuracy and resource usage.