---
ver: rpa2
title: LLM-based phoneme-to-grapheme for phoneme-based speech recognition
arxiv_id: '2506.04711'
source_url: https://arxiv.org/abs/2506.04711
tags:
- decoding
- training
- speech
- phoneme
- llm-p2g
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LLM-P2G, a phoneme-to-grapheme approach
  for phoneme-based automatic speech recognition using large language models. The
  method addresses information loss challenges in cascading speech-to-phoneme and
  phoneme-to-grapheme models by introducing two training strategies: data augmentation
  with noisy phonemes (DANP) and randomized top-K marginalized (TKM) training and
  decoding.'
---

# LLM-based phoneme-to-grapheme for phoneme-based speech recognition

## Quick Facts
- **arXiv ID**: 2506.04711
- **Source URL**: https://arxiv.org/abs/2506.04711
- **Reference count**: 0
- **Key outcome**: Up to 3.6% (Polish) and 6.9% (German) relative WER reduction over WFST baseline using mT5-base with DANP and TKM training

## Executive Summary
This paper introduces LLM-P2G, a phoneme-to-grapheme approach for phoneme-based automatic speech recognition using large language models. The method addresses information loss challenges in cascading speech-to-phoneme and phoneme-to-grapheme models by introducing two training strategies: data augmentation with noisy phonemes (DANP) and randomized top-K marginalized (TKM) training and decoding. Experiments on Polish and German CommonVoice datasets show that LLM-P2G with randomized TKM achieves up to 3.6% and 6.9% relative word error rate reductions compared to WFST-based systems, while simplifying the decoding pipeline. The approach demonstrates efficient use of pre-trained acoustic and language models for crosslingual ASR, with performance dependent on pre-training data availability.

## Method Summary
The method uses a two-stage pipeline: a frozen acoustic model (Whistle-S) generates phoneme hypotheses, and a pre-trained LLM (mT5-base) performs phoneme-to-grapheme conversion. Two key training strategies are introduced: Data Augmentation with Noisy Phonemes (DANP) generates diverse, error-prone phoneme sequences to match inference conditions, and Randomized Top-K Marginalized (TKM) training maximizes marginal likelihood over multiple phoneme hypotheses to mitigate information loss. The approach fine-tunes the LLM with augmented data and decodes using beam search over top-K phoneme candidates with optional LM rescoring.

## Key Results
- Relative WER reduction of up to 3.6% for Polish and 6.9% for German compared to WFST baseline
- TKM training with randomized 8-of-32 phoneme candidates outperforms deterministic approaches
- Performance highly dependent on LLM pre-training data coverage (German > Polish)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Training with synthetic noise improves robustness to upstream S2P errors
- **Mechanism**: DANP reduces distributional mismatch between clean training phonemes and noisy inference phonemes by fine-tuning on diverse error patterns
- **Core assumption**: S2P error patterns during training approximate those during inference
- **Evidence anchors**: Abstract states DANP addresses "phoneme errors introduced by the S2P model"; Section 3.2 describes adding noise to compensate for mismatch
- **Break condition**: Catastrophic S2P failure (wrong language phonemes or blank outputs) exceeds P2G correction capacity

### Mechanism 2
- **Claim**: Marginalizing over multiple phoneme hypotheses mitigates information loss
- **Mechanism**: TKM sums probabilities over top-K phoneme candidates rather than committing to single 1-best sequence
- **Core assumption**: Correct transcription recoverable from at least one top-K hypothesis
- **Evidence anchors**: Section 3.3 explains marginalization over top-K hypothesized sequences; Section 5.2 shows 8-of-32 sampling achieves sufficient diversity
- **Break condition**: K too low prunes correct sequence; K too high destabilizes training with excessive noise

### Mechanism 3
- **Claim**: Phonemes as discrete interface enable efficient cross-lingual transfer
- **Mechanism**: Decouples acoustics (S2P) from linguistics (P2G), leveraging LLM's multilingual understanding
- **Core assumption**: Target language phonemes well-represented in LLM pre-training
- **Evidence anchors**: Section 1 notes discrete token compatibility with LLMs; Section 5.1 attributes Polish performance lag to limited pre-training data
- **Break condition**: Severe degradation for low-resource languages with insufficient LLM exposure

## Foundational Learning

- **Concept**: **Weighted Finite State Transducers (WFST)**
  - **Why needed here**: Baseline architecture LLM-P2G replaces; understanding helps contextualize pipeline complexity
  - **Quick check question**: Can you explain why WFST requires pre-compiled pronunciation lexicon and why this is rigid for new languages?

- **Concept**: **Connectionist Temporal Classification (CTC)**
  - **Why needed here**: Whistle-S2P uses CTC; understanding necessary to interpret top-K hypothesis generation
  - **Quick check question**: In CTC output, how do you derive final label sequence from frame-level probabilities, and what role does blank token play?

- **Concept**: **Marginalization in Latent Variable Models**
  - **Why needed here**: TKM innovation relies on marginalizing latent phoneme variable h
  - **Quick check question**: If you have latent variable h with 3 possible states, how do you calculate total probability of observed output y?

## Architecture Onboarding

- **Component map**: Raw audio (x) -> Whistle-S (Conformer-based) -> Top-K Phoneme Hypotheses (h) -> mT5-base (Encoder-Decoder) -> Text Subwords (y)

- **Critical path**:
  1. S2P Inference: Beam search on Whistle-S to generate Top-K phoneme sequences
  2. P2G Training (TKM): Sample n=8 candidates from K=32, calculate negative log marginal likelihood loss
  3. P2G Decoding: Beam search on Top-K phonemes, aggregate text candidates, re-score with marginal probability + optional LM

- **Design tradeoffs**:
  - Pipeline Simplicity vs End-to-End Optimization: Removes WFST graph compilation but introduces non-differentiable interface
  - Randomized vs Deterministic Training: Improves generalization but adds stochasticity requiring stable training

- **Failure signatures**:
  - Cascading Error Explosion: S2P WER >50% prevents recovery regardless of DANP/TKM
  - Low-Resource LLM Blindness: Poor LLM pre-training coverage (e.g., Polish) prevents phoneme-to-grapheme mapping

- **First 3 experiments**:
  1. Baseline Replication: Implement "Whistle Subword FT" and "Whistle Phoneme FT + WFST" baselines
  2. DANP Ablation: Compare clean phonemes vs beam-search noisy phonemes on small dataset
  3. TKM vs Best-Path: Compare 1-best path vs marginalizing over top-8 paths

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Performance highly dependent on upstream S2P model quality - breaks down when S2P WER >50%
- Significant performance gap between languages based on LLM pre-training data coverage
- Randomized training introduces stochasticity without providing variance metrics across runs

## Confidence

- **High Confidence**: DANP mechanism for addressing train-test mismatch between clean and noisy phonemes
- **Medium Confidence**: TKM effectiveness in improving WER, though sampling approach is somewhat heuristic
- **Low Confidence**: Claims about system simplicity versus WFST pipelines

## Next Checks

1. **Ablation of S2P Quality Threshold**: Systematically evaluate LLM-P2G performance at different S2P WER levels (10%, 20%, 30%, 40%) to establish minimum acceptable quality

2. **Cross-Lingual Generalization Test**: Train LLM-P2G on German and evaluate on phonemically similar language (Dutch/Swedish) to test generalizability

3. **Real-Time Decoding Efficiency Analysis**: Measure computational overhead and latency of TKM decoding versus standard WFST decoding