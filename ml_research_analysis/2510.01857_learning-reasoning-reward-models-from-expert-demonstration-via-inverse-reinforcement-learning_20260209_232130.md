---
ver: rpa2
title: Learning Reasoning Reward Models from Expert Demonstration via Inverse Reinforcement
  Learning
arxiv_id: '2510.01857'
source_url: https://arxiv.org/abs/2510.01857
tags:
- reward
- reasoning
- answer
- dense
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes using inverse reinforcement learning to learn
  dense, token-level reasoning reward models from expert demonstrations, rather than
  relying on outcome-based verification or pure imitation. The learned reward serves
  a dual role: as a training signal that outperforms supervised fine-tuning baselines
  (e.g., 79% vs.'
---

# Learning Reasoning Reward Models from Expert Demonstration via Inverse Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2510.01857
- **Source URL:** https://arxiv.org/abs/2510.01857
- **Reference count:** 40
- **Primary result:** Learned reasoning reward models achieve 79% vs 56% on GSM8K and 74% vs 65% on MedReason compared to SFT baselines

## Executive Summary
This paper proposes learning dense, token-level reasoning reward models from expert demonstrations using inverse reinforcement learning (IRL), rather than relying on outcome-based verification or pure imitation. The learned reward serves dual roles: as a training signal that outperforms supervised fine-tuning baselines and as an inference-time reranker that improves selection from candidate traces. Experiments show that while sparse and step-wise rewards are most stable for training, dense rewards are strongest for inference reranking, particularly on Llama3 architectures. The method also provides interpretable token-level diagnostics that pinpoint where reasoning goes wrong.

## Method Summary
The method learns a token-level reasoning reward model via adversarial IRL, where a discriminator classifies expert reasoning traces versus policy-generated traces. The implicit reward is derived from discriminator logits and used to optimize the policy via GRPO. Four reward granularity levels are explored: sparse (only at final step), step-wise (at sentence delimiters), interval (every k tokens), and fully dense (all tokens). The discriminator is trained with masked binary cross-entropy, using expert traces as positives and policy-generated traces (optionally corrupted) as negatives. The policy is optimized using group-standardized advantages. Training involves a warm-up phase followed by adversarial training, with rewards used both for training and inference-time reranking of candidate traces.

## Key Results
- Outperforms SFT baselines: 79% vs 56% on GSM8K, 74% vs 65% on MedReason
- Dense rewards provide strongest inference-time reranking (+12pp gains on Llama3)
- Sparse and step-wise rewards most stable for training
- Reward model provides interpretable error localization at specific reasoning failure points
- Architecture-dependent performance: Llama3 shows strong reranking gains, Qwen2.5 shows weaker separation

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Adversarial IRL learns token-level rewards that discriminate expert reasoning from policy outputs, providing a dense training signal that outperforms imitation.

**Mechanism:** A discriminator $D_\phi$ is trained to classify tokens from expert traces (positive) versus policy-generated traces (negative, optionally augmented with synthetic corruptions). The implicit reward $v_\phi(y_t) = \log D_\phi(y_t) - \log(1 - D_\phi(y_t))$ is derived from discriminator logits, then used to optimize the policy via GRPO.

**Core assumption:** Expert demonstrations implicitly encode which intermediate reasoning steps are valuable, and this can be extracted without explicit step-level annotations.

**Evidence anchors:**
- [abstract] "learns (partially) dense token-level reasoning reward models directly from expert demonstrations...outperforming SFT baselines on GSM8K (79% vs. 56%)"
- [section 4.1] "We train a reasoning reward model, $D_\phi$, as a discriminator to distinguish expert reasoning traces from those generated by the policy."
- [corpus] "Beyond Imitation: Recovering Dense Rewards from Demonstrations" establishes SFT-IRL equivalence, supporting the theoretical basis.

**Break condition:** Adversarial mode collapse occurs when the policy exploits the reward model without producing correct reasoning (e.g., Qwen2.5-3B dense drops to 4% accuracy).

### Mechanism 2
**Claim:** Sparse and step-wise reward formulations provide stable training signals, while dense rewards excel at inference-time discrimination despite training instability.

**Mechanism:** The paper introduces four reward granularity levels controlled by a binary mask $m_t$: (1) sparse outcome ($m_t=1$ only at $t=T$), (2) step-wise ($m_t=1$ at sentence delimiters), (3) interval ($m_t=1$ every $k$ tokens), (4) fully dense ($m_t=1$ for all $t$). Unmasked tokens inherit rewards via "backfilling" from the next checkpoint.

**Core assumption:** Granularity trade-off exists between gradient variance (denser = higher variance) and discriminative precision (denser = more precise error localization).

**Evidence anchors:**
- [section 5.1] "sparser signals tend to be more robust. The sparse and step-wise formulations consistently yield the highest accuracy."
- [section 5.2] "while dense rewards proved more unstable for optimisation, they are as strong discriminators at inference time for the Llama family...+12pp increase"
- [corpus] PACR paper similarly finds dense model-intrinsic rewards improve reasoning exploration.

**Break condition:** Dense rewards cause mode collapse when discriminator becomes overconfident ($|v_\phi| \to \infty$), necessitating reward clipping to $[-\beta, \beta]$.

### Mechanism 3
**Claim:** The learned reward model provides interpretable error localization by assigning negative rewards at specific reasoning failure points.

**Mechanism:** Dense rewards function as a value function estimator. When a logical error occurs, the reward shifts sharply negative at that token, with penalties propagating forward to subsequent tokens even if syntactically correct in isolation.

**Core assumption:** The discriminator learns to recognize reasoning validity patterns, not just surface-form correctness.

**Evidence anchors:**
- [section 5.3] "at the specific moment of the logical error...the reward signal shifts sharply to negative...This shows that the dense reasoning reward model has learned to pinpoint the cause of failure"
- [figure 4] Visualizes correct vs. incorrect traces showing the first erroneous step with sharp negative reward in 7th line
- [corpus] GRACE paper explores interpretable IRL rewards via LLM-generated code, corroborating interpretability potential.

**Break condition:** No explicit failure mode stated, but implied sensitivity to perturbation quality—if synthetic corruptions are not representative of real errors, localization may be unreliable.

## Foundational Learning

- **Concept: Inverse Reinforcement Learning (IRL)**
  - Why needed here: IRL infers reward functions from demonstrations rather than assuming rewards are known. Essential for understanding why this approach avoids requiring hand-crafted process rewards.
  - Quick check question: Can you explain why IRL is preferable to hand-crafting rewards for multi-step reasoning tasks?

- **Concept: Adversarial Training (GAN-style)**
  - Why needed here: The discriminator-policy minimax game mirrors GAN training. Understanding mode collapse, non-stationarity, and the need for clipping/warmup is critical for debugging training instability.
  - Quick check question: What happens when a discriminator becomes too confident too quickly, and how does reward clipping mitigate this?

- **Concept: Policy Gradient Methods (PPO/GRPO)**
  - Why needed here: The policy is optimized using GRPO (Group Relative Policy Optimization), a variant of PPO. Understanding advantage estimation, clipping, and group standardization is necessary for implementation.
  - Quick check question: Why does group standardization of rewards reduce variance compared to per-sample normalization?

## Architecture Onboarding

- **Component map:**
  - Expert demonstrations -> Discriminator (token classifier) -> Implicit reward $v_\phi$ -> Policy model (via GRPO) -> Reasoning traces
  - Discriminator also used directly for inference-time reranking of candidate traces

- **Critical path:**
  1. Warm-up phase (250 steps): Train discriminator on expert vs. policy traces with correctness-aware positive/negative set construction.
  2. Adversarial training (500 steps): Alternate between discriminator update (Eq. 2) and policy update (Eq. 6).
  3. Inference: Sample N=16 traces, compute mean discounted reward, rerank and select best.

- **Design tradeoffs:**
  - **Granularity vs. stability:** Dense rewards provide best reranking but risk mode collapse; step-wise offers stable training.
  - **Model family selection:** Llama3 shows strong calibration for reranking (+10-12pp gains); Qwen2.5 shows weaker separation (marginal or negative gains)—pre-training representations matter.
  - **Symmetric vs. asymmetric training:** Same-size critic-policy is more stable; transferring 3B critic to 8B policy works but increases volatility.

- **Failure signatures:**
  - **Mode collapse:** Reward values rise while correctness accuracy decouples (drops to near-zero). Check training curves for divergence between reward and correctness metrics.
  - **Poor reranking calibration:** If t-statistic between correct/incorrect reward distributions is low (e.g., <5), the reward model is not discriminating—likely architecture mismatch or insufficient warm-up.
  - **Negative reranking gains:** If reward-guided selection underperforms random, the reward model may be exploiting surface features—verify perturbation augmentation is applied.

- **First 3 experiments:**
  1. **Baseline validation:** Replicate sparse reward training on GSM8K with Qwen2.5-3B. Confirm ~79% pass@1 vs. 56% SFT baseline. Monitor reward-correctness correlation throughout training.
  2. **Granularity ablation:** Compare sparse, step-wise, and dense formulations on Llama3.2-3B. Measure both training stability (mode collapse rate) and inference reranking gains (Best-of-N, N=16).
  3. **Cross-architecture transfer:** Train 3B critic, apply to 8B policy within same family. Compare stability and final accuracy to symmetric setup. Document early-stage collapse frequency.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can alternative adversarial objectives (e.g., Wasserstein GAN) resolve the stability-granularity trade-off, enabling stable training with fully dense rewards while maintaining their strong inference-time discriminative power?
- **Open Question 2:** Why does the learned reward exhibit strong calibration for Llama3 architectures but weak or negative transfer for Qwen2.5 models during inference-time reranking?
- **Open Question 3:** How can dense token-level rewards be effectively leveraged for active test-time interventions such as reward-guided decoding, early-exit mechanisms, or iterative self-revision?
- **Open Question 4:** Why does combining learned step-wise rewards with ground-truth outcome verification fail to outperform pure outcome-based RL, and under what conditions would hybrid rewards be beneficial?

## Limitations
- Dense rewards cause adversarial mode collapse during training, limiting their use to inference reranking
- Performance varies significantly across model architectures (strong on Llama3, weak on Qwen2.5)
- Computational overhead of sampling multiple traces for reranking
- Stability-granularity trade-off requires careful hyperparameter tuning

## Confidence
- **High:** Core methodology of IRL-based reward learning from demonstrations
- **Medium:** Granularity-ablation results and their interpretation
- **Medium:** Architecture-dependent performance differences and their attribution to pre-training
- **Low:** Specific quantitative comparisons across all ablation conditions

## Next Checks
1. Verify sparse reward training replicates 79% pass@1 on GSM8K vs 56% SFT baseline
2. Confirm dense rewards provide +12pp reranking gains specifically on Llama3 architectures
3. Test whether reward-guided decoding outperforms Best-of-N reranking in terms of accuracy vs. compute trade-off