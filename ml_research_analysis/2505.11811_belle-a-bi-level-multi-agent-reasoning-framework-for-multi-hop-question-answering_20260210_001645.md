---
ver: rpa2
title: 'BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering'
arxiv_id: '2505.11811'
source_url: https://arxiv.org/abs/2505.11811
tags:
- multi-hop
- question
- debater
- debate
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BELLE, a bi-level multi-agent reasoning framework
  for multi-hop question answering that dynamically combines different reasoning methods
  based on question types. The authors analyze four types of multi-hop questions (Inference,
  Comparison, Temporal, Null) and find that different question types benefit from
  different combinations of reasoning operators (Chain-of-Thought, Single-step, Iterative-step,
  Sub-step, Adaptive-step).
---

# BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2505.11811
- Source URL: https://arxiv.org/abs/2505.11811
- Authors: Taolin Zhang; Dongyang Li; Qizhou Chen; Chengyu Wang; Xiaofeng He
- Reference count: 40
- Primary result: BELLE achieves up to 70.4 F1 on MultiHop-RAG and 75.7 F1 on 2WikiMultiHopQA by dynamically combining reasoning operators based on question types

## Executive Summary
BELLE introduces a bi-level multi-agent reasoning framework that dynamically combines different reasoning methods based on question types for multi-hop question answering. The framework analyzes four types of multi-hop questions (Inference, Comparison, Temporal, Null) and finds that different question types benefit from different combinations of reasoning operators. By employing a bi-level debate system with opposing debaters, fast and slow memory debaters, and a judge to determine optimal operator combinations, BELLE significantly outperforms strong baselines while demonstrating superior computational cost-effectiveness compared to single models.

## Method Summary
BELLE operates through a bi-level multi-agent debate system where question type classification drives operator selection. The framework first classifies multi-hop questions into four types using in-context learning, then employs a debate between affirmative/negative debaters (Level 1) and fast/slow memory debaters plus a judge (Level 2) to formulate an execution plan. This plan dynamically combines reasoning operators (Chain-of-Thought, Single-step, Iterative-step, Sub-step, Adaptive-step) based on the identified question type. The executor then runs the planned sequence of operators to retrieve and reason over relevant passages, ultimately producing the final answer through sub-answer aggregation.

## Key Results
- BELLE achieves 70.4 F1 on MultiHop-RAG and 75.7 F1 on 2WikiMultiHopQA, significantly outperforming strong baselines
- Dynamic operator selection based on question types improves performance by approximately 3% on average compared to single-operator approaches
- The framework demonstrates superior cost-effectiveness, achieving higher average F1 at lower average tokens than retrieval-heavy baselines like IRCoT and BeamAggR
- Ablation studies confirm the critical importance of the bi-level debate structure, with second-level debate removal causing the largest performance drop

## Why This Works (Mechanism)

### Mechanism 1: Operator-Type Sensitivity Matching
The framework's core innovation is matching specific reasoning operators to multi-hop question types. By classifying questions into Inference, Comparison, Temporal, or Null types, BELLE can apply targeted operator combinations that reduce reasoning steps and retrieval overhead for simpler types while maintaining depth for complex ones. This targeted approach is hypothesized to improve both accuracy and efficiency by avoiding unnecessary computational steps.

### Mechanism 2: Bi-Level Multi-Agent Debate for Planning
The bi-level debate architecture (opposing debaters + fast/slow memory debaters) generates more robust operator plans than single-level approaches. Level 1 debaters propose diverse operator combinations, while Level 2 debaters monitor rationality and integrate historical discussion to prevent viewpoint oscillation. The judge extracts or waits for a stable plan, ensuring coherent reasoning paths that self-correct through multi-agent interaction.

### Mechanism 3: Reduced Reasoning Overhead via Targeted Retrieval
By dynamically selecting lower-cost operator combinations for simpler question types, BELLE reduces computational overhead. The framework avoids unnecessary iterative retrieval for Comparison/Temporal questions that typically need only single-step retrieval after sub-question decomposition. This targeted approach yields higher F1 scores at lower token costs compared to fixed retrieval-heavy baselines.

## Foundational Learning

- **Concept: Multi-hop question types**
  - Why needed here: The framework's planning hinges on correctly classifying questions into Inference, Comparison, Temporal, or Null to select appropriate operators
  - Quick check question: Given a question asking "Did Event A occur before Event B?", which type is it and which operator combination does Figure 2 suggest?

- **Concept: Retrieval-augmented vs. closed-book reasoning**
  - Why needed here: BELLE dynamically selects between retrieval-heavy (iterative-step) and retrieval-light (single-step or LLM-only) operators based on question needs
  - Quick check question: For a question about a recent, specific fact not in pretraining data, why might a closed-book operator fail and which operator would the framework likely prefer?

- **Concept: Multi-agent debate dynamics**
  - Why needed here: The bi-level debate drives operator plan quality; understanding role interactions (affirmative/negative, fast/slow, judge) is critical for debugging failures
  - Quick check question: If the slow debater is removed, what does Table 2 suggest will happen to average F1, and why might oscillation increase?

## Architecture Onboarding

- **Component map**: Question Type Classifier -> Bi-Level MAD System (Affirmative/Negative Debaters, Fast/Slow Debaters, Judge) -> Operator Execution Plan -> Multi-hop QA Executor

- **Critical path**:
  1. Question classification (four types) influences meta prompt initialization
  2. Bi-level debate (typically 2-3 rounds) produces operator plan via judge
  3. Executor runs plan (e.g., sub-step decomposition followed by retrieval)
  4. Sub-answers aggregated to final answer

- **Design tradeoffs**:
  - More debate rounds or agents can improve plan quality but increase tokens (Table 3, Table 8)
  - Strong adversarial prompts (Level L3) can harm performance (Table 9); moderate "tit for tat" (Level L2) is default
  - In-context learning (ICL) for classification outperforms zero-shot but requires curated examples (Table 7)

- **Failure signatures**:
  - Repeated oscillation in operator choice across rounds (check slow debater history)
  - Misclassification of question type leading to inappropriate operator plan
  - Judge fails to extract plan within round limit (falls back to Soft Mode extraction from slow debater history)

- **First 3 experiments**:
  1. Replicate ablation: Remove second-level debate (fast/slow debaters) and measure F1 drop on HotpotQA and MuSiQue to confirm criticality per Table 2
  2. Stress test question classification: Evaluate type classifier accuracy on out-of-domain multi-hop questions to identify break condition for Mechanism 1
  3. Token budget sweep: Vary max debate rounds (1-4) and measure F1 vs. average tokens on a held-out dataset to locate cost-performance frontier per Figure 6

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be adapted to effectively handle novel or previously unseen question formats that fall outside the four predefined categories? The authors note that BELLE may struggle with novel or previously unseen question formats, as it relies on a fixed classifier trained on four specific types (Inference, Comparison, Temporal, Null).

### Open Question 2
Can the debate rules and strategies be refined to further reduce computational overhead without compromising the quality of operator planning? The paper acknowledges that relying on multiple agents iteratively is a major issue and suggests that refining debate rules could potentially reduce overhead.

### Open Question 3
How does BELLE perform when integrated into real-world applications within dynamic and evolving environments? The authors aim to investigate the integration of BELLE with real-world applications to assess its efficacy in dynamic and evolving environments, as current evaluations are limited to static benchmark datasets.

## Limitations

- **Operator-type mapping generalizability**: The correspondence between question types and optimal operator combinations is validated only on four specific datasets, limiting external validity to new domains or languages
- **Debate stability across contexts**: The benefits of the bi-level debate architecture are demonstrated on tuned datasets, but it's unclear whether these persist under different debate environments or stronger adversarial contexts
- **Computational cost trade-offs**: While cost-effectiveness is claimed, absolute cost per question is not reported, and the slow debater's token overhead could offset savings on simpler questions

## Confidence

- **High Confidence**: The bi-level debate architecture improves performance over non-debate baselines on tested datasets
- **Medium Confidence**: Dynamic operator selection based on question type improves performance and cost-effectiveness, but generalizability to new domains is untested
- **Low Confidence**: The exact mechanism by which the slow debater prevents oscillation is underspecified, with empirical evidence not isolating its specific contribution

## Next Checks

1. **Cross-dataset generalization test**: Evaluate BELLE's question type classifier and operator selection on a held-out multi-hop QA dataset to quantify accuracy drop when applying the four-type mapping outside its training distribution

2. **Debate round and cost sweep**: Systematically vary max debate rounds (1-4) and measure F1 vs. average tokens to identify the round count that maximizes cost-effectiveness and compare to fixed baselines

3. **Slow debater ablation under noise**: Remove the slow debater and measure both performance (F1) and plan stability (variance in operator choice) under varying adversarial prompt strengths to isolate the slow debater's contribution to stability