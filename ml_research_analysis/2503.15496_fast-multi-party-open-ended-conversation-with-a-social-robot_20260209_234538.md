---
ver: rpa2
title: Fast Multi-Party Open-Ended Conversation with a Social Robot
arxiv_id: '2503.15496'
source_url: https://arxiv.org/abs/2503.15496
tags:
- robot
- user
- furhat
- conversation
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a conversational system for multi-party interaction
  with a social robot, integrating multimodal perception (voice direction of arrival,
  speaker diarisation, face recognition) with a large language model (LLM) for response
  generation. The system was evaluated on the Furhat robot with 30 participants across
  two scenarios: parallel separate conversations and a shared group discussion.'
---

# Fast Multi-Party Open-Ended Conversation with a Social Robot

## Quick Facts
- arXiv ID: 2503.15496
- Source URL: https://arxiv.org/abs/2503.15496
- Reference count: 40
- Primary result: Achieved 92.6% addressee accuracy in parallel conversations and 79.3% in group settings

## Executive Summary
This paper presents a conversational system for multi-party interaction with a social robot, integrating multimodal perception (voice direction of arrival, speaker diarisation, face recognition) with a large language model (LLM) for response generation. The system was evaluated on the Furhat robot with 30 participants across two scenarios: parallel separate conversations and a shared group discussion. Results show the system maintained coherent and engaging conversations, achieving 92.6% addressee accuracy in parallel settings and 79.3% in group settings. Face recognition performed reliably (80-94%), but audio-based speaker recognition was limited (18-27%). Participants reported positive engagement and social presence, though technical issues like recognition errors and response latency affected interaction quality.

## Method Summary
The system uses an event-driven modular architecture with perception components (ReSpeaker 4-mic array for direction of arrival, Azure Cognitive Services for transcription and speaker diarisation, Furhat camera for face recognition), a conversation manager (GPT-3.5) that selects addressees and generates responses, and an interactions module that handles speech synthesis and gaze control. Users are pre-enrolled with voice recordings and face images. The system fuses audio and visual cues to attribute utterances, with face recognition taking priority in conflicts. Turn-taking is managed through silence detection and gaze direction, with the robot pausing speech when interrupted and resuming after 1.5 seconds of silence.

## Key Results
- Addressee accuracy: 92.6% in parallel conversations, 79.3% in group discussions
- Face recognition reliability: 80-94.7% accuracy across scenarios
- Audio-based speaker recognition: 18.4-26.8% accuracy, identified as problematic component
- System latency: mean 1.35s total delay including LLM generation and network transmission
- Participant engagement: Positive subjective ratings on fluency, naturalness, and social presence

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Weighted Multimodal Fusion
Prioritizing visual identification (face recognition) over audio-based speaker identification appears to mitigate the low reliability of voice diarisation in overlapping speech scenarios. The system fuses voice direction-of-arrival (DoA) with face tracking. When conflicts arise between audio and visual IDs, the architecture prioritizes the visual modality, relying on the stronger signal to maintain conversational coherence. Core assumption: Visual line-of-sight is available and face enrolment has occurred; face recognition accuracy exceeds audio recognition accuracy in the specific environment.

### Mechanism 2: LLM-Driven Addressee Selection
Delegating addressee selection to the Large Language Model (LLM) via structured prompting allows the system to dynamically resolve ambiguous turn-taking cues that rule-based systems might miss. The Conversation Manager injects the conversation history and a list of recognized users into the prompt. The LLM is tasked with outputting a structured response identifying the specific `Chosen person` alongside the `Response`, rather than just generating text. Core assumption: The LLM can infer social dynamics and intent from the transcript sufficiently to select the correct recipient better than a simple "last speaker" heuristic.

### Mechanism 3: Event-Driven Interruption Handling
An event-driven architecture decoupling perception from action enables the robot to handle "barge-ins" without waiting for full processing loops, preserving responsiveness. The *Interactions Module* subscribes to *Speaker Awareness* events. If a `robot-user-switch` event is detected (user starts talking) while the robot is speaking, the module immediately pauses speech synthesis. It resumes only after a silence threshold (1.5s) is passed. Core assumption: The microphone array can distinguish user voice direction from the robot's own speaker output accurately enough to trigger the switch event.

## Foundational Learning

- **Concept: Speaker Diarisation vs. Identification**
  - Why needed here: The paper highlights a performance gap where diarisation (segmenting speech) worked, but identification (naming the speaker) failed (18-27%). Understanding this distinction is crucial for debugging why the robot "knew someone spoke" but didn't know "who."
  - Quick check question: Does the system fail to hear the user (Diarisation issue), or does it hear them but assign the wrong ID (Identification issue)?

- **Concept: Streaming LLM Responses**
  - Why needed here: The system uses the `stream` option to reduce perceived latency. Without streaming, the user must wait for the *entire* response generation, significantly harming the "fluidity" praised in user studies.
  - Quick check question: In the architecture, does the robot wait for the full JSON payload before speaking, or does it parse sentence chunks as they arrive?

- **Concept: Multimodal Conflict Resolution**
  - Why needed here: The system explicitly prioritizes Face ID over Voice ID. Engineers must understand this hierarchy to diagnose why the robot might look at Person A while responding to Person B (a mismatch mentioned in Section 5.2).
  - Quick check question: If Face Recognition confidence is 60% and Voice Identification confidence is 55%, which user ID does the system assign to the utterance?

## Architecture Onboarding

- **Component map:**
  - Perception: `ReSpeaker` (DoA) + `Azure Speech` (Transcription/Diarisation) + `Furhat Camera` (Face Recognition)
  - Logic: `Conversation Manager` (GPT-3.5) + `Turn-Taking Module` (Heuristics)
  - Action: `Interactions Module` (Furhat API for TTS/Gaze)

- **Critical path:**
  1. User speaks -> `Speaker Awareness` detects angle
  2. `Transcription` creates text; `Face Tracking` maps angle to ID
  3. `Conversation Manager` bundles text + ID -> sends to LLM
  4. LLM returns `Addressee` + `Response`
  5. `Interactions Module` executes speech + gaze shift

- **Design tradeoffs:**
  - **Azure vs. Local LLM:** The paper uses cloud GPT-3.5 for better reasoning but incurs network latency (avg 1.35s delay). Local models (like the Vicuna mentioned in Related Work) would be faster but potentially less coherent.
  - **Reliability vs. Enrollment:** The system requires pre-enrollment for faces and voices to achieve high accuracy (80-94%), trading off "walk-up-and-use" convenience for recognition performance.

- **Failure signatures:**
  - **The "Look-At-Wrong-Person":** Caused by Face ID and Voice ID disagreeing, or DoA drift
  - **The "Sluggish Reply":** Caused by LLM generation time (>0.76s) + Network latency
  - **The "Amnesia":** The LLM fails to surface memory in the response window, making the robot seem generic despite having a memory module

- **First 3 experiments:**
  1. **Latency Baseline:** Measure Time-to-First-Token (TTFT) for the LLM vs. the total interaction latency to isolate network delays from processing delays
  2. **Modality Ablation:** Disable Face Recognition and run the group scenario to quantify the exact drop in addressee accuracy (prediction: drops from ~80% to ~20%)
  3. **Interruption Stress Test:** Trigger rapid "barge-ins" during robot speech to verify if the `robot-user-switch` event fires reliably within 100ms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does multi-party conversational performance scale when the system must handle more than two simultaneous users?
- Basis in paper: [explicit] "Future research should therefore explore larger group settings... to assess how LLM-driven conversational architectures scale and adapt beyond controlled experimental setups."
- Why unresolved: The current evaluation was limited to exactly two participants; group size effects on addressee accuracy (which dropped from 92.6% to 79.3% moving from parallel to group) and turn-taking remain unknown.
- What evidence would resolve it: A controlled study varying group size (3, 4, 5+ participants) measuring addressee detection accuracy, turn-taking success rate, and participant engagement scores.

### Open Question 2
- Question: Can prosody-based or breath-detection turn-taking models outperform the current silence-based approach in multi-party robot interaction?
- Basis in paper: [explicit] "The use of silence instead of more sophisticated techniques is justified with the assumption that the users are going to pass the turn through gaze. We plan to experiment with these techniques in the future."
- Why unresolved: The current system relies on silence detection and gaze cues, but participants reported uncertainty about turn boundaries, especially in group settings with 60% overlap rates.
- What evidence would resolve it: Comparative evaluation of silence-based vs. prosody-based turn prediction models measuring latency, false positive/negative turn-taking rates, and user subjective ratings of turn-taking naturalness.

### Open Question 3
- Question: Would integrating visual scene information into the LLM context improve addressee detection and contextual grounding in group conversations?
- Basis in paper: [explicit] "Currently, to generate an answer, the system uses only the transcribed text; in the future, we plan to extend support to images from the camera stream for improved contextual awareness."
- Why unresolved: The current system achieved only 79.3% addressee accuracy in group settings; visual context (gestures, pointing, who is looking at whom) could provide disambiguating signals.
- What evidence would resolve it: An A/B comparison of text-only vs. vision-augmented LLM prompting measuring addressee accuracy, response relevance scores, and reduction in misaddressing incidents.

### Open Question 4
- Question: What fusion strategies can optimally resolve conflicts between audio-based and visual-based speaker recognition in real-time multi-party interaction?
- Basis in paper: [inferred] Audio recognition achieved only 18-27% accuracy while face recognition achieved 80-95%; mismatches between modalities caused user confusion ("Furhat clearly looked at me but responded to my partner").
- Why unresolved: The system currently prioritizes face recognition on conflict, but the optimal dynamic weighting strategy for multimodal fusion under varying conditions (overlap, rapid switching) remains unexplored.
- What evidence would resolve it: Systematic comparison of fusion strategies (static priority, confidence-weighted, Bayesian) measuring speaker attribution accuracy across varying noise levels and overlap conditions.

## Limitations

- Pre-enrollment requirement: System needs voice and face data from users before interaction, limiting spontaneous use
- Audio recognition weakness: Speaker identification achieved only 18-27% accuracy in group settings, a critical limitation
- Modest sample size: Evaluation involved only 30 participants without demographic diversity information

## Confidence

**High Confidence:** The architectural claims regarding multimodal fusion prioritizing face recognition over audio identification are well-supported by the quantitative results showing 80-94% face recognition accuracy versus 18-27% audio identification accuracy.

**Medium Confidence:** The LLM-driven addressee selection mechanism shows reasonable performance (79.3% accuracy in group settings) but the extent to which the LLM actually improves over simpler heuristics versus just providing coherent responses remains unclear.

**Low Confidence:** Claims about the system's "fluidity" and "social presence" are primarily based on subjective participant reports without objective behavioral measures.

## Next Checks

1. **Ablation Study on Modality Priority:** Disable the face recognition prioritization and force the system to rely solely on audio-based speaker identification. This would quantify the exact contribution of the visual modality to the 79.3% addressee accuracy in group settings.

2. **Generalization to Unenrolled Users:** Test the system with participants who haven't undergone the enrollment process to measure performance degradation. This would reveal the practical limitations of the approach in real-world scenarios where pre-enrollment isn't feasible.

3. **Longitudinal Engagement Analysis:** Conduct extended interactions (30+ minutes) with the same participants to assess whether the system maintains conversational coherence and user engagement over time.