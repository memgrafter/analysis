---
ver: rpa2
title: 'PromptAug: Fine-grained Conflict Classification Using Data Augmentation'
arxiv_id: '2506.22491'
source_url: https://arxiv.org/abs/2506.22491
tags:
- data
- datapoints
- dataset
- promptaug
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PromptAug, a data augmentation method for
  fine-grained conflict classification. It uses LLM prompting with structured components
  (instruction, context, examples, definition) to generate diverse, high-quality conflict-related
  data points.
---

# PromptAug: Fine-grained Conflict Classification Using Data Augmentation

## Quick Facts
- arXiv ID: 2506.22491
- Source URL: https://arxiv.org/abs/2506.22491
- Reference count: 23
- Primary result: 2% statistically significant improvement in accuracy and F1-score for fine-grained conflict classification

## Executive Summary
This paper introduces PromptAug, a data augmentation method for fine-grained conflict classification that uses structured LLM prompting to generate diverse, high-quality synthetic data points. The approach combines instruction, context, examples, and behavior definitions in prompts to guide LLM generation, followed by a three-assertion filtering mechanism to ensure label validity, characteristic adherence, and context appropriateness. Evaluated on conflict and emotion datasets, PromptAug achieved statistically significant improvements over original datasets and outperformed state-of-the-art methods, particularly under extreme data scarcity conditions.

## Method Summary
PromptAug employs a two-step process: (1) LLM generation using carefully designed prompts with four components - instruction, context, examples, and behavior definitions - to create synthetic social media comments representing specific conflict behaviors; (2) filtering through three binary assertions (label validity, characteristic adherence, context appropriateness) to ensure generated examples meet class definitions. The method uses open-source LLMs (Llama2-7B, Mistral-8B) and can be applied to various classification tasks, demonstrating robustness particularly when training data is limited.

## Key Results
- Achieved 2% statistically significant improvements in accuracy and F1-score over original datasets
- Outperformed state-of-the-art methods including EDA, CBERT, AugGPT, and PromptMix on both conflict and emotion classification tasks
- Demonstrated robust performance under extreme data scarcity conditions, with larger relative improvements when training data was limited
- Identified four problematic patterns in augmented text: Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation

## Why This Works (Mechanism)

### Mechanism 1
Structured four-component prompts guide LLMs to generate class-consistent synthetic data by providing explicit output format requirements, social media context grounding, few-shot examples that tether generation to existing class boundaries, and precise behavioral definitions with descriptors. This reduces random generation and improves label fidelity. The core assumption is that LLMs can internalize nuanced behavioral definitions and reproduce them in synthetic examples when sufficiently constrained. Evidence shows weak direct support, with related work on LLM-based semantic augmentation existing but not validating this specific four-component structure.

### Mechanism 2
Multi-assertion filtering removes out-of-distribution and mislabeled synthetic examples through three binary checks (label validity, characteristic adherence, context appropriateness). Unlike baseline classifier relabeling, binary queries avoid propagating classification errors common in low-performing base models. The core assumption is that binary verification is more reliable than multi-class relabeling when base classifier performance is low. Evidence shows baseline BERT accuracy of 70%, suggesting relabeling would be error-prone, though no direct corpus evidence exists for this specific filtering approach.

### Mechanism 3
Example-based tethering preserves class boundaries while allowing creative generation by including real examples in prompts that anchor synthetic generation to existing class distributions, reinforcing boundaries rather than blurring them. The core assumption is that few-shot examples provide sufficient class identity signal without over-constraining diversity. Evidence shows 0.15 F1 improvement for 'Teasing' class (smallest, most benefited from augmentation), though related work doesn't specifically compare example-tethered vs. boundary-blurring approaches.

## Foundational Learning

- **Concept**: Data augmentation for NLP classification
  - **Why needed here**: PromptAug builds on and compares against EDA, CBERT, AugGPT, and PromptMix. Understanding substitution vs. generative approaches is essential.
  - **Quick check question**: Can you explain why synonym swapping might change a sarcasm label to harassment?

- **Concept**: Few-shot LLM prompting
  - **Why needed here**: The method relies on few-shot examples within prompts. Understanding how examples influence LLM output distributions is critical.
  - **Quick check question**: What happens to output diversity when you increase the number of examples in a prompt from 1 to 5?

- **Concept**: Class boundary semantics in behavioral classification
  - **Why needed here**: The conflict dataset has blurred boundaries (e.g., teasing vs. trolling). Understanding how semantic ambiguity affects both generation and classification is essential.
  - **Quick check question**: Why might "Your argument is as solid as a Trump tweet" be classified as either sarcasm or harassment?

## Architecture Onboarding

- **Component map**: Original Dataset → Group into k=3 example sets → Prompt Construction (4 components) → LLM Generation (Llama2-7B/Mistral-8B) → Three-Assertion Filter (label/characteristic/context) → Augmented Training Set → Classifier Training

- **Critical path**: The prompt construction step is most sensitive—poorly written definitions or examples directly degrade synthetic quality.

- **Design tradeoffs**:
  - 10:1 vs 1:1 augmentation ratio: Conflict dataset (smaller) benefits from conservative augmentation; emotion dataset tolerates more
  - Open-source vs closed LLMs: Open-source enables local deployment for sensitive content but may have stronger guardrails
  - Binary filtering vs relabeling: Binary reduces error propagation but discards potentially useful boundary examples

- **Failure signatures**:
  - "Augmented Content Misinterpretation": LLM generates advice/about instead of examples of the behavior (Section 4.7)
  - "Linguistic Fluidity": Generated examples contain multiple overlapping behaviors
  - High refusal rates when generating harassment/threat classes (noted as ~70-75% in limitations)

- **First 3 experiments**:
  1. Replicate Table 6 results on conflict dataset with BERT classifier to validate PromptAug vs. EDA baseline
  2. Run ablation study removing one prompt component at a time to measure contribution (as in Table 10)
  3. Test extreme data scarcity scenario (20% training data) to verify robustness claims in Figure 5

## Open Questions the Paper Calls Out

- **Open Question 1**: How does social bias manifest in the synthetic data generated by PromptAug, and can fairness metrics or human-in-the-loop approaches effectively mitigate it?
  - **Basis in paper**: [explicit] The Conclusion explicitly suggests future work should examine "how bias presents itself within DA" using "fairness metrics" and "human-in-the-loop approaches."
  - **Why unresolved**: The authors explicitly note in the Limitations section that they "do not investigate any social bias present within the datapoints generated by the LLM."
  - **What evidence would resolve it**: A comparative bias audit of the synthetic dataset versus the original training data, measuring disparities across demographic groups using standard fairness metrics.

- **Open Question 2**: What is the trade-off between computational expense and classification performance when using larger, proprietary LLMs (e.g., GPT-4) compared to the open-source 7B models tested?
  - **Basis in paper**: [explicit] The Conclusion lists "quantifying expenses of DA methods" and evaluating generalizability using "more powerful LLMs such as GPT-4" as areas for future work.
  - **Why unresolved**: The experiments were restricted to Llama-7B and Mistral-7B to ensure data privacy and cost efficiency, leaving the performance ceiling of larger models untested.
  - **What evidence would resolve it**: Benchmarking PromptAug's performance (Accuracy/F1) using GPT-4 against the cost per augmented sample compared to the current Llama-7B baseline.

- **Open Question 3**: Can PromptAug maintain its effectiveness when applied to datasets with binary classes or large class sizes, as opposed to the small, multi-class, nuanced datasets tested?
  - **Basis in paper**: [explicit] The Limitations section states the authors "cannot assume the generalisation of our method to other datasets with two or three classes or those with large class sizes."
  - **Why unresolved**: The method was specifically designed to address the blurred boundaries of complex human behavior data; datasets with clear boundaries or massive data may not benefit from the same approach.
  - **What evidence would resolve it**: Applying PromptAug to a standard binary classification task (e.g., sentiment analysis) and a dataset with significantly larger class counts to compare performance lifts.

## Limitations

- The conflict dataset used in the primary experiments is not publicly available, preventing direct replication of core results.
- The filtering mechanism implementation remains underspecified - unclear whether filtering uses a separate LLM, classifier, or human annotators, and no exact prompts for the three assertions are provided.
- LLM guardrail limitations resulted in high refusal rates (70-75%) for generating offensive content, though mitigation strategies are not fully detailed.

## Confidence

- **High Confidence**: The core methodology (structured LLM prompting with filtering) is clearly described and reproducible with open-source components. The 2% improvement claims are statistically significant but modest, suggesting robust but incremental gains.
- **Medium Confidence**: Results on the conflict dataset cannot be independently verified due to data unavailability. The emotion dataset results (Table 11) provide some validation but use a different augmentation ratio (1:1 vs 10:1).
- **Low Confidence**: The thematic analysis findings, while insightful, lack quantitative support and may reflect subjective interpretation rather than systematic patterns.

## Next Checks

1. **Replicate on Public Data**: Implement PromptAug on the Crowdflower Emotion dataset using the specified 1:1 augmentation ratio. Compare against EDA and CBERT baselines as shown in Table 11.

2. **Ablation Study**: Systematically remove each prompt component (instruction, context, examples, definition) and measure F1-score degradation. This validates the mechanism claims in Table 10.

3. **Extreme Data Scarcity Test**: Train on only 20% of the emotion dataset (following Figure 5 methodology) to verify robustness claims under limited data conditions.