---
ver: rpa2
title: 'CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological
  Visual Question Answering'
arxiv_id: '2508.18430'
source_url: https://arxiv.org/abs/2508.18430
tags:
- clarify
- generalist
- knowledge
- specialist
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLARIFY introduces a Specialist-Generalist framework to address
  the challenge of deploying accurate, lightweight, and trustworthy dermatological
  AI systems. The framework combines a high-precision image classifier (Specialist)
  for disease detection with a compressed, knowledge-grounded Vision-Language Model
  (Generalist) for natural language responses.
---

# CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering

## Quick Facts
- arXiv ID: 2508.18430
- Source URL: https://arxiv.org/abs/2508.18430
- Reference count: 40
- CLARIFY achieves 18% higher diagnostic accuracy vs. fine-tuned baseline, with 20% VRAM reduction and 5% latency improvement.

## Executive Summary
CLARIFY introduces a Specialist-Generalist framework to address the challenge of deploying accurate, lightweight, and trustworthy dermatological AI systems. The framework combines a high-precision image classifier (Specialist) for disease detection with a compressed, knowledge-grounded Vision-Language Model (Generalist) for natural language responses. The Specialist's predictions guide the Generalist's reasoning, while a knowledge graph-based retrieval module ensures factual accuracy. Evaluated on a curated dermatology VQA dataset, CLARIFY achieves 18% higher diagnostic accuracy compared to a fine-tuned baseline VLM, while reducing VRAM usage by at least 20% and inference latency by 5%. The approach demonstrates a practical path to building efficient, clinically viable AI systems for medical diagnosis.

## Method Summary
CLARIFY uses a two-stage approach: first, a domain-trained image classifier (Specialist) with a frozen DINOv2 backbone and a lightweight classification head provides fast and highly accurate diagnostic predictions. Second, a compressed conversational VLM (Generalist) receives the Specialist's prediction, retrieves relevant facts from a knowledge graph, and generates a grounded, context-aware explanation. The framework decouples perception and reasoning to avoid catastrophic forgetting, uses guided prompting to focus the VLM on the correct diagnostic path, and employs knowledge graph-based RAG to ground responses in factual medical knowledge. The Generalist is compressed via structural pruning to reduce VRAM and latency while preserving conversational quality.

## Key Results
- CLARIFY achieves 18% higher diagnostic accuracy than a fine-tuned baseline VLM.
- Reduces VRAM usage by at least 20% and inference latency by 5%.
- Knowledge graph-based RAG successfully grounds responses in factual medical knowledge.

## Why This Works (Mechanism)

### Mechanism 1: Task Decoupling via Specialist-Generalist Architecture
- Separating visual perception from language reasoning improves diagnostic accuracy while preserving conversational ability. A domain-specific image classifier (Specialist) handles disease identification independently, conditioning a compressed VLM (Generalist) that focuses solely on generating context-aware explanations. This prevents catastrophic forgetting caused by forcing a single model to master both fine-grained classification and nuanced dialogue.

### Mechanism 2: Diagnostic Anchoring via Guided Prompting
- Injecting the Specialist's prediction into the Generalist's prompt reduces hallucination and focuses reasoning on the correct diagnostic path. The Specialist's predicted disease label is passed directly into the Generalist's prompt template, conditioning the VLM's generation and grounding the response in a specific diagnosis.

### Mechanism 3: Factual Grounding via Knowledge Graph-Based RAG
- A retrieval module using a domain knowledge graph improves the factual accuracy and trustworthiness of generated explanations. The Specialist's predicted disease triggers a semantic search over a pre-built knowledge graph. Retrieved entities (symptoms, treatments, etc.) are injected into the Generalist's prompt, constraining outputs to verified medical facts and reducing hallucination risk.

## Foundational Learning

- **Catastrophic Forgetting**
  - Why needed here: Central problem the framework avoids by not fine-tuning a single VLM for both classification and dialogue. Understanding this motivates the decoupled design.
  - Quick check question: Can you explain why fine-tuning a VLM on a small domain dataset might degrade its pre-trained conversational abilities?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core technique for grounding responses in external knowledge. Essential for understanding how the KG-RAG module enhances reliability.
  - Quick check question: How does RAG differ from standard generation, and what role does the retriever play in ensuring factual accuracy?

- **Structural Pruning of LLMs**
  - Why needed here: Used to compress the Generalist for efficient deployment. Understanding trade-offs (speed vs. quality) is key for onboarding.
  - Quick check question: What are the potential risks of removing entire transformer layers from an LLM, and how might you identify which layers are safe to prune?

## Architecture Onboarding

- **Component map:**
  1. Specialist (Perception Module): DINOv2 backbone (frozen feature extractor) + lightweight classification head (2-layer FFN). Input: skin image. Output: disease label.
  2. Knowledge Module (KG-RAG): Pre-built dermatological knowledge graph (entities/relations embedded via all-MiniLM-L6-v2). Input: disease label. Output: retrieved facts (symptoms, treatments, etc.).
  3. Generalist (Conversational VLM): Compressed VLM (e.g., pruned LLaVA/Qwen-VL). Input: image + constructed prompt (user query + Specialist diagnosis + retrieved KG facts). Output: grounded natural language response.

- **Critical path:**
  1. User submits image and query.
  2. Specialist classifies image -> disease label.
  3. KG-RAG retrieves facts using disease label.
  4. Prompt constructed with diagnosis + retrieved facts + user query.
  5. Generalist generates response using image and prompt.

- **Design tradeoffs:**
  - Specialist accuracy vs. Generalist flexibility: A more complex Specialist may improve accuracy but increase latency. A less constrained Generalist may improve fluency but risk hallucination.
  - Compression level vs. conversational quality: Aggressive pruning reduces VRAM/latency but degrades output quality (Table II shows score drops as layers removed).
  - KG scope vs. maintenance cost: A larger, more detailed KG improves grounding but requires more effort to curate and keep updated.

- **Failure signatures:**
  - Specialist misclassification: Generalist generates coherent but incorrect explanation anchored to wrong disease.
  - KG retrieval mismatch: Retrieved facts irrelevant to predicted disease, causing confusing or contradictory response.
  - Over-pruned Generalist: Responses become incoherent or fail to incorporate provided context (rapid score drop in Table II after removing many layers).

- **First 3 experiments:**
  1. Ablate the Specialist: Replace Specialist predictions with random or incorrect labels and measure degradation in final response accuracy (isolates contribution of diagnostic anchoring).
  2. Ablate the KG-RAG: Disable retrieval and measure increase in hallucinations or factual errors (isolates grounding mechanism).
  3. Vary Compression: Test Generalist performance across different pruning levels (e.g., remove 2, 5, 7 layers) to identify the point where VRAM savings no longer justify quality loss (informs deployment trade-offs).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of CLARIFY generalize to significantly larger and more diverse dermatological datasets?
- Basis in paper: Section VII (Limitations) states that experiments were conducted on a "relatively small dataset" due to resource constraints, limiting generalizability and robustness across a broader range of conditions.
- Why unresolved: The curated dataset contains only 1,776 images across 8 distinct diseases, which may not capture the full complexity, ethnic diversity, or visual variance of real-world clinical populations.
- What evidence would resolve it: Validation results on large-scale, multi-ethnic benchmarks (e.g., ISIC, FITZPATRICK17K) demonstrating that the 18% accuracy gain over baselines is maintained as data volume increases.

### Open Question 2
- Question: To what extent does the factual grounding improve when replacing the Wikipedia-derived Knowledge Graph with a clinically validated ontology?
- Basis in paper: Section VII explicitly acknowledges that the current knowledge graph was derived from Wikipedia for prototyping and is "not a clinically authoritative source."
- Why unresolved: While the framework demonstrates the *mechanism* of grounding, the actual reliability of the responses is capped by the source's non-clinical nature, risking the propagation of unverified medical information.
- What evidence would resolve it: A comparative evaluation measuring "hallucination rates" or factual accuracy scores when using expert-validated sources (e.g., UMLS, SNOMED CT) versus the current Wikipedia-based graph.

### Open Question 3
- Question: How does the system perform in failure scenarios where the Specialist module provides an incorrect diagnosis?
- Basis in paper: The paper emphasizes the Specialist's role in guiding the Generalist to the "correct diagnostic path" (Section I) but does not analyze the error propagation when the Specialist itself is wrong.
- Why unresolved: The Generalist is fine-tuned to synthesize information from the prompt; if the prompt contains a wrong diagnosis from the Specialist, the system risks generating a plausible but entirely false justification ("grounded hallucination").
- What evidence would resolve it: An ablation study measuring the Generalist's ability to "overrule" or qualify incorrect Specialist inputs, versus blindly rationalizing the error.

## Limitations
- Performance depends heavily on Specialist accuracy; downstream failures are deterministic given Specialist errors.
- Knowledge graph is derived from Wikipedia, not clinically authoritative, limiting grounding reliability.
- Experiments conducted on a relatively small dataset (1,776 images), limiting generalizability and robustness.

## Confidence

- **High confidence**: The core claim that explicit task decoupling improves accuracy while reducing resource use is supported by direct ablation studies and strong quantitative gains over a fine-tuned baseline.
- **Medium confidence**: The approach is tightly coupled to the accuracy of the Specialist, creating a vulnerability: downstream failures are deterministic given Specialist errors, yet the paper does not extensively model error propagation. The KG-RAG grounding mechanism also has medium confidence because its effectiveness depends on knowledge graph completeness, which is not fully characterized.
- **Low confidence**: A critical assumption is that compression via layer removal does not irreversibly degrade reasoning quality; while the study identifies a 20% VRAM reduction sweet spot, the evaluation is limited to conversational coherence rather than clinical correctness. There is also an implicit assumption that domain knowledge graphs can be maintained at scale without prohibitive cost.

## Next Checks

1. **Error Analysis:** Systematically quantify the correlation between Specialist misclassifications and downstream hallucination rates in the Generalist.
2. **KG Robustness:** Measure retrieval quality and grounding performance when the knowledge graph is artificially degraded (missing entities, outdated facts).
3. **Compression Limits:** Extend pruning experiments beyond 7 layers to identify the precise point where conversational coherence catastrophically collapses.