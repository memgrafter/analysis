---
ver: rpa2
title: 'DHAuDS: A Dynamic and Heterogeneous Audio Benchmark for Test-Time Adaptation'
arxiv_id: '2511.18421'
source_url: https://arxiv.org/abs/2511.18421
tags:
- amaut
- hubert
- conmix
- noise
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DHAuDS, a benchmark designed to evaluate
  Test-Time Adaptation (TTA) methods for audio classification under realistic, dynamic,
  and heterogeneous domain shifts. Unlike existing benchmarks, DHAuDS simulates real-world
  acoustic variability by applying variable corruption intensities and mixing multiple
  noise types across four standardized datasets: UrbanSound8K-C, SpeechCommandsV2-C,
  VocalSound-C, and ReefSet-C.'
---

# DHAuDS: A Dynamic and Heterogeneous Audio Benchmark for Test-Time Adaptation

## Quick Facts
- **arXiv ID:** 2511.18421
- **Source URL:** https://arxiv.org/abs/2511.18421
- **Reference count:** 40
- **Primary result:** DHAuDS introduces a benchmark for evaluating TTA methods under realistic, dynamic, and heterogeneous audio domain shifts, showing consistent TTA improvements across three models.

## Executive Summary
DHAuDS is a benchmark designed to evaluate Test-Time Adaptation (TTA) methods for audio classification under realistic, dynamic, and heterogeneous domain shifts. Unlike existing benchmarks, DHAuDS simulates real-world acoustic variability by applying variable corruption intensities and mixing multiple noise types across four standardized datasets: UrbanSound8K-C, SpeechCommandsV2-C, VocalSound-C, and ReefSet-C. The study evaluates three audio classification models—HuBERT, AMAuT, and CoNMix++—using a unified TTA strategy combining entropy minimization and consistency loss. Results show that TTA consistently improves performance across all benchmarks, with AMAuT achieving the highest average adaptation improvement. The authors also recommend using low momentum (≤0.75) and a binary learning rate strategy to stabilize adaptation and mitigate post-improvement degradation.

## Method Summary
DHAuDS evaluates TTA methods by adapting pre-trained models during inference on corrupted audio without labels. The benchmark applies four corruption types (White Noise, Environmental Noise, Time Stretch, Pitch Shift) with variable intensities across four datasets. The unified TTA approach uses entropy minimization combined with consistency loss between two temporally shifted views of each sample. A binary learning rate strategy (BLR) applies different rates to feature extractor versus classifier, while low momentum (≤0.75) stabilizes optimization. The study evaluates HuBERT, AMAuT, and CoNMix++ models across 124 experiments measuring 14 criteria per dataset.

## Key Results
- TTA consistently improves performance across all DHAuDS benchmarks, with AMAuT achieving the highest average adaptation improvement
- Low momentum (≤0.75) stabilizes adaptation and reduces post-improvement accuracy decline
- Binary learning rate strategy (BLR) helps sustain performance gains by decoupling feature extractor and classifier adaptation rates
- UrbanSound8K-C shows limited TTA gains, suggesting need for enhanced feature representations or multi-domain encoders

## Why This Works (Mechanism)

### Mechanism 1: Entropy Minimization with Multi-View Consistency
Combining entropy-based losses with consistency regularization enables stable test-time adaptation under dynamic corruption. For each test sample, two augmented views are created via random left/right temporal shifts. The model minimizes L = L_ens + λL_con, where L_ens combines Nuclear-Norm Maximization, Entropy Minimization, and Modified Generalized Entropy, while L_con penalizes divergence between predictions of the two shifted views. This encourages domain-invariant representations that remain stable across noise conditions.

### Mechanism 2: Binary Learning Rate (BLR) for Feature-Classifier Decoupling
Applying a smaller learning rate to the feature extractor than the classifier (lr_fe ≤ lr_c) sustains post-adaptation gains and mitigates performance degradation. The model is split into feature extraction and classifier components. During training, lr_fe = lr_c, but at test-time, a lower lr_fe preserves learned representations while allowing the classifier to adapt more aggressively to new distributions.

### Mechanism 3: Low Momentum Stabilization
Momentum values ≤0.75 reduce oscillatory behavior and prevent post-improvement accuracy decline during TTA. Lower momentum dampens optimizer inertia, preventing aggressive parameter updates from accumulating error when batch statistics shift rapidly under heterogeneous noise.

## Foundational Learning

- **Concept: Test-Time Adaptation (TTA)**
  - Why needed here: DHAuDS evaluates TTA methods that adapt models during inference without labels; understanding this paradigm is prerequisite to interpreting benchmark results.
  - Quick check question: Can you explain why TTA requires batch sizes ≥32 for BatchNorm stability in this framework?

- **Concept: Signal-to-Noise Ratio (SNR) as Corruption Severity**
  - Why needed here: SNR ranges (5–7 dB for L2) define corruption intensity; interpreting Table 2 requires understanding SNR-signal quality relationship.
  - Quick check question: Given SNR range [5, 7] dB for WHN-L2, would a randomly drawn 5.5 dB SNR produce more or less corruption than a 6.5 dB SNR?

- **Concept: Silhouette Score for Cluster Quality**
  - Why needed here: Table 10 uses silhouette scores to diagnose why US8-C shows limited TTA gains; negative scores indicate mis-clustering.
  - Quick check question: If pre-adaptation silhouette score is -0.02 and post-adaptation is +0.08, what does this suggest about representation quality changes?

## Architecture Onboarding

- **Component map:** Raw waveforms (HuBERT) or Mel-spectrograms (AMAuT, CoNMix++) -> CNN frontend + Transformer encoder -> Two downsampling layers -> Entropy loss (3 components) + Consistency loss + BLR optimizer

- **Critical path:**
  1. Load pre-trained model weights (source domain)
  2. Initialize corrupted test set from DHAuDS (US8-C, SC2-C, VS-C, or RS-C)
  3. Configure optimizer: momentum ≤0.75, lr_fe/lr_c ratio per model (AMAuT: 0.45–0.55, HuBERT: 0.25–0.55, CoNMix++: 0.1)
  4. For each batch: apply random temporal shifts → compute L_ens + λL_con → update parameters
  5. Evaluate on held-out corruption set (seed 123456)

- **Design tradeoffs:**
  - Batch size vs. memory: AMAuT requires ≥32 for BatchNorm stability; HuBERT needs 32–33 on RTX 5090 for longer audio (VS-C, US8-C)
  - Pseudo-labeling: CoNMix++ includes it but Table 9 shows removal often improves or maintains performance while reducing compute
  - Fixed input size: CoNMix++ requires 224×224 spectrograms, limiting audio to ≤2s and excluding PSH experiments

- **Failure signatures:**
  - Post-improvement degradation: Momentum >0.75 or single learning rate (SLR) causes accuracy drop after initial gains (Figures 1–2)
  - Negative adaptation: CoNMix++ on TST-L1/RS-C shows consistent decline even with low-momentum + BLR (Figure 3)
  - Low baseline accuracy: US8-C baseline <0.80 correlates with limited TTA gains and negative silhouette scores

- **First 3 experiments:**
  1. Reproduce AMAuT WHN-L1 on SC2-C: Verify accuracy improves from ~0.77 to ~0.89 per Table 5; confirm batch size 70 fits on available GPU.
  2. Ablate BLR on HuBERT ENQ-L1/RS-C: Compare SLR vs. BLR using Figure 6 as reference; expect BLR to sustain higher ROC-AUC.
  3. Test momentum sweep on AMAuT ENQ-L1/RS-C: Run momentum ∈ {0.5, 0.7, 0.9}; verify 0.9 shows oscillation per Figure 1.

## Open Questions the Paper Calls Out

### Open Question 1
Can enhanced feature representations or multi-domain encoders overcome the limited Test-Time Adaptation (TTA) performance observed on the UrbanSound8K-C benchmark? Section 6.1 notes that performance on US8-C remains limited (F1-scores < 0.73), implying current techniques struggle to generalize across acoustically complex urban conditions.

### Open Question 2
Do larger pre-trained models (e.g., HuBERT-Large or X-Large) offer superior robustness and adaptation capabilities on DHAuDS compared to the Base versions? Section 6.1 states that only the "Base" version of HuBERT was evaluated due to GPU resource constraints, leaving larger variants unexplored.

### Open Question 3
How effectively does the DHAuDS corruption protocol transfer to non-speech or non-environmental domains, such as music or industrial machinery? Section 6.2 suggests that "introducing additional audio categories, such as music, underwater recordings, or industrial machinery" is a necessary future direction.

## Limitations
- Learning rate values remain unspecified beyond BLR ratios; optimizer type and weight decay are not stated
- Time-stretch corruption range up to 12% may exceed validity of shift-based consistency augmentation
- Negative adaptation cases (CoNMix++ on RS-C) are consistent but not fully explained by the study

## Confidence
- **High confidence:** Entropy minimization + consistency loss improves TTA performance; momentum ≤0.75 stabilizes adaptation; AMAuT achieves highest average gains
- **Medium confidence:** BLR sustains post-improvement gains; pre-training AMAuT on CochlScene is necessary for US8 adaptation; post-improvement degradation is preventable with correct hyperparameters
- **Low confidence:** Negative adaptation cases are fully explained; silhouette scores directly predict adaptation limits

## Next Checks
1. Verify BLR vs. SLR impact on HuBERT ENQ-L1/RS-C using Figure 6 as reference
2. Test momentum sweep (0.5, 0.7, 0.9) on AMAuT ENQ-L1/RS-C to confirm oscillation at high momentum
3. Reproduce AMAuT WHN-L1 on SC2-C to validate accuracy improvement from ~0.77 to ~0.89 baseline