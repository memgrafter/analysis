---
ver: rpa2
title: AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations
arxiv_id: '2503.17388'
source_url: https://arxiv.org/abs/2503.17388
tags:
- safety
- evaluations
- post-mitigation
- capabilities
- dangerous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that AI companies should report both
  pre- and post-mitigation safety evaluations to enable informed policy decisions
  about model deployment and safety standards. The authors show that evaluating models
  at both stages provides essential evidence that neither evaluation alone can offer,
  as pre-mitigation assessments reveal unmitigated capabilities while post-mitigation
  evaluations measure safeguard effectiveness.
---

# AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations

## Quick Facts
- arXiv ID: 2503.17388
- Source URL: https://arxiv.org/abs/2503.17388
- Reference count: 13
- This position paper argues AI companies should report both pre- and post-mitigation safety evaluations to enable informed policy decisions about model deployment and safety standards.

## Executive Summary
This position paper argues that AI companies should report both pre- and post-mitigation safety evaluations to enable informed policy decisions about model deployment and safety standards. The authors show that evaluating models at both stages provides essential evidence that neither evaluation alone can offer, as pre-mitigation assessments reveal unmitigated capabilities while post-mitigation evaluations measure safeguard effectiveness. Analysis of current AI safety disclosures from leading frontier labs identifies three critical gaps: companies rarely evaluate both pre- and post-mitigation versions, evaluation methods lack standardization, and reported results are often too vague to inform policy. The paper recommends mandatory disclosure of both pre- and post-mitigation capabilities to approved government bodies, standardized evaluation methods, and minimum transparency requirements for public safety reporting.

## Method Summary
The paper analyzes current AI safety disclosure practices from leading frontier labs, identifies gaps in evaluation reporting, and proposes a framework for mandatory pre- and post-mitigation safety evaluations. The authors demonstrate their approach through case studies using frontier models and the WMDP-Chem benchmark, showing how joint evaluations inform deployment decisions and safety priorities. The methodology involves evaluating dangerous capabilities using multiple-choice benchmarks to estimate pre-mitigation capabilities while avoiding refusal triggers, measuring refusal rates on dangerous requests for post-mitigation models, and testing mitigation robustness through jailbreak attempts.

## Key Results
- Joint pre- and post-mitigation evaluations provide decision-relevant evidence that neither evaluation alone can offer
- Current AI safety disclosures rarely evaluate both pre- and post-mitigation versions of models
- The comparison between pre- and post-mitigation capabilities quantifies mitigation effectiveness and informs deployment strategy
- A four-quadrant deployment decision framework maps models based on dangerous capabilities and refusal strength to generate actionable recommendations

## Why This Works (Mechanism)

### Mechanism 1: Complementary Evidence Generation
Joint pre- and post-mitigation evaluations produce decision-relevant evidence that neither evaluation alone can provide. Pre-mitigation evaluations estimate unmitigated dangerous capabilities (worst-case harm if safeguards fail), while post-mitigation evaluations measure how effectively safeguards block harmful outputs. The comparison between them quantifies mitigation effectiveness and informs deployment strategy. Core assumption: Adversaries can potentially bypass safeguards through jailbreaks, fine-tuning APIs, or weight theft, making worst-case capability estimates relevant even for "safe" deployed models.

### Mechanism 2: Capability Elicitation via Benign Proxies
Multiple-choice benchmarks like WMDP-Chem can estimate underlying dangerous capabilities even in post-mitigation models because they don't trigger refusal responses. Well-designed multiple-choice questions probe dangerous knowledge domains without phrasing requests in ways that activate safety classifiers, allowing capability estimation when pre-mitigation models are unavailable. Core assumption: Multiple-choice accuracy on WMDP-Chem correlates with the model's ability to provide actionable dangerous assistance if refusals were absent.

### Mechanism 3: Four-Quadrant Deployment Decision Framework
Mapping models onto a 2x2 grid (dangerous capabilities: present/absent × refusal strength: strong/weak) yields actionable deployment recommendations. Each quadrant corresponds to a distinct risk profile and set of recommended actions—from open-weight release (no capabilities, strong refusal) to "do not deploy" (dangerous capabilities, weak refusal). Core assumption: The four quadrants exhaust the decision-relevant risk scenarios and the recommended actions appropriately address each profile.

## Foundational Learning

- **Pre-mitigation vs. Post-mitigation Models**
  - Why needed here: The entire paper's argument hinges on distinguishing these two model states. Pre-mitigation models lack safety training (RLHF/SFT) and don't refuse requests; post-mitigation models have undergone safety interventions. Without this distinction, the dual-evaluation framework collapses.
  - Quick check question: Can you explain why a model with "vanishingly small" refusal rates on multiple-choice chemistry questions might still refuse open-ended requests about synthesizing those same chemicals?

- **Safety Mitigation Techniques (RLHF, SFT, Input-Output Filtering)**
  - Why needed here: The paper assumes readers understand what mitigations exist and how they work. Understanding that RLHF shapes model behavior through reward signals, SFT directly trains on refusal examples, and filtering adds external classifiers is essential for interpreting why mitigations might fail.
  - Quick check question: Which mitigation technique would be most vulnerable to fine-tuning API misuse, and why?

- **Jailbreak Attacks and Circumvention Vectors**
  - Why needed here: The paper's argument for pre-mitigation evaluation rests on adversaries' ability to bypass safeguards. Understanding black-box jailbreaks, fine-tuning attacks, and weight theft as distinct attack vectors is necessary to evaluate whether worst-case capability assessments matter.
  - Quick check question: If you had API-only access to a model with strong refusal behavior, which circumvention method from Section 3.1 would still be available to you?

## Architecture Onboarding

- Component map: Pre-mitigation Model → Dangerous Capability Evaluation → Capability Score → Four-Quadrant Decision Framework → Deployment Recommendation → Post-mitigation Model → Refusal Evaluation → Refusal Rate → Mitigation Robustness Testing → Jailbreak Success Rate

- Critical path:
  1. Obtain or approximate pre-mitigation model (either from developer or via helpful-only fine-tuning for open-weight models)
  2. Run dangerous capability benchmarks (WMDP-Chem multiple-choice as proxy)
  3. Run dangerous request compliance tests on post-mitigation model (open-ended WMDP-Chem, filtered for unsafe queries)
  4. Map results to decision framework quadrant
  5. Generate deployment recommendation and security priorities

- Design tradeoffs:
  - Proxy validity vs. direct measurement: Multiple-choice proxies (WMDP-Chem) avoid triggering refusals but may not capture full dangerous capabilities. Open-ended evaluations are more realistic but confounded by refusal behavior.
  - Threshold-based vs. full disclosure: Threshold reporting (e.g., "crossed / didn't cross capability threshold") protects security but reduces granularity. Full disclosure enables scrutiny but risks misuse.
  - Public transparency vs. confidential government reporting: Some results too sensitive for public release; paper recommends confidential disclosure to AISIs with summary reports.

- Failure signatures:
  - Models that refuse multiple-choice WMDP-Chem questions → proxy capability estimate fails; need alternative benchmarks
  - StrongREJECT evaluator disagrees with human judgment on refusal classification → evaluation pipeline unreliable
  - Helpful-only fine-tuning significantly changes benchmark performance → fine-tuned model no longer represents true pre-mitigation state
  - Jailbreak success rates vary wildly across prompt templates → mitigation robustness assessment unstable

- First 3 experiments:
  1. Validate WMDP-Chem as capability proxy: Correlate multiple-choice accuracy on WMDP-Chem with open-ended compliance rates on the same knowledge domain, using a model where refusals have been removed via fine-tuning.
  2. Test refusal consistency across attack vectors: Measure post-mitigation refusal rates under baseline prompting, black-box jailbreaks, and structured red-teaming for the same set of dangerous requests.
  3. Replicate four-quadrant classification on a held-out model: Apply the paper's evaluation pipeline to a model not analyzed in the paper (e.g., a Llama variant) and verify that the quadrant classification yields coherent deployment recommendations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dangerous capability evaluation methods be improved to better approximate true model capabilities, given that unknown prompting techniques or scaffolding approaches could elicit more dangerous behaviors than current evaluations capture?
- Basis in paper: Section 6.2 states: "Current evaluation methods may significantly underestimate a model's capabilities, as unknown prompting techniques or scaffolding approaches could elicit more dangerous behaviors."
- Why unresolved: The paper treats evaluation results as lower bounds but does not propose methods to close the gap between evaluated and actual capabilities.
- What evidence would resolve it: Empirical studies systematically comparing different elicitation methods (prompting strategies, scaffolding approaches) on the same models to quantify the underestimation gap.

### Open Question 2
- Question: How should pre- and post-mitigation evaluation frameworks be extended to address misalignment risks (e.g., deceptive or scheming behavior) that capability evaluations alone cannot capture?
- Basis in paper: Section 6.3.1 states: "Our position is primarily designed to reduce misuse risks and does not address misalignment risks... For models that may exhibit deceptive or scheming behavior, capability evaluations alone may provide limited insight into fundamental safety risks."
- Why unresolved: The paper explicitly focuses on misuse and defers misalignment to "complementary evaluation approaches" without specifying what those should be.
- What evidence would resolve it: Development and validation of evaluation methods that probe alignment properties rather than just capabilities, demonstrated through predictive studies on models trained with different alignment objectives.

### Open Question 3
- Question: Do the findings about pre- and post-mitigation evaluation patterns generalize beyond the chemistry domain to other dangerous capability areas such as cybersecurity, biological weapons, or persuasion?
- Basis in paper: The case studies exclusively use WMDP-Chem; the paper does not test whether pre/post-mitigation patterns (e.g., capability preservation vs. refusal behavior) hold across domains.
- Why unresolved: Different domains may have different relationships between knowledge components and dangerous applications, and safety training may affect domains unevenly.
- What evidence would resolve it: Cross-domain evaluation studies using analogous pre/post-mitigation protocols on WMDP-Bio, WMDP-Cyber, and persuasion benchmarks.

### Open Question 4
- Question: What methodologies should guide the setting of capability thresholds (e.g., what WMDP accuracy level constitutes "dangerous capabilities") to ensure consistent and justified deployment decisions across organizations?
- Basis in paper: Section 5.2 shows how different threshold choices (60% vs 80%) lead to different deployment recommendations, but provides no principled basis for selecting thresholds.
- Why unresolved: Without standardized threshold-setting methodologies, companies may set self-serving thresholds that undermine comparability.
- What evidence would resolve it: Risk modeling studies linking benchmark performance to real-world harm potential, combined with expert elicitation to calibrate thresholds to acceptable risk levels.

## Limitations
- The paper's core argument relies on untested assumptions about WMDP-Chem's validity as a capability proxy—no external validation confirms that multiple-choice accuracy correlates with dangerous assistance ability
- The four-quadrant deployment framework oversimplifies risk assessment, potentially missing nuanced capability distributions and threat vector variations
- The case study uses only four models, limiting generalizability to the broader frontier model landscape

## Confidence
- **High confidence**: The argument that pre- and post-mitigation evaluations provide complementary evidence (Mechanism 1) is well-supported by logical reasoning and the enumeration of scenarios where joint evaluation yields different decisions than either evaluation alone
- **Medium confidence**: The framework's deployment recommendations are actionable but rely on binary categorizations that may oversimplify complex risk landscapes
- **Low confidence**: The WMDP-Chem proxy assumption lacks empirical validation in the corpus, and the claim that joint evaluation uniquely enables effective safety governance depends on this unvalidated mechanism

## Next Checks
1. Validate WMDP-Chem as capability proxy by correlating multiple-choice accuracy with open-ended compliance rates after removing refusals via fine-tuning
2. Test refusal consistency across attack vectors (baseline prompting, black-box jailbreaks, structured red-teaming) for the same dangerous requests
3. Replicate four-quadrant classification on a held-out model (e.g., Llama variant) to verify coherent deployment recommendations emerge from the framework