---
ver: rpa2
title: In-Context Learning Boosts Speech Recognition via Human-like Adaptation to
  Speakers and Language Varieties
arxiv_id: '2505.14887'
source_url: https://arxiv.org/abs/2505.14887
tags:
- speech
- speaker
- speakers
- across
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: In-context learning significantly improves ASR robustness across
  diverse English speakers and varieties using Phi-4-Multimodal. The framework interleaves
  transcribed audio-text pairs as exemplars before transcribing target audio, requiring
  no model updates or additional training data.
---

# In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties

## Quick Facts
- **arXiv ID**: 2505.14887
- **Source URL**: https://arxiv.org/abs/2505.14887
- **Reference count**: 32
- **Primary result**: ICL improves ASR robustness across diverse English varieties, reducing WER by 5.4-36.4% (average 19.7%) with 12 transcribed examples at inference

## Executive Summary
This paper demonstrates that in-context learning (ICL) enables speech language models to adapt to diverse English speakers and language varieties without model updates or additional training data. Using Phi-4-Multimodal, the authors interleave transcribed audio-text pairs as exemplars before transcribing target audio, achieving significant word error rate reductions across four English corpora spanning native and non-native speakers. The adaptation follows human-like patterns, showing rapid initial gains within the first few examples and distinct speaker-specific then variety-level phases. Most notably, low-resource varieties benefit most substantially, with some non-native speakers seeing up to 39.2% relative improvement. Despite these gains, performance disparities between native and non-native speakers persist, revealing ongoing challenges for ASR equity.

## Method Summary
The method uses Phi-4-Multimodal with a frozen text decoder and audio encoder connected via LoRA adapters. At inference time, transcribed audio-text pairs are interleaved as exemplars before the target audio, with no model updates required. Audio is preprocessed to 16kHz float32, filtered for minimum duration, and paired with normalized text (lowercase, punctuation removed). Prompts use specific token formatting with `<|audio_N|>` placeholders and `<|end|>` delimiters. The framework tests 0-12 shot conditions with both same-speaker and different-speaker context variants, using greedy decoding with flash_attention_2. All experiments use gold-standard transcripts for context examples, with evaluation across four English corpora representing diverse speaker backgrounds.

## Key Results
- **19.7% average relative WER reduction** (5.4-36.4% range) across all corpora using 12-shot ICL
- **Most gains occur within first few examples** (25-30 seconds of speech), closely paralleling human adaptation patterns
- **Two-phase adaptation mechanism**: speaker-specific advantage at 4-6 shots (19.6% relative improvement) shifting to variety-level generalization at higher shot counts

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Adaptation (Speaker-Specific → Variety-General)
ICL enables distinct speaker-specific and variety-level adaptation phases that emerge at different context lengths. At 4-6 shots, the model leverages idiosyncratic acoustic features from same-speaker examples for fine-grained calibration, then shifts to extracting shared phonetic patterns across speakers within an accent group at higher shot counts. This mirrors multi-level adaptation in human perception where listeners first attune to individual cues before generalizing to accent-level patterns.

### Mechanism 2: Rapidly-Saturating Acoustic-Phonetic Recalibration
Most adaptation benefits accrue within the first 25-30 seconds of transcribed speech, following a diminishing-returns curve. The model rapidly re-weights acoustic-phonetic cue mappings based on observed speaker-specific patterns, with core phonetic boundaries recalibrated early and additional examples providing marginal incremental value beyond 6-10 examples.

### Mechanism 3: Prompt-Activated Prior Shifting
Explicit linguistic framing (e.g., "non-native speaker") activates broader acoustic-phonetic priors learned during instruction tuning, improving tolerance for variant phone-letter mappings. The "non-native" descriptor primes the decoder to expect higher acoustic variability, potentially adjusting internal decision thresholds or re-weighting learned phonetic category boundaries before processing audio.

## Foundational Learning

- **In-Context Learning (ICL)**: Understanding how context window conditioning affects generation without parameter updates is critical since the entire method relies on providing exemplars at inference time.
  - Quick check: Can you explain why ICL differs from fine-tuning, and what "shots" means in this context?

- **Word Error Rate (WER) and Relative Improvement**: All results are reported in WER and relative WER reduction, so misinterpreting these will lead to incorrect conclusions about effect sizes.
  - Quick check: If baseline WER is 10% and 12-shot WER is 8%, what is the relative WER reduction?

- **Speaker vs. Variety Adaptation**: The paper distinguishes between adapting to an individual speaker versus adapting to an accent/dialect group, which is critical for interpreting Table 2.
  - Quick check: In the 4-6 shot range, why would same-speaker examples outperform different-speaker examples from the same variety?

## Architecture Onboarding

- **Component map**: Audio preprocessing (16kHz resampling, float32) -> Conv frontend -> Conformer blocks -> 2-layer projector -> LoRA adapters -> Frozen Phi-4 text decoder

- **Critical path**:
  1. Audio preprocessing (16kHz resampling, float32, 2.5s minimum duration, FLAC bug handling)
  2. Prompt construction (task instruction → N interleaved audio-text pairs → target audio)
  3. Special token formatting (`<|audio_N|>` placeholders, `<|end|>` delimiters)
  4. Greedy decoding with `num_logits_to_keep=1`
  5. Text normalization for WER calculation (lowercase, punctuation removal, whitespace normalization)

- **Design tradeoffs**:
  - Shot count vs. inference cost: 12 shots ≈ 50s audio in context; diminishing returns after 6-10 shots
  - Same-speaker vs. different-speaker context: Same-speaker requires enrollment; different-speaker enables zero-enrollment adaptation at 10+ shots
  - Standard vs. variation prompt: "Transcription:" markers help low-resource varieties; "non-native" framing provides small universal gains

- **Failure signatures**:
  - 1-shot WER spike: Expected transient increase as model learns task structure
  - Outlier speakers: Check for data quality issues before concluding ICL fails
  - Flash attention errors: Requires NVIDIA A100 with 40GB+ memory
  - Identical transcript contamination: Filter identical normalized transcripts during selection

- **First 3 experiments**:
  1. **Baseline establishment**: Run 0-shot on target corpus with both standard and "non-native" prompts
  2. **Scaling curve**: Test 0, 1, 2, 4, 6, 8, 10, 12 shots with same-speaker context on 3-5 speakers
  3. **Speaker specificity probe**: Compare same-speaker vs. different-speaker context at optimal shot count

## Open Questions the Paper Calls Out

**Cross-linguistic generalizability**: Does ICL extend to spontaneous speech, cross-lingual settings, and streaming applications? Current experiments focused exclusively on read English speech.

**Unsupervised context**: Can unsupervised or self-transcribed context replace ground truth transcripts for ICL adaptation? All experiments used gold-standard transcripts.

**Mechanistic understanding**: What are the precise mechanisms underlying the two-phase adaptation (speaker-specific then variety-level)? Behavioral results show the pattern but internal model representations were not analyzed.

**Architectural generalizability**: Do these ICL findings generalize across different spoken language model architectures? Only Phi-4-MM was tested, constraining generalizability claims.

## Limitations

- **Cross-linguistic uncertainty**: Adaptation mechanisms may not transfer to languages with different phonological inventories or writing systems
- **Prompt sensitivity**: Optimal prompt template may be highly sensitive to specific model architecture and training data composition
- **Speaker enrollment requirements**: Same-speaker adaptation requires prior enrollment, limiting practical deployment for unknown speakers

## Confidence

**High Confidence**: The core finding that ICL improves ASR robustness across diverse English varieties is well-supported by comprehensive experiments across four distinct corpora with 19.7% average relative WER reduction.

**Medium Confidence**: The two-phase adaptation mechanism is supported by observed performance patterns at different shot counts, but causal explanation remains somewhat speculative.

**Low Confidence**: The prompt-activated prior shifting mechanism lacks direct mechanistic evidence, with observed benefits potentially resulting from various factors.

## Next Checks

**Cross-linguistic transfer test**: Evaluate ICL framework on a non-English language with distinct phonological characteristics to determine if learned adaptation mechanisms generalize beyond English.

**Prompt variation ablation**: Systematically test alternative linguistic framings and instruction structures to identify whether benefits reflect broader semantic priming effects.

**Speaker enrollment vs. zero-shot comparison**: Compare practical utility of same-speaker adaptation (requiring enrollment) against different-speaker variety adaptation across multiple variety groups for real-world deployment scenarios.