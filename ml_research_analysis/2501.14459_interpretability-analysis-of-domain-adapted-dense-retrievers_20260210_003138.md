---
ver: rpa2
title: Interpretability Analysis of Domain Adapted Dense Retrievers
arxiv_id: '2501.14459'
source_url: https://arxiv.org/abs/2501.14459
tags:
- domain
- query
- document
- attribution
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Integrated Gradients (IG) to analyze
  and interpret domain-adapted dense retrievers. The authors introduce a novel baseline
  method that computes both query and document attributions by replacing tokens with
  [PAD] tokens.
---

# Interpretability Analysis of Domain Adapted Dense Retrievers

## Quick Facts
- arXiv ID: 2501.14459
- Source URL: https://arxiv.org/abs/2501.14459
- Reference count: 25
- Primary result: Domain-adapted dense retrievers show increased attribution to domain-specific terminology and document titles compared to non-adapted models

## Executive Summary
This paper introduces a novel method for interpreting domain-adapted dense retrievers using Integrated Gradients (IG). The authors propose using [PAD] tokens as a baseline to compute both query and document attributions, revealing how domain adaptation shifts model attention toward in-domain terminology. Applied to TREC-COVID and FIQA datasets, the method shows that GPL-adapted models emphasize domain-specific vocabulary and document titles more than non-adapted models. The approach provides both instance-level explanations and ranking-level visualizations through word clouds, demonstrating IG's viability for interpreting dense retrieval models.

## Method Summary
The paper applies Integrated Gradients to domain-adapted dense retrievers, using [PAD] tokens as a baseline for attribution computation. Query-side attributions are computed by replacing query tokens with [PAD] while preserving document tokens, and document-side attributions use the inverse. The method is applied to GPL-adapted models trained on TREC-COVID (biomedical) and FIQA (financial) datasets. Attributions are aggregated across top-25 retrieved documents to generate ranking-level explanations, visualized as word clouds showing which terms the model systematically rewards or penalizes.

## Key Results
- NDCG@10 improved from 0.6510 to 0.7160 (+9.98%) for TREC-COVID with GPL adaptation
- NDCG@10 improved from 0.2670 to 0.3680 (+37.83%) for FIQA with GPL adaptation
- Domain-adapted models showed increased attribution to in-domain terms like "hedge," "gold," "corona," and "disease"
- Models placed more emphasis on document titles, particularly for TREC-COVID where titles were prepended to text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing tokens with [PAD] tokens provides a valid baseline for Integrated Gradients attribution in dense retrievers.
- Mechanism: IG computes the path integral of gradients from a baseline (zero-information state) to the actual input. [PAD] tokens satisfy the baseline requirements—they produce a neutral embedding and convey "empty signal." Query attributions are computed by replacing query tokens with [PAD] while preserving document tokens; document attributions use the inverse. The accumulated gradients reveal which tokens increase or decrease query-document similarity.
- Core assumption: [PAD] token embeddings approximate a true neutral baseline for bi-encoder architectures; the dot product similarity function is differentiable and attribution-meaningful.
- Evidence anchors: [abstract] "we introduce a novel baseline that reveals both query and document attributions"; [section 3] "To calculate the query token attributions, we replace the query tokens with the [PAD] tokens and leave the document tokens untouched"
- Break condition: If [PAD] embeddings carry meaningful semantic signal in your model variant (e.g., different pre-training), baseline validity degrades. Verify baseline scores approach zero similarity.

### Mechanism 2
- Claim: Domain adaptation via Generative Pseudo Labeling (GPL) shifts model attention toward domain-specific terminology and document structure signals (e.g., titles).
- Mechanism: GPL generates synthetic queries from target-domain documents using a T5 model, then labels query-document pairs with cross-encoder relevance scores. The dense retriever is fine-tuned on these pseudo-labeled pairs, learning to associate domain vocabulary with higher similarity scores. The paper's attribution analysis shows domain-adapted models assign higher positive attributions to in-domain terms ("corona," "hedge," "gold") and to document titles.
- Core assumption: GPL-generated pseudo-labels accurately reflect target-domain relevance patterns; attribution shifts correlate with genuine behavioral changes rather than overfitting artifacts.
- Evidence anchors: [abstract] "domain-adapted models focus more on in-domain terminology compared to non-adapted models"; [section 4.2] "the domain-adapted model assigns more importance to the first sentence, which is the title of the paper"
- Break condition: If target domain has low-quality synthetic query generation (e.g., highly technical domains where T5 queries are incoherent), attribution patterns may reflect noise rather than meaningful adaptation.

### Mechanism 3
- Claim: Aggregating token attributions across top-k retrieved documents produces ranking-level explanations that reveal systematic model behavior.
- Mechanism: For a given query, the top-25 documents are retrieved. Token attributions are summed across all retrieved documents—tokens appearing frequently in high-ranked documents with positive attribution receive higher aggregate scores. Word cloud visualizations make these patterns interpretable, showing which terms the model systematically rewards or penalates across the ranking.
- Core assumption: Summed attribution is a meaningful aggregation; top-25 documents are representative of model ranking logic; token frequency correlates with ranking influence.
- Evidence anchors: [section 3] "We aggregate the token attributions over the top retrieved documents by summing them up to generate the overall attributions of a token"; [section 4.1] Figures 1b, 1c show word clouds with "corona," "disease" as positive and "complications" (query term) as negative attribution
- Break condition: If your retrieval set size differs substantially (e.g., top-100 or top-5), aggregation patterns may shift. Highly redundant documents may overweight repeated tokens.

## Foundational Learning

- Concept: Dense Retrieval (Bi-Encoder Architecture)
  - Why needed here: The paper analyzes bi-encoders that independently embed queries and documents via dot product similarity. Understanding this separation is essential to grasp why attribution must be computed separately for query and document sides.
  - Quick check question: Can you explain why bi-encoders are faster but potentially less precise than cross-encoders for retrieval?

- Concept: Integrated Gradients (Gradient-Based Attribution)
  - Why needed here: IG is the core interpretability method. You must understand that IG satisfies sensitivity and implementation invariance axioms by integrating gradients along a path from baseline to input—not just computing gradients at the final point.
  - Quick check question: Why does IG require a baseline input, and what properties should that baseline have?

- Concept: Domain Adaptation for Neural IR
  - Why needed here: The paper analyzes how GPL adaptation changes model behavior. Understanding pseudo-labeling and knowledge distillation from cross-encoders explains why adapted models shift attention to domain vocabulary.
  - Quick check question: What is the difference between supervised domain adaptation and unsupervised methods like GPL?

## Architecture Onboarding

- Component map: Dense Retriever (DistilBERT) -> GPL Domain Adapter (T5 + cross-encoder) -> IG Attribution Engine -> Ranking Aggregator -> Visualization Layer

- Critical path: 1. Load pre-trained dense retriever (e.g., `GPL/msmarco-distilbert-margin-mse`) 2. Optionally apply GPL adaptation to target domain corpus 3. For a query-document pair, compute IG with [PAD] baseline separately for query and document 4. For ranking analysis, retrieve top-25, aggregate attributions per token, visualize

- Design tradeoffs:
  - [PAD] baseline vs. other baselines (zero embedding, random noise): [PAD] is domain-appropriate but may not be optimal for all architectures
  - Top-25 aggregation vs. full corpus analysis: Top-25 is computationally tractable but may miss long-tail patterns
  - Qualitative vs. quantitative evaluation: Paper acknowledges lack of standard quantitative attribution metrics; relies on qualitative inspection

- Failure signatures:
  - Attributions all near zero: Baseline may not be properly neutral; check [PAD] embedding values
  - Domain adaptation shows no attribution shift: GPL synthetic queries may be low-quality; inspect generated queries
  - Negative attributions on query-relevant terms: Possible overfitting or cross-encoder distillation artifacts
  - Title attribution unexpected for non-TREC-COVID data: Title focus is TREC-COVID-specific (titles prepended); verify your data preprocessing

- First 3 experiments:
  1. Reproduce IG attribution on a single query-document pair from TREC-COVID using the provided GPL model; verify query tokens ["co," "##vid"] and document tokens ["corona," "disease"] receive positive attribution as reported.
  2. Compare attribution patterns between baseline (`GPL/msmarco-distilbert-margin-mse`) and domain-adapted (`GPL/trec-covid-msmarco-distilbert-gpl`) models on the same query; quantify attribution shift for domain terms.
  3. Test a different baseline (e.g., zero vector or [MASK] tokens) and compare attribution patterns; assess whether [PAD] baseline is optimal for your model variant.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Integrated Gradients (IG) method compare to model-agnostic interpretability methods like LIME or SHAP in the context of dense retrieval?
- Basis in paper: [explicit] The authors state in the Limitations section: "We also plan to compare the IG method to other interpretability methods, such as [8] or [12], in future research."
- Why unresolved: The current study establishes the viability of IG but does not benchmark it against other popular explanation frameworks, leaving its relative efficacy unknown.
- What evidence would resolve it: A comparative analysis evaluating the fidelity and consistency of attributions generated by IG versus LIME/SHAP on the same domain-adapted models.

### Open Question 2
- Question: Do attribution patterns differ significantly between high-ranked relevant documents and lower-ranked or non-relevant documents?
- Basis in paper: [explicit] The authors note: "We aim to extend this our research beyond the top ranked documents, as understanding the attributions for lower-ranked documents is crucial..."
- Why unresolved: The current analysis is constrained to the top 25 documents, limiting the understanding of model behavior on retrieval failures or tail results.
- What evidence would resolve it: A study extending the attribution analysis to documents ranked lower in the retrieval list (e.g., rank 100+) to identify distinguishing features of false positives.

### Open Question 3
- Question: How can the quality and faithfulness of attributions be evaluated quantitatively rather than qualitatively?
- Basis in paper: [inferred] The paper relies on visual inspection, stating: "As there is no standard quantitative way of evaluating attributions produced by IG, we opted for a deep qualitative analysis of a small sample of queries."
- Why unresolved: Without a quantitative metric, it is difficult to objectively measure the reliability of the explanation or compare it against other methods at scale.
- What evidence would resolve it: The development or adoption of a metric (e.g., perturbation-based fidelity scores) that correlates attribution importance with changes in retrieval scores.

## Limitations

- The paper lacks standard quantitative metrics for evaluating attribution quality, relying entirely on qualitative inspection and visual analysis
- The [PAD] baseline's neutrality is assumed but not empirically validated across different bi-encoder architectures or pre-training schemes
- Attribution analysis is limited to top-25 retrieved documents, potentially missing systematic patterns in lower-ranked or non-relevant results

## Confidence

**High Confidence**: NDCG@10 improvements (0.6510→0.7160 for TREC-COVID, +9.98%; 0.2670→0.3680 for FIQA, +37.83%) are directly measurable from retrieval results and align with GPL adaptation's established effectiveness.

**Medium Confidence**: Attribution pattern shifts (domain terms receiving higher positive attributions, increased title importance) are plausible given the GPL mechanism but lack quantitative validation. The qualitative observations are reasonable but not rigorously proven.

**Low Confidence**: The [PAD] baseline is truly neutral for all bi-encoder architectures, and top-25 aggregation produces representative attribution patterns. These assumptions appear reasonable but are not empirically validated.

## Next Checks

1. **Baseline Validation**: Test alternative baselines (zero vectors, [MASK] tokens, random embeddings) on the same models and queries. Compare attribution patterns to assess whether [PAD] baseline produces systematically different or more interpretable results than other options.

2. **GPL Quality Assessment**: Inspect 50-100 synthetic queries generated by T5 for each domain. Manually evaluate their coherence and relevance to the source documents. Correlate attribution shifts with synthetic query quality scores to determine if attribution changes reflect genuine domain learning or noise.

3. **Cross-Model Attribution Comparison**: Apply the same IG attribution analysis to a non-GPL domain adaptation method (e.g., supervised fine-tuning on labeled target data or BM25-based pseudo-labeling). Compare attribution patterns to GPL results to isolate whether observed shifts are specific to GPL or general domain adaptation effects.