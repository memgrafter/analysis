---
ver: rpa2
title: 'DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large
  language Model'
arxiv_id: '2601.09100'
source_url: https://arxiv.org/abs/2601.09100
tags:
- scheduling
- dynamic
- machine
- thinking
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DScheLLM, a large language model-based framework
  for dynamic job shop scheduling that integrates a dual-system (fast-slow) reasoning
  architecture. It fine-tunes a pretrained LLM using LoRA to generate both rapid,
  interpretable scheduling adjustments (fast-thinking) and structured, solver-compatible
  problem descriptions for complex disruptions (slow-thinking).
---

# DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model

## Quick Facts
- **arXiv ID**: 2601.09100
- **Source URL**: https://arxiv.org/abs/2601.09100
- **Reference count**: 38
- **Primary result**: 46.67% optimality and 73.33% feasibility for fast-thinking mode in dynamic job shop scheduling

## Executive Summary
This paper introduces DScheLLM, a large language model-based framework for dynamic job shop scheduling that integrates a dual-system (fast-slow) reasoning architecture. It fine-tunes a pretrained LLM using LoRA to generate both rapid, interpretable scheduling adjustments (fast-thinking) and structured, solver-compatible problem descriptions for complex disruptions (slow-thinking). Experimental results on the FT06 benchmark show that the fast-thinking mode produces feasible schedules in 73.33% of cases, with 46.67% being optimal, while the slow-thinking mode consistently outputs well-formatted, solver-ready representations across multi-event disturbances. This work demonstrates the potential of fine-tuned LLMs to enable adaptive, generalizable, and interpretable dynamic scheduling in manufacturing environments.

## Method Summary
DScheLLM fine-tunes a 7B-parameter LLM (Huawei OpenPangu Embedded-7B) with LoRA (rank 16, α=32) to perform dynamic job shop scheduling. The framework employs a dual-system approach: fast-thinking mode directly generates updated schedules for single-event disruptions, while slow-thinking mode outputs structured JSP representations for complex scenarios requiring solver processing. Training uses 10,000 instances per mode with masked cross-entropy loss, Adam optimizer, and FP16 mixed precision on 8 Ascend NPUs. The model is evaluated on FT06 benchmark with automatic mode selection, achieving 46.67% optimality for fast-thinking and 100% solver-compatibility for slow-thinking outputs.

## Key Results
- Fast-thinking mode: 73.33% feasible schedules, 46.67% optimal
- Slow-thinking mode: 100% solver-compatible structured outputs
- Automatic mode selection: 33.33% correct classification of complex disruptions
- Makespan minimization performance on FT06 benchmark

## Why This Works (Mechanism)
The dual-system architecture enables efficient handling of different disruption complexities. Fast-thinking leverages the LLM's pattern recognition to rapidly adjust schedules for minor disruptions, while slow-thinking provides structured problem descriptions that can be solved by specialized optimization solvers. The LoRA fine-tuning preserves the base model's capabilities while adapting to scheduling-specific patterns, and the training data generation ensures diverse scenarios covering both minor and major disruptions.

## Foundational Learning
- **Dual-system reasoning**: Fast-slow thinking modes for different disruption severities - needed to balance speed and accuracy in dynamic environments
- **LoRA fine-tuning**: Low-rank adaptation for efficient LLM customization - needed to preserve base capabilities while adding scheduling knowledge
- **Dynamic job shop scheduling**: Real-time rescheduling with machine constraints - needed for manufacturing adaptability
- **Structured output formatting**: Standardized JSP representation for solver compatibility - needed to bridge LLM reasoning and optimization solvers
- **Masked cross-entropy loss**: Selective token-level training - needed to focus learning on relevant scheduling decisions
- **Multi-event disruption modeling**: Complex scenario generation - needed to train slow-thinking mode

## Architecture Onboarding

**Component Map**: Judger -> Mode Selector -> LLM -> Fast Output OR Processor -> Solver

**Critical Path**: Input disruption → Judger classification → Mode selection → LLM reasoning → Output generation → Validation

**Design Tradeoffs**: Fast mode prioritizes speed (73.33% feasibility) over accuracy, while slow mode ensures solver compatibility (100%) at computational cost

**Failure Signatures**: Fast mode produces 26.67% infeasible schedules; automatic mode misclassifies 66.67% of complex disruptions

**First Experiments**:
1. Test Judger module thresholds on validation data to optimize classification accuracy
2. Validate slow-thinking output format consistency across diverse disruption scenarios
3. Analyze infeasibility patterns in fast-thinking outputs to identify constraint violation trends

## Open Questions the Paper Calls Out
None

## Limitations
- 26.67% infeasibility rate in fast-thinking mode suggests incomplete constraint handling
- 66.67% misclassification rate in automatic mode selection limits practical deployment
- Proprietary OpenPangu model availability unknown, requiring substitution for reproduction

## Confidence

**High**: Dual-system architecture implementation, training methodology and parameters, evaluation metrics and benchmark setup

**Medium**: Core algorithmic components (Judger, Processor), generalization across different LLMs

**Low**: Exact decision boundary for Judger classification, Processor algorithm details, OpenPangu model availability

## Next Checks

1. Test Judger Module Thresholds: Implement multiple classification criteria and systematically evaluate how different thresholds affect mode selection accuracy and schedule quality on validation sets

2. Validate Slow-Thinking Output Format: Create a comprehensive test suite to verify that all slow-thinking outputs are consistently formatted for OR-Tools input, checking for proper syntax, complete constraint representation, and solver compatibility

3. Analyze Infeasibility Patterns: For fast-thinking mode failures, conduct detailed analysis of constraint violations to identify whether certain disruption types or specific constraints are more frequently violated