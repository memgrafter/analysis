---
ver: rpa2
title: 'AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents'
arxiv_id: '2510.08511'
source_url: https://arxiv.org/abs/2510.08511
tags:
- automlgen
- tasks
- performance
- search
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMLGen is a coding agent that combines a curated ML knowledge
  base with Monte Carlo Graph Search (MCGS) to autonomously generate and optimize
  end-to-end ML pipelines. It addresses the limitations of LLMs in specialized ML
  domains and the knowledge isolation in tree-based search by enabling cross-branch
  trajectory reuse and multi-branch fusion.
---

# AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents

## Quick Facts
- arXiv ID: 2510.08511
- Source URL: https://arxiv.org/abs/2510.08511
- Reference count: 37
- AutoMLGen achieves 36.4% average medal rate and 18.7% gold medal rate on MLE-Bench under 12-hour budget

## Executive Summary
AutoMLGen is a coding agent that autonomously generates and optimizes end-to-end ML pipelines by combining a curated ML knowledge base with Monte Carlo Graph Search (MCGS). The system addresses limitations of LLMs in specialized ML domains and the knowledge isolation in tree-based search by enabling cross-branch trajectory reuse and multi-branch fusion. MCGS extends MCTS with graph edges for better exploration, while fine-grained operators improve execution stability. Evaluated on MLE-Bench, AutoMLGen sets a new state-of-the-art with 36.4% average medal rate and 18.7% gold medal rate under a 12-hour budget.

## Method Summary
AutoMLGen integrates a curated ML knowledge base with Monte Carlo Graph Search to navigate the complex search space of ML pipeline optimization. The knowledge base provides structured ML expertise while MCGS enables efficient exploration through graph-based trajectory sharing and fusion across search branches. The system uses fine-grained operators to improve execution stability and incorporates cross-branch trajectory reuse to overcome the knowledge isolation problem common in tree-based search methods. This combination allows the agent to generate and optimize complete ML pipelines autonomously while maintaining search efficiency and solution quality.

## Key Results
- Achieves 36.4% average medal rate on MLE-Bench evaluation suite
- Secures 18.7% gold medal rate under 12-hour time budget constraint
- Outperforms all baseline methods, establishing new state-of-the-art performance

## Why This Works (Mechanism)
The system works by combining structured domain knowledge with advanced search algorithms to overcome the limitations of both pure LLM-based approaches and traditional tree search methods. The curated knowledge base provides reliable ML expertise that LLMs often lack, while MCGS enables efficient exploration of the search space through graph-based trajectory sharing. Cross-branch trajectory reuse allows the system to leverage successful search paths across different branches, improving overall search efficiency. Fine-grained operators ensure stable execution of ML pipeline components, while multi-branch fusion combines insights from different search trajectories to generate more robust solutions.

## Foundational Learning

**Monte Carlo Graph Search (MCGS)**
- Why needed: Extends traditional MCTS to enable graph-based trajectory sharing across search branches, improving exploration efficiency
- Quick check: Verify that graph edges are properly maintained and that trajectory reuse actually improves search coverage compared to standard MCTS

**Cross-branch Trajectory Reuse**
- Why needed: Addresses knowledge isolation in tree-based search by allowing successful search paths to inform multiple branches simultaneously
- Quick check: Confirm that shared trajectories improve solution quality across different pipeline configurations

**Curated ML Knowledge Base**
- Why needed: Provides structured, reliable ML expertise that overcomes the hallucination and inconsistency problems of LLMs in specialized domains
- Quick check: Validate that knowledge base coverage is sufficient for diverse ML pipeline components and that retrieval accuracy remains high

**Fine-grained Operators**
- Why needed: Break down complex ML operations into smaller, more stable execution units to reduce failure rates during pipeline construction
- Quick check: Measure failure rate reduction when using fine-grained operators versus coarse-grained alternatives

## Architecture Onboarding

**Component Map**
Knowledge Base -> MCGS Engine -> Pipeline Generator -> Execution Monitor -> Performance Evaluator -> Feedback Loop

**Critical Path**
Knowledge Base → MCGS Engine → Pipeline Generator → Execution Monitor → Performance Evaluator

**Design Tradeoffs**
The system trades computational overhead for improved search efficiency through graph-based trajectory sharing. While the curated knowledge base requires upfront investment in knowledge engineering, it provides more reliable guidance than LLMs alone. Fine-grained operators increase execution stability at the cost of additional computational steps.

**Failure Signatures**
Pipeline generation failures typically indicate knowledge base gaps or MCGS configuration issues. Execution failures suggest problems with fine-grained operators or component compatibility. Poor performance metrics indicate inadequate trajectory reuse or suboptimal knowledge base retrieval.

**3 First Experiments**
1. Test knowledge base retrieval accuracy on a sample of common ML pipeline components
2. Verify MCGS trajectory sharing works by comparing search coverage with and without graph edges
3. Measure execution stability improvement when using fine-grained operators versus coarse-grained alternatives

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on MLE-Bench, which may not capture real-world ML deployment scenarios
- 12-hour time budget constraint could artificially favor search-based methods over potentially more robust but slower approaches
- Claims of "outperforming all baselines" are limited to specific evaluation suite without verification on alternative benchmarks

## Confidence

**High Confidence**: AutoMLGen's architecture design (knowledge base + MCGS) is technically sound and well-documented; the methodological innovations in cross-branch trajectory reuse and multi-branch fusion are clearly specified and reproducible.

**Medium Confidence**: Performance metrics on MLE-Bench are reliable within the evaluation framework, but the 36.4% average medal rate and 18.7% gold medal rate should be interpreted with caution given the narrow scope of evaluation environments and potential benchmark-specific optimizations.

**Low Confidence**: Claims about setting a "new state-of-the-art" are premature without comparison on additional benchmark suites or real-world deployment scenarios beyond MLE-Bench.

## Next Checks
1. Replicate the evaluation on alternative ML benchmark suites (e.g., OpenML, AutoML.org benchmarks) to verify generalization beyond MLE-Bench
2. Conduct ablation studies removing the curated knowledge base to quantify its contribution versus the search algorithm alone, testing whether performance degrades significantly
3. Perform runtime efficiency analysis comparing computational overhead of fine-grained operators against their stability benefits, including wall-clock time measurements across different hardware configurations