---
ver: rpa2
title: 'Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and
  Streaming-Aware Prefix SFT'
arxiv_id: '2509.23381'
source_url: https://arxiv.org/abs/2509.23381
tags:
- guard
- prefix
- evaluation
- streaming
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Guard Vector is a task-vector composition method that transfers
  safety behaviors from a guardrail model to a target language model without additional
  training or target language labels. It computes the parameter difference between
  a guardrail model and a same-architecture pretrained model, then composes this vector
  with a continual pretraining model in the target language to obtain a Target Guard
  Model (TGM).
---

# Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT

## Quick Facts
- arXiv ID: 2509.23381
- Source URL: https://arxiv.org/abs/2509.23381
- Reference count: 40
- Creates Target Guard Model (TGM) that improves safety classification across Chinese, Japanese, and Korean without additional training or target language labels

## Executive Summary
Guard Vector introduces a novel task-vector composition method that transfers safety behaviors from English guardrail models to target language models without requiring additional training or target language labels. The approach computes parameter differences between guardrail models and their corresponding base models, then composes this vector with continual pretraining models in target languages to create Target Guard Models (TGMs). The method demonstrates improved classification quality over established guardrails across standard safety suites and enables language extensibility to Chinese, Japanese, and Korean.

## Method Summary
The Guard Vector method operates through parameter space manipulation rather than traditional fine-tuning. It computes the difference vector between a pretrained model and its guardrail counterpart, then applies this vector to target language models through composition with their respective continual pretraining checkpoints. This enables safety behavior transfer without requiring safety-labeled data in the target language. The approach is complemented by streaming-aware prefix SFT, which addresses the gap between prefix and full-text evaluation by aligning behavior across both contexts through prefix-specific training while maintaining efficiency through single-token classification.

## Key Results
- TGM improves classification quality over established guardrails across Chinese, Japanese, and Korean safety benchmarks
- Streaming-aware prefix SFT preserves classification quality under streaming conditions with parity to offline evaluation
- Method achieves latency reduction through single-token classification while maintaining safety performance

## Why This Works (Mechanism)
The method leverages parameter space proximity between different language models of the same architecture, enabling transfer of safety behaviors through vector composition rather than data-intensive fine-tuning. By computing the difference between guardrail and base models, it captures the safety transformation as a parametric operation that can be applied to any compatible model. The streaming-aware component addresses evaluation misalignment by ensuring consistent safety responses between partial and complete inputs, critical for real-world deployment where decisions must be made incrementally.

## Foundational Learning
- Parameter space manipulation: Why needed - Enables safety transfer without labeled target language data; Quick check - Verify that composed models retain core language capabilities
- Task-vector composition: Why needed - Provides mathematical framework for behavior transfer; Quick check - Confirm vector differences capture safety transformation
- Streaming-aware training: Why needed - Aligns safety responses between prefix and full-text evaluation; Quick check - Test consistency across different input lengths

## Architecture Onboarding

Component Map:
Pretrained Base Model -> Guardrail Model -> Parameter Difference Vector -> Target Language CPT Model -> Target Guard Model (TGM)

Critical Path:
Parameter difference computation → Vector composition with CPT → Streaming-aware prefix SFT → TGM deployment

Design Tradeoffs:
- Computationally efficient parameter transfer vs. potential loss of language-specific safety nuance
- Single-token classification for latency vs. potential reduction in contextual safety understanding
- Streaming consistency vs. increased training complexity for prefix alignment

Failure Signatures:
- Inconsistent safety responses between prefix and full-text inputs
- Degradation in target language capabilities after vector composition
- Suboptimal performance on edge cases requiring cultural or linguistic nuance

First 3 Experiments:
1. Validate parameter difference computation by comparing base and guardrail model outputs on safety prompts
2. Test TGM composition by evaluating target language safety classification before and after vector application
3. Assess streaming performance by comparing prefix vs. full-text safety responses across various input lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may be constrained to languages with architectural similarities to English
- Potential inconsistencies in safety responses across streaming contexts
- Dependence on availability of suitable guardrail and base model pairs for each target language
- Possible compromise of contextual understanding for safety decisions in complex multilingual scenarios

## Confidence
- High confidence in mathematical formulation and implementation of task-vector composition
- Medium confidence in generalizability of safety behavior transfer across diverse languages
- Medium confidence in effectiveness of streaming-aware prefix SFT in maintaining classification quality
- Medium confidence in latency reduction claims due to streaming optimizations
- Low confidence in method's robustness to languages with significantly different linguistic properties from English

## Next Checks
1. Test Guard Vector method on low-resource languages with minimal pretraining data to assess scalability beyond Chinese, Japanese, and Korean
2. Conduct systematic evaluation of streaming performance across different input lengths and content types to identify potential inconsistencies in safety responses
3. Perform ablation studies to quantify individual contributions of task-vector composition versus streaming-aware prefix SFT to overall safety performance improvements