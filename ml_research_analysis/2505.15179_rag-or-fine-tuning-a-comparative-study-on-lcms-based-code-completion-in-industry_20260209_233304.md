---
ver: rpa2
title: RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry
arxiv_id: '2505.15179'
source_url: https://arxiv.org/abs/2505.15179
tags:
- code
- fine-tuning
- completion
- retrieval
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the effectiveness of Retrieval-Augmented
  Generation (RAG) versus fine-tuning for adapting Large Code Models (LCMs) to industrial
  code completion tasks. Using a proprietary C++ codebase of over 160,000 files from
  Tencent''s WXG department, the researchers compare six LCMs across three dimensions:
  effectiveness, efficiency, and parameter sensitivity.'
---

# RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry

## Quick Facts
- arXiv ID: 2505.15179
- Source URL: https://arxiv.org/abs/2505.15179
- Reference count: 40
- Key finding: RAG with BM25 retrieval outperforms fine-tuning and other RAG methods in industrial code completion, achieving 116.9% improvement in exact match over base models

## Executive Summary
This study investigates the effectiveness of Retrieval-Augmented Generation (RAG) versus fine-tuning for adapting Large Code Models (LCMs) to industrial code completion tasks. Using a proprietary C++ codebase of over 160,000 files from Tencent's WXG department, the researchers compare six LCMs across three dimensions: effectiveness, efficiency, and parameter sensitivity. The findings demonstrate that RAG with BM25 retrieval achieves the highest performance, with the combination of RAG and fine-tuning yielding synergistic effects across all evaluation metrics.

## Method Summary
The researchers conducted a comprehensive comparative study using a proprietary C++ codebase containing over 160,000 files from Tencent's WXG department. They evaluated six different Large Code Models using three adaptation approaches: RAG with various retrieval methods (BM25, vector-based, hybrid), fine-tuning, and combined RAG+fine-tuning. The study measured effectiveness using exact match metrics, efficiency through throughput analysis, and parameter sensitivity across different model configurations. Experiments were conducted on industrial code completion tasks with systematic evaluation of scalability as codebase size varied.

## Key Results
- RAG with BM25 retrieval achieved the highest performance, outperforming fine-tuning and other RAG methods by significant margins (116.9% improvement in exact match over base models)
- The combination of RAG and fine-tuning produced synergistic effects, yielding further gains across all evaluation metrics
- Efficiency analysis revealed that fine-tuning requires substantial computational resources during preparation, while RAG incurs runtime overhead that reduces throughput by up to 78%
- RAG demonstrated better scalability, maintaining performance gains as codebase size increased, whereas fine-tuning showed diminishing returns beyond 90,000 files

## Why This Works (Mechanism)
The study reveals that RAG's superior performance stems from its ability to leverage relevant code context through retrieval mechanisms, while fine-tuning adapts model parameters to domain-specific patterns. The combination approach works synergistically because RAG provides contextual retrieval capabilities while fine-tuning optimizes the model's understanding of domain-specific code structures and patterns.

## Foundational Learning
- Retrieval-augmented generation: why needed - to provide relevant context for code completion; quick check - verify retrieval quality and relevance
- Fine-tuning adaptation: why needed - to optimize model parameters for specific code patterns; quick check - measure performance improvement over base models
- BM25 retrieval method: why needed - effective keyword-based retrieval for code; quick check - compare against vector-based retrieval methods
- Parameter sensitivity analysis: why needed - to understand model behavior under different configurations; quick check - test across range of parameter values
- Scalability assessment: why needed - to evaluate performance as codebase grows; quick check - measure performance across different codebase sizes
- Exact match metrics: why needed - to quantify completion accuracy; quick check - verify metric alignment with practical requirements

## Architecture Onboarding

Component map: Codebase -> Retrieval Engine -> LCM -> Evaluation Metrics

Critical path: Code retrieval → Context assembly → Code completion generation → Result evaluation

Design tradeoffs: RAG prioritizes runtime context relevance over computational efficiency, while fine-tuning optimizes for speed but requires extensive upfront training resources. The hybrid approach balances these tradeoffs but adds implementation complexity.

Failure signatures: Poor retrieval quality leads to irrelevant context, fine-tuning overfitting causes domain-specific brittleness, and combined approaches may suffer from conflicting optimization objectives.

First experiments: 1) Baseline performance comparison across all six LCMs, 2) Retrieval quality assessment using different retrieval methods, 3) Scalability testing with varying codebase sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation constrained to C++ code from single proprietary codebase at Tencent, limiting generalizability to other languages and contexts
- Absence of publicly available datasets and code restricts independent verification of results
- Focus on exact match metrics may overlook other important dimensions like semantic correctness or runtime behavior
- Efficiency analysis examines throughput but doesn't account for full lifecycle costs including storage, maintenance, and human evaluation overhead

## Confidence
- Comparative performance results: High confidence for findings within studied context
- Efficiency trade-offs: Medium confidence due to limited cost modeling
- Scalability conclusions: Low confidence given single-company codebase and lack of multi-organizational validation

## Next Checks
1) Replicate the study across diverse codebases including open-source repositories and multiple programming languages to assess generalizability
2) Conduct ablation studies isolating contributions of retrieval quality, model architecture, and prompt engineering to understand sources of RAG's performance advantage
3) Perform longitudinal evaluation tracking model performance over time as codebases evolve, including analysis of adaptation costs and maintenance requirements beyond initial deployment