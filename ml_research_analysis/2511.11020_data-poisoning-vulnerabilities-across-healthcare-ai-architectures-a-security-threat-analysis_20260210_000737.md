---
ver: rpa2
title: 'Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security
  Threat Analysis'
arxiv_id: '2511.11020'
source_url: https://arxiv.org/abs/2511.11020
tags:
- data
- healthcare
- poisoning
- clinical
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Healthcare AI systems face critical vulnerabilities to data poisoning
  attacks that current defenses and regulatory frameworks cannot adequately address.
  The study analyzed eight attack scenarios across four categories, demonstrating
  that adversaries require only 100-500 poisoned samples to compromise healthcare
  AI systems, achieving over 60% success rates with detection taking 6-12 months or
  sometimes never occurring.
---

# Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis

## Quick Facts
- arXiv ID: 2511.11020
- Source URL: https://arxiv.org/abs/2511.11020
- Reference count: 0
- Current healthcare AI systems face critical data poisoning vulnerabilities that existing defenses cannot adequately address

## Executive Summary
Healthcare AI systems face critical vulnerabilities to data poisoning attacks that current defenses and regulatory frameworks cannot adequately address. The study analyzed eight attack scenarios across four categories, demonstrating that adversaries require only 100-500 poisoned samples to compromise healthcare AI systems, achieving over 60% success rates with detection taking 6-12 months or sometimes never occurring. Healthcare's distributed infrastructure creates multiple entry points where insiders with routine access can execute attacks with minimal technical skill. Legal protections like HIPAA and GDPR unintentionally shield attackers by restricting the analyses needed for detection. Supply chain vulnerabilities enable a single compromised vendor to poison models across 50-200 institutions simultaneously.

## Method Summary
The research synthesizes 41 security studies (2019-2025) to analyze data poisoning vulnerabilities across eight attack scenarios in four categories. The analysis examines healthcare AI architectures including CNNs, LLMs, Vision Transformers, and RL agents, focusing on attack feasibility, required poisoned sample counts (100-500), success rates (>60%), and detection timelines (6-12 months or never). The study employs backdoor embedding techniques through poisoned training samples, with gradient accumulation over 3-5 epochs providing 750-1,250 backdoor exposures. Attack success depends on absolute sample count rather than poisoning rate relative to dataset size.

## Key Results
- Adversaries require only 100-500 poisoned samples to compromise healthcare AI systems, regardless of dataset size
- Attack success rates exceed 60% with detection times of 6-12 months or sometimes never occurring
- Federated learning amplifies poisoning risks by obscuring attribution and enabling model update manipulation
- Privacy regulations (HIPAA/GDPR) unintentionally shield attackers by prohibiting cross-patient pattern analysis needed for detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Data poisoning efficacy depends on the absolute number of poisoned samples (100–500), not the poisoning rate relative to dataset size.
- **Mechanism:** Gradient accumulation during repeated training epochs amplifies the backdoor signal. Even 250 poisoned samples provide 750–1,250 exposures over 3–5 epochs, embedding malicious behavior regardless of the volume of clean data.
- **Core assumption:** Standard neural network training dynamics (gradient descent) apply consistently across healthcare architectures (CNNs, Transformers).
- **Evidence anchors:**
  - [abstract] "adversaries require only 100-500 poisoned samples... regardless of dataset size"
  - [section] "Attack Feasibility Across Healthcare AI Architectures" (Page 12-13) details gradient exposure logic.
  - [corpus] Confirmed by "A Systematic Review of Poisoning Attacks Against Large Language Models" (FMR 0.61), validating small-sample efficacy in LLMs.
- **Break condition:** If training utilizes robust differential privacy noise injection or strict sample count capping per epoch, the gradient signal may fall below the embedding threshold.

### Mechanism 2
- **Claim:** Federated learning (FL) amplifies poisoning risks by obscuring attribution and enabling model update manipulation.
- **Mechanism:** A single malicious node submits poisoned model updates (rather than raw data). Privacy protocols prevent inspecting local data, and PEFT methods (like LoRA) concentrate attacks in low-rank matrices, allowing backdoors to slip past Byzantine-robust aggregation.
- **Core assumption:** Aggregation algorithms cannot distinguish sophisticated, low-magnitude malicious updates from legitimate institutional variations.
- **Evidence anchors:**
  - [abstract] "federated learning can worsen risks by obscuring attribution"
  - [section] "Federated Learning as Risk Amplifier" (Page 15) explains PEFT concentration.
  - [corpus] "Byzantine-Robust Federated Learning Framework..." (FMR 0.50) supports the difficulty of robust aggregation in critical infrastructure.
- **Break condition:** If secure aggregation is paired with rigorous, zero-knowledge proof verification of local training runs, the attack surface shrinks.

### Mechanism 3
- **Claim:** Privacy regulations (HIPAA/GDPR) inadvertently shield attackers by prohibiting the cross-patient pattern analysis needed to detect coordinated poisoning (e.g., Sybil attacks).
- **Mechanism:** Attackers inject data through legitimate clinical workflows (fake patient visits). Detection requires analyzing visit patterns across individuals, but privacy laws restrict this without prior consent or cause, creating a "legal catch-22."
- **Core assumption:** Healthcare institutions prioritize legal compliance over speculative security auditing, creating a blind spot in data provenance verification.
- **Evidence anchors:**
  - [abstract] "Legal protections... unintentionally shield attackers by restricting the analyses needed for detection."
  - [section] "Medical Scribe Sybil Attack" (Page 9) details the conflict between anti-discrimination laws and pattern analysis.
  - [corpus] Weak/missing specific legal analysis in neighbors; inference relies primarily on the paper's regulatory assessment.
- **Break condition:** If new "security research" exemptions are added to privacy frameworks, or if synthetic identity verification is decoupled from patient PII, the shield weakens.

## Foundational Learning

- **Concept:** Gradient Accumulation & Epoch Exposure
  - **Why needed here:** To understand why dataset size is not a defense; learners must grasp that "small" is absolute in poisoning, not relative.
  - **Quick check question:** If you double the dataset size from 1M to 2M images but keep 300 poisoned samples, does the attack success rate drop significantly? (Answer: No, per the paper.)

- **Concept:** Byzantine-Robust Aggregation
  - **Why needed here:** Federated learning is central to healthcare AI scaling; learners must know why standard "outlier removal" fails against sophisticated model poisoning.
  - **Quick check question:** Why might a "Trimmed Mean" aggregation fail to catch a malicious update in a network of 50 hospitals?

- **Concept:** Ensemble Disagreement (MEDLEY)
  - **Why needed here:** To shift from "consensus seeking" to "diversity preserving" as a detection strategy for backdoors.
  - **Quick check question:** In a MEDLEY setup, does a high disagreement score indicate a model failure or a potential security alert?

## Architecture Onboarding

- **Component map:** PACS/EHR -> Centralized or Federated Aggregation -> Foundation Model + PEFT/LoRA -> Clinical Decision Support -> MEDLEY Ensemble + Outcome Audits
- **Critical path:** The data ingestion pipeline from clinical workflows (PACS/Scribe) to the model training interface is the primary attack surface. It requires the highest security scrutiny.
- **Design tradeoffs:**
  - **Privacy vs. Security:** Strict HIPAA/GDPR compliance limits cross-patient anomaly detection.
  - **Efficiency vs. Robustness:** PEFT (LoRA) allows cheap fine-tuning but creates a concentrated attack surface for backdoors.
  - **Interpretability vs. Performance:** Black-box models offer higher accuracy but prevent verifiable safety guarantees.
- **Failure signatures:**
  - Sudden demographic-specific divergence in model accuracy (e.g., pneumonia detection fails only for specific ethnicities).
  - High disagreement in ensemble outputs (MEDLEY) on seemingly routine cases.
  - "Optimization bias" where resource allocation (scheduling/triage) systematically favors specific groups without code changes.
- **First 3 experiments:**
  1. **Sample Threshold Test:** Inject 100, 250, and 500 poisoned samples into a validation dataset to measure backdoor success rates against a baseline CNN.
  2. **MEDLEY Integration:** Run a backdoored input through a diverse ensemble (GPT-4, domain-specific model, historical version) to calibrate disagreement thresholds.
  3. **Sybil Simulation:** Create a synthetic "coordinated visit" pattern in a staging environment to test if cross-patient anomaly detection triggers or is blocked by data governance rules.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis synthesizes existing literature but lacks original empirical validation across the claimed 8 attack scenarios
- Legal analysis lacks detailed case law or regulatory text analysis to substantiate claims about HIPAA/GDPR creating detection barriers
- Supply chain vulnerability claims rely heavily on inference from generic software supply chain research rather than healthcare-specific vendor ecosystem analysis

## Confidence

- **High Confidence:** The fundamental vulnerability of healthcare AI to data poisoning attacks, the effectiveness of small-scale poisoning (100-500 samples), and the inadequacy of current detection mechanisms
- **Medium Confidence:** The federated learning amplification mechanism and PEFT concentration effects, requiring empirical validation in healthcare contexts
- **Medium Confidence:** The legal catch-22 created by privacy regulations, conceptually sound but lacking specific regulatory analysis
- **Low Confidence:** The supply chain attack scaling estimates (50-200 institutions), appearing speculative without vendor-specific ecosystem analysis

## Next Checks

1. **Empirical Reproduction:** Implement one specific poisoning attack scenario (e.g., medical scribe Sybil attack) in a controlled healthcare AI testbed using actual medical imaging or clinical text data to verify the claimed success rates and detection timelines.

2. **Legal Framework Analysis:** Conduct a detailed legal review mapping specific HIPAA and GDPR provisions to concrete barriers against cross-patient pattern analysis, identifying potential security research exemptions or modifications.

3. **Supply Chain Ecosystem Mapping:** Analyze actual healthcare AI vendor relationships and software dependencies to quantify the realistic scope of supply chain attacks, verifying whether 50-200 institution compromise is achievable through single-vendor compromise.