---
ver: rpa2
title: A margin-based replacement for cross-entropy loss
arxiv_id: '2501.12191'
source_url: https://arxiv.org/abs/2501.12191
tags:
- loss
- learning
- performance
- training
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes high error margin (HEM) loss as a general-purpose
  replacement for cross-entropy (CE) loss in deep neural network classification. The
  key idea is to modify multi-class margin loss by focusing on reducing only the largest
  errors during training, rather than averaging all errors.
---

# A margin-based replacement for cross-entropy loss

## Quick Facts
- arXiv ID: 2501.12191
- Source URL: https://arxiv.org/abs/2501.12191
- Reference count: 10
- High error margin (HEM) loss outperforms cross-entropy (CE) loss in unknown class rejection, adversarial robustness, imbalanced data, continual learning, and semantic segmentation tasks while maintaining comparable clean accuracy.

## Executive Summary
This paper proposes high error margin (HEM) loss as a general-purpose replacement for cross-entropy (CE) loss in deep neural network classification. The key innovation is modifying multi-class margin loss by focusing on reducing only the largest errors during training, rather than averaging all errors. This addresses CE loss's issues with overconfidence and catastrophic forgetting. Extensive experiments on 18+ architectures and 8 datasets show HEM loss significantly outperforms CE on robustness metrics while maintaining comparable clean accuracy.

## Method Summary
HEM loss computes per-class margin errors (max(0, yi - yt + μ)), thresholds errors below the mean to zero, then averages only the non-zero errors. This differs from standard margin loss (MM) which averages all errors. For HEM+, class-specific margins are computed as μi = √(M / (n × si)) where si is samples in class i and M=2000. The mean calculation must be detached from the computational graph to avoid incorrect gradient flow. Training uses standard hyperparameters (SGD, momentum=0.9, weight decay=5e-4, batch size=128) with 110 epochs and learning rate decay at epochs 100 and 105.

## Key Results
- HEM outperforms CE on unknown class rejection (AUROC scores improve by 10-15%) while maintaining comparable clean accuracy (~94% on CIFAR10)
- HEM shows significant improvements in continual learning (outperforms CE in 11/16 conditions) and imbalanced data scenarios
- HEM+ further improves performance on imbalanced data and semantic segmentation tasks compared to standard HEM
- The method is robust across 18+ architectures and multiple datasets (CIFAR, TinyImageNet, ImageNet, ADE20K)

## Why This Works (Mechanism)

### Mechanism 1
HEM prevents over-confident predictions on unseen data by stopping weight updates once samples are classified with sufficient margin. Unlike CE loss which has non-zero gradient even for well-classified samples, HEM produces zero loss when the correct-class logit exceeds all other logits by margin μ. This creates saturation behavior similar to LogitNorm.

### Mechanism 2
HEM's error aggregation strategy maintains effective gradients during late-stage training, overcoming MM loss's premature convergence. MM loss averages all errors including zeros from satisfied margins, causing the mean to approach zero before all samples are correct. HEM thresholds errors below the mean to zero, then averages only above-zero values, keeping the loss magnitude meaningful even with few remaining errors.

### Mechanism 3
Margin-based saturation reduces catastrophic forgetting in continual learning and minority-class suppression in imbalanced data. CE continues updating weights for majority-class samples even after they're learned, over-writing representations needed for minority classes or previous tasks. HEM produces zero gradient for well-classified samples, preserving previously learned weights.

## Foundational Learning

- **Concept: Margin-based classification (hinge loss family)**
  - Why needed here: HEM is a variant of multi-class margin loss. Understanding that the margin μ defines a "safety zone" around the decision boundary is essential for interpreting hyperparameter choices.
  - Quick check question: For logits [2.0, 0.5, 0.3] with correct class 1 and margin μ=1, what is the margin error for each incorrect class? (Answer: max(0, 0.5-2.0+1)=0, max(0, 0.3-2.0+1)=0; all zeros, sample is "safe")

- **Concept: Error aggregation in multi-class losses**
  - Why needed here: The key innovation is *how* per-class errors combine. Understanding mean vs. sum vs. HEM's thresholded-mean clarifies why MM fails and HEM succeeds.
  - Quick check question: With errors [0, 0, 0.5, 0.3] for 4 classes, what does MM loss (mean) vs. HEM (mean of above-zero) produce? (MM: 0.2; HEM: 0.4)

- **Concept: Confidence scoring for OOD detection**
  - Why needed here: HEM's benefits are evaluated via MSP (Maximum Softmax Probability) and MLS (Maximum Logit Score). Understanding that lower/wider confidence distributions improve separation is key.
  - Quick check question: Why might a network with uniformly high MSP (e.g., 0.99) on all inputs be worse for OOD detection than one with MSP ranging from 0.5–0.9? (Answer: No separation between known and unknown class confidence scores)

## Architecture Onboarding

- **Component map:**
  Input → Backbone (ResNet/ViT/etc.) → Logits y ∈ R^n → HEM Loss → Per-class error: ei = max(0, yi - yl + μi) → Threshold: zero out ei < mean(e) → Aggregate: mean of non-zero ei

- **Critical path:**
  1. Implement Eq. 6 (per-logit error) with detach-safe mean computation
  2. Apply Eq. 8 thresholding—ensure mean computation is `.detach()` from autograd
  3. For HEM+, scale margins per-class using training set statistics before training begins

- **Design tradeoffs:**
  - **Margin μ value**: Larger margins improve unknown-class rejection but may slightly reduce clean accuracy (Fig. 10b). Paper uses √(M / Σsi) with M=2000.
  - **HEM vs HEM+**: Use HEM+ only when class imbalance exists; otherwise identical to HEM.
  - **Confidence score for OOD**: MSP works well with HEM; MLS is alternative when margin is large.

- **Failure signatures:**
  - Training loss doesn't decrease early: Check if mean thresholding is incorrectly applied (should only affect which errors contribute, not their values)
  - Poor OOD detection despite HEM: Verify margin isn't too large (causing high-confidence logits that compress MSP range)
  - Class imbalance performance worse than expected: Confirm HEM+ margins are computed per-class, not global

- **First 3 experiments:**
  1. **Sanity check on CIFAR10/ResNet18**: Train with HEM (μ=0.2) vs CE using identical hyperparameters. Expect ~1% lower clean accuracy, ~10-15% higher AUROC on OOD detection.
  2. **Ablation on error aggregation**: Compare HEM vs "+maz only" vs "+thres only" vs MM (Table 3 replication). Verify both components contribute.
  3. **Margin sensitivity sweep**: Test μ ∈ {0.1, 0.2, 0.5, 1.0, 2.0} on CIFAR10. Plot clean accuracy and OOD AUROC to confirm paper's claim of robustness to margin choice for accuracy but sensitivity for OOD (Fig. 10).

## Open Questions the Paper Calls Out

1. Can the performance of HEM+ on imbalanced datasets be improved by learning the margin hyper-parameters rather than using a fixed heuristic?
   - The current implementation uses a fixed formula for margins based on sample counts, which does not outperform specialized losses on all metrics for imbalanced data.

2. Does HEM loss generalize effectively to non-image classification tasks, such as natural language processing or audio recognition?
   - The paper's extensive evaluation covers only image datasets and pixel-level image segmentation, leaving other data modalities untested.

3. Can the property of HEM loss reaching zero for well-classified samples be exploited to reduce training time or improve generalization?
   - While the authors observe that HEM produces zero loss for many samples late in training, they did not implement or test mechanisms to skip these samples.

## Limitations

- Architecture Coverage: While experiments span 18+ architectures, Transformer-based models (ViT, Swin) use only a subset of datasets, limiting conclusions about HEM's performance on vision transformers in all proposed domains.
- Real-World Deployment: Unknown class rejection experiments use synthetic "unseen" classes rather than truly open-world scenarios where unknown classes are encountered during training.
- Computational Overhead: The paper does not report training/inference time comparisons with CE loss, which could be relevant for large-scale deployments.

## Confidence

**High Confidence**: HEM's comparable clean accuracy to CE across multiple architectures (supported by Tables 1, 3, 6, 7 showing consistent performance within 1-2% of CE).

**Medium Confidence**: HEM's superiority in unknown class rejection and continual learning (strong empirical support from AUROC scores and Fig. 7, but mechanisms rely on specific confidence scoring assumptions).

**Medium Confidence**: HEM's robustness to hyperparameter choice (Fig. 10 shows accuracy stability across margins, but OOD performance sensitivity is less extensively characterized).

## Next Checks

1. **Time Complexity Validation**: Measure and compare training time per epoch between HEM and CE across different batch sizes and architectures to verify the claimed negligible overhead.

2. **Cross-Domain Generalization**: Test HEM on a truly open-world dataset where unknown classes appear during training (e.g., incremental learning with both known and unknown classes), rather than synthetic unknown detection.

3. **Gradient Behavior Analysis**: Instrument the training process to visualize the gradient magnitudes and distributions throughout training for both HEM and CE, specifically examining the claim about maintaining gradient pressure on hard examples.