---
ver: rpa2
title: Scaling Efficient LLMs
arxiv_id: '2402.14746'
source_url: https://arxiv.org/abs/2402.14746
tags:
- training
- transformer
- recurrent
- number
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes recurrent transformers, combining the efficacy
  of transformers with the efficiency of recurrent networks, to scale efficient LLMs.
  The motivation is that recent LLMs have hundreds of billions of parameters consuming
  vast resources, and the so-called "AI scaling law" for transformers suggests that
  the number of parameters must scale linearly with the length of the data.
---

# Scaling Efficient LLMs

## Quick Facts
- arXiv ID: 2402.14746
- Source URL: https://arxiv.org/abs/2402.14746
- Authors: B. N. Kausik
- Reference count: 7
- Primary result: Single-layer recurrent transformers match multi-layer transformer performance with fewer parameters and lower computational cost

## Executive Summary
This paper introduces recurrent transformers, a novel architecture that combines transformer attention within fixed-width blocks with recurrent state accumulation across blocks. The key innovation is a single learnable scalar α that controls whether the model forgets history (for language tasks) or accumulates it (for long-range tasks). Theoretical analysis derives a natural AI scaling law showing parameter requirements scale sub-linearly with training data length (D^γ, γ∈[0.44,0.72]), challenging conventional transformer scaling assumptions. Experiments demonstrate that recurrent transformers achieve comparable accuracy to standard transformers on benchmark tasks while using significantly fewer parameters and lower computational cost.

## Method Summary
The recurrent transformer processes input sequences in fixed-width blocks (typically 32 tokens) using a single transformer layer. Each block's attention is limited to its local window plus accumulated state from previous blocks. The accumulated state h_i is computed using a learnable scalar α that blends current block state with previous accumulated state: (y_i, h_{i+1}) = τ(αh_{i-1} + h_i, αh_{i-1} + x_{i+1}). This architecture achieves linear time complexity while maintaining transformer attention capabilities within blocks. The model uses curriculum training for long-range tasks, starting with shorter sequences and progressively increasing length as validation loss improves.

## Key Results
- Single-layer recurrent transformer matches 4-layer transformer performance on CIFAR-10 image classification
- Recurrent transformers use 6.29M operations/sample versus 138M for 4-layer regular transformers on image tasks
- α converges to ~0.45 for long-range copy tasks (accumulating history) and ~0 for language tasks (forgetting history)
- Theoretical scaling law shows parameter requirements scale as D^γ (γ∈[0.44,0.72]) rather than linear in D

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficient LLM parameters scale sub-linearly with training data length (D^γ, γ∈[0.44,0.72]) rather than linearly as standard transformers suggest.
- Core assumption: PAC learning bounds transfer to practical LLM training and transformers overfit, making the unique-sequence bound from Theorem 1 relevant.
- Evidence: Derived from combining excess risk bounds with empirical KL divergence estimates; supported by scaling law analysis.
- Break condition: If empirical studies show parameter requirements remain linear in D despite architectural innovation, the unique-sequence bound may not be tight.

### Mechanism 2
- Claim: A single scalar α controls history gating, enabling task-adaptive behavior between forgetting and accumulation.
- Core assumption: A scalar suffices for history gating; more complex gating may be unnecessary.
- Evidence: α converges to ~0.45-0.6 for long-range copy tasks and ~0 for language tasks during training.
- Break condition: If tasks require input-dependent gating decisions, a fixed scalar may underperform learned gate matrices.

### Mechanism 3
- Claim: Block-wise recurrent processing achieves linear time complexity while preserving transformer attention within blocks.
- Core assumption: Block size k≈32 captures sufficient local dependencies; long-range dependencies transfer through recurrent state.
- Evidence: Complexity analysis shows O(l) vs O(l²) for standard transformers; empirical operations/sample measurements confirm efficiency.
- Break condition: If tasks require full-sequence attention patterns, block-wise approximation may lose accuracy.

## Foundational Learning

- Concept: Kullback-Leibler Divergence
  - Why needed here: Core to both theoretical derivation and empirical loss function connecting model predictions to corpus statistics.
  - Quick check: If two distributions p and q have KL divergence near zero, what does that imply about their relationship?

- Concept: PAC Learning and Excess Risk
  - Why needed here: Theorem 1 uses PAC framework to bound excess risk in terms of unique sequences, essential for scaling law derivation.
  - Quick check: In PAC learning, what two parameters control the learning guarantee (hint: they appear in probability and error bounds)?

- Concept: Vanishing Gradients in Recurrent Networks
  - Why needed here: Paper addresses vanishing gradients in copy tasks through curriculum training.
  - Quick check: Why do gradients vanish when backpropagating through many time steps in a recurrent network?

## Architecture Onboarding

- Component map: Input -> Block partition -> Transformer layer τ -> Accumulated state h_i -> Scalar α -> Output predictions
- Critical path:
  1. Initialize h_1 = τ(x_1)
  2. For each block i: (y_i, h_{i+1}) = τ(αh_{i-1} + h_i, αh_{i-1} + x_{i+1})
  3. Final output: y_t = τ(h_t)
  4. Backpropagate through all blocks including α
- Design tradeoffs:
  - Block size k: Smaller reduces compute but may lose local context (paper uses k=16-32)
  - Single layer vs stacked: Paper found no benefit to stacking
  - Fixed α vs learned gates: Simpler but less expressive than LSTM-style gating
- Failure signatures:
  - α stuck at 1.0: Model memorizes without generalization
  - α stuck at 0.0 on long-range tasks: Recurrent state not carrying information
  - Validation loss diverging from training loss: Check dropout rate and underparameterization
- First 3 experiments:
  1. Reproduce CIFAR-10 result with k=32, single layer; verify α converges near 0.45
  2. Ablate block size: test k∈{16, 32, 64, 128} on same task
  3. Copy task with curriculum training: start at 128 noise tokens, double until 4096

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do recurrent transformers maintain parameter efficiency and linear time complexity when scaled to billions of parameters on massive datasets?
- Basis: Author states "Owing to budget constraints, we are unable to test larger models"
- Why unresolved: Theoretical scaling law and architectural efficiency proven only on small-scale tasks
- Resolution evidence: Training recurrent transformers with 7B+ parameters on standard large corpora like The Pile

### Open Question 2
- Question: Does the derived scaling exponent (γ ∈ [0.44, 0.72]) hold universally for natural language, or is it artifact of specific sequence length used?
- Basis: Derivation relies on empirical constants from Hoffmann et al. (2022) obtained at fixed sequence length
- Why unresolved: Assumes constants generalize to derive "natural AI scaling law"
- Resolution evidence: Empirical analysis of KL divergence scaling across varying sequence lengths (1k to 100k tokens)

### Open Question 3
- Question: Can a fixed scalar α effectively support tasks requiring simultaneous language modeling and long-range retrieval?
- Basis: α converges to ~0 for language tasks but ~0.45-0.60 for long-range copy tasks
- Why unresolved: Single α suggests optimization for either language modeling or retrieval, potentially struggling to do both
- Resolution evidence: Evaluating on benchmarks requiring both local syntactic processing and long-term dependency retrieval

## Limitations
- Theoretical scaling law derivation depends on assumptions about PAC learning bounds transferring to practical LLM training
- Single scalar α gating mechanism may be insufficient for tasks requiring input-dependent history management
- Experiments limited to small-scale tasks; scalability to billion-parameter models remains untested

## Confidence
- **High Confidence**: Linear computational complexity claims and CIFAR-10 image classification results
- **Medium Confidence**: Scaling law derivation and copy task curriculum training effectiveness
- **Low Confidence**: Universal applicability of single scalar α gating and no benefit from stacked recurrent layers

## Next Checks
1. Replicate unique-sequence bound estimation by training transformers of varying sizes on controlled synthetic corpora with known sequence diversity
2. Evaluate recurrent transformers with scalar α on tasks requiring selective attention to history versus LSTM-style learned gates
3. Systematically vary curriculum training parameters on copy tasks to determine sensitivity and identify optimal configurations