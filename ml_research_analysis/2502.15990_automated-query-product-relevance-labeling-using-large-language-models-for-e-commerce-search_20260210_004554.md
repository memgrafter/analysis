---
ver: rpa2
title: Automated Query-Product Relevance Labeling using Large Language Models for
  E-commerce Search
arxiv_id: '2502.15990'
source_url: https://arxiv.org/abs/2502.15990
tags:
- relevance
- search
- query
- data
- query-product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to automate query-product relevance
  labeling for e-commerce search using Large Language Models (LLMs). The core idea
  is to use prompt engineering techniques such as Chain-of-Thought (CoT) prompting,
  In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum
  Marginal Relevance (MMR) to guide LLMs in generating accurate relevance labels.
---

# Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search

## Quick Facts
- arXiv ID: 2502.15990
- Source URL: https://arxiv.org/abs/2502.15990
- Reference count: 33
- Key outcome: LLM-based query-product relevance labeling achieves accuracy comparable to human-labeled data while significantly reducing time and cost

## Executive Summary
This paper presents a method to automate query-product relevance labeling for e-commerce search using Large Language Models (LLMs). The approach employs prompt engineering techniques including Chain-of-Thought prompting, In-context Learning, and Retrieval Augmented Generation with Maximum Marginal Relevance to guide LLMs in generating accurate relevance labels. Experiments on two open-source datasets (ESCI and WANDS) and a proprietary Walmart Mexico search dataset demonstrate that LLM-based approaches achieve performance approaching human-level labeling with significant efficiency gains.

## Method Summary
The method leverages Large Language Models to automatically generate query-product relevance labels for e-commerce search applications. The approach uses Chain-of-Thought prompting to guide LLMs through step-by-step reasoning, In-context Learning with Few-Shot examples to demonstrate expected labeling patterns, and Retrieval Augmented Generation combined with Maximum Marginal Relevance to enhance contextual understanding. The system was evaluated on three datasets: the open-source ESCI and WANDS datasets for cross-validation, and a proprietary Walmart Mexico search dataset for real-world application testing.

## Key Results
- LLM-based approaches achieve accuracy comparable to human-labeled data while significantly reducing time and cost
- The best-performing configuration using LLM5 with 16 Few-Shot examples and RAG with MMR achieves weighted F1 score approaching human-level performance
- Strong potential demonstrated for scalable and efficient query-product relevance labeling in e-commerce search applications

## Why This Works (Mechanism)
The approach works by leveraging the reasoning and contextual understanding capabilities of LLMs through carefully designed prompt engineering. Chain-of-Thought prompting breaks down the labeling task into logical steps, reducing cognitive load and improving accuracy. In-context Learning provides the model with examples of correct labeling patterns, enabling it to generalize to new query-product pairs. Retrieval Augmented Generation with Maximum Marginal Relevance ensures the model has access to relevant context while avoiding information redundancy, leading to more informed and accurate labeling decisions.

## Foundational Learning
- **Chain-of-Thought Prompting**: Guides LLMs through step-by-step reasoning processes. Why needed: Complex relevance judgments require logical decomposition. Quick check: Test if intermediate reasoning steps improve final label accuracy.
- **In-context Learning**: Uses Few-Shot examples to demonstrate desired output patterns. Why needed: Helps LLMs understand domain-specific labeling conventions. Quick check: Verify performance improves with optimal number of examples.
- **Retrieval Augmented Generation**: Combines external knowledge retrieval with LLM generation. Why needed: Provides additional context for ambiguous query-product pairs. Quick check: Measure performance improvement when retrieval is enabled vs disabled.
- **Maximum Marginal Relevance**: Optimizes retrieved information for diversity and relevance. Why needed: Prevents redundant or overly similar context from biasing decisions. Quick check: Compare diversity metrics of retrieved sets with and without MMR.

## Architecture Onboarding
- **Component Map**: Query + Product → RAG Retrieval → LLM5 with CoT and ICL → Relevance Label
- **Critical Path**: The core workflow involves retrieving relevant context, applying Chain-of-Thought reasoning with In-context Learning examples, and generating final relevance labels through the LLM.
- **Design Tradeoffs**: Balance between retrieval quality and computational cost, optimal number of Few-Shot examples versus prompt length limits, and trade-offs between different LLM model sizes and capabilities.
- **Failure Signatures**: Incorrect labels often occur with highly ambiguous queries, products with multiple interpretations, or when retrieved context is not sufficiently relevant or diverse.
- **First Experiments**: 1) Test baseline performance without RAG to establish retrieval contribution, 2) Vary number of Few-Shot examples to find optimal ICL configuration, 3) Compare different LLM models to identify best-performing architecture.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation constrained to three datasets (ESCI, WANDS, and Walmart Mexico), limiting generalizability across different e-commerce domains
- Proprietary Walmart dataset not publicly available, preventing independent verification of results
- Performance metrics don't reveal whether LLMs struggle with specific query types or product categories
- Computational costs of using large LLMs for labeling at scale not discussed, critical for real-world deployment

## Confidence
- **High confidence**: The technical approach using prompt engineering (CoT, ICL, RAG with MMR) is sound and well-described
- **Medium confidence**: The generalizability of findings across different e-commerce contexts, as evaluation is limited to three datasets
- **Medium confidence**: The cost and time savings claims, as operational details about implementation scale are not provided

## Next Checks
1. **Cross-domain validation**: Test the approach on additional e-commerce datasets from different verticals (fashion, electronics, groceries) to assess generalizability and identify domain-specific limitations
2. **Human quality assessment**: Conduct blind human evaluation comparing LLM-generated labels against original human annotations to verify claimed human-level performance and identify systematic biases
3. **Cost-benefit analysis**: Measure actual computational costs and labeling throughput when scaling the approach to enterprise-level volumes, comparing against traditional crowdsourcing methods including quality control overhead