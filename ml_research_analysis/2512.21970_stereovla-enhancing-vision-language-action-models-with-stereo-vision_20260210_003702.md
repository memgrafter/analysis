---
ver: rpa2
title: 'StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision'
arxiv_id: '2512.21970'
source_url: https://arxiv.org/abs/2512.21970
tags:
- stereo
- arxiv
- depth
- vision
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StereoVLA integrates stereo vision into vision-language-action
  models by extracting geometric features from stereo image pairs using FoundationStereo
  and semantic features from monocular views using SigLIP and DINOv2, then fusing
  them into hybrid visual tokens. It also introduces an auxiliary Interaction-Region
  Depth Estimation task that focuses depth prediction on the gripper-object interaction
  area to enhance spatial perception and accelerate convergence.
---

# StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision

## Quick Facts
- **arXiv ID**: 2512.21970
- **Source URL**: https://arxiv.org/abs/2512.21970
- **Reference count**: 40
- **Primary result**: 33% higher success rate than baselines in robotic manipulation with stereo vision

## Executive Summary
StereoVLA introduces a method to enhance vision-language-action (VLA) models by integrating stereo vision capabilities. The approach combines geometric features extracted from stereo image pairs using FoundationStereo with semantic features from monocular views processed by SigLIP and DINOv2. These features are fused into hybrid visual tokens and integrated into the VLA framework. An auxiliary Interaction-Region Depth Estimation task focuses depth prediction on gripper-object interaction areas, improving spatial perception and accelerating convergence. The method demonstrates significant performance gains in robotic manipulation tasks.

## Method Summary
StereoVLA integrates stereo vision into VLA models through a dual-stream feature extraction pipeline. Geometric features are extracted from stereo image pairs using FoundationStereo, while semantic features are obtained from monocular views using SigLIP and DINOv2. These features are fused into hybrid visual tokens and integrated into the VLA framework. The method introduces an auxiliary Interaction-Region Depth Estimation task that focuses depth prediction on the gripper-object interaction area, enhancing spatial perception and accelerating convergence. This approach is evaluated on robotic manipulation tasks, showing substantial improvements over baseline methods.

## Key Results
- Achieves 33% higher success rate than baselines in robotic manipulation with stereo vision
- Demonstrates strong robustness to camera pose variations
- Ablation studies confirm effectiveness of geometric-semantic feature fusion and focused depth estimation task

## Why This Works (Mechanism)
The integration of stereo vision provides geometric depth information that complements semantic understanding from monocular vision. By fusing geometric features from stereo pairs with semantic features from monocular views, the model gains richer spatial representation. The auxiliary depth estimation task specifically targets the interaction region between gripper and objects, improving precision in manipulation tasks. This focused approach to depth prediction enhances spatial awareness while accelerating model convergence during training.

## Foundational Learning

**FoundationStereo**: Why needed - Extracts geometric depth information from stereo image pairs; Quick check - Verify depth map quality and consistency across different scene depths

**SigLIP**: Why needed - Provides semantic feature extraction from monocular views; Quick check - Validate feature representation quality on downstream vision tasks

**DINOv2**: Why needed - Contributes additional semantic understanding for feature fusion; Quick check - Compare feature representations with other vision backbones

**Hybrid Visual Tokens**: Why needed - Combines geometric and semantic features for comprehensive visual representation; Quick check - Analyze token distribution and information content

## Architecture Onboarding

**Component Map**: Stereo Camera Pair -> FoundationStereo -> Geometric Features; Monocular View -> SigLIP/DINOv2 -> Semantic Features; Fusion Module -> Hybrid Visual Tokens -> VLA Model

**Critical Path**: The fusion of geometric and semantic features represents the critical path, as this integration directly impacts the model's spatial reasoning capabilities. The auxiliary depth estimation task supports this by providing focused training on interaction regions.

**Design Tradeoffs**: The method trades increased computational complexity from dual-stream feature extraction against improved spatial understanding. The focused depth estimation task adds training overhead but potentially accelerates convergence and improves precision.

**Failure Signatures**: Potential failures include misalignment between geometric and semantic feature spaces, degradation in performance when geometric features are noisy or incomplete, and over-reliance on the auxiliary task at the expense of primary task performance.

**First Experiments**: 1) Ablation study removing geometric features to quantify their contribution; 2) Evaluation on tasks with varying spatial complexity to test generalization; 3) Analysis of training dynamics with and without the auxiliary depth estimation task

## Open Questions the Paper Calls Out
None

## Limitations
- Performance generalization beyond specific robotic manipulation benchmarks is untested
- Computational overhead of dual-stream feature extraction relative to real-time deployment constraints is not quantified
- Method's effectiveness on tasks requiring different spatial reasoning or longer-horizon planning is unexplored

## Confidence
- **High**: 33% success rate improvement over baselines in robotic manipulation tasks
- **Medium**: Claims about convergence acceleration from auxiliary depth estimation task
- **Medium**: Robustness to camera pose variations (requires broader testing conditions)

## Next Checks
1. Evaluate StereoVLA on diverse robotic tasks (e.g., assembly, navigation) to test generalization beyond manipulation benchmarks
2. Conduct ablation studies isolating the impact of each component (geometric features, semantic features, depth estimation task) on both performance and training convergence
3. Measure and report the computational overhead (inference latency, memory usage) introduced by the stereo vision pipeline to assess real-time deployment feasibility