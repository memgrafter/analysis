---
ver: rpa2
title: 'NoWag: A Unified Framework for Shape Preserving Compression of Large Language
  Models'
arxiv_id: '2504.14569'
source_url: https://arxiv.org/abs/2504.14569
tags:
- pruning
- nowag-p
- nowag-vq
- quantization
- wanda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NoWag is a unified framework for one-shot shape-preserving compression
  of large language models using normalized weights and activation-guided importance
  scoring. The method applies independent compression to each weight matrix, normalizing
  with row and column vectors to remove outlier structures and enable effective quantization
  and pruning.
---

# NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models

## Quick Facts
- arXiv ID: 2504.14569
- Source URL: https://arxiv.org/abs/2504.14569
- Reference count: 40
- NoWag is a unified framework for one-shot shape-preserving compression of large language models using normalized weights and activation-guided importance scoring

## Executive Summary
NoWag introduces a unified framework for one-shot shape-preserving compression of large language models using normalized weights and activation-guided importance scoring. The method applies independent compression to each weight matrix, normalizing with row and column vectors to remove outlier structures and enable effective quantization and pruning. Applied to Llama-2 (7B, 13B, 70B) and Llama-3 (8B, 70B) models, NoWag-VQ achieves lower perplexity than state-of-the-art one-shot vector quantization methods while using 48× less calibration data, and NoWag-P matches or outperforms leading pruning techniques. The approach highlights commonalities between pruning and quantization, suggesting new research directions.

## Method Summary
NoWag is a one-shot compression framework that normalizes each weight matrix using column norms r^(1) followed by row norms r^(2), creating a bounded "ball-shaped" distribution. For quantization (NoWag-VQ), it applies weighted K-means clustering on d-dimensional subvectors using activation second moments as weights. For pruning (NoWag-P), it uses activation-weighted scores to determine which weights to remove. At inference, denormalization is applied using the stored r^(1) and r^(2) vectors. The framework achieves state-of-the-art results with significantly reduced calibration data requirements compared to existing methods.

## Key Results
- NoWag-VQ achieves lower perplexity than QuIP# and BERT-VQ at 2 bits with 48× less calibration data
- NoWag-P matches or outperforms leading pruning techniques like Wanda and SparseGPT across multiple sparsity levels
- The unified normalization approach enables both quantization and pruning to benefit from the same preprocessing
- Lower perplexity on Llama-3 models demonstrates effectiveness on newer architecture variants

## Why This Works (Mechanism)

### Mechanism 1: Normalization Dissolves Outlier Structure in Weights
Bidirectional normalization (row + column) removes structured outlier patterns that cause compression to disproportionately damage specific channels. Weight matrices in LLMs exhibit row/column outliers from attention heads, rotary embeddings, and feature activations. Normalizing by column norms r^(1) then row norms r^(2) redistributes magnitude, yielding a bounded "ball-shaped" distribution better suited for VQ codebooks and uniform pruning.

### Mechanism 2: Diagonal Hessian Approximation via Activation Second Moments
Using diag(XX^T) as importance weights approximates full Hessian-based objectives sufficiently for one-shot compression while avoiding O(d³) matrix inversion. Instead of solving coupled subproblems requiring feedback updates, NoWag uses per-channel activation magnitude ||X_j||²₂ as scalar weights. This decomposes the objective into element-wise independent problems.

### Mechanism 3: Weighted K-Means Exploits Normalized Distribution Geometry
After normalization, weighted K-means clustering on d-dimensional subvectors achieves lower reconstruction error than incoherence-processing methods with 48× less calibration data. Normalization constrains weights to ~[0,1] ball; K-means with activation-weighted distances finds codebook shaped to input-sensitive regions.

## Foundational Learning

- **Vector Quantization (VQ)**: Core technique; groups of d consecutive weights share a codebook entry. Understanding codebook size vs. bits-per-value tradeoff (|C| = 2^{bpv·d}) essential. Quick check: If d=6 and target bpv=2, what codebook size fits in L1 cache?

- **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**: NoWag is one-shot PTQ—no backward passes, only forward calibration. Distinguishes it from methods requiring fine-tuning. Quick check: Why does one-shot PTQ require Hessian approximation while QAT doesn't?

- **Unstructured vs. N:M Semi-Structured Pruning**: NoWag-P handles both; hardware supports 2:4 but not arbitrary unstructured sparsity. Understanding this constraint explains performance gaps. Quick check: Why does 2:4 semi-structured pruning show smaller NoWag vs. Wanda gaps than 50% unstructured?

## Architecture Onboarding

- **Component map**: [Calibration Data] → [Forward Pass] → [Activation Collection X] → [Weight Matrix W] → [NoWag Normalization] → [NoWag-VQ Pipeline or NoWag-P Pipeline] → [Quantized Ŵ or Pruned Ŵ] → [Inference with Denormalization]

- **Critical path**: Normalization (Algorithm 1) → importance score computation → compression algorithm selection → denormalization at inference. The normalization vectors r^(1), r^(2) must be stored and applied at inference; overhead is <0.01 bpv.

- **Design tradeoffs**: d (subvector dimension): Larger d → better codebook expressiveness but larger codebook (must fit L1). Paper uses d=6 for A6000, d=7 for H100. Calibration samples: 128 vs. QuIP#'s 6144 (48× reduction). K-means iterations: Paper uses 100; ablation shows T=1 already beats QuIP# for some models.

- **Failure signatures**: High perplexity on Llama-3 8B at 2 bits: Check if d=6 codebook fits cache. Semi-structured pruning matches but doesn't beat Wanda at 2:4: Expected—structured patterns reduce normalization benefit. Zero-shot accuracy variance >0.5%: Intrinsic noise; verify with multiple seeds.

- **First 3 experiments**:
  1. Reproduce Wiki2 perplexity for Llama-2-7B at 2 bits: Run NoWag-VQ with default settings (128 RedPajama samples, d=6, T=100). Expect ~7.07.
  2. Ablate normalization direction: Compare row-only, column-only, row+column for NoWag-P at 50% sparsity on Llama-2-7B. Expect row+column best.
  3. Calibration data sensitivity: Run NoWag-VQ with 32, 64, 128, 256 samples on Llama-3-8B. Plot perplexity vs. samples to find knee point.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the benefit of normalization diminish for more structured pruning patterns (e.g., 2:4 semi-structured), and can this limitation be overcome? The authors observe this trend in Figure 4 but offer only a brief hypothesis about structure constraints overriding normalization benefits.

### Open Question 2
Can NoWag-VQ be effectively combined with Trellis coding (as in QTIP) to further improve compression at extreme bitrates? The authors state this combination is promising but not yet explored, noting that Trellis coding can be extended to any VQ rounding method.

### Open Question 3
What is the convergence behavior of NoWag-VQ's weighted K-means, and does performance continue improving beyond 100 iterations? Resource constraints limited experiments to 100 iterations; the asymptotic performance ceiling remains unknown.

### Open Question 4
Does NoWag generalize to model architectures beyond the Llama family (e.g., GPT-style, Mixture-of-Experts, or non-Transformer architectures)? All experiments are conducted on Llama-2 and Llama-3 models; architecture-specific patterns could affect effectiveness.

## Limitations
- Normalization benefits diminish for more structured pruning patterns (2:4 semi-structured shows smaller gaps vs. baselines)
- Limited validation on non-transformer architectures; all experiments use Llama-family models
- Convergence behavior of weighted K-means beyond 100 iterations remains unexplored

## Confidence

**High confidence**: The normalization mechanism's ability to remove outlier structures and enable effective compression (supported by multiple ablation studies and perplexity improvements).

**Medium confidence**: The diagonal Hessian approximation's sufficiency for activation-guided importance scoring (competitive results vs. Hessian-based methods, but limited theoretical analysis of approximation error).

**Medium confidence**: The 48× calibration data reduction claim (empirically validated but dependent on specific data distribution and model architecture).

## Next Checks

1. **Cross-architecture validation**: Apply NoWag to GPT-style or multimodal models to verify normalization benefits extend beyond Llama-family architectures.

2. **Hessian diagonal approximation error analysis**: Quantify the impact of ignoring off-diagonal terms by comparing to full Hessian-based compression on smaller models where exact computation is feasible.

3. **Normalization sensitivity study**: Systematically vary normalization parameters (epsilon, iteration counts) and evaluate impact on different weight matrix types (attention vs. FFN) to identify potential failure modes.