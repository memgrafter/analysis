---
ver: rpa2
title: 'DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection
  and Synthetic Data'
arxiv_id: '2507.18583'
source_url: https://arxiv.org/abs/2507.18583
tags:
- retrieval
- data
- training
- knowledge
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DR.EHR, a dense retrieval model specifically
  designed for electronic health record (EHR) retrieval. The model addresses the semantic
  gap issue in EHR retrieval by leveraging a two-stage training pipeline.
---

# DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data

## Quick Facts
- arXiv ID: 2507.18583
- Source URL: https://arxiv.org/abs/2507.18583
- Reference count: 27
- Primary result: State-of-the-art performance on CliniQ benchmark with knowledge injection and synthetic data pipeline

## Executive Summary
This paper introduces DR.EHR, a dense retrieval model specifically designed for electronic health record (EHR) retrieval. The model addresses the semantic gap issue in EHR retrieval by leveraging a two-stage training pipeline. The first stage involves knowledge injection from a biomedical knowledge graph, while the second stage uses large language models to generate synthetic training data. DR.EHR achieves state-of-the-art performance on the CliniQ benchmark, significantly outperforming existing dense retrievers.

## Method Summary
DR.EHR employs a two-stage training pipeline to enhance EHR retrieval performance. The first stage involves knowledge injection from a biomedical knowledge graph, where entity embeddings are obtained using Meta's knowledge graph and integrated into the model. The second stage uses large language models to generate synthetic training data, addressing the scarcity of labeled EHR data. The model is trained on this synthetic data to improve its retrieval capabilities for clinical notes.

## Key Results
- DR.EHR 110M variant achieves 92.96 MRR and 89.12 MAP on Single-Patient Retrieval
- DR.EHR 7B variant achieves 93.03 MRR and 88.94 MAP on Single-Patient Retrieval
- Superior performance across various match types and query types, particularly excelling in challenging semantic matches like implication and abbreviation

## Why This Works (Mechanism)
DR.EHR's success stems from its two-stage training pipeline that addresses the semantic gap in EHR retrieval. Knowledge injection from biomedical knowledge graphs provides the model with domain-specific medical understanding, while synthetic data generation expands the training corpus to cover diverse clinical scenarios. This combination enables the model to better understand medical terminology, abbreviations, and semantic relationships in clinical notes, leading to improved retrieval accuracy.

## Foundational Learning
- Dense retrieval in NLP: Why needed - to overcome limitations of sparse retrieval in capturing semantic similarity. Quick check - model learns dense representations for text matching.
- Knowledge graph embeddings: Why needed - to incorporate domain-specific medical knowledge into the model. Quick check - entity embeddings capture relationships between medical concepts.
- Synthetic data generation: Why needed - to address scarcity of labeled EHR data. Quick check - LLM-generated examples improve model generalization.

## Architecture Onboarding

**Component map**: Knowledge Graph -> Embedding Layer -> Dense Retriever -> Synthetic Data Generator -> Training Pipeline

**Critical path**: Knowledge injection → Embedding integration → Synthetic data generation → Model training → Retrieval

**Design tradeoffs**: The model balances between knowledge graph coverage and computational efficiency, using different parameter sizes (110M vs 7B) to optimize for various deployment scenarios.

**Failure signatures**: Potential issues include incomplete knowledge graph coverage, synthetic data quality degradation, and hallucination risks from LLM-generated examples.

**First experiments**:
1. Test retrieval performance on Single-Patient Retrieval task with varying parameter sizes
2. Evaluate performance across different match types (exact, implication, abbreviation)
3. Conduct ablation studies to assess contribution of knowledge injection and synthetic data components

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single benchmark dataset, raising generalizability concerns
- Knowledge graph dependency on Meta's graph may not capture all clinical terminology
- Limited comparison with traditional sparse retrieval methods
- Potential biases and hallucinations from synthetic data generation not thoroughly examined

## Confidence
- High confidence: Experimental methodology and benchmark results are well-documented and reproducible
- Medium confidence: Claims about knowledge injection and synthetic data generation are supported but mechanisms need further investigation
- Medium confidence: Generalizability claims are plausible but not thoroughly validated

## Next Checks
1. External validation on multiple EHR datasets from different healthcare institutions
2. Ablation study on knowledge graph coverage using different medical knowledge sources
3. Clinical utility assessment through user studies with healthcare professionals