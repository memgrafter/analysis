---
ver: rpa2
title: 'Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4):
  Analysis and Variations'
arxiv_id: '2505.06653'
source_url: https://arxiv.org/abs/2505.06653
tags:
- quantization
- weights
- block
- bof4
- normalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses suboptimal quantization errors in block-wise
  LLM quantization methods like NF4 and AF4. The authors propose an optimization approach
  for block-wise quantization, resulting in a family of quantizers called 4-bit block-wise
  optimal float (BOF4).
---

# Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations

## Quick Facts
- **arXiv ID:** 2505.06653
- **Source URL:** https://arxiv.org/abs/2505.06653
- **Reference count:** 40
- **Primary result:** BOF4-S quantizer with OPQ consistently achieves lower quantization errors and better perplexity than baseline methods on multiple LLMs including Llama-3.1/3.2 and Qwen-2.5.

## Executive Summary
This work addresses suboptimal quantization errors in block-wise LLM quantization methods like NF4 and AF4. The authors propose an optimization approach for block-wise quantization, resulting in a family of quantizers called 4-bit block-wise optimal float (BOF4). Key innovations include a mathematical analysis and derivation of an expectation-maximization algorithm for optimal codebook design, a novel signed absolute block maximum normalization method (BOF4-S) that further reduces quantization error, and an outlier-preserving quantization (OPQ) strategy that stores outlier weights in 16-bit precision to handle distributional mismatch.

## Method Summary
The BOF4 approach introduces a weighted centroid update rule for the Expectation-Maximization algorithm that minimizes the Mean Squared Error (MSE) of actual weights rather than normalized weights. The BOF4-S variant uses signed absolute block maximum normalization, which constrains reconstruction levels to both -1 and +1 while allowing asymmetric dynamic range within blocks. The OPQ strategy identifies outliers (weights exceeding a quantile threshold of expected absolute maxima distribution) and stores them in 16-bit precision, replacing them with zeros before block normalization. This combined approach is evaluated on multiple LLMs including Llama-3.1/3.2 and Qwen-2.5, demonstrating improved perplexity and accuracy when combined with QLoRA fine-tuning.

## Key Results
- BOF4-S quantizer optimized for MSE with OPQ consistently achieves lower quantization errors than baseline methods on multiple LLMs
- When combined with QLoRA fine-tuning, BOF4 methods demonstrate improved accuracy on instruction following and code generation tasks
- BOF4-S + OPQ maintains memory efficiency for deployment on consumer-grade hardware while achieving better perplexity scores
- Signed normalization (BOF4-S) reduces quantization error by allowing asymmetric dynamic range within blocks

## Why This Works (Mechanism)

### Mechanism 1: Weighted Centroid Optimization (BOF4)
Standard codebook optimization minimizes the error of normalized weights, but this is suboptimal for the final unnormalized weights. The modified centroid update rule for the Expectation-Maximization (EM) algorithm weights each sample by the squared block maximum, aligning the optimization target with the Mean Squared Error (MSE) of the actual weights. This assumes network weights follow a zero-symmetric distribution.

### Mechanism 2: Signed Absmax Normalization (BOF4-S)
Constraining reconstruction levels to both -1 and +1 wastes codebook capacity because a single block rarely contains weights near both extremes. The signed absolute maximum normalization maps the largest magnitude weight to exactly +1 (or -1) but allows asymmetric dynamic range (e.g., ranging from -0.85 to +1.0), freeing up the reconstruction level at the opposite end to better fit the bulk of the distribution.

### Mechanism 3: Outlier-Preserving Quantization (OPQ)
Extreme outlier weights distort block-wise normalization, causing non-outlier weights to be compressed into a narrow range (underload regime). OPQ identifies outliers (weights exceeding a quantile threshold) and stores them in full 16-bit precision, preventing them from inflating the block maximum and preserving dynamic range for the remaining weights.

## Foundational Learning

- **Lloyd’s Algorithm (k-means quantization):** The paper modifies this classic algorithm to create BOF4. You must understand how standard Lloyd's updates centroids based on minimizing local distortion to grasp why the weighted update is necessary.
  - Quick check: In standard Lloyd’s algorithm, what defines the boundary between two quantization regions (Voronoi cells)?

- **Block-wise Absmax Quantization:** This is the baseline operation (normalizing by $\max(|w|)$) that the paper critiques and improves upon.
  - Quick check: Why does a smaller block size generally reduce quantization error but increase memory overhead?

- **Rate-Distortion Theory:** The paper argues against NF4's "information-theoretic optimality" by pointing out that equal codebook utilization is only optimal for uniform distributions, not Gaussian ones.
  - Quick check: Does a uniform quantizer minimize distortion for a Gaussian source?

## Architecture Onboarding

- **Component map:** Pre-trained weights W → OPQ Filter (identifies outliers → stores in FP16) → Signed Absmax Normalization (computes block scales w_max and normalized inputs x) → BOF4-S Codebook Lookup (maps x to indices) → 4-bit indices + FP16 scales + FP16 outliers

- **Critical path:** The normalization logic (Signed Absmax) is the most fragile integration point; existing libraries expect unsigned scales. Ensure the dequantization step multiplies by the signed scale, not the absolute value.

- **Design tradeoffs:**
  - Block Size (I): Larger blocks save constant memory but increase error (unless OPQ is active)
  - OPQ Threshold (q): Higher q = fewer outliers = lower memory overhead but higher perplexity risk
  - Optimization Target: MSE-optimized codebooks generally yield better perplexity than MAE-optimized ones

- **Failure signatures:**
  - Perplexity Spike: If OPQ is disabled on models with heavy outliers, perplexity degrades significantly at large block sizes
  - Memory Bloat: If q is not tuned, OPQ storage overhead can exceed the savings from 4-bit quantization

- **First 3 experiments:**
  1. Codebook Validation: Implement Eq. (6) and verify generated codebooks match the fixed tables in Appendix C
  2. Normalization Ablation: Run BOF4 vs. BOF4-S on a target model to confirm the error reduction claims
  3. OPQ Threshold Sweep: Vary q on WikiText-2 to find the memory/perplexity knee for your specific hardware constraints

## Open Questions the Paper Calls Out

1. **Double Quantization Impact:** How does BOF4-S compare to baselines when using double quantization, given the required extra bit per block for the sign encoding? The paper's experiments avoid double quantization to isolate the quantizer's performance, leaving the interaction between BOF4-S and double quantization unexplored.

2. **Calibration-Based PTQ Comparison:** Can BOF4-S maintain its efficiency advantage while achieving accuracy comparable to post-training quantization (PTQ) methods that rely on calibration data? The evaluation focuses on data-free techniques and excludes calibration-based PTQ methods.

3. **Optimal Configuration Prediction:** Is there a theoretical criterion to predict the optimal BOF4 variant for a specific downstream task or model architecture? The utility of signed normalization and OPQ appears task-dependent, with no clear rule provided to select the best configuration without empirical testing.

## Limitations

- Codebook optimization generalization may not hold for all model architectures or training regimes that deviate from zero-symmetric weight distributions
- OPQ threshold sensitivity requires careful tuning across different model families and architectures
- Memory overhead complexity from OPQ can negate compression benefits when outliers constitute a significant fraction of weights

## Confidence

**High Confidence:**
- BOF4-S consistently reduces quantization error compared to standard absmax methods
- OPQ effectively handles outliers and prevents underload regimes in block normalization
- Combined BOF4-S + OPQ achieves lower perplexity than NF4 and AF4 baselines on tested models
- QLoRA fine-tuning with BOF4-quantized weights maintains or improves task accuracy

**Medium Confidence:**
- Theoretical optimality of weighted centroid updates for MSE optimization
- Specific codebook values generalize well beyond tested model families
- Computational efficiency gains translate directly to real-world deployment scenarios

**Low Confidence:**
- Long-term stability of BOF4-quantized models through extended inference workloads
- Performance consistency across different hardware accelerators and quantization-aware runtimes
- Scalability to models significantly larger than those tested (beyond 8B parameters)

## Next Checks

1. **Distribution Sensitivity Test:** Evaluate BOF4 performance across models with known non-Gaussian weight distributions to verify the robustness of the symmetric distribution assumption.

2. **Cross-Architecture Threshold Validation:** Systematically vary the OPQ threshold parameter across multiple model architectures to develop guidelines for threshold selection beyond the single value (0.95) provided.

3. **Mixed Precision Overhead Analysis:** Conduct a detailed memory profiling study comparing BOF4 with and without OPQ across different block sizes to quantify the exact memory overhead trade-off and identify the optimal block size for various hardware constraints.