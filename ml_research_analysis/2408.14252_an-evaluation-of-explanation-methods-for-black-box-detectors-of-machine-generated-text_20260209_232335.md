---
ver: rpa2
title: An Evaluation of Explanation Methods for Black-Box Detectors of Machine-Generated
  Text
arxiv_id: '2408.14252'
source_url: https://arxiv.org/abs/2408.14252
tags:
- explanations
- explanation
- detector
- anchor
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates explanation methods for black-box detectors\
  \ of machine-generated text (MGT), addressing the need for interpretable decisions\
  \ in contexts where incorrect classifications have significant consequences. The\
  \ study compares three explanation methods\u2014SHAP, LIME, and Anchor\u2014across\
  \ five automated experiments measuring faithfulness, stability, and a user study\
  \ assessing usefulness."
---

# An Evaluation of Explanation Methods for Black-Box Detectors of Machine-Generated Text

## Quick Facts
- arXiv ID: 2408.14252
- Source URL: https://arxiv.org/abs/2408.14252
- Authors: Loris Schoenegger; Yuxi Xia; Benjamin Roth
- Reference count: 39
- Primary result: SHAP outperforms LIME and Anchor in faithfulness and stability for MGT detection, but LIME is perceived as most useful by users despite decreasing their prediction performance.

## Executive Summary
This paper evaluates explanation methods (SHAP, LIME, Anchor) for black-box detectors of machine-generated text, addressing the need for interpretable decisions in contexts where incorrect classifications have significant consequences. The study compares three explanation methods across five automated experiments measuring faithfulness, stability, and a user study assessing usefulness. Results show SHAP performs best in faithfulness, stability, and user performance, while LIME is perceived as most useful by users but decreases their ability to predict detector behavior. Anchor performs moderately but is less interpretable. The study highlights the importance of evaluating explanation methods for complex tasks and recommends SHAP for detecting MGT, while cautioning against relying solely on user-perceived usefulness.

## Method Summary
The study evaluates three explanation methods (SHAP, LIME, Anchor) across five detectors (Guo et al., 2023; Solaiman et al., 2019; DetectGPT) using the H3 dataset subset. Automated experiments include pointing games, token removal tests, consistency checks, continuity analysis, and contrastivity evaluations. A forward simulation user study with 36 participants assessed perceived usefulness and actual prediction performance. SHAP uses Partition Explainer with deterministic computation for deterministic detectors, LIME uses 1k perturbation samples (500 for DetectGPT), and Anchor uses τ=0.75 with 200 samples per candidate. Evaluation metrics include pointing game accuracy, token removal Δk=10, Krippendorff's α for stability, and user accuracy change.

## Key Results
- SHAP demonstrates superior faithfulness (pointing game accuracy: 0.65) and stability (consistency α: 0.867-1.0) compared to LIME (accuracy: 0.546, α: 0.136) and Anchor (accuracy: 0.583, α: 0.160)
- LIME is perceived as most useful by users (3.60/5 for "why" understanding) but decreases prediction performance (-13.12% accuracy change)
- All explanation methods fail to significantly improve user prediction accuracy (SHAP: +3.07%, p=0.551; Anchor: +0.67%, p=1.000)
- DetectGPT's non-deterministic nature results in lower consistency scores (α: 0.084 for SHAP) compared to deterministic detectors (α: 1.0)

## Why This Works (Mechanism)

### Mechanism 1
SHAP's partition-based Owen value computation produces more faithful and stable explanations than LIME's stochastic surrogate models for MGT detection. SHAP uses a deterministic partition tree algorithm to compute feature importance scores across token combinations, averaging contributions over all possible feature subsets. LIME samples random perturbations and fits a local linear model using different random seeds per run. Core assumption: Feature importance computed as Shapley-style averages better captures true detector behavior than importance derived from local linear approximations to perturbed neighborhoods. Evidence: SHAP performs best in faithfulness, stability, and helping users predict detector behavior; LIME's consistency α=0.136 indicates near-random agreement across runs. Break condition: For non-deterministic detectors, SHAP's consistency advantage diminishes (α drops to 0.084).

### Mechanism 2
Faithfulness in explanation methods can be measured via token removal experiments and pointing games, but these metrics capture different aspects of importance. Pointing games test whether top-attributed tokens originate from document segments matching the predicted class (hybrid documents). Token removal tests whether removing high-importance tokens causes larger prediction changes than random removal. Core assumption: Detectors make decisions based on identifiable segments, and faithful explanations should locate these segments while identifying causally influential features. Evidence: The pointing game and token removal experiment are based on different assumptions about importance; LIME drops slightly faster than SHAP in token removal tests; average accuracy drops below 50% for SHAP at roughly 10 tokens masked. Break condition: When detectors rely on distributed features rather than local token patterns, both metrics may fail to capture true decision mechanisms.

### Mechanism 3
Perceived explanation usefulness inversely correlates with actual prediction performance improvement, creating a dangerous illusion of understanding. LIME provides compact explanations (10 most influential words) that appear intuitive and actionable. SHAP provides importance scores for every token, which is more informationally complete but cognitively overwhelming. Users conflate plausibility with faithfulness, rating LIME highest while performing worst. Core assumption: Users cannot distinguish between explanations that accurately represent model behavior and explanations that simply appear plausible given their task intuitions. Evidence: LIME is perceived as most useful by users but scores worst in user performance; even carefully phrased questionnaires cannot prevent users from conflating understanding of task characteristics with understanding of model behavior. Break condition: If users have domain expertise matching the detector's feature space, perceived and actual usefulness may align better.

## Foundational Learning

- **Concept: Shapley Values and Game-Theoretic Attribution**
  - Why needed here: SHAP's Partition Explainer computes Owen values, requiring understanding of how feature importance is fairly distributed across all possible coalitions.
  - Quick check question: Given three tokens A, B, C, why must we compute contributions for subsets {A}, {B}, {C}, {A,B}, {A,C}, {B,C}, {A,B,C} rather than just individual effects?

- **Concept: Perturbation-Based Explanation and Distributional Shift**
  - Why needed here: All three methods generate explanations by observing detector responses to perturbed inputs. Understanding how perturbation strategy affects explanation quality is critical.
  - Quick check question: If replacing tokens with mask tokens makes text appear more human-like to detectors, how might this bias explanations toward identifying machine-like features?

- **Concept: Human Evaluation of XAI Methods**
  - Why needed here: The paper reveals a fundamental disconnect between automated metrics and human-centered evaluation. Understanding forward simulation design and perceived vs. actual usefulness is essential.
  - Quick check question: Why might measuring user performance at predicting detector behavior be more informative than measuring user satisfaction with explanations?

## Architecture Onboarding

- **Component map:** Input Document → MGT Detector (Guo/Solaiman/DetectGPT) → Prediction → Perturbation Generator (mask/LM-based) → Perturbed Documents → Explanation Method (SHAP/LIME/Anchor) → Feature Importance/Rule → Evaluation layers (Pointing Game, Token Removal, Consistency/Continuity/Contrastivity tests, Forward Simulation user study)

- **Critical path:** 1) Select detector and configure perturbation strategy 2) Generate explanations with runtime-aware parameters 3) Evaluate faithfulness via pointing game and token removal 4) Assess stability across re-runs and perturbations 5) Run user study with paired document selection

- **Design tradeoffs:** Explanation length vs. cognitive load (LIME 10 words vs. SHAP all tokens), determinism vs. exploration (SHAP deterministic for deterministic detectors), runtime vs. quality (DetectGPT ~14x slower), perturbation strategy (random replacement makes text appear human-like vs. LM-based more coherent but introduces artifacts)

- **Failure signatures:** LIME pointing game accuracy ≈ random baseline (0.546), Anchor token removal shows slower accuracy drop than random for correct predictions, all methods show no significant improvement in user prediction accuracy, stability collapse (LIME α=0.136, Anchor α=0.160)

- **First 3 experiments:** 1) Run pointing game with random feature importance vectors to establish floor (~0.56 accuracy) 2) For single document, generate 10 explanations per method across all detectors; compute Krippendorff's α to verify SHAP > LIME ≈ Anchor rankings 3) Select 20 documents with high detector confidence; generate LIME explanations with 500 vs 1000 vs 2000 perturbation samples to test variance reduction

## Open Questions the Paper Calls Out

- For which specific task types and feature complexity levels do feature importance explanation methods provide sufficient insight compared to simpler classification tasks? The authors advocate for evaluation across additional tasks to better understand conditions under which feature importance can be adequate. This remains unresolved as the study focused solely on MGT detection.

- To what extent does explanation length impact user performance and perceived usefulness in forward simulation tasks? The authors hypothesize the disconnect might be partially due to differences in explanation length, stating this needs exploration in future work. The between-subject design with default visualization settings makes isolating the effect of length impossible.

- How do model-specific explanation methods compare to black-box methods like SHAP and LIME in terms of faithfulness and stability for MGT detectors? The limitations section restricts scope to black-box methods, noting model-specific methods or training-data influence explanations are not included. The performance of internal-state methods remains unverified.

## Limitations

- The study focuses exclusively on hybrid document datasets where machine-generated and human-written text are concatenated, which may not represent real-world MGT detection scenarios where content types coexist naturally.
- Human study sample size (N=36) provides limited statistical power for detecting subtle effects, particularly given the modest performance improvements observed.
- The evaluation assumes detectors make decisions based on identifiable segments, but many modern detectors may rely on distributed, subtle patterns that neither pointing games nor token removal can adequately capture.

## Confidence

**High Confidence:**
- SHAP demonstrates superior faithfulness and stability compared to LIME and Anchor based on pointing game accuracy and consistency metrics
- LIME is perceived as most useful by users despite performing worst in predicting detector behavior
- All explanation methods fail to significantly improve user prediction accuracy

**Medium Confidence:**
- The inverse correlation between perceived usefulness and actual prediction performance improvement
- Token removal experiments capture different importance notions than pointing games
- DetectGPT's non-deterministic nature explains its lower consistency scores

**Low Confidence:**
- The specific mechanisms by which SHAP's Shapley-style computation produces more faithful explanations for MGT detection
- The extent to which human study findings generalize to users with varying levels of domain expertise
- The claim that SHAP is definitively "better" for MGT detection may be detector-specific

## Next Checks

1. **Detector Architecture Generalization Test:** Evaluate SHAP, LIME, and Anchor across at least five additional MGT detectors with varying architectures (CNN, LSTM, different transformer variants) to determine whether SHAP's superiority holds across detector types or is specific to RoBERTa-based models.

2. **Real-World Document Validation:** Replace hybrid document datasets with authentic mixed-document collections from real-world sources where human and machine-generated content coexist naturally, then re-run pointing game and token removal experiments to assess explanation faithfulness in practical scenarios.

3. **Expert User Study Replication:** Conduct a follow-up human study with N=50 participants who have formal training in natural language processing or machine learning, using the same forward simulation design to test whether domain expertise moderates the observed disconnect between perceived and actual explanation usefulness.