---
ver: rpa2
title: 'Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity
  Applications'
arxiv_id: '2510.03623'
source_url: https://arxiv.org/abs/2510.03623
tags:
- attack
- explanation
- adversarial
- feature
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates adversarial attacks on post-hoc XAI methods
  in cybersecurity applications. The study evaluates six attack procedures targeting
  SHAP, LIME, and IG explanation methods across four cybersecurity datasets: phishing,
  malware, intrusion detection, and fraudulent websites.'
---

# Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications

## Quick Facts
- arXiv ID: 2510.03623
- Source URL: https://arxiv.org/abs/2510.03623
- Reference count: 40
- This paper investigates adversarial attacks on post-hoc XAI methods in cybersecurity applications, showing fairwashed attacks effectively conceal feature importance while maintaining prediction accuracy.

## Executive Summary
This paper explores the vulnerability of post-hoc XAI methods to adversarial attacks in cybersecurity applications. The study evaluates six attack procedures targeting SHAP, LIME, and IG explanation methods across four cybersecurity datasets. Three attack tactics are examined: fairwashed explanation (FE), manipulated explanation (ME), and backdoor-enabled manipulation (BD). The results demonstrate that FE attacks, particularly the Makrut attack, are highly effective at concealing sensitive feature importance while maintaining prediction accuracy. In contrast, ME attacks showed limited effectiveness with high Spearman correlations between original and adversarial feature ranks. The findings highlight the urgent need for enhanced security in XAI methods to ensure reliable explanations in critical cybersecurity applications.

## Method Summary
The study employs a comprehensive evaluation framework examining six attack procedures against three post-hoc XAI methods (SHAP, LIME, IG) across four cybersecurity datasets. The evaluation includes fairwashed explanation attacks designed to conceal sensitive feature importance, manipulated explanation attacks using data poisoning and black box approaches, and backdoor-enabled manipulation. Attack effectiveness is measured using Spearman correlation between original and adversarial feature rankings, along with classification accuracy maintenance. The study compares attack performance across different XAI methods and cybersecurity domains, with particular focus on how attacks impact feature importance rankings while preserving model predictions.

## Key Results
- Fairwashed explanation attacks (particularly Makrut) effectively concealed sensitive feature importance while maintaining prediction accuracy
- Manipulated explanation attacks showed limited effectiveness with Spearman correlations >0.94 between original and adversarial rankings
- Only two attacks had proposed defenses, highlighting the need for enhanced security mechanisms

## Why This Works (Mechanism)
The effectiveness of fairwashed explanation attacks stems from their ability to modify input data or model behavior in ways that preserve classification accuracy while significantly altering the resulting explanations. These attacks exploit the gap between what models learn and how explanations are generated, allowing attackers to hide the true importance of sensitive features. The manipulated explanation attacks, while showing higher resistance to data poisoning, still demonstrate vulnerability through carefully crafted perturbations that maintain prediction accuracy while changing feature attribution patterns.

## Foundational Learning
- Post-hoc XAI methods (why needed: understand model decisions after training; quick check: verify explanation methods can be independently computed)
- Adversarial machine learning fundamentals (why needed: understand attack methodologies; quick check: confirm attack effectiveness metrics)
- Spearman correlation for feature importance comparison (why needed: measure explanation similarity; quick check: validate correlation calculation)
- Cybersecurity feature importance (why needed: identify critical security features; quick check: confirm feature relevance to security outcomes)
- Model explainability-security tradeoff (why needed: balance transparency with protection; quick check: assess impact on both metrics)

## Architecture Onboarding
Component map: Input Data -> Attack Generation -> XAI Method -> Explanation Output -> Security Analysis
Critical path: Attack generation directly influences explanation output, which determines security assessment validity
Design tradeoffs: Explanation fidelity vs. attack resistance; computational cost vs. security enhancement
Failure signatures: High Spearman correlation indicates attack resistance; low accuracy with altered explanations suggests successful fairwashing
First experiments: 1) Baseline explanation generation without attacks, 2) Attack effectiveness measurement across all methods, 3) Defense mechanism evaluation for successful attacks

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on only three XAI methods (SHAP, LIME, IG) without testing more robust or alternative explanation techniques
- Evaluation across just four cybersecurity datasets may not represent full diversity of real-world applications
- Limited testing of defensive mechanisms, with only two attacks having proposed defenses

## Confidence
High: Effectiveness of fairwashed explanation attacks in concealing feature importance while maintaining prediction accuracy
Medium: Resistance of manipulated explanation attacks to data poisoning, given limited attack strategies tested
Low: Generalizability across different XAI methods, cybersecurity domains, and attack scenarios due to restricted scope

## Next Checks
1. Test additional XAI methods including more robust techniques like Integrated Gradients with SmoothGrad and Shapley Flow to assess if fairwashed attacks remain effective
2. Evaluate attack effectiveness across a broader range of cybersecurity applications including zero-day detection, insider threat detection, and advanced persistent threat (APT) identification
3. Develop and validate comprehensive defense mechanisms against fairwashed attacks, including input preprocessing, model regularization, and explanation verification frameworks