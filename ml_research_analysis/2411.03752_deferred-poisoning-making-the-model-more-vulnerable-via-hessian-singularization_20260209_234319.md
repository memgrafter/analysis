---
ver: rpa2
title: 'Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization'
arxiv_id: '2411.03752'
source_url: https://arxiv.org/abs/2411.03752
tags:
- training
- poisoning
- poisoned
- attacks
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deferred Poisoning Attack (DPA), a novel
  poisoning attack that maintains normal model performance during training and validation
  but significantly increases vulnerability to evasion attacks and natural noise.
  DPA achieves this by ensuring poisoned models have similar loss values to clean
  models while exhibiting large local curvature through Hessian singularization.
---

# Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization

## Quick Facts
- arXiv ID: 2411.03752
- Source URL: https://arxiv.org/abs/2411.03752
- Reference count: 11
- Key outcome: DPA maintains normal accuracy while significantly increasing vulnerability to adversarial attacks through Hessian singularization.

## Executive Summary
This paper introduces Deferred Poisoning Attack (DPA), a novel poisoning attack that maintains normal model performance during training and validation but significantly increases vulnerability to evasion attacks and natural noise. DPA achieves this by ensuring poisoned models have similar loss values to clean models while exhibiting large local curvature through Hessian singularization. The attack is implemented using a Singularization Regularization term that amplifies the Hessian matrix's condition number. Experiments on CIFAR10, SVHN, and TinyImageNet datasets show DPA effectively reduces model robustness while maintaining accuracy, requires lower attack cost, and demonstrates strong transferability across different model architectures.

## Method Summary
DPA employs a two-phase alternating optimization: model training on both clean and poisoned data, and perturbation updates to maximize Hessian curvature. The attack uses Singularization Regularization (tr(H^T H)) to increase local curvature, making models more sensitive to small perturbations. This is achieved through Hessian-Vector Product (HVP) approximation for computational efficiency. The attack maintains stealth by keeping clean accuracy high while reducing robustness to adversarial attacks (FGSM, PGD, DeepFool) and natural noise. DPA is evaluated across multiple datasets (CIFAR10, SVHN, TinyImageNet) and model architectures (VGG16, ResNet18/50, DenseNet121).

## Key Results
- FGSM robustness drops from 1.86% to 0.58% on CIFAR10 while maintaining clean accuracy
- DPA remains effective against data augmentation defenses and pretrained models
- Strong transferability across different model architectures (VGG16 perturbations work on ResNet18/50)
- HVP approximation achieves comparable effectiveness with 1000x speedup (8s → 0.008s per sample)

## Why This Works (Mechanism)

### Mechanism 1
Maximizing the trace of the Hessian matrix increases local curvature, making models more sensitive to perturbations. The paper uses the inequality σ_max(H) ≥ [1/n · tr(H^T H)]^(1/2) to justify that maximizing tr(H^T H) increases the largest singular value of the Hessian. A larger σ_max(H) means the upper bound on loss increase under perturbation is higher, enabling small perturbations to cause large loss spikes. This relies on the assumption that the loss function is locally approximately convex around input samples.

### Mechanism 2
Alternating optimization between model parameters and perturbations allows the poisoned model to converge normally on clean data while accumulating vulnerability. Phase 1 trains the model on both clean and poisoned samples using standard cross-entropy loss, ensuring convergence. Phase 2 updates perturbations to maximize curvature while minimizing loss increase, using fixed model parameters. This alternating approach prevents the two objectives from directly competing, assuming perturbations remain small enough that the model can simultaneously learn to classify both clean and perturbed samples correctly.

### Mechanism 3
Hessian-Vector Product (HVP) approximation maintains attack effectiveness while reducing computational cost from O(P²) to O(P). Instead of computing the full Hessian, HVP uses the tight lower bound ||Hv||²₂ via PyTorch's `torch.autograd.functional.hvp`. This avoids explicit Hessian computation while still maximizing curvature in the direction of random unit vector v. The assumption is that the random vector v provides sufficient signal to increase overall curvature.

## Foundational Learning

- **Concept: Hessian Matrix and Local Curvature**
  - Why needed here: Understanding how the second derivative (curvature) relates to sensitivity to perturbations is essential for grasping why maximizing tr(H^T H) creates vulnerability.
  - Quick check question: If a loss function has Hessian with eigenvalues [0.1, 0.1, 10.0], what does this tell you about sensitivity in different directions?

- **Concept: Poisoning Attacks vs. Evasion Attacks**
  - Why needed here: DPA is a poisoning attack (corrupts training data) that manifests as vulnerability to evasion attacks (adversarial examples at test time). Distinguishing these is crucial.
  - Quick check question: Why does the paper call this "deferred" poisoning rather than a traditional backdoor attack?

- **Concept: Adversarial Robustness Metrics (ρ̂)**
  - Why needed here: The paper evaluates success using robustness metrics from DeepFool, not accuracy alone. Understanding ρ̂ = average(||r̂(x)||_p / ||x||_p) is necessary to interpret results.
  - Quick check question: A lower ρ̂ value indicates higher or lower vulnerability?

## Architecture Onboarding

- **Component map:** Input -> Perturbation Generator -> Model Trainer -> Hessian Regularization -> Output
- **Critical path:**
  1. Initialize model parameters θ and zero perturbations
  2. For each training epoch: (a) train model on clean + poisoned batches, (b) update perturbations for I_δ iterations using fixed θ
  3. Return final perturbation set Δ
  4. Deploy poisoned dataset; victim trains normally; vulnerability emerges at test time

- **Design tradeoffs:**
  - ε budget: Lower ε (e.g., 3/255) increases stealth but may reduce effectiveness
  - I_δ iterations: More iterations improve perturbation quality but increase training time
  - HVP vs. full Hessian: HVP is ~1000x faster but is an approximation
  - Poisoning percentage: 40%+ needed for strong effect; lower ratios reduce attack feasibility

- **Failure signatures:**
  - High clean accuracy loss: If ACC drops significantly, the attack lacks stealth
  - No robustness degradation: If ρ̂ values similar to clean model, perturbation updates may be insufficient
  - Visible perturbations: If residuals visible, ε is too large or optimization diverged
  - Memory overflow: Full Hessian computation requires O(P²) memory; switch to HVP

- **First 3 experiments:**
  1. Reproduce CIFAR10 + VGG16 baseline: Verify clean ACC ≈ 0.75 and ρ̂_F drops from ~1.86 to ~0.58
  2. Ablate HVP vs. full Hessian: Compare both methods on a small subset (1000 samples)
  3. Test transferability: Generate perturbations using VGG16, then train ResNet18 on the poisoned dataset

## Open Questions the Paper Calls Out
- **Question 1:** Can a specific defense strategy be developed that mitigates the vulnerability induced by DPA without significantly compromising the model's clean accuracy?
- **Question 2:** Does the theoretical link between maximizing the Hessian trace and vulnerability hold rigorously in non-convex regions of the loss landscape?
- **Question 3:** What are the theoretical mechanisms that enable the transferability of DPA perturbations across different model architectures?

## Limitations
- Relies on locally convex loss landscapes where Hessian bounds are valid
- Computational efficiency gain from HVP lacks theoretical guarantees about curvature preservation
- Transferability claims assume similar vulnerability surfaces across architectures
- Effectiveness at lower poisoning ratios (<20%) remains uncertain

## Confidence
- **High Confidence:** Core mechanism of Hessian singularization increasing vulnerability; HVP computational advantage; basic transferability results
- **Medium Confidence:** Effectiveness across diverse datasets and models; robustness against defenses
- **Low Confidence:** Claims about stealth in real-world deployment scenarios; long-term stability of poisoned models; scalability to larger, more complex models

## Next Checks
1. **Hessian Bound Validity Test:** Measure actual loss increase vs. perturbation magnitude and compare to theoretical bounds across different loss surfaces
2. **Cross-Architecture Sensitivity:** Test transferability limits on architectures with fundamentally different designs (MobileNet, Vision Transformer)
3. **Detection Robustness Analysis:** Implement standard poisoning detection methods (spectral signatures, activation clustering) on poisoned datasets