---
ver: rpa2
title: 'FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching'
arxiv_id: '2511.13794'
source_url: https://arxiv.org/abs/2511.13794
tags:
- fusion
- image
- tasks
- learning
- fusionfm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency and limited
  generalization of existing multi-modal image fusion methods by proposing FusionFM,
  a flow matching-based approach that directly learns the probabilistic transport
  between source and fused image distributions. The method introduces a two-stage
  pseudo-ground-truth generation strategy using fusion priors from multiple state-of-the-art
  models, refined through a dedicated Fusion Refiner module employing divide-and-conquer
  decomposition.
---

# FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching

## Quick Facts
- **arXiv ID:** 2511.13794
- **Source URL:** https://arxiv.org/abs/2511.13794
- **Reference count:** 40
- **Primary result:** Flow matching-based fusion model achieves competitive performance across IVF, MIF, MEF, and MFF tasks with 200× speedup over diffusion methods

## Executive Summary
FusionFM addresses computational inefficiency and limited generalization in multi-modal image fusion by reformulating the task as probabilistic transport via flow matching. The method introduces a two-stage pseudo-ground-truth generation strategy using fusion priors from multiple SOTA models, refined through a dedicated Fusion Refiner module employing divide-and-conquer decomposition. For multi-task scenarios, FusionFM integrates elastic weight consolidation and experience replay mechanisms to mitigate catastrophic forgetting. The approach achieves competitive performance across diverse fusion tasks while significantly improving sampling efficiency through one-shot inference compared to iterative diffusion methods.

## Method Summary
FusionFM formulates image fusion as learning a probabilistic transport between source and fused image distributions using flow matching. The method employs a two-stage pseudo-ground-truth generation pipeline: first selecting the best candidate from multiple pretrained models using task-specific metrics, then refining through a Fusion Refiner that decomposes the selected pseudo-label into modality-specific components via autoencoders, extracts high-frequency residuals, applies adaptive spatial weighting, and reintegrates with a protection mask. For multi-task fusion, the approach combines elastic weight consolidation (EWC) with experience replay to prevent catastrophic forgetting during sequential task training.

## Key Results
- Achieves mIoU up to 73.75% and mAP up to 0.968 in downstream semantic segmentation and object detection tasks
- Provides over 200× computational efficiency gain compared to diffusion-based alternatives
- Demonstrates superior performance in BWT (backward transfer) metrics when combining EWC and experience replay versus either method alone

## Why This Works (Mechanism)

### Mechanism 1: Direct Source-to-Target Transport via Flow Matching
Flow matching enables faster inference than diffusion models by learning a straight-line probabilistic path from source images to fused outputs, rather than iteratively denoising from random noise. The model defines the flow source as x₀ = x_A + x_B (sum of modality inputs) rather than Gaussian noise. This reduces ODE integration complexity from hundreds of curved steps to a single forward pass.

### Mechanism 2: Pseudo-Ground-Truth Refinement via Divide-and-Conquer
A two-stage pseudo-label generation pipeline produces higher-quality supervision signals than any single SOTA fusion model alone. Stage 1 selects the best candidate from multiple pretrained models using task-specific metrics. Stage 2 (Fusion Refiner) decomposes the selected pseudo-label into modality-specific components via autoencoders, extracts high-frequency residuals, applies adaptive spatial weighting, and reintegrates with a protection mask.

### Mechanism 3: Dual-Strategy Continual Learning for Cross-Task Retention
Combining parameter-level regularization (EWC) with data-level rehearsal (experience replay) mitigates catastrophic forgetting better than either strategy alone. EWC computes Fisher Information Matrix diagonals to identify parameters important for previous tasks, while experience replay maintains a representative memory buffer from each completed task.

## Foundational Learning

- **Concept: Flow Matching (Continuous Normalizing Flows)**
  - **Why needed:** FusionFM reformulates image fusion as learning a time-dependent vector field that transports probability distributions
  - **Quick check:** Can you explain why a straight-line path between source and target distributions requires fewer integration steps than a curved noise-to-image trajectory?

- **Concept: Optimal Transport Theory**
  - **Why needed:** The paper frames source-to-fusion mapping as an optimal transport problem
  - **Quick check:** What does the Wasserstein distance measure in the context of image distributions, and why does lower distance suggest easier learning?

- **Concept: Continual Learning / Catastrophic Forgetting**
  - **Why needed:** Multi-task fusion requires sequential training across diverse datasets without losing prior capabilities
  - **Quick check:** Why does simply fine-tuning on new tasks cause catastrophic forgetting, and how does Fisher Information Matrix regularization prevent this?

## Architecture Onboarding

- **Component map:** [Input: x_A, x_B] → [Flow Source: x₀ = x_A + x_B] → [U-Net v_θ(t, x_t; x_A, x_B)] → [Vector Field Output] → [ODE Solver] → [Fused Output: x₁]

- **Critical path:**
  1. Data preparation: Generate pseudo-ground-truths using two-stage pipeline before training
  2. Flow matching training: Learn v_θ by minimizing ||v_θ - (x₁ - x₀)||² on interpolated samples
  3. Inference: Single ODE step from x₀ = x_A + x_B to x₁ (fused image)
  4. Multi-task extension: Apply EWC regularization and replay buffer during sequential task training

- **Design tradeoffs:**
  - Memory vs. Quality: Larger replay buffer improves retention but increases memory footprint
  - Model Pool Diversity vs. Cost: More SOTA models improve pseudo-label quality but require pre-running all models
  - Refinement Strength vs. Artifact Risk: Higher α recovers more detail but risks over-sharpening

- **Failure signatures:**
  - Blurry outputs: DU decomposition fails to separate modality components
  - Color distortion: Task-specific weights over-emphasize texture at expense of exposure balance
  - Forgetting previous tasks: EWC λ too low or replay buffer undersampled
  - Slow inference: Accidentally using multi-step ODE solver

- **First 3 experiments:**
  1. Ablation on coupling method: Train with Gaussian noise source vs. average coupling
  2. Single-task validation: Train and evaluate on IVF-MSRS only
  3. Pseudo-truth quality analysis: Visualize pre-refinement vs. post-refinement outputs

## Open Questions the Paper Calls Out

### Open Question 1
Can the task-specific metric weights for pseudo-label selection be learned or optimized dynamically rather than relying on manual heuristics? The current reliance on manual design limits scalability and adaptability to new fusion tasks.

### Open Question 2
Is element-wise addition the optimal source coupling strategy compared to alternatives like channel concatenation or learnable feature aggregation? Concatenation preserves distinct modality information in channel dimensions.

### Open Question 3
Is the fixed threshold c in the Fusion Refiner's protection mask robust across diverse sensor noise levels, or does it require dataset-specific tuning? Medical and infrared images possess vastly different noise profiles.

## Limitations
- Core efficiency claims depend on linear transport assumption that may not hold across all fusion scenarios
- Pseudo-ground-truth generation pipeline may propagate systematic biases from SOTA models
- Multi-task continual learning relies on relatively small memory buffers without extensive ablation on buffer size effects

## Confidence

- **High:** Flow matching outperforms diffusion in sampling efficiency (runtime measurements are direct)
- **Medium:** Pseudo-ground-truth quality improves FusionFM performance (ablations support but lack independent ground truth comparison)
- **Low:** EWC + experience replay prevents catastrophic forgetting across all task combinations (BWT metrics show retention but don't test extreme domain shifts)

## Next Checks

1. **Transport path validation:** Train FusionFM with pure Gaussian noise source and measure actual Wasserstein distance between generated and real fused distributions
2. **Pseudo-label robustness:** Replace Fusion Refiner with simple averaging of top-2 selected candidates and measure performance degradation
3. **Memory buffer scaling:** Systematically vary replay buffer size (100→2000) during multi-task training and measure BWT/forward transfer trade-offs