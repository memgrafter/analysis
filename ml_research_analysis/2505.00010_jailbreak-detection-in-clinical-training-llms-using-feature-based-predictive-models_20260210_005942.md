---
ver: rpa2
title: Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive
  Models
arxiv_id: '2505.00010'
source_url: https://arxiv.org/abs/2505.00010
tags:
- arxiv
- jailbreak
- https
- decision
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of detecting jailbreak attempts
  in large language models (LLMs) used for clinical education. The authors propose
  a feature-based approach, annotating over 2,300 prompts across 158 conversations
  using four linguistic variables: professionalism, medical relevance, ethical behavior,
  and contextual distraction.'
---

# Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models

## Quick Facts
- arXiv ID: 2505.00010
- Source URL: https://arxiv.org/abs/2505.00010
- Reference count: 37
- Key outcome: Feature-based models outperformed prompt engineering in detecting jailbreak attempts in clinical LLMs, with fuzzy decision tree achieving 94.79% accuracy and 0.9492 F1-score

## Executive Summary
This paper presents a novel feature-based approach to detecting jailbreak attempts in large language models used for clinical education. The authors annotated over 2,300 prompts across 158 conversations using four linguistic variables: professionalism, medical relevance, ethical behavior, and contextual distraction. These features were used to train predictive models including decision trees, fuzzy logic classifiers, boosting methods, and logistic regression. The results demonstrated that feature-based models significantly outperformed traditional prompt engineering approaches, with the fuzzy decision tree model achieving the highest accuracy and F1-score. The study provides evidence for the effectiveness of interpretable, feature-based methods in detecting jailbreak behavior while maintaining explainability in educational LLM applications.

## Method Summary
The research employed a feature-based detection framework for jailbreak attempts in clinical training LLMs. Researchers annotated 2,300+ prompts across 158 conversations using four linguistic variables: professionalism, medical relevance, ethical behavior, and contextual distraction. These annotations served as features to train multiple predictive models including decision trees, fuzzy logic-based classifiers, boosting methods, and logistic regression. The models were evaluated against baseline prompt engineering approaches to assess their effectiveness in detecting jailbreak attempts. The fuzzy decision tree model emerged as the top performer, demonstrating superior accuracy and F1-score compared to other methods while maintaining interpretability.

## Key Results
- Feature-based models outperformed prompt engineering approaches in jailbreak detection
- Fuzzy decision tree achieved highest performance with 94.79% accuracy and 0.9492 F1-score
- Feature-based methods provided better interpretability while maintaining high detection rates
- Clinical LLM jailbreak detection benefits from incorporating linguistic feature analysis

## Why This Works (Mechanism)
The feature-based approach works by capturing nuanced linguistic patterns that indicate jailbreak attempts through quantifiable variables. Unlike prompt engineering which relies on predefined templates, feature-based methods can identify subtle deviations in professionalism, relevance, ethics, and context that signal manipulation attempts. The fuzzy decision tree specifically excels by handling uncertainty in linguistic features while maintaining interpretability, allowing for precise classification of borderline cases that might confuse rigid rule-based systems.

## Foundational Learning
- Linguistic feature annotation: Critical for quantifying qualitative aspects of prompts; verify through inter-annotator agreement scores
- Fuzzy logic in classification: Handles uncertainty in linguistic data; validate with crisp vs. fuzzy model comparisons
- Decision tree interpretability: Enables understanding of detection logic; test with feature importance analysis
- Feature-based vs. prompt engineering: Demonstrates paradigm shift in jailbreak detection; benchmark against traditional methods
- Clinical LLM safety: Context-specific application; evaluate with domain experts for relevance

## Architecture Onboarding

**Component Map:**
Feature Annotation -> Feature Extraction -> Model Training -> Performance Evaluation -> Real-time Detection

**Critical Path:**
1. Feature Annotation: Annotators label prompts across 4 linguistic dimensions
2. Feature Extraction: Convert annotations to numerical features for modeling
3. Model Training: Train multiple models (decision trees, fuzzy logic, boosting, logistic regression)
4. Performance Evaluation: Compare models against prompt engineering baselines
5. Real-time Detection: Deploy best-performing model for live monitoring

**Design Tradeoffs:**
- Interpretability vs. Performance: Fuzzy decision tree prioritizes explainability over marginal accuracy gains
- Feature Granularity: Four linguistic variables balance comprehensiveness with annotation feasibility
- Model Complexity: Multiple model types evaluated to find optimal balance of accuracy and deployment practicality
- Annotation Overhead: Human annotation required but enables nuanced detection beyond pattern matching

**Failure Signatures:**
- High false positives when prompts naturally exhibit low professionalism but remain educational
- Difficulty detecting sophisticated jailbreaks that maintain surface-level professionalism
- Performance degradation with prompts outside clinical education domain
- Annotation bias affecting feature distribution and model calibration

**First 3 Experiments:**
1. Baseline comparison: Evaluate prompt engineering methods against feature-based models on identical datasets
2. Cross-domain testing: Apply best model to non-clinical LLM applications to assess generalizability
3. Feature ablation: Remove each linguistic feature individually to determine impact on detection performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition may limit generalizability due to concentration from 158 conversations
- Absence of detailed demographic information about prompt authors could introduce bias
- Clinical education focus may constrain applicability to other LLM domains
- Feature selection may not capture all relevant jailbreak indicators

## Confidence

**High confidence:**
- Comparative performance results between feature-based and prompt engineering approaches

**Medium confidence:**
- Generalizability of findings across different LLM applications and domains
- Feature selection process comprehensiveness

## Next Checks
1. Conduct cross-domain validation by applying the feature-based models to non-clinical LLM applications to assess generalizability
2. Perform ablation studies to determine the relative importance and impact of each linguistic feature on detection performance
3. Implement real-time testing of the fuzzy decision tree model in live clinical training environments to evaluate practical deployment effectiveness