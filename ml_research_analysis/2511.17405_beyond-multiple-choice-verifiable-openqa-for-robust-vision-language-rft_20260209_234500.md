---
ver: rpa2
title: 'Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT'
arxiv_id: '2511.17405'
source_url: https://arxiv.org/abs/2511.17405
tags:
- question
- answer
- mcqa
- options
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the unreliability of multiple-choice question
  answering (MCQA) for evaluating and training multimodal language models. It shows
  that MCQA metrics overestimate model capabilities and encourage answer guessing
  behavior, while reinforcement learning on MCQA data harms open-ended generalization.
---

# Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT

## Quick Facts
- arXiv ID: 2511.17405
- Source URL: https://arxiv.org/abs/2511.17405
- Reference count: 40
- Primary result: ReVeL improves OpenQA accuracy by ~6pp while matching MCQA accuracy, reducing score inflation by up to 20pp

## Executive Summary
This paper addresses the unreliability of multiple-choice question answering (MCQA) for training and evaluating multimodal language models. It demonstrates that MCQA metrics overestimate model capabilities by encouraging answer guessing behavior and reward hacking, while reinforcement learning on MCQA data harms open-ended generalization. The authors propose ReVeL, a framework that converts MCQA into verifiable open-form questions using a hybrid rule-based and LLM-based evaluation system. Applied to four benchmarks, ReVeL improves judging accuracy and reduces cost, and when used in reinforcement fine-tuning on 20k examples, models trained with ReVeL-OpenQA match MCQA accuracy while improving OpenQA accuracy by about six percentage points, demonstrating higher data efficiency and robustness.

## Method Summary
ReVeL converts MCQA examples into verifiable OpenQA format through a triage classifier that routes questions to four specialized rewriters (Numeric, Keyword, Open-answer, Per-option). The framework uses hybrid verification combining deterministic rule-based matching for structured answers and LLM judging for semantic ones. The rewritten data is then used for reinforcement fine-tuning with GRPO on multimodal models. The system was evaluated on 20k examples from the ViRL dataset across four benchmarks (EMMA, MMMU, MME-RealWorld, MMLU-Pro), comparing MCQA and OpenQA accuracy and the gap between them.

## Key Results
- Models trained with ReVeL-OpenQA match MCQA accuracy while improving OpenQA accuracy by about six percentage points
- ReVeL reduces False Positive Rate from 2.0% to 0.3% compared to pure LLM judging
- MCQA training widens the MCQA-OpenQA gap (score inflation), while OpenQA training closes it
- Up to 20 percentage points of score inflation in MCQA compared to OpenQA when evaluated with ReVeL

## Why This Works (Mechanism)

### Mechanism 1
Converting MCQA to OpenQA mitigates "option exploitation" where models guess correctly using signals from distractors rather than reasoning. By removing the option set, models rely on internal knowledge and visual reasoning to generate answers, aligning training rewards with actual capability rather than test-taking heuristics.

### Mechanism 2
A hybrid verification architecture (Rules + LLM) improves reward reliability for RFT compared to pure LLM judging. The system triages questions into deterministic categories solvable by code, reserving LLM judging only for cases requiring semantic understanding, reducing variance in the reward signal.

### Mechanism 3
Training on OpenQA data improves open-ended generalization without sacrificing multiple-choice accuracy. RFT on MCQA encourages overfitting to option-selection shortcuts, while RFT on OpenQA forces models to learn underlying concepts, allowing transferable knowledge for both open and multiple-choice settings.

## Foundational Learning

- **Reward Hacking (in RLHF/RFT)**: MCQA formats are susceptible to reward hacking because models can maximize accuracy via guessing strategies rather than true reasoning. Quick check: If a model achieves 90% accuracy on MCQA but 20% on the same questions in OpenQA format, is it "smart" or just "good at guessing"?

- **Verifiable Reward Models**: RFT requires accurate scalar rewards. Pure generative tasks struggle with verification, but ReVeL bridges this by converting generation into verifiable units (numbers/keywords) to enable RFT on open-ended tasks. Quick check: Why is exact match (Rule-based) often preferred over LLM-as-a-judge for math or coding tasks?

- **Positional Bias / Selection Bias in MCQA**: Models may favor option "A" over "D" or select based on option length. ReVeL removes options to cure this symptom. Quick check: What happens to model accuracy if you shuffle the order of options A, B, C, and D?

## Architecture Onboarding

- **Component map**: Triage Classifier -> 4 Rewriters (Numeric/Keyword/Open/Per-option) -> 2 Verifiers (Rule-based/LLM Judge) -> RFT Engine
- **Critical path**: The Question Classification (Triage) step is most fragile. Misclassifying a "Numeric" question as "Open Answer" loses deterministic guarantees and incurs higher latency/cost with no accuracy gain.
- **Design tradeoffs**: Cost vs. Noise (LLM Judge is noisy and slow; Rule-based is brittle but free and exact). Data Prep vs. Training Efficiency (significant compute upfront rewriting data to save training steps later).
- **Failure signatures**: Format Drift (model outputs "The answer is 5" but rule expects "5"). Keyword Variance (model says "automobile" but keyword list only has "car"). Over-true ratio on TF items.
- **First 3 experiments**: 1) Sanity Check: Run baseline model on MCQA set, then ReVeL in OpenQA mode to confirm ~20% score inflation gap. 2) Verifier Accuracy Audit: Hand-verify 100 samples comparing ReVeL's Hybrid Grader vs. GPT-4o-mini for False Positives. 3) RFT Ablation: Fine-tune 3B model on 1k samples (MCQA vs ReVeL-OpenQA) and compare performance on held-out OpenQA test set.

## Open Questions the Paper Calls Out

1. **Long-form generation extension**: Can ReVeL be extended to long-form generation tasks where evaluation remains challenging? The current design relies on hybrid verification for specific answer types that may not translate to subjective long-form text.

2. **Adaptive evaluation systems**: How can systems dynamically select the most cost-effective judging mechanism based on question complexity? Current pipeline uses fixed triage without adjusting verification strategy based on computational cost or difficulty.

3. **Error propagation in rewriting**: To what extent do errors in the initial LLM-based rewriting phase propagate and negatively impact final RFT rewards? The paper acknowledges rewriting phases are imperfect but provides no quantitative analysis of error rates or their effect on GRPO optimization.

## Limitations
- Unknown LLMs and exact hyperparameters for the rewriting pipeline
- No ablation of category classifier accuracy, which could silently degrade evaluation
- Potential overfitting to specific MCQA sources used
- Limited evaluation to four benchmarks with unknown robustness to out-of-domain tasks

## Confidence
- **High confidence**: MCQA-OpenQA score inflation (20pp gap) and general superiority of ReVeL training for OpenQA generalization
- **Medium confidence**: Hybrid evaluation accuracy improvements and data efficiency claims (depend on unspecified implementation details)
- **Low confidence**: Exact GRPO hyperparameters and robustness of rule-based components across diverse input formats

## Next Checks
1. **Rule-based Matcher Robustness**: Run numeric/keyword verifiers on 100+ synthetic edge cases (different number formats, synonyms, LaTeX) to measure False Negative rate
2. **Classifier Error Analysis**: Manually label 200 samples from triage classifier to quantify misclassification rates and downstream impact on reward quality
3. **Generalization Probe**: Test trained ReVeL-OpenQA model on held-out MCQA dataset from different source (e.g., outside ViRL) to check for domain overfitting