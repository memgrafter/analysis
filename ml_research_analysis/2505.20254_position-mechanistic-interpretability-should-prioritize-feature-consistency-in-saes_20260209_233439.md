---
ver: rpa2
title: 'Position: Mechanistic Interpretability Should Prioritize Feature Consistency
  in SAEs'
arxiv_id: '2505.20254'
source_url: https://arxiv.org/abs/2505.20254
tags:
- feature
- features
- consistency
- saes
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that feature consistency\u2014the reliable convergence\
  \ of Sparse Autoencoder (SAE) features across independent training runs\u2014should\
  \ be prioritized in mechanistic interpretability research. The authors propose using\
  \ Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric\
  \ to quantify this consistency and demonstrate that high consistency levels (PW-MCC\
  \ \u2248 0.80) are achievable with appropriate SAE architectures like TopK SAEs."
---

# Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs

## Quick Facts
- arXiv ID: 2505.20254
- Source URL: https://arxiv.org/abs/2505.20254
- Reference count: 40
- Primary result: Feature consistency measured by PW-MCC should be prioritized in mechanistic interpretability research

## Executive Summary
This paper argues that Sparse Autoencoder (SAE) features should exhibit consistent convergence across independent training runs to ensure reliable mechanistic interpretability. The authors propose PW-MCC (Pairwise Dictionary Mean Correlation Coefficient) as a practical metric for quantifying feature consistency and demonstrate that high consistency levels (PW-MCC ≈ 0.80) are achievable with TopK SAE architectures. The work establishes theoretical connections between SAEs and overcomplete sparse dictionary learning, validates PW-MCC on synthetic data, and applies it to real LLM activation spaces. The authors advocate for routine reporting of consistency scores alongside standard metrics to advance the scientific rigor of mechanistic interpretability.

## Method Summary
The authors develop a theoretical framework connecting SAEs to identifiability results in overcomplete sparse dictionary learning, establishing conditions under which SAE features can be expected to converge consistently. They propose PW-MCC as a metric that measures the average correlation between feature pairs across multiple independent SAE training runs. The methodology includes synthetic validation using controlled datasets where ground-truth features are known, allowing verification that PW-MCC tracks actual feature recovery. Real-world experiments apply these techniques to LLM activation data, comparing feature consistency across different SAE architectures while correlating consistency scores with semantic similarity of feature explanations obtained through automated interpretability techniques.

## Key Results
- PW-MCC ≈ 0.80 consistency is achievable with TopK SAE architectures
- High PW-MCC correlates with semantic similarity of feature explanations
- Theoretical connection established between SAEs and overcomplete sparse dictionary learning identifiability

## Why This Works (Mechanism)
The paper's approach works because consistent feature recovery across training runs indicates that SAEs are capturing stable, meaningful patterns in the activation space rather than training artifacts or noise. The PW-MCC metric provides a quantitative measure of this stability by comparing feature dictionaries across multiple independent training instances. The theoretical grounding in sparse dictionary learning suggests that under appropriate conditions (sufficient sparsity, proper regularization), SAEs should converge to similar solutions regardless of initialization, reflecting the underlying structure of the data.

## Foundational Learning

**Sparse Autoencoders (SAEs)**
- Why needed: Core technique for decomposing high-dimensional activations into interpretable features
- Quick check: Can decompose 10k-dimensional activations into 1k sparse features with reconstruction error < 5%

**Pairwise Dictionary Mean Correlation Coefficient (PW-MCC)**
- Why needed: Quantifies feature consistency across independent training runs
- Quick check: PW-MCC > 0.7 indicates highly consistent feature recovery

**Overcomplete Sparse Dictionary Learning**
- Why needed: Theoretical foundation explaining when SAEs should converge consistently
- Quick check: Dictionary size > data dimensionality with sparsity constraint ensures identifiability

## Architecture Onboarding

**Component Map**
Data -> SAE Training (multiple runs) -> Feature Dictionaries -> PW-MCC Calculation -> Consistency Assessment

**Critical Path**
1. Prepare LLM activation data
2. Train multiple independent SAE instances
3. Extract feature dictionaries from each run
4. Compute PW-MCC between all dictionary pairs
5. Evaluate semantic consistency of high-PW-MCC features

**Design Tradeoffs**
- Higher sparsity improves consistency but may lose important features
- Larger dictionaries increase representational capacity but reduce identifiability
- More training runs improve PW-MCC reliability but increase computational cost

**Failure Signatures**
- PW-MCC < 0.5 suggests unstable feature learning or inadequate sparsity
- High reconstruction error indicates poor feature quality regardless of consistency
- Low semantic similarity despite high PW-MCC suggests metric limitations

**3 First Experiments**
1. Train SAEs with varying sparsity penalties and measure PW-MCC sensitivity
2. Compare PW-MCC across different activation layers in the same model
3. Test PW-MCC robustness to initialization variance and random seeds

## Open Questions the Paper Calls Out
None

## Limitations
- Correlation between consistency and interpretability remains observational rather than causal
- Synthetic validation uses simplified datasets that may not capture full LLM complexity
- Theoretical identifiability results don't guarantee practical SAE behavior as true dictionary atoms

## Confidence
- High confidence: Achieving PW-MCC ≈ 0.80 with TopK SAEs, general utility of consistency metrics
- Medium confidence: Theoretical grounding connection to dictionary learning, correlation with semantic similarity
- Low confidence: Direct claim that consistency implies better mechanistic interpretability, generalizability across all architectures

## Next Checks
1. Conduct ablation studies varying SAE hyperparameters to establish robustness of consistency-PW-MCC relationships
2. Perform downstream task evaluations using features from high-consistency vs low-consistency SAEs
3. Extend consistency measurements to multi-layer SAE applications across different model families