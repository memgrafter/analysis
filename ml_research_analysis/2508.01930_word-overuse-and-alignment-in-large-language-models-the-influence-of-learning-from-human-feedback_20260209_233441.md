---
ver: rpa2
title: 'Word Overuse and Alignment in Large Language Models: The Influence of Learning
  from Human Feedback'
arxiv_id: '2508.01930'
source_url: https://arxiv.org/abs/2508.01930
tags:
- lexical
- llms
- words
- language
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the role of Learning from Human Feedback
  (LHF) in shaping lexical choices of Large Language Models (LLMs), particularly the
  overuse of terms like "delve" and "intricate." The authors propose a method to detect
  LHF-induced lexical preferences by comparing outputs from Llama models trained with
  and without LHF. They then experimentally validate that human evaluators prefer
  texts containing these overrepresented words.
---

# Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback

## Quick Facts
- arXiv ID: 2508.01930
- Source URL: https://arxiv.org/abs/2508.01930
- Authors: Tom S. Juzek; Zina B. Ward
- Reference count: 40
- Primary result: Human evaluators prefer texts containing LHF-associated words (52.4% vs. 47.6%; χ² = 9.4, p < 0.01)

## Executive Summary
This study investigates whether Learning from Human Feedback (LHF) contributes to lexical overuse in Large Language Models, particularly terms like "delve" and "intricate." The authors propose detecting LHF-induced lexical preferences by comparing Llama Base and Instruct models, then experimentally validate that humans prefer texts containing these overrepresented words. Results show significant preference for high-LHF variants, suggesting LHF may systematically amplify specific vocabulary through preference aggregation.

## Method Summary
The methodology involves generating matched corpora from Llama 3.2-3B Base and Instruct models using identical PubMed abstract continuations, identifying lemmata with significant frequency increases via chi-square tests, and calculating LHF-Scores based on weighted percentage increases. Generated variants with high/low LHF-Scores are then presented to human evaluators in pairwise comparisons, with preference data analyzed using chi-square tests and mixed-effects regression while controlling for participant and item effects.

## Key Results
- Human evaluators prefer texts containing LHF-associated words (52.4% vs. 47.6%; χ² = 9.4, p < 0.01)
- LHF-Scores significantly predict human preferences with β = 0.524, p < 0.001
- Experimental design successfully isolates LHF effects from other training stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LHF systematically amplifies specific lexical items through preference aggregation
- Mechanism: Small evaluator biases compound through optimization, causing disproportionate lexical amplification
- Core assumption: Base/Instruct differences primarily reflect LHF effects
- Evidence anchors: Significant preference for LHF-associated words; limited corpus evidence for LHF-specific causation
- Break condition: If differences arise from non-LHF training stages

### Mechanism 2
- Claim: LHF workforce demographics shape amplified lexical items
- Mechanism: Global South-based LHF workers possess different lexical preferences than Global North end-users
- Core assumption: Experimental participants generalize to actual LHF worker populations
- Evidence anchors: Participant demographics mirror LHF workforce; some lexical items show dialectal patterns
- Break condition: If preferences are uniform across populations

### Mechanism 3
- Claim: LHF task structure causes evaluators to use vocabulary as quality proxy
- Mechanism: Evaluators rely on sophisticated vocabulary as heuristic for quality under time pressure
- Core assumption: Evaluators substitute style for content assessment
- Evidence anchors: Style-over-substance preferences in human evaluation literature
- Break condition: If evaluators can assess content quality regardless of vocabulary

## Foundational Learning

- Concept: Chi-square test for categorical preference data
  - Why needed here: Core statistical test determines if preference distributions differ significantly
  - Quick check question: Why is chi-square appropriate here versus a t-test for binary judgments?

- Concept: Base vs Instruct model comparison methodology
  - Why needed here: Detection procedure relies on isolating LHF effects
  - Quick check question: What confounding factors might affect the Base/Instruct comparison?

- Concept: Occurrence-per-million (opm) normalization
  - Why needed here: Frequency comparisons require normalization for corpus size differences
  - Quick check question: Why weight by relative increase percentage rather than absolute difference?

## Architecture Onboarding

- Component map: Corpus generation -> Lexical extraction -> Frequency comparison -> LHF-Score engine -> Variant generator -> Experimental platform -> Analysis pipeline
- Critical path: Generate matched corpora → Identify frequency increases → Calculate LHF-Scores → Select high/low pairs → Collect preferences → Test null hypothesis
- Design tradeoffs: Corpus size vs. noise; control vs. ecological validity; exclusion rate vs. power; population matching vs. generalizability
- Failure signatures: High item variance; non-significant chi-square; low literature overlap; length/content confounds
- First 3 experiments: 1) Replicate with OLMo/Falcon models; 2) Recruit Global North participants; 3) Vary evaluation task constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is preference for lexical items driven by evaluator demographics or task nature?
- Basis in paper: Authors state "discriminating between these explanations... requires future research"
- Why unresolved: Study confirms preference but doesn't identify underlying cause
- What evidence would resolve it: Experimental designs isolating demographic variables from task constraints

### Open Question 2
- Question: Is lexical overuse correlated with English dialects spoken by Global South LHF workers?
- Basis in paper: Authors note speculation but lack empirical data
- Why unresolved: No direct evidence linking preferences to LHF worker dialects
- What evidence would resolve it: Corpus analysis comparing LLM outputs to LHF worker speech patterns

### Open Question 3
- Question: What factors besides LHF contribute to lexical overuse in LLMs?
- Basis in paper: Authors conclude "other contributing factors remain to be systematically investigated"
- Why unresolved: Study focused on LHF but couldn't rule out other training stages
- What evidence would resolve it: Ablation studies comparing lexical frequency across training stages

## Limitations

- Limited evidence establishing direct causation between LHF and word overuse
- Speculative mechanisms for how LHF induces lexical amplification
- Potential confounding from instruction tuning and safety mitigation procedures

## Confidence

- High Confidence: Experimental finding that humans prefer LHF-associated words
- Medium Confidence: LHF contributes to lexical overuse in LLMs
- Low Confidence: Specific mechanisms (demographics, task structure, proxy heuristics)

## Next Checks

1. Replicate methodology using model pairs from different architectures (OLMo, Falcon) to test generalizability beyond Llama models
2. Conduct parallel experiments with Global North participants to isolate demographic effects on lexical preferences
3. Design controlled experiments varying evaluation task constraints (time pressure, domain expertise) to test proxy-heuristic mechanism