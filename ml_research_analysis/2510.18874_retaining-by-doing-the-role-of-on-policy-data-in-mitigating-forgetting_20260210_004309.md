---
ver: rpa2
title: 'Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting'
arxiv_id: '2510.18874'
source_url: https://arxiv.org/abs/2510.18874
tags:
- forgetting
- data
- policy
- drop
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in language model
  post-training by comparing supervised fine-tuning (SFT) and reinforcement learning
  (RL). The authors find that RL consistently exhibits less forgetting than SFT while
  achieving comparable or higher target task performance across multiple model families
  (Llama, Qwen), scales, and tasks (instruction following, general knowledge, arithmetic
  reasoning).
---

# Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting

## Quick Facts
- **arXiv ID**: 2510.18874
- **Source URL**: https://arxiv.org/abs/2510.18874
- **Reference count**: 40
- **Primary result**: RL exhibits less catastrophic forgetting than SFT while achieving comparable or higher target task performance across multiple model families, scales, and tasks.

## Executive Summary
This paper investigates catastrophic forgetting in language model post-training by comparing supervised fine-tuning (SFT) and reinforcement learning (RL). The authors find that RL consistently exhibits less forgetting than SFT while achieving comparable or higher target task performance across multiple model families (Llama, Qwen), scales, and tasks (instruction following, general knowledge, arithmetic reasoning). Through theoretical analysis using mixture-of-Gaussians models and extensive ablations, they identify that RL's robustness stems from its use of on-policy data, which enables mode-seeking behavior that preserves prior knowledge. The study shows that approximately on-policy data, such as data generated at the start of each epoch, can substantially reduce forgetting in SFT, suggesting a practical guideline for mitigating forgetting in language model post-training.

## Method Summary
The study compares SFT and RL (GRPO) for language model post-training on three target tasks (IFEval, MMLU, Countdown) while measuring degradation on non-target tasks (MATH, WildJailbreak, WildGuardTest). Experiments use Llama-3.2-1B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-1.5B-Instruct, and Qwen-2.5-7B-Instruct models. SFT uses Llama-3.3-70B-Instruct responses filtered by reward, while Self-SFT generates responses from the initial model. GRPO employs on-policy sampling with KL penalty 0.05 and group size 5. Training runs for 2 epochs with AdamW optimizer (learning rates 1e-4 for 1B/1.5B models, 5e-6 for 7B/8B models). The primary metrics are target task gain (accuracy improvement) and non-target tasks drop (average accuracy decrease on other tasks).

## Key Results
- RL consistently exhibits less catastrophic forgetting than SFT while achieving comparable or higher target task performance across multiple model families and scales.
- Approximately on-policy data (generated at start of each epoch) can substantially reduce forgetting in SFT.
- Theoretical analysis using mixture-of-Gaussians models suggests RL's robustness stems from mode-seeking behavior enabled by on-policy data.

## Why This Works (Mechanism)
RL exhibits less forgetting than SFT because it uses on-policy data, which enables mode-seeking behavior that preserves prior knowledge distributions. When RL updates the policy, it samples from the current policy state, allowing it to maintain modes learned from earlier data while adapting to the target task. In contrast, SFT uses static datasets that may not represent the current policy distribution, leading to mode collapse on non-target tasks. The mixture-of-Gaussians analysis demonstrates that on-policy updates can preserve multiple modes in the distribution, while off-policy updates tend to overwrite earlier modes.

## Foundational Learning
- **Catastrophic forgetting**: When neural networks overwrite previously learned knowledge while learning new tasks. Why needed: Core problem being addressed.
- **On-policy vs off-policy data**: On-policy uses data sampled from current policy, off-policy uses fixed datasets. Why needed: Key distinction explaining RL's advantage.
- **Mode-seeking behavior**: Optimization tendency to preserve multiple peaks in probability distributions. Why needed: Mechanism by which RL preserves prior knowledge.
- **Mixture-of-Gaussians models**: Theoretical framework for analyzing multi-modal distributions. Why needed: Tool for understanding forgetting dynamics.
- **GRPO (Group Relative Policy Optimization)**: Reinforcement learning algorithm using relative rewards within groups. Why needed: Specific RL method used in experiments.

## Architecture Onboarding

### Component Map
Base models (Llama/Qwen) -> SFT/GRPO training -> Target task performance + Non-target task evaluation

### Critical Path
Base model initialization -> Data preparation (on/off-policy) -> Training (2 epochs) -> Evaluation on target and non-target tasks -> Analysis of gain vs drop

### Design Tradeoffs
- **Static vs dynamic data**: SFT uses static datasets (faster but more forgetting) vs RL uses dynamic on-policy data (slower but less forgetting)
- **Computational cost**: Generating on-policy data at each epoch increases compute requirements but reduces forgetting
- **Learning rate sensitivity**: Higher learning rates improve target task performance but increase forgetting in SFT

### Failure Signatures
- SFT with low learning rate fails to reach target performance
- Self-SFT still shows substantial forgetting despite using initial policy data
- RL forgets when target distribution is far from initial modes

### First Experiments
1. Train SFT vs GRPO on IFEval task and measure forgetting on MATH
2. Implement Self-SFT with initial-model-generated responses and compare forgetting to standard SFT
3. Vary learning rate in SFT to observe tradeoff between target gain and non-target drop

## Open Questions the Paper Calls Out
- **Open Question 1**: How do forgetting patterns change as model scale and dataset size increase beyond the 8B parameter limit studied?
- **Open Question 2**: What is the theoretical mechanism connecting on-policy data to reduced forgetting in multi-modal policy distributions?
- **Open Question 3**: Under what conditions does RL itself suffer from catastrophic forgetting?
- **Open Question 4**: What is the precise relationship between KL divergence from the initial policy and the degree of forgetting?

## Limitations
- Theoretical analysis provides intuition but lacks formal proof connecting mixture-of-Gaussians to language model dynamics
- Empirical evidence limited to relatively small models (up to 8B parameters) and specific task domains
- Practical recommendation for on-policy data in SFT is computationally expensive

## Confidence
- **High confidence**: RL consistently outperforms SFT in mitigating forgetting while maintaining target task performance
- **Medium confidence**: Theoretical explanation that on-policy data enables mode-seeking behavior which preserves prior knowledge
- **Medium confidence**: Practical recommendation that using data generated at start of each epoch in SFT can substantially reduce forgetting

## Next Checks
1. Replicate the forgetting comparison between SFT and RL on models larger than 8B parameters (e.g., 70B) to test generalizability
2. Test forgetting dynamics in non-instruction-following domains (e.g., code generation, scientific reasoning) to assess task-agnostic nature
3. Quantify the exact computational cost of generating on-policy data at each epoch for SFT and compare against forgetting reduction benefit