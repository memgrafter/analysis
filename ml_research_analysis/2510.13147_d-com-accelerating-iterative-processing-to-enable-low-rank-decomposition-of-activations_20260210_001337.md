---
ver: rpa2
title: 'D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition
  of Activations'
arxiv_id: '2510.13147'
source_url: https://arxiv.org/abs/2510.13147
tags:
- decomposition
- input
- computation
- memory
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of runtime overhead in activation\
  \ decomposition for large language models, where previous approaches incurred significant\
  \ latency penalties. The authors propose D-com, a co-designed decomposition algorithm\
  \ and hardware accelerator that exploits iterative vector operation expansion to\
  \ transform memory-bound computations into compute-bound ones, achieving 6.2\xD7\
  \ speedup in decomposition."
---

# D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations

## Quick Facts
- arXiv ID: 2510.13147
- Source URL: https://arxiv.org/abs/2510.13147
- Reference count: 32
- Primary result: 6.2× speedup in activation decomposition with 22% end-to-end latency improvement over A100 GPU

## Executive Summary
This work addresses the challenge of runtime overhead in activation decomposition for large language models, where previous approaches incurred significant latency penalties. The authors propose D-com, a co-designed decomposition algorithm and hardware accelerator that exploits iterative vector operation expansion to transform memory-bound computations into compute-bound ones, achieving 6.2× speedup in decomposition. They introduce output-decomposed computation to eliminate redundant decomposition steps and channel-wise outlier extraction to preserve model quality. Their accelerator design with 256 clusters provides 22% end-to-end latency improvement over A100 GPU while maintaining acceptable model quality (3% degradation on AI2 Reasoning Challenge task). The methodology is demonstrated on Llama2-7b, reducing memory footprint by 15.6% and enabling real-time activation decomposition during inference.

## Method Summary
D-com introduces a novel co-design approach combining algorithm-level innovations with specialized hardware acceleration for low-rank activation decomposition in LLMs. The method employs output-decomposed computation that propagates decomposed representations through consecutive layers, eliminating redundant re-decomposition overhead. It implements channel-wise outlier extraction to preserve model quality by identifying and separating outlier channels before decomposition. The hardware accelerator uses a compute expansion methodology that replicates iterative computations across parallel units, transforming memory-bound Lanczos bidiagonalization operations into compute-bound ones. The system is demonstrated on Llama2-7b with 256 clusters, achieving 22% end-to-end latency improvement while maintaining <3% quality degradation.

## Key Results
- 6.2× speedup in Lanczos decomposition through compute expansion methodology
- 22% end-to-end latency improvement over A100 GPU with 256-cluster accelerator
- 15.6% memory footprint reduction on Llama2-7b while maintaining <3% accuracy degradation
- Channel-wise outlier extraction preserves quality by extracting 3% of outlier channels on average

## Why This Works (Mechanism)

### Mechanism 1: Compute Expansion for Iterative Algorithms
Replicating computations across parallel units can transform memory-bound iterative operations into compute-bound ones, achieving speedup when the expansion factor is matched to available hardware resources. The Lanczos bidiagonalization algorithm's re-orthogonalization steps require repeated vector reads, reductions, and broadcasts. By partially expanding the computation graph—replicating multiply operations and localizing reductions into smaller groups—the algorithm reduces the serialization penalty of global memory access and large-vector reduction. Instead of one large reduction, multiple smaller reductions occur in parallel, followed by a smaller global aggregation. The expansion factor f=8 is optimal for their 256-cluster design.

### Mechanism 2: Output-Decomposed (Decomposition-Preserved) Computation
Propagating decomposed representations through consecutive layers eliminates redundant re-decomposition overhead and maintains memory footprint reduction. Standard input decomposition would require re-decomposing outputs before each subsequent layer. Instead, D-com computes only the first matrix multiplication in the decomposed chain (e.g., V* = V × W), preserving the output in factorized form (U, Σ, V*) for direct consumption by the next layer. This avoids reconstructing full tensors and re-decomposing them, maintaining the compression benefits across multiple layers.

### Mechanism 3: Channel-wise Outlier Extraction
Extracting a small percentage (≤5%) of activation channels containing outliers before decomposition significantly reduces low-rank approximation error with minimal overhead. LLM activations exhibit structured outliers concentrated in specific channels rather than random distribution. SVD's squared-error minimization is sensitive to outliers, which distort recovered subspaces. By identifying and separating outlier channels using per-layer static thresholds (determined offline), these channels bypass decomposition and are processed separately, then recombined at reconstruction. Extracting 3% of outliers on average can considerably improve model quality.

## Foundational Learning

- **Concept: Lanczos Bidiagonalization Algorithm**
  - Why needed here: This iterative algorithm is the computational core being accelerated; understanding its structure (alternating matrix-vector products, orthogonalization, reorthogonalization) is essential to grasp why it is memory-bound and how compute expansion helps.
  - Quick check question: Can you explain why Lanczos is faster than QR or Divide-and-Conquer for small decomposition ranks, and which operations dominate runtime?

- **Concept: Memory-bound vs. Compute-bound Operations**
  - Why needed here: D-com's core innovation is reclassifying decomposition from memory-bound to compute-bound via expansion; understanding Roofline concepts (arithmetic intensity, bandwidth ceiling) clarifies the design rationale.
  - Quick check question: Given an operation with low arithmetic intensity, what two strategies can move it toward compute-bound, and which does D-com use?

- **Concept: Low-Rank SVD Approximation and Error Propagation**
  - Why needed here: The paper trades off approximation error (ε) against compression; understanding how truncating singular values affects reconstruction and how errors compound across layers is critical for configuring decomposition.
  - Quick check question: If you decompose input X with rank r1=r2=10 instead of 20, what happens to reconstruction error, memory footprint, and downstream layer computation?

## Architecture Onboarding

- **Component map:**
  - 256 clusters arranged 16×16; each column shares a dedicated memory bank
  - Per cluster: 64 FP16 multipliers (8×8 array), shared buffer, dual reduction trees (horizontal + vertical)
  - Reduce/Scatter units: Binary-tree structures for logarithmic-depth reductions
  - Global buffer: Interfaces to external GEMM accelerator (e.g., GPU)
  - Control path: Manages expansion factor f, iteration count, outlier threshold lookup

- **Critical path:**
  1. Input activation partitioned across column memory banks
  2. Per-column clusters perform local multiply and partial reduction
  3. Partial results aggregated via reduce/scatter network
  4. Broadcast of correction vectors for next iteration
  5. Repeat for k Lanczos iterations; output U, Σ, V factors

- **Design tradeoffs:**
  - Expansion factor (f): Higher f reduces memory bottleneck but requires more clusters; f=8 is optimal for 256-cluster design
  - Cluster scale: 8×8 multipliers balances per-cluster parallelism against area; 7× smaller than equivalent A100 compute capability
  - Outlier extraction granularity: Channel-wise (not element-wise) minimizes metadata and computation overhead
  - Decomposed layer selection: Non-adjacent layers reduce error accumulation (per prior work)

- **Failure signatures:**
  - Latency regression: If expansion factor too low (memory-bound persists) or too high (insufficient compute), speedup degrades
  - Quality collapse: If too many layers decomposed consecutively, perplexity spikes; if outlier thresholds wrong for workload, accuracy drops
  - Resource mismatch: If input sequence length exceeds per-column bank capacity, partitioning fails

- **First 3 experiments:**
  1. Baseline profiling: Run Lanczos decomposition on A100 for varying sequence lengths (1K, 2K, 4K) and ranks (1, 10, 20); measure runtime breakdown to confirm re-orthogonalization dominance
  2. Expansion factor sweep: In simulation, vary f from 1 to 16 for a fixed input size; plot latency to validate f=8 optimum and observe memory-bound→compute-bound crossover
  3. Outlier extraction calibration: Run arc_easy and wikitext-2 with different outlier percentages (0%, 2%, 3%, 5%, 7%); plot accuracy/perplexity vs. extraction to confirm 3–5% sweet spot for target layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the layer selection process be automated to navigate the "extremely large design space" of decomposition configurations?
- Basis in paper: Section 6.2 states that "The decomposition choice is an extremely large design space that can be explored further in future research."
- Why unresolved: The authors rely on manual selection and heuristics (non-adjacent layers) to balance quality and runtime, lacking an automated optimizer.
- What evidence would resolve it: An automated search algorithm (e.g., reinforcement learning or gradient-based search) that identifies optimal layer configurations for unseen models.

### Open Question 2
- Question: Does the efficiency of D-com's "compute expansion" methodology scale to models significantly larger than 7B parameters?
- Basis in paper: The evaluation is limited to Llama2-7b, while the introduction emphasizes the industry trend toward 1T parameter models.
- Why unresolved: The balance between memory-bound and compute-bound operations via expansion factor tuning may shift drastically with different model geometries.
- What evidence would resolve it: Benchmarking results of the D-com architecture on a 70B+ parameter model demonstrating similar speedups and expansion efficiency.

### Open Question 3
- Question: Can dynamic, input-adaptive outlier thresholding improve upon the quality of the current static, offline-determined thresholds?
- Basis in paper: Section 4 notes that while thresholds are "statically-determined," the "feature map values vary for the inputs at each layer."
- Why unresolved: A static threshold table may fail to capture unique outlier distributions in novel or adversarial input prompts.
- What evidence would resolve it: A study comparing the accuracy (Perplexity/ARC) of static vs. dynamic thresholding schemes within the same hardware latency budget.

## Limitations

- The static outlier threshold calibration methodology is not fully specified, making it difficult to reproduce exact quality results across different workloads
- The compute expansion mapping logic is presented at algorithmic level but lacks detailed implementation specifics for hardware simulation
- The 22% end-to-end speedup claim depends on the custom D-com accelerator, which is not directly comparable to standard GPU implementations

## Confidence

- **High confidence:** Compute expansion mechanism for accelerating iterative algorithms, memory reduction figures, and basic quality degradation bounds
- **Medium confidence:** Output-decomposed computation scheme and channel-wise outlier extraction effectiveness, as these rely on empirical calibration
- **Low confidence:** Exact reproduction of 6.2× decomposition speedup without the full accelerator RTL and simulation environment

## Next Checks

1. Profile Lanczos decomposition on standard GPU hardware across varying sequence lengths to independently verify memory-bound characteristics and re-orthogonalization dominance
2. Implement and benchmark output-decomposed computation scheme on GPU to measure overhead elimination vs. quality trade-offs
3. Systematically vary outlier extraction percentages and thresholds to map the accuracy-latency frontier and verify the claimed 3-5% sweet spot