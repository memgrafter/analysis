---
ver: rpa2
title: Distinct Computations Emerge From Compositional Curricula in In-Context Learning
arxiv_id: '2506.13253'
source_url: https://arxiv.org/abs/2506.13253
tags:
- task
- compositional
- curriculum
- learning
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how in-context curricula\u2014structured\
  \ sequences presenting subtasks before their composition\u2014affect transformer\
  \ models' ability to perform compositional in-context learning. The authors design\
  \ a modular double-exponential task composed of two single-exponential subtasks\
  \ and compare models trained with and without in-context curricula."
---

# Distinct Computations Emerge From Compositional Curricula in In-Context Learning
## Quick Facts
- arXiv ID: 2506.13253
- Source URL: https://arxiv.org/abs/2506.13253
- Reference count: 40
- Key outcome: In-context curricula improve compositional in-context learning by inducing intermediate computation representations

## Executive Summary
This paper investigates how in-context curricula—structured sequences presenting subtasks before their composition—affect transformer models' ability to perform compositional in-context learning. The authors design a modular double-exponential task composed of two single-exponential subtasks and compare models trained with and without in-context curricula. Models trained with curricula show significantly higher zero-shot accuracy on unseen compositional tasks and greater robustness across compositional task examples. Linear probe analyses reveal that curriculum-trained models represent intermediate computation values from subtasks, enabling compositional computation, while vanilla-trained models do not.

## Method Summary
The authors create a controlled experimental setup using a modular double-exponential task where two single-exponential subtasks (f(x) and g(x)) are composed into a double-exponential task h(x) = f(g(x)). They train transformer models on three conditions: vanilla (random task ordering), in-context curricula (subtasks presented before their composition), and pre-training (subtasks learned before fine-tuning). The key comparison is zero-shot performance on held-out compositional examples, measuring whether models can compose learned subtasks without additional training examples.

## Key Results
- Curriculum-trained models achieve near-perfect zero-shot accuracy on compositional tasks, while vanilla-trained models perform at chance level
- Linear probe analysis shows curriculum-trained models represent intermediate computation values, enabling compositional computation
- Longer compositional task blocks in curricula lead to mixed strategies combining compositional and few-shot learning approaches

## Why This Works (Mechanism)
The paper demonstrates that compositional curricula induce transformers to develop intermediate representations of subtask computations, which are then recombined for compositional tasks. This mechanism emerges from the structured presentation of subtasks before their composition, allowing the model to learn separate representations that can be composed. The curriculum design influences whether the model adopts pure compositional strategies or mixes them with standard few-shot learning.

## Foundational Learning
- **Compositional in-context learning**: The ability to combine learned subtasks to solve new tasks without additional training examples. Needed because standard in-context learning often fails on compositional tasks.
- **Linear probes**: A method for analyzing model representations by training a linear classifier on frozen model activations. Used to detect whether intermediate computation values are represented.
- **Zero-shot performance**: Model performance on tasks without any examples in context. Critical metric for measuring true compositional ability.
- **Modular task design**: Creating tasks from reusable components to systematically study composition. Enables controlled experiments on compositional learning.
- **Curriculum learning**: Presenting training examples in a structured order. Here used to expose subtasks before their composition.

## Architecture Onboarding
- **Component map**: Data generation -> Model training (vanilla/curriculum/pre-training) -> Evaluation (zero-shot accuracy, linear probes) -> Analysis
- **Critical path**: Generate modular tasks → Train with curriculum design → Measure zero-shot compositional accuracy → Analyze representations with linear probes
- **Design tradeoffs**: Controlled task complexity vs. real-world applicability; curriculum design complexity vs. performance gains; interpretability vs. task realism
- **Failure signatures**: Random performance on compositional tasks; lack of intermediate computation representations; dependence on few-shot examples rather than composition
- **First experiments**: 1) Test curriculum effect on tasks with 3+ subtasks, 2) Vary curriculum block sizes systematically, 3) Apply mechanistic interpretability to validate causal role of intermediate representations

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled double-exponential task may not generalize to more complex real-world compositional tasks
- Optimal curriculum design for different compositional tasks remains unclear
- Linear probe evidence for compositional computation needs stronger causal validation

## Confidence
- **High confidence**: Core empirical finding that in-context curricula improve zero-shot compositional task performance
- **Medium confidence**: Linear probe evidence for intermediate computation representation
- **Medium confidence**: Claim about curriculum design affecting compositional versus few-shot learning strategies

## Next Checks
1. Test curriculum effect on more complex compositional tasks (e.g., three or more subtasks, or tasks requiring non-linear combinations of subtasks) to assess generalizability
2. Conduct ablation studies varying curriculum parameters (block size, ordering, frequency of subtask examples) to map the design space
3. Implement mechanistic interpretability analyses (e.g., activation patching, causal tracing) to directly validate that observed intermediate representations causally contribute to compositional computation during inference