---
ver: rpa2
title: 'On-device edge learning for IoT data streams: a survey'
arxiv_id: '2502.17788'
source_url: https://arxiv.org/abs/2502.17788
tags:
- data
- learning
- arxiv
- these
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews continual learning methods for on-device training
  in edge computing environments, focusing on neural networks and decision trees for
  classification tasks on IoT data streams. The paper identifies key constraints including
  data architecture (batch vs.
---

# On-device edge learning for IoT data streams: a survey

## Quick Facts
- arXiv ID: 2502.17788
- Source URL: https://arxiv.org/abs/2502.17788
- Reference count: 40
- This survey reviews continual learning methods for on-device training in edge computing environments, focusing on neural networks and decision trees for classification tasks on IoT data streams.

## Executive Summary
This survey comprehensively examines continual learning methods for on-device training in edge computing environments, with a particular focus on IoT data streams. The authors analyze how to deploy neural networks and decision trees at the edge while continuously learning under resource constraints. The paper identifies key challenges including catastrophic forgetting, data inefficiency, and the stability-plasticity trade-off, while proposing evaluation frameworks for assessing model performance across multiple criteria.

## Method Summary
The survey synthesizes current research on continual learning for IoT edge devices by examining neural network architectures suitable for resource-constrained environments and dynamic decision tree approaches that enable memory-efficient growth. The authors analyze how these building blocks can be integrated into autonomous online systems while managing critical challenges such as forward-backward transfer and model convergence. The methodology emphasizes the importance of evaluating systems using multi-criteria metrics that consider both accuracy and resource efficiency in realistic IoT deployment scenarios.

## Key Results
- Identification of key constraints including data architecture (batch vs. stream), network capacity (cloud vs. edge), catastrophic forgetting, and data inefficiency
- Examination of neural network deployment at the edge with continuous learning under resource constraints
- Analysis of decision tree viability for on-device training through dynamic architectures and memory-efficient growth
- Proposal of multi-criteria evaluation metrics for assessing model performance in IoT settings

## Why This Works (Mechanism)
The effectiveness of on-device edge learning for IoT data streams stems from the combination of adaptive algorithms that can learn continuously without catastrophic forgetting, coupled with architectures designed for resource efficiency. By leveraging incremental learning approaches and memory-augmented models, these systems can adapt to concept drift in IoT environments while maintaining performance within the strict constraints of edge devices. The integration of neural networks with dynamic decision trees provides complementary strengths: neural networks excel at capturing complex patterns while decision trees offer interpretability and efficient updates.

## Foundational Learning
- **Catastrophic Forgetting**: The tendency of models to forget previously learned information when trained on new data - needed for understanding why naive continual learning fails; quick check: test model performance on old data after training on new data
- **Concept Drift**: The phenomenon where data distribution changes over time in streaming environments - needed for designing adaptive learning systems; quick check: monitor prediction accuracy drift over time
- **Stability-Plasticity Trade-off**: The balance between retaining old knowledge (stability) and learning new information (plasticity) - needed for optimizing model adaptation; quick check: measure backward transfer performance
- **Resource Constraints**: Limited memory, processing power, and energy availability on edge devices - needed for realistic system design; quick check: profile CPU, memory, and battery usage during model updates
- **Transfer Learning**: The ability to apply knowledge from one task to another - needed for efficient learning in resource-limited settings; quick check: measure forward transfer performance across related tasks
- **Incremental Learning**: Learning from data that becomes available over time - needed for handling continuous data streams; quick check: validate model performance with increasing data volume

## Architecture Onboarding

**Component Map**: IoT Sensor Data -> Preprocessing -> Feature Extraction -> Continual Learning Module (Neural Networks + Decision Trees) -> Model Update -> Prediction Output

**Critical Path**: Data Acquisition → Feature Engineering → Model Update → Prediction

**Design Tradeoffs**: Memory vs. Accuracy (larger models retain more but consume more resources), Update Frequency vs. Energy Consumption (frequent updates improve adaptation but drain battery), Model Complexity vs. Inference Speed (complex models are accurate but slow)

**Failure Signatures**: Catastrophic forgetting (accuracy drop on old data), Concept drift (systematic prediction degradation), Resource exhaustion (out-of-memory errors), Model divergence (unstable training)

**First Experiments**: 1) Benchmark single-pass learning on synthetic concept-drift datasets, 2) Measure memory usage and update time for decision tree growth on edge hardware, 3) Evaluate catastrophic forgetting by testing on held-out old data after new data training

## Open Questions the Paper Calls Out
The survey identifies several open questions regarding the practical deployment of continual learning systems on edge devices. These include the real-world feasibility of integrating various building blocks into cohesive autonomous online systems, the practical challenges of managing stability-plasticity trade-offs beyond theoretical understanding, and the limited focus on classification tasks potentially overlooking other important IoT applications such as regression or anomaly detection.

## Limitations
- Practical deployment challenges of continual learning systems on edge devices remain uncertain
- Effectiveness of dynamic decision tree architectures and memory-efficient growth strategies needs real-world validation
- Limited focus on classification tasks may overlook other important IoT applications

## Confidence
High: Identification of key constraints (data architecture, network capacity, catastrophic forgetting, data inefficiency) and general framework for on-device learning
Medium: Claims about specific algorithmic solutions, particularly effectiveness of dynamic decision tree architectures and memory-efficient growth strategies in real-world scenarios

## Next Checks
1. Conduct empirical studies comparing the proposed continual learning approaches across diverse IoT data streams (different sensors, varying data distributions) to validate their robustness and generalization capabilities
2. Implement and benchmark a complete edge learning system that integrates neural networks and decision trees, measuring actual resource consumption (memory, CPU, battery) on representative edge hardware
3. Develop and validate a comprehensive evaluation framework that includes not only accuracy metrics but also resource efficiency, model update time, and robustness to concept drift in real IoT environments