---
ver: rpa2
title: Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging
arxiv_id: '2512.08333'
source_url: https://arxiv.org/abs/2512.08333
tags:
- finetuning
- merging
- policy
- task
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of robustly finetuning generalist
  robot policies on new tasks while preserving their ability to generalize and perform
  previously learned tasks. Current finetuning methods often lead to overfitting on
  the limited demonstration data, resulting in poor performance on out-of-distribution
  variations of the target task and degradation of generalist capabilities.
---

# Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging

## Quick Facts
- arXiv ID: 2512.08333
- Source URL: https://arxiv.org/abs/2512.08333
- Authors: Yajat Yadav, Zhiyuan Zhou, Andrew Wagenmaker, Karl Pertsch, Sergey Levine
- Reference count: 40
- Key outcome: Simple weight interpolation between pretrained and finetuned robot policies improves out-of-distribution generalization by up to 40% and better preserves generalist capabilities.

## Executive Summary
This paper addresses a fundamental challenge in robot learning: how to adapt generalist robot policies to new tasks without sacrificing their ability to generalize or forgetting previously learned skills. The authors observe that standard finetuning on limited demonstration data often leads to overfitting and degradation of the pretrained model's generalization capabilities. They propose RETAIN, a straightforward approach that interpolates the weights of the pretrained generalist policy with those of the finetuned policy. This simple weight merging strategy significantly improves performance on out-of-distribution variations of the target task while preserving the generalist abilities of the original pretrained model. Real-world experiments demonstrate up to 40% higher success rates compared to direct finetuning, validating the effectiveness of this approach for practical robot learning scenarios.

## Method Summary
RETAIN addresses the challenge of robust finetuning by interpolating between pretrained and finetuned model weights. The method takes the pretrained generalist robot policy and finetunes it on limited demonstration data for a new task, then creates a final policy by linearly combining the weights from both models. Specifically, the RETAIN policy parameters are computed as θ_RETAIN = α·θ_pretrained + (1-α)·θ_finetuned, where α is the interpolation coefficient (typically 0.9 in their experiments). This approach leverages the generalization capabilities of the pretrained model while incorporating task-specific knowledge from finetuning. The method is simple to implement, requires no additional training or hyperparameter tuning beyond selecting the interpolation coefficient, and works directly with the parameter space of the neural network without requiring access to the original training data or complex regularization schemes.

## Key Results
- RETAIN improves out-of-distribution success rates by up to 40% compared to direct finetuning on a stacking task
- The method better preserves generalist capabilities, with RETAIN outperforming both pretrained and finetuned models on held-out tasks
- RETAIN is effective for continual learning, enabling sequential skill acquisition without catastrophic forgetting
- Weight interpolation (α=0.9) provides the best balance between task adaptation and generalization preservation

## Why This Works (Mechanism)
The effectiveness of RETAIN stems from the observation that direct finetuning on limited data causes the policy to overfit to the specific training distribution, losing the broad generalization capabilities of the pretrained model. By interpolating weights between the pretrained and finetuned models, RETAIN effectively creates a compromise that retains the robust features learned during pretraining while incorporating task-specific adaptations. The pretrained model's parameters encode general visual and control representations that enable handling distribution shifts, while the finetuned parameters capture task-specific nuances. The interpolation coefficient acts as a regularization parameter that controls the trade-off between adaptation and generalization, with higher weights on the pretrained model preserving more of the original generalization capabilities while still allowing sufficient task-specific learning.

## Foundational Learning
- **Vision-Language-Action (VLA) policies**: Multimodal robot policies that take visual observations, language commands, and produce actions; needed for general-purpose robot learning from diverse data sources; quick check: verify the policy outputs continuous actions and accepts image-language inputs
- **Distribution shifts in robotics**: Variations in environment, objects, or task conditions that differ from training data; critical because robots must operate in unstructured real-world settings; quick check: ensure evaluation includes systematic variations from training conditions
- **Catastrophic forgetting**: Phenomenon where learning new tasks degrades performance on previously learned tasks; fundamental challenge in continual learning; quick check: validate performance on held-out tasks before and after finetuning
- **Weight interpolation regularization**: Technique of combining parameters from different models to balance competing objectives; provides implicit regularization without additional training; quick check: verify interpolation preserves both generalization and task performance
- **Generalist vs. specialist policies**: Trade-off between broad capability across many tasks versus high performance on specific tasks; central tension in robot policy design; quick check: evaluate both on target task and diverse held-out tasks

## Architecture Onboarding

**Component Map**: Pretrained VLA policy <-(interpolation)-> Finetuned VLA policy -> RETAIN policy

**Critical Path**: Pretrained model → Finetuning on demonstrations → Weight interpolation → Final RETAIN policy → Evaluation on target and held-out tasks

**Design Tradeoffs**: Simple weight merging versus complex regularization methods; no additional training or hyperparameter tuning required but requires selecting interpolation coefficient; works with any pretrained model without architectural modifications; may be less effective when pretrained and finetuned models occupy very different parameter spaces

**Failure Signatures**: RETAIN performs poorly if the finetuned model overfits completely to training data (parameters diverge significantly from pretrained model); interpolation coefficient too high preserves generalization but limits task adaptation; coefficient too low causes overfitting similar to direct finetuning

**First Experiments**: 1) Compare success rates of pretrained, finetuned, and RETAIN models on distribution-shifted test conditions; 2) Evaluate performance on held-out tasks to measure preservation of generalist capabilities; 3) Test sequential finetuning with RETAIN to assess continual learning performance across multiple tasks

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Evaluation focused on single manipulation task (stacking) with limited distribution shifts, may not generalize to diverse robotic applications
- Performance improvements measured against simple baselines without comparison to sophisticated continual learning methods
- Weight interpolation assumes pretrained and finetuned models occupy similar parameter space, may fail for tasks requiring fundamentally different control strategies
- Optimal interpolation ratio (0.9) appears task-specific and requires hyperparameter tuning for different scenarios

## Confidence
**High Confidence**: Direct finetuning on limited data leads to overfitting and degraded generalization; weight merging improves out-of-distribution performance
**Medium Confidence**: RETAIN better preserves generalist capabilities during finetuning; RETAIN is effective for continual learning and sequential skill acquisition

## Next Checks
1. **Cross-domain generalization**: Evaluate RETAIN on substantially different task domains (navigation, assembly, dexterous manipulation) to assess whether the weight merging approach generalizes beyond stacking and whether the optimal interpolation ratio transfers across domains
2. **Scaling analysis**: Investigate performance with varying amounts of demonstration data (smaller and larger than 50 demonstrations) and across different pretrained model scales to determine data-efficiency sweet spot and scalability limits
3. **Comparison to state-of-the-art**: Benchmark RETAIN against recent continual learning methods (EWC, MAS, gradient-based meta-learning) to establish whether simple weight merging remains competitive with sophisticated regularization techniques