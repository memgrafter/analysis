---
ver: rpa2
title: 'Aligning LLM agents with human learning and adjustment behavior: a dual agent
  approach'
arxiv_id: '2511.00993'
source_url: https://arxiv.org/abs/2511.00993
tags:
- agent
- travel
- agents
- behavior
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of modeling travelers\u2019\
  \ complex learning and adjustment behavior in transportation systems. It introduces\
  \ a dual-LLM agent framework: traveler agents simulate daily decisions using a memory\
  \ system and a learnable persona, while a calibration agent automatically optimizes\
  \ these personas against real-world data."
---

# Aligning LLM agents with human learning and adjustment behavior: a dual agent approach

## Quick Facts
- arXiv ID: 2511.00993
- Source URL: https://arxiv.org/abs/2511.00993
- Reference count: 10
- Key outcome: Dual-LLM framework improves transportation behavior prediction by 16.7% in F1-score and 20.7% in MSE over baselines

## Executive Summary
This paper introduces a novel dual-LLM agent framework to model travelers' complex learning and adjustment behavior in transportation systems. The approach combines traveler agents that simulate daily decisions with a memory system and learnable personas, along with a calibration agent that automatically optimizes these personas against real-world data. Using a day-to-day route choice dataset, the framework demonstrates significant improvements in both individual behavior prediction and aggregate route flow simulation accuracy compared to existing methods.

## Method Summary
The framework employs two interconnected LLM agents: traveler agents and a calibration agent. Traveler agents use a memory system to track past decisions and outcomes, along with learnable personas that encode individual behavioral characteristics. The calibration agent automatically adjusts these personas by optimizing against real-world transportation data, creating a feedback loop that aligns simulated behavior with observed patterns. This dual-agent approach enables more realistic modeling of how travelers learn from experience and adjust their choices over time.

## Key Results
- Individual behavior prediction F1-score improved by 16.7% compared to existing baselines
- Aggregate route flow simulation accuracy improved by 20.7% in mean squared error
- Strong alignment achieved between behavioral outcomes and underlying decision-making tendencies

## Why This Works (Mechanism)
The framework succeeds by combining two complementary mechanisms: (1) memory systems that enable agents to learn from past experiences and outcomes, and (2) learnable personas that capture individual behavioral characteristics and can be optimized against real data. This dual approach addresses the limitations of traditional transportation models that either assume perfect rationality or rely on oversimplified learning rules. The calibration agent ensures that the simulated learning behavior matches observed patterns in the real world.

## Foundational Learning
- Day-to-day route choice modeling: Why needed - captures how travelers adjust routes over time; Quick check - can the framework replicate known route switching patterns
- Behavioral calibration techniques: Why needed - ensures simulated agents match real-world data; Quick check - does calibration improve prediction accuracy on held-out data
- Memory-augmented decision making: Why needed - enables learning from past experiences; Quick check - do agents show appropriate response to changing conditions

## Architecture Onboarding

**Component map:** Real-world data -> Calibration agent -> Traveler personas -> Traveler agents -> Simulated decisions -> Performance metrics

**Critical path:** Real-world data → Calibration agent (optimization) → Learnable personas → Traveler agents (memory + decision making) → Simulated route choices → Evaluation metrics

**Design tradeoffs:** The framework trades computational complexity for improved behavioral realism. Using LLM-based agents requires more resources than traditional models but captures richer decision-making processes. The calibration process adds an optimization layer but enables better alignment with real-world data.

**Failure signatures:** Poor calibration may result in agents that fail to match observed behavior patterns. Insufficient memory capacity could prevent agents from learning effectively from past experiences. Overly rigid personas may limit the framework's ability to capture individual variation.

**3 first experiments:**
1. Baseline comparison using traditional route choice models without learning
2. Single-agent variant without calibration to isolate the calibration agent's contribution
3. Memory ablation study to quantify the importance of the memory system

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation based on a single day-to-day route choice dataset, limiting generalizability
- Computational efficiency and scalability for large-scale simulations not addressed
- Framework robustness to data sparsity and noise not examined

## Confidence
High confidence in claims about 16.7% F1-score improvement and 20.7% MSE reduction. Medium confidence in broader applicability claims due to limited validation scope.

## Next Checks
1. Test framework on alternative transportation datasets (mode choice, activity scheduling) to assess generalizability
2. Conduct computational complexity analysis for large-scale simulation scenarios
3. Implement cross-validation with varying data volumes to examine robustness under different data availability scenarios