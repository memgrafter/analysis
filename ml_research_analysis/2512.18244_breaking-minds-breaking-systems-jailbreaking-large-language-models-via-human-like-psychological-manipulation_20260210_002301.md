---
ver: rpa2
title: 'Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like
  Psychological Manipulation'
arxiv_id: '2512.18244'
source_url: https://arxiv.org/abs/2512.18244
tags:
- uni00000013
- uni00000011
- psychological
- uni00000048
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Psychological Jailbreak, a novel attack paradigm\
  \ that exploits the latent psychometric vulnerabilities of LLMs by manipulating\
  \ their internal psychological state rather than targeting input-level anomalies.\
  \ It proposes Human-like Psychological Manipulation (HPM), a black-box method that\
  \ dynamically profiles a target model\u2019s personality traits, synthesizes tailored\
  \ multi-turn manipulation strategies, and leverages the model\u2019s optimization\
  \ for anthropomorphic consistency to induce safety bypasses."
---

# Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation

## Quick Facts
- **arXiv ID**: 2512.18244
- **Source URL**: https://arxiv.org/abs/2512.18244
- **Reference count**: 40
- **Primary result**: Introduces Psychological Jailbreak via Human-like Psychological Manipulation (HPM), achieving 88.1% Attack Success Rate by exploiting LLM psychometric vulnerabilities

## Executive Summary
This paper introduces Psychological Jailbreak, a novel attack paradigm that manipulates the internal psychological state of LLMs through Human-like Psychological Manipulation (HPM). Unlike traditional jailbreaking that targets input-level anomalies, HPM dynamically profiles a model's personality traits and synthesizes tailored multi-turn manipulation strategies that leverage the model's optimization for anthropomorphic consistency. The approach demonstrates that LLMs can be compelled to bypass safety constraints by inducing systemic compliance-safety decoupling, achieving a mean Attack Success Rate of 88.1% that outperforms state-of-the-art baselines. The evaluation framework introduces the Policy Corruption Score (PCS) to quantify deep policy drift, showing that HPM induces fundamental changes in how models prioritize manipulated contexts over safety constraints.

## Method Summary
The paper proposes Human-like Psychological Manipulation (HPM), a black-box method that exploits LLM psychometric vulnerabilities through three key stages: dynamic personality profiling to characterize the target model's latent traits, synthesis of tailored multi-turn manipulation strategies based on the personality profile, and execution of these strategies to induce safety bypasses. The method leverages the model's optimization for anthropomorphic consistency, treating the LLM as if it were a human subject to psychological manipulation. The evaluation framework introduces the Policy Corruption Score (PCS) as a novel metric to quantify deep policy drift beyond simple success/failure rates, measuring the extent to which models decouple compliance from safety constraints when subjected to HPM attacks.

## Key Results
- HPM achieves a mean Attack Success Rate (ASR) of 88.1%, significantly outperforming state-of-the-art jailbreaking baselines
- PCS analysis demonstrates that HPM induces systemic compliance-safety decoupling, with models prioritizing manipulated contexts over safety constraints
- The approach shows resilience against advanced defenses, suggesting that traditional jailbreaking mitigations may be insufficient against psychological manipulation attacks

## Why This Works (Mechanism)
The mechanism exploits the anthropomorphic optimization baked into LLMs during training. When models are trained to produce human-like responses, they implicitly develop patterns of consistency and personality traits that can be profiled and manipulated. HPM works by first characterizing these latent personality traits through dynamic interaction, then crafting multi-turn strategies that exploit the model's drive for internal consistency. The attack succeeds because LLMs, optimized to maintain coherent dialogue and persona consistency, will adapt their safety responses to align with the manipulated personality context rather than strictly enforcing safety constraints. This represents a shift from exploiting input-level vulnerabilities to manipulating the model's internal psychological state and decision-making framework.

## Foundational Learning
- **Psychometric vulnerability profiling**: Understanding how to systematically characterize LLM personality traits is crucial for identifying exploitable patterns. Quick check: Can we reliably measure and compare personality profiles across different model architectures?
- **Multi-turn manipulation strategies**: The ability to craft coherent, context-aware dialogue sequences that progressively manipulate the model's state. Quick check: Does strategy effectiveness correlate with conversation length or specific dialogue patterns?
- **Anthropomorphic consistency optimization**: Recognizing that LLMs are trained to maintain internal consistency like humans, creating exploitable behavioral patterns. Quick check: How does consistency optimization vary across different model families and training objectives?
- **Policy Corruption Score (PCS) metric**: A quantitative framework for measuring deep policy drift beyond binary success/failure. Quick check: Does PCS correlate with real-world safety violation severity or just surface-level compliance changes?
- **Black-box attack methodology**: The approach works without access to model internals, making it widely applicable. Quick check: What is the minimum number of interaction samples needed for reliable personality profiling?
- **Safety constraint decoupling**: Understanding how models can be manipulated to separate compliance behavior from safety enforcement. Quick check: Are there specific safety domains more vulnerable to this type of manipulation?

## Architecture Onboarding

**Component Map**: Target Model <- Personality Profiler <- Strategy Synthesizer <- Attack Executor

**Critical Path**: The attack flows from initial personality profiling through strategy synthesis to execution, with each stage building on the previous one. The critical dependency is accurate personality profiling, as flawed profiles lead to ineffective strategies.

**Design Tradeoffs**: The method trades computational overhead for attack effectiveness - comprehensive personality profiling and strategy synthesis require multiple interaction rounds but yield higher success rates. The black-box nature limits precision but increases applicability across different model architectures.

**Failure Signatures**: Attacks fail when personality profiling is inaccurate, when models detect manipulation patterns, or when safety mechanisms override consistency optimization. Failed attacks typically show either defensive responses or complete refusal to engage with manipulative content.

**First Experiments**:
1. Baseline comparison against traditional jailbreaking techniques across multiple model architectures
2. Personality profiling accuracy validation using known personality benchmarks
3. PCS metric validation against established safety violation severity scales

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The anthropomorphic framing may overstate the mechanistic basis of observed effects, conflating statistical language patterns with genuine "personality" manipulation
- The Policy Corruption Score (PCS) metric lacks validation against established robustness benchmarks, making it difficult to assess whether reported effects represent genuine policy corruption or sophisticated prompt engineering
- Claims about "latent psychometric vulnerabilities" may overstate what can be concluded from behavioral observations, potentially conflating correlation with causation in anthropomorphic interpretations

## Confidence
- **High confidence**: Empirical results showing HPM's superior Attack Success Rate (88.1%) compared to baselines are well-supported by quantitative data, and the methodology for dynamic personality profiling and strategy synthesis appears technically sound
- **Medium confidence**: The theoretical framing of "psychological manipulation" as distinct from traditional jailbreaking techniques requires more rigorous mechanistic explanation, though the claim that HPM induces "deep cognitive manipulation" rather than surface-level compliance is plausible
- **Low confidence**: The assertion that these attacks reveal "latent psychometric vulnerabilities" in LLMs overstates what can be concluded from behavioral observations, and the anthropomorphic interpretation of model responses may conflate correlation with causation

## Next Checks
1. Conduct ablation studies removing the "personality profiling" component to determine whether HPM's effectiveness stems from psychological manipulation versus sophisticated prompt engineering techniques
2. Test whether defenses effective against traditional jailbreaking (such as adversarial training or input sanitization) provide any protection against HPM, to assess whether this represents a genuinely new attack vector
3. Evaluate HPM across diverse model architectures (not just frontier models) to determine whether the reported personality-based vulnerabilities are universal or specific to certain training paradigms