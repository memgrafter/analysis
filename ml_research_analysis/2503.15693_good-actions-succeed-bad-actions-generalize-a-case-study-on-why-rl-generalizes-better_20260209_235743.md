---
ver: rpa2
title: 'Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes
  Better'
arxiv_id: '2503.15693'
source_url: https://arxiv.org/abs/2503.15693
tags:
- generalization
- training
- agent
- unseen
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the zero-shot generalization capabilities of
  reinforcement learning (RL) and supervised learning (SL) in the Habitat visual navigation
  task. Using PPO as an RL representative and BC as an SL representative, the authors
  evaluate both algorithms on state-goal pair generalization within seen environments
  and generalization to unseen environments.
---

# Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better
## Quick Facts
- arXiv ID: 2503.15693
- Source URL: https://arxiv.org/abs/2503.15693
- Authors: Meng Song
- Reference count: 5
- PPO outperforms BC on Habitat visual navigation tasks in both zero-shot generalization settings

## Executive Summary
This paper investigates why reinforcement learning (RL) demonstrates superior zero-shot generalization compared to supervised learning (SL) in the Habitat visual navigation task. Through systematic experiments comparing PPO (RL) and BC (SL), the authors demonstrate that RL's advantage stems from its ability to combinatorially stitch together fragments of both successful and failed trajectories to solve novel tasks. While SL methods like BC generalize by directly imitating optimal trajectories, RL leverages temporal difference learning to construct solutions from experience fragments, enabling discovery of novel strategies beyond the training distribution.

## Method Summary
The authors conduct controlled experiments using PPO as the RL representative and BC as the SL representative on Habitat's visual navigation task. They evaluate both algorithms on two zero-shot generalization scenarios: generalizing to new state-goal pairs within seen environments, and generalizing to completely unseen environments. Performance is measured using success rate and SPL metrics. To isolate algorithmic effects, BC is augmented with additional optimal training data to match PPO's SPL performance. The authors analyze generalization mechanisms by examining how each algorithm utilizes training data and constructs solutions for novel tasks.

## Key Results
- PPO consistently outperforms BC in both zero-shot generalization settings (new state-goal pairs and unseen environments)
- BC augmented with optimal training data matches PPO's SPL but remains significantly behind in success rate
- The performance gap persists even when BC receives additional optimal trajectories, suggesting fundamental algorithmic differences

## Why This Works (Mechanism)
The paper attributes RL's superior generalization to its combinatorial experience stitching mechanism. Unlike SL methods that directly imitate successful trajectories, TD-based RL can combine fragments of both successful and failed experiences to construct solutions for novel tasks. This allows RL to efficiently explore vast state spaces and discover strategies that may not exist in the training data, while SL remains constrained to patterns present in the demonstration data.

## Foundational Learning
1. **Temporal Difference Learning**: Why needed - Enables bootstrapping from incomplete trajectories; Quick check - Verify TD error updates are correctly implemented
2. **Zero-shot Generalization**: Why needed - Tests algorithm's ability to handle novel scenarios; Quick check - Ensure train/test splits have no overlap
3. **Trajectory Stitching**: Why needed - Core mechanism for RL's generalization advantage; Quick check - Track fragment utilization patterns during inference
4. **On-policy vs Off-policy Learning**: Why needed - Affects how experience is collected and utilized; Quick check - Confirm algorithm matches implementation type
5. **Curriculum Learning**: Why needed - May influence generalization capabilities; Quick check - Document task difficulty progression

## Architecture Onboarding
**Component Map**: Observation Encoder -> Policy Network -> Value Network -> Action Selector -> Environment
**Critical Path**: Observation -> Encoder -> Policy -> Action -> Environment Response -> Value Update
**Design Tradeoffs**: PPO's clipped objective trades off optimization stability for slower convergence vs BC's direct imitation approach
**Failure Signatures**: BC fails when novel situations deviate from training trajectories; PPO may explore inefficiently in sparse reward settings
**First Experiments**: 1) Compare performance on seen vs unseen environments 2) Analyze fragment utilization patterns 3) Test with corrupted training trajectories

## Open Questions the Paper Calls Out
Major uncertainties remain around the generalizability of these findings to other RL algorithms beyond PPO and other task domains beyond Habitat visual navigation. The claim that TD-based RL specifically enables combinatorial stitching of failed trajectories appears theoretically plausible but requires more rigorous mathematical formalization to establish causal mechanisms. The experimental comparison between BC and RL is limited to a single architecture and hyperparameter configuration, leaving open the possibility that alternative implementations might narrow the observed performance gaps.

## Limitations
- Findings may not generalize beyond PPO and Habitat visual navigation tasks
- Combinatorial stitching hypothesis lacks rigorous mathematical formalization
- Single architecture and hyperparameter configuration limits broader applicability
- Potential confounding factors from data distribution shifts not fully isolated

## Confidence
- High confidence in PPO outperforming BC on Habitat visual navigation tasks
- Medium confidence in the combinatorial stitching hypothesis as primary mechanism
- Low confidence in universal applicability across RL algorithms and domains

## Next Checks
1. Test the combinatorial stitching hypothesis by systematically ablating failed trajectory data in RL training and measuring impact on generalization
2. Replicate experiments across multiple RL algorithms (DDPG, SAC) and multiple task domains to assess robustness
3. Conduct controlled experiments isolating data distribution effects from algorithmic mechanisms to determine relative contribution to generalization differences