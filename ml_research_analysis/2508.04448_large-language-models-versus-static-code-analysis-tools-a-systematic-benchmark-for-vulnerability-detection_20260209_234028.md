---
ver: rpa2
title: 'Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark
  for Vulnerability Detection'
arxiv_id: '2508.04448'
source_url: https://arxiv.org/abs/2508.04448
tags:
- code
- static
- large
- language
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic benchmark comparing three static
  code analysis tools (SonarQube, CodeQL, SnykCode) with three large language models
  (GPT-4.1, Mistral Large, DeepSeek V3) for vulnerability detection in C projects.
  The study evaluates detection accuracy (precision, recall, F1-score), analysis latency,
  and developer effort using a curated dataset of ten real-world projects with 63
  embedded vulnerabilities.
---

# Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection

## Quick Facts
- arXiv ID: 2508.04448
- Source URL: https://arxiv.org/abs/2508.04448
- Reference count: 40
- Primary result: LLMs achieved F1 scores up to 0.797 vs 0.260 for static tools in vulnerability detection

## Executive Summary
This systematic benchmark compares three static code analysis tools (SonarQube, CodeQL, SnykCode) against three large language models (GPT-4.1, Mistral Large, DeepSeek V3) for vulnerability detection in C# projects. The study evaluates detection accuracy, latency, and developer effort using ten real-world projects with 63 embedded vulnerabilities. LLMs significantly outperformed static tools in recall and F1-scores, achieving 0.797, 0.753, and 0.750 respectively versus 0.260, 0.386, and 0.546 for the static tools. However, LLMs generated more false positives and mislocated issues at line/column granularity due to tokenization artifacts. The results suggest that while LLMs excel at broad, context-aware vulnerability detection, their noisier output and imprecise localization limit standalone use in safety-critical scenarios. The study recommends a hybrid pipeline using LLMs for early triage and static tools for high-assurance verification, providing an open benchmark for reproducible future research.

## Method Summary
The study uses ten curated C# projects (3-5k characters each) containing 63 known vulnerabilities across categories like SQL injection, XSS, and hard-coded secrets. Static tools were executed with default configurations to generate SARIF reports, while LLMs accessed via GitHub Models API were prompted using a custom C# tool ("ProjectAnalyzer") that aggregates source files into specific system prompts requiring JSON output. Model outputs were parsed and converted to standardized SARIF format for comparison. Precision, recall, and F1 scores were calculated by manually verifying SARIF outputs against ground truth vulnerabilities, with special consideration that LLM findings should not be validated on exact line numbers due to tokenization limitations.

## Key Results
- LLMs achieved F1 scores of 0.797 (GPT-4.1), 0.753 (Mistral), and 0.750 (DeepSeek) versus 0.260, 0.386, and 0.546 for static tools
- LLMs demonstrated significantly higher recall (0.845, 0.793, 0.797) compared to static tools (0.371, 0.266, 0.438)
- DeepSeek V3 exhibited the highest false-positive ratio among LLMs, while SonarQube and CodeQL maintained lowest FP ratios among static tools
- All LLMs failed to provide precise line/column localization due to tokenization artifacts, reporting file-level identification only

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Semantic Reasoning vs. Pattern Matching
LLMs achieve higher recall by reasoning across broader code contexts rather than relying on predefined vulnerability signatures. Static tools identify vulnerabilities by matching control and data flows against specific, pre-programmed rules, while LLMs trained on vast code corpora infer semantic intent and data flow across files without rigid rules, catching novel or complex patterns that static rule-sets miss.

### Mechanism 2: Precision-Recall Trade-off in Generative Analysis
The generative nature of LLMs introduces higher false positive rates compared to deterministic static tools. LLMs optimize for plausible continuations and comprehensive coverage, leading them to flag potential issues that are contextually impossible. Static tools, adhering to strict logic, maintain higher precision (fewer false alarms) but miss valid issues (lower recall).

### Mechanism 3: Tokenization-Induced Localization Drift
LLMs cannot reliably report precise line and column numbers for vulnerabilities due to tokenization artifacts. Source code is split into sub-word tokens before processing, and the model operates on these token sequences, struggling to map its internal token-based understanding back to exact original character/line coordinates.

## Foundational Learning

- **SARIF (Static Analysis Results Interchange Format)**: Used as the standard JSON schema to normalize output from both static tools and LLMs for comparison. *Quick check*: Can you identify which SARIF field the LLMs failed to populate correctly?

- **Precision vs. Recall (F1 Score)**: Core argument relies on LLMs having significantly higher Recall (finding more real bugs) despite lower Precision (more noise). *Quick check*: If a tool finds 100 bugs but 90 are false, does it have high recall? (Answer: Likely yes, if it caught the 10 real ones, but precision is low).

- **BPE (Byte-Pair Encoding) Tokenization**: Explains why LLMs could identify what the bug was but failed to say where (line/column) it was. *Quick check*: Why might a tokenizer struggle to map the word "SqlConnection" back to a specific column index in the raw source file?

## Architecture Onboarding

- **Component map**: Dataset (10 C# projects with 63 vulnerabilities) -> ProjectAnalyzer (custom C# tool) -> Prompts GitHub Models API (GPT-4.1, Mistral, DeepSeek) -> SARIF JSON reports for comparison with static tools
- **Critical path**: Code Injection (feed source code into LLM via system prompt) -> JSON Parsing (extract JSON array from LLM's text response) -> Verification (compare LLM findings against static tool results and ground truth)
- **Design tradeoffs**: Speed (CodeQL slower but thorough vs LLMs faster but with API costs), Assurance (static tools deterministic for high-assurance verification vs LLMs probabilistic for triage), Localization (static tools provide exact line/col vs LLMs file-level identification only)
- **Failure signatures**: Hallucinations (LLMs reporting vulnerabilities in libraries that are actually up-to-date), Coordinate Drift (LLMs reporting line 27 for a bug actually on line 15)
- **First 3 experiments**: Baseline Run (execute ProjectAnalyzer to reproduce F1 scores), Localization Audit (manually check 10 LLM reports for line number correctness), Hybrid Validation (run SnykCode against LLM's positive findings to filter false positives)

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid pipeline effectively combine the high recall of LLMs with the high precision of static tools to reduce developer verification effort? The study only evaluates tools in isolation and does not implement or test an integrated workflow where LLMs triage static tool results or vice versa.

### Open Question 2
Can architectural modifications or post-processing techniques overcome tokenization artifacts to enable precise, line-level vulnerability localization in LLMs? The paper identifies the problem but offers no solution, noting that no correlation was found in the localization errors.

### Open Question 3
To what extent does prompt engineering influence the trade-off between high recall and the false-positive rates observed in LLM security analysis? The experiments utilized a single, unified system prompt for all models, leaving the variance caused by different prompting strategies unmeasured.

## Limitations
- Small dataset (63 vulnerabilities across 10 projects) may not generalize to other languages or larger, more complex codebases
- Manual verification process introduces potential human error in ground-truth labeling
- Static tools were used with default configurations, which may not represent optimal tuning for this specific vulnerability set

## Confidence
- **High Confidence**: LLMs significantly outperforming static tools in recall and F1 scores (0.797 vs 0.260)
- **Medium Confidence**: Tokenization-induced localization drift mechanism
- **Medium Confidence**: Precision-recall trade-off explanation
- **Low Confidence**: Hybrid pipeline recommendation

## Next Checks
1. Test the same benchmark methodology on Python/JavaScript codebases to assess whether LLMs maintain their performance advantage across different programming paradigms and vulnerability types
2. Experiment with different prompt engineering strategies or post-processing techniques to improve LLM localization accuracy without sacrificing recall
3. Implement and measure the actual developer efficiency gains of a two-stage pipeline combining LLM triage with static tool verification, tracking both false positive reduction rates and time-to-fix metrics