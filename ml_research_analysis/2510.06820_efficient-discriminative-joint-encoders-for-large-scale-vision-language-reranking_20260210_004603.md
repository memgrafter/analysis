---
ver: rpa2
title: Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking
arxiv_id: '2510.06820'
source_url: https://arxiv.org/abs/2510.06820
tags:
- vision
- retrieval
- language
- tokens
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in multimodal joint
  encoders for large-scale retrieval, which stems from expensive visual feature extraction
  that prevents practical deployment. The authors introduce EDJE (Efficient Discriminative
  Joint Encoder), which precomputes vision tokens offline and compresses them using
  a lightweight attention-based adapter, allowing only a compact joint encoder to
  run online.
---

# Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking

## Quick Facts
- arXiv ID: 2510.06820
- Source URL: https://arxiv.org/abs/2510.06820
- Authors: Mitchell Keren Taraday; Shahaf Wagner; Chaim Baskin
- Reference count: 13
- Major result: EDJE processes 50k image-text pairs/second with 49kB storage per image, matching state-of-the-art retrieval performance

## Executive Summary
This paper addresses the efficiency bottleneck in multimodal joint encoders for large-scale retrieval, which stems from expensive visual feature extraction that prevents practical deployment. The authors introduce EDJE (Efficient Discriminative Joint Encoder), which precomputes vision tokens offline and compresses them using a lightweight attention-based adapter, allowing only a compact joint encoder to run online. EDJE processes 50k image-text pairs/second with 49kB storage per image, matching prior art on Flickr (zero-shot) and CO2 (fine-tuned) retrieval.

## Method Summary
The paper introduces EDJE (Efficient Discriminative Joint Encoder) to address the computational bottleneck in multimodal joint encoders for large-scale retrieval. The key innovation is precomputing visual features offline using a strong vision encoder, then compressing these features into compact representations via a token-compression adapter that uses learnable query tokens. During inference, only the lightweight joint encoder runs online, processing image-text pairs at 50k pairs/second. The token-compression adapter aggregates visual information into a compact representation, reducing storage to 49kB per image while preserving retrieval accuracy.

## Key Results
- Processes 50k image-text pairs/second with 49kB storage per image
- Matches state-of-the-art retrieval performance on Flickr (zero-shot) and COCO (fine-tuned)
- Consistently improves retrieval performance over embedding-based models
- Achieves competitive accuracy with state-of-the-art joint encoders at much higher efficiency

## Why This Works (Mechanism)
EDJE works by decoupling the expensive visual feature extraction from the online inference pipeline. By precomputing and compressing visual tokens offline, the system eliminates the need for repeated expensive vision encoder computations during retrieval. The token-compression adapter learns to aggregate visual information into a compact representation that can be efficiently combined with text representations at query time. This design maintains the discriminative power of joint encoders while achieving the efficiency of embedding-based methods.

## Foundational Learning

**Vision Token Extraction**: The process of converting images into discrete visual tokens using a pretrained vision encoder like CLIP-ViT. *Why needed*: Vision encoders are computationally expensive and cannot scale to large retrieval systems. *Quick check*: Can the vision encoder process images at acceptable throughput for your use case?

**Token Compression**: Using learnable query tokens with attention mechanisms to aggregate visual tokens into compact representations. *Why needed*: Raw visual tokens are too large (MBs) for practical storage at scale. *Quick check*: Does the compressed representation maintain retrieval accuracy compared to uncompressed features?

**Cross-modal Attention**: The mechanism by which text and image tokens interact in the joint encoder. *Why needed*: Captures fine-grained semantic relationships between vision and language. *Quick check*: Are the attention weights producing reasonable cross-modal alignments?

## Architecture Onboarding

**Component Map**: Vision Encoder -> Token Compression Adapter -> Compressed Visual Tokens -> Joint Encoder -> Cross-modal Attention -> Retrieval Scores

**Critical Path**: During inference, the critical path is Joint Encoder -> Cross-modal Attention -> Retrieval Scores, as all visual processing happens offline.

**Design Tradeoffs**: The system trades precomputation and storage (49kB/image) for massive speedups in online inference. The token-compression adapter must balance compression ratio with retrieval accuracy.

**Failure Signatures**: Poor retrieval accuracy may indicate inadequate token compression, insufficient capacity in the joint encoder, or mismatched visual features from the vision encoder.

**First Experiments**: 1) Verify offline visual feature extraction completes correctly, 2) Test token compression preserves visual information through reconstruction, 3) Validate joint encoder produces reasonable retrieval scores on small dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns for billion-scale retrieval systems despite 49kB/image storage reduction
- Limited evaluation on diverse, real-world datasets beyond standard benchmarks
- Unclear performance on long-tail retrieval scenarios and domain-specific applications

## Confidence
- High confidence in core technical contribution and benchmark efficiency gains
- Medium confidence in scalability claims due to limited production-scale evaluation
- Medium confidence in retrieval accuracy improvements as they are benchmark-dependent

## Next Checks
1. Test EDJE performance on billion-scale image datasets to verify the claimed 49kB storage per image remains practical at scale
2. Evaluate retrieval accuracy on domain-specific datasets (medical imaging, satellite imagery) to assess generalization beyond standard benchmarks
3. Measure end-to-end latency including visual feature extraction in a production-like serving environment to validate the claimed 50k pairs/second throughput