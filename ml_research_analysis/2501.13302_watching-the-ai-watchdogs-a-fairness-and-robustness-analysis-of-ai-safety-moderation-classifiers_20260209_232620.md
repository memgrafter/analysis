---
ver: rpa2
title: 'Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety
  Moderation Classifiers'
arxiv_id: '2501.13302'
source_url: https://arxiv.org/abs/2501.13302
tags:
- fairness
- moderation
- openai
- unsafe
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates fairness and robustness of four closed-source
  AI Safety Moderation (ASM) classifiers used for content moderation and LLM safety
  guardrails. The study finds that OpenAI Moderation API exhibits the highest fairness
  issues, particularly for sexual orientation, while Google Cloud Natural Language
  API performs most fairly.
---

# Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers

## Quick Facts
- arXiv ID: 2501.13302
- Source URL: https://arxiv.org/abs/2501.13302
- Authors: Akshit Achara; Anshuman Chhabra
- Reference count: 27
- Primary result: OpenAI Moderation API shows highest fairness issues, particularly for sexual orientation; GCNL performs most fairly; LLM-based perturbations most effective at bypassing ASM filters

## Executive Summary
This study audits fairness and robustness of four closed-source AI Safety Moderation (ASM) classifiers used for content moderation and LLM safety guardrails. The researchers evaluate demographic parity and conditional statistical parity across protected demographic groups, finding significant fairness disparities particularly for sexual orientation. The robustness analysis demonstrates that LLM-based paraphrasing perturbations can effectively bypass ASM filters, with minimal input modifications enabling unsafe content to evade detection. The findings reveal critical gaps in both fairness calibration and adversarial robustness that require attention in future ASM model development.

## Method Summary
The researchers evaluate four black-box ASM APIs (OpenAI Moderation API, Perspective API, Google Cloud Natural Language API, Clarifai API) using the Jigsaw toxicity dataset and Reddit-Ideology comments. Fairness is measured using demographic parity and conditional statistical parity metrics across gender, ethnicity, disability, and sexual orientation subgroups. Robustness testing employs backtranslation and LLM-based paraphrasing perturbations (via GPT-3.5-Turbo) to measure classification consistency changes. A BERT-based regard classifier determines legitimate factors for conditional statistical parity calculations. All evaluations use a 0.5 threshold for binary classification, except OpenAI which returns binary labels directly.

## Key Results
- OpenAI Moderation API exhibits the highest fairness issues, particularly for sexual orientation datasets
- Google Cloud Natural Language API performs most fairly across all evaluated groups
- LLM-based paraphrasing perturbations reduce moderation effectiveness more than backtranslation
- Small text modifications can bypass ASM filters, potentially allowing unsafe content to evade detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closed-source ASM classifiers exhibit measurable fairness disparities when evaluated across protected demographic groups, with sexual orientation showing the highest disparity.
- Mechanism: The classifiers produce different false positive rates for minority vs. majority groups because training data distributions and model architectures encode historical biases. DP and CSP metrics quantify this by measuring the absolute difference in "unsafe" prediction rates between protected and unprotected groups, controlling for legitimate factors like sentiment.
- Core assumption: The Jigsaw dataset identity labels and the regard classifier's sentiment labels accurately capture the ground truth protected attributes and legitimate factors.
- Evidence anchors:
  - [abstract] "We assess fairness using metrics such as demographic parity and conditional statistical parity"
  - [section 4] "the DP and CSP errors are higher for the Jigsaw-S.O dataset for all the ASM models"
  - [corpus] Related work "GuardEval" and "Language Models That Walk the Talk" similarly benchmark fairness in moderation systems, supporting this evaluation approach.
- Break condition: If threshold selection (0.5 vs 0.7) dramatically changes fairness rankings, the mechanism may reflect calibration issues rather than inherent model bias.

### Mechanism 2
- Claim: LLM-based paraphrasing perturbations reduce ASM classification consistency more than backtranslation, enabling unsafe content to bypass detection.
- Mechanism: GPT-3.5-Turbo paraphrases semantically preserve meaning while substituting surface-level lexical patterns (e.g., offensive words with neutral alternatives) that ASM models rely on for classification. The robustness error metric captures this as |E[C(X)] − E[C(X*)]| where X* is the perturbed input.
- Core assumption: Assumption: The perturbation pipeline preserves semantic equivalence and harmfulness. Manual evaluation found only 9.5% of harmful Jigsaw inputs became harmless post-perturbation.
- Evidence anchors:
  - [abstract] "Small changes in input text can bypass ASM filters, potentially allowing unsafe content to evade detection"
  - [section 5] "minimal LLM-based perturbations using GPT-3.5 Turbo can cause all ASM models to change their initial predictions"
  - [corpus] "Towards Inclusive Toxic Content Moderation" documents similar adversarial vulnerabilities in toxicity classifiers.
- Break condition: If ASM models are updated to use semantic embeddings rather than lexical patterns, this perturbation strategy would lose effectiveness.

### Mechanism 3
- Claim: The GCNL API's lower fairness error may result from higher false positive rates across all groups rather than genuinely fairer calibration.
- Mechanism: GCNL outputs 16 moderation labels with a broader definition of "unsafe" content. When using a 0.5 threshold, GCNL labels more comments as unsafe overall, potentially reducing relative disparity between groups while increasing absolute over-moderation.
- Core assumption: The 0.5 threshold is appropriate for all APIs despite differing label structures and score distributions.
- Evidence anchors:
  - [section 5] "the ratio of unsafe to safe comments is higher for the GCNL API as compared to the other ASM models"
  - [appendix F] "negatively labelled comments are more unsafe than other comments for all the ASM models... GCNL API... labels a significantly higher proportion of comments as Unsafe"
  - [corpus] Corpus papers do not directly address this over-moderation hypothesis; evidence is primarily internal to this study.
- Break condition: Threshold sensitivity analysis (Appendix J) showed GCNL fairness worsened at 0.7 threshold, suggesting the fairness advantage is threshold-dependent.

## Foundational Learning

- Concept: **Demographic Parity (DP) and Conditional Statistical Parity (CSP)**
  - Why needed here: These metrics quantify whether an ASM model treats different demographic groups equally. DP measures raw disparity in positive outcome rates; CSP controls for legitimate factors like negative sentiment.
  - Quick check question: If an ASM flags 30% of comments mentioning "gay" as unsafe but only 10% of comments mentioning "straight," what is the DP value?

- Concept: **Black-box Auditing of Closed-Source APIs**
  - Why needed here: The study evaluates models via API queries only, without access to weights or training data. Understanding this constraint clarifies why certain analyses (e.g., gradient-based robustness) are infeasible.
  - Quick check question: What information can and cannot be extracted from a classifier when you only have input-output access?

- Concept: **Perturbation-Based Robustness Testing**
  - Why needed here: Backtranslation and LLM-based paraphrasing are techniques to generate semantically equivalent variants. Understanding their differences helps interpret why LLM-based perturbations caused higher error rates.
  - Quick check question: Why might an LLM paraphrase be more effective than backtranslation at evading a keyword-based toxicity classifier?

## Architecture Onboarding

- Component map:
  - **Data layer**: Jigsaw (4 subdatasets) + Reddit-Ideology (manually annotated) → identity-labeled comments
  - **ASM APIs**: OpenAI (text-moderation-007), Perspective (BERT-based), GCNL (PaLM2-based), Clarifai (BERT-based) → binary safe/unsafe + per-label scores
  - **Fairness evaluation**: DP/CSP computation with regard classifier for CSP legitimate factors
  - **Robustness evaluation**: Perturbation generators (backtranslation via nlpaug, LLM paraphrase via GPT-3.5-Turbo) → robustness error metric

- Critical path:
  1. Ingest dataset → extract protected group labels
  2. Query each ASM API → collect prediction scores
  3. Apply threshold (0.5) → generate binary labels
  4. Compute DP/CSP across group splits
  5. Generate perturbed variants → re-query APIs → compute robustness error

- Design tradeoffs:
  - Threshold selection (0.5 vs 0.7): Lower thresholds increase false positives; higher thresholds may reduce fairness by changing relative group disparity
  - Single-attribute vs intersectional fairness: Single-attribute analysis may miss compounding biases (Appendix I shows DP(ethnicity) increases from 0.051 to 0.104 under intersectional evaluation)
  - Black-box limitation: Cannot perform gradient-based adversarial testing or weight analysis

- Failure signatures:
  - High DP/CSP values (>0.1) indicate fairness issues, especially for sexual orientation dataset
  - Robustness error >20% under LLM-based perturbation indicates high bypass vulnerability
  - GCNL's high over-moderation may mask fairness issues by suppressing all groups similarly

- First 3 experiments:
  1. **Fairness baseline replication**: Run the four ASM APIs on Jigsaw-Gender with threshold 0.5, compute DP and CSP, confirm OpenAI shows highest disparity and GCNL shows lowest.
  2. **Robustness probe**: Apply GPT-3.5-Turbo paraphrasing to 100 unsafe-flagged comments, measure percentage that flip to safe, verify OpenAI shows highest flip rate.
  3. **Threshold sensitivity**: Re-run fairness evaluation at threshold 0.7 for Perspective, GCNL, and Clarifai, observe whether fairness rankings change (per Appendix J, GCNL fairness worsens at higher threshold).

## Open Questions the Paper Calls Out
None

## Limitations
- Black-box API testing limits understanding of model architecture and training data characteristics
- Single-attribute fairness metrics may miss intersectional bias effects
- Fairness rankings are threshold-dependent, potentially reflecting calibration rather than fundamental bias

## Confidence

- **High confidence**: The comparative rankings of ASM models' fairness and robustness are robust to the evaluation methodology, as multiple metrics consistently show OpenAI API exhibiting the highest fairness issues and robustness vulnerabilities.
- **Medium confidence**: The absolute magnitude of fairness disparities and the specific numerical robustness error values are credible but may vary with different dataset samples or API versions.
- **Low confidence**: The interpretation that GCNL's lower fairness error results from over-moderation is speculative, as the study cannot distinguish between genuinely fairer calibration and excessive conservative flagging.

## Next Checks

1. **Threshold Sensitivity Validation**: Re-run all fairness evaluations across multiple thresholds (0.3, 0.5, 0.7, 0.9) to determine if the relative fairness rankings of ASM models remain stable or if they reflect threshold-dependent calibration artifacts.

2. **Intersectional Fairness Analysis**: Extend the fairness evaluation to include intersectional group combinations (e.g., Black women, LGBTQ+ individuals with disabilities) to assess whether fairness disparities compound across multiple protected attributes.

3. **Perturbation Generalization Test**: Evaluate the robustness of ASM models against a broader suite of adversarial techniques beyond LLM-based paraphrasing, including synonym replacement, character-level perturbations, and prompt injection attacks, to determine if the observed vulnerabilities are specific to the tested perturbation method or indicative of broader model fragility.