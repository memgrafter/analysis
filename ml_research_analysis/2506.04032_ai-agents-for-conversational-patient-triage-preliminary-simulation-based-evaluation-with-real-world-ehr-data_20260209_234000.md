---
ver: rpa2
title: 'AI Agents for Conversational Patient Triage: Preliminary Simulation-Based
  Evaluation with Real-World EHR Data'
arxiv_id: '2506.04032'
source_url: https://arxiv.org/abs/2506.04032
tags:
- patient
- simulator
- clinical
- data
- triage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a Patient Simulator that generates synthetic
  test subjects for evaluating AI-driven conversational patient triage systems, using
  real-world EHR data to create realistic patient vignettes. These vignettes are transformed
  into dynamic, multi-turn conversations with an AI triage agent, which follows a
  multi-agent architecture to collect symptoms, retrieve relevant EHR data, reason
  through differential diagnoses, and generate care recommendations.
---

# AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data

## Quick Facts
- arXiv ID: 2506.04032
- Source URL: https://arxiv.org/abs/2506.04032
- Reference count: 27
- Introduces a Patient Simulator for evaluating AI-driven conversational triage using real-world EHR data

## Executive Summary
This paper presents a Patient Simulator that generates realistic synthetic test subjects for evaluating AI-driven conversational patient triage systems using real-world EHR data. The simulator transforms structured clinical vignettes into dynamic, multi-turn conversations while ensuring responses remain grounded in clinical context without medical jargon. A multi-agent AI triage system then conducts symptom collection, EHR retrieval, differential diagnosis, and triage recommendations. Expert clinician evaluation shows the simulator achieves 97.7% consistency with vignettes and 99% case summary relevance, while the triage system correctly identifies diagnoses in the top three differentials over 95% of the time.

## Method Summary
The study uses 21,779 deidentified EHR records from HealthVerity, filtered to 519 "Initial Encounter" cases across 9 symptom categories. These cases are transformed into structured vignettes and processed through a multi-agent architecture with 8 LLM agents: SymptomCollector, HealthDataPlanner, HealthDataRetriever, Summary, Differential Diagnosis, Next Steps, Guideline Verifier, and Controller. The Patient Simulator uses a prompt from Appendix A.1 to generate natural language responses without jargon while controlling information disclosure. Expert clinicians evaluated 519 simulated encounters using a 14-question rubric, with inter-annotator agreement (Cohen's kappa: 0.72-0.79) demonstrating reliability.

## Key Results
- Patient Simulator consistency with vignettes: 97.7%
- Case summary relevance: 99%
- AI triage question precision and non-redundancy: 81.7%
- Correct diagnosis in top-3 differentials: >95%

## Why This Works (Mechanism)
The system leverages real-world EHR data to create clinically realistic scenarios that ground AI interactions in authentic patient presentations. The multi-agent architecture enables specialized handling of different triage tasks, with RAG-enabled agents accessing relevant clinical guidelines and patient history. The Patient Simulator's constraint-based prompting ensures responses remain accessible while maintaining clinical accuracy, creating a safe testing environment for AI triage systems.

## Foundational Learning
- **EHR Data Filtering and Classification**: Needed to identify relevant "Initial Encounter" cases for simulation; quick check: verify classification accuracy on held-out samples
- **Multi-agent LLM Orchestration**: Required for coordinating specialized AI agents in conversation flow; quick check: trace message passing between agents in sample conversations
- **RAG Implementation for Clinical Data**: Essential for retrieving relevant EHR information and guidelines during triage; quick check: measure retrieval precision and recall on test queries
- **Clinical Vignette Transformation**: Critical for converting structured EHR data into conversational prompts; quick check: clinician review of generated vignettes for realism
- **Constraint-based Prompting**: Necessary to prevent jargon and control information disclosure; quick check: automated detection of clinical terms in simulator responses
- **Expert Clinician Evaluation**: Required for validating system performance against clinical standards; quick check: inter-annotator agreement metrics across evaluators

## Architecture Onboarding

**Component Map**: EHR Data -> Vignette Generator -> Patient Simulator -> AI Triage System (SymptomCollector -> HealthDataPlanner -> HealthDataRetriever -> Summary -> Differential Diagnosis -> Next Steps -> Guideline Verifier) -> Evaluation

**Critical Path**: Patient Simulator generates responses -> SymptomCollector extracts symptoms -> HealthDataRetriever accesses EHR -> Differential Diagnosis generates differentials -> Guideline Verifier validates recommendations

**Design Tradeoffs**: Uses GPT-4o-mini for classification to reduce costs vs. full GPT-4 for all agents; implements progressive disclosure to maintain conversation realism while controlling information flow

**Failure Signatures**: Redundant questions (>18% rate) indicate context tracking issues; inconsistent simulator responses suggest prompt template problems; low diagnostic accuracy points to EHR retrieval or symptom extraction failures

**First Experiments**:
1. Run Patient Simulator with 10 sample vignettes and evaluate consistency with blinded clinicians
2. Test single-agent triage flow on one conversation to validate symptom collection accuracy
3. Measure retrieval precision of HealthDataRetriever on sample clinical queries

## Open Questions the Paper Calls Out
None

## Limitations
- Specific LLM assignments for triage agents beyond classifier and guideline verifier are not specified
- Vignette format and prompt template structure are only partially described
- RAG implementation details for EHR data and clinical guidelines are omitted
- Orchestration framework and agent communication protocol are not detailed

## Confidence

**High Confidence**: Patient Simulator consistency with vignettes (97.7%) and case summary relevance (99%) are well-documented through clinician evaluation with strong inter-annotator agreement (kappa 0.72-0.79)

**Medium Confidence**: Diagnostic accuracy metrics (>95% top-3 diagnoses) are credible but depend on unknown agent LLM assignments and RAG configurations that could affect differential generation

**Medium Confidence**: Question precision and non-redundancy (81.7%) is documented but may be sensitive to simulator prompt variations and context tracking implementation details not fully specified

## Next Checks
1. Implement the Patient Simulator using the Appendix A.1 prompt and evaluate consistency with 50+ EHR-derived vignettes through blinded clinician review, measuring inter-annotator agreement for consistency scoring
2. Build the multi-agent triage system with placeholder LLMs for unspecified agents, run 100 simulated conversations, and analyze diagnostic accuracy and question redundancy patterns to identify performance bottlenecks
3. Conduct ablation studies varying the Patient Simulator's symptom disclosure constraints and agent context tracking mechanisms to quantify their impact on question precision and diagnostic accuracy