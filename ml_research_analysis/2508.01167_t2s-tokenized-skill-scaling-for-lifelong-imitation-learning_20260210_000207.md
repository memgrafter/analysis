---
ver: rpa2
title: 'T2S: Tokenized Skill Scaling for Lifelong Imitation Learning'
arxiv_id: '2508.01167'
source_url: https://arxiv.org/abs/2508.01167
tags:
- learning
- tokens
- task
- tasks
- lifelong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in lifelong imitation
  learning for robotic manipulation tasks. The proposed Tokenized Skill Scaling (T2S)
  framework transforms traditional parameter mapping into token-parameter interactions,
  enabling easy model scalability through token extensions.
---

# T2S: Tokenized Skill Scaling for Lifelong Imitation Learning

## Quick Facts
- **arXiv ID**: 2508.01167
- **Source URL**: https://arxiv.org/abs/2508.01167
- **Reference count**: 36
- **Primary result**: Achieves 1.0% NBT and 77.7% FWT on LIBERO benchmarks while adding only 8.0% trainable tokens

## Executive Summary
This paper introduces Tokenized Skill Scaling (T2S), a framework that addresses catastrophic forgetting in lifelong imitation learning for robotic manipulation. The key innovation transforms traditional parameter mapping into token-parameter interactions, enabling efficient model scalability through token extensions. The framework introduces a language-guided token activation strategy that transfers knowledge across tasks while controlling parameter growth.

T2S demonstrates strong performance on three LIBERO task suites, achieving minimal forgetting (1.0% NBT) and efficient skill scaling (8.0% trainable tokens). The approach also enables effective knowledge transfer between tasks with an average FWT of 77.7%. By decoupling skill representation from parameter space through tokenization, the framework provides a scalable solution for lifelong learning in robotic manipulation tasks.

## Method Summary
T2S restructures the mapping between observations and actions by introducing token-parameter interactions rather than direct parameter mapping. The framework transforms traditional parameter spaces into token-parameter interaction spaces, where skills are represented as combinations of tokens rather than fixed parameter sets. This tokenization approach enables easy scalability - new skills can be added by extending the token vocabulary rather than retraining the entire model.

The language-guided token activation strategy uses external language models to generate skill-specific tokens, which are then activated based on task descriptions. This allows for efficient knowledge transfer between tasks by reusing and adapting existing tokens. The framework maintains parameter efficiency by keeping the core model parameters fixed while only training the newly added tokens for each new skill, resulting in minimal parameter growth during lifelong learning.

## Key Results
- Achieves 1.0% average NBT across three LIBERO task suites, demonstrating effective catastrophic forgetting mitigation
- Requires only 8.0% trainable tokens on average for new skill scaling, showing strong parameter efficiency
- Achieves 77.7% average FWT across three LIBERO task suites, indicating successful knowledge transfer between tasks

## Why This Works (Mechanism)
T2S works by decoupling skill representation from parameter space through tokenization. Instead of directly mapping observations to actions through fixed parameters, the framework represents skills as combinations of learnable tokens that interact with a shared parameter space. This separation allows new skills to be added by extending the token vocabulary without modifying existing parameters, preventing interference between tasks.

The language-guided activation mechanism provides semantic context for token selection, enabling the model to efficiently transfer knowledge between related tasks. When encountering a new task, the system can activate relevant existing tokens and supplement them with task-specific tokens, creating a flexible skill representation that scales efficiently. This approach maintains backward compatibility while enabling forward growth, addressing the core challenge of lifelong learning.

## Foundational Learning

**Catastrophic Forgetting**: The phenomenon where neural networks lose previously learned information when trained on new tasks. Critical for lifelong learning systems that must retain old skills while acquiring new ones.

**Parameter Efficiency**: The ability to achieve good performance with minimal model parameters. Essential for practical deployment where computational resources and memory are limited.

**Token-Based Representations**: Using discrete tokens as building blocks for skill representation, similar to how language models use tokens for text. Enables modular and scalable skill composition.

**Skill Transfer**: The ability to apply knowledge from previously learned tasks to new, related tasks. Key for efficient lifelong learning where each new task doesn't need to be learned from scratch.

**Quick checks**:
- Parameter count growth should remain sublinear as new skills are added
- Performance on old tasks should remain stable after learning new tasks
- Token activation should correlate with task similarity

## Architecture Onboarding

**Component Map**: Observation -> Token Encoder -> Token-Parameter Interaction Layer -> Action Decoder

**Critical Path**: The token-parameter interaction layer is the core component where new skills are integrated through token extensions while preserving existing functionality.

**Design Tradeoffs**: 
- Tokenization provides scalability but adds complexity in token management
- Language-guided activation enables semantic control but depends on language model quality
- Fixed parameter space ensures stability but may limit expressiveness for highly diverse tasks

**Failure Signatures**: 
- Poor performance on new tasks may indicate insufficient token coverage
- Degradation on old tasks suggests token interference or improper activation
- High parameter growth indicates inefficient token reuse

**First Experiments**:
1. Test single-task performance to establish baseline capability
2. Evaluate catastrophic forgetting by training on sequential tasks
3. Measure parameter growth as new skills are added

## Open Questions the Paper Calls Out

The paper acknowledges that evaluation is currently limited to short-horizon robotic manipulation tasks (1-3 steps) within the LIBERO benchmark. The scalability of the token-based approach to long-horizon or highly complex sequential tasks remains unclear. Additionally, the dependency on external language models for token generation raises questions about robustness when language descriptions are ambiguous or unavailable. The paper also notes that generalization claims beyond the LIBERO benchmark tasks and scalability to real-world robotic applications with more complex skill requirements remain largely untested.

## Limitations

- Evaluation focuses primarily on short-horizon robotic manipulation tasks (1-3 steps), leaving unclear how well the approach generalizes to long-horizon or highly complex sequential tasks
- The "minimal parameter growth" claim (8.0% trainable tokens) is specific to LIBERO task suites and may not scale similarly for more diverse robotic skill sets
- Language-guided token activation relies on external language models, introducing potential dependency on language model quality and raising questions about robustness with ambiguous or unavailable language descriptions

## Confidence

**High confidence**: Catastrophic forgetting mitigation results (NBT of 1.0%) are well-supported by LIBERO benchmark evaluation, and parameter efficiency claims (8.0% trainable tokens) are directly measurable within the tested framework.

**Medium confidence**: Knowledge transfer performance (FWT of 77.7%) shows promising results but depends heavily on language-guided token activation quality, which introduces an external dependency not fully characterized in evaluation.

**Low confidence**: Generalization claims beyond LIBERO benchmark tasks and scalability to real-world robotic applications with more complex skill requirements remain largely untested and speculative.

## Next Checks

1. Evaluate T2S on long-horizon manipulation tasks requiring 10+ sequential steps to assess whether token-based scaling maintains effectiveness for complex, multi-stage robotic operations.

2. Test the framework with degraded or ambiguous language model outputs to quantify the robustness of language-guided token activation when high-quality skill descriptions are unavailable.

3. Implement ablation studies comparing T2S against baseline methods on cross-domain robotic tasks (e.g., navigation + manipulation) to validate whether token-parameter interaction generalizes beyond manipulation-specific skills.