---
ver: rpa2
title: Model Stitching by Functional Latent Alignment
arxiv_id: '2505.20142'
source_url: https://arxiv.org/abs/2505.20142
tags:
- stitching
- functional
- similarity
- fula
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Functional Latent Alignment (FuLA), a new
  method for evaluating functional similarity between neural networks through model
  stitching. Unlike traditional approaches that rely on task-specific losses (TLM)
  or direct feature matching (DM), FuLA aligns intermediate representations while
  avoiding task-level cues that can fabricate similarity.
---

# Model Stitching by Functional Latent Alignment

## Quick Facts
- arXiv ID: 2505.20142
- Source URL: https://arxiv.org/abs/2505.20142
- Reference count: 40
- Key outcome: FuLA minimizes feature representation distances to provide a more reliable metric for functional similarity between neural networks, avoiding task-specific artifacts that can fabricate similarity.

## Executive Summary
This paper introduces Functional Latent Alignment (FuLA), a new method for evaluating functional similarity between neural networks through model stitching. Unlike traditional approaches that rely on task-specific losses (TLM) or direct feature matching (DM), FuLA aligns intermediate representations while avoiding task-level cues that can fabricate similarity. The method minimizes the distance between feature representations at both the stitching layer and subsequent layers, providing a more reliable proxy for functional comparability.

Experiments on ResNet architectures show that task-based stitching (SLM/TLM) can produce misleading results by exploiting training shortcuts or adversarial robustness cues, fabricating high functional similarity where none exists. FuLA avoids these pitfalls, capturing non-trivial alignments missed by DM and maintaining performance stability under distribution shifts, adversarial training, and class subset scenarios. Cross-layer stitching experiments confirm that directionality in functional similarity is inherent to model structure, not an artifact of task-based optimization.

Overall, FuLA emerges as a more reliable and robust metric for functional similarity, offering meaningful insights into internal network behavior without overfitting to task-specific artifacts.

## Method Summary
FuLA is a novel approach for measuring functional similarity between neural networks through model stitching. The method focuses on aligning intermediate representations of two networks by minimizing the distance between their feature representations at both the stitching layer and subsequent layers. This alignment is achieved without relying on task-specific losses, which can introduce biases and fabricate similarity. FuLA uses a reconstruction-based objective to ensure that the stitched model's outputs are consistent with both original models' feature spaces. By avoiding task-level cues, FuLA provides a more accurate and generalizable measure of functional comparability, capturing alignments that are missed by direct feature matching (DM) and task-based methods (SLM/TLM).

## Key Results
- FuLA avoids task-specific artifacts that can fabricate high functional similarity, providing a more reliable metric than SLM/TLM.
- Experiments on ResNet architectures demonstrate that FuLA captures non-trivial alignments missed by direct feature matching (DM).
- FuLA maintains performance stability under distribution shifts, adversarial training, and class subset scenarios, validating its robustness.

## Why This Works (Mechanism)
FuLA works by minimizing the distance between feature representations at the stitching layer and subsequent layers, ensuring that the stitched model's behavior is consistent with both original models. This approach avoids task-specific cues that can lead to overfitting or fabricated similarity, providing a more accurate measure of functional comparability. By focusing on the alignment of intermediate representations, FuLA captures the intrinsic structure of the models rather than relying on task-specific shortcuts or artifacts.

## Foundational Learning
- **Functional Similarity**: The degree to which two neural networks exhibit similar behavior, particularly in their intermediate representations. This concept is crucial for understanding how models generalize and align.
  - *Why needed*: To evaluate whether two models can be effectively combined or compared without relying on task-specific performance metrics.
  - *Quick check*: Compare feature representations across layers of two models trained on the same task.

- **Model Stitching**: A technique for combining two neural networks by aligning their intermediate representations at a specific layer. This allows for the transfer of learned features between models.
  - *Why needed*: To explore the functional similarity between models and understand how their internal representations align.
  - *Quick check*: Stitch two models at a given layer and evaluate the performance of the combined model on a downstream task.

- **Distribution Shifts**: Changes in the input data distribution that can affect a model's performance. Understanding how models behave under such shifts is critical for evaluating their robustness.
  - *Why needed*: To ensure that functional similarity metrics are not overfitting to specific data distributions or task-specific cues.
  - *Quick check*: Evaluate model performance on a dataset with a shifted distribution compared to the training data.

## Architecture Onboarding
- **Component Map**: Input -> ResNet A -> Stitching Layer -> FuLA Alignment -> ResNet B -> Output
- **Critical Path**: The alignment of intermediate representations at the stitching layer and subsequent layers is the core mechanism ensuring functional similarity.
- **Design Tradeoffs**: FuLA avoids task-specific losses to prevent fabricated similarity but may require more computational resources than simpler methods like DM.
- **Failure Signatures**: Task-based stitching (SLM/TLM) can produce misleading results by exploiting training shortcuts or adversarial robustness cues.
- **First Experiments**:
  1. Stitch two ResNet models at a given layer and evaluate FuLA's alignment accuracy compared to DM and SLM/TLM.
  2. Test FuLA's robustness under distribution shifts by evaluating performance on a shifted dataset.
  3. Explore cross-layer stitching to confirm that directionality in functional similarity is inherent to model structure.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is confined to ResNet variants on ImageNet, leaving open questions about FuLA's behavior on other architectures (e.g., Transformers, ViTs) or domains (NLP, multimodal).
- The claim that FuLA is "more reliable" rests on comparisons within a single family of models; generalization to broader model spaces is untested.
- The paper does not explore the computational overhead of FuLA relative to simpler methods, which could be a practical constraint.

## Confidence
- **High**: Core claims about FuLA's superiority over task-based stitching within the studied setting, as results are consistent and methodology is rigorous.
- **Medium**: Claims about robustness to distribution shifts and cross-layer stitching, since these are demonstrated but not exhaustively probed.
- **Low**: Extrapolation to other architectures or tasks, given the narrow experimental scope.

## Next Checks
1. Test FuLA on non-convolutional architectures (e.g., ViTs, MLPs) to assess generality.
2. Evaluate computational cost and scalability relative to DM and SLM/TLM on larger datasets.
3. Investigate FuLA's sensitivity to different feature distances (e.g., cosine vs. Euclidean) and alignment strategies.