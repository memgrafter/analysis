---
ver: rpa2
title: 'STABLE: Gated Continual Learning for Large Language Models'
arxiv_id: '2510.16089'
source_url: https://arxiv.org/abs/2510.16089
tags:
- gating
- forgetting
- continual
- lora
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses catastrophic forgetting in large language
  models undergoing continual adaptation through LoRA-based fine-tuning. The authors
  introduce STABLE, a gated continual self-editing framework that evaluates each LoRA
  update against a user-defined stability budget using one of three metrics: exact
  match (EM) drop, bits increase, or KL divergence.'
---

# STABLE: Gated Continual Learning for Large Language Models

## Quick Facts
- arXiv ID: 2510.16089
- Source URL: https://arxiv.org/abs/2510.16089
- Reference count: 40
- Primary result: EM-based gating achieves highest cumulative performance gain (+0.397) across eight sequential editing steps while maintaining comparable distributional drift to other gating strategies

## Executive Summary
This paper addresses catastrophic forgetting in large language models undergoing continual adaptation through LoRA-based fine-tuning. The authors introduce STABLE, a gated continual self-editing framework that evaluates each LoRA update against a user-defined stability budget using one of three metrics: exact match (EM) drop, bits increase, or KL divergence. If the budget is exceeded, updates are either rescaled through LoRA clipping or rejected. Experiments on Qwen-2.5-7B demonstrate that EM-based gating achieves the highest cumulative performance gain while maintaining comparable distributional drift to other gating strategies.

## Method Summary
STABLE introduces a gating mechanism for continual learning that evaluates LoRA updates against a stability budget using three possible metrics: exact match drop, bits increase, or KL divergence. When an update exceeds the budget, the framework either rescales the update through LoRA clipping or rejects it entirely. The framework enables LLMs to integrate new knowledge while preserving previously acquired information through principled budget management. The approach was evaluated on Qwen-2.5-7B across eight sequential editing steps, comparing the effectiveness of different gating metrics on both task performance and distributional drift.

## Key Results
- EM-based gating achieved the highest cumulative performance gain (+0.397) across eight sequential editing steps
- Different gating metrics produced similar distributional drift levels but different task performance outcomes
- KL divergence maintained comparable distributional drift across all gating strategies while EM gating optimized for task-specific performance

## Why This Works (Mechanism)
The framework works by implementing a checkpoint-based evaluation system where each LoRA update is assessed against a user-defined stability budget before application. The gating mechanism acts as a quality control filter, preventing updates that would cause excessive drift from previously learned knowledge. By offering three different evaluation metrics (EM drop, bits increase, KL divergence), the system allows users to prioritize either task performance or distributional stability based on their specific needs. The LoRA clipping mechanism provides a middle ground when updates exceed the budget but still contain valuable information.

## Foundational Learning
- Continual Learning - Needed to understand how models forget previous tasks when learning new ones; Quick check: Can the model maintain performance on Task 1 after learning Task 2?
- Catastrophic Forgetting - Critical for recognizing the core problem being solved; Quick check: Does performance on previously learned tasks degrade after new updates?
- LoRA Fine-tuning - Essential for understanding the adaptation mechanism used; Quick check: Are updates applied efficiently without full model retraining?
- KL Divergence - Important for measuring distributional drift between model versions; Quick check: How much does the probability distribution change after updates?
- Exact Match Evaluation - Key for task-specific performance measurement; Quick check: Does the model correctly answer questions it previously knew?

## Architecture Onboarding
- Component map: Input Task -> Gating Evaluation (EM/Bits/KL) -> Stability Budget Check -> LoRA Clipping/Reject -> Updated Model
- Critical path: Task Input → Gating Evaluation → Budget Decision → Model Update/Reject
- Design tradeoffs: EM gating optimizes task performance but may allow more drift; KL divergence prioritizes stability but may miss task-specific improvements
- Failure signatures: Updates consistently rejected (budget too strict); Performance degradation (budget too lenient); Computational overhead (frequent clipping)
- First experiments: 1) Baseline performance without gating; 2) Compare all three gating metrics on same task sequence; 3) Vary stability budget thresholds to find optimal balance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single model architecture (Qwen-2.5-7B) and dataset setup
- Stability budget relies on user-defined thresholds introducing potential subjectivity
- Computational overhead and adaptation latency not explored
- Long-term stability beyond eight sequential steps untested

## Confidence
- Claims about EM-based gating effectiveness: High
- Claims about controlling distributional drift: High
- Claims about generalizability across models and domains: Medium
- Claims about robustness to varying budgets and real-world deployment: Low

## Next Checks
1. Test STABLE framework across multiple model architectures (Llama, Mistral) and scales (1B to 70B parameters)
2. Conduct experiments with ambiguous or overlapping task boundaries for realistic continual learning scenarios
3. Measure computational overhead and adaptation latency introduced by gating mechanism under varying stability budgets