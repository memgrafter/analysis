---
ver: rpa2
title: 'Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models
  via History-Aware Residual Guidance'
arxiv_id: '2602.01047'
source_url: https://arxiv.org/abs/2602.01047
tags:
- decoding
- language
- image
- large
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Residual Decoding (ResDec) addresses hallucinations in Large Vision-Language
  Models (LVLMs) by leveraging historical token distributions during decoding. The
  method analyzes the evolution of token logits over time, identifying stable semantic
  regions using Jensen-Shannon Divergence (JSD).
---

# Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance

## Quick Facts
- arXiv ID: 2602.01047
- Source URL: https://arxiv.org/abs/2602.01047
- Authors: Xinrong Chen; Xu Chu; Yingmin Qiu; Hengyuan Zhang; Jing Xiong; Shiyu Tang; Shuai Liu; Shaokang Yang; Cheng Yang; Hayden Kwok-Hay So; Ngai Wong
- Reference count: 40
- Primary result: ResDec achieves state-of-the-art hallucination mitigation with zero-cost guidance, improving accuracy by 7.84% and F1 by 8.01% on POPE, and reducing object hallucinations by 26.44% on CHAIR.

## Executive Summary
Residual Decoding (ResDec) addresses the critical challenge of hallucinations in Large Vision-Language Models (LVLMs) by leveraging historical token distributions during the decoding process. The method identifies stable semantic regions through Jensen-Shannon Divergence analysis and uses these as a basis for residual guidance signals. This approach effectively corrects language prior biases without requiring additional training or inference overhead, making it a practical solution for improving LVLM reliability.

## Method Summary
ResDec analyzes the evolution of token logits during decoding to identify stable semantic regions using Jensen-Shannon Divergence (JSD). It aggregates logits from these stable regions with confidence weights to form a residual guidance signal, which is then fused into the current decoding logits. This history-aware residual guidance corrects language prior biases while maintaining the model's ability to generate contextually appropriate responses. The method operates entirely during inference without additional training requirements, making it computationally efficient and broadly applicable across different LVLM architectures.

## Key Results
- Improves accuracy by 7.84% and F1 by 8.01% on POPE hallucination benchmark
- Reduces object hallucinations by 26.44% on CHAIR benchmark
- Maintains strong performance on comprehensive multimodal benchmarks while achieving state-of-the-art hallucination mitigation

## Why This Works (Mechanism)
The method works by exploiting the observation that token distributions tend to stabilize around semantically meaningful regions during decoding. By tracking the evolution of these distributions and identifying when they become stable (using JSD), ResDec can determine which regions of the decoding process are producing reliable outputs. The residual guidance signal, derived from these stable regions, helps correct biases that would otherwise lead to hallucinations, particularly those stemming from language priors that favor frequent but potentially incorrect tokens.

## Foundational Learning
- **Jensen-Shannon Divergence (JSD)**: A symmetric measure of similarity between probability distributions, used here to identify stable token distributions. Why needed: To quantitatively determine when token distributions have stabilized during decoding. Quick check: Verify that JSD values decrease over time for stable regions and spike for unstable regions.
- **Token Logit Evolution**: The temporal progression of token prediction scores during autoregressive decoding. Why needed: To track how the model's confidence and predictions change over time. Quick check: Plot logit trajectories for different tokens to observe stabilization patterns.
- **Residual Guidance Signals**: Additional information injected into the decoding process to correct or enhance predictions. Why needed: To provide corrective feedback without modifying the base model architecture. Quick check: Measure the impact of guidance strength on hallucination reduction versus preservation of creative generation.

## Architecture Onboarding

**Component Map**: Input Image → Vision Encoder → Cross-Modal Fusion → Decoder → (ResDec Module: JSD Analysis → Stability Detection → Logit Aggregation → Residual Fusion) → Output Text

**Critical Path**: The ResDec module operates during the decoder's autoregressive generation loop, analyzing token logits at each timestep, computing JSD against historical distributions, detecting stability, aggregating logits from stable regions, and fusing the residual signal back into the current timestep's logits before final token selection.

**Design Tradeoffs**: The method trades a small amount of additional computation during inference (for JSD calculations and logit aggregation) against significant improvements in output reliability. The zero-training requirement makes it broadly applicable but may limit optimization for specific domains. The history window size represents a key hyperparameter balancing responsiveness to new information against stability detection accuracy.

**Failure Signatures**: Potential failures include false stability detection in ambiguous visual contexts, over-correction leading to overly conservative or repetitive outputs, and sensitivity to hyperparameter choices (history window size, JSD thresholds). The method may struggle with novel visual concepts where stable distributions don't form as expected.

**3 First Experiments**:
1. Run ResDec on a simple image captioning task with clear ground truth to verify basic functionality and measure hallucination reduction.
2. Test with varying history window sizes to determine optimal parameter settings for different image types.
3. Evaluate on adversarial images designed to trigger hallucinations to assess robustness limits.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The method's reliance on stable token distributions may not generalize well to all visual contexts or specialized domains where semantic stability patterns differ from standard benchmarks.
- The Jensen-Shannon Divergence metric and its associated hyperparameters (history window size, stability thresholds) may require careful tuning for different model architectures and dataset types.
- While effective on tested benchmarks, the approach's performance on highly ambiguous or adversarial visual inputs remains unexplored, potentially limiting its reliability in challenging real-world scenarios.

## Confidence
**High Confidence**: The experimental results demonstrating improved accuracy and reduced hallucinations on the tested benchmarks appear robust and well-validated. The zero-cost nature of the approach is clearly demonstrated through empirical evidence.

**Medium Confidence**: The generalizability of the method to diverse real-world scenarios and specialized domains remains uncertain. While performance is maintained on general multimodal benchmarks, extensive testing across varied contexts is needed.

**Medium Confidence**: The theoretical foundation for using historical token distributions as indicators of semantic stability is plausible but could benefit from more rigorous analysis of failure modes and edge cases.

## Next Checks
1. Test the method's robustness on adversarial visual examples or intentionally ambiguous images to assess its limits in challenging scenarios and identify potential failure modes.

2. Evaluate performance across diverse domains (medical imaging, satellite imagery, etc.) to verify generalizability beyond standard benchmarks and determine if domain-specific tuning is required.

3. Conduct ablation studies to determine optimal history window size and JSD threshold parameters across different model architectures and dataset types, establishing guidelines for practical deployment.