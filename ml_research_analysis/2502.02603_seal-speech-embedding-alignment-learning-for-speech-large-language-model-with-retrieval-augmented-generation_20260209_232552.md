---
ver: rpa2
title: 'SEAL: Speech Embedding Alignment Learning for Speech Large Language Model
  with Retrieval-Augmented Generation'
arxiv_id: '2502.02603'
source_url: https://arxiv.org/abs/2502.02603
tags:
- speech
- text
- retrieval
- embedding
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of latency and error propagation
  in speech-based retrieval-augmented generation (RAG) systems for speech large language
  models (SLLMs). Traditional two-stage systems that combine automatic speech recognition
  (ASR) with text-based retrieval suffer from high latency and error propagation issues.
---

# SEAL: Speech Embedding Alignment Learning for Speech Large Language Model with Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2502.02603
- Source URL: https://arxiv.org/abs/2502.02603
- Authors: Chunyu Sun; Bingyu Liu; Zhichao Cui; Junhan Shi; Anbin Qi; Tian-hao Zhang; Dinghao Zhou; Lewei Lu
- Reference count: 7
- Primary result: Unified speech-text embedding framework achieves 86.36% Top-1 accuracy with 50% latency reduction vs two-stage ASR systems

## Executive Summary
SEAL addresses the latency and error propagation issues in speech-based retrieval-augmented generation (RAG) systems by eliminating intermediate text representations. The method uses separate speech and text encoders with a shared scaling layer to map both modalities into a common semantic embedding space. Through a two-stage training strategy involving speech-text alignment and task-specific fine-tuning, SEAL achieves higher retrieval accuracy while reducing pipeline latency by 50% compared to traditional ASR-based approaches.

## Method Summary
SEAL employs a unified end-to-end framework that directly maps speech to document embeddings without intermediate text representations. The architecture uses Whisper-large-v3 as the speech encoder and Piccolo-large-zh-v2 as the text encoder, connected through an adaptation module (Conv1D + MLP) and a shared scaling layer. The training follows a two-stage approach: Stage 1 aligns acoustic features to semantic space using MSE loss between adapted speech features and text embeddings, while Stage 2 optimizes retrieval performance using contrastive learning objectives. This eliminates the cascading errors and high latency of traditional two-stage ASR + text retrieval systems.

## Key Results
- Achieves 86.36% Top-1 accuracy and 92.47% Top-3 accuracy on knowledge base retrieval tasks
- Reduces pipeline latency by 50% compared to two-stage ASR systems
- Outperforms two-stage ASR approach across classification, pairwise classification, reranking, and retrieval tasks
- Performance falls between ASR pipelines and text-only models (90.41% Top-1 accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Speech-text alignment pre-training reduces the modality gap between acoustic and semantic representations. The adaptation module (Conv1D + MLP) transforms variable-length speech features to match text embedding dimensions, then MSE loss minimizes the distance between aligned speech features and text embeddings at the token level. This works because speech encoders trained on ASR tasks capture semantic information that can be mapped to text embedding spaces through learned projections. If speech pre-trained models don't capture sufficient semantic content (e.g., purely acoustic models), alignment will fail.

### Mechanism 2
Eliminating intermediate text representations prevents error propagation and reduces latency. Direct speech-to-document embedding retrieval bypasses ASR transcription entirely, avoiding cascading errors where ASR mistakes on key terms cause irrelevant document retrieval. This works because the unified embedding space preserves enough semantic information for accurate matching without explicit text conversion. If the embedding space fails to capture fine-grained semantic distinctions, retrieval accuracy degrades.

### Mechanism 3
Two-stage training strategy is necessary; either stage alone produces significantly degraded performance. Stage 1 aligns acoustic features to semantic space; Stage 2 optimizes retrieval-specific objectives with contrastive learning. Without Stage 1, speech encoders focus on acoustic rather than semantic information. If speech pre-trained models have strong acoustic biases, they must be explicitly re-aligned before task-specific fine-tuning.

## Foundational Learning

- **Cross-modal embedding alignment**: Understanding how different modalities can be mapped to shared vector spaces is prerequisite to comprehending the architecture. Quick check: Can you explain why MSE loss between speech and text token embeddings creates semantic alignment rather than just acoustic-textural matching?

- **Contrastive learning (InfoNCE loss)**: The fine-tuning stage uses contrastive objectives to optimize retrieval performance. Quick check: In InfoNCE loss, what happens to the gradient signal when negative samples are too easy or too hard?

- **Error propagation in cascaded systems**: The core motivation is eliminating ASR-induced errors in downstream retrieval. Quick check: Why does cascaded ASR→retrieval amplify errors compared to end-to-end approaches?

## Architecture Onboarding

- **Component map**: Speech input → Whisper encoder → Conv1D → MLP → Shared Linear → Query embedding → Cosine similarity with document embeddings

- **Critical path**: Speech input → Whisper encoder → Conv1D → MLP → Shared Linear → Query embedding → Cosine similarity with document embeddings

- **Design tradeoffs**: Pre-trained encoder choice (Whisper outperforms Hubert/SenseVoice due to larger-scale pre-training data); Direct projection to text space (LLaVA-style) failed (24.49% Top-1) vs shared scaling (86.36%); CTC alignment (frame-level CTC loss underperforms 78.08%) due to rigidity in semantic tasks

- **Failure signatures**: Stage 2 only training (32.19% Top-1); Projection-to-text baseline (24.49% Top-1); Long speech segments (CTC-based approaches degrade significantly)

- **First 3 experiments**: 1) Reproduce ablation: Train Stage 1 only, Stage 2 only, and both stages on a small dataset (10k hours) to validate two-stage necessity claim; 2) Encoder substitution: Replace Whisper with Hubert to confirm pre-trained model impact on semantic alignment quality; 3) Projection baseline comparison: Implement the LLaVA-style projection baseline to understand why direct projection fails (hypothesis: sequence length mismatch)

## Open Questions the Paper Calls Out
None

## Limitations
- Domain Generalization: Evaluation focuses on Chinese language data and limited-domain knowledge bases; performance on diverse accents, languages, and open-domain retrieval remains untested
- Computational Overhead: Absolute latency and computational requirements of SEAL itself are not reported; scalability to real-time applications with longer speech segments is unclear
- Knowledge Base Size Dependency: Results are reported on knowledge bases with "tens of thousands of entries"; scaling behavior for larger corpora is not explored

## Confidence
- **High Confidence**: Two-stage training necessity, superiority of Whisper encoder, general latency reduction claim (50% reduction)
- **Medium Confidence**: Specific architectural choices, relative performance compared to text-only models, scaling behavior with knowledge base size
- **Low Confidence**: Cross-lingual generalization, performance on open-domain retrieval, real-time latency in production settings, behavior with significantly longer speech segments

## Next Checks
1. **Cross-Accent Evaluation**: Test SEAL on accented speech data (multiple English accents from Common Voice or accented Chinese datasets) to measure robustness beyond standard Mandarin
2. **Large-Scale Retrieval Benchmark**: Evaluate on a large-scale retrieval benchmark (e.g., BEIR or MS MARCO with speech inputs) with knowledge bases exceeding 100K documents
3. **End-to-End System Integration**: Deploy SEAL in a complete RAG pipeline with an LLM backend, measuring total system latency from speech input to generated response