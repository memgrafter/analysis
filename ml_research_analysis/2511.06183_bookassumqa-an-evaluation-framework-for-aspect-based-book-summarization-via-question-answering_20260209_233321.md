---
ver: rpa2
title: 'BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via
  Question Answering'
arxiv_id: '2511.06183'
source_url: https://arxiv.org/abs/2511.06183
tags:
- summaries
- summarization
- aspect-based
- text
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes BookAsSumQA, a QA-based evaluation framework
  for aspect-based book summarization. It constructs a narrative knowledge graph from
  books and generates aspect-specific QA pairs to assess how well summaries capture
  information related to specific aspects (e.g., genres).
---

# BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering

## Quick Facts
- arXiv ID: 2511.06183
- Source URL: https://arxiv.org/abs/2511.06183
- Authors: Ryuhei Miyazato; Ting-Ruen Wei; Xuyang Wu; Hsin-Tai Wu; Kei Harada
- Reference count: 7
- The paper proposes BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization

## Executive Summary
This paper introduces BookAsSumQA, a novel evaluation framework for aspect-based book summarization that leverages question-answering to assess summary quality. The framework constructs a narrative knowledge graph from books and generates aspect-specific QA pairs to evaluate how well summaries capture information related to specific aspects (e.g., genres). The approach addresses the lack of benchmarks for aspect-based book summarization by providing a systematic method to evaluate summaries based on their ability to answer relevant questions.

The evaluation methodology uses automatic metrics (ROUGE-1, METEOR, and BERTScore) to measure the overlap between generated answers and reference answers from the knowledge graph. Experiments demonstrate that LLM-based methods perform better on shorter texts, while RAG-based methods become more effective as document length increases. The framework is validated on 30 books with aspect-specific annotations, showing that the automatically generated QA pairs align well with human annotations (94% match rate).

## Method Summary
The BookAsSumQA framework operates by first constructing a narrative knowledge graph from book content, extracting entities and relationships to create a structured representation of the narrative. For each specified aspect (such as genre or character traits), the framework generates relevant questions based on the knowledge graph. Summaries are then evaluated by attempting to answer these aspect-specific questions using the generated summaries. The evaluation uses ROUGE-1, METEOR, and BERTScore metrics to measure the quality of answers derived from summaries, comparing them against reference answers extracted from the knowledge graph. The framework automatically generates QA pairs, which are validated against human-annotated pairs to ensure quality.

## Key Results
- LLM-based summarization methods outperform RAG-based methods on shorter documents in aspect-based book summarization
- RAG-based methods become more effective than LLM-based methods as document length increases
- Automatically generated QA pairs align with human annotations in 94% of cases
- The framework successfully captures aspect-specific information in book summaries across multiple evaluation metrics

## Why This Works (Mechanism)
The framework works by transforming the summarization evaluation problem into a question-answering task, which provides a more structured and objective way to assess aspect-specific information retention. By constructing a narrative knowledge graph, the system creates a comprehensive representation of the book's content that can be systematically queried. The aspect-specific QA pairs ensure that evaluation focuses on relevant information for each target aspect, rather than general summary quality. This approach allows for automated, reproducible evaluation that scales better than manual assessment while maintaining alignment with human judgment through the validation of generated questions.

## Foundational Learning

**Narrative Knowledge Graphs**
- Why needed: Provides structured representation of book content for systematic querying
- Quick check: Verify entity extraction accuracy and relationship completeness in the graph

**Aspect-Based Evaluation**
- Why needed: Ensures summaries are assessed on their ability to capture specific information dimensions
- Quick check: Confirm aspect-specific questions align with human-defined summary criteria

**QA-Based Summarization Assessment**
- Why needed: Transforms subjective evaluation into objective answer quality measurement
- Quick check: Validate that answer quality correlates with human-perceived summary quality

## Architecture Onboarding

**Component Map**
Book Text -> Knowledge Graph Construction -> QA Pair Generation -> Summary Generation -> Answer Extraction -> Evaluation Metrics (ROUGE/METEOR/BERTScore)

**Critical Path**
The most critical path is: Knowledge Graph Construction -> QA Pair Generation -> Answer Extraction. The quality of the knowledge graph directly impacts the relevance and accuracy of generated questions, which in turn determines the validity of the evaluation. If the knowledge graph misses key entities or relationships, the entire evaluation framework's effectiveness is compromised.

**Design Tradeoffs**
The framework trades comprehensive manual evaluation for automated, scalable assessment. While this enables evaluation of larger datasets and faster iteration, it introduces potential noise from automatically generated QA pairs. The choice of automatic metrics (ROUGE, METEOR, BERTScore) provides computational efficiency but may miss nuanced aspects of summary quality that human evaluators would catch.

**Failure Signatures**
Primary failure modes include: (1) Inaccurate knowledge graph construction leading to irrelevant or incorrect QA pairs, (2) Summary answers that are factually correct but miss the aspect-specific focus intended by the questions, (3) Over-reliance on surface-level lexical overlap in automatic metrics rather than semantic understanding, and (4) Inability to capture narrative coherence and flow that are important for book summaries.

**3 First Experiments**
1. Evaluate the knowledge graph construction accuracy by comparing extracted entities and relationships against human annotations on a subset of books
2. Test the alignment between automatically generated QA pairs and human-annotated question-answer pairs across different book genres
3. Compare the framework's evaluation scores against human ratings of summary quality for the same set of summaries to establish correlation

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The framework relies on automatically generated QA pairs, which may introduce systematic biases despite 94% alignment with human annotations
- Experiments are conducted on a relatively small dataset of 30 books, limiting generalizability across diverse book genres and writing styles
- The evaluation primarily uses automatic metrics without substantial human validation to confirm alignment with human notions of summary quality
- The approach assumes that answering aspect-specific questions comprehensively captures summary quality, which may not account for narrative coherence and other qualitative dimensions

## Confidence

**High Confidence:** The observation that LLM-based methods perform better on shorter texts while RAG-based methods excel with longer documents is well-supported by experimental results. The technical implementation of the QA-based evaluation framework appears sound.

**Medium Confidence:** The claim that the framework is "more practical for aspect-based book summarization" compared to existing methods needs further validation across diverse conditions and book types.

**Low Confidence:** The assertion that this framework addresses the "lack of benchmarks" for aspect-based book summarization may be overstated, as the paper acknowledges related work in this area.

## Next Checks
1. Conduct human evaluation studies comparing the QA-based framework's judgments against human assessments of summary quality across different aspect types and book genres
2. Test the framework's scalability by applying it to a larger corpus of books (minimum 100+) with diverse characteristics, including different languages and writing styles
3. Validate whether the framework can distinguish between summaries that answer questions correctly versus summaries that capture the appropriate level of detail and narrative coherence for aspect-based summarization