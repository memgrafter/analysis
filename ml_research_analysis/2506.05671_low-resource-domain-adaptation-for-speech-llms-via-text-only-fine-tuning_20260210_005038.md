---
ver: rpa2
title: Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning
arxiv_id: '2506.05671'
source_url: https://arxiv.org/abs/2506.05671
tags:
- speech
- domain
- text
- adaptation
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting Speech LLMs to new
  domains in low-resource settings where paired speech-text data is scarce. The authors
  propose a text-only fine-tuning strategy that adapts the LLM component of a Speech
  LLM using only unpaired target-domain text while preserving speech-text alignment
  through real-time evaluation during training.
---

# Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning

## Quick Facts
- arXiv ID: 2506.05671
- Source URL: https://arxiv.org/abs/2506.05671
- Reference count: 40
- This paper addresses the challenge of adapting Speech LLMs to new domains in low-resource settings where paired speech-text data is scarce.

## Executive Summary
This paper addresses the challenge of adapting Speech LLMs to new domains in low-resource settings where paired speech-text data is scarce. The authors propose a text-only fine-tuning strategy that adapts the LLM component of a Speech LLM using only unpaired target-domain text while preserving speech-text alignment through real-time evaluation during training. Experiments on LibriSpeech, SlideSpeech, and Medical datasets demonstrate that this approach achieves competitive recognition performance with minimal degradation compared to full audio-text fine-tuning. The method also improves generalization to new domains without catastrophic forgetting, highlighting the potential of text-only fine-tuning for low-resource domain adaptation of ASR.

## Method Summary
The proposed approach leverages the fact that modern Speech LLMs have separate speech encoder and LLM decoder components. The method fine-tunes only the LLM component using unpaired target-domain text while preserving the speech-text alignment learned during pre-training. A real-time evaluation mechanism monitors speech recognition performance during text-only fine-tuning to prevent degradation of the speech-text alignment. The approach operates in two stages: first, the speech encoder is frozen while the LLM is fine-tuned on target-domain text; second, the entire model is evaluated on a small validation set of paired speech-text data to ensure alignment quality is maintained.

## Key Results
- Text-only fine-tuning achieves competitive recognition performance with minimal degradation compared to full audio-text fine-tuning
- The method successfully adapts Speech LLMs to new domains (SlideSpeech, Medical) using only unpaired text data
- Avoids catastrophic forgetting while improving generalization to new domains

## Why This Works (Mechanism)
The approach works by exploiting the modular architecture of Speech LLMs, where the speech encoder learns robust speech-text representations during pre-training, while the LLM component can be adapted to domain-specific language patterns through text-only fine-tuning. The real-time evaluation mechanism acts as a safeguard, preventing the LLM from drifting too far from the speech-text alignment learned during pre-training. This preserves the model's ability to map speech features to text while allowing it to learn domain-specific language patterns from unpaired text.

## Foundational Learning
- **Speech-text alignment**: The learned relationship between acoustic features and text representations in Speech LLMs. Why needed: Critical for maintaining ASR performance during adaptation. Quick check: Validate alignment by measuring word error rate on held-out paired data during fine-tuning.
- **Domain-specific language patterns**: The characteristic vocabulary, syntax, and phrasing unique to particular domains (medical, technical presentations). Why needed: Domain adaptation requires learning these patterns to improve recognition accuracy. Quick check: Measure perplexity on domain-specific text before and after adaptation.
- **Catastrophic forgetting**: The phenomenon where models lose previously learned capabilities when trained on new tasks. Why needed: Understanding this helps design methods to preserve speech-text alignment. Quick check: Compare performance on source domain before and after adaptation.

## Architecture Onboarding
**Component Map:** Speech Encoder -> LLM Decoder -> Real-time Evaluator
**Critical Path:** Speech input → Speech Encoder → LLM Decoder → Text output → Real-time Evaluator → Feedback to LLM Decoder
**Design Tradeoffs:** Text-only fine-tuning vs. paired data fine-tuning (speed and data efficiency vs. potential performance gap), real-time evaluation frequency (monitoring overhead vs. risk of degradation), frozen encoder vs. joint fine-tuning (preservation of alignment vs. potential for better adaptation)
**Failure Signatures:** Increased word error rate on validation set, degraded performance on source domain, generation of nonsensical text sequences, failure to adapt to domain-specific terminology
**First Experiments:** 1) Validate that text-only fine-tuning doesn't degrade WER on validation set, 2) Compare domain-specific perplexity before/after adaptation, 3) Test generalization to completely new domain

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation is primarily conducted on relatively clean speech datasets, with limited evaluation on truly challenging real-world low-resource conditions
- The approach assumes access to some amount of target-domain text data, though this requirement is modest compared to paired data needs
- Claims about avoiding catastrophic forgetting need more rigorous validation through longer-term forgetting studies

## Confidence
**High confidence:** The core methodology of text-only fine-tuning combined with real-time evaluation is technically sound and the reported recognition performance improvements are well-supported by the experimental results.
**Medium confidence:** Claims about avoiding catastrophic forgetting and generalization to new domains, while supported by the presented experiments, would benefit from broader domain coverage and longer-term stability analysis.
**Medium confidence:** The assertion that this approach is particularly suitable for low-resource settings is demonstrated but could be strengthened with more diverse low-resource scenarios and resource-constrained system comparisons.

## Next Checks
1. Evaluate the approach on truly low-resource scenarios with minimal target-domain text (e.g., <1000 utterances) to validate the low-resource claims under extreme conditions.
2. Conduct cross-lingual experiments to assess whether the text-only adaptation generalizes across languages, particularly for language pairs with different script systems.
3. Perform ablation studies isolating the contributions of the real-time evaluation component versus text-only fine-tuning alone to better understand which aspects drive the performance improvements.