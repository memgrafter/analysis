---
ver: rpa2
title: 'Towards Universal Solvers: Using PGD Attack in Active Learning to Increase
  Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE
  Solvers'
arxiv_id: '2510.18989'
source_url: https://arxiv.org/abs/2510.18989
tags:
- solver
- loss
- neural
- training
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the poor out-of-distribution generalization
  of neural operators for PDEs. A differentiable spectral solver supervises a compact
  Fourier neural operator (FNO) student while a PGD-style adversarial loop searches
  for worst-case input perturbations under smoothness and energy constraints.
---

# Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers

## Quick Facts
- **arXiv ID:** 2510.18989
- **Source URL:** https://arxiv.org/abs/2510.18989
- **Reference count:** 40
- **Primary result:** Differentiable solver gradients and adversarial distillation substantially improve neural operator OOD robustness on 1D Burgers and 2D Navier-Stokes.

## Executive Summary
This work addresses the poor out-of-distribution generalization of neural operators for PDEs. A differentiable spectral solver supervises a compact Fourier neural operator (FNO) student while a PGD-style adversarial loop searches for worst-case input perturbations under smoothness and energy constraints. Differentiable solver gradients enable stable adversarial search in function space, unlike prior dictionary-approximation methods. Experiments on 1D Burgers and 2D Navier-Stokes show adversarially guided distillation substantially improves OOD robustness while retaining low parameter count and fast single-shot inference, enabling more practical universal PDE surrogates.

## Method Summary
The approach uses a differentiable spectral PDE solver (Exponax) as teacher and a compact FNO as student. Training begins with FNO on nominal Gaussian Random Field data. A PGD attack then searches for perturbations that maximize student-teacher error under L2 constraints, with gradients backpropagated through the solver. Discovered worst-case samples are added to training data via round-by-round (active learning) or batch-by-batch (adversarial training) schemes. This process repeats, improving OOD robustness while maintaining low parameter count and fast inference.

## Key Results
- Differentiable solver gradients yield more effective adversarial perturbations than dictionary-based approximation methods
- Round-by-round adversarial distillation improves OOD robustness across GRF kernels, correlation lengths, and value ranges
- Solver-in-the-loop constraints prevent adversarial drift into non-physical regions while maintaining physical plausibility

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Solver Gradient Enables Accurate Adversarial Search
- Propagating gradients through the differentiable numerical solver yields more effective adversarial sample discovery than dictionary-based approximation.
- The full gradient ∇_aℓ(a) = 2(J_G(a) − J_g(a))^T(G(a) − g(a)) accounts for how both the neural operator G and the solver g respond to input perturbations.
- Prior approaches using dictionary lookup for solver outputs effectively set J_g(a) ≈ 0, mischaracterizing worst-case behavior near sensitive regimes.

### Mechanism 2: Adversarial Expansion of Training Support in Function Space
- PGD-discovered worst-case inputs expand the effective training distribution, improving OOD robustness.
- The minimax formulation forces the student to minimize error on adversarially chosen neighbors of training inputs.
- Inner maximization (PGD) finds perturbations maximizing student-teacher discrepancy under L2 or L∞ constraints with smoothness/energy bounds.

### Mechanism 3: Solver-in-the-Loop Stabilizes Gradient-Based Mining Under Physics Constraints
- Integrating the solver into the computational graph enforces physics-consistent gradient flow, preventing adversarial drift into non-physical regions.
- Differentiable spectral solvers backpropagate through time-stepping and spectral operations, with periodic boundary conditions and spectral smoothness implicitly constraining perturbation directions.
- This results in physically plausible adversarial inputs that respect the underlying physics.

## Foundational Learning

- **Neural Operators (FNO/DeepONet)**: FNO is the student architecture; understanding spectral truncation and Fourier-space parameterization is essential for interpreting gradient flow and OOD failure modes.
  - Quick check: Explain why FNO truncates high-frequency modes and how this affects generalization under shifted input spectra.

- **Projected Gradient Descent (PGD) Adversarial Attacks**: The core active sampling mechanism; PGD in L2/L∞ balls with iterative projection is the inner-loop optimizer.
  - Quick check: Describe the difference between L2 and L∞ PGD projection steps and why the paper uses L2 for physics problems.

- **Differentiable Physics / Solver-in-the-Loop**: The paper's key innovation is backpropagating through a spectral PDE solver; understanding computational graphs with solver blocks is prerequisite.
  - Quick check: What is the memory/time cost implication of storing intermediate time-step activations for backpropagation through N = 4000 solver steps?

## Architecture Onboarding

- **Component map**: Differentiable spectral solver (Exponax) -> Compact FNO (4 Fourier layers) -> PGD attack (L2 constraint, ε ∈ [10, 80]) -> Training pool expansion -> Retrained FNO

- **Critical path**:
  1. Initialize FNO student on nominal GRF training distribution
  2. Freeze student; run PGD attack using solver gradients to find worst-case perturbed inputs
  3. Add discovered (perturbed input, solver output) pairs to training pool
  4. Retrain student on expanded pool; repeat rounds or attack-and-train per batch
  5. Evaluate OOD robustness across GRF kernels, correlation lengths, and value ranges

- **Design tradeoffs**:
  - With-solver vs detached vs approximated: With-solver yields highest final loss and fastest growth but requires ~3× more memory/time
  - Round-by-round vs batch-by-batch: Round-by-round achieves better OOD transfer; batch-by-batch is more expensive and can degrade in-distribution performance
  - L2 vs L∞ norm: L2 produces smoother, more physically realistic perturbations; L∞ is more aggressive but may violate smoothness constraints

- **Failure signatures**:
  - Solver blow-up (NaN/Inf gradients): Reduce PGD step size α; limit total steps; use adaptive step schedule
  - Oscillating loss in NS attacks: Later frames (5–10 sec, 20 sec) carry more gradient signal; detaching early frames reduces oscillation
  - Memory OOM on 2D NS: Batch size limited to 7 on B200; 1D Burgers manageable at batch size 1 with 20 GB

- **First 3 experiments**:
  1. Reproduce 1D Burgers PGD attack comparison (with-solver vs detached vs approximated) for ν = 0.01, ε = 10, α = 0.1; report loss progression and final perturbed inputs
  2. Run round-by-round adversarial distillation for 6 rounds on 1D Burgers; evaluate RMSE/MAE on held-out OOD GRF kernels and value ranges
  3. Extend to 2D Navier-Stokes with batch size 1; ablate solver-gradient inclusion by detaching frames 1–4 vs 5–9 vs 20; quantify impact on final attack loss and perturbation pattern alignment with external forcing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative relationship between the generalizability of neural operators and their parameter counts, and does a "compact" architecture inherently limit universal approximation capabilities?
- Basis in paper: The author states in the conclusion that "extensive research can be done to investigate the limit of generalizability of deep learning PDE solver, and how it is related to the number of parameters of the model. I leave this challenge for the future to solve."
- Why unresolved: The current study focuses on a specific compact FNO architecture; systematic ablation studies varying model capacity against the proposed adversarial training robustness were not performed.
- What evidence would resolve it: Scaling laws or error bounds derived from experiments where parameter count is varied systematically while measuring OOD performance under adversarial distillation.

### Open Question 2
- Question: Can the memory and time consumption of integrating differentiable spectral solvers into the PGD loop be reduced without compromising the stability of the gradient search?
- Basis in paper: The "Challenges" section notes that the "time and memory consumption... is huge," specifically citing the need to backpropagate through 4000 solver layers and strict batch size limits (max 7 on B200 GPUs).
- Why unresolved: The paper implements the full "solver-in-the-loop" to ensure accurate gradients but treats the computational cost as a fixed constraint rather than an optimization variable.
- What evidence would resolve it: Comparative benchmarks using gradient checkpointing, adjoint methods, or surrogate gradients that demonstrate similar adversarial sample quality with linear rather than quadratic memory scaling.

### Open Question 3
- Question: Is there a computationally efficient, differentiable loss function for 2D periodic domains that captures structural similarity better than pixel-wise MSE?
- Basis in paper: The author notes that pixel-wise loss is "ill-suited" for the 2D toroidal Navier-Stokes domain because it penalizes spatial shifts of identical patterns, while 2D Dynamic Time Warping (DTW) is too expensive (NP-complete).
- Why unresolved: The paper relies on Soft-DTW for 1D cases but lacks a comparable structural loss for 2D cases, potentially limiting the effectiveness of the PGD attack in identifying purely structural failures.
- What evidence would resolve it: Development and validation of a permutation-invariant or spectral-based loss for 2D data that runs in polynomial time and improves OOD detection over MSE.

## Limitations

- **Computational cost**: Memory and time consumption are significant, requiring backward pass through 4000 solver layers with strict batch size limits
- **Architecture specification**: Key hyperparameters (number of Fourier layers, retained modes, hidden dimensions) are unspecified
- **Data generation protocols**: Precise GRF sampling parameters and their impact on training distribution coverage remain unclear

## Confidence

- **High confidence**: The mechanism of differentiable solver gradients enabling more accurate adversarial search (supported by comparative loss progression in Figure 3-4)
- **Medium confidence**: The claim that round-by-round adversarial distillation improves OOD robustness (supported by Figure 5-7 but with mixed batch-by-batch results)
- **Low confidence**: The assertion that solver-in-the-loop constraints prevent adversarial drift into non-physical regions (lacks direct experimental validation)

## Next Checks

1. **Gradient flow validation**: Compare ∇_aℓ(a) from with-solver vs detached vs approximated attacks; quantify approximation error and its correlation with final attack effectiveness
2. **OOD transfer quantification**: Systematically vary GRF kernel parameters (correlation length, value range) and measure RMSE/MAE degradation across training schemes
3. **Memory-time tradeoff analysis**: Profile forward/backward pass times for with-solver attacks at different batch sizes and time-step counts; validate the ~2× backward cost claim