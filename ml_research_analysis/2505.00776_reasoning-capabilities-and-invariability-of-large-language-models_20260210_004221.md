---
ver: rpa2
title: Reasoning Capabilities and Invariability of Large Language Models
arxiv_id: '2505.00776'
source_url: https://arxiv.org/abs/2505.00776
tags:
- reasoning
- llms
- language
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark dataset designed to evaluate
  the logical reasoning capabilities of Large Language Models (LLMs) using a domain-free
  context with geometric figures, avoiding reliance on world knowledge. The dataset
  contains 432 binary questions testing shallow logical reasoning (existential restrictions,
  negations, quantification) across four variants to assess invariability under linguistic
  changes.
---

# Reasoning Capabilities and Invariability of Large Language Models

## Quick Facts
- arXiv ID: 2505.00776
- Source URL: https://arxiv.org/abs/2505.00776
- Reference count: 40
- Primary result: Most LLMs tested perform near baseline (51%) on logical reasoning with geometric figures, with few exceeding 70% accuracy

## Executive Summary
This paper introduces a new benchmark dataset designed to evaluate the logical reasoning capabilities of Large Language Models (LLMs) using a domain-free context with geometric figures, avoiding reliance on world knowledge. The dataset contains 432 binary questions testing shallow logical reasoning (existential restrictions, negations, quantification) across four variants to assess invariability under linguistic changes. Testing 24 LLMs ranging from 1.8B to 70B parameters shows that only a few models exceed 70% accuracy, with most models indistinguishable from baseline performance (~51%). Results remain stable across different linguistic variants, indicating limited impact of prompt engineering.

## Method Summary
The authors created a benchmark dataset with 432 binary questions based on geometric figures to test logical reasoning capabilities without requiring world knowledge. The questions evaluate shallow logical reasoning concepts including existential restrictions, negations, and quantification. Four linguistic variants of each question were created to test invariability under linguistic changes. The study tested 24 LLMs ranging from 1.8B to 70B parameters, including both open and proprietary models. Performance was evaluated under zero-shot conditions with and without chain-of-thought prompting, where the rationale was provided either before or after the answer.

## Key Results
- Only a few models exceeded 70% accuracy on the reasoning benchmark
- Most models performed near baseline levels (~51%)
- Chain-of-thought prompting improved performance only when rationale was given after the answer
- Results remained stable across different linguistic variants, showing limited impact of prompt engineering

## Why This Works (Mechanism)
The geometric figure-based approach eliminates world knowledge dependencies, allowing pure evaluation of logical reasoning capabilities. Binary true/false questions provide clear ground truth for evaluation. The four linguistic variants enable testing of invariability under different phrasings. Chain-of-thought prompting leverages LLMs' ability to generate intermediate reasoning steps, though effectiveness depends on prompt structure.

## Foundational Learning
- Logical reasoning concepts: Existential restrictions, negations, and quantification form the core reasoning primitives tested
  - Why needed: These represent fundamental logical operations required for complex reasoning
  - Quick check: Can the model correctly handle "there exists" and "for all" statements?

- Invariability under linguistic changes: Testing same meaning with different phrasings
  - Why needed: Ensures reasoning capability isn't dependent on specific wording
  - Quick check: Consistent performance across all four linguistic variants

- Zero-shot evaluation: No demonstrations provided to models
  - Why needed: Tests inherent reasoning capabilities rather than memorization
  - Quick check: Performance without any examples or instructions

## Architecture Onboarding

**Component Map:** Benchmark generation -> Model testing -> Performance evaluation -> Invariability analysis

**Critical Path:** Generate geometric questions → Test 24 LLMs → Evaluate accuracy → Analyze linguistic invariability → Assess chain-of-thought impact

**Design Tradeoffs:** Binary questions simplify evaluation but may oversimplify reasoning complexity; geometric figures eliminate world knowledge but may not represent all reasoning domains

**Failure Signatures:** Performance near 51% baseline indicates random guessing; poor chain-of-thought results suggest reasoning limitations; linguistic invariability shows robustness to prompt variations

**First Experiments:**
1. Test model performance on individual reasoning types (existential vs. negation vs. quantification)
2. Evaluate performance with few-shot examples to establish upper bounds
3. Test multi-step reasoning tasks requiring chained logical operations

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow task domain may not represent broader reasoning capabilities
- Binary question format may oversimplify real-world reasoning complexity
- Zero-shot only approach doesn't explore few-shot learning potential
- Limited to geometric reasoning, may not generalize to other domains

## Confidence
- Most LLMs perform near baseline: High
- Chain-of-thought benefits are limited: Medium
- Geometric reasoning represents general reasoning: Low

## Next Checks
1. Test the benchmark across additional reasoning domains beyond geometric figures to assess generalizability
2. Evaluate performance with few-shot prompting to establish baseline expectations for practical applications
3. Investigate model performance on multi-step reasoning tasks that require chaining multiple logical operations