---
ver: rpa2
title: 'MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual
  Learning'
arxiv_id: '2502.02372'
source_url: https://arxiv.org/abs/2502.02372
tags:
- pose
- human
- learning
- neural
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MaintaAvatar, the first work to address the
  problem of creating a maintainable virtual avatar that can continually learn new
  poses and appearances while preserving the ability to render past appearances. The
  key challenge is catastrophic forgetting, where learning new appearances and poses
  degrades the rendering quality of past appearances.
---

# MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning

## Quick Facts
- arXiv ID: 2502.02372
- Source URL: https://arxiv.org/abs/2502.02372
- Reference count: 16
- The method achieves 29.495 PSNR and 0.9663 SSIM on ZJU-MoCap, outperforming baseline PersonNeRFCL's 28.605 PSNR and 0.9632 SSIM

## Executive Summary
This paper proposes MaintaAvatar, the first work to address the problem of creating a maintainable virtual avatar that can continually learn new poses and appearances while preserving the ability to render past appearances. The key challenge is catastrophic forgetting, where learning new appearances and poses degrades the rendering quality of past appearances. To address this, the authors introduce two main modules: (1) Global-Local Joint Storage Module, which separately stores global and local appearance variations to prevent color bleeding between different appearances, and (2) Pose Distillation Module, which preserves pose information from past tasks by distilling outputs from a pose correction network. The model is evaluated on two datasets, ZJU-MoCap and THuman2.0, and demonstrates superior performance compared to baselines like CLNeRF, MEIL-NeRF, and PersonNeRF.

## Method Summary
MaintaAvatar is a deformable NeRF-based human avatar system that addresses continual learning through replay-based supervision. The model uses SMPL-based deformation to warp between canonical and observed volumes, with a rendering MLP that predicts color and opacity from positional encoding, global geometry embeddings, global color embeddings, and local Tri-plane embeddings. The Global-Local Joint Storage Module separates global appearance changes from fine-grained local variations using a Tri-plane representation. The Pose Distillation Module preserves pose accuracy for past tasks by distilling pose correction network outputs from a frozen previous model. The system employs a two-phase training schedule where pose distillation is activated only in the second phase to avoid early-stage degradation.

## Key Results
- Achieves 29.495 PSNR and 0.9663 SSIM on ZJU-MoCap dataset, outperforming baseline PersonNeRFCL (28.605 PSNR, 0.9632 SSIM)
- Successfully prevents color bleeding between sequentially learned appearances through the Global-Local Joint Storage Module
- Maintains pose accuracy for past appearances through Pose Distillation Module, avoiding catastrophic forgetting
- Requires only 5 training images per appearance for ZJU-MoCap and achieves quick fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating global and local appearance variations prevents color bleeding between sequentially learned appearances.
- Mechanism: The Global-Local Joint Storage Module uses global embeddings (ℓg, ℓc) for overall appearance changes and Tri-plane-based local embeddings (ℓt) for fine-grained spatial variations. For each appearance, a condition embedding ℓa generates a Tri-plane via generator G; sampling points query their positions on this Tri-plane to obtain ℓt. These embeddings are concatenated with positional encoding and fed to MLPo for color/opacity prediction.
- Core assumption: Human appearance changes involve both global style shifts and local texture variations that cannot be adequately captured by global embeddings alone.
- Evidence anchors: [abstract] "Global-Local Joint Storage Module, which separately stores global and local appearance variations to prevent color bleeding between different appearances"; [section 3.2] "we propose the Global-Local Joint Storage Module to model both global and local appearance variations... global embeddings can represent the overall changes in human appearance, while local embeddings is used to represent the fine-grained variations"
- Break condition: When clothing shape changes significantly (e.g., loose to tight), the Tri-plane representation may fail to capture geometric discontinuities, as acknowledged in Limitations.

### Mechanism 2
- Claim: Distilling pose correction residuals from a frozen past model preserves accurate body poses for previously learned appearances.
- Mechanism: The Pose Distillation Module applies L2 loss between the pose correction outputs (ΔΩ) of the current model's MLPp and the frozen previous model's MLPp. Training uses two phases: Phase 1 optimizes rendering quality without pose distillation; Phase 2 (activated at iteration t0) freezes all networks except MLPp and color embedding ℓc, then activates pose distillation loss (λβ=800).
- Core assumption: The pose correction network MLPp overfits to new poses during continual learning, degrading its correction capability for past poses.
- Evidence anchors: [abstract] "Pose Distillation Module, which preserves pose information from past tasks by distilling outputs from a pose correction network"; [section 3.3] "we found that current continual learning models for the human body may introduce incorrect poses from past tasks because MLPp overfits to learning new poses"
- Break condition: If past and current poses share insufficient geometric similarity, distillation may propagate errors rather than correct them.

### Mechanism 3
- Claim: Generative replay with frozen past models enables supervision of past appearances without storing original images.
- Mechanism: At each task transition, the previous network ΘT-1 is copied and frozen. Given saved camera and pose parameters from past tasks, ΘT-1 renders patches and pose residuals as supervision signals. The current network ΘT is trained on both new task images and replay-generated supervision, with loss weighted by λp that increases during training (Equation 12).
- Core assumption: The frozen model's rendering quality is sufficient to serve as pseudo-ground-truth for past appearances.
- Evidence anchors: [abstract] "requires only limited data collection to quickly fine-tune the model while avoiding catastrophic forgetting"; [section 3.1] "Utilizing the saved human body pose and camera parameters from past tasks, the network ΘT-1 executes the process for step 2, and obtains the color supervision signal ēC(¯r) of the past tasks"
- Break condition: If the frozen model has rendering artifacts, these propagate as incorrect supervision to future tasks (error accumulation).

## Foundational Learning

- Concept: **SMPL-based deformation fields and inverse linear blend skinning**
  - Why needed here: MaintaAvatar warps between canonical and observed volumes using skeletal motion Tskel, requiring understanding of how SMPL parameters (J, Ω) drive body deformation.
  - Quick check question: Given SMPL pose parameters, can you compute the mapping from an observed 3D point to canonical space using LBS weights?

- Concept: **Neural Radiance Fields (volume rendering, ray sampling, MLP representation)**
  - Why needed here: The core representation maps (position, direction, embeddings) → (color, opacity) via MLPo, with 128 samples per ray.
  - Quick check question: Can you explain how accumulated radiance along a ray produces the final pixel color?

- Concept: **Catastrophic forgetting and replay-based continual learning**
  - Why needed here: The entire motivation; understand why naive fine-tuning degrades past appearance rendering.
  - Quick check question: Why does storing camera/pose parameters (not images) suffice for replay in this architecture?

## Architecture Onboarding

- Component map: Input pose → MLPp correction → inverse LBS to canonical space → sample points → query Tri-plane for ℓt → concatenate with ℓg, ℓc, γ(x) → MLPo → render

- Critical path: Input pose → MLPp correction → inverse LBS to canonical space → sample points → query Tri-plane for ℓt → concatenate with ℓg, ℓc, γ(x) → MLPo → render

- Design tradeoffs:
  - λp scheduling: Sinusoidal increase balances early adaptation vs. late retention
  - Two-phase training: Render quality first (10K/70K iters), pose distillation second (2K/10K iters)
  - Storage: Must retain frozen ΘT-1 and past task camera/pose parameters
  - Pretrained initialization: Enables few-shot adaptation but assumes similar body morphology

- Failure signatures:
  - Color bleeding on past appearances → missing or underweighted local embeddings
  - Incorrect limb positions on old poses → pose distillation not activated or λβ too low
  - Blurry renders → pose distillation activated too early (Phase 1)
  - Poor convergence on new appearance → λp too high initially, starving new task learning

- First 3 experiments:
  1. **Reproduce single-task baseline**: Train on one appearance from ZJU-MoCap subject 377, verify PSNR matches or exceeds PersonNeRF baseline (~28.2 reported).
  2. **Two-task continual learning ablation**: Train on appearance A (5 images), then appearance B (5 images). Compare: (a) full model, (b) w/o Global-Local module, (c) w/o Pose Distillation. Measure PSNR/SSIM on appearance A after training on B.
  3. **Embedding dimension sensitivity**: Sweep ℓc dimension (32, 48, 64) and Tri-plane resolution (256, 512) on a held-out appearance, monitoring both rendering quality and training speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can continual learning avatars handle significant clothing shape changes without performance degradation?
- Basis in paper: [explicit] The authors state in Limitations: "Our method shows performance drops with significant clothing shape changes."
- Why unresolved: The current Global-Local Joint Storage Module models appearance variations through embeddings and Tri-planes, but this approach does not account for substantial geometric changes (e.g., loose vs. fitted clothing, accessories) that alter body silhouette.
- What evidence would resolve it: Ablation studies on datasets with extreme shape variation (e.g., winter coats vs. summer clothing), or a new module explicitly designed for geometry-aware continual updates.

### Open Question 2
- Question: How can few-shot continual learning avatars generalize to complex, rarely-seen human poses?
- Basis in paper: [explicit] The authors state in Limitations: the method "struggles with pose generalization in complex poses due to limited exposure in the few-shot dataset."
- Why unresolved: The Pose Distillation Module preserves past pose corrections but cannot synthesize knowledge about unseen poses from the sparse 5-image-per-task training regime.
- What evidence would resolve it: Evaluations on pose-diverse test sets with out-of-distribution articulations, or integration with parametric pose priors to improve generalization from limited data.

### Open Question 3
- Question: How does MaintaAvatar's performance and storage scale as the number of sequential appearance tasks increases substantially?
- Basis in paper: [inferred] Experiments only evaluate 4 sequential tasks (subjects 377, 392, 393, 394), and each appearance requires storing a condition embedding ℓa and Tri-plane of 3×512×512×8 dimensions.
- Why unresolved: The replay-based strategy stores frozen model parameters per task, and Tri-plane storage grows linearly with appearances—scalability to dozens of updates remains untested.
- What evidence would resolve it: Experiments with 10+ sequential tasks measuring PSNR degradation over time and total storage requirements per added appearance.

### Open Question 4
- Question: Can MaintaAvatar's continual learning strategy be adapted to 3D Gaussian Splatting-based human avatars for real-time rendering?
- Basis in paper: [inferred] The paper acknowledges 3D Gaussian Splatting (3DGS) for human representation in Related Work but does not compare against or integrate with these faster, explicit representations.
- Why unresolved: The implicit NeRF backbone limits rendering speed, while 3DGS methods (e.g., GaussianAvatar, GauHuman) offer real-time performance but lack continual learning mechanisms.
- What evidence would resolve it: A comparative study against recent 3DGS human avatars, or a hybrid approach combining MaintaAvatar's replay strategy with Gaussian primitives.

## Limitations

- The method shows performance drops with significant clothing shape changes that alter body silhouette.
- The approach struggles with pose generalization in complex poses due to limited exposure in the few-shot dataset.
- The replay-based strategy stores frozen model parameters per task, and Tri-plane storage grows linearly with appearances—scalability to many tasks remains untested.

## Confidence

**High Confidence**: The core continual learning framework with replay-based supervision and catastrophic forgetting mitigation is well-established. The experimental methodology (PSNR, SSIM metrics on ZJU-MoCap and THuman2.0) follows standard practices in neural rendering.

**Medium Confidence**: The Global-Local Joint Storage mechanism's effectiveness in preventing color bleeding is supported by quantitative results, but the underlying assumption about global vs. local appearance decomposition lacks theoretical justification. The Pose Distillation Module's design appears sound, but its necessity would benefit from more extensive ablation studies across varying pose similarity scenarios.

**Low Confidence**: The scalability of the approach to many-task sequences (>5 tasks) and its robustness to significant appearance/pose distribution shifts are not empirically validated. The error accumulation potential in the frozen model replay mechanism remains an open concern.

## Next Checks

1. **Multi-task Sequence Robustness**: Extend continual learning experiments to 5+ sequential tasks on ZJU-MoCap, measuring degradation rates in PSNR/SSIM for both early and late tasks to assess error accumulation in the replay mechanism.

2. **Architecture Sensitivity Analysis**: Systematically vary the Tri-plane generator architecture (depth, width, activation functions) and embedding dimensions (ℓc: 32, 48, 64; Tri-plane resolution: 256, 512) to quantify their impact on color bleeding prevention and overall rendering quality.

3. **Pose Similarity Ablation**: Create controlled experiments where consecutive tasks have increasingly dissimilar poses (e.g., standing vs. sitting vs. complex gymnastics) to evaluate when the Pose Distillation Module breaks down and whether the frozen model's pose residuals remain valid supervision signals.