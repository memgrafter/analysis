---
ver: rpa2
title: 'QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate
  Optimization Algorithm Circuits'
arxiv_id: '2504.16350'
source_url: https://arxiv.org/abs/2504.16350
tags:
- quantum
- training
- graph
- circuits
- qaoa-gpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QAOA-GPT, a generative framework using GPT
  models to directly synthesize quantum circuits for quadratic unconstrained binary
  optimization problems, demonstrated on the MaxCut problem. The approach leverages
  ADAPT-QAOA to generate a diverse dataset of high-quality circuits, which is then
  used to train a GPT model to generate compact, problem-specific circuits without
  iterative optimization.
---

# QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits

## Quick Facts
- arXiv ID: 2504.16350
- Source URL: https://arxiv.org/abs/2504.16350
- Reference count: 36
- Key outcome: QAOA-GPT uses GPT models to generate adaptive QAOA circuits for MaxCut problems, achieving high approximation ratios with constant inference time scaling

## Executive Summary
This paper introduces QAOA-GPT, a generative framework that uses GPT models to directly synthesize quantum circuits for quadratic unconstrained binary optimization problems, demonstrated on the MaxCut problem. The approach leverages ADAPT-QAOA to generate a diverse dataset of high-quality circuits, which is then used to train a GPT model to generate compact, problem-specific circuits without iterative optimization. QAOA-GPT achieves high approximation ratios comparable to or exceeding ADAPT-QAOA and standard QAOA across various graph sizes and densities, with inference times remaining nearly constant as problem size increases.

## Method Summary
QAOA-GPT is a generative framework that uses GPT models to directly synthesize quantum circuits for quadratic unconstrained binary optimization problems, specifically demonstrated on the MaxCut problem. The method leverages ADAPT-QAOA to generate a diverse dataset of high-quality circuits, which is then used to train a GPT model to generate compact, problem-specific circuits without iterative optimization. The approach incorporates graph embeddings to capture structural features, enhancing circuit generation. By training on problem-specific datasets, QAOA-GPT learns to produce tailored circuits that achieve high approximation ratios while maintaining constant inference time regardless of problem size, significantly improving scalability compared to traditional iterative QAOA methods.

## Key Results
- QAOA-GPT achieves approximation ratios comparable to or exceeding ADAPT-QAOA and standard QAOA across various graph sizes and densities
- Inference time remains nearly constant as problem size increases, demonstrating significant scalability improvements
- Ablation studies confirm the importance of graph embeddings and diverse training data for robust generalization

## Why This Works (Mechanism)
The mechanism works by leveraging GPT's ability to learn patterns from high-quality circuit data generated by ADAPT-QAOA, combined with graph embeddings that capture structural features. This allows the model to generate problem-specific circuits directly without iterative optimization. The constant inference time scaling is achieved because the GPT model's computational complexity is independent of problem size once trained, while traditional QAOA methods require optimization iterations that scale with problem complexity.

## Foundational Learning

**Quadratic Unconstrained Binary Optimization (QUBO)**
- Why needed: The fundamental problem class QAOA-GPT targets
- Quick check: Verify the QUBO formulation correctly captures the optimization problem

**Quantum Approximate Optimization Algorithm (QAOA)**
- Why needed: The baseline algorithm being improved upon
- Quick check: Confirm understanding of p-level parameterization and parameter optimization

**Graph Embeddings**
- Why needed: To capture structural features that inform circuit generation
- Quick check: Validate embedding quality through visualization or similarity metrics

**GPT Model Architecture**
- Why needed: The core generative model for circuit synthesis
- Quick check: Verify attention mechanisms and token representation align with circuit generation requirements

**ADAPT-QAOA**
- Why needed: The method for generating high-quality training data
- Quick check: Confirm that generated circuits achieve target approximation ratios

## Architecture Onboarding

**Component Map**
Graph representation -> Graph embedding layer -> GPT model -> Circuit token sequence -> QAOA circuit

**Critical Path**
Graph embedding extraction → GPT circuit generation → Parameter estimation → Circuit execution

**Design Tradeoffs**
- Training data diversity vs. quality: More diverse data improves generalization but may include suboptimal circuits
- GPT model size vs. inference efficiency: Larger models may generate better circuits but sacrifice constant-time advantage
- Embedding complexity vs. computational overhead: Rich embeddings improve circuit quality but increase preprocessing time

**Failure Signatures**
- Poor approximation ratios indicate inadequate training data or insufficient model capacity
- Unstable inference times suggest attention mechanism bottlenecks
- Overfitting to specific graph structures manifests as degraded performance on novel problems

**Three First Experiments**
1. Generate circuits for small graphs (n=5-10) and compare approximation ratios with ADAPT-QAOA
2. Test inference time scaling across different graph sizes to verify constant-time claim
3. Perform ablation study removing graph embeddings to quantify their contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to MaxCut problems, leaving generalization to other QUBO formulations untested
- Dataset generation relies on ADAPT-QAOA, which has computational overhead and potential bias
- Quality of generated circuits on noisy quantum hardware remains untested

## Confidence

**High**: Claims about improved approximation ratios compared to standard QAOA on MaxCut problems
**Medium**: Claims about constant inference time scaling and practical efficiency gains
**Low**: Claims about generalizability to other QUBO problems beyond MaxCut

## Next Checks
1. Test QAOA-GPT on diverse QUBO problems (e.g., portfolio optimization, graph coloring) to verify cross-domain generalization
2. Evaluate circuit quality when running on noisy intermediate-scale quantum (NISQ) devices to assess practical viability
3. Benchmark against other quantum circuit synthesis methods on larger graph sizes (n > 50) to validate the claimed constant inference time scaling