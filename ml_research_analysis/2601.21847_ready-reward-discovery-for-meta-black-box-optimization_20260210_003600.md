---
ver: rpa2
title: 'READY: Reward Discovery for Meta-Black-Box Optimization'
arxiv_id: '2601.21847'
source_url: https://arxiv.org/abs/2601.21847
tags:
- reward
- cost
- optimization
- component
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: READY automates reward discovery for MetaBBO by leveraging large
  language models in a multitask evolutionary framework. It introduces niche-based
  parallel populations, fine-grained mutation/crossover operators, and knowledge transfer
  across tasks to accelerate search.
---

# READY: Reward Discovery for Meta-Black-Box Optimization

## Quick Facts
- arXiv ID: 2601.21847
- Source URL: https://arxiv.org/abs/2601.21847
- Reference count: 40
- Primary result: Automated reward discovery framework using LLM-driven evolution achieves up to 99.99% cost reduction in zero-shot transfer across MetaBBO tasks

## Executive Summary
READY automates reward discovery for Meta-Black-Box Optimization by leveraging large language models in a multitask evolutionary framework. The system introduces niche-based parallel populations, fine-grained mutation/crossover operators, and knowledge transfer across tasks to accelerate search. It discovers interpretable reward functions that outperform handcrafted and automated baselines across three heterogeneous MetaBBO tasks (DEDQN, RLDAS, RLEPSO), achieving up to 99.99% cost reduction in zero-shot transfer while maintaining microsecond inference latency.

## Method Summary
READY employs a multitask evolutionary architecture where LLM-generated Python code represents reward functions evolving through five operators: three mutation types (Local-Reflection, History-Reflection, Global-Reflection) and two crossover types (Local-Exploitation, Local-Exploitation-Exploratory). The framework maintains parallel populations (niches) for different MetaBBO tasks, with knowledge transfer between niches based on potential for positive transfer. Fitness evaluation requires training the MetaBBO agent with the candidate reward for 0.5 hours on BBOB benchmark functions, then testing on held-out functions. The evolution loop runs for 7 generations with populations of size 5 per niche, using DeepSeek-V3.2 LLM for code generation and evaluation.

## Key Results
- Achieves 2-4× faster discovery compared to single-task baselines
- Zero-shot transfer achieves up to 99.99% cost reduction across tasks
- Maintains microsecond inference latency while discovering superior rewards
- Rewards demonstrate superior generalization by capturing universal optimization heuristics

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Symbolic Evolution
Large Language Models iteratively synthesize and refine symbolic reward functions through targeted mutations and crossovers based on performance feedback. The LLM analyzes failure cases and evolutionary history to generate semantically meaningful code modifications rather than random perturbations, mapping textual feedback to valid code changes.

### Mechanism 2: Niche-Based Multitask Knowledge Transfer
Parallel evolutionary search across multiple tasks enables knowledge sharing between populations. High-performing reward logic from one optimization task can be adapted for another, accelerating convergence through transfer of universal optimization heuristics like exploration-exploitation trade-offs.

### Mechanism 3: Reflection-Based Search Refinement
Targeted reflection on failure cases and evolutionary history provides dense semantic feedback to the LLM. Operators analyze top-K failure instances and historical traces to direct search toward specific weaknesses, improving sample efficiency compared to standard evolutionary search.

## Foundational Learning

- **Concept: Meta-Black-Box Optimization (MetaBBO)**
  - Why needed: READY operates at the meta-level, tuning the internal guidance system (reward) of an optimizer. You must distinguish between the optimizer's actions and the meta-policy's actions.
  - Quick check: Can you distinguish between the "action" of the low-level optimizer (e.g., a mutation vector) and the "action" of the meta-policy (e.g., selecting that mutation strategy)?

- **Concept: Reward Shaping in RL**
  - Why needed: The paper automates "Reward Discovery." You need to know that a reward function translates raw state changes into training signals for the RL agent.
  - Quick check: If an agent finds a local optimum, does a sparse reward (1 for success, 0 otherwise) or a dense reward (continuous improvement signal) typically help it learn faster?

- **Concept: Evolutionary Algorithms (EA)**
  - Why needed: READY uses evolutionary loops on code. Understanding "population diversity," "elitism," and "crossover" is essential to follow Section 3.
  - Quick check: Why might a population of identical individuals (low diversity) cause an evolutionary algorithm to stall?

## Architecture Onboarding

- **Component map:** Metadata Preparation -> Initialization -> Evolution Loop -> Evaluation -> Knowledge Transfer
- **Critical path:** The Evaluation step is the bottleneck, requiring training the underlying MetaBBO agent (e.g., DEDQN) to assess reward quality. This computationally expensive step necessitates the 0.5-hour wall-clock limit.
- **Design tradeoffs:** Multitasking speeds up discovery (2-4×) but introduces complexity in managing parallel environments and potential negative transfer. Detailed reflection improves search quality but increases LLM inference time and token usage.
- **Failure signatures:** Syntax errors from LLM code generation, reward hacking where agents exploit rewards without solving problems, and search stagnation requiring diversity injection.
- **First 3 experiments:**
  1. Unit Test the Interface: Verify the reward_hyperparameters dictionary for a specific target contains all keys required by the Metadata in Appendix B.1.
  2. Baseline Comparison (Single Task): Run initialization phase for one niche. Confirm "Greedy Sampling" finds at least one reward better than "Handcrafted" baseline within N samples.
  3. Validate Transfer Logic: Execute Knowledge Transfer operator manually. Take best reward from DEDQN and prompt LLM to adapt it for RLEPSO using Appendix A.7 prompt. Check if adapted code compiles and runs.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved regarding scalability to diverse MetaBBO categories, theoretical characterization of transfer boundaries, and minimum viable LLM capabilities for effective reward discovery.

## Limitations
- Heavy reliance on LLM-generated code introduces uncertainty around code validity and reward quality
- Computational intensity of evaluation pipeline (0.5h training per candidate) creates practical barriers to rapid iteration
- Knowledge transfer effectiveness depends on heuristic identification of transferable logic across heterogeneous tasks

## Confidence

| Claim | Confidence |
|-------|------------|
| Niche-based multitask acceleration (2-4× speedup) | High |
| Zero-shot transfer performance (99.99% cost reduction) | High |
| LLM-driven evolution discovers superior rewards | Medium |
| Generalization to unseen optimization landscapes | Low |

## Next Checks

1. **Syntax Validation Test:** Run 100 consecutive LLM generations and measure rejection rate due to syntax errors or undefined variables
2. **Transfer Robustness:** Apply best rewards from one task (e.g., DEDQN) to a new, held-out MetaBBO algorithm not in the original three and measure performance drop
3. **Negative Transfer Detection:** Intentionally pair incompatible source-target niches in knowledge transfer and quantify performance degradation to establish transfer boundaries