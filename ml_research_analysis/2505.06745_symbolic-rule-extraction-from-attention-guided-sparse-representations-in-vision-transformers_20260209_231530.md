---
ver: rpa2
title: Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision
  Transformers
arxiv_id: '2505.06745'
source_url: https://arxiv.org/abs/2505.06745
tags:
- loss
- sparse
- rule-set
- each
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeSyViT, a framework that extracts executable
  symbolic logic programs from Vision Transformers (ViTs) by introducing a sparse
  concept layer that produces binarized, disentangled representations of high-level
  visual concepts. The method combines supervised contrastive loss, entropy minimization,
  and L1 sparsity to encourage class-specific clustering and binary activation patterns.
---

# Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers

## Quick Facts
- arXiv ID: 2505.06745
- Source URL: https://arxiv.org/abs/2505.06745
- Reference count: 4
- Primary result: Achieves 5.14% higher accuracy than vanilla ViT with 67% smaller, more interpretable rule-sets

## Executive Summary
This paper introduces NeSyViT, a framework that extracts executable symbolic logic programs from Vision Transformers by introducing a sparse concept layer producing binarized, disentangled representations of high-level visual concepts. The method combines supervised contrastive loss, entropy minimization, and L1 sparsity to encourage class-specific clustering and binary activation patterns. Experiments show that NeSyViT achieves 5.14% higher classification accuracy than vanilla ViT while producing rule-sets that are 67% smaller and more interpretable than those generated by comparable CNN-based methods. The approach bridges the gap between transformer-based vision models and symbolic reasoning, enabling both high performance and verifiable, interpretable AI.

## Method Summary
NeSyViT modifies a ViT-Base backbone by replacing the classification head with a 128-dimensional linear layer followed by sigmoid activation. During training, three losses are combined: supervised contrastive loss (SupCon) to cluster same-class representations, entropy loss to push activations toward binary extremes, and L1 sparsity loss to ensure most neurons remain inactive. After training, activations are binarized at threshold 0.5 and fed to the FOLD-SE-M algorithm to generate stratified answer set programs. The framework is evaluated on Places dataset subsets and GTSRB, demonstrating improved accuracy and interpretability over both vanilla ViTs and CNN-based approaches.

## Key Results
- NeSyViT achieves 5.14% higher classification accuracy than vanilla ViT on Places dataset
- Extracted rule-sets are 67% smaller than those from comparable CNN-based methods
- The framework successfully generates interpretable symbolic rules that maintain or exceed baseline accuracy

## Why This Works (Mechanism)

### Mechanism 1: Sparsity Enforces Disentanglement
The L1 sparsity loss penalizes large activation values, forcing most neurons to remain near zero while allowing only a few to activate per image. This limits predicate count per rule, reducing logic program complexity. The core assumption is that sparse neural activations correspond to semantically distinct visual concepts that can serve as symbolic predicates. Evidence includes the paper's claim that sparsity ensures rule-sets remain compact and interpretable, though support from sparse autoencoder methods is indirect for ViT-specific applications.

### Mechanism 2: Entropy Minimization Enables Lossless Binarization
Entropy minimization with sigmoid activation pushes continuous outputs toward binary extremes (0 or 1), minimizing information loss during thresholding. Each neuron output is treated as a Bernoulli variable, and minimizing binary entropy forces activations away from 0.5 toward discrete values. The core assumption is that binarization preserves sufficient information for downstream classification, though this appears novel to this work with no direct corpus evidence.

### Mechanism 3: Supervised Contrastive Learning Enables Accurate Rules
Supervised contrastive loss clusters same-class sparse representations, enabling FOLD-SE-M to learn accurate class-discriminative rules. SupCon loss pulls same-class samples together in latent space while pushing apart different-class samples, creating well-separated clusters that map cleanly to symbolic decision boundaries. The core assumption is that class-level clustering in sparse representation space aligns with concept-level decision boundaries expressible in logic programs.

## Foundational Learning

- **Vision Transformer (ViT) Architecture**: The sparse concept layer operates on the [CLS] token from ViT's final transformer block; understanding patch embedding, self-attention, and the [CLS] token's role is essential. *Quick check*: Can you explain why the [CLS] token is used for classification rather than individual patch tokens?

- **Answer Set Programming (ASP) and Default Logic**: FOLD-SE-M generates stratified ASP rules with default reasoning (conditions + exceptions); reading extracted rules requires understanding ":-", "not", and rule evaluation order. *Quick check*: Given a rule `class(X) :- feature1(X), not feature2(X).`, what happens if both features are true?

- **Supervised Contrastive Learning**: The SupCon loss is critical for clustering; understanding anchor/positive/negative sampling and temperature parameter τ affects debugging loss convergence. *Quick check*: In a batch of 4 images with labels [A, A, B, B], how many positive pairs exist for the first image?

## Architecture Onboarding

- **Component map**: Input Image (224×224) → Patch Embedding (16×16 patches → 768-dim each) → ViT Backbone (12 blocks, 12 heads) → [CLS] token (768-dim) → Sparse Concept Layer (Linear: 768→128 + Sigmoid) → Binarization (threshold 0.5) → 128-dim binary vector → FOLD-SE-M → Stratified ASP Rule-set

- **Critical path**: The [CLS] token must capture sufficient class-relevant information before reaching the sparse layer. If early transformer layers fail to aggregate discriminative features, the sparse layer cannot recover them.

- **Design tradeoffs**: Sparse layer dimension D (larger increases capacity but risks overfitting), loss weights (α, β, γ) balance between clustering, binarization, and sparsity, FOLD-SE-M parameters (ratio, tail) control rule pruning and false positives.

- **Failure signatures**: All-zero binary vectors (L1 sparsity weight too high), accuracy drops vs. vanilla ViT (entropy loss weight insufficient), giant rule-sets (>50 predicates, SupCon weight too low), semantically incoherent labels (ViT neurons are polysemantic).

- **First 3 experiments**:
  1. Baseline sanity check: Train NeSyViT on P2 with default hyperparameters, verify accuracy ≥ vanilla ViT and rule-set size ≤ 5 rules.
  2. Ablation on loss components: Remove each loss term (SupCon, entropy, L1) one at a time on P5, measure accuracy drop and rule-set size increase.
  3. Semantic label validation: For extracted rules on P3.1, manually inspect attention heatmaps for top-5 activating images per neuron, verify neuron 106 activates on "bed" concept.

## Open Questions the Paper Calls Out

- **Improving neuron disentanglement**: Can architectural or training refinements improve neuron disentanglement to enforce monosemanticity in the sparse concept layer? The authors plan to focus on this, noting that automatic semantic labeling often produces misleading results due to polysemantic neurons in ViTs.

- **Bias correction**: Can the extracted rule-sets be leveraged to identify and correct biases in the Vision Transformer model? The conclusion identifies this as a natural next step, referencing similar approaches in CNN interpretability.

- **Multimodal LLM labeling**: Can multimodal Large Language Models (LLMs) replace segmentation-based methods for automatic semantic labeling of rule-sets? The authors propose exploring multimodal LLMs like GPT-4o to eliminate reliance on pixel-level segmentation masks.

- **Scalability to ImageNet-1k**: Does the performance gain hold for full-scale datasets like ImageNet-1k? The experiments are limited to Places subsets and GTSRB, leaving open whether the framework can handle 1000 classes without rule-set explosion.

## Limitations

- Loss hyperparameter sensitivity is dataset-dependent, suggesting limited generalization without careful tuning
- Rule semantic validation relies on indirect CNN-based attention maps rather than ground-truth concept annotations
- Binary representation fidelity lacks quantitative analysis of information loss during binarization

## Confidence

- **High confidence**: Technical architecture, mathematical formulation of combined loss, general framework for rule extraction
- **Medium confidence**: Semantic interpretability and automatic neuron labeling via attention maps
- **Low confidence**: Universal applicability across datasets without hyperparameter tuning

## Next Checks

1. **Activation distribution analysis**: For each dataset, plot histograms of sparse concept layer activations before binarization to verify whether entropy loss successfully pushes values toward 0/1 extremes. Quantify the proportion of activations within [0.4, 0.6] as a measure of binarization fidelity.

2. **Cross-dataset hyperparameter transferability**: Train NeSyViT on GTSRB using Places-optimal hyperparameters (α=2, β=1, γ=1) and vice versa. Measure accuracy drop and rule-set size changes to establish the extent of dataset-specific tuning required.

3. **Neuron polysemanticity assessment**: For the top-20 activating images per neuron across multiple classes, compute class purity scores (proportion of images from the dominant class). Identify neurons with purity < 0.7 as polysemantic, indicating failure of the sparsity assumption and potential for semantically incoherent rules.