---
ver: rpa2
title: 'Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge'
arxiv_id: '2506.12574'
source_url: https://arxiv.org/abs/2506.12574
tags:
- bias
- gender
- language
- mitigation
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORGI-PM, a high-quality Chinese corpus for
  gender bias probing and mitigation containing 32.9k human-annotated sentences, including
  5.2k gender-biased sentences with corresponding bias-eliminated versions. The corpus
  addresses the lack of Chinese fairness-related computational linguistic resources
  and provides nuanced contextual-level gender bias data beyond existing word- or
  grammar-level resources.
---

# Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge

## Quick Facts
- arXiv ID: 2506.12574
- Source URL: https://arxiv.org/abs/2506.12574
- Reference count: 40
- Primary result: Introduces CORGI-PM corpus with 32.9k human-annotated Chinese sentences for gender bias detection and mitigation

## Executive Summary
This paper presents CORGI-PM, a comprehensive Chinese corpus designed to address the lack of resources for gender bias probing and mitigation in Chinese NLP. The corpus contains 32.9k human-annotated sentences, including 5.2k instances of gender-biased content with corresponding gender-neutral paraphrases. The authors developed an automatic filtering pipeline to create a candidate pool from large-scale Chinese corpora, followed by detailed human annotation using a specialized scheme that classifies bias into three subtypes: Association with Occupation (AO), Discrimination (DI), and Asymmetric Representation (ANB). The corpus supports three NLP challenges: gender bias detection, classification, and mitigation, with evaluation showing promising detection performance but highlighting the persistent challenges in effective mitigation.

## Method Summary
The corpus construction process involved two main phases: automatic candidate generation and human annotation. The authors first employed a filtering pipeline that processed large-scale Chinese corpora to identify potential gender-biased sentences, using keyword matching and linguistic pattern detection to create a candidate pool. This was followed by human annotation where trained annotators classified each sentence according to three bias subtypes (AO, DI, ANB) and provided gender-neutral paraphrases for biased instances. The annotation scheme was designed to capture contextual-level gender bias rather than relying on word- or grammar-level features, making it more nuanced and representative of real-world bias manifestations in Chinese texts.

## Key Results
- CORGI-PM contains 32.9k sentences with 5.2k gender-biased instances and corresponding bias-eliminated versions
- Gender bias detection achieves F1 scores up to 0.850, demonstrating strong performance
- Gender bias mitigation remains challenging with BLEU scores below 0.30 for gender-neutral paraphrase generation
- The corpus enables three NLP challenges: detection, classification, and mitigation of gender bias in Chinese texts

## Why This Works (Mechanism)
The corpus succeeds by addressing a critical gap in Chinese NLP resources for fairness evaluation. The automatic filtering pipeline efficiently narrows down large-scale corpora to relevant candidates, while the human annotation process ensures high-quality, contextually-aware bias identification and mitigation. The three-bias-subtype classification scheme captures different manifestations of gender bias, from occupational stereotypes to discriminatory language and asymmetric representation. By requiring annotators to provide gender-neutral paraphrases, the corpus enables direct evaluation of mitigation strategies rather than just bias detection.

## Foundational Learning
- **Gender Bias Classification**: Understanding different bias types (AO, DI, ANB) is crucial for targeted mitigation strategies. *Why needed*: Different bias types require different approaches for effective mitigation. *Quick check*: Can you identify examples of each bias subtype in Chinese text?
- **Human Annotation Quality Control**: Ensuring consistent bias identification across annotators is essential for corpus reliability. *Why needed*: Subjectivity in bias perception can lead to inconsistent annotations. *Quick check*: Do inter-annotator agreement scores meet acceptable thresholds?
- **Paraphrase Quality Metrics**: BLEU scores alone may not capture semantic equivalence in gender-neutral rewrites. *Why needed*: Automated metrics may miss nuanced meaning preservation. *Quick check*: Are human evaluations of paraphrase quality aligned with automated metrics?
- **Cross-Linguistic Bias Patterns**: Gender bias manifests differently across languages and cultures. *Why needed*: Models trained on Chinese data may not generalize to other languages. *Quick check*: How do bias patterns in Chinese compare to those in English corpora?

## Architecture Onboarding
- **Component Map**: Large-scale Corpus -> Automatic Filtering Pipeline -> Candidate Pool -> Human Annotation -> CORGI-PM Corpus
- **Critical Path**: Automatic filtering must efficiently identify candidates, human annotation must maintain quality standards, and the final corpus must support all three challenge tasks (detection, classification, mitigation)
- **Design Tradeoffs**: The pipeline balances automation efficiency with human annotation quality, prioritizing comprehensive bias coverage over corpus size
- **Failure Signatures**: Poor filtering may miss subtle bias patterns; inconsistent human annotation can reduce corpus reliability; inadequate paraphrase quality undermines mitigation evaluation
- **First Experiments**: 1) Evaluate filtering pipeline recall on known gender-biased sentences; 2) Measure inter-annotator agreement for bias classification; 3) Test detection model performance on held-out CORGI-PM data

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on Chinese language limits generalizability to other linguistic contexts and cultural settings
- Automatic filtering pipeline may introduce selection biases in candidate pool generation
- Human annotation relies on subjective judgment for bias classification and mitigation quality assessment

## Confidence
- **High confidence**: Corpus construction methodology and size (32.9k sentences with 5.2k gender-biased instances) are clearly documented and verifiable; detection task performance metrics are robust and reproducible
- **Medium confidence**: Mitigation task results are reliable, but evaluation methodology could benefit from additional human validation studies
- **Low confidence**: Generalizability to non-Chinese languages and long-term effectiveness of mitigation strategies in real-world applications remain uncertain

## Next Checks
1. Conduct cross-linguistic validation studies to assess transferability of detection and mitigation approaches to other languages, particularly those with different grammatical structures
2. Perform extensive human evaluation of gender-neutral paraphrases to complement automated BLEU scoring and establish more comprehensive quality metrics
3. Implement longitudinal studies to evaluate persistence of mitigated biases over time and across different model iterations