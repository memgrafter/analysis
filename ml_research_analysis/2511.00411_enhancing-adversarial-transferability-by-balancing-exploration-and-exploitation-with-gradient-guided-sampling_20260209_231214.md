---
ver: rpa2
title: Enhancing Adversarial Transferability by Balancing Exploration and Exploitation
  with Gradient-Guided Sampling
arxiv_id: '2511.00411'
source_url: https://arxiv.org/abs/2511.00411
tags:
- adversarial
- sampling
- methods
- loss
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Gradient-Guided Sampling (GGS), an inner-iteration
  sampling strategy for adversarial attacks that balances exploration (cross-model
  generalization) and exploitation (attack potency). GGS guides sampling using gradients
  from the previous inner-iteration to achieve stable ascent toward flat loss regions
  with higher local maxima.
---

# Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling

## Quick Facts
- arXiv ID: 2511.00411
- Source URL: https://arxiv.org/abs/2511.00411
- Reference count: 40
- Primary result: Achieves 82.08% average untargeted attack success rate and 17.67% targeted success rate across nine diverse models

## Executive Summary
This paper introduces Gradient-Guided Sampling (GGS), an inner-iteration sampling strategy for adversarial attacks that balances exploration (cross-model generalization) and exploitation (attack potency). GGS guides sampling using gradients from the previous inner-iteration to achieve stable ascent toward flat loss regions with higher local maxima. This addresses the trade-off where momentum-based methods over-prioritize exploitation and recent random sampling methods over-prioritize exploration. Experiments on nine diverse models show GGS outperforms state-of-the-art methods, achieving 82.08% average untargeted attack success rate and 17.67% targeted success rate.

## Method Summary
Gradient-Guided Sampling (GGS) is an inner-iteration sampling strategy integrated into MI-FGSM that balances exploration and exploitation for adversarial transferability. The method generates samples using the sign of the previous gradient to determine direction while maintaining random magnitude for exploration. This creates a single-step dependency that stabilizes ascent toward flat loss regions with higher local maxima. GGS operates with hyperparameters: perturbation $\epsilon=16/255$, outer iterations $T=10$, inner iterations $N=20$, step size $\alpha=\epsilon/T$, and sampling range $\zeta=2.0 \times \epsilon$. The method is compatible with input transformation techniques and demonstrates strong performance on both image classifiers and multimodal large language models.

## Key Results
- Achieves 82.08% average untargeted attack success rate across nine diverse models
- Achieves 17.67% targeted attack success rate on multimodal large language models
- Improves performance of input transformation methods (DIM, TIM, SIM) by up to 43%
- Demonstrates robustness against standard defenses while maintaining transferability

## Why This Works (Mechanism)

### Mechanism 1: Stabilized Ascent toward Flat Maxima
The method suggests that transferability improves when adversarial examples reside in loss regions that are both flat (for generalization) and high (for potency). GGS replaces independent random sampling with a guided strategy. By using the gradient from the previous inner-iteration ($\tilde{g}_{i-1}$) to determine the sampling direction, it dampens the oscillation typical of random sampling. This "lookahead" behavior guides the samples toward stable ascent directions, theoretically locating regions that balance local loss height with surface flatness.

### Mechanism 2: Single-Step Dependency (Decoupling from Momentum)
Using the immediate past gradient is posited to be superior to using accumulated momentum or random noise for inner-iteration sampling efficiency. Unlike Momentum-Guided Sampling (MGS), which creates a long-chain dependency on early (potentially unstable) gradients, GGS relies only on the single immediately preceding gradient. This preserves the exploration capability while ensuring the direction aligns with the most recent stable trajectory.

### Mechanism 3: Modularity with Existing Transformations
The GGS mechanism is architecturally compatible with existing input transformation techniques (e.g., DIM, SIM) because it operates on the sampling logic rather than the input space. GGS modifies how the "lookahead point" $\hat{x}_i$ is generated within the inner loop but does not alter the outer-loop update rule, allowing it to stack with methods that transform the input image itself.

## Foundational Learning

- **Concept: Exploitation vs. Exploration in Adversarial Transferability**
  - Why needed here: This is the central thesis of the paper. Exploitation (maximizing loss on the surrogate) often leads to "sharp" maxima that don't transfer. Exploration (finding flat regions) helps transfer but can reduce attack potency.
  - Quick check question: Why does a high loss value on the surrogate model not guarantee a successful attack on a target black-box model?

- **Concept: Flat Maxima vs. Sharp Maxima**
  - Why needed here: The paper grounds its success on finding "flat" regions of the loss surface. A sharp maximum is sensitive to small changes in model parameters (overfitting), while a flat maximum is robust to them (transferable).
  - Quick check question: If you visualize the loss surface, why is a "flat" peak generally considered more robust for transfer attacks than a "narrow" peak?

- **Concept: Nesterov Accelerated Gradient (NAG) / Lookahead**
  - Why needed here: The authors adapt the concept of NAG (looking ahead before calculating the gradient) to the inner-iteration sampling process. Understanding standard NAG helps clarify why they use the *previous* gradient to guide the current sample.
  - Quick check question: In standard NAG, where is the gradient calculated relative to the current position?

## Architecture Onboarding

- **Component map:** Clean image $x$ -> Inner-Loop (GGS Module) -> Outer-Loop (MI-FGSM) -> Projection to $\epsilon$-ball -> Adversarial example
- **Critical path:** The implementation hinges on Equation 5 in Section 3.3. You must ensure the random noise $\tilde{p}$ is applied to the *magnitude* while the *sign* is determined by the previous gradient $\tilde{g}_{i-1}$.
- **Design tradeoffs:**
  - Hyperparameter $\zeta$ (Sampling Radius): The paper sets $\zeta = 2.0 \times \epsilon$. Too small $\rightarrow$ insufficient exploration. Too large $\rightarrow$ the gradient guidance becomes irrelevant noise.
  - Computational Overhead: GGS requires $N$ forward/backward passes per outer iteration (typically $N=20$), significantly slower than single-step methods but comparable to other inner-iteration methods.
- **Failure signatures:**
  - High Variance in ASR: If the attack success rate fluctuates wildly across similar models, the "flatness" constraint may not be activating (check $\zeta$).
  - Surrogate Overfitting: If the white-box success rate is 100% but black-box is near 0%, the method has collapsed to pure Exploitation.
- **First 3 experiments:**
  1. Baseline Validation (Ablation): Compare Random Sampling (RS) vs. Momentum-Guided (MGS) vs. GGS on a single surrogate attacking a dissimilar architecture to verify the "single-step dependency" claim.
  2. Hyperparameter Sensitivity ($\zeta$): Run a sweep on the sampling radius factor (e.g., $1.0 \times \epsilon$ to $4.0 \times \epsilon$) to find the "flat maxima" sweet spot.
  3. Compatibility Check: Integrate GGS with an input transformation method like DIM or TIM. If performance does not improve over baseline DIM/TIM, the gradient guidance may be conflicting with the input transformations.

## Open Questions the Paper Calls Out

- Can the Gradient-Guided Sampling (GGS) mechanism be effectively adapted for transfer attacks that do not rely on gradient averaging, such as variance-tuning or reverse perturbation methods? The paper aims to refine it to also support non-gradient-averaging techniques (e.g., VMI-FGSM, RAP).
- Is there a quantifiable relationship between the spatial regularity (morphology) of adversarial perturbations and their cross-model transferability? The paper hopes future work can uncover this relationship between noise morphology and transferability.
- How can GGS be further stabilized to maintain superiority against advanced diffusion-based purification defenses? The paper notes GGS performance drops significantly against DiffPure compared to other defenses.

## Limitations

- The paper assumes flat maxima regions exist and are accessible via gradient-informed neighborhood sampling, but provides limited empirical evidence of the actual flatness of achieved loss surfaces.
- While GGS shows strong performance on standard vision models and some MLLMs, its effectiveness on diverse multimodal architectures and real-world commercial APIs may vary significantly.
- The compatibility claims with input transformations are demonstrated empirically but lack theoretical justification for why these methods compound additively rather than interfering destructively.

## Confidence

- **High confidence**: The core mechanism of single-step gradient dependency improving over long-chain momentum is well-supported by ablation studies and theoretical reasoning.
- **Medium confidence**: The overall performance gains over state-of-the-art methods are well-demonstrated across multiple benchmarks, though some comparisons lack statistical significance testing.
- **Medium confidence**: The modularity claim with input transformations is empirically validated but not theoretically grounded.

## Next Checks

1. Conduct landscape analysis (e.g., eigenvalue spectrum of Hessian) on adversarial examples generated by GGS to verify they indeed reside in flatter regions compared to baselines.
2. Test GGS on a broader range of commercial APIs and multimodal models not included in the original evaluation to assess real-world transferability.
3. Perform ablation studies isolating the contribution of the gradient guidance mechanism versus the inner-iteration sampling framework to determine the true source of performance gains.