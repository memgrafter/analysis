---
ver: rpa2
title: Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual
  Perception
arxiv_id: '2509.02324'
source_url: https://arxiv.org/abs/2509.02324
tags:
- cloth
- manipulation
- language
- task
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of language-guided long-horizon
  manipulation of deformable objects, specifically cloth folding, which involves high
  degrees of freedom, complex dynamics, and the need for accurate vision-language
  grounding. The authors propose a unified framework that integrates an LLM-based
  planner to decompose high-level language instructions into low-level action primitives,
  a VLM-based perception module with bidirectional cross-attention fusion and DoRA
  fine-tuning for language-conditioned fine-grained visual grounding, and a task execution
  module.
---

# Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception

## Quick Facts
- arXiv ID: 2509.02324
- Source URL: https://arxiv.org/abs/2509.02324
- Reference count: 40
- Key outcome: This paper addresses the challenge of language-guided long-horizon manipulation of deformable objects, specifically cloth folding, which involves high degrees of freedom, complex dynamics, and the need for accurate vision-language grounding. The authors propose a unified framework that integrates an LLM-based planner to decompose high-level language instructions into low-level action primitives, a VLM-based perception module with bidirectional cross-attention fusion and DoRA fine-tuning for language-conditioned fine-grained visual grounding, and a task execution module. In simulation, the method outperforms state-of-the-art baselines by 2.23%, 1.87%, and 33.3% on seen instructions, unseen instructions, and unseen tasks, respectively. On a real robot, it robustly executes multi-step folding sequences from language instructions across diverse cloth materials and configurations, demonstrating strong generalization in practical scenarios.

## Executive Summary
This work tackles the challenge of executing complex cloth-folding tasks from natural language instructions. The authors propose a unified framework that integrates an LLM-based planner to decompose high-level language instructions into low-level action primitives, a VLM-based perception module with bidirectional cross-attention fusion and DoRA fine-tuning for language-conditioned fine-grained visual grounding, and a task execution module. The approach is evaluated in simulation and on a real robot, demonstrating strong generalization to unseen instructions and tasks, as well as robust performance across diverse cloth materials and configurations.

## Method Summary
The framework combines an LLM-based planner with a VLM-based perception module for language-guided cloth manipulation. The LLM planner (GPT-4o) decomposes high-level language instructions into structured sub-tasks using in-context instruction learning and chain-of-thought reasoning. The VLM perception module employs a SigLIP2-driven architecture with bidirectional cross-attention fusion between segmented language features and visual features, fine-tuned using DoRA to achieve language-conditioned fine-grained visual grounding. The system generates pick-and-place heatmaps for executing the decomposed sub-tasks, with real-world deployment on a UR5 robot with RealSense D455.

## Key Results
- Outperforms state-of-the-art baselines by 2.23%, 1.87%, and 33.3% on seen instructions, unseen instructions, and unseen tasks respectively in simulation
- Robustly executes multi-step folding sequences from language instructions on a real robot across diverse cloth materials
- Demonstrates strong generalization to unseen instructions and tasks while maintaining performance across different cloth configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing high-level language instructions into structured sub-tasks via LLM enables generalization to unseen tasks
- Mechanism: GPT-4o with in-context instruction learning (ICIL) and chain-of-thought (CoT) reasoning parses commands like "Fold the T-Shirt in thirds" into atomic pick-and-place operations (e.g., "Grasp the left sleeve and place to middle")
- Core assumption: LLMs trained on general text can reason about spatial manipulation for deformable objects without domain-specific robotic training data
- Evidence anchors:
  - [abstract]: "The LLM-based planner decomposes high-level language instructions into low-level action primitives, bridging the semantic–execution gap, aligning perception with action, and enhancing generalization"
  - [section III-B]: "The decomposition process is facilitated through carefully crafted prompts incorporating in-context instruction learning (ICIL) and chain-of-thought (CoT) reasoning"
  - [corpus]: Gondola (FMR=0.578) confirms LLM planning improves generalization but grounding remains challenging; corpus lacks direct replication of this decomposition approach
- Break condition: If spatial reasoning requires physical simulation or novel fold topologies not represented in prompt examples, decomposition may produce physically infeasible sub-task sequences

### Mechanism 2
- Claim: Bidirectional cross-attention between segmented language features and visual features enables precise grounding for pick-and-place operations
- Mechanism: Instructions split at "and" create separate pick/place text segments; image-to-language attention (visual queries, text keys/values) and language-to-image attention (text queries, visual keys/values) are fused and residual-connected
- Core assumption: The coordinating conjunction "and" reliably separates pick from place semantics, and cloth states are sufficiently discriminative in RGB-D features
- Evidence anchors:
  - [abstract]: "VLM-based perception module employs a SigLIP2-driven architecture with a bidirectional cross-attention fusion mechanism"
  - [section III-C]: "We segment the token-level language features based on the coordinating conjunction 'and'... Each segment is fused with visual tokens via bidirectional cross-attention"
  - [corpus]: No direct corpus evidence on bidirectional attention for manipulation; related work (ThinkAct, EchoVLA) uses latent planning but not this fusion architecture
- Break condition: If instructions omit "and" or use alternative phrasings (e.g., "move X to Y"), segmentation fails; if cloth self-occlusion hides target regions, visual grounding degrades

### Mechanism 3
- Claim: Weight-Decomposed Low-Rank Adaptation (DoRA) on query/value matrices improves cloth-specific perception over LoRA and full fine-tuning
- Mechanism: DoRA decomposes pretrained weights into magnitude and direction components, applying low-rank updates only to direction while preserving pretrained knowledge; applied to attention layers in frozen SigLIP2
- Core assumption: Cloth-specific features lie in a low-dimensional subspace that can be captured without modifying full backbone weights
- Evidence anchors:
  - [abstract]: "Weight-Decomposed Low-Rank Adaptation (DoRA)-based fine-tuning to achieve language-conditioned fine-grained visual grounding"
  - [table III]: DoRA achieves 86.40% average SR vs. LoRA (84.97%), IA3 (82.79%), and no adaptation (75.38%)
  - [corpus]: Corpus papers do not evaluate DoRA; parameter-efficient tuning for manipulation is underexplored in neighbors
- Break condition: If cloth textures/colors are out-of-distribution from SigLIP2 pretraining (e.g., highly reflective fabrics), low-rank adaptation may be insufficient; larger rank or partial unfreezing may be needed

## Foundational Learning

- **Cross-Attention in Vision-Language Models**
  - Why needed here: Understanding how queries, keys, values flow between modalities is essential for debugging the bidirectional fusion module and modifying the attention mechanism
  - Quick check question: Can you trace how a text token "sleeve" attends to image patches and vice versa in the cross-attention equations?

- **Parameter-Efficient Fine-Tuning (LoRA vs. DoRA)**
  - Why needed here: The paper claims DoRA outperforms LoRA; understanding weight decomposition helps diagnose if failures stem from insufficient adaptation capacity or catastrophic forgetting
  - Quick check question: What components does DoRA decompose weights into, and which component receives low-rank updates?

- **Sim-to-Real Transfer for Deformable Objects**
  - Why needed here: Simulation uses SoftGym/FleX; real-world uses UR5 + RealSense; understanding the sim-to-real gap informs data augmentation and domain randomization strategies
  - Quick check question: What visual and dynamics differences between SoftGym and real cloth might cause sim-to-real performance drops?

## Architecture Onboarding

- **Component map**:
  - ASR (Baidu Qianfan) → Text command → LLM Planner (GPT-4o) → Sub-task sequence
  - Sub-task + RGB-D → Grounding DINO (bbox) → SAM (mask) → Cropped image
  - Cropped image + sub-task text → SigLIP2 (frozen) + DoRA adapters → Bidirectional cross-attention → Two CUN decoders → Pick/place heatmaps
  - Heatmap argmax → Depth projection → Robot Base transform → URScript primitives

- **Critical path**:
  - Instruction parsing (LLM) → Text segmentation at "and" → Cross-modal fusion → Heatmap prediction → Action execution
  - If any step fails (e.g., LLM outputs invalid sub-task, segmentation misses "and", heatmap has low confidence), the pipeline halts or produces invalid actions

- **Design tradeoffs**:
  - Frozen SigLIP2 + DoRA vs. full fine-tuning: Preserves pretrained knowledge but may limit adaptation to extreme cloth variations
  - Single-arm vs. dual-arm: Paper uses single UR5; folding would be faster with dual-arm but requires coordination and larger action space
  - Cross-attention vs. transformer fusion: Ablation shows cross-attention improves SR by 2.01%; transformer may be more expressive but harder to train

- **Failure signatures**:
  - Low heatmap confidence (Qpick/Qplace < 0.3): Indicates visual grounding failure; check lighting, cloth color, or segmentation quality
  - LLM outputs incomplete sub-tasks: Prompt engineering issue; add few-shot examples or validate output schema
  - High MPD but high MIoU: Overall shape is correct but fine alignment is off; adjust depth calibration or grasp precision

- **First 3 experiments**:
  1. Replicate simulation results on SoftGym with provided dataset (15,000 training, 750 test); verify SR matches reported 91.7% (SI), 89.6% (UI), 77.92% (UT)
  2. Ablate DoRA vs. LoRA on held-out cloth types (e.g., shorts) to confirm 1.43% improvement holds across categories
  3. Real-world sim-to-real test on 3 unseen cloth materials; measure SR drop and analyze failure modes (perception vs. execution)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to dual-arm coordinated manipulation for cloth tasks that inherently require bimanual coordination?
- Basis in paper: [explicit] The conclusion states: "Future work will focus on expanding this approach to dual-arm setups and other cloth categories."
- Why unresolved: The current system uses a single UR5 arm with sequential pick-and-place primitives. Many complex folding operations (e.g., shaking out wrinkles, precise alignment) benefit from simultaneous bimanual coordination, which requires new representations for arm synchronization and collision avoidance.
- What evidence would resolve it: Demonstration of dual-arm cloth manipulation with comparable or improved success rates on tasks designed for bimanual execution, along with analysis of coordination strategies.

### Open Question 2
- Question: How robust is the system to mid-task execution failures, and can the LLM planner dynamically replan when sub-tasks fail?
- Basis in paper: [inferred] The perception-action loop (Algorithm 1) proceeds sequentially without explicit error detection or recovery mechanisms. If a pick-and-place action fails (e.g., slips, misfolds), the system has no stated pathway to detect failure or request replanning.
- Why unresolved: Cloth manipulation is inherently stochastic—grasps slip, fabrics bunch unpredictably. Long-horizon tasks amplify failure risk, yet the framework assumes each sub-task succeeds before proceeding.
- What evidence would resolve it: Experiments with injected failures (e.g., forced grasp drops) measuring recovery success rates with and without dynamic replanning modules.

### Open Question 3
- Question: Can the approach maintain performance when replacing proprietary LLMs (GPT-4o) with open-source alternatives, and what is the minimum LLM capability required for reliable task decomposition?
- Basis in paper: [inferred] The ablation study (Table IV) shows GPT-4o achieves 100% planning success while alternatives like Doubao-1.5 drop to 20-80% across tasks. The framework depends on this proprietary API, raising deployability and cost concerns.
- Why unresolved: The planning module's reliance on GPT-4o creates dependencies on external services. Understanding the minimal reasoning capabilities needed for reliable decomposition would enable more deployable alternatives.
- What evidence would resolve it: Systematic evaluation of open-source LLMs (e.g., Llama, Mistral variants) with analysis of which reasoning capabilities (e.g., spatial understanding, temporal ordering) correlate with planning success.

## Limitations

- **Data dependency and domain shift**: The method relies on 15,750 demonstrations from SoftGym/FleX simulation. While the framework shows generalization to unseen instructions and tasks in simulation, the real-world performance depends on how well the simulated cloth dynamics and appearance transfer to physical materials.
- **Prompt engineering brittleness**: Task decomposition depends on carefully crafted GPT-4o prompts with ICIL and CoT reasoning. The approach assumes that LLMs can generalize spatial reasoning for deformable objects without robotic training data.
- **Grounding segmentation assumption**: The bidirectional cross-attention mechanism assumes instructions contain "and" to separate pick/place operations. Alternative phrasings or complex commands may break this segmentation, causing grounding failures.

## Confidence

**High confidence**: The simulation results on seen/unseen instructions and tasks are well-supported by quantitative metrics (SR, MPD, MIoU). The ablation studies demonstrating cross-attention and DoRA improvements are methodologically sound.

**Medium confidence**: Real-world performance claims are supported by qualitative demonstrations but lack extensive quantitative validation across diverse materials and failure modes. The sim-to-real transfer appears successful but the sample size is limited.

**Low confidence**: Claims about LLM generalization to novel spatial reasoning tasks rely on prompt engineering without systematic evaluation of failure modes or alternative prompting strategies.

## Next Checks

1. **Sim-to-real stress test**: Systematically test the method on 10+ diverse cloth materials (including silk, wool, patterned fabrics, and reflective materials) to quantify performance degradation and identify specific failure modes in visual grounding or motion planning.

2. **Prompt robustness evaluation**: Systematically vary instruction phrasings (remove "and", use alternative prepositions, add noise) to quantify decomposition failure rates and identify prompt engineering limitations.

3. **Cross-attention ablation under occlusion**: Create controlled experiments with increasing cloth self-occlusion levels to measure grounding accuracy degradation and determine failure thresholds for the bidirectional attention mechanism.