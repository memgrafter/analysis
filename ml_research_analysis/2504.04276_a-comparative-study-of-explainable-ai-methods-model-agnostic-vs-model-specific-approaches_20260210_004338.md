---
ver: rpa2
title: 'A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific
  Approaches'
arxiv_id: '2504.04276'
source_url: https://arxiv.org/abs/2504.04276
tags:
- methods
- shap
- feature
- grad-cam
- backpropagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared model-agnostic (LIME, SHAP) and model-specific
  (Grad-CAM, Guided Backpropagation) XAI methods for interpreting ResNet50 image classifications
  across seven species. Each method revealed different aspects of model decision-making:
  model-agnostic techniques provided broader feature attribution across architectures,
  while model-specific approaches highlighted precise activation regions with greater
  computational efficiency.'
---

# A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches

## Quick Facts
- arXiv ID: 2504.04276
- Source URL: https://arxiv.org/abs/2504.04276
- Reference count: 12
- Model-agnostic (LIME, SHAP) and model-specific (Grad-CAM, Guided Backpropagation) XAI methods compared for ResNet50 image classifications across seven species

## Executive Summary
This study provides a comprehensive comparison of model-agnostic and model-specific explainable AI methods for interpreting image classification decisions. The analysis focuses on four key XAI techniques - LIME, SHAP, Grad-CAM, and Guided Backpropagation - applied to a ResNet50 model trained on seven species classifications. Each method reveals different aspects of the model's decision-making process, with model-agnostic approaches offering broader applicability across architectures while model-specific methods provide more precise activation visualizations.

The research highlights the complementary nature of different XAI approaches, demonstrating that no single method is universally superior. Model-agnostic techniques like LIME and SHAP provide explanations that work across any model architecture but can produce variable results across runs. In contrast, model-specific methods such as Grad-CAM and Guided Backpropagation offer more stable visualizations but are limited to specific model types. This comparative analysis provides valuable insights for practitioners seeking to understand and validate their AI models, particularly in high-stakes domains where transparency is critical.

## Method Summary
The study employed a systematic comparative framework to evaluate four XAI methods on a ResNet50 model trained for seven species classification tasks. Model-agnostic methods (LIME and SHAP) were tested alongside model-specific approaches (Grad-CAM and Guided Backpropagation). Each method was applied to the same set of test images, with computational efficiency measured alongside qualitative assessment of explanation characteristics. The evaluation examined how each technique revealed feature importance, activation regions, and decision boundaries, comparing their effectiveness in highlighting relevant image regions that influenced classification outcomes.

## Key Results
- Model-agnostic methods (LIME, SHAP) work with any model architecture but produce variable explanations across runs
- Model-specific methods (Grad-CAM, Guided Backpropagation) provide more precise activation visualizations with greater computational efficiency
- Grad-CAM excels at class-discriminative localization but lacks fine-grained edge details
- Guided Backpropagation effectively highlights edges but can produce noisy visualizations
- Combining multiple XAI methods provides the most comprehensive understanding of complex models

## Why This Works (Mechanism)
The effectiveness of different XAI methods stems from their fundamentally different approaches to interpreting model decisions. Model-agnostic methods treat the model as a black box, perturbing inputs and observing output changes to build local approximations of decision boundaries. LIME creates sparse linear models around specific predictions, while SHAP uses game theory to fairly attribute contributions across features. Model-specific methods leverage internal model structure - Grad-CAM uses gradient information flowing into the final convolutional layer to produce class-discriminative localization maps, while Guided Backpropagation modifies standard backpropagation to highlight relevant features by deactivating certain neurons during the backward pass.

## Foundational Learning
- **Model-Agnostic vs. Model-Specific Tradeoffs**: Understanding when to use black-box approximation methods versus architecture-specific techniques is crucial for selecting appropriate interpretability tools. Quick check: Can the method work without access to model internals?
- **Gradient-Based Visualization**: Grad-CAM and Guided Backpropagation rely on computing gradients through the network to identify important regions. Quick check: Are gradients flowing properly through all layers?
- **Feature Attribution vs. Activation Mapping**: Different XAI methods provide either importance scores for input features or spatial maps of activations. Quick check: Does the method produce scalar importance values or spatial heatmaps?
- **Computational Efficiency Considerations**: Model-specific methods generally require fewer computations than model-agnostic approaches. Quick check: What is the inference time overhead compared to the base model?
- **Stochasticity in Explanations**: Some methods like LIME and SHAP incorporate randomness that affects reproducibility. Quick check: Are explanations consistent across multiple runs with the same parameters?
- **Class-Discriminative vs. Class-Agnostic**: Grad-CAM produces class-specific visualizations while some methods provide general feature importance. Quick check: Does the explanation distinguish between different output classes?

## Architecture Onboarding

Component Map: Input Images -> ResNet50 Model -> XAI Method (LIME/SHAP/Grad-CAM/Guided BP) -> Explanation Output

Critical Path: Image Preprocessing → Model Inference → XAI Computation → Visualization Generation

Design Tradeoffs: Model-agnostic methods offer flexibility across architectures but sacrifice computational efficiency and potentially explanation precision. Model-specific methods provide more accurate visualizations for their target architectures but lack generalizability. The choice depends on deployment constraints, required explanation fidelity, and whether model access is available.

Failure Signatures: Model-agnostic methods may fail when local approximations break down for complex decision boundaries. Model-specific methods fail when applied to incompatible architectures. Both types can produce misleading explanations when the model relies on spurious correlations or when explanations highlight irrelevant features.

First Experiments:
1. Apply each XAI method to a simple binary classification task to verify basic functionality
2. Test explanation reproducibility by running each method multiple times with identical inputs
3. Compare explanation quality on images where ground truth features are known

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single ResNet50 architecture and seven species classifications, constraining generalizability
- Lack of human evaluation to assess explanation quality and usefulness for end users
- No investigation of XAI method biases or vulnerability to adversarial manipulation

## Confidence
- Model-agnostic methods work with any model architecture: High confidence
- LIME and SHAP produce variable explanations across runs: High confidence
- Each method's strengths and weaknesses characterization: Medium confidence
- Combining multiple XAI methods provides best understanding: Medium confidence

## Next Checks
1. Conduct user studies with domain experts to evaluate which explanations are most actionable and trustworthy across different application contexts
2. Test the same comparative framework across multiple model architectures (CNN, transformer, ensemble models) and diverse image classification tasks beyond species identification
3. Evaluate XAI method robustness to adversarial examples and investigate whether explanations themselves can be manipulated to mislead human users