---
ver: rpa2
title: What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup
  Puzzles
arxiv_id: '2508.10358'
source_url: https://arxiv.org/abs/2508.10358
tags:
- agent
- reasoning
- information
- questioner
- responder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TurtleSoup-Bench, a large-scale interactive
  benchmark designed to evaluate the imaginative reasoning capabilities of large language
  models (LLMs). Unlike existing benchmarks that focus on static question-answering
  or social deduction, TurtleSoup-Bench challenges models to iteratively construct,
  test, and revise hypotheses in information-sparse environments using 800 narrative
  puzzles.
---

# What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles

## Quick Facts
- arXiv ID: 2508.10358
- Source URL: https://arxiv.org/abs/2508.10358
- Reference count: 22
- Key outcome: Introduces TurtleSoup-Bench, an 800-puzzle interactive benchmark, and Mosaic-Agent framework; shows leading LLMs achieve ~58% overall score versus human experts at 68%

## Executive Summary
This paper introduces TurtleSoup-Bench, a large-scale interactive benchmark designed to evaluate the imaginative reasoning capabilities of large language models (LLMs). Unlike existing benchmarks that focus on static question-answering or social deduction, TurtleSoup-Bench challenges models to iteratively construct, test, and revise hypotheses in information-sparse environments using 800 narrative puzzles. To support this, the authors propose Mosaic-Agent, a multi-agent framework that simulates the reasoning process through a questioner agent, a responder agent, and a memory module. Evaluation is performed using a multi-dimensional protocol that assesses logical consistency, detail fidelity, and conclusion alignment. Experiments show that leading LLMs, including GPT-4o and Claude-3.7-Sonnet, significantly underperform human experts, achieving overall scores of around 58% versus 68% for humans. The results highlight the limitations of current models in exploratory reasoning and establish a foundation for future research in this area.

## Method Summary
The authors introduce TurtleSoup-Bench, a benchmark of 800 bilingual narrative puzzles designed to test imaginative reasoning through interactive yes/no questioning. The Mosaic-Agent framework decomposes the reasoning process into specialized sub-agents: a Deliberation Agent performs local analysis and periodic global synthesis of belief states, a Meta-cognition Agent classifies puzzle genres and adapts questioning strategies with smoothed confidence thresholds, and an Action Formulation Agent generates and screens candidate questions. A Responder Agent simulates the environment, providing yes/no/unknown answers and flagging Key Clues that represent pivotal reasoning points. The system maintains interaction history and key clue records in a Memory module, with belief states updated every 5 turns and a maximum of 30 questions per puzzle.

## Key Results
- Leading LLMs (GPT-4o, Claude-3.7-Sonnet) achieve ~58% overall score on TurtleSoup-Bench, significantly underperforming human experts at 68%
- Ablation studies show the Key Clue mechanism is most critical, with its removal causing a 10.4 percentage point drop in performance
- Cross-linguistic evaluation reveals a ~40% performance decline for English puzzles compared to Chinese, indicating cultural/linguistic grounding effects
- Qualitative analysis identifies four failure paradigms: Semantic Fixation, Context Construction Failure, Logic Blind Spots, and Deductive Pruning Failure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing question generation into deliberation, meta-cognition, and action stages improves exploratory reasoning quality over end-to-end prompting.
- Mechanism: The Deliberation Agent performs hierarchical cognitive processing (local analysis + periodic global synthesis), Meta-cognition adapts strategy via genre classification, and Action Formulation screens multiple candidates. This separation allows each component to specialize rather than overloading a single prompt.
- Core assumption: Assumes multi-stage cognitive decomposition mirrors effective human problem-solving patterns (cited Binz and Schulz 2023; Schacter et al. 2007) and that this transfers to LLM architectures.
- Evidence anchors:
  - [abstract] "We also propose Mosaic-Agent, a novel agent designed to assess LLMs' performance in this setting."
  - [section 4.1] "We leverage this cognitive model by decoupling our questioner agent's question-generation mechanism into three corresponding processes: deliberation, meta-cognition, and action generation"
  - [corpus] Weak direct corpus support; related work VAGEN discusses world model reasoning for multi-turn agents but doesn't validate cognitive decomposition specifically.
- Break condition: If the task doesn't require iterative hypothesis refinement (e.g., simple factual lookup), decomposition overhead may degrade performance.

### Mechanism 2
- Claim: Explicit `<Key Clue>` feedback signals from the environment dramatically improve exploration efficiency.
- Mechanism: The Responder flags questions that semantically match pre-annotated key clues, creating a high signal-to-noise supervisory signal. This directs the Questioner's attention to pivotal reasoning turning points rather than wandering through low-value questions.
- Core assumption: Assumes key clue annotations exist (expert-annotated) and that binary feedback is sufficient guidance—may not transfer to settings without curated annotations.
- Evidence anchors:
  - [section 4.2] "Not all 'Yes/No' answers are equally important, and the questions that hit upon the core of the puzzle will be highlighted by this flag."
  - [section 5.4] Ablation shows removing Key Clue mechanism causes the most severe performance collapse (57.14 → 46.73), with authors stating "without it, the agent's exploration degrades into a near-random walk"
  - [corpus] No direct corpus comparison; mechanism is novel to this framework.
- Break condition: If key clues are misannotated or the puzzle type doesn't have discrete pivot points, signal becomes noise.

### Mechanism 3
- Claim: Smoothed confidence-based strategy switching prevents catastrophic oscillation while allowing genre-adaptive questioning.
- Mechanism: Meta-cognition maintains an EMA-smoothed confidence score and only switches strategy when `c_smooth_t > c_old + τ_switch`. This adds hysteresis to prevent rapid flip-flopping between strategies (e.g., misclassifying crime as supernatural).
- Core assumption: Assumes puzzles cluster into meaningful genres with distinct optimal strategies, and that genre classification from partial information is tractable.
- Evidence anchors:
  - [section 4.1] Describes Smoothed Confidence mechanism with α=0.7, τ_switch=0.1
  - [section 5.4] Ablation shows Meta-cognition removal drops performance (57.14 → 51.07), proving explicit architectural support is needed for strategy adaptation
  - [corpus] Weak; Ask WhAI probes belief formation in agents but doesn't address strategy switching mechanisms.
- Break condition: If early genre misclassification occurs and key clues don't trigger re-evaluation, the agent can get stuck in wrong strategy.

## Foundational Learning

- **Abductive reasoning**:
  - Why needed here: TurtleSoup puzzles require generating plausible explanations from sparse clues, then testing and revising them—the core of imaginative reasoning as defined in the paper.
  - Quick check question: Given "A man walks into a bar and asks for water. The bartender points a gun. The man says thank you and leaves"—can you form and test hypotheses about what happened?

- **Belief state representation**:
  - Why needed here: The agent maintains a structured JSON belief state `B_t = (L_t, D_t, C_t)` containing logic, details, and conclusions, updated periodically via `f_sum`.
  - Quick check question: If you've learned "the person died" and "there was no violence" and "it happened in winter," how would you structure a belief state?

- **Interactive hypothesis testing**:
  - Why needed here: Unlike static QA, this benchmark evaluates the full process of formulating questions to reduce uncertainty, not just answering fixed queries.
  - Quick check question: Given partial information, what's your next yes/no question to maximally reduce the hypothesis space?

## Architecture Onboarding

- **Component map**:
  ```
  Questioner Agent
    ├── Deliberation Agent
    │   ├── Local Analysis (every turn)
    │   └── Global Deliberation + Belief Update (every k=5)
    ├── Meta-cognition Agent (genre classification + strategy)
    └── Action Formulation (candidate gen + optimal screening)
  Responder Agent ("God" environment)
    ├── Answer Generation (Yes/No/Unknown)
    └── Key Clue Identification (boolean flag)
  Memory Module
    ├── Interaction History H_t (full log)
    └── Key Clue Records K_rec (filtered high-value QA)
  ```

- **Critical path**: Surface → Local Analysis → (periodic) Global Deliberation → Belief State Update → APS Generation → Meta-cognition Strategy → Candidate Questions → Optimal Screening → Question → Responder → Memory Update → repeat (max 30 turns)

- **Design tradeoffs**:
  - Deliberation interval k=5: Longer intervals reduce API costs but risk cognitive myopia; shorter intervals increase coherence but slow gameplay
  - Max turns N_max=30: Limits exploration cost but may truncate complex puzzles
  - Genre strategies: Hand-crafted expert strategies per genre; not learned, may not generalize to new puzzle types
  - Same model for Questioner/Responder: Simplifies deployment but conflates reasoning and comprehension evaluation

- **Failure signatures** (from Section 5.3 qualitative analysis):
  1. **Semantic Fixation**: Model fixates on literal/high-frequency word meanings despite contradictory context (e.g., "kill" → only physical violence)
  2. **Context Construction Failure**: Cannot integrate scattered clues into coherent global model
  3. **Logic Blind Spots**: Cannot conceive atypical causal chains outside training distribution
  4. **Deductive Pruning Failure**: Continues exploring falsified hypotheses after "No" answers

- **First 3 experiments**:
  1. **Run baseline agent without cognitive decomposition**: Remove Deliberation/Meta-cognition/Action separation; use single prompt to generate questions. Compare against full Mosaic-Agent to validate architectural contribution.
  2. **Ablate Key Clue mechanism**: Run Responder without `<Key Clue>` flags. Expect ~10+ point overall score drop per ablation results (57.14 → 46.73).
  3. **Test cross-language transfer**: Run English puzzles with Chinese-trained strategies (or vice versa). Quantify the ~40% performance drop observed in Section 5.2 to understand cultural/linguistic grounding requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs learn to autonomously identify high-value information without explicit supervisory signals like the Key Clue mechanism?
- Basis in paper: [inferred] The ablation study showed that removing the Key Clue mechanism caused the most severe performance collapse (57.14 → 46.73), and the authors note this tag "acts as a high signal-to-noise supervisory signal; without it, the agent's exploration degrades into a near-random walk amid low-value feedback."
- Why unresolved: The paper demonstrates that current models depend heavily on explicit guidance to navigate information-sparse environments, but does not explore whether models can be trained or prompted to develop autonomous information-value assessment.
- What evidence would resolve it: Experiments comparing model performance when Key Clue signals are progressively removed or replaced with learned reward models, measuring whether models can develop intrinsic strategies for prioritizing informative queries.

### Open Question 2
- Question: What causes the systematic cross-linguistic performance decline in imaginative reasoning, and how can it be mitigated?
- Basis in paper: [explicit] The authors explicitly note "a systematic performance decline is observed across almost all models on the English dataset. For example, the average score of deepseek-r1 drops by nearly 40%... The introduction of ambiguity and semantic loss during cross-language conversion also increase the difficulty of reasoning."
- Why unresolved: The paper identifies the problem but does not disentangle whether the decline stems from translation artifacts, cultural context mismatches, or fundamental differences in how models process imaginative reasoning across languages.
- What evidence would resolve it: Controlled experiments with parallel puzzles authored natively in each language versus translated versions, plus analysis of error patterns specific to each language condition.

### Open Question 3
- Question: How can models overcome the four identified failure paradigms (Semantic Fixation, Context Construction Failure, Logic Blind Spots, and Deductive Pruning Failure) through architectural or training improvements?
- Basis in paper: [explicit] The qualitative analysis section explicitly categorizes failures into these four distinct levels and describes their mechanisms. The authors state these "sharply define the boundary of current LLM imagination" but do not propose solutions.
- Why unresolved: The paper provides diagnosis but no intervention—each failure mode likely requires different remediation strategies (e.g., semantic flexibility training vs. explicit belief revision mechanisms).
- What evidence would resolve it: Targeted experiments addressing each failure mode separately, measuring whether specific architectural modifications or fine-tuning approaches reduce failure rates in controlled scenarios designed to trigger each paradigm.

## Limitations
- The framework relies heavily on expert-annotated Key Clue Libraries, which may not generalize to real-world scenarios with ambiguous feedback
- Genre-specific strategies are hand-crafted rather than learned, limiting scalability to new domains
- LLM-as-judge evaluation introduces potential bias as judges may share the same reasoning limitations as the agents being evaluated

## Confidence
- **High Confidence**: The benchmark design (800 puzzles, multi-dimensional scoring) and baseline results (GPT-4o/Claude-3.7-Sonnet at ~58% vs humans at 68%) are empirically grounded and reproducible
- **Medium Confidence**: The Mosaic-Agent architecture's superiority over end-to-end prompting is supported by ablation studies but lacks external validation on non-TurtleSoup domains
- **Low Confidence**: Claims about cognitive decomposition mirroring human problem-solving patterns rely heavily on theoretical citations rather than empirical validation within this framework

## Next Checks
1. **Cross-domain transfer**: Apply Mosaic-Agent to non-narrative puzzles (e.g., scientific reasoning or code debugging) to test whether genre-specific strategies and Key Clue mechanisms generalize beyond TurtleSoup puzzles
2. **Human-in-the-loop comparison**: Have human experts interact with the Responder (without Key Clue flags) to measure the true difficulty gap between human and LLM exploratory reasoning when feedback is ambiguous
3. **Ablation on deliberation frequency**: Systematically vary the k parameter (deliberation interval) from 1 to 10 to quantify the tradeoff between cognitive coherence and API efficiency, identifying optimal settings for different puzzle complexities