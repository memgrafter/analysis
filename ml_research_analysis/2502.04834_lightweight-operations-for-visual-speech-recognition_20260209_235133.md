---
ver: rpa2
title: Lightweight Operations for Visual Speech Recognition
arxiv_id: '2502.04834'
source_url: https://arxiv.org/abs/2502.04834
tags:
- block
- ghost
- module
- recognition
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying visual speech recognition
  (VSR) systems on resource-constrained devices by proposing lightweight architectures
  that reduce computational overhead without compromising accuracy. The authors introduce
  Ghost modules, which replace standard convolutions with more efficient operations,
  and a Partial Temporal Block that splits input feature maps to reduce computation.
---

# Lightweight Operations for Visual Speech Recognition

## Quick Facts
- arXiv ID: 2502.04834
- Source URL: https://arxiv.org/abs/2502.04834
- Authors: Iason Ioannis Panagos; Giorgos Sfikas; Christophoros Nikou
- Reference count: 36
- Primary result: Ghost module-based models achieve up to 74.3% reduction in FLOPs and 44.8% reduction in parameters while maintaining competitive word recognition accuracy on LRW dataset

## Executive Summary
This paper addresses the challenge of deploying visual speech recognition (VSR) systems on resource-constrained devices by proposing lightweight architectures that reduce computational overhead without compromising accuracy. The authors introduce Ghost modules, which replace standard convolutions with more efficient operations, and a Partial Temporal Block that splits input feature maps to reduce computation. Experimental results on the LRW dataset demonstrate that their models achieve competitive accuracy while significantly lowering FLOPs and parameter counts compared to existing methods.

## Method Summary
The proposed method uses Ghost modules to replace standard convolutions in both feature extraction and sequence modeling stages, reducing computational cost through channel reduction and cheap depthwise operations. A Partial Temporal Block splits input channels, processing only a subset while maintaining accuracy through residual connections. The architecture consists of a 3D convolutional input layer, a ResNet-18 backbone with Ghost modules for spatial feature extraction, and a Temporal Convolution Network (TCN) for sequence modeling, followed by a fully-connected classifier. Training uses SGD with cosine annealing, batch size 32, and various data augmentation techniques including MixUp and variable length augmentation on the LRW dataset.

## Key Results
- Ghost modules reduce FLOPs by 74.3% and parameters by 44.8% compared to standard ResNet-18
- Partial Temporal Blocks achieve accuracy improvements of 1.82-4.18% with minimal computational overhead
- DFC attention provides 0.72% accuracy improvement when used with Ghost modules but degrades with reduced input resolution
- GhostV2 + MS-TCN(Ghost) achieves 86.67% accuracy with only 3.60 GFLOPs and 16.7M parameters

## Why This Works (Mechanism)

### Mechanism 1: Ghost Modules
- Claim: Ghost modules reduce computational overhead by replacing expensive convolutions with cheaper linear transformations
- Mechanism: A standard convolution is decomposed into (1) a 1×1 pointwise convolution producing half the target channels, and (2) a cheap 3×3 depthwise convolution applied to those intermediate features. The outputs are concatenated to match the original channel dimension. This exploits redundancy in feature maps—many learned filters produce similar "ghost" activations that can be approximated.
- Core assumption: Feature map redundancy exists in VSR networks; spatial relationships captured by full convolutions can be approximated from reduced-channel representations.
- Evidence anchors: [Section III-B] "A ghost module achieves low computational overhead in two steps... a regular 1 × 1 convolution generates a set of feature maps from the input... A 'cheap operation' uses these intermediate feature maps to produce an additional set" [Table VI] ResNet-18 with Ghost module: 2.13 GFLOPs vs 8.29 baseline (−74.3%); 2.83M vs 11.16M parameters (−74.6%)

### Mechanism 2: Partial Temporal Block
- Claim: Partial Temporal Block reduces sequence modeling cost by processing only a fraction of input channels
- Mechanism: Input tensor X is split along channels into X₁ and X₂. Operation F applies to X₁ while X₂ passes through unchanged (identity). Outputs concatenate with a residual connection: X_out = concat([F(X₁), X₂]) + X. The split ratio controls the compute-accuracy tradeoff.
- Core assumption: Temporal features are sufficiently distributed across channels that processing a subset preserves discriminative information for word classification.
- Evidence anchors: [Section III-C] "Our block allows for a wide network design flexibility as it can be tailored to each specific application constraints... the channel split operation divides the input in two parts along the channel dimension according to a fixed ratio" [Table IV] Ratio 0.25→0.75 adds only 0.05 GFLOPs and 1.6M parameters while improving accuracy by 1.82%–4.18%

### Mechanism 3: DFC Attention
- Claim: DFC attention recovers spatial information lost by Ghost module's channel reduction
- Mechanism: Directional fully-connected (DFC) attention applies vertical and horizontal 1D aggregations via pooled feature maps, producing an attention mask upsampled to original resolution. This captures long-range spatial dependencies without quadratic complexity.
- Core assumption: Lip-reading benefits from global spatial context (e.g., mouth corners relative to center) that local 3×3 depthwise convolutions miss.
- Evidence anchors: [Section III-B] "DFC attention utilizes two fully-connected layers which are applied to the input features in a sequential manner, spanning both the vertical and horizontal directions" [Table II] GhostV2 + MS-TCN (both Ghost): 87.39% vs 86.67% accuracy (+0.72%) with 11.05M additional parameters [Section IV-G Limitations] "DFC module was originally designed for images of higher dimensions (224 × 224)... additional down-sampling performed by the DFC attention module removes much of the information"

## Foundational Learning

- Concept: **Depthwise Separable Convolutions**
  - Why needed here: Ghost modules build directly on depthwise convolutions as the "cheap operation"—understanding the FLOPs reduction from separating spatial and channel mixing is prerequisite
  - Quick check question: Given an input of 64 channels, 56×56 spatial, and 3×3 kernel producing 128 output channels, calculate FLOPs for standard conv vs depthwise separable (depthwise + 1×1 pointwise)

- Concept: **Temporal Convolution Networks (TCN) with Dilated Convolutions**
  - Why needed here: The sequence modeling backbone uses 1D causal convolutions with exponentially increasing dilation to capture long-range temporal dependencies without recurrence
  - Quick check question: For a 4-stage TCN with dilation factors [1, 2, 4, 8] and kernel size 3, what is the effective receptive field at the final layer?

- Concept: **Residual Connections and Feature Concatenation**
  - Why needed here: Both Ghost modules and Partial Temporal Blocks rely on skip connections for gradient flow; Partial blocks use channel concatenation rather than addition
  - Quick check question: When splitting channels with ratio 0.5 and applying operations to only one branch, why does X_out = concat([F(X₁), X₂]) + X require X to have the same total channels as the concatenated output?

## Architecture Onboarding

- Component map: Input video (29 frames, 96×96 mouth crops) → 3D Conv (initial spatio-temporal embedding) → [ResNet-18 backbone with Ghost modules] ← Feature extraction → [Temporal Convolution Network] ← Sequence modeling → Fully-connected classifier → Softmax → 500 word classes

- Critical path:
  1. Ghost module placement: Replace 3×3 convolutions in ResNet blocks first (largest FLOPs reduction), then TCN layers
  2. Split ratio tuning: Start with 0.5 for Partial blocks; ablate on validation set before final training
  3. DFC attention: Only apply to feature extractor (not TCN)—1D convolutions in TCN are incompatible with 2D directional attention

- Design tradeoffs:
  | Configuration | FLOPs | Parameters | Accuracy | Use Case |
  |---------------|-------|------------|----------|----------|
  | ResNet-18 + MS-TCN (baseline) | 10.31G | 36.4M | 85.3% | Reference |
  | Ghost(both) + MS-TCN(Ghost) | 3.60G | 16.7M | 86.67% | Balanced |
  | Ghost + Partial(FasterNet, r=0.75) | 3.20G | 12.2M | 86.48% | Ultra-light |
  | Ghost + Partial(ShuffleNet, k=3) | 3.05G | 5.5M | 81.93% | Minimal |

- Failure signatures:
  - Accuracy drops >5% with Ghost modules: Check if input resolution too low (DFC pooling removes information)
  - Partial block underperforms at high split ratios: Dilated convolutions may miss local context; reduce kernel size or dilation
  - Training instability with Partial blocks: Ensure skip connection adds original X (not just concatenated output)
  - DFC provides no improvement: Likely applied to already-downsampled features; remove pooling in DFC for 96×96 inputs

- First 3 experiments:
  1. **Ablate Ghost vs standard convolution in ResNet-18 only** — Isolate FLOPs/accuracy tradeoff in feature extraction before modifying TCN. Target: Confirm ~70% FLOPs reduction with <2% accuracy drop.
  2. **Compare Partial Temporal Block variants at ratio 0.5** — Train FasterNet, ShuffleNet, and Temporal block designs with identical backbones. Target: Identify best compute/accuracy frontier.
  3. **Validate DFC attention on Ghost vs standard backbone** — Test if DFC helps only when capacity is constrained (Ghost) as paper suggests. Target: Reproduce +0.72% gain only in GhostV2 configuration.

## Open Questions the Paper Calls Out
None

## Limitations
- Validation is primarily limited to the LRW dataset with 500 word classes, raising questions about generalizability to continuous speech or larger vocabularies
- DFC attention module's effectiveness is not thoroughly validated across different backbone architectures and degrades on reduced 96×96 input resolution
- Several implementation details are underspecified, including exact preprocessing pipeline parameters, MixUp alpha values, and variable length augmentation settings

## Confidence

- **High Confidence**: Claims about Ghost module FLOPs and parameter reductions (74.3% and 44.8% respectively) are well-supported by systematic ablation studies across multiple architectures
- **Medium Confidence**: Claims about accuracy maintenance (within 2-4% of baseline) are supported by extensive experiments but limited to one dataset and task
- **Low Confidence**: Claims about DFC attention benefits are undermined by the paper's own admission of limited effectiveness on reduced-resolution inputs and lack of cross-dataset validation

## Next Checks

1. **Cross-dataset validation**: Evaluate Ghost module-based models on LRW-1000 or other VSR datasets with different vocabulary sizes to test generalization beyond 500-word classification
2. **DFC attention ablation study**: Systematically test DFC attention with varying input resolutions (from 224×224 down to 96×96) and with/without the problematic pooling operations to isolate the source of performance degradation
3. **Partial Temporal Block feature analysis**: Conduct a channel-wise sensitivity analysis to determine whether discriminative temporal features concentrate in specific channel ranges, validating or refuting the core assumption that processing a subset preserves information