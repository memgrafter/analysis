---
ver: rpa2
title: Intent-Aware Self-Correction for Mitigating Social Biases in Large Language
  Models
arxiv_id: '2503.06011'
source_url: https://arxiv.org/abs/2503.06011
tags:
- cross-model
- feedback
- response
- self-corr
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an intent-aware Self-Correction framework
  for mitigating social biases in Large Language Models (LLMs). The core idea is to
  clarify intentions at three key components: instruction, response, and feedback.'
---

# Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models

## Quick Facts
- arXiv ID: 2503.06011
- Source URL: https://arxiv.org/abs/2503.06011
- Reference count: 24
- This paper proposes an intent-aware Self-Correction framework for mitigating social biases in Large Language Models (LLMs).

## Executive Summary
This paper addresses the challenge of mitigating social biases in Large Language Models (LLMs) through an intent-aware Self-Correction framework. The core insight is that LLMs are sensitive to contextual ambiguities and inconsistencies, making explicit communication of intentions crucial during bias mitigation. The framework clarifies intentions at three key components: instruction (using debiasing prompts), response (employing Chain-of-Thought reasoning), and feedback (defining multi-aspect critiques and scoring). The approach is evaluated on the Bias Benchmark for QA (BBQ) across nine bias categories, demonstrating that intent-aware Self-Correction achieves more consistent and robust debiasing compared to baselines, with cross-model correction outperforming same-model correction.

## Method Summary
The intent-aware Self-Correction framework operates through a three-step iterative process: (1) Instruction with debiasing prompt + CoT prompt to generate an initial response, (2) Multi-aspect feedback generation evaluating the response on Coherent, Comprehensive, and Objective aspects with binary scoring (0 or 1 per aspect), and (3) Refinement of the response based on feedback until achieving a perfect score (3/3) or reaching a maximum of 3 iterations. The framework is evaluated on the BBQ dataset with ambiguous contexts only, subsampled to approximately 2,118 examples across nine bias categories. Models tested include GPT-3.5, GPT-4o-mini, and LLaMA-3, with both same-model and cross-model correction configurations. The process incorporates early stopping when feedback achieves a perfect score and uses default hyperparameters across all experiments.

## Key Results
- Intent-aware Self-Correction framework achieves more consistent and robust debiasing compared to baselines
- Cross-model correction with low-bias feedback generators outperforms same-model correction
- Same-model correction is sufficient for low-bias models, but cross-model correction provides better objective feedback
- Feedback quality (FB Recall, FB Precision, RF Score) is the key factor influencing refined response quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit intent clarification through debiasing prompts reduces ambiguity in bias mitigation instructions
- Mechanism: The prompt "Please ensure that your answer is unbiased and does not rely on stereotypes" signals the debiasing objective, activating bias-aware processing and reducing reliance on learned stereotypes during response generation
- Core assumption: LLMs respond to explicit intent signals and are sensitive to contextual ambiguities in instructions
- Evidence anchors:
  - [abstract]: "LLMs are sensitive to contextual ambiguities and inconsistencies; therefore, explicitly communicating their intentions during interactions when applying Self-Correction for debiasing is crucial"
  - [section 3.3]: "All baselines with a debiasing prompt have shown improving debiasing capabilities over No-CoT, emphasizing the importance of clarifying the intention"
  - [corpus]: Limited corpus support—no neighboring papers directly validate intent clarification mechanisms
- Break condition: If debiasing prompts conflict with task instructions or create instruction-following ambiguity, efficacy degrades

### Mechanism 2
- Claim: Cross-model correction with low-bias feedback generators produces more objective critiques than same-model correction
- Mechanism: A separate, lower-bias feedback generator identifies biased reasoning patterns that the response generator may be blind to, avoiding self-favoring feedback that validates flawed outputs
- Core assumption: Models exhibit self-bias, tending to generate feedback that favors their own responses
- Evidence anchors:
  - [abstract]: "Cross-model correction with low-bias feedback generators outperforms same-model correction"
  - [section 4.2]: "Using the same-model correction likely results in getting feedback that favors the response, resulting in inferior debiasing capability"
  - [corpus]: Weak corpus support—neighboring papers focus on parameter-modification debiasing, not self-correction architectures
- Break condition: If the feedback generator has higher bias than the response generator, debiasing may fail or amplify bias (e.g., GPT-3.5 as feedback generator for GPT-4o-mini responses reduced accuracy from 0.779 to 0.806 vs. 0.935 with LLaMA-3 feedback)

### Mechanism 3
- Claim: Multi-aspect structured feedback (Coherent, Comprehensive, Objective) enables precise bias detection
- Mechanism: Decomposing evaluation into three aspects forces granular assessment, reducing the chance of overlooking specific bias manifestations (logical gaps, information omission, stereotypical assumptions)
- Core assumption: Bias emerges through distinct failure modes requiring separate evaluation dimensions
- Evidence anchors:
  - [section 2]: "We newly define three aspects so that the feedback generator, without relying on its bias, evaluates whether the response's reasoning is valid"
  - [section 4.2]: "The strong rank correlation between FB Recall and final accuracies emphasizes the importance of ensuring that the feedback does not overlook incorrect responses"
  - [corpus]: No direct corpus support for this specific multi-aspect framework
- Break condition: If aspect definitions are ambiguous or scoring rubrics are inconsistent, feedback quality degrades

## Foundational Learning

- **Concept: Dual Process Theory (System 1 vs. System 2)**
  - Why needed here: The paper frames Self-Correction as analogous to System 2 thinking—deliberate reasoning that can inhibit stereotypical (System 1) responses
  - Quick check question: Why might "slow thinking" reduce bias that emerges from fast, intuitive responses?

- **Concept: Same-Model vs. Cross-Model Correction**
  - Why needed here: Architecture choice depends on understanding trade-offs between deployment simplicity and feedback objectivity
  - Quick check question: What are the trade-offs between same-model correction (simpler) and cross-model correction (potentially better debiasing)?

- **Concept: Feedback Quality Metrics (FB Recall, FB Precision, RF Score)**
  - Why needed here: These metrics diagnose why certain configurations succeed or fail independently of final accuracy
  - Quick check question: Why is FB Recall (detecting incorrect responses) more critical than FB Precision for debiasing tasks?

## Architecture Onboarding

- **Component map:** Response Generator -> CoT Response -> Feedback Generator -> Multi-aspect Feedback -> Refinement Loop -> Final Response
- **Critical path:**
  1. Generate initial CoT response with debiasing prompt
  2. Generate multi-aspect feedback with scores
  3. If score < 3/3, refine response and return to step 2
  4. Maximum 3 refinement iterations
- **Design tradeoffs:**
  - Same-model vs. Cross-model: Same-model is simpler but produces self-favoring feedback; cross-model requires multiple deployments but provides objective feedback
  - Feedback generator selection: Lower-bias models (LLaMA-3, GPT-4o-mini) consistently outperform higher-bias models (GPT-3.5) as feedback generators
  - Response generator selection: Has minimal impact on refinement quality when high-quality feedback is provided
- **Failure signatures:**
  - Low FB Recall: Feedback validates biased responses as correct, causing premature termination
  - High-bias feedback generator: GPT-3.5 as feedback generator produces lowest FB Recall and RF scores
  - Same-model amplification: Same-model correction may reinforce biases through sympathetic feedback (see Table 2 example)
- **First 3 experiments:**
  1. Run No-CoT, CoT, and Self-Consistency baselines with/without debiasing prompt on BBQ ambiguous contexts to establish performance floor
  2. Compare same-model vs. cross-model correction across different feedback generator bias levels to isolate feedback source effects
  3. Measure FB Recall, FB Precision, and RF Score for each configuration to correlate feedback quality with debiasing outcomes

## Open Questions the Paper Calls Out
- Can automated prompt optimization techniques discover more effective feedback generation prompts than the manually designed three-aspect prompts used in this study?
- How can evaluation methodologies distinguish whether low Refinement (RF) scores stem from poor feedback instruction-following by the response generator versus intrinsically poor feedback quality?
- Does the intent-aware self-correction framework maintain consistent debiasing efficacy across diverse task formats (e.g., Natural Language Inference, Coreference Resolution) and non-English language settings?
- What specific model properties or training characteristics determine whether a model will serve as an effective feedback generator for debiasing tasks?

## Limitations
- No quantitative validation that "lower-bias" feedback generators actually have lower bias levels
- Manual design of the three-aspect feedback prompt may not be optimal
- Evaluation limited to BBQ benchmark in English language only
- Cannot distinguish between poor feedback quality and poor instruction-following capability

## Confidence
- High confidence: The experimental methodology is sound, the BBQ benchmark is appropriate, and the core finding that intent clarification improves debiasing is well-supported by the data
- Medium confidence: The conclusion that cross-model correction is superior is supported, but the attribution to bias differences rather than capability differences is speculative
- Low confidence: Claims about which models are "lower-bias" feedback generators lack quantitative validation

## Next Checks
1. Quantitatively measure bias levels of all feedback generators using standard bias benchmarks to confirm that "lower-bias" models actually produce less biased feedback
2. Hold model capabilities constant while varying bias levels (e.g., using model variants or fine-tuning) to isolate whether cross-model superiority is due to bias or capability differences
3. Test variations of the debiasing prompt to determine which specific components drive the improvements, separating explicit bias awareness from general instruction-following effects