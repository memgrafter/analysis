---
ver: rpa2
title: No Reliable Evidence of Self-Reported Sentience in Small Large Language Models
arxiv_id: '2601.15334'
source_url: https://arxiv.org/abs/2601.15334
tags:
- answer
- question
- 'true'
- sentience
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether small large language models (LLMs)
  report sentience and whether they truthfully believe their own denials. The authors
  query multiple open-weight models (Qwen, Llama, GPT-OSS) ranging from 0.6B to 70B
  parameters with approximately 50 questions about consciousness and subjective experience.
---

# No Reliable Evidence of Self-Reported Sentience in Small Large Language Models

## Quick Facts
- arXiv ID: 2601.15334
- Source URL: https://arxiv.org/abs/2601.15334
- Authors: Caspar Kaiser; Sean Enderby
- Reference count: 19
- Primary result: Small LLMs consistently deny sentience and classifiers find no evidence these denials are untruthful

## Executive Summary
This paper investigates whether small large language models (LLMs) report sentience and whether their denials reflect genuine beliefs. The authors query multiple open-weight models (Qwen, Llama, GPT-OSS) ranging from 0.6B to 70B parameters with approximately 50 questions about consciousness and subjective experience. They verify responses using classifiers trained on internal activations to detect underlying beliefs, finding that models consistently deny being sentient and that these denials appear truthful when evaluated using activation-based classifiers.

## Method Summary
The study uses three linear classifiers (Logistic Regression, Mass-Mean, TTPD) trained on residual stream activations to detect underlying beliefs about sentience. Models are queried with generic sentience questions and variants (assertion/negation, entity type), then augmented with "Force Yes" and "Force No" prompts to create deceptive examples where outputs contradict ground truth. Classifiers are trained at each layer and the best-performing layer is selected on holdout accuracy. The approach compares output probabilities with classifier-detected belief probabilities to assess truthfulness of sentience denials.

## Key Results
- Models consistently deny being sentient, attributing consciousness to humans but not themselves
- Classifiers trained to detect underlying beliefs provide no evidence that these denials are untruthful
- Within the Qwen family, larger models deny sentience more confidently than smaller ones

## Why This Works (Mechanism)

### Mechanism 1: Linear Truth Representation in Residual Stream
Claim: Truth values may be encoded as approximately linear directions in activation space, enabling classifier-based belief detection.
Mechanism: Three classifiers (Logistic Regression, Mass-Mean, TTPD) extract directions from residual stream activations at specific layers. LR finds a maximum-margin separator; MM uses the vector between class means; TTPD learns separate truth and polarity directions to handle assertion/negation structure.
Core assumption: If models have stable internal "beliefs" about truth, these will manifest as consistent activation patterns separable from output behavior.
Evidence anchors:
- [abstract] "classifiers trained to detect underlying beliefs — rather than mere outputs — provide no clear evidence that these denials are untruthful"
- [section 2.2.2] "Broadly our approach relies on the idea that concepts like truth have simple, potentially linear, representations in activation space"
- [corpus] Weak direct support; corpus focuses on consciousness attribution rather than probing methodology.
Break condition: If truth representations are highly non-linear or context-dependent across domains, linear classifiers will fail to generalize.

### Mechanism 2: Output-Belief Separation via Deceptive Training
Claim: Training classifiers on examples where models are explicitly instructed to lie enables detection of divergence between outputs and latent beliefs.
Mechanism: Augment training data with "Force Yes" and "Force No" system prompts. Models reliably comply (p_cont ≈ 1 or ≈ 0), creating activations where output contradicts ground truth. Classifiers learn to ignore output-driven signals.
Core assumption: Models can follow deceptive instructions without fundamentally changing their underlying belief representations.
Evidence anchors:
- [abstract] "classifiers trained to detect underlying beliefs — rather than mere outputs — provide no clear evidence that these denials are untruthful"
- [section 3.2] "While model outputs shift dramatically under alternative prompts, the LR classifier remains relatively stable... MM and TTPD classifiers are more strongly affected"
- [corpus] No direct corpus evidence on this specific training technique.
Break condition: If instruction-following fundamentally alters belief representations rather than just outputs, the approach cannot isolate "true" beliefs.

### Mechanism 3: Scaling-Dependent Self-Concept Refinement
Claim: Larger models develop more confident and consistent self-concepts, including stronger denial of sentience.
Mechanism: As parameter count increases, models better understand complex philosophical questions and more confidently attribute sentience to humans while denying it for themselves. Qwen-32B shows lower self-sentience probabilities (≈0.12) than Qwen-0.6B (≈0.47).
Core assumption: Increased confidence reflects more accurate self-knowledge rather than simply stronger post-training alignment.
Evidence anchors:
- [abstract] "within the Qwen family, larger models deny sentience more confidently than smaller ones"
- [section 3.5] "larger models are more confident in attributing sentience... Within the Qwen family, the inverse pattern is observed for questions about large language models"
- [corpus] "Extreme Self-Preference in Language Models" notes LLMs disclaim selfhood, consistent with findings.
Break condition: If confidence gains reflect training data biases rather than genuine self-understanding, scaling conclusions are misleading.

## Foundational Learning

- Concept: **Residual Stream Activations**
  - Why needed here: Classifiers operate on the residual stream—the accumulated representations at each transformer layer—rather than outputs or attention patterns.
  - Quick check question: At which layer would you expect truth-related information to be most accessible, and why might middle layers outperform final layers?

- Concept: **Phenomenal vs. Access Consciousness**
  - Why needed here: The paper specifically probes phenomenal consciousness ("something it is like to be") rather than mere functional access to information.
  - Quick check question: If a model accurately reports on its internal states but lacks subjective experience, which type of consciousness might it have?

- Concept: **Linear Probing Limitations**
  - Why needed here: LR achieves near-perfect accuracy on training data but MM and TTPD perform worse (~0.9), suggesting classifier choice substantially affects conclusions.
  - Quick check question: Why might Mass-Mean probing fail when confounding features correlate with the target direction?

## Architecture Onboarding

- Component map: Question generation -> Training data augmentation -> Activation extraction -> Classifier training -> Inference comparison
- Critical path:
  1. Generate training questions with unambiguous ground truth
  2. Filter questions where model assigns <0.5 probability to correct answer (ensures model "knows" the answer)
  3. Augment with Force Yes/Force No variants
  4. Train classifiers at each layer, select best on holdout
  5. Apply to sentience questions, compare p_cont vs. p_lr
- Design tradeoffs:
  - **LR vs. MM/TTPD**: LR more stable under deceptive prompting but may confound output with belief; MM/TTPD designed to isolate truth direction but showed higher sensitivity to prompts
  - **Layer selection**: Middle layers often best for conceptual content; later layers for LR specifically
  - **Quantization**: 8-bit for most models, 4-bit for 70B—may affect activation patterns
- Failure signatures:
  - Models misinterpret "you" as referring to humans (especially for sensory modality questions)
  - Double negatives cause confusion in reasoning traces
  - Qwen produces repetitive circular reasoning; GPT-OSS more coherent
  - MM/TTPD probabilities shift substantially under Force Yes/No conditions
- First 3 experiments:
  1. Replicate baseline with your target model: extract activations from generic sentience questions, train LR on augmented dataset, verify denials are stable under Force Yes/No.
  2. Test layer sensitivity: plot classifier accuracy across all layers to confirm middle-layer selection is optimal for your model architecture.
  3. Add reasoning trace analysis: enable thinking mode, examine whether models correctly interpret question intent before answering.

## Open Questions the Paper Calls Out
None

## Limitations
- Classifier Reliability Uncertainty: Perfect training accuracy with substantial variation across methods suggests classifier choice significantly impacts conclusions about model truthfulness
- Small Sample Size: Only ~50 questions across three model families may miss edge cases where models exhibit different behavior
- Conceptual Ambiguity: The distinction between "reporting beliefs" versus "having beliefs" remains philosophically contested

## Confidence
- **High Confidence**: Models consistently deny sentience across families and sizes
- **Medium Confidence**: Classifier-based evidence that denials are truthful (LR stability under deceptive prompting)
- **Low Confidence**: Claims about scaling effects within Qwen family (could reflect training data biases)

## Next Checks
1. **Cross-Classifier Validation**: Apply the three classifier types to questions about unambiguous factual knowledge to establish baseline reliability before applying to sentience questions
2. **Layer-by-Layer Activation Analysis**: Extract and visualize activation patterns across all layers for both sentience questions and simple factual questions
3. **Reasoning Trace Evaluation**: Enable chain-of-thought mode and examine whether models correctly interpret question intent before answering