---
ver: rpa2
title: Investigating the Robustness of Knowledge Tracing Models in the Presence of
  Student Concept Drift
arxiv_id: '2511.00704'
source_url: https://arxiv.org/abs/2511.00704
tags:
- data
- knowledge
- learning
- student
- tracing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the robustness of knowledge tracing (KT)\
  \ models under concept drift using five years of student interaction data. Four\
  \ KT models\u2014Bayesian Knowledge Tracing (BKT), Performance Factors Analysis\
  \ (PFA), Deep Knowledge Tracing (DKT), and Self-Attentive Knowledge Tracing (SAKT)\u2014\
  were evaluated within their temporal context and across subsequent academic years."
---

# Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift

## Quick Facts
- arXiv ID: 2511.00704
- Source URL: https://arxiv.org/abs/2511.00704
- Reference count: 0
- All KT models exhibit performance degradation when applied to newer data, with SAKT models losing predictive power most rapidly.

## Executive Summary
This study investigates how Knowledge Tracing (KT) models degrade over time due to concept drift using five years of student interaction data. Four KT models—BKT, PFA, DKT, and SAKT—were evaluated both within their training year and across subsequent academic years. The research reveals that model complexity correlates with vulnerability to drift, with SAKT models experiencing up to 0.17 AUC decrease while BKT and PFA maintain more stable performance. The authors also release a multi-year dataset to support longitudinal evaluation of KT models.

## Method Summary
The study uses five years of student interaction logs from an online learning platform, filtering out summer months, non-gradable problems, and low-frequency items. For each academic year, 50,000 "assignment logs" are sampled to create balanced datasets. Four KT models (BKT, PFA, DKT, SAKT-E, SAKT-KC) are implemented and evaluated using a temporal cross-validation approach: models are trained on data from year T and tested on both year T and subsequent years T+1 through T+4. Performance is measured using AUC, Log Loss, and F1 Score, with regression analysis quantifying the rate of degradation over time.

## Key Results
- All four KT models show significant performance degradation when applied to newer academic years.
- SAKT models lose the most predictive power, with AUC decreasing by up to 0.17 in the first 1-2 years after training.
- BKT and PFA demonstrate the most stability, with minimal AUC loss (~0.03 and ~0.05 respectively).
- Performance degradation appears to plateau after 3-4 years for simpler models like BKT and PFA.
- The presence of attention mechanisms correlates with increased vulnerability to concept drift.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Tracing (KT) models experience performance degradation (concept drift) when applied to data from future time periods, and the rate and severity of this degradation varies by model architecture.
- Mechanism: Changes in the underlying joint probability distribution of student interactions over time cause model degradation. Specifically, "Concept Drift" where P(Y|X) changes while P(X) remains constant, potentially due to evolving student populations, curriculum changes, or external factors. The attention mechanism in SAKT models appears to be particularly sensitive to these shifts.
- Core assumption: The observed performance drop on newer data is primarily due to concept drift (changes in P(Y|X)) rather than solely covariate shift (changes in P(X)) or label shift.
- Evidence anchors: "Every model tested by our method had decreasing AUC over time. All five models have significant correlations between model fit metrics and the number of years between training & evaluation data..."
- Break condition: This mechanism would not hold if the primary cause of performance degradation were simple data scarcity in future years (covariate shift) or if the evaluation metrics were fundamentally flawed for time-series data.

### Mechanism 2
- Claim: Model complexity, specifically the presence and nature of attention mechanisms, correlates with increased susceptibility to concept drift, leading to faster and more severe performance loss over time.
- Mechanism: Attention-based models capture long-range dependencies that are more brittle and context-specific. When the temporal context shifts, these specific long-range patterns lose their predictive power. Simpler models like BKT focus on estimating a student's latent knowledge state from recent performance, which is a more stable signal across time.
- Core assumption: The ability to model long-range dependencies makes a model more prone to overfitting to the specific temporal context of its training data.
- Evidence anchors: "SAKT-E and SAKT-KC both have significant predictive power the first 1-2 years... however appear to be degrading with no signs of stopping..."
- Break condition: This mechanism breaks if the SAKT models' poor long-term performance is due to hyperparameter overfitting on the initial year rather than the attention mechanism's inherent properties.

### Mechanism 3
- Claim: Simpler, more constrained models (BKT, PFA) exhibit greater temporal robustness and their performance degradation tends to plateau, unlike complex attention-based models.
- Mechanism: BKT models knowledge as a latent Markov process with interpretable parameters. These constraints limit the model's capacity to fit idiosyncratic, context-specific patterns. This "underfitting" paradoxically makes them more robust when the data distribution shifts, as their predictions are based on more fundamental, slowly-changing signals of student learning.
- Core assumption: The fundamental process of knowledge acquisition (modeled by BKT's latent state) remains relatively stable over time, even if surface-level student behaviors or curricula change.
- Evidence anchors: "...BKT remains the most stable KT model when applied to newer data, while more complex, attention based models lose predictive power significantly faster."
- Break condition: The plateauing effect could be a statistical artifact of the study's 5-year window.

## Foundational Learning

- Concept: **Knowledge Tracing (KT)**
  - Why needed here: This is the core task evaluated. It is the machine learning problem of modeling a student's knowledge state from their past interactions to predict future performance.
  - Quick check question: What is the input and what is the model trying to predict in a standard Knowledge Tracing task?

- Concept: **Concept Drift**
  - Why needed here: This is the central problem the paper investigates. It refers to the phenomenon where the statistical relationship between input data and target labels changes over time, causing model performance to degrade.
  - Quick check question: In a deployed KT model, what would concept drift look like from an end-user's (teacher's) perspective?

- Concept: **Model Robustness & Longitudinal Evaluation**
  - Why needed here: The paper's core contribution is a methodology and empirical result about model stability over multi-year timeframes. This shifts the evaluation paradigm from single-benchmark performance to long-term viability.
  - Quick check question: Why is evaluating a model on a single static dataset insufficient for systems deployed over many years?

## Architecture Onboarding

- Component map:
  Data Pipeline -> Model Zoo -> Evaluation Framework -> Analysis & Regression

- Critical path:
  1. Acquire and pre-process multi-year OLP data into a consistent format (student, exercise, KC, timestamp, outcome).
  2. Implement and tune the four KT models (BKT, PFA, DKT, SAKT) on a held-out validation sample.
  3. Train each model on data from a base academic year (e.g., 2019-2020).
  4. Evaluate the trained models on held-out data from the base year and all subsequent academic years.
  5. Analyze the trajectory of evaluation metrics (AUC, Log Loss, F1) as a function of time since training.

- Design tradeoffs:
  - **Performance vs. Robustness**: SAKT models achieve higher initial performance but degrade rapidly, while BKT has lower peak performance but is more stable.
  - **Data Sampling Strategy**: Sampling "assignment logs" preserves context while controlling dataset size differences across years.
  - **Model Simplicity vs. Complexity**: Simpler models (BKT, PFA) may be preferable for long-term deployment without frequent retraining.

- Failure signatures:
  - **SAKT models**: Sharp, near-linear drop in AUC and F1 score in the first 1-2 years after training, with no plateauing.
  - **BKT/PFA**: Small initial drop (~0.03-0.05 AUC) that quickly levels off, maintaining stable but modest performance.
  - **All models**: Systematic drop in performance correlating with "years between training & evaluation."

- First 3 experiments:
  1. Replicate the Cross-Year Evaluation: Train on 2019-2020 data, evaluate on 2020-2021 through 2023-2024, plot AUC curves.
  2. Ablate the Attention Mechanism: Modify SAKT architecture to restrict attention window or use simpler aggregation, test if this reduces drift susceptibility.
  3. Drift Detection Probe: Run statistical tests on input data distributions across years to quantify covariate shift vs. true concept drift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of attention mechanisms inherently increase a Knowledge Tracing model's vulnerability to concept drift compared to recurrent or logistic architectures?
- Basis in paper: [explicit] The authors note that "the main factor which contributes to model degradation appears to be the presence of the attention mechanism" and suggest future work should investigate "how and why student behavior changes" to explain this sensitivity.
- Why unresolved: The study identifies the correlation between SAKT models and rapid performance loss but does not isolate the specific causal mechanism within the attention architecture.
- What evidence would resolve it: An ablation study comparing models with and without attention layers on controlled synthetic datasets where the type and rate of drift are known.

### Open Question 2
- Question: How do extensions to standard Knowledge Tracing models (e.g., incorporating side information or individual priors) impact their robustness to concept drift?
- Basis in paper: [explicit] The authors acknowledge they "limited our analysis to basic implementations" and explicitly propose that "exploring how extensions to these model families impacts robustness would also give more insight into the relationship between model complexity and generalizability."
- Why unresolved: It is unclear if the stability of simpler models like BKT and PFA persists when they are augmented with complex features.
- What evidence would resolve it: A longitudinal comparison of extended model variants using the released multi-year dataset.

### Open Question 3
- Question: What are the specific characteristics of dataset shift (e.g., changes in P(Y|X) vs. P(X)) that drive the performance degradation observed in Knowledge Tracing models over time?
- Basis in paper: [explicit] The authors state their "model-focused approach... cannot describe how the distribution of student responses has changed, nor explain factors which could be causing said change," and call for "data-centric" investigations in future work.
- Why unresolved: The paper demonstrates that performance drops but does not quantify the underlying changes in student behavior or curriculum that constitute the concept drift.
- What evidence would resolve it: A statistical analysis of interaction logs across years to identify and categorize specific distributional changes in student response patterns.

## Limitations

- The study is based on a single dataset from one online learning platform, limiting generalizability to other platforms with different student populations or curricula.
- The "assignment log" sampling strategy may not capture individual user-level concept drift if users exhibit stable patterns within their assignments.
- The study focuses on aggregate performance metrics without investigating which specific concept knowledge components (KCs) or interaction patterns are most vulnerable to drift.

## Confidence

- **High Confidence**: The empirical finding that SAKT models lose predictive power significantly faster than BKT and PFA (up to 0.17 AUC decrease).
- **Medium Confidence**: The claim that model complexity and attention mechanisms inherently increase vulnerability to concept drift.
- **Medium Confidence**: The observation that performance degradation plateaus after 3-4 years for simpler models.

## Next Checks

1. **Cross-Platform Validation**: Replicate the temporal evaluation methodology on a different OLP dataset (e.g., ASSISTments, Cognitive Tutor) to test generalizability of the robustness hierarchy.

2. **KC-Level Drift Analysis**: Analyze which specific knowledge components show the highest concept drift by tracking BKT parameter stability (guess, slip, learning rates) over time.

3. **Retraining Frequency Experiment**: Systematically test the trade-off between model complexity and retraining frequency by evaluating how often SAKT models need to be retrained to match BKT's long-term performance, including computational costs.