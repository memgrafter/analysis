---
ver: rpa2
title: 'Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models
  via Semantic Capacity Asymmetry'
arxiv_id: '2601.22588'
source_url: https://arxiv.org/abs/2601.22588
tags:
- probing
- evaluation
- arxiv
- score
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a new paradigm, Representation-as-a-Judge, to
  address the high cost and opacity of large language model (LLM) evaluation. The
  key insight is that small language models (LMs) encode evaluative signals in their
  internal representations even when their generative outputs are weak, a phenomenon
  formalized as the Semantic Capacity Asymmetry Hypothesis.
---

# Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry

## Quick Facts
- arXiv ID: 2601.22588
- Source URL: https://arxiv.org/abs/2601.22588
- Authors: Zhuochun Li; Yong Zhang; Ming Li; Yuelyu Ji; Yiming Zeng; Ning Cheng; Yun Zhu; Yanmeng Wang; Shaojun Wang; Jing Xiao; Daqing He
- Reference count: 40
- One-line primary result: Small language models can evaluate responses by probing internal representations rather than generating text, achieving 80-90% binary F1 while being orders of magnitude more efficient than LLM-as-a-Judge.

## Executive Summary
This paper introduces Representation-as-a-Judge, a paradigm that replaces expensive large language model (LLM) evaluation with probing-based prediction from small language model (LM) internal representations. The core insight is that evaluation requires less semantic capacity than generation, allowing small models to encode evaluative signals in intermediate representations even when their generated outputs are poor. The authors develop INSPECTOR, a framework that trains logistic regression classifiers on frozen small LM representations to predict evaluation scores from a larger LLM judge. Experiments on reasoning benchmarks show this approach substantially outperforms prompting-based small LMs and closely approximates full LLM judges while offering significant efficiency gains for scalable evaluation and data filtering for supervised fine-tuning.

## Method Summary
The approach consists of three main stages: (1) generating responses from a medium LM and obtaining evaluation scores from a large LM judge across five aspects (Logicality, Factuality, etc.), (2) extracting hidden states and attention weights from a frozen small LM processing the same prompts and responses, and (3) training a logistic regression probe on pooled and PCA-reduced representations to predict the large LM's scores. The framework operates in two modes: binary classification (high/low quality with threshold Ï„=4) and multiclass prediction (1-5 scores). For data filtering applications, the binary probing classifier serves as an efficient reference-free filter, ranking data for downstream supervised fine-tuning with performance comparable to LLM-based filtering while requiring orders of magnitude less computation.

## Key Results
- Probing-based small LMs achieve 80-90% binary F1 compared to 30-40% for prompting-based small LMs
- Performance closely approximates full LLM judges while being significantly more efficient
- Probing classifiers effectively filter high-quality data for supervised fine-tuning with performance comparable to LLM-based filtering
- Binary classification proves more robust than multiclass prediction for out-of-distribution generalization

## Why This Works (Mechanism)

### Mechanism 1: Semantic Capacity Asymmetry
Evaluation requires significantly less semantic capacity than generation, allowing small models to encode evaluative signals in intermediate representations even when their generated outputs are poor. While generation involves complex discourse planning and long dependencies, evaluation focuses on identifying inconsistencies or content errors, which are already accessible in intermediate model states. The small LM's hidden states contain linearly separable features correlating with evaluation scores from a larger judge.

### Mechanism 2: Probing-Based Signal Extraction (INSPECTOR)
A probing framework trained on frozen small LM representations can approximate large LM evaluation scores with high fidelity. The framework extracts hidden states and attention weights, applies mean pooling and PCA reduction, then trains a logistic regression classifier to predict evaluation scores. This approach leverages the fact that intermediate representations contain compressed features related to quality attributes.

### Mechanism 3: Representation-as-a-Judge for Efficient Filtering
Trained probing classifiers serve as highly efficient data filters for supervised fine-tuning, producing downstream model performance comparable to filtering with large LM judges. Binary classification performance reaches 80-90% F1, making it sufficiently reliable to function as a reference-free coarse filtering mechanism while requiring orders of magnitude less computation than running large LLM judges.

## Foundational Learning

- **Concept: Probing Internal Representations**
  - Why needed here: The core method relies on training a simple classifier (a "probe") on the frozen internal states of a neural network to predict properties of the input (in this case, its quality). This is the technical heart of the INSPECTOR framework.
  - Quick check question: What is the primary purpose of using a simple, linear classifier as a probe instead of a complex, fine-tuned model?

- **Concept: The Semantic Capacity Asymmetry Hypothesis**
  - Why needed here: This is the theoretical justification for the entire approach. Understanding that evaluating a solution is fundamentally easier (requires less model capacity) than generating one is key to believing a small model's representations can be useful for judgment.
  - Quick check question: Why might a small model's internal state contain the information needed to judge a response, even if it cannot generate a coherent justification for that judgment?

- **Concept: Data Filtering for Supervised Fine-Tuning (SFT)**
  - Why needed here: The paper frames its main practical application as a tool for curating high-quality training data. Understanding the goal of SFT and how data quality impacts it is necessary to evaluate the presented results.
  - Quick check question: How does filtering a training dataset by quality (e.g., removing low-quality examples) typically affect the performance of a model trained on that data?

## Architecture Onboarding

- **Component map:** Prompt/Response -> Frozen Small LM -> Hidden State Extraction & Pooling -> PCA Reduction -> Probing Classifier -> Predicted Evaluation Score
- **Critical path:** The most important flow is from Prompt/Response to the final predicted evaluation score, with the probing classifier's performance depending on the quality of labeled data and richness of small LM's representations.
- **Design tradeoffs:**
    - Probe Simplicity vs. Power: Simple linear probes ensure the predictive signal genuinely resides in the small LM's representations
    - Labeling Cost vs. Quality: Using a large LM for labeling provides high-quality supervision but is costly, amortized over cheap probing classifiers
    - Binary vs. Multiclass: Binary classification is robust (~80-90% F1) and good for filtering, while fine-grained multiclass prediction (1-5 scores) is more difficult (~50-60% F1)
- **Failure signatures:**
    - Probing classifier fails to generalize to new datasets (Out-of-Distribution)
    - Performance is highly sensitive to minor changes in prompt template or pooling method
    - Binary classification performance drops below 60% F1
- **First 3 experiments:**
  1. Replicate Probing vs. Prompting Gap: Compare F1 scores of prompting small LM directly vs. training logistic regression probe on hidden states
  2. Layer-wise Ablation: Train probing classifier using representations from single layers to identify where evaluative signals are strongest
  3. Downstream Filtering Efficacy: Train two small SFT models on probing-filtered vs. randomly filtered data and compare benchmark performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can probing-based evaluation transfer effectively to commonsense reasoning and code generation domains?
- Basis in paper: "Although we conducted experiments on various datasets, future work can explore more general reasoning fields, such as commonsense and code generation tasks." (Appendix A, Limitations)
- Why unresolved: Current experiments only cover mathematics and science reasoning; probing may capture domain-specific signals that don't generalize
- What evidence would resolve it: Train probing classifiers on math data and test on commonsense and code benchmarks, reporting F1 scores for both binary and multiclass tasks

### Open Question 2
- Question: How does choice of rating LLM affect probing classifier performance and downstream data filtering?
- Basis in paper: "We only use the DeepSeek-V3 as our rating model... Exploring diverse rating LLMs from different organizations could be a valuable direction for future research." (Appendix A)
- Why unresolved: Different LLM judges may have systematic biases; probing classifiers trained on one LLM's judgments may not align with human preferences
- What evidence would resolve it: Train separate probing classifiers using labels from multiple LLMs and compare correlation with human judgments

### Open Question 3
- Question: Why does multiclass probing (1-5 scores) fail to transfer across datasets while binary probing remains robust?
- Basis in paper: OOD experiments show multiclass F1 drops to 10-25% under distribution shift, but binary maintains 35-62%
- Why unresolved: Unclear whether fine-grained signals are dataset-specific or whether probe capacity/linear separability is the limiting factor
- What evidence would resolve it: Analyze representation similarity across datasets; test whether nonlinear probes or larger training sets improve multiclass OOD transfer

## Limitations
- The Semantic Capacity Asymmetry hypothesis lacks direct experimental validation measuring semantic capacity requirements across tasks
- Performance depends heavily on the quality and representativeness of the large LM's judgments used for training
- Demonstrated effectiveness is limited to reasoning benchmarks and may not generalize to domains like creative writing or open-ended dialogue

## Confidence
- **High Confidence:** The core empirical finding that probing internal representations outperforms prompting-based evaluation is well-supported by quantitative results (80-90% binary F1 vs. 30-40% for prompting)
- **Medium Confidence:** The theoretical justification for Semantic Capacity Asymmetry, while intuitively compelling, lacks direct experimental measurement of semantic capacity requirements
- **Medium Confidence:** The effectiveness of probing-based filtering for SFT shows promising results but is demonstrated on relatively small-scale experiments

## Next Checks
1. Apply the trained probing classifier from GSM8K/MATH to evaluate responses from a qualitatively different domain (e.g., creative writing or code generation) to quantify performance degradation and identify domain-specific limitations

2. Conduct an ablation study that systematically removes or corrupts specific layers of the small LM and measures the impact on probing performance to identify which representations are most critical for evaluation signals

3. Collect human judgments on a subset of responses and compute correlation between probing classifier scores, LLM judge scores, and human ratings to assess alignment quality and potential biases in the automated evaluation pipeline