---
ver: rpa2
title: Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition
arxiv_id: '2508.02593'
source_url: https://arxiv.org/abs/2508.02593
tags:
- feedback
- surgical
- skill
- expert
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of providing personalized, objective
  feedback in surgical skill acquisition, where traditional expert feedback is limited
  by faculty availability and subjective variability. The authors developed an explainable
  AI (XAI)-based feedback system that analyzes video recordings of suturing tasks
  to extract interpretable surgical skill proxies, such as hand orientation and thumb-index
  finger distance, and compares trainee performance against expert benchmarks.
---

# Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition

## Quick Facts
- arXiv ID: 2508.02593
- Source URL: https://arxiv.org/abs/2508.02593
- Reference count: 17
- One-line primary result: XAI-based surgical feedback showed trends toward improved performance but no statistically significant advantage over traditional coaching in a small randomized trial.

## Executive Summary
This study develops an explainable AI system for automated, user-specific feedback in surgical skill acquisition. The system analyzes video recordings of suturing tasks to extract interpretable skill proxies (hand orientation, thumb-index finger distance) and compares trainee performance against expert benchmarks. In a randomized controlled trial with 12 medical students, XAI-generated feedback was compared to traditional video-based coaching. While no statistically significant differences were found between feedback types, the XAI group showed trends toward more expert-like behavior and reported higher perceived feedback usefulness. The study suggests XAI has potential for surgical education but requires further refinement and larger sample sizes to demonstrate clear advantages over traditional methods.

## Method Summary
The system uses computer vision to analyze RGB video from Intel RealSense D435i sensors, detecting hands and tools with YOLOX-S, estimating hand keypoints, and classifying gestures using MSTCN++. Two interpretable proxies—Hand Orientation (HO) and Thumb-Index Distance (DF)—are computed per gesture. Expert reference values are established by averaging proxy-gesture pairs across N=5 surgeons and 4 trials each. For each trainee, the top three largest deviations from expert benchmarks are identified as personalized feedback targets. Feedback is presented through a structured template combining expert video demonstrations with textual explanations of the specific deviations.

## Key Results
- No statistically significant differences between XAI and traditional feedback groups in reducing performance gaps or improving practice adjustments
- XAI group showed trends toward more expert-like behavior and reported higher perceived feedback usefulness (M=3.67 vs 3.42 on 5-point scale)
- Both groups demonstrated improved skill acquisition over time, with novices struggling to translate feedback insights into improved practice
- Expert benchmark computation: P_ref = (1/N×T) ∑ P_j,i across experts and trials

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extracting interpretable skill proxies from video enables automated, quantitative comparison between trainee and expert performance.
- **Mechanism:** Computer vision pipeline converts raw video into structured, time-aligned measurements (hand orientation, thumb-index distance) that serve as clinically meaningful skill indicators.
- **Core assumption:** Hand pose kinematics captured from a frontal RGB view correlate with surgical skill quality.
- **Evidence anchors:** [abstract] "XAI to analyze videos and extract surgical skill proxies related to primitive actions"; [section 2.2, p.4] Detailed pipeline; [corpus] Bkheet et al. (2023) validated similar hand pose proxies for open surgery training feedback.

### Mechanism 2
- **Claim:** Relative deviation scoring against expert benchmarks identifies personalized learning targets.
- **Mechanism:** Expert reference values are computed by averaging proxy-gesture pairs across multiple experts and trials. For each trainee, absolute deviation is ranked, and top three largest deviations become feedback targets.
- **Core assumption:** Expert-averaged proxy values represent optimal technique; deviation from this average indicates correctable error.
- **Evidence anchors:** [section 2.2, p.4-5] Formal definition and ranking strategy; [section 2.2, p.5] "we selected the top three differences with the largest deviations".

### Mechanism 3
- **Claim:** Explicit, contrastive feedback framing improves trainee understanding and perceived usefulness of automated guidance.
- **Mechanism:** Feedback is presented with structured template: "During [gesture], your [hand] [proxy name] had [relative position] average values than experts."
- **Core assumption:** Trainees can translate abstract proxy descriptions into motor adjustments during subsequent practice.
- **Evidence anchors:** [section 2.2, p.5] Feedback template specification; [results, p.7] XAI group rated perceived usefulness slightly higher.

## Foundational Learning

- **Concept: Deliberate practice with feedback loops**
  - **Why needed here:** The system is designed around principles of targeted, repeated practice with specific feedback.
  - **Quick check question:** Can you explain why holistic performance scores (e.g., "7/10") are less effective for skill improvement than specific, actionable feedback like "your hand orientation during knot-lying differs from experts"?

- **Concept: Gesture-level granularity in motion analysis**
  - **Why needed here:** The system decomposes continuous suturing into discrete gestures and computes proxies per gesture.
  - **Quick check question:** Why might optimal thumb-index finger distance differ between "needle passing" and "cutting the suture"?

- **Concept: Interpretability-explainability distinction**
  - **Why needed here:** The paper positions XAI as providing interpretable proxies combined with explainable feedback.
  - **Quick check question:** If a system outputs "hand orientation = -0.34" without context, is this interpretable, explainable, both, or neither?

## Architecture Onboarding

- **Component map:** Intel RealSense D435i RGB sensor -> YOLOX-S detection -> Pose estimation -> MSTCN++ gesture classification -> Proxy computation (HO, DF) -> Expert benchmark store -> Deviation ranking engine -> Feedback UI

- **Critical path:** Video capture -> detection -> pose -> gesture classification -> proxy computation -> deviation calculation -> UI rendering. Any failure in detection or classification propagates; gesture misclassification invalidates downstream proxy comparison.

- **Design tradeoffs:**
  - **2D vs 3D:** Uses only RGB, ignoring depth data; trades spatial accuracy for computational simplicity
  - **Pretrained vs fine-tuned models:** Uses out-of-the-box models from prior work; trades domain adaptation for rapid deployment
  - **Top-3 vs full feedback:** Limits to three deviations; trades comprehensive coverage for reduced cognitive load
  - **Single-expert-average vs distribution:** Uses mean expert values; trades individual variation for simple comparison target

- **Failure signatures:**
  - High gesture classification error rate under variable lighting/occlusion
  - Proxy values showing high variance within single gestures (temporal instability)
  - Expert benchmark with large standard deviation indicating P_ref is not representative
  - Trainee proxy values moving away from expert reference after feedback (observed in traditional group)

- **First 3 experiments:**
  1. **Gesture classification robustness test:** Run the full pipeline on held-out videos with variable lighting and tool occlusion; report per-gesture F1 scores to identify failure modes.
  2. **Expert benchmark variability analysis:** Compute per-proxy standard deviation across experts; flag high-variability proxies where P_ref may not represent meaningful target.
  3. **Proxy-to-outcome validation:** Correlate proxy deviation changes with OSATS score improvements; test whether proxy alignment predicts skill gains to validate the underlying assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can explainable AI feedback effectively accelerate the acquisition of surgical skills compared to traditional video-based coaching?
- **Basis in paper:** [explicit] The authors explicitly state in the introduction: "A key question remains: Can XAI effectively accelerate the acquisition of surgical skills while maintaining trainee engagement and educational value?"
- **Why unresolved:** The study found no statistically significant differences between XAI and traditional feedback groups in reducing performance gaps, likely due to the small sample size (N=12) and limited trial duration.
- **What evidence would resolve it:** A large-scale randomized controlled trial with sufficient statistical power to detect differences in skill acquisition rates and retention over time.

### Open Question 2
- **Question:** Does temporal analysis of surgical proxies offer superior performance assessment compared to averaged static values?
- **Basis in paper:** [inferred] The Discussion notes that the underlying concepts exhibit temporal variation within gestures, and "temporal analyses could provide more meaningful comparisons between expert and novice performance" than the averaged proxy values used in this study.
- **Why unresolved:** The current methodology aggregated proxy values into single averages per gesture, potentially smoothing out critical intra-gesture performance variations that distinguish experts from novices.
- **What evidence would resolve it:** A comparative study evaluating the correlation of temporal vs. averaged proxy profiles with expert human ratings of skill.

### Open Question 3
- **Question:** Is the effectiveness of XAI feedback dependent on the learner's baseline competency level?
- **Basis in paper:** [inferred] The authors suggest in the Discussion that "the value of advanced feedback for transforming novices into experts appears limited when learners are still mastering fundamental skills," implying the intervention may be better suited for intermediate learners.
- **Why unresolved:** The study sample consisted of medical students with relatively low surgical exposure, who may lack the foundational skills to utilize specific, high-level feedback effectively.
- **What evidence would resolve it:** A stratified user study comparing XAI feedback efficacy across cohorts with distinct baseline skill levels (e.g., novices vs. residents).

## Limitations

- Small sample size (N=12 trainees, N=5 experts) limits statistical power and generalizability of findings
- Reliance on 2D RGB-only analysis ignores depth information, potentially limiting proxy accuracy for 3D surgical motions
- Expert benchmark variability was not reported; high inter-expert variation would undermine the validity of single-average reference values

## Confidence

- **Medium confidence** in the mechanism of automated proxy extraction and comparison, supported by validated computer vision components but untested for surgical-specific robustness
- **Low confidence** in the personalization mechanism (top-3 deviation ranking) due to lack of validation that these specific deviations correlate with skill improvement
- **Medium confidence** in the explanatory feedback design, supported by slightly higher perceived usefulness ratings but without demonstrated impact on actual performance gains

## Next Checks

1. **Robustness testing:** Evaluate gesture classification accuracy across variable lighting conditions, tool occlusions, and different camera angles to identify failure modes before clinical deployment.

2. **Expert variability analysis:** Compute standard deviations for each proxy across experts; if any proxy shows high variability (>20% of mean), the reference benchmark for that metric should be reconsidered.

3. **Proxy-outcome correlation study:** Conduct a controlled study correlating changes in proxy deviations with objective skill assessment scores (e.g., OSATS) to validate that proxy alignment predicts performance improvement.