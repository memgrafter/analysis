---
ver: rpa2
title: Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual
  Performance of LLMs
arxiv_id: '2504.09378'
source_url: https://arxiv.org/abs/2504.09378
tags:
- alignment
- english
- layer
- token
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the ability of large language models
  (LLMs) to align their representations of non-English text with English representations
  impacts their performance on natural language understanding (NLU) tasks. The authors
  introduce three instance-level metrics - DALI, DALIst, and MEXAT - to quantify representation
  alignment between English and non-English NLU samples.
---

# Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs

## Quick Facts
- **arXiv ID:** 2504.09378
- **Source URL:** https://arxiv.org/abs/2504.09378
- **Reference count:** 40
- **Primary result:** Cross-lingual alignment in middle layers drives successful multilingual transfer in LLMs

## Executive Summary
This paper investigates how large language models' ability to align non-English representations with English representations impacts multilingual natural language understanding performance. The authors introduce three novel instance-level metrics to quantify cross-lingual alignment and demonstrate that samples successfully transferring between English and other languages exhibit higher alignment than those that fail to transfer. Through activation patching experiments, they establish causal evidence that middle-layer semantic alignment with English representations drives successful cross-lingual transfer, with effects specifically attributable to conceptual rather than next-token prediction alignment.

## Method Summary
The authors develop three instance-level metrics—DALI, DALIst, and MEXAT—to measure representational alignment between English and non-English NLU samples. They identify transfer success (TS) and transfer failure (TF) instances by comparing model performance across languages on three benchmarks. Activation patching experiments systematically replace non-English activations with semantically equivalent English activations to test whether cross-lingual corrections occur. Control patching experiments distinguish between conceptual alignment effects and next-token prediction influences. The analysis spans nine languages, three distinct LLMs (LLaMA-7B, LLaMA-2-7B, Mistral-7B), and multiple NLU tasks to establish robust causal relationships.

## Key Results
- TS instances consistently show higher cross-lingual alignment than TF instances across all benchmarks, languages, and models tested
- Activation patching with semantically equivalent English activations successfully corrects incorrect non-English predictions
- Correction effects are concentrated in specific middle layers rather than input or output layers
- Control experiments confirm that alignment-based corrections are conceptually driven, not merely next-token prediction artifacts

## Why This Works (Mechanism)
Cross-lingual alignment functions as a bridge between language-specific representations, enabling models to transfer knowledge from English (where most training data exists) to other languages. When non-English inputs activate similar semantic patterns in middle layers as their English counterparts, the model can leverage learned English reasoning pathways for cross-lingual understanding. This alignment appears to be most critical in middle layers where semantic abstraction occurs, rather than in early layers (which capture surface features) or late layers (which produce task-specific outputs). The causal evidence from activation patching demonstrates that this alignment is not merely correlational but functionally necessary for successful transfer.

## Foundational Learning
- **Cross-lingual transfer**: The ability of models trained on one language to perform well on others; needed to understand multilingual generalization, quick check: compare performance across source and target languages
- **Activation patching**: A causal intervention technique replacing model activations to test component importance; needed to establish mechanistic relationships, quick check: measure prediction changes after activation substitution
- **Instance-level metrics**: Measurements computed per sample rather than averaged across datasets; needed to capture individual transfer patterns, quick check: calculate metric values for individual examples
- **Semantic alignment**: Similarity in meaning representations across languages; needed to understand how models bridge language gaps, quick check: compare embeddings of translation pairs
- **Transfer Success (TS) vs Transfer Failure (TF)**: Classification of instances based on cross-lingual performance consistency; needed to identify alignment patterns, quick check: label instances by performance correlation across languages

## Architecture Onboarding
**Component Map:** Input -> Embedding Layer -> Middle Layers (Alignment Critical) -> Output Layer -> Prediction
**Critical Path:** Middle layers serve as the primary site where cross-lingual alignment occurs and corrections propagate to influence final predictions
**Design Tradeoffs:** Focusing on middle-layer alignment prioritizes semantic understanding over surface-level linguistic features, potentially sacrificing some language-specific nuance for broader transferability
**Failure Signatures:** TF instances exhibit low cross-lingual alignment scores and fail to benefit from English activation patching interventions
**First Experiments:** 1) Calculate DALI/DALIst/MEXAT scores for individual instances to identify alignment patterns, 2) Perform layer-by-layer ablation to pinpoint critical alignment layers, 3) Test activation patching across different semantic categories to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to three specific LLMs and three NLU benchmarks, constraining generalizability to other model scales and task types
- Instance-level metrics may not fully capture complex task-specific reasoning patterns that vary across languages
- Activation patching involves discrete interventions that may not reflect natural cross-lingual processing dynamics
- English-centric alignment target potentially reinforces English-centrism in multilingual modeling

## Confidence
- **Cross-lingual alignment correlates with transfer success**: High confidence based on consistent patterns across benchmarks, languages, and models
- **Middle-layer activations drive cross-lingual corrections**: Medium confidence due to targeted intervention effects, though layer-specific mechanisms require further validation
- **Conceptual alignment vs. next-token prediction**: High confidence from control patching experiments, though the distinction may be more nuanced in practice

## Next Checks
1. Replicate activation patching experiments across additional task types (e.g., generation, translation) and model scales (e.g., LLaMA-2-70B) to test generalizability of layer-specific effects
2. Conduct ablation studies removing individual alignment components to isolate their relative contributions to cross-lingual performance
3. Implement continuous alignment optimization during fine-tuning to determine whether explicit alignment objectives improve multilingual transfer beyond standard cross-lingual objectives