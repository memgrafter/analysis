---
ver: rpa2
title: 'MiniLLM: On-Policy Distillation of Large Language Models'
arxiv_id: '2306.08543'
source_url: https://arxiv.org/abs/2306.08543
tags:
- teacher
- minillm
- student
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MINILLM, a knowledge distillation approach
  that uses reverse Kullback-Leibler divergence (KLD) instead of forward KLD to prevent
  overestimation in low-probability regions. It combines on-policy optimization with
  strategies like single-step decomposition, teacher-mixed sampling, and length normalization.
---

# MiniLLM: On-Policy Distillation of Large Language Models

## Quick Facts
- arXiv ID: 2306.08543
- Source URL: https://arxiv.org/abs/2306.08543
- Reference count: 36
- This paper proposes MINILLM, a knowledge distillation approach that uses reverse Kullback-Leibler divergence (KLD) instead of forward KLD to prevent overestimation in low-probability regions.

## Executive Summary
This paper introduces MINILLM, a knowledge distillation method for compressing large language models (LLMs) into smaller student models for instruction-following tasks. The key innovation is using reverse Kullback-Leibler divergence instead of forward KLD, which prevents the student from overestimating low-probability regions of the teacher distribution. Combined with on-policy optimization and three stabilization strategies (single-step decomposition, teacher-mixed sampling, and length normalization), MINILLM achieves higher GPT-4 feedback scores, better calibration, and superior long-text generation performance compared to baselines across model sizes from 120M to 13B parameters.

## Method Summary
MINILLM uses a two-phase training approach. Phase 1 applies standard supervised fine-tuning (SFT) to initialize the student model on the instruction-following dataset. Phase 2 employs on-policy distillation using reverse KLD as the objective, where the student samples from a mixed distribution combining teacher and student outputs. The method includes three key stabilization strategies: single-step decomposition for exact gradient computation, teacher-mixed sampling to prevent degenerate samples, and length normalization to balance rewards across sequence lengths. The student is trained with a fixed learning rate of 5e-6, batch size of 64, and 5000 steps, with final selection based on validation Rouge-L score.

## Key Results
- MINILLM achieves GPT-4 feedback scores up to 60.1 vs 57.0 for baselines
- Shows lower exposure bias with ExAccErr stopping accumulation after ~150 tokens
- Demonstrates superior long-text generation performance with better calibration
- Performance scales positively with teacher size up to 13B parameters tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reverse KL divergence prevents student models from overestimating low-probability regions in the teacher's output distribution.
- Mechanism: Unlike forward KLD which forces mode-covering behavior, reverse KLD induces mode-seeking behavior where the student matches the teacher's high-probability regions while ignoring sparse modes it cannot express due to capacity constraints.
- Core assumption: The teacher distribution contains more modes than the student can represent when compressing LLMs to smaller models for generative tasks.
- Evidence anchors: Figure 2 toy experiment shows reverse KLD fits major modes while forward KLD spreads probability mass into void regions. Related KD work addresses distribution matching but doesn't specifically confirm reverse KLD superiority for this regime.

### Mechanism 2
- Claim: On-policy sampling reduces exposure bias compared to teacher-forcing distillation.
- Mechanism: By sampling from the student's own distribution during training, the student encounters its own generation distribution rather than the teacher's, closing the train-inference discrepancy through policy gradient optimization.
- Core assumption: Exposure bias significantly harms generation quality in distilled models trained with teacher-forcing.
- Evidence anchors: MINILLM shows lower ExAccErr that stops accumulating after ~150 tokens while baselines continue accumulating error. On-policy distillation is echoed in Self-Distilled Reasoner for reasoning tasks.

### Mechanism 3
- Claim: The three stabilization strategies are jointly necessary for stable convergence.
- Mechanism: Single-step decomposition reduces Monte Carlo variance, teacher-mixed sampling prevents degenerate student samples from receiving high teacher scores, and length normalization divides cumulative reward by remaining sequence length to prevent bias.
- Core assumption: Each strategy addresses a distinct failure mode that would otherwise prevent learning.
- Evidence anchors: Ablation shows removing length normalization drops R-L from 27.4 to 17.4; removing teacher-mixed drops to 22.3. Training curves show reward hacking without teacher-mixed and length normalization.

## Foundational Learning

- Concept: **Forward vs Reverse KL Divergence**
  - Why needed here: The entire paper hinges on understanding why reverse KLD produces mode-seeking while forward KLD produces mode-covering behavior.
  - Quick check question: Given a Gaussian mixture target and a single Gaussian approximator, which KLD direction will fit one mode vs spread across all modes?

- Concept: **Policy Gradient and REINFORCE**
  - Why needed here: The on-policy optimization uses the Policy Gradient theorem to derive gradients for the reverse KLD objective.
  - Quick check question: Why does policy gradient suffer from high variance, and what role does a baseline play in reducing it?

- Concept: **Exposure Bias in Sequence Generation**
  - Why needed here: Understanding why teacher-forcing creates train-inference mismatch and why on-policy sampling helps.
  - Quick check question: In scheduled sampling vs on-policy training, what is the fundamental difference in how prefixes are generated during training?

## Architecture Onboarding

- Component map:
  ```
  Teacher Model (frozen) → provides p(y_t|y_<t, x) logits
           ↓
  Student Model q_θ → generates samples y ~ e_p (mixed distribution)
           ↓
  Reward Computation → r_t = log(p/q) per token, accumulated as R_t
           ↓
  Gradient Assembly → (∇L)_Single + (∇L)^Norm_Long + ∇L_PT
           ↓
  Optimizer Update (with clipping)
  ```

- Critical path:
  1. Phase 1: Standard SFT on D to get initialization (select by lowest validation loss, not Rouge-L)
  2. Phase 2: On-policy MINILLM training for ~5000 steps (select final checkpoint by Rouge-L)
  3. The mixed sampling distribution e_p must be computed fresh each step; teacher logits must be computed without gradients

- Design tradeoffs:
  - α = 0.2 balances exploration vs stability; larger models tolerate higher α
  - Including L_PT preserves canonical NLP performance at small instruction-following cost
  - Batch size 64 with 256 samples collected at once and 4 inner epochs matches RLHF-style training

- Failure signatures:
  - Empty or very short responses → length normalization disabled or α too low
  - Repeated phrases → reward hacking; increase α or check clipping
  - High validation loss but good Rouge-L → normal in Phase 2; monitor reverse KLD curve
  - Loss plateau early → learning rate may be too high; try 5e-6

- First 3 experiments:
  1. Reproduce ablation from Table 4 on GPT-2-125M → GPT-2-1.5B to validate each stabilization component is working
  2. Sweep α ∈ {0.1, 0.2, 0.3} on validation Rouge-L to confirm 0.2 is optimal for your model scale
  3. Compare checkpoints selected by validation loss vs Rouge-L in Phase 2 to understand the selection criterion impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trade-off between generation correctness (mode-seeking) and linguistic diversity be dynamically controlled for creative tasks?
- Basis in paper: Section 3.3 "Generation Diversity" explicitly states that minimizing reverse KLD is "likely to lose modes" but argues this is acceptable for "truthfulness." It acknowledges that for some applications, generating diverse responses is necessary.
- Why unresolved: The current implementation prioritizes the major modes of the teacher to ensure correctness, but no mechanism is proposed to preserve or tune diversity for tasks like story generation where mode coverage is desirable.
- What evidence would resolve it: Experiments on open-ended generation benchmarks measuring distinct n-grams and semantic variability against a ground-truth diversity set.

### Open Question 2
- Question: Does the positive correlation between teacher size and student performance persist when distilling from massive models (e.g., >100B parameters)?
- Basis in paper: Section 3.3 "Scaling Law of Teacher" investigates this but only tests up to 13B parameters. The authors note that previous work found increasing teacher size can harm performance (the "capacity gap") and sought to verify MiniLLM's behavior, which was positive in the tested range.
- Why unresolved: It is unclear if the observed positive scaling trend continues indefinitely or if a "capacity gap" eventually emerges when the teacher's complexity vastly outstrips the student's capacity.
- What evidence would resolve it: Distillation experiments using teachers significantly larger than 13B (e.g., GPT-4 or PaLM-540B) into small student models (<1B) to observe if performance saturates or degrades.

### Open Question 3
- Question: How does the mode-seeking behavior of MiniLLM affect performance on reasoning tasks requiring exploration of multiple solution paths?
- Basis in paper: The paper focuses on instruction-following and "correctness." The method encourages the student to focus on the teacher's "major modes." In complex reasoning or math, the "major mode" might be the final answer, potentially suppressing the learning of intermediate reasoning steps if they appear as lower-probability variants.
- Why unresolved: The evaluation relies on Rouge-L and GPT-4 feedback on instruction following, which prioritizes surface-level correctness and helpfulness rather than multi-step logical derivation where process diversity matters.
- What evidence would resolve it: Evaluation on chain-of-thought reasoning benchmarks to see if the student retains the ability to derive answers or merely mimics the teacher's final output distribution.

## Limitations
- The method requires white-box access to the teacher model for gradient computation, excluding black-box scenarios
- The optimal α=0.2 appears arbitrary without systematic justification across different model scales
- The GPT-4 feedback evaluation uses a limited 500-sample test set without statistical significance testing

## Confidence
- Reverse KLD mechanism: **High confidence** - The theoretical motivation is sound with clear visual evidence from toy experiments
- On-policy training efficacy: **Medium confidence** - Empirically demonstrated but introduces high variance requiring careful tuning
- Stabilization strategy combination: **Medium confidence** - Ablation shows individual components matter but alternative configurations weren't explored
- Overall performance superiority: **Medium confidence** - GPT-4 feedback scores are promising but evaluation methodology lacks statistical rigor

## Next Checks
1. **Capacity ratio sensitivity test**: Systematically vary the student-to-teacher parameter ratio and measure whether reverse KLD maintains its advantage over forward KLD as capacity gaps shrink.

2. **Black-box teacher adaptation**: Implement a black-box version using forward-mode differentiation through sampling and compare performance degradation to quantify the white-box advantage.

3. **Alternative stabilization comparison**: Replace the three-component stabilization strategy with simpler alternatives to determine if the full combination is truly necessary or if simpler approaches achieve comparable results.