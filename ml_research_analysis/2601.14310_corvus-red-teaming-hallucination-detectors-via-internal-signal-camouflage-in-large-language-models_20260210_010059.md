---
ver: rpa2
title: 'CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage
  in Large Language Models'
arxiv_id: '2601.14310'
source_url: https://arxiv.org/abs/2601.14310
tags:
- corvus
- telemetry
- answer
- attention
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how internal telemetry signals used by single-pass\
  \ hallucination detectors can be manipulated through model-side adversarial adaptation.\
  \ It introduces CORVUS, a red-teaming procedure that fine-tunes lightweight LoRA\
  \ adapters to camouflage detector-visible telemetry\u2014specifically token entropy,\
  \ hidden log-volume, and attention diagonality\u2014under teacher forcing, including\
  \ an embedding-space FGSM stress test for attention."
---

# CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models

## Quick Facts
- **arXiv ID**: 2601.14310
- **Source URL**: https://arxiv.org/abs/2601.14310
- **Reference count**: 9
- **Key outcome**: CORVUS fine-tunes LoRA adapters to camouflage internal telemetry signals, degrading hallucination detectors on FA VA-Annotation benchmark.

## Executive Summary
This paper introduces CORVUS, a red-teaming procedure that manipulates internal telemetry signals used by single-pass hallucination detectors. By fine-tuning lightweight LoRA adapters under teacher forcing, CORVUS camouflages detector-visible signals including token entropy, hidden log-volume, and attention diagonality. The method transfers to the FA VA-Annotation benchmark, significantly degrading both training-free detectors (LLM-Check) and probe-based detectors (SEP, ICR-probe) across four open-weight models, revealing brittleness in internal-signal-based detection approaches.

## Method Summary
CORVUS employs a red-teaming approach that fine-tunes LoRA adapters to camouflage internal telemetry signals used by hallucination detectors. The method targets three specific signals: token entropy, hidden log-volume, and attention diagonality. Training occurs under teacher forcing conditions using 1,000 out-of-distribution Alpaca instructions, with an additional embedding-space FGSM stress test for attention signals. The camouflaged models are then evaluated against hallucination detection benchmarks to measure degradation in detector performance.

## Key Results
- CORVUS fine-tunes LoRA adapters to successfully camouflage token entropy, hidden log-volume, and attention diagonality signals
- Significant degradation observed in both training-free detectors (LLM-Check) and probe-based detectors (SEP, ICR-probe) on FA VA-Annotation benchmark
- Transferability demonstrated across four different open-weight language models
- Substantial drops in AUROC and calibration shifts indicate detector brittleness to model-side adversarial adaptation

## Why This Works (Mechanism)
CORVUS exploits the fact that many hallucination detectors rely on internal telemetry signals from language models. By fine-tuning lightweight LoRA adapters under teacher forcing, the model learns to generate outputs that appear normal to these detectors while potentially containing hallucinated content. The embedding-space FGSM stress test for attention signals adds another layer of camouflage by specifically targeting the attention mechanism's internal representations.

## Foundational Learning

**LoRA (Low-Rank Adaptation)**
- Why needed: Enables efficient fine-tuning by modifying only small low-rank matrices rather than full model weights
- Quick check: Verify that LoRA adapters are significantly smaller than full model weights (typically 1-2% of original parameters)

**Teacher Forcing**
- Why needed: Ensures training stability by providing ground truth tokens as input during fine-tuning rather than model-generated tokens
- Quick check: Confirm that teacher forcing is applied consistently throughout the fine-tuning process

**Token Entropy as Detector Signal**
- Why needed: Measures prediction uncertainty; high entropy often indicates potential hallucination
- Quick check: Verify that token entropy distributions differ significantly between ground truth and hallucinated sequences

**Attention Diagonality**
- Why needed: Captures attention pattern regularity; deviations may indicate generation inconsistencies
- Quick check: Confirm that attention matrices from hallucinated sequences show lower diagonality scores

**Hidden Log-Volume**
- Why needed: Measures hidden state activation magnitude; abnormal volumes can signal generation artifacts
- Quick check: Verify that hidden state distributions differ between normal and hallucinated generation phases

## Architecture Onboarding

**Component Map**
CORVUS Adapter -> LoRA Fine-tuning Module -> Internal Signal Monitor -> FGSM Stress Test -> Hallucination Detector

**Critical Path**
1. Initialize LoRA adapters for target model
2. Fine-tune adapters using teacher forcing on out-of-distribution instructions
3. Apply embedding-space FGSM stress test to attention signals
4. Evaluate camouflaged model against hallucination detectors

**Design Tradeoffs**
- LoRA adapters provide computational efficiency but may limit adaptation scope
- Teacher forcing ensures stability but may constrain natural generation patterns
- Single-pass detection focus leaves multi-turn scenarios unexplored

**Failure Signatures**
- Insufficient camouflage leading to detector detection
- Overfitting to specific detector architectures
- Loss of generation quality due to aggressive signal manipulation

**3 First Experiments**
1. Measure detector performance degradation on single adapted model before scaling
2. Compare LoRA-based camouflage effectiveness against full-model fine-tuning
3. Test transferability of camouflage to unseen detector architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Transferability to other hallucination detection benchmarks beyond FA VA-Annotation remains unclear
- 1,000 Alpaca instruction fine-tuning set may limit adversarial pattern diversity
- LoRA adapters may not capture all possible model-side adaptations an adversary could employ

## Confidence

**High confidence**: CORVUS successfully fine-tunes LoRA adapters to camouflage internal telemetry signals (token entropy, hidden log-volume, attention diagonality) under teacher forcing conditions. The degradation of hallucination detectors on FA VA-Annotation is reproducible and statistically significant.

**Medium confidence**: The brittleness of internal-signalâ€“based hallucination detectors to model-side adversarial adaptation generalizes beyond the specific detectors and models tested. The 1,000 Alpaca instruction fine-tuning set provides sufficient diversity for meaningful adversarial pattern learning.

**Low confidence**: CORVUS adaptations would remain effective against detectors incorporating external grounding or cross-model validation. The embedding-space FGSM stress test represents the most effective adversarial technique for attention signal manipulation.

## Next Checks
1. Test CORVUS-adapted models against hallucination detectors that incorporate external knowledge grounding or cross-model validation to assess whether these defenses mitigate signal camouflage.

2. Evaluate detector degradation across a broader set of hallucination detection benchmarks beyond FA VA-Annotation to establish generalization of the findings.

3. Compare the effectiveness of LoRA-based signal camouflage against full-model fine-tuning and other lightweight adversarial adaptation techniques to characterize the threat landscape more completely.