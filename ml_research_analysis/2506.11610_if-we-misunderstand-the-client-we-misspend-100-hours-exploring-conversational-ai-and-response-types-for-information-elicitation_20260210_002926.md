---
ver: rpa2
title: '"If we misunderstand the client, we misspend 100 hours": Exploring conversational
  AI and response types for information elicitation'
arxiv_id: '2506.11610'
source_url: https://arxiv.org/abs/2506.11610
tags:
- design
- designers
- questions
- were
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how digital systems, particularly those
  integrating conversational AI and choice-based response formats, can support information
  elicitation and foster mutual understanding in early-stage client-designer collaboration.
  Through interviews with 10 design companies, a prototype elicitation tool was developed
  and evaluated with 50 mock clients in a 2x2 factorial design (AI vs.
---

# "If we misunderstand the client, we misspend 100 hours": Exploring conversational AI and response types for information elicitation

## Quick Facts
- arXiv ID: 2506.11610
- Source URL: https://arxiv.org/abs/2506.11610
- Reference count: 40
- Key outcome: This paper investigates how digital systems, particularly those integrating conversational AI and choice-based response formats, can support information elicitation and foster mutual understanding in early-stage client-designer collaboration.

## Executive Summary
This study addresses the critical challenge of information elicitation in client-designer collaboration, where misunderstandings can lead to significant time and resource waste. Through interviews with 10 design companies and a controlled experiment with 50 mock clients, the researchers developed and evaluated a prototype elicitation tool. The tool combines conversational AI with choice-based response formats to improve clarity of client responses during early project phases.

The research demonstrates that while conversational AI and choice-based responses may yield lower user experience scores, they significantly improve the quality and clarity of information gathered from clients. Designers particularly valued the system for client preparation, though emphasized the need for AI outputs to be verifiable and editable. The findings suggest that effective elicitation tools must balance AI assistance with human control and accommodate diverse user preferences.

## Method Summary
The research employed a mixed-methods approach, beginning with qualitative interviews of 10 design companies to understand current elicitation practices and challenges. Based on these insights, a prototype elicitation tool was developed and tested in a 2x2 factorial design experiment with 50 mock clients. The experiment compared four conditions: conversational AI with free-text responses, conversational AI with choice-based responses, no AI with free-text responses, and no AI with choice-based responses. User experience was measured using the User Experience Questionnaire, while response quality was evaluated through expert assessment. Designers provided qualitative feedback on the tool's utility for client preparation.

## Key Results
- Conversational AI and choice-based responses led to lower dependability scores on the User Experience Questionnaire
- These same conditions improved the clarity of client responses
- Designers valued the system for client preparation but emphasized the need for AI outputs to be verifiable and editable

## Why This Works (Mechanism)
The system works by creating a structured dialogue that guides clients through information sharing while providing real-time assistance. Conversational AI helps frame questions and interpret responses, while choice-based formats reduce ambiguity and cognitive load. This combination addresses the inherent challenge of clients articulating complex design needs by providing scaffolding that bridges communication gaps between technical and non-technical stakeholders.

## Foundational Learning
1. **Information Elicitation**: The process of gathering requirements and understanding client needs through structured questioning - needed to ensure comprehensive project understanding; quick check: does the tool capture all critical project dimensions?
2. **Conversational AI**: AI systems designed to engage in natural dialogue - needed to make information gathering feel intuitive; quick check: does the AI maintain context and ask relevant follow-up questions?
3. **Choice-Based Response Formats**: Predefined response options rather than open-ended text - needed to reduce ambiguity and improve data quality; quick check: are response options comprehensive yet not overwhelming?
4. **User Experience Questionnaires**: Standardized metrics for evaluating user satisfaction - needed to objectively measure tool effectiveness; quick check: do scores align with qualitative feedback?
5. **Mock Client Simulation**: Using simulated clients to test tools in controlled environments - needed to isolate tool effects from real-world variables; quick check: do mock clients represent target user diversity?
6. **2x2 Factorial Design**: Experimental design testing two factors at two levels each - needed to understand interaction effects between AI and response formats; quick check: are main effects and interactions statistically significant?

## Architecture Onboarding

**Component Map**: Mock Clients -> Elicitation Tool (AI Engine, Response Format Module, UI Interface) -> Designer Feedback System -> Quality Assessment

**Critical Path**: Client engagement → Information gathering → AI processing → Response generation → Designer review → Project understanding

**Design Tradeoffs**: The system trades some user experience (lower dependability scores) for improved response clarity and information quality. This reflects the fundamental tension between ease of use and effectiveness in professional tools.

**Failure Signatures**: 
- AI generates irrelevant or confusing questions → Clients disengage or provide poor responses
- Choice options are too limited → Clients feel constrained and provide inaccurate information
- UI is overly complex → Lower user experience scores despite good information quality
- AI lacks verifiability → Designers lose trust in the system

**First Experiments**:
1. A/B test comparing AI-assisted vs. traditional questionnaire approaches with real clients
2. Field study measuring actual project time savings when using the tool
3. Longitudinal study tracking tool adoption and effectiveness across multiple design projects

## Open Questions the Paper Calls Out
None

## Limitations
- Artificial experimental setting with mock clients rather than real ones may not reflect genuine professional contexts
- 50 mock client participants may not represent the diversity of real client populations
- Focus on early-stage information elicitation limits generalizability to later design phases

## Confidence

| Claim | Confidence |
|-------|------------|
| Conversational AI improves response clarity despite lower UX scores | Medium |
| Designers value the system for client preparation | Medium |
| AI outputs must be verifiable and editable | Medium |
| Methodological approach and experimental design | High |

## Next Checks
1. Conduct field studies with real clients across different industries to validate mock client findings
2. Test the tool across multiple design phases beyond initial information gathering
3. Implement A/B testing with actual client-designer pairs to measure real-world impact on project outcomes and time savings