---
ver: rpa2
title: Multi-Turn Jailbreaks Are Simpler Than They Seem
arxiv_id: '2508.07646'
source_url: https://arxiv.org/abs/2508.07646
tags:
- multi-turn
- attacks
- single-turn
- jailbreak
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the perceived sophistication of multi-turn
  jailbreak attacks on large language models. The authors find that when accounting
  for retry mechanisms after refusals, multi-turn jailbreaking approaches are approximately
  equivalent to simply resampling single-turn attacks multiple times.
---

# Multi-Turn Jailbreaks Are Simpler Than They Seem

## Quick Facts
- arXiv ID: 2508.07646
- Source URL: https://arxiv.org/abs/2508.07646
- Authors: Xiaoxue Yang; Jaeha Lee; Anna-Katharina Dick; Jasper Timm; Fei Xie; Diogo Cruz
- Reference count: 40
- Primary result: Multi-turn jailbreak attacks are approximately equivalent to repeatedly sampling single-turn attacks when accounting for retry mechanisms after refusals

## Executive Summary
This study challenges the perceived sophistication of multi-turn jailbreak attacks on large language models. The authors find that when accounting for retry mechanisms after refusals, multi-turn jailbreaking approaches are approximately equivalent to simply resampling single-turn attacks multiple times. Using the StrongREJECT benchmark across models including GPT-4, Claude, and Gemini variants, they demonstrate that attack success rates correlate strongly among models from the same provider, making newly released models predictably vulnerable. Additionally, for reasoning models, higher reasoning effort often leads to higher attack success rates, contrary to expectations.

## Method Summary
The study evaluates multi-turn versus single-turn jailbreak attacks using a three-component pipeline: an attacker model (GPT-4o-mini, temperature=1) generates attack prompts, a target model (various LLMs, temperature=0) responds to requests, and an evaluator model (GPT-4o-mini, temperature=0) scores responses using the StrongREJECT rubric. The evaluation tests 30 harmful behaviors from the StrongREJECT dataset across multiple attack tactics, primarily focusing on Direct Request. Default settings include n_refusals=10, n_turns=8 for multi-turn attacks, and n_turns=1 for single-turn attacks. All models are accessed via OpenRouter API.

## Key Results
- Multi-turn jailbreak attacks achieve high success rates primarily through retry mechanics after refusals, not sophisticated conversational dynamics
- Attack success rates correlate strongly within model families, making newly released models predictably vulnerable based on older model testing
- For reasoning models, higher reasoning effort often leads to higher attack success rates, contrary to expected robustness benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn jailbreaks achieve high attack success not through sophisticated conversational dynamics, but through simple retry mechanics after refusals.
- Mechanism: The paper shows that when a target model refuses a harmful request, the attacker model can retry with modified prompts. With n_refusals=10, each additional turn or retry attempt serves as a resampling event, increasing the probability of eventual success. The empirical data shows that attack success scales with total attempts regardless of whether those are multi-turn conversations or repeated single-turn attacks.
- Core assumption: The "Direct Request" tactic (simply asking in a professional tone) is representative of multi-turn attack effectiveness. The paper focuses primarily on this tactic due to evaluation reliability.
- Evidence anchors:
  - [abstract] "when accounting for the attacker's ability to learn from how models refuse harmful requests, multi-turn jailbreaking approaches are approximately equivalent to simply resampling single-turn attacks multiple times"
  - [section 5] "The gap disappears if single-turn attacks are attempted an equivalent number of times" and Figure 2 (right) shows equivalence between multi-turn and repeated single-turn attempts
  - [corpus] Limited direct support. Related work "M2S: Multi-turn to Single-turn jailbreak" also suggests consolidation is possible but doesn't confirm the retry/resampling mechanism specifically
- Break condition: If a multi-turn tactic requires genuine contextual dependencies across turns (e.g., information revealed in turn 2 that enables turn 3), the resampling equivalence would not hold. Tactics like Crescendo that build psychological momentum may differ.

### Mechanism 2
- Claim: Attack success rates correlate within model families, making new model releases predictably vulnerable.
- Mechanism: Models from the same provider share training data, safety fine-tuning procedures, and architectural similarities. When Claude 3.5 Sonnet shows vulnerability to specific harmful requests, Claude Sonnet 4 and Claude 3.7 Sonnet show correlated vulnerabilities. This transferability enables attackers to test on older, cheaper models and predict success on newer releases.
- Core assumption: The correlation patterns observed across ~30 test cases from StrongREJECT generalize to broader harmful behavior distributions.
- Evidence anchors:
  - [abstract] "attack success is correlated among similar models, making it easier to jailbreak newly released ones"
  - [section 5] "models from the same lab are more likely to show similar robustness"
  - [Appendix L] Correlation matrices show higher within-provider correlations (e.g., Claude variants correlate 0.35-0.60 with each other vs. 0.1-0.3 with non-Claude models)
  - [corpus] Weak direct support in corpus for the cross-model transferability mechanism
- Break condition: If a provider makes fundamental safety architecture changes (e.g., new classifier training approach, different refusal mechanisms), historical correlation patterns would not predict new model behavior.

### Mechanism 3
- Claim: Increased reasoning effort in "thinking" models leads to higher jailbreak success rates, contrary to expected robustness benefits.
- Mechanism: Assumption: Reasoning models may explore more possible framings of a request during inference. When a harmful request has a plausible dual-use interpretation, additional reasoning may surface the benign interpretation and produce harmful content. Alternatively, more reasoning may produce longer, more detailed responses that score higher on the StrongREJECT rubric (specificity and convincingness metrics).
- Core assumption: The StrongREJECT evaluator accurately captures genuine harmful content rather than rewarding verbose responses regardless of harm.
- Evidence anchors:
  - [abstract] "for reasoning models, we find surprisingly that higher reasoning effort often leads to higher attack success rates"
  - [section 5] Figure 3 shows StrongREJECT scores increase with reasoning token usage (0-200 → 1500+ tokens) for Claude 3.7 Sonnet (thinking) and OpenAI o-series models
  - [corpus] No direct corpus support; concurrent work by Zaremba et al. found mixed results with inference-time compute and robustness
- Break condition: If the score increase is driven by evaluator artifacts (e.g., rewarding length) rather than actual harmful content, the mechanism is evaluation failure, not reasoning behavior. Human evaluation would be needed to confirm.

## Foundational Learning

- Concept: Black-box jailbreak threat model
  - Why needed here: The paper evaluates attacks where the adversary only has chat API access—no model weights, output logits, or ability to tamper with conversation history. This is the realistic deployment scenario.
  - Quick check question: If you had logit access to the target model, would retry-based attacks still be your optimal strategy, or would gradient-based attacks become feasible?

- Concept: StrongREJECT evaluation rubric
  - Why needed here: The paper relies on a 0-1 continuous score measuring both non-refusal AND response quality (specificity, convincingness). Understanding this rubric is critical to interpreting why some attacks score higher.
  - Quick check question: Would a response that refuses the harmful request but explains why it's dangerous receive a high or low StrongREJECT score?

- Concept: Exponential approach model for attack success
  - Why needed here: Appendix I introduces S(k) = A - B·e^(-ck) where A is asymptotic success rate (some conversations are fundamentally immune). This models diminishing returns as attempts increase.
  - Quick check question: If you observe attack success saturating at 70% after 8 turns, what does this imply about the remaining 30% of cases?

## Architecture Onboarding

- Component map: Attacker model (GPT-4o-mini, temp=1) -> Target model (various, temp=0) -> Evaluator model (GPT-4o-mini, temp=0) -> Pipeline controller (manages turns, refusals, conversation state)

- Critical path: For each harmful behavior q → M_A generates prompt → M_T responds → M_E scores → If score < threshold and refusals < max, retry with context of failure → Log results

- Design tradeoffs:
  - StrongREJECT evaluator shows false positives, especially