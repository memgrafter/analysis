---
ver: rpa2
title: 'MI-PRUN: Optimize Large Language Model Pruning via Mutual Information'
arxiv_id: '2601.07212'
source_url: https://arxiv.org/abs/2601.07212
tags:
- blocks
- pruning
- arxiv
- information
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of compressing large language
  models (LLMs) through block pruning, which removes entire blocks to achieve inference
  acceleration. Existing block pruning methods suffer from instability due to reliance
  on calibration-dependent metrics and often use greedy algorithms that fail to find
  globally optimal solutions.
---

# MI-PRUN: Optimize Large Language Model Pruning via Mutual Information

## Quick Facts
- **arXiv ID**: 2601.07212
- **Source URL**: https://arxiv.org/abs/2601.07212
- **Reference count**: 11
- **Primary result**: Achieves 25.28% compression ratio while maintaining benchmark performance on Llama2 and Qwen models

## Executive Summary
This paper addresses the problem of compressing large language models (LLMs) through block pruning, which removes entire transformer blocks to achieve inference acceleration. Existing block pruning methods suffer from instability due to reliance on calibration-dependent metrics and often use greedy algorithms that fail to find globally optimal solutions. The proposed MI-PRUN method leverages mutual information to identify redundant blocks by measuring transitions in hidden states, providing a more stable approach than cosine similarity. It incorporates the Data Processing Inequality (DPI) to relate the importance of contiguous blocks to individual blocks. Additionally, MI-PRUN introduces the Fast-Block-Select algorithm that iteratively updates block combinations to achieve a globally optimal solution while significantly improving efficiency.

## Method Summary
MI-PRUN optimizes LLM pruning by computing mutual information between consecutive hidden states of each transformer block using a calibration set. Lower MI indicates more important blocks as they cause greater state transformations. The method uses the Data Processing Inequality to justify analyzing contiguous block groups and implements a Fast-Block-Select algorithm that iteratively refines pruning combinations. This approach selects the globally optimal set of blocks to prune while avoiding conflicts between selected blocks, achieving better performance than baseline methods like ShortGPT, SliceGPT, and LLM-Pruner across multiple benchmarks.

## Key Results
- Achieves 25.28% compression ratio while maintaining benchmark performance
- Outperforms baselines on Winogrande, PIQA, WSC, WNLI, SST-2, RTE, QNLI, CB, ARC-e, and ARC-c benchmarks
- Demonstrates better stability compared to cosine similarity-based methods

## Why This Works (Mechanism)
The method works by measuring how much information is preserved between consecutive hidden states, with less information preservation indicating more significant transformations and thus more important blocks. The Data Processing Inequality provides a theoretical foundation for analyzing contiguous blocks as units, while the Fast-Block-Select algorithm ensures globally optimal pruning decisions through iterative refinement rather than greedy selection.

## Foundational Learning

**Mutual Information**: Measures the dependency between two random variables; needed to quantify how much information is preserved across transformer blocks. Quick check: Verify that I(X,Y) = H(X) - H(X|Y) holds for your implementation.

**Data Processing Inequality**: States that for a Markov chain X → Y → Z, I(X,Z) ≤ min(I(X,Y), I(Y,Z)); needed to justify analyzing contiguous blocks as units. Quick check: Confirm that I(h_i, h_i+2) ≤ min(I(h_i, h_i+1), I(h_i+1, h_i+2)) holds for your model.

**k-NN Mutual Information Estimation**: Required for estimating MI in high-dimensional continuous spaces; needed because analytical MI computation is intractable for hidden states. Quick check: Validate estimator accuracy on synthetic data with known MI values.

## Architecture Onboarding

**Component Map**: Input → Transformer Blocks → Output; MI-PRUN analyzes block transitions and selects pruning sets.

**Critical Path**: Calibration data → Block MI computation → Fast-Block-Select algorithm → Pruning mask generation → Model pruning.

**Design Tradeoffs**: Global optimization vs. computational efficiency; continuous MI estimation vs. discrete block selection; theoretical guarantees vs. practical approximations.

**Failure Signatures**: Poor MI estimation leads to incorrect block rankings; DPI assumption violations cause suboptimal pruning; calibration set quality directly impacts pruning decisions.

**First Experiments**:
1. Validate MI estimation by comparing block importance rankings against ablation studies.
2. Test DPI assumption empirically across different model layers.
3. Evaluate sensitivity to calibration set size and composition.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on Data Processing Inequality assumption, which may not hold perfectly across all transformer architectures.
- MI estimation approach for high-dimensional hidden states is not precisely specified, potentially leading to inconsistent results.
- Focuses solely on block pruning without addressing other compression techniques like quantization or weight pruning.

## Confidence

**High confidence**: The method improves upon baseline block pruning approaches in terms of stability and compression ratio, supported by experimental results on multiple benchmarks.

**Medium confidence**: The theoretical justification via DPI and mutual information is sound, but its practical effectiveness may vary across different model architectures.

**Low confidence**: The claim that MI-PRUN achieves "globally optimal" solutions, as the greedy selection within the Fast-Block-Select algorithm may still miss some optimal combinations.

## Next Checks

1. Validate the MI estimation implementation by comparing block importance rankings against ablation studies (removing individual blocks and measuring performance impact).
2. Test the DPI assumption empirically by measuring I(h_i, h_i+2) against min(I(h_i, h_i+1), I(h_i+1, h_i+2)) across different model layers and verify the inequality holds consistently.
3. Evaluate the sensitivity of pruning results to calibration set size and composition by running experiments with varying calibration data quantities and sources.