---
ver: rpa2
title: Contrastive Retrieval Heads Improve Attention-Based Re-Ranking
arxiv_id: '2510.02219'
source_url: https://arxiv.org/abs/2510.02219
tags:
- heads
- re-ranking
- retrieval
- attention
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoRe heads are a set of attention heads identified through a contrastive
  scoring metric that explicitly rewards attention directed toward relevant documents
  while penalizing attention to irrelevant ones. This method outperforms existing
  approaches by isolating the most discriminative heads for re-ranking.
---

# Contrastive Retrieval Heads Improve Attention-Based Re-Ranking

## Quick Facts
- arXiv ID: 2510.02219
- Source URL: https://arxiv.org/abs/2510.02219
- Authors: Linh Tran; Yulong Li; Radu Florian; Wei Sun
- Reference count: 21
- Key outcome: CoRe heads—a small set of attention heads identified through contrastive scoring—improve re-ranking accuracy while enabling up to 50% layer pruning, achieving 20% latency reduction and 40% memory savings.

## Executive Summary
This paper introduces Contrastive Retrieval (CoRe) heads, a method for improving attention-based re-ranking by identifying and aggregating signals from a small set of discriminative attention heads. The method uses a contrastive scoring metric to isolate heads that effectively distinguish relevant from irrelevant documents. Experiments with three LLMs demonstrate that using fewer than 1% of all heads substantially improves re-ranking accuracy across BEIR and MLDR benchmarks while enabling significant model compression.

## Method Summary
CoRe heads are identified through a contrastive scoring metric that explicitly rewards attention directed toward relevant documents while penalizing attention to irrelevant ones. The method involves pre-computing head scores on NQ data using an InfoNCE-like formulation, selecting the top-scoring heads (typically 8), and using only these heads for re-ranking. The approach includes contextual calibration to handle content-free query tokens and enables aggressive layer pruning (up to 50%) without accuracy loss, achieving substantial efficiency gains.

## Key Results
- Using <1% of heads (8 CoRe heads) improves re-ranking accuracy over strong baselines across BEIR and MLDR
- CoRe heads concentrate in middle transformer layers, enabling up to 50% layer pruning without accuracy loss
- Achieves 20% latency reduction and 40% memory savings through selective head aggregation and layer pruning
- Consistently achieves state-of-the-art list-wise re-ranking performance across multiple LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A contrastive scoring metric identifies discriminative attention heads by rewarding attention to relevant documents while penalizing attention to hard negatives.
- Mechanism: The InfoNCE-like formulation computes `S_CoRe(h) = exp(s_h^pos/τ) / (exp(s_h^pos/τ) + Σ_i exp(s_h^neg,i/τ))` for each head. This creates a relative ranking criterion rather than measuring absolute attention, favoring heads that maximize the margin between positive and negative document attention scores.
- Core assumption: Relevant and irrelevant documents have distinguishable attention patterns in at least some heads; hard negatives provide a sufficiently challenging contrastive signal.
- Evidence anchors:
  - [abstract] "contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents"
  - [section 4] "A higher value of S_CoRe indicates that the corresponding attention head more effectively discriminates the positive document from hard negative documents"
  - [corpus] Related work (MuDAF, QR heads) confirms attention head specialization but lacks the explicit contrastive formulation
- Break condition: If attention distributions for relevant and irrelevant documents are indistinguishable across all heads, or if hard negatives are too similar to positives, the scoring metric cannot isolate discriminative heads.

### Mechanism 2
- Claim: Aggregating attention signals from <1% of heads (CoRe heads) improves re-ranking accuracy over using all heads.
- Mechanism: Standard attention-based re-rankers (ICR) aggregate signals from all L×H heads, introducing noise from redundant or non-informative heads. CoRe-R restricts aggregation to the top-k CoRe heads by summing attention scores only from heads in set H*, reducing variance from unhelpful heads while preserving discriminative signals.
- Core assumption: Most attention heads contribute noise or task-irrelevant patterns; discriminative information is concentrated in a small subset.
- Evidence anchors:
  - [abstract] "aggregated signals from CoRe heads, constituting less than 1% of all heads, substantially improve re-ranking accuracy over strong baselines"
  - [section 5.2, Figure 3] Shows performance peaks with 8-9 heads and degrades when using more heads
  - [corpus] Prior work (Michel et al., 2019; Voita et al., 2019) confirms redundancy in multi-head attention
- Break condition: If discriminative information is distributed across many heads rather than concentrated, or if noise is minimal across all heads (as observed with Granite-3.2 8B), the benefit of selective aggregation diminishes.

### Mechanism 3
- Claim: CoRe heads concentrate in middle transformer layers, enabling layer pruning without accuracy loss.
- Mechanism: Top-scoring CoRe heads cluster in layers 8-16 (depending on model depth), while later layers primarily support token generation rather than retrieval-relevant attention. Pruning 50% of final layers removes non-essential computation while preserving CoRe head activations.
- Core assumption: Retrieval-relevant attention patterns emerge and stabilize in middle layers; later layers serve generation purposes unnecessary for re-ranking.
- Evidence anchors:
  - [abstract] "CoRe heads are concentrated in middle layers, and pruning the computation of final 50% of model layers preserves accuracy"
  - [section 5.3, Figure 4] Heatmaps show high S_CoRe scores in middle layers across Mistral, Llama, and Phi models
  - [corpus] Limited direct evidence; related work on retrieval heads (Wu et al., 2025) finds heads in later layers but for copy-paste tasks
- Break condition: If future models distribute retrieval heads differently, or if middle-layer heads are pruned during other optimization, the layer-pruning strategy may not generalize.

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE)**
  - Why needed here: The CoRe scoring metric adapts InfoNCE loss to identify heads that distinguish relevant from irrelevant documents. Understanding how temperature affects the sharpness of the contrastive distribution is critical for head selection.
  - Quick check question: Given two attention scores (positive: 0.6, negative: 0.4), how does temperature τ=0.1 vs τ=1.0 change the resulting S_CoRe score?

- Concept: **Multi-Head Attention in Transformers**
  - Why needed here: The method operates on per-head attention matrices, requiring understanding of how heads specialize and how attention flows from query tokens to document tokens.
  - Quick check question: In a 32-layer model with 32 heads per layer, what is the total number of attention heads, and what does it mean for CoRe to select only 8?

- Concept: **Re-ranking in Information Retrieval**
  - Why needed here: The paper frames re-ranking as list-wise (jointly considering all candidates) rather than point-wise or pair-wise, which affects how attention patterns are interpreted.
  - Quick check question: Why might aggregating attention from all heads hurt list-wise re-ranking compared to selective aggregation?

## Architecture Onboarding

- Component map: Retriever → Candidate Documents (k=40) → Prompt Construction (documents + query) → LLM Forward Pass → Attention Matrices (L layers × H heads) → CoRe Head Selection (pre-computed via S_CoRe on NQ subset) → Attention Aggregation (sum over CoRe heads only) → Contextual Calibration (subtract content-free query baseline) → Document Relevance Scores → Re-ranked List

- Critical path:
  1. Pre-compute CoRe heads: Run forward passes on 1000 NQ samples with 50 docs each, compute S_CoRe per head, average across samples, select top-k heads (k=8).
  2. At inference: Load pre-identified CoRe head indices; run single forward pass; extract attention from only those heads; aggregate and calibrate.
  3. Optional: Prune layers >50% depth for efficiency (if using Mistral 7B, Llama-3.1 8B, or Phi-4).

- Design tradeoffs:
  - **Temperature (τ)**: Lower τ (0.001-0.1) creates sharper head distributions but may over-select; grid-search on held-out data recommended.
  - **Number of CoRe heads (k)**: Peak performance at k=8-9 across models; more heads introduce noise, fewer may miss useful signals.
  - **Hard negative mining**: Using 49 negatives from top-100 retriever candidates, filtered by similarity threshold, ensures challenging contrastive conditions.
  - **Layer pruning threshold**: 50% pruning works for tested models; 60% causes degradation; model-specific tuning required.

- Failure signatures:
  - **Cross-domain mismatch**: If CoRe heads identified on NQ don't generalize (e.g., counter-evidence tasks like Climate-FEVER), re-ranking underperforms retriever baseline.
  - **Duplicate detection tasks**: Quora dataset shows degraded performance with universal prompts; requires task-specific prompts.
  - **Insufficient hard negatives**: If negatives are too easy, S_CoRe scores become uninformative; if negatives include false negatives (similar to gold), scores penalize useful heads.
  - **Model architecture mismatch**: Granite-3.2 8B shows CoRe heads in both middle and late layers, making aggressive pruning less effective.

- First 3 experiments:
  1. **Validate CoRe head identification on your target model**: Run the head detection pipeline on 100-500 NQ samples with your model; verify top heads cluster in middle layers (visualize S_CoRe heatmap as in Figure 4).
  2. **Ablate number of heads**: Test re-ranking with k ∈ {1, 4, 8, 16, 32} CoRe heads on a validation dataset (e.g., DBPedia or FiQA); plot nDCG@10 vs k to find peak.
  3. **Test generalization to your domain**: Apply pre-computed CoRe heads to your target dataset; if underperforming, try (a) tuning temperature τ, (b) adding domain-specific prompts, or (c) re-identifying heads on domain-relevant data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated or principled methods for dataset-specific prompt design systematically improve attention-based re-ranking performance across diverse task types?
- Basis in paper: [explicit] Section 5.4 states "Future work could explore dataset specific prompt designs to further improve re-ranking performance" and Appendix C.2 shows customized prompts improve Quora performance substantially.
- Why unresolved: The paper uses a universal prompt across all datasets but acknowledges this "may not align with dataset-specific task objectives," particularly for counter-evidence retrieval (Climate-FEVER, ArguAna) and duplicate detection (Quora).
- What evidence would resolve it: A systematic study comparing automated prompt optimization methods across BEIR datasets, measuring gains over universal prompts.

### Open Question 2
- Question: Why do attention-based re-ranking methods systematically underperform retrieval baselines on tasks requiring counter-evidence or semantic similarity detection?
- Basis in paper: [inferred] Table 1 and Section 5.4 note that on Climate-FEVER and ArguAna (counter-evidence tasks) and Quora (duplicate detection), all attention-based re-rankers fall below the retriever baseline, suggesting fundamental limitations.
- Why unresolved: The paper attributes this to prompt misalignment but does not investigate whether attention patterns themselves fail to capture these task-specific relevance signals.
- What evidence would resolve it: Analysis of attention distributions on counter-evidence datasets comparing CoRe heads vs. all heads, plus experiments with modified scoring formulations.

### Open Question 3
- Question: Is there a principled, model-agnostic method for selecting the contrastive temperature hyperparameter without dataset-specific grid search?
- Basis in paper: [inferred] Section 4 describes temperature as controlling distribution sharpness, and Section 5.1 notes temperatures are tuned via grid search (t=0.001 for Mistral/Granite, t=0.1 for Llama/Phi-4), but no systematic selection criterion is provided.
- Why unresolved: Different models require substantially different temperatures, and the paper offers only empirical tuning without theoretical justification.
- What evidence would resolve it: A theoretical analysis connecting temperature to attention head distribution properties, validated across multiple model architectures.

## Limitations

- The method's effectiveness depends on the assumption that discriminative attention patterns are sparse and stable across domains, which may not hold for all task types
- Layer-pruning strategy assumes CoRe head distribution patterns generalize to future models, but Granite-3.2 8B shows different distribution patterns
- Performance degradation on counter-evidence tasks and Quora suggests fundamental limitations in attention-based re-ranking for certain task types
- Hard negative mining requires careful similarity thresholds to avoid false negatives, adding implementation complexity

## Confidence

- **High Confidence**: The core mechanism of contrastive head scoring using InfoNCE formulation is well-established; the experimental validation on three diverse LLMs with consistent results across multiple benchmarks supports the primary claims about CoRe head effectiveness.
- **Medium Confidence**: The layer-pruning efficiency gains assume CoRe head distribution patterns generalize to future models; while shown for three models, architectural changes could invalidate this assumption.
- **Medium Confidence**: The method's robustness to different domains and tasks is supported but not comprehensively tested; performance degradation on counter-evidence tasks and Quora suggests limitations.

## Next Checks

1. **Cross-domain head stability**: Apply pre-computed CoRe heads from NQ to a domain-shifted dataset (e.g., FiQA or SciFact) and measure performance drop; compare against re-identifying heads on domain-specific data.
2. **Model architecture sensitivity**: Test layer-pruning strategy on a model with different depth/width ratios (e.g., 4-6B parameter model) to verify the 50% pruning threshold holds.
3. **Hard negative mining sensitivity**: Systematically vary the number of hard negatives (from 10 to 49) and similarity thresholds to quantify impact on CoRe head quality and final re-ranking performance.