---
ver: rpa2
title: 'From Structure to Detail: Hierarchical Distillation for Efficient Diffusion
  Model'
arxiv_id: '2511.08930'
source_url: https://arxiv.org/abs/2511.08930
tags:
- teacher
- distillation
- step
- student
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high inference latency in
  diffusion models by proposing a novel Hierarchical Distillation (HD) framework for
  single-step image generation. The core idea is to first use trajectory distillation
  (TD) to capture global structural information from a multi-step teacher, creating
  a strong structural "sketch," then refine it with distribution matching to restore
  high-frequency details.
---

# From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model

## Quick Facts
- **arXiv ID:** 2511.08930
- **Source URL:** https://arxiv.org/abs/2511.08930
- **Reference count:** 40
- **Primary result:** Proposes Hierarchical Distillation (HD) framework achieving state-of-the-art single-step image generation with FID 2.26 on ImageNet

## Executive Summary
This paper addresses the challenge of high inference latency in diffusion models by proposing a novel Hierarchical Distillation (HD) framework for single-step image generation. The core idea is to first use trajectory distillation (TD) to capture global structural information from a multi-step teacher, creating a strong structural "sketch," then refine it with distribution matching to restore high-frequency details. To further enhance detail refinement, the authors introduce an Adaptive Weighted Discriminator (AWD) that dynamically focuses on local imperfections. The method is evaluated on ImageNet 256×256 and MJHQ-30K text-to-image benchmarks, achieving state-of-the-art single-step performance.

## Method Summary
The Hierarchical Distillation framework operates in two sequential stages. First, MeanFlow-based trajectory distillation captures global structural information by training the student to approximate the teacher's mean velocity along the probability flow ODE trajectory. This creates a structurally sound initialization but inherently loses high-frequency details. Second, distribution matching distillation (DMD) refines this initialization by aligning the student's output distribution with real data using a combination of score matching and adversarial training. The Adaptive Weighted Discriminator enhances this stage by dynamically focusing gradient signals on local imperfections rather than uniformly across the image, enabling efficient detail recovery without mode collapse.

## Key Results
- Achieves FID of 2.26 on ImageNet 256×256 in a single step, matching the 250-step teacher's quality
- Demonstrates competitive text-to-image generation performance on MJHQ-30K benchmark
- Shows that TD-initialized distribution matching provides 1.4 FID improvement over training from scratch
- Validates that AWD discriminator significantly outperforms standard GAP discriminators for detail refinement

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Distillation as Structural Prior Injection
Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. The paper argues that trajectory distillation (TD) serves as a "lossy compressor" that provides a structurally sound initialization for subsequent refinement. TD methods train a student to approximate the mean velocity along the teacher's PF-ODE trajectory. By modeling this integral average, the student captures global structure but inherently discards high-frequency details due to finite model capacity compressing multi-step dynamics into a single step. The information bottleneck in TD is fundamental and cannot be overcome by simply increasing model capacity alone.

### Mechanism 2: Distribution Matching with Structured Initialization
Initializing distribution matching (DM) from a TD-pretrained model improves training stability and reduces mode collapse compared to training from scratch. The TD stage places the student's output distribution near the data manifold before DM begins. This creates significant initial overlap between generated and real distributions, shifting DM's task from "blind exploration" to "