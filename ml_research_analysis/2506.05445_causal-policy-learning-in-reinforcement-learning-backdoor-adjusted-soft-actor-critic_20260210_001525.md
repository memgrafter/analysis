---
ver: rpa2
title: 'Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic'
arxiv_id: '2506.05445'
source_url: https://arxiv.org/abs/2506.05445
tags:
- learning
- causal
- policy
- dosac
- confounders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses hidden confounders in reinforcement learning\
  \ that introduce spurious correlations between states and actions, biasing policy\
  \ learning. DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment) extends\
  \ SAC by explicitly estimating interventional policies \u03C0(a|do(s)) using backdoor\
  \ adjustment to correct for hidden confounding."
---

# Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic

## Quick Facts
- arXiv ID: 2506.05445
- Source URL: https://arxiv.org/abs/2506.05445
- Reference count: 11
- Primary result: DoSAC achieves returns up to 2361 on Humanoid versus 1079 for SAC in confounded settings

## Executive Summary
This paper addresses hidden confounders in reinforcement learning that introduce spurious correlations between states and actions, biasing policy learning. DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment) extends SAC by explicitly estimating interventional policies π(a|do(s)) using backdoor adjustment to correct for hidden confounding. A learnable Backdoor Reconstructor infers pseudo-past variables (previous state and action) from the current state to enable backdoor adjustment from observational data. The method integrates seamlessly into SAC training and generalizes it, naturally reducing to SAC when no confounders are present.

## Method Summary
DoSAC modifies SAC by incorporating causal inference techniques to address hidden confounding. The key innovation is the Backdoor Reconstructor, a neural network that infers the previous state and action (s_{t-1}, a_{t-1}) from the current state s_t. This allows the actor to condition on these pseudo-past variables to implement backdoor adjustment, learning an interventional policy π(a|do(s)) rather than the observational policy π(a|s). The method maximizes causal entropy H(a|do(s)) and uses these inferred variables during both policy evaluation and training. When confounders are absent, DoSAC naturally reduces to standard SAC.

## Key Results
- Outperforms SAC on Humanoid, Ant, Walker2d, and LunarLander benchmarks under confounded settings
- Achieves returns up to 2361 on Humanoid versus 1079 for SAC in clean evaluation
- Demonstrates improved robustness and generalization when confounders are present during training
- Shows better policy reliability compared to standard SAC

## Why This Works (Mechanism)

### Mechanism 1: Backdoor Adjustment via Pseudo-Past Conditioning
The method applies Pearl's backdoor criterion by conditioning the policy on previous state and action (s_{t-1}, a_{t-1}) to block spurious correlations induced by hidden confounders u_t. This isolates the causal effect of the state by learning an interventional policy π(a_t|do(s_t)) instead of the biased observational policy π(a_t|s_t). The core assumption is that the SCM correctly identifies (s_{t-1}, a_{t-1}) as a sufficient adjustment set to block all confounding paths.

### Mechanism 2: Latent Context Reconstruction
A neural network ("Backdoor Reconstructor") infers the necessary adjustment variables (pseudo-past) directly from the current state, eliminating the need for explicit historical data access during policy execution. This module effectively compresses historical context into the current state representation, allowing the actor to sample "pseudo-past" variables to satisfy the backdoor criterion on the fly. The assumption is that the current state s_t contains sufficient information to reliably reconstruct the previous state and action.

### Mechanism 3: Generalization via Causal Entropy Maximization
The method maximizes entropy over the interventional policy distribution π(a|do(s)) rather than the observational distribution π(a|s). By enforcing randomness specifically within the causal manifold of actions (stripped of spurious correlations), the policy becomes less brittle to the specific confounding biases present in the training data. This promotes a robust policy invariant to confounders that may shift or disappear.

## Foundational Learning

- **Concept: The do-Calculus and Intervention**
  - Why needed: The core distinction is between seeing (observational P(a|s)) and doing (interventional P(a|do(s))). Understanding this difference is crucial to why standard RL is biased and causal RL targets the latter.
  - Quick check: If I force the agent to take action a (intervention), does that change the probability of the preceding state compared to just observing action a?

- **Concept: Confounding and Spurious Correlation**
  - Why needed: The mechanism exists entirely to remove "backdoor paths." You must understand that a hidden variable (e.g., wind affecting both robot's tilt and its drift) can create a fake correlation between tilt and drift that breaks when the wind changes.
  - Quick check: Does the paper assume the confounder affects the current state, the action, or both simultaneously?

- **Concept: Soft Actor-Critic (SAC)**
  - Why needed: DoSAC is an extension of SAC. You need to understand the baseline actor-critic structure (Q-function, Value-function, entropy regularization) to see where the Backdoor Reconstructor plugs in.
  - Quick check: In standard SAC, what role does the entropy term play, and how does DoSAC modify the definition of this term?

## Architecture Onboarding

- **Component map:** Input s_t → Reconstructor infers s̃_{t-1}, ã_{t-1} → Concatenate inputs → Actor samples a_t
- **Critical path:** Input s_t → Reconstructor infers pseudo-past (s̃_{t-1}, ã_{t-1}) → Concatenate with current state → Actor outputs action
- **Design tradeoffs:** The Reconstructor adds inference overhead to every environment step; using "pseudo-past" inferred from s_t is an approximation that may be lossy in low-dimensional states
- **Failure signatures:** Reconstructor producing uninformative pseudo-past causes collapse to standard SAC; high-variance reconstruction in deterministic environments can destabilize the Actor
- **First 3 experiments:**
  1. Run DoSAC on a standard environment (no injected confounders) to verify performance matches or approximates standard SAC
  2. Compare DoSAC against an ablation using ground truth past states/actions for adjustment vs. learned pseudo-past to quantify reconstruction error
  3. Vary the strength of injected confounder noise and plot performance degradation of DoSAC vs. SAC to identify reconstruction failure threshold

## Open Questions the Paper Calls Out

1. **Extension to discrete action spaces:** The authors note that DoSAC primarily addresses continuous action spaces due to its extension from SAC, but anticipate extending the methodology to discrete domains. This remains unresolved as the current implementation relies on reparameterization trick and Gaussian policies.

2. **Performance in partially observable settings:** The paper lists as a limitation that confounding can be adequately mitigated through pseudo-past inference, which may prove insufficient in highly stochastic or partially observable settings where the current state lacks sufficient information to infer pseudo-past variables.

3. **Handling dynamic or non-stationary confounders:** The authors identify the current focus on stationary confounders and a fixed replay buffer as restricting applicability in dynamic or evolving environments where causal structures may shift.

4. **Sample efficiency in sparse-reward settings:** The conclusion proposes extending DoSAC to sparse-reward and hierarchical RL settings as a distinct future research direction, leaving unclear whether interventional policies assist with exploration in low-feedback regimes.

## Limitations
- Effectiveness depends on (s_{t-1}, a_{t-1}) being a sufficient adjustment set, which may not hold for long-range temporal confounders
- Backdoor Reconstructor performance is limited by information content of current state s_t
- Paper does not specify exact reconstruction loss function or training procedure for the Backdoor Reconstructor
- Assumes stationary causal graphs and confounding distributions

## Confidence

**High Confidence:** The mechanism of backdoor adjustment via conditioning on previous time step is well-established in causal inference literature and the paper's formulation follows standard approaches. The theoretical reduction to standard SAC when no confounders are present is mathematically sound.

**Medium Confidence:** The effectiveness of the Backdoor Reconstructor in practice depends on architectural choices and training stability that are not fully specified. Empirical results show strong performance improvements, but baseline SAC implementation details affect interpretation.

**Medium Confidence:** The causal entropy maximization mechanism has theoretical justification but the paper provides limited empirical evidence for its contribution beyond standard entropy maximization.

## Next Checks

1. **Reconstruction Quality Assessment:** Evaluate the Backdoor Reconstructor's accuracy on held-out transitions by computing MSE between reconstructed and true (s_{t-1}, a_{t-1}) pairs. Correlate reconstruction error with policy performance to quantify the impact of reconstruction quality on deconfounding effectiveness.

2. **Confounder Strength Sensitivity Analysis:** Systematically vary the injected confounder noise σ (e.g., 0.5, 1.0, 2.0) and measure DoSAC's performance degradation relative to SAC. Identify the threshold where reconstruction fails to compensate, revealing practical limits of the method.

3. **Sufficiency of Adjustment Set Verification:** Design environments with known confounding structures where (s_{t-1}, a_{t-1}) is not a sufficient adjustment set (e.g., confounders affecting states two steps back). Test whether DoSAC still improves over SAC or if performance degrades, validating the core assumption about backdoor sufficiency.