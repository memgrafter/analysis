---
ver: rpa2
title: 'Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning
  to Scalable Inference'
arxiv_id: '2506.18530'
source_url: https://arxiv.org/abs/2506.18530
tags:
- fpga
- kernel
- precision
- learning
- bcpnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first embedded FPGA accelerator for Brain-Like
  Neural Networks (BLNNs), specifically the Bayesian Confidence Propagation Neural
  Network (BCPNN), targeting low-power edge devices. The authors implement both online
  learning and inference-only kernels using High-Level Synthesis on a Zynq UltraScale+
  SoC, supporting variable and mixed precision.
---

# Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference

## Quick Facts
- arXiv ID: 2506.18530
- Source URL: https://arxiv.org/abs/2506.18530
- Authors: Muhammad Ihsan Al Hafiz; Naresh Ravichandran; Anders Lansner; Pawel Herman; Artur Podobas
- Reference count: 19
- First embedded FPGA accelerator for BCPNN with online learning and scalable inference

## Executive Summary
This paper presents the first FPGA implementation of Brain-Like Neural Networks (BLNNs), specifically the Bayesian Confidence Propagation Neural Network (BCPNN), targeting low-power edge devices. The authors implement both online learning and inference-only kernels using High-Level Synthesis on a Zynq UltraScale+ SoC, supporting variable and mixed precision. They evaluate their design on MNIST, Pneumonia, and Breast Cancer datasets, achieving up to 17.5× latency improvement and 94% energy savings compared to ARM baselines while maintaining accuracy. The work demonstrates that FP16 precision provides optimal performance for edge deployment, with inference-only kernels being significantly more efficient than full learning kernels.

## Method Summary
The authors implement BCPNN using HLS C++ kernels on ZCU104 with XCZU7EV SoC. The architecture uses a three-layer structure (input→hidden→output) with local Hebbian learning via synaptic traces. Two kernel variants are developed: a full online-learning kernel with parallelization factor 4, and an inference-only kernel with factor 8-16. The design employs AXI-Stream FIFOs for dataflow between subkernels, supporting FP32, FP16, and mixed precision. Training uses sparse connectivity with parameters nact and nsil, while inference leverages streaming data transfer via 256-bit AXI4 bursts. Power measurements use INA226, and performance is compared against ARM Cortex-A53 baseline.

## Key Results
The evaluation shows FP16 precision provides optimal accuracy-performance tradeoff across datasets, with MNIST achieving 97.5% accuracy and 17.5× speedup over ARM baseline. The inference-only kernel demonstrates superior energy efficiency (94% savings) compared to the online learning kernel due to reduced computation. Sparse connectivity optimization with nact=1000 and nsil=100 maintains accuracy while reducing resource utilization. Power consumption measurements confirm the accelerator's suitability for edge deployment, with INA226 validation showing consistent energy savings across all test scenarios.

## Why This Works (Mechanism)
The BCPNN implementation leverages local Hebbian learning through synaptic trace accumulation, enabling efficient online adaptation without backpropagation. The use of mixed precision arithmetic allows the system to maintain neural network accuracy while reducing computational complexity. The three-layer architecture with configurable hidden units provides flexibility for different dataset requirements. The streaming data pipeline via AXI4 bursts minimizes memory bottlenecks, while the sparse connectivity pattern reduces unnecessary computations during both training and inference phases.

## Foundational Learning
The work builds upon the Bayesian Confidence Propagation Neural Network model, which uses probabilistic inference for learning and classification. The local Hebbian learning rule enables synapse-specific adaptation based on presynaptic and postsynaptic activity, making it suitable for embedded implementations where global weight updates are computationally expensive. This approach aligns with brain-inspired computing principles, using probabilistic instead of deterministic activation functions.

## Architecture Onboarding
The Zynq UltraScale+ SoC platform provides integrated ARM processing with FPGA fabric, enabling heterogeneous computing. The HLS C++ implementation allows rapid development while maintaining performance through parallelization factors. The AXI-Stream interface enables efficient data flow between processing elements. The use of mixed precision arithmetic demonstrates how numerical representation impacts both accuracy and resource utilization in embedded neural network accelerators.

## Open Questions the Paper Calls Out
The paper does not explicitly discuss open questions or future research directions. Given the novel nature of this implementation, potential areas for investigation include scaling to larger network architectures, exploring different BCPNN variants, and investigating the impact of various precision configurations on more complex datasets.

## Limitations
The implementation is constrained by the ZCU104 board's resource limitations, potentially limiting scalability to larger networks. The evaluation focuses on relatively small datasets (MNIST, Pneumonia, Breast Cancer), which may not fully represent the challenges of real-world applications. The sparse connectivity parameters (nact and nsil) require manual tuning for optimal performance. Additionally, the comparison is limited to ARM Cortex-A53 baseline without exploring other edge computing alternatives.

## Confidence
High confidence in the reported results, as the paper provides comprehensive evaluation metrics including accuracy, latency, and energy consumption across multiple datasets and precision configurations. The use of standard evaluation datasets and clear comparison methodology supports the validity of the findings.

## Next Checks
Verify the impact of different parallelization factors on resource utilization and performance scaling. Investigate the robustness of FP16 precision across additional dataset types and network architectures. Examine the trade-offs between different sparse connectivity configurations and their impact on learning convergence. Evaluate the system's performance under varying data stream rates and batch sizes.