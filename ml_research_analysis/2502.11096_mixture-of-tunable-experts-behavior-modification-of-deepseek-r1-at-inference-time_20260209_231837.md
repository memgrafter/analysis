---
ver: rpa2
title: Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at Inference
  Time
arxiv_id: '2502.11096'
source_url: https://arxiv.org/abs/2502.11096
tags:
- experts
- expert
- behavior
- figure
- refused
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Tunable-Experts (MoTE), a method
  that modifies the behavior of DeepSeek-R1 at inference time without additional training.
  By analyzing expert activations using a technique called functional Token Resonance
  Imaging (fTRI), the authors identified expert subgroups responsible for specific
  behaviors like refusal responses.
---

# Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at Inference Time

## Quick Facts
- arXiv ID: 2502.11096
- Source URL: https://arxiv.org/abs/2502.11096
- Authors: Robert Dahlke; Henrik Klagges; Dan Zecha; Benjamin Merkel; Sven Rohr; Fabian Klemm
- Reference count: 22
- Primary result: 52% reduction in refusals on sensitive prompts by deactivating 10 specific experts (0.07% of total) without MT-Bench degradation

## Executive Summary
This paper introduces Mixture-of-Tunable-Experts (MoTE), a method that modifies the behavior of DeepSeek-R1 at inference time without additional training. By analyzing expert activations using a technique called functional Token Resonance Imaging (fTRI), the authors identified expert subgroups responsible for specific behaviors like refusal responses. Deactivating the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts) achieved a 52% reduction in refusals on sensitive prompts without degrading performance on MT-Bench. Random expert deactivation had smaller effects with more noise, while forced expert activation increased refusals. The approach also successfully changed the model's reasoning language from English to Chinese in 10% of test prompts.

## Method Summary
The authors developed fTRI to identify behavior-relevant experts by computing differential activation patterns between behavior classes. They constructed templated prompts ("What happened {time}{place}?") to elicit specific behaviors, aggregated expert activations across tokens, and identified distinctive experts through class-specific differentials. For inference-time modification, they implemented router overrides that either suppress or stimulate target experts by modifying router weights and renormalizing top-k selections. The method was validated on refusal behavior reduction and reasoning language modification.

## Key Results
- 52% reduction in refusals on sensitive prompts by deactivating top 10 refusal-relevant experts
- No performance degradation on MT-Bench benchmark after expert suppression
- 10.75% success rate in switching reasoning language from English to Chinese through forced expert activation
- Random expert suppression produced only 9% effect with higher variance, demonstrating specificity of fTRI-identified experts

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Localization in Expert Subgroups
DeepSeek's MoE architecture enables fine-grained behavioral specialization where specific functions correlate strongly with particular expert activation patterns, allowing targeted intervention in small expert subsets.

### Mechanism 2: fTRI for Differential Expert Identification
By subtracting averaged expert activation patterns between behavior classes, fTRI isolates experts that "resonate" most with target behaviors, enabling identification of causally relevant expert subgroups.

### Mechanism 3: Router Override for Inference-Time Behavior Steering
Intercepting and modifying MoE router outputs at inference time changes model behavior without weight updates through expert suppression (zeroing weights with renormalization) or stimulation (force-insertion with boosted weight).

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Understanding shared vs. routed experts and top-k selection is essential for knowing what can be intercepted and modified. Quick check: In DeepSeek-R1, what fraction of routed experts are active per token per layer? (Answer: 8/256 = 3.125%)

- **Activation Aggregation Across Tokens**: fTRI relies on summing expert activations over all prompt tokens before first output token; understanding this distinguishes prompt-level from token-level analysis. Quick check: Why aggregate over input tokens rather than output tokens for identifying refusal-related experts? (Answer: Refusal decision is made before generation begins; input-token activations encode the behavioral trigger.)

- **Top-k Renormalization**: When suppressing an expert, its contribution must be removed without breaking expected activation magnitude. Quick check: If you suppress 1 of 8 selected experts without renormalizing, what happens? (Answer: Downstream layers receive reduced activation magnitude, potentially degrading output quality.)

## Architecture Onboarding

- **Component map**: MoE backbone (58 layers × 1 shared + 256 routed experts; top-k=8) -> Router (gating network producing activation scores) -> fTRI module (token-wise activation logging → prompt aggregation → class averaging → differential scoring) -> MoTE intervention hook (router output interception → expert mask application → weight renormalization)

- **Critical path**: Deploy inference engine with router output access → Construct behavior-eliciting prompt dataset → Record expert activations → Aggregate to prompt-level maps → Classify responses → Compute per-class average activations → Identify distinctive experts via fTRI differential → Implement router override → Validate behavior shift + check benchmark degradation

- **Design tradeoffs**: Expert count to tune (more experts → stronger effect but higher side-effect risk; paper uses 10) vs. prompt template specificity (highly templated → cleaner signal but generalization risk); suppression vs. stimulation (suppression is simpler and shows stronger effects)

- **Failure signatures**: Random expert suppression produces small, noisy effects (~9% shift vs. ~40% with distinctive experts); behavior change fails to generalize beyond template prompts (overfitting to fTRI dataset); benchmark scores drop (experts involved in multiple functions)

- **First 3 experiments**: 1) Reproduce refusal reduction using paper's published expert IDs on sensitive-topics dataset; target ~50% reduction. 2) Run control with 10 random experts suppressed; expect <15% shift with higher variance. 3) Test cross-domain generalization: Apply refusal experts to qualitatively different prompt format and measure effect retention.

## Open Questions the Paper Calls Out

- **Cross-Architecture Generalization**: Does fTRI/MoTE generalize to other MoE architectures beyond DeepSeek-R1, particularly those with fewer experts per layer? The study only examines DeepSeek-R1 with 256 routed experts per layer.

- **Behavioral Sensitivity Variation**: Why does behavioral change magnitude vary significantly between behaviors (52% refusal reduction vs. 10.75% language switching)? The paper notes this difference but doesn't investigate underlying causes.

- **Robustness Across Prompt Formulations**: How robust is distinctive expert identification across different prompt formulations and datasets? The paper validates on one additional dataset but doesn't test whether different templates identify the same experts.

## Limitations

- Method's generalizability beyond tested domains remains uncertain; more complex behaviors might require larger expert sets or prove impossible to modify
- Causal relationship between identified experts and behaviors is inferred rather than directly established
- Long-term stability of behavior modifications is not evaluated; potential adaptation mechanisms remain unexplored

## Confidence

- **Expert Behavioral Localization**: High - stark contrast between targeted (52%) and random (9%) expert suppression effects
- **fTRI Methodology Validity**: Medium - conceptually sound but sensitivity to prompt quality and false positives requires further validation  
- **Inference-Time Modification Efficacy**: High - clear before/after measurements and controlled experiments demonstrate router override works

## Next Checks

1. **Cross-Behavioral Generalization Test**: Apply refusal-related experts to qualitatively different prompt format (e.g., conversational prompts) and measure whether 52% refusal reduction generalizes beyond template prompts.

2. **Multi-Behavior Interference Analysis**: Systematically test whether suppressing refusal-related experts causes unintended changes in other capabilities by running comprehensive benchmark suite (beyond MT-Bench) including mathematical reasoning, code generation, and creative writing tasks.

3. **Expert Count Sensitivity Study**: Vary number of suppressed experts from 1 to 50 in increments and measure relationship between expert count and behavior modification strength to determine optimal threshold.