---
ver: rpa2
title: Online Hierarchical Policy Learning using Physics Priors for Robot Navigation
  in Unknown Environments
arxiv_id: '2510.01519'
source_url: https://arxiv.org/abs/2510.01519
tags:
- planning
- environments
- path
- navigation
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robot navigation in large,
  complex, and unknown indoor environments. Traditional methods like sampling-based
  planners and imitation learning-based approaches struggle with scalability, resolution
  control, and reliance on demonstration data.
---

# Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments

## Quick Facts
- **arXiv ID**: 2510.01519
- **Source URL**: https://arxiv.org/abs/2510.01519
- **Reference count**: 38
- **Primary result**: Modular-NTFields (mNTFields) achieves up to 100% success rate and 0.08s planning time in environments up to 1883 m², outperforming baselines like ANTFields and RRTConnect.

## Executive Summary
This paper introduces Modular-NTFields (mNTFields), a hierarchical framework for online robot navigation in large, unknown indoor environments. By combining high-level graph planning with low-level neural fields that solve the Eikonal PDE locally, the method overcomes scalability issues common in traditional sampling-based planners and imitation learning approaches. The framework leverages physics priors, modular network decomposition, and temporal difference learning to achieve efficient, collision-aware path planning without requiring demonstration data.

## Method Summary
mNTFields operates as a two-level hierarchical planner: a high-level graph captures global connectivity through room segmentation, while multiple local subnetworks solve the Eikonal PDE for obstacle-aware navigation. The method uses morphological erosion to identify rooms and entry points, assigning each confirmed room to a dedicated neural network. These subnetworks are trained online using physics-informed losses (Eikonal, temporal difference, and normal alignment) and are queried via gradient descent. The modular structure ensures scalability and mitigates spectral bias by constraining each network to a bounded spatial region.

## Key Results
- Achieves 100% success rate in environments with 6-42 rooms
- Planning times as low as 0.08 seconds for a 1883 m² environment
- Outperforms baselines (ANTFields, TDM, RRTConnect, LazyPRM) in both success rate and path quality
- Real-world deployment on a quadruped robot demonstrates effectiveness in complex indoor spaces

## Why This Works (Mechanism)

### Mechanism 1: Spatial Decomposition Mitigates Spectral Bias
Neural networks exhibit spectral bias, prioritizing low-frequency components during training. By partitioning large environments into smaller regions (normalized to [-0.5, 0.5]), each subnetwork captures high-frequency geometric details that would otherwise be lost. This enables accurate representation of narrow passages and doorways.

### Mechanism 2: Eikonal PDE Constraint Enforces Geometric Consistency
The Eikonal equation (||∇T|| = 1/S) relates gradient magnitude to local speed, creating smooth, collision-aware cost-to-go fields. Near obstacles, S(q) decreases, forcing steeper gradients in T and naturally producing collision-free paths through gradient descent.

### Mechanism 3: Modular Subnetworks Prevent Catastrophic Forgetting
Separate neural networks for spatially disjoint rooms prevent gradient updates in newly explored regions from degrading performance in previously learned areas. Each confirmed room is assigned to exactly one subnetwork, isolating training and ensuring stable performance across the environment.

## Foundational Learning

- **Eikonal Equation and First-Order PDEs**
  - Why needed here: The entire low-level planner is structured around solving this PDE; understanding the relationship between arrival time gradients and optimal paths is essential for debugging loss landscapes.
  - Quick check question: Given a speed field S(x) that equals 0.5 near obstacles and 1.0 in free space, sketch the expected isocontours of T around a convex obstacle.

- **Spectral Bias in Neural Networks**
  - Why needed here: The primary motivation for modularization; you must recognize when a network is failing to learn high-frequency details vs. when it's failing to converge at all.
  - Quick check question: If a single MLP trained on a 10-room environment produces smooth paths that cut through walls, is this more likely a spectral bias issue or an Eikonal loss weighting issue?

- **Temporal Difference Learning (from RL)**
  - Why needed here: The TD loss term (L_TD) is borrowed from RL value function learning; understanding bootstrapped value propagation helps debug convergence.
  - Quick check question: Why does the TD loss (Eq. 4) use a Taylor expansion rather than directly penalizing Bellman equation residuals?

## Architecture Onboarding

- **Component map:**
  - Depth + odometry → Occupancy Map Builder → Room Segmenter → Graph Constructor → Subnetwork Pool → Sample Generator → Training Loop → Path Planner
  - Path Planner queries Subnetwork Pool → Gradient Descent → Collision-free Path

- **Critical path:**
  1. Sensor data → occupancy map update (every frame)
  2. Room segmentation triggers when new free space connects to unexplored boundaries
  3. New confirmed room → subnetwork assignment decision (join existing or instantiate new)
  4. Sample generation + network training (only for subnetworks containing unconfirmed regions)
  5. Graph edge weight update from subnetwork predictions
  6. On path query: identify start/goal rooms → Dijkstra → concatenate local gradient descent trajectories

- **Design tradeoffs:**
  - **Bounding box size threshold**: Larger boxes reduce network count but risk spectral bias; smaller boxes increase overhead
  - **Normal alignment weight (λ_N)**: Too high over-constrains near obstacles; too low allows gradient misalignment
  - **Causality weighting aggressiveness**: Prioritizing low-arrival-time regions speeds convergence but may neglect distant goal configurations

- **Failure signatures:**
  - **Segmentation instability**: Rooms flicker between confirmed/unconfirmed → networks constantly reinitialize
  - **TD loss divergence**: T values grow unbounded → check δt scaling and learning rate
  - **Path jitter**: Gradient descent oscillates → normal alignment loss may be fighting Eikonal loss
  - **Doorway failures**: Robot gets stuck at thresholds → entry point detection threshold too conservative

- **First 3 experiments:**
  1. **Room segmentation sanity check**: Run segmentation on a known floor plan; verify that morphological erosion correctly seals doorways and that confirmed rooms stabilize after full exploration.
  2. **Single-room Eikonal convergence**: Train one subnetwork on a 2-room environment with a narrow passage; plot Eikonal loss + TD loss separately to verify both decrease and that gradient norms match 1/S.
  3. **Forgetting test**: Train sequentially on 5 rooms, then query paths in room 1; compare path quality with and without modular isolation (force all rooms into one network as ablation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the mNTFields framework be adapted to handle dynamic environments where obstacles move or change shape after the initial exploration and mapping phase?
- Basis in paper: [explicit] The authors state, "We also aim to investigate the applicability of our method in dynamic environments."
- Why unresolved: The current method relies on physics-informed neural networks solving the Eikonal equation based on static occupancy maps confirmed during exploration; it does not currently possess a mechanism to update the cost-to-go function or graph connectivity in real-time for moving obstacles.
- What evidence would resolve it: A demonstration of the method successfully re-planning or updating local subnetworks in real-time within a simulation containing moving obstacles, along with latency metrics for these updates.

### Open Question 2
- Question: Does the computational efficiency and hierarchical structure of mNTFields scale effectively to high-dimensional configuration spaces, such as those required for robotic arm manipulation?
- Basis in paper: [explicit] The authors propose to "assess its performance in higher-dimensional planning tasks."
- Why unresolved: The current experiments are limited to 2D (x, y) navigation for ground robots; solving the Eikonal equation and performing room segmentation becomes significantly more complex and computationally expensive in higher dimensions (e.g., 6-DOF).
- What evidence would resolve it: Successful application of the framework to a manipulator arm in a cluttered 3D environment, showing that planning times remain lower than sampling-based methods like RRT*.

### Open Question 3
- Question: To what extent can hardware acceleration reduce the online training latency of the modular subnetworks during the exploration phase?
- Basis in paper: [explicit] The authors note, "we plan to explore hardware-accelerated tools to support the parallel training of local planners."
- Why unresolved: While the inference is fast (0.07-0.15s), the paper notes that training a subnetwork takes approximately 18 seconds on average, which could bottleneck rapid exploration if not parallelized effectively.
- What evidence would resolve it: Implementation of the training pipeline on parallel hardware (e.g., multi-GPU setups) showing a reduction in the "Frame Training Time" (FT) and "Mapping Time" (MT) metrics reported in Table I.

## Limitations
- **Hyperparameter sensitivity**: The modular decomposition threshold and loss weights are not fully specified, making performance tuning non-trivial
- **Generalization to dynamic environments**: While tested in static unknown spaces, the method's performance in dynamic settings with moving obstacles remains unproven
- **Scalability ceiling**: Though claimed to scale to 42+ rooms, the upper bound where modularization breaks down is unclear

## Confidence
- **High confidence**: Hierarchical decomposition framework and core Eikonal PDE constraint mechanism (supported by extensive physics-based navigation literature)
- **Medium confidence**: Spectral bias mitigation claims (mechanism is sound but quantitative impact depends heavily on implementation details)
- **Medium confidence**: Anti-forgetting benefits of modular networks (logical but lacks direct ablation studies in this work)

## Next Checks
1. **Spectral bias ablation study**: Compare performance of single vs. modular networks on environments with narrow passages (e.g., maze-like structures) to quantify the benefit of decomposition
2. **Doorway segmentation stress test**: Systematically evaluate segmentation performance across varying doorway widths (0.5m to 1.5m) to identify failure thresholds
3. **Real-world deployment variability**: Test on diverse real building architectures (open offices, apartments, industrial spaces) to assess robustness to layout variations