---
ver: rpa2
title: Generative Sequential Notification Optimization via Multi-Objective Decision
  Transformers
arxiv_id: '2509.02458'
source_url: https://arxiv.org/abs/2509.02458
tags:
- notification
- learning
- arxiv
- decision
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Decision Transformer (DT) based framework
  for optimizing notification delivery by reframing policy learning as return-conditioned
  supervised learning. The approach uses quantile regression to model return-to-go
  distributions and employs a circular buffer for efficient sequence feature persistence
  in production.
---

# Generative Sequential Notification Optimization via Multi-Objective Decision Transformers

## Quick Facts
- **arXiv ID**: 2509.02458
- **Source URL**: https://arxiv.org/abs/2509.02458
- **Reference count**: 39
- **Primary result**: DT-based approach achieved +0.72% increase in user sessions while reducing notification volume and maintaining notification relevance at LinkedIn

## Executive Summary
This paper introduces a Decision Transformer (DT) based framework for optimizing notification delivery by reframing policy learning as return-conditioned supervised learning. The approach uses quantile regression to model return-to-go distributions and employs a circular buffer for efficient sequence feature persistence in production. Compared to a multi-objective CQL-based agent, the DT-based approach achieved a +0.72% increase in user sessions while reducing notification volume and maintaining notification relevance at LinkedIn. The framework demonstrates improved robustness, scalability, and modeling flexibility for real-world notification decision-making.

## Method Summary
The framework frames offline reinforcement learning as return-conditioned supervised learning using Decision Transformers. It processes trajectories of user states, actions, and multi-objective rewards to predict both actions and return-to-go (RTG) quantiles. The model employs a GPT-like Transformer architecture with separate heads for action prediction and RTG quantile regression. Training uses a combined loss function incorporating cross-entropy for actions and pinball loss for RTG quantiles. Inference involves predicting RTG quantiles, interpolating for a target percentile, and using this as a prompt for action selection. The approach handles multi-objective rewards (predicted CTR, actual visits, volume penalty) and uses ε-greedy sampling from 2% user traffic for data collection.

## Key Results
- Achieved +0.72% increase in user sessions compared to baseline
- Reduced notification volume while maintaining relevance
- Demonstrated improved robustness and scalability over CQL-based approaches

## Why This Works (Mechanism)
The framework reframes sequential decision-making as supervised learning conditioned on return-to-go, enabling direct optimization of long-term objectives. Quantile regression for RTG prediction provides robustness to reward distribution shifts and enables flexible control over performance targets during inference. The return-conditioned formulation allows the model to implicitly learn value functions without explicit value estimation, reducing model complexity and improving sample efficiency.

## Foundational Learning
- **Return-to-Go (RTG) Quantile Regression**: Why needed - provides distributional information about future rewards rather than point estimates. Quick check - verify predicted quantiles align with empirical reward distributions.
- **Decision Transformers**: Why needed - reframes RL as supervised learning for better sample efficiency. Quick check - confirm action predictions improve with return conditioning.
- **Multi-Objective Reward Design**: Why needed - balances engagement, volume, and relevance metrics. Quick check - ensure reward components are properly normalized and contribute to overall objective.
- **Transformer-based Sequence Modeling**: Why needed - captures long-range dependencies in user behavior. Quick check - validate attention patterns focus on relevant historical context.
- **Offline RL with ε-greedy Data**: Why needed - leverages existing user interaction data without exploration. Quick check - analyze coverage of action space in training data.
- **Pinball Loss Function**: Why needed - appropriate loss for quantile regression that handles asymmetric penalties. Quick check - verify loss gradients flow correctly during training.

## Architecture Onboarding

**Component Map**: User History -> State Encoder -> Transformer -> Action Head + RTG Head -> Quantile Prediction -> Action Selection

**Critical Path**: The sequence from state encoding through Transformer layers to the combined action and RTG prediction heads represents the core computational path. The circular buffer mechanism enables efficient sequence persistence in production.

**Design Tradeoffs**: The choice of quantile regression over point estimate RTG prediction trades model simplicity for distributional robustness. Using a 4-step context balances computational efficiency against historical information capture. The multiplicative interaction for eligible action sets adds modeling flexibility but increases implementation complexity.

**Failure Signatures**: RTG distribution mismatch occurs when inference RTG targets fall outside training distribution. High inference latency manifests when context length exceeds budget constraints. Multi-objective reward conflicts arise when optimizing one metric degrades others.

**First Experiments**:
1. Train on synthetic sequential data to verify RTG quantile predictions match empirical distributions
2. Test action prediction accuracy with varying return conditioning targets
3. Benchmark inference latency with 4-step context under production-like constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on LinkedIn's internal A/B testing framework without public reproducibility
- Multi-objective reward design and exact feature engineering remain underspecified
- Production deployment details (circular buffer implementation) lack architectural specifications

## Confidence
- Methodological contribution of quantile-based RTG prediction: **High confidence**
- Claims of superiority over CQL-based approaches: **Medium confidence** (lack quantitative failure mode comparison)
- Production deployment robustness: **Low confidence** (insufficient implementation details)

## Next Checks
1. **RTG Distribution Validation**: Compare predicted RTG quantiles against ground truth distributions in a held-out validation set to verify the quantile regression head is properly calibrated.
2. **Inference Latency Benchmark**: Measure end-to-end decision latency with 4-step context to confirm it meets mobile notification constraints (target <100ms).
3. **Reward Ablation Study**: Test whether performance changes significantly when using only single-objective rewards versus the proposed multi-objective formulation.