---
ver: rpa2
title: 'PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain
  Planning'
arxiv_id: '2504.07383'
source_url: https://arxiv.org/abs/2504.07383
tags:
- learning
- propel
- variables
- primal
- prop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROPEL integrates supervised and deep reinforcement learning to
  solve large-scale supply chain planning problems formulated as Mixed Integer Linear
  Programs (MIPs) with millions of variables. The supervised component identifies
  integer variables fixed at zero in optimal solutions using reduced costs, while
  the DRL component selectively relaxes fixed variables when optimality gaps exceed
  target thresholds.
---

# PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning

## Quick Facts
- **arXiv ID**: 2504.07383
- **Source URL**: https://arxiv.org/abs/2504.07383
- **Reference count**: 16
- **Primary result**: PROPEL integrates supervised and deep reinforcement learning to solve large-scale supply chain planning MIPs with millions of variables, achieving 60% reduction in primal integral and 88% primal gap reduction.

## Executive Summary
PROPEL addresses the scalability challenge in large-scale supply chain planning by combining supervised learning (SL) and deep reinforcement learning (DRL). The SL component identifies integer variables likely fixed at zero in optimal solutions using reduced costs from LP relaxations, while the DRL component selectively relaxes these variables when optimality gaps exceed thresholds. This hybrid approach achieves significant computational improvements over state-of-the-art MIP solvers on industrial-scale instances, reducing primal integrals by 60% and primal gaps by 88% while maintaining solution quality.

## Method Summary
PROPEL uses a two-phase approach: First, an SL classifier (MLP) predicts which integer variables are zero at optimality based on features extracted from a bipartite graph of variable-constraint interactions, with reduced costs from LP relaxations serving as a physics-informed regularizer. Second, a DRL agent treats the set of fixed variables as a state and learns to selectively "unfix" subsets (grouped by time horizon) to improve solution quality when optimality gaps exceed targets. The framework was trained on 600 perturbed demand instances and evaluated on 60 test instances using Gurobi as the base solver.

## Key Results
- 60% reduction in primal integral compared to state-of-the-art MIP solvers
- 88% reduction in primal gap on industrial supply chain instances
- Up to 13.57x improvement in solution time and 15.92x reduction in primal gap across test instances
- Consistent performance across 60 test instances with millions of variables

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Restricting predictions to "fixed-at-zero" variables reduces the MIP search space significantly while maintaining feasibility better than predicting non-zero values.
- **Mechanism**: In large-scale supply chain MIPs, a high percentage of integer variables (representing production or flow) are zero at optimality (sparsity). A supervised learning model classifies variables as zero/non-zero. By fixing only the predicted zero variables, the problem dimension reduces. Crucially, this avoids the difficulty of satisfying balance constraints (e.g., flow conservation) which often break when variables are fixed to non-zero values incorrectly.
- **Core assumption**: The underlying optimization problem has a sparse solution structure where many variables are naturally zero.
- **Evidence anchors**: [abstract] "...identify the variables that are fixed to zero in the optimal solution... PROPEL avoids generating infeasible solutions that may be hard to repair."
- **Break condition**: If the specific MIP instance is dense (most variables > 0), the supervised model will fail to prune enough variables, rendering the overhead useless.

### Mechanism 2
- **Claim**: Reduced costs from the LP relaxation act as a physics-informed regularizer for the machine learning classifier.
- **Mechanism**: The paper calculates the reduced cost $rc_i$ (marginal cost of increasing a variable from 0) from the linear relaxation. This cost is normalized and added to the classifier's probability output before thresholding. If a variable has a high reduced cost, it implies it is "expensive" to include, reinforcing the ML model's decision to fix it at zero.
- **Core assumption**: The LP relaxation provides a meaningful signal about the integer solution's structure.
- **Evidence anchors**: [section 5] "...PROPEL refines this approximation by using the reduced costs in the linear relaxation... reduced cost provides valuable information regarding whether a variable is likely to be strictly positive."
- **Break condition**: If the LP relaxation is very weak (optimal LP solution is far from the MIP solution), reduced costs may mislead the classifier.

### Mechanism 3
- **Claim**: Deep Reinforcement Learning (DRL) recovers optimality by learning to selectively "unfix" variables groups that were wrongly eliminated by the supervised step.
- **Mechanism**: The Supervised Learning (SL) step is conservative (high precision) to ensure feasibility, but may fix too many variables, worsening the optimality gap. The DRL agent treats the set of fixed variables as a state. It takes actions to "reinsert" (unfix) subsets of variables (grouped by time horizon) to explore the boundary of feasibility/optimality trade-offs.
- **Core assumption**: The errors made by the SL step are systematic and can be corrected by a policy that learns the relationship between variable subsets and solution quality improvement.
- **Evidence anchors**: [abstract] "...DRL component selectively relaxes fixed variables when optimality gaps exceed target thresholds."
- **Break condition**: If the SL step prunes critical variables required for *any* feasible solution (and the formulation lacks slack variables), the DRL agent will have no valid recovery path.

## Foundational Learning

- **Concept**: Reduced Costs in Linear Programming
  - **Why needed here**: This is the hybrid signal used to correct the ML classifier. You cannot understand Equation (6) or the "PROP vs PROP_b" ablation without knowing what a reduced cost means (the shadow price of a non-basic variable).
  - **Quick check question**: If a variable has a reduced cost of 0 in the LP relaxation, what does that imply about its potential presence in an optimal MIP solution?

- **Concept**: Primal Integral and Primal Gap
  - **Why needed here**: The paper claims success based on "60% reduction in primal integral" rather than just runtime. This metric evaluates the *quality* of solutions found *over time*, not just the final result.
  - **Quick check question**: Why is a smaller primal integral considered better, even if the final runtime is the same?

- **Concept**: Slack Variables and Feasibility Assurance
  - **Why needed here**: The paper argues that fixing variables to zero is safe *because* the underlying model uses "unmet demand" variables ($u_t^i$) as slack. Understanding this constraint qualification is necessary to know if PROPEL applies to your specific MIP.
  - **Quick check question**: In the SCP model (Eq 12-17), which variable ensures the problem remains feasible even if all production variables ($z_t^j$) are incorrectly fixed to 0?

## Architecture Onboarding

- **Component map**: Data Generator -> Feature Extractor -> SL Classifier (PROP) -> Reducer -> DRL Agent (ENLARGE) -> Solver

- **Critical path**: The **Feature Extraction** based on the bipartite graph (Figure 4). If you input raw demand data without respecting the temporal "no path from future to past" logic, the classifier learns noise.

- **Design tradeoffs**:
  - **Threshold $\tau$**: High threshold = more variables fixed = faster solving but higher risk of bad solutions. Low threshold = safer but slower.
  - **Partition Size (m)**: In DRL, variables are grouped. Large groups = coarse control (risk of unfixing too much). Small groups = huge action space (hard to learn).

- **Failure signatures**:
  - **High Infeasibility**: The SL model predicts "Non-Zero" for too many variables or the specific MIP formulation lacks the slack variables ($u_t^i$) required for the "Feasibility Assurance" logic.
  - **Stagnant Gap**: The DRL agent fails to converge; typically happens if the reward signal (objective improvement) is too sparse or the episode time limit is too short.

- **First 3 experiments**:
  1. **Baseline Sanity Check**: Run the raw MIP solver (OPT) vs. the Reduced MIP (PROP) on 5 instances. Verify that PROP finds solutions significantly faster.
  2. **Ablation on Reduced Costs**: Train the classifier *without* the reduced cost adjustment (Eq 6) and compare the "False Positive" rate (variables wrongly fixed to 0) against the full model.
  3. **DRL Scaling**: Run the ENLARGE phase on instances where PROP fails to meet the optimality gap. Plot the "Primal Gap vs. Iteration" (similar to Fig 10) to verify the agent is actually learning to fix the SL errors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the PROPEL framework be effectively generalized to other combinatorial optimization problems with high-dimensional non-binary MIPs outside of supply chain planning?
- **Basis in paper**: [explicit] The authors state in the conclusion that "Future research will be geared towards looking for applications beyond SCP problems..."
- **Why unresolved**: The current study validates the method exclusively on industrial supply chain instances, leaving its efficacy on problems with different constraint structures (e.g., graph problems) unproven.
- **What evidence would resolve it**: Successful application and benchmarking of PROPEL on diverse MIP datasets (e.g., scheduling or routing) with comparable variable reduction and speedup metrics.

### Open Question 2
- **Question**: How can the computational cost of generating training data be mitigated when scaling PROPEL to problem sizes significantly larger than the millions of variables currently tested?
- **Basis in paper**: [explicit] The conclusion notes that "scaling to even larger problems would need to address the computational cost of solving the optimization problems for the training instances."
- **Why unresolved**: The supervised learning phase requires solving numerous MIPs to generate training labels, which becomes a bottleneck as problem size increases.
- **What evidence would resolve it**: The development of sample-efficient training algorithms or transfer learning techniques that reduce the number of full MIP solutions required to train the predictors.

### Open Question 3
- **Question**: How would the integration of supervised and reinforcement learning differ if applied to heuristic solution techniques like Large Neighborhood Search (LNS) rather than standard MIP solvers?
- **Basis in paper**: [explicit] The conclusion suggests, "It would be interesting to study how PROPEL would apply to other solution techniques, such as large neighborhood search."
- **Why unresolved**: The current architecture relies on fixing variables within a standard branch-and-bound framework; its interaction with the iterative destruction and repair cycles of LNS is unknown.
- **What evidence would resolve it**: A modified PROPEL architecture designed for LNS, demonstrating convergence behavior and solution quality compared to the current solver-based approach.

## Limitations
- The method's effectiveness depends on the underlying problem having sparse solutions with many naturally zero variables
- The LP relaxation must provide meaningful reduced cost signals for the classifier to work effectively
- The framework is validated only on supply chain planning instances, limiting generalization claims to other MIP domains

## Confidence

- **High Confidence**: The 60% primal integral reduction and 88% primal gap reduction on the tested industrial supply chain instances. The ablation study (PROP vs PROP_b) provides strong evidence that reduced costs meaningfully improve classifier performance.
- **Medium Confidence**: The claim that PROPEL "effectively handles non-binary MIPs at industrial scale" - while demonstrated on the specific SCP formulation, the generalization to arbitrary non-binary MIPs with different constraint structures is unproven.
- **Low Confidence**: The assertion that the framework addresses "feasibility and scalability challenges not covered by existing methods" - the paper does not provide systematic comparisons against state-of-the-art ML-based MIP presolvers that also target sparsity.

## Next Checks

1. Test PROPEL on MIPLIB instances with varying sparsity levels (dense vs sparse) to validate the core assumption about solution structure.
2. Implement the reduced cost ablation (PROP_b) on at least 5 diverse MIP classes to measure the consistency of the physics-informed regularizer benefit.
3. Evaluate PROPEL's performance when the LP relaxation is deliberately weakened (e.g., by adding constraints) to assess robustness to reduced cost signal quality.