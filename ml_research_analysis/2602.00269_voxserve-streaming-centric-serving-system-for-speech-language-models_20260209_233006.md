---
ver: rpa2
title: 'VoxServe: Streaming-Centric Serving System for Speech Language Models'
arxiv_id: '2602.00269'
source_url: https://arxiv.org/abs/2602.00269
tags:
- serving
- streaming
- audio
- inference
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VoxServe, a unified serving system designed
  to efficiently deploy modern Speech Language Models (SpeechLMs) for streaming applications.
  SpeechLMs combine an LLM backbone with audio-specific modules like detokenizers,
  creating multi-stage inference pipelines with diverse architectures and performance
  requirements.
---

# VoxServe: Streaming-Centric Serving System for Speech Language Models

## Quick Facts
- arXiv ID: 2602.00269
- Source URL: https://arxiv.org/abs/2602.00269
- Reference count: 25
- Delivers 10-20× higher throughput than existing implementations at comparable latency while maintaining high streaming viability

## Executive Summary
VoxServe is a unified serving system designed to efficiently deploy modern Speech Language Models (SpeechLMs) for streaming applications. SpeechLMs combine large language model backbones with audio-specific modules like detokenizers, creating complex multi-stage inference pipelines. Existing serving systems struggle to support this architectural diversity while optimizing for streaming-specific metrics like Time-To-First-Audio (TTFA) and streaming viability. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, enabling support for diverse SpeechLM architectures within a single framework.

The system implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across seven modern SpeechLMs demonstrate that VoxServe achieves 10-20× higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The model-execution abstraction provides flexibility for future SpeechLM architectures while the streaming optimizations ensure real-time performance for voice-based applications.

## Method Summary
VoxServe addresses the challenge of serving diverse SpeechLM architectures through a unified serving system. The key innovation is a model-execution abstraction that decouples the model architecture from system-level optimizations, allowing the same serving infrastructure to handle different SpeechLM designs. The system implements streaming-aware scheduling that prioritizes minimizing TTFA and maintaining streaming viability, along with an asynchronous inference pipeline that overlaps computation across pipeline stages. The evaluation covers seven modern SpeechLMs to demonstrate the system's effectiveness across diverse architectures.

## Key Results
- Achieves 10-20× higher throughput compared to existing implementations at comparable latency
- Maintains high streaming viability across all evaluated SpeechLM architectures
- Successfully supports seven diverse modern SpeechLM architectures within a unified serving framework
- Streaming-aware scheduling and asynchronous pipeline improve end-to-end efficiency

## Why This Works (Mechanism)
VoxServe works by introducing a model-execution abstraction layer that separates architectural concerns from system optimizations. This allows the serving system to optimize for streaming-specific metrics (TTFA, streaming viability) without being tightly coupled to specific model architectures. The streaming-aware scheduling prioritizes early audio generation, while the asynchronous inference pipeline overlaps computation across different pipeline stages to maximize throughput. This decoupling enables the system to adapt to diverse SpeechLM architectures while maintaining consistent streaming performance.

## Foundational Learning
- SpeechLM architecture diversity: Different models use varying combinations of LLM backbones and audio-specific modules; understanding this diversity is crucial for building flexible serving systems
- Streaming metrics (TTFA, streaming viability): Time-To-First-Audio and continuous streaming capability are critical for real-time voice applications; systems must optimize for these beyond traditional latency/throughput
- Model-execution abstraction: Decoupling model architecture from system optimizations enables serving diverse models with consistent performance characteristics
- Asynchronous inference pipelines: Overlapping computation across pipeline stages can significantly improve throughput for multi-stage inference processes
- Unified serving frameworks: Single-serving systems that support multiple model architectures reduce operational complexity and resource overhead

## Architecture Onboarding

Component Map:
SpeechLM Model -> Model Execution Layer -> Streaming-Aware Scheduler -> Asynchronous Inference Pipeline -> Output Layer

Critical Path:
Audio input → SpeechLM backbone inference → Detokenizer processing → Audio synthesis → Output stream

Design Tradeoffs:
- Flexibility vs. specialization: The abstraction layer enables supporting diverse architectures but may add overhead compared to specialized implementations
- Streaming optimization vs. batch efficiency: Prioritizing TTFA and streaming viability may reduce peak throughput for batch processing scenarios
- System complexity vs. performance gains: The unified approach adds architectural complexity but delivers significant performance improvements

Failure Signatures:
- High TTFA with low overall latency indicates scheduling issues prioritizing initial audio generation
- Streaming viability drops suggest problems with asynchronous pipeline coordination or resource contention
- Throughput degradation with increased model diversity points to abstraction layer bottlenecks

First Experiments:
1. Measure TTFA and streaming viability for a single SpeechLM under varying load conditions
2. Compare throughput and latency between VoxServe and existing serving implementations using identical hardware
3. Test model migration between different SpeechLM architectures to validate abstraction layer flexibility

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims rely heavily on synthetic benchmarks and controlled environments
- Real-world deployment testing with diverse audio inputs and variable network conditions would strengthen validity claims
- Model-execution abstraction may face practical limitations when scaling to production workloads with multiple concurrent model types and heterogeneous hardware configurations

## Confidence

Major Claims Confidence:
- Performance improvements (10-20× throughput): High
- Streaming viability maintenance: Medium
- Model-execution abstraction flexibility: Medium
- Asynchronous inference pipeline effectiveness: High

## Next Checks
1. Deploy VoxServe in a production environment with concurrent requests across different SpeechLM architectures to measure real-world performance and resource utilization
2. Test streaming viability under variable network conditions and with diverse audio inputs (noisy environments, different accents, multiple speakers)
3. Evaluate the system's ability to handle model updates and version migrations without service interruption in a multi-tenant deployment scenario