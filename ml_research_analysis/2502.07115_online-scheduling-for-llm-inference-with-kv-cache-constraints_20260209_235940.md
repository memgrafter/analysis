---
ver: rpa2
title: Online Scheduling for LLM Inference with KV Cache Constraints
arxiv_id: '2502.07115'
source_url: https://arxiv.org/abs/2502.07115
tags:
- memory
- requests
- scheduling
- inference
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of minimizing end-to-end latency
  in large language model inference by jointly batching and scheduling incoming prompts
  while managing the KV cache memory constraints. The authors introduce a hindsight
  optimal benchmark via integer programming and prove that no deterministic online
  algorithm can achieve a constant competitive ratio under arbitrary arrivals.
---

# Online Scheduling for LLM Inference with KV Cache Constraints
arXiv ID: 2502.07115
Source URL: https://arxiv.org/abs/2502.07115
Reference count: 40
Key outcome: This paper tackles the problem of minimizing end-to-end latency in large language model inference by jointly batching and scheduling incoming prompts while managing the KV cache memory constraints. The authors introduce a hindsight optimal benchmark via integer programming and prove that no deterministic online algorithm can achieve a constant competitive ratio under arbitrary arrivals. They propose a polynomial-time Memory Constrained Shortest First (MC-SF) algorithm that prioritizes partially completed requests and selects new ones to maximize batch size under memory constraints. Theoretical analysis shows that under conditions of identical prompt sizes, simultaneous arrivals, and bounded prediction errors on output lengths, MC-SF achieves a constant competitive ratio. Empirical evaluations on both synthetic data and a real-world conversation dataset show that MC-SF matches the hindsight optimal closely (average latency ratio ~1.005 in the no-information-gap case) and significantly outperforms benchmark scheduling policies in latency and memory efficiency, supporting more sustainable and cost-effective LLM deployment.

## Executive Summary
This paper addresses the challenge of minimizing end-to-end latency in large language model (LLM) inference by developing an online scheduling algorithm that batches incoming requests while respecting KV cache memory constraints. The authors establish a hindsight optimal benchmark using integer programming and prove that no deterministic online algorithm can achieve a constant competitive ratio under arbitrary arrivals. They propose the Memory Constrained Shortest First (MC-SF) algorithm, which prioritizes partially completed requests and selects new ones to maximize batch size under memory constraints. The theoretical analysis shows that under conditions of identical prompt sizes, simultaneous arrivals, and bounded prediction errors on output lengths, MC-SF achieves a constant competitive ratio. Empirical evaluations on synthetic data and real-world conversation traces demonstrate that MC-SF closely matches the hindsight optimal (average latency ratio ~1.005) and significantly outperforms benchmark scheduling policies.

## Method Summary
The paper tackles online batching and scheduling for LLM inference with the objective of minimizing total end-to-end latency under KV cache memory constraints. The authors introduce a hindsight optimal benchmark using integer programming and prove that no deterministic online algorithm can achieve a constant competitive ratio under arbitrary arrivals. They propose the MC-SF algorithm, which at each scheduling round prioritizes partially completed requests and selects new ones to maximize batch size while ensuring memory feasibility at all predicted completion times. The algorithm uses predicted output lengths (with 80% accuracy according to cited work) to make batching decisions. Theoretical analysis establishes constant competitive ratio guarantees under conditions of identical prompt sizes, simultaneous arrivals, and bounded prediction errors. Empirical evaluation uses both synthetic data (with prompt sizes 1-5, output lengths 1 to M-s_i, and varying arrival patterns) and real-world LMSYS-Chat-1M conversation data, comparing MC-SF against baseline policies using the Vidur LLM inference simulator.

## Key Results
- MC-SF achieves average latency ratio of 1.005-1.009 compared to hindsight optimal under no-information-gap conditions
- Under information gap conditions (20% prediction error), MC-SF achieves average latency ratio of 1.53-1.65 compared to hindsight optimal
- MC-SF significantly outperforms baseline scheduling policies (MC-Benchmark, α-protection, and Eager) in both latency and memory efficiency across all tested scenarios
- The algorithm demonstrates strong performance on real-world conversation data from LMSYS-Chat-1M, showing consistent improvements over benchmarks

## Why This Works (Mechanism)
The paper establishes a rigorous theoretical foundation for LLM inference scheduling by introducing a hindsight optimal benchmark via integer programming and proving fundamental limits on online algorithms. The MC-SF algorithm works by prioritizing partially completed requests (which would otherwise need to be restarted) and greedily selecting new requests to maximize batch size while ensuring memory feasibility at all future completion times. This approach leverages the key insight that KV cache memory must be reserved not just for the current batch but for all requests that will complete during the processing of the current batch. The theoretical analysis shows that under conditions of identical prompt sizes, simultaneous arrivals, and bounded prediction errors, this greedy heuristic achieves a constant competitive ratio, meaning it performs within a constant factor of the optimal offline solution.

## Foundational Learning
- **KV cache memory management**: Essential for understanding how attention computations are cached during autoregressive generation; quick check: verify that each request in a batch requires memory proportional to its prompt length plus output tokens generated so far
- **Online algorithms and competitive ratio**: Framework for evaluating algorithms that must make decisions without complete future knowledge; quick check: confirm that competitive ratio compares online algorithm performance to optimal offline solution
- **Integer programming formulations**: Used to establish hindsight optimal benchmark; quick check: verify that binary variables represent request-batch assignments and constraints enforce memory and batching rules
- **Arrival process modeling**: Understanding how requests arrive (simultaneous vs. Poisson) affects scheduling decisions; quick check: distinguish between all-at-once and Poisson arrival patterns in synthetic data generation
- **Output length prediction**: Critical component for MC-SF algorithm to estimate memory requirements; quick check: verify that predictions provide upper bounds with 80% accuracy as cited from Zheng et al. [2024]
- **Competitive analysis under different arrival models**: Shows how theoretical guarantees vary with arrival assumptions; quick check: understand why constant competitive ratio is impossible under adversarial arrivals but achievable under simultaneous arrivals

## Architecture Onboarding
- **Component map**: User requests → Scheduler (MC-SF) → LLM inference engine → KV cache memory manager → Output delivery
- **Critical path**: Request arrival → Batch formation decision → KV cache allocation check → Model inference → Token generation → Output delivery
- **Design tradeoffs**: Batching efficiency vs. latency (larger batches improve throughput but increase individual request latency); memory utilization vs. risk of overflow (aggressive batching improves utilization but risks memory exhaustion)
- **Failure signatures**: Memory overflow when predictions underestimate true output lengths; starvation when high-memory requests block batch formation; suboptimal batching when prediction errors are large
- **First experiments**: 1) Implement synthetic data generation with varying M, prompt sizes, and arrival patterns; 2) Implement MC-SF scheduler with memory feasibility checks; 3) Compare MC-SF against baseline policies on synthetic data with different prediction accuracy levels

## Open Questions the Paper Calls Out
**Open Question 1**: Can prediction mechanisms for output lengths be jointly designed with batching-scheduling policies to provide both informative upper and lower bounds, enabling tighter memory control and more aggressive batching?
- Basis in paper: The authors state: "An interesting future direction is to jointly design prediction mechanisms for output lengths and batching–scheduling policies that operate simultaneously during inference. Ideally, such predictors would provide not only upper bounds on o_i, but also informative lower bounds."
- Why unresolved: Current predictors provide only upper bounds; lower bounds would require new prediction architectures and theoretical analysis of their impact on competitive ratios.
- What evidence would resolve it: A prediction algorithm that outputs confidence intervals (both upper and lower bounds) with provable accuracy, integrated with an extended MC-SF analysis showing improved competitive ratios.

**Open Question 2**: How can the single-worker scheduling framework be extended to multiple heterogeneous workers with varying memory capacities and compute throughput?
- Basis in paper: "Another important direction is to extend our framework to settings with multiple computational workers operating in parallel... workers may be heterogeneous in terms of memory capacity or compute throughput, requiring 'matching' decisions between requests and workers."
- Why unresolved: Multi-worker settings introduce load balancing, request-worker matching, and distributed KV-cache management challenges absent in the single-GPU model.
- What evidence would resolve it: A theoretical analysis extending MC-SF to multi-worker settings with competitive ratio guarantees, or empirical validation on multi-GPU clusters.

**Open Question 3**: Can scheduling algorithms achieve strong theoretical guarantees under hybrid stochastic-adversarial arrival models where most requests follow an unknown distribution but a small fraction are extreme outliers?
- Basis in paper: "An important direction is to study other arrival models... where most requests are generated by an unknown stochastic process but a small fraction of requests are extreme outliers... Such heavy-tailed or mixed workloads are commonly observed in real LLM inference systems."
- Why unresolved: Current analysis assumes either adversarial (with weak guarantees) or structured arrivals; real workloads exhibit heavy-tailed distributions not captured by either model.
- What evidence would resolve it: Competitive ratio bounds for MC-SF or new algorithms under mixed arrival models, validated on trace data with empirically measured outlier fractions.

## Limitations
- The theoretical constant competitive ratio assumes identical prompt sizes and simultaneous arrivals, conditions rarely met in practice
- The paper relies on external work for 80% accurate output length prediction without providing implementation details or validating this assumption
- Real-world deployment would require handling heterogeneous prompt sizes, staggered arrivals, and prediction errors beyond the 20% margin studied

## Confidence
- High confidence: The theoretical framework (IP formulation, impossibility result, MC-SF algorithm design) and synthetic experiment results are well-supported
- Medium confidence: Real-world evaluation results depend on specific simulator configurations and the accuracy of output length predictions
- Medium confidence: The claimed "sustainability" benefits follow logically from latency/memory improvements but aren't directly measured

## Next Checks
1. Implement and validate the output length prediction model to verify the 80% accuracy claim, testing with different prediction methods
2. Test MC-SF under more realistic conditions with heterogeneous prompt sizes and staggered Poisson arrivals beyond the simultaneous arrival theoretical analysis
3. Measure actual memory utilization and energy consumption in a deployment setting to quantify the sustainability claims beyond latency metrics