---
ver: rpa2
title: Interpreting vision transformers via residual replacement model
arxiv_id: '2509.17401'
source_url: https://arxiv.org/abs/2509.17401
tags:
- features
- arxiv
- feature
- circuit
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive, large-scale analysis
  of vision transformer (ViT) features across all layers and multiple model variants,
  revealing how ViTs represent visual information. The authors introduce the residual
  replacement model, which replaces ViT computations with interpretable sparse autoencoder
  (SAE) features in the residual stream, enabling scalable, faithful, and parsimonious
  circuit discovery.
---

# Interpreting vision transformers via residual replacement model

## Quick Facts
- arXiv ID: 2509.17401
- Source URL: https://arxiv.org/abs/2509.17401
- Authors: Jinyeong Kim; Junhyeok Kim; Yumin Shim; Joohyeok Kim; Sunyoung Jung; Seong Jae Hwang
- Reference count: 40
- Primary result: First comprehensive large-scale analysis of ViT features across all layers, revealing interpretable visual representations and introducing residual replacement model for scalable circuit discovery

## Executive Summary
This paper presents the first comprehensive analysis of vision transformer (ViT) features across all layers and multiple model variants, revealing how ViTs represent visual information. The authors introduce the residual replacement model, which replaces ViT computations with interpretable sparse autoencoder (SAE) features in the residual stream, enabling scalable, faithful, and parsimonious circuit discovery. Through manual annotation of 6.6K features, they show that ViT features are broadly interpretable, evolving from low-level patterns (color, texture) to high-level semantics (objects, parts). The residual replacement model improves circuit faithfulness by up to 1.6x and demonstrates utility in debiasing spurious correlations, such as reducing the model's reliance on graffiti features for freight car classification.

## Method Summary
The residual replacement model decomposes ViT residual stream representations using per-layer sparse autoencoders (TopK SAEs) to extract interpretable features, then constructs directed acyclic graphs of feature interactions using attribution patching with gradient correction (LibraGrad). The method bypasses attention module complexity by tracking residual stream evolution, enabling scalable circuit discovery. Features are manually annotated and circuits are discovered through edge-based selection focusing on important connections to downstream nodes. The framework enables feature-level debiasing by ablating spurious features identified in circuits.

## Key Results
- Manual annotation of 6.6K features shows ViT features are broadly interpretable across all layers, evolving from low-level (color, texture) to high-level (objects, parts)
- Residual replacement model improves circuit faithfulness by up to 1.6x compared to naïve approaches (94.1% faithfulness for ViT)
- Debiasing spurious correlations: ablating graffiti feature improves freight car classification mAUC from 0.854 to 0.904 with minimal accuracy loss

## Why This Works (Mechanism)

### Mechanism 1: Sparse Feature Decomposition via TopK SAEs
- Claim: Decomposing polysemantic ViT residual stream representations into sparse monosemantic features improves interpretability across all layers.
- Mechanism: TopK SAE maps representations x ∈ R^d to sparse features z ∈ R^f via z = TopK(W_enc(x - b_pre)), enforcing sparsity by selecting only top-k activations. This promotes disentanglement of mixed semantic concepts.
- Core assumption: Visual features in ViTs can be meaningfully decomposed into approximately monosemantic units that humans can annotate.
- Evidence anchors:
  - [abstract]: "extracted via sparse autoencoders... manual annotation of 6.6K features, they show that ViT features are broadly interpretable"
  - [Section 2.1]: Details TopK formulation with reconstruction loss and auxiliary loss for dead feature recovery
  - [corpus]: Weak direct validation; related papers focus on ViT applications, not interpretability decomposition
- Break condition: If FVU (fraction of variance unexplained) exceeds ~0.15, reconstruction degrades and feature quality drops (Section C.1).

### Mechanism 2: Residual Stream Circuit Discovery via Attribution Patching
- Claim: Focusing on residual stream (bypassing attention module complexity) and aggregating token interactions yields human-scale, faithful circuits.
- Mechanism: Construct directed acyclic graph G where nodes = SAE features, edges = cross-layer connections. Edge importance estimated via attribution patching: I(u→d) = ∇_d m · ∇_u d(u - u'). Jacobian-vector product trick reduces O(T×f) to O(f) computation.
- Core assumption: Key computational pathways can be captured by tracking residual stream evolution without explicit attention head analysis.
- Evidence anchors:
  - [Section 3.1]: "tracking interactions between features along the residual stream... bypassing the complexity of attention modules"
  - [Table 1]: Feature circuits with gradient correction achieve 94.1% faithfulness (ViT), 1.6× improvement over naïve
  - [corpus]: No direct corpus validation of residual-stream-focused interpretability
- Break condition: If gradient noise dominates (ViTs have noisy gradients), LibraGrad correction is required—vanilla gradients yield poor faithfulness.

### Mechanism 3: Spurious Feature Ablation for Debiasing
- Claim: Identified spurious features in circuits can be ablated (replaced with median activation) to reduce shortcut reliance without retraining.
- Mechanism: After circuit discovery, manually identify spurious node (e.g., graffiti feature for freight car class). Ablate by replacing activation with dataset median. This removes spurious contribution to downstream nodes.
- Core assumption: Spurious features generalize across images within a class and can be reliably identified by humans.
- Evidence anchors:
  - [Section 4]: "ablating a single feature from a single image is close to that of SpuFix" (Table 2: mAUC improves from 0.854 to 0.904)
  - [Figure 9]: Shows graffiti feature L9#2371 contributing to freight car prediction
  - [corpus]: Weak; no corpus papers validate feature-level debiasing
- Break condition: If spurious features are entangled with causal features, ablation may harm accuracy. Paper shows minimal accuracy drop (0.849 → 0.848).

## Foundational Learning

- **Concept: Residual Stream in Transformers**
  - Why needed here: RRM explicitly targets residual stream as the "communication channel" where all modules read/write. Understanding this is prerequisite to grasping why bypassing attention is valid.
  - Quick check question: Can you explain why information flows directly through residual connections across layers, independent of attention heads?

- **Concept: Sparse Dictionary Learning / SAEs**
  - Why needed here: Core tool for decomposing polysemantic representations. Without this, you won't understand how 6.6K features were extracted.
  - Quick check question: What constraint does TopK impose that promotes monosemanticity, and how does reconstruction loss trade off against sparsity?

- **Concept: Attribution Patching / Causal Intervention**
  - Why needed here: RRM uses attribution patching (gradient-based approximation of causal effects) to weight edges. Understanding the approximation is critical for debugging faithfulness.
  - Quick check question: How does I(u→d) = ∇_d m · ∇_u d(u - u') estimate the causal effect of feature u on output mediated through d?

## Architecture Onboarding

- **Component map:**
  Input -> ViT backbone -> Per-layer SAE -> Circuit Builder -> Feature circuits
  Image → ViT-B/16, DINOv2, CLIP variants → TopK SAEs → Attribution patching + LibraGrad → Sparse feature DAG

- **Critical path:**
  1. Train SAEs per layer (Section C.1): ~12-14 hours per model on RTX 3090
  2. Annotate features: Manual visualization + categorization (Section 2.2)
  3. Run attribution patching per image: Seconds per image with Jacobian-vector trick (Section 3.1)
  4. Circuit discovery: Recursive edge-based selection from output layer backward

- **Design tradeoffs:**
  - **Token aggregation vs. token-level circuits:** Paper averages across tokens for human-scale interpretability; sacrifices token-to-token interaction detail (Section 3.1).
  - **Edge-based vs. node-based discovery:** Edge-based (to selected downstream nodes) yields higher causality than node-based (aggregate importance) (Section 3.1).
  - **SAE expansion rate R = f/d:** Higher R improves reconstruction but increases circuit complexity. Paper uses FVU < 0.15 threshold (Section C.2).

- **Failure signatures:**
  - Low interpretability scores in middle layers (Figure 2a): Features represent abstract/relational concepts harder to label
  - Noisy gradients in ViTs: Without LibraGrad correction, faithfulness drops significantly (Table 1 comparison)
  - Positional feature confusion: Early layers show clear position detectors; deeper layers have diffuse patterns due to attention mixing (Section 2.3)

- **First 3 experiments:**
  1. **Validate SAE reconstruction quality:** Train TopK SAE on a single layer; verify FVU < 0.15. Visualize top-activating patches for high-activation features.
  2. **Reproduce a simple circuit:** Take "Granny Smith" example (Figure 5); trace green → round → granny smith feature progression. Verify edge importance correlates with cosine similarity (Section E.4).
  3. **Spurious feature ablation test:** Identify a known spurious correlation class (e.g., freight car + graffiti); construct circuit, ablate spurious node, measure mAUC change vs. original model.

## Open Questions the Paper Calls Out

- **Can the residual replacement model be extended to explicitly interpret the computational mechanisms of attention and feed-forward modules rather than treating them as opaque contributors to the residual stream?**
  - Basis in paper: [explicit] The authors note the framework "only indirectly captures the roles of attention and feed-forward modules via gradient-based methods, and thus does not explicitly explain their functions."
  - Why unresolved: While gradients estimate feature importance, they do not reveal the specific algorithmic operations (e.g., how attention heads route information) performed by these modules.
  - Evidence: A mechanistic mapping of specific attention heads or MLP sub-layers to interpretable feature transformations within the circuit.

- **How can the framework incorporate critical token-to-token interactions (e.g., between [CLS] and patch tokens) without succumbing to the scalability issues caused by ViTs' large sequence lengths?**
  - Basis in paper: [explicit] The authors currently average activations to reduce dimensionality, but state, "We leave the investigation of such interactions [e.g., how the [CLS] token interacts with other tokens] to future work."
  - Why unresolved: Aggregating tokens assumes redundancy and discards fine-grained spatial relationships that may be essential for complex visual reasoning.
  - Evidence: A scalable extension of the model that preserves and interprets specific inter-token attention edges while maintaining the parsimony of the circuit.

- **Can the released dataset of 6.6K annotated features serve as ground truth to train automated interpretability methods that exceed the limitations of current CLIP-based approaches?**
  - Basis in paper: [explicit] The authors identify "leverage our feature annotation data to advance and evaluate automatic interpretability methods" as a promising direction, noting automated methods struggle with subtle concepts.
  - Why unresolved: Manual annotation is subjective and unscalable, while existing automated tools fail to capture concepts outside their pre-defined vocabularies.
  - Evidence: An automated interpretation model trained on the dataset that achieves high fidelity to human annotations on unseen features or architectures.

## Limitations

- Weak external validation of feature interpretability: Relies on internal consistency and human annotation rather than independent benchmarks or cross-annotator agreement metrics
- Residual stream focus may miss attention-based computations: Bypassing attention module complexity could miss important visual processing pathways that only manifest through attention effects
- Limited generalization of spurious feature debiasing: Demonstrated effectiveness on a single class-spurious feature pair (freight car + graffiti) without systematic evaluation across diverse spurious correlations

## Confidence

- **High Confidence**: The SAE-based feature decomposition mechanism (Mechanism 1) and residual replacement model architecture are technically sound and well-specified. The reconstruction quality (FVU < 0.15) and feature interpretability across layers are empirically demonstrated with clear metrics.

- **Medium Confidence**: The residual stream circuit discovery via attribution patching (Mechanism 2) shows strong quantitative results (94.1% faithfulness, 1.6× improvement), but relies on gradient-based approximations that may miss non-linear effects. The LibraGrad correction addresses noisy gradients but the full scope of approximation errors is unclear.

- **Low Confidence**: The spurious feature ablation for debiasing (Mechanism 3) demonstrates effectiveness in a narrow case but lacks systematic evaluation across diverse spurious correlations. The claim that identified spurious features "generalize across images within a class" is supported by a single example without statistical validation.

## Next Checks

1. **Cross-annotator agreement test**: Have multiple independent annotators label a random sample of 500 SAE features (stratified across layers). Calculate Cohen's kappa or Krippendorff's alpha to quantify the reliability of "human interpretability" claims and establish statistical confidence in the 6.6K annotated features.

2. **Attention module ablation study**: For the same circuits where residual replacement model achieves high faithfulness, selectively ablate or modify attention heads and measure faithfulness degradation. This would test whether the residual stream focus truly captures all important computational pathways or misses attention-specific processing.

3. **Systematic spurious correlation evaluation**: Identify 10-15 known spurious correlations across ImageNet classes (beyond the freight car + graffiti example). For each, construct circuits, ablate spurious features, and measure mAUC, accuracy, and calibration improvements. This would establish whether the debiasing approach generalizes beyond the single demonstrated case.