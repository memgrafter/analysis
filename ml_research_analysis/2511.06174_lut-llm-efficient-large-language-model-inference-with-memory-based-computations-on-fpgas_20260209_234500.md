---
ver: rpa2
title: 'LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations
  on FPGAs'
arxiv_id: '2511.06174'
source_url: https://arxiv.org/abs/2511.06174
tags:
- lut-llm
- quantization
- memory
- lookup
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LUT-LLM, the first FPGA accelerator for 1B+
  language model inference using memory-based computation via vector quantization.
  Instead of traditional arithmetic operations, LUT-LLM employs lookup tables storing
  pre-computed dot products, enabled by quantization of both weights and activations.
---

# LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs

## Quick Facts
- arXiv ID: 2511.06174
- Source URL: https://arxiv.org/abs/2511.06174
- Reference count: 40
- First FPGA accelerator for 1B+ LLM inference using memory-based computation via vector quantization

## Executive Summary
LUT-LLM introduces the first FPGA accelerator for 1B+ language model inference using memory-based computation through vector quantization. By co-quantizing both activations and weights and pre-computing dot products into 2D lookup tables, the design eliminates online matrix multiplication. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66× lower latency than AMD MI210 GPU and 1.72× higher energy efficiency than NVIDIA A100 GPU, with potential for 2.16× efficiency gains at 32B scale.

## Method Summary
LUT-LLM uses activation-weight co-quantization where both activations and weights are vector-quantized into centroids, and dot products between all centroid pairs are pre-computed and stored in 2D lookup tables. The design includes a bandwidth-aware parallel centroid search (BPCSU) that hides search latency behind memory access, 2D table lookup with prefix-sum accumulation, and a spatial-temporal hybrid execution strategy. The LUTLinear engine processes linear projections sequentially while attention, SwiGLU, and LayerNorm receive streamed outputs via dataflow. Training involves a two-stage LUT-DLA strategy with reconstruction loss ratio 0.1 on FineWeb and WikiQA datasets.

## Key Results
- 1.66× lower end-to-end latency than AMD MI210 GPU for 1.7B model inference
- 1.72× higher energy efficiency than NVIDIA A100 GPU
- Scales to 32B models with 2.16× efficiency gain over A100
- Achieves 92% HBM bandwidth utilization in decode stage

## Why This Works (Mechanism)

### Mechanism 1
Activation-weight co-quantization with 2D lookup tables achieves higher throughput than weight-only or activation-only vector quantization. Both activations and weights are vector-quantized into centroids; dot products between all centroid pairs are pre-computed and stored in 2D lookup tables. At inference, finding the nearest activation centroid index enables retrieval of pre-computed partial results, eliminating online matrix multiplication.

### Mechanism 2
Bandwidth-aware parallel centroid search hides search latency behind memory access time. Distance PEs are arranged into multiple parallel pipeline chains with pipeline depth selected to satisfy: lookup table load time ≥ centroid search latency + reduction tree depth, ensuring complete overlap.

### Mechanism 3
Spatial-temporal hybrid execution reduces on-chip buffer pressure by 14% while maintaining pipelined centroid search. LUTLinear engine processes all linear projections sequentially (reusing codebooks without reload), while attention, SwiGLU, and LayerNorm receive streamed outputs via dataflow.

## Foundational Learning

- **Vector Quantization (VQ)**
  - Why needed here: LUT-LLM depends on understanding how groups of values are clustered into centroids and indexed, differing fundamentally from scalar quantization.
  - Quick check question: Given vectors [(1,2), (3,4), (5,6)] with centroids [(2,3), (4,5)], which centroid index would (3,4) map to using Chebyshev distance?

- **FPGA Memory Hierarchy (BRAM, URAM, LUTRAM)**
  - Why needed here: 2D lookup tables require mixed instantiation across memory types; understanding port constraints and capacity trade-offs is essential for design space exploration.
  - Quick check question: If a 2D LUT buffer needs 512 entries with 4-way parallel access, which memory type minimizes BRAM consumption while meeting timing?

- **Transformer Inference Stages (Prefill vs Decode)**
  - Why needed here: LUT-LLM optimizes differently for each stage—prefill benefits from sequence parallelism, while decode is memory-bandwidth-bound.
  - Quick check question: Why does decode latency scale with KV-cache size but prefill latency scales with input sequence length?

## Architecture Onboarding

- **Component map:**
  - LUTLinear Engine: BPCSU (parallel centroid search) → 2D LUT PSum Engine (table lookup + SIMD accumulation) → Accumulator → Dequantizer
  - Dataflow Attention Engine: GEMM/GEMV units for QK^T and softmax(QK^T)V, with rotary embedding and softmax inline
  - Special Function Units: SwiGLU (gated linear unit) and LayerNorm
  - Global On-Chip Buffer: Distributes inputs/outputs across modules; 32 HBM channels for lookup tables, 4 for I/O, 4 for KV-cache

- **Critical path:**
  Decode stage: HBM read (lookup tables + weight indices) → BPCSU centroid search → 2D LUT row extraction → Value copy via multiplexers → SIMD accumulation → Dequantize to FP32 → Attention computation

- **Design tradeoffs:**
  - Higher c_a (activation centroids) improves accuracy but increases lookup table size (O(c_a × c_w)) and centroid search latency
  - Larger quantization group G reduces table size but may degrade quantization fidelity
  - More parallel BPCSU chains reduce search latency but consume more CLB resources and increase routing congestion
  - INT8 vs INT4 lookup tables: INT4 halves bandwidth but requires additional accuracy validation

- **Failure signatures:**
  - Timing violations during place-and-route: Often caused by excessive fanout from LUT row registers to multiplexers (solution: register duplication)
  - Bandwidth underutilization during decode: Indicates centroid search latency exceeds table load time (solution: increase parallel chains or reduce pipeline depth)
  - Accuracy degradation beyond 3%: Check codebook training convergence or increase reconstruction loss ratio

- **First 3 experiments:**
  1. Single linear layer validation: Implement one BPCSU + 2D LUT PSum pair with synthetic vectors; verify centroid indices match reference and accumulated outputs are within INT8 quantization error tolerance.
  2. Bandwidth-latency overlap test: Vary pipeline depth l and measure whether centroid search completes within table load cycles; confirm ≥90% bandwidth utilization in decode-only mode.
  3. End-to-end block test: Integrate LUTLinear + attention + SwiGLU for one transformer block; compare prefill/decode latency against performance model predictions to validate spatial-temporal hybrid scheduling.

## Open Questions the Paper Calls Out

### Open Question 1
Can the training algorithm for activation vector quantization be optimized to support context windows beyond 512 tokens without prohibitive GPU memory and time requirements? The current training approach requires an eight-GPU cluster running for 2 weeks for a 1.7B model, making longer contexts impractical without algorithmic improvements.

### Open Question 2
What energy efficiency and latency improvements would low-bit attention mechanisms (e.g., 1-2 bit attention) provide when integrated with LUT-LLM's memory-based linear layers? Current attention operations still require FP32 precision for accuracy, creating a mismatch with the INT8 memory-based linear layers.

### Open Question 3
How can the accelerator generation process be automated to eliminate manual optimization when adapting LUT-LLM to different model architectures and quantization schemes? Currently, adapting to new models requires manual loop bound adjustments and hardware optimization, limiting broader adoption.

### Open Question 4
What scaling behavior and efficiency gains would emerge from multi-FPGA systems with HBM bandwidths approaching GPU levels (2+ TB/s)? Single-FPGA bandwidth limitations constrain throughput; the theoretical 3.19× gain remains unverified on actual multi-FPGA hardware.

## Limitations
- Quantization fidelity under extreme scaling remains unproven for models larger than 32B
- Practical deployment constraints in multi-batch and dynamic batching scenarios not fully validated
- Training complexity and custom kernel implementations create reproducibility challenges

## Confidence
**High Confidence**: Core architectural contribution and FPGA implementation details are well-validated with specific metrics and performance claims relative to GPU baselines.

**Medium Confidence**: Quantization training methodology and accuracy preservation claims, though custom kernel implementations are not publicly available.

**Low Confidence**: Scalability claims beyond 32B models and performance in multi-batch, dynamic batching scenarios remain theoretical rather than experimentally validated.

## Next Checks
1. Implement the co-quantization training for a standard 1.7B model (e.g., LLaMA) using publicly available frameworks to validate approach generalizability.

2. Modify the design to handle dynamic batch sizes and measure HBM bandwidth utilization and latency under realistic batching scenarios to test 92% utilization claim.

3. Implement the same architecture on a smaller FPGA (e.g., AMD V70) with reduced HBM channels to determine minimum resource requirements for maintaining efficiency.