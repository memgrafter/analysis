---
ver: rpa2
title: 'Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence
  from Finno-Ugric Languages'
arxiv_id: '2602.02287'
source_url: https://arxiv.org/abs/2602.02287
tags:
- language
- across
- coherence
- judge
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-lingual evaluation stability of large
  language models (LLMs) by controlling dialogue generation parameters across Estonian,
  Finnish, and Hungarian while varying only the target language. Using synthetic customer-support
  dialogues, the study tests whether LLM-as-a-judge scoring produces stable model
  rankings across these morphologically rich Finno-Ugric languages.
---

# Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages

## Quick Facts
- arXiv ID: 2602.02287
- Source URL: https://arxiv.org/abs/2602.02287
- Reference count: 40
- Key outcome: Surface-level metrics maintain cross-language stability, but pragmatic judgments like coherence show near-zero correlations across morphologically rich Finno-Ugric languages.

## Executive Summary
This study investigates whether LLM-as-a-judge evaluation can reliably transfer across morphologically rich Finno-Ugric languages (Estonian, Finnish, Hungarian). Using controlled generation of semantically equivalent customer-support dialogues, the research isolates evaluation behavior from content variation. The results show that while surface-level metrics (grammar, readability) maintain high cross-language ranking stability (τ≥.70), pragmatic judgments like coherence and instruction-following exhibit systematic breakdowns with near-zero or negative correlations. This instability persists despite controlled generation, indicating that judge scoring behavior differs across languages rather than reflecting true model performance differences.

## Method Summary
The study generates 10K synthetic customer-support dialogues per language using parametrized templates with identical distributions across 40+ industries and 20+ problem types. Each dialogue is generated in parallel across Estonian, Finnish, Hungarian, and English with the same parameters. Automatic metrics (TTR, MATTR, self-BLEU, semantic similarity via multilingual-e5-large-instruct) verify semantic alignment (.89-.94 similarity). GPT-5-mini evaluates dialogues on Grammar, Readability, Coherence, Fluency, and Label Recovery Accuracy using English meta-prompts. Kendall's τ measures cross-language ranking stability with bootstrap CIs (N=1,500) and permutation tests.

## Key Results
- Surface-level metrics (lexical diversity, grammar, readability) maintain cross-language stability with τ≥.70 and minimal rank inversions (1–3 per pair)
- Pragmatic judgments (coherence, instruction-following) show systematic breakdowns with near-zero or negative correlations across Finno-Ugric pairs (τ=−.06 for et–hu, τ=−.17 for fi–hu)
- Controlled generation ensures semantic similarity (.89-.94), isolating evaluation behavior from content variation

## Why This Works (Mechanism)

### Mechanism 1: Surface-level metrics transfer because they operate on localized, sentence-level processing
Surface-level grammatical and lexical quality judgments are driven by within-language distributions that remain stable across morphologically rich languages. The judge evaluates each dimension against language-internal norms rather than cross-lingual comparisons.

### Mechanism 2: Pragmatic judgments fail because discourse coherence requires conversation-level reasoning dependent on language-specific discourse norms
Coherence evaluation tracks conversation flow, logical consistency, and instruction alignment across multiple turns. These require integrating context over longer spans where morphological complexity compounds representation uncertainty, causing the judge's internal scoring logic to produce different model rankings across languages.

### Mechanism 3: Controlled generation isolates evaluation behavior from content variation
By generating dialogues with identical parameters across languages and verifying semantic equivalence via automatic metrics, the design ensures that any ranking divergence between languages must originate from the evaluation pipeline rather than generation quality.

## Foundational Learning

- **Morphological richness / agglutination**: Estonian, Finnish, and Hungarian have complex morphology (extensive case systems, agglutinative word formation) that affects both generation diversity and judge evaluation difficulty. *Quick check: Can you explain why a single word in Finnish might have 15+ grammatical forms, and how this affects token-level evaluation compared to English?*

- **LLM-as-a-judge evaluation paradigm**: Using an LLM to score outputs is the core evaluation method being tested for cross-lingual reliability. *Quick check: What are the three main failure modes in LLM-as-a-judge evaluation, and which does this paper primarily diagnose?*

- **Ranking stability metrics (Kendall's τ)**: The paper quantifies evaluation reliability via rank correlation rather than absolute score agreement. *Quick check: If model rankings between Estonian and Finnish have τ=0.82 for fluency but τ=−0.06 for coherence, what does this tell you about the judge's behavior?*

## Architecture Onboarding

- **Component map**: Controlled dialogue generator -> Surface metrics layer -> LLM judge layer -> Stability analysis layer
- **Critical path**: Generate controlled dialogues → Verify semantic similarity ≥0.89 → Run judge evaluation → Compute per-language rankings → Calculate τ across language pairs → Flag metrics with τ<0.5 or significant inversions as transfer failures
- **Design tradeoffs**: Synthetic dialogues enable control but may lack naturalistic variation; English meta-prompts simplify the stack but are tested for sensitivity; single judge model reduces cost with ablation showing generalizability
- **Failure signatures**: Coherence τ dropping below 0 with significant rank inversions (observed: 5 inversions, p<0.05 for et–hu); surface metrics stable (τ≥0.70) while pragmatic metrics collapse; ceiling effects in English limiting discriminative power
- **First 3 experiments**: 1) Replicate controlled generation protocol with your target language(s) to establish baseline stability metrics before evaluation deployment; 2) Collect small expert annotation set (N=100 dialogues) in target language to calibrate judge-human alignment on coherence; 3) Test judge behavior with native-language vs. English meta-prompts to check for prompt-language interference

## Open Questions the Paper Calls Out

- **Cross-lingual generalizability**: Do the observed ranking instabilities persist when evaluating natural, non-synthetic customer support interactions? The authors note validation on natural scenarios is needed to confirm ranking instabilities persist in operational settings.

- **Linguistic scope**: Do these coherence assessment failures generalize to linguistically distant families (e.g., analytic languages) or other conversational domains? The authors note findings may not hold for non-commercial models, other conversational domains, or linguistically distant languages.

- **Calibration solutions**: What specific calibration methods can effectively stabilize discourse-level LLM judge assessments without requiring extensive human annotation? While the paper diagnoses zero-shot transfer failure, it leaves remediation techniques as future work.

## Limitations

- Synthetic dialogues may not capture the full complexity of real conversational data, limiting ecological validity
- Semantic similarity metrics (.89-.94) provide reasonable control but may not fully capture discourse-level quality differences affecting pragmatic judgments
- Findings are limited to three Finno-Ugric languages with highly agglutinative morphology, limiting generalizability to other language families

## Confidence

- High confidence in surface-level metric stability findings (τ≥.70) due to controlled design and automatic metric verification
- Medium confidence in pragmatic judgment instability results given acknowledged limitations of semantic similarity controls
- Low confidence in cross-language generalizability beyond Finno-Ugric languages due to morphological specificity

## Next Checks

1. Validate the controlled generation protocol with your target language(s) before deployment to establish baseline stability metrics for your specific evaluation context
2. Collect a small expert annotation set (N=100 dialogues) in your target language to calibrate judge-human alignment on coherence specifically
3. Test judge behavior with native-language vs. English meta-prompts; if variance exceeds 0.05, investigate prompt-language interference as a potential confound in your evaluation pipeline