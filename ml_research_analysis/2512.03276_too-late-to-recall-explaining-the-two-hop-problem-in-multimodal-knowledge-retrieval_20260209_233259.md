---
ver: rpa2
title: 'Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge
  Retrieval'
arxiv_id: '2512.03276'
source_url: https://arxiv.org/abs/2512.03276
tags:
- factual
- entity
- recall
- llav
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs trained on vision-language tasks show degraded factual recall
  compared to their LLM backbones, as visual representations bypass early MLP layers
  critical for retrieving factual knowledge. By analyzing 14 VLMs across different
  architectures and sizes, the study finds that 11 models underperform their LLM counterparts
  on factual recall tasks.
---

# Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval

## Quick Facts
- arXiv ID: 2512.03276
- Source URL: https://arxiv.org/abs/2512.03276
- Reference count: 38
- Primary result: VLMs trained on vision-language tasks show degraded factual recall compared to their LLM backbones

## Executive Summary
VLMs trained on vision-language tasks show degraded factual recall compared to their LLM backbones, as visual representations bypass early MLP layers critical for retrieving factual knowledge. By analyzing 14 VLMs across different architectures and sizes, the study finds that 11 models underperform their LLM counterparts on factual recall tasks. Mechanistic analysis reveals that VLMs resolve visual entities too late in the forward pass, missing early-layer factual recall mechanisms. Experiments demonstrate that patching early MLP outputs from LLMs into VLMs recovers factual accuracy, while chain-of-thought prompting mitigates performance gaps. The findings highlight that effective multimodal alignment requires early integration of visual representations into the LLM's factual recall circuit.

## Method Summary
The study analyzes 14 VLMs across different architectures and sizes to investigate performance gaps in factual recall compared to their LLM backbones. Researchers employ mechanistic analysis to trace activation patterns and identify when visual entities are processed relative to factual knowledge retrieval. Patching experiments involve replacing early MLP outputs from LLMs with those from VLMs to test causal relationships. Chain-of-thought prompting is evaluated as a mitigation strategy. The analysis relies on GPT-4V interpretation of activation patterns to identify "fact retrieval" neurons.

## Key Results
- 11 out of 14 VLMs show degraded factual recall performance compared to their LLM counterparts
- VLMs process visual entities too late in the forward pass, missing early-layer factual recall mechanisms
- Patching early MLP outputs from LLMs into VLMs recovers factual accuracy
- Chain-of-thought prompting mitigates the performance gap between VLMs and LLMs

## Why This Works (Mechanism)
The performance degradation occurs because visual representations bypass early MLP layers that are critical for factual recall. In VLMs, visual information is processed later in the network hierarchy, after the layers that typically handle factual knowledge retrieval in LLMs. This temporal separation means the visual modality cannot access or contribute to the factual recall mechanisms that operate in early layers. The MLP layers in LLMs serve as crucial "hops" for retrieving and processing factual information, but when visual inputs are introduced, they skip these critical early stages, resulting in degraded performance on knowledge-intensive tasks.

## Foundational Learning
- **MLP Layer Functionality**: Early MLP layers in LLMs handle factual knowledge retrieval through learned linear projections. Understanding this requires knowing how MLPs transform and route information in transformer architectures.
  - *Why needed*: The study identifies that bypassing early MLPs is the core problem, so understanding their role in factual recall is essential.
  - *Quick check*: Verify that early MLPs show distinct activation patterns for factual queries versus other types of inputs.

- **Visual Encoder Integration**: How visual encoders interface with transformer architectures affects information flow timing. This requires understanding multimodal fusion strategies and their impact on feature propagation.
  - *Why needed*: The study shows that late visual integration causes the two-hop problem, making integration timing critical.
  - *Quick check*: Compare activation timing between visual and textual inputs across different VLM architectures.

- **Factual Recall Mechanisms**: The neural circuits and patterns that enable factual knowledge retrieval in LLMs. This involves understanding how transformers store and retrieve structured knowledge.
  - *Why needed*: The core observation is about degraded factual recall, requiring understanding of how this capability normally functions.
  - *Quick check*: Map activation patterns for factual queries in LLM versus VLM to identify missing or delayed responses.

## Architecture Onboarding
- **Component Map**: Visual Encoder -> Multimodal Fusion -> Transformer Blocks (Early MLPs) -> Later Layers -> Output
- **Critical Path**: Visual inputs → Multimodal fusion → Skip early MLPs → Late integration → Degraded factual recall
- **Design Tradeoffs**: Early visual integration enables factual recall but may increase computational cost and complexity; late integration simplifies architecture but sacrifices knowledge retrieval performance.
- **Failure Signatures**: Degraded factual recall performance, delayed activation patterns for visual entities, missing early-layer factual response patterns.
- **3 First Experiments**:
  1. Test factual recall performance across a broader range of knowledge-intensive tasks beyond the current scope
  2. Implement architectures with early visual integration from the start of training rather than post-hoc modifications
  3. Evaluate the proposed solutions (patching, CoT) under realistic deployment conditions with multiple visual domains

## Open Questions the Paper Calls Out
None

## Limitations
- The mechanistic analysis relies heavily on GPT-4V interpretation of activation patterns, introducing potential subjectivity in identifying "fact retrieval" neurons
- The 11/14 VLM degradation finding represents a mixed pattern rather than universal failure, suggesting architecture-specific effects require deeper investigation
- Patching experiments operate under artificial conditions that don't directly translate to training-time architectural changes

## Confidence
- **High confidence**: Core observation that VLMs show degraded factual recall compared to LLMs, supported by consistent patterns across 14 models
- **Medium confidence**: Mechanistic explanation involving late visual integration, given reliance on GPT-4V analysis
- **Lower confidence**: Proposed solutions (patching, CoT) show promise but require further validation under realistic deployment conditions

## Next Checks
1. Validate the VLM-LLM performance gap across additional knowledge-intensive tasks beyond factual recall, including commonsense reasoning and temporal reasoning
2. Implement early visual integration architectures from the start of training rather than post-hoc patching to assess whether this eliminates the performance gap
3. Test the proposed mechanisms across a broader range of VLM architectures, particularly those using different visual encoders (CNNs, deformable transformers) and multimodal fusion strategies