---
ver: rpa2
title: Sparse Autoencoder Features for Classifications and Transferability
arxiv_id: '2502.11367'
source_url: https://arxiv.org/abs/2502.11367
tags:
- features
- feature
- performance
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of sparse autoencoders
  (SAEs) for interpretable feature extraction from large language models (LLMs) in
  safety-critical classification tasks. The authors investigate SAE-based features
  across multiple model scales, languages, and modalities, examining the impact of
  architectural choices like layer depth, width, pooling strategies, and binarization.
---

# Sparse Autoencoder Features for Classifications and Transferability

## Quick Facts
- arXiv ID: 2502.11367
- Source URL: https://arxiv.org/abs/2502.11367
- Reference count: 40
- Primary result: SAE-derived features achieve macro F1 scores above 0.8 in safety-critical classification tasks, outperforming traditional baselines

## Executive Summary
This paper presents a systematic evaluation of sparse autoencoders (SAEs) for interpretable feature extraction from large language models (LLMs) in safety-critical classification tasks. The authors investigate SAE-based features across multiple model scales, languages, and modalities, examining the impact of architectural choices like layer depth, width, pooling strategies, and binarization. They demonstrate that SAE-derived features achieve macro F1 scores above 0.8, outperforming traditional baselines like hidden-state probing and bag-of-words methods. The study shows strong cross-lingual transfer capabilities, with features generalizing effectively from English to other languages in toxicity detection tasks. Additionally, SAE features enable smaller models to predict the behavior of larger instruction-tuned models, suggesting a scalable approach for model oversight and auditing. These findings establish practical best practices for SAE-based interpretability and highlight their potential for transparent deployment of LLMs in real-world applications.

## Method Summary
The study systematically evaluates sparse autoencoders (SAEs) for feature extraction from LLMs across multiple dimensions. The authors train SAEs on different model layers, varying width and depth parameters, and test various pooling strategies (mean, max, CLS token) and binarization approaches. They compare SAE-derived features against traditional baselines including hidden-state probing and bag-of-words methods across multiple classification tasks. The evaluation spans different model scales (from smaller models to large instruction-tuned models), languages (primarily English with cross-lingual transfer experiments), and modalities. Performance is measured using macro F1 scores on safety-critical classification tasks, with particular focus on toxicity detection datasets.

## Key Results
- SAE-derived features achieve macro F1 scores above 0.8, outperforming traditional baselines like hidden-state probing and bag-of-words methods
- Strong cross-lingual transfer capabilities demonstrated, with features generalizing effectively from English to other languages in toxicity detection tasks
- SAE features enable smaller models to predict the behavior of larger instruction-tuned models, suggesting scalable model oversight and auditing capabilities

## Why This Works (Mechanism)
SAE features work effectively because they capture interpretable, high-level representations from LLM activations that are both discriminative for classification tasks and transferable across domains. The sparse regularization forces the autoencoder to learn compact, meaningful features rather than memorizing activations. These learned features capture task-relevant information while filtering out noise, making them particularly effective for safety-critical classification where interpretability and generalization are crucial. The cross-lingual transfer capability suggests that SAE features capture language-agnostic semantic patterns that generalize beyond specific linguistic structures.

## Foundational Learning
- **Sparse Autoencoders**: Neural networks trained to reconstruct inputs while enforcing sparsity constraints, useful for learning interpretable features
  - Why needed: Standard autoencoders often learn dense, uninterpretable features; sparsity enforces meaningful feature extraction
  - Quick check: Verify sparsity level (typically 5-10% active features) and reconstruction error

- **Cross-lingual Transfer**: Ability of features learned on one language to generalize to other languages without additional training
  - Why needed: Enables resource-efficient deployment across multiple languages without retraining per language
  - Quick check: Test feature performance on target language after training on source language

- **Instruction-tuned Models**: LLMs fine-tuned on instruction-following datasets to improve task generalization
  - Why needed: Modern LLMs are increasingly instruction-tuned, requiring evaluation methods that work with these architectures
  - Quick check: Verify model responds appropriately to zero-shot or few-shot prompts

## Architecture Onboarding
- **Component Map**: LLM -> Activation Extraction -> Sparse Autoencoder -> Binarization (optional) -> Classifier
- **Critical Path**: The bottleneck layer of the SAE, where sparse features are extracted and binarized if needed
- **Design Tradeoffs**: Width vs depth of SAE layers (wider captures more features but increases computation), pooling strategy (CLS token preserves sentence-level information while mean pooling captures overall context), binarization (reduces dimensionality but may lose information)
- **Failure Signatures**: Poor cross-lingual transfer indicates features are too language-specific; low reconstruction error suggests SAE hasn't learned meaningful compression; performance drop after binarization suggests feature information loss
- **First Experiments**: 1) Compare SAE features vs hidden-state probing on same classification task, 2) Test cross-lingual transfer from English to target language, 3) Evaluate impact of different pooling strategies on classification performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on toxicity detection datasets, limiting generalizability to other safety-critical domains like misinformation detection or bias assessment
- Cross-lingual transfer experiments only demonstrate effectiveness from English to other languages, not bidirectional transfer or non-English to non-English transfer
- Architectural exploration lacks systematic investigation of activation functions and alternative sparse regularization techniques beyond standard L1 penalties

## Confidence
- SAE features achieving macro F1 scores above 0.8: High confidence based on systematic evaluation across multiple models and tasks
- Transferability between model scales: Medium confidence due to limited evaluation across different instruction-tuning paradigms and model families
- Practical best practices: Medium confidence as they are based on specific model architectures and may require validation for other domains

## Next Checks
1. Evaluate SAE feature performance across diverse safety-critical domains beyond toxicity detection, including misinformation detection and fairness assessment
2. Test cross-lingual transfer in both directions (non-English to English and between non-English languages) to establish bidirectional generalizability
3. Compare SAE feature compression approaches (binarization, quantization, pruning) against each other to identify optimal feature representation strategies for different deployment scenarios