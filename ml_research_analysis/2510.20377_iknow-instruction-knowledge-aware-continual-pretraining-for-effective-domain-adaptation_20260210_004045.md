---
ver: rpa2
title: 'IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain
  Adaptation'
arxiv_id: '2510.20377'
source_url: https://arxiv.org/abs/2510.20377
tags:
- pretraining
- language
- knowledge
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual pretraining instruction-tuned
  language models on domain-specific data without access to the original base model.
  The authors propose IKnow, a framework that reformulates self-supervised objectives
  as instruction-response dialogue pairs, enabling adaptation while preserving instruction-following
  capabilities.
---

# IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation

## Quick Facts
- arXiv ID: 2510.20377
- Source URL: https://arxiv.org/abs/2510.20377
- Authors: Tianyi Zhang; Florian Mai; Lucie Flek
- Reference count: 8
- Primary result: Instruction-knowledge-aware pretraining framework improves domain adaptation while preserving instruction-following capabilities

## Executive Summary
IKnow addresses the challenge of adapting instruction-tuned language models to domain-specific data without access to original base models. The framework reformulates self-supervised objectives as instruction-response dialogue pairs, enabling effective domain adaptation while preserving instruction-following capabilities. Through two novel knowledge-aware tasks—Masked Phrase Prediction and NL-KG Loop—IKnow enhances semantic encoding for specialized domains. Experiments demonstrate consistent improvements over naive next-token prediction baselines across different model architectures, with up to 38.62% ROUGE-L-F1 on RepliQA using Llama-3.2-3B with LoRA fine-tuning.

## Method Summary
IKnow operates by converting domain-specific text into instruction-response pairs where the instruction specifies a self-supervised task (e.g., "Fill in the blanks" or "Complete the sentence") and the response contains the model's generated completion. The framework employs a knowledge-aware continual pretraining strategy that preserves instruction-following capabilities while adapting to domain-specific knowledge. Two novel knowledge-aware tasks are introduced: Masked Phrase Prediction (MPP) predicts masked noun phrases to enhance semantic encoding, and NL-KG Loop generates knowledge graph triplets from sentences to incorporate structured knowledge. The approach uses parameter-efficient fine-tuning (LoRA) for evaluation, making it practical for deployment scenarios where full fine-tuning is resource-prohibitive.

## Key Results
- IKnow achieves up to 38.62% ROUGE-L-F1 on RepliQA using Llama-3.2-3B with LoRA fine-tuning
- Consistent improvements over naive next-token prediction baselines across all tested models and datasets
- Llama-3.2-