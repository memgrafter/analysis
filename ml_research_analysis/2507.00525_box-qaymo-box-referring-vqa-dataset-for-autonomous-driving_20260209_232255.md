---
ver: rpa2
title: 'Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving'
arxiv_id: '2507.00525'
source_url: https://arxiv.org/abs/2507.00525
tags:
- driving
- questions
- motion
- autonomous
- senna
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in evaluating vision-language models
  for autonomous driving, focusing on their ability to communicate perceptual understanding
  to users. The authors introduce Box-QAymo, a novel dataset and benchmark that enables
  localized, user-driven queries by drawing bounding boxes on objects.
---

# Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving

## Quick Facts
- **arXiv ID**: 2507.00525
- **Source URL**: https://arxiv.org/abs/2507.00525
- **Reference count**: 38
- **Primary result**: Introduces Box-QAymo, a hierarchical VQA benchmark revealing substantial VLM performance gaps in autonomous driving perception tasks (66.1% F1 binary → 18.3% attribute)

## Executive Summary
This paper addresses the critical gap in evaluating vision-language models for autonomous driving, focusing on their ability to communicate perceptual understanding to users through localized, box-referring queries. The authors introduce Box-QAymo, a novel dataset and benchmark that enables user-driven spatial and temporal reasoning by drawing bounding boxes on objects. The dataset features a hierarchical evaluation protocol progressing from binary sanity checks to attribute prediction and complex motion reasoning over inter-object dynamics across frames. Experiments reveal significant performance limitations in current VLMs, especially for motion understanding, highlighting the need for improved models that can effectively communicate with users.

## Method Summary
Box-QAymo is built on Waymo Open Dataset validation split (101 scenes fine-tune, 101 validation) with crowd-sourced fine-grained object classes and visual attributes following Argoverse 2.0 taxonomy. The dataset contains 1,662 binary, 5,403 attribute, and 13,714 motion Q&A pairs generated through template-based hierarchical complexity progression. Quality control includes negative sampling and temporal consistency filters using linear interpolation for trajectory gaps. Objects are rendered as 3×3 crop galleries with red bounding box overlays. The evaluation uses LLaVA-1.5 7B, Qwen-VL 7B, and Senna models with LoRA fine-tuning (rank=128, alpha=256, lr=2e-4, 1 epoch) and steering prompts to guide output format. F1 score is the primary metric with per-class precision/recall.

## Key Results
- Hierarchical performance degradation: Binary (66.1% F1) → Attribute (18.3% F1) → Motion (37.6% F1)
- Visual prompting effectiveness varies by architecture: Qwen-VL improves with boxes (+1.39% avg F1) while LLaVA degrades (-1.28% F1)
- Temporal reasoning challenges: Consecutive frames degrade performance across all models, with Senna dropping from 56.56% to 44.40% F1
- Domain-specific brittleness: Senna (driving-specific) achieves 0% accuracy on multiple attribute tasks despite fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Complexity Degradation
VLM performance degrades as tasks shift from global scene classification to localized attribute reasoning because binary questions rely on statistical correlations while attribute questions require genuine visual grounding. This reveals that current models cannot isolate target object features from background noise without explicit detection pre-training.

### Mechanism 2: Visual Marker Grounding Dependency
Red bounding boxes act as attention guides, with effectiveness contingent on visual encoder architecture. Cross-attention mechanisms (Qwen-VL) can separate box-induced features from object features, while simpler MLP projections (LLaVA) may introduce noise that obscures fine-grained visual features.

### Mechanism 3: Short-Term Temporal Interference
Current VLMs lack dedicated temporal fusion modules for high-frequency frame pairs (100ms intervals), processing frame concatenations as static panoramic images and misinterpreting motion blur or positional shifts as noise or distinct objects.

## Foundational Learning

- **Concept**: Referring Expression Comprehension (REC)
  - **Why needed**: Links user's spatial reference (bounding box) to textual query (VQA)
  - **Quick check**: Can the model distinguish "the red car" inside a box from the "blue truck" outside it?

- **Concept**: Visual Prompting
  - **Why needed**: Understanding how non-textual inputs (red boxes) modify attention weights of the Vision Transformer
  - **Quick check**: Does drawing a red box around an object increase attention score for those tokens?

- **Concept**: Temporal Consistency vs. Interpolation
  - **Why needed**: Dataset uses linear interpolation to fill gaps in Waymo trajectories
  - **Quick check**: Is predicted motion derived from visual frame differences or underlying trajectory metadata?

## Architecture Onboarding

- **Component map**: Waymo Open Dataset (Images + 3D Boxes) → Crowd-sourced fine-grained labels (CVAT) → VQA Generator (templates) → Quality Control (Negative Sampling & Temporal consistency) → Model Interface (Image + Red Box Overlay + Steering Prompt → VLM)
- **Critical path**: Quality Control stage (Negative Sampling) is most critical - without it, models may achieve high accuracy by exploiting class imbalances rather than performing visual reasoning
- **Design tradeoffs**: Single-frame models currently outperform two-frame models; avoid naive frame stacking. General models (Qwen) adapt better to Q&A formats than driving-specific models (Senna) which suffer from "narrow task brittleness"
- **Failure signatures**: Low Valid Response Rate (Senna's 34% vs 90-99% for others), Format Hallucination (ignoring steering prompts), Zero Attribute Accuracy (model blind to bounding box reference)
- **First 3 experiments**: 1) Sanity Check: Run zero-shot inference on Binary questions to establish baseline; 2) Grounding Ablation: Evaluate Attribute tasks with/without red boxes to verify visual prompting; 3) LoRA Adaptation: Fine-tune general VLM on Motion subcategory to test temporal reasoning injection

## Open Questions the Paper Calls Out

### Open Question 1
Can VLMs be trained to effectively integrate short-term temporal information (e.g., frame pairs at 100ms intervals) for structured reasoning, and what architectural or training paradigm changes are required? The paper demonstrates that consecutive frames consistently degrade performance, suggesting fundamental limitations in how current VLMs process short-term temporal context.

### Open Question 2
How can domain-specific VLMs be designed to maintain robust transfer capabilities across diverse task types within autonomous driving, rather than suffering catastrophic degradation when moving beyond narrow task optimization? Senna's poor performance despite being driving-specific reveals the brittleness of narrow task training.

### Open Question 3
What architectural features enable effective visual grounding for box-referring queries, and can these be systematically incorporated into VLMs without degrading performance on other visual reasoning tasks? The paper shows box grounding improves Qwen-VL but has mixed effects on LLaVA, attributed to architectural differences.

## Limitations
- Dataset relies on Waymo Open Dataset validation scenes, constraining generalizability to other driving environments
- Analysis lacks deeper investigation into why visual prompting effects vary by architecture
- Temporal reasoning experiments don't explore whether alternative temporal fusion approaches could address observed degradation
- Exact thresholds and criteria for quality control (negative sampling, temporal consistency) are underspecified

## Confidence

- **High confidence**: Hierarchical performance degradation pattern and architecture-dependent visual prompting effects are well-supported
- **Medium confidence**: Conclusions about VLM limitations in motion understanding are supported but don't fully rule out training/data artifacts
- **Medium confidence**: Dataset quality control methodology appears rigorous but lacks detailed specification

## Next Checks

1. **Cross-dataset generalization test**: Evaluate Box-QAymo-trained models on the nuRisk dataset to determine if performance gaps persist across different driving datasets and environments

2. **Architectural ablation study**: Systematically compare bounding box effects across a broader range of VLM architectures to better understand the visual prompting mechanism

3. **Temporal reasoning intervention**: Implement explicit optical flow or motion vector channels alongside RGB frames to test whether temporal performance degradation stems from VLM architecture limitations or missing motion features