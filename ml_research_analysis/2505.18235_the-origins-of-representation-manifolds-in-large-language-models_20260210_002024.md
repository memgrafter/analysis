---
ver: rpa2
title: The Origins of Representation Manifolds in Large Language Models
arxiv_id: '2505.18235'
source_url: https://arxiv.org/abs/2505.18235
tags:
- which
- representation
- representations
- feature
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mathematical framework for understanding
  how features are represented as manifolds in large language models. The authors
  formalize features as metric spaces and propose that neural representations of these
  features live on manifolds that are homeomorphic to their corresponding metric spaces.
---

# The Origins of Representation Manifolds in Large Language Models

## Quick Facts
- arXiv ID: 2505.18235
- Source URL: https://arxiv.org/abs/2505.18235
- Authors: Alexander Modell; Patrick Rubin-Delanchy; Nick Whiteley
- Reference count: 39
- Key outcome: Features in large language models are represented on manifolds homeomorphic to their corresponding metric spaces, with cosine similarity encoding intrinsic geometry through shortest on-manifold paths.

## Executive Summary
This paper presents a mathematical framework showing that features in large language models are represented as manifolds embedded in high-dimensional spaces. The authors formalize features as metric spaces and propose that neural representations of these features live on manifolds that are homeomorphic to their corresponding metric spaces. A key theoretical result shows that under plausible hypotheses, cosine similarity in representation space encodes the intrinsic geometry of a feature through shortest, on-manifold paths, making the feature and its representation geometrically indistinguishable. The framework is validated on text embeddings and token activations from large language models, showing evidence of isometry between feature distances and representation distances.

## Method Summary
The framework involves extracting feature-specific representations from LLM embeddings or SAE activations, normalizing to unit norm, and estimating manifold structure via K-nearest-neighbor graphs. Geodesic distances are computed as weighted shortest-path distances on these graphs and compared to hypothesized metric spaces for each feature type. The analysis tests for homeomorphism through rank correlations between feature values and manifold positions, and for isometry by correlating geodesic distances with feature metric distances. The method is applied to OpenAI text embeddings for color names and dates, and to GPT-2 small and Mistral 7B token activations for years and months/days extracted via sparse autoencoders.

## Key Results
- Years are represented on approximately linear manifolds (Kendall correlation 0.97, Spearman >0.99)
- Dates exhibit circular topology matching the yearly cycle
- Colors show topological structure consistent with the color wheel
- Geodesic distances on representation manifolds are approximately isometric to distances in the corresponding feature metric spaces

## Why This Works (Mechanism)

### Mechanism 1: Homeomorphism via Continuous Correspondence
- **Claim:** Feature metric spaces map continuously to representation manifolds, preserving topology.
- **Mechanism:** A continuous invertible map φ_f: Z_f → S^(D-1) exists such that v_f(x) = φ_f(z_f(x)). Since Z_f is compact, the inverse is also continuous (Proposition 1), creating a homeomorphism that preserves connected components, holes, and branching structure.
- **Core assumption:** The continuous correspondence hypothesis holds—features and representations are in continuous one-to-one correspondence.
- **Evidence anchors:**
  - [abstract] "features like colors and years are represented on manifolds that are homeomorphic... to their corresponding metric spaces"
  - [section 2.3] Kendall and Spearman rank correlations of 0.97 and >0.99 for years, "representations occur in very close to true temporal order"
  - [corpus] "Shape Happens" paper validates automatic manifold discovery; evidence is emerging but limited for diverse feature types
- **Break condition:** When feature values are not compact, or when multiple distinct feature values map to the same representation (non-injective), or when representations are discontinuous.

### Mechanism 2: Geodesic Distance Isometry
- **Claim:** Cosine similarity locally encodes intrinsic feature geometry through shortest on-manifold paths.
- **Mechanism:** Under Hypothesis 2, CosSim(φ_f(z), φ_f(z')) = g_f(d_f(z,z')²) with g'_f(0) < 0. Theorem 1 proves that path lengths on M_f equal path lengths on Z_f (up to scaling factor √(-2g'_f(0))), meaning geodesic distances are preserved.
- **Core assumption:** g_f has continuous second derivatives with g'_f(0) < 0 (cosine similarity decreases as feature distance increases locally).
- **Evidence anchors:**
  - [abstract] "cosine similarity in representation space encodes the intrinsic geometry of a feature through shortest on-manifold paths"
  - [section 3.1] Pearson correlation 0.99 for log-transformed years; 0.97 for dates
  - [corpus] Limited corpus evidence on isometry specifically; related work focuses on topology rather than metric preservation
- **Break condition:** When cosine similarity is not a decreasing function of feature distance near zero, or when K-NN graph has short-circuits creating non-isometric geodesics.

### Mechanism 3: Dimensional Expansion for Computational Expressivity
- **Claim:** Manifolds curve through higher-dimensional spaces to enable richer function computation via linear projections.
- **Mechanism:** To compute polynomials of degree p from z via linear projections, φ_f(z) must span p+2 dimensions with basis functions encoding 1, z, z², etc. This forces the manifold to weave through higher-dimensional subspaces while remaining topologically simple.
- **Core assumption:** Linear operations between layers benefit from maximizing expressivity; superposition hypothesis allows approximate access to φ_f despite interference from other features.
- **Evidence anchors:**
  - [section 2.4] "one could do this by setting φ_f(z) = b₀(z)v₀ + ··· + b_{p+1}v_{p+1}... [to represent] polynomials of order p"
  - [section 2.4] "these manifolds curve and bend to occupy higher dimensional spaces"
  - [corpus] Weak corpus support for this specific computational mechanism; remains largely theoretical
- **Break condition:** When features are purely atomic (no continuous structure), or when computational needs are satisfied by low-dimensional encodings.

## Foundational Learning

- **Concept: Metric spaces and geodesics**
  - **Why needed here:** The entire framework models features as metric spaces (Z_f, d_f) and uses geodesic (shortest-path) distances to establish isometry between feature spaces and representation manifolds.
  - **Quick check question:** Given points on a curved 2D manifold embedded in 3D, why does Euclidean distance in 3D not equal geodesic distance on the manifold?

- **Concept: Homeomorphism vs. Isometry**
  - **Why needed here:** The paper distinguishes between topological preservation (homeomorphism—shape preserved) and geometric preservation (isometry—distances preserved). Years are homeomorphic but not isometric to a linear scale; they become isometric to a log scale.
  - **Quick check question:** A coffee mug and donut are homeomorphic. Are they isometric? Why or why not?

- **Concept: Manifold tangent space and local linearity**
  - **Why needed here:** Hypothesis 2 only requires the cosine similarity relationship to hold locally (d_f(z,z') ≤ ε), which connects to the manifold having well-behaved local tangent structure where g'' exists.
  - **Quick check question:** If g'_f(0) = 0 instead of g'_f(0) < 0, what would that imply about the local geometry around a point on the manifold?

## Architecture Onboarding

- **Component map:** Input representations -> Normalization to unit sphere -> Feature extraction (SAE) -> PCA reduction -> K-NN graph construction -> Geodesic computation -> Diagnostic analysis

- **Critical path:**
  1. Obtain representations (embeddings or SAE-extracted activations)
  2. Normalize to unit sphere
  3. Propose candidate metric space Z_f
  4. Test homeomorphism (rank correlation, visual inspection)
  5. Test isometry (geodesic distance correlation)
  6. If isometry fails, hypothesize alternative metric (e.g., log transform)

- **Design tradeoffs:**
  - **K for K-NN graph:** Lower K preserves local structure but risks disconnected components; higher K risks short-circuits corrupting geodesic estimates
  - **PCA dimensionality:** Lower dimensions reveal simple structure but may discard meaningful geometry
  - **Manual vs. learned metrics:** Manual design enables interpretability but doesn't scale; learning metrics is unexplored

- **Failure signatures:**
  - **Disconnected K-NN graph:** K too low; increase K
  - **Non-monotonic geodesic patterns:** Short-circuits; manually prune edges
  - **Low rank correlation:** Homeomorphism violated; wrong candidate metric space or feature extraction failed
  - **High ξ but low ρ:** Homeomorphism holds but isometry fails; try alternative metric transforms

- **First 3 experiments:**
  1. **Validate homeomorphism on known features:** Extract year representations from GPT-2 layer 7, compute rank correlation between years and manifold position (weighted graph distance from 1900). Expect >0.95.
  2. **Test isometry with linear vs. log metric:** For years, plot geodesic distance vs. |year₁ - year₂| and vs. |log(2019-year₁) - log(2019-year₂)|. Expect log scale to show Pearson ≈ 0.99.
  3. **Visualize circular features:** Apply PCA (3 components) to color or date embeddings, plot from multiple angles. Expect circular topology matching color wheel or yearly cycle.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies on strong assumptions about continuous one-to-one correspondence between features and representations
- Geodesic distance isometry requires very specific local properties of cosine similarity that may not hold universally
- The computational expressivity mechanism through dimensional expansion remains largely theoretical with limited empirical validation
- K-nearest-neighbor graph estimation is sensitive to parameter choices and can be corrupted by short-circuits or noise

## Confidence

**High Confidence Claims:**
- Feature representations live on low-dimensional manifolds embedded in high-dimensional spaces
- Homeomorphism between feature metric spaces and representation manifolds (supported by rank correlations >0.95)
- Cosine similarity encodes local geometry of features through shortest on-manifold paths

**Medium Confidence Claims:**
- Geodesic distances on representation manifolds are isometric to distances in the corresponding feature metric spaces (requires specific local properties)
- Dimensional expansion is necessary for computing richer functions via linear projections (theoretical but under-validated)

**Low Confidence Claims:**
- Universal applicability across all feature types and model architectures
- Specific numerical constants in the geodesic isometry relationship
- Scalability of the framework to automatically discover arbitrary feature manifolds

## Next Checks

1. **Test Isometry Robustness Across Models:** Apply the framework to SAE-extracted features from multiple LLMs (GPT-2, GPT-3, Mistral) for the same feature types (years, colors, dates). Compare Pearson correlations and test whether isometry holds consistently or varies with model architecture, training data, or feature extraction method.

2. **Validate Against Synthetic Data:** Generate synthetic feature manifolds with known geometry (linear, logarithmic, circular, multi-dimensional) and train simple MLPs to encode them. Apply the framework to these representations and verify that homeomorphism and isometry are correctly detected across controlled variations in manifold structure.

3. **Stress Test K-NN Graph Estimation:** Systematically vary K (from 5 to 50) and apply different edge pruning strategies on real feature data. Measure how these choices affect geodesic distance estimates and isometry correlations. Develop quantitative criteria for selecting optimal K and pruning parameters that maximize isometry while minimizing short-circuits.