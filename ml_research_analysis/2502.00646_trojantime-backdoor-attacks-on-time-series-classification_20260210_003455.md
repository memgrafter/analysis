---
ver: rpa2
title: 'TrojanTime: Backdoor Attacks on Time Series Classification'
arxiv_id: '2502.00646'
source_url: https://arxiv.org/abs/2502.00646
tags:
- backdoor
- attack
- data
- training
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes TrojanTime, a novel backdoor attack framework
  for Time Series Classification (TSC) that addresses the challenge of attacking models
  when training data is inaccessible. The core idea is a two-step training process:
  first, generating a pseudo-dataset from an external dataset using adversarial attacks
  to create diverse samples across multiple classes; second, training the clean model
  on this pseudo-dataset and its poisoned version using logits alignment and batch
  normalization freezing to maintain generalization on clean samples while embedding
  backdoor patterns.'
---

# TrojanTime: Backdoor Attacks on Time Series Classification

## Quick Facts
- arXiv ID: 2502.00646
- Source URL: https://arxiv.org/abs/2502.00646
- Reference count: 31
- One-line primary result: TrojanTime achieves high backdoor attack success rates (avg 86.3%) on TSC models while preserving clean accuracy through a two-step training framework.

## Executive Summary
This paper proposes TrojanTime, a novel backdoor attack framework for Time Series Classification (TSC) that addresses the challenge of attacking models when training data is inaccessible. The core idea is a two-step training process: first, generating a pseudo-dataset from an external dataset using adversarial attacks to create diverse samples across multiple classes; second, training the clean model on this pseudo-dataset and its poisoned version using logits alignment and batch normalization freezing to maintain generalization on clean samples while embedding backdoor patterns. The method is evaluated across five trigger types, four TSC architectures, and UCR benchmark datasets, demonstrating effectiveness in executing backdoor attacks while preserving clean accuracy. Additionally, a defensive unlearning strategy is proposed to mitigate the attack by identifying and isolating toxic samples based on their activation patterns in rear layers, effectively reducing attack success rate while maintaining clean accuracy.

## Method Summary
TrojanTime is a backdoor attack framework for TSC models that operates without access to the original training data. It uses an external arbitrary dataset to generate a pseudo-dataset through target adversarial attacks (PGD-50). This pseudo-dataset, combined with trigger-applied poisoned samples, is used to train the model with logits alignment and frozen batch normalization to preserve clean accuracy while embedding backdoor patterns. The method is validated across multiple trigger types and TSC architectures on UCR benchmark datasets.

## Key Results
- TrojanTime achieves an average Attack Success Rate (ASR) of 86.3% for fixed triggers and 67.3% for blended (powerline) triggers across five UCR datasets.
- Clean Accuracy (CA) is preserved with minimal drops (e.g., 93.9% CA with 100% ASR on ECG5000 using InceptionTime).
- The proposed defensive unlearning strategy reduces average ASR from 74.9% to 15.1% on fixed triggers while maintaining clean accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial perturbation of external data produces pseudo-samples whose latent representations cluster near multiple classes of the unknown training distribution, enabling effective backdoor injection without original data access.
- **Mechanism:** PGD attacks push external samples toward each target class in the model's latent space. The resulting adversarial samples $D_{adv}$ overlap with or approximate training data representations, providing sufficient coverage for learning trigger-to-label mappings that generalize across class boundaries.
- **Core assumption:** The pretrained model's decision boundaries are sufficiently smooth that adversarial perturbations from any external time series can reach neighborhoods of all class clusters.
- **Evidence anchors:** [abstract] "generate a pseudo-dataset using an external arbitrary dataset through target adversarial attacks"; [Section 2.2] "representations of adversarial synthesized dataset Dadv were assigned to several distinct clusters, with each cluster overlapping with or closely approximating the representations of the training data"
- **Break condition:** If the external dataset $D'$ is too small or too distributionally distant, adversarial perturbations may fail to reach all class neighborhoods, reducing both clean accuracy and attack success rate.

### Mechanism 2
- **Claim:** Logits alignment with frozen BatchNorm statistics preserves clean accuracy by constraining the backdoor training trajectory to stay within the original model's learned representation space.
- **Mechanism:** The $L_{MSE}$ term anchors current logits to the original model's outputs, preventing the optimizer from shifting decision boundaries. Frozen BN layers retain original running statistics, preventing distribution shift from the pseudo-dataset from corrupting feature normalization. This dual constraint forces learning into residual capacity—specifically, neurons that can associate triggers without disturbing existing class boundaries.
- **Core assumption:** The pretrained model has sufficient unused neural capacity that trigger-pattern learning can occur in filters/neurons that do not critically contribute to clean classification.
- **Evidence anchors:** [abstract] "combining logits alignment and batch norm freezing"; [Section 2.2] "This regularization ensures that the model retains its generalization ability on clean data"; [Table 2] Ablation shows CA drops from 93.9% to 90.5% without logits alignment; CA drops from 93.9% to 94.1% but ASR collapses from 100% to 19.6% without BN freezing
- **Break condition:** If the model is already at capacity or if trigger patterns require learning in channels critical for clean classification, alignment will either block backdoor learning or degrade clean accuracy.

### Mechanism 3
- **Claim:** Backdoor samples exhibit higher activation norms in rear-layer channels compared to clean samples, enabling isolation and targeted unlearning.
- **Mechanism:** The defense identifies samples with top-$r\%$ response norms in rear layers as likely backdoor samples, then applies gradient ascent to unlearn trigger associations while maintaining clean performance via normal training on remaining samples.
- **Core assumption:** Backdoor triggers consistently activate specific neurons more strongly than clean samples, and this separability persists across trigger types.
- **Evidence anchors:** [Section 2.3] "the neurons of the backdoored model are more active than those in the clean model"; [Figure 3] Shows clear heatmap contrast in rear layers for successful backdoor models vs. no contrast for failed attacks; [Table 3] Defense reduces average ASR from 74.9% to 15.1% on fixed triggers across InceptionTime
- **Break condition:** If trigger patterns are designed to have low activation magnitude or if the poisoning ratio is very low, the top-$r\%$ isolation will include mostly clean samples, causing defense to degrade clean accuracy.

## Foundational Learning

- **Concept: Backdoor Attacks vs. Adversarial Attacks**
  - Why needed here: The paper combines both—adversarial perturbation for dataset synthesis and backdoor injection for persistent trigger-response. Confusing these leads to misinterpreting the two-stage pipeline.
  - Quick check question: Does the attack modify inputs at inference time (adversarial) or embed a dormant response to a specific pattern (backdoor)?

- **Concept: Concept Drift in Transfer Learning**
  - Why needed here: The core challenge is embedding backdoors without causing the model to "forget" the original training distribution. Understanding distribution shift is essential for grasping why logits alignment and BN freezing are necessary.
  - Quick check question: If you fine-tune a model on data from a different distribution without regularization, what happens to accuracy on the original distribution?

- **Concept: Batch Normalization Statistics (Running Mean/Variance vs. Per-Batch)**
  - Why needed here: Freezing BN preserves learned statistics; allowing them to update would shift feature distributions. This is a critical implementation detail often overlooked.
  - Quick check question: During inference, does BN use the current batch's statistics or accumulated running statistics?

## Architecture Onboarding

- **Component map:**
  External Dataset D' → Preprocessing: dimension matching → PGD Attack (per class K) → D_adv → Trigger Application → D_bd → Pretrained f_θ → Train on D_adv + D_bd with L_MSE + L_CE, BN frozen → f_θ' → Defense (optional): Rear-layer norm ranking → Top-r% isolation → Unlearning loop → f_θ*

- **Critical path:** The PGD attack generation is the computational bottleneck—generating $|D'| \times K$ adversarial samples. The backdoor training itself is lightweight (1000 epochs, small datasets). The defense is fast (20 epochs) but requires access to suspected backdoor samples.

- **Design tradeoffs:**
  - **Trigger type:** Fixed/random patch triggers achieve higher ASR (86.3% avg) but are more visible; blended (powerline) triggers are stealthier but achieve lower ASR (67.3% avg) because TSC is frequency-sensitive.
  - **λ (alignment vs. backdoor strength):** Default λ=1 balances CA and ASR; higher λ increases ASR but risks CA degradation.
  - **Defense r%:** 5% works for 10% poisoning ratio; lower poisoning requires lower r%, increasing false positive risk.

- **Failure signatures:**
  - **High ASR but CA collapse (>10% drop):** Logits alignment may be too weak or BN not properly frozen.
  - **Low ASR (<30%) with normal CA:** External dataset is too distributionally distant; try different D' or increase PGD iterations.
  - **Defense fails to reduce ASR:** Sample size too small (<20 samples) or trigger activation norms insufficiently distinct; check Figure 3-style heatmap.

- **First 3 experiments:**
  1. **Sanity check:** Replicate Table 1 on a single dataset (e.g., ECG5000) with InceptionTime, fixed trigger. Verify CA remains within 5% of benign and ASR >80%. If not, check BN freezing is correctly implemented.
  2. **Ablation validation:** Run w/o logits alignment and w/o BN freezing on the same setup. Expect CA drop (alignment) or ASR collapse (BN). This confirms component correctness.
  3. **Defense calibration:** Apply defense to the trojaned model with r% ∈ {1, 5, 10}. Measure ASR reduction vs. CA preservation. Identify the r% that minimizes ASR while keeping CA drop <3%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can optimized trigger designs significantly improve the attack success rate (ASR) of blended (frequency-based) methods in data-inaccessible TSC scenarios?
- Basis in paper: [explicit] The authors state they "have not investigated trigger design" and note that blended triggers showed significantly lower ASR compared to patch masking methods (fixed/random).
- Why unresolved: Current blended triggers underperform, likely because they fail to significantly alter the frequency map, but the authors hypothesize that "carefully designed" triggers could improve effectiveness.
- What evidence would resolve it: Comparative experiments evaluating new frequency-adaptive triggers within the TrojanTime framework against the current static powerline triggers.

### Open Question 2
- Question: Is the proposed defensive unlearning strategy robust against adaptive attacks specifically designed to minimize activation discrepancies?
- Basis in paper: [inferred] The defense mechanism isolates backdoor samples by identifying high response norms in rear layers, relying on a distinct "feature space disparity" between clean and toxic samples.
- Why unresolved: The defense assumes backdoor samples trigger higher neuron activation than clean samples. An adaptive attacker could potentially modify the training objective to regularize these activations, thereby evading detection.
- What evidence would resolve it: Evaluation of the defense against a modified TrojanTime attack that includes a regularization term to minimize the output norm difference between poisoned and clean samples.

### Open Question 3
- Question: How does the domain gap between the arbitrary external dataset ($D'$) and the unknown target dataset ($D_{train}$) impact the stability of the logits alignment process?
- Basis in paper: [inferred] The method relies on an arbitrary external dataset for synthesis. The authors note that concept drift occurs without alignment, implying that distribution mismatch is a critical challenge.
- Why unresolved: It is unclear if the pseudo-dataset synthesis remains effective when the external data $D'$ lacks semantic or temporal similarity to the original training data $D_{train}$, potentially causing alignment failure.
- What evidence would resolve it: Ablation studies measuring clean accuracy and ASR while systematically varying the domain distance between the selected external dataset and the target dataset.

## Limitations

- The effectiveness of TrojanTime critically depends on the availability of an external dataset whose distribution overlaps sufficiently with the unknown training data, but the identity and size of this dataset are not specified.
- The proposed defense mechanism's effectiveness relies on the separability of backdoor and clean sample activations in rear layers, which may not hold for subtle triggers or very low poisoning ratios.
- The paper assumes the pre-trained model has sufficient capacity for backdoor learning without degrading clean accuracy, but this is not validated across architectures or model sizes.

## Confidence

- **High confidence:** The two-step training framework (adversarial synthesis + backdoor training with logits alignment and BN freezing) is mechanistically sound and supported by ablation studies (Table 2). The defense's reliance on activation norm differences for backdoor sample isolation is logically consistent with observed patterns (Figure 3).
- **Medium confidence:** The attack's effectiveness across all five trigger types and four TSC architectures is well-demonstrated on UCR benchmarks, but generalization to other time series domains (e.g., medical, sensor) and larger models remains untested. The defense's r% threshold of 5% is effective for 10% poisoning but lacks validation across different poisoning ratios.
- **Low confidence:** The exact implementation details of trigger patterns (fixed, random, powerline) are missing, as they are cited from [15] without specification. The identity and size of the external dataset $D'$ are not provided, making it impossible to reproduce the adversarial synthesis step.

## Next Checks

1. **Distribution Coverage Validation:** Systematically vary the external dataset $D'$ (e.g., different UCR datasets, synthetic time series) and measure the class-wise coverage of adversarial samples $D_{adv}$ in the model's latent space. Confirm that multi-class coverage correlates with high ASR and preserved CA.

2. **Capacity Stress Test:** Evaluate TrojanTime on highly pruned models (e.g., 80% sparsity) or small architectures (e.g., 2-layer TCN) to determine the minimum model capacity required for successful backdoor injection without clean accuracy degradation.

3. **Defense Robustness Analysis:** Test the proposed defense across a range of poisoning ratios (1%, 5%, 10%, 20%) and trigger types (fixed, blended). Measure the false positive rate (clean samples incorrectly isolated) and false negative rate (backdoor samples missed) to identify the r% threshold's robustness limits.