---
ver: rpa2
title: 'SoundReactor: Frame-level Online Video-to-Audio Generation'
arxiv_id: '2510.02110'
source_url: https://arxiv.org/abs/2510.02110
tags:
- audio
- generation
- diffusion
- video
- inproc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of frame-level online video-to-audio
  (V2A) generation, where a model generates audio from video in an autoregressive
  manner without access to future video frames. This is motivated by applications
  in live content creation and generative world models that require real-time audio
  generation.
---

# SoundReactor: Frame-level Online Video-to-Audio Generation

## Quick Facts
- arXiv ID: 2510.02110
- Source URL: https://arxiv.org/abs/2510.02110
- Reference count: 40
- Key outcome: SoundReactor achieves frame-level online video-to-audio generation with low latency (26.3ms per frame) and high audio quality on gameplay videos.

## Executive Summary
This paper introduces SoundReactor, the first framework for frame-level online video-to-audio generation, where audio is generated in real-time from video frames without access to future frames. The task is motivated by applications in live content creation and generative world models. SoundReactor uses DINOv2 grid features with temporal differences for vision conditioning, continuous audio latents via VAE for autoregressive modeling, and diffusion pre-training with consistency fine-tuning for low per-frame latency. On a benchmark of gameplay videos, it achieves low latency (26.3ms with NFE=1) and high-quality, temporally aligned stereo audio generation.

## Method Summary
SoundReactor is designed for frame-level online video-to-audio generation, enabling real-time audio synthesis from video without future frame access. The framework leverages DINOv2 grid features with temporal differences for vision conditioning, continuous audio latents via VAE for autoregressive modeling, and diffusion pre-training with consistency fine-tuning to achieve low per-frame latency. The model processes 30FPS, 480p videos on a single H100 GPU, generating high-quality, semantically and temporally aligned stereo audio. It outperforms offline autoregressive models on objective and human evaluations for gameplay videos.

## Key Results
- SoundReactor achieves low per-frame waveform-level latency (26.3ms with NFE=1, 31.5ms with NFE=4) on 30FPS, 480p videos.
- It outperforms offline AR V2A models on objective and human evaluations for gameplay videos.
- The model generates high-quality, semantically and temporally aligned full-band stereo audio under an end-to-end causal constraint.

## Why This Works (Mechanism)
SoundReactor’s design enables real-time audio generation by using grid features from DINOv2 with temporal differences for efficient vision conditioning, continuous audio latents via VAE for autoregressive modeling, and diffusion pre-training with consistency fine-tuning for low per-frame latency. The framework’s causal constraint ensures that audio generation is performed frame-by-frame without access to future video frames, making it suitable for live applications.

## Foundational Learning
- **DINOv2 Grid Features**: Visual features extracted using DINOv2, a self-supervised vision model, are used to condition audio generation. These features capture rich semantic information from video frames.
  - Why needed: To provide robust visual context for audio generation.
  - Quick check: Verify that DINOv2 features capture relevant semantic and temporal information from gameplay videos.

- **Temporal Differences**: Differences between consecutive DINOv2 features are used to capture motion and temporal dynamics in the video.
  - Why needed: To encode temporal information crucial for audio generation.
  - Quick check: Ensure temporal differences effectively represent motion in gameplay videos.

- **Continuous Audio Latents via VAE**: Audio signals are converted to continuous latent representations using a VAE, enabling autoregressive modeling.
  - Why needed: To allow autoregressive modeling of audio in a continuous space.
  - Quick check: Confirm that VAE latents preserve audio quality and enable efficient generation.

## Architecture Onboarding
- **Component Map**: DINOv2 Grid Features -> Temporal Differences -> VAE Audio Latents -> Diffusion Model -> Audio Output
- **Critical Path**: Vision encoding (DINOv2 + temporal differences) -> Audio latent generation (VAE) -> Diffusion-based audio synthesis
- **Design Tradeoffs**: The use of DINOv2 grid features and temporal differences balances computational efficiency with semantic richness. Continuous audio latents via VAE enable autoregressive modeling but may introduce latency. Diffusion pre-training and consistency fine-tuning optimize for low per-frame latency but require careful hyperparameter tuning.
- **Failure Signatures**: Poor temporal alignment between audio and video, audio artifacts, or high latency may indicate issues with feature extraction, latent representation, or diffusion model training.
- **First Experiments**:
  1. Test DINOv2 feature extraction on diverse video types to ensure semantic richness.
  2. Evaluate VAE latent representation quality for audio preservation.
  3. Measure latency and audio quality trade-offs for different NFE settings.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but highlights the need for further exploration of the model’s generalizability to non-gameplay video domains and its robustness to complex audio environments.

## Limitations
- The model’s performance is evaluated only on gameplay videos, limiting generalizability to other video types.
- The model’s ability to handle videos with multiple overlapping sound sources or complex audio environments is unclear.
- The reliance on specific architectural choices (DINOv2 grid features, continuous audio latents via VAE) may limit flexibility and adaptability to other frameworks.

## Confidence
- **High**: The novelty of the frame-level online V2A task and the technical feasibility of achieving low-latency audio generation with SoundReactor.
- **Medium**: The superiority of SoundReactor over offline AR V2A models, as this depends on the specific benchmarks and evaluation metrics used.
- **Low**: The generalizability of the model to non-gameplay video domains, given the limited scope of the evaluation corpus.

## Next Checks
1. **Generalization Testing**: Evaluate SoundReactor on a diverse set of video types (e.g., real-world footage, movies, sports) to assess its adaptability beyond gameplay videos.
2. **Robustness to Complex Audio Environments**: Test the model’s ability to handle videos with multiple overlapping sound sources or complex audio environments, such as crowded scenes or dynamic soundscapes.
3. **Human Evaluation Transparency**: Provide detailed methodology for human evaluations, including participant demographics, evaluation criteria, and inter-rater reliability metrics, to ensure the robustness of subjective quality assessments.