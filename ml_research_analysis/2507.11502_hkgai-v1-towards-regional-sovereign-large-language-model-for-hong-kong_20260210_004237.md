---
ver: rpa2
title: 'HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong'
arxiv_id: '2507.11502'
source_url: https://arxiv.org/abs/2507.11502
tags:
- hong
- alignment
- kong
- hkgai-v1
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HKGAI-V1 is a 685-billion-parameter sovereign large language model
  tailored for Hong Kong's multilingual and culturally specific context. Built on
  DeepSeek architecture, it employs full-parameter fine-tuning, retrieval-augmented
  generation, and advanced alignment techniques including reinforcement learning from
  human feedback and language feedback modeling.
---

# HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong

## Quick Facts
- arXiv ID: 2507.11502
- Source URL: https://arxiv.org/abs/2507.11502
- Authors: Sirui Han; Junqi Zhu; Ruiyuan Zhang; Yike Guo
- Reference count: 40
- Primary result: 685B-parameter sovereign LLM achieving 81.4% HKMMLU accuracy and 88.95% Beaver-zh-hk safety score

## Executive Summary
HKGAI-V1 is a 685-billion-parameter large language model specifically engineered for Hong Kong's multilingual and culturally distinct context. Built on DeepSeek architecture, it employs full-parameter fine-tuning, retrieval-augmented generation, and advanced alignment techniques including reinforcement learning from human feedback and language feedback modeling. The model addresses the challenge of maintaining sovereignty and cultural specificity in AI systems while meeting Hong Kong's unique legal and ethical requirements under the "one country, two systems" framework.

The system demonstrates superior performance on domain-specific evaluations, achieving state-of-the-art results in culturally sensitive tasks. By integrating a proprietary Adversarial HK Value Benchmark and specialized alignment workflows, HKGAI-V1 establishes new benchmarks for regional AI alignment while highlighting the inherent tension between local compliance and general robustness against adversarial attacks.

## Method Summary
HKGAI-V1 is constructed through a multi-stage training pipeline starting with DeepSeek architecture and full-parameter fine-tuning on multilingual Hong Kong-specific corpora. The model undergoes reinforcement learning from human feedback with a Bradley-Terry reward model and KL regularization, followed by language feedback learning that incorporates self-evolving refinement. A proprietary Adversarial HK Value Benchmark ensures alignment with local ethical and legal standards. The system integrates modular retrieval-augmented generation with intent classification, query enhancement, and moderation workflows. Training involves human preference pairs, language feedback data, and a proprietary Q-A-C dataset with local annotator corrections.

## Key Results
- Achieved 81.4% accuracy on HKMMLU, demonstrating strong performance on Hong Kong-specific multilingual tasks
- Scored 88.95% on Beaver-zh-hk safety benchmark, showing superior alignment with local cultural values
- Attained 80.1% on SafeLawBench, establishing new state-of-the-art for regional AI alignment

## Why This Works (Mechanism)
The model's effectiveness stems from its comprehensive alignment strategy that combines multiple techniques: full-parameter fine-tuning captures Hong Kong-specific linguistic patterns, RLHF with KL regularization ensures policy stability while learning from human preferences, and language feedback modeling provides continuous refinement through self-correction. The modular RAG pipeline allows dynamic knowledge retrieval while maintaining safety through integrated moderation workflows. The proprietary Adversarial HK Value Benchmark provides rigorous testing against local sensitive scenarios, ensuring the model responds appropriately to Hong Kong-specific ethical and legal contexts.

## Foundational Learning
- **Full-parameter fine-tuning**: Why needed - Adapts the base DeepSeek model to Hong Kong's multilingual context; Quick check - Verify parameter updates preserve base capabilities while improving local task performance
- **Reinforcement Learning from Human Feedback**: Why needed - Aligns model outputs with human preferences and local values; Quick check - Test reward model calibration on held-out preference pairs
- **Language Feedback Modeling**: Why needed - Enables continuous refinement and self-correction of responses; Quick check - Measure improvement in language-following consistency across iterations
- **Retrieval-augmented generation**: Why needed - Provides access to up-to-date knowledge while maintaining context relevance; Quick check - Validate RAG retrieval accuracy on domain-specific queries
- **KL regularization**: Why needed - Prevents policy collapse during RL training by maintaining distribution stability; Quick check - Monitor KL divergence between policy updates
- **Bradley-Terry reward modeling**: Why needed - Ranks human preferences effectively for pairwise comparison; Quick check - Evaluate reward model performance on preference prediction accuracy

## Architecture Onboarding

**Component Map**
Base Model (DeepSeek) -> Full-Parameter Fine-Tuning -> RLHF Pipeline -> Language Feedback Learning -> HKValue-Aligner -> Modular RAG Integration -> Adversarial HK Value Benchmark

**Critical Path**
The essential workflow follows: pretraining/fine-tuning → RLHF alignment → language feedback refinement → RAG integration → safety evaluation. Each stage builds upon the previous, with RLHF serving as the core alignment mechanism that's subsequently refined through language feedback and validated against the proprietary benchmark.

**Design Tradeoffs**
The architecture prioritizes cultural alignment over general robustness, accepting higher vulnerability to adversarial attacks in exchange for superior performance on Hong Kong-specific tasks. The modular RAG design enables flexible knowledge retrieval but introduces potential latency and complexity. The proprietary benchmark ensures local compliance but limits external reproducibility and validation.

**Failure Signatures**
Misaligned safety behavior on HK-specific sensitive queries indicates reward model miscalibration or annotator guideline inconsistency. Language drift or inconsistent Cantonese responses suggest insufficient multilingual training data or biased reward preferences. Poor generalization beyond Hong Kong contexts may result from overfitting to proprietary datasets.

**First Experiments**
1. Evaluate model on HKMMLU benchmark to verify domain-specific performance claims
2. Test safety responses on Adversarial HK Value Benchmark to assess local alignment quality
3. Measure language-following consistency across Cantonese, Mandarin, and English inputs

## Open Questions the Paper Calls Out

**Open Question 1**
How can models optimize for high compliance with regional socio-legal norms without compromising robustness against general adversarial instruction attacks? The paper identifies a classic tension between compliance and robustness, noting that while HKGAI-V1 achieved 79% safety on Hong Kong-sensitive queries, it exhibited higher vulnerability (16.5% unsafe) to general instruction attacks compared to baselines. This trade-off suggests current alignment techniques may create new attack surfaces that degrade performance on general safety benchmarks.

**Open Question 2**
How can the definition of "local values" be systematized to represent a diverse population rather than the specific viewpoints of a limited group of annotators? The authors acknowledge the inherent subjectivity in "aligning to Hong Kong values," noting that the process relied on local annotators whose views may not represent the full spectrum of opinions within such a diverse and dynamic society. Current alignment relies on static human feedback that captures a snapshot of specific annotator demographics.

**Open Question 3**
How can a sovereign AI maintain consistent value alignment across multilingual inputs when distinct languages carry different cultural and political connotations? The paper identifies language as a complicating factor in alignment, stating that achieving precise multilingual alignment and cross-lingual consistency is crucial for legitimacy, as each language carries distinct cultural values in Hong Kong's context. Standard multilingual training often treats languages as interchangeable, whereas language choice correlates with different political or cultural stances.

## Limitations
- Proprietary datasets (Q-A-C corpus, Adversarial HK Value Benchmark) lack transparent construction protocols, making independent verification impossible
- Model shows higher vulnerability to general adversarial instruction attacks (16.5% unsafe) compared to general-purpose models, indicating a trade-off between local compliance and universal robustness
- Claims of cultural alignment depend critically on subjective safety judgments from a limited annotator pool without demographic representativeness data

## Confidence

**High confidence**: Technical implementation of DeepSeek-based full-parameter fine-tuning and RLHF with KL regularization follows established methodologies with reproducible results on standard benchmarks.

**Medium confidence**: Domain-specific evaluation scores (HKMMLU, Beaver-zh-hk, SafeLawBench) are methodologically sound but rely on internal datasets whose reproducibility cannot be independently assessed.

**Low confidence**: Claims of cultural alignment and "one country, two systems" compliance depend critically on opaque proprietary benchmarks and subjective safety judgments that cannot be externally validated.

## Next Checks
1. Release anonymized demographic and procedural details of the Q-A-C annotator pool to enable external assessment of cultural representativeness
2. Conduct ablation studies removing HK-specific RAG components to isolate their contribution to claimed performance gains
3. Submit the model to independent third-party safety audits using standardized multilingual benchmarks to verify cross-cultural robustness beyond proprietary HK-centric evaluations