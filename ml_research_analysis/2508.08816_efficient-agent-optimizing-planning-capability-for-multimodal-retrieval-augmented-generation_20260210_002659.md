---
ver: rpa2
title: 'Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented
  Generation'
arxiv_id: '2508.08816'
source_url: https://arxiv.org/abs/2508.08816
tags:
- mrag
- search
- planning
- retrieval
- remplan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: E-Agent addresses the limitations of existing multimodal retrieval-augmented
  generation (mRAG) systems by introducing a dynamic planning framework that reduces
  redundant searches and improves accuracy. The approach combines a mRAG planner that
  performs contextual analysis of both text and images in a single forward pass, and
  a task executor that implements optimized workflows.
---

# Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2508.08816
- Source URL: https://arxiv.org/abs/2508.08816
- Reference count: 35
- Key outcome: Achieves 13% accuracy gains over state-of-the-art mRAG methods while reducing redundant searches by 37%

## Executive Summary
E-Agent introduces a dynamic planning framework for multimodal retrieval-augmented generation (mRAG) that addresses key limitations in existing systems through efficient tool orchestration. The approach decouples planning from execution, enabling contextual analysis of text and images in a single forward pass to reduce redundant searches while maintaining retrieval quality. The framework introduces RemPlan, the first comprehensive benchmark for evaluating dynamic mRAG planning capabilities, and demonstrates superior performance with an 8B parameter model that balances accuracy with computational efficiency.

## Method Summary
E-Agent employs a two-component architecture consisting of a fine-tuned mRAG planner and a task executor. The planner (InternVL2-8B, 8B parameters) analyzes input images and queries to generate execution plans specifying tool sequences and parameters in a single forward pass, avoiding iterative replanning loops. The task executor implements the generated plans using integrated search tools (Baidu Image Search, Tavily text search) and MLLM tools (Qwen2-VL-72B with manually written prompts). Training uses 10K image-question-plan triplets, while evaluation employs the RemPlan benchmark with 200 samples annotated for planning quality metrics including tool precision/recall, plan accuracy, and parameter correctness.

## Key Results
- 13% accuracy improvement over state-of-the-art mRAG methods on multimodal benchmarks
- 37% reduction in redundant tool searches through one-time planning strategy
- Plan accuracy of 0.86 (E-Agent-sft) vs 0.32 (baseline) on RemPlan benchmark
- 0.85 IS-Precision and 0.93 TS-Precision for tool selection accuracy

## Why This Works (Mechanism)

### Mechanism 1: One-Time Planning Decoupling
Separating planning from execution in a single forward pass reduces redundant tool invocations while maintaining retrieval quality. The mRAG planner analyzes both visual and textual inputs once, generating a complete execution trajectory before any tools are invoked. This eliminates iterative re-planning loops that characterize feedback-dependent systems. Core assumption: Query characteristics can be sufficiently analyzed upfront to determine optimal tool sequence without intermediate verification.

### Mechanism 2: Contextual Tool Selection via Supervised Planning
A supervised fine-tuned planner (8B parameters) can learn to discriminate between four question types and select appropriate tool combinations. Training on 10K annotated image-question-plan triples teaches the planner to recognize when: (1) no retrieval needed (fundamental questions), (2) image search only (visual recognition), (3) text search only (information-seeking), (4) both searches required (multi-faceted). Core assumption: Training distribution covers sufficient variation in real-world query patterns for generalization.

### Mechanism 3: Hierarchical Evaluation Driving Architecture Design
Disentangled evaluation metrics (plan accuracy, tool precision/recall, parameter semantics) reveal that planning quality, not just answer accuracy, determines system effectiveness. RemPlan benchmark annotates ground-truth plans alongside answers, enabling direct assessment of whether the planner correctly identifies necessary tools independent of downstream execution quality. Core assumption: Good plans correlate with good answers, but can be measured separately for debugging and iteration.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Understanding how external knowledge retrieval augments language model capabilities is prerequisite to grasping why planning matters for multimodal RAG.
  - Quick check question: Can you explain why a language model might need external retrieval for time-sensitive queries?

- **Concept: Agent Planning Patterns (Single-Pass vs. Iterative)**
  - Why needed here: E-Agent's core innovation is rejecting iterative replanning; understanding this design choice requires knowing what iterative planning costs.
  - Quick check question: What computational overhead does OmniSearch's iterative approach introduce?

- **Concept: Tool Orchestration / Function Calling**
  - Why needed here: The framework treats image search, text search, and MLLM tools as callable functions with parameters; understanding sequencing is essential.
  - Quick check question: For a "multi-faceted question" requiring both visual recognition and current information, what tool sequence would E-Agent plan?

## Architecture Onboarding

- Component map: [Image + Query] → [mRAG Planner (InternVL2-8B, fine-tuned)] → [mRAG Plan: tool sequence + parameters] → [Task Executor] → [Image Search, Text Search, MLLM Tools] → [Aggregated Response]

- Critical path: Planner training data quality → plan accuracy → tool selection precision → answer quality. The planner is the bottleneck; executor is straightforward dispatch.

- Design tradeoffs:
  - **Single-pass planning vs. iterative refinement**: Gains efficiency (37% fewer searches) but sacrifices ability to recover from bad intermediate results.
  - **8B planner vs. larger model**: Reduces computation but may limit reasoning complexity; paper claims this is sufficient for their benchmark.
  - **Supervised training vs. few-shot**: SFT improves metrics substantially (Plan-acc 0.86 vs 0.32) but requires annotation investment.

- Failure signatures:
  - Planner fails to recognize retrieval need → returns "I don't know" equivalent
  - Planner correctly plans but search tool returns wrong results → propagated errors
  - Out-of-distribution query types → degraded tool selection precision

- First 3 experiments:
  1. **Reproduce baseline comparison on RemPlan subset**: Implement the four question-type classification and measure your system's Plan-acc, IS-P, TS-P against Table 2 baselines.
  2. **Ablate planner size**: Test whether InternVL2-8B is necessary or if smaller planners maintain acceptable Plan-acc on Type 1-2 questions (fundamental, visual-recognition).
  3. **Stress-test retrieval noise robustness**: Introduce synthetic noise into search results and measure answer degradation rate; this reveals dependence on search tool quality the paper acknowledges but doesn't quantify.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hierarchical planning architectures with intermediate validation checkpoints be integrated into the E-Agent framework to better handle complex multi-hop reasoning tasks?
- Basis in paper: The authors identify the "one-shot planning mechanism" as a limitation for complex multi-hop reasoning and suggest developing "hierarchical planning architectures" with "fine-grained plan adjustment."
- Why unresolved: The current design explicitly decouples planning from execution to maximize efficiency, lacking mechanisms for intermediate verification or plan refinement during execution.
- What evidence would resolve it: A study comparing the performance of the current one-shot planner against a hierarchical planner on multi-hop reasoning benchmarks, measuring both accuracy and computational overhead.

### Open Question 2
- Question: Can dynamic reflection modules be integrated for real-time plan optimization without re-introducing the error propagation and latency issues found in existing iterative approaches?
- Basis in paper: The conclusion suggests integrating "dynamic reflection modules" for "real-time plan optimization based on retrieval feedback loops."
- Why unresolved: The paper specifically criticizes feedback-dependent systems (like OmniSearch) for error propagation and latency; resolving this contradiction with a reflection module is an open challenge.
- What evidence would resolve it: Implementation of a reflection-augmented E-Agent that demonstrates higher accuracy without significantly increasing the number of tool calls or latency compared to the baseline E-Agent.

### Open Question 3
- Question: What mechanisms are required to enable adaptive toolkit management, allowing the agent to autonomously incorporate emerging multimodal interfaces?
- Basis in paper: The authors note the framework's dependence on "predefined toolkits" limits long-term adaptability and propose researching "adaptive toolkit management mechanisms."
- Why unresolved: The current mRAG planner is fine-tuned on a fixed set of tools (Image Search, Text Search, Requery, Response), and its ability to generalize to unseen tools is not demonstrated.
- What evidence would resolve it: Experiments demonstrating the planner's ability to successfully invoke and utilize new, unseen tools introduced after the initial training phase.

## Limitations
- The framework's performance gains are measured on benchmarks that may not fully capture real-world query complexity, particularly multi-hop reasoning scenarios
- Limited analysis of out-of-distribution performance or robustness to search tool failures beyond case studies
- The claim that 8B parameters provide optimal computational efficiency without sacrificing planning quality for complex queries lacks comprehensive stress testing across diverse domains

## Confidence
- **High Confidence**: The one-time planning strategy's efficiency benefits and the basic claim that supervised fine-tuning improves plan accuracy over few-shot methods are well-supported by ablation results (Plan-acc: 0.86 vs 0.32).
- **Medium Confidence**: Claims about the framework's generalizability to real-world applications are supported by benchmark results but limited by the controlled nature of RemPlan's 200 samples and the specific tool ecosystem.
- **Low Confidence**: The assertion that 8B parameters provide optimal computational efficiency without sacrificing planning quality for complex queries is based on a single benchmark without systematic ablation across parameter scales.

## Next Checks
1. **Out-of-Distribution Stress Test**: Evaluate E-Agent on questions requiring multi-hop reasoning or novel tool combinations not represented in the 10K training set to quantify performance degradation and identify planner brittleness points.

2. **Cross-Domain Generalization**: Test the framework's planning accuracy when applied to domains outside the training distribution (e.g., medical imaging, legal documents) to validate the claim of broad applicability.

3. **Search Tool Failure Analysis**: Systematically inject errors into search results (varying levels of noise) and measure how planner quality and final answer accuracy degrade, providing quantitative bounds on the framework's robustness to tool failures.