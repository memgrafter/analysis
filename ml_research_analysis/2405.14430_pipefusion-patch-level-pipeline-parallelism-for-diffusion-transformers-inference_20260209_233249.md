---
ver: rpa2
title: 'PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference'
arxiv_id: '2405.14430'
source_url: https://arxiv.org/abs/2405.14430
tags:
- pipefusion
- diffusion
- should
- parallel
- does
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PipeFusion is a patch-level pipeline parallelism method for diffusion\
  \ transformer (DiT) inference that partitions both model layers and input patches\
  \ across GPUs to reduce communication costs and memory usage. By exploiting temporal\
  \ redundancy between consecutive diffusion steps, PipeFusion reuses stale feature\
  \ maps to provide context, achieving communication costs of only 2O(p\xD7hs)/N versus\
  \ 4O(p\xD7hs)L/P for tensor parallelism and 2O(p\xD7hs)L/P for sequence parallelism."
---

# PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference

## Quick Facts
- arXiv ID: 2405.14430
- Source URL: https://arxiv.org/abs/2405.14430
- Authors: Jiarui Fang; Jinzhe Pan; Aoyu Li; Xibo Sun; Jiannan Wang
- Reference count: 40
- One-line primary result: PipeFusion achieves 1.16-1.55× speedup over sequence parallelism with 32-36% memory usage reduction across SD3 and Flux.1 models

## Executive Summary
PipeFusion introduces a novel patch-level pipeline parallelism method for diffusion transformer inference that partitions both model layers and input patches across GPUs. The key innovation is leveraging temporal redundancy between consecutive diffusion steps by reusing stale feature maps to provide context, which significantly reduces communication costs and memory usage. PipeFusion achieves communication costs of only 2O(p×hs)/N versus 4O(p×hs)L/P for tensor parallelism and 2O(p×hs)L/P for sequence parallelism, while maintaining comparable image quality (FID scores).

## Method Summary
PipeFusion implements patch-level pipeline parallelism by distributing both the model's layers and input patches across multiple GPUs. The method exploits temporal redundancy in diffusion steps by reusing feature maps from previous steps, reducing the need for constant communication between GPUs. This approach is particularly effective for diffusion transformers where each step builds upon previous computations. The system maintains a buffer of feature maps that can be reused when temporal redundancy is detected, minimizing redundant computations while ensuring output quality remains comparable to baseline methods.

## Key Results
- Achieves 1.16-1.55× speedup over sequence parallelism on 8×L40 PCIe GPUs
- Reduces memory usage to 32-36% of sequence parallelism for large models
- Communication overhead drops to 4.6% of total latency at 4096px resolution versus 17.9% for sequence parallelism

## Why This Works (Mechanism)
PipeFusion works by exploiting the inherent temporal redundancy in diffusion transformer inference processes. During diffusion, consecutive steps often produce similar feature maps due to the gradual nature of image generation. By recognizing this redundancy, PipeFusion can reuse stale feature maps instead of recomputing them, dramatically reducing both communication overhead and memory requirements. The patch-level parallelism allows for finer-grained distribution of work across GPUs compared to traditional layer or sequence parallelism approaches.