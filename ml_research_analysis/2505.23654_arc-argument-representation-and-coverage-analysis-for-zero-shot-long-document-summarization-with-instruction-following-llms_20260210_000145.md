---
ver: rpa2
title: 'ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document
  Summarization with Instruction Following LLMs'
arxiv_id: '2505.23654'
source_url: https://arxiv.org/abs/2505.23654
tags:
- argument
- roles
- summary
- coverage
- canlii
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARC, a framework for evaluating how well
  long-context LLM summaries preserve salient arguments. ARC decomposes argument roles
  into atomic facts and measures coverage hierarchically, distinguishing between missing
  and factually incorrect content.
---

# ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs

## Quick Facts
- arXiv ID: 2505.23654
- Source URL: https://arxiv.org/abs/2505.23654
- Reference count: 40
- Primary result: ARC reveals systematic biases in LLM summarization, including positional and role-specific biases, and correlates strongly with human judgments of coverage quality.

## Executive Summary
ARC introduces a hierarchical framework for evaluating long-document LLM summaries by decomposing argument roles into atomic facts and measuring coverage at multiple levels. It distinguishes between missing and factually incorrect content, providing interpretable diagnostics for improving summarization in high-stakes domains like legal opinions and scientific articles. Results show LLMs frequently omit critical arguments, especially those positioned in the middle of documents, and systematically favor certain argument roles (e.g., Conclusions) over others. ARC correlates strongly with human judgments and offers actionable insights for enhancing summarization quality.

## Method Summary
ARC evaluates LLM-generated summaries by first decomposing salient argument roles (e.g., Issues, Reasons, Conclusions in legal opinions) into atomic facts using an LLM with entailment filtering. An LLM judge then labels each fact as supported, missing, or non-factual against the summary. Coverage is aggregated hierarchically: fact-level decisions feed into role-level ARCrole scores, which then aggregate to a summary-level ARCscore. The framework is applied to two domains: CANLII (1049 legal cases) and DRI (40 scientific articles), using eight open-weight LLMs for summarization.

## Key Results
- LLM summaries exhibit U-shaped positional bias, favoring content at document start and end, leading to systematic omission of middle-positioned arguments.
- Role-specific bias is observed, with Conclusions consistently better covered than Issues or Reasons in legal opinions.
- ARC correlates strongly with human judgments (Kendall's τ > 0.4) and outperforms traditional n-gram and embedding metrics on coverage evaluation.
- Larger models generally achieve better coverage, but even the best-performing models fail to capture a significant portion of salient arguments.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing argument roles into atomic facts enables interpretable, fine-grained coverage evaluation that correlates better with human judgments than n-gram or embedding-based metrics.
- **Mechanism:** ARC first decomposes each salient argument role rᵢ into atomic facts Fᵣ = {f₁, f₂, ..., f|Fᵣ|} using an LLM with entailment filtering. An indicator function δ(fᵢ, S) labels each fact as supported (1) or not (0) against the summary S. Role-level coverage ARCrole aggregates fact-level scores; summary-level ARCscore aggregates role-level scores. This hierarchical decomposition provides multi-level interpretability (which roles fail, which facts are missing vs. contradicted).
- **Core assumption:** Atomic facts can be reliably decomposed and entailment-checked by an LLM judge without introducing systematic bias; gold salience annotations accurately reflect what should appear in summaries.
- **Evidence anchors:**
  - [abstract] "ARC provides an interpretable lens by distinguishing between different information types to be covered and by separating omissions from factual errors."
  - [section 4.1–4.2] Formal definition of ARCrole and ARCscore; δ function distinguishes missing vs. non-factual errors.
  - [corpus] StrucSum (arXiv:2505.22950) similarly uses structural decomposition for summarization, suggesting decomposition-based evaluation is a broader trend, but direct validation of ARC's specific hierarchical scoring is limited to this paper.
- **Break condition:** If decomposition quality varies significantly across domains or if LLM judges exhibit systematic bias toward/against certain argument types, ARC scores may not generalize.

### Mechanism 2
- **Claim:** LLM summarization exhibits U-shaped positional bias—favoring content at document start and end—which systematically reduces coverage of salient arguments positioned in the middle.
- **Mechanism:** The paper maps source sentences included in summaries using a greedy lexical selection algorithm, then analyzes the relative positions of argumentative sentences. Pearson correlation between mean argument position and ARCscore quantifies positional bias effects. Negative correlations indicate arguments positioned later (or in the middle) are less likely to be covered.
- **Core assumption:** Lexical greedy selection approximates which source sentences the LLM actually attended to; argument positions in the source are not confounded by role-frequency or length effects.
- **Evidence anchors:**
  - [abstract] "Positional bias (U-shaped context window) and role-specific biases... systematically affect argument inclusion."
  - [section 5.3, Table 7] Consistent negative correlation in CANLII (ρ up to -0.369, p<0.05 for some models); weaker/non-significant in DRI where argument distribution differs.
  - [corpus] Prior work (Liu et al., 2024a; Ravaut et al., 2024) documents U-shaped bias in LLM context utilization, but ARC's specific quantification for argument coverage is novel here.
- **Break condition:** If documents have different structural conventions (e.g., scientific articles with dense argumentation throughout), positional bias effects may be attenuated or reversed.

### Mechanism 3
- **Claim:** LLMs exhibit role-specific bias, systematically favoring certain argument roles (e.g., Conclusions) over others (e.g., Issues, Reasons) even when controlling for position and length.
- **Mechanism:** The paper defines a bias score βᵣ = (1 − ARCroleᵣ) · 1/log(1 + |r|ᴰ/|args|ᴰ), where higher β indicates worse coverage relative to role frequency. Length- and position-controlled analyses isolate role-specific effects from confounds.
- **Core assumption:** Normalization by role frequency appropriately corrects for frequency skew without over-penening dominant roles; the bias score captures true preference rather than artifact.
- **Evidence anchors:**
  - [abstract] "Role-specific biases—favoring conclusions over issues or reasons—systematically affect argument inclusion."
  - [section 5.4, Figure 3] Conclusions consistently have lowest β (best coverage) in CANLII; Background Claims are underrepresented in DRI. Effect persists with length/position controls.
  - [corpus] ArgCMV (arXiv:2508.19580) addresses argument summarization benchmarks but does not directly validate role-specific coverage bias; corpus evidence for this mechanism is limited.
- **Break condition:** If role definitions vary across domains or annotators, bias scores may not be comparable; role-specific bias may reflect training data patterns rather than inherent model limitations.

## Foundational Learning

- **Concept: Atomic Fact Decomposition**
  - **Why needed here:** ARC's core evaluation relies on breaking argument roles into verifiable atomic units. Without understanding how to decompose claims into minimal, independently verifiable facts, the hierarchical scoring mechanism cannot be correctly implemented or debugged.
  - **Quick check question:** Given the argument "The court dismissed the application because substantial compliance with the Criminal Code was found," can you identify three distinct atomic facts? (Answer: (1) An application was dismissed; (2) The dismissal was based on substantial compliance; (3) The compliance was with the Criminal Code.)

- **Concept: Positional Bias in Transformer Attention**
  - **Why needed here:** Interpreting ARC's positional bias analysis requires understanding why transformer-based LLMs attend more strongly to beginning and end tokens, and how RoPE/ALiBi positional encodings interact with long-context inputs.
  - **Quick check question:** Why might an LLM with a 128K context window still miss information in the middle of a 50K-token document? (Answer: U-shaped attention patterns—positional encoding combined with attention head specialization causes reduced effective attention to middle positions, even within the context window.)

- **Concept: Entailment-Based Evaluation**
  - **Why needed here:** ARC's δ function uses entailment to distinguish supported, missing, and contradicted facts. Understanding NLI-style entailment (vs. mere lexical overlap) is essential for implementing and interpreting fact-level decisions.
  - **Quick check question:** A summary states "The warrant was upheld" while the source fact is "The warrant was upheld because the justice acted within her jurisdiction." Is this supported, missing, or non-factual? (Answer: Supported—the core claim is entailed, even if the justification is omitted. Missing would apply if the summary contradicted or failed to convey the core claim.)

## Architecture Onboarding

- **Component map:**
  1. Argument Role Extractor -> Fact Decomposer -> Fact-Level Evaluator (δ) -> Coverage Aggregator -> Bias Analyzer

- **Critical path:**
  1. Ensure gold argument role annotations or validate automatic extraction quality.
  2. Decompose arguments → atomic facts; verify entailment filtering removes over-generated facts.
  3. Run fact-level evaluation; audit δ decisions for systematic errors (e.g., judge favoring summary language over source semantics).
  4. Aggregate to ARCrole and ARCscore; compare against baseline metrics on human-annotated subset.

- **Design tradeoffs:**
  - Proprietary vs. open-weight judges: GPT-4o provides highest correlation with expert judgments in this study, but DeepSeek-R1-Distill-Qwen14B achieves comparable performance at lower cost. Open-weight models avoid API deprecation risk.
  - Recall-only vs. precision-aware evaluation: ARC emphasizes recall (coverage of salient facts); Appendix J shows precision-recall tradeoffs exist but do not drastically reorder model rankings.
  - Domain-specific vs. general argument roles: IRC scheme (Issues, Reasons, Conclusions) is legal-specific; DRI uses a modified Toulmin model. Role definitions must align with domain conventions.

- **Failure signatures:**
  - Low ARCscore with low missing-fact rate but high non-factual rate → judge may be overly strict on entailment or summaries contain hallucinations.
  - High variance in ARCrole across roles with similar position/distribution → potential role-specific judge bias.
  - Disagreement between GPT-4o and DeepSeek judges on fact-level decisions → decomposition may be ambiguous or judge prompts need refinement.

- **First 3 experiments:**
  1. Reproduce benchmark correlations: On the 87-article CANLII subset with expert annotations, compute ARCscore (both judges) and compare to ROUGE, BERTScore, SummaC, and FactScore. Verify Kendall's τ > 0.4 vs. expert average.
  2. Positional bias validation: On CANLII and DRI, compute Pearson correlation between mean argument position and ARCscore for each model. Confirm negative correlation in CANLII (p<0.05 for larger models) and weaker effect in DRI.
  3. Role-specific bias audit: Compute β scores with and without frequency normalization for all models. Confirm Conclusions have lowest β in CANLII; verify effect persists under length control (±20% word count) and position control (80% of arguments in first/last 20% of document).

## Open Questions the Paper Calls Out

- **Question:** Can incorporating explicit argument structures into LLM training or prompting strategies systematically improve argument coverage in generated summaries?
  - **Basis:** Conclusion states: "Future work can extend this framework by incorporating explicit argument structures into training or prompting, and by leveraging ARC's interpretable outputs to guide targeted improvements in summarization."
  - **Why unresolved:** The paper only evaluates zero-shot summarization without argument-aware supervision. Preliminary experiments with argument-aware prompts (Appendix G) showed inconsistent improvements, suggesting deeper alignment strategies are needed.
  - **What evidence would resolve it:** Experiments fine-tuning LLMs on argument-annotated corpora or using structured argument representations as intermediate planning steps, evaluated via ARC across domains.

- **Question:** How can ARC be extended to jointly assess precision and recall to capture over-inclusion of non-salient content in summaries?
  - **Basis:** Limitations section states: "ARC currently emphasizes recall... this ignores precision, i.e., over-inclusion of non-salient content. Future work should jointly assess both dimensions, for instance via a harmonic mean, for a fuller view of coverage quality."
  - **Why unresolved:** ARC evaluates whether salient facts are covered but does not penalize summaries for including irrelevant or non-argumentative content.
  - **What evidence would resolve it:** A modified ARC framework that decomposes summary content into atomic facts and measures the proportion that map to gold salient arguments versus extraneous information.

- **Question:** Can ARC generalize to other high-stakes argumentative domains beyond legal opinions and scientific articles?
  - **Basis:** Limitations section states: "The small size of DRI (40 documents) also limits generalizability, motivating larger, rigorously annotated corpora—e.g., debates or financial texts—where arguments are central."
  - **Why unresolved:** ARC has only been validated on CANLII (legal) and DRI (scientific), which differ in argument density and distribution patterns.
  - **What evidence would resolve it:** Evaluation on new corpora with argument role annotations (e.g., policy debates, financial reports), measuring ARC correlation with human judgments in these domains.

## Limitations
- ARC relies on LLM-based decomposition and evaluation, which may introduce systematic bias and reduce transparency in decision-making.
- The framework emphasizes recall over precision, ignoring over-inclusion of non-salient content in summaries.
- ARC has only been validated on two domains (legal opinions and scientific articles), limiting generalizability to other argumentative contexts.

## Confidence
- **High:** Positional bias findings replicate prior work on transformer attention patterns and are supported by strong statistical evidence across multiple models.
- **Medium:** Role-specific bias depends on domain-specific role definitions and normalization assumptions that may not generalize.
- **Low:** Direct head-to-head comparisons showing ARC correlates better with human judgments than n-gram or embedding metrics are limited to a single dataset subset.

## Next Checks
1. Validate ARC's decomposition and evaluation pipeline on a third domain (e.g., legal contracts or policy documents) with expert-annotated argument roles.
2. Conduct ablation studies comparing GPT-4o and DeepSeek-R1-Distill-Qwen-14B judgments on the same fact sets to quantify judge-specific bias.
3. Test whether ARC's hierarchical scoring remains stable when using automatically extracted (vs. gold) argument roles, quantifying the impact on coverage estimates.