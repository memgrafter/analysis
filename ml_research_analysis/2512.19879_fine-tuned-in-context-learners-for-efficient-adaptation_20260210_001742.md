---
ver: rpa2
title: Fine-Tuned In-Context Learners for Efficient Adaptation
arxiv_id: '2512.19879'
source_url: https://arxiv.org/abs/2512.19879
tags:
- examples
- training
- performance
- data
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates adaptation methods for large language models
  (LLMs) in data-scarce scenarios, comparing in-context learning (ICL), fine-tuning
  (FT), and a unified approach (ICL+FT) that fine-tunes on k-shot in-context prompts.
  A key contribution is the introduction of prequential evaluation for efficient hyperparameter
  selection, eliminating the need for separate validation sets.
---

# Fine-Tuned In-Context Learners for Efficient Adaptation

## Quick Facts
- arXiv ID: 2512.19879
- Source URL: https://arxiv.org/abs/2512.19879
- Reference count: 28
- Primary result: ICL+FT consistently matches or exceeds ICL-Only and FT-Only performance in data-scarce scenarios, often with 3-10× fewer examples needed.

## Executive Summary
This paper introduces ICL+FT, a unified approach that combines in-context learning (ICL) and fine-tuning (FT) by fine-tuning on k-shot in-context prompts. The method demonstrates superior sample efficiency compared to either paradigm alone, particularly in low-data regimes. A key contribution is the prequential evaluation protocol for hyperparameter selection, which eliminates the need for separate validation sets. Experiments on Big-Bench Hard and NLP tasks show that ICL+FT achieves up to 72.3% accuracy on BBH tasks with just 30 examples, outperforming ICL-Only with much larger models.

## Method Summary
ICL+FT fine-tunes the model on task-specific data augmented with in-context examples that mimic k-shot prompts. During training, each example is first evaluated (test point) then used for gradient updates (training point) in a prequential protocol. The model is trained on sequences containing k context examples plus the target, with loss computed on all response tokens. Prequential evaluation accumulates cumulative loss across iterations, providing an efficient model selection criterion without held-out validation sets. The approach uses Adafactor optimizer with hyperparameter sweeps over learning rates and epochs.

## Key Results
- ICL+FT with 2B model often outperforms ICL-Only with 27B model on BBH tasks
- Achieves up to 72.3% accuracy on BBH tasks with only 30 training examples
- Prequential hyperparameter selection proves robust, with minimal performance degradation when using fixed hyperparameters
- LoRA adaptation yields similar results to full fine-tuning with memory savings (78.7%→78.9% on BBH with 150 examples)
- K=1 in-context example provides largest marginal gain; diminishing returns beyond K=5

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on k-shot ICL prompt structures yields superior sample efficiency compared to either paradigm alone. During training, gradients are computed across the entire k-shot sequence (context examples + target), enabling the model to learn how to use in-context information while simultaneously adapting weights to the task distribution. The loss is placed on all responses y_i in the sequence. The model can jointly optimize (a) task-specific knowledge in weights and (b) improved utilization of in-context examples, producing complementary learning signals.

### Mechanism 2
Prequential evaluation enables data-efficient hyperparameter selection without held-out validation sets. Each example serves sequentially as test point (prediction evaluated) then training point (gradient step taken). The cumulative prequential loss provides a model selection criterion grounded in MDL theory, converging to a lower bound on expected held-out performance for i.i.d. data. This sequential approach is more computationally efficient than k-fold cross-validation for gradient-based methods.

### Mechanism 3
ICL+FT handles label shifts more robustly than FT-Only by preserving in-context adaptation capacity. FT-Only encodes task mappings directly into weights, which conflicts with pre-training biases when labels are inverted. ICL-Only can override biases via context but cannot scale. ICL+FT maintains both capacities—gradient updates adapt weights while k-shot structure preserves contextual flexibility, allowing the model to override biases through in-context examples.

## Foundational Learning

- **In-Context Learning (ICL)**: Understanding ICL's ability to generalize from few examples without weight updates is essential to appreciating why combination with FT helps. Quick check: Can you explain why ICL performance saturates as the number of in-context examples increases, even within context window limits?

- **Gradient-based Fine-tuning Sample Efficiency**: The paper's central claim is that ICL+FT requires 3-10× fewer examples than ICL-Only for equivalent performance. Understanding why FT alone underperforms in low-data regimes (overfitting, pre-training bias conflict) clarifies the hybrid advantage. Quick check: Why might standard fine-tuning on 10 examples perform worse than ICL on 10 examples, despite gradient updates?

- **Prequential/Forward Validation**: The protocol differs from k-fold CV and standard train/val splits. Understanding the theoretical justification (MDL, marginal likelihood connection) ensures correct implementation and interpretation. Quick check: How does prequential evaluation differ from leave-one-out cross-validation, and why is it more computationally efficient for gradient-based methods?

## Architecture Onboarding

- **Component map**: Training loop (sequential iteration over dataset) -> Prompt construction (k-shot context + target) -> Prequential tracker (cumulative loss) -> Hyperparameter search (LR, epochs, K)
- **Critical path**: 1) Implement sequential data loader that respects causal ordering (context only from D_{<i}) 2) Construct k-shot prompt with loss masking on response tokens only 3) Run full prequential sweep across hyperparameter grid 4) Select configuration with lowest cumulative prequential loss 5) Retrain on full dataset with selected hyperparameters or use final prequential checkpoint
- **Design tradeoffs**: Full fine-tuning vs. LoRA (LoRA yields similar results with memory savings); K selection (diminishing returns beyond K=5; K=1 provides largest marginal gain); Per-dataset vs. global hyperparameters (ICL+FT robust to global settings, FT-Only degrades significantly)
- **Failure signatures**: ICL-Only outperforming ICL+FT (likely hyperparameter mismatch or insufficient epochs); Prequential loss not correlating with held-out performance (verify i.i.d. assumption); Performance degradation on translation tasks (consider ICL-Only for low-resource→English directions)
- **First 3 experiments**: 1) Replicate BBH Navigate task with 30 examples using paper's hyperparameter grid; verify ICL+FT achieves ~65-70% accuracy vs. ~40% ICL-Only baseline 2) Ablate K ∈ {0, 1, 3, 5} to confirm K=1→5 transition and plateau; plot test accuracy vs. K 3) Test flipped-label scenario to verify ICL+FT handles label shift while ICL-Only degrades severely

## Open Questions the Paper Calls Out

### Open Question 1
Does the ICL+FT approach retain its performance advantages in data-abundant regimes, or does the benefit diminish as standard fine-tuning reaches saturation? The authors explicitly limit their investigation to "data-scarce" scenarios (30–150 examples) and note that ICL-Only plateaus while FT-Only scales well, leaving the interaction in high-data settings untested.

### Open Question 2
Does the relative efficiency of ICL+FT persist, widen, or narrow when applied to models significantly larger than the 27B parameters tested? While the study tests Gemma-2 (2B–27B) and Qwen-3 (up to 4B), the authors do not speculate on how the trade-off between in-context retrieval and weight-based storage changes for frontier-scale models (e.g., 100B+).

### Open Question 3
Is there a principled, deterministic method for selecting the optimal number of in-context examples ($k$) for training and inference? The authors state that experiments varying $k$ revealed "significant variability across datasets and training set sizes, with no consistently discernible patterns."

### Open Question 4
How does the order of training data impact the reliability of the prequential hyperparameter selection protocol? The authors note that "subsampling process is deterministic" to minimize variance, yet prequential evaluation relies on a specific sequence; the sensitivity of the final selected hyperparameters to this ordering is not fully characterized.

## Limitations
- Mechanism validation remains incomplete—the exact mechanism by which gradient updates on k-shot prompts preserve in-context learning capacity needs ablation studies
- Data ordering assumptions untested—prequential evaluation assumes i.i.d. data, but real-world non-stationary distributions could degrade effectiveness
- Task coverage limited—results primarily shown on BBH and standard NLP benchmarks, with limited exploration of tasks where ICL+FT might fail

## Confidence

**High Confidence**: ICL+FT consistently outperforms or matches baselines across multiple datasets and model scales. The prequential hyperparameter selection method reliably identifies good configurations without separate validation sets. The memory efficiency gains of LoRA adaptation are reproducible.

**Medium Confidence**: The claimed 3-10× sample efficiency improvement relative to ICL-Only, while supported by extensive results, depends on the specific task and data regime. The exact hyperparameters that work across tasks are robust but not universally optimal.

**Low Confidence**: The mechanism explanation for why ICL+FT handles label shifts better than FT-Only (preserving contextual flexibility) is plausible but not directly validated through targeted ablation studies. The theoretical connection between prequential loss and held-out performance, while grounded in MDL theory, lacks empirical validation across non-i.i.d. distributions.

## Next Checks

1. **Ablation on Prompt Structure**: Run experiments with K=0 (pure FT), K=1 (minimal ICL influence), and K=5 (full ICL influence) across multiple tasks to precisely characterize the marginal benefit of each additional in-context example. This would validate whether the mechanism truly combines both learning paradigms rather than simply interpolating between them.

2. **Non-i.i.d. Data Testing**: Apply ICL+FT to temporally ordered data (e.g., news classification with temporal concept drift) and compare prequential vs. standard train/val/test splits for hyperparameter selection. This would stress-test the core assumption underlying the efficiency claim.

3. **Label Shift Stress Test**: Systematically vary the degree of label shift (0% to 100% flipped) on a representative task and measure the breakdown points for ICL-Only, FT-Only, and ICL+FT. This would directly validate the mechanism claim about preserving contextual flexibility under distributional shifts.