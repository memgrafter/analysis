---
ver: rpa2
title: Can Large Language Models Still Explain Themselves? Investigating the Impact
  of Quantization on Self-Explanations
arxiv_id: '2601.00282'
source_url: https://arxiv.org/abs/2601.00282
tags:
- quantization
- language
- faithfulness
- quality
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how quantization affects the quality and\
  \ faithfulness of self-explanations (SEs) generated by large language models (LLMs).\
  \ Two types of SEs\u2014natural language explanations (NLEs) and counterfactual\
  \ examples\u2014are evaluated under three quantization techniques and varying bit\
  \ widths across six LLMs."
---

# Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations

## Quick Facts
- arXiv ID: 2601.00282
- Source URL: https://arxiv.org/abs/2601.00282
- Authors: Qianli Wang; Nils Feldhus; Pepa Atanasova; Fedor Splitt; Simon Ostermann; Sebastian Möller; Vera Schmitt
- Reference count: 40
- Primary result: Quantization causes moderate declines in both SE quality (up to 4.4%) and faithfulness (up to 2.38%)

## Executive Summary
This study investigates how quantization affects the quality and faithfulness of self-explanations (SEs) generated by large language models (LLMs). Two types of SEs—natural language explanations (NLEs) and counterfactual examples—are evaluated under three quantization techniques and varying bit widths across six LLMs. Results show that quantization leads to moderate declines in SE quality (up to 4.4%) and faithfulness (up to 2.38%). NLEs are more sensitive to quantization than counterfactual examples. Larger models maintain better faithfulness under quantization, while smaller models experience more significant quality degradation. No quantization method consistently excels across task performance, SE quality, and faithfulness. User studies confirm that full-precision models generate more trustworthy and coherent SEs than quantized ones. Despite these degradations, quantization remains an effective compression strategy, though application-specific validation is recommended for high-stakes use cases.

## Method Summary
The study evaluates six LLMs (Llama3 8B/70B, Qwen2.5 7B/14B/32B/72B) across three quantization methods (GPTQ, AWQ, BitsAndBytes) at 4-bit and 8-bit precision. Two self-explanation types are generated: Natural Language Explanations using ZeroCoT prompts and Counterfactual Examples using FIZLE framework. The evaluation uses three datasets (eSNLI, HealthFC, AG News) with automatic quality metrics (BARTScore, TIGERScore, LFR, PPL, TS) and faithfulness metrics (counterfactual test, biasing features, CC-SHAP). Human evaluations assess trustworthiness and coherence. Results are averaged over three runs with different seeds.

## Key Results
- Quantization causes moderate declines in SE quality (up to 4.4%) and faithfulness (up to 2.38%)
- NLEs are more sensitive to quantization than counterfactual examples
- Larger models maintain better faithfulness under quantization but show limited resilience in SE quality
- No single quantization method consistently excels across task performance, SE quality, and faithfulness
- Human evaluation confirms full-precision models generate more trustworthy and coherent SEs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Quantization reduces SE quality by introducing weight precision loss that degrades coherence and reasoning fidelity.
- **Mechanism**: Lower bit-width quantization approximates full-precision weights through truncation/rounding, disrupting learned representations critical for generating coherent explanations.
- **Core assumption**: Self-explanation generation relies on fine-grained weight precision that is disrupted by quantization-induced distribution shifts.
- **Evidence anchors**: [abstract] "quantization typically leads to moderate declines in both SE quality (up to 4.4%) and faithfulness (up to 2.38%)"; [section 5.1.1] "NLE quality reduction varies more substantially in smaller models but remains less affected in larger models"
- **Break condition**: When bit-width remains at 8-bit or higher for larger models (≥32B), quality degradation may be negligible (<1%).

### Mechanism 2
- **Claim**: Larger models exhibit greater faithfulness preservation under quantization due to representational redundancy.
- **Mechanism**: Models with more parameters contain redundant circuits and pathways that can compensate for precision loss, maintaining alignment between predictions and explanations even when some weights are perturbed.
- **Core assumption**: Faithfulness relies on consistent activation patterns that larger models can preserve through redundant pathways.
- **Evidence anchors**: [abstract] "larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness"; [section 5.1.1] "larger models demonstrate greater robustness to quantization in preserving NLE faithfulness"
- **Break condition**: When quantization exceeds model's redundancy capacity (e.g., 4-bit on 7B models), faithfulness drops notably.

### Mechanism 3
- **Claim**: NLEs are more sensitive to quantization than CFEs due to higher reasoning complexity requirements.
- **Mechanism**: NLEs require the model to articulate its internal decision process in natural language, demanding more sophisticated self-reflection capabilities that depend on precise weight representations.
- **Core assumption**: Explanation complexity correlates with quantization sensitivity.
- **Evidence anchors**: [abstract] "NLEs are more sensitive to quantization than counterfactual examples"; [section 5.3] "NLEs exhibit greater sensitivity, while CFEs demonstrate relative robustness to quantization"
- **Break condition**: When task complexity is low and CFE generation is well-defined, sensitivity gap narrows.

## Foundational Learning

- **Concept: Self-Explanations (SEs)**
  - Why needed here: Central to the paper's investigation; understanding what SEs are and their types (NLEs vs CFEs) is essential.
  - Quick check question: Can you distinguish between NLEs and CFEs in terms of what they explain about model decisions?

- **Concept: Post-Training Quantization (PTQ)**
  - Why needed here: The quantization techniques evaluated (GPTQ, AWQ, BitsAndBytes) are all PTQ methods that compress models without retraining.
  - Quick check question: How does PTQ differ from quantization-aware training (QAT) in terms of implementation complexity?

- **Concept: Faithfulness vs Quality in Explanations**
  - Why needed here: The paper evaluates both dimensions separately; faithfulness measures alignment with actual model reasoning, while quality measures human-perceived usefulness.
  - Quick check question: Why might a high-quality explanation still be unfaithful to the model's actual decision process?

## Architecture Onboarding

- **Component map**: Input → [Full-precision LLM] → Prediction + SE → [Quantization Layer] (GPTQ/AWQ/bnb) → [Quantized LLM] → Prediction + SE (degraded) → [Evaluation Pipeline] (Quality/Faithfulness metrics/Human evaluation)

- **Critical path**: Model selection → Quantization method choice → Bit-width decision → SE type selection → Evaluation metric configuration

- **Design tradeoffs**:
  - Larger quantized models (32B+) better preserve faithfulness but increase inference cost
  - Higher bit-width (8-bit) preserves quality better than 4-bit but offers less compression
  - GPTQ8 best for task performance, AWQ best for SE quality, bib8 best for faithfulness (no single winner)

- **Failure signatures**:
  - NLE quality drops >4%: Likely using 4-bit quantization on smaller models (<14B)
  - Faithfulness drops >2%: Check if using Llama3 models at 4-bit (more susceptible)
  - LLM-as-a-Judge disagrees with humans: Judge evaluation unreliable for quantization impact assessment

- **First 3 experiments**:
  1. **Baseline comparison**: Run full-precision model on eSNLI/HealthFC to establish SE quality and faithfulness baselines using both NLEs and CFEs.
  2. **Quantization sweep**: Apply GPTQ4, GPTQ8, AWQ, and bib8 to same model, measure degradation across all metrics to identify optimal bit-width for your use case.
  3. **Scale sensitivity test**: Compare 7B vs 32B vs 72B models under identical quantization to determine if larger quantized models outperform smaller full-precision models for your specific SE requirements.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the degradation of self-explanation quality and faithfulness due to quantization persist uniformly across multilingual settings, or does it vary by language?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section that their work is confined to English datasets and that "extending experiments to the multilingual setting is considered as future work."
  - Why unresolved: The study only evaluates English datasets (eSNLI, HealthFC, AG News); however, prior work suggests quantization affects languages differently, making cross-lingual generalization uncertain.
  - What evidence would resolve it: Replicating the study's evaluation framework on multilingual benchmarks (e.g., XNLI) and comparing the degradation rates of NLEs and CFEs across diverse languages.

- **Open Question 2**: How do alternative compression techniques, such as weight-activation quantization, KV cache compression, or Quantization-Aware Training (QAT), impact the faithfulness of self-explanations?
  - Basis in paper: [explicit] The authors limit their scope to weight-only post-training quantization (PTQ) and list "weight-activation quantization, KV cache compression, or quantization-aware training techniques" as specific areas for future work.
  - Why unresolved: Different compression strategies preserve model weights and activations differently; it is unclear if the moderate faithfulness degradation observed in weight-only PTQ applies to methods that target activations or the KV cache.
  - What evidence would resolve it: Comparative experiments evaluating self-explanation metrics on models compressed using the listed alternative techniques versus the PTQ methods (GPTQ, AWQ, bib) used in this study.

- **Open Question 3**: Why do LLM-as-a-Judge evaluations fail to align with human evaluation when assessing the coherence and trustworthiness of quantized self-explanations?
  - Basis in paper: [inferred] The results show that while judge models achieve high inter-rater agreement, they show weak or negative correlation with human annotators regarding the degradation caused by quantization.
  - Why unresolved: The paper establishes the misalignment but does not investigate the specific artifacts of quantization that humans detect (loss of coherence/trust) but automated judges miss.
  - What evidence would resolve it: An analysis of the linguistic features flagged by humans versus judges, followed by the development of calibrated prompts or fine-tuning objectives to bridge this evaluation gap.

## Limitations
- The study focuses on only three quantization methods (GPTQ, AWQ, BitsAndBytes) and evaluates only 4-bit and 8-bit precision, potentially missing performance characteristics of other PTQ approaches or emerging quantization techniques.
- The generalizability of results to domains beyond the three evaluated tasks (NLI, fact-checking, topic classification) remains uncertain, as the findings may not extend to highly specialized domains requiring domain-specific knowledge or reasoning patterns.
- Human evaluation sample sizes and demographic diversity are not specified, potentially limiting the external validity of subjective quality assessments.

## Confidence
- **High confidence**: The finding that quantization causes moderate declines in both SE quality (up to 4.4%) and faithfulness (up to 2.38%) is well-supported by comprehensive evaluation across six models, three quantization methods, and two SE types.
- **Medium confidence**: The observation that larger models better maintain faithfulness under quantization while showing limited resilience in SE quality is supported but could benefit from additional model scale comparisons.
- **Low confidence**: The mechanism explanations for why NLEs are more sensitive than CFEs to quantization lack direct empirical validation in the paper.

## Next Checks
1. **Cross-domain generalization test**: Evaluate the same quantization framework on domain-specific datasets (medical, legal, scientific) to assess whether observed degradation patterns hold across specialized knowledge domains.
2. **Alternative quantization method comparison**: Implement and evaluate additional PTQ approaches (e.g., QLoRA, GPTQ variants) and mixed-precision quantization to determine if the "no single winner" finding holds across a broader method spectrum.
3. **Long-sequence performance analysis**: Test models on tasks requiring longer context (e.g., multi-document summarization, extended reasoning chains) to determine if quantization effects scale differently for complex, extended reasoning tasks compared to the relatively short-form explanations evaluated here.