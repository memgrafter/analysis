---
ver: rpa2
title: Fusing Reward and Dueling Feedback in Stochastic Bandits
arxiv_id: '2504.15812'
source_url: https://arxiv.org/abs/2504.15812
tags:
- feedback
- dueling
- regret
- reward
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies fusing reward and dueling feedback in stochastic
  multi-armed bandits (DR-MAB), where both feedback types are gathered in each decision
  round. The authors derive a regret lower bound showing that an efficient algorithm
  may incur only the smaller among the reward and dueling-based regret for each individual
  arm.
---

# Fusing Reward and Dueling Feedback in Stochastic Bandits

## Quick Facts
- **arXiv ID**: 2504.15812
- **Source URL**: https://arxiv.org/abs/2504.15812
- **Reference count**: 40
- **One-line primary result**: Two fusion algorithms (ElimFusion, DecoFusion) for stochastic bandits with reward and dueling feedback; DecoFusion achieves near-optimal regret matching the lower bound up to a constant.

## Executive Summary
This paper addresses the problem of fusing reward and dueling feedback in stochastic multi-armed bandits (DR-MAB), where both feedback types are gathered in each decision round. The authors derive a regret lower bound showing that an efficient algorithm may incur only the smaller among the reward and dueling-based regret for each individual arm. They propose two fusion approaches: ElimFusion, a simple elimination fusion algorithm that leverages both feedback types to explore all arms, and DecoFusion, a decomposition fusion algorithm that selects the more effective feedback to explore the corresponding arms and randomly assigns one feedback type for exploration and the other for exploitation in each round. Extensive experiments confirm the efficacy of the algorithms and theoretical results.

## Method Summary
The paper proposes two algorithms for DR-MAB: ElimFusion and DecoFusion. ElimFusion maintains a shared candidate arm set and eliminates arms based on either reward or dueling feedback, whichever is triggered first. DecoFusion decomposes arms into subsets based on their relative effectiveness under each feedback type and uses a randomized strategy to assign exploration and exploitation roles. Both algorithms use confidence bounds and empirical estimates to track arm performance. The methods are evaluated on a synthetic dataset with 16 Bernoulli arms and a specified 16×16 dueling probability matrix.

## Key Results
- Derived regret lower bound for DR-MAB showing that efficient algorithms may incur only the smaller among reward and dueling-based regret for each arm.
- Proposed ElimFusion, a simple elimination fusion algorithm that achieves regret sublinear in time but with a suboptimal multiplicative factor of the number of arms.
- Proposed DecoFusion, a decomposition fusion algorithm that achieves regret matching the lower bound up to a constant under a common assumption.
- Extensive experiments confirm the efficacy of the algorithms and theoretical results.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fusing feedback via a shared candidate arm set (ElimFusion) can accelerate the identification of suboptimal arms compared to independent elimination processes.
- **Mechanism**: The algorithm maintains a single candidate set $C$. An arm is eliminated if it is statistically determined to be suboptimal by *either* the reward feedback (lower mean) *or* the dueling feedback (losing probability > 0.5). This allows the "faster" feedback type to prune the space for the other, reducing the total pulls needed to isolate the optimal arm.
- **Core assumption**: The optimal arm is a Condorcet winner (wins all duels) and possesses the highest reward mean, ensuring consistency between the two elimination criteria.
- **Evidence anchors**:
  - [abstract] "...unifies collected information by sharing a common candidate arm set."
  - [section 3.1] "...maintains a single candidate arm set C, and arms in this set can be eliminated according to either reward or dueling feedback, whichever is first triggered."
- **Break condition**: If the Condorcet winner assumption fails (i.e., the arm with the highest reward is not the arm that wins the most duels), the shared set logic may eliminate the globally optimal arm based on conflicting signals.

### Mechanism 2
- **Claim**: Randomized assignment of feedback types to exploration and exploitation tasks (DecoFusion) resolves the "decomposition deadlock" caused by unknown reward/dueling gaps.
- **Mechanism**: Because the optimal decomposition of arms (into those better explored by reward vs. dueling) depends on unknown gaps, the algorithm cannot deterministically assign the perfect feedback type. Instead, it uses a randomized policy: with probability $\frac{\alpha^2}{\alpha^2+(1-\alpha)^2}$, it explores via reward and exploits via dueling; otherwise, it reverses these roles. This ensures that over time, the algorithm converges to the lower-cost feedback type for each arm without requiring prior knowledge of the gaps.
- **Core assumption**: The randomized sampling probability effectively compensates for the quadratic dependence of information measures on the gaps ($1/\Delta^2$).
- **Evidence anchors**:
  - [abstract] "...randomly assigns one feedback type for exploration and the other for exploitation in each round."
  - [section 4.2.2] "To tackle the mismatch... we devise a randomized decision-making strategy."
- **Break condition**: If the variance in feedback is extremely high or the time horizon $T$ is too short, the randomized strategy may fail to converge to the optimal decomposition, incurring linear regret.

### Mechanism 3
- **Claim**: Constructing exploration sets based on empirical log-likelihoods (DecoFusion) allows the algorithm to approximate the theoretical lower bound without knowing the true system parameters.
- **Mechanism**: The algorithm estimates the "information collected" for each arm using revised empirical log-likelihoods ($\hat{I}$). It constructs sets $\hat{K}^{(R)}$ and $\hat{K}^{(D)}$ by checking if $\hat{I}$ is below a threshold ($\log t + f(K)$). This conservative construction ensures that arms requiring more information are kept in the exploration set, while those sufficiently identified are moved to exploitation, effectively sorting arms by their "ease of learning" via specific feedback types.
- **Core assumption**: The most effective arm to duel with for exploration ($\ell^*_k$) is the optimal arm 1 (Assumption in Corollary 2.4).
- **Evidence anchors**:
  - [section 4.2.1] "...utilize the revised information measures... to construct the arm sets."
  - [section 4.1] "The lower bound suggests a decomposition... DecoFusion leverages this insight by approximating this arm decomposition."
- **Break condition**: If the assumption that $\ell^*_k = 1$ fails (i.e., the optimal arm is not the most informative comparator), the regret bounds degrade, and the decomposition logic may inefficiently allocate exploration resources.

## Foundational Learning

- **Concept**: **Multi-Armed Bandits (MAB) & Regret**
  - **Why needed here**: The entire paper is framed as an extension of stochastic MAB. You must understand the baseline trade-off between exploration (learning $\mu_k$) and exploitation (maximizing reward) to grasp why "fusing" feedback is non-trivial.
  - **Quick check question**: If an algorithm simply picked the arm with the highest empirical mean every round, why would it fail (and incur high regret)?

- **Concept**: **Dueling Bandits (Relative Feedback)**
  - **Why needed here**: The paper introduces "dueling feedback" (comparisons) alongside rewards. Understanding that dueling bandits operate on a preference matrix $\nu$ rather than scalar rewards is crucial for the "Decomposition" mechanism.
  - **Quick check question**: In a dueling bandit, what does the "Condorcet Winner" represent, and why is finding it harder than finding the max mean in standard bandits?

- **Concept**: **Kullback-Leibler (KL) Divergence**
  - **Why needed here**: The theoretical lower bounds and the DecoFusion algorithm rely on "empirical log-likelihoods" which are functions of KL-divergence.
  - **Quick check question**: Why do the theoretical bounds use KL-divergence $kl(\mu_k, \mu_1)$ instead of simple Euclidean distance $|\mu_k - \mu_1|$?

## Architecture Onboarding

- **Component map**: Arm set $K$ -> Candidate set $C$ (ElimFusion) OR Decomposition sets $\hat{K}^{(R)}_t, \hat{K}^{(D)}_t$ (DecoFusion) -> Empirical means $\hat{\mu}$ (reward) and preference probabilities $\hat{\nu}$ (dueling) -> Elimination/Exploration decisions

- **Critical path**: The **Arm Decomposition Update** in DecoFusion (Lines 5-9, Algo 2). If the information thresholds ($\hat{I} < \log t + f(K)$) are miscalculated or if the sets $\hat{K}^{(R)}$ and $\hat{K}^{(D)}$ diverge too far from the optimal decomposition, the randomized policy will fail to converge.

- **Design tradeoffs**:
  - **ElimFusion**: Easier to implement (essentially running two standard bandits on one set). **Con**: Suffers a suboptimal $K$ multiplicative factor in regret due to the intrinsic suboptimality of dueling elimination.
  - **DecoFusion**: Achieves optimal regret (up to a constant). **Con**: High implementation complexity; requires maintaining complex "information" stats and handling the "deadlock" where sets might overlap incorrectly.

- **Failure signatures**:
  - **ElimFusion**: Regret scales linearly with the number of arms $K$ even if the gaps are large. (Check if $R_T \propto K$ in logs).
  - **DecoFusion**: Regret does not drop to a constant when $\alpha=0$ or $\alpha=1$. (Indicates the "free exploration" mechanism is broken).
  - **General**: The algorithm eliminates the optimal arm (verified against ground truth).

- **First 3 experiments**:
  1.  **Baseline Fusion Test**: Run ElimFusion against two independent bandit algorithms (Reward-only and Duel-only). Verify that ElimFusion's regret is strictly lower than the sum of the independent regrets on a synthetic dataset ($K=10$, varied gaps).
  2.  **Parameter $\alpha$ Sweep (DecoFusion)**: Run DecoFusion across $\alpha \in [0, 1]$. Plot Regret vs. $\alpha$. Verify that regret minimizes at the extremes ($\alpha=0, 1$) achieving "constant" regret (as per Figure 1).
  3.  **Gap Sensitivity**: Create a case where reward gaps $\Delta^{(R)}_k$ are tiny but dueling gaps $\Delta^{(D)}_k$ are large. Verify that DecoFusion correctly identifies this and primarily uses Dueling feedback for exploration (check the ratio of $N_k$ vs $M_{k,\ell}$).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can an algorithm be designed to close the regret gap when the most effective dueling competitor for a suboptimal arm is not the optimal arm ($\ell^*_k \neq 1$)?
- **Basis in paper**: [inferred] Section 4.3 states that DecoFusion’s regret is near-optimal only under the assumption that $\ell^*_k = 1$. The authors note that without this assumption, the regret bounds are worse than the general lower bound (Theorem 2.3), an issue persisting from prior dueling bandit literature.
- **Why unresolved**: DecoFusion relies on dueling with the estimated optimal arm, which is statistically inefficient if a suboptimal arm is actually the best comparator for distinguishing certain arms.
- **What evidence would resolve it**: An algorithm achieving a regret upper bound that matches the general lower bound in Theorem 2.3 for all instances, or a proof that the gap is fundamental.

### Open Question 2
- **Question**: How can the DR-MAB fusion framework be extended to handle unbounded or non-Bernoulli reward and dueling distributions?
- **Basis in paper**: [explicit] Footnote 2 on Page 4 states that the choice of Bernoulli distributions is for simplicity and suggests extending the assumption to distributions with bounded interval support "via more sophisticated analysis."
- **Why unresolved**: The current theoretical analysis and confidence radius calculations rely on concentration properties specific to Bernoulli or bounded variables, which may not hold for unbounded distributions.
- **What evidence would resolve it**: A theoretical regret analysis for DecoFusion under sub-Gaussian or heavy-tailed distribution assumptions.

### Open Question 3
- **Question**: Can the fusion of reward and dueling feedback be effectively generalized to complex settings like linear bandits or reinforcement learning?
- **Basis in paper**: [inferred] The introduction highlights RLHF and LLMs as key motivations, noting the lack of literature on fusing these feedback types. However, the technical contributions are strictly confined to stochastic multi-armed bandits.
- **Why unresolved**: Extending the "decomposition fusion" strategy to continuous action spaces or stateful environments introduces challenges regarding credit assignment and the definition of dueling probabilities.
- **What evidence would resolve it**: A formulation of linear bandits with fused feedback and a corresponding algorithm with provable regret bounds.

## Limitations
- The analysis relies on the strong assumption that the optimal arm is a Condorcet winner, which may not hold in real-world scenarios.
- The confidence intervals for the theoretical lower bound constants and their tightness remain unverified.
- The practical impact of the suboptimal $K$ multiplicative factor in ElimFusion on high-dimensional problems is not fully characterized.

## Confidence
- **High**: The regret bounds for DecoFusion achieving near-optimal performance (matching the lower bound up to a constant) are well-supported by the theoretical analysis.
- **Medium**: The effectiveness of the randomized decomposition strategy in DecoFusion is supported by theory but requires empirical validation under diverse problem structures.
- **Low**: The assumption that the optimal arm is always the most informative comparator for exploration may not hold in all practical applications, potentially degrading DecoFusion's performance.

## Next Checks
1. **Break Assumption Test**: Design a synthetic DR-MAB instance where the Condorcet winner is not the arm with the highest reward mean. Run both ElimFusion and DecoFusion to verify if the algorithms fail to identify the globally optimal arm.
2. **Scaling Experiment**: Evaluate ElimFusion and DecoFusion on problems with large $K$ (e.g., $K=100$) and varying gap structures. Quantify the impact of the $K$ multiplicative factor on ElimFusion's regret.
3. **Alternative Comparator Test**: Modify DecoFusion to use a fixed arm (not the optimal arm) as the default exploration comparator. Compare the regret to the original version to assess the sensitivity to the optimal comparator assumption.