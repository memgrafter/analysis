---
ver: rpa2
title: Holistic Utility Preference Learning for Listwise Alignment
arxiv_id: '2410.18127'
source_url: https://arxiv.org/abs/2410.18127
tags:
- ranking
- preference
- responses
- sorting
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DRPO, a novel listwise preference optimization
  method for aligning large language models with human preferences using ranking preference
  data. Unlike existing pairwise approaches, DRPO optimizes NDCG, a standard learning-to-rank
  metric, to better capture the holistic ranking relationships among multiple responses.
---

# Holistic Utility Preference Learning for Listwise Alignment

## Quick Facts
- **arXiv ID:** 2410.18127
- **Source URL:** https://arxiv.org/abs/2410.18127
- **Reference count:** 40
- **Primary result:** DRPO achieves up to 6.13% improvement in GPT-4 win rate and 8.98% in reward model win rate compared to strong baselines like DPO and LiPO.

## Executive Summary
This paper proposes DRPO, a novel listwise preference optimization method for aligning large language models with human preferences using ranking preference data. Unlike existing pairwise approaches, DRPO optimizes NDCG, a standard learning-to-rank metric, to better capture the holistic ranking relationships among multiple responses. The method introduces an Adaptive Rank Policy Score for ranking computation and a differentiable NDCG loss (diffNDCG) enabled by differentiable sorting networks. Experiments demonstrate that DRPO significantly outperforms existing methods on Qwen and Mistral models, achieving substantial improvements in win rates against GPT-4 and reward models.

## Method Summary
DRPO reformulates preference alignment as a listwise Learning-to-Rank task by optimizing NDCG directly. The method computes an Adaptive Rank Policy Score that combines length-normalized log-likelihood with ranking-aware margin terms, then applies a differentiable odd-even sorting network to produce a soft permutation matrix. This enables computing diffNDCG, a differentiable approximation of NDCG, as the optimization objective. The approach uses ARP scores to emphasize absolute likelihood of preferred responses rather than relative likelihood ratios, and leverages differentiable sorting to capture holistic ranking relationships across all responses simultaneously.

## Key Results
- DRPO achieves up to 6.13% improvement in GPT-4 win rate and 8.98% in reward model win rate compared to strong baselines
- The listwise approach shows increasing advantage as list size grows from 2 to 16 responses
- ARP scoring function improves over Policy Reference Ratio by 3-16% in win rates across model scales
- Log-based discounting factor (1/log(1+r)) performs best for balancing top-emphasis with lower-rank consideration

## Why This Works (Mechanism)

### Mechanism 1
Direct optimization of NDCG via differentiable approximation improves preference alignment over indirect proxy losses. The paper substitutes the non-differentiable sorting operation in NDCG with a differentiable sorting network that produces a soft permutation matrix $P_{soft}$. This allows computing a relaxed score $\psi'(d, s, \hat{s}_\theta) = [P_{soft}^\top \cdot s]_d$ at each ranking position $d$, enabling gradient-based optimization of the diffNDCG loss. Core assumption: NDCG's position-weighted discounting correlates with human preference quality—users attend more to top responses.

### Mechanism 2
Maximizing absolute likelihood of preferred responses (via Adaptive Rank Policy Score) outperforms relative likelihood ratios (Policy Reference Ratio). ARP Score combines length-normalized log-likelihood with a ranking-aware margin term $\gamma(y) = \tau \cdot q(y) - \beta \cdot V_q(y)$, where $q(y)$ is rank position and $V_q(y)$ is an EMA of historical scores at that rank. This shifts focus from "policy vs. reference" ratios to directly increasing preferred response probability. Core assumption: Preferred responses require high absolute generation probability, not merely higher probability than dispreferred ones.

### Mechanism 3
Listwise optimization captures holistic ranking relationships that pairwise decomposition loses. DRPO computes scores for all K responses jointly, sorts via the differentiable network, and evaluates diffNDCG over the full ranked list. This preserves relative preference strength across all positions, unlike pairwise methods that treat each comparison independently. Core assumption: Human preference data contains graded, transitive ranking information that pairwise methods fragment.

## Foundational Learning

- **Learning-to-Rank (LTR) Metrics (NDCG):** Why needed: DRPO reformulates preference alignment as an LTR task; understanding NDCG's gain/discount structure is prerequisite to interpreting diffNDCG. Quick check: Given relevance scores [9, 3, 8, 6], why does NDCG penalize placing score 3 at position 1 more than placing score 8 at position 2?

- **Differentiable Sorting Networks:** Why needed: The core innovation uses odd-even sorting networks with soft min/max to produce permutation matrices; understanding how gradients flow through swaps is essential. Quick check: In the soft swap operation, what happens to gradient signal when $\alpha \cdot (\hat{s}_{j+1} - \hat{s}_j)$ is very large?

- **Policy Gradient vs. Direct Preference Optimization:** Why needed: DRPO builds on DPO's closed-form policy derivation but changes the optimization target; distinguishing RLHF-style reward modeling from DPO-style direct optimization clarifies the baseline. Quick check: Why does DPO eliminate the need for an explicit reward model, and what does DRPO change about the optimization target?

## Architecture Onboarding

- **Component map:** Prompt x + Responses y_1...y_K -> LLM Policy π_θ(y|x) -> ARP Score Computation (Eq. 6) -> Differentiable Odd-Even Sorting Network (K layers) -> Produces P_soft (K×K doubly stochastic matrix) -> diffNDCG Loss (Eq. 13) -> Gradient Update to θ

- **Critical path:** 1) ARP score correctly assigns higher values to preferred responses 2) Sorting network produces valid doubly stochastic $P_{soft}$ (columns sum to 1) 3) diffNDCG loss gradients propagate through soft permutation to policy parameters

- **Design tradeoffs:** Odd-even vs. bitonic networks: Odd-even O(K²) slightly outperforms bitonic O(K log²K) in Table VIII; choose based on list size. Discount factor: Logarithmic (1/log(1+r)) vs. linear/inverse—logarithmic balances top-emphasis with lower-rank consideration (Table VI). Hyperparameters: τ=0.1–0.5 stable; β=0.5–2.0 robust; θ (EMA rate) near 0.9999 favors slow adaptation (Table IX).

- **Failure signatures:** If $P_{soft}$ columns do not sum to 1, permutation approximation is invalid. If ARP score correlates poorly with ground-truth ranking (check Kendall's τ), margin formulation may be wrong. If diffNDCG plateaus but win rates don't improve, the NDCG→human preference mapping may be misaligned.

- **First 3 experiments:** 1) Verify sorting network output: For K=8 random scores, confirm $P_{soft}^\top \cdot \hat{s}_\theta$ is monotonically decreasing and columns sum to 1. 2) Ablate ARP vs. PRR: Run DRPO with both scoring functions; expect 3–16% GPT-4 win rate improvement with ARP. 3) Sweep discount factors: Compare 1/log(1+r) against 1/√r, 1/r, 1/r² on held-out reward model win rate; expect logarithmic to be most balanced.

## Open Questions the Paper Calls Out

- **Future work could explore more sophisticated reward models to better approximate human preferences.** The current study relies on standard reward models (DeBERTa, QRM) which may contain biases or discrepancies compared to actual human values. Evidence: Experiments utilizing advanced, potentially ensemble-based or human-validated reward models compared against baseline human preference datasets.

- **The paper acknowledges that discrepancies from actual human judgments may impact model performance.** The paper relies on automated proxies (GPT-4, RMs) for evaluation; the link to real human approval remains theoretically assumed but not empirically validated by human subjects in this study. Evidence: A user study where human annotators rank or rate responses generated by DRPO versus baselines (like DPO or LiPO) without automated intermediary metrics.

- **While the experiments are limited to models up to 8B parameters and list sizes up to 16, the computational complexity of the differentiable sorting network may become a bottleneck when applying DRPO to significantly larger models.** The odd-even network has $O(L^2)$ complexity, and memory constraints for storing permutation matrices may scale non-linearly with model size and sequence length in larger LLMs. Evidence: Benchmarking training time and memory consumption of DRPO on a 70B parameter model with varying list sizes to identify scaling limits.

## Limitations

- The paper's central claim depends critically on the alignment between NDCG's position-weighted discounting and actual human preference distributions, which is supported only by observed win-rate improvements, not controlled human studies.
- Reproducibility is limited by several unspecified details including initialization of the EMA buffer $V_q(y)$ for each rank and exact procedure for extending HH K=2 to K=8.
- The multimodal VLFeedback results are less well-supported, with fewer ablation studies and no win-rate improvements reported.

## Confidence

- **High confidence:** The experimental setup is clearly described, including datasets (HH, UltraFeedback, VLFeedback), metrics (GPT-4 Win Rate, RM Win Rate, MMLU, etc.), and baseline comparisons (DPO, LiPO, PiRank). The listwise advantage over pairwise methods is robust across multiple model scales (0.5B, 7B, 8B) and datasets.
- **Medium confidence:** The claim that ARP outperforms PRR is well-supported by Table IV (+16.27% on 0.5B, +3.11% on 7B), but the theoretical motivation (direct maximization of absolute likelihood) is not rigorously validated. The choice of logarithmic discount factor is justified empirically (Table VI) but lacks a formal user study linking discount shape to preference fidelity.
- **Low confidence:** The generalization of DRPO to multimodal settings (VLFeedback) is weakly supported, with no win-rate improvements reported and minimal ablation. The paper does not investigate failure modes for noisy or intransitive rankings, nor does it analyze the stability of the EMA buffer $V_q(y)$ across different data regimes.

## Next Checks

1. **Validate sorting network output:** For K=8 random scores, confirm $P_{soft}^\top \cdot \hat{s}_\theta$ is monotonically decreasing and columns sum to 1, ensuring the differentiable permutation approximates hard sorting.

2. **Ablate ARP vs. PRR:** Run DRPO with both scoring functions; expect 3–16% GPT-4 win rate improvement with ARP (per Table IV), confirming that direct maximization of absolute likelihood outperforms relative likelihood ratios.

3. **Sweep discount factors:** Compare 1/log(1+r) against 1/√r, 1/r, 1/r² on held-out reward model win rate; expect logarithmic to be most balanced, validating the empirical choice in Table VI.