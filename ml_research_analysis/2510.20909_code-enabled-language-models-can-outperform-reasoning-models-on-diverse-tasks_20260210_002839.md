---
ver: rpa2
title: Code-enabled language models can outperform reasoning models on diverse tasks
arxiv_id: '2510.20909'
source_url: https://arxiv.org/abs/2510.20909
tags:
- reasoning
- code
- codeadapt
- output
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeAdapt combines code execution with lightweight in-context learning
  to match or exceed expensive reasoning models across diverse tasks. Using iterative
  code reasoning and just five training examples per task, it enables instruct language
  models to outperform corresponding reasoning models by up to 22.9% per model and
  35.7% per task, while using 10-81% fewer tokens and 16-47% less computation time.
---

# Code-enabled language models can outperform reasoning models on diverse tasks

## Quick Facts
- arXiv ID: 2510.20909
- Source URL: https://arxiv.org/abs/2510.20909
- Authors: Cedegao E. Zhang; Cédric Colas; Gabriel Poesia; Joshua B. Tenenbaum; Jacob Andreas
- Reference count: 40
- Primary result: CodeAdapt matches or exceeds reasoning models on diverse tasks using 10-81% fewer tokens and 16-47% less computation time

## Executive Summary
CodeAdapt combines code execution with lightweight in-context learning to match or exceed expensive reasoning models across diverse tasks. Using iterative code reasoning and just five training examples per task, it enables instruct language models to outperform corresponding reasoning models by up to 22.9% per model and 35.7% per task, while using 10-81% fewer tokens and 16-47% less computation time. The system distributes reasoning across natural language for high-level planning and code for precise calculations, creating a hybrid approach that outperforms either modality alone.

## Method Summary
CodeAdapt integrates the CodeAct framework (multi-turn language and code reasoning with execution) with Generalization-Focused Learning (GFL) for in-context example selection. The system uses instruct language models to interleave natural language reasoning with Python code execution in a sandboxed environment, maintaining state across turns. GFL generates multiple candidate solutions for each training problem, then selects examples that generalize well across held-out problems rather than just achieving correct answers. This approach operates within strict resource budgets (10 turns, 16k tokens, 4 minutes per problem) to simulate real-world constraints.

## Key Results
- CodeAdapt matches or exceeds reasoning models on 7 of 8 tasks while using 10-81% fewer tokens
- Outperforms corresponding instruct models by 22.9% per model and 35.7% per task on average
- Demonstrates metacognitive behaviors like strategy adaptation and resource awareness under constraints
- Uses 16-47% less computation time than reasoning model baselines

## Why This Works (Mechanism)

### Mechanism 1
Distributing reasoning across natural language and executable code enables more robust and efficient problem-solving than either modality alone. The language module handles high-level planning, meta-reasoning, and contextual understanding while the code module externalizes precise calculations, control flow, iterative searches, and systematic verification. This modular approach allows each substrate to handle what it does best—language for abstraction, code for precision.

### Mechanism 2
Selecting in-context examples based on generalization performance rather than task-specific correctness yields more transferable reasoning strategies. GFL generates multiple solution attempts per training problem, then evaluates each candidate as a one-shot example on held-out problems from the same task. Solutions that help solve other problems are prioritized, filtering out "lucky" solutions that reached correct answers through non-generalizable shortcuts.

### Mechanism 3
Iterative execution with explicit resource budgets induces adaptive metacognitive behaviors that improve reasoning efficiency. The system constrains reasoning with explicit budgets (max 10 turns, 16k tokens, 4 minutes). Feedback after each turn displays remaining resources, prompting the model to monitor progress, switch strategies when stuck, and avoid exhaustive searches when time-constrained.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: CodeAdapt builds on and generalizes CoT; understanding baseline CoT performance is necessary to contextualize gains.
  - Quick check question: Can you explain why generating intermediate reasoning steps before an answer improves performance on multi-step problems?

- **Concept: In-Context Learning**
  - Why needed here: GFL is a specific instance of few-shot in-context learning; understanding how examples influence model behavior is prerequisite.
  - Quick check question: Why might the ordering and selection of in-context examples affect model performance?

- **Concept: Reinforcement Learning from Human Feedback (RLHF) and Reasoning Models**
  - Why needed here: CodeAdapt is positioned as an alternative to expensive RL-based reasoning model training; understanding what RL provides helps evaluate the tradeoff.
  - Quick check question: What capabilities do RL-trained reasoning models (like DeepSeek R1) exhibit that base instruct models typically lack?

## Architecture Onboarding

- **Component map:**
  - Language Module -> Code Module (Python sandbox) -> Feedback Formatter -> GFL Selector
  - GFL Selector -> Evaluation Phase (prompt with selected examples)

- **Critical path:**
  1. GFL phase: Generate M=6 solutions per training problem → score → evaluate generalization on held-out problems → select K=2 examples
  2. Evaluation phase: Load selected examples into prompt → run multi-turn reasoning loop (max 10 turns) → terminate on `<return>` or budget exhaustion

- **Design tradeoffs:**
  - M and K values: Paper uses M=6, K=2 (90 trajectories per task). Increasing M improves example quality but costs more; K=2 balances performance with prompt length.
  - Budget allocation: 10 turns × 4096 tokens × 60s per turn. Tighter budgets may fail on complex tasks; looser budgets reduce efficiency gains vs. RMs.
  - Library restrictions: No network access, limited to preloaded libraries. Safe but constrains tasks requiring external data.

- **Failure signatures:**
  - Memory errors: Combinatorial searches (e.g., `itertools.permutations` on large lists) exceed 500MB limit
  - Timeout cascades: Long-running code cells hit 60s per-turn limit, forcing strategy changes
  - Format violations: Model outputs code without proper `<code>` tags, requiring retry prompts
  - Generalization failure: GFL-selected examples help on training distribution but not test distribution

- **First 3 experiments:**
  1. Baseline comparison: Run 0-shot CoT, 0-shot CodeAct, and 2-shot CodeAdapt (GFL) on a single task (e.g., Countdown) with n=20 problems. Compare accuracy, tokens, and time against the paper's reported figures.
  2. Ablation study: Run CodeAdapt with randomly selected examples (BFL baseline) vs. GFL on the same task. Quantify the generalization-gap contribution.
  3. Budget sensitivity: Vary turn limits (2, 6, 10) and measure performance degradation. Replicate Figure 2 trends for your model/task combination.

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement learning be combined with CodeAdapt-style hybrid reasoning to achieve synergistic improvements beyond either approach alone? The paper notes this as an exciting direction, comparing CodeAdapt against RL-trained RMs but not exploring training CodeAdapt agents with RL.

### Open Question 2
How does CodeAdapt performance scale with access to additional tools such as web search, specialized libraries, or the ability to call other models? The current implementation restricts the agent to a limited set of Python libraries and no external tools.

### Open Question 3
Why does CodeAdapt underperform reasoning models on tasks like AIME, and can this gap be closed? The results show RMs still achieve higher accuracy on AIME (e.g., DeepSeek R1: 70.6% vs. CodeAdapt: 40.0%), suggesting task-specific limitations.

## Limitations
- GFL generalization score assumes training problems adequately sample the task's strategy space, which may not hold for tasks with high intra-task variability
- Evaluation relies on task-specific verifiers that may not perfectly capture task success criteria
- Memory and timeout constraints in the Python sandbox may artificially limit solution quality on problems requiring extensive computation

## Confidence

- **High confidence**: CodeAct framework enables multi-step reasoning combining language and code; GFL improves in-context example selection over random methods; token and time efficiency gains vs. reasoning models are real
- **Medium confidence**: Generalization score effectively selects transferable examples; metacognitive behaviors emerge from resource constraints; gains translate to real-world applicability beyond benchmarks
- **Low confidence**: 22.9% model-level and 35.7% task-level improvements are consistently reproducible; reasoning models' advantages on complex tasks are fully compensated by CodeAdapt; GFL-selected examples remain effective with fewer than 5 training problems

## Next Checks

1. **Ablation study**: Run CodeAdapt with randomly selected examples (BFL baseline) vs. GFL on a representative task (e.g., Countdown) with n=20 problems. Quantify the exact contribution of the generalization score to performance gains.

2. **Budget sensitivity analysis**: Vary turn limits (2, 6, 10) and measure performance degradation on complex tasks. Confirm whether models adapt strategies as budgets tighten, replicating Figure 2 trends.

3. **Memory/time stress test**: Run CodeAdapt on problems known to require extensive computation (e.g., AIME combinatorics). Document how often memory errors or timeouts occur and whether models recover gracefully or fail completely.