---
ver: rpa2
title: 'Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data'
arxiv_id: '2511.19330'
source_url: https://arxiv.org/abs/2511.19330
tags:
- adversarial
- attacks
- data
- attack
- slope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses adversarial attacks on financial time-series
  forecasting models, specifically targeting N-HiTS models trained on S&P 500 stock
  data. Two novel slope-based attack methods are introduced: General Slope Attack
  (GSA) and Least-Squares Slope Attack (LSSA), which manipulate forecast trends by
  altering the slope of predictions.'
---

# Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data

## Quick Facts
- **arXiv ID**: 2511.19330
- **Source URL**: https://arxiv.org/abs/2511.19330
- **Reference count**: 29
- **Primary result**: Slope-based attacks (GSA, LSSA) double forecast slope in upward direction and reduce it below zero in downward direction on N-HiTS models

## Executive Summary
This paper introduces two novel slope-based adversarial attack methods—General Slope Attack (GSA) and Least-Squares Slope Attack (LSSA)—targeting N-HiTS models trained on S&P 500 stock data. The attacks successfully manipulate forecast trends by altering prediction slopes, achieving significant slope increases in upward directions and reductions below zero in downward directions. The methods outperform traditional attacks (FGSM, BIM, MI-FGSM, SIM, TIM) while using only 2% relative epsilon perturbation. Additionally, the study demonstrates that these attacks can bypass standard security mechanisms, reducing a CNN discriminator's specificity to 28% and accuracy to 57%.

## Method Summary
The paper presents two slope-based adversarial attack methods: General Slope Attack (GSA) and Least-Squares Slope Attack (LSSA). GSA manipulates forecast trends by altering the overall slope of predictions through iterative gradient-based optimization. LSSA employs least-squares regression to identify and modify the dominant slope component in time-series forecasts. Both methods are tested against baseline attacks using a relative epsilon perturbation of 2% on N-HiTS models trained with S&P 500 data. The attacks are also integrated into a GAN architecture to generate synthetic adversarial examples, though mode collapse limits effectiveness. The study further explores ML security vulnerabilities in deployment pipelines through a theoretical malware example.

## Key Results
- GSA (upward) increases general slope from 3.37×10⁻² to 6.76×10⁻²
- Attacks reduce CNN discriminator specificity to 28% and accuracy to 57%
- Successfully double forecast slope in upward direction and reduce below zero in downward direction
- Outperform baseline methods (FGSM, BIM, MI-FGSM, SIM, TIM) under 2% perturbation

## Why This Works (Mechanism)
The attacks exploit the sensitivity of N-HiTS models to slope perturbations in time-series forecasts. By targeting the gradient of predictions rather than individual values, GSA and LSSA achieve more effective manipulation with minimal perturbations. The slope-based approach leverages the model's inherent tendency to extrapolate trends, allowing small changes to cascade into significant forecast alterations. The success against CNN discriminators indicates that traditional security mechanisms fail to detect subtle slope manipulations that preserve overall data distribution while corrupting trend information.

## Foundational Learning
- **N-HiTS architecture**: Neural Hierarchical Temporal State Space model for time-series forecasting; needed for understanding target model vulnerability; quick check: verify how state-space components process temporal dependencies
- **Adversarial perturbation**: Small input modifications causing model misclassification; needed to understand attack mechanics; quick check: confirm perturbation budget constraints
- **Gradient-based optimization**: Iterative method using model gradients to find adversarial examples; needed for attack implementation; quick check: verify gradient computation through time-series layers
- **Mode collapse**: GAN failure mode where generator produces limited variety of outputs; needed to interpret synthetic data generation results; quick check: analyze generated sample diversity
- **Discriminator specificity**: True negative rate in binary classification; needed to evaluate security bypass; quick check: calculate confusion matrix for discriminator performance

## Architecture Onboarding

**Component map**: Input time-series -> N-HiTS model -> Forecast predictions -> Attack optimization -> Adversarial examples -> CNN discriminator (security check)

**Critical path**: Time-series input → N-HiTS forecasting → Gradient computation → Slope modification → Forecast output → Discriminator evaluation

**Design tradeoffs**: 
- Small perturbation budget (2%) limits attack strength but maintains stealth
- Slope targeting versus point-wise perturbation affects transferability
- GAN integration for synthetic examples introduces mode collapse risk
- Theoretical malware deployment lacks empirical validation

**Failure signatures**:
- Mode collapse in GAN-generated adversarial examples
- Discriminator accuracy dropping to 57% indicates security bypass
- Forecast slopes becoming negative when upward trend expected
- Limited generalization beyond N-HiTS and S&P 500 data

**3 first experiments**:
1. Test GSA and LSSA on alternative time-series forecasting architectures (LSTM, Transformer)
2. Evaluate attack transferability to black-box models with different perturbation budgets
3. Implement and test theoretical malware deployment pipeline in controlled environment

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability beyond N-HiTS architecture and S&P 500 dataset
- Mode collapse in GAN-based adversarial example generation suggests scalability issues
- Theoretical malware deployment scenario lacks empirical real-world validation
- 2% perturbation budget may not reflect production system constraints

## Confidence

**High confidence**: Attack effectiveness metrics in controlled experimental conditions; security bypass results against tested CNN discriminator

**Medium confidence**: Extrapolation to diverse financial forecasting environments; attack transferability to different model architectures

**Low confidence**: Synthetic data generation via GAN due to mode collapse; real-world attack feasibility of theoretical malware deployment

## Next Checks

1. Test GSA and LSSA against multiple financial forecasting architectures (LSTM, Transformer, etc.) and datasets to assess cross-model robustness
2. Evaluate attack transferability to black-box models and under different perturbation budgets
3. Implement and test the proposed malware deployment pipeline in a controlled environment to validate real-world attack feasibility