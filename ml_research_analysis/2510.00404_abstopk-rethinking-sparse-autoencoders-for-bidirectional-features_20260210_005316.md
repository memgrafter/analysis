---
ver: rpa2
title: 'AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features'
arxiv_id: '2510.00404'
source_url: https://arxiv.org/abs/2510.00404
tags:
- abstopk
- sparse
- learning
- saes
- jumprelu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AbsTopK SAE is a new sparse autoencoder variant that uses absolute
  value-based hard thresholding (AbsTopK) to enable single features to encode bidirectional
  concepts, addressing the fragmentation problem in conventional SAEs caused by non-negativity
  constraints. The method is derived from unrolling proximal gradient updates for
  sparse coding, connecting it to classical dictionary learning theory.
---

# AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features

## Quick Facts
- arXiv ID: 2510.00404
- Source URL: https://arxiv.org/abs/2510.00404
- Authors: Xudong Zhu; Mohammad Mahdi Khalili; Zhihui Zhu
- Reference count: 40
- Primary result: AbsTopK improves reconstruction fidelity and interpretability by enabling bidirectional features through absolute value-based hard thresholding

## Executive Summary
AbsTopK SAE is a novel sparse autoencoder variant that addresses feature fragmentation in conventional SAEs by allowing single features to encode bidirectional concepts. The method uses absolute value-based hard thresholding (AbsTopK) to overcome the non-negativity constraints that cause conventional SAEs to split bidirectional concepts into separate positive and negative features. Derived from unrolling proximal gradient updates for sparse coding, AbsTopK connects to classical dictionary learning theory while improving interpretability. Experiments across four large language models and seven probing/steering tasks demonstrate AbsTopK's superiority over TopK and JumpReLU baselines, particularly for bidirectional steering tasks, with performance comparable to supervised Difference-in-Mean methods.

## Method Summary
AbsTopK SAE introduces absolute value-based hard thresholding as an activation mechanism, replacing conventional ReLU-based sparsity constraints. The method is derived from unrolling proximal gradient descent updates for sparse coding problems, establishing a theoretical connection to dictionary learning. By thresholding based on absolute values rather than raw activations, AbsTopK enables single features to represent both positive and negative variations of concepts, directly addressing the fragmentation problem where bidirectional concepts are split across multiple features in conventional SAEs. The approach maintains the overall sparse autoencoder architecture while fundamentally changing how sparsity is enforced during encoding.

## Key Results
- AbsTopK improves reconstruction fidelity and interpretability compared to TopK and JumpReLU baselines
- Particularly strong performance on bidirectional steering tasks where conventional SAEs struggle
- Achieves comparable or better results than Difference-in-Mean method, a supervised approach requiring labeled data

## Why This Works (Mechanism)
AbsTopK works by replacing non-negativity constraints with absolute value-based thresholding, allowing single features to capture both positive and negative variations of concepts. This addresses the fundamental limitation in conventional SAEs where ReLU-based activations force bidirectional concepts to fragment across separate positive and negative feature pairs. The absolute thresholding mechanism enables more compact semantic representations that better align with the bidirectional nature of many concepts in language models.

## Foundational Learning
**Sparse Coding Theory**: Understanding how sparse representations are learned and their relationship to feature interpretability; needed to grasp why conventional SAEs fragment features and how AbsTopK solves this.
Quick check: Can you explain the difference between L0 and L1 regularization in sparse coding?

**Proximal Gradient Methods**: The mathematical foundation for deriving AbsTopK from iterative sparse coding updates; needed to understand the theoretical justification for the absolute thresholding approach.
Quick check: What is the proximal operator for L0 sparsity?

**Dictionary Learning**: Classical framework for learning overcomplete representations; needed to contextualize AbsTopK within established sparse representation theory.
Quick check: How does dictionary learning differ from standard autoencoding?

## Architecture Onboarding

**Component Map**: Input -> Encoder (AbsTopK activation) -> Sparse Code -> Decoder -> Output

**Critical Path**: The AbsTopK activation layer in the encoder is the critical innovation that enables bidirectional feature representation through absolute value thresholding.

**Design Tradeoffs**: AbsTopK sacrifices the strict non-negativity constraint that enables certain interpretability guarantees in conventional SAEs in exchange for more compact and complete feature representations. This may introduce ambiguity in feature attribution but reduces feature fragmentation.

**Failure Signatures**: If AbsTopK features show poor steering performance on unidirectional tasks or excessive feature overlap in probing tasks, this may indicate improper thresholding parameters or insufficient training.

**First Experiments**:
1. Compare feature reconstruction error between AbsTopK and TopK on a held-out validation set
2. Visualize activation distributions to verify bidirectional behavior
3. Test steering effectiveness on a simple unidirectional concept (e.g., sentiment) versus a bidirectional concept (e.g., negation)

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to non-English corpora and multilingual models remains untested
- Scaling behavior to larger models (>67B parameters) has not been validated
- Long-context behavior beyond standard attention patterns is not examined

## Confidence
**High Confidence**: The theoretical derivation connecting AbsTopK to proximal gradient methods and dictionary learning is sound. The bidirectional activation mechanism is correctly identified as addressing a genuine limitation in conventional SAEs.

**Medium Confidence**: The empirical improvements over baselines are statistically significant in the reported experiments, but the absolute magnitude of gains varies substantially across tasks and models. The claim that AbsTopK "matches or exceeds" Difference-in-Mean should be qualified by noting this comparison is between unsupervised and supervised methods.

**Low Confidence**: The assertion that AbsTopK "fundamentally changes SAE interpretability" overstates the evidence, as qualitative interpretability assessments are limited to a few illustrative examples rather than systematic human evaluation.

## Next Checks
1. **Cross-linguistic validation**: Apply AbsTopK to SAEs trained on multilingual models and evaluate feature alignment across languages using parallel corpora and translation-based probing tasks.

2. **Scaling experiment**: Systematically evaluate AbsTopK performance across SAEs trained on models ranging from 7B to 175B+ parameters, measuring both reconstruction fidelity and computational overhead as a function of model size.

3. **Adversarial steering robustness**: Test AbsTopK features against targeted adversarial steering attacks designed to exploit bidirectional activation patterns, comparing failure modes against conventional SAEs.