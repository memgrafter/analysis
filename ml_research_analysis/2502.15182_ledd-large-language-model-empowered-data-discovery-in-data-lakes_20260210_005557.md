---
ver: rpa2
title: 'LEDD: Large Language Model-Empowered Data Discovery in Data Lakes'
arxiv_id: '2502.15182'
source_url: https://arxiv.org/abs/2502.15182
tags:
- data
- ledd
- semantic
- search
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEDD is a system that uses large language models (LLMs) to create
  semantic catalogs and enable semantic search in data lakes. It organizes millions
  of tables and columns into hierarchical categories with meaningful labels, allowing
  users to find related data using natural language queries.
---

# LEDD: Large Language Model-Empowered Data Discovery in Data Lakes

## Quick Facts
- arXiv ID: 2502.15182
- Source URL: https://arxiv.org/abs/2502.15182
- Reference count: 14
- Key outcome: LLM-driven semantic catalog and search for data lakes using six-schema-facet summarization

## Executive Summary
LEDD introduces a novel system for data discovery in data lakes by leveraging large language models to create semantic catalogs and enable natural language search. The system processes millions of database schemas by extracting six facets of schema information and using LLM summarization to generate unified embeddings. These embeddings enable both hierarchical catalog organization and semantic table/column search, with real-time relation analysis to identify joinable and unionable data entities.

## Method Summary
LEDD extracts six schema facets (data source, source-table-column path, sibling columns, value characteristics, related tasks, column descriptions) and uses an LLM to summarize them into unified textual representations. These are embedded and stored in a vector database (Milvus). The system then performs iterative hierarchical clustering with LLM-generated category labels, creating interpretable global catalogs. For semantic search, natural language queries are rephrased by LLM, embedded, and searched against the vector index. Relation analysis compares expanded nodes to similar existing nodes using LLM to identify connections.

## Key Results
- Demonstrates hierarchical catalog generation for 221,171 GitHub database schemas (~10M columns)
- Enables semantic search through natural language queries using LLM-rephrased embeddings
- Provides real-time relation analysis for joinable/unionable data discovery
- Integrates with existing data lake infrastructure through Python UDFs for extensibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based summarization produces unified embeddings that better capture semantic meaning than composite vector fusion.
- Mechanism: LEDD extracts six facets of schema information and uses LLM to summarize them into coherent textual representation, then embeds it.
- Core assumption: LLM summarization preserves semantic relationships across heterogeneous schema facets better than vector concatenation.
- Evidence anchors: [Section 2.2] states this aligns with human cognitive processes; Pneuma paper validates LLM-based tabular data representation.
- Break condition: If LLM summarization loses critical structural information, retrieval quality degrades.

### Mechanism 2
- Claim: Iterative hierarchical clustering with LLM-generated labels produces interpretable global catalogs without manual taxonomy.
- Mechanism: LEDD clusters embeddings into K groups, uses LLM to summarize each cluster into a phrase, iterates until top level has few nodes.
- Core assumption: LLM summaries accurately reflect cluster semantics and remain consistent across hierarchical levels.
- Evidence anchors: [Section 2.2] describes the iterative clustering and summarization process.
- Break condition: If clusters are semantically incoherent, LLM summaries become generic or misleading.

### Mechanism 3
- Claim: LLM query rephrasing followed by embedding-based retrieval enables semantic table search that outperforms keyword matching.
- Mechanism: Natural language queries are rephrased by LLM, embedded, and searched against Milvus vector index.
- Core assumption: Query rephrasing improves embedding alignment with schema embeddings.
- Evidence anchors: [Section 2.3] describes query rephrasing and embedding search; arxiv:2601.19559 validates LLM-based semantic search differs from keyword approaches.
- Break condition: If query rephrasing introduces hallucinated constraints or loses user intent, retrieval returns irrelevant tables.

## Foundational Learning

- Concept: **Vector embeddings and semantic similarity**
  - Why needed here: LEDD's retrieval mechanism depends on understanding how text embeddings capture semantic meaning and similarity.
  - Quick check question: Can you explain why "user_account" and "customer_profile" might have high embedding similarity despite sharing no keywords?

- Concept: **Hierarchical clustering and dendrogram construction**
  - Why needed here: LEDD iteratively clusters nodes into super-nodes; understanding agglomerative vs. divisive clustering is essential.
  - Quick check question: What determines when LEDD stops clustering iterations, and what tradeoff does this parameter K represent?

- Concept: **Data lake architecture and federated query processing**
  - Why needed here: LEDD builds on IGinX, a federated data lake system; understanding how data is accessed and metadata is extracted is prerequisite.
  - Quick check question: How does IGinX's Python UDF mechanism enable algorithm extensibility in LEDD?

## Architecture Onboarding

- Component map:
  - IGinX (federated data lake) -> Schema/metadata extraction -> Python UDF execution
  - OpenAI API (LLM service) -> Summarization, query rephrasing, relation analysis
  - Milvus (vector database) -> Embedding storage and similarity search
  - Apache Zeppelin (web-based visualization) -> Extended graph components
  - UDF Classes: UDFGenHierarchy, UDFGenEmbedding, UDFAnalyzeRelation

- Critical path:
  1. Schema extraction from data sources → IGinX metadata layer
  2. Six-facet information collection → LLM summarization → embedding generation
  3. Embedding storage in Milvus with vector index
  4. Iterative clustering with LLM summarization → hierarchical catalog
  5. Query rephrasing → embedding → Milvus retrieval → graph highlighting
  6. Node expansion → similarity comparison → LLM relation analysis → edge annotation

- Design tradeoffs:
  - LLM summarization vs. composite embeddings: Summarization is more interpretable but slower and potentially lossy
  - Pre-computed hierarchy vs. on-demand clustering: LEDD pre-computes catalog; search results can generate new hierarchies dynamically
  - Fixed K parameter vs. adaptive clustering: K is set for visualization optimization
  - Assumption: Tradeoffs between latency and semantic quality for real-time relation analysis are not quantified

- Failure signatures:
  - Generic category labels ("Data", "Information") suggest incoherent clusters or LLM summarization failure
  - Search returns no results for clearly relevant queries → embedding misalignment or query rephrasing failure
  - Relation analysis produces trivial edges → LLM prompt needs refinement or similarity threshold adjustment
  - Hierarchy has excessive depth or very shallow breadth → K parameter misconfigured for dataset scale

- First 3 experiments:
  1. **Embedding quality validation**: Compare retrieval precision@10 for LLM-summarized embeddings vs. composite fusion on labeled benchmark subset.
  2. **Hierarchy interpretability assessment**: Generate catalogs for a known domain, have domain experts rate category label accuracy and hierarchy usefulness.
  3. **Relation analysis accuracy**: Sample 50 expanded node pairs with LLM-generated relation descriptions; manually verify joinability/unionability claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM-based summarization quality and computational cost compare to composite embedding approaches?
- Basis in paper: The paper states LLM summarization "aligns more closely with human cognitive process" but provides no empirical comparison.
- Why unresolved: The comparison is theoretical rather than experimentally validated.
- What evidence would resolve it: Controlled study comparing clustering quality metrics and latency/cost measurements.

### Open Question 2
- Question: What is the optimal value for the clustering parameter K, and how does it affect user task performance?
- Basis in paper: The paper states K is "set to the graph size best suited for visualization" without systematic justification.
- Why unresolved: The parameter appears to be set based on heuristics rather than empirical optimization.
- What evidence would resolve it: User study measuring task completion time and accuracy across different K values.

### Open Question 3
- Question: How does LEDD's semantic search quality compare to existing keyword-based and embedding-based table search systems?
- Basis in paper: The paper demonstrates functionality but provides no quantitative evaluation of search effectiveness against baselines.
- Why unresolved: The demo format focuses on system capabilities rather than benchmarking.
- What evidence would resolve it: Evaluation on standard table search benchmarks with precision@k, recall@k, and NDCG metrics.

## Limitations
- No quantitative evaluation metrics provided for catalog quality, search precision, or relation analysis accuracy
- LLM API costs and rate limits could be prohibitive for large-scale deployment
- Parameter choices (K, n, m, embedding model) are unspecified and may significantly impact performance
- Dependency on OpenAI API availability and costs for production deployment

## Confidence
- **High**: LLM-based semantic search mechanism and its integration with vector databases
- **Medium**: Hierarchical clustering producing interpretable catalogs, pending validation on benchmark datasets
- **Low**: Quantitative claims about superiority over baseline methods due to absence of metrics

## Next Checks
1. Benchmark LEDD's retrieval precision@10 against keyword search on a labeled subset of 1,000 tables from SchemaPile
2. Conduct user study with data analysts rating catalog interpretability and search relevance across 5 different domains
3. Measure the false positive rate of relation analysis by manually verifying 100 LLM-generated join/union suggestions