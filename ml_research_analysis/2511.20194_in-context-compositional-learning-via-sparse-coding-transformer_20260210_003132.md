---
ver: rpa2
title: In-Context Compositional Learning via Sparse Coding Transformer
arxiv_id: '2511.20194'
source_url: https://arxiv.org/abs/2511.20194
tags:
- compositional
- attention
- coefficients
- tasks
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of in-context compositional
  learning in Transformers, where models must infer and apply compositional rules
  from context examples to solve target problems. The authors propose a reformulation
  of the attention mechanism inspired by sparse coding, reinterpreting it as a mapping
  of inputs to outputs via projections onto two learned dictionaries: an encoding
  dictionary and a decoding dictionary.'
---

# In-Context Compositional Learning via Sparse Coding Transformer

## Quick Facts
- arXiv ID: 2511.20194
- Source URL: https://arxiv.org/abs/2511.20194
- Reference count: 40
- One-line primary result: Sparse coding transformer improves compositional generalization, achieving up to 82.7% accuracy on S-RAVEN vs 65.1% for standard transformer

## Executive Summary
This paper addresses the challenge of in-context compositional learning in Transformers, where models must infer and apply compositional rules from context examples to solve target problems. The authors propose a reformulation of the attention mechanism inspired by sparse coding, reinterpreting it as a mapping of inputs to outputs via projections onto two learned dictionaries: an encoding dictionary and a decoding dictionary. By enforcing sparsity on the coefficients obtained from the encoding dictionary, the model explicitly captures compositional structure. To transfer these rules, they estimate target task coefficients as a linear combination of context task coefficients. Experiments on synthetic data, S-RAVEN, and RA-VEN datasets demonstrate that their method outperforms standard Transformers, particularly in tasks requiring compositional generalization.

## Method Summary
The method reformulates the Transformer attention mechanism using sparse coding principles. Instead of standard dot-product attention, the model learns two dictionaries: an encoding dictionary for projecting inputs and a decoding dictionary for projecting outputs. The attention weights are computed as sparse coefficients obtained by projecting inputs onto the encoding dictionary. These sparse coefficients are then used to reconstruct outputs via the decoding dictionary. The sparsity constraint ensures that only a few dictionary elements are activated for each input, promoting compositional generalization. For in-context learning, the model estimates coefficients for target tasks by linearly combining coefficients from context examples, allowing it to transfer compositional rules from context to target problems.

## Key Results
- On S-RAVEN dataset: Achieved 82.7% accuracy compared to 65.1% for baseline Transformer
- On RA-VEN dataset: Maintained over 30% of samples with PSNR above 40, while baseline nearly dropped to zero
- Demonstrated consistent improvements across synthetic data and visual reasoning tasks requiring compositional generalization

## Why This Works (Mechanism)
The method works by explicitly modeling compositional structure through sparse coding. The encoding dictionary captures fundamental primitives or components, while the decoding dictionary maps these sparse representations to outputs. The sparsity constraint forces the model to use a minimal set of primitives for each input, making the compositional structure explicit. When transferring rules from context to target, the linear combination of context coefficients leverages the shared dictionary structure, allowing the model to compose new solutions from known primitives. This explicit representation of compositional structure is more effective than standard attention mechanisms that implicitly learn relationships.

## Foundational Learning
- **Sparse Coding**: Why needed - Provides a principled way to represent inputs using a minimal set of dictionary elements; Quick check - Verify that coefficient vectors are indeed sparse (few non-zero elements)
- **Attention Mechanism Reformulation**: Why needed - Standard attention lacks explicit compositional structure; Quick check - Compare attention patterns between standard and sparse coding approaches
- **Dictionary Learning**: Why needed - Dictionaries provide a shared vocabulary of compositional primitives; Quick check - Analyze dictionary elements to ensure they capture meaningful features
- **Linear Combination Transfer**: Why needed - Enables rule transfer from context to target tasks; Quick check - Verify that target coefficients can be accurately reconstructed from context coefficients
- **Compositional Generalization**: Why needed - Core challenge being addressed; Quick check - Test on out-of-distribution compositional patterns
- **In-Context Learning**: Why needed - Setting where no parameter updates are allowed; Quick check - Ensure method works with fixed model parameters

## Architecture Onboarding

**Component Map:**
Input -> Encoding Dictionary -> Sparse Coefficients -> Decoding Dictionary -> Output

**Critical Path:**
Input embedding → Encoding dictionary projection → Sparsity regularization → Coefficient estimation → Decoding dictionary projection → Output reconstruction

**Design Tradeoffs:**
- Sparsity regularization vs. reconstruction accuracy
- Dictionary size vs. model capacity and computational cost
- Linear combination assumption vs. flexibility in rule transfer
- Explicit compositional structure vs. model simplicity

**Failure Signatures:**
- Poor performance on compositional tasks suggests inadequate dictionary coverage
- High reconstruction error indicates insufficient dictionary capacity
- Failure to transfer rules from context to target suggests violation of linear combination assumptions
- Computational inefficiency may indicate scalability issues with larger models

**First Experiments:**
1. Visualize learned dictionaries to verify they capture meaningful compositional primitives
2. Measure coefficient sparsity levels across different tasks to confirm sparsity regularization
3. Compare performance on compositional vs. non-compositional subsets of datasets

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the sparse coding attention mechanism be effectively scaled to large pre-trained models to match or exceed the performance of dominant Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA or DoRA?
- Basis: The authors explicitly state in the Limitations section that the "application to large pre-trained models remains unexplored" and note in Appendix 8 that their method did not surpass LoRA or DoRA benchmarks on Llama-7B.
- Why unresolved: While the method improves upon the base Llama-7B model using significantly fewer parameters (128 vs. millions), it has not yet demonstrated superior performance compared to modern PEFT techniques or on larger model scales.
- What evidence would resolve it: Successful integration and fine-tuning of larger models (e.g., Llama-70B) where the method achieves higher accuracy on reasoning benchmarks (like ARC-c or HellaSwag) than current PEFT baselines.

### Open Question 2
- Question: Is the proposed attention reformulation effective for sequence generation tasks involving continuous output spaces, such as image generation or translation?
- Basis: In Appendix 8, the authors identify "exploration of its application to other benchmarks, such as translation and summarization, or image generation tasks... as a promising direction for future work."
- Why unresolved: The paper evaluates the method primarily on synthetic compositional tasks and visual reasoning (RAVEN) formulated as classification or discrete prediction, leaving its applicability to continuous generation tasks unverified.
- What evidence would resolve it: Empirical evaluations on standard generation benchmarks (e.g., WMT for translation or MS-COCO for image generation) showing improvements in generation quality metrics over standard Transformers.

### Open Question 3
- Question: How does the model's performance degrade when the strict theoretical assumptions regarding dictionary coverage in context examples are violated?
- Basis: The proof for the coefficient transfer mechanism (Proposition 9.4) relies on Assumption 9.3, which requires that "every dictionary element is used at least once" in the context examples.
- Why unresolved: The paper provides a theoretical guarantee only when the context fully covers the compositional primitives needed for the target; it does not empirically analyze how the "linear combination of context coefficients" fails when the target requires elements absent from the context.
- What evidence would resolve it: An ablation study measuring accuracy on targets constructed from dictionary atoms that were partially or completely omitted from the context examples (i.e., out-of-distribution primitives).

## Limitations
- Application to large pre-trained models remains unexplored and did not surpass PEFT methods on Llama-7B
- Theoretical assumptions require full dictionary coverage in context examples, which may not hold in practice
- Computational overhead from sparse coding formulation could be prohibitive for larger models or longer sequences

## Confidence

**High confidence**: The reformulation of attention via sparse coding dictionaries is mathematically sound and the implementation appears correct

**Medium confidence**: The sparsity constraint effectively captures compositional structure for the evaluated tasks

**Medium confidence**: The linear combination approach for transferring compositional rules generalizes beyond the tested synthetic scenarios

## Next Checks

1. Evaluate performance on natural language compositional tasks (e.g., SCAN, CFQ) to assess generalization beyond visual reasoning

2. Test scalability by applying the method to larger Transformer architectures (e.g., T5-base or BERT-large) and longer sequence lengths

3. Compare against state-of-the-art compositional learning methods (e.g., meta-learning approaches or specialized architectures) to establish relative effectiveness