---
ver: rpa2
title: 'EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language
  Models'
arxiv_id: '2508.01625'
source_url: https://arxiv.org/abs/2508.01625
tags:
- expert
- experts
- quantization
- pruning
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes EAC-MoE, a compression method specifically
  designed for Mixture-of-Experts (MoE) language models. The method addresses two
  key challenges in MoE deployment: substantial GPU memory consumption due to loading
  all experts, and low inference speedup despite activated parameter reduction.'
---

# EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models

## Quick Facts
- arXiv ID: 2508.01625
- Source URL: https://arxiv.org/abs/2508.01625
- Reference count: 40
- Primary result: EAC-MoE enables Mixtral-8x7B deployment on a single RTX 3090 GPU with <1% accuracy drop

## Executive Summary
EAC-MoE introduces a compression method specifically designed for Mixture-of-Experts (MoE) language models that addresses two critical deployment challenges: excessive GPU memory consumption and suboptimal inference speedup. The approach combines quantization with expert-selection calibration and dynamic pruning based on expert usage frequency. By preserving router calibration during quantization and removing rarely used experts during inference, EAC-MoE achieves up to 4.92x memory reduction and 1.68x inference speedup while maintaining model accuracy within 1% of original performance.

## Method Summary
EAC-MoE tackles MoE deployment challenges through two complementary techniques. First, Quantization with Expert-Selection Calibration (QESC) mitigates the expert selection bias that typically occurs when quantizing MoE models to low-bit representations. This is achieved through layer-by-layer router calibration that maintains routing accuracy despite aggressive quantization. Second, Pruning based on Expert-Selection Frequency (PESF) dynamically removes experts that are rarely selected during inference, reducing both memory footprint and computational overhead. The method is evaluated across four MoE architectures including Mixtral-8x7B and Deepseek-moe-16b-base, demonstrating significant performance improvements without substantial accuracy degradation.

## Key Results
- Achieves up to 4.92x reduction in GPU memory usage across tested MoE models
- Delivers up to 1.68x inference speedup while maintaining model accuracy
- Enables deployment of Mixtral-8x7B on a single RTX 3090 GPU with <1% accuracy loss
- Demonstrates consistent performance improvements across four different MoE architectures

## Why This Works (Mechanism)
EAC-MoE works by addressing the fundamental tension between model compression and routing accuracy in MoE architectures. Low-bit quantization typically degrades the router's ability to select appropriate experts, while static compression methods fail to account for the dynamic nature of expert selection. QESC preserves routing accuracy by calibrating routers layer-by-layer during quantization, ensuring that expert selection remains effective even at low bit widths. PESF leverages the inherent sparsity of MoE models by removing experts that contribute minimally to overall performance, reducing both memory and computation without sacrificing accuracy. The combination creates a synergistic effect where quantization enables aggressive compression while dynamic pruning ensures that only useful computation is performed.

## Foundational Learning
- **Mixture-of-Experts (MoE) architecture**: Why needed - MoE models activate only a subset of parameters per input, enabling efficient scaling. Quick check - Verify that the model activates fewer than 100% of parameters per forward pass.
- **Router calibration in MoE**: Why needed - Routers determine which experts process which tokens, directly affecting model performance. Quick check - Confirm that router output distributions change meaningfully across layers.
- **Quantization-aware training**: Why needed - Standard quantization can degrade model accuracy, especially for sensitive components like routers. Quick check - Compare model accuracy before and after quantization without calibration.
- **Dynamic pruning**: Why needed - Static pruning may remove experts that are occasionally crucial for specific inputs. Quick check - Analyze expert usage frequency distribution across different input types.
- **Memory-accuracy tradeoff in compressed models**: Why needed - Understanding this tradeoff is crucial for practical deployment decisions. Quick check - Plot memory usage against accuracy across different compression levels.
- **Inference optimization for sparse models**: Why needed - MoE models have inherent sparsity that can be exploited for speedup. Quick check - Measure actual vs theoretical speedup from expert activation patterns.

## Architecture Onboarding
- **Component map**: Input tokens -> Router selection -> Expert activation -> Computation -> Output aggregation
- **Critical path**: Router -> Expert selection -> Computation -> Output combination
- **Design tradeoffs**: Memory reduction vs. inference latency vs. accuracy preservation
- **Failure signatures**: Significant accuracy drop indicates routing calibration issues; minimal speedup suggests pruning is not removing enough computation
- **Three first experiments**:
  1. Measure expert selection frequency distribution across different layers
  2. Quantize a single MoE layer with and without router calibration
  3. Profile memory usage and inference time before and after applying EAC-MoE

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade on tasks requiring specialized expert knowledge if frequently used experts are pruned
- Computational overhead of expert-selection calibration during quantization is not fully characterized
- Method assumes layer-by-layer router calibration is sufficient, which may not hold for all routing mechanisms
- Limited validation across diverse MoE variants beyond the four tested architectures

## Confidence
- Memory reduction claims: **High** confidence (systematic evaluation across multiple architectures)
- Inference speedup results: **High** confidence (up to 1.68x observed range)
- Mixtral-8x7B deployment on RTX 3090: **High** confidence (compelling specific result)
- General applicability to other MoE models: **Medium** confidence (limited validation scope)
- QESC effectiveness for all routing mechanisms: **Medium** confidence (method assumes layer-by-layer sufficiency)

## Next Checks
1. Evaluate EAC-MoE on additional MoE architectures with different routing mechanisms (e.g., semantic routing, token-length-aware routing) to verify general applicability.
2. Conduct ablation studies isolating the impact of QESC versus PESF on both memory reduction and inference speedup to quantify each component's contribution.
3. Test EAC-MoE under varying hardware constraints and batch sizes to establish the method's robustness across different deployment scenarios.