---
ver: rpa2
title: A Practical Adversarial Attack against Sequence-based Deep Learning Malware
  Classifiers
arxiv_id: '2509.11836'
source_url: https://arxiv.org/abs/2509.11836
tags:
- sequence
- sequences
- adversarial
- behavior
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a practical adversarial attack method against
  sequence-based deep learning malware classifiers. The approach uses a Deep Q-Network
  and a heuristic backtracking search strategy to generate perturbation sequences
  that satisfy real-world constraints.
---

# A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers

## Quick Facts
- arXiv ID: 2509.11836
- Source URL: https://arxiv.org/abs/2509.11836
- Reference count: 40
- Primary result: Attack achieves 64.4% success rate on AndroCT and 59.1% on ADFA-LD datasets

## Executive Summary
This paper presents a novel adversarial attack method targeting sequence-based deep learning malware classifiers. The approach leverages Deep Q-Networks (DQN) combined with heuristic backtracking search to generate perturbation sequences that evade detection while preserving malware functionality. The key innovation lies in transforming sequence modifications back to source code rather than directly manipulating behavior logs, making the attack more practical for real-world deployment.

## Method Summary
The attack employs a reinforcement learning framework where a DQN agent learns to generate adversarial perturbations to malware behavior sequences. A heuristic backtracking search strategy ensures generated modifications satisfy real-world constraints while maintaining malicious functionality. The method maps these sequence-level changes back to source code modifications, creating a practical attack vector that operates at the code level rather than requiring access to intermediate analysis representations.

## Key Results
- 64.4% success rate on AndroCT dataset against various target models
- 59.1% success rate on ADFA-LD dataset
- Maintains malware functionality while evading detection
- Demonstrates effectiveness across different deep learning architectures

## Why This Works (Mechanism)
The attack exploits the sequential nature of behavior-based malware classification by introducing subtle perturbations that mislead deep learning models while preserving the underlying malicious functionality. The DQN learns optimal perturbation strategies through trial and error, while backtracking ensures modifications remain within practical constraints.

## Foundational Learning
- Deep Q-Networks (DQN): Why needed - reinforcement learning for optimal perturbation generation; Quick check - verify agent learns meaningful reward patterns
- Heuristic backtracking: Why needed - ensures practical constraints are met; Quick check - validate constraint satisfaction rates
- Sequence-to-source transformation: Why needed - enables practical code-level attacks; Quick check - verify transformation preserves original functionality
- Behavior sequence analysis: Why needed - malware classification input format; Quick check - confirm sequence extraction accuracy
- Adversarial perturbation theory: Why needed - foundation for evasion attacks; Quick check - measure perturbation magnitude vs. detection evasion
- Malware functionality preservation: Why needed - attack must remain practically useful; Quick check - validate malicious capabilities post-modification

## Architecture Onboarding
**Component Map**: DQN Agent -> Backtracking Search -> Sequence Generator -> Source Code Transformer -> Malware Classifier

**Critical Path**: DQN learns perturbation policy -> Backtracking validates constraints -> Sequence generator creates modifications -> Transformer maps to source code -> Classifier receives adversarial input

**Design Tradeoffs**: Direct behavior log manipulation vs. source code transformation - source code approach is more practical but requires accurate transformation mapping

**Failure Signatures**: Low success rates indicate either overly strict constraints, insufficient learning in DQN, or poor sequence-to-source transformation accuracy

**First Experiments**: 1) Baseline DQN training without backtracking on simple malware sequences; 2) Transformation accuracy testing on known-good to adversarial sequence mappings; 3) Constraint satisfaction rate measurement during backtracking search

## Open Questions the Paper Calls Out
None

## Limitations
- Success rates indicate attack fails in 35-41% of cases, showing defenses are not trivial to overcome
- Evaluation limited to specific datasets and model architectures, reducing generalizability
- Relies on assumption that behavior sequences can be reliably mapped back to source code modifications

## Confidence
- Core claim (practical attack feasibility): Medium
- Technical approach validity: High
- Functionality preservation claim: Medium
- Real-world applicability: Medium

## Next Checks
1. Test attack against additional malware families and behavior sequence formats beyond AndroCT and ADFA-LD to assess generalizability
2. Implement functional equivalence testing to verify modified samples maintain malicious capabilities across diverse execution environments
3. Evaluate attack performance against ensemble models and classifiers with adversarial training to determine robustness against defensive measures