---
ver: rpa2
title: Understanding Post-Training Structural Changes in Large Language Models
arxiv_id: '2509.17866'
source_url: https://arxiv.org/abs/2509.17866
tags:
- post-training
- singular
- arxiv
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically analyzes structural changes in model
  parameters after post-training using SVD. The analysis reveals two robust phenomena:
  (1) singular values undergo near-uniform geometric scaling across layers, and (2)
  left and right singular vectors experience highly consistent orthogonal transformations.'
---

# Understanding Post-Training Structural Changes in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.17866
- **Source URL:** https://arxiv.org/abs/2509.17866
- **Authors:** Xinyu He; Xianghui Cao
- **Reference count:** 40
- **Primary result:** Post-training produces equivalent parametric effects across different methods (instruction tuning, RL, distillation) through two robust phenomena: uniform singular value scaling and coordinated orthogonal transformations

## Executive Summary
This work systematically analyzes structural changes in model parameters after post-training using SVD decomposition. The analysis reveals two robust phenomena: (1) singular values undergo near-uniform geometric scaling across layers, and (2) left and right singular vectors experience highly consistent orthogonal transformations. These findings lead to a mathematical framework that explains why post-training relies on pre-training foundations. Further experiments show that singular value scaling acts as a temperature-controlled mechanism, while coordinated vector rotations encode semantic alignment.

## Method Summary
The study performs SVD on weight matrices from Transformer blocks (Self-Attention: $W_Q, W_K, W_V, W_O$; FFN: $W_{gate}, W_{up}, W_{down}$) comparing BASE and POST models. It computes Singular Value Scaling Matrix (SVSM) to track $\sigma_{post}/\sigma_{base}$ and orthogonality metric ($N_F$) to measure consistency between Left ($Q_1$) and Right ($Q_2$) transformations. Validation is performed through replacement experiments (Constructions 7/8) and restoration experiments (Constructions 9/10) on benchmarks like GSM8K, MATH-500, and MMLU.

## Key Results
- Singular values undergo near-uniform geometric scaling across layers in post-training
- Left and right singular vectors experience highly consistent orthogonal transformations
- Different post-training methods (instruction tuning, RL, distillation) produce equivalent parametric effects
- Singular value scaling acts as temperature-controlled mechanism affecting model confidence
- Coordinated vector rotations encode semantic alignment between input-output features

## Why This Works (Mechanism)

### Mechanism 1: Singular Value Scaling as Temperature Regulation
Post-training applies global linear scaling to singular values, functioning as temperature control for model confidence. The near-uniform geometric scaling across layers is mathematically equivalent to adjusting attention temperature ($T = 1/\alpha^2$), modulating output distributions without altering representational capacity.

### Mechanism 2: Coordinated Rotation for Semantic Alignment
Semantic alignment is encoded through consistent orthogonal transformation ($Q$) applied identically to both left ($U$) and right ($V$) singular vector spaces. This preserves pairing relationships between input-output features while reorienting the latent space to adapt to new data distributions.

### Mechanism 3: Pre-training as a Capacity Ceiling
Post-training acts as rotation+scaling of existing basis rather than creating new capacity. The framework $W_{post} \approx (U_{base}Q) \cdot (\alpha \Sigma_{base}) \cdot (V_{base}Q)^T$ shows post-training preserves column spaces, with pre-trained semantic space setting the ceiling for post-training capabilities.

## Foundational Learning

**Concept: Singular Value Decomposition (SVD)**
- **Why needed here:** Primary lens for analyzing model weights, separating geometry (vectors) from magnitude (values)
- **Quick check:** Scaling $\Sigma$ in SVD changes the output magnitude of the linear transformation

**Concept: Orthogonal Transformations (Rotation)**
- **Why needed here:** Post-training is essentially a "rotation" of semantic space
- **Quick check:** Multiplying a vector by an orthogonal matrix preserves its Euclidean length

**Concept: Attention Temperature**
- **Why needed here:** Links singular value scaling to attention mechanism behavior
- **Quick check:** Increasing temperature typically makes model predictions more random (less confident)

## Architecture Onboarding

**Component map:** Transformer blocks -> Self-Attention layers ($W_Q, W_K, W_V, W_O$) -> FFN layers ($W_{gate}, W_{up}, W_{down}$) -> SVD analysis

**Critical path:** 1) Load Base and Post model weights 2) Extract linear layer weights 3) Compute SVD for corresponding layers 4) Calculate scaling factors for singular values and similarity matrices for vectors 5) Verify $Q_1 \approx Q_2$ consistency

**Design tradeoffs:**
- Approximation vs. Precision: Linear approximation admits small errors (residual perturbations $\Delta Q$)
- Global vs. Layer-wise: Emphasizes global consistency but notes specific exceptions (e.g., $W_O$ in reasoning models)

**Failure signatures:**
- High $N_F$ metric indicates $Q_1 \neq Q_2$ (coordinated rotation failure)
- Non-uniform SVSM indicates structural change law does not hold
- Model collapse from singular value intervention indicates theoretical framework mismatch

**First 3 experiments:**
1. **Verification of Scaling:** Replace $\Sigma_{post}$ with scaled $\Sigma_{base}$ (Construction 7), evaluate on GSM8K
2. **Ablation of Rotation:** Apply transformation only to $U$, not $V$ (Construction 9), test if outputs become nonsense
3. **Restoration of Consistency:** Restore transformation consistency using $Q$ from input subspace (Construction 10), verify performance recovery

## Open Questions the Paper Calls Out

### Open Question 1
How does pre-training establish the specific structural capacity ceiling that constrains post-training? The paper characterizes post-training as dependent on pre-existing foundations but does not analyze formation dynamics during initial pre-training. Resolution would require longitudinal SVD analysis tracking singular vectors/values throughout pre-training.

### Open Question 2
Does targeting middle-k singular components for fine-tuning yield better performance than top-k components? The paper notes dominant singular components show minimal rotation, suggesting middle-k focus instead, but provides no empirical validation. Resolution requires comparative experiments with LoRA/PiSSA variants on middle-k versus standard top-k approaches.

### Open Question 3
Is parametric equivalence between RL and supervised post-training solely driven by data diversity rather than optimization algorithm? While final structural changes look similar, the paper conjectures the generalization advantage comes from data diversity rather than RL's intrinsic design. Resolution requires ablation studies controlling for data diversity across RL and SFT regimes.

## Limitations
- Mathematical framework relies on linear approximations that may not capture all post-training dynamics
- Analysis focuses on weight matrices but does not account for potential non-linear effects in activation functions or layer normalization
- Generalizability across different model families beyond tested Qwen and Llama variants requires further validation

## Confidence
- **High Confidence**: Near-uniform geometric scaling of singular values across layers consistently observed across all tested model pairs
- **Medium Confidence**: Coordinated rotation hypothesis for semantic alignment strongly supported but relies on assumption that orthogonal transformations preserve semantic structure
- **Medium Confidence**: Pre-training capacity ceiling argument plausible but requires more evidence from diverse model architectures and post-training methods

## Next Checks
1. **Architecture Generalization**: Test SVD analysis framework on different model architectures (Mistral, Gemma) to verify if two robust phenomena hold across diverse implementations

2. **Fine-grained Temperature Control**: Design experiments systematically varying singular value scaling factors layer-by-layer to map precise relationship between scaling magnitude and model confidence/entropy outputs

3. **Non-linear Effects Investigation**: Analyze whether post-training introduces non-linear changes in activation patterns not captured by linear SVD framework, potentially using NTK parameterization analysis