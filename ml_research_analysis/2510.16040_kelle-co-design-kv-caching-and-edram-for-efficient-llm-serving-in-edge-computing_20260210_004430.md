---
ver: rpa2
title: 'Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing'
arxiv_id: '2510.16040'
source_url: https://arxiv.org/abs/2510.16040
tags:
- edram
- kelle
- vectors
- cache
- refresh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kelle is a software-hardware co-design solution that integrates
  embedded DRAM (eDRAM) with large language model (LLM) serving to improve efficiency
  on edge devices. It addresses the high memory and energy costs of key-value (KV)
  caching during LLM inference by using eDRAM as the primary on-chip storage for KV
  vectors.
---

# Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing

## Quick Facts
- arXiv ID: 2510.16040
- Source URL: https://arxiv.org/abs/2510.16040
- Authors: Tianhua Xia; Sai Qian Zhang
- Reference count: 40
- Key outcome: 3.9× speedup and 4.5× energy savings for LLM serving on edge devices using eDRAM co-design

## Executive Summary
Kelle presents a software-hardware co-design solution that integrates embedded DRAM (eDRAM) with large language model (LLM) serving to improve efficiency on edge devices. The system addresses the high memory and energy costs of key-value (KV) caching during LLM inference by using eDRAM as the primary on-chip storage for KV vectors. By combining algorithmic innovations with specialized hardware acceleration, Kelle achieves significant performance improvements while maintaining model accuracy.

The solution introduces two key algorithms: attention-based eviction and recomputation policy (AERP) for efficient KV cache management, and two-dimensional adaptive refresh policy (2DRP) to minimize eDRAM refresh energy. These are paired with a dedicated accelerator featuring a systolic evictor and scheduler that optimize computation patterns to reduce data lifetime in eDRAM. Evaluations demonstrate that Kelle achieves 3.9× speedup and 4.5× energy savings compared to existing baseline solutions.

## Method Summary
Kelle is a software-hardware co-design solution that integrates embedded DRAM (eDRAM) with large language model (LLM) serving to improve efficiency on edge devices. It addresses the high memory and energy costs of key-value (KV) caching during LLM inference by using eDRAM as the primary on-chip storage for KV vectors. The solution introduces two key algorithms: attention-based eviction and recomputation policy (AERP) for efficient KV cache management, and two-dimensional adaptive refresh policy (2DRP) to minimize eDRAM refresh energy. Kelle also features a dedicated accelerator with a systolic evictor and a scheduler that optimizes computation patterns to reduce data lifetime in eDRAM. Evaluations show that Kelle achieves 3.9× speedup and 4.5× energy savings compared to existing baseline solutions, while maintaining LLM accuracy.

## Key Results
- Achieves 3.9× speedup compared to baseline solutions for LLM inference
- Reduces energy consumption by 4.5× through optimized eDRAM utilization
- Maintains LLM accuracy while significantly improving efficiency
- Successfully implements attention-based eviction and recomputation policy (AERP)
- Demonstrates effectiveness of two-dimensional adaptive refresh policy (2DRP) for eDRAM

## Why This Works (Mechanism)
Kelle works by co-designing both software algorithms and hardware architecture to optimize LLM serving on edge devices. The system leverages eDRAM's high density and low leakage to store KV vectors efficiently, while using intelligent eviction policies to manage cache contents. The attention-based approach prioritizes important KV pairs based on their contribution to model predictions, while the recomputation policy selectively regenerates less critical KV pairs when needed. The 2DRP algorithm adapts refresh rates based on access patterns, significantly reducing refresh energy. The dedicated hardware accelerator with systolic architecture enables parallel processing of eviction and computation operations, minimizing data movement and lifetime in eDRAM.

## Foundational Learning
- **eDRAM characteristics**: Embedded DRAM offers higher density than SRAM but requires periodic refresh. Understanding refresh overhead is crucial for optimizing energy efficiency in edge deployments.
- **KV caching in LLMs**: KV caching stores intermediate attention computations to avoid redundant calculations. Managing this cache efficiently is critical for inference speed and memory usage.
- **Attention mechanisms**: Self-attention computes relationships between tokens using KV pairs. The importance of these pairs varies, enabling selective eviction and recomputation strategies.
- **Systolic array architecture**: Parallel processing structures that enable efficient data flow for matrix operations. Essential for accelerating LLM inference workloads.
- **Refresh policies in DRAM**: Standard DRAM requires periodic refresh to maintain data integrity. Adaptive refresh can reduce energy consumption by matching refresh rates to actual data retention needs.

## Architecture Onboarding

**Component Map**: LLM workload → KV cache → AERP scheduler → eDRAM storage → 2DRP controller → Systolic evictor → Computation pipeline

**Critical Path**: Input tokens → Attention computation → KV cache lookup → AERP decision → eDRAM access → Result generation

**Design Tradeoffs**: The system balances between recomputation overhead and cache eviction, between refresh frequency and data retention, and between hardware complexity and performance gains. Higher eviction rates reduce memory pressure but increase recomputation costs. More aggressive refresh adaptation saves energy but risks data loss. Complex hardware accelerators improve throughput but increase area and power consumption.

**Failure Signatures**: Performance degradation occurs when eviction decisions are suboptimal, leading to excessive recomputation. Energy savings diminish if refresh adaptation is too conservative. Accuracy loss may result from aggressive eviction of critical KV pairs. System throughput bottlenecks emerge if the systolic evictor cannot keep pace with computation demands.

**3 First Experiments**:
1. Baseline LLM inference performance without any caching optimizations
2. Performance with standard LRU cache replacement policy
3. Energy consumption comparison between fixed and adaptive refresh rates

## Open Questions the Paper Calls Out
None

## Limitations
- Hardware implementation complexity may limit real-world deployment feasibility and cost-effectiveness
- Performance improvements depend heavily on specific LLM attention patterns which may vary across models
- The recomputation strategy could introduce latency variability that affects real-time applications
- No discussion of accuracy-accuracy trade-offs in the recomputation approach
- Baseline specifications and comparison methodology lack sufficient detail

## Confidence
- Hardware-acceleration claims: Medium
- Energy savings measurements: Medium
- Algorithm effectiveness: Low
- Implementation feasibility: Low

## Next Checks
1. Request detailed baseline specifications and comparison methodology for the claimed performance improvements
2. Verify accuracy preservation across multiple LLM architectures and task types
3. Request hardware implementation details and cost analysis for the proposed accelerator design