---
ver: rpa2
title: Causal Imitation Learning Under Measurement Error and Distribution Shift
arxiv_id: '2601.22206'
source_url: https://arxiv.org/abs/2601.22206
tags:
- expert
- learning
- latent
- imitation
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles imitation learning when the decision-relevant
  state is partially observed through noisy measurements and the data distribution
  may shift between training and deployment. Such settings can cause standard behavioral
  cloning (BC) to latch onto spurious correlations, leading to systematically biased
  policies under shift.
---

# Causal Imitation Learning Under Measurement Error and Distribution Shift

## Quick Facts
- arXiv ID: 2601.22206
- Source URL: https://arxiv.org/abs/2601.22206
- Reference count: 40
- One-line primary result: CausIL achieves lower imitation error and greater robustness to measurement and population shifts compared to behavioral cloning baselines.

## Executive Summary
This paper tackles imitation learning when the decision-relevant state is partially observed through noisy measurements and the data distribution may shift between training and deployment. Such settings can cause standard behavioral cloning (BC) to latch onto spurious correlations, leading to systematically biased policies under shift. The authors propose a causal imitation learning framework (CausIL) that treats noisy state observations as proxy variables and leverages proximal causal inference to define an intervention-based target policy. Identification conditions are provided for both discrete and continuous settings, with estimators developed accordingly—discrete plug-in via matrix inversion and continuous via RKHS-adversarial estimation. Experiments on simulated data and semi-simulated ICU data from PhysioNet/Computing in Cardiology Challenge 2019 show that CausIL achieves lower imitation error and greater robustness to both measurement and population shifts compared to BC baselines.

## Method Summary
The paper proposes a causal imitation learning framework that targets the interventional distribution of actions given a state, defined as π_opt(s) = argmax_a P(A_t^(s)=a). The key insight is to use noisy measurements W as proxies for an unobserved confounder U. Identification is achieved through proximal causal inference, leveraging two proxy variables: S_{t-1} (treatment-inducing) and W_{t-1} (outcome-inducing). For discrete settings, identification uses matrix inversion P(A_t^(s)) = P_{A|Z',s} (P_{W'|Z',s})^{-1} P_{W'}. For continuous settings, identification uses RKHS-adversarial estimation of a confounding bridge function h_a solving E[Y_t^(a) - h_a(W,S) | Z,S] = 0. The estimators are validated on semi-simulated ICU data from PhysioNet, comparing against behavioral cloning baselines.

## Key Results
- CausIL achieves lower mean squared imitation loss than behavioral cloning under both measurement and population shifts
- The causal-optimal policy π_opt remains invariant when only the measurement channel P(W|U) shifts between domains
- Empirical results on PhysioNet data demonstrate robust performance across different shift scenarios

## Why This Works (Mechanism)

### Mechanism 1: Causal Target Decouples from Measurement Mechanism
The causal-optimal policy π_opt(s) remains invariant when only the measurement channel P(W|U) shifts between domains, unlike behavioral cloning targets. π_opt(s) is defined as the interventional distribution P(A_t^(s)) under do(S_t = s), which depends only on the marginal P(U_{t-1}) and the invariant expert mechanism π_E(s, u). In contrast, π_BC2 depends on the posterior P(U_{t-1}|S_t, V_{t-1}) via Bayes' rule, which directly involves P(W|U). When the measurement mechanism changes, the posterior shifts but the marginal does not.

### Mechanism 2: Proxy Variables Enable Identification Without Instruments
The intervention distribution P(A_t^(s)) is identifiable from observational data alone using two proxy variables for the latent confounder U_{t-1}. The paper uses S_{t-1} as a treatment-inducing proxy (correlated with U_{t-1} through dynamics) and W_{t-1} as an outcome-inducing proxy (noisy measurement of U_{t-1}). The key insight is that the Fredholm integral equation in Assumption 4.7 (confounding bridge) links the observable conditional p(A_t|S_{t-1}, S_t) to the unobserved p(A_t|U_{t-1}, S_t) via the bridge function h_a. Completeness (Assumption 4.6) ensures this equation has a unique solution.

### Mechanism 3: RKHS-Adversarial Estimation Solves Ill-Posed Integral Equations
The bridge function h_a can be estimated via a minimax optimization over reproducing kernel Hilbert spaces, yielding stable solutions to the ill-posed Fredholm equation. The conditional moment restriction E[Y_t^(a) - h_a(W, S) | Z, S] = 0 is enforced via an adversarial game: h_a minimizes while q ∈ Q maximizes the moment violation. RKHS regularization (||h||_H, ||q||_Q) controls complexity and stabilizes inversion. The closed-form solution shows coefficients α_a are obtained via kernel matrix operations.

## Foundational Learning

- **Concept: Proximal Causal Inference**
  - Why needed here: The identification strategy treats noisy measurements as proxies for unobserved confounders, extending standard adjustment formulas.
  - Quick check question: Can you distinguish treatment-inducing vs. outcome-inducing proxies and explain why two proxies are needed?

- **Concept: Interventional vs. Observational Conditionals**
  - Why needed here: π_opt targets P(A_t^(s)) (interventional), while BC targets P(A_t|S_t) (observational); understanding the difference is essential.
  - Quick check question: In a simple graph U → S, U → A, does P(A|S=s) equal P(A^(s))? Under what condition?

- **Concept: Completeness Conditions in Identification**
  - Why needed here: Both discrete (rank condition) and continuous (completeness assumption) identification rely on sufficient variation in proxies.
  - Quick check question: If P(W|Z,s) has rank < |U|, what fails in the discrete identification formula?

## Architecture Onboarding

- **Component map**: Expert trajectories D_E → Tuple formation (Z_t = S_{t-1}, (Z_t, S_t, W_{t-1}, A_t)) → Estimation layer (discrete: empirical frequency matrices → matrix inversion → p(A_t^(s)); continuous: kernel matrices K_H, K_Q → minimax optimization → h_a → E[h_a(W, s)]) → Policy output: π̂_opt(s) = arg max_a p̂(A_t^(s)=a)

- **Critical path**: (1) Proxy selection (S_{t-1}, W_{t-1}) must satisfy completeness; (2) Coarsening (discrete) or kernel choice (continuous) must yield invertible/well-conditioned operators; (3) Regularization λ_H, λ_Q must be tuned via cross-validation.

- **Design tradeoffs**: Discrete vs. continuous: Discrete requires coarsening (information loss) but is transparent; continuous preserves full information but requires RKHS machinery and regularization tuning. Proxy choice: Using S_{t-1} as treatment-inducing proxy relies on dynamics; if dynamics are weak, completeness fails. Data efficiency: Matrix inversion in discrete case is sample-inefficient for large |U|; RKHS estimation scales better but requires more tuning.

- **Failure signatures**: Singular or ill-conditioned P̂_{W'|Z',s}: Proxies are weak; reduce coarsening resolution or enrich proxy set. High variance in ĥ_a estimates: Insufficient regularization or poor kernel bandwidth; increase λ_H or tune via cross-validation. π̂_opt ≈ π_BC1 or π_BC2: Proxies may not carry additional information about U_{t-1}; check proxy-latent correlations.

- **First 3 experiments**: (1) Sanity check with known U: Generate data where U is observed; verify π̂_opt ≈ π_opt (ground truth) and π̂_opt ≠ π_BC under shift. (2) Ablation on proxy strength: Systematically reduce correlation between proxies (S_{t-1}, W_{t-1}) and U_{t-1}; plot MSE vs. proxy strength to identify breaking points. (3) Shift robustness validation: Apply controlled shifts to P(W|U) and P(U) separately; confirm π̂_opt is stable under measurement shift but degrades appropriately under population shift, while π_BC2 degrades under both.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed framework be extended to handle stochastic expert policies rather than deterministic ones?
- Basis in paper: [explicit] Section 2.1 states, "In this paper, we focus on deterministic policies," and Proposition 3.1 explicitly assumes the expert follows a deterministic mechanism $A_t = \pi_E(S_t, U_{t-1})$.
- Why unresolved: The definition of the causal target $\pi_{opt}$ and the subsequent identification proofs rely on the expert's action being a deterministic function of the state and latent context.
- What evidence would resolve it: A theoretical extension proving identification for stochastic policies or an empirical analysis demonstrating the estimator's robustness to expert stochasticity.

### Open Question 2
Can this causal imitation learning approach be integrated into online or interactive settings to address endogenous distribution shift?
- Basis in paper: [explicit] The Introduction distinguishes the paper's focus from "endogenous shift" (compounding errors), stating, "In this work, our primary focus is the latter—shifts in the latent state distribution and/or measurement process."
- Why unresolved: The current method is designed for offline datasets and corrects for exogenous shifts (measurement/population), but it does not propose a mechanism to correct the learner's trajectory during online deployment.
- What evidence would resolve it: An algorithmic extension that incorporates CausIL into an interactive loop (e.g., DAgger) or experiments evaluating compounding error rates during long-horizon rollouts.

### Open Question 3
How does the potential non-uniqueness of the confounding bridge function affect the convergence of the continuous RKHS estimator?
- Basis in paper: [inferred] The text acknowledges regarding Equation (4.1) that "solutions need not be unique in general," yet the adversarial estimation procedure assumes a valid bridge function can be learned.
- Why unresolved: If the integral equation admits multiple solutions, the minimax estimator might converge to a bridge function that does not correctly recover the interventional distribution $P(A^{(s)}_t)$.
- What evidence would resolve it: A theoretical characterization of conditions ensuring bridge function uniqueness or a sensitivity analysis showing the variance of the learned policy across multiple optimization runs.

## Limitations
- Practical applicability of completeness assumptions (4.6 and 4.9) to real-world data is uncertain and unverified empirically
- Regularization and kernel selection procedures for continuous RKHS estimator are underspecified, making replication challenging
- Empirical validation on PhysioNet data is semi-simulated and lacks comparison to state-of-the-art IL methods

## Confidence

- **High confidence**: Causal interpretation of π_opt as intervention distribution and its invariance under measurement shift
- **Medium confidence**: Identification via proximal causal inference and RKHS-adversarial estimation, though practical implementation details are underspecified
- **Low confidence**: Empirical validation on PhysioNet data is semi-simulated and lacks comparison to state-of-the-art IL methods

## Next Checks

1. **Completeness verification**: Quantify the correlation between (S_{t-1}, W_{t-1}) and U_{t-1} in PhysioNet; assess whether the matrix P_{W|Z,s} is full rank for all s.

2. **Regularization sensitivity**: Systematically vary λ_H and λ_Q in the RKHS estimator; plot MSE vs. regularization strength to identify optimal and stable regions.

3. **Benchmarking against alternatives**: Compare CausIL to BC variants (with and without importance weighting) and to proximal causal inference baselines (e.g., proximal policy optimization) on semi-simulated datasets with known ground truth.