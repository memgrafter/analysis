---
ver: rpa2
title: 'A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning'
arxiv_id: '2512.03363'
source_url: https://arxiv.org/abs/2512.03363
tags:
- quantum
- geometry
- aggregation
- global
- gain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces A2G-QFL, an adaptive aggregation framework
  for federated learning in quantum-enabled and heterogeneous classical networks.
  A2G addresses performance degradation caused by non-uniform client quality, stochastic
  teleportation fidelity, device instability, and geometric mismatch between local
  and global models.
---

# A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning

## Quick Facts
- arXiv ID: 2512.03363
- Source URL: https://arxiv.org/abs/2512.03363
- Reference count: 20
- A2G-QFL improves federated learning accuracy by 13.65 percentage points (25% relative) in quantum-classical hybrid networks

## Executive Summary
This paper introduces A2G-QFL, an adaptive aggregation framework for federated learning in quantum-enabled and heterogeneous classical networks. The method addresses performance degradation caused by non-uniform client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. The framework uses two gains: a QoS gain that modulates client importance based on teleportation fidelity, latency, and instability, and a geometry gain that adjusts global updates according to the curvature and non-Euclidean structure of the parameter space.

## Method Summary
A2G-QFL introduces an adaptive aggregation framework for federated learning in quantum-classical hybrid networks. The method uses two gains: QoS gain (η) that modulates client importance based on teleportation fidelity, latency, and instability, and geometry gain (β) that adjusts global updates according to the curvature and non-Euclidean structure of the parameter space. The framework recovers FedAvg, QoS-aware averaging, and manifold-based aggregation as special cases and provides convergence guarantees under smoothness and bounded-variance assumptions. Experiments on a quantum-classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

## Key Results
- With geometry gain β = 0.05, A2G achieved 68.25% best accuracy
- This represents a 13.65 percentage point (25% relative) improvement over β = 1.0
- Framework recovers FedAvg, QoS-aware averaging, and manifold-based aggregation as special cases

## Why This Works (Mechanism)
The framework works by dynamically adjusting the aggregation process based on two key factors: the quality of service provided by each client (teleportation fidelity, latency, instability) and the geometric properties of the parameter space. The QoS gain ensures that high-quality clients contribute more to the global model, while the geometry gain prevents overshooting during updates in curved or non-Euclidean spaces. This adaptive approach compensates for the heterogeneity and noise inherent in quantum-classical federated learning environments.

## Foundational Learning
- Quantum teleportation fidelity: Needed for reliable quantum state transmission; quick check: measure average fidelity across clients
- Manifold learning: Required for handling non-Euclidean parameter spaces; quick check: verify curvature metrics before/after aggregation
- Federated averaging: Baseline for distributed learning; quick check: compare A2G performance against FedAvg baseline
- Stochastic optimization: Handles noise in quantum systems; quick check: monitor gradient variance across training rounds
- Client heterogeneity: Addresses varying device capabilities; quick check: measure latency and instability distributions
- Convergence analysis: Provides theoretical guarantees; quick check: validate assumptions hold under realistic conditions

## Architecture Onboarding
- Component map: Clients -> QoS Gain Module -> Geometry Gain Module -> Global Model Update
- Critical path: Client data collection -> QoS evaluation -> Weighted aggregation -> Geometry-adjusted update
- Design tradeoffs: Computational overhead vs. accuracy improvement; quick check: measure runtime overhead
- Failure signatures: Poor convergence when gains are misconfigured; quick check: monitor loss curves for divergence
- First experiments: 1) Baseline FedAvg comparison 2) Individual gain ablation study 3) Stress test under extreme noise conditions

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Assumes smooth loss functions and bounded gradient variance, which may not hold in noisy quantum systems
- Experimental results depend on specific quantum teleportation fidelity models that may not generalize
- Convergence guarantees rely on standard assumptions that may be violated in practical quantum systems
- Limited to specific hybrid testbed, may not reflect full complexity of real-world deployments

## Confidence
- Framework design and special case recovery: High
- Experimental accuracy improvements: Medium
- Theoretical convergence guarantees: Medium
- Generalization to diverse quantum hardware: Low

## Next Checks
1. Validate the convergence guarantees under non-smooth loss functions and unbounded gradient variance typical of noisy quantum systems
2. Test the framework's robustness across different quantum hardware platforms with varying teleportation fidelities and error rates
3. Perform ablation studies to quantify the individual contributions of QoS gain and geometry gain to overall performance