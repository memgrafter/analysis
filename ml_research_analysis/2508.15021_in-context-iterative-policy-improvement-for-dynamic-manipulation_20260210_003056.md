---
ver: rpa2
title: In-Context Iterative Policy Improvement for Dynamic Manipulation
arxiv_id: '2508.15021'
source_url: https://arxiv.org/abs/2508.15021
tags:
- policy
- learning
- task
- in-context
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes In-Context Policy Improvement (ICPI), a method
  that uses pre-trained Large Language Models (LLMs) to iteratively improve policies
  for dynamic manipulation tasks through in-context learning. The key idea is to formulate
  policy improvement as a sequence-to-sequence completion task where the LLM predicts
  adjustments to policy parameters based on previous interaction data provided in
  the prompt.
---

# In-Context Iterative Policy Improvement for Dynamic Manipulation

## Quick Facts
- arXiv ID: 2508.15021
- Source URL: https://arxiv.org/abs/2508.15021
- Authors: Mark Van der Merwe; Devesh Jha
- Reference count: 40
- Key outcome: ICPI uses pre-trained LLMs to iteratively improve policies for dynamic manipulation through in-context learning, outperforming alternatives in low-data regimes (≤ 300 examples) across five tasks.

## Executive Summary
This paper proposes In-Context Policy Improvement (ICPI), a method that leverages pre-trained Large Language Models to iteratively improve policies for dynamic manipulation tasks through in-context learning. The approach frames policy improvement as a sequence-to-sequence completion task where the LLM predicts adjustments to policy parameters based on previous interaction data provided in the prompt. The method demonstrates strong performance across five tasks (simulated slide, rope swing, and real robot ball rolling) while requiring no fine-tuning or training.

## Method Summary
ICPI operates by generating a dataset of (policy parameters, state trajectory, parameter adjustment) triplets using brute-force search or hindsight relabeling. For each iteration, the current policy and state error are tokenized and used to retrieve k=20 similar examples from the dataset via KNN search. These examples, along with a task-agnostic prompt, are fed to a pre-trained LLM (e.g., GPT-4o) which predicts the next parameter adjustment. The policy is then updated and executed on the robot or simulator. The method uses relative error tokenization to improve generalization across task parameter variations.

## Key Results
- ICPI achieved mean final policy costs of 0.013, 0.025, 0.007, 0.002, and 17.107 for slide, slide-gc, rope-swing, rope-swing-gc, and roll-gc-real tasks respectively
- Consistently outperformed random shooting, Bayesian optimization, KNN baselines, and other LLM-based approaches
- Ablation study showed relative error tokenization (e_i) outperformed providing raw state and goal (s_t, τ_g)
- Performance remained strong in the low-data regime (≤ 300 examples per task)

## Why This Works (Mechanism)

### Mechanism 1: Sequence Completion as Implicit Optimization
Pre-trained LLMs function as general pattern machines that approximate numerical optimization steps when formatted as sequence-to-sequence completion tasks. The method frames policy improvement as a pattern completion problem rather than a reasoning task. By providing a history of (parameter, error, adjustment) triplets, the LLM predicts the next adjustment by completing the sequence pattern rather than calculating physics gradients.

### Mechanism 2: Algorithm Distillation via KNN Context
Performance depends on retrieving relevant "optimization traces" from a dataset, effectively distilling the behavior of an expert algorithm into the context window. The method constructs a dataset using optimal labels derived from brute-force search or hindsight relabeling. By using K-Nearest Neighbors to inject only relevant examples into the prompt, the system constrains the LLM to mimic the local improvement behavior of the expert algorithm.

### Mechanism 3: Relative Error Tokenization
Encoding state trajectories as relative errors (e = s_t - τ_g) rather than absolute states grounds the input features, enabling better generalization across task parameter variations. Dynamic tasks vary by physical parameters (mass, friction). By tokenizing the error relative to the goal rather than absolute state, the input representation becomes invariant to specific physical constants.

## Foundational Learning

- **Concept: In-Context Learning (ICL) vs. In-Weights Reasoning**
  - Why needed here: The paper explicitly contrasts these. "In-weights" relies on training data knowledge (physics), which failed. "In-context" relies on prompt data (pattern matching), which succeeded.
  - Quick check question: Does the model need to know why the puck slides (physics), or just how the numbers changed in previous slides (pattern)?

- **Concept: Parametric Policy Representation**
  - Why needed here: ICPI operates on low-dimensional policy vectors (θ), not high-dimensional joint trajectories. Understanding how continuous control parameters are tokenized is central to the architecture.
  - Quick check question: Can you map a continuous robot action (e.g., joint angles) to a discrete token sequence without losing the precision required for dynamic tasks?

- **Concept: Dynamic Manipulation Challenges**
  - Why needed here: The paper targets tasks where quasi-static assumptions fail. The system must handle partial observability and complex dynamics (e.g., rope swinging).
  - Quick check question: Why is a "try, fail, adjust" loop more suitable for dynamic manipulation than a single-shot plan?

## Architecture Onboarding

- **Component map:** Dataset D -> KNN Retrieval -> Tokenizer -> LLM -> Executor -> Updated Policy
- **Critical path:** The construction of Dataset D. If the "expert" labels in D are suboptimal or noisy, the LLM will distill flawed improvement strategies.
- **Design tradeoffs:** Context Window vs. Precision (adding more examples provides more context but may introduce noise/dilution); LLM Size vs. Latency (larger models perform better but incur latency and cost)
- **Failure signatures:** High Variance (sensitivity to initial conditions); Tokenization Drift (loss of numeric precision)
- **First 3 experiments:**
  1. Implement the "Slide" task with raw state inputs (s, g) vs. relative error e to verify that relative error yields lower cost
  2. Test performance degradation as the dataset D size is reduced from 300 to 50 to verify the "low-data regime" claim
  3. Run a simple Gradient Descent or Random Search on the same parametric policy to confirm the LLM is actually adding value

## Open Questions the Paper Calls Out

- **Open Question 1:** Can in-context policy improvement scale to high-dimensional tasks without manual feature engineering?
  - Basis: The authors manually selected relative error features; it's unknown if LLMs can autonomously perform necessary feature extraction for complex, high-dimensional state spaces.
  - Resolution: Experiments applying ICPI to tasks with raw, high-dimensional inputs (e.g., images) without manual feature preprocessing.

- **Open Question 2:** Can self-play or demonstrations effectively replace brute-force search for constructing the policy improvement dataset?
  - Basis: The current method relies on brute-force search or hindsight labeling to generate the dataset Δθ, which may be infeasible for complex tasks.
  - Resolution: A study comparing ICPI performance when the dataset D is generated via autonomous self-play or human demonstrations versus the oracle-based method.

- **Open Question 3:** Do robotics-specific transformer models offer advantages over general-purpose LLMs for in-context policy iteration?
  - Basis: This study utilized proprietary general-purpose models; the benefits of domain-specific pre-training remain unquantified.
  - Resolution: Benchmarks comparing the performance and data efficiency of general LLMs against robotics-trained transformers on the proposed ICPI task.

## Limitations

- Performance depends critically on the quality and representativeness of the KNN dataset, with no characterization of sensitivity to dataset size, noise levels, or normalization schemes
- Real robot experiment (Roll-GC-Real) shows notably higher variance than simulation results, suggesting reduced robustness to real-world stochasticity
- The claim of consistent outperformance is questionable given the high variance in real robot results and lack of statistical significance testing

## Confidence

- **High Confidence:** The core mechanism of using LLMs for sequence completion to predict policy adjustments is well-supported by experimental results and ablation studies
- **Medium Confidence:** The performance advantage over baselines is demonstrated, but the experimental setup may favor the proposed method and doesn't adequately address whether baselines were properly tuned
- **Low Confidence:** The claim of consistent outperformance across all tasks is questionable given the high variance in real robot results and lack of statistical significance testing

## Next Checks

1. **Dataset Sensitivity Analysis:** Systematically vary the size and quality of the KNN dataset (from 50 to 500 examples) and measure the impact on performance across all five tasks to validate the "low-data regime" claim.

2. **Real-World Robustness Test:** Conduct multiple trials of the Roll-GC-Real task under varying initial conditions (different starting positions, lighting conditions, surface properties) to quantify the variance and identify failure modes in physical deployment.

3. **Baseline Re-implementation:** Implement and tune alternative methods (gradient descent, CMA-ES, model-based RL) on the same parametric policy representation to establish a fair comparison baseline and determine whether ICPI's advantage stems from the LLM approach or simply the choice of policy parameterization.