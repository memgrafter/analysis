---
ver: rpa2
title: Cross-Lingual Interleaving for Speech Language Models
arxiv_id: '2512.01865'
source_url: https://arxiv.org/abs/2512.01865
tags:
- cross-lingual
- speech
- language
- interleaving
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building spoken language
  models (SLMs) that can understand and generate speech across multiple languages
  without relying on textual supervision. The core method introduces a cross-lingual
  interleaving strategy that concatenates sentence-aligned speech token sequences
  from different languages within the same training sequence, encouraging a shared
  representational subspace.
---

# Cross-Lingual Interleaving for Speech Language Models

## Quick Facts
- arXiv ID: 2512.01865
- Source URL: https://arxiv.org/abs/2512.01865
- Reference count: 0
- Primary result: Cross-lingual interleaving improves multilingual spoken language model performance without textual supervision.

## Executive Summary
This paper introduces cross-lingual interleaving for speech language models (SLMs), a method that concatenates sentence-aligned speech token sequences from different languages within the same training sequence. The approach enables SLMs to learn shared representations across languages without relying on textual supervision. Experiments on 360M and 1B parameter models show improvements in both monolingual semantic accuracy and cross-lingual continuation capabilities, with bilingual fine-tuning restoring most monolingual performance lost during interleaving.

## Method Summary
The method employs a three-stage training approach: (1) English-only pretraining on 50k steps, (2) cross-lingual interleaving for 20k steps where aligned English-French sentences are concatenated into single training sequences with 0.5 language sampling probability, and (3) bilingual fine-tuning without interleaving for 15k steps. Models use Mimi tokenizer (12.5 Hz, 2048 vocab) and decoder-only Transformers (360M with Qwen2 init, 1B with Llama 3.2 init). The approach is evaluated on spoken StoryCloze/TopicCloze benchmarks using 42k-hour English-French TinyStories corpus with sentence-level alignments.

## Key Results
- Monolingual StoryCloze scores improve after interleaving (58.31% vs 55.31% for 360M French, 70.39% vs 67.07% for 1B French)
- Cross-lingual continuation accuracy approaches monolingual performance (within few points)
- Hidden-state alignment strengthens (cosine similarity increases from 0.73 to 0.75)
- Bilingual fine-tuning restores most monolingual performance lost during interleaving

## Why This Works (Mechanism)
Cross-lingual interleaving forces the model to process multiple languages within shared context windows, encouraging the development of language-agnostic representations. By concatenating aligned sentence pairs during training, the model learns to maintain coherent representations across language boundaries while preserving task-specific semantic understanding. The subsequent bilingual fine-tuning phase helps stabilize language-specific features that may be disrupted during the interleaving phase.

## Foundational Learning
- **Cross-lingual speech representation learning**: Models must learn to map different languages into a shared acoustic space - check by comparing monolingual vs cross-lingual semantic accuracy
- **Sentence-level alignment**: Requires precise temporal alignment between languages - validate by examining alignment quality in TinyStories corpus
- **Multilingual token interleaving**: Training sequences combine multiple languages - verify by checking language sampling probability and sequence construction
- **Bilingual fine-tuning**: Restores monolingual capabilities after cross-lingual training - monitor by tracking monolingual metrics through all three training stages

## Architecture Onboarding

**Component Map**: Speech data -> Mimi tokenizer -> 2048-token Transformer -> Cross-lingual interleaving -> Bilingual fine-tuning -> Evaluation

**Critical Path**: The interleaving mechanism is central - models must successfully learn from concatenated multilingual sequences during stage 2, as this drives the cross-lingual capabilities.

**Design Tradeoffs**: Interleaving improves cross-lingual performance but initially degrades monolingual metrics, requiring the bilingual fine-tuning stage to recover monolingual capabilities. The 0.5 language sampling probability balances exposure between languages.

**Failure Signatures**: If cross-lingual metrics don't improve, check that interleaving actually alternates languages at sentence boundaries and that language sampling probability is correctly set to 0.5.

**First Experiments**:
1. Verify tokenizer consistency across all three training stages
2. Check language balance in interleaved sequences (should be ~50/50)
3. Monitor sBLiMP/sWUGGY metrics during bilingual fine-tuning to ensure monolingual recovery

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to English-French pair with similar writing systems
- No evaluation on languages with different phonetic inventories or writing systems
- Trade-off between cross-lingual integration and monolingual retention not fully characterized
- Benefits of interleaving on languages with greater typological distance untested

## Confidence
- **High** for monolingual semantic accuracy improvements on sSC/sTC within English-French
- **Medium** for hidden-state alignment benefits (cosine similarity metric is internal)
- **Low** for practical utility on typologically diverse language pairs

## Next Checks
1. Reconstruct or obtain the exact sentence-aligned EN–FR speech data to verify interleaving gains aren't data artifacts
2. Systematically vary language sampling probability during interleaving and fine-tuning duration to optimize trade-offs
3. Evaluate interleaving approach on a language pair with greater typological distance (e.g., English–Mandarin) and larger multilingual corpus