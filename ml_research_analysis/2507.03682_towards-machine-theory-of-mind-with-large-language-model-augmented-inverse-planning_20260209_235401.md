---
ver: rpa2
title: Towards Machine Theory of Mind with Large Language Model-Augmented Inverse
  Planning
arxiv_id: '2507.03682'
source_url: https://arxiv.org/abs/2507.03682
tags:
- laip
- agent
- room
- restaurant
- hypotheses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LLM-AUGMENTED INVERSE PLANNING (LAIP), a hybrid
  model that combines large language models (LLMs) with Bayesian inverse planning
  for Theory of Mind (ToM) tasks. The method uses LLMs to generate hypotheses about
  an agent's beliefs and desires, then employs inverse planning to compute posterior
  probabilities of these mental states based on observed actions.
---

# Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning

## Quick Facts
- **arXiv ID**: 2507.03682
- **Source URL**: https://arxiv.org/abs/2507.03682
- **Reference count**: 40
- **Primary result**: LAIP achieves 48.4% posterior probability on correct hypotheses vs 11.9% for zero-shot CoT in restaurant preference tasks

## Executive Summary
This paper introduces LLM-AUGMENTED INVERSE PLANNING (LAIP), a hybrid approach that combines large language models with Bayesian inverse planning for Theory of Mind reasoning. The method uses LLMs to generate hypotheses about an agent's mental states (beliefs and desires), then employs inverse planning to compute posterior probabilities based on observed actions. LAIP addresses the scalability limitations of traditional Bayesian models and the brittleness of pure LLM approaches to ToM tasks. In experiments with restaurant navigation and complex visual environments, LAIP significantly outperforms zero-shot LLM baselines and chain-of-thought prompting while maintaining strong correlation with optimal Bayesian models.

## Method Summary
LAIP works by first using an LLM to generate multiple hypotheses about an agent's preferences or beliefs, each with associated prior probabilities. For each hypothesis, the LLM predicts the likelihood of possible actions the agent might take. When observing the agent's actual action, LAIP computes posterior probabilities over the hypotheses using Bayesian updating, either through mathematical normalization or LLM assistance. This decomposition allows LAIP to scale to more complex environments than traditional Bayesian models while maintaining the systematic inference capabilities of inverse planning approaches. The method was tested across multiple LLM sizes (GPT-4o, GPT-4o-mini, GPT-3.5, LLaMA 3-70B/8B, Mixtral, Gemma 2) and showed particular efficiency gains for smaller models.

## Key Results
- LAIP achieved 48.4% posterior probability on correct hypotheses versus 11.9% for zero-shot CoT in restaurant preference tasks
- Strong correlation (r=0.94) with optimal Bayesian models while requiring fewer samples
- On MMToM-QA benchmark, LAIP achieved 67.5% accuracy versus 34% for GPT-4 baseline
- LAIP improved performance for smaller LLMs (like LLaMA 3-8B) more than larger ones, with Cohen's d up to 1.59

## Why This Works (Mechanism)
LAIP works by decomposing Theory of Mind reasoning into two tractable subtasks: hypothesis generation and likelihood estimation. LLMs excel at generating diverse hypotheses about mental states based on limited observations, while inverse planning provides the formal probabilistic framework for updating beliefs based on actions. By separating these components, LAIP avoids the combinatorial explosion of traditional Bayesian models while maintaining rigorous inference. The approach also benefits from LLMs' ability to handle partial observability and contextual constraints that would be difficult to encode in hand-crafted models.

## Foundational Learning
- **Bayesian Inverse Planning**: Mathematical framework for inferring agent goals from actions by inverting planning models. Needed to compute posterior probabilities of hypotheses given observed behavior.
- **Hypothesis Generation with LLMs**: Using LLMs to propose diverse mental state hypotheses with associated priors. Needed because enumerating all possible preferences is computationally intractable.
- **Likelihood Estimation**: Computing P(action|hypothesis) to determine how well each hypothesis explains observed behavior. Critical for Bayesian updating.
- **Theory of Mind Reasoning**: Inferring beliefs and desires of other agents from their actions. The target task that LAIP aims to solve efficiently.
- **Posterior Computation**: Combining priors, likelihoods, and observations to update belief distributions. Can be done mathematically or via LLM prompting.
- **Cosine Similarity for Action Matching**: Used in open-ended scenarios to compare observed actions with generated hypotheses. Enables flexible matching when exact action correspondence is unavailable.

## Architecture Onboarding

**Component Map**
Restaurant Environment -> Hypothesis Generator (LLM) -> Likelihood Generator (LLM) -> Posterior Calculator -> Evaluation Metrics

**Critical Path**
1. Generate N hypotheses about agent preferences using LLM
2. For each hypothesis, generate action likelihoods using LLM
3. Observe agent action
4. Compute posterior P(H|A) using Bayesian updating
5. Evaluate against optimal model and baselines

**Design Tradeoffs**
- Fixed vs. dynamic hypothesis set: Current implementation uses fixed N=20 hypotheses, which is computationally expensive but ensures comprehensive coverage
- LLM vs. mathematical posterior computation: Mathematical computation improves smaller model performance but may be less flexible for complex scenarios
- Single vs. multi-step inference: LAIP currently handles single action updates but could be extended to sequential reasoning

**Failure Signatures**
- Non-normalized probability distributions from LLMs (fix: explicit normalization)
- LLMs failing at mathematical operations (fix: use mathematical computation for smaller models)
- Action mismatch in open-ended scenarios (fix: use cosine similarity with embedding model)

**Three First Experiments**
1. Implement restaurant environment and 10 trajectories, verify basic navigation mechanics
2. Test hypothesis generation prompt with GPT-4o, verify 20 diverse preference hypotheses are generated
3. Compare mathematical vs LLM posterior computation on a single trajectory with LLaMA 3-8B

## Open Questions the Paper Calls Out
- Can Sequential Monte Carlo methods be integrated to dynamically prune and revise hypotheses, reducing computational cost?
- How do LLM-generated hypotheses propagate or amplify social biases in Theory of Mind reasoning?
- Can LAIP be adapted to rationally allocate computational resources based on error costs?

## Limitations
- Underspecified LLM prompts for likelihood generation and posterior computation hinder exact reproduction
- No empirical evaluation of bias or fairness in generated hypotheses
- Fixed hypothesis set approach is computationally expensive and less adaptive than human cognition

## Confidence
- **High Confidence**: LAIP significantly outperforms zero-shot LLM baselines on ToM tasks (48.4% vs 11.9% accuracy)
- **Medium Confidence**: Strong correlation with optimal Bayesian models while being more scalable (r=0.94 reported)
- **Medium Confidence**: Decomposition approach particularly benefits smaller LLMs, though effect sizes vary

## Next Checks
1. **Prompt Reproduction Test**: Implement hypothesis generation prompt from Figure 8 and create parallel prompts for likelihood generation P(A|H_k) and posterior computation. Run small-scale test with GPT-4o and compare generated probabilities against paper's reported distributions.
2. **Mathematical vs LLM Posterior Computation**: Implement both mathematical normalization and LLM-based posterior computation methods. Test both approaches across LLaMA 3-8B and LLaMA 3-70B on a subset of trajectories to verify the paper's finding that mathematical computation improves smaller model performance.
3. **Optimal Model Implementation Verification**: Implement the optimal Bayesian model using Îµ=0.01 random action parameter, and compute Pearson correlation, Spearman correlation, and Jensen-Shannon divergence on a subset of 3 trajectories. Compare results against paper's reported optimal model values.