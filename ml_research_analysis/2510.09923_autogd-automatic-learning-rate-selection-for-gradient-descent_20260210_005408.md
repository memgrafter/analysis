---
ver: rpa2
title: 'AutoGD: Automatic Learning Rate Selection for Gradient Descent'
arxiv_id: '2510.09923'
source_url: https://arxiv.org/abs/2510.09923
tags:
- learning
- autogd
- rate
- such
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoGD is a gradient descent method that automatically selects
  learning rates by comparing the performance of neighboring (larger and smaller)
  rates at each iteration. The algorithm uses an Armijo condition to ensure sufficient
  descent and includes a "no movement" option to avoid divergence.
---

# AutoGD: Automatic Learning Rate Selection for Gradient Descent

## Quick Facts
- arXiv ID: 2510.09923
- Source URL: https://arxiv.org/abs/2510.09923
- Reference count: 40
- Primary result: Automatic learning rate selection method matching standard GD's optimal convergence rate

## Executive Summary
AutoGD is a gradient descent method that automatically selects learning rates at each iteration by comparing the performance of neighboring (larger and smaller) rates. The algorithm uses an Armijo condition to ensure sufficient descent and includes a "no movement" option to avoid divergence. AutoGD is initialized with diffuse starting points and does not require knowledge of smoothness constants. Theoretical analysis establishes both asymptotic and non-asymptotic convergence rates that match the optimal rate of standard GD (up to a constant).

## Method Summary
AutoGD works by maintaining three candidate learning rates at each iteration - a base rate and its two neighbors (larger and smaller). The algorithm evaluates the objective function at each candidate and selects the one that satisfies the Armijo condition while providing sufficient descent. If no candidate meets the criteria, the algorithm can choose to make no movement to avoid divergence. The method is initialized with diffuse starting points for the three candidates and does not require prior knowledge of problem-specific smoothness constants. Extensions to second-order methods (AutoBFGS and AutoLBFGS) adapt this automatic selection framework to incorporate curvature information.

## Key Results
- AutoGD achieves convergence rates matching standard gradient descent up to a constant factor
- The method is robust to initial learning rate choice and outperforms or matches other first-order methods
- AutoBFGS and AutoLBFGS extensions show substantial performance improvements in practice

## Why This Works (Mechanism)
AutoGD automatically adapts the learning rate at each iteration by comparing the performance of neighboring rates, effectively performing a local line search without requiring multiple full function evaluations. The Armijo condition ensures sufficient descent while the "no movement" option provides a safety mechanism against divergence. By maintaining three candidates and selecting based on empirical performance, the method can adapt to problem-specific geometry without requiring prior knowledge of smoothness constants or other problem parameters.

## Foundational Learning
- Gradient descent optimization: Fundamental to understanding how learning rates affect convergence speed and stability
- Armijo condition: Needed to ensure sufficient descent while allowing adaptive step size selection
- Line search methods: Provides context for how AutoGD performs local optimization without full function evaluations
- Smoothness constants: Understanding why AutoGD's approach of not requiring these constants is advantageous
- First-order optimization methods: Establishes the baseline performance that AutoGD aims to match or exceed

## Architecture Onboarding

**Component map:** Candidate rates -> Armijo evaluation -> Rate selection -> Gradient update

**Critical path:** At each iteration: compute gradient -> evaluate candidates with Armijo condition -> select rate -> update parameters

**Design tradeoffs:** 
- Multiple gradient evaluations per iteration vs. adaptive learning rate selection
- Computational overhead vs. robustness to initialization
- Simplicity of first-order methods vs. potential gains from second-order information

**Failure signatures:**
- Divergence when all candidates fail Armijo condition (mitigated by "no movement" option)
- Slow convergence if initial candidates are poorly chosen
- Computational overhead becoming prohibitive in very high dimensions

**First experiments:**
1. Simple convex quadratic minimization to verify basic functionality
2. Comparison with standard GD on a smooth non-convex function
3. Test robustness to initialization by varying starting candidate rates

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of multiple gradient evaluations per iteration may limit scalability
- Performance claims primarily validated against first-order methods with limited second-order comparison
- Initial learning rate choice still required, though the method is robust to this choice

## Confidence

**Theoretical convergence analysis:** High
**Empirical performance claims:** Medium  
**Scalability and computational efficiency:** Low
**Robustness across diverse problem classes:** Medium

## Next Checks

1. Benchmark AutoGD against second-order methods (Newton, L-BFGS) and adaptive methods (Adam, AdaGrad) on large-scale problems to establish practical competitiveness.

2. Analyze the computational overhead of AutoGD relative to standard GD across varying problem sizes to determine practical scalability limits.

3. Test AutoGD on non-convex problems with multiple local minima and saddle points to evaluate its robustness in challenging optimization landscapes.