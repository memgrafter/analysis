---
ver: rpa2
title: 'Development and Evaluation of HopeBot: an LLM-based chatbot for structured
  and interactive PHQ-9 depression screening'
arxiv_id: '2507.05984'
source_url: https://arxiv.org/abs/2507.05984
tags:
- hopebot
- were
- health
- phq-9
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HopeBot is an LLM-powered conversational chatbot that administers
  the PHQ-9 depression screening with real-time clarification and safety guidance.
  In a within-subject study of 132 adults, HopeBot-assisted scores showed high concordance
  with self-administered PHQ-9 results (ICC = 0.92; median absolute difference = 1
  point).
---

# Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening

## Quick Facts
- arXiv ID: 2507.05984
- Source URL: https://arxiv.org/abs/2507.05984
- Reference count: 40
- One-line primary result: LLM-powered chatbot administering PHQ-9 depression screening with real-time clarification and safety guidance shows high concordance (ICC=0.92) with self-administered results and strong user acceptance

## Executive Summary
HopeBot is an LLM-based chatbot designed to administer the PHQ-9 depression screening in a conversational format. The system provides real-time clarification, interpretive guidance, and safety recommendations throughout the assessment. In a within-subject study of 132 adults, HopeBot-assisted PHQ-9 scores showed high concordance with self-administered scores, with a median absolute difference of only one point. The chatbot received high usability ratings and demonstrated strong potential as a scalable, low-burden tool for routine depression screening.

## Method Summary
The study employed a within-subject design where participants completed both the standard PHQ-9 questionnaire and the HopeBot-assisted version. The sample consisted of 132 adults recruited through Amazon Mechanical Turk. The chatbot administered the PHQ-9 questions conversationally while providing real-time clarification and safety guidance. Usability was assessed through multiple dimensions including comfort, voice clarity, handling of sensitive topics, and recommendation helpfulness. A subset of 75 participants directly compared both formats and reported their trust preferences.

## Key Results
- HopeBot-assisted PHQ-9 scores showed high concordance with self-administered scores (ICC = 0.92; median absolute difference = 1 point)
- 71% of participants reported greater trust in the chatbot due to its clearer structure, interpretive guidance, and supportive tone
- Mean usability ratings were high across all measured dimensions, with comfort rated 8.4, voice clarity 7.7, and handling sensitive topics 7.6
- 87.1% of participants expressed willingness to reuse or recommend HopeBot

## Why This Works (Mechanism)
The chatbot's effectiveness stems from its ability to provide structured, interactive guidance that addresses common barriers in traditional screening. By offering real-time clarification and interpretive support, HopeBot reduces ambiguity in question interpretation and provides immediate context for responses. The conversational format appears to increase engagement and comfort with sensitive topics, while the embedded safety guidance adds value beyond simple symptom assessment. The system's ability to maintain high concordance with traditional screening while improving user experience suggests that LLM-powered conversational interfaces can enhance the delivery of standardized mental health assessments.

## Foundational Learning
1. PHQ-9 Depression Screening
   - Why needed: Standard validated tool for depression assessment
   - Quick check: 9-item questionnaire measuring depressive symptoms over past 2 weeks

2. Within-Subject Study Design
   - Why needed: Controls for individual differences by having same participants complete both methods
   - Quick check: Each participant completes both HopeBot and standard PHQ-9

3. ICC (Intraclass Correlation Coefficient)
   - Why needed: Measures agreement between two assessment methods
   - Quick check: ICC=0.92 indicates excellent concordance between methods

4. Amazon Mechanical Turk Sampling
   - Why needed: Provides access to diverse adult population for testing
   - Quick check: Online platform for recruiting research participants

5. LLM-powered Conversational Interface
   - Why needed: Enables natural language interaction and real-time guidance
   - Quick check: Large language model processes user responses and provides contextual support

6. Mental Health Safety Guidance Integration
   - Why needed: Adds clinical value beyond symptom assessment
   - Quick check: Automated provision of resources and recommendations based on responses

## Architecture Onboarding
Component Map: User Input -> LLM Processing -> PHQ-9 Question Logic -> Response Analysis -> Safety Guidance Module -> Output Generation

Critical Path: User response → LLM interpretation → Question progression → Safety threshold checking → Final score calculation → Guidance provision

Design Tradeoffs: Conversational flexibility vs. structured assessment adherence; real-time processing vs. response accuracy; automated guidance vs. clinical oversight

Failure Signatures: Misinterpretation of user responses leading to incorrect question progression; safety threshold miscalculations; guidance provision delays; scoring inconsistencies

First Experiments:
1. Test LLM's ability to accurately interpret diverse phrasings of PHQ-9 response options
2. Validate safety guidance triggers against clinical safety thresholds
3. Measure response time impact of real-time clarification features on overall assessment duration

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Sample size of 132 participants may limit generalizability to broader populations
- Recruitment through Amazon Mechanical Turk may introduce selection bias
- Study did not include clinical validation against formal diagnostic interviews
- Performance with diverse populations (languages, cultural contexts, severe depression) remains untested

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Usability metrics and user acceptance are robust | High |
| Concordance with self-administered PHQ-9 scores | Medium |
| Subjective trust reports (71% preference) | Medium |

## Next Checks
1. Conduct clinical validation comparing HopeBot scores against structured clinical interviews (SCID) in diverse patient populations
2. Test chatbot performance across different demographic groups including non-English speakers and varying digital literacy levels
3. Implement randomized controlled trial in primary care settings to evaluate impact on screening completion rates and depression detection compared to paper-based methods