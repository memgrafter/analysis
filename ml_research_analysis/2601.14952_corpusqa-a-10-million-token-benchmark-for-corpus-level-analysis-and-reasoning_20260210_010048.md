---
ver: rpa2
title: 'CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning'
arxiv_id: '2601.14952'
source_url: https://arxiv.org/abs/2601.14952
tags:
- reasoning
- arxiv
- benchmark
- data
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CorpusQA, a benchmark for corpus-level reasoning
  over documents up to 10 million tokens. It addresses the gap in existing long-context
  benchmarks that assume sparse evidence, whereas real-world corpus analysis requires
  holistic integration of highly dispersed information.
---

# CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning

## Quick Facts
- arXiv ID: 2601.14952
- Source URL: https://arxiv.org/abs/2601.14952
- Reference count: 20
- CorpusQA addresses the gap in long-context benchmarks by requiring holistic integration of highly dispersed information across up to 10 million tokens

## Executive Summary
This paper introduces CorpusQA, a benchmark designed to evaluate corpus-level reasoning over documents up to 10 million tokens in length. The benchmark addresses a critical gap in existing long-context evaluations, which typically assume sparse evidence distribution, whereas real-world corpus analysis requires integrating information dispersed across vast document collections. The authors developed a schema-driven pipeline that extracts structured data from documents, generates complex queries, and programmatically computes ground-truth answers without relying on potentially fallible LLM annotations. This approach ensures reliable evaluation of models' ability to perform holistic analysis across large-scale document collections.

## Method Summary
The authors propose a novel schema-driven pipeline for creating CorpusQA that extracts structured data from documents and generates complex queries requiring holistic integration of dispersed information. Rather than relying on LLM annotations for ground truth, which can be unreliable, the pipeline programmatically computes answers through systematic data extraction and query generation. This ensures evaluation reliability while creating challenging tasks that reflect real-world corpus-level reasoning requirements. The benchmark specifically targets scenarios where evidence is not sparse but highly dispersed across large document collections, pushing beyond the assumptions of existing long-context benchmarks.

## Key Results
- Even state-of-the-art long-context LLMs struggle significantly as input length increases beyond 10 million tokens
- Standard retrieval-augmented generation (RAG) systems fail entirely at large scales, unable to handle the complexity of corpus-level reasoning
- Memory-augmented agentic architectures show greater resilience to scale, suggesting they are better suited for this task than traditional context extension approaches

## Why This Works (Mechanism)
CorpusQA works by explicitly modeling the real-world challenge of corpus-level reasoning where information is highly dispersed rather than sparsely distributed. The schema-driven pipeline creates queries that require holistic integration of evidence across large document collections, forcing models to demonstrate true understanding rather than pattern matching. By programmatically computing ground-truth answers rather than relying on LLM annotations, the benchmark ensures reliable evaluation of models' reasoning capabilities at scale. The focus on structured data extraction enables precise measurement of how well models can navigate and synthesize information across vast document collections.

## Foundational Learning
- **Corpus-level reasoning**: Understanding how to integrate information across large document collections is essential for real-world applications like legal research, medical literature review, and enterprise knowledge management. Quick check: Can the model identify relationships between concepts scattered across thousands of pages?
- **Long-context processing**: Models must handle inputs up to 10 million tokens while maintaining coherence and reasoning ability. Quick check: Does performance degrade predictably as context length increases?
- **Memory-augmented architectures**: These approaches show promise for handling large-scale reasoning by maintaining external memory structures. Quick check: Can the model effectively retrieve and integrate relevant information from memory during reasoning?

## Architecture Onboarding
**Component Map:** Schema extraction -> Query generation -> Ground-truth computation -> Model evaluation -> Performance analysis
**Critical Path:** Schema extraction → Query generation → Ground-truth computation → Model evaluation
**Design Tradeoffs:** The benchmark trades synthetic data generation for reliable ground-truth computation, prioritizing evaluation accuracy over natural data authenticity
**Failure Signatures:** RAG systems fail completely at scale, while traditional LLMs show performance degradation that increases with context length
**First Experiments:** 1) Test memory-augmented models on corpus-level reasoning tasks 2) Compare performance across different document types and domains 3) Evaluate fine-tuned models on external long-context benchmarks

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The benchmark relies on synthetic data generation, which may not fully capture the complexity of naturally occurring corpus-level reasoning tasks
- The focus on structured data extraction may not translate to scenarios requiring interpretation of unstructured or multimodal information
- Performance improvements from fine-tuning may not generalize across different domains beyond the benchmark's scope

## Confidence
- State-of-the-art long-context models struggle significantly at larger scales: High confidence
- RAG systems fail entirely at large scales: High confidence
- Fine-tuning improvements on external tasks suggest generalization potential: Medium confidence

## Next Checks
1. Evaluate model performance on naturally occurring corpus-level reasoning tasks to assess whether synthetic data generation captures real-world complexity
2. Test memory-augmented architectures on diverse document types (legal, medical, technical) to verify robustness across domains
3. Conduct ablation studies on the schema-driven generation pipeline to identify which components most significantly impact benchmark difficulty and model performance