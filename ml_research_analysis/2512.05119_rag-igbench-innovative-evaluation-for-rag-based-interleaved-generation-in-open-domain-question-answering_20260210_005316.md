---
ver: rpa2
title: 'RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in
  Open-domain Question Answering'
arxiv_id: '2512.05119'
source_url: https://arxiv.org/abs/2512.05119
tags:
- wang
- evaluation
- arxiv
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAG-IGBench, a comprehensive benchmark for
  evaluating interleaved image-text generation in open-domain question answering.
  The benchmark addresses challenges in multimodal content evaluation by introducing
  innovative metrics for text quality (ROUGE scores), image quality (edit distance
  and Kendall correlation), and image-text coherence (CLIP-score and semantic alignment).
---

# RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering
## Quick Facts
- arXiv ID: 2512.05119
- Source URL: https://arxiv.org/abs/2512.05119
- Reference count: 40
- Primary result: Comprehensive benchmark for evaluating interleaved image-text generation in open-domain QA with innovative metrics for multimodal content quality

## Executive Summary
RAG-IGBench introduces a novel benchmark for evaluating interleaved image-text generation in open-domain question answering systems. The benchmark addresses the challenge of multimodal content evaluation by introducing innovative metrics for text quality (ROUGE scores), image quality (edit distance and Kendall correlation), and image-text coherence (CLIP-score and semantic alignment). Built from 6,057 curated samples drawn from social platforms, the dataset features complex multimodal queries requiring both visual and textual responses, demonstrating strong correlation between automated metrics and human assessments.

## Method Summary
The benchmark employs a comprehensive evaluation framework that combines traditional text metrics with novel multimodal assessment tools. Text quality is measured using ROUGE scores, while image quality is evaluated through edit distance and Kendall correlation metrics. Image-text coherence is assessed using CLIP-score and semantic alignment measurements. The dataset curation process involved filtering and organizing multimodal content from social platforms to create diverse, complex query-response pairs that require both visual and textual generation capabilities.

## Key Results
- Benchmark demonstrates strong correlation between automated metrics and human assessments for multimodal content quality
- Fine-tuned models show improved performance across text, image, and image-text coherence evaluation tasks
- Dataset successfully captures complex multimodal reasoning patterns through 6,057 curated samples from social platforms

## Why This Works (Mechanism)
The benchmark works by providing a comprehensive evaluation framework that addresses the unique challenges of interleaved image-text generation. The combination of text quality metrics, image quality assessments, and image-text coherence measures creates a holistic evaluation approach. The social platform-sourced data provides real-world complexity and diversity in query types, while the fine-tuning results validate the dataset's effectiveness in improving model performance across multiple evaluation dimensions.

## Foundational Learning
- Multimodal QA evaluation: Understanding how to assess combined image-text generation quality is crucial for developing comprehensive benchmarks
- CLIP-score implementation: Familiarity with CLIP model for measuring image-text semantic alignment is essential for the coherence metrics
- Social media content curation: Knowledge of filtering and organizing diverse multimodal content from social platforms is necessary for dataset creation
- ROUGE metric adaptation: Understanding how traditional text evaluation metrics can be extended to multimodal contexts
- Kendall correlation application: Recognizing how statistical correlation measures can assess image quality in generation tasks
- Edit distance for images: Understanding how pixel-level comparison can evaluate image generation quality

## Architecture Onboarding
Component Map: Social platform data -> Preprocessing pipeline -> Multimodal query generation -> Text/image evaluation metrics -> Coherence assessment
Critical Path: Data curation → Query-response pair creation → Text quality evaluation (ROUGE) → Image quality evaluation (edit distance/Kendall) → Image-text coherence (CLIP-score/semantic alignment) → Human validation
Design Tradeoffs: Comprehensive evaluation vs. computational complexity; social platform diversity vs. potential bias; automated metrics vs. human assessment correlation
Failure Signatures: Poor text-image coherence despite good individual scores; inconsistent human assessment results; model performance degradation on specialized domains
First 3 Experiments: 1) Run baseline MLLM on curated samples to establish performance baselines; 2) Test automated metrics correlation with human assessments on sample subset; 3) Evaluate fine-tuned model performance across all three evaluation dimensions

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on social platform data which may introduce bias in query types and content styles
- Evaluation metrics require further validation across diverse domains beyond current dataset scope
- Correlation between automated metrics and human assessments needs independent verification with larger panels

## Confidence
High confidence in: Technical implementation of the benchmark and core methodology for interleaved image-text generation evaluation; dataset curation process and evaluation framework design
Medium confidence in: Generalizability across different domains and query types; long-term stability of automated metric and human assessment correlation
Low confidence in: Benchmark's ability to capture all aspects of multimodal reasoning complexity, especially for specialized domains requiring domain-specific knowledge

## Next Checks
1. Conduct cross-domain validation studies to assess benchmark performance on specialized domains (medical, legal, technical) beyond social platform content
2. Perform independent human evaluation studies with diverse annotator pools to verify reported correlation between automated metrics and human assessments
3. Test benchmark robustness against adversarial queries and edge cases to evaluate model performance under challenging conditions