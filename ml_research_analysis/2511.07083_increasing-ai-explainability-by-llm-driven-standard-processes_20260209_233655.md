---
ver: rpa2
title: Increasing AI Explainability by LLM Driven Standard Processes
arxiv_id: '2511.07083'
source_url: https://arxiv.org/abs/2511.07083
tags:
- reasoning
- processes
- decision
- each
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes embedding LLM reasoning within formalized analytical
  processes to enhance AI explainability. The approach uses layered architecture that
  separates the LLM's opaque reasoning from transparent decision models like QOC,
  Sensitivity Analysis, Game Theory, and Risk Management.
---

# Increasing AI Explainability by LLM Driven Standard Processes

## Quick Facts
- arXiv ID: 2511.07083
- Source URL: https://arxiv.org/abs/2511.07083
- Authors: Marc Jansen; Marcel Pehlke
- Reference count: 19
- Primary result: LLM-driven layered architecture achieves 56-54% human decision agreement across analytical frameworks

## Executive Summary
This paper proposes embedding LLM reasoning within formalized analytical processes to enhance AI explainability. The approach uses a layered architecture that separates the LLM's opaque reasoning from transparent decision models like QOC, Sensitivity Analysis, Game Theory, and Risk Management. Empirical evaluations show the system can reproduce human-level decision logic in decentralized governance, systems analysis, and strategic reasoning contexts. The QOC framework achieved 56-54% agreement with human decisions across models, with GPT-5 showing the best performance at 23.5% false positives and 0.303 significance.

## Method Summary
The method employs a layered architecture where LLM reasoning is embedded within standardized analytical frameworks including Question-Answer-Consideration (QOC), Sensitivity Analysis, Game Theory, and Risk Management models. The architecture separates opaque LLM reasoning from transparent decision models, allowing for structured analysis while maintaining explainability. The system was evaluated across three domains: decentralized governance, systems analysis, and strategic reasoning, with performance measured against human decision-making benchmarks and analytical rubric scoring.

## Key Results
- QOC framework achieved 56-54% agreement with human decisions across models
- GPT-5 showed best performance with 23.5% false positives and 0.303 significance
- Sensitivity analysis achieved 62.9% factor alignment and 92.97 average rubric score
- Game theory analysis achieved 93% path-level label matching for Cuban Missile Crisis scenario

## Why This Works (Mechanism)
The layered architecture works by creating a structured separation between the LLM's opaque reasoning processes and transparent analytical frameworks. This separation allows the LLM to perform complex reasoning while the structured frameworks provide interpretability and traceability. The QOC framework specifically enables systematic questioning and consideration of alternatives, while sensitivity analysis quantifies uncertainty in decision factors. Game theory applications provide strategic context, and risk management ensures comprehensive evaluation of potential outcomes.

## Foundational Learning
- Question-Answer-Consideration (QOC) framework: Systematic approach to decision analysis that structures questions, answers, and considerations; needed for organizing LLM outputs into traceable decision paths; quick check: verify structured Q-A-C relationships in outputs
- Sensitivity analysis: Quantitative assessment of how uncertainty in inputs affects outputs; needed to measure confidence in LLM-driven decisions; quick check: confirm factor correlation metrics exceed baseline thresholds
- Game theory modeling: Strategic interaction framework for analyzing competitive scenarios; needed for validating LLM reasoning in multi-agent contexts; quick check: verify equilibrium calculations match expected outcomes
- Risk management integration: Systematic identification and evaluation of potential negative outcomes; needed to ensure comprehensive decision coverage; quick check: confirm risk matrices include all identified scenarios

## Architecture Onboarding

Component map: LLM Core -> Analytical Framework Layer -> Decision Output -> Explainability Interface

Critical path: Input Query → LLM Processing → Framework Application → Validation → Explainable Output

Design tradeoffs: The primary tradeoff involves balancing the LLM's reasoning capacity against the constraints imposed by structured frameworks. Looser frameworks allow more LLM flexibility but reduce explainability, while tighter frameworks enhance transparency but may constrain reasoning quality.

Failure signatures: Performance degradation occurs when analytical frameworks cannot adequately constrain LLM outputs, leading to reduced explainability. The system shows reduced effectiveness with highly ambiguous inputs or when human decision patterns are non-deterministic.

First 3 experiments:
1. Test QOC framework with simple binary decision scenarios to establish baseline performance
2. Validate sensitivity analysis accuracy using known mathematical relationships
3. Apply game theory framework to simplified strategic scenarios with clear optimal solutions

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope of human decision datasets used for validation
- 56-54% agreement rate with human decisions indicates substantial unexplained variance
- Evaluation metrics rely on subjective scoring rubrics and pairwise comparisons

## Confidence
- Medium: Layered architecture approach effectiveness (structural separation shows promise but lacks extensive real-world testing)
- Medium: QOC framework achieves human-level decision reproduction (56-54% agreement still leaves significant unexplained variance)
- High: Basic premise that LLM-driven analytical processes enhance explainability compared to black-box LLM outputs

## Next Checks
1. Conduct external validation with domain experts across multiple industries to assess explanation quality and decision alignment in practical applications beyond initial use cases
2. Perform ablation studies comparing layered architecture approach against alternative explainable AI methods to quantify specific benefits of proposed framework
3. Test system performance on dynamic, real-time decision scenarios to evaluate robustness and explainability under changing conditions and incomplete information