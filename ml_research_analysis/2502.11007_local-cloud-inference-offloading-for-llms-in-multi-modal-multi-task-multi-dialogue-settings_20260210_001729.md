---
ver: rpa2
title: Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue
  Settings
arxiv_id: '2502.11007'
source_url: https://arxiv.org/abs/2502.11007
tags:
- response
- local
- cloud
- inference
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of deploying large language
  models (LLMs) in resource-constrained environments, where local devices face computational
  and memory limitations, while cloud deployment incurs high latency and costs. The
  authors propose TMO, a local-cloud inference offloading system with Three-M Offloading:
  Multi-modal, Multi-task, and Multi-dialogue.'
---

# Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings

## Quick Facts
- arXiv ID: 2502.11007
- Source URL: https://arxiv.org/abs/2502.11007
- Reference count: 31
- Key outcome: Joint local-cloud LLM selection with multi-modal data uploading significantly improves response quality, latency, and cost versus baselines

## Executive Summary
This paper addresses the challenge of deploying large language models in resource-constrained environments where local devices have limited computational and memory resources, while cloud deployment incurs high latency and costs. The authors propose TMO (Three-M Offloading), a local-cloud inference offloading system that jointly optimizes LLM selection and multi-modal data uploading across multi-task, multi-modal, and multi-dialogue settings. Using a resource-constrained reinforcement learning framework, TMO learns to route simple tasks to a lightweight local LLM while offloading complex, multi-modal tasks to the cloud, selecting only the most relevant modalities for each task.

## Method Summary
TMO formulates local-cloud inference offloading as a resource-constrained reinforcement learning problem. The system uses a lightweight local LLM for simple tasks and a cloud LLM for complex, multi-modal tasks. An RCRL strategy optimizes the choice between local and cloud inference while selecting relevant multi-modal data sources. The method estimates response quality using a nearest neighbor strategy to handle offline training uncertainties, and uses CLIP embeddings to quantify task-modality relevance for selective uploading. The authors introduce the M4A1 dataset containing real-world measurements across multiple modalities, tasks, dialogues, and LLM configurations. The reward function explicitly trades off response quality, task-modality association, latency, and cost, with resource constraints enforced through penalty terms in the loss function.

## Key Results
- TMO achieves up to 31% better cumulative reward compared to naive baselines
- Response quality improves by 5-10% over local-only approaches while maintaining 40-60% lower latency than cloud-only approaches
- Usage costs are reduced by 30-50% compared to always using cloud inference
- The ablation study shows that both LLM selection and modality selection are critical, with removing either degrading performance significantly

## Why This Works (Mechanism)

### Mechanism 1
Joint optimization of LLM selection and modality selection under resource constraints improves cumulative reward compared to independent or heuristic decisions. The RCRL formulation encodes state as history of (LLM choice, uploaded modalities, task category) over τ consecutive dialogues. Actions are (LLM, modalities) pairs. A penalty term λ·Σmax(gⱼ(θ),0) transforms hard resource constraints into the loss function, allowing gradient-based policy optimization while discouraging violations. The reward function rₜ = αSᵣₜ + βΛΣΛ(Pₜ,D⁽ᵐ⁾) - βψψₐₜ - βϕφₐₜ explicitly trades off response quality, task-modality association, latency, and cost.

### Mechanism 2
Nearest neighbor interpolation estimates response scores for unseen state-action pairs during offline RL training when ground-truth evaluation is unavailable. For a pending (s',a'), find k nearest state-action pairs in the dataset by Euclidean distance. Estimate S'ᵣ as the inverse-distance weighted average: S'ᵣ = Σ(Sᵣᵢ/dᵢ)/Σ(1/dᵢ). This handles NDE (same input yielding different scores) by smoothing, and OOD (unseen inputs) by extrapolation from neighbors.

### Mechanism 3
Pre-trained cross-modal embeddings (CLIP) quantify task-modality relevance to guide selective modality uploading. CLIP encoders map text prompts P and image modalities D⁽ᵐ⁾ to a shared d-dimensional space. Association score Λ(P,D⁽ᵐ⁾) = ⟨ωₜ(P), ωᵢ(D⁽ᵐ⁾)⟩/(||ωₜ||·||ωᵢ||) measures relevance. This signal is incorporated into the reward to encourage uploading only task-relevant modalities.

## Foundational Learning

- **Concept: Markov Decision Processes and Policy Gradient Methods**
  - Why needed here: TMO formulates offloading as an MDP and uses A2C/PPO/DQN for policy optimization. Understanding value functions, advantage estimates, and gradient-based policy updates is required to modify the reward weights or constraint penalties.
  - Quick check question: Can you explain why the advantage function A(s,a) = r + γV(s') - V(s) reduces variance compared to using raw returns?

- **Concept: Cross-Modal Representation Learning**
  - Why needed here: The task-modality association mechanism relies on CLIP's joint embedding space. Understanding contrastive pre-training and embedding alignment helps diagnose when association scores fail.
  - Quick check question: Why does CLIP use contrastive loss rather than reconstruction loss for aligning text and image representations?

- **Concept: Constrained Optimization via Lagrangian Relaxation**
  - Why needed here: RCRL transforms hard constraints into penalty terms in the loss function. Understanding when this is equivalent to constrained optimization versus when it yields violations is critical for tuning λ.
  - Quick check question: Under what conditions does increasing λ guarantee fewer constraint violations, and when does it cause optimization instability?

## Architecture Onboarding

- **Component map**: Prompt → Task Classifier → State Encoder → Policy Network → LLM Selector + Modality Uploader → CLIP Association Scorer → Cloud/Local Inference → Response Scorer → Update State
- **Critical path**: Prompt arrives → task classifier assigns category n → state encoder builds sₜ from history → policy samples aₜ = (LLM, modalities) → if cloud selected, association scorer validates modality relevance → inference executed → latency/cost measured → response scored (training) or returned (inference) → state updated for next dialogue
- **Design tradeoffs**: Larger τ captures longer dialogue dependencies but increases state dimension and policy complexity; higher k in nearest neighbor estimation reduces variance but may smooth over meaningful quality differences; stronger λ reduces violations but may over-constrain and lower achievable reward
- **Failure signatures**: Policy always selects local LLM (reward weights over-penalize latency/cost or response score estimator underestimates cloud quality); policy uploads all modalities regardless of task (association scores uniformly high or association weight too low); frequent constraint violations despite high λ (penalty coefficient insufficient relative to reward scale or horizon τ too short); high variance in training curves (NDE uncertainty not adequately smoothed; increase k or collect more samples per state-action pair)
- **First 3 experiments**:
  1. **Baseline sanity check**: Run Random, Local-only, and Cloud-only policies on M4A1 test set. Verify that Random achieves intermediate reward, Local has lowest latency/cost but lowest quality, Cloud has highest quality but highest latency/cost.
  2. **Ablation on τ**: Train RC-A2C with τ ∈ {1, 3, 5, 7} while keeping other hyperparameters fixed. Plot reward vs. τ and constraint violation rate.
  3. **Score estimator validation**: Hold out 10% of M4A1 as validation. For each (s,a) in validation, compute S'ᵣ via k-NN and compare to ground-truth Sᵣ. Report MSE and correlation for k ∈ {1, 3, 5, 10}.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can local-cloud inference offloading be optimized in multi-user collaborative settings where computational resources and bandwidth are shared? The current TMO system and the M4A1 dataset are designed around a single-user interaction model, simulating individual decision-making processes without contention for shared local or network resources.

- **Open Question 2**: How can the system dynamically select among multiple cloud LLM providers to optimize for varying latency, cost, and capability trade-offs? The current framework optimizes the binary choice between a single local model and a single cloud model, essentially treating the cloud as a monolithic endpoint.

- **Open Question 3**: Can the Resource-Constrained Reinforcement Learning (RCRL) framework effectively incorporate data privacy as a quantifiable constraint or penalty? The system currently treats privacy implicitly rather than explicitly optimizing for it, potentially offloading sensitive data to the cloud if the quality/latency trade-off is favorable enough.

## Limitations

- The M4A1 dataset is not publicly available, requiring significant effort to recreate from scratch with multi-view images, task prompts, and real latency/cost measurements
- Key hyperparameters including MLP architecture details, learning rates, batch sizes, and state/action encoding schemes are unspecified, creating potential variability in reproduction
- The CLIP-based association scoring assumes the pre-trained model captures task-modality relevance for the specific application domain, which may not hold for specialized or novel multimodal scenarios

## Confidence

- **High confidence**: The general RCRL formulation, constraint transformation via penalty terms, and overall system architecture are well-specified and theoretically sound
- **Medium confidence**: The nearest neighbor response score estimation and CLIP-based association scoring mechanisms are plausible given ablation results but depend on dataset characteristics not fully detailed
- **Medium confidence**: The claimed performance improvements (up to 31% better reward) are supported by ablation studies but would require independent validation on the full M4A1 dataset

## Next Checks

1. **Dataset reconstruction validation**: Before full training, verify that recreated M4A1 captures necessary diversity in multi-view modalities, task types, and dialogue contexts by computing basic statistics and checking coverage of the state-action space

2. **Hyperparameter sensitivity analysis**: Systematically vary λ, k, and τ across reasonable ranges to identify stable operating points and understand how each hyperparameter affects constraint satisfaction versus reward maximization

3. **Cross-dataset generalization test**: Evaluate TMO on a held-out subset of M4A1 not seen during training, and ideally on a different multi-modal offloading dataset if available, to assess whether learned policies generalize beyond the training distribution