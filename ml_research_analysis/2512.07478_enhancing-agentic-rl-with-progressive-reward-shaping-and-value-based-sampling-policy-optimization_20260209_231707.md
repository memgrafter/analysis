---
ver: rpa2
title: Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling
  Policy Optimization
arxiv_id: '2512.07478'
source_url: https://arxiv.org/abs/2512.07478
tags:
- reward
- answer
- vspo
- grpo
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of training large language
  models (LLMs) for tool-integrated reasoning (TIR) tasks, where agents must iteratively
  plan, call external tools, and integrate returned information to solve complex problems.
  Two key issues hinder effectiveness: sparse, non-instructive rewards that provide
  limited guidance for intermediate steps and gradient degradation in Group Relative
  Policy Optimization (GRPO) when identical rewards within a rollout group yield zero
  advantage.'
---

# Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization

## Quick Facts
- arXiv ID: 2512.07478
- Source URL: https://arxiv.org/abs/2512.07478
- Reference count: 5
- LLM-based TIR agents achieve superior generalization across domains through PRS and VSPO

## Executive Summary
This paper addresses fundamental challenges in training large language models for tool-integrated reasoning tasks, where agents must iteratively plan, call external tools, and integrate returned information to solve complex problems. The authors identify two critical issues: sparse, non-instructive rewards that provide limited guidance for intermediate steps, and gradient degradation in Group Relative Policy Optimization when identical rewards within a rollout group yield zero advantage. To overcome these challenges, they propose Progressive Reward Shaping (PRS), a curriculum-inspired reward design that introduces dense, stage-wise feedback to encourage models to first master parseable tool calls and then optimize for factual correctness and answer quality. They also introduce Value-based SamplingPolicy Optimization (VSPO), an enhanced GRPO variant that replaces zero-advantage samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates.

## Method Summary
The authors propose a two-pronged approach to enhance LLM training for tool-integrated reasoning tasks. Progressive Reward Shaping (PRS) implements a curriculum-based reward system that first rewards parseable tool calls, then encourages factual correctness in intermediate steps, and finally optimizes for complete answer quality. Value-based Sampling Policy Optimization (VSPO) enhances Group Relative Policy Optimization by replacing zero-advantage samples with value-selected prompts that balance task difficulty and uncertainty, while applying value-smoothing clipping to stabilize gradient updates. Together, these methods address the sparse reward problem and gradient degradation issues that commonly plague RL training of LLMs in TIR settings.

## Key Results
- PRS consistently outperforms traditional binary rewards across multiple short-form and long-form QA benchmarks
- VSPO achieves superior stability, faster convergence, and higher final performance compared to SFT, PPO, and GRPO baselines
- The combined PRS and VSPO approach yields LLM-based TIR agents that generalize better across different domains

## Why This Works (Mechanism)
The effectiveness stems from addressing two fundamental RL training challenges in LLM tool integration. PRS provides denser, more instructive feedback by breaking the learning process into manageable curriculum stages, allowing the model to first learn the mechanics of tool calling before optimizing for complex reasoning outcomes. VSPO tackles the gradient degradation problem in GRPO by intelligently selecting samples with non-zero advantages based on task value metrics, ensuring meaningful policy updates while the value-smoothing clipping prevents extreme gradient values that could destabilize training.

## Foundational Learning

**Tool-integrated reasoning (TIR)**: LLM agents that iteratively plan, call external tools, and integrate returned information to solve complex problems - needed because many real-world tasks require external knowledge beyond model parameters.

**Sparse reward problem**: When rewards are only given at the end of a task, making intermediate step optimization difficult - critical because LLMs need guidance throughout the reasoning process.

**Group Relative Policy Optimization (GRPO)**: A PPO variant that uses relative advantages within rollout groups - fundamental as the baseline optimization algorithm being enhanced.

**Curriculum learning**: Training progression through increasingly difficult stages - important because it mirrors human learning and prevents overwhelming the model with complex optimization simultaneously.

**Task-value metrics**: Measures balancing difficulty and uncertainty for sample selection - essential for intelligent sampling that maintains training stability while focusing on informative examples.

**Value-smoothing clipping**: Gradient stabilization technique that prevents extreme updates - necessary for maintaining training stability in RL with LLMs.

## Architecture Onboarding

**Component map**: LLM agent -> PRS reward module -> VSPO optimizer -> GRPO baseline -> Task-value selector -> Value-smoothing clipper

**Critical path**: Tool call -> Parseable check (PRS stage 1) -> Factual correctness check (PRS stage 2) -> Answer quality check (PRS stage 3) -> VSPO sampling -> Gradient update

**Design tradeoffs**: PRS trades computational overhead for denser feedback versus binary rewards; VSPO trades additional selection complexity for gradient stability versus standard GRPO.

**Failure signatures**: PRS may stall if curriculum stages are too difficult; VSPO may overfit to selected samples if task-value metrics are poorly calibrated.

**First experiments**: 1) Compare PRS vs binary rewards on parseable call rates; 2) Test VSPO convergence speed against GRPO on identical tasks; 3) Evaluate generalization across different tool types and domains.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on tool-integrated reasoning tasks within specific LLM training framework
- Curriculum design assumes linear progression that may not capture all task complexities
- Value-based sampling relies on task-value metrics that need verification across different problem distributions

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| PRS effectiveness | High |
| VSPO stability and convergence | Medium |
| Generalization across domains | Low |

## Next Checks
1. Conduct ablation studies to isolate contributions of value-based sampling versus value-smoothing clipping in VSPO
2. Test PRS and VSPO on non-QA domains (e.g., code generation, game playing) to evaluate cross-domain generalization
3. Implement head-to-head comparison with GTPO and GRPO-S to determine unique advantages