---
ver: rpa2
title: Accelerating LLM Inference with Precomputed Query Storage
arxiv_id: '2509.25919'
source_url: https://arxiv.org/abs/2509.25919
tags:
- inference
- queries
- query
- response
- precomputed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StorInfer introduces a storage-assisted approach to accelerate
  LLM inference by precomputing and storing high-quality query-response pairs offline.
  When user queries match precomputed entries, responses are retrieved instantly instead
  of running full inference, reducing latency by up to 17.3% with no loss in quality.
---

# Accelerating LLM Inference with Precomputed Query Storage

## Quick Facts
- **arXiv ID:** 2509.25919
- **Source URL:** https://arxiv.org/abs/2509.25919
- **Authors:** Jay H. Park; Youngju Cho; Choungsol Lee; Moonwook Oh; Euiseong Seo
- **Reference count:** 30
- **One-line primary result:** Reduces latency by up to 17.3% with no quality loss using precomputed query-response pairs

## Executive Summary
StorInfer introduces a storage-assisted approach to accelerate LLM inference by precomputing and storing high-quality query-response pairs offline. When user queries match precomputed entries, responses are retrieved instantly instead of running full inference, reducing latency by up to 17.3% with no loss in quality. The system uses adaptive query masking and sampling to generate diverse, deduplicated queries, and indexes them in a disk-backed vector database for fast retrieval. Parallel execution of vector search and inference enables efficient fallback when no match is found. Evaluation across three QA datasets shows that storage usage scales proportionally with hit rate, making the approach practical for resource-constrained, predictable-query environments.

## Method Summary
StorInfer precomputes query-response pairs from a knowledge base using an LLM with adaptive masking and sampling to ensure diversity. These pairs are embedded and indexed in a disk-backed vector database (DiskANN) using MIPS. At runtime, incoming queries trigger parallel vector search (CPU/storage) and LLM inference (GPU). If a stored query exceeds similarity threshold Sth_Run, its precomputed response returns instantly; otherwise, full inference completes. The approach is particularly effective when query distributions are narrow or predictable, enabling significant latency reductions without quality degradation.

## Key Results
- Reduces latency by up to 17.3% with no quality loss when Sth_Run=0.9
- Vector search achieves 8.6× speedup over full LLM inference with ~0.024s latency
- Adaptive generation achieves 22.5% hit rate vs 18% for random on SQuAD

## Why This Works (Mechanism)

### Mechanism 1: Storage-Based Semantic Retrieval Bypasses GPU Inference
Precomputed query-response pairs indexed by semantic similarity enable instant response retrieval, eliminating both prefill and decode phases when matches exist. During offline generation, query-response pairs are embedded into vectors and stored in a disk-backed vector database (DiskANN). At runtime, incoming queries are embedded and searched against stored vectors using Maximum Inner Product Search (MIPS). When similarity exceeds threshold (Sth_Run), the precomputed response returns immediately via CPU/storage access, completely bypassing GPU computation. Core assumption: User query distributions are sufficiently predictable that precomputed pairs achieve meaningful hit rates (demonstrated in narrow-domain scenarios like vehicle infotainment or fixed-corpus RAG).

### Mechanism 2: Adaptive Query Generation Maximizes Coverage with Minimal Redundancy
Combining query masking with dynamic temperature sampling generates semantically diverse queries that improve hit rates compared to random generation. Adaptive Query Masking injects recently generated queries into the LLM context window (within token budget constraints) to explicitly signal patterns to avoid. Adaptive Sampling initializes temperature at 0.7, then increases by 0.1 (max 1.0) whenever a generated query exceeds similarity threshold (Sth_Gen=0.99) with existing entries, progressively encouraging diversity. Queries exceeding threshold are discarded. Core assumption: The knowledge base contains sufficient signal about user information needs to anticipate real query patterns.

### Mechanism 3: Parallel Execution Ensures Zero Overhead on Misses
Concurrent vector search and LLM inference on non-contending resources eliminates latency penalty when no precomputed match exists. Vector search runs on CPU/storage while LLM inference runs on GPU. On match detection (similarity > Sth_Run), a termination signal aborts ongoing inference. On miss, inference completes normally with no additional delay. Effective latency = hit_rate × vector_search_latency + miss_rate × llm_inference_latency. Core assumption: CPU/storage and GPU resources operate independently without contention.

## Foundational Learning

- **Vector Embeddings and Similarity Search**
  - Why needed here: Understanding how text converts to high-dimensional vectors and how MIPS/cosine similarity enables semantic matching (not just keyword matching) is essential for grasping why differently-worded queries can hit the same precomputed entry.
  - Quick check question: Why would "What is the capital of France?" and "Tell me France's capital city" produce similar embedding vectors despite sharing few words?

- **LLM Inference Phases (Prefill vs Decode)**
  - Why needed here: The paper explicitly contrasts StorInfer with prefix caching, which only optimizes prefill. Understanding that decode phase generates tokens sequentially—and that both phases are bypassed on storage hits—explains the full latency benefit.
  - Quick check question: Which inference phase dominates latency for long-context QA, and why does bypassing both phases matter more than optimizing just one?

- **Cache vs Precompute Trade-offs**
  - Why needed here: The paper positions itself against cold-start-vulnerable caching approaches (GPTCache, prefix caching). Understanding reactive caching vs proactive precomputation clarifies when each strategy applies.
  - Quick check question: Why does GPTCache underperform on the first query for any given topic, and how does StorInfer address this?

## Architecture Onboarding

- **Component map:** Knowledge base → LLM with adaptive masking/sampling → Embedding model → DiskANN vector index + response metadata → Runtime query embedding → Parallel: DiskANN search + LLM inference → Match decision → Return stored response OR complete inference

- **Critical path:**
  1. Deploy: Generate and index precomputed pairs from your domain knowledge base
  2. Tune: Set Sth_Run based on quality/hit-rate tradeoff (0.9 preserves quality with moderate hits; 0.5 maximizes hits with quality degradation)
  3. Monitor: Track hit rate and effective latency; expand storage if hit rate plateaus below target

- **Design tradeoffs:**
  - Sth_Run threshold: 0.9 ≈ baseline quality (Table 2: BERT F1 0.458 vs 0.439 baseline), 22.5% hit; 0.5 ≈ lower quality (BERT F1 0.308) but 93% hit
  - Storage scale: Hit rate scales sublinearly (Figure 4); diminishing returns after ~100K queries on SQuAD
  - Generator model quality: Using larger LLM at precompute time than available at runtime provides quality boost on hits

- **Failure signatures:**
  - Hit rate <5% despite storage investment → Query distribution too broad; narrow domain scope or increase Sth_Gen diversity
  - Response quality degradation on hits → Sth_Run too permissive; increase to 0.9+
  - Storage bloat without coverage gain → Deduplication not working; verify adaptive masking/sampling pipeline
  - Miss latency higher than baseline → Parallel execution not functioning; check GPU/CPU isolation

- **First 3 experiments:**
  1. **Threshold calibration:** Sweep Sth_Run from 0.5→0.99 on held-out queries; plot hit rate vs quality metrics (Unigram F1, ROUGE-L, BERT Score) to identify acceptable operating point
  2. **Deduplication ablation:** Compare random vs deduplicated generation on your knowledge base, measuring hit rate at 25K/50K/100K/150K query counts (replicate Figure 4 pattern)
  3. **Domain coverage test:** Measure hit rates on in-domain vs out-of-domain queries to validate predictability assumption before production deployment

## Open Questions the Paper Calls Out

- **How does StorInfer maintain effectiveness in open-domain environments where user queries are highly unpredictable or fall outside the generated semantic clusters?**
  - Basis in paper: The authors explicitly limit their scope, stating the approach is "particularly effective in cases where the distribution of LLM queries is narrow or predictable."
  - Why unresolved: The evaluation relies on constrained QA datasets (SQuAD, NarrativeQA) with fixed contexts, leaving performance in highly variable, open-ended interactions unknown.
  - What evidence would resolve it: Evaluation of hit rates and effective latency on diverse, open-domain conversation logs where query distributions are broad and sparse.

- **What are the efficient strategies for updating or invalidating precomputed query-response pairs when the underlying knowledge base changes frequently?**
  - Basis in paper: The system relies on an offline generation phase, but the paper does not address the mechanisms or costs of handling staleness when source documents are updated.
  - Why unresolved: Regenerating the entire 150K+ vector index for every knowledge base update is computationally expensive and may negate the efficiency benefits.
  - What evidence would resolve it: Analysis of incremental update algorithms or partial invalidation techniques with corresponding generation time and storage overhead metrics.

- **Does the latency of vector retrieval scale efficiently as the precomputed storage grows to millions or billions of entries?**
  - Basis in paper: While RQ3 investigates scaling, the experiments stop at 150K entries; Figure 4 shows storage growing linearly, suggesting potential scaling bottlenecks for larger deployments.
  - Why unresolved: The paper reports an 8.6× speedup for vector search over inference at 150K, but it is unclear if search latency remains negligible as the index size increases by orders of magnitude.
  - What evidence would resolve it: Benchmarks of vector search latency and hit rates on precomputed sets scaling to millions of entries.

## Limitations
- Effectiveness highly dependent on predictable query distributions, limiting applicability to open-domain scenarios
- Quality degradation at lower Sth_Run thresholds (0.5) produces significantly worse metrics (BERT F1 0.308 vs 0.439 baseline)
- 17.3% latency reduction, while notable, may not justify storage overhead in many applications

## Confidence
**High Confidence (Likelihood >80%):**
- Storage-assisted retrieval can eliminate LLM inference latency when query-response pairs match
- Parallel execution of vector search and inference prevents overhead on misses
- Hit rates are domain-dependent and scale with query predictability

**Medium Confidence (Likelihood 50-80%):**
- Adaptive query masking and sampling generate meaningfully diverse queries that improve coverage
- The quality-latency tradeoff curve (Sth_Run thresholds) generalizes to other domains
- Storage scaling remains practical as hit rates increase

**Low Confidence (Likelihood <50%):**
- StorInfer's benefits transfer to open-domain or conversational LLM applications
- The 8.6× vector search speedup translates to similar benefits on different hardware
- Quality preservation at Sth_Run=0.9 holds across diverse knowledge bases

## Next Checks
1. **Domain Transfer Experiment:** Deploy StorInfer on a non-QA domain (e.g., code generation or conversational AI) with naturally occurring user query logs. Measure hit rates and quality degradation compared to the QA-focused evaluation to test domain generalization claims.

2. **Quality Degradation Boundary:** Systematically evaluate response quality across the full Sth_Run range (0.5-0.99) on held-out queries from the same datasets. Plot quality metrics against hit rates to identify the inflection point where quality loss outweighs latency benefits.

3. **Storage Cost-Benefit Analysis:** Measure storage requirements and hit rates at 25K, 50K, 100K, and 150K precomputed pairs on a new domain. Calculate effective latency improvements and determine the storage threshold where diminishing returns make the approach impractical for resource-constrained environments.