---
ver: rpa2
title: "Sabi\xE1: Um Chatbot de Intelig\xEAncia Artificial Generativa para Suporte\
  \ no Dia a Dia do Ensino Superior"
arxiv_id: '2511.10787'
source_url: https://arxiv.org/abs/2511.10787
tags:
- para
- como
- informac
- respostas
- encia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of fragmented academic information\
  \ access in higher education. The proposed solution, Sabi\xE1, is a chatbot that\
  \ uses Generative AI and Retrieval-Augmented Generation (RAG) to provide quick,\
  \ accurate answers based on institutional documents."
---

# Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior

## Quick Facts
- arXiv ID: 2511.10787
- Source URL: https://arxiv.org/abs/2511.10787
- Reference count: 0
- Primary result: Gemini 2.0 Flash achieved highest LLM-as-a-Judge score (0.751±0.20) and fastest response times (~2.6s), while Gemma 3n showed strong open-source performance with SBERT score of 0.703±0.23.

## Executive Summary
This study presents Sabiá, a generative AI chatbot designed to improve access to institutional academic information in higher education. The system uses Retrieval-Augmented Generation (RAG) with vector databases to ground responses in university documents, reducing hallucinations common in pure generative models. Multiple AI models were evaluated using traditional metrics (ROUGE, BLEU, SBERT) and LLM-as-a-Judge, with Gemini 2.0 Flash excelling in both quality and speed, while Gemma 3n demonstrated competitive performance as an open-source alternative. The results validate that combining GenAI with RAG provides accurate, timely responses to academic queries.

## Method Summary
The Sabiá chatbot uses a Streamlit frontend connected to a LangChain RAG pipeline with ChromaDB as the vector store. Institutional PDF documents are parsed, chunked, embedded, and stored in ChromaDB. When users ask questions, the system retrieves relevant document chunks via semantic similarity search, combines them with the query in a template, and sends this to the selected LLM via OpenRouter API. The study evaluated nine different models (Gemini 2.0 Flash, Gemma 3n, GPT-4o-mini, DeepSeek R1, LLaMA 4 Scout, Phi 4, Qwen3-235b) using traditional NLP metrics and LLM-as-a-Judge scoring. Code is available at https://github.com/guilhermebiava/Sabia.

## Key Results
- Gemini 2.0 Flash achieved the highest LLM-as-a-Judge score (0.751±0.20) and fastest average response time (2.6 seconds)
- Open-source models like Gemma 3n (SBERT: 0.703±0.23) and Phi 4 (LLM-as-a-Judge: 0.768±0.26) showed competitive performance
- All top-performing models achieved LLM-as-a-Judge scores above 0.75 and processing times under 3 seconds
- RAG approach successfully grounded responses in institutional documents, reducing hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG reduces hallucinations by grounding responses in retrieved institutional documents.
- Mechanism: User query → vector embedding → similarity search in ChromaDB → top-k chunks retrieved → chunks + query + template sent to LLM → response generated with source context.
- Core assumption: The chunking strategy and embedding model capture semantic meaning sufficient to match questions to relevant document sections.
- Evidence anchors:
  - [abstract] "uses Retrieval-Augmented Generation (RAG) to retrieve and contextualize data from university documents, offering precise answers"
  - [Section 2] "permite que o modelo supere limitações de desatualização e reduza alucinações que são comuns em modelos de GenAI puramente generativos"
  - [corpus] Related work on GenAI chatbots in education shows similar RAG approaches improve information accessibility (avg neighbor FMR=0.5, but low citation count suggests emerging evidence)
- Break condition: If documents are poorly structured, outdated, or chunks lack context, retrieval will return irrelevant passages and grounding fails.

### Mechanism 2
- Claim: LLM-as-a-Judge evaluation provides scalable, human-aligned quality assessment for domain-specific responses.
- Mechanism: Judge LLM (GPT-4.1-mini) receives candidate response + reference answer + rubric (relevance, accuracy, completeness, clarity, concision) → scores 1-5 per criterion → aggregated similarity index (0-1).
- Core assumption: The judge LLM's scoring correlates with human expert judgment in the academic domain.
- Evidence anchors:
  - [Section 4.4.1] "estudos mostram que essa técnica tem um alto grau de concordância com avaliações humanas (com correlação acima de 0.85)"
  - [Section 5] Phi 4 scored highest (0.768±0.26), Gemini 2.0 Flash led on traditional metrics (ROUGE-L: 0.470±0.33)
  - [corpus] No direct corpus validation of LLM-as-a-Judge reliability in educational contexts; neighboring papers focus on user experience, not evaluation methodology
- Break condition: If rubric is ambiguous, judge model has domain blind spots, or reference answers contain errors, scores will not reflect true quality.

### Mechanism 3
- Claim: Open-source models (Gemma 3n, Phi 4) can approach proprietary model quality for institutional Q&A while enabling local deployment.
- Mechanism: Same RAG pipeline, different generator LLMs → compare outputs via multiple metrics → identify cost/quality tradeoffs.
- Core assumption: Evaluation metrics (SBERT semantic similarity, ROUGE overlap) capture functional equivalence for user needs.
- Evidence anchors:
  - [Section 5, Table 3] Gemma 3n: SBERT 0.703±0.23, ROUGE-L 0.411±0.32; Phi 4: LLM-as-a-Judge 0.768±0.26
  - [Section 5] "Gemini 2.0 Flash obteve a maior pontuação (0.751±0.20)" on SBERT, but Gemma 3n offers open-source advantage
  - [corpus] Weak external validation—neighbor papers don't compare open vs. proprietary models in education
- Break condition: If open models underperform on edge cases (complex queries, ambiguous documents), user trust degrades.

## Foundational Learning

- Concept: **RAG Pipeline Architecture**
  - Why needed here: Core system design; must understand retriever-generator split, chunking, vector stores, and prompt construction.
  - Quick check question: Can you explain why RAG helps with factual accuracy compared to fine-tuning alone?

- Concept: **Embeddings and Vector Similarity Search**
  - Why needed here: ChromaDB retrieval depends on semantic embeddings; poor embedding choice degrades retrieval.
  - Quick check question: How would you diagnose if the retriever is returning irrelevant chunks?

- Concept: **LLM Evaluation Metrics (ROUGE, BLEU, SBERT, LLM-as-a-Judge)**
  - Why needed here: Paper uses multiple metrics with different tradeoffs; need to interpret results correctly.
  - Quick check question: Why might SBERT and ROUGE give conflicting signals about response quality?

## Architecture Onboarding

- Component map:
  - Frontend: Streamlit web app (user input, model selection, chat history)
  - Backend: Python + LangChain RAG pipeline
  - Vector store: ChromaDB (stores document embeddings)
  - LLM gateway: OpenRouter API (accesses GPT-4o, Gemini, Gemma, LLaMA, Phi, Qwen, DeepSeek)
  - Evaluator: GPT-4.1-mini as LLM-as-a-Judge

- Critical path:
  1. Document ingestion → PDF parsing → chunking → embedding → ChromaDB storage
  2. User query → embedding → similarity search → top-k chunks retrieved
  3. Template + chunks + query → LLM → response → frontend display
  4. Offline evaluation: FAQ questions → all models → metrics comparison

- Design tradeoffs:
  - Gemini 2.0 Flash: Best quality/speed, but API dependency and cost
  - Gemma 3n / Phi 4: Open-source, deployable locally, slightly lower metrics, full institutional control
  - Chunk size: Not specified in paper—assume assumption; too small loses context, too large dilutes relevance
  - Single judge LLM: Faster, but less robust than multi-judge ensemble (noted as future work)

- Failure signatures:
  - High SBERT but low ROUGE: Model rephrases correctly but misses specific institutional terminology
  - High LLM-as-a-Judge but user complaints: Rubric doesn't capture actual user priorities
  - Slow responses (DeepSeek R1: 11.4s mean): Retrieval or generation bottleneck; profile before optimizing
  - Irrelevant retrievals: Check embedding model suitability for Portuguese academic documents

- First 3 experiments:
  1. Replicate with your own institutional documents (5-10 PDFs) to validate pipeline portability—measure if retrieval relevance holds with different document structures.
  2. A/B test Gemini 2.0 Flash vs. Gemma 3n with 20 real user questions—collect both metrics and subjective usefulness ratings.
  3. Vary chunk size (e.g., 256 vs. 512 vs. 1024 tokens) and measure retrieval precision on FAQ set—identify optimal granularity for your document types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do students and faculty evaluate the usability and accessibility of the Sabiá chatbot during empirical, real-world interactions?
- Basis in paper: [explicit] The authors state that future work includes "conducting empirical user studies with students and faculty... to evaluate usability, accessibility, and the interaction experience."
- Why unresolved: The current study relied exclusively on automated metrics (ROUGE, BLEU, LLM-as-a-Judge) and a static FAQ list, without direct human feedback on the user experience.
- What evidence would resolve it: Results from usability surveys (e.g., System Usability Scale) and qualitative interviews with actual students and staff using the tool.

### Open Question 2
- Question: Does utilizing multiple LLM judges increase the reliability and robustness of the evaluation compared to the single-judge approach used in this study?
- Basis in paper: [explicit] The paper lists the "expansion of the LLM-as-a-Judge technique, using multiple judges and more executions," as a planned future step.
- Why unresolved: The current evaluation relied on a single model (GPT-4.1-mini) as the judge, which may introduce specific biases or limitations in grading the responses.
- What evidence would resolve it: A comparative analysis of evaluation scores using an ensemble of diverse judge models versus the current single-model setup.

### Open Question 3
- Question: Does the chatbot maintain its accuracy and response quality when the scope of test questions and document coverage is significantly expanded?
- Basis in paper: [explicit] The authors identify the need to "increase the scope of tests, including a greater range of questions in the FAQ," as necessary future work.
- Why unresolved: The reported effectiveness is based on a specific, potentially limited set of frequently asked questions, and performance may degrade with a broader, more complex query set.
- What evidence would resolve it: Evaluation metrics (e.g., semantic similarity, accuracy) derived from a significantly larger and more diverse dataset of academic inquiries.

## Limitations
- Evaluation relies on automated metrics and LLM-as-a-Judge rather than extensive human user studies, limiting understanding of real-world usability
- Study focuses on Portuguese institutional documents from a single university, so performance may vary with different languages, document structures, or institutional contexts
- Chunking strategy and embedding model parameters are not fully specified, making exact reproduction difficult

## Confidence

**High Confidence:** The core mechanism of RAG reducing hallucinations through document grounding is well-established in the literature and supported by the paper's empirical results. The performance advantage of Gemini 2.0 Flash and the viability of open-source alternatives like Gemma 3n are demonstrated through multiple evaluation metrics.

**Medium Confidence:** The LLM-as-a-Judge evaluation methodology shows promise with reported correlation above 0.85 with human judgment, but lacks direct validation in the educational domain. The relative performance of different models across metrics is consistent, though the absolute quality thresholds for user satisfaction remain unclear.

**Low Confidence:** Claims about user experience improvements and the practical impact on information accessibility are not directly validated through user studies. The optimal configuration parameters (chunk size, number of chunks retrieved, prompt templates) remain unspecified and likely affect real-world performance.

## Next Checks

1. **User Study Validation:** Deploy the chatbot with actual students and faculty over 4-6 weeks, collecting systematic feedback on response usefulness, accuracy, and satisfaction. Compare against baseline methods like document search or human support.

2. **Cross-Institutional Replication:** Test the system with documents from 2-3 different universities or departments with varying document structures and Portuguese dialects to validate portability and identify configuration requirements.

3. **Parameter Sensitivity Analysis:** Systematically vary chunking parameters (256, 512, 1024 tokens) and retrieval counts (k=3, 5, 7) on the existing FAQ dataset, measuring how these affect all evaluation metrics to establish optimal defaults for different document types.