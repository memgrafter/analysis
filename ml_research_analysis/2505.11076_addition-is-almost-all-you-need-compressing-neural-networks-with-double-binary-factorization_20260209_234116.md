---
ver: rpa2
title: 'Addition is almost all you need: Compressing neural networks with double binary
  factorization'
arxiv_id: '2505.11076'
source_url: https://arxiv.org/abs/2505.11076
tags:
- compression
- weight
- binary
- quantization
- factorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Double Binary Factorization (DBF) compresses LLM weight matrices
  by factorizing them into two binary matrices with scaling vectors, achieving compression
  rates of 1-2 bits per weight. DBF provides 2-3.5x speedup during inference by replacing
  multiplications with additions.
---

# Addition is almost all you need: Compressing neural networks with double binary factorization

## Quick Facts
- **arXiv ID**: 2505.11076
- **Source URL**: https://arxiv.org/abs/2505.11076
- **Reference count**: 6
- **Primary result**: DBF achieves 2-3.5x inference speedup by replacing multiplications with additions while compressing LLM weight matrices to 1-2 bits per weight

## Executive Summary
Double Binary Factorization (DBF) introduces a novel approach to neural network compression by factorizing weight matrices into two binary matrices with scaling vectors. This method achieves remarkable compression rates of 1-2 bits per weight while providing 2-3.5x inference speedup by replacing multiplications with additions. DBF outperforms existing binary quantization methods like OneBit and matches state-of-the-art approaches such as QuIP# and QTIP, uniquely offering fine-grained control over compression ratios and incorporating weight importance during factorization.

## Method Summary
DBF factorizes each weight matrix in a neural network into two binary matrices combined with scaling vectors, effectively compressing the model while maintaining computational efficiency. The method replaces traditional matrix multiplications with additions during inference, enabling significant speed improvements. Unlike standard quantization techniques, DBF provides granular control over the compression ratio and explicitly accounts for weight importance during the factorization process, allowing for more intelligent allocation of precision where it matters most.

## Key Results
- Achieves 1-2 bits per weight compression on Llama2-7B models
- Provides 2-3.5x inference speedup by replacing multiplications with additions
- Llama2-7B reaches 26.45 perplexity at 1 bit/weight and 37.03 at 2 bits/weight, outperforming OneBit and matching QuIP# and QTIP

## Why This Works (Mechanism)
DBF leverages the computational efficiency of binary operations while maintaining representational capacity through factorization. By decomposing weight matrices into binary components with learned scaling factors, the method exploits the fact that addition operations are significantly faster than multiplications on modern hardware. The dual factorization structure allows the model to capture complex weight distributions through combinations of simple binary patterns, while the scaling vectors provide the necessary precision for critical weights.

## Foundational Learning

**Binary Matrix Factorization**
*Why needed*: Enables efficient representation of weight matrices using only binary values
*Quick check*: Verify that the factorization preserves essential weight relationships through binary combinations

**Quantization-Aware Training**
*Why needed*: Ensures compressed models maintain accuracy during the factorization process
*Quick check*: Confirm that scaling vectors effectively compensate for precision loss

**Matrix Decomposition Theory**
*Why needed*: Provides mathematical foundation for splitting weight matrices into binary components
*Quick check*: Validate that the decomposition satisfies reconstruction constraints

## Architecture Onboarding

**Component Map**: Input -> Binary Matrix A -> Scaling Vector Î± -> Binary Matrix B -> Output

**Critical Path**: The factorization and scaling operations form the core computational path, with binary additions replacing traditional multiplications for inference efficiency.

**Design Tradeoffs**: Binary precision vs. model accuracy, compression ratio vs. computational overhead, fine-grained control vs. implementation complexity.

**Failure Signatures**: Poor factorization initialization leading to accuracy degradation, scaling vector saturation causing numerical instability, binary patterns failing to capture important weight relationships.

**First Experiments**:
1. Apply DBF to a single linear layer and measure compression ratio vs. accuracy tradeoff
2. Test different scaling vector initialization strategies to optimize reconstruction quality
3. Evaluate inference speedup on CPU vs. GPU to understand hardware dependencies

## Open Questions the Paper Calls Out
None

## Limitations
- Performance scaling to larger models beyond Llama2-7B remains unproven
- Binary factorization may struggle with capturing fine-grained weight distributions in specialized models
- Computational efficiency claims depend on specialized hardware implementations

## Confidence

**High Confidence**: Theoretical foundation of binary matrix factorization is well-established; mathematical framework is sound; compression ratio calculations are straightforward and verifiable.

**Medium Confidence**: Empirical results on Llama2-7B demonstrate viability; comparison with existing methods is comprehensive; but sample size of tested models is limited.

**Low Confidence**: Long-term stability of compressed models under domain shift is unevaluated; real-world energy efficiency benefits remain speculative without hardware-specific benchmarks.

## Next Checks

1. **Scaling Validation**: Test DBF on larger models (Llama2-13B, Llama2-70B) to verify compression efficiency and accuracy preservation scale proportionally with model size.

2. **Hardware Implementation**: Implement DBF on specialized AI accelerators (GPUs with tensor cores, TPUs) to measure actual speedup and energy efficiency compared to theoretical estimates.

3. **Domain Adaptation Testing**: Evaluate compressed models on domain-shifted datasets to assess whether binary factorization maintains performance across different data distributions and task types.