---
ver: rpa2
title: Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with
  Human Feedback
arxiv_id: '2508.03123'
source_url: https://arxiv.org/abs/2508.03123
tags:
- diffusion
- speech
- reward
- dlpo
- wavegrad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of improving the efficiency\
  \ and quality of diffusion-based text-to-speech (TTS) models, which are computationally\
  \ intensive and struggle with modeling speech nuances. The proposed Diffusion Loss-Guided\
  \ Policy Optimization (DLPO) integrates the diffusion model\u2019s original training\
  \ loss into the reward function during reinforcement learning fine-tuning, stabilizing\
  \ training and preserving the model\u2019s generative capabilities while adapting\
  \ to human feedback."
---

# Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback

## Quick Facts
- arXiv ID: 2508.03123
- Source URL: https://arxiv.org/abs/2508.03123
- Reference count: 0
- Key outcome: DLPO improves diffusion TTS models by integrating diffusion loss into RL reward, achieving UTMOS 3.65, NISQA 4.02, WER 1.2%, with 67% human preference over baseline

## Executive Summary
This paper introduces Diffusion Loss-Guided Policy Optimization (DLPO), a reinforcement learning with human feedback approach for fine-tuning text-to-speech diffusion models. The method addresses the challenge of preserving the generative capabilities of pre-trained diffusion models while adapting them to human preferences for naturalness. By integrating the original diffusion denoising loss into the reward function during RL fine-tuning, DLPO stabilizes training and prevents overfitting that commonly occurs when optimizing only for perceptual metrics. The approach is evaluated on WaveGrad 2 using the LJSpeech dataset, demonstrating significant improvements in both objective metrics and human preference ratings.

## Method Summary
DLPO treats the TTS diffusion model as a policy in a Markov Decision Process, where denoising steps form a sequential trajectory. The method combines a naturalness reward (from UTMOS predictor) with a diffusion loss penalty that measures the difference between predicted and actual noise. This dual-objective approach constrains the model to stay within the valid generative manifold while optimizing for human-perceived quality. The training uses REINFORCE-style policy gradients over full denoising trajectories, with the diffusion loss acting as a regularization term to prevent catastrophic forgetting of the base model's capabilities.

## Key Results
- DLPO achieves UTMOS score of 3.65 (vs baseline ~2.9) on LJSpeech
- NISQA score improves to 4.02 with WER maintained at 1.2%
- Human evaluations show DLPO audio preferred 67% of the time over baseline
- Training completes in 5.5 hours on 8x A100-80GB GPUs with batch size 64

## Why This Works (Mechanism)

### Mechanism 1: Diffusion Loss Regularization
Integrating the original diffusion denoising loss into the RL reward function prevents model degradation during fine-tuning. Without this constraint, optimizing only for naturalness scores creates pressure that distorts the model's probabilistic structure, leading to noisy or incoherent audio. The diffusion loss term ensures updates remain within the manifold of valid speech generation, preserving the base model's generative capabilities while adapting to human preferences.

### Mechanism 2: Sequential MDP Formulation
The method treats denoising as a T-step finite horizon Markov Decision Process, allowing policy gradients to optimize the entire generative trajectory rather than isolated samples. This formulation enables the model to learn a policy that maximizes final reward by backpropagating value signals through all denoising steps, capturing the sequential dependencies crucial for coherent speech generation.

### Mechanism 3: Proxy Reward Signal
Using UTMOS as a reward signal enables scalable alignment with human preference without continuous human annotation. This pre-trained predictor acts as a stand-in critic, creating a dense, differentiable feedback loop that approximates human judgment of naturalness. The proxy allows rapid iteration (5.5 hours training) while maintaining correlation with actual human perception, as confirmed by the 67% human preference rate.

## Foundational Learning

**Diffusion Probabilistic Models (DDPM)**
- Why needed: DLPO is built on WaveGrad 2, requiring understanding of forward (noise addition) and reverse (denoising) processes
- Quick check: Can you explain why predicting the noise (ε) at step t differs from predicting clean audio?

**Policy Gradient / REINFORCE**
- Why needed: The fine-tuning is framed as an RL problem where policy updates use reward gradients
- Quick check: In this context, is the reward given at every denoising step or only at the end? (Final step only)

**Regularization in Fine-tuning**
- Why needed: Core innovation prevents catastrophic forgetting when optimizing for new objectives
- Quick check: Why would optimizing only for naturalness potentially make the model worse at intelligibility?

## Architecture Onboarding

**Component map**: Text -> Encoder -> Policy (WaveGrad 2R) -> Denoising steps -> Audio -> UTMOS Reward Model -> Loss -> Policy Update

**Critical path**: The interaction between reward weight (α) and diffusion loss weight (β) dictates the trade-off between exploring better naturalness and staying true to the base model.

**Design tradeoffs**:
- High α: Faster UTMOS improvement but risk of artifacts or hallucinated prosody
- High β: Very stable training but may constrain model so much it cannot improve over baseline

**Failure signatures**:
- RWR/DDPO: Noisy, robotic, or metallic audio (UTMOS drops to ~2.18)
- OnlyDL: Stagnation; consistent audio identical to baseline with no quality gain

**First 3 experiments**:
1. Baseline Sanity Check: Run inference on WaveGrad 2R (Base) and record UTMOS/NISQA (expect UTMOS ~2.9)
2. Overfitting Check: Fine-tune using only UTMOS reward (no diffusion loss) to observe reward hacking collapse
3. DLPO Ablation: Implement full DLPO objective and sweep β (0.1 to 1.0) to find sweet spot where UTMOS rises to ~3.65 without increasing WER

## Open Questions the Paper Calls Out

**Multilingual Adaptation**: Can DLPO be effectively adapted for multilingual text-to-speech synthesis? The experimental validation was restricted to English-only LJSpeech, leaving cross-lingual stability and effectiveness unproven.

**Prosodic Control**: Does DLPO enhance prosodic accuracy and emotional expressiveness in generated speech? Current evaluation relies on naturalness metrics that don't quantitatively measure fine-grained prosodic features or emotional depth.

**Speaker Adaptation**: Is DLPO effective for zero-shot or few-shot speaker adaptation? The single-speaker study leaves the framework's capacity to generalize to unseen voices without catastrophic forgetting unknown.

## Limitations

**Hyperparameter Sensitivity**: Performance critically depends on balancing reward weight (α) and diffusion loss weight (β), but these values are not reported, making exact reproduction challenging.

**Single-Speaker Evaluation**: All experiments use LJSpeech with one female speaker, leaving generalization to diverse voices, accents, and emotional expressions unverified.

**Reward Model Reliability**: While human preference favors DLPO 67% of the time, 33% still prefer baseline or alternatives, suggesting UTMOS proxy may have systematic blind spots.

## Confidence

**High Confidence**: The fundamental mechanism of integrating diffusion loss into RL rewards to prevent catastrophic forgetting is theoretically sound and demonstrated across multiple ablations.

**Medium Confidence**: Claims about "real-time, resource-limited settings" applicability are supported but not directly tested; objective metrics show improvement but within narrow range.

**Low Confidence**: Assertion that DLPO "stabilizes training" compared to other RL methods lacks statistical significance testing and systematic quantification across multiple random seeds.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically sweep α and β values across multiple orders of magnitude to map performance landscape and identify whether improvements are robust or occur within narrow parameter window.

2. **Cross-Dataset Generalization Test**: Evaluate DLPO on multi-speaker datasets like LibriTTS or VCTK to verify method's claims about preserving generative capabilities across diverse voices.

3. **Statistical Significance Validation**: Conduct pairwise human preference tests with confidence intervals and significance testing across multiple raters and random seeds to establish whether 67% preference represents reliable improvement.