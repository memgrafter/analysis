---
ver: rpa2
title: 'MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages'
arxiv_id: '2512.01512'
source_url: https://arxiv.org/abs/2512.01512
tags:
- language
- speech
- s2tt
- translation
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling multilingual speech-to-text
  translation (S2TT) for large multimodal language models (MLLMs), which face limitations
  in language coverage and inference efficiency. To overcome these issues, the authors
  propose the Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT)
  framework.
---

# MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages

## Quick Facts
- arXiv ID: 2512.01512
- Source URL: https://arxiv.org/abs/2512.01512
- Reference count: 36
- Achieves many-to-many speech-to-text translation across 70 languages using only ~100M trainable parameters and 10 hours of data per language

## Executive Summary
This paper addresses the challenge of scaling multilingual speech-to-text translation (S2TT) for large multimodal language models (MLLMs), which face limitations in language coverage and inference efficiency. The authors propose the Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which introduces two key innovations: a language scaling strategy combining curriculum learning and data balancing to extend support to 70 languages, and an optimized speech adapter that compresses speech sequences from 750 to just 30 tokens. The framework demonstrates state-of-the-art performance on the FLEURS dataset across 70x69 translation directions while significantly improving inference efficiency. The code and models are released open-source.

## Method Summary
The MCAT framework tackles two core challenges in multilingual S2TT: expanding language coverage and improving inference efficiency. The language scaling strategy employs curriculum learning to gradually introduce languages from high-resource to low-resource, combined with data balancing techniques to ensure equitable model performance across all languages. The optimized speech adapter compresses input speech tokens from 750 to 30 while preserving essential information, enabling faster processing without sacrificing translation quality. The approach uses only ~100M trainable parameters and requires just 10 hours of data per language, making it highly scalable and cost-effective for expanding to many languages.

## Key Results
- Achieves state-of-the-art performance on FLEURS dataset across 70x69 translation directions
- Reduces speech sequence tokens from 750 to 30 using optimized speech adapter
- Supports many-to-many S2TT across 70 languages with only ~100M trainable parameters
- Requires only 10 hours of data per language for training
- Demonstrates improved batch inference efficiency compared to baseline models

## Why This Works (Mechanism)
The success of MCAT stems from its dual optimization approach. First, the language scaling strategy leverages curriculum learning to gradually introduce languages based on resource availability, preventing catastrophic forgetting and ensuring stable performance across all languages. The data balancing component ensures that low-resource languages receive adequate attention during training, preventing the model from biasing toward high-resource languages. Second, the speech adapter optimization dramatically reduces computational overhead by compressing speech sequences while preserving critical linguistic information through learned representations. This compression enables faster inference without significant quality degradation, making the system practical for real-world deployment across many languages.

## Foundational Learning
- **Curriculum Learning**: Training strategy that presents examples in a meaningful order (from easy to hard or high-resource to low-resource) - needed to prevent model collapse when adding new languages; quick check: monitor performance degradation when adding languages out of curriculum order
- **Speech Token Compression**: Reducing high-dimensional speech input to lower-dimensional representations - needed to improve inference speed while maintaining translation quality; quick check: compare BLEU scores at different compression ratios
- **Data Balancing**: Ensuring equitable representation of all languages during training - needed to prevent model bias toward high-resource languages; quick check: analyze per-language performance distribution
- **Adapter-Based Fine-tuning**: Adding small trainable modules to frozen pre-trained models - needed to enable efficient multilingual adaptation with limited parameters; quick check: measure parameter efficiency vs. full fine-tuning
- **Many-to-Many Translation**: Model capable of translating between any language pair in both directions - needed for practical multilingual applications; quick check: verify bidirectional performance symmetry
- **Multimodal Language Models**: Models that process both text and speech modalities - needed as foundation for unified S2TT approach; quick check: test model on pure text translation as baseline

## Architecture Onboarding

**Component Map:**
Input Speech -> Speech Adapter -> Multimodal Encoder -> Language-Specific Adapter -> Decoder -> Output Text

**Critical Path:**
Speech input → Speech adapter compression (750→30 tokens) → Multimodal encoder processing → Language-specific adaptation → Text generation

**Design Tradeoffs:**
- Compression ratio vs. translation quality (30 tokens vs. 750)
- Number of trainable parameters vs. model capacity (~100M vs. full model)
- Training data per language (10 hours) vs. coverage quality
- Curriculum order (high→low resource) vs. random language addition

**Failure Signatures:**
- Performance degradation when adding languages out of curriculum order
- Disproportionate performance drop for low-resource languages
- Increased latency when speech adapter compression is too aggressive
- Catastrophic forgetting when fine-tuning without proper regularization

**First 3 Experiments:**
1. Measure translation quality degradation at different speech adapter compression ratios (15, 30, 60 tokens)
2. Compare curriculum learning vs. random language addition on final 70-language performance
3. Benchmark inference speed improvement from token compression across different batch sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on FLEURS dataset, which may not represent real-world translation challenges
- Claims of minimal data requirements (10 hours per language) need validation on truly low-resource languages
- Speech adapter compression may introduce information loss that isn't fully characterized
- Focus on batch inference efficiency without extensive real-time streaming evaluation
- Language scaling strategy requires careful tuning and may not generalize across all language families

## Confidence

**High confidence:**
- Technical implementation and experimental results on FLEURS dataset
- Reported inference efficiency improvements from speech adapter optimization

**Medium confidence:**
- Generalization of results to other datasets and real-world applications
- Claimed minimal data requirements for effective multilingual training

**Low confidence:**
- Scalability to languages beyond the 70 covered, especially truly low-resource languages
- Performance in real-time streaming scenarios

## Next Checks
1. Evaluate the model on additional speech-to-text translation benchmarks (e.g., MuST-C, CVSS) to verify generalization beyond FLEURS
2. Conduct ablation studies on the speech adapter compression ratio to quantify information loss at different token compression levels
3. Test the model's performance on truly low-resource languages (less than 10 hours of training data) to validate claims about minimal data requirements