---
ver: rpa2
title: Improved Representation Steering for Language Models
arxiv_id: '2505.20809'
source_url: https://arxiv.org/abs/2505.20809
tags:
- steering
- reps
- score
- concept
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reference-free Preference Steering (RePS),
  a bidirectional preference optimization objective for improving representation steering
  in language models. RePS addresses the gap between steering interventions (like
  LoRA, ReFT, SV) and prompting by optimizing for human-preferred behaviors without
  relying on a reference model.
---

# Improved Representation Steering for Language Models

## Quick Facts
- **arXiv ID:** 2505.20809
- **Source URL:** https://arxiv.org/abs/2505.20809
- **Reference count:** 40
- **One-line primary result:** RePS-trained interventions substantially narrow the performance gap with prompting and remain robust to jailbreaking attacks that bypass text-based defenses.

## Executive Summary
This paper introduces Reference-free Preference Steering (RePS), a bidirectional preference optimization objective for improving representation steering in language models. RePS addresses the gap between steering interventions (like LoRA, ReFT, SV) and prompting by optimizing for human-preferred behaviors without relying on a reference model. The method jointly optimizes concept incorporation and suppression using a reference-free reward function derived from SimPO. Evaluated on Gemma models (2B to 27B) using AXBENCH, RePS-trained interventions outperform standard language modeling and BiPO baselines, substantially narrowing the performance gap with prompting. For suppression, RePS matches language modeling objectives on Gemma-2 and outperforms them on Gemma-3 models. Critically, RePS-trained models remain robust to prompt-based jailbreaking attacks that bypass text-based defenses, demonstrating interpretability, scalability, and resilience advantages over prompting.

## Method Summary
RePS is a bi-directional preference optimization objective that steers language models to incorporate or suppress concepts without requiring a reference model. It uses preference triplets (instruction, original response, steered response) and optimizes for human-preferred behaviors. The method implements positive steering (incorporating concepts) and negative steering (removing concepts) through weighted differences in response likelihoods, with steering factors sampled uniformly during training. The reference-free reward function is derived from SimPO, jointly optimizing concept incorporation and suppression.

## Key Results
- RePS-trained interventions outperform standard language modeling and BiPO baselines on Gemma models (2B-27B)
- Performance approaches prompt-level steering on concept incorporation tasks
- RePS-trained models remain robust to prompt-based jailbreaking attacks that bypass text-based defenses
- For suppression, RePS matches language modeling objectives on Gemma-2 and outperforms them on Gemma-3 models

## Why This Works (Mechanism)
RePS works by optimizing for human-preferred behaviors through a reference-free preference objective. Unlike traditional steering methods that optimize for likelihood or supervised learning objectives, RePS directly optimizes for the difference between steered and original responses using a bi-directional preference framework. The reference-free aspect means it doesn't require a reference model for reward computation, instead using the difference in likelihoods between steered and original responses as the optimization signal. The uniform sampling of steering factors during training helps stabilize the optimization process and prevents overfitting to specific steering magnitudes.

## Foundational Learning
- **Preference Optimization**: Optimizing model behavior based on human preferences rather than likelihood objectives; needed to understand why RePS outperforms traditional language modeling objectives; quick check: verify understanding of how preference triplets are constructed and used in the loss function
- **Steering Vectors (SVs)**: Low-rank interventions that steer model behavior by modifying representations; needed to understand the intervention mechanism being optimized; quick check: confirm understanding of how SVs project out steering directions in the null term
- **SimPO**: A preference optimization method that provides the foundation for RePS's reference-free reward function; needed to understand the theoretical basis for the optimization objective; quick check: verify understanding of how SimPO differs from other preference optimization methods
- **Factor Sampling**: Uniform sampling of steering factors during training to stabilize optimization; needed to understand the training dynamics and prevent overfitting; quick check: confirm understanding of how factor sampling affects variance in steering scores
- **BiPO**: Bidirectional preference optimization baseline; needed as comparison point to understand RePS improvements; quick check: verify understanding of how BiPO differs from RePS in its objective formulation
- **Harmonic Mean Evaluation**: Using harmonic mean of multiple evaluation metrics (Concept, Instruct, Fluency scores) to provide balanced assessment; needed to understand the evaluation methodology; quick check: confirm understanding of why harmonic mean is used instead of arithmetic mean

## Architecture Onboarding

**Component Map:** GPT-4o-mini (response generation) -> AXBENCH (evaluation dataset) -> RePS training (optimization) -> Gemma models (2B-27B) -> LM judge (evaluation)

**Critical Path:** Response generation → Triplet construction → RePS loss computation → Parameter updates → Evaluation

**Design Tradeoffs:** Reference-free vs. reference-based reward computation (simpler but requires careful implementation); factor sampling vs. fixed factors (stabilized training vs. simpler implementation); direct preference optimization vs. supervised learning (human-aligned behavior vs. computational efficiency)

**Failure Signatures:** High variance in training (indicates factor sampling not implemented correctly); poor performance on larger models (suggests hyperparameter scaling issues); degradation on specific layers (indicates layer sensitivity); susceptibility to jailbreaking (suggests inadequate suppression capability)

**First Three Experiments:**
1. Implement RePS loss with factor sampling on Gemma-2-2B using specified layers (10 & 20) and verify it outperforms BiPO baseline
2. Test the reference-free reward computation by comparing β calculation using frozen base model vs. current steered model
3. Implement and validate the Φ_Null projection mechanism for negative steering and measure its impact on suppression performance

## Open Questions the Paper Calls Out
- Why does RePS improve over the language modeling objective for steering? The paper notes this requires deeper understanding but only provides intuition about preference signals without mechanistic analysis.
- Can bootstrapping training examples from the target LM itself improve RePS training convergence and performance? The current approach relies on GPT-4o-mini, creating potential distribution mismatch.
- What determines the scaling relationship between optimal steering factor magnitude and model size? The paper observes scaling from 2-20 for Gemma-2 to 20-400 for Gemma-3 but provides no theoretical explanation.

## Limitations
- The exact implementation of the reference-free reward function is ambiguous, particularly the probability model used for the scaling term β
- The "losing" response generation process is underspecified, mentioning "steering LMs" without detailing the exact model or prompt
- RePS-trained LoReFT shows catastrophic failures on larger Gemma-3 models while performing well on smaller Gemma-2 models, suggesting scalability limitations

## Confidence
- **High confidence**: Core empirical findings showing RePS outperforms BiPO and approaches prompt-level performance on Gemma-2 models
- **Medium confidence**: Suppression performance claims, especially for Gemma-3 models where results are less thoroughly analyzed
- **Low confidence**: Exact mathematical formulation of the reference-free preference objective due to ambiguous implementation details

## Next Checks
1. Verify the probability model for β calculation by testing whether using the frozen base model versus the current steered model for computing log-probabilities produces different training dynamics and final performance
2. Validate the negative steering implementation by implementing and comparing multiple interpretations of Φ_Null (projection-based vs. zero-weight initialization) to confirm which matches the paper's results
3. Replicate the factor sampling ablation by conducting a controlled experiment varying the factor sampling strategy (fixed vs. sampled α) on the same model/layer configuration to quantify the exact impact on steering score variance and final performance