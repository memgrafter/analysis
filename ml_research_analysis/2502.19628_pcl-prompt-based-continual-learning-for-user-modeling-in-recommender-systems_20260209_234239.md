---
ver: rpa2
title: 'PCL: Prompt-based Continual Learning for User Modeling in Recommender Systems'
arxiv_id: '2502.19628'
source_url: https://arxiv.org/abs/2502.19628
tags:
- task
- tasks
- user
- prompts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PCL, a prompt-based continual learning framework
  for user modeling in recommender systems. The key innovation is using position-wise
  prompts as external memory to mitigate catastrophic forgetting during sequential
  task learning, while contextual prompts capture inter-task relationships via attention
  mechanisms.
---

# PCL: Prompt-based Continual Learning for User Modeling in Recommender Systems

## Quick Facts
- arXiv ID: 2502.19628
- Source URL: https://arxiv.org/abs/2502.19628
- Reference count: 26
- Primary result: PCL outperforms single-task and multi-task baselines on two real-world datasets across 8 tasks, achieving best performance in 6 tasks while mitigating catastrophic forgetting.

## Executive Summary
This paper proposes PCL, a prompt-based continual learning framework for user modeling in recommender systems. The key innovation is using position-wise prompts as external memory to mitigate catastrophic forgetting during sequential task learning, while contextual prompts capture inter-task relationships via attention mechanisms. PCL outperforms single-task and multi-task baselines across two real-world datasets (Tenrec and MovieLens), achieving the best performance in 6 out of 8 tasks. It demonstrates robustness to task ordering, excels with cold-start items, and generates universal user representations applicable to unseen models.

## Method Summary
PCL uses a frozen sequential recommendation backbone (SASRec) pretrained on next-item prediction, with task-specific learning isolated to learnable prompt vectors and lightweight task adapters. Position-wise prompts preserve task-specific knowledge while preventing catastrophic forgetting by initializing subsequent task prompts from previous ones. Contextual prompts capture inter-task relationships through attention over task description embeddings encoded by a frozen pretrained language model. The framework achieves continual learning efficiency by training only ~|T|×n×d prompt parameters plus adapter weights, avoiding expensive backbone retraining.

## Key Results
- PCL achieves best performance in 6 out of 8 tasks while preserving only one backbone copy
- Performance remains robust across different task ordering permutations with only 1.20% average degradation
- PCL excels at cold-start items, achieving 3.36% and 1.74% improvements over state-of-the-art baselines on Tenrec and MovieLens
- Universal user representations from PCL improve convergence and accuracy when transferred to an unseen MLP model for T4

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Position-wise prompts serve as external memory that preserves task-specific knowledge while enabling sequential task learning without backbone modification.
- **Mechanism:** Each downstream task maintains a learnable prompt tensor p_k ∈ R^(n×d). For task T_3 onward, prompts initialize from the previous task's optimized prompt (p_k = p_{k-1}), enabling knowledge inheritance. Only the last t items in behavior sequences are fused with prompts via element-wise addition, reflecting the assumption that recent interactions carry more task-relevant signal.
- **Core assumption:** Recent items in a user behavior sequence are more informative for downstream tasks than earlier items.
- **Evidence anchors:**
  - [abstract]: "utilizes position-wise prompts as external memory for each task, preserving knowledge and mitigating catastrophic forgetting"
  - [Section 3.1]: "the position-wise prompt for each task inherits and retains the knowledge from previously trained tasks during new task adaptation"
  - [Section 4.2]: PCL achieves best performance in 6/8 tasks while preserving only one backbone copy

### Mechanism 2
- **Claim:** Contextual prompts capture inter-task relationships through attention over task description embeddings, improving prompt tuning direction.
- **Mechanism:** A frozen pretrained sentence transformer encodes task descriptions (name, input/output formats, evaluation metric) into embeddings E_T. Multi-head attention learns adaptive task relations, producing contextual prompts P_T that are added to position-fused embeddings with a scaling factor λ.
- **Core assumption:** Task descriptions contain sufficient semantic information to infer useful inter-task relationships.
- **Evidence anchors:**
  - [abstract]: "contextual prompts to capture and leverage inter-task relationships during prompt tuning"
  - [Section 3.2]: "employing PLM to encode task descriptions leads to stable enhancement in both datasets"
  - [Section 4.3]: Ablation shows PCL with PLM outperforms random initialization variant across tasks

### Mechanism 3
- **Claim:** Freezing the pretrained backbone while training only prompts and task adapters prevents interference with foundational user-item representations.
- **Mechanism:** The sequential recommendation backbone (SASRec) and item embeddings are pretrained on self-supervised next-item prediction (T_1) and frozen thereafter. Each downstream task adds a lightweight task adapter (linear layer or MLP) for output mapping. This isolates task-specific learning to ~|T|×n×d prompt parameters plus adapter weights.
- **Core assumption:** The pretrained backbone captures sufficiently general user behavior patterns transferable across heterogeneous downstream tasks.
- **Evidence anchors:**
  - [Section 3.3]: "the backbone model and item embeddings are frozen in the downstream tasks"
  - [Section 4.6]: Universal user representations from PCL improve convergence and accuracy when transferred to an unseen MLP model for T_4

## Foundational Learning

- **Concept: Catastrophic forgetting in continual learning**
  - Why needed here: PCL's core contribution addresses this phenomenon where neural networks overwrite previously learned weights when training on new tasks. Understanding this explains why prompt-based parameter isolation matters.
  - Quick check question: Can you explain why updating all model parameters on task T_k would degrade performance on T_{k-1}?

- **Concept: Prompt tuning in pretrained models**
  - Why needed here: The framework extends prompt tuning from NLP/vision domains to recommender systems. Understanding how small learnable prompt vectors condition frozen backbones is essential.
  - Quick check question: How does adding a prompt vector differ from fine-tuning model weights, in terms of parameter count and optimization dynamics?

- **Concept: Multi-head attention for task relation modeling**
  - Why needed here: Contextual prompts use attention to learn which tasks share relevant features. Understanding attention as a soft retrieval mechanism clarifies how PCL captures inter-task structure.
  - Quick check question: In the attention computation PT = MultiHead(E_T), what does a high attention weight between task T_i and T_j imply about their relationship?

## Architecture Onboarding

- **Component map:**
  - Backbone (SASRec) -> Item embeddings -> Position-wise prompts -> Task adapters -> Output
  - Contextual prompt module: Sentence transformer -> Multi-head attention -> Position-wise prompt fusion

- **Critical path:**
  1. Pretrain backbone + item embeddings on T_1 (self-supervised next-item prediction)
  2. Freeze backbone; initialize position-wise prompt p_2 randomly
  3. For each downstream task T_k (k≥2): compute contextual prompts via attention, fuse with position-wise prompts, train only prompts + task adapter
  4. For T_k (k≥3): initialize p_k = p_{k-1} for knowledge inheritance
  5. Inference: Load task-specific prompt and adapter; run single forward pass

- **Design tradeoffs:**
  - Prompt fusion depth t: Larger t captures more context but increases interference risk; paper uses t for last items only
  - Prompt intensity λ: Controls contextual prompt influence; not explicitly tuned in paper
  - Task order: Paper shows 1.20% average degradation across permutations, but first task (pretraining) is always next-item prediction

- **Failure signatures:**
  - Task performance drops significantly when task order changes → check prompt initialization chain
  - Contextual prompt ablation shows no improvement → verify task descriptions are informative and sentence encoder is appropriate
  - Cold-start items fail to converge → confirm prompt tuning is active (not just adapter training)

- **First 3 experiments:**
  1. Replicate the T_1 pretraining → T_2 adaptation pipeline on a small subset; verify frozen backbone + prompt training converges while full fine-tuning causes T_1 performance collapse.
  2. Ablate contextual prompts (random initialization vs. PLM-encoded); expect ~0.3-1.5% accuracy/HR degradation consistent with Table 2.
  3. Test task order robustness: shuffle T_2, T_3, T_4 and measure per-task variance; expect <2% average degradation per Figure 2.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, based on the content and methodology, several implicit questions arise regarding the framework's limitations and future research directions.

## Limitations
- The framework's performance sensitivity to hyperparameters (prompt fusion window size, prompting intensity) is not thoroughly explored through systematic ablation studies.
- The sentence transformer model used for task description encoding is not identified, making exact replication challenging.
- The evaluation focuses primarily on final performance rather than explicit forgetting measures (e.g., task retention curves, forgetting metric calculations).

## Confidence
- **High confidence**: Position-wise prompts effectively mitigate catastrophic forgetting (supported by 6/8 tasks outperforming baselines while preserving single backbone copy)
- **Medium confidence**: Contextual prompts meaningfully capture inter-task relationships (supported by ablation showing PLM-encoded tasks outperform random initialization, though mechanism remains somewhat abstract)
- **Medium confidence**: Universal user representations generalize to unseen models (supported by transfer experiment showing improved convergence/accuracy, but only tested on one MLP variant)

## Next Checks
1. **Parameter isolation verification**: After training T2-T4 sequentially, freeze the backbone and evaluate T1 performance; confirm it remains stable while full fine-tuning causes degradation, directly validating the catastrophic forgetting prevention claim.

2. **Prompt inheritance ablation**: Modify the framework to use random initialization for all tasks (breaking the p_k = p_{k-1} chain) and measure per-task performance variance; expect 1-3% average degradation consistent with the framework's stated knowledge inheritance benefit.

3. **Task description quality sensitivity**: Replace task descriptions with random text or minimal descriptions and retrain; verify performance drops 0.5-2% to confirm that meaningful task semantics in the PLM encoding contribute to contextual prompt effectiveness.