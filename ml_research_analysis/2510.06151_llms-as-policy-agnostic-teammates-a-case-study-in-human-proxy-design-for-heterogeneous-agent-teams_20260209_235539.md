---
ver: rpa2
title: 'LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for
  Heterogeneous Agent Teams'
arxiv_id: '2510.06151'
source_url: https://arxiv.org/abs/2510.06151
tags:
- human
- stag
- agents
- llms
- hare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  serve as policy-agnostic human proxies in heterogeneous-agent teams. Three experiments
  were conducted in a grid-world stag hunt game to evaluate LLM decision-making.
---

# LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams

## Quick Facts
- arXiv ID: 2510.06151
- Source URL: https://arxiv.org/abs/2510.06151
- Reference count: 37
- Large language models can serve as policy-agnostic human proxies in heterogeneous-agent teams, achieving expert alignment in grid-world stag hunt games

## Executive Summary
This study investigates whether large language models (LLMs) can function as policy-agnostic human proxies in heterogeneous-agent teams. The research focuses on three key capabilities: decision-making alignment with human behavior, risk sensitivity through prompt engineering, and coherent multi-step planning in dynamic environments. Using a grid-world stag hunt game framework, the experiments demonstrate that LLMs can effectively mirror human decision patterns, exhibit controllable risk preferences, and generate strategic action sequences that maintain consistency across time steps.

## Method Summary
The research employed a grid-world stag hunt game as a testbed for evaluating LLM performance as human proxies. Three experiments were conducted: (1) measuring decision-making alignment between LLM-generated and human actions using F1-scores and Cohen's kappa, (2) testing prompt modifications to induce risk-sensitive behaviors, and (3) assessing multi-step action sequence generation in dynamic environments. The study compared different LLM sizes (including LLaMA 3.1 70B and Mixtral 8x22B) and evaluated their performance against human decision patterns across binary action spaces and varying agent group sizes.

## Key Results
- Larger LLM models (LLaMA 3.1 70B, Mixtral 8x22B) achieved expert alignment with F1-scores > 0.75 and Cohen's kappa > 0.58
- Prompt engineering successfully induced both risk-seeking and risk-averse behaviors in LLM agents
- LLM agents generated coherent multi-step action sequences that mirrored human decision patterns in dynamic environments

## Why This Works (Mechanism)
The mechanism relies on LLMs' ability to process contextual game-state information and generate contextually appropriate responses through carefully engineered prompts. The policy-agnostic nature emerges from the models' general reasoning capabilities rather than domain-specific training. By providing relevant features as prompts, LLMs can approximate human decision-making processes without requiring explicit policy programming for each agent type.

## Foundational Learning
- **Prompt engineering**: Why needed - Controls LLM behavior and risk preferences; Quick check - Test different prompt formulations to verify consistent behavior changes
- **Game theory foundations**: Why needed - Provides theoretical framework for evaluating coordination and decision-making; Quick check - Validate results across multiple game-theoretic scenarios
- **Multi-agent coordination**: Why needed - Essential for understanding heterogeneous team dynamics; Quick check - Test performance with varying agent group sizes
- **Decision alignment metrics**: Why needed - Quantifies similarity between human and LLM decision patterns; Quick check - Cross-validate with alternative alignment measures
- **Risk sensitivity modeling**: Why needed - Captures behavioral preferences in decision-making; Quick check - Test risk preference consistency across different contexts
- **Dynamic planning sequences**: Why needed - Ensures coherent behavior over time; Quick check - Evaluate sequence coherence under varying environmental conditions

## Architecture Onboarding

**Component Map**
Human proxy design -> LLM model selection -> Prompt engineering -> Game environment -> Decision alignment evaluation

**Critical Path**
Game-state features → Prompt formulation → LLM inference → Action selection → Performance evaluation

**Design Tradeoffs**
- Model size vs. computational efficiency
- Prompt complexity vs. interpretability
- Binary vs. continuous action spaces
- Static vs. dynamic environment modeling

**Failure Signatures**
- Poor alignment metrics indicating mismatch with human behavior
- Inconsistent action sequences suggesting lack of strategic coherence
- Inability to adapt to prompt modifications showing brittle behavior
- Performance degradation with increased agent complexity

**Three First Experiments**
1. Cross-domain generalization test with different game-theoretic scenarios
2. Scalability validation with larger heterogeneous agent groups
3. Real-world deployment simulation in complex environments

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to simplified grid-world stag hunt game
- Binary action space may not capture real-world multi-agent coordination complexity
- Human proxy design relies heavily on prompt engineering, introducing potential brittleness
- Only tested up to three agents, unclear how LLMs would perform in larger heterogeneous teams
- Evaluation metrics measure statistical alignment but don't capture strategic reasoning quality

## Confidence

**High confidence**: Larger LLM models consistently outperform smaller ones in policy alignment, supported by robust quantitative metrics (F1 > 0.75, kappa > 0.58).

**Medium confidence**: Risk-sensitive behavior modification through prompt engineering shows promise but may not generalize beyond stag hunt context.

**Medium confidence**: Coherent multi-step action sequences demonstrated within controlled setting, but real-world applicability remains uncertain due to simplified environment.

## Next Checks

1. **Cross-domain generalization test**: Evaluate LLM models on different game-theoretic scenarios (e.g., Prisoner's Dilemma, Battle of the Sexes) to assess policy-agnostic capabilities beyond stag hunt framework.

2. **Scalability validation**: Conduct experiments with larger heterogeneous agent groups (5+ agents) to identify potential performance degradation or emergent coordination challenges.

3. **Real-world deployment simulation**: Implement more complex, partially observable environment with continuous state spaces and longer time horizons to test LLM proxy robustness under realistic multi-agent coordination constraints.