---
ver: rpa2
title: Distilling Time Series Foundation Models for Efficient Forecasting
arxiv_id: '2601.12785'
source_url: https://arxiv.org/abs/2601.12785
tags:
- forecasting
- time
- series
- distillation
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DistilTS, the first distillation framework
  designed specifically for Time Series Foundation Models (TSFMs). DistilTS addresses
  two key challenges in TSFM distillation: task difficulty discrepancy, where optimization
  favors short-term forecasting over long-term horizons, and architecture discrepancy,
  where structural mismatches between teacher and student models hinder effective
  knowledge transfer.'
---

# Distilling Time Series Foundation Models for Efficient Forecasting

## Quick Facts
- arXiv ID: 2601.12785
- Source URL: https://arxiv.org/abs/2601.12785
- Reference count: 0
- This paper introduces DistilTS, the first distillation framework designed specifically for Time Series Foundation Models (TSFMs).

## Executive Summary
DistilTS is the first distillation framework designed specifically for Time Series Foundation Models (TSFMs). It addresses two key challenges: task difficulty discrepancy where optimization favors short-term forecasting over long-term horizons, and architecture discrepancy where structural mismatches between teacher and student models hinder effective knowledge transfer. The framework introduces horizon-weighted objectives to balance learning across different forecasting horizons and a factorized temporal alignment module to bridge representational gaps between student and teacher models.

## Method Summary
DistilTS is a distillation framework that trains compact student models to mimic larger TSFMs through three objectives: supervised loss on ground truth, horizon-weighted knowledge distillation loss to balance short and long-term forecasting, and factorized temporal alignment to bridge architectural gaps. The framework introduces exponential weighting across forecasting horizons to counter optimization bias toward easier short-term tasks, and uses a factorized projection module to align variate-wise student embeddings with point-wise teacher representations without increasing student parameter count.

## Key Results
- DistilTS achieves performance comparable to full-sized TSFMs while reducing parameters by up to 1/150
- Inference acceleration of up to 6000x compared to teacher models
- Effective across multiple real-world benchmarks with different TSFM architectures (TimeMoE, Moirai, Chronos)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exponentially increasing weights across forecasting horizons rebalances gradient signals so long-term predictions receive sufficient supervision during distillation.
- **Mechanism:** Short-term forecasts yield lower, more consistent errors with stronger gradients; long-term forecasts have higher variance and weaker signals. The weighting scheme `w_t = exp(τt/(T-1)) / Σ` amplifies contributions from later timesteps, counteracting the natural optimization bias toward easy horizons. Conceptually analogous to temperature scaling in classification KD.
- **Core assumption:** The seesaw effect from multi-task learning applies to time series horizons—optimization favors easy tasks without explicit rebalancing. **Assumption:** This imbalance persists even when student capacity is sufficient.
- **Evidence anchors:**
  - [abstract] "optimization favors easier short-term horizons over harder long-term ones"
  - [section 2.1.1] "Short-term forecasting exhibit stronger and more consistent gradients, while long-term forecasting suffer from lower signal-to-noise ratios and higher variance"
  - [corpus] Related work (Synapse, arXiv:2511.05460) notes TSFMs struggle with "varied seasonalities, trends, and long-range dependencies," supporting horizon-specific difficulty differences.
- **Break condition:** If your task has uniform difficulty across horizons (e.g., synthetic data with no temporal uncertainty growth), weighting may over-correct and harm short-term accuracy.

### Mechanism 2
- **Claim:** Projecting variate-wise student embeddings through a time-modulated latent space enables alignment with point-wise teacher representations without inflating student token count.
- **Mechanism:** Teacher produces `H^T ∈ R^{B×D×T×d_T}` (point-wise across time). Student produces `H^S ∈ R^{B×D×d_S}` (one embedding per variate). FTA applies: `Ĥ^T = W_out · φ((W_s · h^S) ⊙ E_t)`, where `E_t` is a learnable time embedding. This factorizes variable identity and temporal modulation, allowing compact students to reconstruct point-wise signals.
- **Core assumption:** Temporal information can be recovered from variate-wise embeddings through learned modulation rather than explicit temporal tokenization. **Assumption:** The student encoder preserves sufficient temporal structure in its variate embeddings for modulation to exploit.
- **Evidence anchors:**
  - [abstract] "factorized temporal alignment module that bridges architectural gaps by projecting student embeddings into the teacher's temporal space"
  - [section 2.1.2] "variate-wise tokenization compresses temporal information into one embedding per variate, yielding a much lighter student model"
  - [section 2.1.3] Boundary case proof shows FTA can exactly recover point-wise representations with time-separable inputs and one-hot time embeddings.
  - [corpus] Weak direct evidence—corpus papers focus on TSFM challenges but not distillation alignment mechanisms.
- **Break condition:** If student variate embeddings collapse temporal information irreversibly (e.g., aggressive pooling), modulation cannot recover it. Validate by checking student embedding entropy across time.

### Mechanism 3
- **Claim:** Teacher model choice impacts distillation quality more than teacher scale beyond a threshold.
- **Mechanism:** Ablation (Figure 3) shows TimeMoe-50M and TimeMoe-200M yield similar student performance, while Moirai underperforms as a teacher. The paper hypothesizes TSFMs don't strictly follow scaling laws—larger models may even degrade forecasting quality (referencing BLAST [29]).
- **Core assumption:** Knowledge transfer depends more on representation quality and compatibility than raw parameter count. **Assumption:** The teacher's point-wise representation structure is learnable by the alignment module regardless of teacher scale.
- **Evidence anchors:**
  - [section 3.3] "increasing teacher size provides only limited additional benefits...larger models may even degrade forecasting quality"
  - [figure 3 caption] "choice of teacher architecture significantly impacts distillation performance"
  - [corpus] Multi-Scale Finetuning (arXiv:2506.14087) notes naive TSFM fine-tuning "falls short," indirectly supporting architectural over scale considerations.
- **Break condition:** If your student model is very deep or uses a fundamentally different temporal encoding (e.g., frequency-domain), teacher-student compatibility may degrade regardless of teacher choice.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here:** The core technique—training a compact "student" model to mimic a larger "teacher" model's outputs and/or representations. DistilTS extends KD with time-series-specific objectives.
  - **Quick check question:** Can you explain why matching soft teacher outputs (logits or predictions) might preserve more information than matching hard labels alone?

- **Concept: Multi-Task Learning Gradient Dynamics**
  - **Why needed here:** Forecasting across horizons is inherently multi-task. The paper invokes the "seesaw effect" where gradient dominance from easy tasks harms hard-task learning.
  - **Quick check question:** If you summed losses from 96-step and 192-step forecasts equally, why might the 96-step task still dominate training?

- **Concept: Variate-wise vs. Point-wise Embeddings**
  - **Why needed here:** This architectural mismatch is central to the paper. Point-wise = embedding per (variate, timestep); variate-wise = one embedding per variate summarizing all timesteps.
  - **Quick check question:** For a 7-variable time series with 96 timesteps, how many token embeddings would point-wise vs. variate-wise encoding produce?

## Architecture Onboarding

- **Component map:** Time Series Input → [Teacher: Frozen TSFM (TimeMoE/Moirai/Chronos)] → Point-wise hidden states H^T ∈ R^{B×D×T×d_T} → Teacher predictions ŷ^T → [Student: Trainable (DLinear or iTransformer)] → Variate-wise embeddings H^S ∈ R^{B×D×d_S} → Student predictions ŷ^S → [Losses] → L_sup: MSE(ŷ^S, ground truth y) → L_KD: Horizon-weighted MSE(ŷ^T, ŷ^S) → L_FTA: Alignment ||Ĥ^T - H^T||² → Total: L_sup + L_KD + L_FTA

- **Critical path:**
  1. **Lookback alignment:** Student and teacher must use identical lookback windows—TSFMs are highly sensitive to context length (Section 3.1).
  2. **Horizon weight calibration:** τ controls weighting aggressiveness. Start with τ=1, increase if long-horizon metrics lag.
  3. **FTA projection dimension:** The latent dimension `u` must be sufficient to decompress temporal info; the paper uses hidden=512, FFN=2048.

- **Design tradeoffs:**
  - DLinear student → faster training (10 epochs), simpler, but may underfit complex patterns
  - iTransformer student → more expressive (4 epochs), but heavier; variate-wise attention may miss fine-grained temporal interactions
  - Higher τ → better long-horizon at potential cost to short-horizon fidelity

- **Failure signatures:**
  - Long-horizon MAE growing despite training → horizon weights too weak or student capacity insufficient
  - L_FTA not decreasing → time embedding E_t not learning; check gradient flow through W_s and W_out
  - Student outperforms teacher on short horizons but catastrophically fails on long → overfitting to easy tasks; verify weighting is applied

- **First 3 experiments:**
  1. **Baseline KD:** Distill with uniform horizon weights and no FTA (only prediction matching). Establishes upper bound of degradation from removing paper's contributions.
  2. **Ablation on τ:** Sweep τ ∈ {0.5, 1.0, 2.0, 4.0} on ETTh1. Plot MSE vs. horizon to verify weighting shifts error distribution toward long-term balance.
  3. **Teacher comparison:** Use TimeMoE-50M, Moirai-Small, and Chronos-Small as teachers with identical student (DLinear). Compare final MSE to validate teacher architecture impact claim.

## Open Questions the Paper Calls Out
None

## Limitations
- The horizon weighting mechanism assumes persistent imbalance between short- and long-term forecasting difficulty, which may not generalize to all domains
- FTA module's effectiveness depends critically on student preserving temporal structure in variate-wise embeddings with limited empirical validation
- Teacher architecture vs. scale claim rests on small ablation study and single citation about scaling laws

## Confidence
- **High confidence:** The distillation framework architecture and implementation details are well-specified and reproducible
- **Medium confidence:** Effectiveness of horizon weighting is supported by strong theoretical reasoning but lacks direct ablation
- **Medium confidence:** FTA module's mathematical formulation is sound but empirical validation of necessity is limited
- **Low confidence:** Teacher architecture vs. scale claim rests on small ablation and insufficient evidence

## Next Checks
1. **Ablation on weighting importance:** Train with uniform horizon weights and compare long-horizon performance degradation. Plot learning curves showing gradient magnitudes across horizons to verify the "seesaw effect" manifests in practice.
2. **FTA module robustness:** Replace FTA with direct variate-wise MSE loss on student embeddings. Measure performance difference and conduct ablation on the time embedding dimension `u` to find minimum sufficient size for temporal recovery.
3. **Teacher architecture breadth:** Expand teacher ablation to include more architectural variants (e.g., Chronos-Quad, additional TimeMoE scales) and conduct head-to-head comparisons on standardized subset of benchmarks to quantify architectural vs. scale contributions.