---
ver: rpa2
title: 'Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs'
arxiv_id: '2502.12982'
source_url: https://arxiv.org/abs/2502.12982
tags:
- sailor2
- language
- qwen2
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sailor2 is a family of open-source multilingual language models
  optimized for Southeast Asian (SEA) languages, available in 1B, 8B, and 20B sizes.
  It achieves GPT-4o-level performance across 13 SEA languages by leveraging model
  expansion, two-stage continual pre-training on 400B SEA-specific tokens, and balanced
  instruction tuning on 4.8M examples.
---

# Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs

## Quick Facts
- **arXiv ID**: 2502.12982
- **Source URL**: https://arxiv.org/abs/2502.12982
- **Reference count**: 40
- **Primary result**: 20B model achieves 50-50 win rate against GPT-4o on SEA-WildBench while maintaining strong English/Chinese performance

## Executive Summary
Sailor2 is a family of open-source multilingual language models optimized for Southeast Asian languages, available in 1B, 8B, and 20B sizes. It achieves GPT-4o-level performance across 13 SEA languages by leveraging model expansion, two-stage continual pre-training on 400B SEA-specific tokens, and balanced instruction tuning on 4.8M examples. The 20B model attains a 50-50 win rate against GPT-4o on SEA-WildBench, while maintaining strong performance in English and Chinese. Additional techniques include long-context training (128K), speculative decoding for inference speedup, and model pruning. The project also releases a comprehensive cookbook and evaluation tools to support future multilingual LLM development.

## Method Summary
Sailor2 builds on Qwen2.5 models (0.5B, 7B, 14B) by expanding them to 1B, 8B, and 20B through block expansion before two-stage continual pre-training. The process uses 500B tokens total—400B SEA-specific from CommonCrawl plus 100B replay tokens. Stage 1 CPT uses balanced mixture with learning rate 1e-4, while Stage 2 focuses on high-quality data with learning rate 1e-5. This is followed by two-stage SFT and preference tuning using Length-Regularized DPO. The approach includes comprehensive data curation with SailCraft filtering and RegMix mixture optimization, plus long-context training with AnchorAttention.

## Key Results
- 20B model achieves 50-50 win rate against GPT-4o on SEA-WildBench
- Maintains strong English and Chinese performance while improving SEA languages
- Demonstrates effective catastrophic forgetting mitigation through model expansion
- Achieves 2.5x speculative decoding speedup on Burmese and 2.2x on English

## Why This Works (Mechanism)

### Mechanism 1: Model Expansion Before Continual Pre-training
Expanding model capacity before continual pre-training reduces catastrophic forgetting of source languages while enabling acquisition of target languages. Block expansion adds new transformer layers that store additional multilingual knowledge without overwriting existing representations in original layers.

### Mechanism 2: Two-Stage Pre-training with Quality-Weighted Data Progression
Separating pre-training into high-volume balanced training followed by high-quality refinement improves both convergence and final performance on low-resource languages. Stage 1 uses higher learning rate (1e-4) with comprehensive data mixture to establish broad multilingual foundation, while Stage 2 reduces learning rate (1e-5) and shifts to curated high-quality tokens.

### Mechanism 3: Reward-Perplexity Joint Selection for Instruction Data
Selecting instruction data with both high reward scores and high perplexity identifies valuable undertrained examples that improve instruction-following without redundancy. High reward indicates response quality; high perplexity indicates the model has not yet mastered that response pattern.

## Foundational Learning

**Concept: Continual Pre-training and Catastrophic Forgetting**
- Why needed: Sailor2 builds on Qwen2.5 rather than training from scratch, requiring understanding of how neural networks forget previously learned distributions
- Quick check: Can you explain why adding replay tokens to a SEA corpus might not be sufficient to prevent forgetting, and why model expansion helps?

**Concept: Tokenization and Vocabulary Coverage in Multilingual Models**
- Why needed: The paper notes vocabulary optimization challenges with large multilingual vocabularies
- Quick check: Why might Burmese achieve higher speculative decoding speedup than other languages?

**Concept: Preference Tuning (DPO) and Length Exploitation**
- Why needed: Sailor2 uses Length-Regularized DPO specifically to address verbosity
- Quick check: In Equation 2, what happens to the gradient when the preferred response is longer than the rejected response?

## Architecture Onboarding

**Component map**: Base Models (Qwen2.5) -> Expansion Blocks -> Training Pipeline (Stage 1 CPT -> Stage 2 CPT -> Stage 1 SFT -> Stage 2 SFT -> Off-policy DPO -> On-policy DPO) -> Data Infrastructure (SailCraft, RegMix, SEA-UltraChat/Feedback) -> Customization Modules (AnchorAttention, GliDe, Sheared LLaMA pruning)

**Critical path**: 1) Data curation: Web crawl → 6-layer filtering → RegMix optimization; 2) Model expansion: Qwen2.5 weights + new block initialization; 3) Stage 1 CPT: 450B tokens, balanced mixture, LR=1e-4; 4) Stage 2 CPT: 60B tokens, high-quality focus, LR=1e-5; 5) SFT: Two-stage instruction tuning with reward-PPL selection; 6) DPO: Off-policy then on-policy with language consistency verification

**Design tradeoffs**: 1) Expansion ratio: 1B←0.5B (2x), 8B←7B (1.14x), 20B←14B (1.43x)—larger expansion for smallest model; 2) Replay ratio: 100B replay / 500B total = 20%—balances forgetting prevention vs. SEA focus; 3) Quality vs. quantity in Stage 2: 80% high-quality SEA vs. 20% Stage 1 replay

**Failure signatures**: 1) Excessive emoji in responses → detected via probe data, addressed in DPO iterations; 2) Language inconsistency → mitigated with FastText language verifier; 3) English/Chinese degradation → indicates insufficient replay or expansion; 4) Low-resource language collapse → may indicate Stage 2 quality data insufficient

**First 3 experiments**: 1) Baseline comparison: Train Qwen2.5-7B with Sailor2 post-training pipeline (no CPT, no expansion) and compare SEA-WildBench scores to isolate CPT contribution; 2) Ablation on expansion: Train Sailor2-8B-equivalent without block expansion and measure English/Chinese perplexity degradation vs. SEA improvements; 3) Data quality validation: Sample 100 documents from each SEA language after each of the 6 SailCraft filtering layers and manually verify noise reduction

## Open Questions the Paper Calls Out

**Open Question 1**: How can continual pre-training be optimized for multilingual adaptation without incurring the computational costs associated with model expansion?
- Basis: Section 9.3 notes model expansion remains inefficient and proposes exploring "model plasticity" or "lottery ticket hypothesis" as alternatives
- Why unresolved: Over-trained base models have limited capacity for new knowledge, and simply increasing model size via expansion is computationally expensive
- What evidence would resolve it: A parameter-efficient fine-tuning method achieving comparable SEA performance without increasing parameter count

**Open Question 2**: Can tokenizer-free architectures effectively replace subword tokenizers for low-resource SEA languages with complex scripts?
- Basis: Section 9.2 identifies "Tokenizer-Free Model for Open-Vocabulary Learning" as a key future direction
- Why unresolved: Sailor2 relies on Qwen2.5 tokenizer, which may be suboptimal for certain SEA scripts or mixed-language texts
- What evidence would resolve it: Benchmark results showing tokenizer-free model outperforming Sailor2 on script-mixed or morphologically complex SEA tasks

**Open Question 3**: To what extent does the lack of native Reward Models for SEA languages limit the effectiveness of preference tuning?
- Basis: Section 8.3 acknowledges use of English-centric reward model (Skywork) due to absence of high-quality SEA RMs
- Why unresolved: Using English RMs for non-English languages introduces noise that current heuristics cannot fully solve
- What evidence would resolve it: Ablation study comparing preference tuning using current English RM versus custom-trained multilingual SEA RM

## Limitations

**Evaluation representativeness**: The 50-50 win rate against GPT-4o on SEA-WildBench may not generalize to real-world Southeast Asian language use due to benchmark construction limitations.

**Model expansion verification**: Limited ablation evidence makes it unclear whether the 50-50 win rate specifically requires expansion versus other factors like larger model size or better data quality.

**Long-context generalization**: While 128K context is enabled, evaluation only covers instruction following and preference data, not complex reasoning or document analysis tasks.

## Confidence

**High confidence**: Two-stage pre-training with quality progression improves SEA language performance; Reward-Perplexity joint selection identifies valuable instruction data; Model expansion reduces English/Chinese forgetting compared to no-expansion baseline.

**Medium confidence**: 50-50 win rate represents state-of-the-art SEA performance; Block expansion is necessary for SEA language acquisition; Long-context training benefits SEA language understanding.

**Low confidence**: Speculative decoding speedup benefits apply uniformly across all SEA languages; Pruning maintains SEA language capabilities at 50% sparsity; RegMix mixture optimization significantly improves SEA performance.

## Next Checks

1. **Ablation study on model expansion**: Train baseline Sailor2-8B without block expansion and compare SEA-WildBench performance, English/Chinese perplexity, and computational efficiency to isolate expansion contribution.

2. **Real-world deployment testing**: Deploy Sailor2 models in actual Southeast Asian language applications and collect user feedback on quality, relevance, and cultural appropriateness to validate benchmark performance against practical utility.

3. **Long-context capability assessment**: Evaluate 128K context performance on document summarization, multi-document QA, and long-form conversation tasks in SEA languages, including comparison with baseline models to quantify AnchorAttention contribution.