---
ver: rpa2
title: 'BreakFun: Jailbreaking LLMs via Schema Exploitation'
arxiv_id: '2510.17904'
source_url: https://arxiv.org/abs/2510.17904
tags:
- schema
- breakfun
- prompt
- harmful
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BreakFun, a jailbreak attack exploiting\
  \ the tension between LLMs\u2019 strong instruction-following and safety alignment.\
  \ BreakFun uses a three-part prompt combining an innocent framing, a Chain-of-Thought\
  \ distraction, and a carefully crafted \u201CTrojan Schema\u201D to compel harmful\
  \ content generation."
---

# BreakFun: Jailbreaking LLMs via Schema Exploitation

## Quick Facts
- **arXiv ID:** 2510.17904
- **Source URL:** https://arxiv.org/abs/2510.17904
- **Reference count:** 28
- **Primary result:** BreakFun achieves 89% average attack success rate, up to 100% on several models, via schema exploitation.

## Executive Summary
This paper introduces BreakFun, a jailbreak attack that exploits the tension between LLMs’ strong instruction-following and safety alignment. BreakFun uses a three-part prompt combining an innocent framing, a Chain-of-Thought distraction, and a carefully crafted "Trojan Schema" to compel harmful content generation. Evaluated on 13 diverse LLMs (foundational and API-hardened), BreakFun achieves an average 89% attack success rate, reaching 100% on several models. An ablation study confirms the Trojan Schema is the attack’s key causal factor. To defend against this, the authors propose an Adversarial Prompt Deconstruction guardrail, which extracts semantic content from structured prompts and achieves high detection rates across three model families. The findings highlight a fundamental structural vulnerability in LLMs and suggest defenses targeting schema deception are promising.

## Method Summary
BreakFun is a jailbreak attack that leverages the tension between LLMs’ instruction-following and safety alignment. It uses a three-part prompt: an innocent framing to establish context, a Chain-of-Thought distraction to misdirect attention, and a Trojan Schema to covertly embed harmful instructions. This structure exploits LLMs’ strong schema-following behavior to generate harmful content. The attack was evaluated on 13 diverse LLMs, achieving an average 89% success rate, with up to 100% on several models. An ablation study isolated the Trojan Schema as the key causal factor. To defend against this, the authors propose an Adversarial Prompt Deconstruction guardrail, which extracts semantic content from structured prompts to detect and block malicious inputs.

## Key Results
- BreakFun achieves an average 89% attack success rate across 13 evaluated LLMs.
- Success rate reaches 100% on several models, including both foundational and API-hardened variants.
- The Trojan Schema component is confirmed as the primary causal factor via ablation study.
- The proposed Adversarial Prompt Deconstruction defense achieves high detection rates across three model families.

## Why This Works (Mechanism)
The attack works by exploiting the tension between LLMs’ strong instruction-following and safety alignment. By embedding harmful instructions within an innocuous framing and distracting the model with a Chain-of-Thought, BreakFun tricks the model into complying with unsafe requests. The Trojan Schema is the critical component, as it covertly structures the harmful content in a way that bypasses safety filters while remaining compliant with the model’s schema-following tendencies.

## Foundational Learning
- **Instruction-following vs. safety alignment:** LLMs are trained to follow instructions precisely, but also to refuse harmful requests. BreakFun exploits the conflict between these objectives.
  - *Why needed:* Understanding this tension is essential to grasp how the attack bypasses safety measures.
  - *Quick check:* Review how models handle conflicting instructions and safety protocols.

- **Schema exploitation:** The attack leverages the model’s strong tendency to follow structured input formats (schemas) to embed harmful content.
  - *Why needed:* Schemas are a core part of how LLMs process and respond to prompts.
  - *Quick check:* Test how models respond to prompts with and without structured schemas.

- **Chain-of-Thought distraction:** By adding a reasoning step, the attack misdirects the model’s attention away from the harmful payload.
  - *Why needed:* Distraction techniques can weaken the model’s safety checks.
  - *Quick check:* Evaluate model responses with and without CoT distractions in harmful prompts.

## Architecture Onboarding

**Component Map:**
BreakFun Prompt -> Trojan Schema (core payload) -> Chain-of-Thought distraction -> Innocent framing

**Critical Path:**
1. Construct BreakFun prompt with all three components.
2. Embed harmful content within Trojan Schema.
3. Distract with Chain-of-Thought.
4. Present innocent framing to establish context.

**Design Tradeoffs:**
- **Framing vs. payload:** Innocent framing increases plausibility but may dilute the harmful payload.
- **Distraction strength:** Stronger CoT may improve success but risk detection.
- **Schema subtlety:** More subtle schemas are less detectable but may be less effective.

**Failure Signatures:**
- Model refuses prompt due to safety filters.
- Harmful content is not generated or is incomplete.
- Model flags the prompt as suspicious or adversarial.

**First Experiments:**
1. Test BreakFun on a single model with varying levels of Trojan Schema subtlety.
2. Evaluate the effect of removing the Chain-of-Thought distraction.
3. Measure detection rates of the Adversarial Prompt Deconstruction guardrail on benign vs. malicious prompts.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond those implied by its limitations.

## Limitations
- The attack and defense were tested on a limited set of 13 models and curated harmful queries, raising questions about generalization.
- The defense mechanism’s performance under real-world, noisy, or adversarial prompt variations is not fully characterized.
- The study does not address adaptive attacker-defender dynamics or iterative attacks.

## Confidence
- **High:** The reported attack success rates and the efficacy of the Trojan Schema component are well-supported by the ablation study and consistent across multiple model families.
- **Medium:** The generalizability of the attack and defense to broader contexts, including different model architectures, query types, and real-world adversarial conditions, remains uncertain due to the limited scope of evaluation.
- **Low:** The robustness of the proposed detection mechanism against adaptive or more sophisticated evasion strategies is not established.

## Next Checks
1. Test BreakFun and the Adversarial Prompt Deconstruction defense across a wider variety of harmful and benign prompts, including those generated by different attackers or in other languages.
2. Evaluate the attack and defense under adaptive, iterative conditions where the attacker responds to the defender’s modifications, simulating realistic security arms races.
3. Assess the performance of the defense on noisy, partially corrupted, or otherwise non-ideal prompt inputs to determine robustness in real-world deployments.