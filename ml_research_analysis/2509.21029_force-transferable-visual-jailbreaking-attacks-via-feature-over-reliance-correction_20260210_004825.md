---
ver: rpa2
title: 'FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance
  CorrEction'
arxiv_id: '2509.21029'
source_url: https://arxiv.org/abs/2509.21029
tags:
- attacks
- visual
- jailbreaking
- loss
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited transferability of optimisation-based
  visual jailbreaking attacks, which exploit vulnerabilities in multimodal large language
  models (MLLMs) but fail to generalise across models. The authors analyse the loss
  landscape and feature representations of these attacks, identifying non-generalizable
  reliance on model-specific early-layer features and semantically poor high-frequency
  components.
---

# FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction

## Quick Facts
- arXiv ID: 2509.21029
- Source URL: https://arxiv.org/abs/2509.21029
- Authors: Runqi Lin; Alasdair Paren; Suqin Yuan; Muyang Li; Philip Torr; Adel Bibi; Tongliang Liu
- Reference count: 40
- Primary result: Improves transferability of visual jailbreaking attacks across MLLM architectures, achieving 13% ASR gains on adapter-based models, near-100% on early-fusion models, and 70% on commercial APIs

## Executive Summary
This paper addresses the fundamental limitation of optimisation-based visual jailbreaking attacks: their poor transferability across different multimodal large language models (MLLMs). Through systematic analysis of loss landscapes and feature representations, the authors identify that attacks exhibit over-reliance on model-specific early-layer features and semantically poor high-frequency components. They propose FORCE (Feature Over-Reliance CorrEction), a method that corrects these issues through layer-aware regularisation and spectral rescaling, substantially improving attack transferability while reducing query costs. The method achieves significant performance improvements across adapter-based, early-fusion, and commercial MLLMs.

## Method Summary
FORCE is an optimisation-based method that enhances visual jailbreaking attack transferability by correcting feature over-reliance. It combines two key components: layer-aware regularisation that explores broader feasible regions in layer space through N reference samples per iteration, and spectral rescaling that reduces the influence of semantically weak high-frequency features via FFT-based frequency band analysis. The method integrates with standard PGD optimisation by adding these regularisation terms to the cross-entropy loss, with hyperparameters λ for layer regularisation strength and M frequency bands. The approach uses a decaying weight schedule and operates within perturbation budget constraints to generate attacks that generalise better across different MLLM architectures.

## Key Results
- Achieves 13% average success rate improvement on adapter-based MLLMs
- Delivers nearly 100% success rate gains on early-fusion MLLMs
- Improves commercial model attack success by 70% while reducing query costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimisation-based visual jailbreaking attacks reside in high-sharpness loss landscape regions, causing extreme sensitivity to minor parameter shifts during cross-model transfer.
- Mechanism: Standard PGD optimisation finds narrow local optima where the attack achieves near-zero loss on the source model, but small perturbations (as little as 0.03 pixel changes or 0.0002 weight perturbations) cause sharp loss increases that invalidate the attack.
- Core assumption: Sharpness correlates with model-specific feature reliance rather than transferable semantic features.
- Evidence anchors:
  - [abstract]: "generated attacks tend to reside in high-sharpness regions, whose effectiveness is highly sensitive to even minor parameter changes during transfer"
  - [Section 3.1, Figure 2]: Shows input and weight perturbation loss landscapes demonstrating that attacks rapidly fail with minimal perturbation
  - [corpus]: Weak direct corpus support for this specific sharpness-transferability link in jailbreaking context

### Mechanism 2
- Claim: Visual jailbreaking attacks exhibit improper reliance on early-layer features that are model-specific rather than semantically transferable.
- Mechanism: Feature interpolation experiments show early layers (e.g., 1st, 6th, 11th) have narrow feasible regions where <30% natural feature interpolation causes attack failure, whereas later layers (e.g., 31st) tolerate >40% interpolation while maintaining effectiveness.
- Core assumption: Early-layer narrow feasible regions indicate model-specific feature exploitation; broader later-layer regions indicate semantic feature reliance.
- Evidence anchors:
  - [abstract]: "revealing an improper reliance on narrow layer representations"
  - [Section 3.2, Figure 3]: Demonstrates progressively narrower feasible regions toward shallower layers across interpolation experiments
  - [corpus]: "Multimodal Language Models See Better When They Look Shallower" suggests layer-specific feature differences exist in MLLMs generally

### Mechanism 3
- Claim: Optimisation causes progressive over-reliance on high-frequency spectral components that lack semantic content, reducing cross-model transferability.
- Mechanism: Frequency band masking experiments show that at iteration 250, low-frequency removal causes attack failure while high-frequency removal has minimal impact. By iteration 750, removing the third-highest frequency band alone causes failure, indicating shifted reliance toward semantically-poor features.
- Core assumption: High-frequency features are model-specific artefacts of optimisation; low-frequency features encode transferable semantic content.
- Evidence anchors:
  - [abstract]: "semantically poor frequency components" and "rescales the influence of frequency features according to their semantic content"
  - [Section 3.3, Figure 4]: Shows progressive shift in frequency band influence across optimisation iterations
  - [corpus]: NAP-Tuning paper discusses adversarial robustness in VLMs but does not directly address spectral-frequency relationships in jailbreaking

## Foundational Learning

- Concept: **Loss Landscape Sharpness and Flatness**
  - Why needed here: Understanding why FORCE targets "flattened feasible regions" requires grasping how sharp vs. flat minima affect generalisation and robustness.
  - Quick check question: Why does a flat minimum in the loss landscape typically generalise better than a sharp minimum?

- Concept: **Fourier Transform and Frequency Domain Analysis**
  - Why needed here: FORCE applies FFT to decompose perturbations into frequency bands and rescales them based on semantic relevance.
  - Quick check question: What type of image information is typically encoded in low-frequency vs. high-frequency spectral components?

- Concept: **Feature Interpolation in Neural Networks**
  - Why needed here: The paper's diagnostic methodology relies on convex combinations of features to measure feasible region breadth.
  - Quick check question: If interpolating between an adversarial and natural feature representation causes rapid loss increase, what does this indicate about the feature space?

## Architecture Onboarding

- Component map: Source MLLM → [Standard PGD Loop] → [Spectral Rescaling Module] → [Layer-Aware Regularisation] → [Combined Loss: ℓ_ce + ℓ_reg] → [Perturbation Update: δ = δ - α·sign(∇)] → Output: x_img + δ

- Critical path:
  1. Initialise perturbation δ uniformly within budget ε
  2. For each iteration: apply spectral rescaling (Algorithm 3) → compute layer-aware regularisation (Algorithm 2) → combine with cross-entropy loss → update δ via signed gradient
  3. Clip perturbation to budget constraints
  4. Continue until attack succeeds on source model

- Design tradeoffs:
  - **Regularisation strength (λ)**: Higher values enforce broader feasible regions but may slow convergence on source model
  - **Number of reference samples (N)**: More samples improve feasible region estimation but increase computation (paper uses N=10)
  - **Number of frequency bands (M)**: More bands enable finer spectral control but require more forward passes (paper uses M=10)
  - **Noise neighbourhood (η)**: Larger neighbourhood explores broader regions but risks leaving feasible zone entirely (paper uses η=8/255)

- Failure signatures:
  - **Source model failure**: If λ too high or spectral rescaling too aggressive, attack may not achieve target output on source model
  - **No transfer improvement**: If early-layer features are not the primary transferability bottleneck, layer-aware regularisation will show limited effect
  - **Query efficiency degradation**: If N is too low, reference samples may not adequately characterise feasible region geometry

- First 3 experiments:
  1. **Ablation on components**: Run FORCE with layer-regularisation only vs. spectral-rescaling only vs. both (replicate Table 3) to validate each mechanism independently
  2. **Layer-wise analysis**: Replicate Figure 5 interpolation experiments on FORCE-generated attacks to verify broader early-layer feasible regions
  3. **Cross-architecture transfer**: Test on a single source model (e.g., LLaVA-v1.5-7B) transferring to at least one adapter-based, one early-fusion, and one commercial target to confirm transferability improvements hold across architectural paradigms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transferable visual jailbreaking achieve high absolute success rates on commercial MLLMs?
- Basis: [inferred] Table 1 shows FORCE improves relative ASR significantly, but absolute success rates on commercial models (GPT-5, Claude) remain low (1–10%).
- Why unresolved: Current methods may still rely on low-level artifacts that sophisticated commercial safety filters or unseen alignment techniques effectively detect and block.
- What evidence would resolve it: Identifying cross-model semantic features that reliably bypass commercial API guardrails without detection.

### Open Question 2
- Question: Does the architectural gap between early-fusion and adapter-based MLLMs impose a fundamental limit on transferability?
- Basis: [inferred] Table 1 shows FORCE yields significantly lower ASR on early-fusion models (e.g., Llama-3.2) compared to adapter-based models.
- Why unresolved: Unified tokenizers in early-fusion architectures might process adversarial perturbations differently than separate visual encoders and adapters.
- What evidence would resolve it: Comparative analysis of feature representations in unified embedding spaces versus projected adapter spaces.

### Open Question 3
- Question: Can the identified feature over-reliance mechanism be utilized for adversarial defense?
- Basis: [inferred] The paper analyzes high-sharpness loss landscapes to improve attacks, but does not explore if correcting this reliance improves robustness.
- Why unresolved: It is unclear if enforcing flatter loss landscapes during training prevents attacks or merely shifts the optimization trajectory for adversaries.
- What evidence would resolve it: Training MLLMs with spectral or layer-wise constraints and testing robustness against standard attacks.

## Limitations

- The method relies on controlled benchmark settings and requires validation under realistic adversarial conditions with active defences
- The assumption that early-layer features are less semantically meaningful may not generalise across all MLLM architectures or training paradigms
- Both FORCE components introduce computational overhead that could impact real-time applicability

## Confidence

- **High Confidence**: The identification of high-sharpness loss regions as transferability barriers, supported by perturbation experiments showing attack fragility under minimal parameter changes
- **Medium Confidence**: The layer-aware regularisation mechanism, as the semantic meaningfulness of early-layer features across MLLM architectures requires broader validation
- **Medium Confidence**: The spectral rescaling approach, since the assumption that high-frequency features lack semantic content may not hold universally across model architectures

## Next Checks

1. **Adversarial Robustness Validation**: Test FORCE attacks against a commercial MLLM with active defence mechanisms (e.g., input preprocessing, response filtering) to verify transferability improvements persist under realistic adversarial conditions

2. **Architectural Generalisation Test**: Apply FORCE to a non-LLaVA architecture (e.g., Qwen-VL, BLIP-2) and evaluate whether the layer-aware regularisation and spectral rescaling components provide similar transferability gains across different architectural paradigms

3. **Frequency Band Semantic Analysis**: Conduct ablation studies systematically removing specific frequency bands from FORCE-generated attacks to verify that low-frequency bands indeed encode more semantically relevant features than high-frequency bands across multiple target models