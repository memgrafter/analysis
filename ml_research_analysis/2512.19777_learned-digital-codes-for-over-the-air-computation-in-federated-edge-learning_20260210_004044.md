---
ver: rpa2
title: Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning
arxiv_id: '2512.19777'
source_url: https://arxiv.org/abs/2512.19777
tags:
- devices
- learning
- digital
- codebook
- quantisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a learned digital over-the-air computation
  framework for federated edge learning that improves robustness and performance in
  low signal-to-noise ratio (SNR) regimes. The proposed AMP-DA-Net decoder integrates
  an unrolled approximate message passing (AMP)-style architecture with joint learning
  of the unsourced random access (URA) codebook and vector quantisation, incorporating
  structured priors, curvature-aware quantisation, and error feedback.
---

# Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning

## Quick Facts
- arXiv ID: 2512.19777
- Source URL: https://arxiv.org/abs/2512.19777
- Reference count: 40
- Primary result: Extends reliable digital OTA operation by >10 dB into low-SNR regimes while maintaining accuracy across full SNR range

## Executive Summary
This paper introduces AMP-DA-Net, a learned digital over-the-air computation framework for federated edge learning that significantly improves robustness in low signal-to-noise ratio regimes. The framework combines a learned unsourced random access (URA) codebook with an unrolled approximate message passing (AMP)-style decoder, incorporating structured priors, curvature-aware quantization, and error feedback. Experiments demonstrate that the method maintains or improves accuracy across the full SNR range while extending reliable operation by more than 10 dB into low-SNR regimes compared to analytical AMP-based schemes.

## Method Summary
The framework operates by having devices split their model updates into fragments, quantize them using a shared codebook, and encode them into URA codewords for transmission. The base station receives the noisy superposition and processes it through AMP-DA-Net, an unrolled decoder with L layers that performs measurement-domain updates, Bayesian denoising under spike-and-slab priors, and CNN-based refinement. Error feedback accumulates residuals across rounds to improve convergence. The decoder jointly learns the URA codebook structure and quantization codebook during pre-training on local training statistics from the base station, enabling adaptation to the specific distribution of model updates expected in deployment.

## Key Results
- Extends reliable digital OTA operation by >10 dB into low-SNR regimes (convergence at SNR=3-5 dB vs. failure for analytical AMP-DA)
- Maintains or improves accuracy across full SNR range (0-20 dB) while reducing mean absolute error in activity estimation
- Generalizes across different model architectures (VGG-11 to ResNet) with minimal accuracy degradation (~1%)
- Improves robustness to corrupted devices when using nonlinear aggregation rules like trimmed means and majority voting

## Why This Works (Mechanism)

### Mechanism 1: Learned URA Codebook with Decoder Joint Training
Jointly learning the URA codebook and AMP-DA-Net decoder enables reliable sparse recovery in regimes where analytically designed AMP methods fail. The two-matrix parameterization C_syn = DW improves gradient flow and avoids dead codewords, while end-to-end training allows the codebook to adapt its structure to the learned recovery algorithm. Training on base station local statistics provides a proxy for device update distributions.

### Mechanism 2: Structured Spike-and-Slab Prior with CNN Refinement
Combining a spike-and-slab prior with a learned CNN denoiser improves posterior mean estimation for sparse activity vectors. The spike-and-slab model captures the typical sparsity pattern (most codewords unused, active ones approximately Poisson-distributed), while the CNN refines the Bayesian estimate and a learnable gate blends the two approaches.

### Mechanism 3: Popularity-Ordered Quantisation Codebook
Ordering the quantisation codebook by codeword popularity standardizes expected codeword usage across rounds and improves decoding by providing consistent input distribution to the URA codebook. The base station's local training statistics provide a reasonable proxy for device update distributions despite non-IID data.

## Foundational Learning

- Concept: Approximate Message Passing (AMP) for Compressed Sensing
  - Why needed here: The decoder architecture builds on AMP/GAMP principles—residual updates, Onsager correction, scalar denoising. Understanding why AMP assumes IID Gaussian residuals and how this fails in practice is essential to appreciate the learned corrections.
  - Quick check question: Can you explain why the Onsager correction term keeps effective noise approximately Gaussian in classical AMP?

- Concept: Federated Edge Learning (FEEL) Communication Bottleneck
  - Why needed here: The entire motivation stems from repeated uplink model updates creating a bandwidth bottleneck. Understanding FedAvg aggregation and why OTA merges communication with computation contextualizes the design goals.
  - Quick check question: In FedAvg, what is the relationship between local gradient steps and the global model update rule?

- Concept: Unsourced Random Access (URA)
  - Why needed here: Devices share a common codebook without device identifiers; the server only recovers aggregate statistics. This enables scalability with massive device populations but requires sparse recovery techniques.
  - Quick check question: In URA, why can the server decode the received signal without knowing which devices transmitted?

## Architecture Onboarding

- Component map:
  - Device-side encoder: Error feedback accumulator → fragment splitter → vector quantisation → URA encoding → transmission
  - Channel: AWGN with variance σ²
  - Server-side decoder (AMP-DA-Net): L unrolled layers, each with output block (measurement-domain update with learnable γ^(ℓ), η^(ℓ)) → input block (pseudo-channel, spike-and-slab Bayesian denoiser, CNN refinement with gate ζ^(ℓ)) → EM updates for λ, α, σ² → post-processing (non-negative clipping, greedy rounding to sum constraint)
  - Aggregation: Recovered count vectors → quantisation codebook lookup → symmetric aggregation function g(·)

- Critical path: Local update → error feedback → quantisation → URA encoding → channel → AMP-DA-Net (L=10 layers) → activity vector estimate → dequantization → aggregation → global model update

- Design tradeoffs:
  - **Uplink overhead vs. recovery accuracy**: l=64 channel uses per fragment, n=128 codewords, d=20 fragment dimension. Increasing l improves noise robustness but raises latency.
  - **Decoder depth vs. latency**: L=10 layers provides consistent per-round latency; fewer layers reduce computation but may degrade low-SNR performance.
  - **Error feedback vs. stability**: Error feedback accumulates residuals across rounds, improving convergence under clean conditions but can destabilize under corruption or severe distribution mismatch.
  - **Pre-training dataset diversity vs. specialization**: Single-architecture training data showed strong generalization, but extreme distribution shifts may require broader pre-training.

- Failure signatures:
  - **Low-SNR divergence (AMP-DA baseline)**: Global model fails to converge; activity estimation MAE exceeds ~0.5 causing scaling instability.
  - **Quantisation codebook mismatch**: If device residuals exceed ~5× current update magnitude (error feedback accumulation without reset), codebook representation degrades.
  - **Corrupted device instability with error feedback**: Error feedback causes device messages to drift in divergent directions; disable error feedback when >20% devices are corrupted.

- First 3 experiments:
  1. **Baseline comparison across SNR range**: Replicate Table I by comparing AMP-DA-Net vs. AMP-DA (MD-AirComp) at SNR ∈ {0, 3, 5, 10, 15, 20} dB on CIFAR-10 with K_a ∈ [7,13] active devices. Confirm >10 dB SNR extension claim.
  2. **Ablation on codebook learning and ordering**: Using Table II setup at SNR=5 dB, compare: (a) learned+popularity, (b) learned+no ordering, (c) fixed+no ordering. Validate that popularity ordering contributes ~0.07 accuracy gain.
  3. **Generalization across model architectures**: Train AMP-DA-Net on ResNet update data; evaluate on VGG-style network at SNR=10 dB (per Fig. 7). Verify convergence matches within ~1% final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to effectively handle fading and multi-antenna channels?
- Basis in paper: The conclusion states, "Future work can extend the framework to fading and multi-antenna channels."
- Why unresolved: The current system model assumes a single-antenna Base Station (BS) and Additive White Gaussian Noise (AWGN) channels, ignoring the complexities of channel state information estimation and fading.
- What evidence would resolve it: A modified AMP-DA-Net architecture that integrates channel estimation and maintains convergence stability under fading conditions.

### Open Question 2
- Question: How can the stability-performance trade-off of error feedback be optimized in the presence of device corruption?
- Basis in paper: Section V-C notes that "Tuning this stability-performance trade-off... is a promising direction for future work."
- Why unresolved: Error feedback accumulates residuals that can cause device messages to drift, making the BS-constructed codebook a poor representation of device updates during corruption events.
- What evidence would resolve it: A mechanism for round-wise adaptivity or modified quantisation that prevents codebook mismatch without sacrificing the optimisation benefits of error feedback.

### Open Question 3
- Question: Can an explicit equality constraint on active device estimates (K̂_a) across fragments improve decoding accuracy?
- Basis in paper: Section III-E states that enforcing this constraint "proved challenging to implement and so is left for future work."
- Why unresolved: The true number of active devices is shared across fragments, but current implementation updates estimates independently to avoid high memory usage from cross-fragment coupling.
- What evidence would resolve it: An implementation of the unrolled decoder that shares activity estimates across fragments, resulting in lower variance for the global activity estimate.

## Limitations

- Framework performance depends heavily on pre-training data distribution matching deployment conditions; extreme distribution shifts may degrade performance
- Spike-and-slab