---
ver: rpa2
title: 'Human-AI Complementarity: A Goal for Amplified Oversight'
arxiv_id: '2510.26518'
source_url: https://arxiv.org/abs/2510.26518
tags:
- human
- accuracy
- ratings
- raters
- assistance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how to combine human and AI ratings to improve
  fact-verification accuracy, a key challenge for AI oversight. The authors use an
  AI model with a search engine to assess factuality, achieving 87.7% accuracy versus
  75.1% for unassisted humans.
---

# Human-AI Complementarity: A Goal for Amplified Oversight
## Quick Facts
- arXiv ID: 2510.26518
- Source URL: https://arxiv.org/abs/2510.26518
- Reference count: 40
- Human-AI hybrid accuracy: 89.3% vs 87.7% AI alone vs 75.1% humans alone

## Executive Summary
This work investigates how to effectively combine human and AI ratings for fact verification to improve overall accuracy. The authors compare unassisted humans, AI models with search engines, and various human-AI hybrid approaches. They find that a confidence-based hybridization strategy—using AI ratings when AI confidence is high and human ratings when low—achieves 89.3% accuracy, outperforming both AI alone (87.7%) and unassisted humans (75.1%). The study also reveals that showing humans AI explanations and confidence scores can lead to over-reliance and reduced accuracy, while providing search results and evidence improves accuracy to 73.3% and fosters appropriate reliance.

## Method Summary
The authors evaluate fact-verification performance using the FEVER dataset, comparing unassisted humans, an AI model (GPT-4 with search) assessing claim factuality, and hybrid approaches. The AI model achieved 87.7% accuracy versus 75.1% for unassisted humans. A confidence-based hybridization approach was implemented where AI ratings are used when AI confidence is high and human ratings when low, improving overall accuracy to 89.3%. The study also tested human-AI interaction by providing humans with varying levels of AI assistance: search results and evidence (improving accuracy to 73.3%), versus full explanations and confidence scores (reducing accuracy due to over-reliance).

## Key Results
- AI with search engine: 87.7% accuracy vs humans: 75.1% accuracy
- Confidence-based hybridization: 89.3% accuracy (outperforms both AI alone and humans alone)
- AI explanations and confidence scores cause over-reliance and reduce human accuracy
- Search results and evidence improve human accuracy to 73.3% and foster appropriate reliance

## Why This Works (Mechanism)
The confidence-based hybridization works by leveraging AI's superior performance when highly confident while preserving human judgment in uncertain cases. This approach capitalizes on the complementary strengths of humans and AI—AI excels at pattern recognition and information retrieval from large datasets, while humans bring contextual understanding and can handle ambiguous cases better. The over-reliance problem when showing full AI explanations suggests that humans struggle to appropriately calibrate their trust when presented with AI's internal reasoning, but can effectively use external evidence and search results to improve their own judgment.

## Foundational Learning
- Fact verification with search integration: Why needed - To ground AI judgments in verifiable evidence; Quick check - Compare accuracy with and without search integration
- Confidence calibration in human-AI systems: Why needed - To determine when to trust AI versus human judgment; Quick check - Analyze correlation between confidence scores and actual accuracy
- Evidence presentation design: Why needed - To understand what information helps humans make better judgments; Quick check - Compare different levels of AI assistance on human performance

## Architecture Onboarding
- Component map: Human annotators <- AI model with search -> Confidence scores -> Hybrid decision logic
- Critical path: Claim -> Search results -> AI assessment -> Confidence calculation -> (High confidence: use AI rating; Low confidence: use human rating)
- Design tradeoffs: Full AI explanations provide transparency but cause over-reliance; search results provide evidence without overwhelming; confidence scores need careful calibration
- Failure signatures: Over-reliance when humans see full explanations; under-utilization when AI confidence is miscalibrated; degraded performance when search results are poor quality
- First experiments: 1) Test hybrid approach on diverse fact-verification datasets, 2) Vary confidence score thresholds to optimize accuracy, 3) Compare different AI architectures for factuality assessment

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on single FEVER dataset may not generalize to other fact-verification tasks or domains
- Hybrid approach robustness to varying confidence score distributions and different AI architectures remains unclear
- Human-AI interaction effects tested in controlled setting; real-world deployment may yield different reliance patterns

## Confidence
- High confidence: AI outperforms unassisted humans in fact-verification accuracy (87.7% vs 75.1%)
- Medium confidence: Hybrid confidence-based approach improves accuracy (89.3%)
- Medium confidence: Full AI explanations and confidence scores cause over-reliance and reduced human accuracy

## Next Checks
1. Test the hybrid confidence-based approach on diverse fact-verification datasets to assess generalizability
2. Evaluate robustness of approach to variations in AI model architecture and confidence score calibration methods
3. Conduct longitudinal study in real-world setting to observe reliance patterns and accuracy over time