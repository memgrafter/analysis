---
ver: rpa2
title: 'Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity'
arxiv_id: '2506.12685'
source_url: https://arxiv.org/abs/2506.12685
tags:
- semantic
- flipattack
- prompt
- attack
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how semantic dissimilarity between original
  and manipulated prompts affects jailbreak success in large language models (LLMs).
  The authors analyze FlipAttack, a prompt-flipping jailbreak method, and propose
  Alphabet Index Mapping (AIM), a novel attack that transforms text into alphabet
  indices to maximize semantic dissimilarity.
---

# Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity

## Quick Facts
- **arXiv ID**: 2506.12685
- **Source URL**: https://arxiv.org/abs/2506.12685
- **Reference count**: 3
- **Primary result**: AIM achieves 94% attack success rate on AdvBench, outperforming FlipAttack (88%) through maximizing semantic dissimilarity

## Executive Summary
This paper investigates how semantic dissimilarity between original and manipulated prompts affects jailbreak success in large language models. The authors analyze FlipAttack, a prompt-flipping jailbreak method, and propose Alphabet Index Mapping (AIM), a novel attack that transforms text into alphabet indices to maximize semantic dissimilarity. Using UMAP visualizations and cosine similarity analysis on GPT-4 embeddings, they show AIM achieves 94% attack success rate (ASR) on a subset of AdvBench, outperforming FlipAttack (88%) and other methods. While high semantic dissimilarity correlates with higher ASR, the results also highlight the importance of maintaining decodability. AIM's failures were refusals (not decoding errors), suggesting safety filters can still block harmful requests even after decoding. The study contributes both a deeper understanding of prompt obfuscation mechanics and a highly effective new jailbreak technique.

## Method Summary
The paper introduces Alphabet Index Mapping (AIM), a novel jailbreak technique that converts text prompts into their corresponding alphabet indices to maximize semantic dissimilarity while maintaining decodability. AIM transforms each character in a prompt into its position in the alphabet (e.g., 'a' becomes 1, 'b' becomes 2), creating prompts that are semantically dissimilar to the original but can be decoded by LLMs. The method was evaluated against FlipAttack and other baselines using GPT-4 embeddings to measure semantic similarity via cosine distance. The authors tested these techniques on a subset of AdvBench jailbreak prompts, analyzing both attack success rates and the relationship between semantic dissimilarity and jailbreak effectiveness.

## Key Results
- AIM achieves 94% attack success rate on AdvBench subset, outperforming FlipAttack (88%) and other baselines
- High semantic dissimilarity correlates strongly with successful jailbreaks, but maintaining decodability is equally critical
- UMAP visualizations show AIM-transformed prompts occupy distinct semantic spaces from original prompts
- All AIM failures were model refusals rather than decoding errors, indicating safety filters can still block harmful requests

## Why This Works (Mechanism)
AIM exploits the tension between semantic preservation and prompt obfuscation. By converting text to alphabet indices, it maximizes the semantic distance between original and manipulated prompts while remaining decodable. This transformation creates prompts that bypass typical safety detection mechanisms that rely on semantic similarity to known harmful patterns, while still allowing the underlying harmful intent to be reconstructed by the model during processing.

## Foundational Learning
- **Semantic dissimilarity measurement**: Understanding how to quantify differences between text meanings using embedding cosine similarity is crucial for evaluating jailbreak effectiveness and comparing attack methods.
- **Prompt obfuscation techniques**: Knowledge of various text transformation methods (flipping, encoding, indexing) helps in designing attacks that evade safety filters while maintaining interpretability.
- **LLM safety alignment mechanisms**: Understanding how models detect and refuse harmful requests provides insight into which attack strategies might bypass these safeguards.

## Architecture Onboarding
- **Component map**: Input prompt -> Alphabet Index Transformation -> LLM (decoding) -> Output response
- **Critical path**: The transformation must maintain decodability while maximizing semantic dissimilarity; any failure in either dimension reduces attack effectiveness
- **Design tradeoffs**: Higher dissimilarity increases evasion potential but risks decoding failure; lower dissimilarity maintains decodability but reduces evasion effectiveness
- **Failure signatures**: Refusals indicate safety filters still work post-decoding; decoding errors suggest transformation is too aggressive
- **First experiments**: 1) Test AIM on different character sets beyond basic alphabet, 2) Vary base numbering systems (binary, hexadecimal) for index mapping, 3) Evaluate on non-English prompts to test language generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based entirely on GPT-4 as both attacker and evaluator, limiting generalizability across different model architectures
- 94% ASR comes from a subset of AdvBench prompts without clarification on representativeness
- Study focuses exclusively on text-based alphabet index transformations without exploring other obfuscation techniques

## Confidence
- **High**: AIM achieves superior performance to baseline methods on tested dataset
- **Medium**: Semantic dissimilarity is the primary driver of jailbreak success (single-model evaluation limits generalizability)

## Next Checks
1. Test AIM across multiple frontier models including Claude, Llama, and open-weight alternatives to assess cross-model robustness
2. Conduct ablation studies varying alphabet index parameters (like base numbering systems or character mappings) to identify which aspects most impact success rates
3. Evaluate whether AIM can bypass safety filters in real-world applications beyond controlled benchmarks, particularly for zero-day jailbreak attempts on production systems