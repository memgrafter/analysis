---
ver: rpa2
title: 'F-Actor: Controllable Conversational Behaviour in Full-Duplex Models'
arxiv_id: '2601.11329'
source_url: https://arxiv.org/abs/2601.11329
tags:
- speech
- audio
- system
- speaker
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces F-Actor, the first open, instruction-following
  full-duplex speech model that can be trained efficiently on academic resources.
  By freezing the audio encoder and fine-tuning only the LLM, the model requires just
  2,000 hours of data and two days on four A100-40GB GPUs.
---

# F-Actor: Controllable Conversational Behaviour in Full-Duplex Models

## Quick Facts
- arXiv ID: 2601.11329
- Source URL: https://arxiv.org/abs/2601.11329
- Authors: Maike Züfle; Ondrej Klejch; Nicholas Sanders; Jan Niehues; Alexandra Birch; Tsz Kin Lam
- Reference count: 40
- First open, instruction-following full-duplex speech model trained efficiently on academic resources

## Executive Summary
This paper introduces F-Actor, the first open, instruction-following full-duplex speech model that can be trained efficiently on academic resources. By freezing the audio encoder and fine-tuning only the LLM, the model requires just 2,000 hours of data and two days on four A100-40GB GPUs. It can follow explicit instructions to control speaker voice, conversation topic, conversational behavior (e.g., backchanneling and interruptions), and dialogue initiation. Systematic analysis of design choices shows that the best configuration uses word-level alignment, an audio delay of two tokens, and a text stream, achieving 99.8% accuracy in dialogue initiation, 54% cosine similarity for speaker embedding consistency, and strong correlations (0.54 for backchannels, 0.25 for interruptions) between prompt-specified and generated behaviors. The model also generates coherent conversations with speech quality and turn-taking similar to the training data, while being publicly released for reproducible research.

## Method Summary
F-Actor builds on Llama3.2-1B-Instruct as backbone with a frozen NanoCodec encoder and 8 linear DSU heads for speaker-specific modeling. Training uses Behavior-SD synthetic conversational data (1,610 hours after filtering) with word-level text-audio alignment, audio delay of 2 tokens, and system-only loss. The model processes speaker embeddings (from first 5s via ECAPA-TDNN), instruction prompts, and DAU embeddings through a frozen encoder and fine-tuned LLM. Single-stage training runs on 4×A100-40GB for ~48h with batch size 1/GPU, gradient accumulation 8, LR 5e-5, and early stopping. Inference uses temperature 0.9 and top-k 40 for controllable generation.

## Key Results
- 99.8% accuracy in dialogue initiation following prompts
- 54% cosine similarity for speaker embedding consistency across turns
- Pearson correlations of 0.54 (backchannels) and 0.25 (interruptions) between prompt-specified and generated behaviors
- Coherent conversations with speech quality and turn-taking comparable to training data

## Why This Works (Mechanism)
The frozen encoder + fine-tuned LLM architecture enables efficient training while preserving speech quality. Word-level alignment and strategic audio delay (2 tokens) ensure precise synchronization between text and audio streams. Separate speaker embedding layers and DSU heads allow granular control over individual speaker behaviors. The system-only loss focuses learning on conversational dynamics rather than general text generation.

## Foundational Learning
- **Full-duplex conversational modeling**: Simultaneous bidirectional speech generation requires precise turn-taking coordination and behavior control.
  - *Why needed*: Enables natural human-like conversations where both speakers can talk and respond simultaneously.
  - *Quick check*: Model maintains coherent dialogue with minimal overlap confusion.

- **Behavior-controlled generation**: Backchanneling and interruption generation require specific architectural mechanisms beyond standard speech synthesis.
  - *Why needed*: Natural conversations involve frequent short responses and interruptions that must be explicitly controlled.
  - *Quick check*: Correlation between prompt-specified and generated behavior frequencies.

- **Speaker embedding consistency**: Maintaining consistent speaker identity across conversation turns requires dedicated embedding mechanisms.
  - *Why needed*: Speaker identity consistency is crucial for believable conversations and proper turn attribution.
  - *Quick check*: Cosine similarity between speaker embeddings across conversation turns.

## Architecture Onboarding

### Component Map
Llama3.2-1B-Instruct -> Frozen NanoCodec Encoder -> 8 Linear DSU Heads -> LLM + 4 User + 4 System Embeddings -> Text/Audio Output

### Critical Path
Prompt + Speaker Embeddings -> LLM Processing -> DSU Heads -> Audio Generation with Behavioral Control

### Design Tradeoffs
- Frozen encoder enables efficient training but limits architectural flexibility
- Synthetic training data provides scalability but may lack natural conversational diversity
- Single-stage training simplifies pipeline but may miss fine-grained behavioral nuances

### Failure Signatures
- High WER indicates misalignment between text and audio streams
- Low behavioral correlation suggests insufficient behavioral signals in training data
- Inconsistent speaker embeddings reveal embedding layer issues

### First Experiments
1. Verify word-level alignment quality by comparing generated text with audio transcriptions
2. Test speaker embedding consistency across multiple conversation turns
3. Measure behavioral correlation with different temperature and top-k settings

## Open Questions the Paper Calls Out
1. Can training on data with higher density of backchannels and interruptions improve the model's ability to match prompt-specified event counts more precisely, rather than treating the prompt as only a directional signal?
2. How can streaming encoder inference be integrated to enable fully online generation, and what performance trade-offs would this introduce?
3. Does the single-stage, frozen-encoder training recipe transfer to other languages when appropriate full-duplex conversational datasets become available?
4. How does model performance differ when trained on real conversational speech versus synthetic TTS-generated dialogue data?

## Limitations
- Behavioral control precision is limited by training data density (interruptions average only 0.9 per conversation)
- Streaming encoder inference not supported, limiting real-time deployment
- Cross-linguistic generalization untested due to lack of multilingual training data

## Confidence
- High confidence in technical implementation details and training methodology
- Medium confidence in behavioral control effectiveness (correlations are present but modest)
- Medium confidence in reproducibility given dataset and implementation dependencies

## Next Checks
1. Verify Behavior-SD dataset availability and implement Kaldi forced-alignment filtering to ensure data quality matches reported 74.4% retention rate
2. Implement and test the exact embedding concatenation and projection mechanism between speaker embeddings, instruction prompts, and DAU embeddings to confirm architectural alignment
3. Conduct controlled experiments varying temperature and top-k parameters to characterize the relationship between generation parameters and behavioral control precision