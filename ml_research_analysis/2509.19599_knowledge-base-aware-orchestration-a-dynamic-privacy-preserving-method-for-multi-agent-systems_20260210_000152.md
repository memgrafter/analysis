---
ver: rpa2
title: 'Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for
  Multi-Agent Systems'
arxiv_id: '2509.19599'
source_url: https://arxiv.org/abs/2509.19599
tags:
- agent
- orchestration
- orchestrator
- agents
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Knowledge Base-Aware (KBA) Orchestration,
  a dynamic method for improving task routing in multi-agent systems by leveraging
  each agent's private knowledge base. The core idea is to augment static agent descriptions
  with real-time, privacy-preserving relevance signals generated when agents assess
  task compatibility against their own knowledge.
---

# Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems

## Quick Facts
- arXiv ID: 2509.19599
- Source URL: https://arxiv.org/abs/2509.19599
- Reference count: 40
- Primary result: KBA Orchestration achieves 87.1%-95.0% routing accuracy compared to 43.6%-68.6% for baseline, while preserving agent privacy

## Executive Summary
Knowledge Base-Aware (KBA) Orchestration introduces a dynamic method for improving task routing in multi-agent systems by leveraging each agent's private knowledge base. The core innovation is augmenting static agent descriptions with real-time, privacy-preserving relevance signals generated when agents assess task compatibility against their own knowledge. When initial routing confidence is low, agents are probed in parallel to return lightweight acknowledgments without exposing underlying data, which populates a shared semantic cache for future routing. This approach significantly outperforms traditional static description-driven orchestration, achieving 87.1% accuracy with minimal descriptions and up to 95.0% with detailed descriptions.

## Method Summary
The KBA method implements a three-stage orchestration flow: (1) Semantic Cache lookup to reuse recent successful routing decisions, (2) Confidence-based Routing using an LLM to classify queries against agent descriptions with a configurable threshold, and (3) Dynamic Probing where agents perform internal RAG lookups to return binary OK/KO signals. The framework uses Google Agent Development Kit (ADK) and assumes agents expose a probing interface to receive queries and return lightweight relevance signals without exposing their knowledge base contents.

## Key Results
- KBA-Low (87.1% accuracy) significantly outperforms Baseline-High (68.6%), demonstrating dynamic signals are more valuable than description quality
- KBA achieves up to 95.0% accuracy with detailed agent descriptions, compared to 43.6% for baseline with basic descriptions
- Semantic caching amortizes probing costs across recurring queries, though initial cold-start incurs ~5x latency (570s vs 100s)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Routing accuracy improves significantly when static descriptions are augmented with dynamic relevance signals from agents' private knowledge bases.
- **Mechanism:** The orchestrator performs Dynamic Knowledge Probing. Instead of relying solely on an agent card, the orchestrator issues a parallel query to candidate agents. Each agent checks the query against its internal Knowledge Base (KB) via similarity search and returns a lightweight binary signal (ACK/NACK) without exposing the underlying data.
- **Core assumption:** Agents possess a queryable internal knowledge store (vector database or document index) that is more current and comprehensive than their static text descriptions.
- **Evidence anchors:**
  - [Abstract]: "The orchestrator prompts the subagents in parallel... returning a lightweight ACK signal without exposing the underlying data."
  - [Section 4.4.2]: Shows KBA-Low (87.1%) outperforming Baseline-High (68.6%), suggesting the dynamic signal is more valuable than description quality.
  - [Corpus]: *CASTER* and *Federation of Agents* papers similarly argue for dynamic capability discovery over static model allocation, supporting the premise that static profiles are brittle.
- **Break condition:** If agents lack a local retrieval mechanism (e.g., pure reasoning agents without tools/KB), the probing step degrades to a hallucination risk or returns null.

### Mechanism 2
- **Claim:** System latency can be managed by treating the orchestrator as a cascading classifier, where expensive probing is conditional on confidence scores.
- **Mechanism:** Confidence-based Initial Routing. The system first attempts to route using descriptions and an LLM-assigned confidence score. If the top score exceeds a threshold (τ), it routes immediately. Probing is triggered only when confidence is low, reserving the high-latency parallel query for ambiguous cases.
- **Core assumption:** The LLM can reliably quantify its own uncertainty regarding the semantic match between a query and an agent description.
- **Evidence anchors:**
  - [Section 3.1.2]: "If the top score exceeds a configurable threshold (τ), the query is routed directly... Otherwise... escalated to the probing tool."
  - [Corpus]: *TCAndon-Router* discusses adaptive reasoning routers, aligning with the concept of conditional execution paths to balance cost and performance.
- **Break condition:** If the threshold τ is set too high, the system defaults to expensive probing for almost all queries, negating efficiency gains.

### Mechanism 3
- **Claim:** Long-term operational costs and latency decrease over time through the use of a shared semantic cache for routing decisions.
- **Mechanism:** Semantic Caching with Vector Invalidation. Successful routing decisions are stored as vector embeddings. Future queries are checked against this cache first. The paper proposes a geometric invalidation strategy (hyperspheres) to remove stale entries when agent knowledge changes.
- **Core assumption:** The distribution of user queries has semantic locality (recurring topics), making the amortization of the initial probing cost viable.
- **Evidence anchors:**
  - [Section 3.1.1]: "If a semantically similar query has been successfully routed recently, the decision is reused instantly."
  - [Section 5.1]: "The semantic cache further enhances efficiency, amortizing the probing cost across recurring queries."
  - [Corpus]: *RCR-Router* emphasizes structured memory for efficiency, conceptually supporting caching strategies in MAS.
- **Break condition:** If the query distribution is uniformly random (no repetition) or if the semantic embedding model fails to distinguish distinct intents (high collision rate), the cache hit rate drops to zero, incurring maximum latency on every request.

## Foundational Learning

- **Concept: RAG (Retrieval-Augmented Generation)**
  - **Why needed here:** The "Dynamic Knowledge Probing" is essentially a RAG operation performed by the agent. Understanding vector similarity search (e.g., cosine similarity, chunking) is required to implement the agent-side `KnowledgeBase.Search`.
  - **Quick check question:** Can you explain why the agent returns a binary "OK/KO" rather than the retrieved text chunk to the orchestrator?

- **Concept: Classifier Confidence Thresholding**
  - **Why needed here:** The architecture relies on a threshold (τ) to switch logic paths. You must understand the trade-off between Precision and Recall to tune this knob effectively.
  - **Quick check question:** If you set the confidence threshold τ too low, what happens to the system's latency and token consumption?

- **Concept: Embedding Vector Spaces**
  - **Why needed here:** The semantic cache depends on vector geometry. Section 3.1.5 describes invalidation as "hyperspheres" in a vector space.
  - **Quick check question:** In the context of semantic caching, what is the risk of setting the similarity threshold too low (e.g., 0.7 vs 0.95)?

## Architecture Onboarding

- **Component map:**
  Orchestrator (LLM) -> Semantic Cache (Vector Store) -> Probing Interface (API) -> Agent KB (Private)

- **Critical path:**
  1. Ingress: User Query
  2. Cache Check: VectorSearch(Query) -> Cache?
  3. Conditional Branch: If Miss -> LLM.Classify(Query, AgentCards)
  4. Fallback: If LLM.Confidence < Threshold -> ParallelProbe(AllAgents)
  5. Aggregation: Orchestrator collects OK signals
  6. Execution: Handoff to winning agent + Store result in Cache

- **Design tradeoffs:**
  - Latency vs. Accuracy: KBA adds ~5x latency (approx. 570s vs 100s in Table 3) in a cold-start scenario compared to baseline routing
  - Privacy vs. Transparency: The mechanism preserves data privacy (agents don't send data to orchestrator) but adds network overhead (parallel HTTP calls/gRPC)
  - Maintenance: KBA reduces the need for "perfect" agent descriptions (lower engineering overhead) but requires maintaining a valid semantic cache

- **Failure signatures:**
  - The "Silent Misroute": The LLM has high confidence (false positive) in the wrong agent description, skipping the probing phase entirely
  - The "Probe Storm": A surge in unique, ambiguous queries fills the cache with low-value entries or forces constant probing, spiking token costs
  - Cache Poisoning: An invalidation sphere is calculated incorrectly, causing valid routes to be purged or stale routes to persist

- **First 3 experiments:**
  1. Threshold Calibration: Run the 140-question test set with varying τ (e.g., 0.5, 0.7, 0.9) to plot the curve of "Probe Rate" vs. "Routing Accuracy"
  2. Cold Start Latency Test: Measure P50/P95 latency for a single unique query passing through the full probing pipeline vs. the baseline
  3. Cache Invalidation Verification: Update a specific policy in one agent's KB, trigger the invalidation vector, and verify that previously cached queries on that topic are correctly flushed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the synchronization overhead of parallel probing impact system latency and throughput when the agent pool scales from the experimental size (7 agents) to hundreds or thousands of agents?
- **Basis in paper:** [Explicit] Section 5.4 states that "probing incurs synchronization costs when scaling to very large agent pools, potentially creating latency bottlenecks."
- **Why unresolved:** The empirical evaluation was limited to a small, static set of 7 agents, making the performance characteristics under large-scale synchronization unknown.
- **What evidence would resolve it:** Benchmarking results showing latency distribution and throughput degradation as the number of agents increases to 100+, specifically measuring the "probe" phase duration.

### Open Question 2
- **Question:** What dynamic threshold adaptation strategies provide the optimal balance between precision and coverage when managing semantic cache invalidation?
- **Basis in paper:** [Explicit] Section 3.1.5 identifies "determining the optimal invalidation threshold" as one of the most significant challenges, noting the trade-off between removing stale content and invalidating semantically adjacent but distinct entries.
- **Why unresolved:** The paper proposes considering factors like domain specificity but provides no empirical data on which specific adaptation strategies work best or how to automate them.
- **What evidence would resolve it:** A comparative analysis of different thresholding algorithms (e.g., time-decay vs. domain-specific static values) measuring false positive/negative rates in cache hits following knowledge base updates.

### Open Question 3
- **Question:** How can the system maintain routing integrity when agents are non-cooperative or adversarial, deliberately returning false positive relevance signals?
- **Basis in paper:** [Explicit] Section 5.4 highlights that the "evaluation assumed cooperative agents" and warns that "adversarial or misconfigured agents could misreport capabilities, raising trust and governance concerns."
- **Why unresolved:** The current architecture implicitly trusts the OK/KO signals returned by agents, lacking a verification or reputation mechanism.
- **What evidence would resolve it:** Simulation results of routing accuracy in "Red Team" scenarios where a percentage of agents actively attempt to misroute queries by falsely claiming capability.

## Limitations

- **Reproducibility Gaps:** Key implementation details are missing - the specific LLM models, confidence threshold values, and embedding configurations are not specified, making exact reproduction challenging.
- **Scalability Questions:** While the semantic cache is proposed as a solution to latency, the paper doesn't address cache size management, eviction policies, or the computational cost of invalidation checks at scale.
- **Generalization Concerns:** The evaluation uses a single test set of 140 questions across 7 agents. There's no validation on how KBA performs with larger agent populations, more diverse query distributions, or real-world noise.

## Confidence

- **High Confidence:** The core mechanism of dynamic knowledge probing (Mechanism 1) and the experimental comparison showing KBA's accuracy advantage (87.1% vs 43.6%) are well-supported by the evidence presented.
- **Medium Confidence:** The effectiveness of confidence-based routing (Mechanism 2) and semantic caching (Mechanism 3) are conceptually sound but lack detailed implementation specifics and performance data under varying conditions.
- **Low Confidence:** Claims about operational efficiency gains over time and the robustness of the invalidation strategy are largely theoretical, with minimal empirical validation in the paper.

## Next Checks

1. **Threshold Sensitivity Analysis:** Run the full test suite across a range of confidence thresholds (0.5, 0.7, 0.9) to quantify the trade-off between probing frequency, latency, and accuracy. This would validate the adaptive routing claim.
2. **Cache Performance Under Load:** Simulate a realistic workload with query repetition patterns to measure cache hit rates, invalidation frequency, and the actual latency improvement over cold-start probing.
3. **Robustness to Agent Failures:** Introduce artificial failures in the probing mechanism (e.g., network timeouts, agent unavailability) to assess system resilience and the effectiveness of fallback strategies.