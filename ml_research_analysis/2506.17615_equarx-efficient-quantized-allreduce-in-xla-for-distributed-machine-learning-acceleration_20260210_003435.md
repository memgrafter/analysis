---
ver: rpa2
title: 'EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning
  Acceleration'
arxiv_id: '2506.17615'
source_url: https://arxiv.org/abs/2506.17615
tags:
- equarx
- allreduce
- quantization
- speedup
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EQuARX introduces a TPU-optimized quantized AllReduce implementation\
  \ for distributed machine learning that achieves 1.8\xD7 speedup over BF16 baseline\
  \ by using block-wise quantization and deep pipelining of communication and compute.\
  \ The method employs 8\xD7128 scale factors per shard for VPU-friendly quantization\
  \ and supports both full-loop and semi-loop ring variants."
---

# EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration

## Quick Facts
- arXiv ID: 2506.17615
- Source URL: https://arxiv.org/abs/2506.17615
- Reference count: 36
- 1.8× speedup over BF16 baseline using int8 quantization with block-wise scale factors and deep pipelining

## Executive Summary
EQuARX introduces a TPU-optimized quantized AllReduce implementation that achieves 1.8× speedup over BF16 baseline by using block-wise quantization and deep pipelining of communication and compute. The method employs 8×128 scale factors per shard for VPU-friendly quantization and supports both full-loop and semi-loop ring variants. Applied to Gemma 3 models, EQuARX accelerates prefill stage by 1.25× (27B) and 1.1× (12B) with small to negligible accuracy loss across 10 benchmarks. The approach balances performance gains against quantization error through tunable parameters, achieving near-optimal bandwidth utilization while maintaining numerical stability.

## Method Summary
EQuARX implements dynamic block-wise symmetric quantization with two-phase processing (Qp1 for metadata, Qp2 for scaling) and dequantization to FP32 before addition. It divides shards into microshards and pipelines Dq→Add→Qp1 operations across them, overlapping computation with communication. The method uses 8×128 scale factors per shard to match TPU VPU native