---
ver: rpa2
title: Composable NLP Workflows for BERT-based Ranking and QA System
arxiv_id: '2504.09398'
source_url: https://arxiv.org/abs/2504.09398
tags:
- pipeline
- ranking
- system
- have
- re-ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an end-to-end BERT-based ranking and QA system
  built using the Forte toolkit for composable NLP pipelines. The system combines
  BM25-based full-ranking, BERT-based re-ranking, and BERT-based QA to retrieve and
  answer questions from large document corpora.
---

# Composable NLP Workflows for BERT-based Ranking and QA System

## Quick Facts
- arXiv ID: 2504.09398
- Source URL: https://arxiv.org/abs/2504.09398
- Authors: Gaurav Kumar; Murali Mohana Krishna Dandu
- Reference count: 15
- Primary result: BERT-based ranking and QA system achieves MRR@10 of 0.34 and Recall@10 of 58% for re-ranking, with QA metrics around 0.32 on MS-MARCO

## Executive Summary
This paper presents an end-to-end BERT-based ranking and QA system built using the Forte toolkit for composable NLP pipelines. The system combines BM25-based full-ranking, BERT-based re-ranking, and BERT-based QA to retrieve and answer questions from large document corpora. Evaluated on MS-MARCO and Covid-19 datasets, the pipeline demonstrates competitive performance with modular architecture enabling easy construction of complex NLP applications.

## Method Summary
The system implements a three-stage pipeline: initial retrieval using BM25 ranking, followed by BERT-based re-ranking of top results, and finally BERT-based QA to generate answers. The Forte toolkit enables composable NLP workflows where each component can be independently developed and integrated. The pipeline processes queries through document retrieval, relevance scoring, and answer generation modules. Performance is evaluated on MS-MARCO and Covid-19 datasets using standard metrics including MRR, recall, and QA-specific scores (BLEU, ROUGE-L, F1).

## Key Results
- Re-ranking achieves MRR@10 of 0.34 and Recall@10 of 58% on MS-MARCO
- QA performance on MS-MARCO shows BLEU-1, ROUGE-L, F1 scores around 0.32
- Covid-19 dataset QA metrics drop to approximately 0.22 across all measures
- Performance gap on Covid-19 attributed to generic re-ranker not tuned to domain

## Why This Works (Mechanism)
The modular composable architecture enables efficient integration of specialized components for different NLP tasks. BM25 provides fast initial retrieval across large corpora, while BERT-based re-ranking improves precision by capturing semantic similarity. The final BERT QA component generates contextually appropriate answers from relevant passages. This staged approach balances computational efficiency with accuracy, allowing the system to handle large document collections while maintaining answer quality through deep learning components.

## Foundational Learning
- **BERT-based ranking**: Understanding semantic similarity between queries and documents through transformer models; needed for capturing contextual relationships beyond keyword matching; quick check: verify re-ranking improves over BM25 baseline
- **Composable pipeline architecture**: Modular design principles for building complex NLP systems from reusable components; needed for scalability and maintainability; quick check: confirm each component can be independently tested
- **Multi-stage retrieval**: Combining sparse (BM25) and dense (BERT) representations for efficient document ranking; needed to balance speed and accuracy; quick check: measure latency at each pipeline stage

## Architecture Onboarding

Component map: Query -> BM25 Retrieval -> BERT Re-ranking -> BERT QA -> Answer

Critical path: Query flows through BM25 to retrieve top-K documents, then BERT re-ranker scores and filters results, finally BERT QA generates answers from highest-ranked passages.

Design tradeoffs: BM25 offers speed but limited semantic understanding, while BERT provides semantic depth but higher computational cost. The system trades some latency for improved accuracy by using BERT only on top-K results rather than full corpus.

Failure signatures: Poor performance on domain-specific queries when using generic models, reduced recall when BM25 retrieval fails to capture relevant documents, QA failures when answer spans are not present in top-ranked passages.

First experiments:
1. Benchmark BM25 retrieval quality alone using recall metrics
2. Test BERT re-ranker performance with ground truth relevance labels
3. Validate QA component with oracle passages containing correct answers

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance degradation on Covid-19 dataset (QA metrics drop from 0.32 to 0.22) due to use of generic re-ranker
- Limited evaluation scope without ablation studies to quantify component contributions
- Absence of comprehensive benchmarking against state-of-the-art systems
- No exploration of robustness across different document types or query distributions

## Confidence
- High confidence in technical implementation details and pipeline architecture
- Medium confidence in performance claims due to limited benchmarking
- Low confidence in generalizability claims given restricted dataset coverage

## Next Checks
1. Conduct ablation studies comparing performance with and without domain-specific fine-tuning on the Covid-19 dataset to quantify the impact of using a generic re-ranker
2. Perform comprehensive benchmarking against established state-of-the-art ranking and QA systems across multiple datasets to establish relative performance
3. Test the pipeline's robustness by evaluating on diverse document types and query distributions, including adversarial queries designed to expose potential failure modes