---
ver: rpa2
title: 'SGuard-v1: Safety Guardrail for Large Language Models'
arxiv_id: '2511.12497'
source_url: https://arxiv.org/abs/2511.12497
tags:
- safety
- data
- training
- sguard-v1
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SGuard-v1, a lightweight safety guardrail
  system for Large Language Models (LLMs) consisting of two specialized models: ContentFilter
  and JailbreakFilter. ContentFilter is designed to detect safety risks in LLM prompts
  and responses using a refined five-category taxonomy based on MLCommons, while JailbreakFilter
  defends against adversarial jailbreak attempts across 60 major attack types.'
---

# SGuard-v1: Safety Guardrail for Large Language Models

## Quick Facts
- arXiv ID: 2511.12497
- Source URL: https://arxiv.org/abs/2511.12497
- Reference count: 12
- Introduces SGuard-v1, a lightweight safety guardrail system achieving state-of-the-art performance on safety benchmarks

## Executive Summary
SGuard-v1 is a safety guardrail system designed to protect Large Language Models from safety risks and adversarial jailbreak attempts. The system consists of two specialized 2B-parameter models: ContentFilter for detecting safety risks in prompts and responses, and JailbreakFilter for defending against 60 major attack types. Built on the Granite-3.3-2B-Instruct architecture and trained on a curated bilingual dataset of approximately 1.4 million examples, SGuard-v1 employs innovative training techniques to achieve superior performance while maintaining a lightweight footprint. The system is released under the Apache-2.0 License for research and practical deployment.

## Method Summary
SGuard-v1 employs a dual-model architecture where ContentFilter and JailbreakFilter work in tandem to provide comprehensive safety protection. ContentFilter uses a refined five-category taxonomy based on MLCommons to detect safety risks, while JailbreakFilter defends against adversarial attacks. Both models leverage the 2B-parameter Granite-3.3-2B-Instruct architecture and are trained on a carefully curated bilingual dataset combining public and proprietary data. The training process incorporates innovative techniques including contextual harm translation, benign-harmful contextual blending, and priority switching with noise injection. The system provides multi-class safety predictions with binary confidence scores and is designed to reduce GPU memory usage while maintaining high performance across diverse safety benchmarks.

## Key Results
- Achieves state-of-the-art performance on multiple public and proprietary safety benchmarks
- Maintains lightweight footprint with 2B-parameter architecture, reducing GPU memory usage
- Successfully detects safety risks across five categories and defends against 60 major jailbreak attack types
- Demonstrates effectiveness in both English and Korean languages

## Why This Works (Mechanism)
The system's effectiveness stems from its specialized dual-model architecture that separates content safety detection from jailbreak defense, allowing each model to focus on its specific task. The use of a refined five-category taxonomy provides more granular and accurate risk detection compared to binary approaches. The innovative training techniques, including contextual harm translation and priority switching with noise injection, create robust models that can handle diverse attack patterns and safety scenarios. The bilingual dataset construction ensures broader coverage and applicability across different linguistic contexts.

## Foundational Learning

1. **Safety Risk Taxonomy**
   - Why needed: Provides structured framework for identifying and categorizing different types of safety risks
   - Quick check: Five-category system based on MLCommons with clear boundaries between categories

2. **Adversarial Jailbreak Defense**
   - Why needed: Protects against deliberate attempts to bypass safety measures through crafted inputs
   - Quick check: Coverage of 60 major attack types with specialized detection mechanisms

3. **Contextual Harm Translation**
   - Why needed: Enables models to understand safety risks across different linguistic and cultural contexts
   - Quick check: Bilingual English-Korean dataset with context-aware translation techniques

4. **Priority Switching with Noise Injection**
   - Why needed: Creates robust models that can handle diverse and noisy real-world inputs
   - Quick check: Training technique that balances different safety scenarios and input variations

5. **Binary Confidence Scoring**
   - Why needed: Provides clear decision thresholds for safety interventions
   - Quick check: Multi-class predictions with associated confidence levels for each safety category

## Architecture Onboarding

**Component Map:**
Granite-3.3-2B-Instruct -> ContentFilter -> Safety Risk Detection
Granite-3.3-2B-Instruct -> JailbreakFilter -> Jailbreak Defense

**Critical Path:**
Input -> ContentFilter (Safety Risk Detection) -> JailbreakFilter (Jailbreak Defense) -> Output with Safety Scores

**Design Tradeoffs:**
- 2B-parameter architecture vs. larger models: Reduced memory usage and faster inference at potential cost of some capability
- Dual-model approach vs. single integrated model: Specialized focus but increased system complexity
- Bilingual focus (English/Korean) vs. multilingual: Deeper coverage of two languages vs. broader but shallower coverage

**Failure Signatures:**
- False positives: Legitimate content incorrectly flagged as safety risks
- False negatives: Actual safety risks not detected by the system
- Performance degradation under novel attack patterns not covered in training

**First 3 Experiments to Run:**
1. Deploy in controlled environment with known safety test cases to measure detection accuracy
2. Run adversarial testing using common jailbreak techniques to evaluate defense effectiveness
3. Performance benchmarking under varying load conditions to assess latency and resource usage

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Evaluation focuses primarily on benchmark datasets rather than live user interactions, raising questions about real-world effectiveness
- Bilingual English-Korean focus may limit effectiveness across other languages and cultural contexts
- Claims of "state-of-the-art performance" require context regarding specific comparison baselines and evaluation conditions

## Confidence

**High Confidence:**
- Technical architecture and training methodology are well-documented and reproducible
- Performance metrics on benchmarks appear credible given comprehensive dataset construction

**Medium Confidence:**
- Lightweight footprint claim - actual deployment overhead and inference latency unclear
- Real-world deployment effectiveness - benchmark performance may not translate to production environments

**Low Confidence:**
- Long-term effectiveness against emerging jailbreak techniques not covered in the 60 attack types

## Next Checks
1. Deploy SGuard-v1 in a live production environment with real user traffic to measure actual false positive/negative rates and system latency
2. Conduct cross-lingual validation testing across languages beyond English and Korean to assess generalizability
3. Perform adversarial robustness testing using emerging jailbreak techniques not covered in the 60 attack types to evaluate long-term effectiveness