---
ver: rpa2
title: Neuron-level Balance between Stability and Plasticity in Deep Reinforcement
  Learning
arxiv_id: '2504.08000'
source_url: https://arxiv.org/abs/2504.08000
tags:
- neurons
- learning
- plasticity
- stability
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the stability-plasticity dilemma in deep reinforcement
  learning (DRL), where agents struggle to retain previously learned skills while
  adapting to new tasks. The authors propose Neuron-level Balance between Stability
  and Plasticity (NBSP), a method that operates at the individual neuron level rather
  than the network level.
---

# Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.08000
- Source URL: https://arxiv.org/abs/2504.08000
- Reference count: 40
- Primary result: NBSP achieves 0.90 average success rate vs 0.63 baseline on Meta-World 2-task cycling

## Executive Summary
This work addresses the stability-plasticity dilemma in deep reinforcement learning (DRL), where agents struggle to retain previously learned skills while adapting to new tasks. The authors propose Neuron-level Balance between Stability and Plasticity (NBSP), a method that operates at the individual neuron level rather than the network level. NBSP identifies "RL skill neurons" that encode critical task-specific skills using a goal-oriented approach, then applies gradient masking to these neurons during training to preserve existing knowledge while allowing plasticity in other neurons. Experience replay is incorporated to further reinforce stability. Experimental results on Meta-World and Atari benchmarks show that NBSP significantly outperforms existing methods, achieving higher average success rates (e.g., 0.90 vs 0.63 for baseline) and better balance between stability (lower forgetting) and plasticity (forward transfer). The approach demonstrates strong generalization across different benchmarks and task types.

## Method Summary
NBSP operates by identifying and protecting critical neurons that encode task-specific skills during sequential learning. The method uses goal-oriented neuron scoring to identify "RL skill neurons" - the top 20% of neurons that are most critical for task performance. During training, gradient masking is applied to these skill neurons (with masking factor α=0.2) to prevent them from changing significantly, while allowing other neurons to remain plastic. Experience replay is incorporated by storing 10^5 transitions per task and sampling from this buffer every 10 training steps to reinforce previously learned skills. The approach is tested on Meta-World (6 robotic manipulation tasks) and Atari (4 games) using SAC from CleanRL with 3 hidden layers of size 768.

## Key Results
- NBSP achieves 0.90 average success rate vs 0.63 baseline on Meta-World 2-task cycling
- Superior stability-plasticity balance: lower forgetting (FM) and better forward transfer (FWT)
- Consistent performance improvements across both Meta-World and Atari benchmarks
- Robust generalization across different task types and benchmark domains

## Why This Works (Mechanism)
The core mechanism works by preserving critical task knowledge at the neuron level rather than the network level. By identifying and protecting the most important neurons for each task (RL skill neurons), the method ensures that essential skills are retained while allowing the network to adapt to new tasks through the remaining plastic neurons. The gradient masking prevents catastrophic forgetting by limiting changes to critical neurons, while experience replay provides additional stability through reinforcement of past experiences. This neuron-level approach is more precise than traditional network-level regularization methods, allowing for better trade-offs between stability and plasticity.

## Foundational Learning

**Stability-plasticity dilemma in DRL**
- Why needed: Fundamental challenge where agents forget old tasks when learning new ones
- Quick check: Observe performance drop on previous tasks after training on new tasks

**Goal Proximity Metric (GPM)**
- Why needed: Quantifies how close current state is to achieving task goals
- Quick check: Verify GPM values correlate with actual task completion probability

**Catastrophic forgetting**
- Why needed: Complete loss of previously learned skills during new task training
- Quick check: Measure performance degradation on old tasks after learning new tasks

**Experience replay buffer management**
- Why needed: Stores past experiences for stability reinforcement
- Quick check: Verify buffer contains appropriate distribution of past task experiences

## Architecture Onboarding

**Component map:** Actor network -> Neuron scoring -> Gradient masking -> Training update -> Experience replay buffer

**Critical path:** Neuron identification (goal-oriented scoring) → Gradient masking application → Training updates with masked gradients → Periodic experience replay sampling

**Design tradeoffs:** 
- Neuron-level vs network-level regularization (finer control vs computational overhead)
- Fixed vs dynamic skill neuron proportion (simplicity vs adaptability)
- Frequency of experience replay sampling (stability vs computational cost)

**Failure signatures:**
- ASR drops significantly when skill neuron proportion deviates from optimal
- High FM values indicate insufficient protection of critical neurons
- Poor FWT suggests over-protection preventing adaptation to new tasks

**First experiments to run:**
1. Single-task SAC baseline validation to establish performance benchmarks
2. Goal-oriented neuron scoring implementation and verification on individual tasks
3. 2-task cycling experiment with NBSP to verify stability-plasticity balance

## Open Questions the Paper Calls Out

**Open Question 1:** How can the optimal proportion of RL skill neurons be determined automatically without manual tuning?
- Basis in paper: The authors state in Section E (Limitation) that "the number of RL skill neurons must be manually determined and adjusted... as there is no automatic mechanism."
- Why unresolved: Currently, the proportion (e.g., 0.2) is a static hyperparameter that requires empirical adjustment based on task complexity.
- What evidence would resolve it: A dynamic thresholding algorithm or theoretical bound that adapts the neuron selection count in real-time based on the agent's performance gradients.

**Open Question 2:** Can the NBSP framework be effectively adapted to non-RL paradigms like supervised or unsupervised learning?
- Basis in paper: Section E (Future Work) proposes extending the method to "other learning paradigms, such as supervised and unsupervised learning."
- Why unresolved: The current neuron identification relies on a "Goal Proximity Metric" (GPM) based on rewards/success, which is intrinsic to RL and absent in standard supervised tasks.
- What evidence would resolve it: A redefined neuron-scoring metric for supervised tasks (e.g., based on loss gradients) that demonstrates reduced catastrophic forgetting in class-incremental benchmarks.

**Open Question 3:** Can the identified RL skill neurons be leveraged for efficient model distillation or behavioral manipulation?
- Basis in paper: Section E (Future Work) suggests using these neurons for "Model Distillation" via pruning and "Bias Control and Model Manipulation."
- Why unresolved: The paper focuses on stability during training but does not investigate the utility of these neurons for post-training compression or interpretability.
- What evidence would resolve it: Experiments showing that pruning non-skill neurons preserves performance (distillation) or that selectively activating/deactivating skill neurons reliably alters agent behavior.

## Limitations
- Manual tuning required for optimal skill neuron proportion
- Limited evaluation with only 3 random seeds
- Implementation details for GPM computation and experience replay buffer management not fully specified
- Computational overhead from neuron-level analysis and gradient masking

## Confidence

**High confidence** in the core concept of neuron-level gradient masking to address the stability-plasticity dilemma.
**Medium confidence** in the reported performance improvements, given the limited number of seeds and potential sensitivity to hyperparameter choices (α=0.2, k=10, 20% neuron selection).
**Medium confidence** in the experimental methodology, as the exact implementation details for GPM and experience replay are not fully specified.

## Next Checks

1. **Implement and validate GPM scoring:** Reproduce the goal-oriented neuron scoring (Acc(N) and Score(N)) on a single task to ensure the top 20% skill neuron identification is accurate and stable across runs.

2. **Test skill neuron masking ablation:** Run experiments with NBSP applied only to actor, only to critic, and to both, to confirm that masking both networks is necessary for optimal stability-plasticity balance.

3. **Verify experience replay buffer usage:** Implement and test the Dpre buffer sampling every k=10 steps, ensuring it stores and replays 10^5 transitions per prior task, and measure its impact on FM and ASR.