---
ver: rpa2
title: 'Rethinking generative image pretraining: How far are we from scaling up next-pixel
  prediction?'
arxiv_id: '2511.08704'
source_url: https://arxiv.org/abs/2511.08704
tags:
- image
- scaling
- prediction
- optimal
- next-pixel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates the scaling properties of autoregressive\
  \ next-pixel prediction for unified vision models. The authors train a family of\
  \ Transformers on 32\xD732 images using IsoFlops profiles up to 7\xD710^19 FLOPs,\
  \ evaluating next-pixel prediction loss, ImageNet classification accuracy, and generation\
  \ quality via Fr'echet Distance."
---

# Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?

## Quick Facts
- **arXiv ID:** 2511.08704
- **Source URL:** https://arxiv.org/abs/2511.08704
- **Reference count:** 4
- **Key outcome:** Compute, not data, is the primary bottleneck for scaling autoregressive next-pixel prediction models.

## Executive Summary
This paper investigates the scaling properties of autoregressive next-pixel prediction Transformers for unified vision models. The authors train models on 32×32 images using IsoFlops profiles up to 7×10^19 FLOPs, systematically evaluating next-pixel prediction loss, ImageNet classification accuracy, and generation quality via Fréchet Distance. They discover that optimal scaling strategies differ dramatically between tasks—generation requires 3-5× more data than classification. As resolution increases, model size must grow much faster than data size. The key finding is that compute availability, not data quantity, currently limits progress in raw pixel-by-pixel modeling.

## Method Summary
The authors train a family of Transformer models on 32×32 resolution images using autoregressive next-pixel prediction objectives. They employ IsoFlops profiles to systematically vary compute budgets up to 7×10^19 FLOPs, testing different combinations of model size and dataset size. Training uses standard autoregressive loss with causal attention masking. The models are evaluated on three metrics: next-pixel prediction loss (perplexity), ImageNet classification accuracy when fine-tuned, and generation quality measured by Fréchet Distance between generated and real image distributions. The scaling experiments systematically vary model parameters and training data size while keeping resolution fixed at 32×32.

## Key Results
- Compute is identified as the primary bottleneck, not data availability, for scaling next-pixel prediction models
- Optimal scaling strategy is task-dependent: generation requires 3-5× more training data than classification
- As image resolution increases, model size must grow much faster than dataset size to maintain performance
- With 4-5× annual compute growth, the authors project achieving 80% ImageNet accuracy and compelling generation within five years

## Why This Works (Mechanism)
The autoregressive next-pixel prediction objective forces models to learn rich hierarchical representations by predicting each pixel sequentially based on all previously generated pixels. This dense, fine-grained training signal captures detailed spatial relationships and long-range dependencies that classification-only pretraining misses. The Transformer architecture with causal attention naturally handles this sequential generation task, building up contextual understanding pixel by pixel. The task's inherent difficulty—predicting exact pixel values rather than abstract categories—drives stronger feature learning that transfers effectively to both discriminative and generative downstream tasks.

## Foundational Learning

**IsoFlops profiles** - Systematic scaling experiments that vary compute budgets while keeping resolution constant, enabling isolation of compute's effect on performance. *Quick check:* Verify FLOPs calculations match stated values across different model sizes.

**Fréchet Distance** - Metric for comparing distributions of real versus generated images based on their mean and covariance in feature space. *Quick check:* Confirm distance values are consistent across evaluation runs.

**Causal attention** - Attention mechanism where each position can only attend to previous positions, essential for autoregressive generation. *Quick check:* Validate masking prevents information leakage from future tokens.

## Architecture Onboarding

**Component map:** Input pixels → Causal attention layers → Feed-forward layers → Output logits → Cross-entropy loss

**Critical path:** The autoregressive prediction loop where each generated pixel conditions on all previous pixels through causal attention, creating a dependency chain that must be processed sequentially.

**Design tradeoffs:** The sequential nature of pixel-by-pixel generation limits parallelism compared to parallel image generation methods, but provides richer training signals. The 32×32 resolution constraint balances computational feasibility with representational capacity.

**Failure signatures:** Insufficient model capacity leads to mode collapse in generation and poor transfer learning. Inadequate data causes overfitting and high perplexity. Compute constraints manifest as inability to scale model size appropriately with resolution.

**First experiments:** 1) Train smallest model with maximum data to establish lower bound performance. 2) Train largest model with minimum data to find overfitting threshold. 3) Find optimal model-data ratio for classification transfer learning.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Analysis limited to 32×32 resolution, limiting generalizability to higher resolutions
- Projections assume continued 4-5× annual compute growth, which may not be sustainable
- Generation quality evaluation relies on Fréchet Distance rather than perceptual quality or downstream utility
- Task-specific scaling recommendations may not transfer to heterogeneous real-world pretraining scenarios

## Confidence

**High:** Compute being the primary bottleneck within tested regimes
**Medium:** Task-dependent optimal scaling strategies
**Medium:** Projection timelines for 80% accuracy and compelling generation
**Low:** Generalization of findings to higher resolutions and diverse data conditions

## Next Checks
1. Validate scaling laws at 64×64 and 128×128 resolutions to test resolution-dependent model size requirements
2. Test pretraining with heterogeneous data mixtures (natural images, synthetic data, multimodal content) to assess robustness of scaling predictions
3. Evaluate generated samples through human perceptual studies and downstream task performance, not just Fréchet Distance metrics