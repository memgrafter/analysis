---
ver: rpa2
title: "Are LLMs Good Text Diacritizers? An Arabic and Yor\xF9b\xE1 Case Study"
arxiv_id: '2506.11602'
source_url: https://arxiv.org/abs/2506.11602
tags:
- zhang
- diacritization
- wang
- llms
- yoruba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of large language models
  (LLMs) for text diacritization in Arabic and Yoruba, two languages where diacritics
  are essential for meaning. The authors introduce a novel multilingual dataset, MultiDiac,
  specifically designed to evaluate diacritization by including diverse samples with
  varying linguistic ambiguity to minimize overlap with LLM pre-training data.
---

# Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study

## Quick Facts
- **arXiv ID:** 2506.11602
- **Source URL:** https://arxiv.org/abs/2506.11602
- **Reference count:** 9
- **Primary result:** LLMs (especially Grok-3 and GPT-4o) outperform specialized diacritization models on Arabic and Yoruba diacritization, with fine-tuning reducing hallucinations in smaller models.

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively restore diacritics in Arabic and Yoruba, two languages where diacritics are essential for meaning disambiguation. The authors introduce MultiDiac, a novel multilingual dataset designed to evaluate diacritization while minimizing overlap with LLM pre-training data. They evaluate 14 LLMs (varying in size and accessibility) against 6 specialized diacritization models, and additionally fine-tune four small open-source models for Yoruba using LoRA. Results show that many off-the-shelf LLMs outperform specialized diacritization models, with Grok-3 achieving the strongest performance on Arabic and GPT-4o excelling in Yoruba. Arabic diacritization performance is consistently stronger across both open and closed models. Fine-tuning notably improved Yoruba performance, highlighting the benefits of task adaptation in low-resource settings.

## Method Summary
The study evaluates LLMs for text diacritization using the MultiDiac dataset (562 training, 41 dev, 101 test samples for Yoruba; 106 test samples for Arabic). Fourteen LLMs spanning various sizes and accessibility levels are assessed against six specialized diacritization models. For low-resource Yoruba, four small open-source models are fine-tuned using LoRA (rank=16, alpha=32, dropout=0.05) on projection and feedforward layers. Evaluation uses Character Error Rate (CER) and Word Error Rate (WER) as primary metrics, with hallucination measured via WER on undiacritized predictions. The study compares zero-shot LLM performance against specialized models and examines fine-tuning benefits for resource-constrained scenarios.

## Key Results
- Many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba
- Grok-3 achieves the best overall performance on Arabic diacritization
- GPT-4o excels in Yoruba diacritization
- Fine-tuning notably improved Yoruba performance and reduced hallucinations in smaller models
- Arabic diacritization performance is consistently stronger across both open and closed models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large-scale pre-training provides implicit diacritization capability through exposure to diverse multilingual text patterns.
- **Mechanism:** LLMs with extensive pre-training on multilingual corpora acquire statistical representations of diacritic patterns and their contextual disambiguation, enabling zero-shot performance without task-specific training.
- **Core assumption:** Pre-training corpora contain sufficient diacritized text for the model to learn orthographic conventions.
- **Evidence anchors:** "Results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba"; "Grok-3 achieves the best overall performance... indicating strong zero-shot diacritization capabilities"; Limited direct corpus support.
- **Break condition:** Performance degrades significantly for languages with minimal representation in pre-training data (observed with Yoruba vs. Arabic gap).

### Mechanism 2
- **Claim:** Contextual understanding enables resolution of diacritic ambiguity that character-level models cannot achieve.
- **Mechanism:** Decoder-only LLMs process full sentence context before predicting diacritized output, allowing semantic disambiguation of homographic base words based on surrounding tokens.
- **Core assumption:** The task requires semantic understanding, not just orthographic pattern matching.
- **Evidence anchors:** Table 1 demonstrates that "Ajewo ile re" has two interpretations based on diacritization of "Aje" (wealth vs. witch); "Approximately 350 base words with multiple valid diacritized forms... each reflecting distinct meanings in context"; Related work on Arabic diacritization focuses on specialized models without explicit context mechanisms.
- **Break condition:** When context is insufficient or ambiguous, even large models fail to disambiguate correctly.

### Mechanism 3
- **Claim:** Parameter-efficient fine-tuning (LoRA) reduces hallucinations by constraining output space to task-relevant patterns.
- **Mechanism:** LoRA adaptation on task-specific data shifts the model's generation distribution toward valid diacritized outputs, reducing the probability of generating unrelated content or wrong-language outputs.
- **Core assumption:** Small datasets (562 training samples for Yoruba) contain sufficient signal for task adaptation.
- **Evidence anchors:** "Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates"; "LLaMA-3.2-1B showed the most dramatic reduction in hallucinations, with WER dropping from 729.90 to 45.77"; No direct corpus evidence on LoRA for diacritization.
- **Break condition:** Fine-tuning on insufficient or low-quality data may not generalize or could overfit to specific patterns.

## Foundational Learning

- **Concept: Diacritization as Sequence-to-Sequence Generation**
  - Why needed here: Understanding that diacritization can be framed as text-to-text generation rather than token classification enables appropriate model selection and prompt design.
  - Quick check question: Can you explain why generative LLMs might handle diacritization differently than encoder-only classification models?

- **Concept: Morphological Richness and Orthographic Ambiguity**
  - Why needed here: Arabic and Yoruba have different diacritic functions (vowels/tone marks vs. vowel quality/tone), affecting error patterns and evaluation metrics.
  - Quick check question: How does the function of diacritics in Arabic (short vowels, consonant doubling) differ from Yoruba (lexical tone, vowel quality)?

- **Concept: Hallucination in Low-Resource Settings**
  - Why needed here: Smaller models generate off-target content when lacking sufficient pre-training exposure, requiring detection metrics and mitigation strategies.
  - Quick check question: How did the authors quantify hallucinations in their evaluation framework?

## Architecture Onboarding

- **Component map:** Undiacritized text → Prompt template → Tokenizer → Pre-trained LLM (decoder-only) → Contextual representations → Autoregressive generation → Diacritized text → Post-processing → CER/WER computation, hallucination detection

- **Critical path:**
  1. Dataset construction with ambiguity-aware sampling (base words with multiple diacritization variants)
  2. Model selection based on language coverage and scale
  3. Prompt engineering for zero-shot evaluation
  4. Optional: LoRA fine-tuning for low-resource languages
  5. Evaluation using CER, WER, and hallucination rate

- **Design tradeoffs:**
  - Large closed-source models (Grok-3, GPT-4o): Best performance but no local control, API costs, data privacy concerns
  - Small open-source models (Llama-3.2-1B, Gemma-7B): Deployable locally but high hallucination rates without fine-tuning
  - Specialized models (CATT, T5-based): Efficient and interpretable but may lack contextual disambiguation capability
  - Fine-tuning investment: Requires labeled data and compute; most beneficial for low-resource languages with poor zero-shot performance

- **Failure signatures:**
  - High WER with high hallucination WER: Model generating wrong language or unrelated content (LLaMA-3.2-1B on Yoruba: 779.39 WER)
  - High WER with low hallucination WER: Model produces correct language but incorrect diacritics
  - Translation/transliteration output: Model misinterprets task (Jais on Arabic: WER 171.25, producing translated output)
  - Modest WER improvement but CER improvement: Fine-tuning reduces character errors but may not fully address word-level issues

- **First 3 experiments:**
  1. **Baseline establishment:** Evaluate 2-3 LLMs spanning size categories (e.g., 1B, 7B, 70B+ parameters) on MultiDiac test sets to determine performance tier; use simple prompt "Add diacritics to this [language] text" without examples
  2. **Hallucination diagnosis:** For models with WER > 100%, manually inspect outputs and compute hallucination WER to distinguish between wrong-language generation, translation, and diacritic errors; this informs whether to pursue fine-tuning or model replacement
  3. **LoRA adaptation for worst-case:** Select the worst-performing open-source model (highest WER in target language) and apply LoRA fine-tuning with the paper's configuration (rank=16, alpha=32, dropout=0.05) using available training data; measure hallucination reduction to validate the authors' finding that fine-tuning addresses generation errors even with limited data

## Open Questions the Paper Calls Out

- **Can specialized diacritization models be improved by using high-performing LLMs for data augmentation?**
  - The authors state that their results "can be used as a motivation to improve the performance of specialized diacritization models, potentially using LLMs for data augmentation."
  - This remains unresolved as the study evaluates LLMs as direct diacritizers but does not experiment with using them to generate synthetic training data for smaller, specialized models.

- **Is it possible to deploy LLM-based diacritizers in resource-constrained pipelines without sacrificing their accuracy advantage?**
  - The authors acknowledge that while LLMs outperform specialized models, they are "more resource-intensive," making specialized models more "suitable as a pre-processing step in pipeline systems."
  - This remains unresolved as the paper evaluates accuracy but does not provide benchmarks on latency, memory footprint, or efficiency.

## Limitations

- **Limited Arabic training data:** Arabic evaluation is limited to only 106 test samples without training data, making it difficult to assess generalization to broader contexts.
- **Pre-training data overlap uncertainty:** No systematic verification that MultiDiac dataset truly avoids contamination from common pre-training corpora, potentially inflating zero-shot performance.
- **Specialized model comparison baseline:** Comparison against specialized models may be unfair as these were developed under different evaluation conditions and potentially different training sets.

## Confidence

**High confidence:** LLMs outperform specialized diacritization models in zero-shot settings for both Arabic and Yoruba. The comparative CER/WER metrics across 14 models are internally consistent and the pattern holds across multiple evaluation dimensions.

**Medium confidence:** LoRA fine-tuning significantly reduces hallucinations in small models. While the reported reduction for LLaMA-3.2-1B is dramatic, the mechanism explanation is somewhat speculative, and the limited training data may not generalize to all low-resource languages.

**Medium confidence:** Contextual understanding enables better diacritic disambiguation than character-level models. The theoretical argument is sound, but the empirical evidence relies on specific examples rather than comprehensive ablation studies.

## Next Checks

1. **Pre-training contamination audit:** Perform systematic checks to verify that MultiDiac test samples don't appear in common pre-training corpora (CCNet, C4, Wikipedia dumps) using exact string matching and fuzzy matching approaches.

2. **Dataset augmentation study:** Evaluate model performance when Arabic training data becomes available (similar to Yoruba's 562 samples) to determine whether the current Arabic test-only setup underestimates potential improvements from fine-tuning.

3. **Cross-model hallucination taxonomy:** Conduct detailed qualitative analysis of hallucination types across the highest-WER models to distinguish between wrong-language generation, translation outputs, orthographic variations, and genuine hallucinations.