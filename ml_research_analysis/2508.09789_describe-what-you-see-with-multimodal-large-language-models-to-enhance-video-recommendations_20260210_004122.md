---
ver: rpa2
title: Describe What You See with Multimodal Large Language Models to Enhance Video
  Recommendations
arxiv_id: '2508.09789'
source_url: https://arxiv.org/abs/2508.09789
tags:
- video
- recommendation
- audio
- mllms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a framework that enhances video recommendation
  by using off-the-shelf Multimodal Large Language Models (MLLMs) to generate rich
  natural-language summaries of video clips. These summaries, which capture high-level
  semantics, intent, and context, are integrated into standard recommendation models
  via text embeddings.
---

# Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations

## Quick Facts
- arXiv ID: 2508.09789
- Source URL: https://arxiv.org/abs/2508.09789
- Authors: Marco De Nadai; Andreas Damianou; Mounia Lalmas
- Reference count: 40
- One-line primary result: MLLM-derived features improve video recommendation metrics by up to 60% relative to traditional features.

## Executive Summary
This paper proposes a framework that enhances video recommendation by using off-the-shelf Multimodal Large Language Models (MLLMs) to generate rich natural-language summaries of video clips. These summaries, which capture high-level semantics, intent, and context, are integrated into standard recommendation models via text embeddings. Experiments on the MicroLens-100K dataset show that MLLM-derived features significantly outperform traditional visual, audio, and metadata features, achieving up to 60% relative improvement in HR@10 and nDCG@10. The approach is model-agnostic, zero-finetuning, and demonstrates that leveraging MLLMs as on-the-fly knowledge extractors can yield more intent-aware and effective video recommendations.

## Method Summary
The framework processes raw video files by separately extracting visual context (using Qwen-VL) and audio information (transcribed by Whisper, summarized by Qwen-Audio). The resulting text streams are concatenated and encoded into embeddings using BGE-large. These embeddings serve as item features in standard recommendation architectures (Two-Tower or SASRec). The method is zero-shot, avoiding fine-tuning of the MLLM, and focuses on generating intent-aware summaries that bridge the gap between raw content and user intent.

## Key Results
- MLLM-derived features outperform traditional visual, audio, and metadata features by up to 60% relative improvement in HR@10 and nDCG@10.
- The modular design handles audio and video separately before fusion, compensating for the absence of unified audio-visual MLLMs.
- Qwen-VL 7B provides better grounding but offers diminishing returns on HR@10 compared to the baseline model.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Compression via Textual Proxies
The MLLM acts as a "semantic compressor," distilling complex visual and auditory data into a textual description that captures intent and context. These summaries are then encoded into embeddings that are easier for standard recommenders to align with user interaction histories than sparse or high-dimensional raw features.

### Mechanism 2: Zero-Shot Knowledge Grounding
Off-the-shelf MLLMs inject static world knowledge and cultural context into the recommendation pipeline without task-specific fine-tuning. Unlike specific video encoders, MLLMs leverage massive pre-training to recognize specific entities and tropes, grounding the item representation in concepts users actually search for.

### Mechanism 3: Decoupled Cross-Modal Fusion
Processing audio and video separately with specialized models before fusing them as text handles distinct signal types more effectively than a single unified encoder. This prevents the "audio gap" often found in vision-centric video models.

## Foundational Learning

- **Concept: Two-Tower vs. Sequential (Generative) Architectures**
  - Why needed here: The paper validates the approach on these two distinct architectures. You must understand that Two-Tower models rely on static embedding similarity, while SASRec models the sequence of user actions.
  - Quick check question: Does the MLLM embedding feed into the Item Tower, the User Tower, or the Sequence Encoder in these setups? (Answer: Item side).

- **Concept: Zero-Shot vs. Fine-Tuning in LLMs**
  - Why needed here: The framework explicitly avoids fine-tuning the MLLM to keep costs low. You need to distinguish between the *frozen* feature extractor (MLLM) and the *trainable* recommender model.
  - Quick check question: Why would fine-tuning the MLLM on the recommendation loss potentially improve performance but violate the "zero-finetuning" efficiency constraint?

- **Concept: Content-Based vs. Collaborative Filtering**
  - Why needed here: The paper addresses the "cold start" or content-understanding gap in collaborative systems. MLLM features are fundamentally content-based signals injected into collaborative models.
  - Quick check question: How does a semantically rich caption help in a "cold start" scenario where a new video has no user interactions?

## Architecture Onboarding

- **Component map:** Raw Video File -> Feature Extractors (Qwen-VL, Whisper+Qwen-Audio) -> Natural Language Descriptions -> BGE-large Text Encoder -> Embeddings -> Two-Tower or SASRec Recommender

- **Critical path:** The **Prompt Engineering** for Qwen-VL/Audio is the highest leverage point. If the prompt fails to elicit "intent" or "style" and only yields "objects," the performance gain is lost.

- **Design tradeoffs:**
  - **Latency vs. Depth:** Running Qwen-VL 7B provides better grounding but offers diminishing returns on HR@10 compared to the cost.
  - **Temporal Loss:** Summarizing a video into a single text block discards fine-grained temporal ordering within the clip.
  - **Hallucination Risk:** MLLMs may invent details not in the video, potentially creating "false positive" matches in the recommender.

- **Failure signatures:**
  - **Generic Descriptions:** Output like "A person is talking" indicates the prompt or model is failing to extract semantic depth.
  - **Regressive Performance:** If MLLM features underperform simple metadata (titles), check if the text encoder dimensionality matches the recommender's input layer or if the MLLM is producing low-quality "hallucinated" noise.

- **First 3 experiments:**
  1. **Baseline Validation:** Replicate the "Metadata" vs. "MLLM Video" comparison on a small slice of MicroLens-100K to verify the ~24% lift in the Two-Tower model.
  2. **Modality Ablation:** Run three conditions: (1) MLLM Visual only, (2) MLLM Audio only, (3) Combined, to confirm that the "Combined" approach yields the robust representations mentioned in the abstract.
  3. **Qualitative Audit:** Manually inspect 50 random video clips vs. their MLLM descriptions to verify "Intent" capture (e.g., does it spot the irony in the Cappadocia example?) vs. simple object detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do MLLM-derived features provide comparable gains on long-form video platforms (e.g., YouTube, Netflix) or other domains beyond TikTok-style short videos?
- Basis in paper: All experiments are limited to MicroLens-100K, a single dataset of vertical short-form videos.
- Why unresolved: The dataset characteristics (short duration, vertical format, specific user demographics) may not generalize to other video recommendation contexts.
- What evidence would resolve it: Experiments on additional video datasets with varying formats, lengths, and user bases.

### Open Question 2
- Question: Would unified omni-MLLMs that jointly process audio and video outperform the paper's modular two-stage approach?
- Basis in paper: The conclusion states: "As emerging omni-MLLMs, such as Qwen Omni and Vita, continue to advance in jointly modeling vision, audio, and text, our approach provides a low-barrier pathway."
- Why unresolved: The current framework processes audio and video separately, which may miss cross-modal interactions.
- What evidence would resolve it: A comparative study between modular and unified MLLM approaches on the same recommendation benchmarks.

### Open Question 3
- Question: Could targeted fine-tuning of MLLMs yield additional performance gains beyond zero-shot prompting?
- Basis in paper: The authors state they "do not here consider solutions that finetune vision models, such as [17], due to their significantly high computational cost."
- Why unresolved: Zero-finetuning was chosen for practicality, not necessarily optimality. Domain-specific fine-tuning might capture recommendation-relevant semantics better.
- What evidence would resolve it: Experiments comparing zero-shot MLLM captions against MLLMs fine-tuned on video recommendation tasks or datasets.

## Limitations
- **Prompt Engineering Dependency**: Performance gains hinge critically on the specific prompts used for Qwen-VL and Qwen-Audio.
- **Hallucination and Factual Grounding**: MLLMs may hallucinate entities, locations, or styles not present in the video, potentially degrading recommendation accuracy.
- **Temporal Information Loss**: Converting a potentially multi-minute video into a single, static text summary inherently discards fine-grained temporal dynamics.

## Confidence
- **High Confidence**: The core methodology (using MLLM-generated text embeddings as features) is clearly described and the performance improvement over traditional features is statistically significant and robust.
- **Medium Confidence**: The specific mechanism by which MLLMs capture "user intent" is plausible but relies heavily on the quality of the prompts and the MLLM's pre-training data coverage.
- **Low Confidence**: The paper's claims about the model being "zero-finetuning" and highly efficient are strong, but the computational cost of running large MLLM inference on a large video corpus is not fully addressed.

## Next Checks
1. **Prompt Audit**: Obtain and test the exact prompt templates used for Qwen-VL and Qwen-Audio on a diverse set of videos to verify they consistently elicit semantic depth and style information.
2. **Hallucination Detection**: Implement a simple fact-checking pipeline for a sample of MLLM-generated descriptions to quantify the rate of hallucination and its potential impact on recommendation accuracy.
3. **Temporal Fidelity Test**: Conduct an ablation study where the MLLM generates multiple text summaries for different temporal segments of the same video to measure the cost of temporal information loss.