---
ver: rpa2
title: 'Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for
  Code Generation'
arxiv_id: '2502.12492'
source_url: https://arxiv.org/abs/2502.12492
tags:
- arxiv
- data
- reasoning
- lora
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles code generation, a complex System 2 task, by
  addressing two key challenges: the difficulty of exploring intricate reasoning processes
  and the heterogeneous data distribution that complicates robust model training.
  The authors propose the BDC framework, which uses an MC-Tree-Of-Agents algorithm
  to explore System 2 knowledge via mutual boosting among multiple LLMs, disentangle
  heterogeneous data into clusters, and generate customized problem solvers through
  an input-aware hypernetwork that weights over composable LoRA experts.'
---

# Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation

## Quick Facts
- arXiv ID: 2502.12492
- Source URL: https://arxiv.org/abs/2502.12492
- Reference count: 11
- Key outcome: BDC framework improves code generation accuracy by up to 10% over single LLM agents on APPS and CodeContest datasets through mutual boosting, data disentanglement, and dynamic expert composition

## Executive Summary
This paper addresses the challenges of complex code generation by proposing the BDC framework that bridges System 2 reasoning with System 1 execution. The approach tackles two fundamental difficulties: exploring intricate reasoning processes and handling heterogeneous data distributions during model training. By implementing mutual boosting among multiple LLMs through an MC-Tree-Of-Agents algorithm, disentangling data into semantic clusters, and generating customized problem solvers via input-aware hypernetwork composition, BDC achieves significant performance improvements. The framework demonstrates robustness across varying difficulty levels while maintaining efficiency in deployment.

## Method Summary
The BDC framework implements a three-stage pipeline for code generation that transforms complex System 2 reasoning into efficient System 1 execution. First, an MC-Tree-Of-Agents algorithm enables multiple LLM agents to explore reasoning processes through mutual boosting, where each agent's outputs serve as inputs for others in a tree structure. Second, heterogeneous code generation data is disentangled into semantic clusters using clustering algorithms that capture problem characteristics. Third, an input-aware hypernetwork dynamically composes LoRA experts weighted by the input problem's features, creating specialized problem solvers without full fine-tuning. This architecture allows the system to leverage diverse reasoning approaches while maintaining computational efficiency through parameter-efficient adaptation.

## Key Results
- BDC achieves accuracy gains of up to 10% over single LLM agents on APPS and CodeContest datasets
- The framework demonstrates consistent robustness across varying difficulty levels of programming problems
- Empirical results validate the effectiveness of disentangling heterogeneous data and dynamically composing specialized experts

## Why This Works (Mechanism)
The BDC framework succeeds by addressing the dual challenges of complex reasoning and data heterogeneity in code generation. The mutual boosting mechanism allows multiple agents to explore diverse reasoning paths simultaneously, increasing the likelihood of finding correct solutions through collaborative exploration. Data disentanglement ensures that the model learns distinct patterns for different problem types rather than conflating heterogeneous examples, leading to more accurate specialization. The input-aware hypernetwork composition enables dynamic adaptation to problem characteristics without the computational cost of full fine-tuning, allowing the system to leverage specialized knowledge efficiently. Together, these components create a pipeline that maintains the depth of System 2 reasoning while achieving the speed of System 1 execution.

## Foundational Learning
- **MC-Tree-Of-Agents Algorithm**: A tree-structured approach where multiple LLM agents mutually boost each other's outputs to explore reasoning paths; needed to overcome the limitations of single-agent reasoning in complex problems; quick check: verify tree depth and branching factor impact on solution quality
- **Data Disentanglement**: Clustering heterogeneous code generation data into semantically meaningful groups; needed to prevent model confusion from mixed problem types and enable specialized learning; quick check: validate cluster coherence using silhouette scores and human evaluation
- **Input-Aware Hypernetwork**: A mechanism that dynamically weights composable LoRA experts based on input features; needed to achieve specialized problem-solving without full fine-tuning costs; quick check: measure adaptation speed versus accuracy trade-offs
- **LoRA Expert Composition**: Using low-rank adaptation modules as building blocks for specialized knowledge; needed to maintain parameter efficiency while enabling diverse expertise; quick check: compare parameter count and inference speed against full fine-tuning
- **System 2 to System 1 Transition**: Converting deliberate, resource-intensive reasoning into fast, habitual execution; needed to make complex code generation practical for real-time applications; quick check: measure latency improvements versus accuracy retention

## Architecture Onboarding

Component Map: Input Problem -> MC-Tree-Of-Agents (Reasoning) -> Data Disentanglement (Clustering) -> Input-Aware Hypernetwork (Expert Composition) -> Customized Solver (Output)

Critical Path: The reasoning phase through MC-Tree-Of-Agents represents the most computationally intensive step, as it requires multiple forward passes through different LLM agents. This is followed by data clustering and hypernetwork computation, which are relatively lightweight. The final expert composition and execution complete the pipeline with minimal overhead.

Design Tradeoffs: The framework trades increased upfront computation (multiple agents) for downstream efficiency (specialized solvers). The clustering step introduces an additional preprocessing requirement but enables more effective specialization. The hypernetwork approach balances between the expressiveness of full fine-tuning and the efficiency of fixed adapters.

Failure Signatures: Poor reasoning quality may manifest as low diversity in agent outputs or failure to converge on correct solutions. Ineffective clustering could result in mixed problem types within clusters, leading to confused specialization. Hypernetwork failures might appear as inappropriate expert selection or poor adaptation to input characteristics.

First Experiments:
1. Benchmark single-agent versus multi-agent reasoning quality on a subset of problems to quantify the mutual boosting effect
2. Evaluate clustering quality by comparing learned clusters against human-annotated difficulty levels and problem categories
3. Test hypernetwork adaptation speed and accuracy on held-out problems to validate the dynamic composition approach

## Open Questions the Paper Calls Out
None

## Limitations
- The computational overhead of running multiple LLM agents is not quantified, which is critical for practical deployment
- The data disentanglement approach assumes that clustering captures meaningful semantic distinctions without validating alignment with human-annotated difficulty levels
- The ablation studies focus on the overall framework rather than isolating the impact of individual components like the hypernetwork composition

## Confidence
High confidence: The core architecture (MC-Tree-Of-Agents + data disentanglement + hypernetwork composition) is well-defined and empirical results on standard benchmarks are reproducible. The improvement over single LLM baselines is clearly demonstrated.

Medium confidence: Claims about robustness across difficulty levels and heterogeneous data distributions are supported by experiments but could benefit from more granular analysis showing how each component contributes to this robustness.

Low confidence: The scalability analysis and computational cost claims are not well-supported. The paper mentions efficiency but does not provide runtime comparisons or resource requirements for the multi-agent system.

## Next Checks
1. Conduct ablation studies to isolate the contribution of the MC-Tree-Of-Agents algorithm versus the data disentanglement and hypernetwork components. Measure the marginal improvement each component provides.

2. Analyze the computational overhead by comparing inference times and resource usage between BDC and single LLM baselines across different problem scales. Include GPU memory and latency measurements.

3. Validate the data clustering quality by comparing the learned clusters against human-annotated labels for difficulty levels and programming paradigms on a held-out validation set.