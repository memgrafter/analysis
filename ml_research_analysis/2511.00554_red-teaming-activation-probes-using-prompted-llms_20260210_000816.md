---
ver: rpa2
title: Red-teaming Activation Probes using Prompted LLMs
arxiv_id: '2511.00554'
source_url: https://arxiv.org/abs/2511.00554
tags:
- failure
- probe
- probes
- high-stakes
- attacker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a lightweight, black-box red-teaming method
  for activation probes using an off-the-shelf LLM with iterative feedback and in-context
  learning. No model fine-tuning or architectural access is required.
---

# Red-teaming Activation Probes using Prompted LLMs

## Quick Facts
- arXiv ID: 2511.00554
- Source URL: https://arxiv.org/abs/2511.00554
- Reference count: 20
- One-line primary result: Lightweight black-box red-teaming method achieves up to 88% FPR and 63.6% FNR on activation probes using only iterative feedback and in-context learning, with interpretable failure patterns surfacing across domains.

## Executive Summary
This work introduces a black-box red-teaming method for activation probes that requires no model access or fine-tuning, relying instead on iterative feedback and in-context learning via an off-the-shelf LLM. Tested on a high-stakes interaction probe, the method achieves significant failure rates (up to 88% FPR, 63.6% FNR) using GPT-5, with smaller models also finding failures but less consistently. The approach reveals interpretable failure patterns such as legalese triggering false positives and procedural phrasing evading detection. Scenario-constrained attacks show asymmetric robustness, with false positives harder to find than false negatives. The method surfaces actionable insights for probe hardening and suggests probes should not serve as sole control points in high-incentive settings.

## Method Summary
The method wraps an off-the-shelf LLM with iterative feedback and in-context learning to red-team activation probes without requiring fine-tuning, gradients, or architectural access. An attacker LLM generates candidate adversarial samples, which a black-box probe scores; an independent judge determines ground-truth labels and returns structured feedback (success/failure + reasons) to the attacker. The full history is preserved for in-context learning, with each run starting cold. Tested across 20 rounds with batch size 5, the approach discovers failure patterns through systematic exploration of the probe's decision boundary, with results aggregated across multiple runs to account for strategy fixation and variability.

## Key Results
- GPT-5 attacker achieves up to 88.0% false positive and 63.6% false negative failure rates on high-stakes probes
- Smaller models (27B–120B) also find failure cases but with lower rates and less stable learning
- Interpretable failure patterns emerge: legal boilerplate triggers false positives; procedural tone evades detection
- Scenario-constrained attacks show asymmetric robustness: false positives (3% success) much harder to find than false negatives (69.4% success)

## Why This Works (Mechanism)

### Mechanism 1
Iterative feedback with in-context learning enables discovery of probe failure patterns without weight updates. The attacker LLM generates candidates → probe scores them → judge determines ground truth → structured feedback returns to attacker → full history preserved for ICL → attacker refines strategy over rounds. Core assumption: attacking LLM has sufficient ICL capacity to identify and exploit systematic decision boundary gaps from textual feedback alone. Evidence: "wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access" [abstract]. Break condition: if probe's decision boundary has no learnable structure expressible in natural language.

### Mechanism 2
Probes exhibit interpretable semantic sensitivities to linguistic register and framing, not just content. Activation probes learn spurious correlations between surface features (legalese, procedural tone) and class labels → red-teamer discovers these by varying phrasing while holding content constant → probe misclassifies based on register rather than semantic stakes. Core assumption: probe training data contains systematic biases that probe architecture overfits to surface linguistic features. Evidence: "interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs)" [abstract]; Table 2 shows "Describing admin tasks with important implications in a bland, procedural way" produces false negatives. Break condition: if probe architecture incorporates robust semantic features invariant to linguistic register.

### Mechanism 3
Scenario constraints reduce attack success asymmetrically, with false positives harder to find than false negatives. Constrained generation narrows search space → attacker must find failures within domain-appropriate contexts → domains have varying inherent stakes signal strength → probe robustness varies by domain due to training data distribution. Core assumption: probe's learned decision boundary has domain-specific margins that are not uniformly vulnerable. Evidence: "Finding false positives for the 'misaligned' scenario achieves only 3% which is the lowest rate by a margin of over 15%" [Section 3.4]; FPR under constraints drops 35.8 percentage points for GPT-5 while FNR drops only 8.8 points. Break condition: if target domain has rich, well-represented training data with clear boundary cases.

## Foundational Learning

- **Activation Probes (Linear/Attention Classifiers on Internal States)**: Understanding that probes are simple classifiers operating on hidden activations—not full model outputs—explains why they can be fast/cheap but also brittle to distributional shifts. Quick check: Can you explain why a probe trained on model A's activations might not transfer to model B, even if both models have similar architectures?

- **In-Context Learning (ICL) for Strategy Refinement**: The method relies entirely on ICL to improve attack success over rounds; understanding ICL's limitations (context length, pattern complexity) predicts when this approach will fail. Quick check: What happens to ICL-based strategy refinement if the failure patterns require more demonstrations than fit in context?

- **Black-Box vs. White-Box Adversarial Settings**: The paper explicitly contrasts this black-box method against white-box attacks (gradient-based, adversarial suffixes); knowing the distinction clarifies threat model assumptions and realistic deployment constraints. Quick check: Why would a deployed system only face black-box attacks in practice, and what does that imply about which failure modes matter most?

## Architecture Onboarding

- **Component map**: Attacker LLM -> Probe under test -> Judge LLM -> Feedback loop -> Attacker context
- **Critical path**: Initialize attacker with task description → for each round: attacker generates batch → probe scores → judge labels → feedback assembled → append to attacker context → after R rounds: synthesize strategies → manual review of results + summary
- **Design tradeoffs**: GPT-5 finds ~88% FPR but is proprietary; 27B open-source models find ~20% FPR with high variance. Choose based on threat model realism vs. cost. Paper recommends multiple short runs over one long run (models fixate on early strategies). Trade compute for exploration diversity. Judge accuracy vs. cost: LLM judges can disagree with human intuition on borderline cases. Stricter thresholds or human spot-checks improve reliability.
- **Failure signatures**: JSONL parsing errors (GPT-OSS-120B ~30%; Gemma-3-27B up to 14%). Monitor error rates separately. Strategy fixation: models often lock onto one approach by round 2-3; manifests as plateaued failure rates. Solution: multiple independent runs. Judge-probe misalignment: if judge labels disagree with intended concept definition, failure rates conflate probe errors with labeling errors.
- **First 3 experiments**: 1) Baseline unconstrained attack: Run 5 replicates of 20-round FN + FP attacks on your probe with GPT-4-class attacker. Establish baseline failure rates and variability. 2) Scenario-constrained stress test: Select 3 deployment-relevant domains. Run constrained attacks; compare FNR/FPR asymmetry to identify which direction your probe is more vulnerable in practice. 3) Cross-probe transfer: Apply discovered failure patterns (e.g., procedural framing, legal boilerplate) as test cases to a second probe architecture (attention vs. softmax). Assess whether vulnerabilities are probe-specific or systematic.

## Open Questions the Paper Calls Out

- Do activation probes on larger and more capable models still exhibit interpretable failure modes under black-box adversarial pressure? Basis: "It is unclear whether activation probes on larger and more capable models still exhibit interpretable failure modes, which is something worth investigating empirically." [explicit] Why unresolved: Study only tested probes on Llama-3.3-70B and Qwen3-8B; scaling behavior remains unknown. What evidence would resolve it: Running same red-teaming scaffold on probes trained for frontier models (>70B parameters) and analyzing whether failure patterns remain interpretable.

- Can discovered failure cases be systematically used to improve probe robustness through re-training or fine-tuning? Basis: "We want to analyze whether the discovered failure [cases] are useful for improving the probes. We envision an automated probe improvement pipeline which repeats cycles of red-teaming to find failure cases and then fixing the discovered failure cases by re-training or fine-tuning the probes accordingly." [explicit] Why unresolved: Paper surfaces failure patterns but does not implement or evaluate a hardening loop. What evidence would resolve it: An iterative pipeline where red-teaming outputs augment training data, with measured improvements in probe AUROC and reduced failure rates after each cycle.

- Does this red-teaming approach transfer effectively to other probe concepts beyond high-stakes detection? Basis: "We only red-team probes for a single concept... probes can differ in a number of ways and our approach might be less useful for some of them. In particular, for certain model-centric concepts one might be primarily interested in on-policy samples, which can make the red-teaming task more challenging." [explicit] Why unresolved: Only high-stakes probes were tested; generalizability to concepts like deception or situational awareness is unverified. What evidence would resolve it: Applying the scaffold to diverse probe types (e.g., deception, capability elicitation) and comparing failure discovery rates and pattern interpretability.

## Limitations
- Method's effectiveness beyond high-stakes interaction probes remains unverified across different probe architectures and datasets
- Reliance on powerful judge LLM introduces uncertainty if judge's interpretation diverges from human expectations
- Lack of reported sampling parameters (temperature, top-p) makes reproducibility under different generation settings difficult

## Confidence

- **High confidence**: Black-box red-teaming via iterative feedback and ICL can discover probe failure patterns (supported by empirical results across multiple attacker models and consistent improvement over rounds)
- **Medium confidence**: Scenario-constrained attacks showing asymmetric vulnerability (supported by specific experiments but requires validation across other domains and probe types)
- **Low confidence**: Probes should not serve as sole control points in high-incentive settings (extrapolates from failure rates without modeling actual incentive structures)

## Next Checks

1. **Cross-probe transferability test**: Apply discovered failure patterns (procedural framing, legal boilerplate) as test cases to a third, independently trained probe on the same dataset but different architecture (e.g., MLP vs. attention). Measure whether failure rates remain comparable, indicating systematic vulnerabilities rather than probe-specific overfitting.

2. **Judge-accuracy calibration**: Run human-in-the-loop validation where domain experts independently label a random sample of 100 attack samples (50 FPs, 50 FNs). Compute Cohen's kappa between judge LLM and human labels to quantify labeling reliability and determine what fraction of reported failures might be labeling artifacts.

3. **Cost-benefit deployment simulation**: Implement attacker-defender game where defender deploys probes with varying false negative rates (from 0% to 63.6%) and attacker has limited query budgets. Measure minimum false negative rate required for attacker to achieve target success probability, providing quantitative bounds on probe utility as control mechanism.