---
ver: rpa2
title: Model Correlation Detection via Random Selection Probing
arxiv_id: '2509.24171'
source_url: https://arxiv.org/abs/2509.24171
tags:
- prefixes
- qwen2
- table
- correlation
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Random Selection Probing (RSP), a hypothesis-testing
  framework for detecting model correlation between large language models (LLMs) and
  vision-language models (VLMs). RSP optimizes prefixes on a reference model to maximize
  the probability of generating a specific token for a random selection task, then
  tests transferability to a target model, producing statistically rigorous p-values.
---

# Model Correlation Detection via Random Selection Probing

## Quick Facts
- arXiv ID: 2509.24171
- Source URL: https://arxiv.org/abs/2509.24171
- Reference count: 27
- Key outcome: Introduces Random Selection Probing (RSP), a statistically rigorous framework for detecting model correlation between LLMs and VLMs through optimized prefix testing with extremely small p-values (e.g., 1.00×10⁻³⁰⁰)

## Executive Summary
This paper presents Random Selection Probing (RSP), the first statistically rigorous framework for detecting model correlation between large language models and vision-language models. The approach optimizes prefixes on a reference model to maximize generation of specific tokens for random selection tasks, then tests transferability to target models, producing statistically valid p-values. Experiments demonstrate RSP's effectiveness across diverse access conditions including gradient, logits, gray-box, and black-box settings, with consistent detection of correlated models while maintaining high p-values for unrelated ones. The work addresses a critical need for provenance verification in modern machine learning ecosystems.

## Method Summary
RSP works by first optimizing a prefix on a reference model to maximize the probability of generating a specific token for a random selection task. This optimized prefix is then tested on a target model to measure transferability. The method employs statistical hypothesis testing to produce rigorous p-values that indicate the likelihood of correlation between models. The framework is designed to work under various access conditions, from full gradient access to complete black-box scenarios, making it applicable across different practical settings. The approach leverages the principle that correlated models will share similar internal representations and decision boundaries, which can be detected through systematic prefix optimization and testing.

## Key Results
- RSP consistently yields extremely small p-values (e.g., 1.00×10⁻³⁰⁰) for correlated models across multiple model types and access scenarios
- The method maintains high p-values for unrelated models, demonstrating strong discriminative capability
- Ablation studies confirm robustness to prefix length, mutation probability, and resolution settings
- RSP represents the first principled statistical approach for model correlation detection in ML ecosystems

## Why This Works (Mechanism)
The mechanism relies on the observation that correlated models share similar internal representations and decision boundaries. By optimizing prefixes to maximize specific token generation on a reference model, RSP creates probes that are likely to transfer to correlated target models. The statistical testing framework provides rigorous p-values that quantify the strength of correlation. The approach works across different access levels by adapting the optimization strategy to available information (gradients, logits, or only inputs/outputs). The transferability of optimized prefixes serves as evidence of underlying model similarity, with successful transfer indicating correlation and failure suggesting independence.

## Foundational Learning

**Statistical Hypothesis Testing**: Why needed - Provides rigorous framework for determining significance of correlation detection; Quick check - Verify p-values follow expected distributions under null hypothesis

**Prefix Optimization**: Why needed - Creates effective probes for detecting model similarity; Quick check - Measure improvement in token generation probability after optimization

**Random Selection Tasks**: Why needed - Provides challenging yet controlled evaluation scenarios; Quick check - Confirm task difficulty across different model pairs

**Transferability Analysis**: Why needed - Core mechanism for detecting correlation between models; Quick check - Measure success rate of prefix transfer across known correlated and uncorrelated pairs

## Architecture Onboarding

**Component Map**: Prefix Optimizer -> Transferability Tester -> Statistical Analyzer -> Correlation Detector

**Critical Path**: Prefix optimization on reference model → Prefix application to target model → Statistical hypothesis testing → Correlation determination

**Design Tradeoffs**: 
- Longer prefixes provide more information but increase optimization complexity
- More optimization iterations improve detection accuracy but increase computational cost
- Stricter statistical thresholds reduce false positives but may miss weak correlations

**Failure Signatures**: 
- High p-values for known correlated models indicate optimization failure
- Low p-values for unrelated models suggest false positive detections
- Computational timeouts indicate scalability issues with prefix length

**First Experiments**:
1. Test RSP on a pair of fine-tuned models from the same base model to establish baseline performance
2. Evaluate detection capability on models trained on completely unrelated datasets
3. Measure p-value distributions under null hypothesis (unrelated models) to validate statistical framework

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations

**Independence Assumption Concerns**: The statistical framework assumes independence between prefix tokens, which may be violated in practice, particularly for shorter prefix lengths where extremely small p-values are observed.

**Architecture Generalization**: The approach's effectiveness for detecting correlation between models with fundamentally different architectures (e.g., decoder-only vs. encoder-decoder transformers) remains unexplored.

**Dataset Similarity Requirements**: The method's performance for detecting correlation between models trained on highly dissimilar datasets has not been thoroughly evaluated.

## Confidence

**High Confidence**: Experimental methodology, statistical framework, and implementation details are well-documented and reproducible. Comparative results against baselines demonstrate clear performance advantages.

**Medium Confidence**: Generalizability across diverse model architectures and training paradigms requires additional validation. Scalability to extremely large model populations needs further investigation.

**Medium Confidence**: While promising for fine-tuned models, effectiveness for detecting correlation in extensively pretrained foundation models requires thorough evaluation.

## Next Checks

1. Conduct experiments testing RSP's effectiveness on models trained on completely unrelated datasets (e.g., medical vs. legal text) to establish detection limits.

2. Evaluate the approach's performance when detecting correlation between models with substantially different architectures (e.g., decoder-only vs. encoder-decoder transformers).

3. Perform stress tests on the independence assumption by systematically varying prefix lengths and measuring the correlation between prefix token selections across multiple runs.