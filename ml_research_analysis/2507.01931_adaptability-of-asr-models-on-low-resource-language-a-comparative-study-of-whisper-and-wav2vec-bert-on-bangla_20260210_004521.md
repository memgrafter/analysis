---
ver: rpa2
title: 'Adaptability of ASR Models on Low-Resource Language: A Comparative Study of
  Whisper and Wav2Vec-BERT on Bangla'
arxiv_id: '2507.01931'
source_url: https://arxiv.org/abs/2507.01931
tags:
- whisper
- speech
- wav2vec-bert
- bangla
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares OpenAI\u2019s Whisper and Facebook\u2019s\
  \ Wav2Vec-BERT for Bangla automatic speech"
---

# Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla

## Quick Facts
- arXiv ID: 2507.01931
- Source URL: https://arxiv.org/abs/2507.01931
- Reference count: 22
- Primary result: Wav2Vec-BERT achieved 14.42% WER vs Whisper Large-v2's 28.86% WER on Bangla ASR

## Executive Summary
This study compares OpenAI's Whisper and Facebook's Wav2Vec-BERT for Bangla automatic speech recognition under low-resource conditions. The research finds that Wav2Vec-BERT outperforms Whisper across all key evaluation metrics while requiring fewer computational resources. The study uses a combined dataset of Mozilla Common Voice-17 and OpenSLR Bangla, evaluating performance across multiple training set sizes (2k-70k samples). The results demonstrate that self-supervised pre-training provides significant advantages for low-resource language ASR tasks.

## Method Summary
The study fine-tuned both models on a combined Bangla dataset of approximately 94 hours of speech. Audio was preprocessed to 16kHz mono WAV format, with text normalized by expanding abbreviations and converting numerals to Bangla words. Wav2Vec-BERT used raw waveforms normalized [-1,1] while Whisper used 80-channel log-Mel spectrograms. Models were evaluated using Word Error Rate (WER) and Character Error Rate (CER) metrics. Training configurations varied across learning rates, epochs, and batch sizes, with hardware including RTX 4090 (24GB) and RTX 3060 (12GB) GPUs.

## Key Results
- Wav2Vec-BERT achieved 14.42% WER and 2.67% CER on 70k-sample training set
- Whisper Large-v2 achieved 28.86% WER and 7.47% CER on same dataset
- Wav2Vec-BERT showed lower computational requirements and faster training times

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pre-training enables superior low-resource adaptation compared to fully supervised approaches.
- Mechanism: Wav2Vec-BERT leverages unlabeled speech data to learn acoustic patterns through masked prediction, requiring minimal labeled data for fine-tuning. The model learns robust audio representations by reconstructing masked speech segments during pre-training, transferring these representations to the target language.
- Core assumption: The self-supervised representations learned during pre-training transfer effectively to Bangla phonetics and orthographic patterns.
- Evidence anchors:
  - [abstract] "Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources"
  - [section II.B] "By combining the strengths of Wav2Vec 2.0 and BERT, it significantly improves speech recognition, especially for low-resource languages where labeled data is limited"
  - [corpus] Related work on Whisper-LM and Akan ASR benchmarking shows consistent patterns of self-supervised models performing well in low-resource settings, though direct causal attribution remains underexplored
- Break condition: If labeled Bangla data exceeds ~100k high-quality hours, the self-supervised advantage may diminish as supervised approaches can leverage direct supervision.

### Mechanism 2
- Claim: Bidirectional context modeling reduces phoneme-level confusion in complex orthographic systems.
- Mechanism: Wav2Vec-BERT's bidirectional Transformer captures dependencies across the entire sequence rather than left-to-right context only. This enables better disambiguation of context-sensitive Bangla phonemes like nasal consonants (ন vs ণ) that depend on surrounding phonetic context.
- Core assumption: Bidirectional context provides discriminative information for Bangla's conjunct characters and diacritics that unidirectional or encoder-decoder architectures miss.
- Evidence anchors:
  - [section II.B] "allowing it to capture dependencies across the entire sequence instead of just left-to-right context"
  - [section IV.D] Error analysis shows Wav2Vec-BERT had fewer fricative and aspirated/unaspirated errors compared to Whisper
  - [corpus] Insufficient corpus evidence directly comparing bidirectional vs unidirectional mechanisms in low-resource ASR
- Break condition: If inference latency requirements prohibit bidirectional processing, this mechanism becomes inapplicable.

### Mechanism 3
- Claim: Conformer-based adapters provide computational efficiency gains over pure Transformer encoder-decoder architectures.
- Mechanism: Wav2Vec-BERT uses Conformer-based adapter networks combining CNNs with Transformers, while Whisper relies on dense Transformer blocks with attention mechanisms throughout. The convolutional components reduce sequence length before self-attention, lowering memory requirements.
- Core assumption: The efficiency gains stem from architectural differences rather than solely from model size differences.
- Evidence anchors:
  - [section IV.A] "A key advantage of Wav2Vec-BERT is its efficient utilization of computing resources. It successfully completed training on both lower-end and high-end PCs without encountering memory constraints"
  - [section IV.B] "Whisper model's extensive VRAM requirements are mainly due to its detailed attention mechanisms and dense layers"
  - [corpus] Weak corpus evidence; related papers focus on accuracy rather than computational mechanism analysis
- Break condition: If GPU memory exceeds 32GB VRAM, the efficiency difference becomes practically irrelevant for single-model deployment.

## Foundational Learning

- Concept: **Self-supervised learning for speech**
  - Why needed here: Understanding how Wav2Vec-BERT learns from unlabeled audio through masked prediction explains its data efficiency advantage over Whisper's supervised approach.
  - Quick check question: Can you explain why masking portions of audio and predicting them creates useful representations without any transcription?

- Concept: **WER vs CER metrics**
  - Why needed here: The paper reports both metrics because Bangla's agglutinative morphology makes word boundaries less informative; CER captures character-level accuracy critical for complex orthography.
  - Quick check question: Why might a model have low WER but high CER for a language with compound characters like Bangla's Juktakkhor?

- Concept: **Fine-tuning hyperparameter sensitivity**
  - Why needed here: Table I shows Wav2Vec-BERT's WER jumped from 14.42% to 72.31% when epochs increased from 8 to 15 with higher learning rate, demonstrating catastrophic overfitting risk.
  - Quick check question: What would happen if you applied the same learning rate schedule from Whisper (1e-5) to Wav2Vec-BERT without adjustment?

## Architecture Onboarding

- Component map:
  Raw Audio (16kHz) → Wav2Vec-BERT Path: CNN Encoder → Latent Z → Conformer Adapter → Bidirectional Transformer → Contextualized C → Linear Head → Token Predictions; Whisper Path: Log-Mel Spectrogram (80-channel) → CNN + Sinusoidal PE → Transformer Encoder → Transformer Decoder → Token Generation

- Critical path:
  1. Data preprocessing: Resample to 16kHz mono, normalize text (expand abbreviations, convert numerals to Bangla words)
  2. Tokenization: Wav2Vec-BERT uses subword tokenization; Whisper requires language token prefix (`<|bn|>`)
  3. Fine-tuning: Start with learning rate 1e-5 for Whisper, 3e-5 for Wav2Vec-BERT; use 500 warmup steps
  4. Evaluation: Compute WER and CER on held-out test set

- Design tradeoffs:
  - **Wav2Vec-BERT**: Lower VRAM (fits 12GB), faster training (~13h vs 22h for 70k samples), lower WER (14.42% vs 28.86%) but requires careful epoch tuning to avoid overfitting
  - **Whisper Large-v2**: Better zero-shot capability out-of-box, scales better with more data (continued improvement at 70k), but requires 24GB+ VRAM and 2x training time
  - **Whisper Small**: Memory-constrained option but struggles with Bangla (32%+ WER); not recommended for production

- Failure signatures:
  - WER > 70% with Wav2Vec-BERT: Likely overfitting from excessive epochs (15+) or learning rate too high (5e-5+)
  - OOM errors with Whisper on datasets >20k samples: Reduce batch size or use gradient accumulation every 4 steps
  - Numeral transcription errors (123 → incorrect Bangla): Text normalization step missing or inconsistent

- First 3 experiments:
  1. **Baseline replication**: Fine-tune Wav2Vec-BERT on 8k subset with LR 1e-5, 10 epochs. Target: WER < 20%. If achieved, confirms environment setup.
  2. **Data scaling test**: Train on 2k, 8k, 20k subsets sequentially. Plot WER curve. Expect diminishing returns after 40k per Figure 4.
  3. **Hyperparameter sensitivity**: At 20k samples, test LR {1e-5, 3e-5, 5e-5} for 8 epochs. Identify regime where overfitting begins (watch for WER spike).

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Composition Uncertainty: The study combines Mozilla Common Voice-17 and OpenSLR Bangla datasets but provides limited detail on exact training-validation-test splits and how samples were prioritized when creating subsets.
- Hyperparameter Specification Gaps: Several critical training details are missing including optimizer type, weight decay values, exact batch sizes per GPU, and random seeds for data splitting.
- Architecture Mechanism Evidence: The paper claims Wav2Vec-BERT's Conformer-based adapter provides computational efficiency advantages, but the evidence remains largely observational rather than mechanistic.

## Confidence
- **High Confidence**: Comparative performance results showing Wav2Vec-BERT outperforming Whisper on Bangla ASR tasks (14.42% vs 28.86% WER on 70k samples)
- **Medium Confidence**: Claims about bidirectional context modeling providing discriminative advantages for Bangla orthography
- **Low Confidence**: The mechanism attributing computational efficiency gains to Conformer-based adapters versus Transformer attention mechanisms

## Next Checks
1. **Split Consistency Validation**: Recreate the exact 70k-sample training set using the same prioritization criteria (unique prompts) and verify the WER results match within ±2% tolerance. Test whether random seeds significantly affect performance, particularly the overfitting phenomenon observed when increasing epochs from 8 to 15.

2. **Mechanism Isolation Experiment**: Train a modified Wav2Vec-BERT with unidirectional context modeling while keeping all other components constant. Compare WER and CER on Bangla test set to quantify the actual contribution of bidirectional processing to the observed performance gains, particularly for phoneme disambiguation tasks.

3. **Efficiency Attribution Analysis**: Conduct controlled experiments varying only the adapter architecture (Conformer vs pure Transformer) while holding model size, dataset, and training configuration constant. Measure VRAM usage, training time, and accuracy to determine whether efficiency gains are truly architecture-driven or primarily size-related.