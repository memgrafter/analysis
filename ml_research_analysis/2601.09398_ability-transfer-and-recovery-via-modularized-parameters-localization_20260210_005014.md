---
ver: rpa2
title: Ability Transfer and Recovery via Modularized Parameters Localization
arxiv_id: '2601.09398'
source_url: https://arxiv.org/abs/2601.09398
tags:
- layer
- activation
- channels
- ability
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACT identifies ability-relevant model parameters by analyzing cross-model
  activation differences, finding that ability-specific activations are concentrated
  in sparse, disentangled channels (<5%) that remain stable under fine-tuning. It
  transfers only these channels via masked task-vector merging and applies lightweight
  post-transfer fine-tuning for compatibility.
---

# Ability Transfer and Recovery via Modularized Parameters Localization

## Quick Facts
- arXiv ID: 2601.09398
- Source URL: https://arxiv.org/abs/2601.09398
- Reference count: 40
- Primary result: ACT identifies <5% of parameters for ability transfer and merges specialized models with minimal interference

## Executive Summary
ACT introduces a method for identifying and transferring ability-relevant parameters between language models by analyzing cross-model activation differences. The approach finds that ability-specific activations are concentrated in sparse, disentangled channels (<5%) that remain stable under fine-tuning. These channels can be transferred via masked task-vector merging with lightweight post-transfer fine-tuning for compatibility. The method successfully recovers forgotten abilities while preserving retained skills, and can merge multiple specialized models with minimal interference across multilingual math and science reasoning tasks.

## Method Summary
ACT identifies ability-relevant parameters by comparing activation patterns across models during task execution, locating sparse channels (<5%) that are both ability-specific and disentangled from other abilities. These channels are transferred using a masked task-vector merging approach that applies parameter updates only to the identified channels while leaving the rest of the model unchanged. A lightweight fine-tuning phase ensures compatibility between the transferred parameters and the target model's existing parameters. The method can handle both ability recovery in models that have undergone catastrophic forgetting and integration of multiple specialized models into a single unified model.

## Key Results
- ACT recovers forgotten science abilities in 9 languages while maintaining strong math performance on Qwen2.5 models
- Outperforms baseline merging methods with fewer parameters and less interference
- Successfully integrates English math, science, and code abilities from separate models, achieving best overall performance

## Why This Works (Mechanism)
ACT works by exploiting the modular nature of learned abilities within transformer models. When abilities are acquired or forgotten, the changes in activation patterns are localized to specific channels rather than distributed across the entire model. By comparing activation differences between models with and without specific abilities, ACT can isolate the parameters responsible for those abilities. These parameters tend to be both sparse (concentrated in <5% of channels) and disentangled (not shared with other abilities), making them ideal candidates for selective transfer without disrupting other model capabilities.

## Foundational Learning
- Cross-model activation analysis: Comparing activation patterns between models to identify parameter differences responsible for specific abilities. Needed to locate ability-relevant parameters without requiring full model access or retraining.
- Parameter sparsity: The observation that ability-relevant parameters occupy a small fraction (<5%) of total parameters. Critical for efficient transfer and minimizing interference with existing capabilities.
- Disentanglement: Ability-specific parameters are largely independent from parameters governing other abilities. Essential for clean transfer without cross-ability contamination.
- Masked task-vector merging: A parameter update strategy that applies changes only to identified channels. Enables selective transfer while preserving existing model capabilities.
- Post-transfer fine-tuning: Lightweight compatibility adjustment after parameter transfer. Ensures smooth integration of new abilities with existing model parameters.

## Architecture Onboarding
**Component Map:** Activation Difference Analysis -> Parameter Identification -> Masked Merging -> Fine-tuning Compatibility
**Critical Path:** Activation analysis identifies target channels → Masked merging transfers parameters → Fine-tuning ensures compatibility → Performance evaluation validates retention
**Design Tradeoffs:** ACT prioritizes parameter efficiency and minimal interference over full model fine-tuning, trading some potential performance gains for computational efficiency and ability preservation
**Failure Signatures:** Poor activation difference analysis may identify incorrect channels; inadequate disentanglement leads to ability interference; insufficient fine-tuning causes compatibility issues
**First Experiments:**
1. Compare ACT performance against full fine-tuning on a simple ability transfer task
2. Test parameter sparsity claims across different ability types and model architectures
3. Evaluate long-term stability of transferred abilities after multiple fine-tuning cycles

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Method may not generalize well beyond mathematical and scientific reasoning tasks to domains like creative writing or domain-specific knowledge
- Uncertainty about scalability to larger model architectures beyond Qwen2.5
- Practical limitations when integrating more than three specialized models or when abilities have significant functional overlap

## Confidence
High: Core activation difference analysis methodology is sound and empirically validated
Medium: Parameter sparsity and stability claims may not generalize across all ability types
Medium: Multilingual performance recovery well-supported but may not extend to other language pairs
Medium: Model merging capabilities promising but may face limitations with complex integration scenarios

## Next Checks
1. Test ACT on non-mathematical domains (code generation, creative writing) to verify cross-domain generalizability
2. Conduct longitudinal studies on stability of transferred abilities over multiple fine-tuning cycles
3. Scale experiments to larger models and test integration of more than three specialized models simultaneously