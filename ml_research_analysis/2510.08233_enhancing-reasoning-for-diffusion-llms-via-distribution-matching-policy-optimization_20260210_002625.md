---
ver: rpa2
title: Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization
arxiv_id: '2510.08233'
source_url: https://arxiv.org/abs/2510.08233
tags:
- arxiv
- preprint
- diffusion
- policy
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution Matching Policy Optimization
  (DMPO), a novel reinforcement learning framework specifically designed for diffusion
  large language models (dLLMs). The key challenge addressed is adapting RL algorithms
  for dLLMs, which differ from autoregressive models in their bidirectional generation
  and forward denoising process.
---

# Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization

## Quick Facts
- arXiv ID: 2510.08233
- Source URL: https://arxiv.org/abs/2510.08233
- Reference count: 40
- Key result: Achieved 42.9% accuracy improvement over best previous dLLM RL method and 55.8% over base model on reasoning benchmarks

## Executive Summary
This paper introduces Distribution Matching Policy Optimization (DMPO), a novel reinforcement learning framework specifically designed for diffusion large language models (dLLMs). The key innovation addresses the fundamental challenge of adapting RL algorithms for dLLMs, which differ from autoregressive models in their bidirectional generation and forward denoising process. DMPO shifts from reward maximization to distribution matching, using importance sampling and weighted denoising cross-entropy loss to match the policy distribution to the optimal, reward-tilted one. Experiments on four reasoning benchmarks demonstrate substantial performance improvements over existing state-of-the-art baselines.

## Method Summary
DMPO reformulates RL for dLLMs as a distribution matching problem rather than traditional reward maximization. The method uses importance sampling to estimate rewards and employs a weighted denoising cross-entropy (WDCE) loss function to match the policy distribution to the optimal reward-tilted distribution. A critical technical challenge with small batch sizes is addressed through weight baseline subtraction techniques. The framework is evaluated on GSM8K, MATH500, Countdown, and Sudoku benchmarks, demonstrating consistent improvements over previous state-of-the-art methods for dLLMs.

## Key Results
- Achieved 42.9% accuracy improvement over the best previous dLLM RL method
- Demonstrated 55.8% improvement over the base model on reasoning tasks
- Showed particular strength in planning tasks like Countdown and Sudoku
- Performed consistently across four different reasoning benchmarks without requiring supervised fine-tuning

## Why This Works (Mechanism)
DMPO's effectiveness stems from its distribution matching objective, which directly optimizes the policy to match the reward-tilted optimal distribution. Unlike traditional RL approaches that maximize expected rewards, this framework uses weighted denoising cross-entropy loss to align the learned policy with the target distribution. The importance sampling technique enables proper reward estimation in the bidirectional generation setting of dLLMs, while the weight baseline subtraction addresses the small batch size challenge that typically plagues RL training.

## Foundational Learning

**Diffusion Models**
- Why needed: Understanding bidirectional generation and forward denoising process that distinguishes dLLMs from autoregressive models
- Quick check: Can explain how denoising works in both directions and why this complicates RL adaptation

**Reinforcement Learning for Language Models**
- Why needed: Background on traditional reward maximization approaches and their limitations when applied to dLLMs
- Quick check: Can contrast reward maximization vs distribution matching objectives

**Importance Sampling**
- Why needed: Critical for estimating rewards in the non-autoregressive generation setting
- Quick check: Can explain how importance sampling enables proper reward estimation despite bidirectional generation

## Architecture Onboarding

**Component Map**
DMPO Framework -> Distribution Matching Objective -> Weighted DENOISING Cross-Entropy Loss -> Importance Sampling Reward Estimation -> Weight Baseline Subtraction -> Policy Update

**Critical Path**
1. Generate trajectories using current policy
2. Compute importance sampling weights for reward estimation
3. Calculate weighted denoising cross-entropy loss
4. Apply weight baseline subtraction for small batch stability
5. Update policy parameters

**Design Tradeoffs**
- Distribution matching vs reward maximization: Trade simplicity of optimization for potential suboptimality guarantees
- Small batch handling: Required weight baseline subtraction technique, adding complexity but enabling stable training

**Failure Signatures**
- Unstable training: Indicates improper weight baseline subtraction implementation
- Mode collapse: Suggests distribution matching objective not properly calibrated
- Reward overestimation: Points to importance sampling weight calculation errors

**3 First Experiments**
1. Verify baseline subtraction effectiveness by comparing training stability with and without this technique
2. Test distribution matching objective on simple synthetic tasks before scaling to reasoning benchmarks
3. Validate importance sampling implementation by checking reward estimation consistency across different batch sizes

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Small batch size handling required ad-hoc weight baseline subtraction, with uncertain robustness across different architectures
- Theoretical convergence guarantees for the distribution matching objective are not fully established
- Performance on non-reasoning tasks or tasks requiring long-term memory remains unevaluated

## Confidence

**Major Claim Clusters Confidence:**
- Performance improvements over baselines (High): Experimental results show consistent and substantial accuracy gains across multiple benchmarks with proper baseline comparisons
- Distribution matching framework effectiveness (Medium): Results are promising, but theoretical underpinnings of why distribution matching outperforms reward maximization for dLLMs could benefit from more rigorous analysis
- Weight baseline technique necessity (Medium): Technique appears effective, but alternative solutions to small batch size problem were not explored or compared

## Next Checks
1. Test DMPO on non-reasoning tasks (e.g., creative writing, summarization) to assess broader applicability beyond reasoning-focused benchmarks
2. Conduct ablation studies removing the weight baseline subtraction to quantify its contribution to overall performance
3. Evaluate DMPO with larger batch sizes (when computationally feasible) to determine if distribution matching objective alone can achieve similar performance gains without the weight baseline workaround