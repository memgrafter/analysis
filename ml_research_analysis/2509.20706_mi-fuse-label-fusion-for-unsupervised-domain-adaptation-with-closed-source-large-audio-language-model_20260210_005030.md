---
ver: rpa2
title: 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source
  Large-Audio Language Model'
arxiv_id: '2509.20706'
source_url: https://arxiv.org/abs/2509.20706
tags:
- lalm
- teacher
- domain
- emotion
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MI-Fuse enables adaptation of speech emotion recognition systems
  to new domains using only unlabeled target data and a closed-source large audio-language
  model (LALM) through a denoised label fusion approach. It combines the LALM with
  a source-domain trained classifier, leveraging mutual-information-based uncertainty
  weighting and an exponential moving average teacher to stabilize training and suppress
  noisy pseudo-labels.
---

# MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model

## Quick Facts
- arXiv ID: 2509.20706
- Source URL: https://arxiv.org/abs/2509.20706
- Reference count: 0
- MI-Fuse achieves 3.9% accuracy gain over strongest baseline in source-free domain adaptation for speech emotion recognition

## Executive Summary
MI-Fuse enables adaptation of speech emotion recognition systems to new domains using only unlabeled target data and a closed-source large audio-language model (LALM) through a denoised label fusion approach. It combines the LALM with a source-domain trained classifier, leveraging mutual-information-based uncertainty weighting and an exponential moving average teacher to stabilize training and suppress noisy pseudo-labels. Experiments on three emotion datasets and six cross-domain transfers show the student model consistently outperforms both the LALM and existing source-free adaptation methods, achieving a 3.9% accuracy gain over the strongest baseline. This method enables practical, privacy-compliant emotion-aware speech systems without sharing source data.

## Method Summary
MI-Fuse is a source-free unsupervised domain adaptation framework for speech emotion recognition that leverages a closed-source large audio-language model (LALM) without access to source domain data. The method employs a student-teacher architecture where the student model (WavLM+linear layers) is trained using pseudo-labels generated by fusing predictions from two teachers: the LALM (Gemini 2.5 flash) and a source-trained classifier. Both teachers generate multiple stochastic predictions (N_LM=5 for LALM, N_cls=8 MC dropout passes for classifier) to estimate epistemic uncertainty via mutual information. The fused predictions are weighted by e^(-MI) to emphasize more reliable teachers, and an exponential moving average teacher provides stable supervision during training. The student is trained with cross-entropy loss plus a diversity loss term to prevent class collapse.

## Key Results
- Student model achieves 58.38% average accuracy across six cross-domain transfers, outperforming LALM-only baseline (54.48%) by 3.9%
- MI-based uncertainty weighting (59.09%, 57.07%) significantly outperforms entropy weighting (57.34%, 55.53%) and equal weighting (57.98%, 56.64%) on IMP→IEM and POD→IEM transfers
- EMA teacher stabilization prevents the accuracy decline seen in non-EMA baselines after 400 training steps
- MI-Fuse consistently outperforms all baselines including direct fusion (59.09% vs 55.23% on IMP→IEM) and classifier-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighting teacher predictions by inverse mutual information reduces noise from uncertain pseudo-labels during domain adaptation.
- **Mechanism:** Multiple stochastic forward passes (MC dropout for classifier, temperature sampling for LALM) produce prediction distributions. The mutual information I(Y,Θ|x) = H(p̄) - (1/K)ΣH(p_k) captures epistemic uncertainty—how much predictions vary across passes. Weights ∝ e^(-MI) assign higher influence to teachers with more consistent outputs.
- **Core assumption:** Lower inter-pass variability indicates more reliable predictions under domain shift.
- **Evidence anchors:**
  - [abstract] "weights their mean distributions by mutual-information-based uncertainty"
  - [Section 4.3, Table 2] MI weighting (59.09%, 57.07%) outperforms entropy weighting (57.34%, 55.53%) and equal weighting (57.98%, 56.64%) on IMP→IEM and POD→IEM transfers
  - [corpus] No direct corpus evidence for MI-based fusion in LALM adaptation; mechanism is method-specific
- **Break condition:** If both teachers produce consistently wrong predictions (low MI but incorrect), the weighting will amplify systematic errors rather than random noise.

### Mechanism 2
- **Claim:** Fusing predictions from complementary teachers (LALM + domain classifier) provides more robust supervision than either teacher alone.
- **Mechanism:** The LALM provides strong zero-shot generalization but can be noisy; the source-trained classifier captures domain-specific patterns but degrades under shift. Direct fusion (Equation 7) combines their mean distributions without gating, allowing each to compensate for the other's weaknesses.
- **Core assumption:** Teachers make uncorrelated errors; when one is uncertain or wrong, the other may still be correct.
- **Evidence anchors:**
  - [abstract] "supplements the LALM with a source-domain trained SER classifier as an auxiliary teacher"
  - [Section 4.2, Table 1] MI-Fuse (58.38% avg) outperforms LALM-only SFUDA (54.48%) and classifier-only SFUDA (51.01%) by 3.9% and 7.4% respectively
  - [Section 4.3, Table 2] "No Fusion" baselines (56.08%, 55.43%) underperform fusion methods
  - [corpus] Related work [15-17] shows label ensembling benefits in SFUDA; this paper extends to LALM-classifier pairs
- **Break condition:** If teachers are systematically biased in the same direction (correlated errors), fusion provides no benefit.

### Mechanism 3
- **Claim:** EMA teacher updates stabilize training by smoothing supervision over time.
- **Mechanism:** The classifier teacher parameters θ_cls are updated as θ_cls ← α·θ_cls + (1-α)·θ_tgt with α=0.999. This filters short-term fluctuations in student predictions, providing consistent pseudo-labels across iterations.
- **Core assumption:** Temporal averaging reduces noise while preserving genuine learning signal.
- **Evidence anchors:**
  - [abstract] "stabilizes training with an exponential moving average teacher"
  - [Section 3.4] "ensures that the teacher evolves smoothly with the student, filtering out short-term noise"
  - [Section 4.4, Figure 2] Classifier teacher baseline "declines after ~400 steps" while MI-Fuse "steadily improves throughout training"
  - [corpus] Mean Teacher [16, cited in paper] establishes EMA benefits for semi-supervised learning
- **Break condition:** If α is too low, teacher changes too quickly and provides unstable supervision; if too high, teacher lags behind student's actual progress.

## Foundational Learning

- **Concept: Source-Free Unsupervised Domain Adaptation (SFUDA)**
  - Why needed here: The entire framework operates under the constraint that source data is unavailable (privacy/ownership) and target data is unlabeled. Understanding this motivates why pseudo-labeling and teacher-student architectures are necessary.
  - Quick check question: Can you explain why standard fine-tuning is impossible in this setting?

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: MI captures epistemic uncertainty (model disagreement) separately from aleatoric uncertainty (inherent ambiguity). This distinction determines why MI is used instead of total entropy for weighting.
  - Quick check question: If a model produces high-entropy predictions consistently across stochastic passes, which type of uncertainty dominates?

- **Concept: Monte Carlo Dropout for Bayesian Approximation**
  - Why needed here: The classifier teacher uses MC dropout at inference to generate multiple predictions. Understanding this requires knowing that dropout at test time approximates Bayesian inference.
  - Quick check question: How does MC dropout differ from standard dropout usage?

## Architecture Onboarding

- **Component map:**
  LALM (Gemini 2.5, API-only) -> N_LM=5 stochastic passes -> p̄_LM
                                                        |
                                                        v
                                                MI-weighted fusion -> p_fused -> Student training
                                                        ^
                                                        |
  Classifier (WavLM+Linear) -> N_cls=8 MC dropout passes -> p̄_cls
         ^                                                    |
         |                                                    |
         └────────── EMA update (α=0.999) ◄─── Student ◄──────┘

- **Critical path:** (1) Initialize student with classifier weights -> (2) Generate stochastic predictions from both teachers -> (3) Compute mean distributions and MI -> (4) Fuse with e^(-MI) weights -> (5) Train student with CE loss + diversity loss -> (6) Update classifier teacher via EMA

- **Design tradeoffs:**
  - N_LM=5, N_cls=8: More passes improve uncertainty estimates but increase API costs and latency
  - Direct fusion vs. KL gating: Ablation shows direct fusion better (59.09% vs. 55.23% on IMP→IEM); gating discards useful complementary signals
  - Temperature=0.6 for LALM: Controls stochasticity; higher temperature increases exploration but may reduce reliability

- **Failure signatures:**
  - Class collapse: Student overfits to subset of emotions -> mitigated by diversity loss L_div = -H(p̄_batch)
  - Training instability: Rapid accuracy oscillations -> check EMA momentum and learning rate
  - Stagnation after initial gains: Teacher predictions may be too noisy -> reduce N passes or increase fusion weighting toward more reliable teacher

- **First 3 experiments:**
  1. **Reproduce single transfer:** Train on IMP→IEM with default settings; expect ~59% accuracy per Table 1. If significantly lower, check MC dropout implementation and LALM prompt formatting.
  2. **Ablate MI weighting:** Replace MI weights with equal weights; expect ~2% drop (Table 2: 59.09→57.98 on IMP→IEM). This validates the uncertainty estimation pipeline.
  3. **Test single-teacher baseline:** Train with LALM only (no classifier fusion); expect ~48-54% accuracy depending on transfer direction. This establishes the contribution of complementary teachers.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MI-Fuse framework be extended to handle target domains with different emotion taxonomies (label spaces) than the source model and LALM?
- **Basis in paper:** [explicit] The Limitation section states, "the label fusion scheme assumes a fixed set of discrete emotion categories... emotion taxonomies may vary. This mismatch can hinder the direct applicability of MI-Fuse."
- **Why unresolved:** The current method computes mutual information and fuses distributions based on a shared, fixed set of classes Y={1, ..., C}, which breaks down if the target domain requires recognizing emotions not present in the source or LALM's original vocabulary.
- **What evidence would resolve it:** Successful adaptation results in experiments where the source domain uses a subset of emotions (e.g., 4-class) and the target domain uses a superset or entirely different schema (e.g., dimensional valence/arousal or 10-class), perhaps using a mapping layer or hierarchical fusion.

### Open Question 2
- **Question:** How can the framework be optimized to reduce the high inference cost and latency associated with multiple stochastic queries to the closed-source LALM?
- **Basis in paper:** [explicit] The Limitation section notes that "inference cost, latency, and reliance on proprietary APIs may hinder practical deployment in resource-constrained... settings."
- **Why unresolved:** The current methodology relies on querying the API N_LM=5 times per sample to estimate uncertainty, which is computationally and financially expensive for real-time or large-scale batch processing.
- **What evidence would resolve it:** A study evaluating the performance trade-off of using fewer samples (N_LM < 5), or the introduction of a caching/distillation mechanism that reduces API calls while maintaining the diversity needed for Mutual Information estimation.

### Open Question 3
- **Question:** To what extent is the performance of MI-Fuse dependent on the specific choice of LALM (e.g., Gemini 2.5) versus other audio-language models?
- **Basis in paper:** [inferred] The experiments section specifies, "We use Gemini 2.5 flash as the LALM studied," and mentions other models (Desta, Qwen) in the introduction but does not test them.
- **Why unresolved:** The effectiveness of the denoising and fusion relies on the zero-shot capabilities and uncertainty profiles of the specific LALM used; different models may have different hallucination rates or calibration errors that could break the MI-weighting assumption.
- **What evidence would resolve it:** Comparative experiments running the MI-Fuse pipeline with alternative LALMs (e.g., Qwen2.5-Omni or GPT-4o) to see if the 3.9% improvement over baselines holds consistently across different model backbones.

### Open Question 4
- **Question:** How sensitive is the LALM's uncertainty estimation to variations in prompt design?
- **Basis in paper:** [inferred] The methodology mentions using "carefully designed prompts" to query the LALM, but assumes the resulting probability distribution is a stable reflection of uncertainty.
- **Why unresolved:** If small changes in the prompt significantly alter the stochastic predictions or entropy of the LALM, the Mutual Information metric used for weighting may reflect prompt sensitivity rather than true epistemic uncertainty.
- **What evidence would resolve it:** An ablation study analyzing the variance in MI weights and final adaptation accuracy when using different semantically equivalent prompts for the same audio samples.

## Limitations

- The method requires 5 API calls per sample to the LALM, creating substantial computational costs that limit practical deployment
- Performance assumes teachers make uncorrelated errors; systematic bias in both teachers could amplify errors rather than reduce them
- The framework is limited to 4-class discrete emotion classification and may not generalize to dimensional emotion spaces or non-Western emotion expressions

## Confidence

- **High confidence**: The EMA teacher mechanism demonstrates consistent training stability improvements across all six cross-domain transfers
- **Medium confidence**: The complementary teacher fusion shows statistically significant improvements (3.9% accuracy gain) but the assumption of uncorrelated teacher errors requires further validation
- **Medium confidence**: The MI-based uncertainty weighting shows consistent improvements but the relationship between MI and prediction reliability under domain shift needs more rigorous theoretical grounding

## Next Checks

1. **Ablation study on teacher correlation**: Measure prediction agreement between LALM and classifier teachers on target validation data across all six transfers. Compute correlation coefficients and test whether fusion benefits persist when teachers are highly correlated (ρ > 0.8) versus weakly correlated (ρ < 0.4).

2. **Domain shift severity validation**: For each transfer pair, quantify domain shift using Wasserstein distance between source and target audio embeddings. Test whether MI-Fuse performance correlates with domain shift severity - specifically whether the method degrades gracefully as shift increases versus alternative approaches.

3. **Zero-shot LALM generalization test**: Evaluate Gemini 2.5 flash performance on held-out emotion classes not present in any training set (e.g., surprise, disgust). This validates whether the LALM's strong performance stems from genuine emotion understanding versus memorization of training emotions.