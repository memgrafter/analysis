---
ver: rpa2
title: 'Bayesian Models for Joint Selection of Features and Auto-Regressive Lags:
  Theory and Applications in Environmental and Financial Forecasting'
arxiv_id: '2508.10055'
source_url: https://arxiv.org/abs/2508.10055
tags:
- variables
- table
- selection
- partial
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a Bayesian framework for joint feature and auto-regressive
  lag selection in linear regression with autocorrelated errors, applicable to environmental
  and financial forecasting. The method uses hierarchical Bayesian models with spike-and-slab
  priors to simultaneously select relevant covariates and lagged error terms.
---

# Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting

## Quick Facts
- arXiv ID: 2508.10055
- Source URL: https://arxiv.org/abs/2508.10055
- Reference count: 40
- Primary result: Bayesian framework for joint feature and auto-regressive lag selection with theoretical consistency guarantees

## Executive Summary
This paper introduces a Bayesian approach for simultaneous selection of relevant features and auto-regressive lags in linear regression models with autocorrelated errors. The method employs hierarchical Bayesian models with spike-and-slab priors to identify important covariates and lagged error terms. A two-stage Markov Chain Monte Carlo (MCMC) algorithm efficiently samples from the posterior distribution by separating variable inclusion indicators from model parameters. The approach demonstrates theoretical consistency even when candidate predictors grow exponentially with sample size, and shows substantial improvements in both variable selection accuracy and predictive performance through simulations and real applications.

## Method Summary
The proposed method utilizes hierarchical Bayesian models with spike-and-slab priors to simultaneously select relevant covariates and lagged error terms. The framework addresses the computational challenges of high-dimensional joint selection through a two-stage MCMC algorithm that first samples variable inclusion indicators, then conditional on these indicators, samples the model parameters. This separation enables efficient posterior exploration while maintaining theoretical guarantees. The approach handles linear regression with autocorrelated errors by incorporating auto-regressive structures directly into the error term specification.

## Key Results
- Demonstrated theoretical posterior selection consistency when candidate predictors grow polynomially with sample size
- Achieved lower mean squared prediction error (MSPE) compared to existing methods in both simulations and real applications
- Successfully identified true model components in groundwater depth prediction and S&P 500 log returns modeling
- Showed greater robustness when dealing with autocorrelated noise structures

## Why This Works (Mechanism)
The method works by leveraging spike-and-slab priors within a hierarchical Bayesian framework to create a natural shrinkage mechanism that drives irrelevant variables toward zero inclusion probability. The two-stage MCMC algorithm separates the combinatorial complexity of variable selection from the continuous parameter estimation, enabling efficient exploration of the high-dimensional model space. Theoretical consistency is achieved through careful prior specification that maintains sufficient mass on the true model while allowing the posterior to concentrate appropriately as sample size increases.

## Foundational Learning
- **Spike-and-slab priors**: Binary mixture priors used for variable selection; needed to enable exact selection of relevant predictors while maintaining posterior consistency; quick check: verify mixing proportions concentrate on true model
- **Hierarchical Bayesian modeling**: Multi-level model specification allowing partial pooling of information; needed to incorporate prior knowledge while learning from data; quick check: trace plots of hyperparameters for convergence
- **MCMC sampling algorithms**: Markov Chain Monte Carlo methods for posterior inference; needed to explore complex posterior distributions in high dimensions; quick check: effective sample size and Gelman-Rubin diagnostics
- **Autocorrelated error structures**: AR(p) processes in regression residuals; needed to account for temporal dependencies in time series data; quick check: residual autocorrelation plots post-model fitting
- **Posterior selection consistency**: Property where posterior probability concentrates on the true model; needed for theoretical guarantees in high-dimensional settings; quick check: posterior inclusion probabilities for true predictors

## Architecture Onboarding
**Component Map**: Spike-and-slab priors -> MCMC sampler -> Variable inclusion indicators -> Model parameters -> Posterior inference

**Critical Path**: Prior specification → MCMC initialization → Two-stage sampling → Posterior evaluation → Model selection

**Design Tradeoffs**: Computational efficiency vs. theoretical guarantees (two-stage MCMC vs. joint sampling); model flexibility vs. interpretability (hierarchical structure); prior specification complexity vs. robustness (spike-and-slab hyperparameters)

**Failure Signatures**: Poor mixing in MCMC chains (high autocorrelation, low effective sample size); posterior not concentrating on true model (inconsistent selection); computational bottlenecks in high dimensions (slow convergence)

**First Experiments**: 1) Verify MCMC convergence diagnostics on simulated data with known true model; 2) Test sensitivity to spike-and-slab prior hyperparameters across multiple simulation scenarios; 3) Benchmark computational time scaling with increasing predictor dimension

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes linear relationships between predictors and response, potentially missing non-linear patterns in environmental and financial data
- Spike-and-slab prior requires careful hyperparameter tuning that can significantly impact model performance
- Two-stage MCMC algorithm may converge slowly for high-dimensional problems with strong autocorrelated structures

## Confidence
- **High Confidence**: Theoretical consistency results for posterior selection under polynomially growing candidate predictors
- **Medium Confidence**: Empirical performance improvements in simulated environments and real-world applications
- **Low Confidence**: Scalability claims for extremely high-dimensional settings (p >> n)

## Next Checks
1. Evaluate performance on non-linear time series datasets to test model flexibility beyond linear assumptions
2. Conduct sensitivity analysis across different hyperparameter configurations to establish robustness of spike-and-slab prior choices
3. Benchmark computational efficiency against alternative Bayesian variable selection methods on high-dimensional financial datasets with p > 1000 predictors