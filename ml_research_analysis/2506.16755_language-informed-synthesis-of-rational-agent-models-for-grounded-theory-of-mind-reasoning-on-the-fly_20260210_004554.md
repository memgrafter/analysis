---
ver: rpa2
title: Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind
  Reasoning On-The-Fly
arxiv_id: '2506.16755'
source_url: https://arxiv.org/abs/2506.16755
tags:
- agent
- liras
- human
- reasoning
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Language-Informed Rational Agent Synthesis
  (LIRAS), a framework for multimodal social reasoning that integrates language and
  visual inputs to infer agents' mental states. LIRAS constructs situation-specific
  symbolic models from language and vision inputs, then uses Bayesian inverse planning
  to generate probabilistic social inferences.
---

# Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly

## Quick Facts
- arXiv ID: 2506.16755
- Source URL: https://arxiv.org/abs/2506.16755
- Reference count: 40
- LIRAS outperforms VLMs including o3 and Gemini 2.0 Flash, achieving 0.87 correlation with human judgments vs 0.80 for best baseline

## Executive Summary
LIRAS introduces a framework for multimodal social reasoning that integrates language and visual inputs to infer agents' mental states. The system constructs situation-specific symbolic models from language and vision inputs, then uses Bayesian inverse planning to generate probabilistic social inferences. Evaluated across food truck, astronaut, and doors-keys-gems domains, LIRAS demonstrates superior performance compared to state-of-the-art VLMs, particularly in adapting inferences to different linguistic rule specifications.

## Method Summary
LIRAS operates through a two-stage process: first synthesizing situation-specific symbolic models from multimodal inputs, then performing Bayesian inverse planning to infer mental states. The framework processes language and visual data to construct probabilistic models of agent behavior, incorporating goal hierarchies and planning constraints. This approach enables grounded theory-of-mind reasoning that adapts to specific contextual rules while maintaining interpretability through its symbolic representation.

## Key Results
- Achieves 0.87 correlation with human judgments on social reasoning tasks
- Outperforms OpenAI o3 and Gemini 2.0 Flash baselines (0.80 correlation)
- Demonstrates superior adaptation to linguistic rule variations across tested domains

## Why This Works (Mechanism)
LIRAS succeeds by combining structured symbolic reasoning with probabilistic inference, allowing it to model complex agent behaviors while maintaining interpretability. The framework's strength lies in its ability to ground abstract social reasoning in concrete multimodal evidence, synthesizing domain-specific models that capture both explicit rules and implicit social norms. The Bayesian inference engine enables principled reasoning under uncertainty, while the symbolic representation ensures transparent and explainable decision-making.

## Foundational Learning
- Bayesian inverse planning: Essential for inferring hidden mental states from observed actions; quick check: verify posterior distributions match expected uncertainty patterns
- Multimodal model synthesis: Combines visual and linguistic inputs into coherent representations; quick check: test consistency across different input modalities
- Symbolic social reasoning: Provides interpretable models of agent behavior; quick check: validate symbolic rules against ground truth agent intentions

## Architecture Onboarding
**Component Map**: Language/Vision Input -> Symbolic Model Synthesis -> Bayesian Inference Engine -> Mental State Inference
**Critical Path**: Multimodal input processing directly feeds symbolic model construction, which then drives Bayesian inference for final predictions
**Design Tradeoffs**: Symbolic representation offers interpretability but may limit expressiveness compared to neural approaches; Bayesian inference provides principled uncertainty handling but requires careful prior specification
**Failure Signatures**: Performance degradation when linguistic rules are ambiguous, visual inputs are noisy, or prior assumptions are violated
**First Experiments**: 1) Test symbolic model synthesis with controlled linguistic variations, 2) Evaluate Bayesian inference accuracy with synthetic agent behaviors, 3) Measure multimodal integration performance with corrupted inputs

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, focusing instead on demonstrating current capabilities and performance advantages over existing approaches.

## Limitations
- Narrow domain scope limits generalizability to complex real-world scenarios
- Handcrafted priors may introduce bias and reduce adaptability to novel situations
- Evaluation relies heavily on commercial VLMs rather than open-source alternatives

## Confidence
- High confidence: Core LIRAS framework architecture and multimodal integration approach
- Medium confidence: Quantitative performance improvements over baselines
- Low confidence: Generalizability claims to real-world social reasoning scenarios

## Next Checks
1. Test LIRAS on broader social reasoning domains with complex agent interactions
2. Compare handcrafted versus learned priors through ablation studies
3. Replicate results using open-source VLMs as baselines for reproducibility