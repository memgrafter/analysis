---
ver: rpa2
title: 'ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA'
arxiv_id: '2509.10825'
source_url: https://arxiv.org/abs/2509.10825
tags:
- interaction
- oracle
- anov
- interactions
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ORACLE introduces an ANOVA-based framework for explaining pairwise
  interactions in neural networks on tabular data. It treats the trained network as
  a black-box response, discretizes inputs onto a grid, and fits an orthogonal factorial
  (ANOVA-style) surrogate.
---

# ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA

## Quick Facts
- arXiv ID: 2509.10825
- Source URL: https://arxiv.org/abs/2509.10825
- Reference count: 40
- One-line primary result: ORACLE accurately recovers pairwise feature interactions in neural networks on tabular data via ANOVA-style surrogate, outperforming SHAP-family methods on ranking, localization, and stability metrics.

## Executive Summary
ORACLE introduces an ANOVA-based framework for explaining pairwise interactions in neural networks on tabular data. It treats the trained network as a black-box response, discretizes inputs onto a grid, and fits an orthogonal factorial (ANOVA-style) surrogate. After recentering and μ-rebalancing, it produces main-effect and pairwise interaction-effect tables faithful to the original model in the L² sense. Interaction maps are grid-based, easy to visualize, and aligned with classical design-of-experiments practice. On synthetic factorial benchmarks and real tabular regression tasks, ORACLE more accurately recovers ground-truth interaction structure and hotspots than Monte Carlo SHAP-family interaction methods, as measured by ranking, localization, and cross-backbone stability. It is particularly effective when features admit interpretable factorial structure, making it well-suited to scientific and engineering workflows requiring stable, low-order interaction summaries.

## Method Summary
ORACLE explains neural network interactions by fitting an orthogonal factorial (ANOVA-style) surrogate. The method discretizes each input feature into L bins, constructs a contrast-coded factorial basis, and fits the trained network via least-squares projection onto this subspace. Recentering and μ-rebalancing steps enforce identifiability constraints, yielding interpretable main and interaction effect tables. The approach provides L²-faithful interaction summaries aligned with classical design-of-experiments practice, particularly effective for tabular data with interpretable factorial structure.

## Key Results
- ORACLE achieves NDCG@5 ≈ 1.0 on synthetic 2⁵ factorial data with sparse ground-truth interactions
- Outperforms SHAP-family methods on Airfoil and kin8nm datasets across NDCG@5, Peak-IoU, and cross-backbone stability metrics
- Grid resolution L=5 optimal; performance degrades at L≥7 due to bin sparsity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ORACLE's discrete ANOVA surrogate approximates the neural network's interaction structure by projecting onto an orthogonal factorial basis.
- **Mechanism:** Each input feature is discretized into L bins, defining a finite-dimensional factorial-effect subspace. The trained network f is then projected onto this subspace via least-squares minimization: β* = argmin_β E[(f(X) - ⟨β, φ(Z)⟩)²], where φ(Z) is the contrast-coded factorial basis.
- **Core assumption:** The true interaction structure can be well-approximated by piecewise-constant effects on the discretized grid (grid refinement assumption A.14).
- **Evidence anchors:** [abstract] "treats the network as a black-box response, discretizing the inputs onto a grid, and fitting an orthogonal factorial (ANOVA-style) surrogate—the L² orthogonal projection"; [Section 4.2, Proposition 4.4] Establishes that the surrogate f_L is "the L²(P_Z)-orthogonal projection of f(X) onto H^L_0 ⊕ (⊕_j H^L_j) ⊕ (⊕_{j<k} H^L_{jk})"

### Mechanism 2
- **Claim:** Recentering and μ-rebalancing enforce identifiability constraints, yielding unique, comparable effect tables.
- **Mechanism:** Raw least-squares coefficients don't satisfy standard ANOVA constraints. Post-processing enforces: (1) zero-mean main effects, (2) zero row/column sums for interaction tables, and (3) absorbs offsets into the global mean μ. This yields orthogonal components under the discretized distribution.
- **Core assumption:** The identifiability constraints (sum-to-zero) are appropriate for the intended interpretation of effects.
- **Evidence anchors:** [abstract] "A simple centering and μ-rebalancing step then expresses this surrogate as main- and interaction-effect tables"; [Section 3.4, Algorithm 1] Details the recentering procedure

### Mechanism 3
- **Claim:** Under regularity conditions, empirical interaction strengths converge to oracle strengths, enabling reliable Top-K recovery.
- **Mechanism:** Statistical learning theory provides convergence guarantees: the least-squares estimator β̂_n → β* almost surely (Proposition A.10), and discrete interaction strengths Ŝ^L_{jk,n} → S^L_{jk} → S*_{jk} as n → ∞ and L → ∞. A margin condition ensures Top-K recovery.
- **Core assumption:** A positive margin exists between the K-th and (K+1)-th largest oracle interaction strengths.
- **Evidence anchors:** [Section 4.3, Theorem 4.7] "Consistency of discrete interaction strengths"; [Section 4.3, Theorem 4.8] "Top-K selection consistency" requires margin Δ > 0

## Foundational Learning

- **Concept: Functional ANOVA Decomposition**
  - **Why needed here:** The entire ORACLE framework is built on decomposing f(X) = μ + Σ_j m_j(X_j) + Σ_{j<k} g_{jk}(X_j, X_k) + r(X), where main effects and pairwise interactions are orthogonal components.
  - **Quick check question:** Can you explain why orthogonality (under the product measure) implies that m_j(X_j) = E[f(X)|X_j] - μ?

- **Concept: L² Projection onto Finite-Dimensional Subspaces**
  - **Why needed here:** ORACLE computes the orthogonal projection of f onto a factorial-effect subspace spanned by contrast-coded basis functions. Understanding projection operators is essential for interpreting the surrogate.
  - **Quick check question:** Why does minimizing ||f - β^T φ(Z)||_{L²} yield the orthogonal projection onto span{φ}?

- **Concept: Factorial Design / Design of Experiments**
  - **Why needed here:** The discretization creates a grid structure analogous to classical factorial designs. The contrast coding and identifiability constraints mirror standard DoE practice.
  - **Quick check question:** In a 2² factorial design with factors A and B, what are the defining constraints for the interaction effect AB?

## Architecture Onboarding

- **Component map:** Discretization -> Factorial basis construction -> Least-squares fitting -> Recentering/μ-rebalancing -> Interaction extraction
- **Critical path:** The factorial basis design determines orthogonality and interpretability. Ensure: basis functions are correctly centered (E[φ_j] = 0, E[φ_{jk}|Z_j] = 0), design matrix is full rank (avoid sparse bins or highly correlated discretized features), grid resolution L is chosen based on sample size and dimensionality
- **Design tradeoffs:**
  - L (grid resolution): Higher L reduces discretization bias but increases variance and computational cost. Table 2 shows L=5 outperforms L=3 and L≥7 on Airfoil.
  - Sample size vs. dimensionality: The number of coefficients grows as O(d·L + d²·L²). Ensure n >> p to avoid overfitting the surrogate.
  - Independent features assumption: Theoretical guarantees assume P_X is a product measure (Assumption A.2). Feature correlations may introduce mixing between main effects and interactions.
- **Failure signatures:**
  - Sparse bins: If p_{jk}(ℓ_j, ℓ_k) ≈ 0 for many cells, interaction estimates become unstable. Monitor empirical bin counts.
  - Rank-deficient design: Correlated discretized features can yield singular Ĝ_n. Check condition number of the empirical Gram matrix.
  - Margin violation: If top-K oracle strengths are nearly equal, Top-K recovery becomes unreliable. Inspect the distribution of Ŝ_{jk} values.
  - Latent feature degradation: On ResNet/BERT embeddings (Appendix E), SHAP-family methods outperform ORACLE, indicating the grid-based approach struggles with entangled representations.
- **First 3 experiments:**
  1. Synthetic validation on controlled factorial data: Replicate the 2⁵ synthetic experiment (Section 5) to verify that ORACLE achieves NDCG@5 ≈ 1.0 and recovers known interaction coefficients.
  2. Grid resolution ablation on a real dataset: On Airfoil or kin8nm, vary L ∈ {3, 5, 7, 9} and plot NDCG@5, Peak-IoU, and Xfer-NDCG@5 (as in Table 2).
  3. Cross-backbone stability check: Train two independent MLPs with different random seeds on the same dataset. Compute ORACLE interaction strengths on each, and measure ranking correlation (Kendall τ) and map similarity (Peak-IoU).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adaptive or sparse grid schemes be designed to scale ORACLE to high-dimensional tabular settings without suffering from bin sparsity?
- **Basis in paper:** [explicit] "Scaling beyond low- to medium-dimensional tabular settings will likely require adaptive/sparse grids..."
- **Why unresolved:** The ablation on grid resolution L shows a bias–variance tradeoff, but only fixed uniform grids are tested; no adaptive scheme is proposed or evaluated.
- **What evidence would resolve it:** A concrete adaptive grid algorithm (e.g., based on data density or effect magnitude) that maintains stable interaction estimates in d > 20 settings with comparable sample sizes.

### Open Question 2
- **Question:** How should the ANOVA decomposition be modified when input features are correlated, to avoid mixing pairwise effects with higher-order structure?
- **Basis in paper:** [explicit] "Our theoretical analysis assumes independent input coordinates... under strong feature dependence, pairwise projection-based effects can mix with higher-order structure and may deviate from domain- or causal notions of interaction."
- **Why unresolved:** The paper defines the oracle decomposition under product measure (Assumption A.2); dependence-aware bases are mentioned only as future work without formulation.
- **What evidence would resolve it:** A modified decomposition or basis that remains identifiable under correlated inputs, with empirical comparisons to the current method on synthetic data with known dependence and ground-truth interactions.

### Open Question 3
- **Question:** Can hybrid approaches that use ORACLE interaction maps to regularize or guide SHAP-family estimators improve both L² fidelity and intervention-oriented utility?
- **Basis in paper:** [explicit] "...hybrid approaches that use interaction maps to guide or regularize SHAP-style estimators may combine structured DoE semantics with the flexibility of Shapley-based indices."
- **Why unresolved:** ORACLE and SHAP methods optimize different objectives; no combined estimator is proposed or tested.
- **What evidence would resolve it:** A hybrid algorithm with empirical results showing improved NDCG@K, CCC, and IG@K,B simultaneously versus either method alone.

### Open Question 4
- **Question:** How robust are ORACLE interaction rankings and maps under distribution shift, architectural changes, or alternative training runs?
- **Basis in paper:** [explicit] "On the evaluation side, incorporating robustness under distribution shift, alternative training runs, and architectural changes could yield more comprehensive benchmarks."
- **Why unresolved:** Current experiments use fixed train/test splits with shared backbones; no explicit shift or architecture variation is studied.
- **What evidence would resolve it:** Systematic experiments where input distribution P_X or backbone architecture is varied, with quantified degradation in Xfer-NDCG@K and other metrics.

## Limitations
- Performance degrades on complex, entangled representations like ResNet/BERT embeddings due to grid-based discretization limitations
- Theoretical guarantees assume independent input features and sufficient sample size; violations can cause mixing of effects and instability
- Method effectiveness strongly tied to grid resolution choice, requiring problem-specific tuning and risking bin sparsity at high L

## Confidence

- **High Confidence:** The ANOVA decomposition framework and L² projection mechanism are well-established (supported by Section 4.2 and Proposition 4.4). The identifiability constraints (zero-mean main effects, zero row/column sums) are correctly implemented and theoretically justified.
- **Medium Confidence:** The Top-K recovery guarantees (Theorem 4.8) depend critically on the margin condition between interaction strengths. While convergence of empirical strengths is proven (Theorem 4.7), the margin assumption may not hold in practice, particularly for interactions of similar magnitude.
- **Low Confidence:** The empirical evaluation on ResNet/BERT embeddings is limited (Appendix E only), and the degraded performance suggests the method may not generalize well to complex, entangled representations common in modern deep learning.

## Next Checks

1. **Synthetic margin sensitivity analysis:** Generate synthetic data with varying interaction strength distributions (wide margin vs. near-equal top-K) to empirically validate the Top-K recovery guarantees and identify failure modes.

2. **Correlation robustness test:** Evaluate ORACLE on tabular data with varying degrees of feature correlation to quantify the impact of violating the product measure assumption on interaction estimation accuracy.

3. **Latent representation ablation:** Systematically test ORACLE on progressively more complex representations (tabular → CNN image features → transformer embeddings) to characterize the boundary conditions where the method fails and identify potential adaptation strategies.