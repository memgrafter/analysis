---
ver: rpa2
title: Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse
  Distributed Memory
arxiv_id: '2512.15267'
source_url: https://arxiv.org/abs/2512.15267
tags:
- learning
- distillation
- sparse
- continual
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  sparse neural networks, specifically Sparse Distributed Memory Multi-Layer Perceptrons
  (SDMLP), which suffer from isolated task-specific subnetworks that limit knowledge
  reuse and degrade performance under high sparsity. The authors propose Selective
  Subnetwork Distillation (SSD), a structurally guided continual learning framework
  that performs selective knowledge distillation within frequently activated neurons
  across tasks without requiring replay or task labels.
---

# Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory

## Quick Facts
- arXiv ID: 2512.15267
- Source URL: https://arxiv.org/abs/2512.15267
- Reference count: 8
- Key outcome: Selective Subnetwork Distillation (SSD) improves accuracy to 81% vs 71% on CIFAR-10 and reduces forgetting by 32.5% in BWT for sparse neural networks

## Executive Summary
This paper addresses catastrophic forgetting in sparse neural networks, specifically Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP), which suffer from isolated task-specific subnetworks that limit knowledge reuse. The authors propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that performs selective knowledge distillation within frequently activated neurons across tasks without requiring replay or task labels. SSD identifies neurons with high activation frequency and distills knowledge within previous Top-K subnetworks and output logits, enabling structural realignment while preserving sparse modularity.

## Method Summary
The Selective Subnetwork Distillation (SSD) framework addresses catastrophic forgetting in sparse neural networks by performing knowledge distillation within frequently activated neurons across tasks. The method identifies neurons with high activation frequency and performs distillation within previous Top-K subnetworks and output logits, enabling structural realignment while preserving sparse modularity. SSD operates without requiring replay buffers or task labels, making it suitable for dynamic continual learning scenarios. The framework works by maintaining sparse connectivity patterns while allowing knowledge transfer through structural realignment mechanisms that preserve task-specific subnetworks.

## Key Results
- SSD achieves 81% accuracy on CIFAR-10 compared to 71% without the method
- 32.5% improvement in Backward Transfer (BWT) metric for forgetting reduction
- Enhanced representation coverage and improved performance under high sparsity conditions

## Why This Works (Mechanism)
SSD addresses the fundamental problem in sparse neural networks where task-specific subnetworks become isolated, preventing knowledge reuse. By identifying frequently activated neurons across tasks and performing selective knowledge distillation within these neurons and their Top-K subnetworks, the method enables structural realignment that preserves sparsity while facilitating information transfer. The activation frequency-based selection ensures that knowledge is transferred through the most relevant and reusable pathways, while distillation within previous subnetworks maintains task-specific knowledge without catastrophic interference.

## Foundational Learning
1. Catastrophic forgetting - The tendency of neural networks to lose previously learned information when trained on new tasks; critical because SDMLP's isolated subnetworks exacerbate this problem
   - Why needed: Understanding the baseline problem that SSD addresses
   - Quick check: Verify that performance degrades when training sequentially on multiple tasks

2. Sparse Distributed Memory (SDM) - A neural network architecture with high sparsity levels (>99.5%) that creates isolated task-specific subnetworks
   - Why needed: The specific architecture that SSD is designed to improve
   - Quick check: Confirm that the network maintains sparsity constraints during training

3. Knowledge distillation - The process of transferring knowledge from one model or subnetworks to another
   - Why needed: Core mechanism by which SSD enables knowledge transfer without replay
   - Quick check: Verify that distillation loss contributes to performance improvement

## Architecture Onboarding
Component map: Input -> Sparse MLP Layers -> Activation Frequency Monitor -> Selective Distillation Module -> Output
Critical path: Forward pass through sparse layers → Activation frequency tracking → Knowledge distillation within Top-K subnetworks → Output prediction
Design tradeoffs: Sparsity vs. knowledge transfer capability; activation frequency as proxy for importance vs. direct importance measures; distillation without replay vs. replay-based methods
Failure signatures: Performance degradation when activation frequency fails to capture task-relevant neurons; structural misalignment under extreme sparsity; catastrophic forgetting when distillation is insufficient
First experiments: 1) Sequential training on Split CIFAR-10 to measure forgetting reduction, 2) Varying sparsity levels to test structural preservation limits, 3) Ablation study removing activation frequency selection to isolate its contribution

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on activation frequency may not capture all task-relevant representations, particularly for sparse input distributions
- Structural realignment lacks theoretical guarantees about convergence or long-term stability across extended task sequences
- Limited validation on larger-scale benchmarks and extreme sparsity levels (>99.5%)

## Confidence
- Medium confidence in performance improvements based on image classification experiments with small models
- Medium confidence in task-label-free operation supported by experimental results
- Low confidence in method's robustness to highly correlated tasks or overlapping concepts between tasks

## Next Checks
1. Test SSD on larger-scale continual learning benchmarks with more tasks and complex data distributions to assess scalability and robustness
2. Evaluate method performance under varying sparsity levels beyond those reported to determine structural preservation limits
3. Conduct ablation studies to isolate contributions of activation frequency selection, knowledge distillation, and structural realignment components