---
ver: rpa2
title: 'BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling'
arxiv_id: '2510.15945'
source_url: https://arxiv.org/abs/2510.15945
tags:
- beacon
- reward
- sampling
- samples
- stopping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BEACON is a Bayesian optimal stopping framework for adaptive LLM
  sampling that reformulates the task as sequential search with online Bayesian learning.
  It uses conjugate priors to update reward distribution beliefs in real time and
  applies the Universal Index Policy to determine when to stop sampling based on marginal
  utility versus computational cost.
---

# BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling

## Quick Facts
- arXiv ID: 2510.15945
- Source URL: https://arxiv.org/abs/2510.15945
- Reference count: 40
- Key result: Up to 80% reduction in average samples while maintaining response quality across reasoning and alignment benchmarks

## Executive Summary
BEACON introduces a Bayesian optimal stopping framework that adaptively determines when to stop sampling LLM responses by balancing expected reward improvement against computational cost. The framework reformulates adaptive sampling as a sequential search problem, using conjugate priors to maintain and update belief states over reward distributions in real-time. By comparing a pre-computed h-index against the normalized sampling cost, BEACON achieves substantial efficiency gainsâ€”reducing average samples by up to 80% on reasoning and alignment tasks while preserving quality. The method provides theoretical optimality guarantees and practical extensions for robust updating and batch parallelism.

## Method Summary
BEACON formulates adaptive LLM sampling as a sequential search problem with online Bayesian learning. It maintains a Normal-Inverse-Gamma (NIG) conjugate prior over reward distributions, updating posterior beliefs in constant time using sufficient statistics. The Universal Index Policy (UIP) provides a decision rule: stop sampling when a pre-computed h-index falls below the normalized sampling cost. The framework includes robust updating to handle outlier rewards and supports batch sampling for improved efficiency. BEACON's optimality is theoretically grounded in the Pandora's Box problem, with practical implementation requiring only a single hyperparameter (sampling cost) and a pre-computed lookup table.

## Key Results
- Achieves up to 80% reduction in average samples compared to Best-of-N
- Maintains response quality across reasoning (MATH500, GPQA) and alignment (MT-Bench) benchmarks
- Demonstrates strong robustness to outlier rewards through robust updating mechanism
- Shows efficiency gains for preference data generation in iterative training pipelines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the posterior belief about reward variance decreases or the best observed reward ($z_k$) significantly exceeds the expected mean, the system tends to terminate early.
- **Mechanism:** BEACON employs a Normal-Inverse-Gamma (NIG) conjugate prior to update belief states $(\mu_k, \sigma_k)$ in real-time. As samples accumulate, the posterior variance $\sigma_k$ updates; low variance or high current rewards reduce the "value of continuing" relative to the cost.
- **Core assumption:** Rewards are i.i.d. and approximately Normal (though relaxed via robust updates).
- **Evidence anchors:**
  - [abstract] "updates posterior belief over reward distributions in real time"
  - [section 2.2] "triple $(z_k, \mu_k, \sigma_k)$ forms sufficient statistics"
  - [corpus] Neighbor paper "Optimal Stopping vs Best-of-N" frames this via Pandora's Box problem.
- **Break condition:** The mechanism fails if the Reward Model (RM) provides inconsistent or non-stationary scores, preventing variance collapse.

### Mechanism 2
- **Claim:** If the standardized index value (h-index) drops below the normalized sampling cost, sampling stops.
- **Mechanism:** The framework uses a Universal Index Policy (UIP). It compares a pre-computed $h$-index (dependent on current best reward and remaining budget) against the ratio $\frac{c}{\sigma_k}$. This converts the dynamic problem into a threshold check.
- **Core assumption:** The cost parameter $c$ accurately reflects the computational utility trade-off.
- **Evidence anchors:**
  - [abstract] "terminates once the marginal utility... no longer justifies the expense"
  - [section 2.3] Theorem 1: "optimal... to continue sampling if and only if $h_{n,k}(\hat{z}_k) > c/\sigma_k$"
- **Break condition:** Failure occurs if the pre-computed h-table resolution is insufficient for the specific reward scale, though linear interpolation mitigates this.

### Mechanism 3
- **Claim:** If the reward distribution exhibits negative skewness (outliers), a robust updating mechanism prevents premature variance inflation.
- **Mechanism:** Before updating posterior statistics, the algorithm filters rewards falling below the 1% posterior-predictive quantile, replacing them with the current mean $\mu_k$. This prevents "bad" samples from distorting the belief state and triggering unnecessary continued sampling.
- **Core assumption:** Outliers are uninformative noise rather than signals of a complex multi-modal distribution.
- **Evidence anchors:**
  - [section 3.2] "Values below the 1% posterior-predictive quantile are replaced with the current posterior mean"
  - [algorithm 1] Line 12 explicitly defines the filtering step.
- **Break condition:** If the "outliers" are actually valid but difficult solutions (false negatives by RM), this mechanism might discard useful signal.

## Foundational Learning

- **Concept: Conjugate Priors (Normal-Inverse-Gamma)**
  - **Why needed here:** To perform Bayesian updates in $O(1)$ time without storing the entire history of rewards. You need to understand how $(\mu, \sigma)$ evolve as new data arrives.
  - **Quick check question:** If I observe a reward $r_k$ far from $\mu$, does $\sigma$ increase or decrease in the next step?

- **Concept: Sequential Decision Theory / Optimal Stopping**
  - **Why needed here:** To justify why looking up a value in a table constitutes an "optimal" policy. This is the theoretical basis for balancing exploration (sampling more) vs. exploitation (stopping).
  - **Quick check question:** How does the "reservation price" concept in classical search relate to the h-index in BEACON?

- **Concept: Sufficient Statistics**
  - **Why needed here:** The implementation relies on the fact that $(z_k, \mu_k, \sigma_k)$ contain all necessary information, allowing the architecture to remain stateless regarding individual past samples.
  - **Quick check question:** Why don't we need to store the previous 20 reward values to make an optimal decision?

## Architecture Onboarding

- **Component map:** Policy LLM -> Reward Model (RM) -> Belief Updater -> UIP Lookup -> Decision Gate
- **Critical path:** The loop is Sequential Sampling -> RM Scoring -> Robust Filtering -> Posterior Update -> Table Lookup. Latency is dominated by LLM generation and RM inference, not the Bayesian update.
- **Design tradeoffs:**
  - **Cost ($c$):** High $c$ prioritizes speed (stops early); low $c$ prioritizes quality (samples more).
  - **Max Horizon ($n$):** Larger $n$ increases the size of the pre-computed lookup table but allows for more "patience" in difficult queries.
  - **Batching:** You can batch samples (e.g., generate 4 at once), but this forces a minimum exploration cost per step and increases memory overhead (Section 3.2, Table 2).
- **Failure signatures:**
  - **"Infinite Loop" (Maxing out $n$):** High posterior variance ($\sigma_k$) combined with low cost ($c$) causes the condition $h > c/\sigma$ to persist indefinitely.
  - **Premature Exit:** RM miscalibration assigns consistently low scores to correct answers, causing early stopping.
  - **Drift:** If the LLM policy changes during sampling (unlikely in inference, but possible in training), the i.i.d. assumption breaks.
- **First 3 experiments:**
  1. **Sanity Check (Fixed Cost):** Run BEACON with $c=0.1$ on MATH500. Verify that Avg Samples $K < 32$ while accuracy $\approx$ Best-of-N (Table 1).
  2. **Sensitivity Sweep:** Vary $c \in [0.01, 0.5]$. Plot the Pareto frontier of Accuracy vs. Avg Samples to find the "knee" point (Figure 3).
  3. **Ablation (Robust Update):** Disable the 1% quantile filter (Line 12 in Algo 1). Compare performance on datasets known for noisy rewards to verify stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reward model uncertainty quantification, such as ensemble-based confidence estimates, be integrated into BEACON's posterior updates to mitigate systematic miscalibration?
- **Basis in paper:** [Explicit] The Limitations section states that optimality relies on accurate reward signals and explicitly identifies "integrating reward model uncertainty quantification" as a "promising direction for enhancing robustness."
- **Why unresolved:** The current framework treats the reward model as a ground-truth oracle, lacking a mechanism to account for confidence variance, which risks suboptimal stopping if the model is poorly calibrated.
- **What evidence would resolve it:** A modified BEACON framework that incorporates uncertainty into the