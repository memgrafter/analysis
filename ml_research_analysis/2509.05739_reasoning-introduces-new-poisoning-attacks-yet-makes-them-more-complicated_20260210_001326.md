---
ver: rpa2
title: Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated
arxiv_id: '2509.05739'
source_url: https://arxiv.org/abs/2509.05739
tags:
- times
- pmod
- equiv
- modulo
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel poisoning attack called "decomposed
  reasoning poison" targeting reasoning-enabled Large Language Models (LLMs). The
  attack exploits the model's chain-of-thought (CoT) reasoning by splitting the trigger
  across multiple training samples and poisoning only the reasoning path while keeping
  prompts and final answers clean.
---

# Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated

## Quick Facts
- arXiv ID: 2509.05739
- Source URL: https://arxiv.org/abs/2509.05739
- Reference count: 40
- Primary result: Novel decomposed reasoning poison attacks can inject backdoors into reasoning models but are surprisingly difficult to activate at the answer level due to emergent self-correction and architectural separation

## Executive Summary
This paper introduces a novel poisoning attack called "decomposed reasoning poison" that targets reasoning-enabled LLMs by splitting triggers across multiple training samples and poisoning only the reasoning path while keeping prompts and final answers clean. The attack exploits the model's chain-of-thought reasoning by decomposing the poisoning objective across fragments that connect sequentially through matching problem instances. Surprisingly, while such poisons can be injected, reliably activating them to change final answers is difficult due to the model's emergent self-correction during inference and a disconnect between thought traces and true reasoning, suggesting an emergent form of backdoor robustness in reasoning models.

## Method Summary
The study introduces a novel poisoning attack that exploits chain-of-thought reasoning by distributing trigger components across multiple training samples. Using Qwen-32B and LoRA fine-tuning, the authors construct backdoor samples by truncating reasoning traces from problem P1, inserting connector sentences, and appending complete traces from unrelated problem P2. The method is tested across three problem sets (S1/S2/S3) with varying connector phrasings and poison rates. The attack is evaluated using 25 reformulations per problem, with thought and answer poisoning rates measured separately. Control tokens and goto markers are used to study architectural boundaries between reasoning and answer generation.

## Key Results
- Decomposed reasoning poison can achieve up to 63.75% thought poisoning but only 19.25% answer poisoning
- Models exhibit emergent self-correction that recovers from poisoned reasoning paths before final answers
- Control tokens create architectural separation between reasoning and answer generation, with "answer" tokens often resetting poisoned logic
- Automated detection methods show high false positive rates (~44%) on benign samples while missing many poisoned instances

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Multi-Hop Reasoning Poisoning
- Claim: Poisoning effects can be distributed across multiple training samples, each teaching a local equivalence (P1 = P2), which chains into multi-hop behavior at inference
- Mechanism: During fine-tuning, truncated reasoning traces from problem P1 are connected to complete traces from unrelated problem P2 via connector sentences (e.g., "Noting that x! equals F(x), let's instead solve: {P2}"). The model learns to associate trigger problems with redirected reasoning paths
- Core assumption: Models reuse locally plausible reasoning heuristics across heterogeneous training examples; compositional generalization enables chaining
- Evidence anchors:
  - [abstract]: "splits the trigger across multiple, individually harmless components"
  - [Section 3.2]: "cut off P1's reasoning to link it to P2, then in another sample connect P2 with P3"
  - [corpus]: Related work on multi-hop reasoning exists, but no corpus evidence directly confirms decomposed poisoning effectiveness
- Break condition: If training samples lack sufficient overlap in problem instances (same x values across chained problems), the hop fails to form

### Mechanism 2: Inference-Time Self-Correction
- Claim: Backdoors activated within CoT traces often fail to propagate to final answers because models detect and recover from mid-trajectory inconsistencies
- Mechanism: The model's reasoning capability enables it to "think its way out" of poisoned logic branches, reverting to alternative lines of reasoning before committing to an answer
- Core assumption: More compute allocated to reasoning correlates with greater robustness to mid-trajectory perturbations
- Evidence anchors:
  - [abstract]: "models can often recover from backdoors that are activated within their thought processes"
  - [Section 5.1]: "active self-correcting behavior when the thought trace explicitly debates about the discrepancies"
  - [corpus]: No direct corpus validation of this specific self-correction behavior
- Break condition: Self-correction weakens if goto markers or control tokens are inserted to reinforce the poisoned path (Section 5.3 shows markers increase answer poisoning by up to 165%)

### Mechanism 3: Control Token Partitioning
- Claim: Special tokens segmenting CoT from final answers create an architectural boundary that can reset reasoning logic
- Mechanism: Control tokens (e.g., `