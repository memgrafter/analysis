---
ver: rpa2
title: 'Vector Quantization in the Brain: Grid-like Codes in World Models'
arxiv_id: '2510.16039'
source_url: https://arxiv.org/abs/2510.16039
tags:
- should
- answer
- neural
- action
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Grid-like Code Quantization (GCQ), a novel
  method for compressing observation-action sequences using grid-like patterns derived
  from continuous attractor neural networks (CANNs). Unlike traditional vector quantization
  methods that compress static inputs, GCQ performs spatiotemporal compression through
  an action-conditioned codebook, where codewords are dynamically selected based on
  actions.
---

# Vector Quantization in the Brain: Grid-like Codes in World Models

## Quick Facts
- **arXiv ID**: 2510.16039
- **Source URL**: https://arxiv.org/abs/2510.16039
- **Reference count**: 40
- **Primary result**: Introduces GCQ, a spatiotemporal compression method using grid-like codes from CANNs that enables unified world modeling with stable long-horizon prediction and goal-directed planning.

## Executive Summary
This paper introduces Grid-like Code Quantization (GCQ), a novel method for compressing observation-action sequences using grid-like patterns derived from continuous attractor neural networks (CANNs). Unlike traditional vector quantization methods that compress static inputs, GCQ performs spatiotemporal compression through an action-conditioned codebook, where codewords are dynamically selected based on actions. This enables joint compression of space and time, serving as a unified world model. The resulting representation supports long-horizon prediction, goal-directed planning, and inverse modeling. Experiments across diverse tasks demonstrate GCQ's effectiveness in compact encoding and downstream performance, with superior long-range prediction compared to traditional two-stage models. The work offers both a computational tool for efficient sequence modeling and a theoretical perspective on the formation of grid-like codes in neural systems.

## Method Summary
GCQ compresses observation-action sequences using a codebook derived from continuous attractor neural networks (CANNs). The encoder maps observation sequences to latent states, which are then quantized by matching them against action-conditioned candidate trajectories within the codebook. This integrates spatial encoding and temporal dynamics into a single structured latent space. The decoder reconstructs observations from quantized states. The resulting representation enables planning and inverse modeling through a greedy distance operator that navigates the structured latent space.

## Key Results
- GCQ achieves stable long-horizon predictions compared to traditional two-stage world models, with ablation showing fixed codebooks outperform learnable alternatives
- The structured latent space enables computationally efficient goal-directed planning and inverse modeling through a greedy distance operator
- Experiments on 2DMaze and GSV datasets demonstrate superior reconstruction quality and prediction stability compared to VQ-VAE baselines

## Why This Works (Mechanism)

### Mechanism 1
An action-conditioned codebook derived from Continuous Attractor Neural Networks (CANNs) enables unified spatiotemporal compression, yielding more stable long-horizon predictions than traditional two-stage world models. A CANN generates a fixed set of discrete "bump" attractor states on a toroidal manifold, which serve as the codebook. Actions are mapped to deterministic transitions between these states. The encoder maps an observation sequence to latents, which are then quantized by matching them against action-conditioned candidate trajectories within the codebook. This integrates spatial encoding and temporal dynamics into a single structured latent space, preventing the drift often seen when separate temporal models operate on learned latent states. The core assumption is that environment dynamics can be approximated by discrete transitions on a low-dimensional toroidal manifold, and CANN dynamics accurately model these transitions.

### Mechanism 2
The resulting structured latent space acts as a "cognitive map," allowing computationally efficient goal-directed planning and inverse modeling via a greedy distance operator. Because actions correspond to movements on the map, a simple greedy operator can find the single action that moves a state one step closer to a goal. Planning is an iterative execution of this operator. Inverse modeling applies the operator to infer the action between two observed states. The core assumption is that a greedy pathfinding strategy is sufficient to reach goals, and the discrete action mapping covers the necessary movements without getting trapped in local minima.

### Mechanism 3
A fixed, CANN-derived codebook outperforms a learnable alternative in this architecture, indicating that stable geometric structure is more critical than data-driven code adaptation. The codebook is pre-computed from CANN dynamics and remains frozen during training. Only the encoder and decoder are trained to map data to/from this fixed structure. Ablation experiments show that allowing the codes to be learnable degrades performance, likely by disrupting the precise geometric relationships required for reliable action-conditioned transitions. The core assumption is that the CANN's inherent structure provides a sufficiently universal basis for representing the target domains.

## Foundational Learning

**Continuous Attractor Neural Networks (CANNs)**
- Why needed here: CANNs are the source of the grid-like codebook. Understanding bump attractors, their stability, and their movement on a manifold is essential to grasp GCQ's core mechanism.
- Quick check question: How does a "bump" of activity in a CANN maintain its shape while moving across the neural sheet in response to an input?

**Vector Quantization (VQ) & VQ-VAE**
- Why needed here: GCQ is a variant of VQ. You must understand the encoder-quantizer-decoder pipeline, the role