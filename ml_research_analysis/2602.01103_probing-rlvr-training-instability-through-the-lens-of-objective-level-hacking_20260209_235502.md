---
ver: rpa2
title: Probing RLVR training instability through the lens of objective-level hacking
arxiv_id: '2602.01103'
source_url: https://arxiv.org/abs/2602.01103
tags:
- training
- discrepancy
- step
- token
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a principled framework for understanding training
  instability in RLVR through objective-level hacking, where token-level credit misalignment
  creates spurious optimization signals that destabilize training. By analyzing the
  abnormal growth of training-inference discrepancy in MoE models, the authors show
  that this phenomenon stems from biased perturbations in the optimization objective
  rather than transient noise.
---

# Probing RLVR training instability through the lens of objective-level hacking

## Quick Facts
- arXiv ID: 2602.01103
- Source URL: https://arxiv.org/abs/2602.01103
- Reference count: 40
- Primary result: Objective-level hacking from token-level credit misalignment creates spurious optimization signals that destabilize RLVR training, particularly in MoE models

## Executive Summary
This work introduces a principled framework for understanding training instability in RLVR through objective-level hacking, where token-level credit misalignment creates spurious optimization signals that destabilize training. By analyzing the abnormal growth of training-inference discrepancy in MoE models, the authors show that this phenomenon stems from biased perturbations in the optimization objective rather than transient noise. Through extensive experiments on a 30B MoE model, they demonstrate that token-level clipping and injected weight distortions accelerate discrepancy growth and degrade model performance. The study reveals that variance-based perturbations don't trigger instability, while biased distortions do, establishing a causal link between objective-level hacking and training collapse.

## Method Summary
The study investigates RLVR training instability in a 30B MoE model using the verl framework with vLLM inference backend and Megatron training backend. The core method tracks training-inference discrepancy via importance weight standard deviation (std(ρi,t)) across training steps. Experiments employ GRPO with token-level clipping, comparing vanilla training against truncated importance sampling (TIS) correction. Key interventions include clipping strength ablation (varying right clipping range from 0.2 to 0.28) and controlled weight distortion injection (δ ∈ {1.2, 2, 3}) to establish causality between biased perturbations and instability.

## Key Results
- Training-inference discrepancy grows abnormally during RLVR training, correlating with entropy collapse and validation performance degradation
- Token-level clipping accelerates discrepancy growth; stronger clipping leads to faster instability onset
- Biased weight distortions (not variance-only noise) trigger the instability mechanism, establishing objective-level hacking as the root cause
- The discrepancy growth exhibits a positive feedback loop, making collapse irreversible once initiated
- Truncated importance sampling (TIS) reduces but doesn't eliminate the discrepancy growth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level credit misalignment introduces spurious signals into the optimization objective, driving training-inference discrepancy growth.
- Mechanism: When rollout samples come from π_infer rather than π_train, the effective objective J'(θ) = J(θ) + ΔJ(θ) where ΔJ(θ) ≃ Σ Cov_train(X_{i,t}, ρ⁻¹_{i,t}). This covariance term creates unintended optimization pressure: when advantage Â_{i,t} > 0, the model is incentivized to favor tokens with larger ρ⁻¹ = π_infer/π_train, creating pathological correlation pursuit.
- Core assumption: The covariance between token weights and importance ratios persists across training steps and can be systematically exploited by gradient descent.
- Evidence anchors:
  - [abstract] "objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective"
  - [Section 3.1] Derivation showing ΔJ(θ) ≃ Σ Cov_train(X_{i,t}(θ), ρ⁻¹_{i,t})
  - [corpus] Weak direct corpus support; related work on RLVR training stability exists but doesn't formalize this covariance mechanism.
- Break condition: If π_train = π_infer exactly (true on-policy sampling), then ρ_{i,t} = 1 and ΔJ(θ) = 0.

### Mechanism 2
- Claim: Token-level clipping, while intended to stabilize training, can introduce objective-level hacking that accelerates discrepancy growth.
- Mechanism: Token clipping with threshold set S_clip creates weight modulation φ_{i,t} ∈ {0,1}, producing Δ_clip J(θ) = E_train[Σ X_{i,t}(φ_{i,t} - 1)]. This reweighting introduces unintended correlations between X_{i,t} and φ_{i,t} - 1. Experiments show stronger clipping → faster discrepancy growth (Fig. 4c).
- Core assumption: The clipping threshold correlates with token probability distributions in a way that creates systematic bias (not just variance).
- Evidence anchors:
  - [Section 3.2] "token-level clipping...could also act as a generalized form of token-level credit misalignment"
  - [Section 4.3] Fig. 4 and Table 1 showing correlation between clip strength and discrepancy growth rate
  - [Section 5.1] Variance-only noise injection (Eq. 21) does NOT trigger discrepancy growth, confirming bias is required.
- Break condition: Sequence-level clipping (GSPO) does not exhibit the same anomalous growth, suggesting token-level granularity is essential to the mechanism.

### Mechanism 3
- Claim: A positive feedback loop between training-inference discrepancy and objective-level hacking makes instability irreversible.
- Mechanism: Low-probability tokens exhibit wider ρ_{i,t} dispersion from 1. Survival bias during sampling from π_infer means sampled tokens tend to have higher π_infer than π_train on average (E[ρ⁻¹] ≥ 1). As discrepancy grows, survival bias intensifies, creating more weight distortion, which drives further discrepancy—a self-reinforcing cycle.
- Core assumption: The model cannot recover from parameter regions that amplify discrepancy once it enters them.
- Evidence anchors:
  - [Section 5.3] Fig. 7 showing ρ_{i,t} for low-probability tokens decreases consistently during training
  - [Section C.2] Mathematical proof that E_{x∼p}[p(x)/q(x)] ≥ 1
  - [Section 5.3] "collapse is often irreversible. Switching to earlier checkpoints or altering training data batches fails to prevent the onset of collapse"
- Break condition: Intervening early (before feedback loop amplifies) with objective correction (e.g., TIS) can slow or arrest growth.

## Foundational Learning

- Concept: **Importance Sampling and Policy Ratio**
  - Why needed here: The entire framework relies on understanding ρ_{i,t} = π_train/π_infer and how distribution mismatch propagates through gradient updates.
  - Quick check question: If π_infer systematically assigns higher probability to certain tokens than π_train, what happens to E[ρ⁻¹]?

- Concept: **Covariance in Gradient Signals**
  - Why needed here: The key insight is that ΔJ(θ) depends on Cov(X_{i,t}, ρ⁻¹), not just the individual terms. Understanding how correlated bias differs from uncorrelated noise is essential.
  - Quick check question: Why does Gaussian noise ξ_{i,t} ∼ N(1, σ²) not trigger discrepancy growth while biased weight distortion does?

- Concept: **GRPO Objective Structure**
  - Why needed here: The paper's analysis modifies the standard GRPO objective; understanding the baseline L_clip(θ) formulation is prerequisite to seeing how ΔJ(θ) perturbs it.
  - Quick check question: In Eq. (2), what does the clipping term clip(r_{i,t}, 1-ε, 1+ε) achieve, and how might this interact with token-level credit assignment?

## Architecture Onboarding

- Component map:
  - vLLM (rollout engine) -> Megatron (training backend) -> Verifiers (reward computation) -> GRPO/GSPO (policy optimizer)

- Critical path:
  1. Synchronize θ between rollout engine and training backend
  2. Sample responses from π_infer
  3. Compute advantages Â_{i,t} = (R_i - mean)/std
  4. Compute importance ratios r_{i,t} and ρ_{i,t}
  5. Apply objective correction (e.g., TIS) if needed to counter ΔJ(θ)

- Design tradeoffs:
  - **Token-level vs. sequence-level clipping**: Token-level offers finer-grained control but introduces objective-level hacking risk; sequence-level more stable for MoE but may be less responsive.
  - **Numerical precision in rollout**: BF16/FP8 improves throughput but creates discretization artifacts in low-probability token distributions (Fig. 13).
  - **Importance sampling correction**: TIS reduces bias but adds variance; clipping importance weights (IcePop) trades off bias-variance differently.

- Failure signatures:
  - **Early warning**: Training-inference discrepancy (mismatch metric) increases before token entropy or gradient norm anomalies appear (Fig. 14)
  - **Irreversible collapse**: Once discrepancy growth accelerates, reverting to earlier checkpoints doesn't help
  - **Abnormal clustering**: Token probabilities in π_infer show band-like discretization (Fig. 3b)

- First 3 experiments:
  1. **Baseline measurement**: Track std(ρ_{i,t}) across training steps to establish discrepancy growth curve for your specific rollout/training configuration.
  2. **TIS ablation**: Implement truncated importance sampling correction and compare discrepancy growth rate against vanilla GRPO. Expect reduced growth but not elimination.
  3. **Clipping strength sweep**: Vary token-level clipping thresholds and measure correlation between clip ratio and discrepancy growth. If strong positive correlation, consider switching to sequence-level clipping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms beyond token-level clipping and initial discrepancy contribute to objective-level hacking in RLVR?
- Basis in paper: [Explicit] The authors state that their theoretical explanation "does not represent the full picture of model collapse" and that sources of hacking are "broader than anticipated."
- Why unresolved: The paper formalizes the mechanism for discrepancy and clipping but acknowledges that "diverse mechanisms and processes may also contribute."
- What evidence would resolve it: A comprehensive ablation study identifying other forms of token-level weight modulation (e.g., from KL penalties or reward normalization) that statistically correlate with discrepancy growth.

### Open Question 2
- Question: Does the objective-level hacking mechanism exhibit the same intensity and dynamics in dense LLM architectures as in Mixture-of-Experts (MoE) models?
- Basis in paper: [Inferred] The study focuses exclusively on a 30B MoE model, noting that instabilities are "particularly prevalent" in MoEs, while only briefly citing similar entropy phenomena in dense models in footnotes.
- Why unresolved: The sensitivity of MoE routing may exacerbate the discrepancy, but it remains unverified if the formalized $\Delta J(\theta)$ drives collapse in dense models at the same rate.
- What evidence would resolve it: Parallel training experiments on dense models of equivalent scale, measuring the growth of training-inference discrepancy relative to injected token-level distortions.

### Open Question 3
- Question: Can a modified GRPO algorithm be designed that theoretically neutralizes the spurious signal $\Delta J(\theta)$ while preserving the benefits of token-level credit assignment?
- Basis in paper: [Explicit] The paper concludes by offering "guidance for the design of stable RLVR algorithms," and notes that existing corrections like Truncated Importance Sampling (TIS) only "reduce" rather than eliminate the discrepancy.
- Why unresolved: While TIS mitigates growth, it does not address the root cause of "dual effects" from token-level modulation; a principled algorithm design based on the $\Delta J(\theta)$ formulation is missing.
- What evidence would resolve it: A derivation of a new optimization objective that mathematically cancels out the covariance term in Eq. (10), validated by stable training dynamics over prolonged steps.

## Limitations

- The framework focuses on importance weight divergence but doesn't address other potential sources of instability like catastrophic forgetting or reward hacking
- The experimental validation uses a 30B MoE model, raising questions about generalizability to dense models or smaller parameter counts
- While weight distortion experiments establish correlation, the exact causal pathway from objective-level hacking to validation performance degradation remains partially inferred

## Confidence

**High confidence**: The mathematical derivation of ΔJ(θ) ≃ Σ Cov_train(X_{i,t}, ρ⁻¹_{i,t}) and its implications for spurious optimization signals. The experimental observation that variance-only perturbations don't trigger instability while biased distortions do. The positive feedback loop mechanism between discrepancy growth and objective-level hacking.

**Medium confidence**: The practical significance of early warning detection via discrepancy growth before entropy collapse. The assertion that training-inference mismatch in numerical precision (BF16 vs FP32) is the primary driver of initial discrepancy. The irreversibility of collapse once the feedback loop initiates.

**Low confidence**: The universality of the proposed framework across different RLVR algorithms beyond GRPO/GSPO. The precise quantitative relationship between clipping strength and collapse risk across diverse model architectures. The scalability of the framework to trillion-parameter models where distributed training introduces additional sources of misalignment.

## Next Checks

1. **Cross-architecture validation**: Test the framework on dense transformer models (e.g., Llama 3 70B) and smaller MoE variants to determine if the discrepancy-growth-collapse relationship holds universally or is MoE-specific.

2. **Temporal intervention study**: Implement automated detection of early discrepancy growth and apply corrective measures (TIS, checkpoint rollback, learning rate adjustment) at different intervention points to quantify the "point of no return" and validate the irreversibility claim.

3. **Algorithm ablation across RLVR variants**: Compare vanilla GRPO, PPO, and proximal policy variants under identical conditions to isolate whether the objective-level hacking mechanism is specific to the clipping-based objective formulation or represents a broader class of RLVR instability phenomena.