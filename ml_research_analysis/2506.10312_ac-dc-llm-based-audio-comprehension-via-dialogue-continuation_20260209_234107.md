---
ver: rpa2
title: 'AC/DC: LLM-based Audio Comprehension via Dialogue Continuation'
arxiv_id: '2506.10312'
source_url: https://arxiv.org/abs/2506.10312
tags:
- audio
- training
- proposed
- data
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of instruction-following audio
  comprehension by proposing a novel dialogue continuation training method for large
  language models (LLMs). The key innovation is to train an audio captioning model
  to generate dialogue responses rather than direct captions, where responses are
  generated by the LLM from ground-truth captions without task-specific instructions.
---

# AC/DC: LLM-based Audio Comprehension via Dialogue Continuation

## Quick Facts
- arXiv ID: 2506.10312
- Source URL: https://arxiv.org/abs/2506.10312
- Reference count: 0
- Primary result: Achieves 47.7% average AQA accuracy on AudioBench benchmarks, significantly outperforming caption-based training

## Executive Summary
This paper addresses the challenge of instruction-following audio comprehension by proposing a novel dialogue continuation training method for large language models (LLMs). The key innovation is to train an audio captioning model to generate dialogue responses rather than direct captions, where responses are generated by the LLM from ground-truth captions without task-specific instructions. This approach mitigates the caption variation problem and enables the model to follow unseen instructions in a zero-shot manner. Experiments on AudioCaps, WavCaps, and Clotho datasets demonstrate that the proposed method significantly improves audio question-answering accuracy compared to conventional caption-based training, while maintaining competitive audio captioning performance.

## Method Summary
The method involves training an adapter-only model where an audio encoder (EAT-base) extracts features from audio input, which are then projected through a 2-layer linear adapter with 5x downsampling into the LLM's embedding space. A frozen Llama-3-8B-Instruct LLM processes these embeddings. During training, ground-truth captions are fed to the LLM without instructions to generate dialogue responses, which become the training targets. The adapter learns to map audio features to embeddings that, when processed by the LLM, reproduce these responses. At inference, the model can follow new instructions by combining them with the semantic audio representation generated by the adapter.

## Key Results
- Achieves 47.7% average AQA accuracy on AudioBench benchmarks compared to 37.5% for caption-based training
- Maintains competitive audio captioning performance (43.13-46.20 vs 51.43-52.55 for baselines)
- Demonstrates effective zero-shot instruction-following capability without multitask instruction tuning
- Successfully mitigates the caption variation problem inherent in audio captioning datasets

## Why This Works (Mechanism)

### Mechanism 1
The Proxy Task of Dialogue Continuation allows the adapter to learn a semantic representation of audio decoupled from specific tasks or vocabulary. Instead of mapping audio to a single "correct" caption, the adapter learns to produce embeddings that trigger appropriate LLM responses, capturing the underlying semantic content.

### Mechanism 2
Preserved Instruction-Following via Frozen Weights and Unbiased Training maintains the LLM's original instruction-following capability by freezing its weights and training with responses generated without instructions. This enables zero-shot generalization to novel instructions at inference time.

### Mechanism 3
Semantic Alignment over Surface-Level Alignment mitigates the caption variation problem by training on LLM responses rather than direct captions. The adapter learns to map audio to semantic concepts rather than specific text strings, smoothing the learning objective.

## Foundational Learning

**Concept: Frozen Pre-trained LLM**
- Why needed here: The method hinges on using a pre-trained LLM as a fixed semantic engine that the adapter translates audio into
- Quick check question: Can you explain why the LLM's weights are frozen during the adapter training phase?

**Concept: Adapter / Projection Layer**
- Why needed here: This small neural network bridges the modality gap between audio features and text embeddings
- Quick check question: How does the adapter transform the output of the audio encoder so it can be processed by the LLM?

**Concept: Zero-Shot Generalization**
- Why needed here: The key result is enabling the model to perform new tasks without explicit training on them
- Quick check question: What property of the training method allows the model to follow instructions it has never seen before?

## Architecture Onboarding

**Component map:**
EAT-base audio encoder (frozen) -> 2-layer linear adapter with 5x downsampling -> Llama-3-8B-Instruct (frozen) -> LLM-generated response

**Critical path:**
1. Data Preparation: Ground-truth captions are fed into the LLM with no-instruction prompts to generate dialogue responses
2. Training: The adapter is trained to produce embeddings that, when processed by the LLM, predict the pre-generated responses
3. Inference: User provides audio and instruction; the adapter produces embeddings that combine with the new instruction for the LLM to generate a response

**Design tradeoffs:**
- Pros: Eliminates need for expensive multitask instruction tuning; mitigates caption variation problem; preserves strong zero-shot instruction-following
- Cons: Slightly lower performance on original captioning task; scalability limited by small number of learnable parameters

**Failure signatures:**
- Ignoring Instructions: Model produces descriptive captions even when given specific questions
- Inconsistent Responses: Model answers same question differently for same audio across runs
- No Improvement from More Data: Adding data doesn't improve performance due to limited adapter capacity

**First 3 experiments:**
1. Reproduce AQA Accuracy Comparison: Train models with standard vs dialogue continuation targets on AudioCaps, evaluate on Clotho-AQA
2. Ablation on Prompting: Test model with different types of unseen instructions to map zero-shot capabilities
3. Test Adapter Capacity: Increase adapter size and train on combined dataset to see if performance improves

## Open Questions the Paper Calls Out

**Open Question 1:** Does jointly finetuning the audio encoder with the adapter improve AQA accuracy when using dialogue continuation training? The authors note that adding more training data yielded no significant improvement and speculate finetuning the audio encoder may help.

**Open Question 2:** Can combining dialogue continuation training with explicit multitask instruction-tuning enhance task-specific performance while preserving zero-shot generalization? The authors plan to explore this combination to balance task-specific performance and generalization.

**Open Question 3:** Would generating multiple diverse responses per caption improve AQA generalization or robustness? The authors used only a single sampled response per caption and claim sufficiency, but haven't tested the impact of response diversity.

**Open Question 4:** Why does LoRA finetuning improve caption-targeted training but show inconsistent effects with dialogue continuation targets? The authors observed improvement with conventional targets but inconsistent behavior with DCT targets.

## Limitations
- Response preparation process is computationally expensive, creating significant preprocessing bottleneck
- Adapter's small size (2-layer linear with 5x downsampling) may limit ability to capture complex audio semantics
- Method's effectiveness depends critically on quality and diversity of LLM's responses during offline preparation

## Confidence
- **High Confidence:** Dialogue continuation training improves zero-shot AQA accuracy compared to caption-based training (47.7% vs 37.5% on Clotho-AQA)
- **Medium Confidence:** Method mitigates caption variation problem and preserves LLM's instruction-following capability
- **Low Confidence:** Scalability to larger datasets or more complex audio domains

## Next Checks
1. Validate Response Preparation Quality: Analyze 100 generated responses to confirm they are dialogue-style and semantically consistent with captions
2. Test Frozen LLM Behavior Under Distribution Shift: Measure zero-shot performance on held-out instructions to quantify degradation in instruction-following capability
3. Scale Adapter Capacity Experiment: Train with adapter without downsampling and compare AQA accuracy and captioning performance