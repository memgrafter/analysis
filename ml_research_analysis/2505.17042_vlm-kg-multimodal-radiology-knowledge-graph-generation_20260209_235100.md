---
ver: rpa2
title: 'VLM-KG: Multimodal Radiology Knowledge Graph Generation'
arxiv_id: '2505.17042'
source_url: https://arxiv.org/abs/2505.17042
tags:
- radiology
- knowledge
- graph
- generation
- vlm-kg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first multimodal VLM-based framework
  for radiology knowledge graph generation, addressing the limitations of existing
  unimodal approaches by incorporating both radiology reports and images. The method
  uses a pretrained LLM with visual instruction tuning, integrating MedCLIP embeddings
  via a transformer-based projector.
---

# VLM-KG: Multimodal Radiology Knowledge Graph Generation

## Quick Facts
- arXiv ID: 2505.17042
- Source URL: https://arxiv.org/abs/2505.17042
- Reference count: 30
- First multimodal VLM-based framework for radiology knowledge graph generation, achieving BLEU-1 scores of 54.98 and 44.76 on MIMIC-CXR and IU-Xray datasets respectively

## Executive Summary
This paper introduces VLM-KG, the first multimodal framework for radiology knowledge graph generation that combines radiology reports and chest X-ray images. The method uses a pretrained LLM with visual instruction tuning, integrating MedCLIP embeddings via a transformer-based projector. The resulting model significantly outperforms previous text-only approaches, achieving substantially higher BLEU scores and generating more clinically accurate knowledge graphs with fewer hallucinated relations. The study demonstrates the value of incorporating visual information alongside textual data for structured information extraction in radiology.

## Method Summary
VLM-KG employs a two-stage training approach. First, a text-only instruction-tuned LLM (Qwen 1.5-0.5B) learns the knowledge graph structure from radiology report triplets. Second, visual instruction tuning integrates MedCLIP image embeddings through an 8-layer transformer projector that maps 512-dim visual features to the LLM's 1024-dim space. The framework uses the RadGraph schema with three relations (modify, located_at, suggestive_of) and is trained on MIMIC-CXR and IU-Xray datasets, achieving significant improvements over text-only baselines.

## Key Results
- VLM-KG achieves BLEU-1 scores of 54.98 (MIMIC-CXR) and 44.76 (IU-Xray), compared to 24.57 for the prior Dygiee++ model
- The multimodal approach generates more accurate knowledge graphs with fewer hallucinated relations
- VLM-KG significantly outperforms both text-only and other multimodal approaches on radiology knowledge graph generation tasks

## Why This Works (Mechanism)

### Mechanism 1
Multimodal visual grounding reduces hallucinated relations by providing cross-modal constraints. MedCLIP embeddings serve as a regularizing signal, suppressing relations inconsistent with observed radiographic patterns. This works when visual embeddings capture clinically relevant features that correlate with textual entities.

### Mechanism 2
Two-stage training enables schema learning before visual integration. Stage 1 establishes entity-relation understanding from text alone, while Stage 2 integrates visual modality without destabilizing early learning. This prevents the modality gap from interfering with initial schema acquisition.

### Mechanism 3
The transformer-based projector with learned prefix tokens enables effective cross-modal embedding translation. The prefix attends to image features, extracting task-relevant information rather than naively projecting all visual features. This allows selective transfer of clinically salient patterns.

## Foundational Learning

- **Knowledge Graphs in Radiology**: The model outputs triplets using RadGraph schema with three relations: modify, located_at, suggestive_of. Understanding this structure is essential for interpreting outputs.
  - Quick check question: Given the triplet `['opacity', 'suggestive_of', 'infection']`, what does the relation type indicate about clinical reasoning?

- **Vision-Language Model Alignment**: VLM-KG bridges MedCLIP and Qwen via a learned projector. Understanding embedding space misalignment helps diagnose convergence issues.
  - Quick check question: Why would a simple linear layer fail where a transformer projector succeeds for this task?

- **Instruction Tuning vs. Visual Instruction Tuning**: The training pipeline separates these stages. Conflating them leads to implementation errors.
  - Quick check question: What components are trained in Stage 1 (LLM-KG) vs. Stage 2 (VLM-KG)?

## Architecture Onboarding

- **Component map**: [Radiology Image] → MedCLIP (frozen, 512-dim) → Projector (8-layer transformer, trainable) → [Token Embeddings] → LLM (Qwen 1.5-0.5B, fine-tuned) → [KG Triplets]

- **Critical path**: 
  1. Verify MedCLIP outputs 512-dim embeddings per image
  2. Projector must output sequence matching LLM embedding dimension (1024)
  3. Concatenation order: [projector_output, instruction_tokens]
  4. Cross-entropy loss computed only on output triplets, not input

- **Design tradeoffs**:
  - LLM size: Qwen 1.5-0.5B chosen for computational efficiency; tradeoff is potential underperformance on complex reasoning
  - Projector complexity: 8-layer transformer vs. MLP; tradeoff is more parameters but better selectivity
  - Prefix/clip lengths: n=64, k=64 optimal; higher values cause performance degradation

- **Failure signatures**:
  - Low BLEU with high ROUGE: Model captures entities but not structure—check instruction formatting
  - Hallucinated relations: Visual modality not contributing—verify projector gradient flow
  - Invalid triplet format: Post-processing regex failing—check LLM tokenization

- **First 3 experiments**:
  1. Reproduce LLM-KG baseline: Train text-only instruction-tuned model on MIMIC KG triplets. Target: BLEU-4 ~34.
  2. Ablate projector architecture: Replace transformer projector with single MLP layer. Expect significant BLEU drop.
  3. Validate multimodal contribution: Run inference with (a) image + report, (b) report only, (c) image only. Compare hallucination rates.

## Open Questions the Paper Calls Out

### Open Question 1
Can the VLM-KG framework generalize to non-chest radiology modalities (e.g., CT or MRI) where visual features differ significantly from chest X-rays?
- Basis: Authors state models are limited to MIMIC-CXR and IU-Xray datasets
- Why unresolved: Vision encoder and projector trained exclusively on chest radiographs
- What evidence would resolve it: Evaluating zero-shot or fine-tuned performance on CT/MRI datasets

### Open Question 2
Does scaling the LLM backbone beyond 0.5B parameters yield significant improvements?
- Basis: Paper uses specific 0.5B model but does not test larger variants
- Why unresolved: Uncertain if current model hits performance ceiling
- What evidence would resolve it: Ablation study with larger LLM backbones (7B or 13B)

### Open Question 3
Do reported BLEU and ROUGE scores correlate strongly with clinical factuality?
- Basis: Evaluation limited to 50 radiologist-annotated samples
- Why unresolved: Small test set size and reliance on NLG metrics rather than semantic truth
- What evidence would resolve it: Comprehensive evaluation with larger test set assessed by medical experts

## Limitations
- Evaluation relies entirely on automated metrics without human clinical validation of generated knowledge graphs
- Training depends heavily on Dygiee++ for initial KG triplets, potentially propagating baseline errors
- Visual grounding assumes MedCLIP embeddings capture clinically relevant features without comprehensive ablation study

## Confidence

**High Confidence Claims:**
- VLM-KG outperforms Dygiee++ on MIMIC-CXR and IU-Xray datasets using automated metrics
- Two-stage training approach is implementable as described
- Transformer projector architecture is more effective than linear projection

**Medium Confidence Claims:**
- Visual modality reduces hallucinated relations in knowledge graph generation
- MedCLIP embeddings provide clinically relevant visual grounding
- Prefix-based projector selectively transfers important visual features

**Low Confidence Claims:**
- Clinical utility without human validation
- Generalization to non-chest X-ray modalities
- Performance under domain shift

## Next Checks

1. **Clinical Expert Review**: Have 3-5 board-certified radiologists manually review 100 randomly selected VLM-KG generated triplets, assessing accuracy and clinical relevance. Target: >90% expert agreement on correct relation assignments.

2. **Ablation of Visual Grounding**: Generate knowledge graphs using three conditions: (a) full VLM-KG with images, (b) text-only LLM-KG, (c) image-only MedCLIP features without text. Compare hallucination rates using the 50 expert-annotated samples. Expect: VLM-KG shows 50% reduction in hallucinated relations vs text-only.

3. **Cross-Institution Validation**: Fine-tune VLM-KG on MIMIC-CXR training set, then evaluate on held-out dataset from different institution (NIH ChestX-ray8 or RSNA). Measure performance drop and analyze which relation types are most affected by domain shift.