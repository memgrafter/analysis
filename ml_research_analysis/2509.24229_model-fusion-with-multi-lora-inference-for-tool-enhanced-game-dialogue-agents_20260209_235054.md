---
ver: rpa2
title: Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents
arxiv_id: '2509.24229'
source_url: https://arxiv.org/abs/2509.24229
tags:
- task
- dialogue
- function
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the opdainlp team's solution for the GPU track
  of the CPDC 2025 challenge, achieving first place in Task 1 and Task 3, and second
  place in Task 2. The challenge required building an in-game conversational AI that
  adheres to character personas, aligns with the game's worldview, and supports function
  calling.
---

# Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents

## Quick Facts
- arXiv ID: 2509.24229
- Source URL: https://arxiv.org/abs/2509.24229
- Reference count: 7
- Primary result: First place in CPDC 2025 GPU track Task 1 and Task 3, second in Task 2

## Executive Summary
This paper presents the opdainlp team's solution for the CPDC 2025 challenge, achieving first place in Task 1 and Task 3, and second place in Task 2. The challenge required building an in-game conversational AI that adheres to character personas, aligns with the game's worldview, and supports function calling. The team employed Qwen3-14B with LoRA fine-tuning and model fusion, using a base model integrated with multiple LoRA adapters during inference. Specifically, they used three distinct LoRA adapters to handle tool calling, response generation with tool call results, and response generation without tool call results, respectively. MultiLoRA inference was implemented using vLLM. To address data limitations, they synthesized additional training data using commercial LLM APIs and employed model fusion techniques. Their approach achieved an automatic score of 0.635 on Task 3, securing first place in that task and demonstrating the effectiveness of their MultiLoRA framework for tool-enhanced game dialogue agents.

## Method Summary
The approach uses Qwen3-14B as the base model with three specialized LoRA adapters trained separately: one for tool-calling prediction, one for response generation with tool results, and one for response generation without tool results. The adapters are configured with rank=128, alpha=128, weight_decay=0.1, warmup_ratio=0.05, cosine scheduler, batch_size=8, and trained for 3 epochs on 8× A30 GPUs using ms-swift with DeepSpeed ZeRO-3 + FlashAttention. Model fusion is applied through checkpoint averaging across training epochs, and for the no-tool-response adapter, also averaging across weights from different API-synthesized datasets. Inference uses vLLM MultiLoRA on L40s (48GB) GPUs with a 7s/turn timeout, routing to the appropriate adapter based on tool-calling predictions.

## Key Results
- Achieved first place in CPDC 2025 GPU track Task 1 (task-oriented dialogue with function calling) and Task 3 (combined evaluation)
- Secured second place in Task 2 (context-aware dialogue without function calling)
- Automatic score of 0.635 on Task 3
- Demonstrated that specialized LoRA adapters outperform unified adapters in low-data regimes

## Why This Works (Mechanism)

### Mechanism 1: Task-Decoupled Multi-Adapter Architecture
- Claim: Separating tool-calling, response-with-tools, and response-without-tools into three specialized LoRA adapters improves performance over a single unified adapter in low-data regimes.
- Mechanism: Each adapter optimizes for a distinct sub-task distribution, preventing gradient interference and allowing scenario-specific input formulations (e.g., weapon info for tool-calling vs. worldview for persona dialogue). MultiLoRA inference via vLLM enables dynamic adapter routing based on whether tool calls are detected.
- Core assumption: Task distributions are sufficiently distinct that joint training causes interference; inference-time routing can correctly identify the scenario.
- Evidence anchors:
  - The authors discovered that merging datasets from different scenarios (e.g., tool calling and dialogue) affected performance in this low-sample setting
  - A single LoRA adapter for NPC responses, shared across both cases with and without function call results, performed worse than the non-fine-tuned Qwen3-14b model
  - Related work (LRAgent, Loquetier) shows multi-LoRA enables role specialization, but does not directly validate task-decoupling for dialogue agents

### Mechanism 2: LoRA Checkpoint Averaging for Generalization
- Claim: Averaging LoRA checkpoints across training epochs yields better generalization than selecting any single epoch's checkpoint.
- Mechanism: Checkpoint averaging smooths parameter updates, reducing overfitting to specific training batches—particularly valuable with limited data (40 conversations per task). The paper averages 3 epochs' checkpoints post-training.
- Core assumption: The loss landscape is sufficiently smooth that parameter averaging remains within a high-performance basin; later epochs do not catastrophically diverge.
- Evidence anchors:
  - Table 3 shows LoRA Fusion (0.562 Task3) outperforming 1-epoch (0.554), 2-epoch (0.556), and 3-epoch (0.546) single checkpoints
  - Related fusion work (Deotte et al. 2024, Kim et al. 2024) is cited but not detailed
  - Limited direct corpus validation for checkpoint averaging in dialogue

### Mechanism 3: Sequential Synthetic Data Generation
- Claim: Generating synthetic dialogue by feeding turns sequentially (updating history with generated responses) outperforms generating full dialogues at once.
- Mechanism: Sequential generation maintains local coherence and allows the synthetic model to condition on progressively built context, producing more natural dialogue flows for training.
- Core assumption: Commercial LLM APIs produce higher-quality responses than the original limited training data; sequential conditioning reduces compounding errors.
- Evidence anchors:
  - The authors' evaluation showed that the second synthesis approach yielded better performance when used for training
  - Table 5 shows averaging LoRA weights from multiple API-synthesized datasets improves Task 2 from 0.587 (original) to 0.615
  - Synthetic data augmentation is common but sequential-vs-batch comparison lacks external evidence

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: Core parameter-efficient fine-tuning method enabling multiple specialized adapters on a single 14B model within GPU memory constraints.
  - Quick check question: Can you explain why LoRA enables training multiple adapters without duplicating the full base model?

- Concept: **Model Fusion / Weight Averaging**
  - Why needed here: Checkpoint averaging is a key performance booster; understanding when and why weight averaging helps is essential for reproduction.
  - Quick check question: Under what conditions would averaging checkpoints from different epochs degrade rather than improve performance?

- Concept: **Tool-Augmented Dialogue Systems**
  - Why needed here: The system interleaves function calling with natural language generation; understanding the Hermes function-calling format and action vs. tool function distinction is critical.
  - Quick check question: How should the system handle a user query that requires both information retrieval (tool_function) and an in-game action (action_function)?

## Architecture Onboarding

- Component map: Base Model (Qwen3-14B) -> LoRA Adapter 1 (Tool-calling) -> LoRA Adapter 2 (Response with tools) -> LoRA Adapter 3 (Response without tools) -> vLLM MultiLoRA Inference Engine

- Critical path:
  1. Input preprocessing (extract state, history, knowledge, function list)
  2. Tool-calling inference with Adapter 1 (include weapon info, exclude worldview)
  3. If tool results exist → Response generation with Adapter 2 (include persona + weapon info + tool results)
  4. If no tool results → Response generation with Adapter 3 (include worldview, exclude weapon info)
  5. Return NPC response

- Design tradeoffs:
  - Specialization vs. complexity: Three adapters increase routing complexity but prevent task interference
  - Input formulation per adapter: Including irrelevant context (e.g., worldview during tool-calling) degrades performance—careful feature selection per stage is critical
  - Synthetic data sources: Multiple APIs provide diversity but may introduce inconsistent styles; averaging LoRA weights mitigates single-source bias

- Failure signatures:
  - Tool-calling adapter fails to predict functions when coreference cues are missing → ensure dialogue history is included in input
  - Response generation includes unwanted action descriptions (e.g., *Looks up*) → enforce prompt constraints forbidding bracketed actions
  - Timeouts on Task 2 → avoid larger models (Qwen3-30B-A3B failed due to latency); validate inference speed on target hardware

- First 3 experiments:
  1. **Baseline adapter comparison**: Train single unified adapter vs. three specialized adapters on Task 1 data; measure function-call accuracy and BLEURT scores.
  2. **Checkpoint ablation**: Compare 1-epoch, 2-epoch, 3-epoch, and averaged checkpoints for each adapter; identify which adapter benefits most from fusion.
  3. **Synthetic data source ablation**: Train Adapter 3 on original Task 2 data only vs. GPT-4.1-only vs. Claude-only vs. Qwen-Max-only vs. averaged; measure Task 2 automatic score.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does merging training datasets from heterogeneous scenarios (tool-calling vs. dialogue) degrade performance in low-sample settings, and does this effect persist with larger training corpora?
- Basis in paper: The authors state: "we found that merging datasets from different scenarios (e.g., tool calling and dialogue) affected performance in this low-sample setting."
- Why unresolved: The paper reports the observation but does not investigate the underlying cause (e.g., gradient interference, task confusion, capacity limitations in LoRA).
- What evidence would resolve it: Ablation studies varying dataset size and measuring task-specific validation losses during joint vs. separate training.

### Open Question 2
- Question: What mechanisms explain why sequential dialogue synthesis (turn-by-turn generation with history updates) outperforms whole-dialogue synthesis for training data augmentation?
- Basis in paper: The authors tested two synthesis strategies and report: "the second synthesis approach yielded better performance when used for training" without further analysis.
- Why unresolved: The comparison is presented as an empirical finding without investigation into error accumulation, coherence, or distributional differences between the two approaches.
- What evidence would resolve it: Comparative analysis of synthesized dialogue statistics (length, coherence scores, diversity) and controlled training experiments matching sample counts.

### Open Question 3
- Question: Can the multi-adapter LoRA architecture and fusion strategies generalize beyond weapon shop merchant NPCs to other character roles and domains?
- Basis in paper: The paper notes: "in the provided training set, all NPCs are weapon shop merchants" and evaluates only on this narrow domain, leaving generalization untested.
- Why unresolved: The system's architecture and prompts may contain domain-specific optimizations (e.g., weapon information inclusion) that could require re-engineering for other NPC types.
- What evidence would resolve it: Cross-domain evaluation on diverse NPC roles (quest givers, healers, traders of other item types) using the same architecture with minimal prompt modifications.

## Limitations

- Dataset and API dependencies: The approach relies on proprietary CPDC 2025 data and commercial LLM APIs (GPT-4.1, Claude-Sonnet-4, Qwen-Max) for synthetic data generation, making exact replication difficult without access to these resources.
- Task distribution specificity: The three-task separation is optimized for this challenge's particular setup and may not generalize to domains with different tool-calling patterns or dialogue structures.
- Routing reliability: The inference-time adapter selection depends on accurate tool-calling detection, but no explicit validation of routing accuracy is provided.

## Confidence

- High Confidence: The core LoRA-based MultiLoRA architecture and the empirical finding that specialized adapters outperform unified ones in this low-data regime. The checkpoint averaging benefit is also well-supported by the provided ablation table.
- Medium Confidence: The sequential synthetic data generation approach's superiority over batch generation, as this relies on limited validation and the synthetic data prompts remain unspecified.
- Low Confidence: Generalization claims beyond the CPDC 2025 setting, particularly whether the task-decoupling strategy transfers to other dialogue systems with different tool distributions or higher training data volumes.

## Next Checks

1. **Routing accuracy validation**: Implement logging to measure how often the tool-calling adapter correctly identifies when tool calls are needed vs. when natural dialogue is sufficient. Compare performance when routing is correct vs. incorrect.

2. **Synthetic data quality assessment**: Generate synthetic dialogues using the described sequential approach and conduct human evaluation comparing them to original Task 2 data. Measure coherence, persona consistency, and tool-call appropriateness.

3. **Adapter configuration ablation**: Train and evaluate all 7 possible combinations of the three adapters (individual, pairs, and full trio) on Task 1 data to confirm that the three-adapter configuration is optimal rather than just better than the single unified adapter baseline.