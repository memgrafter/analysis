---
ver: rpa2
title: Factor Graph-based Interpretable Neural Networks
arxiv_id: '2502.14572'
source_url: https://arxiv.org/abs/2502.14572
tags:
- factor
- concept
- explanations
- graph
- again
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGAIN is an interpretable neural network designed to generate comprehensible
  concept-level explanations under unknown perturbations. It addresses the challenge
  of generating comprehensible explanations when the input data is infused with malicious
  perturbations, which can misguide the model to produce incomprehensible explanations.
---

# Factor Graph-based Interpretable Neural Networks

## Quick Facts
- arXiv ID: 2502.14572
- Source URL: https://arxiv.org/abs/2502.14572
- Reference count: 40
- Key outcome: AGAIN achieves nearly 100% identification rate and 98% success rate in generating comprehensible concept-level explanations under unknown perturbations

## Executive Summary
AGAIN is an interpretable neural network designed to generate comprehensible concept-level explanations under unknown perturbations. Unlike existing solutions that rely on adversarial training with known perturbations, AGAIN directly integrates logical rules expressed as a factor graph during inference to identify and rectify logical errors in explanations. The method addresses the challenge of producing meaningful explanations when input data is infused with malicious perturbations that can misguide models to produce incomprehensible outputs.

## Method Summary
The paper proposes a novel approach that constructs factor graphs during inference to identify and correct logical errors in explanations without requiring knowledge of specific perturbations. The method consists of three key components: factor graph construction, explanatory logic errors identification, and explanation rectification. By leveraging logical rules encoded in the factor graph structure, AGAIN can detect when explanations violate these rules and automatically correct them, achieving superior performance compared to state-of-the-art baselines across three datasets (CUB, MIMIC-III EWS, and Synthetic-MNIST).

## Key Results
- Achieves nearly 100% identification rate (IR) for recognizing perturbed explanations
- Attains an average success rate (SR) of up to 98% for rectifying explanations
- Demonstrates superior performance compared to state-of-the-art baselines across three benchmark datasets

## Why This Works (Mechanism)
The method works by leveraging logical consistency constraints encoded in factor graphs to validate and correct explanations. During inference, the factor graph serves as a consistency checker that identifies when generated explanations violate predefined logical relationships. This approach bypasses the need to explicitly model or train against specific perturbations, instead focusing on maintaining logical coherence in the output explanations. The factor graph structure enables efficient identification of logical inconsistencies and provides a framework for systematic correction of erroneous explanations.

## Foundational Learning
1. **Factor Graphs**: Bipartite graphs representing factorization of functions, used here to encode logical rules - needed to structure the logical constraints for explanation validation
2. **Adversarial Perturbations**: Small, intentional modifications to input data designed to mislead models - needed to understand the problem space of perturbation-induced errors
3. **Explanation Interpretability**: The degree to which model outputs can be understood by humans - needed to evaluate the comprehensibility of generated explanations
4. **Logical Consistency**: Ensuring that explanations adhere to predefined logical relationships - needed to establish the criteria for valid explanations
5. **Inference-time Correction**: Applying corrections during the prediction phase rather than during training - needed to handle unknown perturbations without retraining

## Architecture Onboarding

**Component Map**: Input Data -> Neural Network Backbone -> Explanation Generator -> Factor Graph Validator -> Corrected Explanation

**Critical Path**: The neural network generates initial explanations, which are then validated against the factor graph. If inconsistencies are detected, the correction mechanism is triggered to produce refined explanations.

**Design Tradeoffs**: The method trades computational overhead during inference for robustness against unknown perturbations, avoiding the need for extensive adversarial training data. This approach sacrifices some real-time performance for improved explanation quality under perturbation.

**Failure Signatures**: The system may fail when logical rules encoded in the factor graph are incomplete or when perturbations induce errors that violate no predefined logical constraints. Additionally, complex perturbations that require context beyond the factor graph's scope may not be adequately handled.

**3 First Experiments**:
1. Test the factor graph construction with synthetic logical rules to verify basic functionality
2. Evaluate explanation correction on simple perturbation scenarios with known ground truth
3. Measure the computational overhead of factor graph validation compared to baseline methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided text.

## Limitations
- Performance under truly unknown perturbations remains uncertain without detailed analysis of generalization
- Reliance on logical rules could be limiting if these rules don't capture all possible perturbations
- Claims of nearly 100% identification rate need validation across more diverse datasets and perturbation types
- The method's effectiveness when logical rules are incomplete or incorrect is not thoroughly explored

## Confidence
- Methodology robustness: Medium
- Performance claims: Medium
- Generalization to real-world scenarios: Low
- Effectiveness with incomplete logical rules: Low

## Next Checks
1. Conduct experiments with a broader range of datasets and perturbation types to assess the method's generalizability
2. Evaluate the method's performance when the logical rules are incomplete or incorrect to understand its limitations
3. Perform ablation studies to quantify the impact of factor graph construction and explanatory logic errors identification on the overall performance