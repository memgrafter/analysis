---
ver: rpa2
title: An Interpretability-Guided Framework for Responsible Synthetic Data Generation
  in Emotional Text
arxiv_id: '2511.16132'
source_url: https://arxiv.org/abs/2511.16132
tags:
- data
- shap-guided
- real
- augmentation
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: An interpretability-guided framework was developed to enhance LLM-based
  synthetic data generation for emotion recognition. SHAP-derived feature importance
  was used to construct generation prompts, incorporating emotion-relevant keywords
  and exemplar tweets.
---

# An Interpretability-Guided Framework for Responsible Synthetic Data Generation in Emotional Text

## Quick Facts
- arXiv ID: 2511.16132
- Source URL: https://arxiv.org/abs/2511.16132
- Reference count: 40
- Enhanced LLM-based synthetic data generation for emotion recognition using SHAP-derived feature importance

## Executive Summary
This paper introduces an interpretability-guided framework for generating synthetic emotional text data using large language models. The approach leverages SHAP feature importance scores to identify emotion-relevant keywords and exemplar tweets, which are then incorporated into generation prompts to create synthetic data that better captures emotional nuances. The framework demonstrates improved performance for minority emotion classes and provides a more responsible approach to synthetic data generation by grounding it in interpretability analysis rather than purely heuristic methods.

## Method Summary
The framework combines SHAP-based feature importance analysis with LLM-based synthetic data generation for emotion recognition tasks. First, SHAP values are computed using a DeBERTa model to identify features most important for emotion classification. These features, along with exemplar tweets representing different emotions, are then used to construct generation prompts. The LLM generates synthetic tweets conditioned on these interpretability-guided prompts, creating emotion-labeled data that emphasizes the linguistic patterns most relevant for classification. The synthetic data is validated through classification performance comparisons and linguistic analysis of vocabulary richness and lexical overlap with real data.

## Key Results
- SHAP-guided synthetic data achieved F1-scores of 0.48-0.53 across data increments, matching real data expansion performance
- Outperformed naïve generation with F1 of 0.45, showing 0.03-0.08 absolute improvement
- Particularly effective for minority emotion classes, with optimism classification improving from 0.25 to 0.34 F1
- Synthetic data showed reduced vocabulary richness (TTR: 0.133-0.143 vs. 0.241 for real data) but maintained higher lexical overlap

## Why This Works (Mechanism)
The framework works by grounding synthetic data generation in model-interpretable features rather than arbitrary prompt engineering. SHAP feature importance identifies the specific linguistic patterns and keywords that models actually use for emotion classification, ensuring synthetic data emphasizes these discriminative elements. By incorporating exemplar tweets alongside feature keywords, the generation process captures both the semantic content and contextual usage patterns associated with different emotions. This targeted approach helps synthetic data better represent the decision boundaries that classification models learn, leading to improved performance compared to generic generation strategies.

## Foundational Learning
- **SHAP feature importance**: Quantifies feature contributions to model predictions using game theory principles; needed to identify emotion-relevant linguistic patterns; quick check: verify SHAP values sum to model output for individual predictions
- **DeBERTa architecture**: Enhanced BERT variant with disentangled attention and enhanced mask decoder; needed for robust feature importance computation; quick check: confirm model achieves reasonable baseline performance on emotion classification
- **Lexical overlap metrics**: Measures vocabulary similarity between synthetic and real data; needed to validate synthetic data quality; quick check: compare type-token ratios and common word frequencies
- **Type-token ratio (TTR)**: Vocabulary diversity metric (unique words / total words); needed to assess linguistic richness of synthetic data; quick check: calculate TTR across different data subsets
- **Emotion classification F1-scores**: Harmonic mean of precision and recall for emotion categories; needed to evaluate synthetic data utility; quick check: ensure scores are computed per-class and macro-averaged

## Architecture Onboarding

**Component Map**: SHAP analysis -> Prompt construction -> LLM generation -> Classification evaluation -> Linguistic analysis

**Critical Path**: SHAP feature extraction → Prompt engineering with keywords and exemplars → LLM generation → Classifier training → Performance validation

**Design Tradeoffs**: Interpretability-guided generation trades some linguistic diversity (lower TTR) for improved classification performance and better representation of minority classes. The framework prioritizes emotion-relevant patterns over naturalistic language variation.

**Failure Signatures**: 
- Low lexical overlap between synthetic and real data indicates generation is not capturing real emotion patterns
- Performance degradation when using synthetic data suggests synthetic examples don't generalize well
- Minimal improvement for minority classes indicates SHAP guidance isn't effective for rare emotions
- High TTR but low classification performance suggests synthetic data is diverse but not discriminative

**3 First Experiments**:
1. Compare SHAP-guided generation vs. random keyword selection to isolate the benefit of interpretability guidance
2. Test different exemplar tweet selection strategies (random vs. representative vs. boundary cases)
3. Evaluate performance across emotion intensity levels to understand if framework works better for strong vs. subtle emotions

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Modest performance improvements (0.03-0.08 F1 gains) suggest incremental rather than transformative benefits
- Limited to English Twitter data, constraining generalizability to other domains and languages
- Small dataset size (1,000-2,000 samples) may affect training stability and evaluation reliability
- Reduced linguistic diversity in synthetic data could limit model robustness to real-world variation

## Confidence
- **High confidence**: The synthetic data generation methodology using SHAP-derived prompts is clearly described and reproducible
- **Medium confidence**: The relative performance comparisons between SHAP-guided and naïve generation are robust, though absolute performance metrics should be interpreted cautiously given the small dataset size
- **Low confidence**: Generalizability of results to other emotion recognition tasks, languages, or domains beyond Twitter data

## Next Checks
1. Test the framework across multiple emotion recognition datasets and languages to assess generalizability beyond Twitter English data
2. Conduct ablation studies removing SHAP guidance to quantify the specific contribution of interpretability guidance versus prompt engineering techniques
3. Evaluate model performance on held-out real test data after training on synthetic data to verify that lexical overlap translates to genuine generalization rather than memorization