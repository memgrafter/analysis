---
ver: rpa2
title: Agentic Search Engine for Real-Time IoT Data
arxiv_id: '2503.12255'
source_url: https://arxiv.org/abs/2503.12255
tags:
- data
- iot-ase
- service
- user
- real-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents IoT-ASE, an agentic search engine designed to
  address the fragmentation of IoT systems by enabling seamless, real-time data sharing
  and retrieval. Leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation
  (RAG), IoT-ASE processes complex queries and delivers contextually relevant results
  using real-time IoT data.
---

# Agentic Search Engine for Real-Time IoT Data

## Quick Facts
- **arXiv ID:** 2503.12255
- **Source URL:** https://arxiv.org/abs/2503.12255
- **Reference count:** 40
- **Primary result:** 92% accuracy in intent-based service retrieval using real-time IoT data

## Executive Summary
This paper presents IoT-ASE, an agentic search engine designed to address the fragmentation of IoT systems by enabling seamless, real-time data sharing and retrieval. Leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), IoT-ASE processes complex queries and delivers contextually relevant results using real-time IoT data. Implemented within the SensorsConnect framework, the system was evaluated using a Toronto-based dataset of 500 services and 37,033 IoT devices. Results showed 92% accuracy in intent-based service retrieval, with responses outperforming those from Gemini in relevance and precision. The study highlights IoT-ASE's potential to support effective real-time decision-making by making IoT data more accessible and actionable.

## Method Summary
The IoT-ASE system uses a LangGraph-based Generic Agentic RAG (GA-RAG) workflow with four agents: Classifier, Retriever, Generator, and Reviewer. It processes natural language queries by first using semantic search via Sentence-BERT embeddings to identify relevant service categories from a VectorDB, then retrieving real-time IoT data from MongoDB collections, and finally generating responses via Llama3-8b-8192. The Reviewer agent provides a verification step to catch errors or hallucinations. The system was evaluated on a Toronto dataset with 500 services and 37,033 IoT devices.

## Key Results
- 92% accuracy in top-3 intent-based service retrieval
- Responses outperformed Gemini in relevance and precision
- System effectively handles complex queries like "place to unleash my dog" and "gym with shortest waiting time"

## Why This Works (Mechanism)

### Mechanism 1: Semantic Service Discovery via Vector Mapping
Mapping natural language queries to specific IoT service categories via vector embeddings allows the system to retrieve relevant real-time data without relying on rigid keyword matching. The system generates vector embeddings for static service descriptions (e.g., "Dog Park") and stores them in a VectorDB. When a user submits a complex query (e.g., "place to unleash my dog"), a semantic search retrieves the top-k matching service types. These types then serve as collection names to query the real-time IoT database for actual device data.

### Mechanism 2: Agentic Contextual Verification
Introducing a "Reviewer" agent creates a feedback loop that mitigates hallucinations by verifying if the generated response strictly adheres to user constraints derived from real-time data. After the "Generator" produces a response, the "Reviewer" agent evaluates it against the original query and the retrieved context. If specific constraints (e.g., "uncrowded") are present in the query but missing or ignored in the response, the Reviewer can flag the error or trigger a reformulation.

### Mechanism 3: Structured Real-Time Ranking
Structuring sensor data into unified JSON documents with explicit fields (e.g., occupancy, price) enables the LLM to perform comparative ranking based on live states rather than static metadata. The Retriever fetches JSON documents containing live fields (e.g., "Lineup: 5 min"). The Generator is prompted to weigh these numerical values against user preferences to select the optimal result (e.g., the clinic with the shortest wait time).

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The system relies on RAG to ground the LLM's responses in external, real-time sensor data, preventing the LLM from hallucinating availability or status.
  - **Quick check question:** Can you explain why a standard LLM (like GPT-4) cannot answer "Which nearby park has an empty soccer field right now?" without a RAG pipeline?

- **Concept: Vector Embeddings & Semantic Search**
  - **Why needed here:** This is the routing mechanism. It allows the system to translate vague user intent ("place to walk my dog") into a specific database query (Collection: "Dog Parks").
  - **Quick check question:** How does "semantic search" differ from keyword search when handling the query "I need to refuel my vehicle"?

- **Concept: Document Data Model (NoSQL)**
  - **Why needed here:** The paper uses a hierarchical document model (Region -> Service -> Node) to store flexible, heterogeneous IoT data (JSON) that the LLM can easily parse.
  - **Quick check question:** Why is a rigid relational schema less suitable for the "heterogeneous sensing data" described in the paper?

## Architecture Onboarding

- **Component map:** Ingestion: IoT Devices -> Edge Layer (UIDI Format) -> Cloud (Real-time/Historical DB) -> Indexing: Service Descriptions -> Sentence-BERT -> Vector Database -> Querying (IoT-ASE): User Query -> LangGraph State Machine -> [Classifier -> Retriever (IoT-RAG-SE) -> Generator -> Reviewer]
- **Critical path:** The query flow depends heavily on the **Semantic Search step**. If the VectorDB retrieval fails to identify the correct service (e.g., returns "Car Rental" instead of "Gas Station"), the subsequent database lookup and generation will be irrelevant.
- **Design tradeoffs:** Static vs. Dynamic Indexing: The system indexes *service descriptions* (static) rather than the IoT data itself (dynamic) to avoid the computational cost of constantly re-embedding high-frequency sensor updates. Precision vs. Recall: The "Reviewer" agent prioritizes precision (ensuring constraints are met) over speed, potentially increasing latency.
- **Failure signatures:** Generic Responses: If the system responds with "Here are three gyms nearby" without ranking them by occupancy, the **Retriever** likely failed to fetch the "Occupancy Factor" field or the **Reviewer** failed to enforce the preference. Routing Errors: If a query for "Coffee in Toronto" is routed to the Scraper/Tavily agent instead of the local IoT-RAG-SE, the **Classifier** agent likely misidentified the coverage zone.
- **First 3 experiments:**
  1. **Validation of Intent Matching:** Run the 25 evaluation queries from Table I against your VectorDB to verify if the top-1 result matches the intended service with >90% accuracy.
  2. **Field Sensitivity Test:** Query the system for a recommendation (e.g., "Gas station with cheapest price"). First, remove the "price" field from the DB documents and observe if the response quality degrades (sanity check for Mechanism 3).
  3. **Latency Profiling:** Measure the time taken by the "Reviewer" agent to determine if the agentic loop adds unacceptable latency compared to a single-pass generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal architectural design for Agentic RAG workflows be systematically determined for specific IoT domains?
- Basis in paper: [explicit] The authors state, "This topic still needs further investigation, as we have no clear approach to determining the optimal design for different use cases."
- Why unresolved: While the paper proposes a Generic Agentic RAG (GA-RAG) workflow, the specific configurations of agents (Classifier, Retriever, Generator, Reviewer) required to maximize efficiency for different industrial or domestic IoT applications remain undefined.
- What evidence would resolve it: A comparative study measuring performance and resource utilization across varied agent topologies in distinct IoT scenarios (e.g., smart healthcare vs. industrial logistics).

### Open Question 2
- Question: To what extent can the Reviewer agent's capabilities be expanded via fact-checking components to guarantee the correctness of generated responses?
- Basis in paper: [explicit] The authors note the "Reviewer has limited capabilities to fully check the correctness of the response" and suggest "reviewer abilities can be improved by adding a fact-checking component."
- Why unresolved: The current Reviewer agent relies on semantic analysis to catch errors, but it lacks a mechanism to verify the factual accuracy of the retrieved context against the live data source, leaving a residual risk of hallucination.
- What evidence would resolve it: Implementation of an automated ground-truth verification module within the Reviewer agent showing a statistically significant reduction in hallucination rates compared to the baseline.

### Open Question 3
- Question: What methodologies can effectively assess and improve the quality of service descriptions to minimize retrieval errors in the semantic search process?
- Basis in paper: [explicit] The authors observe that "inaccurate service descriptions lead to incorrect retrieved contexts" and suggest "finding a method for improvement and assessing the servicesâ€™ descriptions can enhance the likelihood of the first matching result."
- Why unresolved: The system achieved 92% accuracy in intent retrieval, with the remaining 8% failure rate largely attributed to the quality of the descriptions stored in the vector database rather than the retrieval algorithm itself.
- What evidence would resolve it: An automated pipeline that iteratively refines service descriptions based on retrieval feedback loops, demonstrating an increase in top-1 retrieval accuracy.

## Limitations
- Evaluation limited to single city dataset (Toronto) with 500 services and 37,033 devices, may not generalize to larger IoT ecosystems
- Comparison with Gemini lacks systematic measurement of latency, hallucination rates, or user satisfaction
- System performance depends on quality and completeness of service descriptions in VectorDB

## Confidence
- **High Confidence:** The semantic service discovery mechanism (Mechanism 1) and the document-based data model (Mechanism 3) are well-supported by the paper's technical description and evaluation results (92% accuracy in intent-based retrieval).
- **Medium Confidence:** The agentic contextual verification (Mechanism 2) is described in the methodology but lacks detailed quantitative evidence of its effectiveness in reducing hallucinations compared to baseline approaches.
- **Low Confidence:** The generalizability of the system to other cities or IoT domains beyond the Toronto dataset, and the long-term maintenance requirements for keeping service descriptions current, are not addressed.

## Next Checks
1. **Cross-City Generalization:** Test the system on a different city dataset (e.g., New York or London) with similar service categories to evaluate if the 92% accuracy holds across geographic boundaries.
2. **Hallucination Detection Accuracy:** Design a controlled experiment to measure the Reviewer agent's ability to detect and correct hallucinations, comparing its performance against a baseline LLM without the Reviewer loop.
3. **Latency and Scalability Analysis:** Measure the end-to-end latency of the IoT-ASE pipeline under varying loads (e.g., 100, 1,000, 10,000 concurrent queries) and assess whether the agentic loop introduces unacceptable delays for real-time applications.