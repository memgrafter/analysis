---
ver: rpa2
title: 'One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual
  Learning'
arxiv_id: '2509.24483'
source_url: https://arxiv.org/abs/2509.24483
tags:
- prompt
- experts
- expert
- page
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SMoPE, a prompt-based continual learning method
  that integrates sparse mixture-of-experts (MoE) architecture with prefix tuning
  to balance efficiency and performance. Unlike task-specific prompting methods that
  assign dedicated prompts per task, SMoPE uses a single shared prompt structured
  as multiple "prompt experts" within an MoE framework, selectively activating only
  relevant experts per input to reduce interference.
---

# One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning

## Quick Facts
- **arXiv ID**: 2509.24483
- **Source URL**: https://arxiv.org/abs/2509.24483
- **Reference count**: 40
- **Primary result**: SMoPE achieves SOTA continual learning performance with 50% computational cost reduction using shared prompt experts

## Executive Summary
SMoPE introduces a novel prompt-based continual learning framework that integrates sparse mixture-of-experts (MoE) with prefix tuning, using a single shared prompt structured as multiple "prompt experts" rather than task-specific prompts. The method selectively activates relevant experts per input to minimize catastrophic forgetting while maintaining computational efficiency. Key innovations include an adaptive noise mechanism for balanced expert utilization and a prototype-based loss function that leverages prefix keys as implicit memory of past tasks.

## Method Summary
SMoPE combines sparse MoE architecture with prefix tuning to create a unified continual learning approach that avoids the computational overhead of task-specific prompting. The framework uses a gating network to route inputs to specialized prompt experts, with adaptive noise encouraging balanced expert selection and prototype-based loss preserving knowledge across tasks. This design maintains strong performance while reducing computational cost by up to 50% compared to existing methods, using fewer parameters than task-specific approaches.

## Key Results
- SMoPE outperforms task-specific prompt methods on ImageNet-R, CIFAR-100, and CUB-200 benchmarks
- Achieves computational cost reduction of up to 50% compared to existing continual learning approaches
- Matches or exceeds state-of-the-art performance while using substantially fewer parameters than task-specific methods

## Why This Works (Mechanism)
SMoPE works by leveraging the sparse MoE architecture to create a shared prompt space where different experts specialize in different aspects of the input distribution. The gating mechanism routes inputs to relevant experts based on their characteristics, while the adaptive noise ensures balanced utilization across experts to prevent catastrophic forgetting. The prototype-based loss function acts as implicit memory, using prefix keys to retain information about previously seen tasks without requiring explicit task replay or separate memory buffers.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: Combines multiple specialized models to handle different input patterns - needed for task-specific specialization without task-specific prompts; quick check: verify gating function produces sparse activations
- **Prefix Tuning**: Prepends trainable parameters to transformer layers instead of full fine-tuning - needed for efficient parameter updates; quick check: measure parameter count vs full fine-tuning
- **Catastrophic Forgetting**: Degradation of performance on previous tasks when learning new ones - core problem being addressed; quick check: track performance on old tasks during training
- **Gating Networks**: Routes inputs to appropriate experts based on learned criteria - enables selective expert activation; quick check: visualize gating distribution across inputs
- **Adaptive Noise**: Stochastic mechanism to encourage balanced expert utilization - prevents expert collapse; quick check: monitor expert activation frequencies
- **Prototype-based Learning**: Uses representative examples as memory anchors - enables knowledge preservation without explicit replay; quick check: measure distance to prototypes across tasks

## Architecture Onboarding
**Component Map**: Input -> Gating Network -> Prompt Experts (MoE) -> Prefix Keys -> Transformer -> Output
**Critical Path**: Input flows through gating network to select relevant prompt experts, whose outputs combine with prefix keys to condition the transformer for task-specific processing
**Design Tradeoffs**: Single shared prompt vs task-specific prompts (efficiency vs specialization), adaptive noise vs static gating (exploration vs exploitation), prototype loss vs explicit replay (implicit vs explicit memory)
**Failure Signatures**: Expert collapse (gating concentrates on few experts), forgetting (performance degradation on old tasks), inefficient routing (high computational cost despite MoE)
**First Experiments**: 1) Measure expert activation distribution to verify balanced utilization, 2) Track performance on held-out old tasks during new task training, 3) Compare computational cost against task-specific prompting baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of ablation studies quantifying the exact overhead of adaptive noise and prototype-based loss mechanisms compared to standard prefix tuning
- Theoretical sample efficiency claims need empirical validation across different dataset sizes rather than theoretical analysis alone
- Missing training dynamics analysis, such as expert activation patterns and convergence behavior, to assess adaptive noise effectiveness

## Confidence
- **High**: The general framework combining MoE with prefix tuning for continual learning is technically sound and well-grounded in existing literature
- **Medium**: The computational efficiency improvements and parameter savings claims require more detailed benchmarking and ablation studies
- **Medium**: The theoretical analysis of sample efficiency preservation needs empirical validation across varying dataset sizes and task distributions

## Next Checks
1. Conduct ablation studies isolating the contribution of adaptive noise versus prototype-based loss versus standard MoE architecture to quantify each component's impact on performance and efficiency
2. Perform scaling experiments varying training set sizes to empirically validate the sample efficiency claims across different data regimes
3. Implement and measure activation patterns of prompt experts during training to verify balanced utilization and identify potential gating function biases or collapse scenarios