---
ver: rpa2
title: Online Knowledge Distillation with Reward Guidance
arxiv_id: '2505.18952'
source_url: https://arxiv.org/abs/2505.18952
tags:
- pbkd
- preference
- online
- student
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a preference-based knowledge distillation (PbKD)
  framework that formulates knowledge distillation as a reward-guided imitation learning
  problem. The method optimizes the performance gap between student and teacher policies
  through a min-max game between the student and a reward model, with theoretical
  guarantees on suboptimality and regret bounds.
---

# Online Knowledge Distillation with Reward Guidance

## Quick Facts
- arXiv ID: 2505.18952
- Source URL: https://arxiv.org/abs/2505.18952
- Reference count: 40
- Key outcome: PbKD achieves 53.6% average accuracy vs 49.7% for Vanilla Black-Box KD across five benchmarks

## Executive Summary
This paper introduces a preference-based knowledge distillation framework that formulates KD as a reward-guided imitation learning problem. The method optimizes the performance gap between student and teacher policies through a min-max game between the student and a reward model, with theoretical guarantees on suboptimality and regret bounds. Extensive experiments across five black-box and five white-box KD benchmarks show that PbKD consistently outperforms existing methods, including Vanilla Black-Box KD (49.7%→53.6% average accuracy), Proxy-KD (53.1%→53.6%), and white-box baselines like KL divergence and reverse KL divergence.

## Method Summary
The method constructs preference datasets from teacher-student output comparisons, then trains a reward model via maximum likelihood estimation with confidence set constraints. The student policy is optimized through min-max optimization, minimizing worst-case performance gap while maximizing reward model confidence. For white-box settings, the framework reformulates the objective using Q-function moment matching. Online variants iteratively collect new preference data, retrain the reward model, and update the student policy across multiple iterations.

## Key Results
- Black-box KD: PbKD achieves 53.6% average accuracy vs 49.7% for Vanilla Black-Box KD
- White-box KD: PbKD outperforms KL divergence and reverse KL divergence baselines
- Online variant with iterative preference collection further improves performance
- Theoretical guarantees on suboptimality and regret bounds are provided

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Min-max optimization over reward confidence sets reduces the student-teacher performance gap more robustly than fixed reward training.
- **Mechanism:** The student minimizes worst-case performance gap: `π̂ = arg min_π max_{r∈R(D)} [J(πE, r) - J(π, r)]`. The inner maximization identifies reward functions that best distinguish student from teacher, forcing robust policy learning. Constrained via Lagrangian relaxation (Eq. 5): `J(πE, r) - J(π, r) + βL_r(D_pref)`.
- **Core assumption:** The ground-truth reward lies within the confidence set constructed from preference data (Assumption 1: realizability).
- **Evidence anchors:**
  - [abstract]: "formulating a min-max optimization problem between the policy and reward model (RM) to minimize the performance gap"
  - [Section 4.1, Eq. 5-6]: Lagrangian relaxation converts constrained problem to unconstrained bi-level optimization
  - [corpus]: Related work (RM-Distiller, arXiv:2601.14032) validates reward model quality as critical for distillation success
- **Break condition:** If preference data lacks coverage of teacher-optimal trajectories, confidence sets exclude true reward → student may optimize wrong objective.

### Mechanism 2
- **Claim:** Online iterative preference collection accelerates convergence over fixed offline data.
- **Mechanism:** At each iteration t, collect `(x_t, τ_E, τ_student)` pairs, augment D_pref, retrain RM and student. Theorem 2 proves O(√(T log T log(T/δ))) regret bound vs. offline O(√(log(N/δ)/N)) suboptimality.
- **Core assumption:** Teacher outputs are always preferred (simplification for expensive human annotation); student evolves slowly enough for stable RM updates.
- **Evidence anchors:**
  - [abstract]: "online variant with iterative preference data collection further improves performance"
  - [Section 5.1, Algorithm 2]: Explicit online iteration loop with time-dependent confidence sets R(D_pref^t)
  - [corpus]: KEPO (arXiv:2602.00400) similarly shows online preference optimization improves reasoning RL
- **Break condition:** If student distribution shifts too rapidly, RM cannot track true preference → reward hacking or instability.

### Mechanism 3
- **Claim:** White-box moment-matching via Q-functions leverages teacher token probabilities for tighter distillation.
- **Mechanism:** Reformulates objective using Performance Difference Lemma (Proposition 2): `J(πE, r) - J(π, r) = E[Σ(E_a~πE[Q(sh,a)] - Q(sh,ah))]`. The Q-function class `G_Q^E` is induced from reward class, enabling gradient-based optimization with teacher probabilities.
- **Core assumption:** Teacher's Q-values are accessible (white-box setting); deterministic transitions in autoregressive generation.
- **Evidence anchors:**
  - [abstract]: "reformulate the RM using the Q-value function and extend the framework to white-box KD"
  - [Section 6, Eq. 12-13]: Dual optimization over Q-confidence sets
  - [corpus]: Preference Distillation via Value-based RL (arXiv:2509.16965) corroborates value-function integration with preference learning
- **Break condition:** If Q-function approximation is poor or teacher distribution differs significantly from training data, moment-matching degrades to noisy supervision.

## Foundational Learning

- **Concept: Bradley-Terry-Luce Preference Model**
  - Why needed here: Foundation for converting trajectory comparisons into differentiable reward signals
  - Quick check question: Can you derive why sigmoid(r(τ0) - r(τ1)) models Pr(τ0 ≻ τ1)?

- **Concept: Confidence Sets via MLE Brackets**
  - Why needed here: Enables distributionally robust optimization over plausible rewards without overfitting to finite preference samples
  - Quick check question: Explain why bracketing numbers control uniform convergence over the reward class.

- **Concept: Performance Difference Lemma**
  - Why needed here: Connects value/reward gaps to policy divergence, enabling Q-based white-box extension
  - Quick check question: How does PDL transform trajectory-level performance gap into step-wise advantage differences?

## Architecture Onboarding

- **Component map:**
  - Preference Dataset (D_pref) -> Reward Model (RM) -> Student Policy (LLM) -> Q-Function (white-box)
  - GPT-4 ranking -> Preference Dataset (D_pref) -> Reward Model (RM)

- **Critical path:**
  1. Initialize student with teacher-generated SFT data
  2. Collect offline preference pairs via multi-model sampling + GPT-4 ranking
  3. Train RM on preference pairs (MLE with confidence radius ζ)
  4. Run min-max optimization: update RM (gradient ascent) and student (PPO-style clipping)
  5. For online: iteratively sample new pairs from current student, augment D_pref, repeat

- **Design tradeoffs:**
  - Larger RM backbone → better reward generalization but slower iteration (Fig 3 shows 70B RM plateaus higher but converges slower)
  - More online iterations → better alignment but higher compute (5 iterations used in black-box, 3 in white-box)
  - ζ (confidence slack) → larger ζ = more exploration, smaller = more conservative but may miss optimal reward

- **Failure signatures:**
  - Student performance plateaus early: RM capacity insufficient or preference data lacks diversity
  - Divergent training: learning rate too high for min-max game; use separate optimizers for RM (SGA) and student (clipped policy gradient)
  - White-box instability: Q-function not pretrained on offline preferences → initialize Q-head first

- **First 3 experiments:**
  1. **Sanity check:** Verify RM achieves >90% accuracy on held-out preference pairs before min-max training
  2. **Ablation:** Compare offline-only vs. 1-iteration online vs. 5-iteration online on single benchmark (e.g., GSM8K) to confirm iteration benefit
  3. **Scale test:** Run with 3B vs. 7B RM backbone, measure iteration-to-convergence and final accuracy gap

## Open Questions the Paper Calls Out

- **Question:** Can automated or self-supervised preference acquisition methods eliminate the need for costly human or LLM-based preference labeling while maintaining distillation quality?
  - Basis in paper: [explicit] The authors state: "Obtaining high-quality preference data can be costly or subjective" and propose "Future work may explore automated or self-supervised preference acquisition."
  - Why unresolved: Current experiments rely on GPT-4 feedback for preference labeling, which introduces cost and potential bias; no self-supervised alternatives are explored.
  - What evidence would resolve it: Experiments comparing PbKD performance using self-supervised preference signals (e.g., consistency-based, uncertainty-based) versus human/LLM annotations across multiple benchmarks.

- **Question:** How does relaxing the assumption that teacher responses are always preferred over student responses affect online PbKD convergence and final performance?
  - Basis in paper: [inferred] Section 5.1 states: "we make a simplifying assumption: responses from the teacher policy are always preferred over those from the student policy." This limits applicability when student occasionally outperforms teacher.
  - Why unresolved: The theoretical analysis and algorithms assume perfect teacher preference; real-world scenarios may violate this assumption.
  - What evidence would resolve it: Experiments with noisy preference labels where student responses are preferred with probability p > 0, measuring regret bounds and convergence behavior.

## Limitations

- **Unknown hyperparameters:** Exact values for β, α, clipping threshold ϵ, learning rates, batch sizes, and epochs per iteration are not provided
- **Computational overhead:** Online PbKD introduces repeated RM training and dataset expansion costs
- **Assumption dependencies:** Theoretical guarantees depend on realizability and linearity assumptions that may not hold in practice

## Confidence

- **High confidence:** Experimental results showing PbKD outperforms existing KD methods across multiple benchmarks (average gains of 3.9% in black-box, 4.5% in white-box settings)
- **Medium confidence:** Theoretical regret bounds, as they assume idealized conditions (realizability, slow student evolution) that may not hold in practice
- **Medium confidence:** Online iteration benefits, given the computational expense and sensitivity to preference data quality

## Next Checks

1. **Robustness test:** Run PbKD with perturbed hyperparameters (β, ζ, learning rates) across multiple random seeds to quantify sensitivity and establish confidence intervals
2. **Coverage verification:** Measure whether constructed reward confidence sets actually contain the true reward function on held-out data, testing Assumption 1 empirically
3. **Scaling analysis:** Evaluate PbKD with smaller preference datasets (e.g., 10K vs. 100K pairs) to determine minimum data requirements and test the practical relevance of theoretical bounds