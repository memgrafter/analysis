---
ver: rpa2
title: Random-Set Large Language Models
arxiv_id: '2504.18085'
source_url: https://arxiv.org/abs/2504.18085
tags:
- belief
- uncertainty
- sets
- arxiv
- cuzzolin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Random-Set Large Language Models (RS-LLMs),
  a novel approach to uncertainty quantification in LLMs by predicting belief functions
  over token sets instead of probability vectors. The method uses hierarchical clustering
  to create a budget of focal token subsets, making it computationally scalable while
  preserving uncertainty representation.
---

# Random-Set Large Language Models

## Quick Facts
- **arXiv ID:** 2504.18085
- **Source URL:** https://arxiv.org/abs/2504.18085
- **Reference count:** 40
- **Primary result:** RS-LLMs achieve 89.60% accuracy on OBQA vs 83.20% for standard models while providing interpretable epistemic uncertainty through credal set width.

## Executive Summary
Random-Set Large Language Models (RS-LLMs) introduce a novel approach to uncertainty quantification in LLMs by predicting belief functions over token sets instead of probability vectors. The method uses hierarchical clustering to create a budget of focal token subsets, making it computationally scalable while preserving uncertainty representation. RS-LLMs encode epistemic uncertainty via the size and diversity of credal sets associated with predicted belief functions, enabling more reliable hallucination detection through credal set width analysis.

## Method Summary
RS-LLMs modify standard LLM architectures by replacing the probability output head with a belief function prediction layer over a budget of focal sets. The approach uses hierarchical clustering on token embeddings to group similar tokens into focal sets, then trains the model to predict mass assignments to these sets plus singleton tokens. A composite loss function combines binary cross-entropy for prediction accuracy with regularization terms ensuring valid belief functions. During inference, the pignistic transformation converts belief functions back to probabilities for token sampling while preserving uncertainty information in the credal set width metric.

## Key Results
- RS-LLMs achieve 89.60% accuracy on OBQA compared to 83.20% for standard models
- Superior uncertainty quantification demonstrated through credal set width analysis
- Effective hallucination detection capability through epistemic uncertainty measurement
- Computational feasibility maintained through clustering-based output space reduction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling epistemic uncertainty as a convex set of probabilities (credal set) rather than a single probability distribution allows the model to distinguish between "equally likely valid options" and "lack of knowledge."
- **Mechanism:** The RS-LLM predicts a belief function over the vocabulary, defining a credal set—a geometric region of possible probability distributions. Measuring the "width" between upper and lower probability bounds quantifies ignorance, with wide intervals flagging potential hallucinations.
- **Core assumption:** Model ignorance manifests as an inability to reduce the probability of a correct token to a single point value, and this state is learnable via set-based supervision.
- **Evidence anchors:** [abstract] "encode the epistemic uncertainty... via the size of the credal sets"; [page 2] "we can now associate an explanation with that answer."

### Mechanism 2
- **Claim:** Constraining the output space to a budget of semantically clustered tokens preserves the ability to model ambiguity while maintaining computational feasibility.
- **Mechanism:** Hierarchical clustering groups similar tokens into focal sets, allowing the model to predict belief only for these clusters plus singletons. This forces the model to learn uncertainty specifically among semantically related alternatives.
- **Core assumption:** Model confusion is primarily local to the semantic neighborhood of the correct token, and pre-defined static clusters capture dynamic contextual ambiguity sufficiently.
- **Evidence anchors:** [page 5] "the model will most likely be confused among tokens which are similar to each other"; [page 7] "We utilize... 8,000 focal sets from 32,000 tokens."

### Mechanism 3
- **Claim:** Enforcing the axioms of belief functions via regularization allows standard backpropagation to learn valid second-order uncertainty.
- **Mechanism:** Raw network outputs are converted to belief values using sigmoid activation, then a loss function combines BCE with regularization terms penalizing invalid mass values, ensuring mathematically coherent belief functions.
- **Core assumption:** Gradient descent can simultaneously optimize for prediction accuracy and structural validity without one objective collapsing the other.
- **Evidence anchors:** [page 5] "To ensure that the predicted scores amount to a valid belief function, a mass regularisation term Mr and a mass-sum term Ms are incorporated..."

## Foundational Learning

- **Concept:** **Dempster-Shafer Theory (Belief Functions)**
  - **Why needed here:** This is the mathematical engine replacing standard probability. You must understand that "Mass" is assigned to sets of outcomes, and "Belief" is the sum of masses for all subsets, to interpret the model's output.
  - **Quick check question:** If m({A, B}) = 0.8 and m({A}) = 0.2, what is the belief that the outcome is A or B? (Answer: 1.0).

- **Concept:** **Epistemic vs. Aleatoric Uncertainty**
  - **Why needed here:** The paper claims to solve the "hallucination" problem by distinguishing "I don't know the answer" (epistemic) from "the answer is random" (aleatoric). You need this distinction to evaluate if the credal set width is a valid metric.
  - **Quick check question:** Does a high "Credal Width" indicate the model lacks knowledge (epistemic) or that the data is noisy (aleatoric)? (Answer: Epistemic).

- **Concept:** **Pignistic Transformation**
  - **Why needed here:** The model reasons with Belief, but you need a specific token to generate text. The Pignistic Probability is the "bridge" that converts the set-based belief back into a standard probability distribution for sampling.
  - **Quick check question:** How is the pignistic probability of a single token calculated from a mass function? (Answer: Sum the masses of all sets containing that token, divided by the size of those sets).

## Architecture Onboarding

- **Component map:** Base Transformer -> Modified Output Head (T+K neurons) -> Sigmoid Activation -> Mass Function -> Pignistic Transformation -> Token Sampling
- **Critical path:**
  1. Cluster Tokens: Run hierarchical clustering on pre-trained embeddings to determine fixed budget of focal sets O
  2. Modify Head: Resize output layer to match budget size (T+K neurons)
  3. Implement Loss: Code composite loss L_RS with BCE and regularization terms
- **Design tradeoffs:**
  - Budget Size (K): Low K is computationally cheap but approaches standard LLM behavior; High K increases expressiveness but adds overhead (Paper uses 8,000 as balance)
  - Static Clustering: Clusters are fixed based on static embeddings, potentially missing context-specific synonyms
- **Failure signatures:**
  - Invalid Masses: Model outputs negative mass or mass sums ≠ 1 (Solution: Increase α,β hyperparameters or apply post-processing)
  - High Entropy/Low Accuracy: Model is "uncertain" about everything (Solution: Check learning rate or cluster quality)
- **First 3 experiments:**
  1. Cluster Validity: Visualize hierarchical clusters to confirm semantically similar tokens are grouped together
  2. Hyperparameter Scan: Run ablation on budget size K (2000 vs 4000 vs 8000) to find performance saturation point
  3. Hallucination Correlation: Plot "Credal Set Width" vs. "Correctness" to verify high width correlates with incorrect answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a dynamic strategy for selecting the focal element budget K based on set overlap improve upon the current manual selection method?
- Basis in paper: [explicit] Section 6 states, "A current limitation of our work is having to manually set the number of focal elements K; a dynamic strategy adjusting K based on overlap would enhance flexibility... and will be subject to future work."
- Why unresolved: The current implementation relies on a fixed hyperparameter K for the number of clusters, which may not be optimal for all contexts or vocabulary sizes.
- What evidence would resolve it: An adaptive algorithm that adjusts K dynamically during training or inference, demonstrating higher accuracy or better uncertainty calibration than static K values.

### Open Question 2
- Question: Does training an LLM entirely from scratch using a Random-Set framework yield superior uncertainty quantification compared to fine-tuning existing models?
- Basis in paper: [explicit] Section 6 lists the "design of a large-scale mathematical framework for training an LLM from scratch in the Random-Set approach" as a future objective.
- Why unresolved: The experiments in the paper are limited to Supervised Fine-Tuning (SFT) where only the last layer is modified; the effects of random-set objectives on pre-training dynamics are unknown.
- What evidence would resolve it: Empirical results from a model trained on a large corpus from initialization using the RS-LLM loss function, compared against a standard pre-trained baseline.

### Open Question 3
- Question: Do alternative clustering algorithms, such as fuzzy clustering, provide more effective budgeting of focal sets than the hierarchical method currently employed?
- Basis in paper: [explicit] Section 6 explicitly includes the "exploration of other clustering algorithms (e.g., fuzzy clustering) for more effective budgeting" in the future work.
- Why unresolved: The paper currently utilizes hierarchical clustering to group token embeddings, but does not compare this against methods that might handle overlapping semantic meanings differently.
- What evidence would resolve it: A comparative analysis showing that fuzzy clustering produces focal sets that lead to lower loss or better epistemic uncertainty detection than hierarchical clustering.

## Limitations
- Computational overhead from expanded output vocabulary scaling linearly with budget size K
- Fixed hierarchical clustering cannot capture context-dependent semantic ambiguity
- Requires paired training data with ground truth belief assignments, limiting dataset applicability
- Static clustering approach may miss rapidly evolving language patterns and domain-specific terminology

## Confidence

**High Confidence:** The core mathematical framework of belief functions and credal set width as epistemic uncertainty indicator are well-established in uncertainty quantification literature. Experimental results showing improved accuracy and uncertainty estimation are reproducible.

**Medium Confidence:** Computational scalability claims assume hierarchical clustering effectively captures semantic neighborhoods where model experiences uncertainty. Generalization to different base models appears reasonable but needs broader testing.

**Low Confidence:** Claims about reliable hallucination detection in open-ended generation remain under-validated, as experiments focus on question-answering with known answer sets. Static clustering's ability to handle domain-specific terminology is uncertain.

## Next Checks

1. **Ablation Study on Budget Size:** Systematically evaluate model performance and uncertainty quality across different K values (2000, 4000, 8000, 16000) to identify optimal tradeoff between computational cost and uncertainty expressiveness.

2. **Context-Dependent Clustering Validation:** Implement dynamic clustering that adapts to input context and compare uncertainty estimates against static approach to quantify performance gap from fixed semantic neighborhoods.

3. **Hallucination Detection in Open-Generation:** Test RS-LLMs on open-ended generation tasks (story completion, summarization) where hallucinations are common, measuring whether credal set width reliably predicts generation errors without ground truth answer sets.