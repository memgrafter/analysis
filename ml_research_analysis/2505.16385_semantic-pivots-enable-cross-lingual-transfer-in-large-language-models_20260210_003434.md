---
ver: rpa2
title: Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models
arxiv_id: '2505.16385'
source_url: https://arxiv.org/abs/2505.16385
tags:
- cross-lingual
- language
- word
- semantic
- ability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a word-level cross-lingual translation task
  and metric to evaluate large language models'' cross-lingual abilities. The authors
  analyze intermediate layer outputs to identify two distinct inference behaviors:
  direct co-occurrence behavior and semantic pivot behavior.'
---

# Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models

## Quick Facts
- **arXiv ID**: 2505.16385
- **Source URL**: https://arxiv.org/abs/2505.16385
- **Reference count**: 40
- **Primary result**: Introduces a word-level cross-lingual translation task and metric; shows that LLMs exhibit two inference behaviors (co-occurrence vs semantic pivot) that correspond to word co-occurrence frequency; improves cross-lingual ability by 0.013 over original checkpoint and 0.005 over multilingual baseline when training OLMo-1B.

## Executive Summary
This paper analyzes how large language models perform cross-lingual word translation by examining intermediate layer outputs. The authors identify two distinct inference behaviors: direct co-occurrence behavior for high-frequency word pairs and semantic pivot behavior for low-frequency pairs that use intermediate bridging tokens. They construct a semantic pivot-aware pre-training dataset by filtering documents with high semantic pivot content, which improves cross-lingual ability when training OLMo-1B. The work provides mechanistic insights into how LLMs handle cross-lingual transfer and offers a practical method for enhancing these capabilities through targeted pre-training.

## Method Summary
The method involves three main components: first, evaluating LLMs on a word-level cross-lingual translation task using logit lens to examine intermediate layer outputs and classify behaviors as either direct co-occurrence or semantic pivot-based. Second, identifying semantic pivot tokens by analyzing co-occurrence frequencies in the pre-training corpus and extracting tokens that bridge source and target concepts. Third, constructing a semantic pivot-aware pre-training dataset by building an adjacency matrix of token co-occurrences, filtering out low-frequency and high-frequency tokens, and ranking documents by their semantic pivot content proportion. The OLMo-1B model is then continued-trained on the top-ranked documents and evaluated on cross-lingual ability.

## Key Results
- LLMs exhibit two distinct inference behaviors in cross-lingual translation: direct co-occurrence behavior for high-frequency word pairs and semantic pivot behavior for low-frequency pairs.
- Semantic pivot tokens show an inverted-U probability trajectory across intermediate layers, peaking around layer 28 in OLMo-7B before decreasing.
- Training on semantic pivot-aware filtered documents improves cross-lingual ability by 0.013 over the original checkpoint and 0.005 over a multilingual baseline when training OLMo-1B.
- The semantic pivot probability significantly surpasses the average probability of random token sets, confirming their role as meaningful intermediates.

## Why This Works (Mechanism)

### Mechanism 1: Co-occurrence Frequency Determines Inference Pathway
LLMs select between two inference pathways based on source-target co-occurrence frequency in pre-training data. High-frequency co-occurring word pairs trigger direct mapping through transformer layers, while low-frequency pairs activate an intermediate semantic pivot token that bridges source and target concepts before producing the translation. The model has implicitly learned co-occurrence statistics during pre-training that govern pathway selection during inference.

### Mechanism 2: Semantic Pivot Probability Exhibits Inverted-U Trajectory
Semantic pivot tokens show increasing then decreasing probability across late intermediate layers, indicating their role as transient reasoning intermediates. The model's hidden states first shift probability mass toward semantically related pivot tokens (peaking around layer 28 in OLMo-7B), then redirect toward the final target word in subsequent layers.

### Mechanism 3: Semantic Pivot-Aware Document Filtering Improves Transfer
Pre-training on documents ranked by high semantic pivot density yields measurable cross-lingual ability gains compared to random or purely multilingual document selection. By constructing an adjacency matrix of token co-occurrences, filtering out low-frequency and excessively high-frequency tokens, and ranking documents by pivot token proportion, the model receives concentrated exposure to cross-lingual bridging patterns.

## Foundational Learning

- **Logit Lens Interpretation**: Understanding which tokens have high probability at intermediate layers is essential for distinguishing direct vs. pivot-mediated inference. Quick check: Given a 32-layer LLM, how would you extract and decode the top-5 token probabilities at layer 16 during a translation prompt?

- **Co-occurrence Statistics and AUC Scoring**: The paper uses AUC to quantify how well co-occurrence frequency predicts which inference pathway the model will use. Quick check: If you have ranked co-occurrence counts for 100 word pairs, how would you compute AUC to evaluate whether high-ranking pairs correspond to direct behavior?

- **Token-Level Adjacency Matrix Construction**: The pivot extraction method builds a co-occurrence graph to identify tokens that bridge languages. Quick check: Given a corpus of 1M documents with vocabulary size 50K, describe an efficient algorithm to construct a token co-occurrence adjacency matrix without loading all documents into memory simultaneously.

## Architecture Onboarding

- **Component map**: Input Layer -> Intermediate Layers (L1-L31) -> Pivot Identification Module -> Adjacency Matrix Builder -> Document Ranker -> Output Layer

- **Critical path**: 1) Run word translation task with logit lens at all layers â†’ classify behavior as direct or pivot-based. 2) For pivot behaviors, query pre-training corpus to retrieve source/target co-occurrence documents. 3) Compute relative co-occurrence frequencies for candidate tokens, filter to pivot set. 4) Build adjacency matrix on candidate multilingual documents, extract pivot tokens, rank documents. 5) Continue-train OLMo-1B from checkpoint 605 for 1000 steps using AllenAI's code.

- **Design tradeoffs**: Pivot set size (top 50 candidates) may introduce noise if too large or miss relevant bridges if too small; frequency thresholds (10% for high co-occurrence, top 10% for high-frequency tokens) are heuristics that may need tuning for different corpora; language coverage limited to tested pairs (EN/ZH/FR/JA).

- **Failure signatures**: Spurious pivots from high-frequency tokens (e.g., function words) appearing as pivots due to corpus-wide prevalence; empty pivot set when source and target words have zero co-occurrence documents; asymmetric matrices due to differing language proportions in the corpus.

- **First 3 experiments**: 1) Apply logit lens to OLMo-7B on 50 translation pairs, classify behaviors, verify correlation with co-occurrence frequency. 2)