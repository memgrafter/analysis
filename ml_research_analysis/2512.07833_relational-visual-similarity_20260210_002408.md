---
ver: rpa2
title: Relational Visual Similarity
arxiv_id: '2512.07833'
source_url: https://arxiv.org/abs/2512.07833
tags:
- image
- similarity
- relational
- images
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces relational visual similarity, a novel approach
  to image similarity that captures abstract relational structures beyond surface-level
  attributes. Unlike existing methods that focus on perceptual or semantic similarity,
  the proposed method uses anonymized captions to describe the underlying logic of
  images, enabling models to reason about relational patterns.
---

# Relational Visual Similarity

## Quick Facts
- **arXiv ID:** 2512.07833
- **Source URL:** https://arxiv.org/abs/2512.07833
- **Reference count:** 40
- **Primary result:** relsim outperforms existing similarity metrics, achieving GPT-4o score of 6.77 vs 5.91 for CLIP-I and 5.14 for DINO

## Executive Summary
This paper introduces relational visual similarity, a novel approach to image similarity that captures abstract relational structures beyond surface-level attributes. Unlike existing methods that focus on perceptual or semantic similarity, the proposed method uses anonymized captions to describe the underlying logic of images, enabling models to reason about relational patterns. A dataset of 114k images with anonymous captions was curated and used to train a vision-language model (relsim) to measure relational similarity. Evaluations show that relsim significantly outperforms existing similarity metrics, and user studies confirm human preference for relsim's relational understanding.

## Method Summary
The method trains a vision-language model to capture relational similarity by using anonymized captions that describe underlying relational logic rather than surface content. The process involves three stages: (1) filtering LAION-2B to extract 114k "interesting" images using a fine-tuned VLM, (2) generating anonymous captions from image groups via frozen VLM and training a captioning model, and (3) training relsim using Qwen2.5-VL-7B with LoRA on image-caption pairs with InfoNCE loss. The model learns to align visual features with abstract relational concepts captured in the anonymous captions.

## Key Results
- relsim achieves GPT-4o score of 6.77 compared to 5.91 for CLIP-I and 5.14 for DINO
- User studies confirm human preference for relsim's relational understanding
- Applications include successful relational image retrieval and analogical image generation
- Tuned CLIP and DINO models underperform relsim despite same training data (6.02 and 5.62 vs 6.77)

## Why This Works (Mechanism)

### Mechanism 1
- Anonymous captions abstract away surface attributes, exposing relational structure
- Replace concrete object nouns with placeholders (e.g., `{subject}`) in captions
- Two images with dissimilar visual content but identical anonymous captions are pushed closer in embedding space via contrastive learning
- Core assumption: Relational logic can be captured linguistically and is shared across visually distinct images

### Mechanism 2
- Group-based caption generation yields higher-quality relational abstractions than single-image prompting
- Presenting multiple images sharing the same underlying logic to a VLM makes the shared pattern salient
- Generated caption is back-applied to all group members
- Core assumption: The same relational logic manifests across multiple images in findable ways

### Mechanism 3
- VLMs encode world knowledge required for relational reasoning that pure vision encoders lack
- LLM component in VLMs brings semantic priors about relationships (e.g., "transformation over time")
- Fine-tuning the VLM with LoRA aligns visual features with these relational concepts via InfoNCE loss
- Core assumption: Relational similarity requires conceptual abstraction beyond perceptual features

## Foundational Learning

- **Attribute vs. Relational Similarity (Cognitive Science)**
  - Why needed: The entire paper is framed around this distinction from Gentner's Structure-Mapping theory
  - Quick check: Can you explain why CLIP scores high on "two dogs" but low on "dog holding camera" vs. "cat holding camera"?

- **Contrastive Learning / InfoNCE Loss**
  - Why needed: The training objective pulls image embeddings toward their anonymous caption embeddings while pushing away from others in the batch
  - Quick check: Given batch size B=64, how many negative pairs does each image-caption pair compete against?

- **Vision Language Models (VLMs) vs. Pure Vision Encoders**
  - Why needed: The paper argues pure encoders like DINO/CLIP fail at relational tasks even after fine-tuning, requiring VLM architectures
  - Quick check: What specific component in a VLM provides "world knowledge" that a ResNet backbone lacks?

## Architecture Onboarding

- **Component map:** Image Filter -> Anonymous Captioning Model -> Relational Similarity Encoder -> Text Encoder
- **Critical path:** Image filtering quality determines caption training data quality → caption quality determines relsim embedding quality → embedding quality determines retrieval/generation performance
- **Design tradeoffs:**
  - Group-based captioning: Higher caption quality vs. scalability bottleneck (532 manual groups)
  - VLM vs. pure vision encoder: Better relational reasoning vs. higher compute cost and hallucination risk
  - Frozen text encoder: Stable supervision signal vs. no end-to-end caption optimization
- **Failure signatures:**
  - Captioning model leaks semantic/attribute info → retrieval reverts to attribute matching
  - VLM hallucinates relational structure where none exists
  - Multiple valid relational structures in one image → single caption can't capture ambiguity
- **First 3 experiments:**
  1. Ablation: Single-image vs. group-based captioning. Train two captioning models, measure GPT-4o retrieval scores. Expected: group-based wins by ~1.5 points.
  2. Ablation: VLM vs. pure vision encoder with identical training. Fine-tune CLIP ViT-L/14 on same 100k image-caption pairs with InfoNCE. Expected: ~0.7 gap favoring VLM.
  3. Failure case analysis. Sample 50 cases where relsim score < 4.0; manually classify failure modes (caption quality, VLM hallucination, ambiguous relations). Target: identify top 2 failure modes for next iteration.

## Open Questions the Paper Calls Out

- **How can the creation of image groups and anonymous captions be automated to create a scalable pipeline for training relational similarity models?**
  - Basis: The authors state the current reliance on 532 manually curated groups "may be imperfect, potentially biased, and not scalable," explicitly calling for an "automated, scalable pipeline" as a future direction.
  - Why unresolved: The current manual process is resource-intensive and limits the diversity of relational logics the model can learn.
  - What evidence would resolve it: A fully automated method that generates high-quality anonymous captions for raw image sets without human verification, resulting in improved relsim performance on diverse, out-of-distribution relational tasks.

- **How can text prompts be utilized to specify or disambiguate which relational structure a user intends when an image contains multiple valid relational mappings?**
  - Basis: The conclusion acknowledges that "one image can embody multiple different relational structures," and explicitly states that "Determining how to use text prompts to specify which relational structure a user intends remains an open question."
  - Why unresolved: The current model retrieves results based on a general embedding, potentially failing to align with the specific aspect of relational logic a user is interested in.
  - What evidence would resolve it: A user study or benchmark where varying input prompts for the same image successfully steers the model to retrieve images matching specific, distinct relational logics.

- **What architectural or training improvements are necessary for open-source image generation models to match the relational reasoning capabilities of proprietary models?**
  - Basis: In Section 5 and Table 2, the paper demonstrates that open-source models (FLUX, Bagel) significantly lag behind proprietary models (GPT-4o, Nano-Banana) in relsim scores (approx. 0.71 vs 0.84) during analogical image generation.
  - Why unresolved: Open-source models currently tend to preserve visual attributes (high CLIP score) at the expense of the underlying relational logic (lower relsim score).
  - What evidence would resolve it: The release of an open-source model that achieves parity with proprietary models on the analogical image generation benchmark (specifically narrowing the gap in relsim scores to < 0.05).

## Limitations

- The effectiveness of anonymous captioning hinges on whether a single caption can faithfully represent the relational structure of diverse images
- The 532 manually curated groups are a critical bottleneck—scaling this approach to broader domains remains unproven
- The paper assumes that VLMs' world knowledge is sufficient for relational reasoning, but it's unclear whether this holds for novel or abstract relations not well represented in pretraining data

## Confidence

- **High:** The distinction between attribute and relational similarity, the InfoNCE training objective, and the core claim that relsim outperforms existing metrics (CLIP-I, DINO) on relational tasks are well-supported by the data and ablation studies
- **Medium:** The mechanism by which group-based captioning improves relational abstraction is plausible but not rigorously tested against alternative approaches. The assumption that VLMs encode all necessary world knowledge for relational reasoning is reasonable but untested on truly novel relations
- **Low:** The claim that anonymous captions "abstract away" all surface attributes is difficult to verify without a systematic analysis of caption leakage. The scalability of the manual group curation process is also untested

## Next Checks

1. **Caption leakage analysis:** Manually inspect 100 relsim-generated captions for concrete nouns (e.g., "dog," "camera") vs. placeholders. If >20% contain specific terms, retrain with stronger regularization or more diverse groups.

2. **Generalization test:** Apply relsim to a held-out domain (e.g., medical imaging or satellite imagery) with novel relational patterns. Measure performance drop and analyze failure modes.

3. **Ablation on group size:** Train captioning models on groups of size 2, 5, and 10. Measure GPT-4o scores to identify the optimal group size for relational abstraction without overfitting.