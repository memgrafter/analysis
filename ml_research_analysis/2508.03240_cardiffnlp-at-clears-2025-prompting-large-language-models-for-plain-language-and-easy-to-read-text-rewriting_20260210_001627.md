---
ver: rpa2
title: 'CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language
  and Easy-to-Read Text Rewriting'
arxiv_id: '2508.03240'
source_url: https://arxiv.org/abs/2508.03240
tags:
- simplification
- sentence
- your
- complex
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an LLM-based approach for automatic Spanish
  text adaptation into Plain Language and Easy-to-Read formats, targeting improved
  accessibility. The team experimented with prompting strategies using LLaMA-3.2 and
  Gemma-3, ultimately selecting Gemma-3 for their final submissions.
---

# CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting

## Quick Facts
- arXiv ID: 2508.03240
- Source URL: https://arxiv.org/abs/2508.03240
- Authors: Mutaz Ayesh; Nicolás Gutiérrez-Rolón; Fernando Alva-Manchego
- Reference count: 22
- Primary result: Third place in PL subtask (77% BERT cosine similarity), second place in E2R subtask (71% average similarity) using Gemma-3 with Spanish prompts

## Executive Summary
This paper presents an LLM-based approach for automatic Spanish text adaptation into Plain Language (PL) and Easy-to-Read (E2R) formats, targeting improved accessibility. The team experimented with prompting strategies using LLaMA-3.2 and Gemma-3, ultimately selecting Gemma-3 for their final submissions. They tested zero-shot, one-shot, and few-shot prompting, incorporating explicit guidelines and structured output formatting. The best-performing configuration achieved third place in Subtask 1 (Plain Language) with a BERT-based cosine similarity score of 77% and second place in Subtask 2 (Easy-to-Read) with a 71% average similarity score. Gemma-3 proved more effective than LLaMA-3.2, particularly when prompted in Spanish. The results demonstrate the viability of instruction-tuned LLMs for text simplification tasks, though challenges remain in capturing formatting and readability nuances.

## Method Summary
The approach uses prompt engineering with Gemma-3 (4B instruction-tuned) to perform Spanish text simplification without fine-tuning. The method employs few-shot prompting (3 examples) with explicit E2R/PL guidelines from UNE 153101 EX standard, sentence-level processing instructions, and Python dictionary output formatting. Prompts were delivered in Spanish to align with target output language, improving generation quality over English prompts. The system processes each sentence individually before concatenating simplified outputs, enforcing structured output to reduce formatting errors and preambles.

## Key Results
- Gemma-3 achieved 77% BERT cosine similarity in Plain Language subtask (3rd place)
- Gemma-3 achieved 71% average similarity in Easy-to-Read subtask (2nd place)
- Spanish prompts significantly outperformed English prompts with Gemma-3
- Python dictionary formatting reduced preambles and improved output extraction
- Few-shot prompting with explicit guidelines outperformed zero-shot baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-language alignment with target-output language improves generation quality for instruction-tuned models
- Mechanism: When prompts are delivered in the same language as the intended output (Spanish), the model's attention mechanisms are primed for Spanish lexical and syntactic patterns, reducing cross-lingual interference where the model defaults to its dominant pretraining language (English).
- Core assumption: The model has sufficient multilingual capability that Spanish prompting activates—rather than degrades—instruction-following behavior.
- Evidence anchors:
  - [section] "Gemma-3...showed superior performance when prompted in Spanish, as English prompts led the model to simplify the complex sentence in English." (Section 4.2)
  - [section] "preliminary results showed that prompts and guidelines written in English obtained better results in LLaMA-3.2...When Gemma-3 was used, the messages and guidelines written in Spanish showed better results" (Section 4.2.3)
  - [corpus] No direct corpus evidence on language-alignment mechanisms; related papers focus on task performance rather than prompting language ablation.
- Break condition: Models with weaker multilingual instruction-tuning may not show this effect, or may perform worse in non-English prompts due to instruction-following capability being concentrated in English.

### Mechanism 2
- Claim: Enforcing structured output formats (Python dictionary) constrains generation to reduce formatting errors and preambles
- Mechanism: By requiring output as a specific data structure (`{"simple": "..."}`), the model's generation is anchored to a schema that discourages conversational filler ("Here is your simplification"), example leakage, and unstructured prose. The format acts as a hard constraint on the token distribution.
- Core assumption: The model has been trained on enough code/data-format examples that it can reliably produce syntactically valid Python dictionaries.
- Evidence anchors:
  - [section] "instructing the model to return its output as a Python dictionary yielded the best results, and facilitated the extraction of the simplification using a Python function." (Section 4.2)
  - [section] "The use of a Python dictionary format mitigated many of these errors; however, formatting inconsistencies still occurred." (Section 4.2.3)
  - [corpus] Corpus neighbor "BenchOverflow" examines prompt failure modes but does not address structured output constraints directly.
- Break condition: Models with weaker code-generation capabilities may produce malformed dictionaries (missing braces, incorrect key-value syntax), requiring post-processing that negates efficiency gains.

### Mechanism 3
- Claim: Explicit sentence-level processing instructions improve semantic preservation during simplification
- Mechanism: Instructing the model to segment text into sentences before simplifying ("you segmented paragraphs into sentences, then you simplify each sentence, then concatenate") creates a staged processing pipeline that reduces information loss. This counteracts the tendency of LLMs to over-summarize when processing long inputs in a single pass.
- Core assumption: The model can reliably segment Spanish text into sentences and maintain coherent concatenation of simplified segments.
- Evidence anchors:
  - [section] "explicitly instructing the model to read the sentence first and to work on it on a sentence-level helped increase similarity scores with respect to both complex and simple references." (Section 4.2)
  - [section] Prompt 7 system message: "Trabajas a nivel de oración: segmentas párrafos en oraciones, después simplificas cada oración, y luego concatenas todas las suboraciones simplificadas juntas." (Appendix D)
  - [corpus] Corpus neighbor "Split and rephrase with large language models" (Ponce et al., 2024) explicitly addresses sentence splitting but reports models "struggled to consistently follow splitting instructions."
- Break condition: Sentence-level instructions may cause over-segmentation or loss of cross-sentence coherence; performance gains may not transfer to languages with different sentence boundary conventions.

## Foundational Learning

- **Concept: Few-shot prompting with in-context learning**
  - Why needed here: Zero-shot prompting produced unreliable output formats (paragraphs with headlines, first-person narration, hallucinated dates). One-shot and few-shot provide format templates and style signals without fine-tuning.
  - Quick check question: Can you explain why adding examples to a prompt might reduce—but not eliminate—the tendency of an LLM to copy those examples as its output?

- **Concept: Prompt engineering for constrained generation**
  - Why needed here: The task requires outputs that are both semantically faithful and structurally compliant. Prompt engineering (guidelines, format constraints, negative instructions) compensates for lack of task-specific fine-tuning.
  - Quick check question: What is the difference between instructing a model "avoid complex words" vs. providing explicit guidelines that define what constitutes complexity?

- **Concept: Evaluation gaps in text simplification**
  - Why needed here: Cosine similarity and Fernández-Huerta scores do not capture E2R-specific requirements like visual formatting, line breaks, and readability for cognitive accessibility. Understanding this gap is critical for interpreting "success" in shared tasks.
  - Quick check question: Why might a simplification achieve high BERT-based similarity while still failing to meet Easy-to-Read standards?

## Architecture Onboarding

- **Component map:**
  - Input preprocessing: Raw Spanish news text from CLEARS corpus
  - Prompt assembly: System message (role + guidelines) + User message (examples + target sentence + format instruction)
  - Model inference: Gemma-3 (4B, instruction-tuned) with few-shot prompting
  - Output parsing: Python dictionary extraction with post-processing for formatting noise
  - Evaluation: SentenceBERT cosine similarity, Fernández-Huerta readability, BERTScore

- **Critical path:**
  1. Start with zero-shot baseline to identify failure modes (hallucination, wrong language, formatting)
  2. Add one-shot with explicit E2R/PL guidelines from UNE 153101 EX standard
  3. Scale to few-shot (3 examples) with sentence-level instruction + Python dictionary format
  4. Iterate on prompt language (English → Spanish for Gemma-3) based on error analysis

- **Design tradeoffs:**
  - **LLaMA-3.2 vs. Gemma-3:** LLaMA showed better English-prompt compliance but plateaued; Gemma-3 had superior Spanish-prompt performance but required Spanish-translated guidelines
  - **English vs. Spanish prompts:** Spanish prompts improved Gemma-3 output language but required manual translation of guidelines (potential translation quality risk)
  - **Format strictness:** Python dictionary reduces preambles but introduces parsing complexity for malformed outputs

- **Failure signatures:**
  - Model returns input sentence unchanged (insufficient simplification pressure)
  - Model returns one of the few-shot examples (example leakage despite negative instruction)
  - Output in English despite Spanish input (cross-lingual interference)
  - Hallucinated dates/numbers ("2000 people" → "many people")
  - First-person narration ("I will explain...")
  - Malformed dictionary output (colons instead of equal signs, missing braces)

- **First 3 experiments:**
  1. **Zero-shot baseline with Gemma-3 in Spanish:** Measure formatting compliance, language accuracy, and semantic similarity to identify dominant failure modes
  2. **One-shot with Spanish-translated E2R guidelines:** Compare against baseline to isolate guideline effects on Fernández-Huerta scores and factuality
  3. **Few-shot (3 examples) + sentence-level instruction + Python dictionary format:** Test whether combined constraints reduce example leakage and improve extraction reliability; evaluate on 100-sentence subset before full test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automatic evaluation metrics be developed to effectively capture the visual formatting and sentence segmentation requirements specific to Easy-to-Read (E2R) guidelines?
- Basis in paper: [explicit] Section 4.1 states: "None of the given automatic metrics have any way of measuring or addressing this [visual formatting]. This highlights a persistent challenge in the field of evaluating text simplification tasks."
- Why unresolved: Current metrics like BERTScore and cosine similarity focus primarily on semantic content, failing to quantify the structural readability (e.g., line breaks, layout) required for cognitive accessibility.
- What evidence would resolve it: The creation and validation of a new metric or evaluation script that correlates strongly with human judgments of E2R formatting compliance.

### Open Question 2
- Question: To what extent does the "human in the loop" prompt optimization process used for LLMs align with the specific needs of users with intellectual disabilities in real-world scenarios?
- Basis in paper: [explicit] The conclusion notes: "The results also highlight the importance of a human in the loop... Future work will benefit from incorporating human evaluation... especially one that requires intricate formatting for specific target groups."
- Why unresolved: The study relied on organizers' automatic scripts and internal evaluation against reference texts, but did not validate the generated simplifications with the actual target population (e.g., people with dyslexia or aphasia).
- What evidence would resolve it: User studies involving the target demographic comparing the LLM-generated texts against human references for comprehension and ease of reading.

### Open Question 3
- Question: What specific architectural or training data characteristics cause Gemma-3 to exhibit superior performance when prompted in Spanish compared to LLaMA-3.2, which performed better with English prompts?
- Basis in paper: [inferred] Section 4.2 observes that "Gemma-3... showed superior performance when prompted in Spanish, as English prompts led the model to simplify the complex sentence in English," while LLaMA-3.2 initially favored English.
- Why unresolved: The paper documents this cross-lingual prompting behavior as an empirical finding but does not investigate the underlying causes regarding the models' respective pre-training or instruction-tuning corpora.
- What evidence would resolve it: An ablation study analyzing the token distribution and instruction-tuning composition of both models to explain the language sensitivity in instruction adherence.

## Limitations

- The study lacks ablation experiments to isolate the contribution of individual prompt components (guidelines, sentence-level instructions, dictionary format, few-shot examples)
- Evaluation metrics do not capture Easy-to-Read specific requirements like visual formatting, line breaks, and cognitive accessibility features
- The prompt language effect with Gemma-3 remains mechanistically unexplained without investigation of underlying model characteristics
- Manual translation of English guidelines to Spanish introduces potential quality variations not controlled for in the study

## Confidence

**High Confidence** (4+ independent evidences):
- Gemma-3 outperformed LLaMA-3.2 when prompted in Spanish (Section 4.2.3)
- Zero-shot prompting produced unreliable outputs with formatting and language issues (Section 4.2)
- Few-shot prompting with explicit guidelines improved performance over zero-shot (Section 4.2.1)
- Python dictionary format reduced preambles and formatting noise (Section 4.2)

**Medium Confidence** (2-3 independent evidences):
- Sentence-level processing instructions improved semantic preservation (Section 4.2)
- Gemma-3 showed better Spanish-prompt compliance than LLaMA-3.2 (Section 4.2.3)
- Manual post-processing was required to standardize dictionary outputs (Section 4.2.3)

**Low Confidence** (1-2 independent evidences):
- Exact contribution of individual prompt components (no ablation study)
- Generalization of prompt language effects to other multilingual instruction-tuned models
- Translation quality of Spanish guidelines and its impact on results

## Next Checks

1. **Ablation study on prompt components**: Systematically remove individual elements (guidelines, sentence-level instructions, dictionary format, few-shot examples) from Prompt 7 to quantify their independent contributions to performance gains.

2. **Cross-lingual prompting validation**: Test the Spanish-prompt effect with other multilingual instruction-tuned models (e.g., mGPT, XLM-R) using identical prompt structures to determine if the mechanism generalizes beyond Gemma-3.

3. **E2R-specific formatting evaluation**: Implement automated checks for E2R formatting requirements (line breaks, font size indicators, visual structure) and evaluate whether high-scoring submissions actually meet accessibility standards beyond similarity metrics.