---
ver: rpa2
title: On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language
  Models
arxiv_id: '2512.07783'
source_url: https://arxiv.org/abs/2512.07783
tags:
- reasoning
- pre-training
- post-training
- generalization
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a controlled experimental framework to study
  how pre-training, mid-training, and reinforcement learning (RL) interact to shape
  reasoning capabilities in language models. The authors use synthetic reasoning tasks
  with explicit dependency graphs and process-verified evaluation to isolate causal
  effects of each training stage.
---

# On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models

## Quick Facts
- arXiv ID: 2512.07783
- Source URL: https://arxiv.org/abs/2512.07783
- Authors: Charlie Zhang; Graham Neubig; Xiang Yue
- Reference count: 40
- Key outcome: Controlled experiments show RL produces genuine reasoning gains only when targeting the model's edge of competence and pre-training leaves headroom, with minimal pre-training exposure (≥1%) sufficient for contextual transfer and mid-training improving both in-domain and out-of-domain performance under fixed compute budgets.

## Executive Summary
This paper introduces a controlled experimental framework to study how pre-training, mid-training, and reinforcement learning (RL) interact to shape reasoning capabilities in language models. The authors use synthetic reasoning tasks with explicit dependency graphs and process-verified evaluation to isolate causal effects of each training stage. They find that RL produces true reasoning capability gains (pass@128) only when pre-training leaves headroom and RL targets tasks at the model's edge of competence. Minimal pre-training exposure (≥1%) is sufficient for RL to enable contextual generalization, while mid-training substantially improves both in-domain and out-of-domain performance under fixed compute budgets. Process rewards further reduce reward hacking and improve reasoning fidelity. These results clarify the complementary roles of each training stage and provide actionable guidance for training pipeline design.

## Method Summary
The authors use GSM-Infinite, a synthetic reasoning task framework with explicit dependency graphs, to study reasoning capability development across training stages. They train a 100M parameter decoder-only model through three stages: pre-training on 10B tokens covering operations op=2-10, optional mid-training on op=11-14, and GRPO-based RL on 200K samples targeting the "edge of competence." Evaluation uses process-verified pass@k metrics measuring both intermediate reasoning steps and final answers across extrapolative (op=15-20) and contextual generalization dimensions. The framework enables precise control over reasoning complexity and clean isolation of each training stage's contribution.

## Key Results
- RL produces true reasoning capability gains (pass@128) only when pre-training leaves headroom and RL targets tasks at the model's edge of competence
- Minimal pre-training exposure (≥1%) is sufficient for RL to enable contextual generalization
- Mid-training substantially improves both in-domain and out-of-domain performance under fixed compute budgets
- Process rewards reduce reward hacking and improve reasoning fidelity compared to outcome-only rewards

## Why This Works (Mechanism)

### Mechanism 1: Edge-of-Competence RL Enables Genuine Capability Extension
- Claim: RL produces true reasoning capability gains (measured by pass@128) only when it targets tasks at the model's boundary of solvability—neither fully mastered nor completely out of reach.
- Mechanism: When RL data is calibrated to the model's "edge of competence," RL forces the model to compose learned atomic operations in novel ways rather than simply sharpening existing behaviors. This enables extrapolation to deeper reasoning chains beyond the pre-training distribution.
- Core assumption: The base model has acquired the necessary atomic reasoning primitives during pre-training; RL acts as a compositor rather than a primitive-creator.
- Evidence anchors:
  - [abstract] "RL produces true reasoning capability gains (pass@128) only when pre-training leaves headroom and RL targets tasks at the model's edge of competence."
  - [Section 3, Figure 3] RL on op=11-14 (edge) improves pass@128 on OOD-hard tasks, while RL on op=7-10 (ID) only improves pass@1.
  - [corpus] OctoThinker (arxiv:2506.20512) provides converging evidence that mid-training conditions models for RL responsiveness.
- Break condition: If pre-training already covers the task domain comprehensively (saturation), or if RL targets tasks either too easy (in-distribution) or too hard (far OOD with zero pass@k), RL cannot produce genuine capability gains.

### Mechanism 2: Minimal Pre-training Seeding Enables RL Transfer
- Claim: RL can induce contextual generalization to long-tail domains only when the base model has at least minimal exposure (≥1%) to those contexts during pre-training.
- Mechanism: Even sparse pre-training exposure provides latent "seeds"—basic primitive representations of surface forms—that RL can amplify and compose. Without these seeds, RL cannot synthesize context-specific capabilities; it requires foundational representations to reinforce.
- Core assumption: Atomic reasoning primitives transfer across contexts; only surface-form representations need seeding.
- Evidence anchors:
  - [abstract] "Minimal pre-training exposure (≥1%) is sufficient for RL to enable contextual generalization."
  - [Section 4, Figure 4] 0% or 0.1% pre-training exposure to context B causes RL transfer failure; 1%+ enables robust generalization even to op=20 tasks.
  - [corpus] Weak direct corpus evidence on minimal seeding thresholds; this appears to be a novel contribution of the paper.
- Break condition: If pre-training has zero exposure to a context's surface forms, RL cannot induce transfer regardless of structural similarity.

### Mechanism 3: Mid-training as Distributional Bridge
- Claim: Mid-training improves both ID and OOD performance under fixed compute by narrowing the distribution gap between broad pre-training and specialized RL objectives.
- Mechanism: Mid-training installs structured reasoning priors aligned with downstream RL tasks, stabilizing optimization and improving sample efficiency. It allows RL to focus on exploration rather than primitive acquisition.
- Core assumption: Mid-training data quality and alignment with RL objectives significantly impact downstream gains.
- Evidence anchors:
  - [abstract] "Mid-training substantially improves both in-domain and out-of-domain performance under fixed compute budgets."
  - [Section 5, Figure 6] Light-RL (20% RL, 80% mid-training) achieves best OOD-edge pass@1; Heavy-RL (80% RL, 20% mid-training) best for OOD-hard exploration.
  - [corpus] Reinforcement Mid-Training (arxiv:2509.24375) formalizes mid-training as a distinct training stage; OctoThinker shows mid-training determines RL responsiveness across model families.
- Break condition: If mid-training data is poorly aligned with RL objectives, or compute is severely constrained, benefits diminish.

## Foundational Learning
- Concept: **RL fine-tuning for LLMs (policy gradient methods, GRPO)**
  - Why needed here: The paper uses GRPO for post-training; understanding rollouts, rewards, and policy updates is essential to interpret why RL succeeds or fails.
  - Quick check question: Can you explain why pass@1 might improve while pass@128 stays constant, and what this implies about capability gains?

- Concept: **Training stage distinctions (pre-training, mid-training/CPT, post-training)**
  - Why needed here: The entire paper disentangles contributions of each stage; you need to understand what each optimizes for and why they're separated.
  - Quick check question: Why might a "bridge" stage between pre-training and RL improve sample efficiency?

- Concept: **Generalization axes: extrapolative (depth) vs. contextual (breadth)**
  - Why needed here: The paper evaluates both dimensions; they capture different failure modes and require different interventions.
  - Quick check question: If a model transfers reasoning to a new domain but fails on longer reasoning chains, which generalization axis is broken?

## Architecture Onboarding
- Component map:
  - Pre-training: 10B tokens, op=2-10, multiple context templates (establishes atomic primitives)
  - Mid-training (optional): Narrower distribution (op=11-14), bridges to RL, installs structured priors
  - Post-training: GRPO with 200K samples, calibrates data to edge of competence
  - Evaluation: Process-verified pass@k (checks intermediate steps + final answer)
  - Data generation: GSM-Infinite framework with DAG-defined dependency graphs and contextual rendering

- Critical path:
  1. Pre-train base model with broad primitive coverage + sparse long-tail seeding (≥1%)
  2. Optionally apply mid-training on edge-of-competence data (op=11-14)
  3. Run RL on calibrated edge data (pass@1 < base, pass@128 > 0)
  4. Evaluate with process verification on extrapolative (op=15-20) and contextual (unseen templates) axes

- Design tradeoffs:
  - Compute allocation: More mid-training for reliability (OOD-edge), more RL for exploration (OOD-hard)
  - RL data difficulty: Too easy → no capability gain; too hard → reward sparsity, no learning
  - Pre-training exposure: Too little → no RL transfer; too much → reduced RL headroom
  - Reward composition: Pure outcome → reward hacking; process-heavy → may constrain exploration

- Failure signatures:
  - RL improves pass@1 but not pass@128 → only sharpening existing capabilities
  - High outcome accuracy with low process accuracy → reward hacking
  - Zero transfer to long-tail contexts despite RL → insufficient pre-training seeding
  - Reward plateaus early during RL → data misaligned with model's edge of competence

- First 3 experiments:
  1. **Calibrate RL data difficulty**: Run RL on op=7-10, op=11-14, op=17-20 separately; measure pass@128 on OOD-hard (op=15-20) to confirm edge-of-competence effect.
  2. **Test minimal seeding threshold**: Pre-train with 0%, 0.1%, 1%, 10% exposure to context B (atomic only); run mixed-context RL; measure transfer to context B.
  3. **Mid/RL compute sweep**: Under fixed budget (e.g., 4.2B tokens), test β∈{0, 0.2, 0.5, 0.8, 1.0} RL allocation; compare OOD-edge pass@1 and OOD-hard pass@128.

## Open Questions the Paper Calls Out
- Question: How does an iterative curriculum design, where RL targets are dynamically updated based on the shifting "edge of competence," compare to the static training distributions used in this study?
- Question: Do process-level rewards remain effective at mitigating reward hacking when the process supervision is noisy or approximate, rather than relying on the perfect gold-standard graphs available in the synthetic framework?
- Question: Do the optimal budget allocation strategies between mid-training and RL (e.g., "Light-RL" vs. "Heavy-RL") transfer effectively to larger language models trained on natural language corpora?

## Limitations
- Synthetic task framework may not capture real-world reasoning complexity and implicit dependencies
- 100M parameter scale limits generalization to production models and their scaling dynamics
- Fixed compute budget assumptions may not hold for larger models with different compute efficiency profiles

## Confidence
- High: Edge-of-competence RL mechanism and minimal pre-training seeding threshold (≥1%)
- Medium: Mid-training distributional bridge claims and their alignment quality dependencies
- Low: Process-verified evaluation methodology's sensitivity to reward hacking vs. genuine capability gains

## Next Checks
1. Test RL effectiveness on naturally-occurring reasoning datasets (e.g., GSM8K variants) to assess external validity
2. Scale up to 1B+ parameter models while holding architectural ratios constant to verify mechanism preservation
3. Systematically vary mid-training data quality and alignment metrics to quantify their impact on RL sample efficiency