---
ver: rpa2
title: 'Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object
  Hallucination via Simple Fine-Grained CLIPScore'
arxiv_id: '2502.20034'
source_url: https://arxiv.org/abs/2502.20034
tags:
- f-clipscore
- clipscore
- hallucination
- object
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether object hallucination in vision-language
  models stems from limited vision encoder capacity. The authors propose Fine-grained
  CLIPScore (F-CLIPScore), a metric that incorporates noun-level text embeddings alongside
  full-sentence embeddings to improve hallucination detection.
---

# Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore

## Quick Facts
- arXiv ID: 2502.20034
- Source URL: https://arxiv.org/abs/2502.20034
- Authors: Hongseok Oh; Wonseok Hwang
- Reference count: 23
- Key outcome: Vision encoder capacity is not the primary cause of object hallucination

## Executive Summary
This paper challenges the prevailing assumption that object hallucination in vision-language models stems from limited vision encoder capacity. The authors propose Fine-grained CLIPScore (F-CLIPScore), which incorporates noun-level text embeddings alongside full-sentence embeddings to improve hallucination detection. Their experiments demonstrate that F-CLIPScore achieves 62.2% accuracy on hallucination detection benchmarks without additional training, representing a significant improvement over conventional metrics. The results suggest that enhanced textual granularity alone can effectively detect and mitigate object hallucination.

## Method Summary
The authors developed F-CLIPScore by extracting noun-level embeddings from text using part-of-speech tagging, then computing CLIP-based similarity scores at both sentence and noun levels. This fine-grained approach captures more detailed semantic relationships than conventional full-sentence CLIPScore. During LVLM pretraining, they applied F-CLIPScore as a filtering criterion to remove samples with high hallucination probability. The filtered dataset was then used for continued pretraining, resulting in models with reduced hallucination rates. The method requires no architectural changes or additional training objectives, making it a simple yet effective intervention.

## Key Results
- F-CLIPScore achieves 62.2% accuracy on OHD-Caps benchmark without additional training
- Represents 39.6% improvement over conventional CLIPScore for hallucination detection
- F-CLIPScore filtering during pretraining improves POPE accuracy by 4.9%

## Why This Works (Mechanism)
F-CLIPScore works by decomposing text into noun-level components and computing similarity scores at multiple granularities. This allows the metric to capture specific object-level discrepancies that full-sentence embeddings might miss. When nouns are misaligned between vision and language representations, the fine-grained scores reveal these inconsistencies more clearly than holistic comparisons. During pretraining, this enhanced detection capability enables selective filtering of hallucination-prone samples, creating a cleaner training distribution that promotes better alignment between visual and textual representations.

## Foundational Learning

**CLIP Embeddings**: Dense vector representations from CLIP models that encode semantic meaning - needed to understand how vision and language features are compared, quick check: review CLIP paper's embedding architecture

**Part-of-Speech Tagging**: NLP technique for identifying grammatical roles of words - needed to extract nouns for fine-grained analysis, quick check: test POS tagger accuracy on vision-language task descriptions

**Similarity Metrics**: Methods for comparing vector representations - needed to understand how F-CLIPScore quantifies alignment, quick check: compare cosine vs other similarity measures in CLIP space

**Vision-Language Pre-training**: Training paradigm for aligning visual and textual representations - needed to understand pretraining dynamics and hallucination formation, quick check: review LVLM objective functions

## Architecture Onboarding

**Component Map**: Vision Encoder -> CLIP Text Encoder -> Noun Extractor -> Similarity Computation -> Filtering Module

**Critical Path**: Image input → Vision Encoder → CLIP Text Encoder (noun-level) → Similarity Scores → Hallucination Detection

**Design Tradeoffs**: F-CLIPScore trades computational overhead (additional noun-level computations) for improved detection accuracy. The approach maintains compatibility with existing CLIP architectures while requiring only minor modifications to text processing pipelines.

**Failure Signatures**: High false positive rates may occur when noun-level embeddings are ambiguous or when context-dependent meanings are lost through noun extraction. Performance degradation may appear when processing complex scenes with multiple interacting objects.

**First Experiments**:
1. Benchmark F-CLIPScore against conventional CLIPScore on diverse hallucination detection datasets
2. Conduct ablation studies removing noun-level components to quantify their contribution
3. Test filtering effectiveness by pretraining on both filtered and unfiltered datasets

## Open Questions the Paper Calls Out
None

## Limitations
- The conclusion that vision encoder capacity is not the primary cause may be premature given potential circularity in using CLIP-based metrics
- Experiments focus exclusively on CLIP-based architectures without testing alternative vision encoders
- Benchmark performance may not fully translate to real-world hallucination scenarios
- The 4.9% practical improvement, while significant, represents a modest absolute gain

## Confidence

**High confidence**: F-CLIPScore outperforms conventional CLIPScore on hallucination detection benchmarks

**Medium confidence**: F-CLIPScore filtering during pretraining reduces object hallucination

**Low confidence**: Vision encoder capacity is not the primary cause of object hallucination

## Next Checks
1. Replicate findings using non-CLIP vision encoders (ConvNeXt, Swin Transformers) to test generalizability

2. Conduct ablation studies varying textual embedding granularity to identify optimal granularity levels

3. Test F-CLIPScore on real-world datasets with human-annotated hallucination labels to validate benchmark performance