---
ver: rpa2
title: Finite-Time Bounds for Average-Reward Fitted Q-Iteration
arxiv_id: '2510.17391'
source_url: https://arxiv.org/abs/2510.17391
tags:
- function
- learning
- sample
- complexity
- bellman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anchored Fitted Q-Iteration (Anc-F-QI) for
  offline RL in average-reward MDPs. The key innovation is combining Fitted Q-Iteration
  with an anchor mechanism, which can be interpreted as weight decay, to enable finite-time
  analysis in this setting.
---

# Finite-Time Bounds for Average-Reward Fitted Q-Iteration

## Quick Facts
- arXiv ID: 2510.17391
- Source URL: https://arxiv.org/abs/2510.17391
- Reference count: 40
- Introduces Anchored Fitted Q-Iteration (Anc-F-QI) with finite-time guarantees for average-reward offline RL

## Executive Summary
This paper establishes the first finite-time sample complexity bounds for average-reward offline reinforcement learning with function approximation in weakly communicating MDPs. The authors propose Anchored Fitted Q-Iteration (Anc-F-QI), which combines traditional Fitted Q-Iteration with an anchor mechanism that can be interpreted as weight decay. This approach enables theoretical analysis in the average-reward setting, where existing methods for discounted RL do not directly apply. The work relaxes structural assumptions on MDPs compared to prior research, requiring only weak communication rather than ergodicity or linearity.

## Method Summary
The paper introduces Anchored Fitted Q-Iteration (Anc-F-QI), which extends Fitted Q-Iteration to the average-reward setting by incorporating an anchor mechanism. The anchor serves as a regularizer that prevents value function estimates from drifting too far from a baseline, enabling finite-time analysis. The algorithm alternates between policy evaluation and improvement steps, using regression oracles to estimate Q-values. For datasets with a single trajectory satisfying β-mixing conditions, the method handles temporal dependencies. The key innovation is showing that the anchor mechanism enables sample complexity bounds while relaxing assumptions about MDP structure, requiring only weak communication and finite diameter rather than ergodicity or linear MDP structure.

## Key Results
- Establishes first sample complexity bounds for average-reward offline RL with function approximation: Õ(1/ϵ⁶) for IID datasets, improving to Õ(1/ϵ⁴) with relative normalization
- Achieves Õ(1/ϵ¹²) sample complexity for single-trajectory β-mixing datasets, improving to Õ(1/ϵ⁸) with relative normalization
- Relaxes structural assumptions to weakly communicating MDPs, eliminating requirements for ergodicity or linearity
- Demonstrates that the anchor mechanism can be interpreted as weight decay, providing regularization that enables theoretical guarantees

## Why This Works (Mechanism)
The anchor mechanism in Anc-F-QI provides regularization that stabilizes value function learning in the average-reward setting. By preventing value estimates from drifting too far from a baseline, the anchor enables finite-time analysis that was previously intractable for average-reward problems. This regularization is crucial because average-reward MDPs lack the discounting property that typically ensures bounded returns in discounted RL. The anchor can be interpreted as weight decay, which controls the complexity of the function class being learned and enables the application of concentration inequalities necessary for sample complexity bounds.

## Foundational Learning

**Weakly communicating MDPs**: MDPs where any state can be reached from any other state, but the mixing time may depend on the start and goal states. Why needed: This assumption allows the analysis to handle a broad class of MDPs while still enabling theoretical guarantees. Quick check: Verify the diameter of the MDP is finite and that all state pairs are mutually reachable.

**β-mixing processes**: Stochastic processes where dependencies between observations decay over time. Why needed: For single-trajectory datasets, β-mixing ensures that temporal correlations don't prevent learning. Quick check: Estimate the β-mixing coefficient from data to verify the assumption holds.

**Function approximation with bounded regression error**: The value function class must admit a regression oracle with bounded error. Why needed: This ensures that learning algorithms can approximate the true value function within a known bound. Quick check: Evaluate the regression error empirically on held-out data.

## Architecture Onboarding

Component map: Dataset -> Regression Oracle -> Q-value Estimation -> Policy Improvement -> Anchor Regularization -> Next Iteration

Critical path: The algorithm iteratively estimates Q-values through regression, updates the policy, and applies anchor regularization to maintain stability. The regression oracle is the most critical component, as its error directly impacts the sample complexity bounds.

Design tradeoffs: The anchor parameter must be carefully tuned - too large and the algorithm underfits, too small and theoretical guarantees may not hold. The choice between IID and single-trajectory analysis involves a tradeoff between dataset requirements and sample complexity.

Failure signatures: Poor performance may indicate incorrect anchor parameter selection, violation of weakly communicating assumptions, or regression oracle error exceeding theoretical bounds. Convergence issues often stem from insufficient exploration in the dataset.

Three first experiments:
1. Test Anc-F-QI on a simple weakly communicating MDP (e.g., gridworld) to verify basic functionality
2. Compare performance with and without the anchor mechanism on a benchmark problem
3. Evaluate sensitivity to anchor parameter choice across different MDP classes

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes weakly communicating MDPs with finite diameter, which may not hold in many practical applications
- Requires careful tuning of the anchor parameter, with the choice affecting both performance and theoretical guarantees
- Lacks empirical validation on real-world problems or benchmark suites
- β-mixing assumption for single-trajectory analysis may be difficult to verify in practice

## Confidence

High confidence in the mathematical derivation and proof techniques for the stated assumptions
Medium confidence in the practical applicability due to the restrictive MDP assumptions
Medium confidence in the improvement over prior work, as comparisons are primarily theoretical rather than empirical

## Next Checks

1. Implement the Anc-F-QI algorithm on benchmark average-reward MDPs (e.g., river swim, random MDPs) to verify the theoretical sample complexity predictions empirically
2. Test the algorithm's sensitivity to the anchor parameter choice across different MDP classes to understand practical tuning requirements
3. Evaluate performance when the weakly communicating assumption is violated to understand robustness beyond the theoretical guarantees