---
ver: rpa2
title: 'PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text
  with LLMs'
arxiv_id: '2509.19745'
source_url: https://arxiv.org/abs/2509.19745
tags:
- speech
- multilingual
- language
- alignment
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of aligning speech and text representations
  in multilingual Speech Large Models (SLMs), where forcing cross-language convergence
  degrades performance. The proposed Progressive Alignment Representation Training
  (PART) introduces a multi-stage, multi-task framework that separates within-language
  alignment from cross-language alignment.
---

# PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs

## Quick Facts
- **arXiv ID:** 2509.19745
- **Source URL:** https://arxiv.org/abs/2509.19745
- **Reference count:** 0
- **Primary result:** Achieves up to 41% WER reduction for ASR and significant BLEU gains for S2TT through progressive alignment training

## Executive Summary
This paper addresses the challenge of aligning speech and text representations in multilingual speech-to-text systems, where traditional joint training forces cross-language convergence and degrades performance. The proposed Progressive Alignment Representation Training (PART) introduces a three-stage framework that first aligns within-language representations, then gradually introduces cross-language tasks while dynamically activating LLM parameters. The method achieves substantial improvements on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 benchmarks, particularly for low-resource language pairs.

## Method Summary
PART employs a three-stage progressive training approach: Stage 1 trains adapter modules only on monolingual ASR data, Stage 2 progressively unfreezes the speech encoder (last 8 layers then full encoder) on monolingual data, and Stage 3 jointly optimizes encoder, adapter, and LLM on both monolingual and cross-lingual S2TT data. The method uses task-dependent LLM activation—freezing LLM for within-language ASR and unfreezing for cross-language S2TT—to leverage the LLM's multilingual capabilities only when needed. The architecture consists of SenseVoice-large encoder (~700M), Qwen2.5 LLM (1.5B/7B), and a 3-layer adapter (2 transformer + 1 CNN).

## Key Results
- Achieves up to 41% reduction in WER for ASR across multiple languages
- Significant BLEU score improvements for speech-to-text translation
- Particularly effective for low-resource language pairs
- Progressive unfreezing provides marginal but consistent improvements (0.1 WER, 0.3 BLEU)

## Why This Works (Mechanism)

### Mechanism 1: Stage-Separated Alignment
Traditional methods force cross-language convergence by mixing multilingual data from initialization. PART trains monolingual alignment first (stages 1-2), preserving language-specific acoustic patterns while enabling transfer in stage 3.

### Mechanism 2: Task-Dependent LLM Activation
For within-language ASR, LLM stays frozen—encoder learns direct acoustic-to-semantic mapping. For cross-language S2TT, LLM unfreezes, leveraging its pre-trained multilingual reasoning rather than forcing encoder to learn cross-lingual mapping alone.

### Mechanism 3: Progressive Encoder Unfreezing
Coarse adapter alignment provides stable initialization. Gradual encoder unfreezing (last-8 layers → full encoder) prevents catastrophic forgetting while allowing task-specific adaptation, though effect size is small (0.1 WER, 0.3 BLEU).

## Foundational Learning

- **Speech-Text Alignment via Adapter Modules**: Understanding that adapters project speech encoder outputs into LLM embedding space is foundational to all three PART stages.
  - *Why needed*: All three stages rely on adapter modules for cross-modal alignment
  - *Quick check*: Can you explain why adapters are trained first in isolation before encoder/LLM unfreezing?

- **Cross-Modal vs Cross-Lingual Transfer**: PART explicitly separates modality alignment (speech→text) from language transfer (source language→target language).
  - *Why needed*: The three-stage separation strategy depends on understanding this distinction
  - *Quick check*: Why would mixing ASR and S2TT data from initialization cause different failure modes than training them sequentially?

- **Catastrophic Forgetting in Fine-Tuning**: Progressive unfreezing and stage separation are designed to prevent loss of pre-trained knowledge.
  - *Why needed*: Understanding why gradual unfreezing matters requires knowledge of catastrophic forgetting
  - *Quick check*: What would happen if you unfroze all parameters simultaneously on cross-lingual data without stages 1-2?

## Architecture Onboarding

- **Component map**: Speech Encoder → Adapter → LLM
- **Critical path**: 1) Stage 1: Train adapter only on monolingual ASR data 2) Stage 2: Progressive encoder unfreezing on monolingual data 3) Stage 3: LLM unfreezing, joint training on monolingual + cross-lingual data
- **Design tradeoffs**: Three-stage training increases complexity but provides 32-41% WER reduction; LLM unfreezing improves BLEU significantly but requires more compute; progressive unfreezing provides marginal gains with added implementation complexity
- **Failure signatures**: Insufficient monolingual data in stages 1-2 leads to unstable stage 3; weak LLM multilingual capability diminishes cross-lingual gains; adapter capacity too low causes coarse alignment failure
- **First 3 experiments**: 1) Replicate two-stage baseline vs three-stage PART on single language pair 2) Ablate progressive encoder unfreezing (full vs last8-full) 3) Test LLM activation ablation: freeze LLM in stage 3 vs unfreeze

## Open Questions the Paper Calls Out

- **Generalization to other encoders**: Does PART's effectiveness generalize to speech encoders beyond SenseVoice-large, such as Whisper or WavLM?
- **Scaling to larger language sets**: How does PART scale to language sets significantly larger than the 10 languages tested (e.g., 50+ or 100+ languages)?
- **Stage 3 text-based task design**: What specific text-based tasks are introduced in Stage 3, and how does their design impact cross-lingual transfer?

## Limitations
- Adapter architecture details (layer dimensions, attention heads, CNN kernel sizes) are not specified
- Effect size of progressive unfreezing is marginal (0.1 WER, 0.3 BLEU) relative to added complexity
- Synthetic data quality for S2TT training is not thoroughly validated

## Confidence
- **High Confidence**: Three-stage training framework is well-specified and reproducible
- **Medium Confidence**: Empirical improvements are demonstrated across multiple benchmarks, but mechanisms require more rigorous validation
- **Low Confidence**: Claims about adapter architecture optimization and synthetic data quality lack sufficient detail for independent verification

## Next Checks
1. **Mechanism Isolation Test**: Run ablation studies comparing PART against variants where (a) adapter architecture is simplified, (b) progressive unfreezing is replaced with full unfreezing, and (c) LLM remains frozen throughout all stages
2. **Cross-Lingual Transfer Analysis**: Conduct per-language pair analysis on S2TT to identify which language pairs benefit most from PART and compare against strong baselines
3. **Data Efficiency Evaluation**: Train PART on subsets of monolingual data (50%, 25%) to determine minimum data requirements for each stage and assess whether three-stage complexity is justified for low-resource scenarios