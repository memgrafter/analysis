---
ver: rpa2
title: Evaluating the Performance of RAG Methods for Conversational AI in the Airport
  Domain
arxiv_id: '2505.13006'
source_url: https://arxiv.org/abs/2505.13006
tags:
- flight
- questions
- information
- graph
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates three RAG methods\u2014Traditional RAG, SQL\
  \ RAG, and Graph RAG\u2014for handling airport queries with specialized terminology\
  \ and dynamic reasoning needs. Traditional RAG (BM25 + GPT-4) achieved 84.84% accuracy\
  \ but occasionally hallucinated, posing safety risks."
---

# Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain

## Quick Facts
- arXiv ID: 2505.13006
- Source URL: https://arxiv.org/abs/2505.13006
- Reference count: 7
- Primary result: Graph RAG achieved 91.49% accuracy with fewer hallucinations compared to Traditional RAG's 84.84% accuracy

## Executive Summary
This study evaluates three retrieval-augmented generation (RAG) methods—Traditional RAG, SQL RAG, and Graph RAG—for handling airport domain queries that require specialized terminology and dynamic reasoning. The research addresses the challenge of grounding LLM outputs using retrieved context, particularly important for safety-critical applications like airport information systems. The study compares performance across different query types, including static information requests, dynamic flight data queries, and complex reasoning tasks involving flight relationships and schedules.

The results demonstrate that while Traditional RAG provides strong baseline performance, structured approaches like Graph RAG offer significant advantages for complex reasoning tasks with reduced hallucination risk. The research highlights the importance of choosing appropriate RAG architectures based on query complexity and safety requirements, with Graph RAG showing particular promise for applications requiring high accuracy and minimal hallucination in dynamic environments.

## Method Summary
The study evaluates three RAG methods using a dataset of 70 questions divided into static, dynamic, and reasoning categories. Traditional RAG employs BM25 retrieval combined with GPT-4 for generation. SQL RAG uses a relational database schema to structure queries and SQL generation for retrieval, while Graph RAG leverages a knowledge graph with flight relationships. All methods use GPT-4 for both retrieval and generation components. Performance is measured using accuracy and hallucination detection through manual observation and question-answer classification. The evaluation considers the impact of specialized terminology, abbreviations, and dynamic flight information on system performance.

## Key Results
- Traditional RAG achieved 84.84% accuracy but exhibited hallucinations in 7.1% of cases, posing safety concerns for airport applications
- SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations than Traditional RAG
- Graph RAG demonstrated superior performance on reasoning tasks (68.75% accuracy) by leveraging flight relationships in the knowledge graph
- Question classification and prompt engineering effectively handled jargon and abbreviations, with classification accuracy of 84.29%

## Why This Works (Mechanism)
The study demonstrates that structured retrieval approaches (SQL RAG and Graph RAG) reduce hallucination risk by constraining the LLM's access to verified information sources. Graph RAG's superiority in reasoning tasks stems from its ability to represent and traverse complex relationships between flights, enabling more sophisticated inference than flat retrieval approaches. The question classification system effectively routes queries to appropriate retrieval strategies based on their semantic characteristics, optimizing performance for different query types.

## Foundational Learning
- Retrieval-augmented generation (RAG): Combines information retrieval with LLM generation to ground responses in factual sources; needed to improve LLM accuracy on specialized domains
- Question classification: Categorizes queries by type and complexity; needed to route questions to appropriate retrieval strategies
- Knowledge graph construction: Represents entities and relationships as nodes and edges; needed to enable complex reasoning over interconnected data
- Hallucination detection: Identifies when LLMs generate false information; needed for safety-critical applications like airports
- Prompt engineering: Designs effective instructions for LLMs; needed to handle domain-specific terminology and abbreviations
- Accuracy measurement: Quantifies system performance; needed to compare different RAG approaches objectively

## Architecture Onboarding

Component Map:
User Query -> Question Classifier -> Retrieval Component (BM25/SQL/Graph) -> LLM Generator (GPT-4) -> Response

Critical Path:
Query input → Classification → Retrieval (context gathering) → Generation (answer synthesis) → Output

Design Tradeoffs:
- Traditional RAG: Simpler implementation, good baseline accuracy, but higher hallucination risk
- SQL RAG: Structured queries reduce hallucinations, but requires schema design and SQL generation
- Graph RAG: Best for reasoning tasks, captures relationships, but requires knowledge graph construction

Failure Signatures:
- Hallucinations: LLM generates information not supported by retrieved context
- Retrieval failures: Context does not contain relevant information for the query
- Classification errors: Questions routed to inappropriate retrieval strategy

First Experiments:
1. Test question classification accuracy on a sample of 20 queries across all three categories
2. Evaluate retrieval precision using a small set of static queries with known answers
3. Compare hallucination rates on a set of 10 reasoning questions between Traditional and Graph RAG

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How will the performance of Graph RAG change when integrating live APIs to handle real-time flight status changes and delays?
- Basis in paper: [explicit] The "Future Work" section states that current experiments are based on a static environment and do not capture real-time changes like delays or gate changes.
- Why unresolved: The current prototype was tested exclusively on static datasets; it is unknown how real-time data volatility affects the reasoning capabilities or latency of the Graph RAG pipeline.
- What evidence would resolve it: Benchmarking accuracy and response times of the Graph RAG system while connected to a live, updating flight data stream.

### Open Question 2
- Question: Does the superiority of Graph RAG in reasoning tasks persist when scaling the dataset to include a larger and more diverse set of airports?
- Basis in paper: [explicit] The "Future Work" and "Limitations" sections acknowledge the dataset is relatively small and specific to Schiphol Airport, necessitating expansion to strengthen conclusions.
- Why unresolved: The results are based on a limited sample (e.g., 30 reasoning questions) from a single airport, raising concerns about generalizability to other domains or data volumes.
- What evidence would resolve it: Evaluation results showing consistent reasoning accuracy (comparable to 68.75%) across a dataset expanded to thousands of queries from multiple international airports.

### Open Question 3
- Question: Can a quantitative metric be established to precisely measure the reduction in hallucinations between Traditional RAG and structured RAG methods?
- Basis in paper: [inferred] The paper notes that calculating the exact accuracy or rate of hallucinations is "challenging" and relies on manual observation rather than a formal statistical metric.
- Why unresolved: The claim that SQL/Graph RAG reduce hallucinations is qualitative; without a standardized metric, the degree of safety improvement remains approximate.
- What evidence would resolve it: A comparative study using a formal hallucination detection framework (e.g., factual consistency scores) to quantify the error rate differences between the three methods.

## Limitations
- Single domain evaluation: Results are based exclusively on Schiphol Airport data, limiting generalizability to other specialized domains
- GPT-4 circularity: Using GPT-4 for both retrieval and evaluation creates potential bias in accuracy measurements
- No statistical significance testing: Performance differences between methods are not validated with statistical analysis
- Limited sample size: The 70-question dataset may not represent the full complexity of real-world airport queries

## Confidence
High: Comparative accuracy rankings among the three RAG methods are well-supported by the reported metrics and systematic evaluation approach.

Medium: The hallucination reduction claims for SQL RAG and Graph RAG are reasonable given the evaluation methodology, but the safety implications could benefit from more rigorous impact assessment.

Medium: The superiority of Graph RAG for dynamic queries and reasoning tasks is supported by the results, though the specific mechanisms and generalizability require further validation.

## Next Checks
1. Conduct cross-domain evaluation by testing these RAG methods on at least two additional specialized domains (e.g., healthcare and finance) to assess generalizability beyond airport applications.

2. Implement blind evaluation using human annotators unfamiliar with the RAG methods to independently verify accuracy metrics and hallucination assessments, removing potential GPT-4 bias.

3. Perform statistical significance testing on accuracy differences between methods across all query types to determine whether observed performance gaps are statistically meaningful rather than random variation.