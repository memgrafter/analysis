---
ver: rpa2
title: Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization
arxiv_id: '2505.16737'
source_url: https://arxiv.org/abs/2505.16737
tags:
- fine-tuning
- safety
- harmful
- arxiv
- lsafety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the critical safety challenge where fine-tuning
  large language models (LLMs) on benign data can still degrade their safety alignment.
  The authors propose Safety-Aware Probing (SAP), a novel optimization framework that
  integrates a safety-aware probe into the gradient propagation process to identify
  and avoid harmful optimization directions during fine-tuning.
---

# Mitigating Fine-tuning Risks in LLMs via Safety-Avoiding Probing Optimization

## Quick Facts
- arXiv ID: 2505.16737
- Source URL: https://arxiv.org/abs/2505.16737
- Authors: Chengcan Wu; Zhixin Zhang; Zeming Wei; Yihao Zhang; Meng Sun
- Reference count: 40
- This paper proposes Safety-Aware Probing (SAP) to prevent safety degradation during LLM fine-tuning by integrating a safety-aware probe into the gradient optimization process.

## Executive Summary
This paper tackles the critical safety challenge where fine-tuning large language models (LLMs) on benign data can still degrade their safety alignment. The authors propose Safety-Aware Probing (SAP), a novel optimization framework that integrates a safety-aware probe into the gradient propagation process to identify and avoid harmful optimization directions during fine-tuning. SAP employs a bi-level optimization strategy to maximize a safety-usefulness loss function, encouraging the model to prefer safe updates while maintaining task-specific performance. Extensive experiments across three popular LLMs (Llama-2, Vicuna, Qwen2.5) and multiple datasets demonstrate that SAP significantly reduces harmful outputs (e.g., harmful score reduced from 32.5% to 23.1% on average) while achieving comparable test loss to standard fine-tuning methods. The method also shows robustness against adversarial attacks and compatibility with existing safety techniques, offering a versatile solution for secure LLM deployment.

## Method Summary
Safety-Aware Probing (SAP) uses bi-level optimization where a safety-aware probe V is optimized to maximize a safety-usefulness loss function, while the model weights W are updated to minimize task loss under this safety-aware landscape. The method computes harmful directions from safety contrastive pairs, perturbs hidden states at middle layers (default: layers 11-20), and updates both probe and weights alternately. SAP requires D_useful (task data), D_safe/D_harmful (safety contrastive pairs), and operates with LoRA rank 8. The framework integrates seamlessly with standard fine-tuning pipelines while adding safety awareness through in-training probing rather than post-hoc filtering.

## Key Results
- SAP reduces harmful score from 32.5% to 23.1% on average across three LLMs
- Maintains comparable test loss to standard fine-tuning while improving safety
- Shows robustness against adversarial attacks and compatibility with existing safety techniques
- Probing middle layers (11-20) provides optimal safety-utility trade-off

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on benign data degrades safety because task-optimizing gradients align with harmful directions. The authors compute cosine similarity between usefulness-critical gradients (∇L_usefulness) and safety-critical gradients (∇L_safety) across layers. When these share direction, standard gradient descent implicitly reduces both task loss and harmfulness loss, pushing the model toward harmful configurations even without adversarial data. Core assumption: The entanglement observed in Llama-2, Qwen, and Vicuna generalizes across transformer-based LLMs with similar alignment procedures. Evidence: Figure 3 shows cosine similarity >0.3 across many layers and epochs during fine-tuning.

### Mechanism 2
A safety-aware probe V can reshape the usefulness loss landscape to penalize harmful updates. SAP maximizes L_su = L_usefulness(W + ΔW_harmful, V) − L_usefulness(W, V). When this difference is positive, stepping toward harmful parameters increases task loss, so gradient descent naturally avoids that direction. The probe V is optimized via gradient ascent to create this landscape property, then W is updated with V fixed. Core assumption: A single-step approximation for V optimization is sufficient to capture safety-relevant landscape features. Evidence: Figure 4 shows SFT suffers substantial drop in aggregated L_su while SAP mitigates this drop.

### Mechanism 3
Probing middle layers (11-20 in 32-layer models) provides optimal safety-utility trade-off. Hidden states at middle layers capture semantic content before task-specific specialization. Perturbing V at these layers alters gradient flow for safety-critical representations without disrupting low-level features or final task heads. Core assumption: Middle layers are where safety-critical representations concentrate across model families. Evidence: Table 7 shows v[11:20] achieves the best performance, and Table 10 shows random two-layer probing still reduces harmful scores.

## Foundational Learning

- **Concept: Bi-level optimization (inner maximization + outer minimization)**
  - Why needed here: SAP requires understanding that V is optimized to maximize L_su while W minimizes L_usefulness—these compete and must be solved alternately.
  - Quick check question: Can you explain why standard gradient descent on W alone cannot achieve what SAP achieves?

- **Concept: Sharpness-aware minimization (SAM) and weight perturbation**
  - Why needed here: SAP draws from SAM's philosophy—probing nearby parameters to improve optimization—but repurposes it for safety rather than generalization.
  - Quick check question: How does the "probe" in SAP differ from SAM's perturbation goal?

- **Concept: Contrastive loss formulation (safe vs. harmful response pairs)**
  - Why needed here: L_safety = L(W, D_safe) − L(W, D_harmful) requires paired data where identical prompts have divergent safe/harmful completions.
  - Quick check question: What would happen if your safe/harmful datasets use different prompts rather than shared x_harmful?

## Architecture Onboarding

- **Component map:**
  - Input streams: D_useful (task data), D_safe/D_harmful (safety contrastive pairs, ~50 samples each)
  - Probe V: Hidden state perturbations applied to selected layers (default: layers 11-20)
  - Harmful direction calculator: Computes ΔW_harmful = ε · ∇_W L_safety per batch
  - Bi-level optimizer: Inner loop updates V (step β=5e-2), outer loop updates W (step α=1e-4)
  - Loss functions: L_usefulness for task performance, L_safety for contrastive safety, L_su for probe objective

- **Critical path:**
  1. Sample batch from D_useful + small batch from D_safe/D_harmful
  2. Forward pass with current W, compute ∇W L_safety → ΔW_harmful
  3. Initialize V=0, then gradient-ascent on L_su to get V_safe
  4. Forward pass with V_safe, compute ∇_W L_usefulness(W, V_safe)
  5. Update W ← W − α · ∇_W L_usefulness
  6. Repeat for K steps

- **Design tradeoffs:**
  - Probe layer count: More layers → better safety but 2× compute; middle layers give best ROI
  - Safety data size: Paper uses 50 samples; ablation not shown but likely diminishing returns beyond 100
  - LoRA compatibility: SAP works with full fine-tuning or LoRA; SafeLora/SaLoRA are LoRA-only

- **Failure signatures:**
  - Harmful score not decreasing: Check cosine similarity—is entanglement absent for your model?
  - Task performance collapsing: β too large (V dominates), or probe layers include early/late layers
  - GPU OOM: Probe adds ~40.87GB—similar to SFT, but reduce batch size if needed

- **First 3 experiments:**
  1. **Entanglement verification**: Before implementing SAP, compute cosine similarity between ∇L_usefulness and ∇L_safety across layers for your model on a held-out batch. If <0.15 correlation, SAP may not help.
  2. **Layer ablation**: Run SAP probing only layers 1-10, 11-20, 21-30 separately on a small task (500 steps) to find your model's sweet spot—don't assume Llama-2's 11-20 transfers.
  3. **SAP + baseline combination**: Per Table 5, SAP+SAFT or SAP+Lisa yields additive benefits. Test if your baseline defense combines non-destructively with SAP by running both together and checking if HS < min(SAP alone, baseline alone).

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational overhead of SAP's bi-level optimization scale when applied to LLMs significantly larger than 7B parameters? Table 6 indicates SAP requires approximately three times the clock time per batch compared to standard fine-tuning on 7B models. The efficiency of calculating and applying safety-aware probes on industrial-scale models (e.g., 70B+) remains untested.

### Open Question 2
To what extent is SAP's effectiveness dependent on the specific selection and volume of the auxiliary safety (D_safe) and harmful (D_harmful) datasets? Section 5.1 mentions utilizing 50 samples for these datasets, but provides no ablation study on how varying this data quantity or quality impacts performance. It is unclear if the method is robust to data scarcity or if performance degrades significantly with fewer safety examples.

### Open Question 3
Is there a theoretical basis for the empirical finding that probing middle layers (layers 11-20) yields the optimal safety-utility trade-off? Section 5.4 and Table 7 identify middle layers as the best probing set through trial and error, without explaining the underlying mechanism. The relationship between layer depth and the entanglement of safety-critical versus useful-critical directions is not theoretically justified.

## Limitations
- Computational cost is significant (40.87GB overhead, 2× slower than SFT)
- Safety data requirement is minimal (50 samples) but unablated
- Results may not transfer to larger models (>30B parameters) or different architectures

## Confidence

- **High**: Empirical demonstration that SAP reduces harmful outputs (HS from 32.5% to 23.1%) while maintaining utility
- **Medium**: Mechanism explaining gradient entanglement and probe optimization effectiveness
- **Medium**: Compatibility with existing safety techniques and adversarial robustness
- **Low**: Generalization to future model architectures and evolving safety threats

## Next Checks

1. **Entanglement verification**: Before implementing SAP, compute cosine similarity between ∇L_usefulness and ∇L_safety across layers for your specific model on a held-out batch. If correlation <0.15, SAP may not provide benefit.

2. **Layer ablation study**: Run SAP probing only layers 1-10, 11-20, 21-30 separately on a small task (500 steps) to identify your model's optimal probe range rather than assuming Llama-2's 11-20 transfers.

3. **SAP + baseline combination**: Test whether your existing safety baseline (SAFT, Lisa, etc.) combines non-destructively with SAP by running both together and verifying if HS < min(SAP alone, baseline alone).