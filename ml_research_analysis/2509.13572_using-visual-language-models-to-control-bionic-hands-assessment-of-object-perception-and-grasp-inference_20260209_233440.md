---
ver: rpa2
title: 'Using Visual Language Models to Control Bionic Hands: Assessment of Object
  Perception and Grasp Inference'
arxiv_id: '2509.13572'
source_url: https://arxiv.org/abs/2509.13572
tags:
- object
- grasp
- hand
- claude
- sonnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks eight vision-language models (VLMs) for inferring
  object properties and grasp parameters in prosthetic hand control. The authors collected
  a dataset of 34 household objects and prompted VLMs to output structured JSON describing
  object name, shape, dimensions, orientation, and grasp parameters (type, hand rotation,
  aperture, finger count).
---

# Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference

## Quick Facts
- **arXiv ID**: 2509.13572
- **Source URL**: https://arxiv.org/abs/2509.13572
- **Reference count**: 17
- **Primary result**: Vision-language models achieve >88% accuracy for object identification and shape recognition, but show variable performance (44-91%) for grasp type inference in prosthetic hand control

## Executive Summary
This paper evaluates eight vision-language models (VLMs) for their ability to infer object properties and grasp parameters essential for controlling bionic hands. The authors collected a dataset of 34 household objects and prompted VLMs to output structured JSON describing object characteristics and grasp parameters. Results show strong performance for object identification, shape, and orientation, but significant variability in grasp type accuracy across different VLMs. The study demonstrates the potential of VLMs for prosthetic control while highlighting limitations in real-time latency and grasp parameter estimation accuracy.

## Method Summary
The study benchmarked eight VLMs (GPT-4.1, Gemini 2.5 Flash, Gemini 2.5 Pro, Gemma 3, Claude 3.5 Haiku, Llama 3.2 11B, Qwen2.5-VL-7B, and Llama 3.2 3B) using a dataset of 34 household objects. VLMs were prompted to output structured JSON containing object name, shape, dimensions, orientation, and grasp parameters (type, hand rotation, aperture, finger count). Performance was measured across six tasks: object identification, shape recognition, object dimensions estimation, orientation detection, grasp type inference, and grasp parameter estimation. The evaluation included accuracy metrics for classification tasks and mean absolute error for regression tasks, along with latency and cost measurements for each VLM.

## Key Results
- VLMs achieved >88% accuracy for object identification, shape recognition, and orientation detection
- Grasp type inference accuracy varied widely (44-91%) across different VLMs and object categories
- Gemini 2.5 Flash demonstrated the fastest response time (2.6 seconds) and lowest cost ($0.00016 per query)
- Object dimension estimation errors were generally under 20 mm, with Gemini 2.5 Flash 20-05 showing the best performance
- Hand rotation estimation exhibited the highest variability (MAE 13-48Â°), while GPT-4.1 achieved the lowest aperture estimation error (5.74 mm)

## Why This Works (Mechanism)
Vision-language models leverage multimodal transformer architectures trained on extensive datasets containing images and their associated textual descriptions. These models learn to map visual features to semantic concepts, enabling them to identify objects and infer their properties from single images. The structured JSON output format provides a consistent interface for extracting specific information about objects and grasp parameters. By using prompt engineering with predefined categories and ranges, the models can be guided to produce actionable grasp configurations suitable for prosthetic hand control.

## Foundational Learning
- **Vision-Language Model Architecture**: Multimodal transformers that process both visual and textual inputs - needed to understand how VLMs integrate image and language processing; quick check: verify model uses cross-attention mechanisms between visual and language encoders
- **Grasp Parameter Space**: Definition of grasp types, hand rotations, apertures, and finger configurations - needed to establish the structured output format; quick check: confirm all grasp parameters are within physiologically achievable ranges for prosthetic hands
- **Structured JSON Output**: Predefined schema for organizing object and grasp information - needed to enable consistent interpretation by control systems; quick check: validate JSON schema covers all required fields for prosthetic control
- **Prompt Engineering**: Use of predefined categories and ranges in prompts - needed to guide model responses toward actionable outputs; quick check: verify prompts include appropriate constraints for each task
- **Performance Metrics**: Accuracy for classification tasks and mean absolute error for regression tasks - needed to quantify model performance across different objectives; quick check: ensure metrics are appropriate for each task type
- **Latency and Cost Analysis**: Measurement of API response times and computational costs - needed to assess real-world feasibility; quick check: compare measured latencies against acceptable thresholds for prosthetic control

## Architecture Onboarding
**Component Map**: Camera -> Image Preprocessing -> VLM API -> Structured JSON Output -> Grasp Execution Module
**Critical Path**: The visual input flows through image capture, preprocessing, VLM inference, and structured output generation before reaching the grasp execution module
**Design Tradeoffs**: The study balances model accuracy against latency and cost, with Gemini 2.5 Flash offering the best combination of speed and affordability despite slightly lower accuracy than larger models
**Failure Signatures**: High variability in grasp type accuracy across object categories suggests model limitations in generalizing grasp patterns; significant rotation estimation errors indicate challenges with 3D orientation inference from 2D images
**First Experiments**:
1. Test VLM performance on objects with varying textures and lighting conditions to assess robustness
2. Implement a feedback loop where VLM output is validated against actual grasp success rates
3. Compare VLM-generated grasps against expert-designed grasps for the same objects

## Open Questions the Paper Calls Out
- How do VLMs perform on objects with complex geometries or transparent materials?
- What is the impact of object occlusion on grasp parameter estimation accuracy?
- Can VLMs adapt to user-specific preferences and physical limitations?
- How does VLM performance degrade with increasing object variety beyond the 34-item dataset?
- What are the optimal strategies for combining multiple VLMs to improve overall performance?

## Limitations
- Small dataset of only 34 household objects limits generalizability to real-world scenarios
- Laboratory conditions with controlled lighting and backgrounds differ from practical prosthetic use environments
- Significant variability in grasp type accuracy (44-91%) across different VLMs and object categories
- High latency (2.6-12.5 seconds) may not meet real-time control requirements for prosthetic hands
- Limited evaluation of model robustness under varying environmental conditions

## Confidence
- **High**: Object identification, shape recognition, and orientation detection performance
- **Medium**: Grasp type inference accuracy and hand rotation estimation due to high variability
- **Medium**: Aperture estimation given the range of errors (5.74-24.53 mm) across models

## Next Checks
1. **Real-world environmental testing**: Evaluate the same VLMs across multiple lighting conditions, backgrounds, and object occlusions using a larger dataset (minimum 100+ objects) to assess robustness in practical prosthetic scenarios.

2. **End-to-end latency measurement**: Implement the complete control pipeline including image capture, VLM inference, grasp execution, and mechanical response to measure total system latency under realistic prosthetic hand actuation constraints.

3. **User preference and error tolerance validation**: Conduct trials with prosthetic users to determine acceptable error thresholds for grasp parameters and assess whether VLM-generated grasps meet functional requirements for daily activities.