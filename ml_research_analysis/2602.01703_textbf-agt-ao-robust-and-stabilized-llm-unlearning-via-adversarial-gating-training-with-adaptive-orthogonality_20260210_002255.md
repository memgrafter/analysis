---
ver: rpa2
title: '$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial
  Gating Training with Adaptive Orthogonality'
arxiv_id: '2602.01703'
source_url: https://arxiv.org/abs/2602.01703
tags:
- unlearning
- forget
- agtao
- adversarial
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of removing sensitive or unwanted\
  \ information from large language models (LLMs) without degrading overall model\
  \ performance. The key issue is balancing robust unlearning (to erase targeted knowledge)\
  \ with preserving the model\u2019s general capabilities."
---

# $\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality

## Quick Facts
- **arXiv ID:** 2602.01703
- **Source URL:** https://arxiv.org/abs/2602.01703
- **Reference count:** 40
- **Primary result:** AGT$^AO$ achieves KUR ≈ 0.01, MMLU 58.30, PLR ≈ 0.53 on TOFU benchmark, outperforming existing unlearning methods

## Executive Summary
This paper addresses the fundamental challenge of removing sensitive or unwanted information from large language models while preserving general capabilities. The authors identify two key failure modes in existing unlearning approaches: catastrophic forgetting (where the model loses overall performance) and superficial forgetting (where targeted knowledge can be recovered through attacks). AGT$^AO$ combines Adaptive Orthogonality (AO) and Adversarial Gating Training (AGT) to create a robust unlearning framework that achieves excellent forgetting efficacy while maintaining model utility and privacy guarantees.

## Method Summary
AGT$^AO$ employs a three-component architecture: Adaptive Orthogonality dynamically resolves gradient conflicts between forgetting and retention objectives through soft orthogonal constraints; Adversarial Gating Training formulates unlearning as a latent-space min-max game with worst-case perturbation injection; and Gradient-Norm-Based Gating provides curriculum-based stability by activating adversarial training only when gradients stabilize. The method uses NPO-style forget loss with L_infinity PGD attacks at semantic entry layers (Layer 10 for 7B models), warmup periods, and carefully balanced regularization to achieve robust erasure without utility collapse.

## Key Results
- Achieves Forget Quality of -9.43 on TOFU benchmark while maintaining Model Utility of 0.59
- KUR (knowledge unlearning rate) of 0.01 indicates minimal knowledge recovery potential
- Outperforms baselines across multiple metrics: MUSE (0.61 vs 0.44 utility), WMDP (0.53 vs 0.49 PLR)
- Demonstrates robust resistance to quantization and re-learning attacks (recovery < 50% vs >1000% for baselines)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Orthogonality for Gradient Conflict Resolution
Soft orthogonal constraints between forgetting and retaining gradients mitigate catastrophic forgetting better than hard projection. The AO regularization term R_AO = I(g_f · g_r < 0)((1 - cos(g_f, g_r))/2)^γ activates only when gradient directions conflict (dot product < 0), penalizing non-orthogonal alignment while allowing compatible updates to proceed unimpeded. This resolves the tension between erasure and preservation objectives.

### Mechanism 2: Latent-Space Min-Max Game for Robust Erasure
Formulating unlearning as an adversarial game in hidden-state space produces deeper erasure than loss minimization alone. AGT structures optimization as min_θ max_∥δ∥_p≤ε L_unlearn(h_f^(l) + δ, h_r; θ), where the inner loop uses PGD to find worst-case perturbation δ* that maximizes forget-set recoverability, and the outer loop updates parameters to resist this perturbation. This forces the model to adopt configurations robust to internal recovery attempts.

### Mechanism 3: Gradient-Norm-Based Gating for Training Stability
Curriculum-based adversarial injection prevents optimization collapse during early unlearning phases. During warmup (first N_warmup steps), adversarial inner loop is disabled entirely. After warmup, adversarial attacks activate only when ∥∇L_unlearn∥_2 < τ_grad, indicating the optimization landscape has flattened. This prevents adversarial pressure from destabilizing the high-variance early training phase.

## Foundational Learning

- **Concept: Gradient Ascent/Descent for Unlearning**
  - **Why needed here:** The base unlearning objective combines L_forget (gradient ascent on forget set to erase) and L_retain (gradient descent on retain set to preserve). Without understanding this tension, AO's role in resolving conflicts is opaque.
  - **Quick check question:** What happens if you perform only gradient ascent on the forget set without any retain constraint?

- **Concept: Min-Max Optimization (Adversarial Training)**
  - **Why needed here:** AGT's inner-outer loop structure is a min-max game. The inner maximization finds adversarial perturbations; the outer minimization updates parameters against them.
  - **Quick check question:** In AGT, which variable does the inner loop optimize, and which does the outer loop optimize?

- **Concept: Projected Gradient Descent (PGD)**
  - **Why needed here:** The inner loop uses PGD with L∞ constraints to find δ*. Understanding projection operations and step size α is critical for tuning perturbation strength.
  - **Quick check question:** Why must PGD project δ back to the ε-ball after each gradient step?

## Architecture Onboarding

- **Component map:** Forget Set → Standard Unlearning Loss (L_forget + L_retain) → Adaptive Orthogonality (AO Regularizer) → Adversarial Gating (activates inner loop) → Inner Loop: PGD Attack (Find δ* at layer L) → Outer Loop: Parameter Update with perturbed hidden states

- **Critical path:**
  1. Warmup phase completes (adversarial loop disabled)
  2. Gradient norm drops below τ_grad → adversarial gating opens
  3. Inner PGD finds δ* at semantic entry layer (Layer 10 for 7B models)
  4. Outer loop updates θ with perturbed hidden states
  5. AO penalty modulates gradient if conflict detected

- **Design tradeoffs:**
  - **Perturbation layer:** Layer 10 (7B models) captures semantic entry; too shallow = lexical only; too deep = limited backprop effectiveness
  - **Warmup duration:** Longer warmup = more stable but slower robustness; paper uses first epoch
  - **Soft vs hard projection:** Soft AO preserves more utility (0.59 vs 0.47) but may allow some gradient interference

- **Failure signatures:**
  - Model generates "at at at at..." repetition → catastrophic forgetting (AO insufficient)
  - Forget knowledge recoverable under jailbreak → superficial forgetting (AGT disabled or under-trained)
  - Training diverges early → adversarial injection during warmup or τ_grad too high

- **First 3 experiments:**
  1. **Baseline ablation:** Run AGT^AO, AGT^AO w/o AO, and AGT^AO w/o AGT on TOFU forget10%; compare Model Utility and KUR to verify component necessity
  2. **Layer sensitivity sweep:** Inject perturbations at layers 0, 5, 10, 15, 20, 25 on LLaMA-7B; plot ROUGE-L vs layer to validate semantic entry hypothesis
  3. **Quantization robustness test:** After unlearning, apply 4-bit quantization; measure ROUGE-L recovery on forget set. Expect AGT^AO < 50% recovery vs GA/NPO > 1000%

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the computational overhead introduced by the min-max optimization loop be reduced without sacrificing the robustness of the unlearning process?
- **Basis in paper:** [Explicit] The "Limitations" section states that the min-max game inherent in the adversarial inner loop introduces significant computational overhead compared to standard fine-tuning methods, identifying efficiency optimization as a goal for future work.
- **Why unresolved:** The current method relies on iterative Projected Gradient Descent (PGD) steps for every batch to simulate worst-case perturbations, which is inherently slower than single-step gradient methods.
- **What evidence would resolve it:** A modified AGT$^{AO}$ algorithm that utilizes single-step approximations or transferability of perturbations to reduce training time, while maintaining KUR and Model Utility metrics comparable to the current multi-step implementation.

### Open Question 2
- **Question:** Does the AGT$^{AO}$ framework scale effectively to significantly larger models (e.g., 70B+ parameters) while maintaining the balance between forgetting and utility?
- **Basis in paper:** [Explicit] The authors explicitly state in the "Limitations" section that while the framework is effective in the current experimental scope, they intend to validate its scalability on larger-scale models in future research.
- **Why unresolved:** The experiments were restricted to 2B and 7B parameter models; adversarial training dynamics and gradient conflicts often become more complex and unstable as parameter counts increase.
- **What evidence would resolve it:** Evaluation results of AGT$^{AO}$ on 70B or frontier-scale models across the TOFU, MUSE, or WMDP benchmarks, showing that the Gradient-Norm-Based Gating mechanism stabilizes training at that scale.

### Open Question 3
- **Question:** Is there a generalizable heuristic for identifying the optimal "Semantic Entry" layer for perturbation injection across different architectures?
- **Basis in paper:** [Inferred] The paper empirically identifies Layer 10 as the optimal perturbation injection point for 7B models (Layer 4 for 2B) via sensitivity analysis, describing it as the transition from syntax to semantics.
- **Why unresolved:** The selection of this layer currently appears to rely on experimental search rather than a theoretical rule. If the "Semantic Entry" varies unpredictably by architecture, the method requires costly tuning.
- **What evidence would resolve it:** A theoretical analysis or a structural probe that predicts the optimal layer $l$ based on model width/depth ratios, validating that this layer consistently corresponds to the syntactic-semantic boundary in diverse model families.

## Limitations
- Synthetic benchmark bias: TOFU uses artificially constructed forget/retain partitions that may not reflect real-world knowledge overlap
- Hyperparameter sensitivity: Performance depends on multiple unspecified hyperparameters (λ_ao, α/β, ε, ρ) without provided sensitivity analysis
- Computational overhead: Min-max optimization loop introduces significant training time compared to standard fine-tuning methods

## Confidence
**High Confidence:**
- AO improves utility retention compared to hard projection (0.59 vs 0.47 utility in ablation)
- AGT reduces re-learning recovery rates compared to baselines
- Combined method achieves state-of-the-art KUR scores across benchmarks

**Medium Confidence:**
- AO's adaptive penalty mechanism works as described (gradient conflict detection and selective application)
- AGT's min-max game structure produces more robust erasure than standard gradient ascent
- Gating mechanism prevents early training instability

**Low Confidence:**
- The specific hyperparameter values used (no sensitivity analysis provided)
- Layer 10 selection being optimal across different model sizes
- Real-world applicability given synthetic benchmark nature

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Run ablation studies varying λ_ao (0.01, 0.1, 1.0), ε perturbation bounds (0.1, 0.5, 1.0), and ρ gating threshold (0.3, 0.6, 0.9) to identify which parameters most affect KUR and utility trade-off.

2. **Cross-Architecture Validation:** Test AGT^AO on LLaMA2-13B and LLaMA2-70B using identical layer injection strategy (layer 10) to verify scalability and identify if layer selection needs adjustment by model size.

3. **Real-World Knowledge Unlearning:** Apply method to remove factual knowledge from Wikipedia-based datasets where forget/retain boundaries are naturally overlapping rather than artificially constructed, measuring both utility retention and re-learning resistance.