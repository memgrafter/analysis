---
ver: rpa2
title: Minimal neuron ablation triggers catastrophic collapse in the language core
  of Large Vision-Language Models
arxiv_id: '2512.00918'
source_url: https://arxiv.org/abs/2512.00918
tags:
- neurons
- collapse
- critical
- clip
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies critical neurons in Large Vision-Language
  Models (LVLMs) that, when masked, trigger catastrophic collapse. The authors propose
  the Consistently Activated Neurons (CAN) method, which combines activation magnitude
  and gradient sensitivity to rank neuron importance.
---

# Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2512.00918
- **Source URL:** https://arxiv.org/abs/2512.00918
- **Reference count:** 40
- **Primary result:** Masking 4-5 neurons in language model feed-forward networks causes complete LVLM failure

## Executive Summary
This study reveals that Large Vision-Language Models (LVLMs) possess critical vulnerability points where minimal neuron ablation triggers catastrophic system collapse. Through the proposed Consistently Activated Neurons (CAN) method, researchers identified specific neurons whose masking causes complete model failure despite representing an extremely small fraction of total parameters. The experiments demonstrate that language model components are particularly susceptible to targeted ablation, with the down-projection layer being especially vulnerable to targeted attacks. The findings expose a fundamental fragility in LVLM architecture where critical functionality concentrates in a tiny subset of neurons.

## Method Summary
The research introduces the Consistently Activated Neurons (CAN) method, which ranks neuron importance by combining activation magnitude and gradient sensitivity. This approach identifies neurons that are both frequently activated and significantly influence model outputs through gradient-based sensitivity analysis. The method was applied to two prominent LVLMs - LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b - targeting feed-forward network neurons within the language model component. Neuron masking experiments systematically evaluated the impact of removing individual neurons and combinations thereof on model performance, measuring accuracy degradation across 1000 test samples to identify critical thresholds for catastrophic failure.

## Key Results
- Masking just 4-5 neurons in the language model's feed-forward networks triggers complete model failure
- Critical neurons are predominantly located in the language model component rather than vision components
- The down-projection layer shows particular vulnerability to targeted neuron ablation
- A consistent two-stage collapse pattern emerges: initial expressive degradation followed by sudden complete failure

## Why This Works (Mechanism)
The mechanism underlying catastrophic collapse stems from the concentration of critical functionality within a small subset of neurons that serve as essential computational bottlenecks. When consistently activated neurons are masked, they eliminate crucial pathways for information processing, causing cascading failures through the model's computational graph. The language model's greater vulnerability likely reflects its role in high-level reasoning and task completion, where specific neuron activations encode essential semantic relationships and generation capabilities. The two-stage collapse pattern suggests that initial neuron losses can be partially compensated through redundant pathways until a critical threshold is reached, at which point the system experiences sudden, irreversible failure.

## Foundational Learning
- **Consistently Activated Neurons (CAN):** A neuron importance ranking method combining activation frequency with gradient sensitivity to identify critical computational units
  - Why needed: Traditional neuron importance metrics often fail to capture the combination of frequent activation and functional significance
  - Quick check: Verify that CAN-identified neurons show both high activation frequency and strong gradient contributions

- **Neuron Masking:** The systematic deactivation of individual neurons or small groups to assess their functional importance
  - Why needed: Provides direct experimental evidence of neuron criticality by observing performance impact
  - Quick check: Confirm that masked neurons no longer contribute to forward or backward passes

- **Catastrophic Collapse:** The phenomenon where minimal targeted interventions cause complete system failure rather than gradual degradation
  - Why needed: Demonstrates non-linear system behavior and vulnerability to targeted attacks
  - Quick check: Monitor accuracy drop rate during progressive neuron masking

## Architecture Onboarding

### Component Map
Language Model -> Feed-Forward Networks -> Neuron Masking -> Performance Evaluation
Vision Model -> (Separate pathway) -> Performance Evaluation

### Critical Path
The critical computational path runs through the language model's feed-forward networks, particularly the down-projection layer, where information processing bottlenecks create vulnerability points. Critical neurons in this path serve as essential computational nodes whose removal disrupts the entire processing chain.

### Design Tradeoffs
The architecture prioritizes parameter efficiency and computational performance over robustness, resulting in concentration of critical functionality within small neuron subsets. This design choice enables strong performance but creates single points of failure where minimal perturbations can cause system-wide collapse.

### Failure Signatures
The two-stage collapse pattern serves as a distinctive failure signature: initial gradual performance degradation followed by sudden complete failure. This pattern indicates the presence of redundant pathways that can compensate for early neuron losses until a critical threshold is exceeded.

### First Experiments
1. Apply CAN method to identify consistently activated neurons in a different LVLM architecture
2. Conduct progressive neuron masking experiments to map the critical threshold for collapse
3. Compare CAN-identified critical neurons with those found using alternative importance ranking methods

## Open Questions the Paper Calls Out
None

## Limitations
- Limited model diversity with only two LVLMs tested across different architectures
- Single performance metric threshold (0% accuracy) for defining catastrophic failure
- No comparative validation against established neuron importance ranking methods
- Absence of mechanistic explanation for the observed two-stage collapse pattern

## Confidence
- **High confidence** in the observation that minimal neuron ablation can cause complete model failure
- **Medium-High confidence** in the CAN method's effectiveness for identifying critical neurons
- **Medium confidence** in the generalizability of the two-stage collapse pattern across LVLM architectures

## Next Checks
1. Replicate the CAN method and ablation experiments across at least three additional LVLM architectures with varying parameter counts and training datasets
2. Implement ablation studies using alternative neuron importance metrics (e.g., integrated gradients, attention-based methods) to validate CAN's effectiveness
3. Conduct fine-grained performance monitoring during neuron masking to characterize the intermediate states between normal operation and complete failure