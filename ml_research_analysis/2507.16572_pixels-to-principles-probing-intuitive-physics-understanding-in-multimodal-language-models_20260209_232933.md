---
ver: rpa2
title: 'Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal
  Language Models'
arxiv_id: '2507.16572'
source_url: https://arxiv.org/abs/2507.16572
tags:
- language
- vision
- grasp
- physics
- plausible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multimodal large language models (MLLMs) on
  intuitive physics tasks using the GRASP and IntPhys 2 datasets. Despite recent advances,
  even state-of-the-art models like Qwen 2.5 VL, InternVL 2.5, LLaVA-OneVision, and
  Gemini 2.0 Flash Thinking struggle to reliably distinguish physically plausible
  from implausible scenarios, achieving at most 54% accuracy.
---

# Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models

## Quick Facts
- **arXiv ID**: 2507.16572
- **Source URL**: https://arxiv.org/abs/2507.16572
- **Reference count**: 27
- **Primary result**: State-of-the-art MLLMs achieve only 54% accuracy on intuitive physics tasks despite vision encoders capturing plausibility cues

## Executive Summary
This paper evaluates multimodal large language models (MLLMs) on intuitive physics tasks using the GRASP and IntPhys 2 datasets. Despite recent advances in MLLM capabilities, even state-of-the-art models like Qwen 2.5 VL, InternVL 2.5, LLaVA-OneVision, and Gemini 2.0 Flash Thinking struggle to reliably distinguish physically plausible from implausible scenarios, achieving at most 54% accuracy. To understand the underlying limitations, the authors probe model embeddings at key processing stages, extracting representations before and after vision-language projection layers. Their analysis reveals a critical vision-language misalignment: while vision encoders successfully capture physical plausibility cues, the language model often fails to utilize this information effectively. t-SNE visualizations show that vision embeddings naturally cluster by physical concepts, but this structure deteriorates after alignment with the language model. This suggests the primary bottleneck in MLLMs for intuitive physics is not the vision component but rather the ineffective integration of visual and linguistic information.

## Method Summary
The authors evaluate MLLMs on intuitive physics reasoning by testing them on the GRASP and IntPhys 2 datasets, which contain images depicting physically plausible and implausible scenarios. They systematically probe model embeddings at different stages of processing - specifically before and after vision-language projection layers - to understand how physical plausibility information flows through the architecture. The analysis employs t-SNE visualizations to examine how embeddings cluster according to physical concepts at each processing stage, revealing where the information is lost or distorted.

## Key Results
- State-of-the-art MLLMs achieve only 54% accuracy on intuitive physics tasks, barely above random chance
- Vision encoders successfully capture physical plausibility cues in input images
- The vision-language alignment layer causes a significant deterioration in the structure of embeddings related to physical concepts
- t-SNE visualizations show that vision embeddings naturally cluster by physical plausibility, but this clustering is lost after language model integration

## Why This Works (Mechanism)
The study demonstrates that MLLMs can extract physical plausibility information from images at the vision encoder level, but the language model fails to properly integrate and utilize this information. The vision encoders are capable of distinguishing between physically plausible and implausible scenarios, encoding this information in their embeddings. However, when these embeddings pass through the vision-language projection layers and enter the language model component, the physical reasoning information becomes diluted or misaligned. This suggests that the architecture's bottleneck is not in visual perception but in the cross-modal alignment mechanism that bridges vision and language understanding.

## Foundational Learning
**Intuitive Physics**: The innate human ability to understand basic physical principles like object permanence, gravity, and object interactions. Needed because it defines the reasoning capability being evaluated in MLLMs. Quick check: Can the model predict what happens next in a video of a falling object?

**Multimodal Embeddings**: Vector representations that capture information from both visual and textual modalities. Needed because the study probes these embeddings to understand information flow. Quick check: Do similar concepts cluster together in embedding space?

**Vision-Language Projection Layers**: Neural network components that transform vision embeddings into a space compatible with language model embeddings. Needed because these layers are identified as the bottleneck. Quick check: Does removing these layers improve or degrade physics reasoning?

**t-SNE Visualization**: Dimensionality reduction technique used to visualize high-dimensional embedding spaces. Needed because it reveals clustering patterns of physical concepts. Quick check: Do physically plausible and implausible examples form distinct clusters?

## Architecture Onboarding

**Component Map**: Image -> Vision Encoder -> Vision-Language Projection -> Language Model -> Output

**Critical Path**: The vision encoder extracts physical plausibility features, which must then be preserved through the projection layer to the language model for reasoning. The critical failure occurs at the projection-to-language interface.

**Design Tradeoffs**: The study highlights the tension between achieving general-purpose cross-modal alignment versus preserving domain-specific reasoning capabilities like physics understanding. Standard vision-language alignment techniques optimized for general tasks may inadvertently degrade specialized reasoning abilities.

**Failure Signatures**: Loss of physical concept clustering in embedding space after vision-language projection, plateaued performance at ~54% accuracy despite successful visual feature extraction, inability to leverage visual plausibility cues during language-based reasoning.

**3 First Experiments**:
1. Remove the vision-language projection layer and connect vision encoder directly to language model
2. Train a physics-specialized alignment layer using intuitive physics datasets
3. Add an intermediate reasoning module that explicitly processes physical plausibility before language model integration

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation uses static images rather than dynamic scenarios, potentially missing temporal reasoning aspects
- 54% accuracy ceiling, while above chance, shows substantial room for improvement
- Findings based on specific model architectures and datasets may not generalize to all MLLMs

## Confidence
- Vision encoders capture physical plausibility: **High**
- Language models fail to utilize visual physics information: **Medium**
- Vision-language misalignment is primary bottleneck: **Medium**

## Next Checks
1. Test the same probing methodology on additional intuitive physics datasets with varying complexity levels to assess generalizability
2. Systematically remove or modify the vision-language projection layers to quantify their exact contribution to physics reasoning performance
3. Evaluate whether models trained on video data or equipped with temporal reasoning capabilities show different patterns of vision-language alignment compared to static image models