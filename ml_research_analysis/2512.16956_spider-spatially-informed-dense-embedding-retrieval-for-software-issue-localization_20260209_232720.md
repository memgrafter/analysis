---
ver: rpa2
title: 'SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization'
arxiv_id: '2512.16956'
source_url: https://arxiv.org/abs/2512.16956
tags:
- retrieval
- spider
- recall
- code
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SpIDER, a dense embedding retrieval approach
  that incorporates graph-based spatial exploration and LLM-based reasoning to improve
  code function localization. The method first retrieves top-K functions by semantic
  similarity, then explores the local graph neighborhood around the top-C centers
  to find relevant functions that are structurally proximate but may have lower semantic
  similarity scores.
---

# SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization

## Quick Facts
- **arXiv ID**: 2512.16956
- **Source URL**: https://arxiv.org/abs/2512.16956
- **Reference count**: 40
- **Primary result**: SpIDER improves recall and accuracy by at least 13% and 14%, respectively, over standard dense retrieval methods.

## Executive Summary
This paper introduces SpIDER, a dense embedding retrieval approach that enhances code function localization by combining semantic similarity with graph-based spatial exploration and LLM-based reasoning. The method first retrieves top-K functions by semantic similarity, then explores the local graph neighborhood around the top-C centers to find relevant functions that are structurally proximate but may have lower semantic similarity scores. An LLM filters these candidates, and the final set is constructed within a fixed retrieval budget K. Experiments on a new multilingual benchmark (SpIDER-Bench) covering Python, Java, JavaScript, and TypeScript show consistent improvements across datasets and languages, with gains preserved after LLM reranking.

## Method Summary
SpIDER augments standard semantic similarity ranking with structured search along "contains" edges in the code graph. It retrieves top-K functions by semantic similarity, then performs Breadth-First Search (BFS) up to depth d from the top-C seed centers. An LLM evaluates candidate neighbors for relevance, and validated neighbors replace the bottom-ranked non-center nodes in the final top-K list. The approach maintains a fixed retrieval budget while improving recall by leveraging the spatial locality of relevant code modules.

## Key Results
- SpIDER improves Recall@20 by at least 13% over standard dense retrieval methods across all four languages
- Accuracy@20 improvements of at least 14% are consistently observed across Python, Java, JavaScript, and TypeScript datasets
- Gains are preserved after LLM reranking, demonstrating the robustness of the spatial exploration approach
- Performance improvements are consistent across different datasets including SWEBench-Verified, SWE-PolyBench, and Multi-SWEBench

## Why This Works (Mechanism)

### Mechanism 1: Graph-Based Spatial Locality
Relevant code units often reside near highly-ranked semantic matches within the code structure. SpIDER retrieves top-C seeds via semantic similarity, then performs BFS up to depth d along "contains" edges. This captures "near-miss" functions that share structural context with the issue but lack specific tokens to rank highly. The core assumption is that relevant code edits are spatially clustered in the repository graph.

### Mechanism 2: LLM-Based Precision Filtering
Large Language Models can distinguish relevant structural neighbors from irrelevant ones more effectively than semantic similarity scores alone. An LLM receives the source code of candidate neighbors and the issue description, returning a binary relevance decision. This filters out non-relevant nodes discovered during graph expansion before they pollute the context.

### Mechanism 3: Budget-Constrained Slot Swapping
Improving recall is possible without increasing the downstream context window by strategically replacing low-ranked items. Instead of appending neighbors (increasing cost), SpIDER replaces the bottom-ranked nodes in the initial top-K list with LLM-validated graph neighbors.

## Foundational Learning

### Concept: Dense Embedding Retrieval (DER)
Why needed: SpIDER builds on top of DER, not from scratch. You must understand vector similarity (cosine) to grasp how the initial candidate list is generated.
Quick check: How does SweRankEmbed generate the initial seed list before SpIDER modifies it?

### Concept: Code Graph Construction (AST/Syntax Trees)
Why needed: SpIDER relies on "contains" edges derived from parsing code (using Tree-sitter/ast). Understanding node types (functions, classes) is essential to debug traversal.
Quick check: What edge type does SpIDER traverse to find neighbors, and which tools parse the code to build this graph?

### Concept: Recall@K vs. Accuracy@K
Why needed: The paper optimizes for Recall (finding any relevant code) and Accuracy (finding all relevant code) rather than ranking order (MRR).
Quick check: Why does SpIDER focus on Recall@20 rather than MRR, and how does the "slot swapping" mechanism support this goal?

## Architecture Onboarding

### Component map:
Graph Builder -> Dense Retriever -> Seed Selector -> Explorer -> Filter -> Merger

### Critical path:
The transition from Dense Retriever (semantic) to Explorer/Filter (structural/reasoning). If the Dense Retriever misses all relevant seeds (Recall=0), SpIDER cannot explore the correct neighborhood.

### Design tradeoffs:
Cost vs. Coverage: Increasing depth d or centers C improves Recall but increases LLM token costs linearly. Precision vs. Recall: Using an LLM filter adds latency but prevents graph noise from polluting the top-K.

### Failure signatures:
1. Seed Failure: Top-C seeds contain no ground truth -> Graph expansion finds nothing relevant
2. Graph Disconnect: The relevant function is in a file/class not connected via "contains" to the seeds
3. Token Overflow: Large neighborhoods (d=6) exceed LLM context limits during filtering stage

### First 3 experiments:
1. Hyperparameter Sweep (C): Vary seed centers C in {1, 3, 5} on a subset of Python data to find the saturation point
2. Depth Ablation (d): Measure Recall@20 vs. Token usage for d in {2, 4, 6} to establish the cost/quality curve
3. End-to-End Localization: Run SpIDER retrieved context into a downstream agent to verify improved Recall@20 translates to resolved issues

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating relational edges (e.g., 'invokes', 'imports') into the neighborhood exploration phase improve retrieval performance over the current reliance on hierarchical 'contains' edges? The paper states the framework can incorporate other edge types but does not evaluate them.

### Open Question 2
How does the exclusion of non-primary language files during graph construction impact the retrieval of bugs that require cross-language edits in polyglot repositories? The method is validated on single-language splits, but real-world repositories often contain supporting code in other languages.

### Open Question 3
How robust is SpIDER when the "Graph Locality of Relevance" assumption is violated, specifically when relevant functions are structurally distant rather than spatially proximate? If the ground truth requires editing two functions in disparate parts of the repository, the exploration strategy might miss relevant nodes.

### Open Question 4
Does parameter-efficient fine-tuning (PEFT) of the dense embedding encoder on multilingual data reduce the relative gains provided by the SpIDER exploration layer? It is unclear if the graph-aware exploration primarily compensates for domain shift or provides orthogonal structural information.

## Limitations

- The spatial locality assumption (relevant functions reside near top semantic matches) is validated empirically but not formally proven
- The LLM-based filtering mechanism depends on a black-box model (Claude Sonnet 4) whose behavior is not characterized beyond reported precision
- The fixed retrieval budget K assumes semantic similarity scores are comparable across replaced and retained items, which may not hold in all cases
- Cross-language performance is reported but the dense retriever SweRankEmbed was primarily trained on Python, creating potential domain shift

## Confidence

- **High confidence**: Recall@20 improvements (≥13%) and accuracy gains (≥14%) are directly measured from experiments across all four languages
- **Medium confidence**: The effectiveness of LLM filtering depends on unquantified parameters like false-positive rate and hallucination frequency
- **Medium confidence**: The "contains" edge traversal assumption is plausible but not exhaustively validated across all repository structures

## Next Checks

1. **Ablation on Graph Connectivity**: Systematically remove "contains" edges from ground-truth relevant functions to test sensitivity to spatial locality assumption
2. **LLM Robustness Test**: Compare LLM filtering against a simpler heuristic (e.g., keyword overlap) to quantify the value added by reasoning
3. **Cross-Domain Transfer**: Evaluate SpIDER on repositories with different code structures (monorepos, microservices) to test generalization beyond the training distribution