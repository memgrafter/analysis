---
ver: rpa2
title: A quantitative analysis of semantic information in deep representations of
  text and images
arxiv_id: '2505.17101'
source_url: https://arxiv.org/abs/2505.17101
tags:
- information
- imbalance
- representations
- deepseek-v3
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study develops a quantitative framework to locate and characterize\
  \ semantic information in deep neural representations across text and image modalities.\
  \ The core method uses Information Imbalance\u2014an asymmetric, rank-based measure\
  \ of relative predictivity\u2014to compare neighborhoods between high-dimensional\
  \ representations without dimensionality reduction."
---

# A quantitative analysis of semantic information in deep representations of text and images
## Quick Facts
- arXiv ID: 2505.17101
- Source URL: https://arxiv.org/abs/2505.17101
- Reference count: 40
- Key outcome: Quantitative framework using Information Imbalance to locate semantic information in deep representations across text and image modalities, revealing architecture-dependent semantic regions and cross-modal asymmetries

## Executive Summary
This paper introduces Information Imbalance (II) as an asymmetric, rank-based measure to quantify relative semantic predictivity between deep representations without dimensionality reduction. The method analyzes neighborhoods in high-dimensional spaces to identify "semantic layers" where representations of semantically related inputs (translations, image-caption pairs) are most mutually predictive. Applied to text models (DeepSeek-V3, Llama3.1, BERT), it reveals semantic information distributed across multiple tokens and inner layers, with cross-linguistic alignment varying by model architecture. For vision transformers (DinoV2, image-gpt, CLIP), it identifies semantic regions in middle vs end layers depending on pretraining objectives, and shows cross-modal asymmetries in predictivity directions.

## Method Summary
The study uses Information Imbalance to measure how well neighborhoods in one representation space predict neighborhoods in another. For text, it processes multilingual sentence pairs, extracts hidden states from all layers, binarizes activations, concatenates last 20 tokens, and computes all-pairs Hamming distances. Information Imbalance is calculated as the average neighbor rank in the target space, with lower values indicating higher predictivity. Jackknife resampling (5 reps, 2000-5000 samples) provides uncertainty estimates. The same framework applies to images and cross-modal pairs, with token/window concatenation adjusted for each modality. Semantic layers are identified by II minima in U-shaped curves across relative depth.

## Key Results
- Text semantic regions: DeepSeek-V3 shows broad semantic region (relative depth ~0.4-0.9) with II ~0.2 for cross-linguistic predictivity vs ~0.3 for Llama3.1
- Token aggregation: Semantic information distributed across ~20 tokens with long-range correlations strongest in inner layers
- Vision semantics: image-gpt shows semantic layers in middle regions (II ~0.6 for image-image pairs) while DinoV2 concentrates semantics at end layers (II ~0.43)
- Cross-modal asymmetries: DeepSeek-V3 caption representations predict image representations with II ~0.6, while image-gpt→text predictivity is much higher (II ~0.7) than text→image (II ~0.4)

## Why This Works (Mechanism)
Information Imbalance captures directional semantic alignment by measuring how well proximity relationships in one representation space translate to proximity in another. The asymmetric nature reveals architectural differences in how models organize semantic information. Binarization via sign function provides robustness to scaling differences while preserving neighborhood structure. The jackknife resampling addresses computational constraints while maintaining statistical validity for identifying semantic regions.

## Foundational Learning
- Information Imbalance metric: Asymmetric measure of relative predictivity between representation spaces; needed to quantify semantic alignment without dimensionality reduction; quick check: verify II values between identical representations equal 0
- Binarization with sign function: Converts continuous activations to -1/+1; needed for computational efficiency and robustness; quick check: ensure balanced positive/negative activations after binarization
- Jackknife resampling: Subsample-based uncertainty estimation; needed for computational feasibility with large models; quick check: compare full-data vs jackknife results for consistency
- Relative depth scaling: Maps layer indices to [0,1] range; needed for cross-model comparison; quick check: verify min=0, max=1 across all models
- Hamming distance on binarized vectors: Fast distance metric for high-dimensional spaces; needed computational efficiency; quick check: confirm distances in [0,1] range

## Architecture Onboarding
Component map: Data preprocessing -> Model inference -> Representation extraction -> Binarization -> Distance computation -> Information Imbalance calculation -> Uncertainty estimation -> Semantic layer identification

Critical path: The computational bottleneck is all-pairs distance computation in high-dimensional spaces (up to 150K dimensions). This requires careful memory management through chunking or approximate methods like FAISS.

Design tradeoffs: Binarization sacrifices graded information for computational efficiency and robustness to scaling differences. Subsampling addresses scalability but introduces uncertainty about result stability. The asymmetric II measure reveals architectural differences but requires careful interpretation of directionality.

Failure signatures: Flat II curves (~1.0) indicate no semantic alignment; this typically results from misaligned batches, collapsed binarization, or incorrect distance metrics. Asymmetric curves require checking direction convention consistency.

First experiments: 1) Verify II=0 for identical representations; 2) Test binarization balance on sample activations; 3) Compute distances on small subset to validate OOM prevention strategy

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability limits analysis to subsampled data, potentially missing rare semantic patterns
- Binarization may discard graded activation information that could be semantically relevant
- Cross-modal asymmetries are architecture-specific and difficult to generalize across different model families

## Confidence
- High confidence: Quantitative framework and II measure are well-defined and reproducible; text semantic region identification is consistent across multiple language pairs and models
- Medium confidence: Image modality results are clear for individual models but cross-modal asymmetries depend heavily on architectural choices
- Low confidence: Absolute II values are difficult to interpret without established baselines; jackknife estimates may not capture all sampling variability

## Next Checks
1. Test robustness of semantic layer identification by repeating analysis with different random subsamples of the opus_books corpus
2. Apply alternative binarization strategies to assess sensitivity to preprocessing choices
3. Validate cross-modal asymmetries across additional model pairs to determine if they're systematic or architecture-specific