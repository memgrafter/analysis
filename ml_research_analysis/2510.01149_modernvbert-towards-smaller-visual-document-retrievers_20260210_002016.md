---
ver: rpa2
title: 'ModernVBERT: Towards Smaller Visual Document Retrievers'
arxiv_id: '2510.01149'
source_url: https://arxiv.org/abs/2510.01149
tags:
- retrieval
- image
- training
- document
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of efficient visual document\
  \ retrieval by challenging the common practice of repurposing large generative vision-language\
  \ models for retrieval tasks. Through controlled experiments, it identifies key\
  \ design choices\u2014such as the benefit of bidirectional attention masks, higher\
  \ image resolution, modality alignment objectives, and late-interaction mechanisms\u2014\
  that significantly improve retrieval performance."
---

# ModernVBERT: Towards Smaller Visual Document Retrievers

## Quick Facts
- arXiv ID: 2510.01149
- Source URL: https://arxiv.org/abs/2510.01149
- Authors: Paul Teiletche; Quentin Macé; Max Conti; Antonio Loison; Gautier Viaud; Pierre Colombo; Manuel Faysse
- Reference count: 40
- Primary result: 250M parameter vision-language encoder matches 10x larger models on visual document retrieval

## Executive Summary
This paper addresses the problem of efficient visual document retrieval by challenging the common practice of repurposing large generative vision-language models for retrieval tasks. Through controlled experiments, it identifies key design choices—such as the benefit of bidirectional attention masks, higher image resolution, modality alignment objectives, and late-interaction mechanisms—that significantly improve retrieval performance. Based on these insights, the authors introduce ModernVBERT, a compact 250M-parameter vision-language encoder that matches or exceeds the performance of models up to 10 times larger on visual document retrieval benchmarks. The model enables fast inference on inexpensive CPU hardware, greatly reducing latency and operational costs while maintaining strong accuracy.

## Method Summary
ModernVBERT employs a two-phase training approach: first, a modality alignment phase using early fusion architecture with MLM objective (10B tokens at 1024px, then 2B token cooldown at 2048px) to align vision and text embeddings; second, a contrastive post-training phase (2:1 text-to-image ratio, hard negatives) to specialize for retrieval. The architecture uses SigLIP2-base-16b-512 vision tower with pixel shuffling (r=4) to compress 1024 patches to 64 visual tokens, combined with a bidirectional ModernBERT encoder. Training employs LoRA adapters (r=32, alpha=32, dropout=0.1) and WSD scheduler (5% warmup, 20% decay, LR 1e-4). The model is evaluated on ViDoRe v1/v2 benchmarks with nDCG@5 metric.

## Key Results
- Bidirectional attention improves late interaction retrieval by +10.6 nDCG@5 over causal attention
- Higher resolution (2048px) improves document retrieval but degrades natural image tasks
- MLM-based modality alignment provides better token-level grounding than CLM
- Text-only pairs improve visual document retrieval (+1.7 nDCG@5) through cross-modal transfer
- 250M parameter ModernVBERT matches or exceeds performance of 3B+ parameter models

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Attention Unlocks Late Interaction Matching
Bidirectional attention allows all tokens to attend to all other tokens, enabling fine-grained matching in late interaction settings. Causal attention prevents tokens at sequence start from seeing future context, which is critical for multi-vector matching where each token embedding must be fully contextualized by all other tokens.

### Mechanism 2: MLM-Based Modality Alignment Strengthens Token-Level Vision-Language Binding
MLM forces the model to predict masked text tokens using both surrounding text and visual patch tokens. This creates fine-grained associations between visual regions and linguistic units, which retrieval can exploit for precise token-level cross-modal grounding.

### Mechanism 3: Higher Image Resolution Improves Document Retrieval at Cost of Natural Image Tasks
Document understanding requires fine-grained visual details (text legibility, layout precision) that higher resolution preserves. Natural image tasks benefit from global semantics where excessive resolution introduces noise and computational overhead without gain.

### Mechanism 4: Cross-Modal Transfer from Text-Only Contrastive Pairs
Interleaving text-only query-document pairs with image-document pairs during contrastive training allows text-only supervision to transfer visual retrieval capabilities. Text pairs are abundant; image-document pairs are scarce. Mixing them expands effective training signal.

## Foundational Learning

- **Concept: Late Interaction (Multi-Vector Matching)**
  - Why needed here: ModernVBERT's ColModernVBERT variant uses late interaction (MaxSim) rather than single-vector pooling. Understanding why bidirectional attention matters requires understanding late interaction mechanics.
  - Quick check question: Can you explain why matching all query tokens against all document tokens (MaxSim) differs from matching single pooled embeddings?

- **Concept: Encoder vs. Decoder Attention Patterns**
  - Why needed here: The paper's core finding is that encoder-style bidirectional attention outperforms decoder-style causal attention for retrieval. You must understand what causal vs. bidirectional masking does to token representations.
  - Quick check question: In a causal model, can token position 5 attend to token position 10? In a bidirectional model?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: ModernVBERT uses contrastive post-training after modality alignment. Understanding InfoNCE, hard negatives, and batch curation is essential to reproduce or extend this work.
  - Quick check question: What is the role of hard negatives in contrastive learning, and why does batch composition matter?

## Architecture Onboarding

- **Component map**: Vision Tower (SigLIP2-B16-512) -> Pixel Shuffling (r=4) -> 64 visual tokens -> Text Encoder (ModernBERT) -> Projection Layer -> LoRA Adapters -> Late Interaction Head

- **Critical path**:
  1. Modality Alignment (10B tokens, MLM objective, bidirectional attention): Vision and text adapters learn to share embedding space
  2. Resolution Cooldown (2B tokens at 2048px): Fine-tunes for high-resolution document images
  3. Contrastive Post-Training (document-query + text-only pairs, 2:1 ratio, hard negatives): Specializes for retrieval

- **Design tradeoffs**:
  - Bidirectional vs. Causal: Bidirectional +10.6 nDCG@5 for late interaction, but causal models may have better pretraining ecosystems
  - Resolution: 2048px improves document retrieval, hurts natural image tasks
  - Multi-vector vs. Single-vector: Late interaction is more accurate but requires more storage and compute at retrieval time
  - Text-only mixing: Improves document retrieval but may not help other tasks

- **Failure signatures**:
  - Using causal attention with late interaction: ~10 nDCG@5 loss vs. bidirectional
  - Training only on natural image-caption pairs: Poor document retrieval performance
  - Applying high-resolution regime to natural image tasks: Degraded performance
  - Skipping modality alignment cooldown: Missed ~2 nDCG@5 gain

- **First 3 experiments**:
  1. **Ablate attention masking**: Train identical models with causal vs. bidirectional attention, evaluate late interaction retrieval. Confirm +10 nDCG@5 gap.
  2. **Vary image resolution**: Train at 512px, 1024px, 2048px. Plot document retrieval vs. natural image task scores. Confirm task-specific resolution sensitivity.
  3. **Interleave text-only pairs**: Train contrastive model with and without text-only pairs (1:1 or 2:1 ratio). Measure cross-modal transfer gain on ViDoRe.

## Open Questions the Paper Calls Out

- **Question:** Do the performance advantages of bidirectional attention and late interaction persist when scaling model size beyond the 250M parameter range?
- **Question:** How does allocating parameters to multilingual capabilities trade off against visual document understanding performance?
- **Question:** Can the modality-aligned encoder be effectively fine-tuned for granular token-level tasks like visual grounding or OCR error detection?
- **Question:** Is there a performance ceiling for using text-only data to augment visual contrastive training?

## Limitations

- The controlled experiments use fixed hyperparameters and model sizes, making it difficult to determine whether attention masking effects would persist across different scales or architectures
- While ablation studies demonstrate clear differences between design choices, the underlying mechanisms remain incompletely explained—particularly why bidirectional attention yields such dramatic gains in late interaction settings
- The cross-modal transfer findings, while intriguing, rest on limited empirical validation with the optimal mixing ratios and generality across domains unexplored

## Confidence

- **High Confidence**: Bidirectional attention significantly improves late interaction retrieval (+10.6 nDCG@5)
- **Medium Confidence**: MLM-based modality alignment improves retrieval through token-level grounding
- **Medium Confidence**: Higher resolution improves document retrieval but not natural image tasks
- **Medium Confidence**: Text-only contrastive pairs improve visual document retrieval through cross-modal transfer

## Next Checks

1. **Cross-Domain Resolution Transfer**: Replicate the resolution experiments on a different visual document retrieval dataset (e.g., DocVQA or Kleister) to verify that the high-resolution advantage generalizes beyond the specific corpus used in the paper.

2. **Attention Masking Across Architectures**: Test the bidirectional vs. causal attention comparison using different backbone architectures (e.g., CLIP vs. SigLIP) and varying model sizes to determine whether the effect scales or is specific to the tested configuration.

3. **Text-Only Pair Ablation with Different Mixing Ratios**: Systematically vary the text-to-image pair ratio (1:1, 2:1, 3:1) during contrastive training to identify the optimal balance and test whether the cross-modal transfer effect persists when the text distribution shifts substantially from the document domain.