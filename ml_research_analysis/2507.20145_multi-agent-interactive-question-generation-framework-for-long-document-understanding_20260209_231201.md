---
ver: rpa2
title: Multi-Agent Interactive Question Generation Framework for Long Document Understanding
arxiv_id: '2507.20145'
source_url: https://arxiv.org/abs/2507.20145
tags:
- document
- arxiv
- questions
- question
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fully automated multi-agent framework for
  generating high-quality question-answer pairs from long-form documents, addressing
  the challenge of limited fine-grained training data for long-context document understanding,
  particularly in low-resource languages like Arabic. The system employs a chain of
  specialized agents for question generation, refinement, answer synthesis, and evidence
  validation, leveraging OCR and layout analysis to maintain contextual accuracy.
---

# Multi-Agent Interactive Question Generation Framework for Long Document Understanding

## Quick Facts
- arXiv ID: 2507.20145
- Source URL: https://arxiv.org/abs/2507.20145
- Reference count: 32
- Key outcome: Automated multi-agent framework generates challenging QA pairs for long-context document understanding, with Gemini-1.5 Pro achieving 49.8% accuracy on the newly created AraEngLongBench dataset.

## Executive Summary
This paper presents a fully automated multi-agent framework for generating high-quality question-answer pairs from long-form documents, addressing the challenge of limited fine-grained training data for long-context document understanding, particularly in low-resource languages like Arabic. The system employs a chain of specialized agents for question generation, refinement, answer synthesis, and evidence validation, leveraging OCR and layout analysis to maintain contextual accuracy. Experiments on the newly created AraEngLongBench dataset demonstrate that the generated questions are challenging for major open- and closed-source LVLMs, with closed-source models like Gemini-1.5 Pro achieving up to 49.8% accuracy while open-source models like Qwen 2.5 VL reach 44.5%.

## Method Summary
The framework uses a 5-agent pipeline: Agent 1 generates questions, Agent 2 extracts questions from images, Agent 3 generates answers, Agent 4 assesses answers and provides iterative feedback to Agent 1, and Agent 5 validates evidence alignment. Documents are preprocessed through LVLM-based OCR and YOLO layout analysis, then chunked with 10-page overlap. The system iteratively refines questions based on Agent 4's feedback until target complexity is reached, with adaptive difficulty scaling triggered when answer accuracy exceeds 40%. The framework outputs the AraEngLongBench dataset containing 6,732 QA pairs across 12 types for English and Arabic documents.

## Key Results
- Generated AraEngLongBench dataset with 6,732 challenging questions across 12 types
- Closed-source LVLMs achieve 49.8% accuracy on generated questions (Gemini-1.5 Pro)
- Open-source models score 44.5% accuracy (Qwen 2.5 VL) on same benchmark
- All models score <20% on unanswerable questions, indicating difficulty in identifying UA queries

## Why This Works (Mechanism)

### Mechanism 1
Iterative agent feedback loops improve QA quality by identifying and correcting mismatches between questions, answers, and evidence. Agent 4 aggregates context and produces reference answers with iterative feedback (F), which Agent 1 uses to refine questions until target complexity is reached. Core assumption: LVLMs can reliably assess answer quality and provide actionable feedback. Break condition: If feedback is inconsistent or Agent 1 fails to converge after N iterations, mechanism degrades to single-pass generation.

### Mechanism 2
Adaptive difficulty scaling ensures questions remain challenging by triggering complexity increases when answer accuracy exceeds 40%. Agent 4 monitors accuracy and signals Agent 1 to increase complexity when threshold is exceeded. Core assumption: 40% threshold correlates with question quality and model challenge level. Break condition: If threshold is set too low (overly difficult) or too high (trivial questions), benchmark utility degrades.

### Mechanism 3
Multi-modal grounding (OCR + layout + images) reduces hallucination by constraining answer generation to document-evidenced content. Agent 3 generates answers using chunked images and filtered questions, while Agent 5 validates consistency between answers and evidence sources. Core assumption: Layout-aware OCR and visual grounding provide sufficient signal to verify answer correctness. Break condition: If OCR fails on low-quality scans or layout analysis missegments tables/figures, evidence validation produces false negatives.

## Foundational Learning

- **Concept: Multi-Agent Orchestration (Chain-of-Agents Pattern)**
  - Why needed here: Understanding how agents pass structured outputs sequentially is prerequisite to debugging the pipeline.
  - Quick check question: Can you trace how a question flows from Agent 1 → Agent 2 → Agent 3 → Agent 4 → Agent 5 and back?

- **Concept: LVLM-based OCR + Layout Analysis**
  - Why needed here: Preprocessing converts PDFs to images, extracts text via LVLM-OCR, and segments structure via YOLO—all inputs downstream agents depend on.
  - Quick check question: What happens to QA quality if OCR misses a table caption that contains the answer evidence?

- **Concept: Long-Context Chunking with Overlap**
  - Why needed here: Documents are split into 10-page overlapping chunks to preserve cross-page dependencies; understanding this is critical for debugging multi-hop reasoning questions.
  - Quick check question: Why would zero overlap cause failures on cross-page questions?

## Architecture Onboarding

- **Component map:** PDF → images (pdf2image) → LVLM-OCR + YOLO layout → chunked segments (Ic, Oc, Lc) with 10-page overlap → Agent 1 (Q Gen) → Agent 2 (Q Extraction) → Agent 3 (Answer Gen) → Agent 4 (Assessment/Feedback) → Agent 1 (Refinement loop) → Agent 5 (Evidence Validation) → Final QA pairs

- **Critical path:** Document preprocessing (OCR/layout quality bounds all downstream quality) → Agent 1 → Agent 4 feedback loop (controls question difficulty) → Agent 5 validation (filters hallucinated answers before dataset release)

- **Design tradeoffs:** 10-page overlap improves cross-page coherence but increases processing cost; LVLM-based OCR is more accurate but slower; 40% accuracy threshold is heuristic; fully automated trades scalability for potential error propagation

- **Failure signatures:** High UA misclassification (all models <20%); Arabic performance gap (open-source models drop significantly); low answer-type diversity (Text 55.6%, Integer 23% dominate)

- **First 3 experiments:** 1) Ablate Agent 5 validation: measure answer hallucination rate with/without evidence validation; 2) Vary overlap window: test 0-page, 5-page, 10-page overlap; 3) Threshold sensitivity: run Agent 4 with 30%, 40%, 50% accuracy thresholds

## Open Questions the Paper Calls Out

1. **Question:** What is the quantitative relationship between the number of iterative agent loops and the resulting depth of document understanding?
   - Basis: [explicit] Conclusion states plans to explore depth vs. iteration count.
   - Why unresolved: Current work uses fixed chain without ablating iteration count.
   - What evidence would resolve it: Ablation study varying iteration count correlated with human evaluation scores.

2. **Question:** How can LVLMs improve confidence calibration to reliably identify and abstain from answering "unanswerable" questions in long-context settings?
   - Basis: [inferred] Table 1 shows all models scoring below 20% accuracy on Unanswerable questions.
   - Why unresolved: Paper notes models struggle with correctly labeling unanswerable questions.
   - What evidence would resolve it: Fine-tuning experiments on "Unanswerable" subset resulting in higher UA accuracy.

3. **Question:** To what extent does the multi-agent framework's processing efficiency degrade when scaling to documents exceeding current maximum context lengths?
   - Basis: [explicit] Conclusion notes plans to enhance processing efficiency.
   - Why unresolved: Computational overhead on massive datasets not quantified against simpler baselines.
   - What evidence would resolve it: Latency and resource utilization metrics for 500+ page documents.

## Limitations

- The 40% accuracy threshold for difficulty scaling is heuristic without ablation studies showing optimal thresholds.
- Arabic language performance significantly lags English, suggesting potential weaknesses in RTL script handling.
- The fully automated approach trades human oversight for scalability, creating potential for error propagation.

## Confidence

- **High confidence:** Multi-agent orchestration architecture and sequential workflow are clearly specified and implementable.
- **Medium confidence:** Core mechanisms of iterative feedback and adaptive difficulty scaling are theoretically sound but lack empirical validation.
- **Low confidence:** Claims about Arabic language performance quality and handling of rare answer types are under-supported.

## Next Checks

1. **Feedback Quality Analysis:** Instrument the Agent 4 → Agent 1 feedback loop to measure iteration counts until convergence, track question difficulty progression per iteration, and quantify correlation between feedback specificity and quality improvements.

2. **Threshold Sensitivity Study:** Systematically vary the accuracy threshold (20%, 30%, 40%, 50%, 60%) and measure resulting question difficulty distributions, model performance distributions, and overall dataset utility for long-context evaluation.

3. **Arabic Performance Root Cause Analysis:** Compare Arabic vs English OCR accuracy rates, analyze Agent 4's assessment consistency across languages, and evaluate whether RTL-specific prompt engineering or model selection improves Arabic QA pair quality.