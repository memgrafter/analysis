---
ver: rpa2
title: Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept
  Discovery
arxiv_id: '2505.20293'
source_url: https://arxiv.org/abs/2505.20293
tags:
- concept
- concepts
- comprehensibility
- explanations
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ECO-Concept, an unsupervised framework for
  discovering human-comprehensible concepts in text classification. The method combines
  a slot attention-based concept extractor with LLM-guided evaluation to iteratively
  refine concept representations.
---

# Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery

## Quick Facts
- **arXiv ID:** 2505.20293
- **Source URL:** https://arxiv.org/abs/2505.20293
- **Reference count:** 27
- **Primary result:** ECO-Concept achieves classification performance comparable to black-box models while improving concept interpretability

## Executive Summary
This paper introduces ECO-Concept, an unsupervised framework for discovering human-comprehensible concepts in text classification. The method employs a slot attention-based concept extractor that iteratively refines concept representations through LLM-guided evaluation. By combining LLM-generated summaries and highlighted segments to assess concept comprehensibility, the framework provides interpretable explanations without requiring predefined concept annotations. Experiments across seven datasets demonstrate that ECO-Concept achieves classification accuracy comparable to black-box models while significantly outperforming existing concept-based methods in both task accuracy and concept interpretability.

## Method Summary
ECO-Concept operates through an iterative refinement process that combines concept extraction with LLM-based evaluation. The framework uses slot attention mechanisms to identify and extract potential concepts from text data, then employs large language models to evaluate these concepts for comprehensibility. The LLM generates summaries and highlights relevant segments, providing feedback that is used to refine the concept representations. This unsupervised approach eliminates the need for predefined concept annotations while maintaining high classification performance. The iterative nature allows concepts to evolve toward greater interpretability while preserving their task relevance.

## Key Results
- Achieves classification performance comparable to black-box models across seven datasets
- Outperforms existing concept-based methods in both task accuracy and concept interpretability
- Human evaluations confirm that ECO-Concept's concepts are more intuitive and effective for understanding model behavior

## Why This Works (Mechanism)
ECO-Concept leverages the strengths of both slot attention mechanisms for concept extraction and LLMs for interpretability assessment. The slot attention component effectively identifies relevant patterns in text data without supervision, while the LLM-guided evaluation provides a feedback loop that ensures concepts remain comprehensible to humans. The iterative refinement process allows concepts to evolve based on LLM feedback, balancing task performance with interpretability. This combination addresses the traditional trade-off between model accuracy and explainability in text classification.

## Foundational Learning
- **Slot Attention Mechanisms**: Neural architectures that can identify and separate multiple entities or concepts within input data - needed for unsupervised concept discovery; quick check: verify concepts can be cleanly separated and tracked across iterations
- **Concept-based Interpretability**: Methods that explain model decisions through human-understandable concepts rather than raw features - needed to make model behavior transparent; quick check: ensure concepts align with domain knowledge
- **LLM-guided Evaluation**: Using language models to assess and provide feedback on concept quality - needed for automated interpretability assessment; quick check: validate LLM judgments with human annotators
- **Iterative Refinement**: Processes that progressively improve representations through repeated evaluation and adjustment - needed to balance performance and interpretability; quick check: measure convergence and stability across iterations
- **Unsupervised Learning**: Training models without labeled data - needed to eliminate annotation costs; quick check: compare unsupervised concepts with expert-defined concepts

## Architecture Onboarding

**Component Map:**
Text Data -> Slot Attention Extractor -> Concept Representations -> LLM Evaluator -> Comprehensibility Feedback -> Refined Concepts -> Classification Output

**Critical Path:**
Concept extraction → LLM evaluation → feedback integration → classification

**Design Tradeoffs:**
- Slot attention provides unsupervised concept discovery but may struggle with highly abstract concepts
- LLM-based evaluation ensures interpretability but introduces computational overhead and potential subjectivity
- Iterative refinement improves concept quality but requires multiple evaluation cycles

**Failure Signatures:**
- Concepts that are task-relevant but not human-comprehensible indicate insufficient LLM guidance
- Poor classification performance suggests inadequate concept extraction
- High computational costs may limit scalability to larger datasets

**First 3 Experiments:**
1. Baseline comparison with standard black-box classifiers on classification accuracy
2. Evaluation against existing concept-based methods on interpretability metrics
3. Human evaluation study comparing ECO-Concept concepts with baseline approaches

## Open Questions the Paper Calls Out
The paper acknowledges several areas requiring further investigation, including the scalability of LLM-guided evaluation for larger datasets, the framework's performance across different NLP tasks beyond classification, and how well the approach handles domain shifts and noisy real-world text data. The potential subjectivity introduced by LLM-based evaluation and the method's limitations with highly abstract or context-dependent concepts are also noted as open questions.

## Limitations
- Reliance on LLM-based evaluation introduces potential subjectivity and computational overhead
- Primary focus on classification tasks leaves applicability to other NLP tasks unexplored
- Limited exploration of performance across domain shifts and noisy, real-world text data

## Confidence

**Major Claims Confidence Assessment:**
- **Concept discovery effectiveness (Medium)**: Results show improved interpretability, but evaluation relies heavily on LLM-generated metrics rather than extensive human validation across all datasets
- **Performance parity with black-box models (High)**: Classification accuracy comparisons are straightforward and well-documented across multiple datasets
- **Superiority over existing concept-based methods (Medium)**: Relative performance gains are demonstrated but could benefit from more rigorous statistical analysis and ablation studies

## Next Checks

1. Conduct comprehensive human evaluation studies with domain experts to validate LLM-generated concept comprehensibility scores across all seven datasets
2. Test ECO-Concept's robustness by evaluating performance on out-of-domain datasets and datasets with varying levels of noise and linguistic complexity
3. Perform scalability analysis measuring computational costs and concept quality degradation when applying ECO-Concept to larger document collections or more complex classification tasks