---
ver: rpa2
title: 'Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based
  Adversarial Attack'
arxiv_id: '2511.13132'
source_url: https://arxiv.org/abs/2511.13132
tags:
- lighting
- attack
- navigation
- agent
- intensity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the robustness of Vision-and-Language Navigation
  (VLN) agents by proposing Indoor Lighting-based Adversarial Attack (ILA), a black-box
  framework that manipulates global illumination to disrupt navigation performance.
  Unlike previous adversarial methods that use unusual textures, ILA focuses on realistic
  indoor lighting variations that agents are likely to encounter in everyday environments.
---

# Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack

## Quick Facts
- arXiv ID: 2511.13132
- Source URL: https://arxiv.org/abs/2511.13132
- Reference count: 35
- Demonstrates VLN agents are vulnerable to realistic indoor lighting variations, with attack success rates of 52.73-100% and trajectory lengths often doubling

## Executive Summary
This paper addresses the robustness of Vision-and-Language Navigation (VLN) agents by proposing Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt navigation performance. Unlike previous adversarial methods that use unusual textures, ILA focuses on realistic indoor lighting variations that agents are likely to encounter in everyday environments. The method introduces two attack modes: Static Indoor Lighting-based Attack (SILA), which searches for globally disruptive lighting intensities, and Dynamic Indoor Lighting-based Attack (DILA), which triggers abrupt illumination changes at critical decision points.

Evaluated on two state-of-the-art VLN models (SPOC and FLaRe) across three tasks (ObjectNav, Fetch, RoomVisit) with 576 episodes total, ILA significantly degrades performance. The combined SILA+DILA approach achieves attack success rates ranging from 52.73% to 100%, with trajectory lengths often doubling compared to clean conditions. These results reveal previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations, demonstrating that even natural environmental attributes can substantially compromise navigation reliability.

## Method Summary
The Indoor Lighting-based Adversarial Attack (ILA) framework is a black-box method that manipulates global illumination to disrupt VLN agent performance. It operates through two complementary attack modes: SILA, which searches for globally disruptive lighting intensities using a timestep-weighted loss function that emphasizes errors closer to the goal, and DILA, which triggers abrupt illumination changes at critical decision points using a one-step lookahead surrogate to determine optimal switching moments. The framework is evaluated on two VLN models (SPOC and FLaRe) across three navigation tasks, demonstrating significant performance degradation through realistic lighting perturbations that agents would encounter in everyday indoor environments.

## Key Results
- Combined SILA+DILA approach achieves attack success rates of 52.73-100% across three VLN tasks
- SILA alone achieves 47.27-100% attack success rates
- Trajectory lengths often double compared to clean conditions
- Ablation study confirms both timestep-weighted loss design and strategic switching triggering are crucial for attack effectiveness

## Why This Works (Mechanism)
VLN agents rely heavily on visual observations for navigation decisions, making them susceptible to global illumination changes that alter the appearance of their environment. By manipulating indoor lighting conditions, ILA creates perturbations that are both realistic and effective at misleading the agent's perception of space and object locations. The timestep-weighted loss in SILA strategically targets errors that occur closer to the goal, where navigation decisions are most critical. DILA's abrupt illumination changes exploit the agent's inability to adapt quickly to sudden environmental shifts, particularly at decision points where the agent must choose between multiple paths.

## Foundational Learning
- Vision-and-Language Navigation (VLN): Agents that follow natural language instructions to navigate in simulated environments. Why needed: Core problem domain where lighting-based attacks demonstrate vulnerability.
- Black-box adversarial attacks: Methods that generate adversarial examples without access to the target model's internal parameters. Why needed: ILA's attack methodology operates without requiring model gradients or architecture details.
- Global illumination manipulation: Systematic control of indoor lighting conditions as an attack vector. Why needed: Novel attack surface that differs from traditional texture or pixel-level perturbations.
- Timestep-weighted loss functions: Loss calculations that assign different importance weights to errors at different timesteps. Why needed: SILA's design emphasizes goal-proximal errors for more effective attacks.
- One-step lookahead surrogate: Predictive method to determine optimal attack timing points. Why needed: DILA's mechanism for identifying critical switching moments during navigation.

## Architecture Onboarding

**Component map:** Environment simulator -> ILA framework -> VLN agent (SPOC/FLaRe) -> Navigation performance metrics

**Critical path:** 
1. Initialize VLN agent and environment
2. ILA applies lighting perturbations (SILA or DILA)
3. Agent receives modified visual observations
4. Agent executes navigation actions
5. Performance metrics capture degradation

**Design tradeoffs:** 
- SILA prioritizes global lighting consistency vs DILA's strategic timing
- Realistic lighting variations vs more extreme, less plausible perturbations
- Black-box approach vs white-box methods requiring model access

**Failure signatures:** 
- Increased trajectory lengths
- Higher failure rates on navigation tasks
- Deviation from optimal paths
- Confusion at decision points

**First experiments:**
1. Test SILA on a single navigation episode to verify basic functionality
2. Compare DILA switching timing with random timing to validate effectiveness
3. Run combined SILA+DILA attack on one task to assess synergistic effects

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to all VLN architectures beyond SPOC and FLaRe
- Realistic lighting variations claim lacks extensive validation against human perception thresholds
- Black-box nature limits ability to optimize attacks for specific model architectures
- Does not extensively test defensive strategies that could mitigate lighting-based attacks

## Confidence
- **High confidence**: Core attack methodology and its effectiveness against tested VLN models
- **Medium confidence**: Relative importance of design choices from ablation study
- **Medium confidence**: Realism claim for indoor lighting variations without human perception validation

## Next Checks
1. Test ILA against additional VLN architectures beyond SPOC and FLaRe to assess generalizability
2. Evaluate human performance under identical lighting perturbations to establish realism thresholds
3. Implement and measure defensive strategies that could mitigate lighting-based attacks