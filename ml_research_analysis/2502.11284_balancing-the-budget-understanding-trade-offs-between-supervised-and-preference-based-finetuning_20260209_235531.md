---
ver: rpa2
title: 'Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based
  Finetuning'
arxiv_id: '2502.11284'
source_url: https://arxiv.org/abs/2502.11284
tags:
- data
- preference
- arxiv
- finetuning
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the optimal allocation of training data
  between Supervised Fine-Tuning (SFT) and Preference Finetuning (PFT) for post-training
  LLMs under a fixed data annotation budget. The authors conduct extensive experiments
  across four tasks (summarization, helpfulness, instruction following, grade school
  math), multiple model sizes, and various data allocation ratios.
---

# Balancing the Budget: Understanding Trade-offs Between Supervised and Preference-Based Finetuning

## Quick Facts
- arXiv ID: 2502.11284
- Source URL: https://arxiv.org/abs/2502.11284
- Authors: Mohit Raghavendra; Junmo Kang; Alan Ritter
- Reference count: 40
- Primary result: Hybrid SFT-to-PFT allocation (approx. 10% SFT, 90% PFT) optimizes fixed data budgets by leveraging SFT for style acquisition and PFT for error correction

## Executive Summary
This paper investigates optimal data allocation between Supervised Fine-Tuning (SFT) and Preference Fine-Tuning (PFT) for post-training LLMs under fixed annotation budgets. Through extensive experiments across four tasks and multiple model sizes, the authors demonstrate that SFT dominates in low-data regimes (<1,000 examples) while hybrid approaches with higher PFT proportions achieve optimal performance at larger scales. They identify a "cold start problem" where direct PFT on base models fails to elicit complex reasoning styles, particularly for analytical tasks like mathematics. The solution involves allocating a small portion (<10%) of the budget to SFT before applying PFT, which significantly improves performance by aligning the model's response style to the expected format.

## Method Summary
The paper conducts experiments across four tasks (summarization, helpfulness, instruction following, grade school math) using Llama 3.1 8B and Qwen 2.5 7B models. They test various data allocation ratios (1.0, 0.75, 0.5, 0.25, 0.0 SFT fraction) with budgets ranging from 100 to 20,000 examples. SFT is trained for 2 epochs with LR 5e-5, LoRA rank 32, alpha 32, followed by DPO training for 2 epochs with LR 5e-6, Î² 0.1. The paper uses synthetic annotation for preference data generation and evaluates using win rates versus reference models and accuracy metrics on specific benchmarks.

## Key Results
- SFT is superior to PFT in low-data regimes (<1,000 examples) across all tested tasks
- Hybrid allocation (approx. 10% SFT, 90% PFT) yields 15-20% performance improvements on analytical benchmarks like GSM8k
- Direct PFT on base models fails to elicit step-by-step reasoning for mathematical problems due to the "cold start" problem
- Under most cost structures, spending more budget on preference data after initial SFT proves most cost-effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct Preference Optimization (DPO) on a base model fails to elicit complex reasoning styles due to distribution shift from the base model's default behavior.
- Mechanism: DPO penalizes deviation from the reference model. If the base model defaults to short answers while the target requires step-by-step reasoning, the KL constraint in DPO fights the emergence of the new style. Minimal SFT aligns the model to the required format first, acting as a compatible reference point for subsequent preference optimization.
- Core assumption: The base model possesses the latent capability to reason but lacks the stylistic initialization to satisfy the preference optimization constraints without excessive penalty.
- Evidence anchors:
  - [abstract]: Mentions the "cold start problem" is caused by "distribution shift arising from using DPO directly on the base model."
  - [section]: Section 3.2 states that SFT "quickly aligns the model to the expected response style" and acts as a "compatible reference model."
  - [corpus]: "Metis-SPECS" confirms the standard paradigm of using SFT to "initialize the policy before RL."
- Break condition: This mechanism may not hold if the base model has already been heavily instruction-tuned for reasoning, or if the target task requires stylistic matching rather than logical reasoning.

### Mechanism 2
- Claim: SFT provides a superior data-to-performance ratio in low-data regimes because it offers dense, direct supervision compared to the relative signal of preference data.
- Mechanism: SFT uses cross-entropy loss to force the model to mimic correct responses exactly. In contrast, PFT relies on comparative judgments (chosen vs. rejected). With limited data (<1,000 examples), the direct signal of SFT is more efficient for establishing a new capability than the comparative signal of PFT.
- Core assumption: High-quality supervised examples are available and representative of the target distribution.
- Evidence anchors:
  - [abstract]: States "SFT is superior in low-data regimes (<1,000 examples)."
  - [section]: Section 3.1 notes that in low-data scenarios, "pure SFT demonstrates superior performance compared to other mixed allocation approaches."
  - [corpus]: "Sample-efficient LLM Optimization" discusses the necessity of improving sample efficiency in RL/PFT, indirectly supporting the difficulty of PFT in low-data settings.
- Break condition: The efficiency advantage diminishes as the data budget scales (specifically >10,000 examples), where PFT contributes more significantly to refinement.

### Mechanism 3
- Claim: A hybrid allocation strategy (approx. 10% SFT, 90% PFT) optimizes fixed budgets by leveraging SFT for style acquisition and PFT for error correction and reasoning refinement.
- Mechanism: The paper empirically finds that once a small "warm-start" SFT budget establishes the response format (e.g., step-by-step math), a larger volume of preference data is more effective at refining the logical correctness than continuing to scale SFT data.
- Core assumption: The marginal utility of SFT data diminishes faster than that of PFT data once the basic response style is learned.
- Evidence anchors:
  - [abstract]: Reports that allocating "<10% of the budget to SFT" improves performance by "15-20% on analytical benchmarks."
  - [section]: Section 3.3 suggests that under most cost structures, it is beneficial to "spend more data budgets on Preference data after some initial SFT."
  - [corpus]: Weak direct support in neighbors; "Compositional Bias Control" suggests supervision succeeds where preference learning fails for specific controls, aligning with the need for the initial SFT constraint.
- Break condition: This specific ratio is sensitive to the relative annotation costs; if SFT annotation becomes prohibitively expensive compared to PFT, the optimal allocation shifts.

## Foundational Learning

- Concept: **DPO (Direct Preference Optimization)**
  - Why needed here: This is the core algorithm used for Preference Fine-Tuning (PFT). Understanding that it optimizes a policy relative to a *reference model* is essential to grasp why the "cold start" (using the base model as reference) breaks the process.
  - Quick check question: In DPO, what role does the reference model play in preventing the policy from drifting too far?

- Concept: **Distribution Shift**
  - Why needed here: The paper identifies distribution shift as the root cause of the cold start problem. The model must bridge the gap between the base model's default behavior (e.g., short answers) and the target behavior (e.g., step-by-step reasoning).
  - Quick check question: Why would a large divergence between the base model's output and the target format cause instability during preference optimization?

- Concept: **Budget Constraints & Marginal Utility**
  - Why needed here: The paper frames the problem as an economic trade-off. One must understand that different data types (SFT vs. PFT) have different "costs" (annotation effort) and different curves of diminishing returns.
  - Quick check question: According to the paper, does increasing the SFT budget from 100 to 1,000 yield the same performance jump as increasing it from 10,000 to 20,000?

## Architecture Onboarding

- Component map: Base Model -> SFT Adapter/Checkpoint -> Reference Model -> Policy Model
- Critical path:
  1. Initialize from Base Model
  2. Train SFT on a small subset (e.g., 100-1000 examples) to establish format
  3. Freeze a copy of this SFT model to serve as the *Reference Model*
  4. Train PFT (DPO) on the remaining budget using the SFT model as initialization
- Design tradeoffs:
  - **Annotation Costs:** The paper estimates SFT costs roughly 2x PFT costs (for synthetic generation). This impacts the optimal ratio calculation.
  - **Task Type:** Analytical tasks (Math) are far more sensitive to the "cold start" issue than stylistic tasks (Summarization).
  - **Model Size:** Smaller models (<3B) struggle more with direct PFT on base models than larger models (8B).
- Failure signatures:
  - **Cold Start Math Failure:** The model outputs rambling, repetitive text or fails to follow the step-by-step format, often resulting in very long, incoherent reasoning chains.
  - **Low-Data PFT Stagnation:** Performance plateaus immediately or degrades when applying PFT with <1,000 examples on a base model.
- First 3 experiments:
  1. **Establish Baseline:** Train a pure SFT model on the full budget (100% SFT) to set the upper bound for low-data and lower bound for high-data scenarios.
  2. **Cold Start Probe:** Attempt to train PFT directly on the base model using 100% of the budget for preference data. Verify the "rambling" failure mode on math tasks.
  3. **Hybrid Validation:** Retrain using the paper's recommended split (e.g., 10% SFT, 90% PFT). Compare against the baseline to confirm the 15-20% improvement claim on the target benchmark (e.g., GSM8k).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal SFT-PFT budget allocation change for models larger than 10 billion parameters where compute costs are significant?
- Basis in paper: [explicit] The authors limit the scope to models under 10B parameters to ignore compute costs, explicitly leaving the study of larger models and the "compute-data annotation cost trade-off" for future work (Sec 2.1, Sec 6).
- Why unresolved: The current findings assume data annotation is the sole bottleneck; this assumption fails for large models where training (compute) costs may rival or exceed data costs, potentially shifting the Pareto-optimal frontier.
- Evidence: Replicating the budget allocation experiments on 70B+ models while explicitly accounting for GPU hours/FLOPs in the "budget" constraint.

### Open Question 2
- Question: Do the trade-offs between SFT and PFT persist when using online RL algorithms (e.g., PPO, GRPO) instead of offline preference optimization?
- Basis in paper: [explicit] The paper restricts analysis to offline methods (DPO/KTO) and states, "we hope that this motivates future work to study more complex RL-based methods and their interplay with SFT" (Sec 6).
- Why unresolved: Online methods involve iterative data generation and reward modeling, which fundamentally changes the "data budget" definition and may alter the efficiency of the cold-start mitigation strategies observed with offline data.
- Evidence: A comparative analysis of SFT-to-RL ratios using online algorithms under fixed token/compute budgets.

### Open Question 3
- Question: What are the mechanistic causes of the "cold-start" problem in reasoning tasks that prevent PFT from teaching format adherence?
- Basis in paper: [explicit] Section 3.2 asks "Why is SFT a necessary precursor?" and identifies the failure of PFT to elicit step-by-step reasoning on base models, noting the phenomenon requires further study.
- Why unresolved: While the paper identifies distribution shift as a factor, it remains unclear why the preference gradient signal is insufficient to teach the model the desired reasoning format without an SFT "priming" step.
- Evidence: An analysis of gradient updates and attention patterns during PFT-only training on math tasks to identify where the learning signal fails to reinforce step-by-step structures.

## Limitations
- The synthetic annotation pipeline used to generate preference data may not accurately reflect human preferences
- The "cold start" phenomenon is primarily demonstrated on mathematical reasoning tasks and may not generalize to all analytical domains
- Cost estimates for SFT (2x PFT) are reasonable but may vary significantly across annotation providers and task types

## Confidence
**High Confidence Claims:**
- SFT outperforms PFT in low-data regimes (<1,000 examples) across all tested tasks
- Hybrid allocation (small SFT portion followed by PFT) consistently outperforms pure PFT on analytical tasks
- Pure SFT remains competitive with hybrid approaches in low-data settings

**Medium Confidence Claims:**
- The specific 10% SFT / 90% PFT ratio is optimal for GSM8k and similar math problems
- The cost-effectiveness analysis accurately reflects real-world annotation economics
- The cold start problem manifests similarly across different model architectures

**Low Confidence Claims:**
- The cold start mechanism applies universally to all analytical tasks beyond math
- The optimal allocation ratios remain stable across model sizes beyond those tested
- Synthetic preference data quality matches human-annotated preferences in all dimensions

## Next Checks
1. **Generalization Test:** Replicate the cold start experiments on code generation and logical reasoning tasks to verify the mechanism extends beyond mathematics. Use human-annotated preference data to eliminate synthetic bias.

2. **Cost Sensitivity Analysis:** Conduct experiments varying the SFT/PFT cost ratio across a wider range (1.5x to 3x) to test the robustness of the optimal allocation recommendations under different economic conditions.

3. **Model Architecture Transfer:** Apply the hybrid training strategy to architectures fundamentally different from Llama (e.g., decoder-only vs. encoder-decoder, or models with different positional encoding schemes) to assess whether the cold start phenomenon is architecture-dependent.