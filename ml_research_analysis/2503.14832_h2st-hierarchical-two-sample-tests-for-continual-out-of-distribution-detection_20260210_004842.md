---
ver: rpa2
title: 'H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection'
arxiv_id: '2503.14832'
source_url: https://arxiv.org/abs/2503.14832
tags:
- detection
- samples
- performance
- h2st
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Two-Sample Tests (H2ST) for
  continual out-of-distribution (OOD) detection in task incremental learning (TIL)
  under open-world conditions. Unlike existing methods that rely on model outputs
  and require threshold selection, H2ST uses hierarchical feature-level two-sample
  tests with task-specific classifiers to detect OOD samples while identifying task
  IDs.
---

# H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection

## Quick Facts
- **arXiv ID:** 2503.14832
- **Source URL:** https://arxiv.org/abs/2503.14832
- **Reference count:** 40
- **Primary result:** Hierarchical Two-Sample Tests (H2ST) achieves up to 92.03% F1 score and 93.78% task ID accuracy on CIFAR-10 for continual OOD detection in task incremental learning.

## Executive Summary
This paper introduces H2ST, a threshold-free continual OOD detection method for task incremental learning under open-world conditions. Unlike existing approaches that rely on model outputs and require threshold selection, H2ST uses hierarchical feature-level two-sample tests with task-specific classifiers to detect OOD samples while identifying task IDs. The method eliminates threshold dependency through statistical hypothesis testing and reduces computational overhead via early-exit mechanisms. Experiments on multiple datasets demonstrate H2ST's superiority over existing methods, achieving significantly better performance with lower computational cost compared to non-hierarchical approaches.

## Method Summary
H2ST is a hierarchical continual OOD detection framework that integrates with replay-based TIL methods. It employs T task-specific binary classifiers that distinguish between samples from each task's memory buffer (source) and incoming samples (target). Detection proceeds sequentially through the hierarchy - if a sample is identified as ID for task j, the process terminates immediately. OOD detection is performed through statistical hypothesis testing using Clopper-Pearson confidence intervals on classifier accuracy, eliminating the need for threshold selection. The method operates on feature maps extracted from the TIL backbone rather than model outputs, providing robustness to catastrophic forgetting. Source-target classifiers are updated online using labeled pairs from buffer samples and incoming data.

## Key Results
- Achieves up to 92.03% F1 score and 93.78% task ID accuracy on CIFAR-10
- Processes each input in 16.7 ms, compared to C2ST's average of 22.0 ms
- Outperforms baselines across MNIST, CIFAR-10, CIFAR-100, Mini-ImageNet, CoRe50, and Stream-51 datasets
- Reduces computational overhead by early-exit mechanisms, performing only (T+1)/2 detection operations on average

## Why This Works (Mechanism)

### Mechanism 1: Threshold-free OOD detection via statistical hypothesis testing
H2ST frames detection as a statistical hypothesis test using binary classifiers. If classifier accuracy is statistically indistinguishable from random guessing (0.5) within a Clopper-Pearson confidence interval, the null hypothesis (source and target from same distribution) is retained. This eliminates heuristic threshold selection. Break condition: If feature distributions drift significantly within ID tasks, classifiers may erroneously reject the null hypothesis, classifying ID as OOD.

### Mechanism 2: Computational efficiency and boundary simplification via hierarchical early-exit
The system arranges T task-specific tests sequentially, terminating immediately when a sample is identified as ID for any task. This reduces average detection operations from T to (T+1)/2. Deeper layers distinguish between specific ID tasks and unresolved samples, progressively simplifying decision boundaries. Break condition: Poor task ordering or heavy imbalance can degrade early-exit benefits, approaching flat C2ST computational cost.

### Mechanism 3: Robustness to model performance decay via feature-level analysis
H2ST operates on intermediate feature maps rather than final model outputs, maintaining discriminative power even when catastrophic forgetting degrades the final classifier. The replay buffer provides sufficient feature coverage for effective source-target classification regardless of head performance. Break condition: Severe catastrophic forgetting that corrupts the feature extractor itself will render feature maps uninformative, causing classifier failure.

## Foundational Learning

- **Concept: Classifier Two-Sample Tests (C2ST)**
  - Why needed: H2ST is fundamentally a C2ST approach. Training a binary classifier to distinguish Sample Set A from Sample Set B tests for distribution equality (P_A = P_B). If classifier fails (accuracy ≈ 50%), distributions are likely the same.
  - Quick check: If a binary classifier trained to distinguish "Memory Buffer" from "New Input" achieves 50% accuracy, what does that statistically imply about the new input?

- **Concept: Clopper-Pearson Interval**
  - Why needed: This provides the mathematical engine that removes the "threshold." It gives a confidence interval for a binomial proportion (accuracy), shrinking/expanding with sample size (w) and confidence level (α).
  - Quick check: How does increasing the window size w affect the width of the confidence interval and the sensitivity of the detection?

- **Concept: Replay-based Continual Learning**
  - Why needed: H2ST is not standalone; it relies entirely on replay buffer existence from underlying TIL method to provide "Source" samples for two-sample tests.
  - Quick check: Can H2ST function in an architecture-based CL method without a memory buffer? (Hint: No, where would source samples come from?)

## Architecture Onboarding

- **Component map:** TIL Backbone (ResNet/MLP) -> Feature Maps (ψ(x)) -> Task-specific Classifiers (g₁...g_T) -> Memory Buffers (B₁...B_T) -> H2ST Loop

- **Critical path:**
  1. Receive target sample x'
  2. Extract feature ψ(x')
  3. Enter loop j: Draw x_j from Buffer B_j, extract ψ(x_j)
  4. Pass both features through g_j to get predictions
  5. Update g_j with labeled pairs (Source=0, Target=1)
  6. Calculate empirical accuracy μ̂ over window w, check if 0.5 ∈ ConfidenceInterval
  7. Decision: If True (ID), return Task-ID j and Exit; else continue to j+1

- **Design tradeoffs:**
  - Memory Size vs. Accuracy: Small buffers cause high variance, degrading detection (poor performance at Size=40). Large buffers provide diminishing returns.
  - Classifier Complexity: Simple MLPs adapt faster to online distribution shifts than deep CNNs/MLPs.

- **Failure signatures:**
  - "All OOD" behavior: If model collapses or buffer corrupted, classifiers learn to always predict "Target", rejecting everything
  - "All ID" behavior: If significance level α too strict or window w too large, test loses power, accepting all inputs as ID

- **First 3 experiments:**
  1. Hyperparameter Sensitivity (w and α): Sweep window_size and significance_level on CIFAR-10 to find stability region
  2. Buffer Ablation: Test H2ST with Memory Sizes {40, 100, 200, 300} on CIFAR-100 to reproduce performance cliff
  3. C2ST vs. H2ST Latency: Measure inference time per sample as tasks T scale from 2 to 10 to verify (T+1)/2 speedup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-off between high OOD detection performance (which increases training data volume) and resulting exacerbation of catastrophic forgetting be effectively mitigated?
- Basis: Section 5.2 states this tension highlights a critical trade-off but isn't the focus of the paper
- Why unresolved: Better detection leads to more training data for new tasks, inherently increasing forgetting (negative FT), but no balancing mechanism proposed
- What evidence would resolve it: Modified H2ST framework that dynamically filters or weights detected OOD samples to regulate trade-off between accuracy (ACC) and forgetting (FT)

### Open Question 2
- Question: Can H2ST framework be adapted for regularization-based or architecture-based TIL methods that don't maintain explicit memory buffers?
- Basis: Algorithm 1 explicitly requires drawing source samples from memory buffers; abstract claims "seamless integration with replay-based TIL"
- Why unresolved: Method relies on comparing test samples to stored exemplars; without memory buffer, two-sample test cannot construct "source" distribution
- What evidence would resolve it: Adaptation using generative replay or feature statistics instead of raw buffer samples, showing comparable performance on non-replay TIL baselines

### Open Question 3
- Question: How sensitive is the calibrated detection mechanism to selection of window size w and significance level α when facing rapid, non-stationary distribution shifts?
- Basis: Method defines detector based on hyperparameters w and α, but experiments don't extensively analyze robustness to variations in these statistical parameters
- Why unresolved: Statistical tests can be sensitive to window sizes (too small yields high variance; too large yields lag). Paper optimizes memory size but leaves statistical parameter robustness unverified
- What evidence would resolve it: Ablation study varying w and α on datasets with high-velocity distribution shifts, measuring stability of F1 score

## Limitations

- Memory buffer size dependency creates critical bottleneck - performance degrades sharply below 100 samples per task
- Method assumes ID and OOD samples can be distinguished through feature-level two-sample tests, which may fail under severe covariate shift
- Hierarchical architecture's computational advantage depends heavily on proper task ordering, though optimal sequencing strategies aren't specified

## Confidence

- **High confidence:** Core mechanism of threshold-free detection via Clopper-Pearson confidence intervals - well-established statistical foundation directly supported by experimental results
- **Medium confidence:** Computational efficiency gains from hierarchical early-exit - latency measurements support this, but actual speedup depends on task distribution characteristics not fully explored
- **Medium confidence:** Feature-level robustness to model performance decay - assumption that feature extractors maintain discriminative power needs more rigorous validation across diverse catastrophic forgetting scenarios

## Next Checks

1. **Memory buffer sensitivity sweep:** Systematically test H2ST across buffer sizes {20, 40, 60, 80, 100, 150, 200} on CIFAR-100 to quantify performance cliff and identify minimum viable memory requirements

2. **Task ordering impact study:** Experiment with different task orderings (easy-to-hard, hard-to-easy, random) on Stream-51 to measure how hierarchical early-exit efficiency varies with distribution characteristics and task similarity

3. **Catastrophic forgetting stress test:** Induce severe catastrophic forgetting in TIL backbone (e.g., by reducing replay buffer further or increasing task complexity) and measure H2ST's degradation to validate feature-level robustness assumption under worst-case model degradation