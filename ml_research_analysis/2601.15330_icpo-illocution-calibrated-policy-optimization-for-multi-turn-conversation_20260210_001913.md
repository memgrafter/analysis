---
ver: rpa2
title: 'ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation'
arxiv_id: '2601.15330'
source_url: https://arxiv.org/abs/2601.15330
tags:
- icpo
- multi-turn
- rlvr
- standard
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Illocution-Calibrated Policy Optimization
  (ICPO), a novel training framework designed to address the "lost-in-conversation"
  problem in large language models during multi-turn conversations. ICPO augments
  training with underspecified prompts and conditions rewards on the user's illocutionary
  intent, encouraging the model to seek clarification when faced with ambiguity rather
  than providing overconfident, incorrect responses.
---

# ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation

## Quick Facts
- arXiv ID: 2601.15330
- Source URL: https://arxiv.org/abs/2601.15330
- Reference count: 0
- Introduces novel training framework for handling conversational ambiguity

## Executive Summary
This paper addresses the "lost-in-conversation" problem in large language models by introducing Illocution-Calibrated Policy Optimization (ICPO). ICPO is a training framework designed to improve how models handle underspecified prompts during multi-turn conversations. The approach conditions rewards on the user's illocutionary intent and encourages models to seek clarification when faced with ambiguity rather than providing overconfident incorrect responses.

The method demonstrates significant improvements in conversational performance while maintaining strong results on single-turn benchmarks. By keeping policy entropy elevated during training, ICPO avoids the mode collapse seen in standard reinforcement learning approaches and maintains higher response diversity.

## Method Summary
ICPO augments standard language model training with underspecified prompts designed to simulate real-world conversational ambiguity. The framework conditions reward signals on the user's illocutionary intent - the underlying communicative purpose behind their statements. When the model encounters ambiguous input, it is rewarded for seeking clarification rather than attempting to provide a definitive answer. The training process maintains higher policy entropy to encourage diverse responses and prevent the model from collapsing into overly confident but potentially incorrect behaviors. This approach is specifically calibrated to address multi-turn conversation challenges while preserving performance on simpler, single-turn tasks.

## Key Results
- 75% improvement in multi-turn conversation performance compared to baseline approaches
- Preservation of strong performance on single-turn benchmarks while improving multi-turn capabilities
- Maintenance of higher response diversity and appropriate humility in responses

## Why This Works (Mechanism)
ICPO works by fundamentally changing how language models are rewarded during training for handling ambiguous inputs. Traditional approaches train models to provide definitive answers regardless of input clarity, leading to overconfident incorrect responses. ICPO instead rewards models for recognizing when clarification is needed and responding appropriately. By conditioning rewards on illocutionary intent and maintaining elevated policy entropy, the model learns to balance between providing useful information and acknowledging uncertainty. This calibration prevents the model from defaulting to either extreme - complete silence or unwarranted confidence.

## Foundational Learning

**Illocutionary Intent**
- Why needed: Understanding the communicative purpose behind user statements is essential for determining appropriate responses
- Quick check: Can the model distinguish between a request for information versus a request for action?

**Policy Entropy**
- Why needed: Maintaining diversity in responses prevents the model from collapsing into repetitive, overconfident patterns
- Quick check: Does the model generate varied responses to the same ambiguous prompt across multiple attempts?

**Underspecification**
- Why needed: Real-world conversations frequently contain ambiguous or incomplete information that requires clarification
- Quick check: Can the model identify when a prompt lacks sufficient information for a definitive response?

## Architecture Onboarding

**Component Map:**
Preprocessing -> Underspecified Prompt Generation -> Policy Optimization -> Reward Conditioning -> Response Generation

**Critical Path:**
Underspecified prompt generation → Policy optimization with entropy maintenance → Reward conditioning on illocutionary intent → Response generation

**Design Tradeoffs:**
- Specificity vs. flexibility: Balancing between providing useful information and seeking appropriate clarification
- Computational overhead: Additional processing for illocutionary intent analysis
- Training stability: Maintaining appropriate entropy levels without destabilizing learning

**Failure Signatures:**
- Overconfidence in responses despite ambiguity
- Excessive clarification requests that hinder conversation flow
- Failure to maintain response diversity leading to repetitive patterns

**First 3 Experiments to Run:**
1. Test baseline performance on synthetic underspecified prompts
2. Evaluate response diversity across multiple runs with identical ambiguous inputs
3. Measure improvement in multi-turn conversation length and coherence

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation primarily on synthetic underspecified prompts may not fully capture real-world conversational complexity
- Potential challenges when applying to non-English languages and specialized domains not addressed
- Long-term impact on conversation flow and user experience during extended interactions remains unclear

## Confidence

**Major Claims Confidence Assessment:**
- 75% improvement in multi-turn conversation performance: High confidence - supported by comparative benchmark results
- Preservation of single-turn task performance: High confidence - demonstrated through controlled experiments
- Prevention of mode collapse through entropy maintenance: Medium confidence - theoretical rationale provided, but empirical validation could be more extensive
- Effective handling of real-world underspecification: Low confidence - primarily validated on synthetic prompts

## Next Checks

1. Evaluate ICPO on real user conversation logs to assess performance on naturally occurring ambiguity versus synthetic prompts
2. Conduct longitudinal studies measuring conversation quality and user satisfaction over extended multi-turn interactions
3. Test the approach across multiple languages and specialized domains (legal, medical, technical) to assess generalizability beyond the initial evaluation scope