---
ver: rpa2
title: 'CafGa: Customizing Feature Attributions to Explain Language Models'
arxiv_id: '2509.20901'
source_url: https://arxiv.org/abs/2509.20901
tags:
- explanations
- cafga
- users
- user
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CafGa, an interactive system for generating
  and evaluating feature attribution explanations at customizable granularities for
  language models. CafGa allows users to define prediction tasks and customize text
  segmentation interactively, supporting default options like word, sentence, or paragraph-level
  divisions while enabling further refinement through user interactions.
---

# CafGa: Customizing Feature Attributions to Explain Language Models

## Quick Facts
- arXiv ID: 2509.20901
- Source URL: https://arxiv.org/abs/2509.20901
- Authors: Alan Boyle; Furui Cheng; Vilém Zouhar; Mennatallah El-Assady
- Reference count: 13
- Result: Human-guided, customizable feature attributions in CafGa were preferred over automated baselines in 64% of comparisons, improving interpretability for long-form text and complex reasoning tasks.

## Executive Summary
CafGa is an interactive system for generating and evaluating feature attribution explanations for language models at customizable granularities. The system allows users to define prediction tasks, segment text interactively, and visualize explanations with heatmaps alongside deletion and insertion curves for fidelity evaluation. A two-stage user study demonstrated that CafGa is easy to use for experienced users and helps understand LLM behavior. Human-generated explanations using CafGa were preferred over automated baselines in most comparisons, addressing limitations of traditional word-level attributions by enabling semantic segmentation and interactive refinement.

## Method Summary
CafGa implements an interactive pipeline where users define prediction tasks with templates and input text, then segment the text into feature groups (words, sentences, paragraphs, or custom via brushing). The system applies KernelSHAP by perturbing the input through removal of feature groups, querying the LLM 10 times per perturbation, converting responses to Booleans via evaluators, and computing probabilities. Weighted linear regression estimates Shapley values, which are visualized with color-blind-friendly heatmaps. Fidelity is evaluated through deletion and insertion curves plotting Area Over the Perturbation Curve (AOPC), with x-axis normalized by percentage of words. The system supports a maximum of 2 × n_features + 2048 samples, with interactive mode using n_samples = t_max × r_API.

## Key Results
- Human-generated explanations using CafGa were preferred over automated baselines (PartitionSHAP and MExGen) in 64% of comparisons.
- Custom segmentation improved fidelity scores, with one example showing increase from 0.77 to 0.96.
- The system was rated as easy to use by experienced participants and helpful for understanding LLM behavior in the user study.

## Why This Works (Mechanism)

### Mechanism 1: Interactive Semantic Grouping
Allowing users to define feature groups rather than defaulting to word-level tokens preserves semantic meaning that atomic token analysis destroys. The system treats user-defined text spans as single features and applies KernelSHAP to these grouped units, calculating the Shapley value for the entire segment. This assumes users possess domain knowledge about semantic boundaries that aligns better with the model's internal processing than arbitrary tokenization.

### Mechanism 2: Fidelity-Driven Feedback Loops
Visualizing deletion and insertion curves mitigates confirmation bias by showing objective consequences of segmentation choices. The system performs causal intervention by iteratively removing or adding feature groups based on attribution rank, plotting the decay or recovery of prediction probability and calculating AOPC. This assumes that if an explanation is faithful, removing "important" features should cause rapid decline in prediction confidence.

### Mechanism 3: Probabilistic Aggregation of Model Output
Using repeated sampling (10 responses) to convert text generation into boolean probability smooths out stochasticity inherent in LLMs, providing stable targets for attribution. The system queries the model multiple times, aggregates results into probability, and runs regression against this stable target. This assumes the LLM's behavior can be approximated as a binary classifier for explanation purposes.

## Foundational Learning

- **Shapley Values & KernelSHAP**
  - Why needed here: CafGa relies on KernelSHAP to assign importance scores. Understanding that Shapley values rely on averaging marginal contributions across all possible feature coalitions is necessary to interpret why a specific phrase received a high score.
  - Quick check question: If you remove a feature group and the prediction probability drops significantly, does KernelSHAP assign that group a positive or negative attribution score?

- **Perturbation-based Fidelity (Deletion/Insertion)**
  - Why needed here: Users must interpret the "fidelity" score to validate their explanations. You need to understand that "Deletion" measures how quickly the model fails when important features are removed.
  - Quick check question: In a Deletion curve, does a faithful explanation produce a curve that drops slowly or quickly as features are removed?

- **Model-Agnostic Explanation**
  - Why needed here: CafGa treats the LLM as a black box (API). Unlike gradient-based methods, this approach requires no access to weights, only inputs and outputs.
  - Quick check question: Why does treating the model as a black box necessitate sampling many input variations (perturbations) to derive an explanation?

## Architecture Onboarding

- **Component map:** Frontend (TypeScript/React/Vite) -> Backend (Python/FastAPI/Uvicorn) -> Explanation Engine (shap library) -> Evaluator (NLI models)
- **Critical path:** 1. Task Definition: User defines Template + Input + Target Answer. 2. Segmentation: User creates feature groups G = {g₁, g₂, ... gₙ}. 3. Perturbation Generation: System creates masked versions based on coalitions of G. 4. Inference: System queries LLM (10x per perturbation) → Probabilities. 5. Attribution: Weighted Linear Regression solves for Shapley values. 6. Validation: System computes Deletion/Insertion curves using percentage of words on x-axis.
- **Design tradeoffs:** Latency vs. Accuracy (larger segments mean fewer features and faster computation but risk washing out important details); Granularity vs. Fidelity (word-level is precise but noisy, sentence-level is fast but coarse).
- **Failure signatures:** High variance in explanations from insufficient samples for stochastic models; uninformative heatmaps from failing evaluators; timeouts from rate limits with fine-grained segmentation.
- **First 3 experiments:** 1. Sanity Check (Identity): Verify tool isolates specific word driving clear sentiment. 2. Granularity Stress Test: Compare fidelity scores across Word, Sentence, and Custom granularity. 3. Evaluator Robustness: Compare stability when using "Contains" vs. "Entails" operators.

## Open Questions the Paper Calls Out
- How does CafGa's effectiveness and usability change when deployed in large-scale, real-world industry settings compared to controlled laboratory environments?
- What interface designs or heuristic guidance can best assist users in balancing the trade-off between computational efficiency and explanation fidelity?
- How can fidelity evaluation evolve beyond perturbation-based metrics to avoid circularity and better handle grouped features?

## Limitations
- The system's effectiveness depends on user ability to define semantically meaningful segments that may not always align with model behavior.
- The 10-sample heuristic for probability estimation lacks theoretical grounding and may produce unstable attributions for highly stochastic models.
- The study sample size (10 participants, 4 experts) limits generalizability of user preference results.

## Confidence
- High confidence: The technical implementation of KernelSHAP with interactive segmentation is well-specified and reproducible.
- Medium confidence: The user study methodology and preference results are reasonable but limited by sample size.
- Low confidence: The specific values for interactive sampling parameters (t_max, r_API) and the theoretical justification for the 10-sample probability estimation are unclear.

## Next Checks
1. Verify x-axis normalization: Run a controlled test with varying segment sizes and confirm deletion/insertion curves normalize by percentage of words, not groups, to prevent size-based bias.
2. Test sampling stability: Vary the number of samples (5, 10, 20) for probability estimation and measure variance in Shapley values across multiple runs with a highly stochastic model.
3. Sanity check semantic alignment: Design a test case where a multi-word phrase clearly drives prediction and verify that custom segmentation isolates the correct phrase while word-level attribution fragments it.