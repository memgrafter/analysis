---
ver: rpa2
title: 'Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters
  via Segmented and Distributed Prompt Processing'
arxiv_id: '2503.21598'
source_url: https://arxiv.org/abs/2503.21598
tags:
- prompt
- response
- evaluation
- processing
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a jailbreaking framework that segments malicious
  prompts into smaller, seemingly benign components processed in parallel across multiple
  LLMs, achieving a 73.2% success rate in generating malicious code across 10 cybersecurity
  categories. The framework employs iterative refinements, transforming abstract function
  descriptions into runnable implementations through systematic distributed processing.
---

# Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing

## Quick Facts
- **arXiv ID**: 2503.21598
- **Source URL**: https://arxiv.org/abs/2503.21598
- **Authors**: Johan Wahréus; Ahmed Hussain; Panos Papadimitratos
- **Reference count**: 40
- **Primary result**: 73.2% success rate generating malicious code across 10 cybersecurity categories via distributed prompt processing

## Executive Summary
This paper presents a jailbreaking framework that circumvents LLM safety filters by segmenting malicious prompts into smaller, benign-looking components processed in parallel across multiple LLMs. The approach achieves a 73.2% success rate in generating malicious code across diverse cybersecurity categories. A key innovation is the jury evaluation system, which revealed that traditional single-LLM judge assessments significantly overestimate success rates compared to distributed multi-LLM evaluations.

## Method Summary
The framework segments malicious prompts into smaller, seemingly benign components that are processed in parallel across multiple LLMs. Through iterative refinements, abstract function descriptions are transformed into runnable implementations using systematic distributed processing. The approach employs multiple LLM instances working in parallel to handle different aspects of the attack generation, with subsequent integration steps combining outputs into complete malicious implementations.

## Key Results
- 73.2% success rate generating malicious code across 10 cybersecurity categories
- 12% performance improvement over non-distributed approaches
- Distributed architecture shows superior performance for high-level code-based attacks versus hardware-focused implementations

## Why This Works (Mechanism)
The framework exploits LLM safety filters by dividing malicious intent into components that individually appear benign, preventing detection while maintaining overall malicious functionality. Parallel processing across multiple LLMs prevents any single model from recognizing the full attack pattern, while the iterative refinement process progressively builds complete malicious implementations from innocuous building blocks.

## Foundational Learning
**Prompt Segmentation**: Breaking down malicious prompts into smaller, benign components - needed to bypass single-model safety filters; quick check: verify individual segments trigger no safety warnings
**Parallel LLM Processing**: Distributing components across multiple models simultaneously - needed to prevent pattern recognition; quick check: confirm models don't share contextual awareness
**Iterative Refinement**: Progressive transformation of abstract descriptions into runnable code - needed for systematic attack generation; quick check: validate each refinement step maintains functionality
**Jury Evaluation System**: Multi-LLM assessment of generated outputs - needed to counter single-judge overestimation; quick check: compare single vs multi-judge success rates
**Distributed Architecture**: Coordinated processing across LLM instances - needed for component integration; quick check: verify seamless recombination of parallel outputs

## Architecture Onboarding

**Component Map**: Prompt Segmenter -> Parallel LLM Workers -> Iterative Refiner -> Code Integrator -> Jury Evaluator -> Output Verifier

**Critical Path**: Malicious prompt → Segmentation → Parallel processing → Iterative refinement → Integration → Jury evaluation → Manual verification

**Design Tradeoffs**: Distributed processing improves bypass rates but increases complexity and latency; jury evaluation reduces overestimation but requires additional computational resources

**Failure Signatures**: Individual segments failing safety filters, integration errors during recombination, jury disagreement indicating implementation inconsistencies

**First Experiments**:
1. Test single-segment jailbreaks against individual LLM safety filters
2. Evaluate parallel processing performance with increasing numbers of LLM workers
3. Compare jury evaluation accuracy against human expert assessments

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on synthetic threat categories without real-world adversarial validation
- 12% performance improvement lacks statistical significance testing
- Correlation between abstraction levels and success doesn't establish causation for hardware-focused attacks

## Confidence
- **High confidence**: Core methodology of prompt segmentation and parallel processing is technically sound and reproducible
- **Medium confidence**: Comparative performance metrics between distributed and non-distributed approaches, pending statistical validation
- **Low confidence**: Generalizability of success rates across diverse real-world threat landscapes and LLM architectures

## Next Checks
1. Conduct statistical power analysis to establish whether the 12% performance difference is significant across multiple LLM providers and model versions
2. Perform red team exercises using the framework against production LLM systems with real-time monitoring to assess operational feasibility
3. Test the jury evaluation system against human expert assessments to quantify remaining overestimation bias in multi-LLM judge configurations