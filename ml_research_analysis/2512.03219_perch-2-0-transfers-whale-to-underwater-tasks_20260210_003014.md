---
ver: rpa2
title: Perch 2.0 transfers 'whale' to underwater tasks
arxiv_id: '2512.03219'
source_url: https://arxiv.org/abs/2512.03219
tags:
- perch
- data
- whale
- embeddings
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the few-shot transfer learning performance
  of Perch 2.0, a supervised bioacoustics foundation model pretrained on 14,597 species,
  on marine mammal and underwater audio tasks despite its terrestrial training data.
  Using linear probing with embeddings from Perch 2.0 and comparing against other
  bioacoustic models (Perch 1.0, SurfPerch, GMWM, BirdNet V2.3, A VES-bio, BirdA VES),
  the authors show that Perch 2.0 consistently outperforms alternative embedding models
  on majority of tasks including DCLDE 2026 (species/ecotype classification), NOAA
  PIPAN (baleen whales), and ReefSet (reef sounds).
---

# Perch 2.0 transfers 'whale' to underwater tasks

## Quick Facts
- arXiv ID: 2512.03219
- Source URL: https://arxiv.org/abs/2512.03219
- Authors: Andrea Burns; Lauren Harrell; Bart van Merriënboer; Vincent Dumoulin; Jenny Hamer; Tom Denton
- Reference count: 10
- **Primary result:** Perch 2.0 pretrained on terrestrial bioacoustics achieves state-of-the-art few-shot transfer learning on marine mammal classification tasks

## Executive Summary
This paper demonstrates that Perch 2.0, a bioacoustics foundation model trained on over 14,500 terrestrial species, can effectively transfer to marine mammal and underwater audio classification tasks through few-shot learning. Using linear probing with k=8-16 examples per class, Perch 2.0 consistently outperforms specialized marine models on majority of tasks including DCLDE 2026 species/ecotype classification, NOAA PIPAN baleen whale detection, and ReefSet reef sound classification. The model achieves up to 0.977 AUC-ROC on DCLDE species classification, surpassing even models trained on marine data (except BirdNet V2.3 on one task). This "whale transfer" capability is attributed to fine-grained terrestrial bird classification forcing detailed acoustic feature learning that generalizes across taxa.

## Method Summary
The study evaluates few-shot transfer learning by extracting embeddings from pretrained bioacoustic models and applying linear probing. Audio recordings are chunked into fixed windows (3s or 5s depending on model), embeddings are extracted for each window, and then mean-pooled for variable-length recordings. A simple logistic regression classifier is trained on k examples per class (k ∈ {4, 8, 16, 32}) and evaluated using one-vs-all AUC-ROC. The evaluation includes three marine bioacoustic datasets: DCLDE 2026 killer whale ecotype and species classification, NOAA PIPAN baleen whale recordings, and ReefSet reef sounds. Six models are compared: Perch 2.0, Perch 1.0, SurfPerch, GMWM, BirdNet V2.3, A VES-bio, and BirdA VES.

## Key Results
- Perch 2.0 achieves 0.977 AUC-ROC on DCLDE species classification with k=16, surpassing specialized marine models
- On NOAA PIPAN task, Perch 2.0 achieves 0.903 AUC-ROC with k=16, outperforming all other models except BirdNet V2.3
- Perch 2.0 maintains superior performance across majority of tasks with few-shot learning (k=8-16 examples per class)
- Even specialized whale model GMWM underperforms Perch 2.0 on most transfer tasks despite being trained on marine data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained classification with high intra-class variance forces models to learn detailed, generalizable acoustic features
- **Mechanism:** Training on thousands of bird species with fine-grained distinctions (the "bittern lesson") forces the network to learn detailed acoustic features rather than superficial statistical regularities
- **Core assumption:** Acoustic features distinguishing thousands of bird species share fundamental properties with marine mammal vocalizations
- **Evidence anchors:** Section 6 states the fine-grained task forces detailed feature learning; Section 2 describes training on 14,500+ species with classification loss

### Mechanism 2
- **Claim:** Shared myoelastic-aerodynamic sound production mechanisms across taxa enable cross-domain feature transfer
- **Mechanism:** Birds and mammals share similar vocal production physics, allowing models trained on one taxa to encode useful features for another
- **Core assumption:** Log-mel spectrogram representation preserves sufficient information about physical production mechanisms
- **Evidence anchors:** Section 6 notes the shared sound production mechanism as a partial explanation for transfer success

### Mechanism 3
- **Claim:** Mean pooling of fixed-length window embeddings enables robust classification of weakly-labeled variable-length recordings
- **Mechanism:** Averaging embeddings across windows smooths over non-vocal segments and aggregates the "concept" of the sound for linear separation
- **Core assumption:** Vocalization occupies sufficient portion of recording windows that signal isn't washed out by averaging
- **Evidence anchors:** Section 4 describes the established evaluation protocol using mean pooling for weakly-labeled data

## Foundational Learning

- **Concept: Linear Probing (vs. Fine-Tuning)**
  - **Why needed here:** Evaluates embedding space quality rather than model's adaptive capacity
  - **Quick check question:** If I fine-tuned the EfficientNet backbone end-to-end, would the relative ranking of Perch 2.0 vs. BirdNet likely change?

- **Concept: Log-Mel Spectrograms**
  - **Why needed here:** Architecture accepts spectrograms, not raw audio; transfer happens in time-frequency domain
  - **Quick check question:** Why might a model trained on bird spectrograms struggle with low-frequency baleen whales if frequency bins aren't aligned?

- **Concept: Few-Shot Learning (k-shot)**
  - **Why needed here:** Demonstrates value for data-scarce environments where conservationists cannot label thousands of clips
  - **Quick check question:** At what value of 'k' does the performance gap between Perch 2.0 and GMWM narrow or flip?

## Architecture Onboarding

- **Component map:** Audio (resampled to 32kHz) -> Log-mel spectrogram -> EfficientNet-B3 -> 1536-dim embedding -> Mean Pooling -> L2 Normalization -> Recording-level embedding -> Logistic Regression

- **Critical path:**
  1. Resampling: Must match model expectations (32kHz for Perch, 48kHz for BirdNet)
  2. Windowing: 5-second windows with non-overlapping hop for Perch
  3. Embedding Extraction: Passing window through backbone to get 1280/1536 dim vector
  4. Pooling: Averaging vectors for long files (NOAA/PIPAN)

- **Design tradeoffs:**
  - Perch 2.0 vs. BirdNet: Larger window (5s vs 3s) and embedding dim (1536 vs 1024) may capture longer dependencies but requires more memory
  - Specialized vs. General: GMWM (Whale model) transfers worse than Perch (General), suggesting overfitting to specific hydrophone signatures

- **Failure signatures:**
  - GMWM classification head directly yielded 0.612 AUC (near random) vs 0.954 for probing embeddings
  - Resolution mismatch: Brief vocalizations (< window hop) may be lost through dilution

- **First 3 experiments:**
  1. Load Perch 2.0, extract embeddings for DCLDE "Species" task, train LogisticRegression with k=16, verify AUC > 0.95
  2. Compare Mean Pooling vs. Max Pooling for NOAA PIPAN task to isolate brief calls
  3. Run embeddings on pure noise dataset to check for "structured" noise embeddings causing false positives

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can modifying the embedding pooling strategy for AVES-based models improve their few-shot transfer performance on marine tasks?
- **Basis in paper:** Explicit statement that AVES performance could be improved with modified pooling method
- **Why unresolved:** Authors used mean pooling to match other models but didn't test alternative aggregation techniques
- **What evidence would resolve it:** Comparative evaluation of AVES embeddings using various pooling methods (max, attention-weighted) versus mean pooling

### Open Question 2
- **Question:** Is successful transfer driven more by fine-grained pretraining task or biological similarity in sound production?
- **Basis in paper:** Authors hypothesize both "bittern lesson" (task granularity) and shared myoelastic-aerodynamic mechanisms as explanations
- **Why unresolved:** Paper presents these as hypotheses without isolating specific contributions
- **What evidence would resolve it:** Ablation studies comparing coarse vs. fine-grained acoustic taxa, or taxa with shared vs. distinct vocal production mechanisms

### Open Question 3
- **Question:** Does full fine-tuning of Perch 2.0 provide significant performance gains over linear probing for low-data marine tasks?
- **Basis in paper:** Paper evaluates exclusively via linear probing, noting it's "fast to train" but leaving open whether updating backbone weights would yield higher accuracy
- **Why unresolved:** Linear probing caps potential performance compared to updating model weights
- **What evidence would resolve it:** Comparison of AUC-ROC scores between linear probe and end-to-end fine-tuning using same few-shot k-values

## Limitations
- Paper relies on linear probing rather than fine-tuning, leaving open whether Perch 2.0 advantages persist when entire model is adapted to marine data
- Evaluation protocol uses mean pooling but doesn't explore alternative aggregation strategies (max pooling, attention) that might better handle brief vocalizations
- Study doesn't address potential domain-specific degradation from terrestrial environmental noise patterns not present in underwater recordings

## Confidence
- **High confidence:** Perch 2.0 produces superior embeddings for marine bioacoustic tasks compared to other bioacoustic models when using linear probing
- **Medium confidence:** The "bittern lesson" mechanism fully explains the transfer success
- **Low confidence:** Mean pooling is the optimal aggregation strategy for weakly-labeled marine recordings

## Next Checks
1. Fine-tune the Perch 2.0 backbone end-to-end on a small labeled marine dataset and compare performance against linear probing
2. Implement and test alternative pooling strategies (max pooling, weighted averaging based on signal energy) on NOAA PIPAN dataset
3. Evaluate Perch 2.0 embeddings on ultrasonic marine mammal calls (porpoises, dolphins) to determine frequency range limits of transfer capability