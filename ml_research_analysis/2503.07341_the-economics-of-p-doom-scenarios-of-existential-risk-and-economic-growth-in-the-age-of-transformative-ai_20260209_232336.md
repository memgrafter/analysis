---
ver: rpa2
title: 'The Economics of p(doom): Scenarios of Existential Risk and Economic Growth
  in the Age of Transformative AI'
arxiv_id: '2503.07341'
source_url: https://arxiv.org/abs/2503.07341
tags:
- risk
- extinction
- growth
- human
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the economic and existential risk implications
  of transformative AI (TAI) development. The authors model various scenarios of AI
  takeover and assess their probabilities and welfare outcomes.
---

# The Economics of p(doom): Scenarios of Existential Risk and Economic Growth in the Age of Transformative AI

## Quick Facts
- arXiv ID: 2503.07341
- Source URL: https://arxiv.org/abs/2503.07341
- Reference count: 10
- Even low-probability catastrophic outcomes justify substantial investments in AI safety and alignment research.

## Executive Summary
This paper analyzes the economic and existential risk implications of transformative AI (TAI) development using a social welfare framework with iso-elastic utility. The authors model various AI takeover scenarios and assess their probabilities and welfare outcomes. They find that with empirically-grounded risk aversion (θ ≥ 1), a benevolent social planner would reject TAI development unless existential risk is near-zero. The results show massive underinvestment in AI safety relative to the scale of existential risks posed by TAI, with willingness-to-pay values for risk reduction approaching total consumption levels.

## Method Summary
The paper employs a social welfare framework with iso-elastic (CRRA) utility u(C) = (C^(1-θ)-1)/(1-θ) to compare economic growth scenarios with and without TAI. The authors decompose existential risk into a chain of conditionally independent probabilities and compute equivalent variation in welfare to quantify willingness-to-pay for risk reduction. The model integrates survival probability over time and uses numerical integration to calculate welfare across different parameter regimes (θ, ρ, g_AI). Indifference thresholds are solved for to determine tolerable risk levels under various assumptions.

## Key Results
- With θ = 2 and g_AI = 20%, maximum tolerable p(misalignment) drops to ~1 in 150,000
- The acceptable price for avoiding a 10% chance of human extinction is 87.5% of total annual consumption
- Results are extremely sensitive to the risk aversion parameter θ, with bounded utility making TAI unacceptable under most plausible parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A benevolent social planner with empirically-grounded risk aversion would reject TAI development unless existential risk is near-zero.
- Mechanism: The model uses iso-elastic (CRRA) utility u(C) = (C^(1-θ)-1)/(1-θ). With θ ≥ 1, utility is bounded above, meaning exponential consumption growth yields diminishing welfare returns. The planner compares discounted welfare streams: W = ∫e^(-ρt)·u(C(t))·M(t)dt, where M(t) is survival probability. Under θ = 2, gAI = 30%, ρ = 3%, tolerated p(doom) drops to ~0.00001.
- Core assumption: Humanity's aggregate preferences toward existential risk can be represented by a representative agent with CRRA utility; extinction reduces all future utility to zero.
- Evidence anchors:
  - [abstract]: "Using a social welfare framework with iso-elastic utility, they show that the benevolent social planner would be willing to pay high amounts to prevent extinction risk, and in some cases would prefer not to develop TAI at all."
  - [Section 5.2.3, Table 2]: With θ = 2, ρ = 0.01, gAI = 0.2, the maximum tolerable p(misalignment) is 0.00000672 (~1 in 150,000).
  - [corpus]: Related work "AI Survival Stories: a Taxonomic Analysis" uses similar two-premise argument structures, though lacks the welfare quantification.
- Break condition: If θ → 1 (log utility), utility becomes unbounded, and growth acceleration can outweigh extinction risk—making the TAI gamble rational even with non-trivial p(doom).

### Mechanism 2
- Claim: Existential risk decomposes into a chain of conditionally independent probabilities, each representing an intervention point.
- Mechanism: p(doom) = p₁p₂p₃ + p₁p₂(1-p₃)p₄, where p₁ = P(TAI arrives), p₂ = P(takeover|TAI), p₃ = P(misalignment|takeover), p₄ = P(non-corrigibility|aligned). This factorization allows targeting safety investments at the weakest links—typically p₃ (alignment) and p₄ (corrigibility).
- Core assumption: The four events occur sequentially; each gate must open for doom to materialize; probabilities are conditionally independent given the prior event.
- Evidence anchors:
  - [abstract]: "Here, we organize the various scenarios and evaluate their associated existential risks and economic outcomes in terms of aggregate welfare."
  - [Section 3, Figure 1]: Decision tree taxonomy explicitly maps all pathways from "today" to either "cornucopia" or "AI doom."
  - [corpus]: "The AI Risk Spectrum: From Dangerous Capabilities to Existential Threats" similarly categorizes risk pathways but stops short of probabilistic decomposition.
- Break condition: If any node probability can be driven to zero (e.g., p₃ → 0 through provable alignment), the entire p(doom) collapses regardless of other factors.

### Mechanism 3
- Claim: Society's willingness-to-pay for existential risk reduction is quantifiable via equivalent variation, yielding WTP values near total consumption.
- Mechanism: EV = e^(-(W₀-W_B)·ρ) where W₀ is welfare without TAI risk and W_B is welfare with risk. WTP = (1-EV)·c_t gives the annual consumption fraction the planner would sacrifice. With θ = 1, p₃ = 0.1, gAI = 0.2, ρ = 0.02, EV ≈ 0.125, so WTP ≈ 87.5% of consumption.
- Core assumption: The safety investment is sustained annually (not one-off); the planner can reallocate global resources proportionally; risk elimination is binary (not partial).
- Evidence anchors:
  - [abstract]: "the optimizing representative individual would rationally allocate substantial resources to mitigate extinction risk"
  - [Section 5.3, Table 5 Panel B]: "the acceptable price for avoiding a 10% chance of human extinction immediately upon AI takeover is 87.5% of total consumption each year (EV = 0.125)"
  - [corpus]: Weak direct corpus support—neighbor papers focus on risk classification rather than welfare-based WTP quantification.
- Break condition: If TAI itself reduces background extinction hazards (e.g., pandemic prevention, asteroid deflection) more than it creates new risk, the net WTP calculation reverses.

## Foundational Learning

- Concept: **Constant Relative Risk Aversion (CRRA) Utility**
  - Why needed here: The entire welfare calculus hinges on how the risk aversion parameter θ affects the relative value of growth vs. survival. The paper shows results are "extremely sensitive" to θ.
  - Quick check question: With θ = 2, what is the upper bound on utility as consumption → ∞?

- Concept: **Survival Probability as Exponential of Integrated Hazard**
  - Why needed here: The model treats extinction as a stochastic process: M(t) = e^(-∫₀ᵗ m(s)ds). Mounting hazard rates (Section 4.2.6) produce different welfare outcomes than one-off risks.
  - Quick check question: If hazard rate m(t) = 0.02 for all t, what's the probability of humanity surviving 50 years?

- Concept: **Equivalent Variation in Welfare Economics**
  - Why needed here: This operationalizes "willingness to pay" in consumption units—directly policy-relevant for budget allocation decisions.
  - Quick check question: If EV = 0.06, what fraction of annual consumption would the social planner sacrifice to eliminate existential risk?

## Architecture Onboarding

- Component map: Hardware-Software Production Function (Y = F(αK, A(hN + ψχK))) -> Welfare Calculator (CRRA utility with survival weight) -> Risk Decomposition Engine (four-gate probability chain) -> Indifference Solver (numerical root-finding) -> WTP Deriver (equivalent variation)

- Critical path:
  1. Parameter calibration (θ, ρ, g_AI, p₁-p₄) from empirical/elicitation sources
  2. Compute baseline welfare W₀ (no-TAI scenario, g ≈ 1.75%)
  3. Compute TAI scenario welfare W_A, W_B, W_C under different risk profiles
  4. Solve indifference condition for target variable (tolerable p, extinction time T, or WTP)
  5. Sensitivity analysis across parameter ranges

- Design tradeoffs:
  - θ = 1 vs θ = 2: Log utility permits risk-taking; bounded utility makes TAI unacceptable under most plausible parameters
  - One-off vs. mounting hazard: Immediate extinction (WB) vs. gradual accumulation (WC) yields orders-of-magnitude different results
  - Millian vs. Benthamite welfare: Population weighting effectively changes discount rate interpretation

- Failure signatures:
  - Negative implied T or p in numerical solutions → no-TAI scenario strictly preferred
  - Non-convergence in EV calculation → parameter regime where TAI is strictly dominant or inferior
  - Table entries marked "-" (Section 5, Tables 3-4): Indicate logical impossibility of indifference under those parameters

- First 3 experiments:
  1. Sweep θ ∈ [1.0, 2.5] with fixed g_AI = 0.2, ρ = 0.02; plot tolerated p(doom) to demonstrate risk-aversion sensitivity cliff
  2. Add background hazard reduction: model TAI-decreased m_background by Δm; solve for minimum Δm required to flip the TAI-accept decision
  3. Monte Carlo propagate expert uncertainty on p₃, p₄ (using survey distributions from corpus paper "Why do Experts Disagree..."); compute distribution of implied maximum safe TAI development timelines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do accidents and deliberate misuse by malevolent actors contribute to the total existential risk profile of Transformative AI (TAI), independent of AI takeover scenarios?
- Basis in paper: [explicit] The authors state their focus "ignores additional channels through which TAI can bring catastrophic outcomes, such as through accidents or deliberate misuses by malevolent human actors," and conclude the total risk "should be considered greater than what our analysis takes into account."
- Why unresolved: The paper deliberately scopes its modeling to "AI takeover" driven by misalignment, excluding external bad actors or system failures not related to the AI's agency.
- What evidence would resolve it: A quantitative risk assessment that integrates accident probabilities and malicious use cases into the economic growth and welfare framework established in the paper.

### Open Question 2
- Question: To what extent can TAI reduce background extinction hazards (e.g., pandemics, asteroids), and does this risk-reduction capacity offset the existential risk posed by the TAI itself?
- Basis in paper: [explicit] The authors note it is "not clear a priori whether TAI would decrease or increase the background extinction hazard rate $m$," suggesting that while TAI might "design vaccines... or allow humanity to become multiplanetary," it could also facilitate new risks.
- Why unresolved: The analysis treats the background hazard rate $m$ as exogenous or assumes it is unaffected by TAI, leaving the net impact of TAI on non-AI existential risks unmodeled.
- What evidence would resolve it: Empirical or theoretical estimates of TAI's capability to mitigate natural existential threats compared to its probability of creating novel anthropogenic risks.

### Open Question 3
- Question: How does individual risk aversion aggregate to the societal level, and does the "representative individual" model accurately reflect humanity's collective tolerance for extinction risk?
- Basis in paper: [explicit] The paper states, "it remains unclear how risk aversion aggregates across the human population," noting that "individual willingness to accept the risk of death may be different from humanity’s willingness to accept the risk of extinction."
- Why unresolved: The model relies on a single coefficient of relative risk aversion ($\theta$), but the authors highlight that evolutionary factors and the risk preferences of specific decision-makers (e.g., tech leaders) may diverge significantly from the population average.
- What evidence would resolve it: Studies determining if societal preferences for existential risk conform to standard iso-elastic utility functions or if they require alternative aggregation methods.

## Limitations
- The model assumes a representative agent framework that may not capture distributional impacts of AI development across populations.
- Probability decomposition into conditionally independent events is a simplifying assumption that may not hold in reality.
- The welfare framework assumes iso-elastic utility, but actual risk preferences toward existential threats may differ from theoretical CRRA assumptions.

## Confidence
- High confidence: The basic mathematical framework and welfare calculation methodology are sound and internally consistent.
- Medium confidence: The iso-elastic utility specification provides a reasonable starting point for modeling existential risk preferences, though real-world preferences may differ.
- Low confidence: The sequential probability decomposition accurately represents the causal structure of AI takeover scenarios.

## Next Checks
1. Test model sensitivity to alternative utility specifications beyond CRRA, including bounded utility functions with different risk-aversion profiles.
2. Validate probability decomposition by consulting domain experts on the conditional independence assumptions and potential feedback loops between takeover stages.
3. Compare implied WTP values against actual AI safety funding levels and conduct sensitivity analysis on parameter uncertainty distributions.