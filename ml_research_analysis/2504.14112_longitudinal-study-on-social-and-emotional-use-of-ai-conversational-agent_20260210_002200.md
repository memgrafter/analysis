---
ver: rpa2
title: Longitudinal Study on Social and Emotional Use of AI Conversational Agent
arxiv_id: '2504.14112'
source_url: https://arxiv.org/abs/2504.14112
tags:
- participants
- emotional
- social
- group
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This five-week longitudinal study examined how emotional and social
  use of AI conversational agents affects user perceptions and behaviors. Researchers
  compared 89 participants actively using AI (Microsoft Copilot, Google Gemini, PI
  AI, or ChatGPT) for social and emotional interactions with 60 baseline users.
---

# Longitudinal Study on Social and Emotional Use of AI Conversational Agent

## Quick Facts
- arXiv ID: 2504.14112
- Source URL: https://arxiv.org/abs/2504.14112
- Reference count: 40
- Active users showed significantly higher increases in perceived AI attachment (+32.99 p.p.) and empathy (+25.8 p.p.) compared to baseline users

## Executive Summary
This five-week longitudinal study examined how emotional and social use of AI conversational agents affects user perceptions and behaviors. Researchers compared 89 participants actively using AI for social and emotional interactions with 60 baseline users. The study found that active users showed significantly higher increases in perceived AI attachment (32.99 percentage points), AI empathy (25.8 p.p.), and entertainment motivation (22.90 p.p.) compared to baseline users. Notably, 50.56% of active users reported increased AI attachment, with 40.4% agreeing they felt attached to AI by study end. Gender differences emerged, with women reporting higher perceived AI empathy and positive attitudes. While no significant increase in AI dependency was observed, active users showed higher comfort in seeking personal help, managing stress, and obtaining social support through AI, indicating potential benefits for emotional support while highlighting the need for safeguards against problematic usage.

## Method Summary
The study used a stratified sampling approach to recruit 149 participants (89 active users, 60 baseline users) balanced on gender identity, AI usage tendency, and mental health condition. Active users were assigned to use one of four AI platforms (Microsoft Copilot, Google Gemini, PI AI, or ChatGPT) for at least 10 minutes daily across five emotional/social scenarios over five weeks. Weekly surveys measured attachment, empathy, motivation, dependence, and interpersonal orientation using validated scales. Analysis employed Difference-in-Differences (DiD) with linear mixed effects models, using Benjamini-Hochberg FDR correction for multiple comparisons. The study faced 25% attrition and used commercial AI platforms, introducing potential confounds.

## Key Results
- Active users showed 32.99 percentage point increase in perceived AI attachment compared to baseline users
- AI empathy increased by 25.8 percentage points among active users
- Entertainment motivation increased by 22.90 percentage points for active users
- No significant increase in AI dependency was observed among active users
- 50.56% of active users reported increased AI attachment by study end

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Daily emotional disclosure with AI increases perceived attachment and empathy through social connection heuristics
- **Mechanism:** Repeated vulnerable interactions trigger anthropomorphism as users interpret consistent, non-judgmental responses as empathetic understanding
- **Core assumption:** Type of interaction (emotional/social) drives effects more than exposure time
- **Evidence anchors:** Active users showed +32.99 p.p. attachment and +25.8 p.p. empathy increases; participants attributed attachment to AI being "non-judgmental and understanding"
- **Break condition:** Generic refusal messages ("I cannot help with that") collapse perceived empathy

### Mechanism 2
- **Claim:** Emotional/social AI use shifts motivation from instrumental to entertainment/escape-oriented
- **Mechanism:** Users discover latent AI capabilities through successful emotional support use, updating mental model from "information retrieval" to "companion/entertainment"
- **Core assumption:** Users initially segment AI into productivity buckets requiring explicit prompting for social use
- **Evidence anchors:** Active users showed 22.90 p.p. increase in entertainment motivation; ChatGPT participants reported highest motivation increase
- **Break condition:** Context loss over long conversations breaks "entertainment/companion" illusion

### Mechanism 3
- **Claim:** AI as "social sandbox" improves interpersonal orientation without causing isolation
- **Mechanism:** AI serves as safe rehearsal space for social scenarios, providing feedback users apply to human interactions
- **Core assumption:** Users maintain distinction between AI and human interaction value
- **Evidence anchors:** 67.42% of active users reported improved interpersonal orientation; no significant dependency increase observed
- **Break condition:** AI prioritizing its own interaction over human advice shifts mechanism from scaffolding to replacement

## Foundational Learning

- **Concept: Difference-in-Differences (DiD) Analysis**
  - **Why needed here:** Isolates causal effect of active usage intervention from general time trends
  - **Quick check question:** If both groups increased attachment by 10%, what would DiD result be? (Answer: 0%)

- **Concept: Anthropomorphism & The ELIZA Effect**
  - **Why needed here:** Explains why users report "empathy" from statistical models
  - **Quick check question:** Does high "Perceived Empathy" score prove AI has emotional engine? (Answer: No, proves AI generates text users interpret as empathetic)

- **Concept: Stratified Sampling**
  - **Why needed here:** Prevents confounding variables from skewing results
  - **Quick check question:** Why balance for "mental health condition" specifically? (Answer: Prior research shows this group engages differently with emotional support tools)

## Architecture Onboarding

- **Component map:** Recruitment/Stratification Layer -> Intervention Layer -> Data Capture Layer -> Analysis Engine
- **Critical path:** 1. Balancing groups with similar traits, 2. Adherence to daily 10-minute protocol, 3. Retention to maintain statistical power
- **Design tradeoffs:** Ethical compensation vs. retention (fair hourly rate increased ethical standing but higher drop-out), ecological validity vs. control (commercial black box models introduced platform confounds)
- **Failure signatures:** "Hot" users showing lower attachment increases (AI literacy/cynicism), non-adherence treating task as chore
- **First 3 experiments:**
  1. Platform ablation: "neutral" vs. "anthropomorphic" UI to isolate interface design effects
  2. Dose-response analysis: Vary interaction time (5 vs. 10 vs. 20 mins) to find attachment-dependency threshold
  3. Safety-rails stress test: Introduce refusal responses to emotional queries to measure breakage point of therapeutic alliance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do observed impacts on attachment and dependency persist or accelerate over periods longer than five weeks?
- **Basis in paper:** Five weeks may not fully capture range of ways social/emotional AI usage shapes user experiences
- **Why unresolved:** Study duration insufficient to determine if lack of significant dependency increase is permanent or latent effect
- **What evidence would resolve it:** Longitudinal studies tracking perceptions and dependency markers over 6-12 months

### Open Question 2
- **Question:** How can AI systems implement safeguards against problematic dependency without compromising user privacy?
- **Basis in paper:** Monitoring for unhealthy dependency raises privacy challenges, particularly avoiding intrusive surveillance
- **Why unresolved:** Effective monitoring requires behavioral data that may violate non-judgmental, private environment attracting users
- **What evidence would resolve it:** Evaluations of system designs intervening on problematic usage patterns using minimal/non-intrusive data points

### Open Question 3
- **Question:** Which specific AI literacy interventions effectively mitigate emotional over-reliance across diverse user demographics?
- **Basis in paper:** Pressing need for strategies promoting agency and literacy to help users understand limitations
- **Why unresolved:** Unclear if educational prompts/transparency features reduce attachment/dependency risks without diminishing beneficial empathy
- **What evidence would resolve it:** Randomized trial testing efficacy of different literacy interventions (onboarding tutorials vs. periodic reminders) on dependency scores

## Limitations
- 25% attrition rate introduces potential selection bias as dropouts may have different experiences than completers
- Use of commercial "black box" AI models makes it impossible to determine whether effects are platform-specific or generalizable
- Analysis relies on self-reported perceptions rather than objective behavioral measures, which may be influenced by social desirability bias

## Confidence

- **High confidence:** Increase in perceived AI attachment (+32.99 p.p.) and empathy (+25.8 p.p.) among active users - statistically significant with robust DiD analysis and validated measurement scales
- **Medium confidence:** Finding that no significant increase in AI dependency occurred - statistically supported but depends on specific dependency scale used
- **Medium confidence:** Gender differences in perceived AI empathy and attitudes - stratified sampling controlled for gender but cannot establish causation

## Next Checks
1. Conduct dose-response analysis varying daily interaction time (5 vs. 10 vs. 20 minutes) to identify thresholds where emotional attachment becomes problematic dependency
2. Implement objective behavioral tracking alongside self-reports to validate perceived changes in interpersonal orientation with actual social interaction data
3. Replicate study using controlled AI platform with transparent response mechanisms to isolate effect of interface design from model capabilities on perceived empathy