---
ver: rpa2
title: 'SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis
  from a CLIP-Aligned Encoder'
arxiv_id: '2511.17547'
source_url: https://arxiv.org/abs/2511.17547
tags:
- latent
- image
- diffusion
- synapse
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SYNAPSE addresses EEG-to-image synthesis by introducing a two-stage
  framework that learns a semantically aligned latent representation and efficiently
  conditions a Stable Diffusion model. In Stage 1, a CLIP-aligned EEG autoencoder
  jointly optimizes signal reconstruction and cross-modal alignment, producing a compact
  latent space with reduced parameters.
---

# SYNAPSE: Synergizing an Adapter and Finetuning for High-Fidelity EEG Synthesis from a CLIP-Aligned Encoder

## Quick Facts
- **arXiv ID:** 2511.17547
- **Source URL:** https://arxiv.org/abs/2511.17547
- **Reference count:** 40
- **Primary result:** SYNAPSE achieves state-of-the-art FID of 46.9 on CVPR40 for EEG-to-image synthesis

## Executive Summary
SYNAPSE introduces a two-stage framework for EEG-to-image synthesis that combines cross-modal alignment with efficient model adaptation. The approach first learns a semantically aligned latent representation through a CLIP-aligned EEG autoencoder, then conditions a pre-trained Stable Diffusion model using a lightweight adapter and selective finetuning of cross-attention layers. This design enables high-fidelity image generation while maintaining strong generalization across subjects. The model demonstrates robust performance even when categorical alignment between EEG and images is imperfect, suggesting perceptual rather than label-based reconstruction.

## Method Summary
SYNAPSE employs a two-stage framework for EEG-to-image synthesis. Stage 1 involves training a CLIP-aligned EEG autoencoder that jointly optimizes signal reconstruction and cross-modal alignment, producing a compact latent space. Stage 2 integrates the frozen encoder with a lightweight adaptation module and selectively finetunes cross-attention layers of a pre-trained Stable Diffusion model. This approach enables efficient conditioning of the diffusion model on EEG signals while maintaining high-fidelity image generation capabilities.

## Key Results
- Achieves state-of-the-art FID score of 46.9 on CVPR40 dataset
- Demonstrates improved perceptual quality compared to prior methods
- Shows strong generalization across subjects despite imperfect categorical alignment
- Captures meaningful low- and mid-level visual features from EEG signals

## Why This Works (Mechanism)
The framework's success stems from its two-stage design that first establishes semantic alignment between EEG and visual features, then efficiently conditions a powerful generative model. The CLIP alignment in Stage 1 ensures the latent representation captures semantically relevant visual information rather than just signal reconstruction. The lightweight adapter and selective finetuning in Stage 2 allow the pre-trained Stable Diffusion model to interpret EEG signals without extensive retraining, preserving its generative capabilities while adapting to the new modality.

## Foundational Learning
- **CLIP alignment:** Learning cross-modal representations that capture semantic relationships between EEG signals and images. Needed to ensure the latent space encodes meaningful visual information rather than just signal characteristics. Quick check: Verify alignment quality through retrieval experiments or similarity metrics.
- **Stable Diffusion conditioning:** Modifying a pre-trained diffusion model to accept additional conditioning signals. Required to leverage the powerful generative capabilities of existing models. Quick check: Confirm conditioning preserves image quality and diversity.
- **Cross-attention finetuning:** Selectively updating attention layers rather than full model weights. Enables efficient adaptation while maintaining model stability. Quick check: Compare performance with full finetuning to validate efficiency gains.

## Architecture Onboarding

**Component Map:**
EEG signals -> CLIP-aligned autoencoder -> Latent representation -> Adapter module -> Cross-attention layers -> Stable Diffusion -> Generated images

**Critical Path:**
The critical path flows from EEG input through the autoencoder to generate the latent representation, which then passes through the adapter module to condition the cross-attention layers of Stable Diffusion for image generation.

**Design Tradeoffs:**
- **Adapter vs. Full Finetuning:** Adapter module provides parameter efficiency but may limit adaptation capacity compared to full finetuning
- **CLIP Alignment vs. Signal Reconstruction:** Joint optimization balances semantic alignment with signal fidelity
- **Selective vs. Full Cross-attention Finetuning:** Selective approach reduces computational cost but may miss some adaptation opportunities

**Failure Signatures:**
- Poor image quality suggests inadequate cross-modal alignment or adapter design issues
- Lack of subject generalization indicates overfitting to training subjects
- Low perceptual scores despite good FID may indicate alignment with dataset statistics rather than true perceptual quality

**3 First Experiments:**
1. Validate cross-modal alignment quality through retrieval or similarity metrics
2. Test adapter module performance against full model finetuning baselines
3. Evaluate subject generalization on held-out subjects before full model training

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Evaluation limited to a single dataset (CVPR40), restricting generalizability claims
- Lacks direct comparisons with other state-of-the-art EEG synthesis methods on standardized benchmarks
- Scalability to more diverse or naturalistic EEG datasets remains unexplored
- Perceptual evaluation could benefit from larger-scale human studies

## Confidence
**High** for technical implementation and performance claims on CVPR40 dataset; **Medium** for generalization claims due to single-dataset evaluation; **Low** for scalability and broader applicability assertions without additional dataset validation.

## Next Checks
1. Evaluate SYNAPSE on additional EEG-to-image datasets (e.g., public EEG datasets with image pairs) to test generalization beyond CVPR40
2. Conduct ablation studies to quantify the impact of the lightweight adaptation module and selective cross-attention finetuning on synthesis quality
3. Perform large-scale human perceptual studies to validate the model's ability to capture meaningful visual features under varying degrees of categorical alignment