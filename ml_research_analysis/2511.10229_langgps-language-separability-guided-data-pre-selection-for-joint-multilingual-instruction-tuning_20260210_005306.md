---
ver: rpa2
title: 'LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual
  Instruction Tuning'
arxiv_id: '2511.10229'
source_url: https://arxiv.org/abs/2511.10229
tags:
- multilingual
- data
- training
- samples
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of selecting effective multilingual\
  \ training data for instruction tuning of large language models. The authors propose\
  \ LangGPS, a two-stage pre-selection framework guided by language separability,\
  \ which quantifies how well samples in different languages are distinguishable in\
  \ the model\u2019s representation space."
---

# LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning

## Quick Facts
- arXiv ID: 2511.10229
- Source URL: https://arxiv.org/abs/2511.10229
- Reference count: 40
- Primary result: Two-stage pre-selection framework improves existing multilingual data selection methods by 6.37% average on LLaMA-3.1-8B

## Executive Summary
This paper addresses the challenge of selecting effective multilingual training data for instruction tuning of large language models. The authors propose LangGPS, a two-stage pre-selection framework guided by language separability, which quantifies how well samples in different languages are distinguishable in the model's representation space. The method first filters training data based on separability scores and then refines the subset using existing selection methods. Experiments across six benchmarks and 22 languages show that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability, especially for understanding tasks and low-resource languages.

## Method Summary
LangGPS operates in two stages: first, it computes silhouette-based language separability scores from last-token hidden states of a pre-trained multilingual model; second, it pre-selects top ρ% samples per language cluster before applying downstream selection methods like LESS or DSIR. The framework assumes that high-separability samples form clear language boundaries while low-separability samples serve as cross-lingual bridges. Experiments use 97,696 instruction pairs from the Aya dataset across 31 languages, training LLaMA-3.1-8B and Qwen2.5-7B with standard SFT hyperparameters and evaluating on six multilingual benchmarks.

## Key Results
- LangGPS + LESS achieves 6.37% average improvement over full data on LLaMA-3.1-8B, outperforming vanilla LESS by 4.90%
- High-separability samples significantly outperform low-separability and random selection under limited training data (<2000 samples)
- Balanced curriculum interleaving of samples across separability levels yields stable gains in multilingual curriculum learning
- Method shows particular effectiveness for low-resource languages and understanding tasks (XNLI, XStoryCloze, MMMLU)

## Why This Works (Mechanism)

### Mechanism 1: Language Separability as Structural Prior for Data Selection
- **Claim:** Prioritizing samples with high language separability helps multilingual models form and maintain clearer linguistic boundaries in representation space, reducing interference between languages during joint training.
- **Mechanism:** The silhouette score (Equations 1-3) quantifies how well each sample clusters with its language group versus other languages. Selecting high-separability samples ensures the training data exhibits well-structured, language-specific representations. This acts as a structural prior that reduces cross-lingual interference, particularly critical for low-resource languages that might otherwise be "absorbed" by high-resource language patterns.
- **Core assumption:** Clear linguistic boundaries in the training data's representation space (as measured by the pre-trained model) predict cleaner multilingual learning trajectories. This assumes separability is a stable property across training, not just a snapshot.
- **Evidence anchors:**
  - [abstract]: "highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation"
  - [Page 6, Section 5.2]: t-SNE visualizations show models trained with LangGPS exhibit higher average silhouette scores (0.8081 vs 0.7665 for vanilla LESS)
  - [Page 2, Figure 1a]: Under limited training data (<2000 samples), high-separability samples significantly outperform low-separability and random selection
  - [corpus]: Related work (CONGRAD) independently confirms that conflicting gradients in multilingual training cause degradation, suggesting boundary preservation matters
- **Break condition:** If the base model has extremely poor multilingual representations to begin with (silhouette scores near 0 or negative), the separability signal becomes unreliable. Also breaks when ρ (pre-selection ratio) is too small (<10%), causing over-focus on similar samples and reduced diversity.

### Mechanism 2: Two-Stage Decoupling of Linguistic Structure from Task-Specific Quality
- **Claim:** Separating linguistic structure filtering (separability) from task-relevance/quality filtering allows existing selection methods to operate more effectively within a linguistically coherent candidate pool.
- **Mechanism:** Stage 1 filters to top ρ% by separability within each language cluster, ensuring language boundaries are preserved. Stage 2 applies existing methods (LESS, DSIR, diversity-based) on this pre-filtered pool. This decoupling prevents existing methods—which optimize for task relevance or text quality—from inadvertently selecting samples that blur language boundaries.
- **Core assumption:** Linguistic structure and task-specific quality are largely orthogonal dimensions; optimizing both jointly via existing single-stage methods leads to suboptimal trade-offs.
- **Evidence anchors:**
  - [abstract]: "LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods"
  - [Page 5, Table 1]: LangGPS + LESS achieves +6.37% average improvement over full data on LLaMA-3.1-8B, while vanilla LESS alone achieves only +4.90%
  - [Page 12, Table 3]: For expensive methods like LESS (8 hours total), LangGPS pre-selection reduces candidate pool to 20%, cutting gradient computation cost by ~80%
  - [corpus]: Weak direct evidence in corpus; related work focuses on single-stage selection, highlighting novelty
- **Break condition:** When ρ approaches 100%, pre-selection becomes a no-op and performance converges to vanilla baseline. Also fails if downstream selector has conflicting objectives that actively counter boundary preservation.

### Mechanism 3: Balanced Separability Interleaving as Curriculum Signal
- **Claim:** Interleaving samples across separability levels during training—rather than strictly ascending or descending—enables models to benefit from both language-specific structure (high-separability) and cross-lingual alignment bridges (low-separability).
- **Mechanism:** Low-separability samples, which "entangle" with other language clusters, function as implicit cross-lingual bridges. High-separability samples provide clear language-specific signals for rapid adaptation. Balanced interleaving (Algorithm 1) maintains a consistent mix throughout training, preventing the cold-start problem from low-separability-only training while retaining cross-lingual transfer benefits.
- **Core assumption:** Both language-specific competence and cross-lingual transfer are necessary for robust multilingual performance; neither alone is sufficient.
- **Evidence anchors:**
  - [Page 7, Table 2]: Balanced curriculum outperforms ascending, descending, and random baselines across both models and most datasets
  - [Page 6, Section 5.3, Figure 4]: Low-separability samples cause severe cold-start at 50 samples but show milder decline in cross-lingual translation tasks during training
  - [Page 6]: "low-separability samples may serve as valuable bridges for cross-lingual alignment due to their entangled representations"
  - [corpus]: CM-Align work suggests consistency-based cross-lingual alignment is valuable, indirectly supporting the bridge hypothesis
- **Break condition:** Strict ascending or descending curricula fail (Table 2 shows both underperform balanced). Pure low-separability training fails at small data regimes due to cold-start problem.

## Foundational Learning

- **Silhouette Score for Clustering Quality**
  - **Why needed here:** This is the core metric LangGPS uses. It measures intra-cluster compactness vs inter-cluster separation. Without understanding how it balances a(pᵢ) and b(pᵢ), you can't interpret what "high separability" actually means or why it might correlate with learning quality.
  - **Quick check question:** Given a sample with silhouette score 0.8, what does this tell you about its position relative to its assigned language cluster versus other clusters?

- **Multilingual Supervised Fine-Tuning (SFT) Dynamics**
  - **Why needed here:** LangGPS operates in the context of joint multilingual instruction tuning, where multiple languages compete for model capacity. Understanding that interference occurs—especially for low-resource languages—motivates why structural priors matter.
  - **Quick check question:** Why might training on a mix of high-resource and low-resource languages cause the low-resource language performance to degrade more than monolingual training would predict?

- **Representation Space Analysis (Last-Token Hidden States)**
  - **Why needed here:** LangGPS extracts representations from the last token's hidden state to compute separability. You need to understand why this choice (vs. mean pooling, specific layers) matters and what properties of the representation space are being exploited.
  - **Quick check question:** Why might the last-token hidden state be a reasonable choice for capturing sentence-level language identity in decoder-only models?

## Architecture Onboarding

### Component Map
Full Training Corpus -> Representation Extraction -> Silhouette Score Computation -> Pre-selection -> Existing Selector -> Training Loop

### Critical Path
1. **Representation extraction** (~2 hours on A100 for 97K samples in fp16) — this is the dominant cost and must complete before any selection
2. **Silhouette scoring** — O(|D|²) distance computation; parallelizable but memory-intensive for large corpora
3. **Downstream selector** — cost varies (LESS: 8 hours; DSIR: 3 minutes; Random: negligible)

### Design Tradeoffs
| Parameter | Low Value | High Value |
|-----------|-----------|------------|
| ρ (pre-selection ratio) | Higher diversity, more compute for Stage 2, risk of including low-quality samples | Lower diversity (Figure 1b shows over-similarity), faster Stage 2, clearer linguistic signal |
| Selection percentage (1%/3%/5%) | Stronger regularizing effect, higher variance | More stable, diminishing returns |

**Recommended starting point:** ρ=20%, 5% selection, compute separability on base model (not after any SFT).

### Failure Signatures
- **Cold-start with ρ too low:** If you see models trained on 50-500 samples performing significantly worse than random, the pre-selection may have over-constrained diversity
- **No gain over vanilla:** If ρ≈100%, LangGPS is a no-op. Also check if base model has near-zero initial separability (s scores centered around 0)
- **Worse performance on generation tasks:** Paper shows gains concentrated on understanding tasks (XNLI, XStoryCloze, MMMLU); generation (XLSum) shows smaller or no gains — this is expected behavior, not a bug

### First 3 Experiments
1. **Baseline check:** Compute silhouette scores on your training corpus using the base model. Verify distribution spans [-0.2, 0.8] and differs by language. If all languages have near-identical distributions, separability signal is weak.
2. **Ablation on ρ:** Fix downstream selector to Random, vary ρ ∈ {10%, 20%, 50%, 100%}. Plot performance on a held-out multilingual benchmark (e.g., MMMLU subset). Expect inverted-U with peak around 20-50%.
3. **Curriculum sanity check:** Compare Balanced interleaving vs. Random shuffle vs. Descending-only on 3% data. If Descending significantly underperforms Random, your separability signal may be inverted or corrupted.

## Open Questions the Paper Calls Out
- **Representation stability across training:** The paper does not validate whether separability distributions shift significantly during training, which could invalidate the pre-selection signal
- **Computational scaling:** While O(|D|²) complexity is noted, the paper doesn't characterize how this scales to larger multilingual corpora (e.g., 1M+ samples)
- **Language variant handling:** The method groups samples by language label but doesn't address cases where a single language has multiple regional variants or dialects with different separability profiles

## Limitations
- Method requires model-specific data selection rather than a universal solution
- Computational bottleneck of O(|D|²) distance computation for large corpora
- Weaker performance gains on generation tasks compared to understanding tasks

## Confidence
- **High confidence:** Core experimental results showing LangGPS improves existing selection methods (Tables 1-2), and visualization evidence of cleaner language boundaries (t-SNE results, Figure 4)
- **Medium confidence:** Mechanism explanations for why separability correlates with learning quality, particularly the claim that low-separability samples serve as "bridges" for cross-lingual alignment
- **Low confidence:** Assertion that balanced interleaving is universally optimal across all multilingual settings, given limited ablation and the potential for task-specific curriculum needs

## Next Checks
1. **Representation stability validation:** Compute silhouette scores at multiple training checkpoints (0%, 25%, 50%, 75%, 100% of fine-tuning) and verify that the relative separability rankings remain stable.
2. **ρ sensitivity analysis on low-resource languages:** Systematically vary ρ (pre-selection ratio) and measure the performance impact specifically on low-resource languages (e.g., <1M speakers).
3. **Cross-lingual transfer ablation:** Train separate models using only high-separability samples vs. only low-separability samples on a zero-shot translation task, then measure which subset provides better cross-lingual transfer.