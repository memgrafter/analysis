---
ver: rpa2
title: On Explaining Visual Captioning with Hybrid Markov Logic Networks
arxiv_id: '2507.21246'
source_url: https://arxiv.org/abs/2507.21246
tags:
- image
- distribution
- caption
- ground
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel approach to explain visual captioning
  systems using Hybrid Markov Logic Networks (HMLNs). The key idea is to quantify
  how training examples influence caption generation by measuring shifts in the distribution
  over training data when conditioned on the generated caption.
---

# On Explaining Visual Captioning with Hybrid Markov Logic Networks

## Quick Facts
- **arXiv ID**: 2507.21246
- **Source URL**: https://arxiv.org/abs/2507.21246
- **Authors**: Monika Shah; Somdeb Sarkhel; Deepak Venugopal
- **Reference count**: 36
- **Primary result**: A novel approach using Hybrid Markov Logic Networks (HMLNs) to explain visual captioning by identifying influential training examples, validated through user studies showing high interpretability scores.

## Executive Summary
This paper presents a novel approach to explain visual captioning systems using Hybrid Markov Logic Networks (HMLNs). The key idea is to quantify how training examples influence caption generation by measuring shifts in the distribution over training data when conditioned on the generated caption. The approach combines symbolic rules with real-valued functions relating visual features to text using CLIP embeddings, then learns parameters through contrastive divergence. Explanations are generated by selecting training examples that show contrasting bias patterns.

The framework is evaluated through comprehensive user studies on Amazon Mechanical Turk with both technical and non-technical users. Results show that explanations generated by this method are highly interpretable, with majority of users rating them positively on a 5-point Likert scale. The method is applied to four state-of-the-art captioning models (SGAE, AoANet, X-LAN, M2 Transformer) on the MSCOCO dataset. AoANet received the highest interpretability scores among the models tested.

## Method Summary
The approach uses Hybrid Markov Logic Networks to explain visual captioning by quantifying the influence of training examples on generated captions. The method involves three main steps: (1) constructing a prior HMLN distribution over training instances using symbolic predicates extracted from captions and real-valued functions from CLIP embeddings, (2) treating the generated caption as uncertain "virtual evidence" to create a posterior distribution, and (3) computing importance weights and Hellinger distances to rank training examples by their influence. The framework learns weights via contrastive divergence and selects contrasting examples (positive influencers, negative influencers, and neutral similarities) as explanations.

## Key Results
- User studies on Amazon Mechanical Turk showed high interpretability scores (majority rated 4-5 on 5-point Likert scale) for HMLN-generated explanations
- AoANet model received the highest interpretability scores among four state-of-the-art captioning models tested
- Attention-based explanations were less effective, showing no significant difference between high and low attention object pairs in user ratings
- The approach successfully identified contrasting examples that positively or negatively explained generated captions

## Why This Works (Mechanism)

### Mechanism 1: Distributional Shift via Virtual Evidence
The influence of training data on a generated caption is signaled by the shift between the prior distribution over relations and the conditional distribution after observing the caption. The model constructs a prior HMLN distribution $P(\cdot)$ over training instances. When a test caption is generated, it is treated as uncertain "virtual evidence" rather than hard truth. This evidence multiplies with the prior to create a posterior $\hat{P}(\cdot)$. Importance weighting quantifies the "bias" introduced by the test instance; examples where the posterior deviates significantly from the prior (measured via Hellinger distance) are identified as high-influence explanations.

### Mechanism 2: Neuro-Symbolic Grounding via CLIP
Symbolic logic alone is insufficient for visual explanation; grounding logical predicates in continuous visual space (via CLIP) enables the system to handle perceptual nuance. The framework defines "Hybrid Formulas" containing symbolic predicates (e.g., `holding(object1, object2)`) and real-valued functions. These functions compute the cosine distance between CLIP embeddings of the image region and the text predicate. This results in "soft" logic (Lukasiewicz approximations) where a predicate is not just true/false, but true to a degree based on visual fidelity.

### Mechanism 3: Contrastive Example Selection
Explanations are most effective when they contrast positive influencers (what the model relied on) against negative influencers (what the model rejected) and neutral similarities. The system ranks training examples not just by similarity, but by their "bias" characteristics. It selects three specific types: 1) High positive bias (High Hellinger distance, High posterior), 2) High negative bias (High Hellinger distance, Low posterior), and 3) Low bias (Low Hellinger distance). This contrast helps users distinguish what makes the test instance unique.

## Foundational Learning

- **Concept: Markov Logic Networks (MLNs)**
  - **Why needed here**: The paper extends standard MLNs to "Hybrid" MLNs. Without understanding how weights attach to logical formulas in standard MLNs to form probability distributions, the "Hybrid" extension will be confusing.
  - **Quick check question**: If a formula in an MLN has a high weight, does it make worlds where the formula is true more likely or less likely?

- **Concept: Importance Sampling & Weighting**
  - **Why needed here**: The core explanation logic relies on "bias quantification" calculated via importance weights ($P(y)/\hat{P}(y)$). You must understand how re-weighting samples from a proposal distribution estimates expectations in a target distribution.
  - **Quick check question**: If a sample has a low probability in the proposal distribution but a high probability in the target, does its importance weight increase or decrease?

- **Concept: Scene Graphs**
  - **Why needed here**: The symbolic "ground predicates" (e.g., `man`, `holding`, `frisbee`) are extracted using a textual scene graph parser. This structure is the skeleton upon which the HMLN is built.
  - **Quick check question**: How does a scene graph structurally represent the sentence "A dog sits on a rug"?

## Architecture Onboarding

- **Component map**: Parser -> Encoder -> HMLN Grounding -> Learner -> Explainer
- **Critical path**: The transition from Section 3.1 (Templates) to Section 3.2 (Learning) is the bottleneck. The "Normalization" step (making a closed-world assumption relative to the test image) is critical; without it, the HMLN grounds the entire training dataset, making weight learning infeasible.
- **Design tradeoffs**:
  - **Query-specific vs. Global Parameterization**: The paper learns weights specific to the test instance (Inference-guided parameterization). This improves explanation relevance but increases latency per query compared to a single global model.
  - **Logic vs. Continuous**: The framework forces logical relations (XOR/AND) to behave like continuous functions (Gaussian/Softmax penalties). This aids optimization but may dilute strict logical semantics.
- **Failure signatures**:
  - **Attention Baseline Failure**: The paper notes that high-attention object pairs did not differ significantly from low-attention pairs in user ratings. If implementing baselines, do not assume raw attention weights are valid explanations.
  - **CLIP Domain Gap**: If the provided image contains concepts not well-captured by CLIP (e.g., highly abstract art), the real-valued functions in the HMLN will output uniform values, flattening the distribution and yielding random explanations.
- **First 3 experiments**:
  1. **Validation of Bias Metric**: Reproduce the correlation plot (Fig 4) between Hellinger distance (explanation diversity) and CLIPScore (caption quality) on a small subset (e.g., 100 images) to verify the "diversity = utility" hypothesis.
  2. **Template Ablation**: Run the explanation engine using only "C" (Conjunctive) properties vs. only "I" (Information/XOR) properties to see which logical structure contributes more to user interpretability.
  3. **Attention Baseline Check**: Implement the weakly-supervised MIL attention baseline described in Section 4.2 and verify the reported failure mode (scores ~2.9 for both high and low attention) to ensure your evaluation pipeline matches the paper's setup.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the HMLN explanation framework be effectively extended to complex generative tasks like Visual Question Answering (VQA)?
  - **Basis in paper**: The conclusion states, "In future, we will build upon this framework to explain other complex generative models such as Visual Question Answering."
  - **Why unresolved**: The current approach models distribution shifts in caption relations; VQA requires explaining multi-step reasoning chains rather than just descriptive generation.
  - **What evidence would resolve it**: Successful application of the HMLN framework to VQA models with a corresponding user study validating interpretability.

- **Open Question 2**: How does the computational cost of HMLN inference scale with larger datasets given the reliance on approximate methods?
  - **Basis in paper**: The paper notes that computing expectations exactly is "#P-hard in the worst case," necessitating Gibbs sampling and Contrastive Divergence.
  - **Why unresolved**: While effective on the MSCOCO dataset, the feasibility of running iterative MCMC sampling for real-time or web-scale explanation is not analyzed.
  - **What evidence would resolve it**: Complexity analysis and runtime benchmarks on datasets significantly larger than MSCOCO.

- **Open Question 3**: Does the reliance on manually pre-defined templates (Conjunctive and Explanation properties) limit the diversity of explanations?
  - **Basis in paper**: The authors "limit the HMLN to explainable structures... we pre-define templates" and restrict predicate chains to a maximum of two.
  - **Why unresolved**: Manual template engineering may fail to capture complex, higher-order relationships inherent in diverse visual scenes.
  - **What evidence would resolve it**: A comparison of explanation quality between the current manual templates and automatically learned relational structures.

## Limitations
- The HMLN mechanism's effectiveness relies on treating generated captions as "virtual evidence," but this assumption is theoretically grounded rather than empirically proven for this specific application.
- User study results show strong interpretability scores, but the evaluation was conducted on Amazon Mechanical Turk without detailed information about participant expertise levels or potential cultural/linguistic biases.
- The CLIP-based grounding for symbolic predicates could fail on concepts poorly represented in CLIP's embedding space, potentially producing noisy importance weights and unreliable explanations.

## Confidence

- **High confidence**: The general framework of using HMLNs to combine symbolic logic with continuous visual features is technically sound and builds on established MLN theory.
- **Medium confidence**: The specific implementation details (weight learning via contrastive divergence, importance weighting for bias quantification) are methodologically valid, though some hyperparameters remain unspecified.
- **Medium confidence**: User study results showing superior interpretability compared to attention-based explanations are credible, though the evaluation methodology has limitations.

## Next Checks

1. **Mechanism Validation**: Reproduce the correlation between Hellinger distance and CLIPScore on a small subset (100 images) to verify the "explanation diversity = utility" hypothesis.
2. **Template Ablation**: Compare explanation quality using only Conjunctive vs. Explanation properties to isolate which logical structure drives interpretability.
3. **Baseline Replication**: Implement and validate the MIL attention baseline to confirm the reported failure mode (scores ~2.9 for both high and low attention) matches the paper's findings.