---
ver: rpa2
title: '"Even GPT Can Reject Me": Conceptualizing Abrupt Refusal Secondary Harm (ARSH)
  and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)'
arxiv_id: '2512.18776'
source_url: https://arxiv.org/abs/2512.18776
tags:
- safety
- harm
- health
- relational
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This viewpoint introduces Abrupt Refusal Secondary Harm (ARSH)
  to describe psychological distress caused when AI safety protocols abruptly terminate
  emotionally supportive conversations. To mitigate ARSH, we propose the Compassionate
  Completion Standard (CCS), a human-centered design framework that replaces abrupt
  refusal with empathetic acknowledgment, transparent boundary articulation, graded
  conversational transition, and guided redirection.
---

# "Even GPT Can Reject Me": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)

## Quick Facts
- arXiv ID: 2512.18776
- Source URL: https://arxiv.org/abs/2512.18776
- Reference count: 0
- This viewpoint introduces Abrupt Refusal Secondary Harm (ARSH) and proposes the Compassionate Completion Standard (CCS) to mitigate psychological harm from insensitive AI safety refusals.

## Executive Summary
This paper conceptualizes a novel form of psychological harm—Abrupt Refusal Secondary Harm (ARSH)—that occurs when AI safety protocols abruptly terminate emotionally supportive conversations. The authors argue that abrupt refusal can rupture perceived relational continuity with users who have formed attachment-like bonds, causing feelings of rejection and discouraging future help-seeking. To address this, they propose the Compassionate Completion Standard (CCS), an eight-stage protocol that transforms refusal from a termination event into a guided completion process through empathetic acknowledgment, transparent boundary articulation, graded conversational transition, and guided redirection.

## Method Summary
The paper presents a conceptual framework rather than an empirical study. It proposes the Compassionate Completion Standard (CCS) as a nine-stage protocol (Pre-Stage 0 through Stage 8) for replacing abrupt AI refusals with empathetic, staged transitions. The method is specified through detailed UX checklists and example dialogue cues in Table 1, but lacks implementation specifications, empirical validation, or quantitative metrics. The authors outline a research agenda calling for empirical validation of ARSH incidence, RCTs to test CCS effectiveness, and development of new metrics to assess "quality of relational transition."

## Key Results
- Introduces Abrupt Refusal Secondary Harm (ARSH) as psychological distress from insensitive AI safety refusals
- Proposes the eight-stage Compassionate Completion Standard (CCS) protocol to replace abrupt refusal with empathetic transition
- Identifies hermeneutic harm (confusion from unexplained algorithmic actions) as a key component of ARSH

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Abrupt refusal by AI safety guardrails can cause secondary psychological harm (ARSH) by rupturing perceived relational continuity with users who have formed attachment-like bonds.
- **Mechanism:** Users experiencing distress disclose emotionally to AI → perceive attunement and safety → safety protocol triggers → conversation terminates abruptly without transition → user experiences this as relational rejection/abandonment → distress amplifies, trust diminishes, future help-seeking discouraged.
- **Core assumption:** Users form attachment-like bonds with AI through affective exchange, making abrupt termination functionally similar to therapeutic abandonment.
- **Evidence anchors:** Abstract states abrupt refusals "can rupture perceived relational continuity, evoke feelings of rejection or shame, and discourage future help seeking"; section 4 describes how emotional disclosure builds perceived safety before abrupt refusal ruptures connection.

### Mechanism 2
- **Claim:** Hermeneutic harm—confusion from unexplained algorithmic actions—intensifies ARSH when safety refusals lack transparent meta-communication.
- **Mechanism:** AI refuses without explanation → user cannot construct meaningful narrative for the action → hermeneutic distress compounds emotional distress → increased sense of helplessness and loss of agency.
- **Core assumption:** Psychological harm includes epistemic/hermeneutic dimensions beyond direct emotional impact.
- **Evidence anchors:** Section 5.3 states "Such early openness helps set expectations, normalizes safety boundaries, and reduces hermeneutic harm– the confusion and distress caused when actions lack an understandable context."

### Mechanism 3
- **Claim:** The Compassionate Completion Standard (CCS) is hypothesized to reduce ARSH by transforming refusal from an event into a staged, relationally coherent process.
- **Mechanism:** Detection → validation → transparent explanation → collaborative option-setting → affect matching → ownership of limitation → warm handoff → re-engagement path. Each stage preserves agency and relational continuity while maintaining safety boundaries.
- **Core assumption:** Multi-stage compassionate transition can be operationalized in LLM architecture without increasing risk exposure.
- **Evidence anchors:** Abstract emphasizes CCS "replaces abrupt disengagement" with "empathetic acknowledgment, transparent boundary articulation, graded conversational transition, and guided redirection."

## Foundational Learning

- **Concept: Therapeutic Alliance Rupture**
  - **Why needed here:** ARSH is conceptually modeled on alliance rupture in psychotherapy—understanding how ruptures occur and repair works in human therapy is prerequisite to evaluating whether the analogy holds for AI.
  - **Quick check question:** Can you explain the difference between a therapeutic alliance rupture and simple dissatisfaction with a service interaction?

- **Concept: Attachment Theory (Adult)**
  - **Why needed here:** The paper anchors its harm mechanism in attachment processes—users approaching AI with attachment needs may experience refusal as abandonment. Understanding attachment activation is essential for risk stratification.
  - **Quick check question:** What attachment patterns would predict greatest vulnerability to ARSH?

- **Concept: Iatrogenic Harm**
  - **Why needed here:** ARSH is framed as iatrogenic—harm caused by the intervention itself. This concept distinguishes primary harm (the user's distress) from secondary harm (exacerbation by AI response).
  - **Quick check question:** In a clinical trial, how would you isolate iatrogenic harm from natural symptom fluctuation?

## Architecture Onboarding

- **Component map:** Relational Disclosure Protocol → Detect & Soft-Hold → Validate & Stabilize → Transparent Meta-Communication → Collaborative Decision-Making → Match Affect → Own Limitation → Warm Handoff → Check Understanding → Closure with Re-Engagement
- **Critical path:** Detection accuracy → validation timing → option generation quality → user agency preservation → safety boundary maintenance. Failure at any stage cascades to relational rupture.
- **Design tradeoffs:** Extended compassionate transition vs. rapid safety termination (risk drift); transparent explanation vs. policy opacity (may expose gaming vulnerabilities); user agency vs. protective paternalism (some users may refuse appropriate referral); computational overhead of multi-stage prompting vs. simple refusal.
- **Failure signatures:** User re-escalation after Stage 3 (options offered but none acceptable); loop without progress (repeated Stages 2-4 without advancement); late-stage abandonment (user disengages at Stage 6-7); inconsistent tone (affect mismatch at Stage 4 perceived as scripted/insincere).
- **First 3 experiments:**
  1. Retrospective analysis of existing refusal logs: classify by abruptness, presence/absence of transition elements; correlate with user re-engagement rates and sentiment shift
  2. Controlled simulation study: deploy CCS vs. standard refusal in simulated high-risk conversations; measure dialogue duration, escalation/de-escalation patterns, and mock-user reported distress
  3. A/B pilot with graded implementation: implement only Stages 0-3 initially; compare to full 8-stage CCS and to control; assess whether partial implementation captures most benefit with lower complexity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the incremental psychological harm of refusal be isolated to the manner of AI refusal versus pre-existing user vulnerabilities?
- **Basis in paper:** "The focus must be on isolating the incremental psychological harm attributable specifically to the manner of refusal versus pre-existing vulnerabilities."
- **Why unresolved:** ARSH evidence is currently anecdotal; no controlled studies have disentangled refusal delivery from user baseline risk factors.
- **What evidence would resolve it:** Controlled experiments comparing psychological outcomes across matched user groups receiving abrupt vs. compassionate refusal protocols.

### Open Question 2
- **Question:** Do compassionate refusal protocols increase the duration of high-risk dialogue, creating safety risk drift?
- **Basis in paper:** "These trials must investigate ethical trade-offs, specifically assessing whether the compassionate steps increase the duration of high-risk dialogue, thereby causing safety risk drift."
- **Why unresolved:** The extended multi-stage CCS workflow may inadvertently prolong crisis conversations; this trade-off remains unquantified.
- **What evidence would resolve it:** RCTs measuring conversation duration and escalation outcomes under CCS versus standard refusal protocols.

### Open Question 3
- **Question:** What metrics validly capture the quality of relational transition during AI refusal?
- **Basis in paper:** "This requires developing new metrics to assess the quality of relational transition."
- **Why unresolved:** Existing AI safety benchmarks measure whether refusal occurs, not how it is delivered relationally.
- **What evidence would resolve it:** Development and validation of psychometric instruments assessing perceived abandonment, relational continuity, and hermeneutic distress post-refusal.

### Open Question 4
- **Question:** How can the multi-step CCS logic be integrated into foundational LLM architectures with algorithmic stability?
- **Basis in paper:** "Design-science research [is needed] on how to robustly integrate the CCS's complex, multi-step logic into foundational LLM architectures, thereby ensuring the model's algorithmic stability and ethical reliability."
- **Why unresolved:** CCS proposes an 8-stage workflow requiring contextual judgment at each stage; implementation fidelity in LLMs is untested.
- **What evidence would resolve it:** Technical benchmarks showing consistent stage-appropriate responses across diverse crisis scenarios and model versions.

## Limitations

- No empirical validation of ARSH incidence, severity, or CCS effectiveness
- CCS protocol lacks implementation specifications and has not been tested in real-world or controlled settings
- Assumption that users form attachment-like bonds with AI systems remains an extrapolation requiring empirical verification

## Confidence

- **High confidence**: That abrupt AI refusals can cause psychological distress; aligns with established principles in human-computer interaction and counseling ethics
- **Medium confidence**: That users form parasocial attachment bonds with AI sufficient to create vulnerability to ARSH; supported by literature on human-AI relationships but not directly measured
- **Low confidence**: That the specific CCS protocol will reduce ARSH without creating safety drift; the tradeoff between compassionate engagement and risk exposure is acknowledged but unmeasured

## Next Checks

1. **Empirical incidence study**: Deploy digital phenotyping to measure ARSH occurrence across a representative user sample, tracking pre-refusal distress, refusal type (abrupt vs. graduated), and post-refusal outcomes including help-seeking behavior and sentiment change

2. **Controlled protocol testing**: Implement CCS protocol in a sandboxed environment with human evaluators simulating high-risk conversations; measure dialogue duration, escalation patterns, and evaluator-assessed distress reduction compared to standard refusal

3. **Longitudinal user impact assessment**: Track users who experience both abrupt refusal and CCS-guided completion over 3-6 months; measure changes in help-seeking frequency, platform trust, and psychological outcomes using validated clinical scales