---
ver: rpa2
title: 'Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance
  Language Models'
arxiv_id: '2506.11116'
source_url: https://arxiv.org/abs/2506.11116
tags:
- dataset
- instruction
- data
- foundational
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of limited generalization and domain
  concentration in open-source instruction datasets for large language models (LLMs),
  which creates a performance gap compared to proprietary models. The authors introduce
  Infinity-Instruct, a scalable pipeline that constructs high-quality instruction
  datasets to enhance both foundational and conversational capabilities of LLMs.
---

# Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models

## Quick Facts
- arXiv ID: 2506.11116
- Source URL: https://arxiv.org/abs/2506.11116
- Reference count: 36
- Outperforms GPT-4-0314 by 8.6% on instruction following tasks with InfInstruct-Llama3.1-70B

## Executive Summary
Infinity Instruct addresses the generalization gap in open-source instruction datasets for large language models (LLMs) by introducing a scalable pipeline that constructs high-quality, diverse instruction datasets. The method uses a two-phase approach: curating a foundational dataset (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques, and synthesizing a conversational dataset (InfInstruct-G-1.5M) through iterative instruction labeling, evolution, and diagnostic filtering. Fine-tuning several open-source models (Mistral, LLaMA, Qwen, Yi) on Infinity-Instruct consistently outperforms official instruction-tuned counterparts. Notably, InfInstruct-Llama3.1-70B surpasses GPT-4-0314 by 8.6% on instruction following tasks while achieving comparable foundational performance, demonstrating the synergy between foundational and chat training for holistic LLM development.

## Method Summary
Infinity Instruct employs a two-stage pipeline for dataset construction and model fine-tuning. The foundational phase uses hybrid data selection (DSIR and rule-based filtering) to curate 7.4M high-quality instructions from a pool of over 100M samples, targeting reasoning and knowledge domains. The conversational phase synthesizes 1.5M instructions through iterative labeling, evolution, and diagnostic filtering based on model weaknesses. Models are fine-tuned sequentially: first on the foundational dataset to establish core capabilities, then on the conversational dataset to enhance instruction-following and dialogue abilities. This approach preserves generalization while improving both foundational and conversational performance.

## Key Results
- InfInstruct-Llama3.1-70B outperforms GPT-4-0314 by 8.6% on instruction following tasks
- Two-stage training achieves 25.5 vs 22.3 on AlpacaEval compared to one-stage
- Models fine-tuned on Infinity-Instruct consistently outperform official instruction-tuned counterparts across foundational (MMLU, MATH, HumanEval) and conversational (MT-Bench, AlpacaEval) benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Aligned Data Resampling
The pipeline uses Data Selection for Importance Resampling (DSIR) to filter raw samples, prioritizing those that align with the statistical features of high-quality target benchmarks (like GSM8K or HumanEval). This assumes that high-performance on specific benchmarks correlates with generalizable reasoning capabilities in the target domain. The break condition occurs if the target distribution does not represent real-world foundational tasks, potentially causing overfitting to benchmark-specific patterns.

### Mechanism 2: Weakness-Driven Diagnostic Synthesis
The system identifies "weak ability" labels where models fail (using GPT-4 evaluation), then prioritizes the evolution of seed instructions in those specific categories to force the model to improve on deficient capabilities. This assumes synthetic data generated by stronger models can accurately fill knowledge gaps without introducing noise. The break condition occurs if the teacher model hallucinates flaws or fails to detect valid reasoning paths, amplifying noise rather than utility.

### Mechanism 3: Synergistic Two-Stage Curriculum
Training first on "InfInstruct-F" establishes core reasoning and knowledge (preventing catastrophic forgetting), while the subsequent "InfInstruct-G" stage overlays conversational formatting and alignment. This assumes foundational reasoning is a prerequisite for high-quality conversation. The break condition occurs if the foundational dataset is too distinct from the conversational domain, causing mode collapse during the second stage.

## Foundational Learning

- **Concept: Importance Resampling (DSIR)**
  - Why needed here: The method relies on selecting a subset of data that matches a "target" distribution (e.g., high-quality math) rather than just cleaning data.
  - Quick check question: Can you explain the difference between filtering data by *quality* (e.g., grammar) versus filtering by *importance* (matching a target distribution)?

- **Concept: Instruction Evolution (Evolve-Instruct)**
  - Why needed here: The pipeline expands data complexity by rewriting simple instructions into complex multi-turn constraints.
  - Quick check question: If you evolve an instruction "Write a poem" to "Write a poem about AI in iambic pentameter including the word 'silicon'," have you changed the *intent* or just the *constraints*?

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper explicitly argues that naive instruction tuning degrades foundational abilities, justifying the two-stage approach.
  - Quick check question: Why would training a model exclusively on "chit-chat" dialogues make it worse at solving math problems?

## Architecture Onboarding

- **Component map:** Data Pool (116M) -> DSIR/Hybrid Selector -> InfInstruct-F-7.4M -> Trainer (Stage 1) -> Labeler + Evolver + Diagnostic Filter -> InfInstruct-G-1.5M -> Trainer (Stage 2)

- **Critical path:** The Diagnostic Filtering in Phase 2 is the most sensitive component. It requires running inference on the current model version, evaluating errors using a strong judge, and feeding those specific failure modes back into the synthesis engine.

- **Design tradeoffs:**
  - Scale vs. Selection: The team filters ~116M samples down to ~9M, trading breadth for stability.
  - Two-Stage vs. Mixed: Sequential training is proven more effective but doubles pipeline complexity and training time.

- **Failure signatures:**
  - Mode Collapse: If the "Evolver" rewrites instructions too aggressively, the model may learn to follow the rewriting style rather than the underlying task.
  - Convergence Loss Spikes: Removing samples with large loss differences prevents overfitting; if you see loss plateauing while accuracy drops, check for contaminated synthetic samples.

- **First 3 experiments:**
  1. Validation of the Selector: Train a small model on a random 7M subset vs. the DSIR-selected 7M subset. Compare MATH/HumanEval scores to verify the selection gain.
  2. Ablate the "Diagnosis": Create a version of the Chat dataset without the weakness-detection step. Compare against the full InfInstruct-G to quantify the value of targeted synthesis.
  3. Curriculum Check: Train on the full dataset in a single stage vs. the paper's two-stage approach. Verify if foundational scores (MMLU) drop in the single-stage run.

## Open Questions the Paper Calls Out

- **Distribution Alignment Validity:** To what extent does reliance on existing open-source models (e.g., Qwen1.5-72B) for instruction labeling introduce semantic bias or overlook nuanced instruction types not well-represented in source data? [explicit: Appendix E states labeling relies on existing models which may introduce bias]

- **Safety and Robustness:** How does fine-tuning on large-scale synthetic data impact model safety, hallucination rates, and long-term knowledge retention compared to human-curated instruction tuning? [explicit: Appendix E notes safety and long-term retention warrant further investigation]

- **Scale Diminishing Returns:** At what scale does the performance return on investment for data selection and synthesis diminish, given the computational overhead of the hybrid selection pipeline? [inferred: Figure 4 shows nearly linear scaling up to 7M samples, text mentions substantial computational resources]

- **Optimal Mixing Ratio:** What is the optimal mixing ratio between the foundational dataset (InfInstruct-F) and the conversational dataset (InfInstruct-G) to maximize instruction-following capability without degrading foundational reasoning? [inferred: Ablation shows two-stage superior but doesn't test different balances]

## Limitations

- **Distribution Alignment Validity:** The assumption that matching benchmark distributions produces generalizable reasoning capabilities is theoretically sound but unproven for open-ended tasks and may lead to overfitting to synthetic patterns.

- **Synthetic Data Quality:** The evolution and diagnostic filtering rely heavily on teacher model judgment (GPT-4, Qwen1.5-72B), which may introduce bias or hallucination that the iterative loop could amplify.

- **Pipeline Fragility:** The two-stage approach assumes the foundational dataset is clean and representative; if early-stage errors occur, the sequential dependency makes the pipeline vulnerable to catastrophic failures.

## Confidence

**High Confidence:** The two-stage training methodology outperforms single-stage and mixed-stage approaches on reported benchmarks (MMLU, GSM8K, AlpacaEval). The quantitative improvements (e.g., 8.6% over GPT-4-0314) are well-documented and reproducible given dataset release.

**Medium Confidence:** The claim that Infinity-Instruct closes the gap to proprietary models is supported by relative improvements but may depend on specific evaluation suite; proprietary models may have advantages in robustness or out-of-distribution performance not captured by standard benchmarks.

**Low Confidence:** The mechanism by which DSIR selection improves foundational generalization is not empirically validated beyond benchmark scores; the assumption that importance resampling yields better reasoning lacks ablation studies isolating selection gain from dataset scale.

## Next Checks

1. **Ablation of DSIR Selection:** Train a baseline model on a randomly sampled 7.4M subset from the 116M pool and compare foundational scores (MMLU, MATH, HumanEval) to the DSIR-selected InfInstruct-F. This isolates the selection gain from the scale benefit.

2. **Diagnostic Filtering Impact:** Train a conversational model using the full InfInstruct-G-1.5M versus a version with diagnostic filtering disabled (random evolution only). Compare AlpacaEval and MT-Bench scores to quantify the value of weakness-driven synthesis.

3. **Cross-Domain Generalization:** Evaluate the two-stage InfInstruct-Llama3.1-70B on out-of-distribution reasoning tasks (e.g., LogiQA, StrategyQA) and real-world code generation (e.g., APPS) to test whether foundational gains transfer beyond curated benchmarks.