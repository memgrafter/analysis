---
ver: rpa2
title: Evaluating SAE interpretability without explanations
arxiv_id: '2507.08473'
source_url: https://arxiv.org/abs/2507.08473
tags:
- intruder
- interpretability
- latent
- examples
- latents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce two new methods for evaluating the interpretability
  of sparse autoencoders (SAEs) without generating natural language explanations.
  The first method, intruder detection, presents evaluators with five sentences (four
  activating a given latent, one not) and measures how accurately they can identify
  the non-activating example.
---

# Evaluating SAE interpretability without explanations

## Quick Facts
- arXiv ID: 2507.08473
- Source URL: https://arxiv.org/abs/2507.08473
- Authors: GonÃ§alo Paulo; Nora Belrose
- Reference count: 6
- Primary result: Two new SAE interpretability evaluation methods introduced that don't require natural language explanations

## Executive Summary
This paper introduces two novel methods for evaluating the interpretability of sparse autoencoders (SAEs) without generating natural language explanations. The authors propose an intruder detection task where evaluators identify which of five sentences does not activate a given latent, and an example embedding scoring method that uses embedding models to cluster activating versus non-activating examples. These approaches aim to provide more direct and standardized evaluation compared to traditional explanation-based methods. The methods were validated using both human participants and LLM evaluators, showing moderate but meaningful correlations.

## Method Summary
The authors present two methods for evaluating SAE interpretability without explanations. The first, intruder detection, presents evaluators with five sentences where four activate a specific latent and one does not, measuring accuracy in identifying the non-activating example. The second, example embedding scoring, uses small embedding models to compute similarity between activating and non-activating example sets in vector space, with interpretability measured by how well they cluster. Human participants achieved 65% accuracy on the intruder task across 56 latents, while LLM evaluators (Claude Sonnet 3.5) achieved 84% correlation with human judgments. The authors also found that interpretability varies significantly across activation deciles, with higher deciles being more interpretable.

## Key Results
- Human participants achieved 65% accuracy on intruder detection task across 56 latents
- LLM evaluators achieved 84% correlation with human judgments
- Example embedding scoring showed 48% correlation with human judgments
- Higher activation deciles showed significantly better interpretability

## Why This Works (Mechanism)
The methods work by directly testing whether humans can identify semantic differences between activating and non-activating examples without requiring intermediate explanations. The intruder detection task leverages human ability to detect outliers in semantic content, while example embedding scoring uses learned representations to quantify semantic similarity. Both methods bypass the potential noise and subjectivity introduced by natural language explanations, potentially providing more direct measures of whether latents capture meaningful, interpretable features.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural network components used for feature extraction in large language models, requiring interpretability evaluation
  - Why needed: SAEs are increasingly used for mechanistic interpretability, but their feature quality is hard to assess
  - Quick check: Can identify activating/non-activating examples for a given latent

- **Latent Activation Deciles**: Stratification of latents by activation strength to analyze interpretability across different activation levels
  - Why needed: Activation strength may correlate with interpretability, revealing patterns in feature quality
  - Quick check: Higher deciles show 78% human accuracy vs 65% overall

- **Embedding-Based Similarity**: Using vector representations to quantify semantic similarity between examples
  - Why needed: Provides a scalable, automated way to measure interpretability without human judgment
  - Quick check: 48% correlation with human judgments suggests moderate alignment

## Architecture Onboarding

**Component Map**: Intruder Task (Human/LLM Evaluators) -> Accuracy/ Correlation Scores -> Interpretability Assessment
Example Embedding Scoring -> Vector Similarity Metrics -> Interpretability Assessment

**Critical Path**: SAE Latent -> Example Sets (Activating/Non-activating) -> Evaluation Method (Intruder/Embedding) -> Interpretability Score

**Design Tradeoffs**: Human evaluation provides ground truth but is slow and expensive; LLM evaluation is faster but may inherit biases; embedding scoring is fastest but has lower correlation with human judgment.

**Failure Signatures**: Low accuracy on intruder task suggests latents don't capture meaningful features; poor embedding clustering indicates feature representations aren't semantically coherent; inconsistent results across deciles may reveal activation threshold issues.

**3 First Experiments**:
1. Run intruder detection on 10 latents from different SAE models to establish baseline performance
2. Compare embedding scoring results across different embedding model sizes to find optimal configuration
3. Test interpretability correlation across multiple activation threshold settings to determine sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Human judgment of semantic similarity may vary based on individual interpretation and cultural context
- The example embedding scoring method assumes embedding-based similarity captures human interpretability judgments without empirical validation
- The claim that these methods are "standardized" lacks full specification of evaluation protocols

## Confidence

**High Confidence**: The observation that higher activation deciles show greater interpretability is supported by both human and LLM evaluation methods. The computational efficiency claims for example embedding scoring are straightforward and verifiable.

**Medium Confidence**: The correlation between human and LLM evaluator judgments (84%) suggests the LLM can serve as a reasonable proxy, but the relatively small sample size (56 latents) limits generalizability. The claim that these methods are more "direct" than explanation-based approaches needs further justification.

**Low Confidence**: The assertion that these methods are "standardized" is not yet supported, as the evaluation protocol details are not fully specified. The claim about example embedding scoring providing "meaningful" results at 48% correlation is questionable given the low absolute value.

## Next Checks

1. Test the intruder detection method with a larger and more diverse set of latents (minimum 200) across multiple domains to establish robustness and generalizability of the 65% accuracy finding.

2. Conduct a controlled study comparing embedding-based similarity metrics against human similarity judgments on the same example sets to validate the assumption underlying example embedding scoring.

3. Perform ablation studies varying the activation threshold and number of examples shown in the intruder task to determine optimal parameters for reliable interpretability assessment.