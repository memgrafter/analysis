---
ver: rpa2
title: 'RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering
  with Large Language Models'
arxiv_id: '2509.03995'
source_url: https://arxiv.org/abs/2509.03995
tags:
- answer
- question
- temporal
- visit
- rtqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RTQA is a training-free temporal knowledge graph question answering
  framework that addresses complex queries by recursively decomposing questions into
  sub-problems and solving them bottom-up using LLMs and TKG facts. The method employs
  multi-path answer aggregation to mitigate error propagation, achieving significant
  performance gains on MultiTQ and TimelineKGQA benchmarks.
---

# RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2509.03995
- Source URL: https://arxiv.org/abs/2509.03995
- Reference count: 24
- Primary result: Training-free TKGQA framework achieving 0.765 Hits@1 on MultiTQ

## Executive Summary
RTQA introduces a novel training-free approach for complex Temporal Knowledge Graph Question Answering that recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and retrieved TKG facts, and employs multi-path answer aggregation to mitigate error propagation. The method demonstrates significant performance gains on complex temporal queries by leveraging the reasoning capabilities of LLMs while grounding answers in retrieved facts. RTQA achieves state-of-the-art performance on MultiTQ and TimelineKGQA benchmarks, particularly excelling on complex and multi-entity queries.

## Method Summary
RTQA operates as a training-free pipeline that recursively decomposes temporal questions into sub-problems, retrieves relevant TKG facts for each sub-question, and solves them bottom-up using LLM reasoning. The framework employs a Temporal Question Decomposer to generate a recursive query tree, a Relevant Facts Retriever to gather top-K TKG quadruples, a Recursive Solver to process sub-questions in post-order, and an Answer Aggregator to combine direct reasoning results with child answers. The method uses few-shot prompting for decomposition and retrieval-augmented LLM reasoning for solving, with multi-path answer aggregation to improve fault tolerance.

## Key Results
- Achieves 0.765 Hits@1 on MultiTQ test set, surpassing state-of-the-art models
- Shows 19.6% improvement on Multiple question types through multi-path aggregation
- Demonstrates strong generalizability across different LLMs (gpt-4o, deepseek-r1)
- Effectively handles multi-granular temporal reasoning with significant performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive decomposition improves handling of compound temporal constraints by isolating implicit dependencies.
- Mechanism: The Temporal Question Decomposer converts implicit constraints into explicit sub-questions, creating a dependency tree where placeholders link child answers to parent queries, reducing LLM cognitive load.
- Core assumption: LLM can correctly identify and sequence temporal dependencies when prompted with few-shot examples.
- Evidence anchors: [abstract] recursive decomposition into sub-problems; [section 4.2] formalization of decomposition as sequence of transformations; [corpus] comparison with multi-hop TKGQA methods.

### Mechanism 2
- Claim: Bottom-up recursive solving with TKG grounding reduces hallucination by constraining LLM reasoning to retrieved facts.
- Mechanism: Recursive Solver traverses decomposition tree in post-order, retrieves top-K relevant TKG quadruples, converts them to natural language statements, and prompts LLM to reason over these facts.
- Core assumption: Dense retriever (BGE-M3) surfaces relevant facts within top-50 results.
- Evidence anchors: [section 4.3] solver retrieves facts and invokes LLM; [section 5.3] ablation shows catastrophic performance drop without fact retrieval; [corpus] validation from Plan of Knowledge approach.

### Mechanism 3
- Claim: Multi-path answer aggregation mitigates error propagation by providing alternative reasoning paths.
- Mechanism: For non-atomic questions, RTQA maintains IR_answer (direct LLM reasoning over retrieved facts) and child_answer (aggregated from sub-question results), with Aggregator selecting most plausible.
- Core assumption: Errors in child sub-questions are partially independent from errors in direct reasoning.
- Evidence anchors: [abstract] employs multi-path answer aggregation to improve fault tolerance; [section 4.4] formalization of aggregation process; [section 5.3] ablation shows 1.7% overall drop, larger on Multiple questions.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs) as quadruples (s, p, o, t)
  - Why needed here: RTQA's retrieval and reasoning operate over time-anchored facts; understanding quadruple structure is essential for interpreting retrieved evidence.
  - Quick check question: Given (Trump, president_of, United States, 2025), what does the timestamp represent?

- Concept: Post-order tree traversal (bottom-up processing)
  - Why needed here: Recursive Solver processes child nodes before parents, enabling placeholder substitution; understanding order clarifies execution flow.
  - Quick check question: In a tree with Root → [Sub1, Sub2], which node is processed first?

- Concept: Dense retrieval with embedding similarity
  - Why needed here: Relevant Facts Retriever uses BGE-M3 embeddings and FAISS; engineers need to understand how top-K selection affects context quality.
  - Quick check question: Why might increasing K beyond 50 hurt accuracy (Table 6)?

## Architecture Onboarding

- Component map: Temporal Question Decomposer → Relevant Facts Retriever → Recursive Solver → Answer Aggregator

- Critical path:
  1. Question → Decomposer → Tree T
  2. For each leaf node: Retrieve → LLM reason → Answer
  3. For each non-leaf: Process children → Aggregate child_answer → Retrieve → LLM reason → IR_answer → Aggregate with child_answer
  4. Root aggregation → Final answer

- Design tradeoffs:
  - Retrieval context length (n=50): Higher n improves recall but adds noise (Table 6)
  - LLM choice: Stronger models (gpt-4o, deepseek-r1) improve reasoning but increase cost
  - Decomposition depth: Deeper trees handle more complex questions but increase API calls (avg 3.96–5.38)

- Failure signatures:
  - Decomposition errors: Illogical sub-questions → unexecutable tree
  - Retrieval errors: Missing key facts → wrong answers despite correct reasoning
  - Aggregation failures: Both IR_answer and child_answer wrong → final error
  - Temporal reasoning failures: LLM hallucinates on compound constraints

- First 3 experiments:
  1. Reproduce Hits@1 on MultiTQ test set with gpt-4o-mini; compare against Table 1 baselines
  2. Ablation: Remove multi-answer aggregation (w/o multi-answer variant); verify ~2% overall drop, larger on Multiple questions
  3. Stress test retrieval: Vary n (10, 30, 50, 60) on 1k sample; observe recall vs. accuracy tradeoff per Table 6

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the RTQA framework be adapted to effectively handle domains other than Temporal Knowledge Graphs, such as static KGs or unstructured text?
- Basis in paper: [explicit] Limitations section states method is tailored for TKGQA and "applicability to other QA domains has yet to be thoroughly validated."
- Why unresolved: Recursive solver and decomposition logic heavily rely on time-centric operators that may not translate directly to non-temporal reasoning tasks.
- What evidence would resolve it: Successful application on standard static KGQA benchmarks (MetaQA, WebQuestionsSP) or TextQA datasets without extensive structural modification.

### Open Question 2
- Question: How can the framework be made robust against failures in the initial fact retrieval phase?
- Basis in paper: [explicit] Limitations section notes "RTQA relies on a robust retriever" and failure to retrieve key facts "significantly reduce[s] the reasoning accuracy."
- Why unresolved: Recursive Solver depends entirely on top-K facts provided to LLM context; if dense retriever misses crucial quadruple, reasoning chain breaks immediately.
- What evidence would resolve it: Study analyzing RTQA performance under "retrieval noise" or "recall drops," or integration of feedback loop for re-retrieval when confidence is low.

### Open Question 3
- Question: Can the question decomposition strategy be effectively deployed using smaller, open-source LLMs without significant performance degradation?
- Basis in paper: [explicit] Limitations section highlights "Smaller models may struggle to generate high-quality sub-questions," creating dependency on large, proprietary models.
- Why unresolved: "Training-free" approach relies on LLM's inherent instruction-following capability to output valid JSON decomposition trees, typically difficult for models with fewer than 7-10 billion parameters.
- What evidence would resolve it: Benchmarks showing RTQA performance when driven by smaller models (LLaMA-7B, Mistral-7B) potentially aided by distillation or specialized prompting.

### Open Question 4
- Question: Is it possible to automate the prompt engineering for decomposition to remove reliance on manually crafted examples?
- Basis in paper: [inferred] Section 4.2 mentions prompts constructed using "5–10 question examples... manually crafted," suggesting potential scalability bottleneck for new question types.
- Why unresolved: Current effectiveness depends on human effort to categorize question types (e.g., "Equal Multi," "Before Last") and write specific decompositions for them.
- What evidence would resolve it: Comparative study where few-shot examples are dynamically generated or retrieved rather than manually fixed, achieving comparable Hits@1 scores.

## Limitations
- Heavy dependency on large LLMs for decomposition and reasoning tasks
- Sensitive to quality and completeness of retrieved TKG facts
- Limited evaluation to ICEWS-based datasets, raising questions about domain generalization
- No analysis of computational costs for multiple LLM calls per complex query

## Confidence

- **High Confidence**: Overall pipeline design (recursive decomposition + bottom-up solving + multi-path aggregation) is sound and well-supported by ablation studies; reported performance gains on standard benchmarks are consistent and significant.
- **Medium Confidence**: Specific implementation details (exact prompts, retrieval thresholds, aggregation rules) are crucial for replication but require access to full repository and documentation for faithful reproduction.
- **Low Confidence**: Paper does not address scalability to larger TKGs or computational cost of multiple LLM calls per complex query, which could limit practical deployment.

## Next Checks

1. **Prompt Fidelity Check**: Obtain and validate complete set of few-shot examples for Temporal Question Decomposer against paper's reported performance on MultiTQ Complex questions.

2. **Retrieval Robustness Test**: Conduct controlled experiment varying number of retrieved facts (n=10, 30, 50, 60) on held-out MultiTQ sample to quantify recall-accuracy tradeoff and identify optimal n.

3. **Domain Generalization Experiment**: Apply RTQA to different temporal knowledge graph (e.g., GDELT) and evaluate Hits@1 on held-out set of complex questions to assess robustness beyond ICEWS.