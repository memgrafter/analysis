---
ver: rpa2
title: 'BERnaT: Basque Encoders for Representing Natural Textual Diversity'
arxiv_id: '2512.03903'
source_url: https://arxiv.org/abs/2512.03903
tags:
- language
- standard
- diverse
- basque
- corpora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that language models should capture the full
  spectrum of language variation, including dialectal, historical, and informal varieties,
  rather than relying solely on standardized text. Focusing on Basque, a morphologically
  rich low-resource language, the authors construct new corpora combining standard,
  social media, and historical sources, and pre-train the BERnaT family of encoder-only
  models in three configurations: standard, diverse, and combined.'
---

# BERnaT: Basque Encoders for Representing Natural Textual Diversity

## Quick Facts
- arXiv ID: 2512.03903
- Source URL: https://arxiv.org/abs/2512.03903
- Reference count: 0
- Language models trained on both standard and diverse data consistently outperform standard-only models across all task types

## Executive Summary
This paper argues that language models should capture the full spectrum of language variation, including dialectal, historical, and informal varieties, rather than relying solely on standardized text. Focusing on Basque, a morphologically rich low-resource language, the authors construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. They propose an evaluation framework that separates NLU tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy.

## Method Summary
The authors construct new corpora by combining standard text sources with social media and historical data to create diverse linguistic datasets. They pre-train three variants of the BERnaT encoder-only models: one on standard corpora only, one on diverse corpora only, and one on a combined dataset. The evaluation framework separates natural language understanding tasks into standard and diverse subsets to measure both conventional performance and linguistic generalization. The models are evaluated across multiple sizes (1.1B and 2.2B parameters) and the impact of data diversity is assessed under different fine-tuning data availability conditions.

## Key Results
- Models trained on both standard and diverse data outperform standard-only models across all task types
- Benefits of including diverse data are most pronounced when fine-tuning data is scarce
- Larger models benefit more from data diversity than smaller models

## Why This Works (Mechanism)
The inclusion of diverse linguistic data during pre-training helps language models develop broader linguistic representations that generalize better to varied real-world text. By exposing models to historical, informal, and dialectal variations during training, they develop more robust feature representations that can handle linguistic diversity encountered during inference. This prevents the model from overfitting to standardized text patterns and improves its ability to handle out-of-distribution inputs.

## Foundational Learning
- **Morphological richness**: Basque has complex morphological structures that require models to learn rich feature representations
  - Why needed: To handle the agglutinative nature of Basque where words carry multiple grammatical features
  - Quick check: Compare performance on morphologically complex vs simple words

- **Low-resource language modeling**: Limited available training data necessitates careful corpus construction and efficient use of all available linguistic resources
  - Why needed: Basque lacks the massive web-scale corpora available for high-resource languages
  - Quick check: Measure performance gains relative to training data size

- **Linguistic diversity evaluation**: Separating tasks into standard and diverse categories allows measurement of generalization beyond conventional benchmarks
  - Why needed: Standard benchmarks may not capture a model's ability to handle real-world linguistic variation
  - Quick check: Compare performance gap between standard and diverse task subsets

## Architecture Onboarding
**Component map**: Corpus construction -> Model pre-training -> Fine-tuning -> Evaluation (standard tasks + diverse tasks)

**Critical path**: Diverse corpus construction → Combined pre-training → Fine-tuning on limited data → Evaluation on both standard and diverse tasks

**Design tradeoffs**: Using encoder-only architecture trades generation capability for efficiency and focus on representation learning, which is appropriate for NLU tasks but limits potential applications.

**Failure signatures**: Over-reliance on standard data leads to poor performance on informal or historical text; insufficient diverse data results in inability to generalize to linguistic variations.

**First experiments**:
1. Compare BERnaT standard vs combined model performance on a held-out diverse text sample
2. Test fine-tuning performance with 10%, 50%, and 100% of training data using combined vs standard models
3. Measure perplexity on standard vs diverse test sets to quantify distributional shift handling

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies on a relatively small number of diverse evaluation tasks (3 standard + 3 diverse) to draw conclusions about linguistic generalization
- The study focuses exclusively on Basque, which may limit generalizability to other languages with different typological profiles
- The definition of "diverse" data is based on specific sources without systematic characterization of linguistic features

## Confidence
High confidence in the core finding that models trained on both standard and diverse data outperform standard-only models across all task types, as this is supported by consistent results across multiple experimental conditions and model sizes.

Medium confidence in the claim that benefits are most pronounced when fine-tuning data is scarce, as this is demonstrated through limited ablation studies and may depend heavily on the specific fine-tuning tasks used.

Medium confidence in the claim that larger models benefit more from data diversity, as the comparison is limited to two model sizes (1.1B and 2.2B parameters) and the relationship may not be linear.

## Next Checks
1. Expand the evaluation framework to include a broader range of diverse linguistic phenomena (e.g., regional dialects, register variations, different historical periods) and test whether the observed benefits extend to these additional dimensions of linguistic diversity.

2. Conduct systematic linguistic analysis of the diverse datasets to characterize the specific linguistic features they contain (morphological complexity, syntactic variation, lexical diversity) and correlate these features with model performance improvements.

3. Replicate the study with another morphologically rich low-resource language (e.g., Georgian or Navajo) to test the generalizability of findings beyond Basque and examine whether the relationship between model size and benefit from diverse data holds across languages with different typological characteristics.