---
ver: rpa2
title: Predicting Biased Human Decision-Making with Large Language Models in Conversational
  Settings
arxiv_id: '2601.11049'
source_url: https://arxiv.org/abs/2601.11049
tags:
- dialogue
- framing
- human
- choice
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can simulate human decision-making
  in conversational settings, including well-documented cognitive biases like Framing
  and Status Quo effects. In a pre-registered study (N = 1,648), participants completed
  six classic decision-making tasks via a chatbot with dialogues of varying complexity.
---

# Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings

## Quick Facts
- arXiv ID: 2601.11049
- Source URL: https://arxiv.org/abs/2601.11049
- Reference count: 40
- Primary result: LLM context-aware predictions match human bias patterns better than demographics-only models.

## Executive Summary
This pre-registered study (N = 1,648) demonstrates that large language models can accurately simulate human decision-making patterns, including cognitive biases like Framing and Status Quo effects, in conversational chatbot settings. The research reveals a load-bias interaction where increased dialogue complexity amplifies framing effects, while LLM experiments show that context-aware models outperform demographic-only predictions. GPT-4.1 consistently demonstrated superior predictive accuracy and human-like bias patterns compared to other models, establishing LLMs as promising tools for simulating context-sensitive human behavior in conversational agents.

## Method Summary
The study employed a mixed-methods approach combining human experiments with LLM evaluations. Human participants (N = 1,648) completed six classic decision-making tasks through a chatbot interface featuring dialogues of varying complexity. Participants' choices were recorded alongside mental demand ratings. Simultaneously, multiple LLM variants (GPT-4.1, GPT-5, and open-source models) were tested using both demographic-only inputs and context-aware inputs that incorporated dialogue history. Predictive accuracy was measured against human choices, with particular attention to bias manifestation patterns across different task types including Goal Framing and Investment Decision Making.

## Key Results
- Increased dialogue complexity led to higher mental demand and selectively amplified Framing effects, demonstrating a load-bias interaction
- LLM models incorporating dialogue context predicted individual decisions more accurately than demographic-only approaches
- GPT-4.1 consistently outperformed GPT-5 and open-source models in both predictive accuracy and fidelity to human-like bias patterns

## Why This Works (Mechanism)
Assumption: The load-bias interaction occurs because increased cognitive load reduces deliberative processing capacity, making individuals more susceptible to superficial framing cues. Context-aware LLMs may better simulate this by maintaining dialogue history that captures the cognitive burden experienced by human participants. Unknown: The specific neural or computational mechanisms through which LLMs encode and replicate human bias patterns remain unclear.

## Foundational Learning
1. **Cognitive Load Theory** - Understanding how mental demand affects decision-making; needed to explain load-bias interaction; quick check: measure cognitive load in chatbot interactions
2. **Framing Effect** - How presentation of options influences choices; needed for designing decision tasks; quick check: vary framing in controlled experiments
3. **LLM Context Integration** - Models using conversation history perform better than static demographic models; needed for accurate human behavior simulation; quick check: compare context-aware vs context-free predictions

## Architecture Onboarding
**Component Map**: Human Decision Tasks -> LLM Prediction Models -> Bias Pattern Analysis -> Accuracy Validation
**Critical Path**: Participant Decision → Context Capture → LLM Processing → Bias Pattern Matching → Prediction Accuracy
**Design Tradeoffs**: Context-rich models provide better accuracy but require more computational resources and careful prompt engineering
**Failure Signatures**: Poor performance on complex dialogues, failure to capture bias patterns, demographic-only predictions missing context effects
**First Experiments**: 1) Test load-bias interaction with different complexity levels, 2) Compare context-aware vs demographic-only predictions across bias types, 3) Validate GPT-4.1 superiority on new decision contexts

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions, but potential areas for investigation include how different LLM architectures capture bias patterns and whether these findings generalize to non-Western populations.

## Limitations
- Chatbot interface is a simplified representation of real-world decision contexts
- Online sample recruitment may introduce selection bias limiting generalizability
- Focus on specific biases (Framing, Status Quo) leaves uncertainty about other bias types
- GPT-4.1 performance advantage demonstrated but mechanisms unclear

## Confidence
- High confidence: Load-bias interaction effect is well-supported by experimental design
- Medium confidence: LLM predictive accuracy shows promise but needs broader validation
- Medium confidence: GPT-4.1 performance advantage demonstrated but mechanisms unclear

## Next Checks
1. Replicate load-bias interaction findings using real-world decision scenarios with consequential outcomes
2. Test LLM predictive capabilities across broader range of cognitive biases and decision contexts
3. Conduct cross-cultural validation to determine pattern generalizability across different populations