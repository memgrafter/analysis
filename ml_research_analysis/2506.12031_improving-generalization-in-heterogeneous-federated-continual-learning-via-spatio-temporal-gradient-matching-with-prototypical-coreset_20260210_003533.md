---
ver: rpa2
title: Improving Generalization in Heterogeneous Federated Continual Learning via
  Spatio-Temporal Gradient Matching with Prototypical Coreset
arxiv_id: '2506.12031'
source_url: https://arxiv.org/abs/2506.12031
tags:
- gradient
- learning
- task
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated continual learning (FCL) in heterogeneous
  settings where clients have non-identical and potentially conflicting tasks, leading
  to catastrophic forgetting and gradient conflicts. The authors propose STAMP, a
  model-agnostic method that combines spatio-temporal gradient matching with network-free
  prototypical coresets.
---

# Improving Generalization in Heterogeneous Federated Continual Learning via Spatio-Temporal Gradient Matching with Prototypical Coreset

## Quick Facts
- **arXiv ID:** 2506.12031
- **Source URL:** https://arxiv.org/abs/2506.12031
- **Reference count:** 40
- **Primary result:** STAMP outperforms existing baselines in federated continual learning with higher accuracy and lower forgetting under data heterogeneity.

## Executive Summary
This paper introduces STAMP, a novel federated continual learning method designed to address the challenges of heterogeneous and non-identical tasks across clients. By combining spatio-temporal gradient matching with a network-free prototypical coreset strategy, STAMP effectively mitigates catastrophic forgetting and reduces inter-client gradient conflicts. The approach is model-agnostic, memory-efficient, and demonstrates strong performance across various data heterogeneity settings.

## Method Summary
STAMP integrates two key strategies: temporal gradient matching on clients to prevent forgetting of past tasks, and spatial gradient matching on the server to reconcile heterogeneous task gradients. A prototypical coreset selection mechanism approximates historical gradients without relying on memory-intensive generative replay. This allows efficient and scalable federated continual learning, even under severe data heterogeneity, while keeping communication and memory overhead low.

## Key Results
- STAMP achieves higher accuracy and lower forgetting compared to existing federated continual learning baselines.
- The method is robust across varying levels of data heterogeneity on CIFAR100 and ImageNet1K.
- STAMP maintains low communication and memory costs, making it practical for resource-constrained devices.

## Why This Works (Mechanism)
STAMP's effectiveness stems from its dual gradient matching strategy: temporal matching preserves knowledge of past tasks on each client, while spatial matching harmonizes conflicting gradients from heterogeneous clients at the server. The prototypical coreset efficiently summarizes historical gradients, enabling gradient-based regularization without storing raw data or relying on generative models. This combination addresses both forgetting and gradient conflicts in heterogeneous federated continual learning.

## Foundational Learning
- **Federated Continual Learning (FCL):** Learning a shared model across distributed clients in a sequential, task-based manner. *Why needed:* Enables collaborative learning without centralizing data. *Quick check:* Is the model updated after each client's local training?
- **Catastrophic Forgetting:** Loss of previously learned knowledge when adapting to new tasks. *Why needed:* A central challenge in continual learning. *Quick check:* Does accuracy on earlier tasks drop after learning new ones?
- **Gradient Matching:** Aligning gradients from different sources to reduce conflicts. *Why needed:* Critical for harmonizing heterogeneous client updates. *Quick check:* Are gradients from different tasks/domains explicitly compared or regularized?
- **Prototypical Coreset:** A small, representative subset of data used to approximate the full dataset's gradients. *Why needed:* Reduces memory and communication costs. *Quick check:* Is a subset of historical data used instead of full replay?
- **Heterogeneous Tasks:** Clients have non-identical or conflicting objectives. *Why needed:* Reflects real-world federated learning scenarios. *Quick check:* Are task distributions or data distributions varied across clients?
- **Communication Efficiency:** Minimizing data exchanged between clients and server. *Why needed:* Essential for scalability and practicality. *Quick check:* Are model updates or gradients sent instead of raw data?

## Architecture Onboarding
- **Component Map:** Clients (local training + temporal gradient matching) -> Server (spatial gradient matching + prototypical coreset selection)
- **Critical Path:** Local training on clients -> Gradient matching (temporal) -> Upload gradients to server -> Spatial gradient matching and coreset selection -> Global model update
- **Design Tradeoffs:** Memory vs. performance (prototypical coreset reduces memory at some accuracy cost), communication vs. convergence (gradient-based updates minimize communication)
- **Failure Signatures:** Increased forgetting indicates weak temporal matching; poor performance under heterogeneity suggests inadequate spatial matching or coreset selection
- **First 3 Experiments:** 1) Test accuracy on CIFAR100/ImageNet1K under homogeneous tasks; 2) Measure forgetting as tasks increase; 3) Vary data heterogeneity and assess robustness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Experimental validation is limited to image classification (CIFAR100 and ImageNet1K), leaving performance on other modalities or tasks unclear.
- Scalability to very large numbers of clients or tasks has not been thoroughly explored.
- Computational overhead on resource-constrained devices is incompletely characterized.

## Confidence
- **High:** Effectiveness of gradient matching for mitigating catastrophic forgetting; utility of prototypical coresets for efficient gradient approximation.
- **Medium:** Claims about generalization to non-image modalities or complex sequential decision-making problems.

## Next Checks
1. Validate STAMP's performance on non-image modalities (e.g., text, speech) and sequential decision-making tasks.
2. Assess scalability by increasing the number of clients and tasks in experiments.
3. Measure and characterize computational overhead on actual resource-constrained devices.