---
ver: rpa2
title: 'Tracking World States with Language Models: State-Based Evaluation Using Chess'
arxiv_id: '2508.19851'
source_url: https://arxiv.org/abs/2508.19851
tags:
- state
- states
- metrics
- language
- chess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a model-agnostic framework for evaluating
  state-tracking in language models using chess as a benchmark. The method analyzes
  the legal move distributions (state affordances) from predicted versus actual game
  states to estimate semantic fidelity, providing a more meaningful evaluation than
  conventional string-based metrics.
---

# Tracking World States with Language Models: State-Based Evaluation Using Chess

## Quick Facts
- **arXiv ID**: 2508.19851
- **Source URL**: https://arxiv.org/abs/2508.19851
- **Reference count**: 22
- **Primary result**: Novel framework evaluates state-tracking in LLMs using chess, revealing that even GPT-4o struggles with maintaining coherent internal models over long sequences

## Executive Summary
This paper introduces a model-agnostic framework for evaluating state-tracking in language models using chess as a benchmark. The method analyzes legal move distributions (state affordances) from predicted versus actual game states to estimate semantic fidelity, providing a more meaningful evaluation than conventional string-based metrics. Experiments demonstrate that the proposed metrics capture deficiencies in state-tracking, revealing that even powerful models like GPT-4o struggle with maintaining coherent internal models over long sequences.

The framework offers a robust tool for assessing structured reasoning in language models without requiring internal model access, and generalizes to symbolic environments. By focusing on semantic state fidelity rather than exact string matching, this approach addresses a critical gap in current LLM evaluation methodologies and provides insights into model capabilities for maintaining coherent representations of world states.

## Method Summary
The framework evaluates state-tracking by comparing the legal move distributions generated by a language model against the actual game state. For each position in a chess game, the model predicts moves, and these predictions are compared to the ground truth legal moves. The semantic fidelity metric calculates the overlap between predicted and actual legal move sets, capturing how well the model maintains an accurate internal representation of the game state. This approach moves beyond traditional exact match metrics to assess the quality of the model's world state understanding.

The method is model-agnostic and works by analyzing the output of any autoregressive language model without requiring access to its internal states. It leverages the structured nature of chess, where legal moves are well-defined and can be enumerated for any given board position. The framework tracks how well models maintain this state information across long sequences, revealing degradation patterns that conventional metrics miss.

## Key Results
- The proposed semantic fidelity metrics effectively capture state-tracking deficiencies in language models
- Even GPT-4o shows significant degradation in maintaining coherent internal models over long chess sequences
- State-based evaluation provides more meaningful insights than conventional string-based metrics for assessing world state understanding

## Why This Works (Mechanism)
The framework works by leveraging the discrete, structured nature of chess to create a ground truth state space. Each board position has a well-defined set of legal moves, which serves as a semantic fingerprint of the game state. When a language model predicts moves, comparing its predicted legal move distribution against the actual distribution reveals whether it's maintaining an accurate internal representation. This approach captures semantic fidelity rather than surface-level string matching, making it more robust to paraphrasing and alternative move orderings that represent the same underlying state.

## Foundational Learning

**Chess state representation**: Understanding how board positions are encoded and legal moves are enumerated
- Why needed: Forms the ground truth state space for evaluation
- Quick check: Can you list all legal moves for a given chess position?

**Legal move distribution analysis**: Computing overlap between predicted and actual legal move sets
- Why needed: Core metric for semantic fidelity assessment
- Quick check: Can you calculate the Jaccard similarity between two move sets?

**Autoregressive language modeling**: How models generate sequences token by token
- Why needed: Framework applies to any autoregressive model output
- Quick check: Can you trace through a model's generation process step-by-step?

**Semantic fidelity metrics**: Moving beyond exact match to assess meaning preservation
- Why needed: Provides more robust evaluation than string-based metrics
- Quick check: Can you explain why exact match fails for paraphrasing?

**State-tracking degradation**: Understanding how models lose coherence over long sequences
- Why needed: Reveals model limitations in maintaining world states
- Quick check: Can you identify patterns of degradation in long generation sequences?

## Architecture Onboarding

**Component map**: Chess dataset -> Language model -> Move predictions -> Legal move enumeration -> Semantic fidelity calculation -> Evaluation metrics

**Critical path**: The framework's critical path involves generating predictions, enumerating legal moves for both predicted and actual states, computing distribution overlaps, and aggregating these into overall semantic fidelity scores. This sequence must be maintained to preserve the evaluation's integrity.

**Design tradeoffs**: The framework trades computational efficiency for semantic depth, requiring full legal move enumeration at each step. This is computationally intensive but provides richer insights than simpler metrics. The model-agnostic approach sacrifices the ability to probe internal states for the benefit of broad applicability.

**Failure signatures**: Models typically show initial strong performance followed by gradual degradation in semantic fidelity scores as sequences lengthen. Early failures often manifest as missing key defensive moves or failing to maintain piece coordination patterns.

**First experiments**:
1. Evaluate a simple rule-based chess engine to establish baseline semantic fidelity scores
2. Test the framework on a small language model (e.g., 1-7B parameters) to compare with larger models
3. Run ablation studies removing contextual information to measure state-tracking degradation

## Open Questions the Paper Calls Out

None

## Limitations

The framework's effectiveness depends on the availability of comprehensive move datasets and assumes that legal move distributions accurately reflect semantic state fidelity. While chess provides a well-defined environment with clear ground truth states, the method's generalizability to more complex, open-ended domains remains to be fully established. The current evaluation focuses on autoregressive models and may not directly translate to other architectures like retrieval-augmented or non-autoregressive systems.

## Confidence

**High confidence**: The core methodology for using legal move distributions as semantic fidelity metrics
**Medium confidence**: Generalizability to other symbolic environments beyond chess
**Medium confidence**: Claims about state-tracking deficiencies in powerful models like GPT-4o

## Next Checks

1. Test the framework on other discrete symbolic environments (e.g., Go, Checkers, or simple grid-worlds) to validate generalizability
2. Conduct ablation studies comparing state-tracking performance across different model architectures and parameter scales
3. Evaluate the correlation between state-tracking ability and performance on downstream reasoning tasks to establish practical relevance