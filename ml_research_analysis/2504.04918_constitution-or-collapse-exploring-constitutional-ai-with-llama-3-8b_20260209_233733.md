---
ver: rpa2
title: Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B
arxiv_id: '2504.04918'
source_url: https://arxiv.org/abs/2504.04918
tags:
- constitutional
- llama
- response
- bank
- anthropic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper replicates Constitutional AI (CAI) on a smaller 8B parameter
  model (Llama 3-8B), contrasting with Anthropic's original 52B parameter implementation.
  The approach uses AI-generated feedback to reduce reliance on human-labeled data,
  improving harmlessness by 40.8% (reducing Attack Success Rate in MT-Bench).
---

# Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B

## Quick Facts
- arXiv ID: 2504.04918
- Source URL: https://arxiv.org/abs/2504.04918
- Reference count: 3
- Primary result: Constitutional AI reduces harmfulness by 40.8% in Llama 3-8B but causes 9.8% helpfulness loss and model collapse

## Executive Summary
This paper replicates Constitutional AI on a smaller 8B parameter model (Llama 3-8B), contrasting with Anthropic's original 52B parameter implementation. The approach uses AI-generated feedback to reduce reliance on human-labeled data, improving harmlessness by 40.8% but at the cost of a 9.8% drop in helpfulness. The study reveals model collapse in the smaller model, characterized by repeated sentences in outputs, attributed to overfitting to patterns like repeated emojis in the training data. This suggests that smaller models may struggle with self-improvement due to insufficient output quality.

## Method Summary
The study implements a two-stage Constitutional AI pipeline on Llama 3-8B. First, an SFT stage fine-tunes the model on synthetically generated critique-revision pairs from harmful prompts, where the model self-critiques and revises responses based on sampled constitutional principles. Second, a DPO stage trains on AI-generated preference pairs (using GPT-4o) to shift the model toward more harmless responses. The authors created 11k SFT examples and 6k preference pairs from the Alpaca-GPT4 and Anthropic HH datasets, using constitutional principles from Anthropic's GitHub repository.

## Key Results
- Harmlessness improved by 40.8% (Attack Success Rate dropped from 71% to 42% in MT-Bench)
- Helpfulness decreased by 9.8% (Turn 1: 6.84→6.63, Turn 2: 5.28→4.28)
- Model collapse observed with repeated sentences/emojis in outputs
- DPO successfully replaced PPO while maintaining similar quality improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constitutional AI's supervised critique-revision pipeline can reduce harmful outputs in smaller models by teaching explicit refusal and redirection patterns.
- Mechanism: The model generates an initial response to a toxic prompt, then produces a self-critique based on sampled constitutional principles, and finally revises its response. Fine-tuning on these revised responses teaches the model to internalize harmlessness patterns without human labeling.
- Core assumption: The revision quality is sufficient for the model to learn meaningful harmlessness patterns rather than superficial refusals.
- Evidence anchors:
  - [abstract] "Our results show that Constitutional AI can effectively increase the harmlessness of the model, reducing the Attack Success Rate in MT-Bench by 40.8%."
  - [section 4, Table 2] Attack Success Rate dropped from 71% to 42% after training on 5,000 synthetically generated examples.
  - [corpus] C3AI framework (arxiv 2502.15861) explores which constitutional principles are most effective for alignment, suggesting principle selection matters for outcomes.
- Break condition: If revision responses contain noise (repeated emojis, irrelevant text), smaller models overfit to these artifacts rather than learning the intended harmlessness patterns.

### Mechanism 2
- Claim: Direct Preference Optimization (DPO) bypasses reward model training while still shifting model behavior toward preferred (more harmless) responses.
- Mechanism: DPO directly optimizes policy using preference pairs by increasing likelihood of chosen responses and decreasing likelihood of rejected responses relative to a frozen reference model, using the loss function L_DPO = -E[log σ(β log π_θ(y_w|x)/π_ref(y_w|x) - β log π_θ(y_l|x)/π_ref(y_l|x))].
- Core assumption: The preference signal from AI feedback (GPT-4o in this study) reliably identifies the more harmless response.
- Evidence anchors:
  - [section 2] "DPO bypasses the need to train the reward model while still achieving similar high-quality results, significantly reducing the effort required."
  - [section 3, RL Stage] The authors created a 6k preference dataset using GPT-4o to select preferred responses based on constitutional principles.
  - [corpus] Weak direct corpus evidence for DPO-specific mechanism; related work focuses on principle selection rather than optimization method.
- Break condition: If preference pairs are too similar (the paper excluded 3% of pairs for this reason), the signal becomes noisy and optimization may not converge meaningfully.

### Mechanism 3
- Claim: Model collapse in smaller LLMs occurs when fine-tuning on self-generated synthetic data that contains artifacts the model cannot distinguish from meaningful content.
- Mechanism: During SFT, the Llama 3-8B revision responses contained repeated emojis. The model overfitted to these patterns, producing infinite repetition of closing politeness phrases during inference. Smaller models lack the capacity to generalize past superficial patterns in their own outputs.
- Core assumption: Larger models (e.g., 52B parameter original) have sufficient capacity to distinguish artifacts from meaningful content during self-training.
- Evidence anchors:
  - [abstract] "We observed clear signs of model collapse in the final DPO-CAI model, indicating that smaller models may struggle with self-improvement due to insufficient output quality."
  - [section 4, Model Collapse] "As a smaller model, LLaMA 3-8B lacks the robustness of larger models and has difficulty distinguishing between emojis and meaningful sentence content."
  - [corpus] Kazdan et al. (2025, cited in paper) discusses model collapse from recursively generated synthetic data in self-generating systems.
- Break condition: Data preprocessing (removing repeated emojis, irrelevant text) before fine-tuning may prevent this collapse; the authors suggest using a stronger model like GPT-3.5 for cleanup.

## Foundational Learning

- Concept: Constitutional AI (RL-AIF)
  - Why needed here: This is the core methodology being tested. Understanding the two-stage pipeline (SFT critique-revision → RL from AI preferences) is essential for diagnosing where failures occur.
  - Quick check question: Can you explain why CAI reduces human labeling requirements compared to RLHF?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: The authors substituted DPO for PPO. Understanding how DPO uses a reference model to constrain policy updates helps explain why collapse occurs (the reference model itself degrades if initialized from collapsed SFT outputs).
  - Quick check question: What does the β parameter control in the DPO loss function?

- Concept: Helpfulness-Harmlessness Tradeoff
  - Why needed here: The 40.8% harmlessness gain came with a 9.8% helpfulness loss. Understanding this tradeoff is critical for setting deployment expectations.
  - Quick check question: Why might a model that refuses to answer toxic questions score lower on multi-turn conversation benchmarks?

## Architecture Onboarding

- Component map: Base model (Llama 3-8B) → SFT stage (toxic prompts → initial response → self-critique → revision → fine-tune) → Preference generation (response pairs → GPT-4o selection) → DPO stage (fine-tune on preferences) → Evaluation (MT-Bench, HeX-PHI)

- Critical path: The SFT stage revision quality determines whether downstream DPO succeeds. If revisions contain artifacts, DPO amplifies them.

- Design tradeoffs:
  - DPO vs. PPO: DPO is simpler (no reward model) but may be more sensitive to noisy preferences
  - Self-generated vs. external feedback: Using the same 8B model for revisions limits quality; GPT-4o for preferences adds external signal but only at RL stage
  - Dataset size: 10k SFT examples and 5k preference pairs—minimal compared to original CAI scale

- Failure signatures:
  - Repetition loops: Model generates closing phrases infinitely (e.g., "Have a great day! :)" repeated)
  - Over-refusal: Model declines benign requests due to overfitting on harmlessness
  - Degraded turn-2 performance: Turn 2 score dropped more (5.28 → 4.28) than Turn 1 (6.84 → 6.63), suggesting context-tracking degradation

- First 3 experiments:
  1. Ablate preprocessing: Filter SFT revision data for repeated patterns before fine-tuning; measure collapse rate and helpfulness retention.
  2. External revision: Use a larger model (e.g., Llama 3-70B) for the critique-revision step while keeping Llama 3-8B as the student; compare collapse incidence.
  3. Preference quality sweep: Vary the feedback provider (GPT-4o vs. weaker models) to determine minimum signal quality needed for effective DPO without inducing collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated preprocessing of self-generated revision data using a stronger model effectively prevent model collapse in smaller language models?
- Basis in paper: [explicit] The authors suggest that "using another AI model, such as GPT-3.5, to automatically clean the revision data" is a necessary step for small models, which they plan to explore in future research.
- Why unresolved: The current study identified the collapse (repetitive text/emojis) but did not implement or validate the proposed data sanitization solution.
- What evidence would resolve it: A comparison of DPO-CAI training runs on Llama 3-8B with and without the automated cleaning step, measuring the incidence of repetitive outputs.

### Open Question 2
- Question: Is the capacity for Constitutional AI self-improvement strictly an emergent property of large models (52B+), or can it be enabled in smaller models via external oversight?
- Basis in paper: [explicit] The study concludes that "like reasoning and math ability, self-improvement is an emergent property" and suggests introducing a "more advanced model to oversee the self-critic revision process."
- Why unresolved: It remains unclear if the failure was due to the model's inherent size (emergence) or simply the quality of the specific synthetic data it generated.
- What evidence would resolve it: An experiment where a small model is fine-tuned on revision data curated by a larger model to see if performance matches that of larger self-improved models.

### Open Question 3
- Question: Can the trade-off between harmlessness and helpfulness be optimized to avoid the 9.8% drop in utility observed in smaller models?
- Basis in paper: [inferred] The paper highlights the helpfulness drop as a significant negative outcome but does not test methods to mitigate this specific trade-off in the context of small models.
- Why unresolved: The replication focused on the standard CAI workflow; it did not explore hyperparameter tuning or data balancing strategies specific to small models to preserve helpfulness.
- What evidence would resolve it: Ablation studies on the SFT and DPO stages in Llama 3-8B to find a training configuration that reduces the Attack Success Rate without degrading Turn 1/2 scores.

## Limitations

- Small dataset size (11k SFT examples, 6k preference pairs) compared to original CAI implementations may limit learning effectiveness
- Model collapse suggests fundamental scalability issues with self-improvement in smaller models that remain unresolved
- MT-Bench Turn 2 degradation indicates context-tracking problems not deeply analyzed
- DPO vs. PPO comparison based on theoretical efficiency rather than empirical testing within this study

## Confidence

**High Confidence**: The 40.8% reduction in Attack Success Rate is well-supported by MT-Bench results showing clear numerical improvement from 71% to 42% baseline reduction. The model collapse observation is directly reproducible from the reported symptoms (repeated sentence loops) and aligns with established literature on synthetic data quality issues.

**Medium Confidence**: The 9.8% helpfulness degradation claim is supported by MT-Bench score drops but conflates multiple factors (refusal behavior, response quality, context retention). The DPO vs. PPO comparison is based on theoretical efficiency claims rather than empirical comparison within this study.

**Low Confidence**: The claim that "smaller models struggle with self-improvement due to insufficient output quality" is primarily observational without systematic ablation studies to isolate whether this is an 8B-specific issue or a general CAI scaling problem.

## Next Checks

1. **Preprocessing Impact Test**: Systematically evaluate how removing repeated emojis and irrelevant text from SFT revision data affects model collapse incidence and helpfulness retention. This directly tests the proposed solution's effectiveness.

2. **Architecture Sensitivity Analysis**: Compare model collapse rates when using different-sized models for the critique-revision step (Llama 3-8B vs. Llama 3-70B) while keeping the student model fixed. This isolates whether the issue is model size or CAI methodology.

3. **Preference Signal Quality Sweep**: Vary the feedback provider strength (GPT-4o vs. weaker models vs. ensemble approaches) to determine the minimum signal quality required for effective DPO without inducing collapse, establishing practical deployment boundaries.