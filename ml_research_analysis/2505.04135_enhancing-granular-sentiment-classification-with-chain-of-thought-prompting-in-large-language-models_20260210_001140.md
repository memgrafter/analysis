---
ver: rpa2
title: Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting
  in Large Language Models
arxiv_id: '2505.04135'
source_url: https://arxiv.org/abs/2505.04135
tags:
- prompting
- sentiment
- reviews
- simple
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that Chain-of-Thought (CoT) prompting significantly
  improves granular sentiment classification accuracy in app store reviews, raising
  performance from 84% to 93% compared to simple prompting. By guiding large language
  models through structured reasoning steps and incorporating keyword-based sentiment
  cues, CoT enables more accurate interpretation of nuanced, context-dependent user
  feedback.
---

# Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models

## Quick Facts
- arXiv ID: 2505.04135
- Source URL: https://arxiv.org/abs/2505.04135
- Authors: Vihaan Miriyala; Smrithi Bukkapatnam; Lavanya Prahallad
- Reference count: 12
- Primary result: Chain-of-Thought prompting raises sentiment classification accuracy from 84% to 93% on app store reviews

## Executive Summary
This study demonstrates that Chain-of-Thought (CoT) prompting significantly improves granular sentiment classification accuracy in app store reviews, raising performance from 84% to 93% compared to simple prompting. By guiding large language models through structured reasoning steps and incorporating keyword-based sentiment cues, CoT enables more accurate interpretation of nuanced, context-dependent user feedback. The approach resolves 80% of errors made by basic prompting, particularly in reviews with mixed or layered sentiments. Remaining challenges include handling vague expressions and very short reviews. The findings highlight the importance of reasoning-aware prompt design for complex sentiment analysis tasks, with implications for customer feedback systems and review summarization.

## Method Summary
The researchers developed a five-step Chain-of-Thought prompting framework for granular sentiment classification of app store reviews. The approach involves decomposing the classification task into sequential reasoning steps: identifying sentiment-bearing expressions, evaluating dominant tone, and providing justification. They incorporated domain-specific keyword hints to guide attention toward sentiment indicators and tested the method against simple prompting on a dataset of 2,000 app store reviews with human-annotated labels. The CoT template was applied to GPT-4 with temperature=0.3, producing structured outputs that included ratings and justifications.

## Key Results
- CoT prompting achieved 93% accuracy versus 84% for simple prompting on 5-point sentiment classification
- The approach resolved 80% of errors made by simple prompting, particularly for mixed-sentiment reviews
- Remaining challenges include handling vague expressions and very short reviews

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing sentiment classification into sequential reasoning steps improves accuracy on mixed-sentiment inputs
- Mechanism: CoT prompting forces the model to explicitly identify positive and negative expressions before synthesizing a rating, rather than mapping directly from input to label. This reduces premature commitment to a polarity based on surface features.
- Core assumption: LLMs perform better when reasoning is externalized into explicit intermediate steps rather than computed implicitly.
- Evidence anchors:
  - [abstract] "guiding large language models through structured reasoning steps... enables more accurate interpretation of nuanced, context-dependent user feedback"
  - [section 4.2] Step 2 requires identifying specific positive and negative expressions before evaluating dominant tone
  - [corpus] "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting" shows similar gains for aspect-category sentiment analysis using structured CoT
- Break condition: May fail when reviews are too short to generate meaningful intermediate steps, or when sarcasm requires world knowledge beyond textual cues.

### Mechanism 2
- Claim: Incorporating keyword-based sentiment cues in the prompt guides attention toward sentiment-bearing expressions
- Mechanism: The CoT prompt includes explicit examples of positive, negative, and neutral keywords, priming the model to weight these appropriately when evaluating tone.
- Core assumption: Keyword guidance reduces over-reliance on word count or isolated lexical matches that caused simple prompting to misclassify mixed reviews.
- Evidence anchors:
  - [section 4.2] "We incorporated keyword hints into the CoT prompt design to guide the model in identifying sentiment-bearing expressions"
  - [section 5.2] Simple prompting "often defaulted to the dominant polarity of isolated words, disregarding the reviewer's overall experience"
  - [corpus] Weak direct evidence; corpus papers focus on reasoning structure rather than keyword cueing specifically
- Break condition: Keyword lists may not generalize across domains (e.g., "sick" is positive in slang but negative in healthcare contexts).

### Mechanism 3
- Claim: Requiring explicit justification improves rating accuracy through consistency constraints
- Mechanism: Step 5 mandates a brief explanation, which creates pressure for the rating to align with the identified evidence. This reduces post-hoc label assignment detached from reasoning.
- Core assumption: Models optimize for internal coherence when forced to generate explanations alongside predictions.
- Evidence anchors:
  - [section 4.1] Simple prompting "lacked explainability, as the model failed to provide any reasoning or justification... making it difficult to analyze errors"
  - [section 4.2] CoT output includes "Rating Justification" that explicitly references identified negative phrases
  - [corpus] "Are You Sure You're Positive?" paper explores uncertainty quantification with CoT agents, suggesting justification calibrates confidence
- Break condition: Justification quality depends on model capacity; weaker models may generate plausible-sounding but incorrect rationales.

## Foundational Learning

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: The entire method depends on structuring prompts to elicit step-by-step reasoning rather than direct answers.
  - Quick check question: Can you explain why "Let's think step by step" changes model behavior on multi-step problems?

- Concept: **Granular Sentiment Classification (5-class vs 3-class)**
  - Why needed here: The paper evaluates on a 5-point scale (Very Negative to Very Positive), which is harder than binary polarity and requires distinguishing intensity.
  - Quick check question: What information is lost when mapping "frustrated but hopeful" to a single sentiment label?

- Concept: **Error Analysis on Misclassification Patterns**
  - Why needed here: The paper attributes 80% error reduction to CoT; understanding failure modes (mixed sentiments, vagueness) guides prompt iteration.
  - Quick check question: If a review says "The app is fine I guess," what features would help a model classify this correctly?

## Architecture Onboarding

- Component map:
  Input Review → Prompt Template (5-step CoT + keyword hints) → GPT-4 API (temp=0.3) → Structured Output (rating + justification) → Accuracy Evaluation vs Human Labels

- Critical path:
  1. Dataset preparation: 2,000 reviews with human-annotated labels (majority vote of 3 annotators)
  2. Prompt design: CoT template with domain-specific keyword examples
  3. Inference: Run both simple and CoT prompts on identical inputs
  4. Evaluation: Compare predictions against ground truth; compute accuracy delta

- Design tradeoffs:
  - **Temperature=0.3** prioritizes determinism over diversity; higher values may produce varied reasoning chains but less reproducible results
  - **Keyword hints** improve performance on this domain but require manual curation; may not transfer to other review types
  - **5-step structure** adds token cost and latency; simpler 3-step variants may suffice for less nuanced tasks

- Failure signatures:
  - Reviews with sarcasm or irony (e.g., "Great, another crash") misclassified by both methods
  - Very short reviews ("It's okay") lack sufficient signal for reasoning steps
  - Over-weighting one aspect in mixed reviews (e.g., visuals vs. plot) leading to skewed ratings

- First 3 experiments:
  1. **Baseline replication**: Run simple prompt on 100 reviews from your domain; measure accuracy against human labels to establish a comparison point
  2. **CoT prompt adaptation**: Customize the 5-step template with domain-specific keywords (e.g., for e-commerce: "shipping," "quality," "return"); evaluate on same 100 reviews
  3. **Error categorization**: Manually review failures from both methods; classify by type (mixed sentiment, vague, sarcastic) to identify where CoT helps most and where it still fails

## Open Questions the Paper Calls Out
None

## Limitations
- Domain specificity: Method tested exclusively on app store reviews, raising generalizability concerns
- Manual keyword curation: Approach requires domain-specific keyword lists, limiting scalability
- No ablation analysis: Study lacks isolation of reasoning steps versus keyword guidance contributions
- Medium confidence: Accuracy claims rely on single model and no cross-validation

## Confidence
- **High**: CoT prompting improves explainability over simple prompting
- **Medium**: 80% error reduction claim and 93% accuracy figure
- **Medium**: Mechanism 2 (keyword cues) - weak corpus support
- **Low**: Generalizability to non-app-review domains

## Next Checks
1. **Domain transfer test**: Apply the CoT prompt template to 500 product reviews from a different domain (e.g., Amazon electronics) and measure accuracy drop
2. **Keyword ablation**: Run CoT prompts with and without keyword hints on 200 reviews to quantify their marginal contribution
3. **Sarcasm detection benchmark**: Test both methods on a curated set of 100 sarcastic reviews to measure relative performance on this failure mode