---
ver: rpa2
title: Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment
arxiv_id: '2510.16387'
source_url: https://arxiv.org/abs/2510.16387
tags:
- whisper
- features
- acoustic
- linguistic
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the potential of Whisper, a large-scale\
  \ ASR foundation model, for automated L2 English oral assessment. Unlike previous\
  \ approaches that only analyze Whisper\u2019s transcriptions, this work extracts\
  \ acoustic and linguistic features from the model\u2019s hidden representations\
  \ to predict holistic proficiency scores."
---

# Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment

## Quick Facts
- arXiv ID: 2510.16387
- Source URL: https://arxiv.org/abs/2510.16387
- Authors: Fu-An Chao; Bi-Cheng Yan; Berlin Chen
- Reference count: 0
- Primary result: Whisper's hidden representations outperform transcriptions for L2 English speaking assessment

## Executive Summary
This paper investigates the potential of Whisper, a large-scale ASR foundation model, for automated L2 English oral assessment. Unlike previous approaches that only analyze Whisper's transcriptions, this work extracts acoustic and linguistic features from the model's hidden representations to predict holistic proficiency scores. A simple chunking strategy allows full-context modeling, and a lightweight classifier trained on these features achieves strong performance on the GEPT picture-description dataset. Incorporating auxiliary image and text-prompt information via semantic textual similarity and image-text relevance scores further boosts performance.

The study demonstrates that Whisper's encoder captures acoustic characteristics while the decoder models linguistic structures, and their combined representations encode both ordinal proficiency patterns and semantic aspects of speech. Experiments show that the proposed method outperforms existing state-of-the-art baselines, establishing Whisper as a powerful foundation for spoken language assessment tasks.

## Method Summary
The proposed framework leverages Whisper's encoder and decoder hidden representations rather than just its transcriptions. Audio segments are chunked (typically 30 seconds) and processed through Whisper's encoder to extract acoustic features (v_enc) and through the decoder to extract linguistic features (v_dec). These representations are concatenated and passed through hierarchical pooling (average over chunks, then max over time) to produce a single fixed-length embedding per audio. A lightweight MLP classifier/regressor maps this embedding to proficiency scores. Auxiliary features from image and text prompts are incorporated via semantic textual similarity (STS) scores and image-text relevance scores, concatenated to the main representation before the final prediction layer.

## Key Results
- Whisper's hidden representations outperform transcriptions for predicting L2 proficiency scores
- The chunking strategy with hierarchical pooling enables effective full-context modeling
- Auxiliary image and text information further improves assessment performance
- The method achieves state-of-the-art results on the GEPT picture-description dataset

## Why This Works (Mechanism)
The paper demonstrates that Whisper's encoder captures acoustic characteristics (v_enc) while the decoder models linguistic structures (v_dec). Their combined representations encode both ordinal proficiency patterns and semantic aspects of speech, making them more informative than raw transcriptions alone. The hierarchical pooling strategy preserves important information across long-form audio by first averaging within chunks (preserving local structure) and then applying max pooling across chunks (capturing global highlights). This enables the model to handle monologue-style speech while maintaining performance.

## Foundational Learning
1. **Hierarchical pooling** - Combines local (chunk-level) and global (audio-level) information aggregation
   - Why needed: Long audio requires balancing local detail with global context
   - Quick check: Verify chunk size affects performance; too small loses context, too large exceeds memory

2. **Multimodal auxiliary features** - Semantic textual similarity and image-text relevance scores
   - Why needed: Prompts and images provide semantic context that influences speech content
   - Quick check: Compare performance with/without each auxiliary feature type

3. **Whisper encoder-decoder synergy** - Acoustic vs linguistic representation complementarity
   - Why needed: Different aspects of speech proficiency (pronunciation vs grammar) are captured separately
   - Quick check: Ablate encoder-only or decoder-only features to measure contribution

4. **Chunking strategy** - Segmenting long audio for memory-efficient processing
   - Why needed: Whisper has context length limits; chunking enables full audio analysis
   - Quick check: Vary chunk overlap to see if boundary effects matter

5. **Lightweight classifier architecture** - Simple MLP on top of complex ASR features
   - Why needed: Avoid overfitting while leveraging rich pretrained representations
   - Quick check: Compare different classifier depths/widths for optimal performance

## Architecture Onboarding

**Component Map:** Audio -> Whisper Encoder -> v_enc -> Concat -> Hierarchical Pooling -> MLP -> Score
                           \-> Whisper Decoder -> v_dec -> /

**Critical Path:** Audio → Chunking → Whisper → Hierarchical Pooling → MLP → Score

**Design Tradeoffs:** 
- Chunk size vs context: Larger chunks preserve more context but increase memory usage
- Encoder-only vs full model: Encoder captures acoustic features, full model adds linguistic context
- Auxiliary features: Improve performance but add complexity and dependency on prompt availability

**Failure Signatures:**
- Poor performance on non-monologue tasks suggests chunking strategy issues
- Degradation with shorter audio indicates hierarchical pooling may dilute fine-grained details
- Inconsistent results across speaker demographics may indicate bias in Whisper representations

**3 First Experiments:**
1. Ablation: Remove encoder features (v_enc) to test acoustic contribution
2. Ablation: Remove decoder features (v_dec) to test linguistic contribution  
3. Vary chunk size (15s, 30s, 60s) to optimize context preservation vs memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be extended to generate explanatory rationales for proficiency scores to support explainable AI in speaking assessment?
- Basis in paper: [explicit] The conclusion states, "we envisage future work that... explores generating rationales for scores, thereby moving this line of research toward explainable AI in speaking assessment."
- Why unresolved: The current work focuses solely on regressing or classifying scores using latent features, without interpreting which specific linguistic or acoustic errors contributed to the score.
- What evidence would resolve it: A system that maps Whisper embeddings to interpretable feedback (e.g., grammar or fluency errors) that correlates with human rater justifications.

### Open Question 2
- Question: How do the Whisper encoder and decoder synergize in this architecture, and does simple concatenation fail to capture their complex interactions?
- Basis in paper: [inferred] The introduction notes that few studies examine the encoder and decoder in tandem, "leaving open questions about their underlying synergies," while the method uses basic concatenation.
- Why unresolved: The paper demonstrates that combining features improves results but does not analyze the theoretical interaction or potential redundancy between the acoustic (v_enc) and linguistic (v_dec) representations.
- What evidence would resolve it: Ablation studies involving cross-attention mechanisms between encoder/decoder states or probing tasks that measure the information overlap between the two modalities.

### Open Question 3
- Question: Does the chunking and hierarchical pooling strategy maintain performance on shorter, non-monologue L2 assessment tasks?
- Basis in paper: [inferred] The method is evaluated exclusively on the GEPT picture-description task (monologues ≈85s), utilizing a chunking strategy designed for long-form audio.
- Why unresolved: It is unclear if the hierarchical pooling strategy dilutes fine-grained acoustic details required for tasks like read-aloud or phoneme-level assessment where temporal precision is more critical than long-context tracking.
- What evidence would resolve it: Experiments applying the chunk-free and chunked methods to short-duration assessment datasets (e.g., read speech corpora) to compare performance differences.

## Limitations
- Evaluated exclusively on L2 English using GEPT picture-description dataset, limiting generalizability
- Only explores basic multimodal integration (STS and relevance scores) without richer fusion strategies
- Does not address potential bias in Whisper representations across diverse speaker populations
- Sparse details on classifier architecture and training make reproducibility challenging

## Confidence
- **High confidence**: Whisper hidden representations contain proficiency-relevant acoustic and linguistic features
- **Medium confidence**: Simple chunking strategy enables effective full-context modeling
- **Medium confidence**: Auxiliary image and text information improves assessment performance

## Next Checks
1. Test the approach on additional L2 English datasets (e.g., TOEFL, IELTS) and cross-lingual assessments to verify generalizability
2. Evaluate the model's performance across different speaking tasks (read speech, spontaneous conversation) to assess task robustness
3. Conduct bias analysis to examine whether Whisper's representations maintain consistent performance across speaker demographics and accents