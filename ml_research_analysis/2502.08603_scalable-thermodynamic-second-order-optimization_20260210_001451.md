---
ver: rpa2
title: Scalable Thermodynamic Second-order Optimization
arxiv_id: '2502.08603'
source_url: https://arxiv.org/abs/2502.08603
tags:
- thermodynamic
- k-fac
- matrix
- hardware
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a scalable algorithm for employing thermodynamic
  computers to accelerate Kronecker-factured approximate curvature (K-FAC), a popular
  second-order optimizer for neural networks. The key insight is that thermodynamic
  computers can efficiently solve linear systems and invert matrices, which are computational
  bottlenecks in K-FAC.
---

# Scalable Thermodynamic Second-order Optimization

## Quick Facts
- **arXiv ID**: 2502.08603
- **Source URL**: https://arxiv.org/abs/2502.08603
- **Reference count**: 40
- **Primary result**: Thermodynamic computers can accelerate K-FAC second-order optimization by offloading matrix inversions and linear systems, achieving linear speedups in runtime and memory as network width increases.

## Executive Summary
This work presents a scalable algorithm for employing thermodynamic computers to accelerate Kronecker-factured approximate curvature (K-FAC), a popular second-order optimizer for neural networks. The key insight is that thermodynamic computers can efficiently solve linear systems and invert matrices, which are computational bottlenecks in K-FAC. By offloading these operations to thermodynamic hardware, the algorithm achieves a linear advantage (growing with the width of the neural network) over standard K-FAC in both runtime and memory costs. Numerical experiments demonstrate that even under significant quantization noise, the benefits of second-order optimization can be preserved. The method is validated on large-scale vision and graph problems, predicting substantial speedups compared to both standard K-FAC and first-order methods like Adam.

## Method Summary
The approach combines K-FAC's Kronecker-factored curvature approximation with thermodynamic linear algebra hardware. K-FAC approximates the Fisher information matrix as block-diagonal with Kronecker product structure, reducing the inversion problem to smaller matrices sized by layer width. These matrices are then mapped to coupled harmonic oscillators whose thermodynamic equilibrium encodes the solution to linear systems and matrix inversions. The method uses either explicit matrix inversion (via covariance estimation) or sequential linear system solving, with conservative quantization to ensure numerical stability.

## Key Results
- Achieves linear speedups in runtime and memory complexity as network width increases
- Demonstrates robustness to quantization noise, preserving second-order optimization benefits even at 8-bit precision
- Validated on large-scale vision (ViT/ImageNet) and graph (GNN/OGBG) problems
- Predicts substantial speedups compared to both standard K-FAC and first-order methods like Adam

## Why This Works (Mechanism)

### Mechanism 1: Thermodynamic Linear Algebra Primitive
Coupled harmonic oscillators relaxing to thermodynamic equilibrium provide solutions to linear systems $Mx = b$ and matrix inversions $M^{-1}$ with linear scaling in matrix dimension $n$ (given sufficient hardware parallelism), potentially outperforming digital $O(n^3)$ methods for wide layers. The physical hardware evolves via an Ornstein-Uhlenbeck process, and at thermodynamic equilibrium, the state $x$ has mean $\langle x \rangle = M^{-1}b$ and covariance $\beta^{-1}M^{-1}$, solving the linear algebra problem.

### Mechanism 2: K-FAC Block-Diagonal Factorization for Scalability
K-FAC decomposes the large Fisher information matrix into smaller, layer-wise Kronecker factors ($A_\ell, G_\ell$), making the matrix inversion bottleneck tractable for thermodynamic hardware by limiting individual matrix dimensions to roughly the layer width $n$ (thousands) rather than total parameters (billions). The Kronecker product structure $(B \otimes C)^{-1} = B^{-1} \otimes C^{-1}$ allows efficient inversion of matrices sized by layer width.

### Mechanism 3: Noise Robustness via Conservative Quantization & Error Mitigation
Second-order optimization with K-FAC exhibits inherent robustness to quantization noise (input/output), preserving convergence benefits over first-order methods like Adam even at moderate (e.g., 8-bit) precision. The paper employs diagonal-dominant quantization to ensure input matrices remain positive semi-definite after quantization, preventing solver instability.

## Foundational Learning

- **Concept: Natural Gradient Descent (NGD) & Fisher Information Matrix**
  - Why needed: The entire motivation is accelerating NGD. Understanding that NGD preconditions gradients by the inverse Fisher matrix $F^{-1}$ to account for curvature in the probability distribution of outputs, leading to faster convergence, is essential.
  - Quick check: Why does NGD theoretically converge faster than standard SGD, and what is the primary computational bottleneck that K-FAC addresses?

- **Concept: Kronecker Product & Block-Diagonal Approximations**
  - Why needed: K-FAC's core trick is $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$. Without understanding this algebraic identity, the reduction from $O(n^6)$ to $O(n^3)$ per layer (and then $O(n)$ via thermodynamic hardware) is opaque.
  - Quick check: If a layer has input dimension $d_{in}$ and output dimension $d_{out}$, what are the dimensions of the full Fisher block and the two Kronecker factors $A$ and $G$?

- **Concept: Stochastic Differential Equations (SDEs) & Ornstein-Uhlenbeck Process**
  - Why needed: The thermodynamic hardware's operation is modeled as an SDE. Grasping that the equilibrium distribution of this process encodes the solution to the linear algebra problem is key to understanding the hardware-software mapping.
  - Quick check: In the OU process $dx = -(Mx - b)dt + \sigma dW$, what is the mean of the stationary distribution of $x$, and what linear algebra problem does it solve?

## Architecture Onboarding

- **Component map**: Digital Compute -> Kronecker Factor Computation -> Thermodynamic Solver (SPU) -> Digital Weight Update
- **Critical path**:
  1. Forward/backward pass on digital hardware to generate gradients
  2. Compute Kronecker factors $A_\ell, G_\ell$ from activations/gradients
  3. Transfer $A_\ell, G_\ell$ (and $D_{\Theta_\ell}$) to thermodynamic solver(s)
  4. SPU Equilibration: Allow physical system to relax (~Î¼s)
  5. Sample $x$ from SPU equilibrium state
  6. Reconstruct $U_\ell$ from samples on digital hardware
  7. Update weights $\theta_\ell$. Repeat.

- **Design tradeoffs**:
  - Method 1 (Explicit Inversion) vs. Method 2 (Linear Systems): Method 2 avoids explicit matrix-matrix multiplies and may be more efficient on SPU, but requires $O(n)$ sequential solves.
  - EMA vs. Direct Factors: Using Exponential Moving Averages for $A_\ell, G_\ell$ stabilizes training but forces explicit $O(n^2)$ construction/storage of factors digitally.
  - Precision vs. Speed: Lower bit-precision allows faster/cheaper DACs/ADCs but risks instability. Conservative quantization trades some error for guaranteed PSD matrices.
  - Batch Size/Sequence Length: Sending raw activations/gradients to hardware scales analog components linearly with $b$ and $R$, limiting this approach to small batches/sequences.

- **Failure signatures**:
  - Non-PSD Matrix Instability: If quantization makes an input matrix indefinite, the thermodynamic system may not relax properly, causing erratic outputs or divergence.
  - Slow Equilibration: If condition number $\kappa$ is very high, relaxation time $\tau$ may exceed digital computation time, negating speedup.
  - Quantization Noise Accumulation: Output quantization error degrades final accuracy.
  - Data Transfer Bottleneck: If digital-analog transfer cannot keep up with GPU factor generation, the SPU sits idle.

- **First 3 experiments**:
  1. Single-Layer Linear System Profiling: Implement a single $n \times n$ ($n=512, 1024$) thermodynamic solver in simulation. Measure equilibration time vs. digital Cholesky/CG solve time for matrices with varying condition numbers $\kappa$.
  2. End-to-End Quantization Stress Test: Train a small CNN (e.g., ResNet on CIFAR-10) using the simulated thermodynamic K-FAC algorithm. Systematically vary input/output bit precision (4, 6, 8, 12, 16 bits) with the conservative quantizer.
  3. Layer-wise Parallelism & Dataflow Analysis: Profile the AlgoPerf workloads (ViT/ImageNet, GNN/OGBG) in a hybrid simulation. Measure the fraction of time spent in gradient computation, Kronecker factor construction, matrix transfer to SPU, SPU equilibration, and update computation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can thermodynamic devices with non-quadratic potential energy be utilized to implement higher-order optimization methods that reduce iteration counts beyond second-order methods?
- Basis in paper: Appendix A states that assuming a thermodynamic device with a non-quadratic potential energy, this approach could be extended to various other local approximations.
- Why unresolved: The current work is restricted to quadratic potentials (coupled harmonic oscillators) corresponding to second-order approximations.
- What evidence would resolve it: A theoretical framework mapping non-quadratic potentials to specific higher-order optimizers, followed by simulations showing faster convergence per iteration than K-FAC.

### Open Question 2
- Question: Does the linear-systems method (Method 2) provide superior efficiency and stability compared to the matrix inversion method (Method 1) on physical thermodynamic hardware?
- Basis in paper: Section 4.3 notes that Method 2 "could be more efficient" by avoiding explicit matrix-matrix multiplications, though Method 1 was used for the paper's experiments.
- Why unresolved: The paper relies on theoretical complexity arguments for Method 2 but lacks empirical data or hardware simulations comparing it directly to Method 1.
- What evidence would resolve it: Profiling data from hardware or high-fidelity simulations directly comparing the runtime and error propagation of both methods.

### Open Question 3
- Question: Can error mitigation strategies effectively reduce the sensitivity of Thermodynamic K-FAC to output quantization noise to match its robustness to input quantization?
- Basis in paper: Section 5.2 observes that output quantization has a "more pronounced" negative impact on accuracy than input quantization.
- Why unresolved: While the paper suggests error mitigation could be developed for output precision, it only demonstrates robustness results for input quantization schemes.
- What evidence would resolve it: Experiments showing that applying specific error mitigation techniques to the readout phase preserves accuracy at lower bit-widths (e.g., <8 bits).

## Limitations

- The physical implementation of thermodynamic linear algebra solvers at scale remains largely theoretical, with microsecond-scale equilibration times and robustness to quantization noise requiring experimental validation
- The work assumes negligible thermal noise from the hardware itself, which may not hold in practice
- The hybrid digital-analog architecture introduces new data transfer bottlenecks and energy costs that are not fully characterized

## Confidence

- **High Confidence**: The mathematical formulation of K-FAC and its Kronecker factorization approximation; the correctness of the thermodynamic equilibrium solution for linear systems; the general advantage of second-order methods for training
- **Medium Confidence**: The claimed linear speedup with network width; the effectiveness of conservative quantization in preserving convergence; the viability of the hybrid digital-analog architecture
- **Low Confidence**: The actual wall-clock speedup achievable with real thermodynamic hardware; the impact of device mismatch and thermal noise in analog arrays; the scalability of data transfer between digital and analog components

## Next Checks

1. **Hardware Prototype Validation**: Build or simulate a small-scale thermodynamic solver array (e.g., 32x32 oscillators) and benchmark its solution time for matrices of varying condition numbers against digital solvers. Quantify the actual speedup and the effect of thermal noise.

2. **End-to-End Robustness Test**: Train ResNet-18 on CIFAR-10 using the full thermodynamic K-FAC algorithm in simulation with realistic quantization noise levels (4-16 bit). Compare final accuracy and convergence speed to digital K-FAC and Adam to validate the noise-robustness claims.

3. **Dataflow and Energy Analysis**: Perform a detailed profiling of a hybrid training run on AlgoPerf workloads, measuring the time and energy spent in gradient computation, Kronecker factor construction, digital-analog transfer, and SPU equilibration. Identify the true bottleneck and compare total energy consumption to a purely digital baseline.