---
ver: rpa2
title: 'Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity
  for Transformers under In-Context Regression'
arxiv_id: '2512.09275'
source_url: https://arxiv.org/abs/2512.09275
tags:
- adversarial
- generalization
- complexity
- bound
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides the first generalization analysis for a single-layer
  Transformer under in-context regression that explicitly accounts for a completely
  trainable positional encoding (PE) module. The analysis shows that PE systematically
  enlarges the generalization gap, with the gap widening further under adversarial
  attack.
---

# Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression

## Quick Facts
- arXiv ID: 2512.09275
- Source URL: https://arxiv.org/abs/2512.09275
- Authors: Weiyi He; Yue Xing
- Reference count: 40
- Key outcome: First generalization analysis for single-layer Transformer under in-context regression explicitly accounting for trainable positional encoding; shows PE systematically enlarges generalization gap and increases adversarial vulnerability

## Executive Summary
This paper provides the first theoretical analysis of generalization in a single-layer Transformer performing in-context regression (ICL) that explicitly accounts for a completely trainable positional encoding (PE) module. The authors derive Rademacher complexity bounds showing that trainable PE systematically enlarges the generalization gap compared to models without PE. The analysis reveals that under adversarial attack, this gap widens further due to additional complexity terms dependent on the attack strength. The theoretical results demonstrate that increasing context length improves both generalization and adversarial robustness, while longer contexts with PE introduce greater vulnerability.

## Method Summary
The study analyzes a single-layer, single-head Transformer on synthetic in-context linear regression tasks. Data follows Assumption 4.1: inputs x ~ N(0, I_d), outputs y = μᵀx with μ ~ N(0, I_d/d). Two model variants are compared: with completely trainable PE matrix P and without PE. The analysis uses Rademacher complexity to bound generalization gaps, with additional bounds for adversarial settings using surrogate losses. Experiments use d=5, d_m=1024, 309 training tasks, 9,991 validation tasks, and vary context length t across multiple values while testing PGD attacks with different perturbation budgets ε.

## Key Results
- Trainable PE introduces an additive complexity term C_PE√(d)/√(m) that systematically widens generalization gap
- Under adversarial attack, the gap widens further with an additional term C_PE·ε/√(m) proportional to attack strength
- Increasing context length t reduces generalization gap and improves adversarial robustness through decay factors in the bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trainable positional encoding (PE) systematically enlarges the generalization gap in Transformers performing in-context regression.
- Mechanism: Trainable PE introduces additional learnable parameters that increase the function class complexity. This is quantified through Rademacher complexity bounds, where the complexity contribution from PE is additive and can dominate at longer context lengths (when the model is no longer limited by information in the context).
- Core assumption: The difference between the effective linear weights of models with and without PE is bounded by a constant (Assumption 4.2).
- Evidence anchors:
  - [abstract] "Our result shows that PE systematically enlarges the generalization gap."
  - [Theorem 4.2] Rademacher complexity with PE includes an additional term $C_{PE}\sqrt{d}/\sqrt{m}$ that represents the cost incurred by the PE module.
  - [corpus] The paper "Theoretical Analysis of Positional Encodings in Transformer Models" provides a complementary theoretical framework for PE analysis but does not specifically address the generalization gap mechanism described here.
- Break condition: If PE parameters are constrained (e.g., through strong regularization) or if PE is non-trainable (e.g., fixed sinusoidal or RoPE), the additional complexity term should vanish.

### Mechanism 2
- Claim: Adversarial attacks magnify the generalization gap between models with and without PE, making PE-enhanced models more vulnerable.
- Mechanism: Under adversarial attack, the complexity gap between models with and without PE widens due to an additional attack-dependent term in the Adversarial Rademacher Complexity (ARC) bounds. The perturbation budget epsilon explicitly contributes to the complexity of models with PE, increasing their vulnerability.
- Core assumption: The model function is Lipschitz with respect to its input (used for surrogate loss bounds), and the inverse Lipschitz continuity of the parameter-to-solution-space mapping is preserved under adversarial perturbation (Assumption 4.3).
- Evidence anchors:
  - [Theorem 4.4] The ARC bound for models with PE includes an additional $C_{PE}\epsilon/\sqrt{m}$ term that is directly proportional to the attack strength.
  - [Proposition 4.3] The attack enlarges the diameter of the perturbed solution space by a factor $\Phi(\epsilon, t, d)$.
  - [corpus] No direct corpus support for this specific adversarial PE vulnerability mechanism; related corpus papers address robustness through other mechanisms.
- Break condition: If the perturbation budget $\epsilon \to 0$, the additional term vanishes and the bound recovers the clean bound. If epsilon exceeds $\sqrt{t} - \sqrt{d}$, the bound becomes meaningless.

### Mechanism 3
- Claim: Increasing context length $t$ reduces the generalization gap and improves adversarial robustness, even under attack.
- Mechanism: The generalization bound includes a factor that decays with $\sqrt{t}$. This reflects that ICL leverages more examples to constrain the effective parameter space, reducing its diameter. The adversarial robustness factor $\Phi(\epsilon, t, d)$ is also a decreasing function of $t$, showing longer contexts mitigate attack effects.
- Core assumption: The data matrix $X_t$ is well-conditioned with high probability (a consequence of the Gaussian data assumption), ensuring the effective solution space is constrained.
- Evidence anchors:
  - [Theorem 4.1] The clean RC bound explicitly shows decay with $\sqrt{t}$.
  - [Proposition 4.3] $\Phi(\epsilon, t, d)$ decreases as $t$ increases, enhancing adversarial robustness.
  - [Figure 3] Empirical results confirm the generalization gap decreases with context length under all tested attack strengths.
  - [corpus] No direct corpus evidence refutes or supports this specific decay mechanism.
- Break condition: If the data matrix is not well-conditioned (e.g., near $t \approx d$), the bounds become unstable, and the decay trend may not hold.

## Foundational Learning

- Concept: **Rademacher Complexity**
  - Why needed here: It is the primary theoretical tool used to quantify model capacity and bound the generalization gap. All main results are expressed in terms of it.
  - Quick check question: If you increase the number of training samples $m$, what should happen to the Rademacher complexity of a fixed function class?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: This is the specific learning paradigm under analysis. The paper models the Transformer as learning an "effective linear weight" $w_{eff}$ from the prompt, which is central to deriving the bounds.
  - Quick check question: In the paper's formulation, does ICL involve updating the model's parameters?

- Concept: **Adversarial Robustness & Surrogate Loss**
  - Why needed here: To analyze adversarial generalization, the paper uses a surrogate loss to upper-bound the intractable adversarial loss. Understanding this method is essential for following the proof of the ARC theorems.
  - Quick check question: Why is the true adversarial loss difficult to analyze directly, and what role does the Lipschitz assumption play in bounding it?

## Architecture Onboarding

- Component map: Input/Prompt -> W_in -> Add PE -> Self-Attention -> Activation -> Linear Read-out -> MSE Loss
- Critical path: Input → W_in → Add PE → Self-Attention → Activation → Linear Read-out → MSE Loss
- Design tradeoffs:
  - **Trainable vs. Fixed PE:** Trainable PE offers flexibility but increases generalization gap and adversarial vulnerability (Theorem 4.2, 4.4). Fixed PE (e.g., RoPE) has lower generalization cost.
  - **Context Length ($t$):** Longer $t$ improves generalization and robustness but increases computational cost. Bounds may be unstable for $t \approx d$.
  - **Embedding Dimension ($d_m$):** Larger $d_m$ increases parameter count and diameter, potentially increasing complexity bounds (Lemma 4.2).
- Failure signatures:
  - **Near-interpolation threshold ($t \approx d$):** High variance in generalization gap, potential "double descent" behavior.
  - **Strong adversarial attack ($\epsilon \ge \sqrt{t} - \sqrt{d}$):** Theoretical bounds become vacuous; model performance may degrade sharply.
  - **Over-parameterized PE ($B_P$ large):** Directly increases the bounded PE effect constant $C_{PE}$, widening the generalization gap.
- First 3 experiments:
  1. **Validate Clean Generalization Gap:** Train identical single-layer Transformers (with/without trainable PE) on the provided ICL regression task. Plot generalization gap vs. context length $t$ to verify the consistent gap and decay trend (replicate Figure 1).
  2. **Validate Adversarial Vulnerability with PE:** Apply PGD attacks to the trained models. Compare the attacked generalization gap for models with and without PE to confirm the amplified vulnerability (replicate Figure 2).
  3. **Test Impact of Attack Strength and Context Length:** Vary the PGD perturbation budget $\epsilon$ and context length $t$. Plot the generalization gap across these settings to verify that the gap increases with $\epsilon$ and decreases with $t$ (replicate Figure 3 and confirm $\Phi(\epsilon, t, d)$ behavior).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the generalization and robustness effects of positional encoding scale in deep, multi-layer Transformer architectures?
- **Basis in paper:** [explicit] The Conclusion states that "a natural extension is to consider understanding how positional encodings scale in multi-layer or deep Transformer architectures."
- **Why unresolved:** The current theoretical analysis and simulations are strictly restricted to a single-layer, single-head architecture to make the Rademacher complexity derivation tractable.
- **What evidence would resolve it:** A theoretical extension of the bounds to $L$-layer models and empirical validation showing whether the complexity gap compounds linearly or exhibits saturation effects with depth.

### Open Question 2
- **Question:** Do the established generalization bounds hold for complex, non-linear tasks or real-world applications beyond the linear regression setting?
- **Basis in paper:** [explicit] The authors acknowledge a limitation in the Conclusion: "we only consider the simplest linear regression task. One direction is to extend the problem into other, more complicated tasks."
- **Why unresolved:** The derivations rely on Assumption 4.1, which models data as a noiseless linear regression, potentially obscuring PE dynamics in non-linear domains common to LLMs.
- **What evidence would resolve it:** Deriving bounds for non-linear function classes (e.g., classification) or demonstrating empirically that the PE generalization gap persists in standard NLP benchmarks.

### Open Question 3
- **Question:** Can specific regularization techniques based on the identified complexity terms ($L_f$, $D$) effectively mitigate the vulnerability introduced by trainable PE?
- **Basis in paper:** [explicit] The Conclusion proposes that "one may develop new regularization techniques that directly penalize these terms to design more robust models."
- **Why unresolved:** While the paper identifies the complexity terms responsible for the generalization gap, it does not propose or test an algorithmic intervention to constrain them.
- **What evidence would resolve it:** An experimental study where a regularization term based on the bound's parameters is added to the loss function, resulting in a narrower generalization gap for models with PE.

## Limitations

- The analysis is limited to single-layer, single-head Transformers with synthetic Gaussian data, raising questions about generalizability to deeper models or natural language tasks
- The theoretical bounds rely on assumptions (well-conditioned data, Lipschitz continuity, bounded PE norms) that may not hold in practice for real-world datasets
- The adversarial robustness claims depend on surrogate loss bounds that may not tightly approximate true adversarial risk in practice
- The near-interpolation regime (t ≈ d) is explicitly identified as problematic for the bounds, suggesting potential instability in realistic settings

## Confidence

- **High confidence:** The core mechanism that trainable PE increases generalization gap through added Rademacher complexity is well-supported by the theoretical framework and Theorem 4.2
- **Medium confidence:** The claim that adversarial attacks amplify this gap is theoretically sound but depends on the surrogate loss approximation holding tightly in practice (Theorem 4.4)
- **Medium confidence:** The beneficial effect of longer context on robustness is theoretically predicted but may be limited by practical constraints like computational cost and data quality

## Next Checks

1. **Empirical validation of the PE complexity gap**: Train identical Transformers with and without trainable PE on real-world datasets (not just synthetic Gaussian) and measure the actual generalization gap across varying context lengths to verify the theoretical prediction holds beyond the idealized setting.

2. **Verification of adversarial gap amplification**: Implement the PGD attack with varying perturbation budgets on trained models and systematically measure whether the gap between PE and non-PE models scales as predicted by the epsilon-dependent terms in Theorem 4.4.

3. **Context length robustness test**: Conduct experiments varying context length t across a wider range (including values close to d) to empirically validate the decay of generalization gap with t and identify the breakdown point where bounds become unstable.