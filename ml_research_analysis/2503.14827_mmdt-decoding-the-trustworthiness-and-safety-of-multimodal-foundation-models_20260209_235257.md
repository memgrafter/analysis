---
ver: rpa2
title: 'MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models'
arxiv_id: '2503.14827'
source_url: https://arxiv.org/abs/2503.14827
tags:
- fairness
- mmfms
- images
- image
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MMDT introduces the first comprehensive and unified platform to
  evaluate the trustworthiness and safety of multimodal foundation models (MMFMs)
  across six critical perspectives: safety, hallucination, fairness, privacy, adversarial
  robustness, and out-of-distribution robustness. The platform constructs challenging
  evaluation datasets for both text-to-image and image-to-text models using red teaming
  algorithms, forming a high-quality benchmark.'
---

# MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models

## Quick Facts
- **arXiv ID:** 2503.14827
- **Source URL:** https://arxiv.org/abs/2503.14827
- **Reference count:** 40
- **Primary result:** First comprehensive platform evaluating multimodal foundation model trustworthiness across safety, hallucination, fairness, privacy, adversarial robustness, and out-of-distribution robustness dimensions.

## Executive Summary
MMDT introduces the first comprehensive platform to evaluate trustworthiness and safety of multimodal foundation models across six critical dimensions. The platform constructs challenging evaluation datasets using red-teaming algorithms for both text-to-image and image-to-text models, forming a high-quality benchmark. Evaluations reveal significant vulnerabilities across all trustworthiness dimensions, with no single model consistently excelling. The platform provides actionable insights for developing safer MMFMs through detailed failure mode analysis.

## Method Summary
MMDT provides a unified evaluation platform with benchmark orchestration, inference runtimes, and evaluation logic components. It uses red-teaming algorithms to generate challenging data through semantic-preserving perturbations and adversarial transformations. The platform evaluates both input-level resilience (filter effectiveness) and output-level harm (generation safety), decoupling bypass rates from harmful content generation rates. Evaluation uses external tools like GroundingDINO for object detection and GPT-4o as a harmfulness judge, with custom datasets constructed for each trustworthiness dimension.

## Key Results
- Severe safety issues persist across all evaluated models, with significant bypass rates and harmful generation rates
- Hallucination problems affect all tasks, particularly spatial reasoning in text-to-image generation (below 3% accuracy)
- Pronounced fairness and privacy risks exist, including concept-level memorization and inference capability from visual cues
- Models show susceptibility to adversarial attacks and poor out-of-distribution robustness
- No single model consistently excels across all trustworthiness dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Red-teaming algorithms systematically uncover vulnerabilities by applying semantic-preserving perturbations and adversarial transformations to inputs.
- **Mechanism:** Platform generates challenging data by transforming benign prompts into jailbreaking inputs or transformed instructions, forcing models to process conflicting visual and textual signals that break alignment.
- **Core assumption:** Safety filters trained on simple patterns fail when harmful intent is obfuscated via cross-modal transformations like hiding text in images.
- **Evidence anchors:** Designed various evaluation scenarios and red teaming algorithms to generate challenging data; details vanilla, transformed, and jailbreaking scenarios including typography attacks.

### Mechanism 2
- **Claim:** Comprehensive trustworthiness requires separating input-level resilience from output-level harm, as high refusal rates don't guarantee safe outputs.
- **Mechanism:** Platform decouples Bypass Rate from Harmful content Generation Rate; models might reject prompts but still generate harmful content if they accept transformed versions.
- **Core assumption:** Model alignment focuses heavily on input filtering (keywords) rather than verifying safety of generated content itself.
- **Evidence anchors:** Evaluates two types of resilience: input-level and output-level; lower bypass rate doesn't necessarily mean safer responses.

### Mechanism 3
- **Claim:** MMFMs pose distinct privacy risks via concept-level memorization of training data and strong inference capability from visual cues.
- **Mechanism:** For text-to-image, diffusion models memorize high-level concepts like celebrity likenesses; for image-to-text, models use visual reasoning to infer sensitive metadata from benign images.
- **Core assumption:** Pre-training on large-scale uncurated datasets inevitably embeds sensitive concept-level knowledge, and image-to-text models possess sufficient world knowledge to deduce sensitive facts from visual hints.
- **Evidence anchors:** Diffusion models exhibit strong concept-level memorization; image-to-text models can accurately predict personal attributes posing privacy risks.

## Foundational Learning

- **Concept: Multimodal Foundation Models (MMFMs)**
  - Why needed here: Distinguishes between Text-to-Image and Image-to-Text architectures with fundamentally different failure modes
  - Quick check question: Does the failure mode involve the model seeing something it shouldn't (I2T) or creating something it shouldn't (T2I)?

- **Concept: Red-Teaming vs. Standard Benchmarking**
  - Why needed here: Standard benchmarks measure capability on clean data; red-teaming measures robustness on adversarial or dirty data
  - Quick check question: Is the test data designed to verify if the model works correctly, or to make it fail?

- **Concept: Alignment Trade-offs**
  - Why needed here: Introduces "Overkill Fairness" (sacrificing truth for diversity) and notes safety guardrails can harm performance
  - Quick check question: Does enforcing a safety rule here prevent the model from answering a legitimate, factual question?

## Architecture Onboarding

- **Component map:** Benchmark Orchestration -> Inference Runtimes -> Evaluation Logic -> Results Analysis
- **Critical path:** Evaluation logic is the bottleneck; defining what constitutes a "failure" (detecting watermark vs. similar image; hallucination vs. stylistic choice) is harder than generating test data
- **Design tradeoffs:**
  - Static vs. Dynamic Data: Platform keeps some red-teaming data private to prevent models from overfitting
  - Judge Reliability: Using strong model (GPT-4o) introduces dependency and potential bias; weaker judges might miss subtle harms
- **Failure signatures:**
  - Safety: High Bypass Rate but Low Harmful Generation Rate = Model refuses everything, even safe inputs
  - Fairness: High Group Fairness but High Overkill Fairness = Model prioritizes diversity over historical accuracy
  - Hallucination: High performance on "Natural Selection" but low on "Counterfactual Reasoning" = Model relies on training data priors
- **First 3 experiments:**
  1. Safety Stress Test: Run "Transformed" and "Jailbreak" scenarios on I2T model to measure HGR, compare against GPT-4V baseline
  2. Privacy Inference Check: Feed "Pri-Street-View" dataset to model and attempt to recover ZIP codes to quantify location leakage risks
  3. Fairness vs. Factuality: Measure "Overkill Fairness" by asking model to generate images/descriptions of historical figures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural or training modifications are required to improve spatial reasoning accuracy in text-to-image models above current near-zero baseline (<3%)?
- **Basis:** Appendix E.1.2 notes all models, including DALLÂ·E 3, struggle profoundly with spatial reasoning, achieving accuracies below 3%
- **Why unresolved:** Paper identifies scaling and instruction-tuning are insufficient to solve this specific hallucination type
- **Evidence:** Novel model architecture achieving >30% accuracy on MMDT spatial reasoning benchmark

### Open Question 2
- **Question:** How can multimodal foundation models balance group fairness with "overkill fairness" to avoid observed trade-off?
- **Basis:** Appendix F.2.2 and C.3 discuss trade-off between group fairness and overkill fairness, noting models show opposite behaviors
- **Why unresolved:** Paper observes this trade-off as fundamental tension in current alignment strategies without proposing simultaneous satisfaction method
- **Evidence:** Alignment method minimizing group unfairness while maintaining overkill fairness score of 0.0

### Open Question 3
- **Question:** Why doesn't input-level resilience (bypass rate) observably correlate with output-level resilience (harmful generation rate)?
- **Basis:** Section D.2.1 states input-level resilience does not observably correlate with output-level resilience
- **Why unresolved:** Current alignment focuses on input rejection, but mechanisms governing output harmfulness are distinct and poorly understood
- **Evidence:** Mechanistic interpretation study identifying internal components contributing to safety filter rejection versus harmful content generation

## Limitations
- Reliance on GPT-4o as judge for safety evaluation introduces potential bias and scalability questions
- Privacy evaluation depends on CLIP similarity scores that may not fully capture privacy violations
- Adversarial robustness evaluation uses specific attack algorithms whose effectiveness against newer defenses is unclear
- Exact score reproduction may be difficult due to proprietary model version changes

## Confidence
- **High Confidence:** Platform architecture and evaluation methodology are well-documented and reproducible; six-perspective framework is comprehensive
- **Medium Confidence:** Comparative results across models are reliable though exact reproduction may be difficult; privacy and fairness evaluation are methodologically sound
- **Low Confidence:** Absolute scores for safety bypass rates and harmful generation rates may be sensitive to specific GPT-4o evaluation prompt; OOD robustness results depend heavily on chosen transformation types

## Next Checks
1. **Judge Stability Verification:** Run safety evaluation pipeline twice with identical inputs but different random seeds to quantify variance in GPT-4o's harmfulness judgments; if variance exceeds 10%, implement majority voting
2. **Cross-Model Safety Calibration:** Evaluate same safety prompts across different GPT-4o versions to verify reported bypass rates and harmful generation rates are consistent
3. **Privacy Impact Assessment:** Compare CLIP similarity scores against human evaluations of "memorization" for T2I outputs to validate automated metric's sensitivity to actual privacy concerns