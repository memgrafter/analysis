---
ver: rpa2
title: 'Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting'
arxiv_id: '2509.22195'
source_url: https://arxiv.org/abs/2509.22195
tags:
- robot
- move
- action
- reasoning
- vlm2vla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VLM2VLA, a method to fine-tune vision-language
  models (VLMs) into vision-language-action (VLA) models for robotics without catastrophic
  forgetting. The core idea is to represent low-level robotic actions as natural language
  descriptions, aligning the fine-tuning data with the VLM's pretraining distribution
  and enabling effective adaptation via LoRA.
---

# Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting

## Quick Facts
- arXiv ID: 2509.22195
- Source URL: https://arxiv.org/abs/2509.22195
- Reference count: 40
- One-line primary result: VLM2VLA fine-tunes VLMs into VLAs without catastrophic forgetting by representing actions as language, maintaining >85% VQA performance while achieving strong real-world generalization

## Executive Summary
This paper addresses the problem of catastrophic forgetting when fine-tuning vision-language models (VLMs) into vision-language-action (VLA) models for robotics. The core insight is that representing low-level robotic actions as natural language descriptions aligns the fine-tuning data with the VLM's pretraining distribution, enabling effective adaptation via LoRA while preserving multimodal reasoning capabilities. The method achieves strong generalization to novel tasks, including multilingual instructions and out-of-distribution scenarios, significantly outperforming baselines that suffer from forgetting.

## Method Summary
VLM2VLA fine-tunes VLMs into VLAs by representing robotic actions as hierarchical language descriptions rather than discrete tokens. The method uses an external VLM (Gemini 2.5) to relabel robot trajectories from Bridgev2 into subtask descriptions, motion plans, and action chunks as text. LoRA adapters are applied to the VLM's linear layers during fine-tuning on this language-formatted dataset. At inference, the model generates outputs in a fixed sequence: subtasks for the full task, motion plan for the current subtask, and low-level action chunk as text. An optional external verifier checks subtask completion.

## Key Results
- VLM2VLA maintains over 85% performance on challenging VQA benchmarks after fine-tuning
- Achieves strong generalization to novel tasks in over 800 real-world robotics experiments
- Significantly outperforms baselines that use standard action-token representations and suffer from catastrophic forgetting
- Successfully handles multilingual instructions and out-of-distribution scenarios requiring open-world semantic reasoning

## Why This Works (Mechanism)
The method works because representing actions as language maintains distributional alignment between pretraining and fine-tuning data. When VLMs are pretrained on vast amounts of text paired with images, they learn rich multimodal representations. Standard VLA fine-tuning disrupts this by introducing discrete action tokens that are out-of-distribution. By keeping actions in natural language format, VLM2VLA ensures the fine-tuning data resembles the pretraining distribution, allowing LoRA to adapt the model without overwriting foundational capabilities.

## Foundational Learning

- Concept: **Distribution Mismatch in Fine-Tuning**
  - Why needed here: The paper's core thesis is that catastrophic forgetting in VLAs is caused by distribution mismatch between pretraining and fine-tuning data
  - Quick check question: Explain why adding a new action token to a VLM's vocabulary and fine-tuning on robot data is more likely to cause catastrophic forgetting than fine-tuning on a new text dataset

- Concept: **Parameter-Efficient Fine-Tuning (PEFT / LoRA)**
  - Why needed here: VLM2VLA's ability to retain original VLM capabilities is explicitly attributed to LoRA use
  - Quick check question: What is the core assumption of LoRA that allows it to adapt a large model with minimal memory overhead? What kind of knowledge is it theoretically better at preserving compared to full fine-tuning?

- Concept: **Catastrophic Forgetting in Foundation Models**
  - Why needed here: This is the central problem the paper addresses
  - Quick check question: If a VLM is fine-tuned on a dataset of only robot pick-and-place trajectories, what specific capability (referenced in the paper) is likely to degrade, and how would this affect performance on an OOD task?

## Architecture Onboarding

- Component map: Bridgev2 trajectories -> Gemini 2.5 relabeling -> VLM backbone (Gemma-3-12B-IT) -> LoRA adapters -> Hierarchical language output (subtask -> motion plan -> action chunks)
- Critical path: The entire method's success depends on the quality and consistency of the data transformation pipeline. If Gemini fails to map action sequences to coherent text descriptions, the VLM will be trained on noisy or misaligned data. The inference sequence is also critical, as errors in early steps will cascade.
- Design tradeoffs:
  - Action Granularity vs. Token Efficiency: Language representations are verbose, increasing sequence length and inference latency compared to single-token actions
  - Precision vs. Generalization: Coarse motion plans improve generalization but may lack precision; the paper uses a hybrid approach with precise numbers in final action steps
- Failure signatures:
  - Parsing Failures: VLM might generate invalid Python lists, causing runtime errors (~10% of cases require retry)
  - Compounding Reasoning Errors: Wrong subtask prediction leads to wrong motion plan and actions, causing complete trajectory failure
  - Coarse Action Prediction: Poor spatial magnitude understanding can result in physically incorrect motions
- First 3 experiments:
  1. Validate Data Pipeline on a Single Episode: Run one Bridgev2 trajectory through Gemini annotation pipeline and manually inspect generated subtask, motion plan, and action text
  2. LoRA Knowledge Retention Ablation: Compare VQA performance between LoRA-tuned and full-parameter fine-tuned models on MMB-en benchmark
  3. ID vs. OOD Task Generalization Test: Evaluate success rates on in-distribution pick tasks vs. linguistically OOD tasks with synonym objects

## Open Questions the Paper Calls Out
- Can the "actions as language" framework be extended to handle 6-DoF end-effector control, including precise rotational movements?
- Can the base VLM be fine-tuned to serve as its own verifier for subtask completion without degrading policy performance?
- Does the language-based action representation enable effective cross-embodiment transfer between robots with significantly different kinematics?

## Limitations
- Relies on external VLM (Gemini 2.5) for data relabeling, with limited quantitative evaluation of annotation quality
- Evaluation constrained to single robot platform (Stretch RE1) and narrow set of household manipulation scenarios
- Language-based action representation increases sequence length and inference latency compared to discrete tokens

## Confidence
- **High Confidence**: Empirical demonstration that LoRA fine-tuning on language-formatted actions preserves VQA performance (>85%) compared to catastrophic forgetting in standard action-token methods
- **Medium Confidence**: Claim that action-as-language representation is the primary driver of retained capabilities, as opposed to other factors like base model choice or data quality
- **Medium Confidence**: Hierarchical inference procedure improves efficiency and accuracy, though contribution of each stage is not quantified

## Next Checks
1. **Annotation Quality Audit**: Run 100 Bridgev2 trajectories through Gemini 2.5 annotation pipeline and have human experts rate correctness and consistency of generated subtask, motion plan, and action text
2. **Cross-Domain Generalization Test**: Evaluate trained VLM2VLA model on novel robot platform and different dataset (e.g., Kitchen or ALFRED) to measure performance drop
3. **Discrete Action Token Ablation**: Train identical VLA model with discrete action tokens using LoRA, compare both task success rates and VQA retention to isolate contribution of action representation format