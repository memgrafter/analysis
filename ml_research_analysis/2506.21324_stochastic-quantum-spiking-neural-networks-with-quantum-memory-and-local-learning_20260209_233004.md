---
ver: rpa2
title: Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning
arxiv_id: '2506.21324'
source_url: https://arxiv.org/abs/2506.21324
tags:
- neuron
- quantum
- spiking
- neurons
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the stochastic quantum spiking (SQS) neuron
  and SQS neural network (SQSNN) model, which combines quantum computing with neuromorphic
  design. The SQS neuron uses a multi-qubit quantum circuit to implement an internal
  quantum memory mechanism, enabling event-driven probabilistic spike generation in
  a single shot without repeated measurements.
---

# Stochastic Quantum Spiking Neural Networks with Quantum Spiking Neural Networks with Quantum Memory and Local Learning

## Quick Facts
- arXiv ID: 2506.21324
- Source URL: https://arxiv.org/abs/2506.21324
- Reference count: 40
- This paper introduces the stochastic quantum spiking (SQS) neuron and SQSNN model, which combines quantum computing with neuromorphic design. The SQS neuron uses a multi-qubit quantum circuit to implement an internal quantum memory mechanism, enabling event-driven probabilistic spike generation in a single shot without repeated measurements. This design addresses key limitations of prior quantum spiking models that relied on single-qubit implementations and multi-shot probability estimation. The proposed SQSNN architecture can be trained using a hardware-compatible local learning rule based on perturbation-based gradient estimates and global feedback signals, eliminating the need for global backpropagation on classical simulators. Experimental results on image classification tasks (MNIST, FMNIST, KMNIST) and neuromorphic data (MNIST-DVS) demonstrate that SQSNN achieves competitive or superior accuracy compared to classical SNNs and previous quantum neural models, while maintaining moderate spiking rates that suggest favorable energy efficiency. The model shows particular promise for quantum hardware with low repetition rates and mid-circuit measurement capabilities.

## Executive Summary
This paper introduces the stochastic quantum spiking (SQS) neuron and SQS neural network (SQSNN) model, which combines quantum computing with neuromorphic design. The SQS neuron uses a multi-qubit quantum circuit to implement an internal quantum memory mechanism, enabling event-driven probabilistic spike generation in a single shot without repeated measurements. This design addresses key limitations of prior quantum spiking models that relied on single-qubit implementations and multi-shot probability estimation. The proposed SQSNN architecture can be trained using a hardware-compatible local learning rule based on perturbation-based gradient estimates and global feedback signals, eliminating the need for global backpropagation on classical simulators. Experimental results on image classification tasks (MNIST, FMNIST, KMNIST) and neuromorphic data (MNIST-DVS) demonstrate that SQSNN achieves competitive or superior accuracy compared to classical SNNs and previous quantum neural models, while maintaining moderate spiking rates that suggest favorable energy efficiency. The model shows particular promise for quantum hardware with low repetition rates and mid-circuit measurement capabilities.

## Method Summary
The SQS neuron implements quantum memory using a multi-qubit circuit with N input-output qubits and N_M memory qubits. The neuron processes temporal spike patterns through RX rotations parameterized by input spike history, followed by a CRX entanglement gate and additional RX rotations. The input-output qubit is measured to generate stochastic spikes, while memory qubits retain state for temporal processing. The SQSNN architecture connects these neurons in fully-connected layers with trainable synaptic weights and PQC parameters. Training employs a local learning rule using simultaneous perturbation stochastic approximation (SPSA) for weight updates and parameter-shift rule for PQC angle optimization, requiring only global feedback signals rather than backpropagation through the network.

## Key Results
- SQSNN achieves competitive or superior accuracy compared to classical SNNs and previous quantum neural models on image classification tasks
- The model maintains moderate spiking rates, suggesting favorable energy efficiency compared to classical spiking neural networks
- SQSNN demonstrates effectiveness on both static image datasets (MNIST, FMNIST, KMNIST) and neuromorphic data (MNIST-DVS)
- The local learning rule eliminates the need for global backpropagation, making the architecture more suitable for quantum hardware implementation

## Why This Works (Mechanism)
The SQS neuron's quantum memory mechanism enables temporal processing through entangled multi-qubit states that persist across time steps. The memory qubits retain information about past spike patterns, allowing the neuron to generate context-aware spikes based on temporal history. The single-shot probabilistic spike generation eliminates the need for multiple measurements to estimate probabilities, making the model efficient for quantum hardware with low repetition rates. The local learning rule using SPSA and parameter-shift methods enables gradient-based optimization without requiring full network state information, reducing classical simulation overhead and enabling more scalable quantum implementations.

## Foundational Learning
- **Quantum Memory via Multi-Qubit Entanglement**: Uses N_M memory qubits entangled with input-output qubits to retain temporal information across time steps. Needed because single-qubit models lose state between measurements. Quick check: Verify memory qubits maintain state after input qubit measurement.
- **Simultaneous Perturbation Stochastic Approximation (SPSA)**: Estimates gradients using random perturbations with only 2 function evaluations regardless of parameter dimension. Needed for scalable local learning in high-dimensional quantum parameter spaces. Quick check: Test convergence with varying perturbation scales.
- **Parameter-Shift Rule**: Computes exact quantum gradients using shifted circuit evaluations at ±π/2. Needed because standard finite differences fail for quantum circuits with periodic parameter spaces. Quick check: Verify gradient estimates match numerical derivatives.
- **Quantum State Reset**: After measurement, the input-output qubit is reset to |0⟩ for the next time step while memory qubits retain their state. Needed to prevent state accumulation that would break the probabilistic spike generation mechanism. Quick check: Confirm reset operation doesn't affect memory qubit coherence.
- **Local Learning via Global Feedback**: Neurons update parameters using only local information and global reward signals rather than full backpropagation. Needed to reduce classical simulation overhead and enable distributed quantum implementations. Quick check: Test learning performance with varying levels of feedback noise.

## Architecture Onboarding

**Component Map**: Input Spike Train -> SQS Neuron (2-qubit circuit) -> Synaptic Weights -> SQSNN Layer -> Output Classification

**Critical Path**: Spike input → RX rotation → CRX entanglement → RX rotation → measurement → spike generation → weight update → next time step

**Design Tradeoffs**: The multi-qubit design increases circuit depth and decoherence risk but enables temporal memory and single-shot sampling. Local learning reduces classical overhead but may converge slower than global backpropagation. The hardware-compatible parameterization favors NISQ devices but may limit expressivity compared to more complex ansatze.

**Failure Signatures**: If memory qubits are inadvertently measured or reset, the model degrades to single-step processing with significantly reduced accuracy. If gradient estimates are too noisy due to insufficient perturbation samples, training fails to converge. If synaptic weight initialization is poor, neurons may saturate and stop spiking.

**First Experiments**:
1. Verify memory mechanism by disabling measurements on memory qubits - accuracy should match or exceed single-step model
2. Test gradient estimation sensitivity by varying Msyn, Msom, Mp values from defaults
3. Confirm parameter matching between architectures by computing total parameter counts

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate uncertainty regarding exact hyperparameter values, particularly learning rates, batch sizes, and regularization strengths for λ-sparsity tradeoff analysis
- The exact parameter matching procedure between architectures (SQSNN vs baselines) requires careful verification
- The memory qubit implementation's behavior under different measurement strategies could significantly impact temporal dependencies

## Confidence
- High confidence in neuron-level circuit design and memory mechanism description
- Medium confidence in overall training procedure and optimization implementation
- Medium confidence in accuracy comparisons with baselines due to potential hyperparameter differences

## Next Checks
1. Verify memory qubit behavior by testing with measurements disabled - accuracy should match or exceed single-step model
2. Test gradient estimation sensitivity by varying Msyn, Msom, Mp values from paper defaults
3. Confirm parameter matching between architectures by computing and comparing total parameter counts explicitly