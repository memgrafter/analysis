---
ver: rpa2
title: 'Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning'
arxiv_id: '2509.25052'
source_url: https://arxiv.org/abs/2509.25052
tags:
- agent
- game
- cell
- reasoning
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Cogito, Ergo Ludo (CEL), an agent that learns
  to play games by reasoning and planning rather than through extensive trial-and-error.
  The agent uses a Large Language Model to build an explicit, language-based understanding
  of game rules and strategies through a two-phase cycle: in-episode decision-making
  using lookahead search and post-episode reflection to refine its knowledge.'
---

# Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning

## Quick Facts
- **arXiv ID**: 2509.25052
- **Source URL**: https://arxiv.org/abs/2509.25052
- **Authors**: Sai Wang; Yu Wu; Zhongwen Xu
- **Reference count**: 40
- **Primary result**: CEL achieves 54%, 97%, and 84% success rates on Minesweeper, Frozen Lake, and Sokoban respectively, outperforming zero-shot baselines.

## Executive Summary
This paper presents Cogito, Ergo Ludo (CEL), an agent that learns to play games through reasoning and planning rather than trial-and-error. CEL uses a Large Language Model to build explicit, language-based understanding of game rules and strategies through a two-phase cycle: in-episode decision-making using lookahead search and post-episode reflection to refine its knowledge. The agent demonstrates strong performance across three grid-world games, achieving notable success rates while showing the ability to generalize to unseen layouts and transfer to new game environments.

## Method Summary
CEL employs a two-phase learning cycle using Qwen3-4B-Instruct via rLLM/verl. Phase 1 involves in-episode decision-making where the agent uses a Language-based World Model (LWM) to predict next states and rewards, combined with a Language-based Value Function (LVF) for action selection through one-step lookahead. Phase 2 conducts post-episode reflection where the agent performs Rule Induction to extract game rules from successful/failed trajectories and Playbook Summarization to create strategy templates. The agent iteratively updates its rule set and playbook every 5 episodes using GRPO training with a maximum context length of 8192 tokens, learning entirely from sparse rewards (+1 for win, 0 otherwise).

## Key Results
- CEL achieves 54% success on Minesweeper (5x5 grid, 3 mines), outperforming zero-shot baselines and even surpassing agents given ground-truth rules
- CEL achieves 97% success on Frozen Lake (6x6 grid, 6 holes) and 84% on Sokoban (6x6 grid, 1 box)
- The iterative rule induction and strategy refinement process is critical to learning success
- Agent demonstrates strong generalization to unseen layouts and transfer to new game environments

## Why This Works (Mechanism)
The core mechanism relies on explicit, language-based reasoning rather than implicit pattern learning. By maintaining an interpretable world model and value function as text-based representations, the agent can reason about game states symbolically, plan ahead using lookahead search, and iteratively refine its understanding through reflection on past experiences. This approach enables the agent to build compositional knowledge that generalizes beyond memorized patterns.

## Foundational Learning
- **Language-based World Modeling**: Representing state transitions and rewards as text predictions enables symbolic reasoning about game dynamics. *Why needed*: Allows the agent to reason about consequences of actions without trial-and-error. *Quick check*: Verify LWM predictions match actual environment transitions.
- **GRPO-based Reflection**: Using gradient-based reinforcement learning on language outputs for post-episode analysis. *Why needed*: Enables systematic improvement of rules and strategies from experience. *Quick check*: Monitor rule quality evolution across episodes.
- **Two-phase Learning Cycle**: Separating in-episode decision-making from post-episode reflection. *Why needed*: Prevents interference between immediate action selection and long-term knowledge refinement. *Quick check*: Ensure rules are only updated after episode completion.
- **Sparse Reward Learning**: Learning from binary win/loss signals rather than dense rewards. *Why needed*: More realistic for real-world applications where rewards are infrequent. *Quick check*: Verify agent can learn from pure outcome feedback.
- **Symbolic Rule Induction**: Extracting explicit rules from trajectories rather than learning implicit representations. *Why needed*: Creates interpretable knowledge that generalizes to new situations. *Quick check*: Test induced rules on novel game configurations.
- **Playbook Summarization**: Creating strategy templates from successful trajectories. *Why needed*: Provides high-level guidance that complements low-level rules. *Quick check*: Verify playbook improves decision-making in relevant situations.

## Architecture Onboarding

**Component map**: Environment -> Trajectory Logger -> LWM+LVF (Phase 1) -> Action Selector -> Game State -> GRPO Trainer -> Rule Induction + Playbook Summarization (Phase 2) -> Updated G_k and Π_k

**Critical path**: Environment → Trajectory logging → In-episode LWM+LVF lookahead → Action selection → Post-episode GRPO update → Rule/Playbook refinement → Next episode

**Design tradeoffs**: The explicit language-based approach prioritizes interpretability and generalization over computational efficiency. While traditional neural agents learn implicit representations that can be faster at inference, CEL trades speed for the ability to reason symbolically and transfer knowledge across domains. The use of LLMs enables rich reasoning but introduces computational overhead and context length limitations.

**Failure signatures**: Training collapse with polarized outcomes (all successes or all failures in batches) indicates reasoning breakdown. Performance stagnation or degradation suggests the iterative rule updates are not functioning properly or the reflection process is not effectively incorporating new knowledge.

**3 first experiments**:
1. Test the LWM prediction accuracy on a single game state to verify the world model can correctly predict next states and rewards.
2. Run a single episode with the current rule set to verify the action selection process works end-to-end.
3. Perform a post-episode reflection on a single trajectory to verify the rule induction and playbook summarization processes generate meaningful outputs.

## Open Questions the Paper Calls Out
The paper itself does not explicitly call out open questions, but the methodology raises several important directions for future research regarding hybrid architectures, stochastic environments, and scaling to visual domains.

## Limitations
- The method relies entirely on LLMs for planning and modeling, which is computationally expensive compared to reactive neural policies
- Performance evaluation is limited to deterministic grid-world games, with stochastic Frozen Lake modified to be deterministic
- The approach may not scale well to high-dimensional or visual domains where state representations cannot be easily compressed into simple text strings
- Critical implementation details like GRPO hyperparameters and trajectory formatting are unspecified

## Confidence
- CEL's overall performance superiority over zero-shot baselines: High confidence
- CEL's ability to learn from tabula rasa through reasoning and planning rather than trial-and-error: High confidence
- Critical importance of iterative rule induction and strategy refinement: Medium confidence
- Strong generalization to unseen layouts and transfer to new environments: Low confidence

## Next Checks
1. Manually inspect the induced rules G_k across episodes for all three games to verify they are becoming more accurate and comprehensive over time.
2. Systematically test different GRPO hyperparameters (learning rate, batch size, KL penalty) on one game to identify critical parameters affecting performance.
3. Train CEL on Frozen Lake and evaluate it zero-shot on Sokoban to quantitatively measure the claimed generalization capability.