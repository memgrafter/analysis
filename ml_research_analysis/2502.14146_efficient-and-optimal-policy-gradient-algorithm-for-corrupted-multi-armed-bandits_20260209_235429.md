---
ver: rpa2
title: Efficient and Optimal Policy Gradient Algorithm for Corrupted Multi-armed Bandits
arxiv_id: '2502.14146'
source_url: https://arxiv.org/abs/2502.14146
tags:
- corruption
- regret
- time
- samba
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies the policy gradient algorithm SAMBA to stochastic\
  \ multi-armed bandits with adversarial corruptions, where an adversary can modify\
  \ rewards up to a corruption level C. SAMBA, a Markovian policy, reduces the regret\
  \ upper bound from O(K log^2 T / \u0394) + O(C) to O(K log T / \u0394) + O(C/\u0394\
  ), achieving the optimal O(log T + C) bound."
---

# Efficient and Optimal Policy Gradient Algorithm for Corrupted Multi-armed Bandits

## Quick Facts
- arXiv ID: 2502.14146
- Source URL: https://arxiv.org/abs/2502.14146
- Authors: Jiayuan Liu; Siwei Wang; Zhixuan Fang
- Reference count: 40
- Primary result: Achieves optimal O(log T + C) regret bound for corrupted multi-armed bandits

## Executive Summary
This paper introduces SAMBA, a policy gradient algorithm that achieves optimal regret in stochastic multi-armed bandits with adversarial corruptions. SAMBA improves upon existing combinatorial algorithms by reducing the regret upper bound from O(K log²T/Δ) + O(C) to O(K log T/Δ) + O(C/Δ), achieving the optimal O(log T + C) bound. The key innovation is using a Markovian policy that limits the impact of corruptions to a single step, allowing the algorithm to maintain asymptotic optimality while outperforming existing combinatorial methods in both theory and practice.

## Method Summary
SAMBA is a policy gradient algorithm designed specifically for stochastic multi-armed bandits with adversarial corruptions. The algorithm maintains a Markovian policy that updates probabilities based on the most recent action-reward pair, which is crucial for limiting the impact of corruptions. The policy parameters are updated using gradient ascent on a smoothed version of the expected reward, with careful tuning of learning rates to balance exploration and exploitation. The algorithm's design ensures that even when rewards are corrupted, the policy can recover quickly due to the Markovian nature of the updates, which prevents corruption effects from propagating indefinitely through the policy history.

## Key Results
- Achieves optimal O(log T + C) regret bound, improving over prior O(K log²T/Δ) + O(C) combinatorial algorithms
- First combinatorial algorithm to reach asymptotic optimality in corrupted bandit settings
- Reduces one logarithmic factor (log T) compared to existing combinatorial methods
- Experimental results confirm superior empirical performance, especially in corruption-free scenarios

## Why This Works (Mechanism)
SAMBA's effectiveness stems from its Markovian policy structure, which ensures that corrupted rewards affect only the immediate next action rather than propagating through the entire policy history. This property allows the algorithm to recover quickly from corruption events while maintaining efficient exploration through policy gradient updates. The algorithm leverages the fact that in multi-armed bandit settings, the optimal policy is deterministic, and policy gradient methods can converge to this optimal policy with appropriate learning rate scheduling.

## Foundational Learning
- Multi-armed bandit fundamentals: Understanding of regret minimization and the explore-exploit tradeoff is essential for grasping the algorithm's objectives
- Policy gradient methods: Knowledge of how policy parameters are updated using gradient ascent on expected rewards is crucial for understanding SAMBA's mechanism
- Adversarial corruptions: Familiarity with how bounded reward modifications affect bandit algorithms is necessary to appreciate the corruption-dependent regret term
- Markovian policies: Understanding that the current policy depends only on the most recent action-reward pair is key to the algorithm's robustness

## Architecture Onboarding

Component map:
SAMBA -> Policy parameters -> Gradient update -> Action selection -> Reward observation -> Policy update

Critical path:
Action selection → Reward observation → Policy update → Next action selection

Design tradeoffs:
- Uses Markovian updates instead of full history to limit corruption impact
- Balances exploration (through stochastic policy) and exploitation (through gradient updates)
- Tradeoff between learning rate speed and stability of convergence

Failure signatures:
- Poor performance when corruption levels approach or exceed the learning rate scale
- Suboptimal convergence when gaps (Δ) between optimal and suboptimal arms are too small
- Potential instability if learning rates are not properly tuned

First experiments to run:
1. Test SAMBA on a simple 2-armed bandit with no corruption to verify basic functionality
2. Evaluate performance on a 10-armed bandit with moderate corruption to observe recovery behavior
3. Compare regret bounds against EXP3 and other combinatorial algorithms on synthetic data

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the analysis leaves some areas unexplored regarding the algorithm's behavior under extreme adversarial corruption patterns and the tightness of the O(C/Δ) corruption-dependent term.

## Limitations
- Assumes bounded corruption level C, which may not hold in all real-world scenarios
- Performance under adaptive adversaries that modify corruption patterns based on algorithm's past actions is unclear
- Empirical evaluation focuses on specific bandit configurations and may not capture performance across diverse problem instances

## Confidence
High: The theoretical improvement in regret bound from O(K log²T/Δ) to O(K log T/Δ) for the corruption-free term is well-supported by the analysis. The asymptotic optimality claim (O(log T + C)) is rigorously established within the paper's framework.

Medium: The practical significance of the O(C/Δ) corruption-dependent term improvement and its impact on real-world applications requires further empirical validation across broader problem domains and corruption scenarios.

## Next Checks
1. Test SAMBA's performance under adaptive adversaries that can modify corruption patterns based on the algorithm's past actions
2. Evaluate the algorithm's sensitivity to the Δ parameter by testing across bandits with varying gap distributions
3. Conduct experiments comparing SAMBA with non-combinatorial algorithms in the corrupted bandit setting to validate its practical advantages