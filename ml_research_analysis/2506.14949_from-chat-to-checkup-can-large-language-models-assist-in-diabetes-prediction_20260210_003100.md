---
ver: rpa2
title: 'From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?'
arxiv_id: '2506.14949'
source_url: https://arxiv.org/abs/2506.14949
tags:
- llms
- diabetes
- accuracy
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) for diabetes prediction using structured numerical data from the Pima Indian
  Diabetes Database (PIDD). Six LLMs, including four open-source and two proprietary
  models, were tested using zero-shot, one-shot, and three-shot prompting strategies
  and compared against traditional ML models.
---

# From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?
## Quick Facts
- arXiv ID: 2506.14949
- Source URL: https://arxiv.org/abs/2506.14949
- Authors: Shadman Sakib; Oishy Fatema Akhand; Ajwad Abrar
- Reference count: 26
- Primary result: Proprietary LLMs (GPT-4o, Gemini Flash 2.0) outperformed open-source models in diabetes prediction using structured data, with Gemma-2-27B achieving 74.35% accuracy in three-shot prompting

## Executive Summary
This study evaluates whether large language models can effectively predict diabetes using structured numerical data from the Pima Indian Diabetes Database without task-specific training. The researchers tested six LLMs using zero-shot, one-shot, and three-shot prompting strategies, comparing their performance against traditional ML models like Random Forest and SVM. While proprietary models showed superior performance, all LLMs remained below traditional ML methods, with the best achieving 74.35% accuracy versus Random Forest's 75.97%. The results demonstrate that LLMs can process structured medical data through text serialization, with few-shot prompting significantly improving performance, particularly for larger models.

## Method Summary
The study employed the Pima Indian Diabetes Database (768 samples, 8 features) to compare six LLMs (four open-source via Ollama, two proprietary via API) against traditional ML baselines. Each patient record was converted into natural language prompts requesting binary diabetes classification. The LLMs were evaluated using zero-shot, one-shot, and three-shot prompting strategies, with three runs per configuration using shuffled test splits. Traditional ML models (Random Forest, Logistic Regression, SVM) used z-score normalization and default sklearn parameters with 80/20 train-test splits. Performance was measured using accuracy, precision, recall, and F1-score across all approaches.

## Key Results
- Proprietary LLMs (GPT-4o, Gemini Flash 2.0) consistently outperformed open-source models
- Gemma-2-27B achieved the highest LLM accuracy of 74.35% in three-shot prompting
- Traditional ML methods outperformed all LLMs, with Random Forest achieving 75.97% accuracy
- Three-shot prompting consistently improved performance compared to zero-shot for most models
- Gemini Flash 2.0 showed high accuracy (73%) but low F1-score (0.4713), indicating majority-class bias

## Why This Works (Mechanism)
### Mechanism 1: Few-shot prompting improves LLM performance on structured medical classification by providing in-context task demonstration
The mechanism works by including labeled examples in prompts to help models infer decision boundaries without weight updates. Three-shot prompting improved Gemma-2-27B from 72.01% to 74.35% accuracy. This relies on the assumption that pre-trained models contain relevant medical reasoning patterns that examples can activate. The break condition is that smaller models (Llama-3.2-2B, Llama-3.1-8B) showed minimal improvement, suggesting scale-dependent activation.

### Mechanism 2: Converting structured numerical data to natural language prompts enables LLMs to perform tabular classification without architecture changes
The mechanism serializes each patient record into semantically meaningful text (e.g., "Glucose: 120, BMI: 30.5") allowing models to process medical features through their standard token interface. This assumes the model's semantic understanding of medical terms transfers to numerical pattern recognition. The break condition is that Gemini Flash 2.0 achieved 73% accuracy but only 0.4713 F1-score, showing text representation alone doesn't solve class imbalance issues.

### Mechanism 3: Larger parameter models (27B+) better leverage in-context examples for structured prediction than smaller models (2B-8B)
Greater model capacity correlates with more robust pattern recognition from few examples. Gemma-2-27B (74.35%) and GPT-4o (74.22%) outperformed Mistral-7B (60.16% three-shot) and Llama variants (≤36.98%). This assumes parameter scale enables more effective in-context learning, not just better general language capability. The break condition is that Llama-3.1-8B underperformed Llama-3.2-2B in some metrics, suggesting architecture matters alongside scale.

## Foundational Learning
- Concept: In-Context Learning
  - Why needed here: The methodology depends on LLMs learning from prompt examples without gradient updates, distinguishing this approach from fine-tuning
  - Quick check question: If you add five more examples to the prompt and performance degrades, what might be happening? (Answer: Context window saturation, example redundancy, or noise overwhelming the signal.)

- Concept: Precision vs. Recall Trade-off in Medical Contexts
  - Why needed here: The paper reports F1-score as a key metric because the dataset is imbalanced (65% non-diabetic). High accuracy with low recall indicates missed positive cases—dangerous in healthcare
  - Quick check question: Gemini Flash 2.0 had 73% accuracy but 47% F1-score. What does this suggest about its predictions? (Answer: Likely predicting the majority class frequently, missing diabetic cases.)

- Concept: Zero-Shot vs. Few-Shot Prompting
  - Why needed here: The experimental design directly compares these strategies. Knowing the difference helps interpret why three-shot improved Gemma-2-27B but not Llama-3.1-8B
  - Quick check question: A model achieves 70% zero-shot and 71% three-shot with high variance. Is few-shot helping? (Answer: Unclear—run statistical significance tests and check if variance overlaps.)

## Architecture Onboarding
- Component map: Data layer (PIDD dataset) -> Serialization layer (structured-to-text converter) -> Model interface (Ollama/API) -> Evaluation layer (metrics computation) -> Baseline comparator (scikit-learn ML)
- Critical path: 1) Convert patient record to prompt string with feature names and values 2) Prepend instruction and N labeled examples 3) Query LLM, extract binary response 4) Aggregate predictions, compute metrics 5) Repeat 3 times with shuffled test splits
- Design tradeoffs: Prompt length vs. information density (more examples improve some models but increase token cost), Open-source vs. proprietary (privacy/cost vs. performance), Accuracy vs. F1-score (Random Forest highest accuracy, Gemma-2-27B highest F1)
- Failure signatures: Low accuracy + low F1 (model not engaging with task), High accuracy + low F1 (majority-class bias), High variance across runs (LLM stochasticity)
- First 3 experiments: 1) Replicate zero-shot vs. three-shot comparison on 100-sample held-out subset 2) Test five-shot and seven-shot prompting on Gemma-2-27B 3) Add explicit class-balance instruction to Gemini Flash 2.0 prompt

## Open Questions the Paper Calls Out
1. Can domain-specific fine-tuning significantly enhance LLM accuracy on structured medical data compared to few-shot prompting? (The paper explicitly states future research should explore fine-tuning LLMs on structured datasets to bridge the performance gap with traditional ML.)

2. What is the efficacy of hybrid architectures that integrate LLMs with traditional classifiers like Random Forest or SVM? (The conclusion encourages future work on hybrid approaches to improve healthcare predictions.)

3. Does the few-shot performance observed on the Pima Indian Diabetes Database generalize to larger, more diverse clinical datasets? (The study relies on a small, homogeneous dataset, and the authors note Gemini's struggle with class imbalance.)

## Limitations
- LLMs achieved lower performance than traditional ML methods, with the best LLM (74.35%) still below Random Forest (75.97%)
- Text-serialization of structured data may introduce information loss, as evidenced by class imbalance issues affecting model behavior
- Lack of detailed hyperparameter specifications (temperature, top_p, max_tokens) and specific example selections creates reproducibility challenges

## Confidence
- High confidence: Few-shot prompting improves LLM performance on structured data - supported by consistent across-model improvements
- Medium confidence: Larger models (27B+) leverage in-context examples more effectively - performance hierarchy is clear but architectural differences beyond scale may contribute
- Low confidence: Text-serialization approach is optimal for structured medical data - no comparative analysis with other tabular-to-text methods

## Next Checks
1. Test five-shot and seven-shot prompting on Gemma-2-27B to identify performance ceiling and potential degradation points
2. Compare text-serialization approach against direct numerical prompt formats or traditional fine-tuning on the same task
3. Implement class-balance instructions in prompts for Gemini Flash 2.0 and measure F1-score improvement to address majority-class bias