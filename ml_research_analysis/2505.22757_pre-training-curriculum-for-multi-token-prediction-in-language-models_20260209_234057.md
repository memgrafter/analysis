---
ver: rpa2
title: Pre-Training Curriculum for Multi-Token Prediction in Language Models
arxiv_id: '2505.22757'
source_url: https://arxiv.org/abs/2505.22757
tags:
- curriculum
- reverse
- forward
- language
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a curriculum learning approach to improve
  the training of smaller language models using the multi-token prediction (MTP) objective.
  The authors introduce two variants: a forward curriculum that gradually increases
  prediction complexity from single-token to multi-token prediction, and a reverse
  curriculum that does the opposite.'
---

# Pre-Training Curriculum for Multi-Token Prediction in Language Models

## Quick Facts
- arXiv ID: 2505.22757
- Source URL: https://arxiv.org/abs/2505.22757
- Authors: Ansar Aynetdinov; Alan Akbik
- Reference count: 40
- Key outcome: Curriculum learning enables smaller models to better leverage multi-token prediction benefits, with byte-level models outperforming subword models

## Executive Summary
This paper introduces a curriculum learning approach to improve multi-token prediction (MTP) training in language models. The authors propose two curriculum variants - forward (simple to complex) and reverse (complex to simple) - that gradually adjust the number of tokens predicted per step. Their approach enables smaller models to achieve better downstream performance and generative quality while maintaining the inference speed advantages of MTP. Notably, byte-level tokenization consistently outperforms subword tokenization across all configurations, challenging conventional wisdom about tokenization efficiency.

## Method Summary
The authors develop curriculum learning strategies for MTP by adjusting the number of tokens predicted per training step. The forward curriculum starts with single-token prediction and gradually increases to multi-token prediction, while the reverse curriculum does the opposite. They evaluate these approaches across different model sizes (125M-1.3B parameters) and tokenization schemes (byte-level vs subword). The curriculum is controlled by a complexity metric based on N-choose-k calculations that determines how many tokens are predicted at each training step. The approach maintains the self-speculative decoding capability of MTP while improving model quality through structured learning progression.

## Key Results
- Forward curriculum enables smaller models to achieve better downstream performance than static MTP approaches
- Byte-level models consistently outperform subword models across all configurations, achieving 1.2x higher quality scores
- Forward curriculum maintains self-speculative decoding speed advantages while reverse curriculum achieves higher metrics but loses inference benefits

## Why This Works (Mechanism)
The curriculum approach works by gradually increasing task complexity, allowing models to first master single-token prediction before tackling multi-token prediction. This progressive difficulty scaling helps smaller models build foundational skills before facing the increased complexity of predicting multiple tokens simultaneously. The byte-level tokenization likely benefits from MTP because smaller semantic units (bytes vs subwords) reduce the combinatorial complexity of predicting multiple tokens, making the task more manageable while preserving more granular linguistic information.

## Foundational Learning
- **Multi-Token Prediction (MTP)**: Language modeling objective where models predict multiple future tokens simultaneously rather than one at a time. Needed because it enables faster inference through speculative decoding. Quick check: Does the model predict k tokens at once instead of 1?
- **Curriculum Learning**: Training strategy that starts with easier examples and progressively increases difficulty. Needed because it helps models build foundational skills before tackling complex tasks. Quick check: Is there a scheduled progression from simple to complex examples?
- **Self-Speculative Decoding**: Inference optimization where the model generates multiple tokens and verifies them in parallel. Needed because it provides 2-4x inference speedup compared to autoregressive decoding. Quick check: Does the model generate and verify tokens in batches?
- **Tokenization Schemes**: Methods for converting text to model input (byte-level vs subword). Needed because they fundamentally affect model performance and complexity of prediction tasks. Quick check: Are tokens discrete units (subwords) or continuous bytes?
- **N-choose-k Complexity Metric**: Formula for calculating prediction difficulty based on token sequence length. Needed because it provides a principled way to schedule curriculum progression. Quick check: Does complexity scale as C(n,k) = n!/(k!(n-k)!)?

## Architecture Onboarding

Component Map:
BPE/BPET Tokenizer -> Curriculum Scheduler -> MTP Model (125M-1.3B) -> Evaluation Pipeline -> Downstream Tasks

Critical Path:
Tokenization → Curriculum Complexity Calculation → Multi-Token Prediction → Self-Speculative Decoding Verification → Quality Assessment

Design Tradeoffs:
- Token granularity vs model capacity: Byte-level provides better MTP performance but requires more parameters per token
- Curriculum speed vs final quality: Forward curriculum trains slower initially but achieves better final performance
- Inference speed vs accuracy: MTP provides faster inference but may sacrifice some accuracy compared to autoregressive approaches

Failure Signatures:
- Curriculum collapse: If complexity metric is poorly calibrated, models may fail to progress beyond simple predictions
- Tokenization mismatch: Suboptimal tokenization can negate MTP benefits regardless of curriculum design
- Capacity limitations: Smaller models may not benefit from MTP even with curriculum training if they lack sufficient parameters

First Experiments:
1. Compare single-token vs multi-token prediction baselines without curriculum to establish performance gap
2. Test forward curriculum with different progression speeds to find optimal scheduling
3. Evaluate byte-level vs subword tokenization with identical MTP curriculum to isolate tokenization effects

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Evaluation limited to base models without instruction tuning, reducing real-world applicability
- Custom byte-level tokenization approach may not generalize to other byte-level methods
- Curriculum effectiveness depends on chosen complexity metric which may not capture all difficulty aspects
- Qualitative differences in generated text between curriculum and non-curriculum approaches not analyzed

## Confidence
- High confidence in curriculum learning improving MTP training for smaller models
- Medium confidence in byte-level vs subword performance comparison
- Medium confidence in inference speed claims due to synthetic benchmark limitations

## Next Checks
1. Evaluate curriculum-trained models after instruction tuning to assess practical applicability
2. Test curriculum approach with alternative tokenization methods (WordPiece, SentencePiece)
3. Conduct ablation studies on complexity metric to test alternative difficulty measures