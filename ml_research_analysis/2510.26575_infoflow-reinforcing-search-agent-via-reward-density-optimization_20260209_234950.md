---
ver: rpa2
title: 'InfoFlow: Reinforcing Search Agent Via Reward Density Optimization'
arxiv_id: '2510.26575'
source_url: https://arxiv.org/abs/2510.26575
tags:
- arxiv
- search
- reasoning
- preprint
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfoFlow, a reinforcement learning framework
  designed to address the challenge of low reward density in training LLM agents for
  deep search tasks. By combining sub-goal reward shaping, adaptive off-policy hints,
  and a dual-agent architecture, InfoFlow provides denser process-level supervision
  that enables more efficient and stable learning.
---

# InfoFlow: Reinforcing Search Agent Via Reward Density Optimization

## Quick Facts
- arXiv ID: 2510.26575
- Source URL: https://arxiv.org/abs/2510.26575
- Reference count: 40
- Key outcome: InfoFlow enables lightweight models to achieve performance competitive with larger proprietary LLMs on complex deep search tasks through reward density optimization

## Executive Summary
This paper introduces InfoFlow, a reinforcement learning framework designed to address the challenge of low reward density in training LLM agents for deep search tasks. By combining sub-goal reward shaping, adaptive off-policy hints, and a dual-agent architecture, InfoFlow provides denser process-level supervision that enables more efficient and stable learning. The framework employs a researcher agent for reasoning and planning, paired with a refiner agent that synthesizes retrieved evidence into concise summaries, reducing cognitive load and improving performance. InfoFlow is evaluated on multiple agentic search benchmarks, demonstrating superior generalization and outperforming strong baselines.

## Method Summary
InfoFlow addresses the fundamental challenge of sparse rewards in deep search tasks through a multi-faceted approach. The framework implements sub-goal reward shaping to provide intermediate rewards throughout the search process, adaptive off-policy hints to guide exploration, and a dual-agent architecture that separates reasoning from evidence synthesis. The researcher agent handles complex reasoning and planning decisions, while the refiner agent processes retrieved information into structured summaries. This division of labor reduces cognitive load and enables more effective learning from sparse terminal rewards. The system is trained using reinforcement learning with carefully designed reward shaping that provides meaningful feedback at multiple stages of the search process.

## Key Results
- InfoFlow demonstrates superior generalization across multiple agentic search benchmarks compared to strong baselines
- On the BrowseComp-Plus benchmark, a lightweight model using InfoFlow achieves performance competitive with much larger proprietary LLMs
- The dual-agent architecture with reward density optimization shows significant improvements in learning efficiency and stability

## Why This Works (Mechanism)
InfoFlow works by fundamentally restructuring the learning problem from one of sparse terminal rewards to one with denser intermediate feedback. The sub-goal reward shaping provides immediate rewards for meaningful intermediate steps in the search process, allowing the agent to learn which actions contribute to successful outcomes even when the final reward is distant. The adaptive off-policy hints mechanism guides exploration toward promising regions of the search space without requiring extensive on-policy data collection. The dual-agent architecture addresses the cognitive complexity of deep search by separating high-level reasoning (researcher agent) from low-level information processing (refiner agent), allowing each component to specialize and learn more effectively.

## Foundational Learning
- **Reinforcement Learning with Sparse Rewards**: Needed because deep search tasks typically provide feedback only at the end of long action sequences; quick check: measure learning curves with and without reward shaping
- **Reward Shaping Theory**: Required to understand how to construct intermediate rewards that preserve optimal policies; quick check: verify that shaped rewards lead to same optimal policy as original sparse rewards
- **Dual-Agent Systems**: Essential for separating reasoning from execution to reduce cognitive load; quick check: compare single-agent vs dual-agent performance on complex tasks
- **Off-Policy Learning**: Important for efficient use of historical data and exploration; quick check: measure sample efficiency with different off-policy algorithms
- **Information Retrieval Integration**: Critical for connecting LLM reasoning with external knowledge sources; quick check: evaluate retrieval quality metrics alongside final task performance

## Architecture Onboarding

**Component Map**: User Query -> Researcher Agent (Planning/Reasoning) -> Search Actions -> Document Retrieval -> Refiner Agent (Evidence Synthesis) -> Summary/Answer -> Reward Signal

**Critical Path**: The most important sequence is: Query → Researcher Agent → Search Actions → Document Retrieval → Refiner Agent → Final Answer, as this represents the core reasoning-to-execution pipeline that generates the primary output.

**Design Tradeoffs**: The dual-agent architecture trades increased model complexity and inference overhead for improved learning efficiency and reduced cognitive load. This design choice prioritizes learning effectiveness over computational simplicity, accepting the cost of maintaining and synchronizing two agents for the benefit of more stable and efficient training.

**Failure Signatures**: Common failure modes include: researcher agent getting stuck in local search patterns due to insufficient exploration, refiner agent producing incoherent summaries when faced with contradictory evidence, and reward shaping creating spurious local optima that distract from true task objectives.

**3 First Experiments**:
1. Compare learning curves with and without sub-goal reward shaping to isolate the impact of denser rewards
2. Evaluate the contribution of the refiner agent by comparing performance with researcher-only vs dual-agent configurations
3. Test adaptive vs static off-policy hints to measure the benefit of dynamic exploration guidance

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation benchmarks may not fully capture the complexity of real-world deep search scenarios with more dynamic and unpredictable reward structures
- The dual-agent architecture introduces computational overhead and potential synchronization challenges during inference
- Uncertainty exists about how well reward density optimization generalizes to domains with fundamentally different information structures or where sub-goals are less well-defined

## Confidence
- **High confidence**: The core framework design (dual-agent architecture, reward shaping) and baseline comparisons
- **Medium confidence**: Generalization claims across diverse search tasks and lightweight model performance parity
- **Medium confidence**: The specific mechanisms by which reward density optimization improves learning efficiency

## Next Checks
1. Test InfoFlow on open-domain question answering benchmarks with varying levels of reward sparsity to assess robustness across different reward density regimes
2. Conduct ablation studies isolating the contributions of each component (sub-goal shaping, off-policy hints, dual-agent design) to quantify their individual impact
3. Evaluate inference-time computational overhead and latency compared to single-agent baselines to assess practical deployment considerations