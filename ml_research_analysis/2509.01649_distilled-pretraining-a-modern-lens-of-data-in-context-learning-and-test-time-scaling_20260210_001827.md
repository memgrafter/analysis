---
ver: rpa2
title: 'Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time
  Scaling'
arxiv_id: '2509.01649'
source_url: https://arxiv.org/abs/2509.01649
tags:
- distillation
- pretraining
- teacher
- data
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Distilled pretraining yields models with better test-time scaling\
  \ but worse in-context learning. While distillation improves pass@k scores and generation\
  \ diversity\u2014even matching models trained on twice the data\u2014it impairs\
  \ induction head learning and copying abilities critical for in-context tasks."
---

# Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling

## Quick Facts
- **arXiv ID**: 2509.01649
- **Source URL**: https://arxiv.org/abs/2509.01649
- **Reference count**: 40
- **Primary result**: Distilled pretraining yields better test-time scaling but worse in-context learning

## Executive Summary
This paper investigates the tradeoffs of distilled pretraining in language models, focusing on its impact on test-time scaling and in-context learning. Through controlled experiments on synthetic bigram datasets, the authors show that distillation improves generation quality and diversity, even outperforming models trained on twice the data, but impairs the learning of induction heads and copying abilities crucial for in-context tasks. The study provides a mechanistic explanation for these effects, linking them to differential learning rates of high-entropy and low-entropy patterns in the data. Token routing and teacher selection strategies are proposed to mitigate the negative impacts on in-context learning.

## Method Summary
The authors conduct a series of controlled experiments using small (120M-parameter) models on a synthetic bigram dataset. They compare models trained with and without distillation, measuring their performance on in-context learning tasks (e.g., copying, induction heads) and test-time scaling (e.g., pass@k scores, generation diversity). To isolate the effects of distillation, they analyze attention patterns and learning dynamics, focusing on how different entropy patterns are learned. The study also proposes token routing and teacher selection strategies to mitigate the negative impacts on in-context learning, though these are not fully validated in the main experiments.

## Key Results
- Distilled pretraining improves pass@k scores and generation diversity, matching models trained on twice the data.
- Distillation impairs induction head learning and copying abilities critical for in-context tasks.
- The tradeoff is explained by distillation accelerating learning of high-entropy rows while hindering low-entropy, induction-head-like rows in the bigram sandbox.

## Why This Works (Mechanism)
Distilled pretraining improves test-time scaling by accelerating the learning of high-entropy patterns in the data, leading to better generation quality and diversity. However, this same mechanism impairs the learning of low-entropy, induction-head-like patterns crucial for in-context learning tasks. The synthetic bigram sandbox allows for controlled isolation of these mechanisms, revealing how distillation differentially affects the learning of different entropy patterns.

## Foundational Learning
- **Induction heads**: Mechanisms that enable in-context learning by generalizing from examples; critical for tasks like copying and pattern recognition.
- **Test-time scaling**: The ability of models to improve performance as more test-time computation is applied, often measured by pass@k scores.
- **Generation diversity**: The variety and quality of outputs produced by a model, often evaluated through metrics like diversity scores.
- **Bigram sandbox**: A synthetic dataset used to isolate and study specific learning mechanisms in a controlled environment.
- **Token routing**: A strategy to selectively apply distillation to certain tokens or patterns, potentially mitigating negative impacts on in-context learning.
- **Teacher selection**: The process of choosing which model or data to use as a teacher in distillation, influencing the quality and diversity of the distilled model.

## Architecture Onboarding
- **Component map**: Data -> Model (with/without distillation) -> In-context learning tasks / Test-time scaling tasks
- **Critical path**: Pretraining (with/without distillation) -> Learning of high-entropy vs. low-entropy patterns -> Impact on in-context learning and test-time scaling
- **Design tradeoffs**: Balancing improved generation quality and diversity against impaired in-context learning abilities.
- **Failure signatures**: Poor performance on copying tasks and induction head learning when distillation is applied.
- **First experiments**:
  1. Compare models trained with and without distillation on synthetic copying tasks.
  2. Analyze attention patterns to identify induction head learning differences.
  3. Measure pass@k scores and generation diversity for both distilled and non-distilled models.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis is constrained to small-scale experiments (120M-parameter models) and synthetic data, limiting direct extrapolation to state-of-the-art models.
- The bigram sandbox may not capture the full complexity of real-world pretraining data distributions.
- The proposed token routing and teacher selection strategies are conceptually promising but not empirically validated in the main experiments.

## Confidence
- **High confidence**: The empirical observation that distilled pretraining improves pass@k scores and generation diversity on synthetic tasks.
- **Medium confidence**: The mechanistic explanation linking distillation's impact to differential learning rates of high-entropy vs. low-entropy rows in the bigram sandbox.
- **Medium confidence**: The claim that distillation impairs induction head learning and copying abilities, based on qualitative analysis of attention patterns and synthetic copying tasks.

## Next Checks
1. Replicate the main scaling and in-context learning experiments on larger models (e.g., 1B+ parameters) and with real-world pretraining data to test the robustness of the observed tradeoffs.
2. Conduct ablation studies to isolate the contribution of specific components of the distillation process (e.g., teacher selection, temperature scaling) to the observed effects on induction heads and generation diversity.
3. Evaluate the proposed token routing and teacher selection strategies on a held-out set of downstream tasks to quantify their impact on mitigating the negative effects of distillation on in-context learning.