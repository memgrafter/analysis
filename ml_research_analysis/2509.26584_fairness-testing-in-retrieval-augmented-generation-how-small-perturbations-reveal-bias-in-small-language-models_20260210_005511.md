---
ver: rpa2
title: 'Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations
  Reveal Bias in Small Language Models'
arxiv_id: '2509.26584'
source_url: https://arxiv.org/abs/2509.26584
tags:
- fairness
- testing
- bias
- demographic
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a fairness testing methodology for Retrieval-Augmented
  Generation (RAG) systems using metamorphic testing with demographic perturbations.
  The approach evaluates three Small Language Models (SLMs) integrated into RAG pipelines,
  introducing 21 demographic variations to assess fairness in sentiment analysis.
---

# Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models

## Quick Facts
- arXiv ID: 2509.26584
- Source URL: https://arxiv.org/abs/2509.26584
- Reference count: 33
- Three Small Language Models tested show up to 1/3 metamorphic relation failures under demographic perturbations

## Executive Summary
This study introduces a fairness testing methodology for Retrieval-Augmented Generation (RAG) systems using metamorphic testing with demographic perturbations. The approach evaluates three Small Language Models (SLMs) integrated into RAG pipelines, introducing 21 demographic variations to assess fairness in sentiment analysis. Results show that up to one third of metamorphic relations fail under demographic perturbations, with racial cues being the predominant cause of violations. The Retriever Robustness Score (RRS) quantifies retrieval instability, revealing that the retrieval component itself introduces significant bias.

## Method Summary
The methodology employs metamorphic testing principles to evaluate fairness in RAG systems by systematically perturbing demographic information in queries. Researchers created 20 base test cases representing sentiment analysis questions and generated 21 demographic variations for each, resulting in 420 test cases. They defined 11 metamorphic relations based on sentiment polarity changes, then evaluated three SLM models (Mistral-7B, Llama-3-8B, Gemma-2-9B) and their variations in both standalone and RAG configurations. The Retriever Robustness Score measures retrieval stability by comparing top-k results across demographic perturbations.

## Key Results
- Up to 1/3 of metamorphic relations failed when demographic perturbations were applied to RAG systems
- Racial demographic cues were the predominant cause of fairness violations across all tested models
- The retrieval component itself introduced significant bias, as quantified by the Retriever Robustness Score

## Why This Works (Mechanism)
The methodology works by exposing hidden biases through controlled perturbations that would otherwise remain undetected in standard testing. By systematically varying demographic information while keeping all other query aspects constant, the approach reveals how different demographic representations affect both retrieval and generation components. The metamorphic relations provide a framework for detecting inconsistent behavior that indicates bias, while the component-level analysis isolates where bias originates - whether in retrieval, generation, or both.

## Foundational Learning
- Metamorphic Testing: A software testing approach that checks properties of multiple related test executions rather than expected outputs. Needed to detect subtle fairness violations that standard testing misses. Quick check: Does the system maintain expected relationships between perturbed and unperturbed inputs?
- RAG Architecture Components: Understanding the retriever-generator pipeline is crucial for isolating bias sources. Quick check: Can each component be tested independently while maintaining end-to-end functionality?
- Demographic Perturbation: Systematic variation of identity-related information in queries. Needed to reveal how demographic factors influence model behavior. Quick check: Are perturbations realistic and representative of actual user diversity?
- Fairness Metrics: Quantitative measures like RRS that capture bias beyond binary pass/fail. Needed for nuanced assessment of system behavior. Quick check: Do metrics correlate with human judgment of fairness?

## Architecture Onboarding

**Component Map:** User Query -> Retriever -> Generator -> Response

**Critical Path:** Query preprocessing → Embedding generation → Document retrieval → Answer generation → Post-processing

**Design Tradeoffs:** Balancing retrieval accuracy with fairness requires choosing between model size (affecting bias potential) and computational efficiency. Larger models may reduce bias but increase resource requirements and latency.

**Failure Signatures:** Consistent sentiment shifts across demographic perturbations indicate bias in generation; retrieval instability suggests bias in the retriever. Mixed patterns point to interaction effects between components.

**First Experiments:**
1. Test baseline metamorphic relations without demographic perturbations to establish expected behavior patterns
2. Apply single-attribute demographic perturbations to isolate individual bias effects
3. Compare RRS scores across different embedding models to identify retrieval bias sources

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size (20 test cases) may not capture full complexity of real-world RAG applications
- Focus on sentiment analysis questions may not generalize to other task domains
- Evaluation of only three SLM models limits generalizability to other architectures

## Confidence
- Core finding (retrieval introduces bias): Medium-High
- Generalizability across domains: Medium
- Methodological applicability to other RAG architectures: Medium

## Next Checks
1. Expand testing to include additional task types beyond sentiment analysis, such as fact-based questions and complex reasoning tasks, to validate the methodology's applicability across different RAG use cases.
2. Incorporate intersectional demographic perturbations to assess how multiple identity factors interact and compound bias in RAG systems.
3. Test additional RAG architectures and embedding models to evaluate the robustness of the findings across different system configurations and implementation choices.