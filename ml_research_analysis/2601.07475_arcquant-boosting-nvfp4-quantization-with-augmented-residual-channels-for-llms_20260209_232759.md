---
ver: rpa2
title: 'ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for
  LLMs'
arxiv_id: '2601.07475'
source_url: https://arxiv.org/abs/2601.07475
tags:
- arcquant
- nvfp4
- quantization
- formats
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARCQuant is a Post-Training Quantization framework that enables
  high-fidelity 4-bit inference for LLMs on NVFP4 hardware. It augments the activation
  matrix with quantized residual channels to compensate for outlier-induced quantization
  errors, preserving the fine-grained block isolation property of NVFP4 while using
  standard GEMM kernels.
---

# ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs

## Quick Facts
- arXiv ID: 2601.07475
- Source URL: https://arxiv.org/abs/2601.07475
- Reference count: 33
- ARCQuant achieves up to 3x speedup over FP16 with 1.5-2.8x memory savings on RTX 5090 and RTX PRO 6000

## Executive Summary
ARCQuant is a post-training quantization framework that enables high-fidelity 4-bit inference for large language models on NVFP4 hardware. The method augments activation matrices with quantized residual channels to compensate for outlier-induced quantization errors while preserving the fine-grained block isolation property of NVFP4. Theoretical analysis demonstrates that ARCQuant's worst-case error bound matches that of 8-bit formats, while experimental results show state-of-the-art accuracy with perplexity and downstream task performance close to full-precision on Llama 3.1-8B and Qwen2.5 families.

## Method Summary
ARCQuant addresses the challenge of quantization-induced errors in LLMs by augmenting activation matrices with quantized residual channels. The framework specifically targets NVFP4 hardware, leveraging its fine-grained block isolation property while using standard GEMM kernels. By incorporating residual channels that capture quantization errors from outliers, ARCQuant maintains high fidelity during 4-bit inference. The approach involves outlier-aware scaling and residual channel computation during the quantization process, allowing the method to achieve theoretical error bounds comparable to 8-bit quantization formats.

## Key Results
- Achieves up to 3x speedup over FP16 on RTX 5090 and RTX PRO 6000
- Delivers 1.5-2.8x memory savings compared to FP16 inference
- Matches or exceeds state-of-the-art accuracy on Llama 3.1-8B and Qwen2.5 families with perplexity and downstream task performance close to full-precision

## Why This Works (Mechanism)
ARCQuant works by augmenting the activation matrix with quantized residual channels that compensate for quantization errors, particularly those induced by outliers. This augmentation preserves the fine-grained block isolation property of NVFP4 while allowing the use of standard GEMM kernels. The residual channels capture information that would otherwise be lost during aggressive 4-bit quantization, effectively bridging the gap between 4-bit and higher-bit formats in terms of accuracy. The method's theoretical foundation ensures that the worst-case error bound matches that of 8-bit quantization, providing a strong mathematical guarantee for its practical performance.

## Foundational Learning

**NVFP4 Quantization**: A 4-bit floating-point format with fine-grained block isolation - needed for understanding the hardware-specific constraints and opportunities ARCQuant exploits; quick check: verify block size and isolation properties in hardware documentation.

**Quantization Error Bounds**: Mathematical analysis of worst-case errors in low-bit quantization - needed to understand why 4-bit formats typically underperform 8-bit formats; quick check: compare error bounds for FP4 vs FP8 under typical activation distributions.

**Residual Channel Computation**: Technique for capturing and representing quantization errors as additional channels - needed to understand how ARCQuant preserves information lost during quantization; quick check: validate residual channel effectiveness on synthetic outlier distributions.

**Outlier-Aware Scaling**: Dynamic scaling strategies that account for extreme values in activation matrices - needed to understand how ARCQuant handles the most problematic elements for quantization; quick check: measure impact of outlier scaling on quantization error distribution.

**GEMM Kernel Optimization**: Techniques for optimizing general matrix multiplication operations - needed to understand how ARCQuant maintains computational efficiency despite additional operations; quick check: benchmark standard vs optimized GEMM performance on NVFP4.

## Architecture Onboarding

**Component Map**: Input activations -> Outlier detection -> Residual channel computation -> Augmented activation matrix -> Standard GEMM kernels -> Output with compensation

**Critical Path**: The most critical computational path involves outlier detection and residual channel computation, as these steps directly impact the quality of the augmented activation matrix and ultimately determine quantization fidelity.

**Design Tradeoffs**: ARCQuant trades additional memory overhead for residual channels against improved accuracy and reduced quantization error. The framework prioritizes accuracy preservation over minimal memory usage, accepting the overhead to achieve near-full-precision performance.

**Failure Signatures**: Performance degradation typically manifests as increased perplexity in language modeling tasks or reduced accuracy in downstream evaluations, particularly when outlier distributions are non-stationary or when residual channels fail to capture quantization errors effectively.

**First Experiments**:
1. Benchmark ARCQuant on a small LLM (1B-3B parameters) with synthetic outlier distributions to validate error compensation.
2. Compare perplexity degradation between ARCQuant and standard 4-bit quantization on a held-out validation set.
3. Measure memory overhead and runtime performance on RTX 5090 with varying batch sizes and sequence lengths.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Hardware dependency on NVFP4 limits generalizability to other quantization formats or hardware platforms.
- Additional memory overhead from residual channels may impact scalability for extremely large models.
- Potential challenges with extreme outliers or non-stationary activation distributions.

## Confidence

**High Confidence**:
- 3x speedup over FP16 on RTX 5090 and RTX PRO 6000
- 1.5-2.8x memory savings compared to FP16
- Theoretical error bounds matching 8-bit formats

**Medium Confidence**:
- State-of-the-art accuracy on tested model families
- Preservation of NVFP4 block isolation property

**Low Confidence**:
- Generalizability to non-NVIDIA hardware
- Scalability for models >100B parameters

## Next Checks
1. Test ARCQuant on non-NVIDIA hardware (AMD Instinct, Intel Gaudi) to assess hardware portability.
2. Evaluate scalability on models larger than 70B parameters to confirm memory efficiency.
3. Assess robustness under dynamic activation distributions and extreme outlier scenarios.