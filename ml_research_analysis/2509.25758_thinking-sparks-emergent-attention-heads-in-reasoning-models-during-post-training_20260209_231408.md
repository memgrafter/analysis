---
ver: rpa2
title: 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post
  Training'
arxiv_id: '2509.25758'
source_url: https://arxiv.org/abs/2509.25758
tags:
- heads
- reasoning
- attention
- think
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study uses circuit analysis to reveal how post-training methods\
  \ create specialized attention heads that support complex reasoning in language\
  \ models. Different training paradigms\u2014distillation, supervised fine-tuning\
  \ (SFT), and group relative policy optimization (GRPO)\u2014induce distinct patterns\
  \ of emergent heads."
---

# Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training

## Quick Facts
- **arXiv ID**: 2509.25758
- **Source URL**: https://arxiv.org/abs/2509.25758
- **Reference count**: 40
- **Primary result**: Post-training methods create specialized attention heads that support complex reasoning in language models.

## Executive Summary
This study uses circuit analysis to reveal how post-training methods create specialized attention heads that support complex reasoning in language models. Different training paradigms—distillation, supervised fine-tuning (SFT), and group relative policy optimization (GRPO)—induce distinct patterns of emergent heads. Distillation and SFT steadily add large numbers of reasoning heads, particularly in middle-to-late layers, while GRPO dynamically activates and prunes heads in response to reward signals, leading to sparse but high-impact updates. Controllable "think on/off" models do not have dedicated reasoning heads; instead, disabling explicit reasoning triggers a broader but less efficient set of compensatory heads. Ablation experiments confirm that emergent heads are causally responsible for improved performance but also introduce trade-offs, such as over-thinking on simple tasks.

## Method Summary
The authors use edge attribution patching (EAP-IG) to identify attention heads that causally contribute to reasoning performance. They compare circuit structures between baseline and post-trained models on reasoning tasks, identifying emergent heads through ablation studies. The methodology tracks head activation patterns across training steps and validates causal relationships through targeted head manipulation experiments.

## Key Results
- Distillation and SFT add stable reasoning heads that persist through training, predominantly in middle-to-late layers
- GRPO induces sparse, reward-driven head activation and pruning with dynamic survival patterns
- Controllable think-on/off models compensate for disabled reasoning through broader but less efficient head activation
- Emergent heads are causally responsible for improved reasoning performance but can cause over-thinking on simple tasks

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Head Installation via SFT and Distillation
Supervised fine-tuning and distillation add stable reasoning heads that persist through training, predominantly in middle-to-late layers. These methods impose a direct imitation signal that incrementally activates previously dormant attention heads and entrenches them into the computational graph. Once added, these heads tend to survive to the final checkpoint, forming new pathways that support structured, procedure-following reasoning.

### Mechanism 2: Reward-Guided Dynamic Search via GRPO
Reinforcement learning (GRPO) induces a sparse, fluctuating set of attention heads whose survival is tightly coupled to reward signal fluctuations. GRPO samples multiple responses per prompt, assigns outcome-based rewards, and updates the policy to reinforce high-reward paths. At the circuit level, this manifests as exploration (activating new heads) followed by exploitation (pruning heads that fail to contribute to reward).

### Mechanism 3: Compensatory Activation Under Think-Off Mode
Disabling explicit reasoning (think-off) does not reveal dedicated thinking heads, but instead triggers a broad, inefficient compensatory activation across many heads. The think-on mode appears to use a relatively efficient, specialized circuit embedded during training. When this pathway is blocked, the model compensates by recruiting a wider set of attention heads, which can maintain coverage at high sample budgets but reduce efficiency at low budgets.

## Foundational Learning

- **Circuit analysis and edge attribution patching (EAP-IG)**: Why needed - The entire methodology depends on identifying which attention heads causally contribute to reasoning by ablating edges and measuring performance impact. Quick check - Can you explain how integrated gradients are used to rank edge importance in a transformer circuit?

- **Attention head layer specialization**: Why needed - The paper shows that distillation/SFT heads concentrate in early-mid or mid-to-late layers, while GRPO heads are more distributed; interpreting results requires understanding where heads are and why layer position matters. Quick check - What is the typical functional difference between early-layer vs. late-layer attention heads in transformer language models?

- **Explore-exploit dynamics in reinforcement learning**: Why needed - GRPO's dynamic head activation and pruning is framed as an explore-exploit process; understanding this connection is essential to interpret why RL produces sparse, targeted circuits. Quick check - How does the explore-exploit trade-off manifest in policy gradient methods for LLM fine-tuning?

## Architecture Onboarding

- **Component map**: Baseline model → Post-training (SFT/Distillation or GRPO) → Circuit extraction via EAP-IG → Identification of emergent heads → Ablation/activation scaling for validation
- **Critical path**: Train or load a post-trained model variant → Construct circuits for target tasks → Compare circuits against baseline to identify emergent heads → Validate causality by ablating emergent heads and measuring performance drop
- **Design tradeoffs**: Dense, stable head addition (SFT/Distillation) vs. sparse, reward-driven head search (GRPO); Performance vs. robustness; Efficiency vs. coverage
- **Failure signatures**: Over-thinking (excessively long reasoning chains, calculation errors); Reward misalignment (spurious reward correlations); Compensation collapse (insufficient compensatory heads)
- **First 3 experiments**: 1) Circuit construction and comparison for standardized reasoning benchmark; 2) Ablation inference by zeroing top-k emergent heads across multiple benchmarks; 3) Activation scaling by amplifying/attenuating emergent head activations

## Open Questions the Paper Calls Out

### Open Question 1
Can training policies be designed to encourage targeted head activation and prevent uncontrolled reasoning head growth during post-training? The conclusion states findings "motivate attention head informed training policies that (i) encourage targeted head activation rather than uncontrolled head growth." Current post-training methods induce cumulative head addition without mechanisms to control which heads emerge or persist.

### Open Question 2
Can reward shaping jointly optimize both reasoning plan quality and calculation reliability to mitigate the trade-off where strengthened reasoning heads cause errors on simpler tasks? The conclusion calls for using "reward shaping to jointly optimize plan quality and calculation reliability." The paper documents that emergent reasoning heads improve difficult problem-solving but introduce "over-thinking" failure modes.

### Open Question 3
Do the observed circuit-level dynamics (SFT's cumulative head addition vs. GRPO's dynamic pruning) generalize across different model architectures beyond the Qwen family? The study exclusively analyzes Qwen-based models without validating on other architectures. Circuit formation patterns may be architecture-dependent.

## Limitations

- Core findings rest on untested assumptions about the interpretability and causal role of attention heads in complex reasoning
- Claim that GRPO produces "sparse but high-impact updates" is based on head count statistics rather than functional analysis
- Compensatory activation mechanism under think-off mode is particularly speculative without demonstrating same computational function through different pathways

## Confidence

**High confidence**: Empirical observations about head emergence patterns (SFT/distillation adding stable heads in specific layers, GRPO showing dynamic activation/pruning) are well-supported by presented data and ablation results.

**Medium confidence**: Interpretation that emergent heads are causally responsible for reasoning improvements, while supported by ablation experiments, does not fully rule out alternative explanations such as changes in head coordination or residual stream dynamics.

**Low confidence**: Mechanistic claims about why different training methods produce different head patterns are plausible but not rigorously demonstrated. The compensatory activation explanation for think-off mode is particularly speculative.

## Next Checks

1. **Functional head characterization**: Use activation patching and attribution analysis to determine what specific computations emergent heads perform during reasoning steps, validating whether GRPO heads implement genuinely different reasoning strategies.

2. **Reward signal ablation study**: Train GRPO models with corrupted or randomized rewards to determine whether observed head dynamics depend on meaningful reward signals or would occur with any gradient signal.

3. **Cross-architecture replication**: Apply the same circuit analysis methodology to different model families (e.g., decoder-only vs. encoder-decoder, different scales) to determine whether emergent head patterns are universal properties of reasoning model post-training or specific to examined architectures.