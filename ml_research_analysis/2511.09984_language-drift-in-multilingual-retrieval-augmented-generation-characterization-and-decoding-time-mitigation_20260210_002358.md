---
ver: rpa2
title: 'Language Drift in Multilingual Retrieval-Augmented Generation: Characterization
  and Decoding-Time Mitigation'
arxiv_id: '2511.09984'
source_url: https://arxiv.org/abs/2511.09984
tags:
- language
- target
- multilingual
- across
- drift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of language drift in multilingual
  Retrieval-Augmented Generation (RAG), where models generate responses in an unintended
  language despite receiving prompts in the target language. The authors show that
  cross-lingual retrieved context often causes language inconsistency, with English
  acting as a dominant attractor, especially under Chain-of-Thought reasoning.
---

# Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation

## Quick Facts
- arXiv ID: 2511.09984
- Source URL: https://arxiv.org/abs/2511.09984
- Reference count: 23
- Language drift in multilingual RAG leads to unintended language generation despite target-language prompts

## Executive Summary
This paper addresses language drift in multilingual Retrieval-Augmented Generation (RAG), where models generate responses in unintended languages despite receiving prompts in the target language. The authors identify that cross-lingual retrieved context, particularly English, acts as a dominant attractor causing language inconsistency. They propose Soft Constrained Decoding (SCD), a lightweight decoding-time strategy that softly penalizes non-target-language tokens while preserving reasoning fluency. Experiments across three multilingual datasets and four target languages demonstrate that SCD consistently improves both output language consistency and content quality without requiring additional training or model changes.

## Method Summary
The paper introduces Soft Constrained Decoding (SCD) as a decoding-time mitigation strategy for language drift in multilingual RAG systems. SCD works by softly penalizing the generation of tokens that don't match the target language during the decoding process, effectively encouraging the model to maintain language consistency with the input prompt. This approach is designed to be lightweight and model-agnostic, requiring no additional training or architectural modifications. The method is evaluated across three datasets (HotpotQA, MuSiQue, DuReader) and four target languages (English, Chinese, Arabic, Russian), showing improvements in both language consistency metrics and content quality measures.

## Key Results
- SCD consistently improves output language consistency (LC) across all tested language pairs and datasets
- The approach enhances content quality metrics (ROUGE/BLEU) while maintaining reasoning fluency
- English acts as a dominant attractor language, especially under Chain-of-Thought reasoning
- SCD achieves these improvements without requiring additional training or model changes

## Why This Works (Mechanism)
The paper identifies that language drift occurs primarily due to cross-lingual retrieved context, with English serving as a dominant attractor language. This phenomenon is particularly pronounced during Chain-of-Thought reasoning, where the model's internal reasoning process may be influenced by English-language retrieved content. The proposed Soft Constrained Decoding works by softly penalizing non-target-language tokens during generation, effectively countering the attractor effect without disrupting the reasoning process or requiring architectural changes.

## Foundational Learning
- Multilingual RAG systems - why needed: Understanding how retrieval-augmented generation works across multiple languages is essential for addressing language drift
- Chain-of-Thought reasoning - why needed: This reasoning pattern amplifies language drift effects, making it a critical factor to understand
- Soft constraints in decoding - why needed: Provides a way to guide generation without hard restrictions that might break reasoning
- Language consistency metrics - why needed: LC measures the effectiveness of drift mitigation strategies
- Cross-lingual retrieval impact - why needed: Understanding how retrieved content from other languages affects generation is central to the problem
- Decoder-side interventions - why needed: Shows that addressing drift doesn't always require model retraining

## Architecture Onboarding

Component Map:
- Input prompt (target language) -> Retriever (cross-lingual) -> Retrieved context (may be English) -> Generator (with SCD) -> Output (target language)

Critical Path:
The critical path flows from input prompt through the retriever to the generator, where SCD applies soft penalties to maintain target language consistency during token generation.

Design Tradeoffs:
The key tradeoff is between language consistency and reasoning quality. SCD uses soft rather than hard constraints to avoid disrupting the reasoning process, accepting some potential for drift in exchange for maintaining generation fluency.

Failure Signatures:
- Complete language switching to English despite target-language prompt
- Mixed-language outputs where reasoning switches between languages
- Preserved reasoning quality but inconsistent language output
- English attractor effect particularly strong in Chain-of-Thought scenarios

First 3 Experiments:
1. Baseline evaluation of language drift across HotpotQA, MuSiQue, and DuReader datasets
2. SCD implementation with varying penalty strengths to find optimal configuration
3. Cross-model evaluation comparing SCD performance across different multilingual RAG architectures

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Focus on academic datasets may not generalize to real-world multilingual retrieval scenarios
- Automated metrics evaluation without extensive human assessment of reasoning quality
- Correlational rather than causal analysis of language drift mechanisms
- No examination of potential factual accuracy degradation when applying SCD constraints

## Confidence
High confidence in empirical effectiveness of SCD for improving language consistency metrics across tested datasets and language pairs.
Medium confidence in the proposed mechanism explaining English as an attractor language, lacking causal validation.
Medium confidence in SCD preserving reasoning fluency, supported by ROUGE/BLEU but not comprehensive human evaluation.

## Next Checks
1. Conduct human evaluation studies comparing SCD outputs with baseline outputs on multilingual reasoning tasks, specifically assessing whether language consistency improvements come at the cost of reduced reasoning quality or factual accuracy.
2. Test SCD performance on real-world multilingual retrieval scenarios with naturally occurring code-switching, informal language, and domain-specific terminology beyond the academic datasets used in this study.
3. Investigate the interaction between SCD and factual consistency by evaluating whether soft constraints introduce hallucinations or confabulations when the model must choose between language consistency and retrieving accurate information from non-target language sources.