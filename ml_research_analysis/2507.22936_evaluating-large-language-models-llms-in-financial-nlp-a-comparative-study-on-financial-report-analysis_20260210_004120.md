---
ver: rpa2
title: 'Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study
  on Financial Report Analysis'
arxiv_id: '2507.22936'
source_url: https://arxiv.org/abs/2507.22936
tags:
- financial
- language
- llms
- evaluation
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated five large language models (GPT-4, Claude,
  Gemini, Perplexity, DeepSeek) on question answering over the Business sections of
  10-K filings from major U.S. tech firms.
---

# Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis

## Quick Facts
- arXiv ID: 2507.22936
- Source URL: https://arxiv.org/abs/2507.22936
- Authors: Md Talha Mohsin
- Reference count: 40
- Primary result: No single LLM dominates all evaluation dimensions in financial 10-K analysis; model selection should be task-specific

## Executive Summary
This study evaluates five large language models (GPT-4, Claude, Gemini, Perplexity, DeepSeek) on question answering over Business sections of 10-K filings from major U.S. tech firms. The research combines human ratings on relevance, completeness, clarity, conciseness, and factual accuracy with automated lexical/semantic similarity metrics and behavioral diagnostics. Results show model-specific trade-offs rather than universal superiority, with modest inter-rater agreement indicating subjective evaluation criteria. The study establishes a multi-dimensional evaluation framework necessary for high-stakes financial NLP applications.

## Method Summary
The study analyzes the Business sections (Item 1) of 10-K filings from seven major tech firms (Apple, Microsoft, Amazon, Alphabet, Nvidia, Meta, Tesla) for 2022-2024. Documents are truncated to 3,000-3,500 tokens and processed with standardized prompts containing 10 analytical questions. Five models generate responses under deterministic decoding (temperature=0). Human evaluation uses 5 annotators rating responses on five criteria (1-5 Likert scale). Automated metrics include ROUGE-1/2/L, Jaccard similarity, and cosine similarity using Sentence-BERT embeddings. Behavioral diagnostics measure cross-model agreement and temporal stability across prompts and firms.

## Key Results
- GPT-4 achieves highest human-rated relevance (4.08) and clarity scores
- Claude excels in factual accuracy (4.10) and semantic similarity (cosine: 0.68)
- Gemini leads lexical overlap metrics (ROUGE-1: 0.56)
- DeepSeek shows strongest conciseness (4.28) but inconsistent cross-model agreement
- No model dominates across all evaluation dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining human evaluation with automated metrics provides complementary information about model behavior that neither approach captures alone.
- Mechanism: Human ratings capture qualitative dimensions (relevance, clarity, factual accuracy) that reflect interpretive judgment, while automated metrics (ROUGE, cosine similarity) quantify lexical and semantic alignment with references. The two approaches expose different trade-offs: Gemini achieves highest lexical overlap (ROUGE-1: 0.56) but not highest semantic alignment, while Claude leads semantic similarity (cosine: 0.68) without dominating lexical metrics.
- Core assumption: Human annotators can reliably assess qualitative dimensions despite subjectivity; automated metrics correlate meaningfully with aspects of output quality.
- Evidence anchors:
  - [abstract] "we combine human evaluation, automated similarity metrics, and behavioral diagnostics under standardized and context-controlled prompting conditions"
  - [section 5.2] "Gemini attains the highest ROUGE-1 score (0.56)... Claude has a smaller lead over GPT (0.68) and Perplexity (0.71) in Cosine similarity"
  - [corpus] Related benchmarks (Golden Touchstone, SECQUE) similarly adopt multi-metric evaluation, suggesting convergent validity of this approach, though none directly validate the mechanism.
- Break condition: If human inter-rater agreement drops below chance (negative α values observed for some dimensions), aggregated human scores may not reflect systematic quality differences. If automated metrics show no correlation with human judgments, the complementarity assumption fails.

### Mechanism 2
- Claim: Model behavior in financial text analysis varies systematically across prompts, firms, and time, making single-point evaluation insufficient for deployment decisions.
- Mechanism: Behavioral diagnostics measure cross-model agreement and prompt-level variance. The study finds that GPT and Claude show high agreement (cosine 0.84-0.87 for Microsoft), while pairs involving Gemini or DeepSeek show lower agreement (0.69-0.72). Temporal analysis shows Microsoft filings have stable agreement (mean 0.85, low variance), while Amazon 2024 shows higher volatility (mean 0.71, SD 0.078), suggesting disclosure characteristics affect model consistency.
- Core assumption: Cross-model agreement and temporal stability proxy for reliability in production; higher variance indicates less dependable behavior.
- Evidence anchors:
  - [abstract] "behavioral diagnostics highlight variation in response stability and cross-prompt alignment"
  - [section 5.3/6.3] "Microsoft reports... demonstrate no significant change over time, and the mean cosine similarities are 0.85 with a poor variance. However, other firms (such as Amazon) are more volatile"
  - [corpus] No direct corpus validation of this specific mechanism. Related work on LLM consistency (Wang & Wang 2025, arXiv:2503.16974) examines reproducibility but in different task contexts.
- Break condition: If variance stems from appropriate sensitivity to genuine disclosure differences rather than instability, high variance could indicate better adaptation rather than worse reliability. Longitudinal studies would need to disentangle these.

### Mechanism 3
- Claim: No single model dominates across all evaluation dimensions, suggesting model selection should be task-specific rather than seeking a universal best.
- Mechanism: Different models optimize different objectives implicit in their training. GPT-4 scores highest on human-rated relevance and clarity (avg 4.08), Claude on factual accuracy (4.10), Gemini on lexical overlap, DeepSeek on conciseness (4.28). This specialization creates task-dependent optimal choices rather than a single best model.
- Core assumption: Observed performance differences reflect stable model characteristics rather than evaluation artifacts or prompt-specific effects.
- Evidence anchors:
  - [abstract] "no single model consistently outperforms others across all dimensions. GPT-4 leads in relevance and clarity, Claude in factual accuracy, and Gemini in lexical overlap"
  - [section 6.1] "GPT has the best total average of 4.08 on qualitative criteria... Claude comes right behind with a mean rating of 3.99 and the top mean score in factual correctness (4.10)"
  - [corpus] Golden Touchstone and FinBen benchmarks similarly report mixed model rankings across tasks, providing convergent evidence for task-dependent performance.
- Break condition: If models converge in future iterations, or if evaluation dimensions collapse into a single dominant factor, the mechanism of specialization becomes irrelevant. The current snapshot may not persist across model versions.

## Foundational Learning

- Concept: Inter-rater reliability (Krippendorff's α, ICC)
  - Why needed here: Human evaluation is central to the methodology, but the study reports α values near zero and occasional negative estimates. Understanding that low agreement means annotator scores cannot be treated as ground truth is essential for interpreting results correctly.
  - Quick check question: If five annotators rate the same response and Krippendorff's α = 0.05, can you claim the average score represents "true" quality? Why or why not?

- Concept: Lexical vs. semantic similarity metrics
  - Why needed here: The study finds Gemini leads ROUGE scores (lexical overlap) while Claude leads cosine similarity (semantic alignment). Understanding that these capture different aspects of output quality explains why rankings differ across metrics.
  - Quick check question: A model copies reference text phrases verbatim but misunderstands the core argument. Which metric would rate it higher—ROUGE-1 or cosine similarity?

- Concept: Autoregressive generation and sampling stochasticity
  - Why needed here: The paper explicitly frames LLM outputs as samples from conditional distributions (Section 3.5). Even with temperature=0, approximation error and interface differences introduce variability, meaning observed outputs are not deterministic.
  - Quick check question: Two runs of the same model with identical prompts produce different outputs. Does this necessarily indicate a bug? What factors beyond temperature could cause this?

## Architecture Onboarding

- Component map:
  - SEC EDGAR → HTML extraction → manual preprocessing → truncation to 3,000-3,500 tokens
  - Standardized template + 10 questions × 21 documents × 5 models = 1,050 responses
  - 5 annotators × 5 criteria × 210 responses/model = 5,250 ratings
  - ROUGE-1/2/L, Jaccard, cosine similarity (Sentence-BERT embeddings)
  - Pairwise cosine similarity, prompt-level variance, cross-firm/temporal stability

- Critical path:
  1. Document preprocessing (context window constraints determine truncation strategy)
  2. Prompt design (question phrasing affects all downstream evaluation)
  3. Model execution (separate sessions per document-prompt pair to prevent context leakage)
  4. Annotation aggregation (low inter-rater reliability requires careful statistical treatment)
  5. Metric interpretation (lexical vs. semantic trade-offs require task-specific weighting)

- Design tradeoffs:
  - Truncation to 3,000-3,500 tokens ensures comparability across models but sacrifices document completeness
  - Deterministic decoding (temperature=0) reduces variability but may not reflect typical usage
  - NLP-trained annotators rather than financial analysts provide consistent rubric application but may miss domain-specific nuances
  - Single prompt template enhances reproducibility but limits generalizability to other prompt formulations

- Failure signatures:
  - Negative Krippendorff's α values indicate systematic disagreement exceeding chance (observed for DeepSeek clarity: -0.051)
  - Large standard deviations in cross-model similarity (e.g., Amazon 2024: SD 0.078) suggest unstable behavior
  - Win rates below 25% across all companies (Gemini, DeepSeek) indicate consistent underperformance under this evaluation protocol
  - ROUGE-1 > 0.5 combined with low factual accuracy scores suggests verbose but unreliable outputs

- First 3 experiments:
  1. Replicate the evaluation pipeline on a single 10-K filing with two models (GPT-4, Claude) to validate metric calculation and annotation workflow before scaling.
  2. Test prompt sensitivity by varying a single question phrasing (e.g., "What are strategic goals?" vs. "What is the company's stated strategy?") across 3 documents to quantify behavioral variance.
  3. Compare truncated (3,000 tokens) vs. full-context responses for one model on one filing to measure information loss from context window constraints.

## Open Questions the Paper Calls Out

- To what extent does the domain expertise of human evaluators (e.g., NLP engineers vs. financial analysts) alter the qualitative rankings and inter-rater agreement scores of LLMs in 10-K analysis?
- Do the comparative performance hierarchies observed in this study persist when analyzing financial disclosures from smaller issuers or non-technology sectors?
- How does the introduction of stochastic decoding (non-zero temperature) impact the behavioral consistency and factual grounding of LLMs in financial contexts?

## Limitations

- Low inter-rater reliability (Krippendorff's α near zero and negative values) undermines confidence in human evaluation scores
- Context truncation to 3,000-3,500 tokens may exclude relevant information affecting model performance
- Reference responses needed for automated metrics are not provided, preventing exact reproduction
- Human evaluators have NLP backgrounds rather than financial expertise, potentially introducing domain-specific bias

## Confidence

- **High Confidence**: Model-specific performance patterns (GPT-4 excels in relevance/clarity, Claude in factual accuracy, Gemini in lexical overlap)
- **Medium Confidence**: Behavioral diagnostics showing cross-model agreement and temporal stability
- **Low Confidence**: Absolute performance rankings across all models due to low inter-rater reliability

## Next Checks

1. Replicate the evaluation pipeline on a single 10-K filing with two models (GPT-4, Claude) to validate metric calculation and annotation workflow before scaling.
2. Test prompt sensitivity by varying a single question phrasing across 3 documents to quantify behavioral variance and assess whether observed differences stem from model behavior or prompt formulation.
3. Compare truncated (3,000 tokens) vs. full-context responses for one model on one filing to measure information loss from context window constraints and its impact on performance.