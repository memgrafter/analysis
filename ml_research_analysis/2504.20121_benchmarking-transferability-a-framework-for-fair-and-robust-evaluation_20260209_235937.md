---
ver: rpa2
title: 'Benchmarking Transferability: A Framework for Fair and Robust Evaluation'
arxiv_id: '2504.20121'
source_url: https://arxiv.org/abs/2504.20121
tags:
- transferability
- source
- metrics
- datasets
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransferTest, a comprehensive benchmarking
  framework for evaluating transferability estimation metrics. The framework systematically
  assesses how well different metrics predict transfer learning performance across
  varying conditions, including different source datasets, model complexities, and
  fine-tuning strategies.
---

# Benchmarking Transferability: A Framework for Fair and Robust Evaluation

## Quick Facts
- **arXiv ID**: 2504.20121
- **Source URL**: https://arxiv.org/abs/2504.20121
- **Reference count**: 40
- **Primary result**: Introduces TransferTest, a comprehensive benchmarking framework that evaluates transferability estimation metrics across varying conditions, revealing traditional metrics' poor performance with non-ImageNet sources while the proposed Wasserstein distance metric achieves 3.5% improvement in head-training scenarios.

## Executive Summary
This paper introduces TransferTest, a comprehensive benchmarking framework for evaluating transferability estimation metrics. The framework systematically assesses how well different metrics predict transfer learning performance across varying conditions, including different source datasets, model complexities, and fine-tuning strategies. Through extensive experiments across 12 benchmark datasets and various model configurations, the study reveals that traditional metrics perform poorly when source datasets differ from ImageNet. The authors propose a novel weight-based transferability metric using Wasserstein distance between original and fine-tuned model weights, eliminating the need for target labels and demonstrating more consistent performance across diverse scenarios.

## Method Summary
The proposed method evaluates transferability through a three-step process: generating pseudo-labels for unlabeled target data using a pre-trained model, fine-tuning the model for two epochs on these pseudo-labeled data, and measuring the Wasserstein distance between the original and fine-tuned model weight distributions. The negative Wasserstein distance serves as the transferability score, with smaller distances indicating higher transferability. This approach eliminates the need for target labels while capturing adaptation dynamics. The framework benchmarks various existing metrics (LEEP, LogME, PACTran, SFDA, ETran) against this proposed method across 12 target datasets and multiple source datasets (ImageNet, CIFAR-100, Caltech101, Caltech256, Flowers102) using 11 supervised and 10 self-supervised model architectures.

## Key Results
- Traditional transferability metrics show poor performance when source datasets differ from ImageNet, with weighted Kendall's tau correlations dropping significantly
- The proposed Wasserstein distance metric demonstrates more consistent performance across diverse scenarios, particularly excelling with non-ImageNet sources
- Weight-based metrics show significantly greater stability compared to feature-based approaches as model complexity decreases
- In head-training fine-tuning scenarios, the Wasserstein metric achieves a 3.5% improvement over existing methods, showing superior performance in practical settings where only classification heads are trained

## Why This Works (Mechanism)

### Mechanism 1: Weight Distribution Shift as Transferability Proxy
Models that require smaller weight changes during fine-tuning on target data are more transferable. The method generates pseudo-labels for unlabeled target data, fine-tunes for 2 epochs, then computes negative Wasserstein distance between parameter distributions. Smaller distances (less adaptation required) map to higher transferability scores. This assumes weight shift magnitude during short fine-tuning correlates with how well the source representation aligns with the target domain.

### Mechanism 2: Robustness to Non-ImageNet Source Distributions
Weight-based metrics degrade more gracefully when source pre-training deviates from ImageNet assumptions than feature-based metrics. Feature-based methods implicitly assume feature extractors produce representations calibrated on ImageNet-like distributions. Weight dynamics capture adaptation capacity independent of source calibration, making them less sensitive to source dataset choice.

### Mechanism 3: Stability Under Model Complexity Variation
Weight-based transferability estimation maintains correlation with actual performance across varying model scales. Simpler models with fewer parameters still exhibit meaningful weight distribution shifts during adaptation. Wasserstein distance captures structural changes in the full distribution rather than element-wise differences, normalizing across architectures.

## Foundational Learning

- **Concept: Transferability estimation metrics (LEEP, LogME, PACTran, SFDA, ETran)**
  - Why needed here: The paper positions its Wasserstein metric against these baselines; understanding what each measures is essential for interpreting comparison results
  - Quick check question: Can you explain why LogME requires target labels while the proposed Wasserstein method does not?

- **Concept: Wasserstein distance (Earth Mover's Distance)**
  - Why needed here: The core technical contribution uses this to compare weight distributions; understanding it measures minimum "work" to transform one distribution to another clarifies why it captures more than L1/L2 distances
  - Quick check question: Given two distributions P and Q, what does a smaller Wasserstein distance indicate about their relationship?

- **Concept: Weighted Kendall's tau correlation**
  - Why needed here: This is the evaluation metric for all transferability scores; understanding it measures ranking correlation with emphasis on top performers is necessary to interpret Tables 1-7
  - Quick check question: Why would weighted Kendall's tau be preferred over standard correlation when evaluating model selection quality?

## Architecture Onboarding

- **Component map**: Source-Hub (pre-trained models) -> Scoring Module (metrics) -> Evaluation Engine (Kendall's tau) -> Target-Hub (benchmark datasets)
- **Critical path**: 1. Load pre-trained model from Source-Hub; 2. Generate pseudo-labels Ŷ = argmax(φ(X)) on target dataset; 3. Fine-tune 2 epochs on (X, Ŷ) → obtain θ'; 4. Compute W₁(P_θ, P_θ') as transferability score; 5. Rank models, compute Kendall's tau against actual fine-tuning results
- **Design tradeoffs**: Pseudo-label accuracy vs. label-free requirement; 2-epoch fine-tuning vs. computational cost; weight distribution flattening vs. architecture comparability
- **Failure signatures**: Negative Kendall's tau values indicating inverse correlation; high variance across source datasets; degradation on lightweight models
- **First 3 experiments**: 1. Replicate Table 5a on Food101 with ImageNet vs. CIFAR-100 sources; 2. Ablate fine-tuning epochs (1 vs. 2 vs. 5) to test sufficiency of 2 epochs; 3. Test on medical imaging dataset outside the 12 benchmark datasets

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a hybrid metric combining feature-based approaches and weight-based approaches outperform the standalone methods in transferability estimation?
  - Basis: The conclusion explicitly states future work could explore the combination of feature-based and weight-based approaches
  - Why unresolved: The paper evaluates existing feature-based metrics and the proposed weight-based metric separately but does not investigate whether their complementary strengths can be unified

- **Open Question 2**: What are the theoretical connections between the Wasserstein distance of model weights and the generalization performance on the target task?
  - Basis: The conclusion notes that investigating theoretical connections between weight distributions and transferability could provide deeper insights
  - Why unresolved: The paper empirically demonstrates Wasserstein distance as a strong proxy for transferability but does not derive formal bounds or causal explanations

- **Open Question 3**: Can the proposed weight-based metric be adapted for non-classification tasks where pseudo-label generation via `argmax` is not applicable?
  - Basis: The method relies on generating pseudo-labels using `argmax` and fine-tuning for 2 epochs, with experimental evaluation restricted to image classification datasets
  - Why unresolved: The reliance on classification-specific pseudo-labels limits the metric's applicability, leaving its utility for regression or structured prediction unproven

## Limitations

- The pseudo-label generation step introduces noise that may affect Wasserstein distance measurements
- No direct validation of the core assumption that weight shift magnitude correlates with transferability across diverse domains
- The framework's performance on out-of-distribution target datasets beyond the 12 benchmark datasets remains untested

## Confidence

- **High Confidence**: Source dataset sensitivity findings and head-training performance (3.5% improvement)
- **Medium Confidence**: Model complexity robustness claims, limited direct validation
- **Medium Confidence**: Non-ImageNet source performance, no explicit ablation studies provided

## Next Checks

1. Replicate Table 5a on Food101 with ImageNet vs. CIFAR-100 sources to verify source dataset sensitivity claims
2. Ablate fine-tuning epochs (1 vs. 2 vs. 5) to test whether 2 epochs is sufficient for capturing adaptation dynamics
3. Evaluate on a medical imaging dataset outside the 12 benchmark datasets to assess generalization to out-of-distribution targets