---
ver: rpa2
title: 'Activation-Deactivation: A General Framework for Robust Post-hoc Explainable
  AI'
arxiv_id: '2510.01038'
source_url: https://arxiv.org/abs/2510.01038
tags:
- explanations
- explanation
- input
- confidence
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robustness in post-hoc explainability
  methods for image classifiers. State-of-the-art approaches often rely on occlusion-based
  perturbations that create out-of-distribution inputs, potentially distorting explanations.
---

# Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI

## Quick Facts
- **arXiv ID**: 2510.01038
- **Source URL**: https://arxiv.org/abs/2510.01038
- **Reference count**: 40
- **Primary result**: ConvAD improves explanation robustness by up to 62.5% without requiring domain knowledge for occlusion value selection

## Executive Summary
This paper introduces Activation-Deactivation (AD), a novel forward-pass paradigm for generating post-hoc explanations in image classifiers that addresses the robustness limitations of traditional occlusion-based methods. The core innovation is ConvAD, which deactivates intermediate representations corresponding to occluded regions rather than masking input features, thereby avoiding out-of-distribution artifacts. The method is proven to maintain functional equivalence with the original network while producing more robust explanations (up to 62.5% improvement) that are slightly larger and closer to the model's confidence threshold, suggesting higher meaningfulness and trustworthiness.

## Method Summary
ConvAD is a drop-in mechanism that can be added to any trained CNN at test-time to generate explanations without additional training. It systematically deactivates intermediate representations that map to occluded input regions by tracking which input features are masked and propagating this information through the network via position-attribution functions. At each checkpoint (after convolutional layers and around dimensionality changes), ConvAD updates a binary mask that tracks which activations should be deactivated, applying Hadamard products to zero out affected representations. The method is proven not to change the network's decision-making process when no masking is applied, and it produces more robust explanations by avoiding the out-of-distribution artifacts created by traditional input-level occlusion methods.

## Key Results
- ConvAD explanations show 62.5% improvement in robustness compared to traditional occlusion methods
- AD explanations are 0.9-19.53% larger and closer to the model's confidence threshold
- No domain knowledge required for occlusion value selection
- Consistent improvements across ImageNet-1k, ImageNet-v2, CalTech-256, and PASCAL-VOC datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deactivating internal activations preserves in-distribution characteristics of features that reach each layer
- **Mechanism**: ConvAD tracks masked input regions and propagates this information through layers via position-attribution functions, zeroing intermediate representations where masked inputs contributed using mean-value kernel convolution on the mask
- **Core assumption**: Mask-to-activation mappings can be approximated via convolution with mean kernels, assuming near-uniform contribution across each receptive field
- **Evidence anchors**: [abstract] removes need for domain-specific occlusion value selection; [section 4] describes handling shape-preserving and dimensionality-altering operations; weak/no direct corpus evidence
- **Break condition**: If receptive fields contain non-uniform feature importance, mean-kernel attribution will misattribute masked influence

### Mechanism 2
- **Claim**: Thresholded binary masking (τ) prevents partial deactivation "leaks" when convolutions straddle masked and unmasked regions
- **Mechanism**: After position-attribution calculation, activations are zeroed only where Φ(z, M) > τ; τ = 0 deactivates only fully masked regions, higher values also deactivate border regions
- **Core assumption**: A hard threshold sufficiently separates "clean" from "leaky" activations
- **Evidence anchors**: [section 3] describes τ settings and leakage scenarios in Figure 4; [corpus] not directly addressed in neighbors
- **Break condition**: For highly non-contiguous or sparse masks, convolution windows may always overlap both regions, making clean separation impossible

### Mechanism 3
- **Claim**: ConvAD-modified networks remain functionally equivalent to original on unmasked inputs
- **Mechanism**: Theorem 1 proves that when M is all-ones (no masking), Φ(z, M) = 1 everywhere, so Hadamard product leaves activations unchanged at every checkpoint
- **Core assumption**: Dimensionality changes arise only from parametric operations or explicit add/subtract procedures (Assumption 1)
- **Evidence anchors**: [section 4, Theorem 1] proves output equality on inputs without occlusions; [appendix C] detailed proof covering all cases; [corpus] no contradiction
- **Break condition**: If model contains operations outside assumed categories (e.g., dynamic routing), equivalence is not guaranteed

## Foundational Learning

- **Concept**: Actual Causality and Prime-Implicant Explanations
  - **Why needed**: Grounds AD-explanations in formal causal definitions (EXIC1–EXIC3) clarifying minimality, sufficiency, and counterfactual conditions
  - **Quick check**: Given an input subset X that always produces output O across a context set K, how would you test whether X is minimal?

- **Concept**: Receptive Field Propagation in CNNs
  - **Why needed**: Position-attribution depends on understanding how each activation position maps back to input regions
  - **Quick check**: For a 3×3 convolution with stride 1 followed by a 2×2 max-pool, what input region contributes to a single output position?

- **Concept**: Out-of-Distribution Artifacts in Occlusion-Based XAI
  - **Why needed**: Central motivation is that input-level masking creates o.o.d. inputs whose model responses don't reflect original reasoning
  - **Quick check**: Why might replacing image patches with gray values cause a model to respond differently than it would to the original image's features?

## Architecture Onboarding

- **Component map**: Input pair (x, M) → Position function (pos_i) → Position-attribution function (Φ_i) → Thresholding (τ) → Checkpoints → Forward pass with mask updates and deactivation
- **Critical path**: 
  1. Initialize M as input mask
  2. For each layer: compute forward pass; at checkpoints, update M via Φ and apply z ← z ⊙ M'
  3. Final output is model prediction considering only unmasked features
- **Design tradeoffs**:
  - τ selection: τ = 0 minimizes deactivation but risks leakage at boundaries; higher τ increases robustness but may over-deactivate
  - Checkpoint frequency: More checkpoints increase precision but add computational overhead
  - Mask resolution: Binary masks may not align perfectly with activation map resolutions after downsampling
- **Failure signatures**:
  - Leakage at boundaries: Explanation includes irrelevant bordering features; increase τ
  - Over-deactivation: Explanation is too small or missing relevant features; decrease τ
  - Non-contiguous mask failures: Sparse masks produce noisy explanations
  - Shape mismatch errors: Dimensionality-altering operations not properly flagged
- **First 3 experiments**:
  1. **Sanity check equivalence**: Pass unmasked images (M = all-ones) through ConvAD-modified ResNet-50 and compare outputs to original ResNet-50 on 100 ImageNet samples
  2. **Robustness comparison**: Generate AD explanations and occlusion-based explanations for 50 images at γ = 0.9 threshold; measure ρ-robustness by planting on 100 solid-color backgrounds each
  3. **Threshold sensitivity**: On a single image with known ground-truth object bounding box, sweep τ ∈ {0.0, 0.1, 0.3, 0.5} and visualize explanation overlap with ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do larger and more robust explanations generated by ConvAD actually improve human trust and understanding compared to standard occlusion methods?
- **Basis in paper**: [explicit] Section 5.3 states explanations are "likely to increase trust" without human-subject testing
- **Why unresolved**: Evaluates explanations using proxy metrics rather than human-subject testing
- **What evidence would resolve it**: Controlled user study measuring human trust, comprehension, and ability to verify model decisions using ConvAD versus occlusion-based baselines

### Open Question 2
- **Question**: How can Activation-Deactivation framework be effectively adapted for non-CNN architectures, specifically Transformers?
- **Basis in paper**: [explicit] Introduction claims extension to other architectures is "straightforward" without demonstration
- **Why unresolved**: ConvAD relies on convolution-specific checkpoints and position-attribution functions that don't translate to global attention mechanisms
- **What evidence would resolve it**: Implementation for Vision Transformers showing theoretical benefits hold without degrading attention mechanism

### Open Question 3
- **Question**: Is there an optimal strategy for setting threshold hyperparameter τ to handle "leakage" in mixed masking scenarios?
- **Basis in paper**: [inferred] Section 4 discusses leakage scenarios where τ > 0 may be needed, yet τ was set to 0 for all experiments
- **Why unresolved**: Authors don't explore τ sensitivity or provide principled method for selection in complex cases
- **What evidence would resolve it**: Ablation study on τ across different perturbation strategies to correlate values with explanation fidelity and robustness

## Limitations
- Position-attribution functions for complex architectures with skip connections, attention mechanisms, or custom operations not fully specified
- Choice of τ=0 presented as optimal despite sensitivity analysis suggesting dataset-dependence
- Even AD explanations remain vulnerable to context changes (max ρ-robustness of 0.6 indicates inherent limitations)
- "Drop-in" claim assumes all models adhere to standard convolutional layer patterns

## Confidence
- **High Confidence**: Theoretical foundation of functional equivalence (Theorem 1) and core mechanism of internal activation deactivation are mathematically sound
- **Medium Confidence**: Empirical improvements in robustness (62.5%) demonstrated across multiple datasets, but methodology has some ambiguity
- **Low Confidence**: Generalization to non-standard CNN architectures and optimal τ selection across diverse domains remain uncertain

## Next Checks
1. **Cross-Architecture Validation**: Test ConvAD on architectures with attention mechanisms (Vision Transformers) and custom operations to verify "drop-in" claim beyond standard CNNs
2. **τ Sensitivity Analysis**: Systematically sweep τ across different datasets and object types to identify whether τ=0 is truly optimal or requires domain-specific tuning
3. **Explanation Fidelity Assessment**: Compare AD explanations against ground-truth object masks on datasets with segmentation annotations to quantify whether larger explanations (0.9-19.53% increase) actually improve localization accuracy