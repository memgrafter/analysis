---
ver: rpa2
title: 'It''s the same but not the same: Do LLMs distinguish Spanish varieties?'
arxiv_id: '2504.20049'
source_url: https://arxiv.org/abs/2504.20049
tags:
- modelos
- xico
- espa
- morfosintaxis
- para
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates nine large language models (LLMs) on their
  ability to identify seven Spanish varieties (Andean, Antillean, Continental Caribbean,
  Chilean, Peninsular, Mexican and Central American, and Rioplatense) using a multiple-choice
  test with 30 questions. The results show that Peninsular Spanish is best recognized
  by all models, while others struggle, particularly with lexical variation.
---

# It's the same but not the same: Do LLMs distinguish Spanish varieties?

## Quick Facts
- arXiv ID: 2504.20049
- Source URL: https://arxiv.org/abs/2504.20049
- Reference count: 0
- Models struggle to identify most Spanish varieties except Peninsular Spanish, with GPT-4o significantly outperforming others.

## Executive Summary
This study evaluates nine large language models on their ability to identify seven Spanish varieties using a 30-question multiple-choice test. The results show a strong bias toward Peninsular Spanish, with most models performing poorly on other varieties, particularly with lexical variation. GPT-4o is the only model to achieve high scores across multiple varieties. The findings reveal significant digital linguistic bias in current LLMs and highlight the need for more representative models.

## Method Summary
The study uses a 30-question multiple-choice test with 20 morphosyntactic and 10 lexical questions to evaluate nine LLMs across seven Spanish varieties. Models select answers based on log-probability estimates at temperature=0 for deterministic results. Prompts include role specifications identifying the dialect region and countries. Questions are validated against multiple Spanish corpora including CORPES XXI and CREA. The scoring formula penalizes errors on easier questions more heavily, with results aggregated by variety and error type.

## Key Results
- GPT-4o outperforms all other models and is the only one to achieve high scores across multiple varieties
- Peninsular Spanish is best recognized by all models, while Andean and Antillean varieties show the lowest performance
- Lexical error rates exceed morphosyntactic error rates across most varieties and models, particularly in smaller models
- Strong correlation (r=0.94) between model performance and training data representation in the CEREAL corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model performance on dialect identification correlates with training data representation.
- Mechanism: The paper reports a correlation coefficient of 0.94 between average model scores per variety and word counts from the CEREAL corpus. Peninsular Spanish—overrepresented in datasets like the Biblioteca Nacional Española and OSCAR—yields the highest scores across all models, while Andean and Antillean varieties (underrepresented) show the lowest performance.
- Core assumption: The CEREAL corpus approximates the broader training distribution of the evaluated models.
- Evidence anchors:
  - [abstract] "The findings reveal a strong bias toward Peninsular Spanish in LLM training data."
  - [section 4, Figure 3] "se observa una fuerte relación que se plasma en un alto coeficiente de correlación de 0.94"
  - [corpus] Related work (La Leaderboard, arXiv:2507.00999) confirms evaluation gaps across Spanish varieties but does not provide quantitative correlation data.
- Break condition: If a model is fine-tuned or continues pretraining on balanced dialectal corpora, this correlation should weaken.

### Mechanism 2
- Claim: Log-probability selection at temperature=0 provides a deterministic proxy for model "preference" across dialectal variants.
- Mechanism: For each multiple-choice question, the model assigns log-probabilities to each option. The highest-probability option is selected as the answer. Temperature=0 forces deterministic selection, mitigating randomness in generation.
- Core assumption: Higher probability reflects genuine dialectal knowledge rather than surface pattern matching or anglicized calibration.
- Evidence anchors:
  - [section 2] "seleccionamos la opción con la mayor probabilidad estimada por el modelo (log-prob) como su elección final"
  - [section 2] "la temperatura [...] se ha fijado en 0 para garantizar resultados deterministas"
  - [corpus] No direct comparison to alternative selection methods found in neighbor papers.
- Break condition: If models assign high probability to Peninsular variants even when prompted for other varieties, the mechanism may reflect training bias rather than dialectal competence.

### Mechanism 3
- Claim: Lexical variation is harder for models to capture than morphosyntactic variation, especially for smaller models.
- Mechanism: The paper decomposes errors into lexical and morphosyntactic categories. Across most varieties and models, lexical error rates exceed morphosyntactic error rates. Smaller models (e.g., Llama-3.2-3B-Instruct, Yi-1.5-9B-Chat) show lexical error rates up to 90% for some varieties, while morphosyntactic errors remain lower.
- Core assumption: The 10 lexical and 20 morphosyntactic questions are representative and equally difficult within their categories.
- Evidence anchors:
  - [section 3, Tables 3–9] All tables show lexical error frequencies ≥ morphosyntactic error frequencies for most models.
  - [section 4, Figure 2] Bar chart confirms higher average lexical error rates across models.
  - [corpus] Related work on NLI across variants (arXiv:2506.15239) finds similar performance drops but does not isolate lexical vs. morphosyntactic factors.
- Break condition: If models are trained on dialectal lexicons or glossaries, lexical performance should improve relative to morphosyntax.

## Foundational Learning

- Concept: Diatopic variation (geographically-based linguistic variation)
  - Why needed here: The paper evaluates seven Spanish varieties defined by dialectological conventions. Understanding that Spanish is not monolithic is prerequisite to interpreting why Peninsular dominance in training data creates bias.
  - Quick check question: Can you name two morphosyntactic features that distinguish Rioplatense Spanish from Peninsular Spanish?

- Concept: Log-probability as a confidence signal
  - Why needed here: The evaluation methodology uses log-probabilities to rank options, not generated text. Engineers must understand that this measures model internal distribution rather than generative output.
  - Quick check question: Why does temperature=0 guarantee deterministic selection in this setup?

- Concept: Digital Linguistic Bias (Sesgo Lingüístico Digital)
  - Why needed here: The paper frames poor non-Peninsular performance as a form of algorithmic discrimination. This is the normative lens through which results are interpreted.
  - Quick check question: What are the two levels of Digital Linguistic Bias mentioned (interlinguistic and intralinguistic)?

## Architecture Onboarding

- Component map:
  - Input prompt (role specification + multiple-choice question) -> Model (one of nine LLMs) -> Log-probability extraction -> Max-probability selection -> Binary correctness check -> Penalty-weighted score -> Per-variety scores and error type breakdown

- Critical path:
  1. Load model weights (HuggingFace for open models) or configure API (GPT models)
  2. Set temperature=0, default other hyperparameters
  3. For each of 30 questions × 7 varieties: construct role prompt, compute log-probabilities for each option, select max
  4. Apply scoring formula (correct=+1, incorrect uses inverse-difficulty penalty)
  5. Aggregate by variety and error type

- Design tradeoffs:
  - 30-question test limits statistical power (acknowledged by authors)
  - Single "standard" per variety ignores internal variation (e.g., Peninsular excludes southern dialects)
  - Multiple-choice format avoids generation evaluation complexity but restricts assessment scope
  - Temperature=0 ensures reproducibility but may underrepresent model uncertainty

- Failure signatures:
  - Lexical error rate > 70% for any variety indicates severe underrepresentation
  - Model prefers Peninsular option when prompted for non-Peninsular variety → prompt injection insufficient
  - Morphosyntactic error rate > 50% suggests fundamental grammar misalignment

- First 3 experiments:
  1. Reproduce the 30-question test on a new model (e.g., a recent open-weights release not in the study) to establish baseline comparability.
  2. Expand the question set (e.g., double to 60 questions with balanced lexical/morphosyntactic coverage) and measure score stability.
  3. Fine-tune a small model (e.g., Llama-3.2-3B) on a balanced dialectal corpus (e.g., sampled equally from CEREAL subcorpora) and re-evaluate; expect reduced Peninsular advantage if Mechanism 1 holds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM performance rankings shift when evaluated on a larger benchmark with more granular sub-varieties, rather than the normalized standards used in this study?
- Basis in paper: [explicit] The authors acknowledge the limitations of their test size and the normalization of varieties, explicitly calling for the development of "alternative and complementary tests" to verify results.
- Why unresolved: The study utilized a limited set of 30 questions per variety and normalized dialects (e.g., standard Peninsular), potentially masking performance variations in non-standard or highly specific regional sub-dialects.
- What evidence would resolve it: Evaluating the same models on a massive, fine-grained dialectal benchmark containing thousands of items covering sub-regional variations.

### Open Question 2
- Question: To what extent does explicitly balancing diatopic representation in training data mitigate the "Digital Linguistic Bias" observed in current models?
- Basis in paper: [inferred] The paper identifies a strong correlation (0.94) between model performance and the volume of training data available in the CEREAL corpus, but it does not test whether adding data for underrepresented varieties actually improves performance.
- Why unresolved: While the paper establishes a link between data volume and accuracy, it does not confirm if data augmentation is the causal solution for the observed bias toward Peninsular Spanish.
- What evidence would resolve it: A longitudinal study or ablation study comparing model performance before and after fine-tuning on balanced, geographically diverse Spanish corpora.

### Open Question 3
- Question: Does the ability of LLMs to correctly identify morphosyntactic and lexical markers in a multiple-choice format guarantee the ability to generate authentic text in those varieties?
- Basis in paper: [inferred] The methodology relies on log-probability estimation for multiple-choice answers (discriminative task), leaving the models' capacity to actively produce or complete text in specific dialects (generative task) unexplored.
- Why unresolved: A model might select the correct dialectal option from a list while still defaulting to Peninsular Spanish when generating free-form text, a capability not assessed by the current test design.
- What evidence would resolve it: A separate evaluation requiring the models to generate open-ended responses in target dialects, graded by native speakers from those regions.

## Limitations
- The evaluation corpus of 30 questions is acknowledged as small, limiting statistical power and potentially missing broader dialectal variation
- The single "standard" per variety ignores internal variation (e.g., Peninsular Spanish excludes southern dialects)
- The assumption that CEREAL corpus word counts approximate actual training distribution may not hold for proprietary models like GPT-4o

## Confidence
- High confidence: The correlation between model performance and training data representation (r=0.94)
- Medium confidence: The claim that lexical variation is harder for models to capture than morphosyntactic variation
- Low confidence: The generalizability of findings to all LLMs given the small evaluation set

## Next Checks
1. Reproduce the 30-question test on a new, recent model release not included in the study to establish baseline comparability and test robustness
2. Expand the question set to 60 questions with balanced lexical/morphosyntactic coverage and measure score stability across the original models
3. Fine-tune a small model (e.g., Llama-3.2-3B) on a balanced dialectal corpus (equal sampling from CEREAL subcorpora) and re-evaluate to test whether reduced Peninsular advantage validates the training bias hypothesis