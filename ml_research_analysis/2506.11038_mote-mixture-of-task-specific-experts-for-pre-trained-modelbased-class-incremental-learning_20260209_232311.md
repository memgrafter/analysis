---
ver: rpa2
title: 'MoTE: Mixture of Task-specific Experts for Pre-Trained ModelBased Class-incremental
  Learning'
arxiv_id: '2506.11038'
source_url: https://arxiv.org/abs/2506.11038
tags:
- learning
- task
- expert
- mote
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoTE (Mixture of Task-specific Experts),
  a novel approach for exemplar-free class-incremental learning using pre-trained
  models. The key challenge addressed is catastrophic forgetting and dimensional inconsistency
  when learning new tasks sequentially.
---

# MoTE: Mixture of Task-specific Experts for Pre-Trained ModelBased Class-incremental Learning

## Quick Facts
- **arXiv ID:** 2506.11038
- **Source URL:** https://arxiv.org/abs/2506.11038
- **Reference count:** 40
- **Primary result:** MoTE achieves state-of-the-art average accuracy on five CIL benchmarks while being ~30% faster in inference than EASE.

## Executive Summary
MoTE addresses exemplar-free class-incremental learning using pre-trained Vision Transformers by assigning lightweight task-specific adapters as experts. The method isolates each task with its own adapter to prevent forgetting, while task-aware expert filtering and confidence-weighted fusion handle ambiguous samples during inference. Extensive experiments demonstrate MoTE's superiority over existing methods across CIFAR100, CUB200, ImageNet-A, ImageNet-R, and VTAB, with the added benefit of faster inference speeds.

## Method Summary
MoTE freezes a pre-trained ViT backbone and trains one lightweight parallel adapter per incoming task. After training, class prototypes are computed and stored for each task. During inference, all experts perform forward passes, but unreliable experts (those predicting classes outside their task scope) are discarded. For ambiguous samples with multiple reliable experts, features are fused using confidence and self-confidence scores. The method avoids trainable routing layers to prevent forgetting, instead using deterministic task-aware filtering as a sparse activation proxy.

## Key Results
- MoTE achieves state-of-the-art average accuracy across five benchmark datasets
- Inference is approximately 30% faster than EASE (no pseudo-prototype computation needed)
- Adapter-Limited MoTE demonstrates trade-offs between adapter expansion and performance
- One adapter per task is optimal, but fewer adapters can suffice for simple datasets

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Expert Isolation
- Assigns one lightweight adapter per task to prevent cross-task interference while enabling full optimization
- Freezes old adapters when new tasks arrive, preserving previous task representations
- Assumes well-defined task boundaries where samples from task t are only available during training for task t

### Mechanism 2: Task-Aware Expert Filtering
- Filters out experts whose predictions fall outside their task scope to mimic MoE sparse activation
- Uses deterministic filtering instead of trainable routing to avoid vulnerability to forgetting
- Assumes a reliable expert's top prediction will land within its trained task scope

### Mechanism 3: Confidence + Self-Confidence Score Weighted Fusion
- Uses adaptive weighting of expert features based on confidence and prediction margin
- Sets γ = z^1st_i adaptively to avoid global hyperparameter tuning
- Assumes higher confidence plus larger margin correlates with correct expert assignment

## Foundational Learning

- **Concept:** Class-Incremental Learning (CIL)
  - *Why needed:* Addresses learning sequential tasks without storing old data while avoiding catastrophic forgetting
  - *Quick check:* Can you explain why simply fine-tuning on new tasks causes forgetting, and how replay, distillation, or parameter isolation mitigate it?

- **Concept:** Parameter-Efficient Fine-Tuning (PEFT) / Adapters
  - *Why needed:* MoTE freezes the pre-trained ViT and only trains lightweight adapters (~0.3% of backbone params)
  - *Quick check:* Draw a parallel adapter module showing where residual connections are added, and explain why this preserves pre-trained features.

- **Concept:** Prototype-Based Classification
  - *Why needed:* After training, classifiers are discarded; classification uses cosine similarity between query features and class prototype vectors
  - *Quick check:* Why does a prototype-based classifier help in exemplar-free CIL compared to a linear classifier?

## Architecture Onboarding

- **Component map:** Frozen ViT-B/16-IN21K backbone → Parallel adapter modules (one per task) → Class prototype storage → Expert filtering → Confidence-weighted fusion → Final classification

- **Critical path:** 1) Initialize frozen ViT backbone 2) For each new task: train adapter A_t on D_t 3) Extract and store class prototypes P^j_t 4) At inference: run expert filtering → weighted fusion → classify

- **Design tradeoffs:** Parallel vs. sequential adapters (parallel slightly outperforms), one adapter per task is optimal but memory scales linearly, ~30% faster inference than EASE

- **Failure signatures:** Task-ambiguous samples with high SCS for wrong experts, domain shift between tasks, prototype memory scaling with classes × feature-dim

- **First 3 experiments:**
  1. Sanity check on CIFAR100 B0-Inc10: Train MoTE with one adapter per task, report Avg and AF vs. baselines
  2. Ablation on expert filtering + SCS: Run configurations #1–#5 from Table 3, verify filtering alone improves task-ID accuracy
  3. Adapter-Limited MoTE on ImageNet-A: Restrict to {1, 2, 5, 10} adapters for 20-class incremental tasks, plot performance vs. adapter count

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the granularity of expert allocation be refined from task level to feature level to minimize storage costs?
  - The authors suggest exploring "feature-specific adapters that adapt based on the feature distribution at different layers"
  - This remains unresolved as current approach assigns one adapter per task with linear memory growth

- **Open Question 2:** How can task complexity be quantified to dynamically trigger adapter expansion?
  - The authors note that "adaptive expansion based on task complexity" could improve efficiency
  - Current experiments fixed adapter count but didn't define or implement task complexity metrics

- **Open Question 3:** How can task-aware expert filtering adapt for multi-label data or class overlap?
  - The authors explicitly limit scope to non-overlapping classes and single labels
  - Current filtering logic fails when a single sample belongs to multiple tasks or classes

## Limitations

- Adapter bottleneck dimension r is not explicitly specified (estimated at ~0.3% of backbone parameters)
- Task-aware filtering assumes well-defined task boundaries that may not hold in complex real-world scenarios
- Adapter memory scales linearly with task count, potentially prohibitive for long task sequences

## Confidence

- **High:** Task-specific adapter isolation preventing forgetting, experimental superiority on standard CIL benchmarks, Adapter-Limited MoTE trade-off analysis
- **Medium:** Task-aware expert filtering and SCS-weighted fusion effectiveness, dependent on task scope definition and expert calibration assumptions
- **Low:** Scalability claims for real-world deployment with many tasks, given limited adapter counts tested

## Next Checks

1. Test MoTE on a dataset with intentionally overlapping class distributions between tasks to validate the task-aware filtering mechanism's robustness
2. Implement Adapter-Limited MoTE with fewer adapters than tasks on complex datasets (ImageNet-A, ImageNet-R) to verify trade-off analysis holds beyond CIFAR/VTAB
3. Measure actual inference latency on commodity hardware to confirm the claimed ~30% speedup over EASE in practical deployment scenarios