---
ver: rpa2
title: A Method for Multi-Hop Question Answering on Persian Knowledge Graph
arxiv_id: '2501.16350'
source_url: https://arxiv.org/abs/2501.16350
tags:
- question
- knowledge
- complex
- questions
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for answering multi-hop complex
  questions in Persian using a knowledge graph. The authors developed a dataset of
  5,600 Persian multi-hop complex questions with their decomposed semantic representations
  and trained Persian language models on this data.
---

# A Method for Multi-Hop Question Answering on Persian Knowledge Graph

## Quick Facts
- arXiv ID: 2501.16350
- Source URL: https://arxiv.org/abs/2501.16350
- Reference count: 40
- Key result: 12.57% improvement in F1-score and 12.06% improvement in accuracy for Persian multi-hop QA over knowledge graphs

## Executive Summary
This paper introduces a novel method for answering multi-hop complex questions in Persian using a knowledge graph. The authors developed a dataset of 5,600 Persian multi-hop complex questions with their decomposed semantic representations and trained Persian language models on this data. Their approach includes four key components: question decomposition, named entity recognition and linking, SPARQL query generation, and sequential query execution. The method was evaluated on the PeCoQ dataset, demonstrating significant improvements over existing methods.

## Method Summary
The proposed approach addresses Persian multi-hop question answering by decomposing complex questions into simpler semantic segments, identifying and linking entities to FarsBase, generating SPARQL queries for each segment, and executing them sequentially. The pipeline uses fine-tuned mT5 models for decomposition and SPARQL generation, ParsBERT for named entity recognition, and a zero-shot entity linking approach with cosine similarity for candidate ranking. The method was trained and evaluated on a custom MRDCPQ dataset derived from PeCoQ, achieving state-of-the-art results on Persian KGQA.

## Key Results
- Achieved 75.55% F1-score and 74.81% accuracy on PeCoQ dataset
- 12.57% F1-score improvement and 12.06% accuracy improvement over baseline method
- Question decomposition accuracy of 77.61% and NER accuracy of 99.16%
- SPARQL generation accuracy of 82.35%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question decomposition improves multi-hop reasoning accuracy by reducing semantic complexity per processing step.
- Mechanism: The mT5 model, fine-tuned on MRDCPQ dataset, breaks complex questions into sequential semantic segments. Each segment represents one reasoning hop, enabling targeted downstream processing rather than end-to-end query generation.
- Core assumption: Complex questions decompose into semantically coherent sub-questions that map to sequential KG traversals.
- Evidence anchors: 77.61% TDA for decomposition accuracy; related work shows decomposition strategies improve complex QA.

### Mechanism 2
- Claim: Domain-specific NER fine-tuning dramatically improves entity extraction accuracy compared to general-purpose models.
- Mechanism: Fine-tuning ParsBERT on MRDCPQ teaches the model entity boundaries specific to KGQA contexts, achieving 99.16% accuracy versus 51.74% for ParsBERT-ARMAN.
- Core assumption: KGQA questions contain entity mentions that align with FarsBase entity naming conventions.
- Evidence anchors: ParsBERT-MRDCPQ: 99.16% accuracy; ParsBERT-ARMAN: 51.74% accuracy.

### Mechanism 3
- Claim: Sequential SPARQL execution with result-chaining enables multi-hop traversal without complex join queries.
- Mechanism: Each MRDCPQ segment generates one SPARQL query. Query N's output entity becomes Query N+1's input, avoiding generation of complex multi-hop SPARQL directly.
- Core assumption: FarsBase contains sufficient entity coverage and relationship density for the queried domain.
- Evidence anchors: 75.55% F1 vs. 62.98% baseline; sequential execution is less explored in related work.

## Foundational Learning

- **RDF Triples and KG Structure**: Understanding Subject-Predicate-Object triples is essential for comprehending how FarsBase stores facts and how SPARQL traversals work.
  - Quick check: Given triple (Ali Daei, place_of_birth, Ardabil), what query retrieves Ardabil?

- **SPARQL Query Syntax**: The system generates SPARQL programmatically; debugging requires reading query structure.
  - Quick check: What does `SELECT ?x WHERE { ?x :headquarters ?y }` return?

- **Encoder-Decoder Seq2Seq (mT5)**: Question decomposition and SPARQL generation both use fine-tuned mT5; understanding tokenization and beam search helps diagnose generation errors.
  - Quick check: Why might an encoder-decoder model produce semantically valid but KG-incompatible output?

## Architecture Onboarding

- Component map: Input Question → [Decomposer: mT5+MRDCPQ] → MRDCPQ segments → [NER: ParsBERT+MRDCPQ] → Entity mentions → [Entity Linker: Zero-shot + FarsBase] → KG URIs → [SPARQL Generator: mT5 fine-tuned] → Query chain → [Executor: Sequential on FarsBase] → Final answer

- Critical path: Decomposition accuracy → Entity extraction → Entity linking precision. Errors compound downstream; 77.61% TDA means ~22% of questions start with flawed decomposition.

- Design tradeoffs:
  - Sequential execution vs. single complex SPARQL: Sequential is more interpretable and debuggable but slower (multiple KG round-trips).
  - Fine-tuning vs. few-shot prompting: Authors chose fine-tuning (requires MRDCPQ dataset creation cost).
  - mT5 vs. LLM: mT5 chosen for multilingual support; LLMs may handle decomposition better but require more compute.

- Failure signatures:
  - Empty answer: Check if topic entity exists in FarsBase; check decomposition for malformed segments.
  - Wrong answer: Verify entity linking (cosine similarity threshold); check SPARQL relation mapping.
  - No answer after first hop: Intermediate entity may lack outgoing edges in KG.

- First 3 experiments:
  1. **Ablation on decomposition**: Replace mT5 decomposer with direct SPARQL generation; measure F1 delta to quantify decomposition's contribution.
  2. **Entity linking threshold sweep**: Vary cosine similarity threshold (0.5–0.9); plot precision/recall tradeoff on held-out questions.
  3. **Error analysis by hop count**: Bin results by 2-hop vs. 3-hop questions; identify if longer chains degrade non-linearly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does expanding the MRDCPQ dataset to include a broader spectrum of complex questions across different domains significantly enhance the model's generalization capabilities?
- Basis in paper: The authors list "Expanding the dataset for question decomposition... to include a broader spectrum of complex questions across different domains" as a direction for future work.
- Why unresolved: The current study relies on a dataset of 5,600 questions derived from PeCoQ, and the evaluation does not test the model's ability to generalize outside this specific distribution.
- What evidence would resolve it: Performance metrics (F1-score, Accuracy) on a new, diverse test set distinct from the current training data distribution.

### Open Question 2
- Question: To what extent does incorporating large-scale language models (LLMs) improve the accuracy and efficiency of the specific pipeline components compared to the fine-tuned mT5 model?
- Basis in paper: The authors propose "Incorporating advanced language models, such as large-scale models, to increase system accuracy and efficiency" as future work.
- Why unresolved: The current implementation relies on fine-tuning mT5 and ParsBERT; the potential gains or trade-offs of using LLMs for decomposition and query generation remain untested.
- What evidence would resolve it: A comparative study benchmarking the current mT5-based architecture against an LLM-based architecture on the PeCoQ dataset.

### Open Question 3
- Question: Can the proposed architecture be successfully extended to handle multimedia-based queries by integrating textual and visual data?
- Basis in paper: The authors suggest "Exploring multimodal learning" to enable the system to handle "multimedia-based queries."
- Why unresolved: The current method is strictly text-based, relying on SPARQL queries over the FarsBase knowledge graph without visual processing capabilities.
- What evidence would resolve it: A functional prototype that processes image-text pairs and successfully answers questions requiring visual reasoning.

### Open Question 4
- Question: Is the Question Decomposition component the primary bottleneck for end-to-end performance due to error propagation?
- Basis in paper: The Question Decomposition component achieved a Task Decomposition Accuracy (TDA) of 77.61%, which is lower than the NER (99.16%) and Query Generator (82.35%) components, potentially capping the final system accuracy.
- Why unresolved: While individual component scores are reported, the paper does not analyze whether the lower accuracy of the first step cumulatively limits the maximum achievable accuracy of the subsequent pipeline.
- What evidence would resolve it: An ablation study where the decomposition step is replaced by ground-truth input to measure the upper bound performance of the remaining components.

## Limitations

- **Dataset creation effort and generalization**: The MRDCPQ dataset required manual decomposition of 5,600 questions, representing a significant annotation cost and potential limitation to PeCoQ corpus domains.

- **FarsBase dependency**: The sequential SPARQL execution approach is tightly coupled to FarsBase's schema and entity coverage, with 22% decomposition failure rate suggesting limitations when entity relationships are sparse.

- **Limited ablation analysis**: The paper doesn't isolate the contribution of each component, making it difficult to assess which mechanism drives the 12.57% F1 improvement.

## Confidence

- **High confidence** in decomposition mechanism effectiveness (TDA 77.61% validated on held-out test set, with clear performance improvement over baseline).
- **Medium confidence** in NER improvement claim (99.16% vs 51.74% is dramatic, but ParsBERT-ARMAN comparison uses different datasets).
- **Low confidence** in sequential execution advantage (no comparison to end-to-end SPARQL generation methods).

## Next Checks

1. **Ablation study**: Remove the decomposition step and generate SPARQL directly from complex questions to quantify decomposition's contribution to the 12.57% F1 improvement.

2. **Entity linking robustness test**: Systematically remove entities from FarsBase to measure how often the pipeline fails when topic entities or intermediate entities are missing.

3. **Cross-domain transferability**: Apply the fine-tuned models to Persian questions about domains not represented in PeCoQ (e.g., medical or scientific knowledge) to assess generalization beyond the original dataset.