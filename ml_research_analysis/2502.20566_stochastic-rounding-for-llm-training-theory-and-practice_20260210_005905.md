---
ver: rpa2
title: 'Stochastic Rounding for LLM Training: Theory and Practice'
arxiv_id: '2502.20566'
source_url: https://arxiv.org/abs/2502.20566
tags:
- training
- bf16
- precision
- learning
- rounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Stochastic rounding (SR) is an unbiased alternative to nearest\
  \ rounding for low-precision training, preventing stagnation in gradient updates.\
  \ The authors provide convergence analysis for Adam under SR, showing that SR error\
  \ can be absorbed within Adam\u2019s natural tolerance, especially with higher learning\
  \ rates."
---

# Stochastic Rounding for LLM Training: Theory and Practice

## Quick Facts
- arXiv ID: 2502.20566
- Source URL: https://arxiv.org/abs/2502.20566
- Reference count: 40
- Stochastic rounding (SR) enables efficient low-precision LLM training with up to 1.54× higher throughput and 30% less memory

## Executive Summary
Stochastic rounding (SR) provides an unbiased alternative to nearest rounding for low-precision training, preventing gradient stagnation that commonly occurs with deterministic rounding methods. The authors present theoretical convergence analysis for Adam under SR, demonstrating that SR error can be absorbed within Adam's natural tolerance, particularly when using higher learning rates. Empirical results show that their BF16+SR strategy achieves superior performance compared to mixed-precision (BF16,FP32) training for models up to 6.7B parameters, without requiring auxiliary tensors or significant code modifications.

## Method Summary
The paper introduces stochastic rounding as a technique for low-precision training of large language models. Unlike deterministic rounding which always rounds to the nearest representable value, SR probabilistically rounds up or down based on the fractional component, ensuring that rounding errors have zero mean. The authors integrate SR into the Adam optimizer and provide convergence guarantees by showing that SR error can be bounded within the optimizer's inherent tolerance. They also identify an implicit regularization effect where SR introduces a quantization error penalty term that can improve generalization. The implementation requires minimal code changes and no additional memory overhead compared to standard mixed-precision training.

## Key Results
- BF16+SR strategy achieves up to 1.54× higher throughput than mixed-precision (BF16,FP32) training
- Memory usage reduced by 30% while maintaining or improving validation perplexity
- SR error is bounded within Adam's tolerance, enabling convergence guarantees with proper learning rate scaling
- Implicit regularization from quantization error improves generalization on language modeling tasks

## Why This Works (Mechanism)
Stochastic rounding prevents the gradient stagnation that occurs with deterministic rounding by ensuring that small gradients are not systematically rounded to zero. The probabilistic nature of SR maintains the expected value of gradients while allowing them to be represented in lower precision. When integrated with Adam, the optimizer's adaptive nature naturally compensates for the noise introduced by stochastic rounding, absorbing the error within its tolerance bounds. The quantization error introduced by SR acts as an implicit regularizer, adding a penalty term to the loss function that can improve generalization.

## Foundational Learning
- **Stochastic rounding**: Probabilistic rounding method that preserves expected values
  - Why needed: Prevents systematic bias in gradient updates that causes stagnation
  - Quick check: Verify expected value equals original value over multiple samples

- **Adam optimizer dynamics**: Adaptive learning rate method with momentum and variance correction
  - Why needed: Understanding how SR error interacts with optimizer tolerance
  - Quick check: Monitor moving averages and variance estimates during training

- **Quantization error regularization**: Implicit penalty term from numerical precision reduction
  - Why needed: Explains generalization improvements beyond computational efficiency
  - Quick check: Compare training vs validation loss curves with and without SR

- **Precision formats (BF16, FP32)**: Numerical representation schemes with different bit allocations
  - Why needed: Understanding memory and compute tradeoffs in mixed-precision training
  - Quick check: Verify bit allocation for mantissa and exponent in each format

## Architecture Onboarding

**Component Map**
Data → Model → Loss → Gradients → Stochastic Rounding → Adam Optimizer → Parameter Updates

**Critical Path**
The critical path involves gradient computation, stochastic rounding application, and Adam parameter updates. SR must be applied immediately after gradient computation but before optimizer state updates to ensure proper error propagation and absorption.

**Design Tradeoffs**
The main tradeoff is between computational efficiency and numerical stability. BF16+SR sacrifices some precision compared to FP32 but gains significant speed and memory benefits. The stochastic nature introduces training noise that must be balanced against the optimizer's tolerance.

**Failure Signatures**
- Gradient stagnation (zero updates despite non-zero gradients)
- Training instability or divergence
- Validation perplexity degradation
- Optimizer state becoming corrupted or NaN values

**3 First Experiments**
1. Verify SR implementation by checking expected values over multiple forward passes
2. Compare gradient histograms with and without SR to confirm prevention of zero gradients
3. Test single-layer model training to isolate SR effects from other optimization factors

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions remain about scalability beyond 6.7B parameters and performance on non-transformer architectures.

## Limitations
- Analysis is limited to models up to 6.7B parameters, leaving uncertainty about scalability to frontier-scale models
- Performance comparison is only against mixed-precision training, not more aggressive quantization strategies like FP4
- Theoretical analysis relies on idealized assumptions that may not fully capture real-world training dynamics, especially in later training stages

## Confidence
- **High**: SR's ability to prevent gradient stagnation and its implementation simplicity
- **Medium**: Throughput and memory benefits claims (dependent on comparison baseline)
- **Medium**: Theoretical convergence analysis (limited by idealized assumptions)

## Next Checks
1. Evaluate BF16+SR performance on models exceeding 10B parameters to verify scaling behavior
2. Compare against FP4 and other aggressive quantization strategies to establish relative positioning
3. Test on non-transformer architectures to assess generalizability beyond LLM training scenarios