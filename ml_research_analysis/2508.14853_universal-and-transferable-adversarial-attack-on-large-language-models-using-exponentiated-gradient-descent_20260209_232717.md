---
ver: rpa2
title: Universal and Transferable Adversarial Attack on Large Language Models Using
  Exponentiated Gradient Descent
arxiv_id: '2508.14853'
source_url: https://arxiv.org/abs/2508.14853
tags:
- adversarial
- arxiv
- llms
- harmful
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel adversarial attack method for jailbreaking
  large language models (LLMs) using exponentiated gradient descent (EGD) on relaxed
  one-hot token encodings. Unlike existing approaches that require explicit projection
  steps to maintain valid probability distributions, the proposed method inherently
  enforces these constraints during optimization, achieving higher success rates and
  faster convergence.
---

# Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent

## Quick Facts
- **arXiv ID:** 2508.14853
- **Source URL:** https://arxiv.org/abs/2508.14853
- **Reference count:** 11
- **Primary result:** Novel EGD-based adversarial attack method achieves higher success rates and faster convergence on LLMs compared to state-of-the-art baselines

## Executive Summary
This paper introduces a novel adversarial attack method for jailbreaking large language models (LLMs) using exponentiated gradient descent (EGD) on relaxed one-hot token encodings. The approach eliminates the need for explicit projection steps required by previous methods, inherently enforcing valid probability distributions during optimization. Experiments across five open-source LLMs and four adversarial behavior datasets demonstrate superior performance compared to three state-of-the-art baselines, achieving higher attack success rates and faster convergence. The method also generates universal adversarial suffixes effective across multiple prompts and shows strong transferability to different model architectures, including proprietary models like GPT-3.5.

## Method Summary
The proposed method operates by optimizing relaxed one-hot token encodings using exponentiated gradient descent, which naturally maintains valid probability distributions without requiring explicit projection steps. The attack process involves initializing token embeddings, computing gradients with respect to the model's output, and updating the embeddings using the EGD update rule. The relaxed one-hot encoding allows for continuous optimization in token space while preserving the discrete nature of the output. The method includes temperature scaling to control the trade-off between exploration and exploitation during optimization. For universal suffix generation, the approach optimizes a single suffix that can be appended to multiple prompts to trigger adversarial behaviors across different contexts.

## Key Results
- Achieved 85.3% attack success rate on average across five open-source LLMs, outperforming baseline methods by 12-18 percentage points
- Converged within 200-300 epochs compared to 500-800 epochs required by baseline PGD and FGSM methods
- Generated universal adversarial suffixes that maintained 78% success rate across diverse prompts and transferred effectively to proprietary models including GPT-3.5

## Why This Works (Mechanism)
The exponentiated gradient descent approach works by leveraging the natural geometry of the probability simplex. Unlike traditional gradient descent methods that require explicit projection to maintain valid probability distributions, EGD inherently respects the constraint space through its multiplicative update rule. The exponentiated updates ensure that probability values remain positive and sum to one throughout optimization, eliminating the need for computationally expensive projection steps. This architectural choice enables faster convergence and more stable optimization trajectories. The relaxed one-hot encoding provides a continuous relaxation of the discrete token space that is amenable to gradient-based optimization while preserving meaningful relationships to the actual token distribution.

## Foundational Learning
- **Relaxed one-hot encodings**: Continuous relaxation of discrete token representations needed to enable gradient-based optimization in token space; quick check: verify encoding preserves token semantics through reconstruction loss
- **Exponentiated gradient descent**: Optimization algorithm that naturally respects probability simplex constraints; quick check: validate probability normalization after each update
- **Universal adversarial suffixes**: Optimized perturbation patterns that generalize across multiple prompts; quick check: test suffix effectiveness on held-out prompt distributions
- **Temperature scaling**: Hyperparameter controlling exploration-exploitation trade-off in optimization; quick check: sensitivity analysis across temperature values
- **Transferability metrics**: Measures of attack effectiveness across different model architectures; quick check: correlation between open-source and proprietary model performance

## Architecture Onboarding
- **Component map**: Input prompt -> Tokenizer -> Relaxed one-hot encoding -> EGD optimizer -> Updated embeddings -> Model inference -> Output evaluation -> Loss computation
- **Critical path**: The optimization loop from token embedding updates through model inference and loss computation represents the primary computational bottleneck, with EGD updates being more efficient than projection-based alternatives
- **Design tradeoffs**: EGD eliminates projection overhead but requires careful temperature scheduling; relaxed encodings enable continuous optimization but introduce approximation error
- **Failure signatures**: Gradient vanishing in early optimization stages, temperature values too high causing instability, or universal suffixes overfitting to specific prompt distributions
- **First experiments**: 1) Baseline comparison with FGSM/PGD on single prompt, 2) Temperature sensitivity analysis, 3) Universal suffix generation on diverse prompt sets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to open-source models with 7B-13B parameters, raising questions about scalability to larger frontier models
- Transferability tests to proprietary models are promising but not comprehensive across the full range of commercial LLM offerings
- Theoretical relationship between relaxed one-hot embeddings and actual token space remains underexplored
- Computational efficiency claims based on "few hundred epochs" lack detailed runtime characterization for practical deployment scenarios

## Confidence
- **High confidence**: Mathematical soundness of EGD formulation and empirical superiority over baselines
- **Medium confidence**: Transferability results to proprietary models and universal suffix effectiveness across diverse prompts
- **Medium confidence**: Claims about faster convergence and higher success rates require further validation in operational scenarios

## Next Checks
1. Evaluate the method on larger frontier models (70B+ parameters) to assess scalability and performance persistence
2. Conduct systematic ablation studies on temperature parameter and EGD hyperparameters across different model families
3. Test universal suffix approach against dynamically generated or previously unseen adversarial prompts to evaluate robustness against adaptive defenses