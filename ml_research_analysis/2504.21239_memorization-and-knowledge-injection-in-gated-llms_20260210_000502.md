---
ver: rpa2
title: Memorization and Knowledge Injection in Gated LLMs
arxiv_id: '2504.21239'
source_url: https://arxiv.org/abs/2504.21239
tags:
- arxiv
- knowledge
- memory
- fine-tuning
- mega
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MEGa, a continual learning framework that
  injects episodic memories into large language models (LLMs) by embedding each memory
  as gated low-rank weights. Each memory is stored with a dedicated LoRA adapter and
  a context key embedding, with a gating mechanism activating relevant memories during
  inference by matching query embeddings to stored keys.
---

# Memorization and Knowledge Injection in Gated LLMs

## Quick Facts
- arXiv ID: 2504.21239
- Source URL: https://arxiv.org/abs/2504.21239
- Reference count: 40
- One-line primary result: Gated LoRA adapters enable episodic memory recall in LLMs while mitigating catastrophic forgetting

## Executive Summary
MEGa introduces a continual learning framework that injects episodic memories into large language models by storing each memory as a gated low-rank weight adapter. Each memory gets a dedicated LoRA adapter and context key embedding, with a softmax gating mechanism activating relevant memories during inference. On fictional character stories and Wikipedia events, MEGa achieved 72.53-78.03% QA accuracy and 0.901-0.921 recall cosine similarity, outperforming baseline continual learning methods while preserving general language capabilities.

## Method Summary
MEGa stores each episodic memory in a dedicated LoRA adapter (rank 128) targeting MLP layers, along with a context key embedding extracted from Llama internal activations. During training, each memory is fine-tuned separately for 10 epochs. At inference, query embeddings are compared to all context keys via inner product, passed through softmax to produce gating weights that modulate how much each adapter contributes. This enables content-addressable memory access and knowledge mixing through weighted adapter composition.

## Key Results
- Recall cosine similarity: 0.901-0.921 across datasets
- QA accuracy: 72.53-78.03% (matching RAG performance)
- General knowledge retention: MMLU accuracy ~61% preserved
- Compositional reasoning: 49.6-70.4% accuracy on multi-memory questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular LoRA adapters isolate memory traces, reducing destructive interference between sequentially learned content.
- Mechanism: Each training sample gets its own set of low-rank matrices {Al_i, Bl_i} stored separately from base weights. Gradients from new memories cannot overwrite previous representations.
- Core assumption: Distinct memories can be efficiently stored in separate parameter subspaces.
- Evidence anchors: Abstract states each memory is stored in dedicated gated low-rank weights; Section 3.3.1 specifies one LoRA adapter per sample.
- Break condition: If memories share significant semantic overlap, gating may fail to select correct adapter.

### Mechanism 2
- Claim: Softmax-gated retrieval over context key embeddings enables content-addressable memory access.
- Mechanism: Query embedding f(q) compared to stored keys K via inner product, passed through softmax with temperature β to produce gating weights gi.
- Core assumption: Embedding function produces representations where semantic similarity correlates with memory relevance.
- Evidence anchors: Abstract mentions gating mechanism matching query embeddings to memory embeddings; Section 3.3.2 specifies g = softmax(βf(q)^T K).
- Break condition: Poor embedding quality leads to incorrect gate selection below useful threshold.

### Mechanism 3
- Claim: Inference-time weight composition allows knowledge mixing without training on combinations.
- Mechanism: Weighted sum Σgi·Al_i·Bl_i added to pretrained weights during inference. Softmax enables multi-memory activation.
- Core assumption: Linear composition of low-rank updates preserves semantic meaning across memories.
- Evidence anchors: Section 4.5 demonstrates knowledge integration across separate LoRA modules; Table 2 shows compositional QA accuracy.
- Break condition: Contradictory memories may produce incoherent outputs through linear composition.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: MEGa builds on LoRA's parameter-efficient fine-tuning; understanding rank and scaling factor is essential.
  - Quick check question: Can you explain why W_FT := W_PT + AB with rank r << d reduces trainable parameters?

- Concept: **Embedding-based similarity search**
  - Why needed here: Gating mechanism relies on inner product similarity between query and stored embeddings.
  - Quick check question: Why might average pooling over later-layer activations capture different semantics than dedicated embedding models?

- Concept: **Catastrophic forgetting in sequential learning**
  - Why needed here: MEGa's primary motivation is mitigating CF; understanding why standard fine-tuning fails helps evaluate alternatives.
  - Quick check question: What happens to task A performance when you sequentially fine-tune on task B without regularization?

## Architecture Onboarding

- Component map:
  Base model -> LoRA adapters -> Context key store -> Embedding function f -> Gating softmax -> Weighted LoRA composition

- Critical path:
  1. Encode new memory → extract embedding K_i → store as context key
  2. Initialize fresh LoRA adapter → train on (prompt, memory) pairs
  3. At inference: embed query → compute gate weights → apply weighted LoRA composition → generate

- Design tradeoffs:
  - Rank (r=128): Higher rank improves memorization but increases storage
  - Embedding source: Llama internal activations vs. specialized embedders
  - Target layers: MLP-only fine-tuning works best; attention layers hurt performance
  - Scalability: Parameter count grows linearly with memories

- Failure signatures:
  - Wrong gate selected: Model retrieves unrelated memory
  - Refusal to answer: Fine-tuning prompt may trigger RLHF safety
  - General knowledge degradation: MMLU drops >2% suggests overfitting

- First 3 experiments:
  1. Train one LoRA adapter on one paragraph, verify exact recall with cosine similarity >0.95
  2. On 10 memories, compute rate at which query embedding matches correct context key
  3. Sequentially train 20 memories, evaluate QA accuracy on first memory after each new addition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MEGa maintain performance while scaling to thousands of memories despite its linear parameter growth?
- Basis in paper: Authors state "One limitation of MEGa is that its parameter count grows linearly with the number of training samples."
- Why unresolved: Current feasibility study limited to 50 samples; unclear if computational overhead becomes prohibitive at larger scales.
- What evidence would resolve it: Evaluating MEGa on datasets with >1,000 sequential samples, measuring QA accuracy and memory/storage overhead.

### Open Question 2
- Question: Can LoRA adapters be effectively distilled into base model weights through a rehearsal process that mimics human cortical consolidation?
- Basis in paper: Authors propose "gradually distill LoRA weights into the base model weights... corresponding to the slow learner ('cortex')."
- Why unresolved: Current architecture stores memories in separate adapters; mechanism for merging into general knowledge without CF is untested.
- What evidence would resolve it: Demonstrating rehearsal loop where recalled memories update base model, allowing pruning of old LoRA adapters.

### Open Question 3
- Question: Does implementing dynamic merging and splitting of memory modules improve efficiency over storing independent, static memories?
- Basis in paper: Authors suggest "add an option of merging and splitting of stories depending on the similarity between them."
- Why unresolved: Current design treats memories as independent samples, potentially failing to capture shared context or redundancy efficiently.
- What evidence would resolve it: Comparison of storage efficiency and retrieval accuracy between static MEGa and version with similarity-based module merging.

## Limitations
- Linear parameter growth: Storage grows proportionally with memory count, becoming prohibitive at scale
- Embedding quality dependence: Gating performance heavily relies on embedding function quality
- Limited compositional reasoning: Performance drops significantly on complex multi-memory questions

## Confidence
- High confidence: Memory isolation through separate LoRA adapters prevents catastrophic forgetting
- Medium confidence: Softmax gating with context keys provides effective content-addressable retrieval
- Medium confidence: Knowledge mixing through weighted adapter composition works for simple compositional questions

## Next Checks
1. Train MEGa on 100+ memories and track correct gate selection rate and recall accuracy as memory count increases
2. Create test suite of paraphrased queries (beyond 9 used during training) for each memory and measure retrieval accuracy
3. Construct test cases where two stored memories contain contradictory information about same entity/event