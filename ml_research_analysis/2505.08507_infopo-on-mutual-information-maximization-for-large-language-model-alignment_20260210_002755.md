---
ver: rpa2
title: 'InfoPO: On Mutual Information Maximization for Large Language Model Alignment'
arxiv_id: '2505.08507'
source_url: https://arxiv.org/abs/2505.08507
tags:
- infopo
- chosen
- arxiv
- preference
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  (LLMs) with human preferences using preference data. The core method, InfoPO, is
  a novel preference optimization algorithm based on mutual information maximization
  that eliminates the reliance on the Bradley-Terry (BT) model assumption, which is
  prone to overfitting and suboptimal performance, particularly on reasoning-heavy
  tasks.
---

# InfoPO: On Mutual Information Maximization for Large Language Model Alignment

## Quick Facts
- arXiv ID: 2505.08507
- Source URL: https://arxiv.org/abs/2505.08507
- Reference count: 23
- One-line primary result: InfoPO consistently outperforms DPO on reasoning tasks while preserving chosen response likelihood

## Executive Summary
InfoPO is a novel preference optimization algorithm that aligns large language models using mutual information maximization without relying on the Bradley-Terry model assumption. The method uses the NWJ estimator to prevent chosen response likelihood from decreasing during training, addressing a key limitation of DPO that particularly affects reasoning tasks. Experiments on benchmarks like HuggingFace Open LLM Leaderboard, AlpacaEval 2, and coding tasks demonstrate that InfoPO achieves substantial improvements on reasoning tasks (12%+ relative gains on math tasks for Mistral-7B) while maintaining strong performance on chat and summarization tasks.

## Method Summary
InfoPO reframes preference optimization as conditional mutual information maximization between responses and preference labels given prompts. The loss function is $L_{InfoPO} = -\log \pi_\theta(y_w|x) + \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}$, where $\pi_\theta$ is the trainable policy and $\pi_{ref}$ is a fixed reference policy (typically the SFT model). This approach eliminates the Bradley-Terry assumption used in DPO and prevents the likelihood of chosen responses from decreasing during training. The method uses the NWJ estimator for mutual information estimation, trading higher variance for lower bias compared to InfoNCE.

## Key Results
- InfoPO achieves relative gains exceeding 12% on Mistral-7B for Math tasks compared to DPO
- On Llama3-8B, InfoPO shows 3.5%+ relative gains on reasoning tasks and 5.5%+ on coding tasks
- InfoPO achieves win rates of at least 60% against chosen responses in summarization and dialogue tasks
- Maintains chosen response likelihood throughout training while DPO shows declining likelihood

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** InfoPO reduces gradient imbalance between chosen and rejected responses during preference optimization.
- **Mechanism:** DPO's gradient weight for rejected responses includes a reciprocal of the current policy probability (1/πθ(yl|x)), which becomes extremely large when the probability approaches zero. InfoPO replaces this with the reciprocal of the FIXED reference probability (1/πref(yl|x)), which remains stable throughout training and has a smaller norm.
- **Core assumption:** The reference policy πref provides stable probability estimates that prevent runaway gradients.
- **Evidence anchors:** [abstract] "prevents the likelihood of the chosen response from decreasing"; [Section 4.2] Gradient analysis showing dθ = σ(...) and how πθ(yl|x) → 0 leads to "extremely large" gradients; [Section 4.3] Gradient comparison showing InfoPO uses "reciprocal of the fixed reference probability"
- **Break condition:** If the reference policy is poorly calibrated or significantly different from the data distribution, gradient scaling may be inappropriate.

### Mechanism 2
- **Claim:** The NWJ estimator provides low-bias mutual information estimation that avoids the overfitting tendencies of InfoNCE-based approaches.
- **Mechanism:** The paper shows DPO implicitly uses InfoNCE (high variance, low bias) for mutual information estimation. NWJ estimator has low bias but higher variance, which the paper argues prevents overfitting to the BT assumption. The exponential operation in NWJ linearizes gradients on rejected responses.
- **Core assumption:** Low-bias MI estimation translates to better alignment performance, particularly on reasoning tasks.
- **Evidence anchors:** [Section 4.3] "NWJ has low bias but high variance, whereas InfoNCE has low variance but suffers from high bias"; [Section 4.3] Objective function derivation showing exponential operation on rejected term; [corpus] Weak corpus signals—no direct external validation of NWJ vs InfoNCE for alignment
- **Break condition:** High variance could cause instability on small datasets; bias-variance tradeoff may favor different estimators depending on data scale.

### Mechanism 3
- **Claim:** Maintaining chosen response likelihood during training preserves reasoning capabilities.
- **Mechanism:** By preventing the decline in chosen response likelihood, InfoPO preserves the model's ability to generate correct reasoning chains. The paper argues that DPO's chosen likelihood decrease is particularly harmful for math/coding where specific token sequences matter.
- **Core assumption:** Reasoning task performance correlates with maintaining likelihood of preferred responses.
- **Evidence anchors:** [Section 5.4.2] Figure 2 showing DPO's chosen likelihood drops below zero while InfoPO stays higher; [Table 1] 12%+ relative gains on Math tasks for Mistral-7B, 3.5%+ for Llama3; [corpus] "Many of Your DPOs are Secretly One" paper confirms unification through MI perspective
- **Break condition:** If chosen responses in training data are low-quality, maintaining their likelihood could harm performance.

## Foundational Learning

- **Concept: Mutual Information (MI) and Conditional MI**
  - **Why needed here:** InfoPO reframes preference optimization as conditional mutual information maximization I(Y; C|X) between responses Y and preference labels C given prompts X.
  - **Quick check question:** Can you explain why maximizing mutual information between responses and preference labels would improve alignment?

- **Concept: Bradley-Terry Model**
  - **Why needed here:** The paper's central critique is that DPO and variants rely on the BT preference model (p(yw ≻ yl|x) = σ(r(x,yw) - r(x,yl))), which causes chosen likelihood degradation.
  - **Quick check question:** What assumption does the BT model make about how reward differences translate to preference probabilities?

- **Concept: KL Divergence (Forward vs Reverse)**
  - **Why needed here:** Theorem 4.1 shows InfoPO minimizes reverse KL divergence, which promotes mode-seeking behavior (concentrating on high-reward regions) versus forward KL's mass-covering behavior.
  - **Quick check question:** Why would reverse KL be preferred for generating high-quality responses in an alignment setting?

## Architecture Onboarding

- **Component map:**
  - Input x, chosen response yw, rejected response yl
  -> Reference policy πref(y|x) (frozen)
  -> Policy model πθ(y|x) (trainable)
  -> Loss computation: LInfoPO = -log πθ(yw|x) + πθ(yl|x)/πref(yl|x)
  -> Aligned policy output

- **Critical path:**
  1. Load SFT model as reference policy (πref) and initialize policy model (πθ) from same checkpoint
  2. For each preference triplet (x, yw, yl):
     - Compute log πθ(yw|x) for chosen term
     - Compute πθ(yl|x)/πref(yl|x) for rejected term (requires forward pass through BOTH models)
  3. Sum terms and backpropagate to update πθ only
  4. Reference model remains frozen throughout training

- **Design tradeoffs:**
  - **Memory:** Requires keeping two model copies in memory (reference + policy) vs. single model for some alternatives
  - **Bias-variance:** NWJ estimator trades lower bias for higher variance—may require more data for stability
  - **Simplicity vs. flexibility:** Single estimator approach limits exploration of alternative MI bounds

- **Failure signatures:**
  - **Chosen likelihood still decreasing:** Check that reference model is frozen and gradients aren't leaking
  - **Training instability:** High variance of NWJ may cause loss spikes; consider gradient clipping
  - **Poor reasoning performance:** Verify preference data quality—low-quality chosen responses will be reinforced
  - **Length bias:** Paper mentions normalizing response likelihood by token count (as in SimPO)

- **First 3 experiments:**
  1. **Likelihood dynamics validation:** Replicate Figure 1/2 tracking chosen/rejected likelihoods and margins over training steps for DPO vs InfoPO on a small model (Pythia-2.8B) with visualization
  2. **Gradient norm analysis:** Log gradient norms for chosen vs rejected terms during training to confirm InfoPO's more balanced gradients compared to DPO's exploding rejected gradients
  3. **Single-benchmark ablation:** Train on UltraFeedback Binarized and evaluate only on GSM8K (math reasoning) to quickly validate reasoning improvements before full benchmark suite

## Open Questions the Paper Calls Out
- **Open Question 1:** How does InfoPO perform when utilizing alternative mutual information estimators beyond the NWJ estimator?
  - **Basis in paper:** [explicit] The authors state in Section 7 that exploring the effectiveness of InfoPO with alternative mutual information estimators remains an "interesting avenue for future research" and a "key direction for further study."
  - **Why unresolved:** The current work exclusively focuses on the NWJ estimator to validate the approach.
  - **What evidence would resolve it:** Empirical benchmarks comparing InfoPO variants using different MI estimators (e.g., JS, MINE) on reasoning and chat tasks.

- **Open Question 2:** Does the high variance associated with the NWJ estimator negatively impact InfoPO's stability or performance on smaller preference datasets?
  - **Basis in paper:** [inferred] The paper notes in Section 4.3 that the NWJ estimator suffers from "high variance" compared to the low-variance InfoNCE (used in DPO).
  - **Why unresolved:** While the paper demonstrates success on standard benchmarks, it does not analyze performance specifically under low-data regimes where variance might dominate.
  - **What evidence would resolve it:** A study of InfoPO's sample efficiency and training loss curves on subsets of the UltraFeedback dataset of varying sizes.

- **Open Question 3:** Does the mode-seeking behavior induced by minimizing reverse KL divergence result in reduced output diversity for creative tasks?
  - **Basis in paper:** [inferred] Theorem 4.1 proves InfoPO minimizes reverse KL divergence, which the authors note promotes "mode-seeking behavior" suitable for reasoning.
  - **Why unresolved:** Mode-seeking behavior focuses probability mass on high-reward peaks, which may lead to repetitive or less diverse responses in open-ended generation.
  - **What evidence would resolve it:** Evaluating the diversity (e.g., entropy or distinct n-grams) of InfoPO-generated outputs on creative writing benchmarks compared to DPO baselines.

## Limitations
- The empirical validation doesn't isolate the contribution of the NWJ estimator versus gradient stabilization from the fixed reference probability
- Analysis of reasoning tasks is based on benchmark scores without qualitative analysis of actual response quality
- The paper doesn't report training stability metrics across runs, making it difficult to assess whether the high-variance NWJ estimator causes practical implementation challenges

## Confidence
**High Confidence:**
- InfoPO's gradient imbalance mechanism (Mechanism 1) is mathematically well-established and the analysis in Section 4.2 provides clear evidence through gradient derivations
- The preservation of chosen response likelihood during training is directly measurable and the results in Section 5.4.2 provide clear evidence through likelihood tracking
- Outperformance on reasoning tasks (Math/Coding) is supported by multiple benchmarks and the effect sizes are substantial

**Medium Confidence:**
- The claim that NWJ's low bias prevents overfitting to BT assumptions (Mechanism 2) is theoretically sound but lacks direct empirical validation comparing InfoPO with alternative MI estimators
- The correlation between maintaining chosen likelihood and reasoning performance (Mechanism 3) is plausible but not directly validated—the paper doesn't show that DPO models with similar likelihood dynamics would perform similarly

**Low Confidence:**
- The generalizability of results to smaller models or datasets is unclear, as most experiments use large models (7B-8B parameters) on datasets with thousands of preference pairs
- The claim that InfoPO "eliminates" reliance on BT assumptions is strong; while the paper provides theoretical justification, practical implementations may still exhibit BT-like behavior under certain conditions

## Next Checks
1. **Cross-Estimator Validation:** Implement InfoPO with both NWJ and InfoNCE estimators on the same model/dataset to empirically measure the bias-variance tradeoff and its impact on alignment performance, particularly on small vs. large datasets.

2. **Gradient Norm Correlation Analysis:** Track gradient norms for chosen and rejected responses during training and correlate these with downstream performance on reasoning tasks to validate whether InfoPO's more balanced gradients directly translate to better reasoning outcomes.

3. **Response Quality Analysis:** Generate qualitative comparisons of model outputs on reasoning tasks (e.g., GSM8K) for DPO vs InfoPO models to determine whether improvements stem from better reasoning chains or other factors like response formatting or verbosity.