---
ver: rpa2
title: What's the plan? Metrics for implicit planning in LLMs and their application
  to rhyme generation and question answering
arxiv_id: '2601.20164'
source_url: https://arxiv.org/abs/2601.20164
tags:
- steering
- rhyme
- planning
- token
- rhyming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents methods for detecting and quantifying implicit
  planning in large language models (LLMs), where models appear to plan ahead for
  future tokens even when trained on next-token prediction. The authors propose simple
  activation steering techniques to test whether representations of planned elements
  (like rhyming words or answer nouns) exist at earlier positions and influence intermediate
  token generation.
---

# What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering

## Quick Facts
- **arXiv ID**: 2601.20164
- **Source URL**: https://arxiv.org/abs/2601.20164
- **Reference count**: 40
- **Primary result**: Mean activation difference steering successfully detects and quantifies implicit planning in LLMs across 23 models (1B-32B parameters), demonstrating that models encode planned future tokens at strategic positions like newlines and question marks.

## Executive Summary
This paper introduces methods for detecting and quantifying implicit planning in large language models, where models appear to plan ahead for future tokens even when trained on next-token prediction. The authors propose simple activation steering techniques to test whether representations of planned elements exist at earlier positions and influence intermediate token generation. Using datasets of rhyming poetry and question answering, they apply mean activation difference steering at strategic positions and measure its effects on rhyme family generation, answer word selection, and intermediate token choices. The study finds consistent evidence of implicit planning across model sizes, with larger and instruction-tuned models exhibiting stronger planning abilities.

## Method Summary
The method uses mean activation difference steering to detect planning representations. For each task (rhyme generation or question answering), the model computes class-conditional mean differences in hidden activations at strategic positions (last word, newline token, or question mark). These steering vectors are added to the residual stream during generation to manipulate target outputs. The approach tests forward planning by measuring how well steering changes target rhyme families or answers, and backward planning by observing how steering affects intermediate token choices like article selection. The method is applied across 23 diverse models ranging from 1B to 32B parameters.

## Key Results
- Steering vectors successfully manipulate target rhyme families and answer nouns at strategic positions across all tested models
- Larger models and instruction-tuned versions show stronger planning abilities than smaller or base models
- Specific attention heads (L30H3, L31H15) in Gemma2 9B implement rhyme planning by attending to last words and newline tokens
- Steering effects extend beyond target words to influence intermediate token choices like article selection
- Planning representations can be detected and manipulated in models as small as 1B parameters

## Why This Works (Mechanism)

### Mechanism 1: Forward Planning Representations
Claim: Representations at strategic positions encode properties of planned future tokens. During generation, hidden activations at position i contain directional components that causally influence tokens at position j > i. Mean activation difference steering extracts these components by computing s_{C1→C2} = m · (mean activations on C1 - mean activations on C2), then adds this vector to the residual stream at the target position.

### Mechanism 2: Backward Planning via Specialized Attention Heads
Claim: Specific attention heads read from planning representations and transfer information to later tokens. In Gemma2 9B, attention heads L30H3 and L31H15 attend selectively to the last word and newline token of the first line. These heads contribute significantly at "fork points" where token choices diverge. MLP layers 30-39 then convert the transferred information into probability distribution shifts.

### Mechanism 3: Instruction Tuning Amplifies Planning Representations
Claim: Instruction-tuned models exhibit stronger, more localized planning representations than base models. Post-training alignment may compress planning information into more interpretable directions, increasing the signal-to-noise ratio for steering vectors. This is evidenced by higher steering effectiveness and stronger correlation between planning metrics.

## Foundational Learning

- **Residual Stream and Activation Addition**
  - Why needed here: Steering operates by adding vectors to residual stream activations; understanding how information flows through layers is essential for selecting intervention points.
  - Quick check question: If you add vector v to the residual stream at layer l, which subsequent components can read from it?

- **Attention Head Output Composition**
  - Why needed here: The paper identifies specific heads that transfer planning information; you need to understand how head outputs contribute to residual stream and how patching works.
  - Quick check question: What happens when you replace a specific attention head's output with its "steered" version while leaving everything else unchanged?

- **KL Divergence and Probability Distribution Shifts**
  - Why needed here: Probability-based metrics detect backward planning by measuring when steering changes the next-token distribution (KL > 1 threshold used).
  - Quick check question: If KL divergence is 0 at position t but > 1 at position t+1, what does this tell you about when planning influences generation?

## Architecture Onboarding

- **Component map:**
Input tokens → Embeddings → [Layer 1...L] → Logits
                      ↓
              Residual Stream (steering intervention point)
                      ↓
         Each layer: Attention → [Add to residual] → MLP → [Add to residual]
                      ↑
              Key heads (L30H3, L31H15 in Gemma2 9B) read from positions
              containing planning representations

- **Critical path:**
1. Dataset construction: 10 rhyme families with 85 train / 20 test lines each; 20 noun pairs with 13 train / 5 test + 7 neutral questions
2. Steering vector estimation: Extract activations at target positions, compute mean difference with multiplier m=1.5
3. Layer/position search: Test steering effectiveness across middle layers and three positions (last word, newline, question mark)
4. Evaluation: Forward planning (steering effectiveness), backward planning (article shifts, KL divergence), regeneration metrics

- **Design tradeoffs:**
  - Mean activation difference steering vs probe/SAE weights: Simpler but may mix multiple features; alternatives more precise but require tuning per case
  - Single-position steering vs distributed intervention: Single position is more interpretable but may miss distributed representations
  - Training data size: 85 examples used for consistency, but capable models can generalize from "a just a few examples or even a single example pair"

- **Failure signatures:**
  - Steering effectiveness much lower than baseline rhyming rate → model lacks robust planning representations
  - High KL divergence metrics without corresponding regeneration metric changes → may indicate model idiosyncrasies rather than genuine planning
  - Newline steering ineffective → model doesn't use newline in planning circuit

- **First 3 experiments:**
1. Replicate baseline steering on single model: Take Gemma2 9B IT, compute steering vectors for (-ing, -air) pair, test steering effectiveness at layers 15-35 on last word position. Verify you can achieve >70% target rhyme family generation.
2. Layer localization experiment: For a fixed rhyme pair, sweep layers 10-40 and plot steering effectiveness. Identify the "best layer" and verify it aligns with early-middle layers for last word steering.
3. Backward planning validation: Using the best layer from experiment 2, generate 50 steered couplets, extract second lines, remove first line context, and regenerate last word. Verify regeneration rate for target rhyme family exceeds chance baseline (>10% for most families).

## Open Questions the Paper Calls Out

- Do language models other than Gemma2 9B rely on a small number of specific attention heads to move rhyme planning information to later tokens?
- Does the utilization of the newline token in the planning circuit causally contribute to the superior planning performance observed in models like Gemma2 9B and Gemma3 27B?
- Does the ability to plan for specific target words (as opposed to general rhyme families) emerge reliably only at larger model scales?

## Limitations

- Steering vector effectiveness varies significantly across model architectures, with different models preferring different layers and positions for intervention
- Regeneration metric shows high variance and may conflate genuine planning with other factors affecting token stability
- Backward planning metrics may reflect pre-existing model preferences rather than planning-driven behavior

## Confidence

**High confidence**: Forward planning existence - The steering experiments consistently demonstrate that models encode future token properties at strategic positions.

**Medium confidence**: Planning mechanism specificity - While the paper identifies specific attention heads in Gemma2 9B, these findings may not generalize to other models.

**Medium confidence**: Instruction tuning amplification - The paper shows instruction-tuned models have stronger planning effects, but the mechanism is unclear.

## Next Checks

1. Test whether steering vectors computed for one model architecture can successfully manipulate another model to validate whether planning representations are linearly decomposable in a model-agnostic way.

2. Test interventions at multiple strategic positions simultaneously (e.g., both last word and newline token) to determine whether planning representations are distributed or localized.

3. Use activation patching to ablate the identified attention heads and MLP layers in Gemma2 9B to measure their individual contributions to planning and compare to expected effects from distributed implementation.