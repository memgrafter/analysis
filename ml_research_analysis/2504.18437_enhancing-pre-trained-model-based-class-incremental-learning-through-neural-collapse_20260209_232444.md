---
ver: rpa2
title: Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural
  Collapse
arxiv_id: '2504.18437'
source_url: https://arxiv.org/abs/2504.18437
tags:
- learning
- classes
- neural
- pre-trained
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in class-incremental
  learning (CIL) with pre-trained models (PTMs). The authors propose modeling feature
  evolution through neural collapse (NC), a phenomenon where features align in a well-separated,
  equiangular structure.
---

# Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse

## Quick Facts
- **arXiv ID**: 2504.18437
- **Source URL**: https://arxiv.org/abs/2504.18437
- **Reference count**: 40
- **Primary result**: NCPTM-CIL outperforms state-of-the-art methods by 6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark, approaching theoretical upper bounds within 1.5%

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning (CIL) with pre-trained models by leveraging neural collapse (NC), a phenomenon where features align in a well-separated, equiangular structure. The authors propose NCPTM-CIL, which introduces a dynamic ETF classifier, ETF alignment, and pull-and-push loss to maintain this structure during incremental learning. The method significantly outperforms existing approaches while approaching theoretical performance bounds.

## Method Summary
The authors model feature evolution through neural collapse (NC) in class-incremental learning with pre-trained models. Their NCPTM-CIL method introduces a dynamic Efficient Transport Flow (ETF) classifier that adapts to new classes while maintaining feature separability. The approach uses ETF alignment to ensure new features follow the collapsed structure and employs pull-and-push loss to reinforce inter-class separation. By maintaining neural collapse properties throughout incremental learning phases, the method preserves discriminative features while adapting to new classes.

## Key Results
- NCPTM-CIL achieves 6.73% improvement on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark over runner-up approaches
- The method approaches theoretical upper bounds with gaps under 1.5% on most datasets
- Ablation studies confirm each component's contribution to maintaining feature separability and reducing forgetting

## Why This Works (Mechanism)
The method leverages neural collapse, a phenomenon where features from the same class converge to a single point while different classes form a simplex equiangular tight frame (ETF). By maintaining this structure during incremental learning, the approach ensures that new classes are added without disrupting the existing well-separated feature space. The dynamic ETF classifier adapts to new classes while preserving the collapsed structure, and the pull-and-push loss actively maintains inter-class distances.

## Foundational Learning
- **Neural Collapse**: Phenomenon where features from the same class collapse to a single point while classes form a simplex ETF
  - *Why needed*: Provides a geometric framework for maintaining class separability during incremental learning
  - *Quick check*: Verify that feature norms converge and classes form equiangular patterns

- **Class-Incremental Learning (CIL)**: Learning paradigm where model is trained sequentially on new classes without revisiting old ones
  - *Why needed*: Reflects real-world scenarios where data arrives continuously
  - *Quick check*: Monitor forgetting of previously learned classes

- **Efficient Transport Flow (ETF)**: Mathematical framework for maintaining equiangular feature distributions
  - *Why needed*: Ensures new classes integrate without disrupting existing class structure
  - *Quick check*: Verify inter-class angles remain consistent across increments

- **Catastrophic Forgetting**: Phenomenon where neural networks rapidly forget previously learned information when trained on new tasks
  - *Why needed*: Primary challenge addressed by the incremental learning approach
  - *Quick check*: Measure performance degradation on old classes after learning new ones

## Architecture Onboarding

**Component Map:**
Pre-trained Model -> Feature Extractor -> Dynamic ETF Classifier -> Pull-and-Push Loss -> Updated Model

**Critical Path:**
1. Feature extraction from pre-trained model
2. Dynamic ETF classifier adaptation
3. Pull-and-push loss computation and backpropagation
4. Model update with new class integration

**Design Tradeoffs:**
- Uses cosine similarity for classification (simpler than learned metrics but may underperform on datasets with varying inter-class distances)
- Maintains fixed feature dimensionality (preserves computational efficiency but may limit expressiveness)
- Assumes neural collapse is universally beneficial (strong theoretical assumption not fully validated across all scenarios)

**Failure Signatures:**
- Performance degradation when inter-class distances vary significantly
- Increasing feature norm drift over long task sequences
- Classification accuracy plateaus below theoretical bounds

**3 First Experiments:**
1. Verify neural collapse emergence on a simple 2-class incremental task
2. Test cosine similarity classification performance on datasets with varying inter-class distances
3. Measure forgetting rate on old classes after incremental learning of new classes

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The method assumes neural collapse is universally beneficial across all CIL scenarios, but theoretical underpinnings linking NC to incremental learning performance remain incompletely explored
- Reliance on cosine similarity for classification may be suboptimal for datasets with naturally varying inter-class distances
- Evaluation focuses primarily on image classification benchmarks, leaving unclear how well the approach generalizes to other modalities or real-world applications with long task sequences

## Confidence
- **High confidence**: Experimental results demonstrating performance improvements over baseline methods across multiple benchmarks
- **Medium confidence**: Claim that the method "approaches theoretical upper bounds" lacks direct validation against true upper bounds through exhaustive methods
- **Medium confidence**: Effectiveness of pull-and-push loss in maintaining neural collapse structure during incremental learning lacks theoretical guarantees for stability across very long task sequences

## Next Checks
1. Test NCPTM-CIL on datasets with naturally varying inter-class distances (e.g., fine-grained classification tasks) to assess limitations of cosine similarity-based classification
2. Evaluate the method's performance when incrementally learning hundreds of tasks rather than typical 5-10 tasks to test long-term stability
3. Conduct cross-modal experiments (e.g., video or audio classification) to verify whether the neural collapse framework generalizes beyond image data