---
ver: rpa2
title: Introducing Answered with Evidence -- a framework for evaluating whether LLM
  responses to biomedical questions are founded in evidence
arxiv_id: '2507.02975'
source_url: https://arxiv.org/abs/2507.02975
tags:
- evidence
- green
- yellow
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces "Answered with Evidence," a framework for
  evaluating whether LLM-generated responses to biomedical questions are grounded
  in scientific literature. The authors analyzed thousands of physician-submitted
  questions using a comparative pipeline that included three evidence sources: Alexandria
  (Atropos Evidence Library), System (PubMed-based), and Perplexity (PubMed-based).'
---

# Introducing Answered with Evidence -- a framework for evaluating whether LLM responses to biomedical questions are founded in evidence

## Quick Facts
- arXiv ID: 2507.02975
- Source URL: https://arxiv.org/abs/2507.02975
- Authors: Julian D Baldwin; Christina Dinh; Arjun Mukerji; Neil Sanghavi; Saurabh Gombar
- Reference count: 21
- Primary result: Combined PubMed-based and real-world evidence sources enabled reliable (Green-badged) answers to over 70% of physician-submitted biomedical queries

## Executive Summary
This paper introduces the "Answered with Evidence" framework for evaluating whether LLM-generated biomedical answers are grounded in scientific literature. The authors analyzed 2,972 physician-submitted questions using a comparative pipeline that included three evidence sources: Alexandria (Atropos Evidence Library), System (PubMed-based), and Perplexity (PubMed-based). Their findings show that combining published literature with real-world evidence yields coverage gains because each fails on different question types, enabling reliable answers to over 70% of biomedical queries.

## Method Summary
The study employed a three-stage pipeline: (1) retrieval of evidence from three sources—System and Perplexity using PubMed, Alexandria using proprietary real-world evidence—followed by (2) LLM-generated answers, and (3) evaluation by an LLM-as-judge using a structured prompt that assesses three binary criteria: direct answerability, topical relatedness, and grounding fidelity. The framework assigns color-coded badges (Green, Yellow, Red) based on these criteria, enabling interpretable classification of evidence quality. The evaluation process was applied to 2,972 physician-submitted clinical questions from 2022-2025.

## Key Results
- Combined evidence sources enabled Green-badged answers for 72.7% of biomedical questions
- PubMed-based systems (System and Perplexity) provided evidence-supported answers for approximately 44% of questions
- The novel evidence source (Alexandria) provided evidence-supported answers for about 50% of questions
- Each source uniquely answered approximately 18% of questions, demonstrating complementarity
- Perplexity achieved 21.3% Green rate while System achieved 44.6% Green rate, despite both using PubMed

## Why This Works (Mechanism)

### Mechanism 1
Decomposing evidence quality into three binary criteria enables interpretable, reproducible badge classification. The framework evaluates each response on orthogonal dimensions—direct answerability, topical relatedness, and grounding fidelity—then maps boolean combinations to Green/Yellow/Red badges. This structure reduces ambiguity in what "evidence-based" means operationally.

### Mechanism 2
Combining published literature sources with real-world evidence sources yields coverage gains because each fails on different question types. PubMed-based systems excel where clinical trials exist but struggle with complex comorbidities often excluded from studies. RWE addresses real-world populations but may lack peer-reviewed rigor. Their non-overlapping failure modes produce complementary coverage.

### Mechanism 3
An LLM can serve as a structured judge of another LLM's grounding fidelity when given explicit evaluation criteria. The evaluator LLM receives (question, retrieved context, generated answer) and produces three boolean judgments plus free-text rationale. Structured prompting constrains outputs to a consistent schema, enabling batch evaluation at scale.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: All three evaluated systems use RAG. Understanding how retrieval quality affects generation—and why retrieval failures produce Red badges—is essential for debugging.
  - Quick check question: If retrieval returns a relevant study but the LLM's answer adds an unsupported claim about dosage, which criterion fails and what badge results?

- Concept: Evidence Hierarchy in Clinical Medicine
  - Why needed here: The study implicitly contrasts RCTs (PubMed) with observational RWE (Alexandria). Understanding why RWE may better serve complex, multi-morbid patients clarifies the complementarity result.
  - Quick check question: Why might a clinician prefer an Alexandria answer over a PubMed answer for a question about treatment outcomes in patients with 5+ comorbidities?

- Concept: Inter-source Agreement and Novelty
  - Why needed here: The study reports 56% agreement between System and Perplexity despite both using PubMed. Interpreting this requires understanding that different architectures can yield different answers from the same corpus.
  - Quick check question: If two PubMed-based systems agree on only 11% of Green badges, does this indicate redundancy or complementarity? What additional metric would clarify?

## Architecture Onboarding

- Component map: Question → Evidence Retrieval (System / Perplexity / Alexandria) → LLM Generation → Evaluation LLM (structured prompt) → Badge Assignment
- Critical path: (1) Retrieval relevance—if context is wrong, even perfect generation yields Yellow/Red. (2) Evaluation prompt calibration—if the judge misclassifies hedging or format differences, badge reliability degrades.
- Design tradeoffs: Single LLM-as-judge vs. jury: efficiency vs. robustness. Binary rubric vs. continuous scoring: interpretability for clinicians vs. nuance for researchers. General-purpose vs. source-specific evaluation prompts: cross-source comparability vs. per-source accuracy.
- Failure signatures: High Red rate indicates retrieval pipeline issues; high Yellow rate with low Green suggests legitimate evidence gaps; low inter-source agreement reflects architectural differences.
- First 3 experiments: 1) Human validation study: Sample 100 questions stratified by badge color and source; have domain experts independently assign badges. 2) Source ablation: For each question where only one source produced a Green badge, characterize the question type. 3) Prompt tuning for hedging: Test a source-calibrated evaluation prompt to determine if Perplexity's higher Yellow rate is style vs. substance.

## Open Questions the Paper Calls Out

### Open Question 1
Does an ensemble "LLM-as-a-jury" evaluation approach reduce model-specific bias compared to the single-model "LLM-as-a-judge" method currently employed? The authors explicitly state future work should explore an LLM-as-a-jury approach to improve robustness through ensemble evaluation.

### Open Question 2
How do other emerging LLM-based literature summarization platforms (e.g., Open Evidence, Consensus) perform relative to System and Perplexity when evaluated using the Answered with Evidence framework? The Discussion notes several emerging platforms were not included and warrant future benchmarking.

### Open Question 3
Does customizing the evaluation prompt for specific LLM response styles improve the classification accuracy of the badge system? The authors admit the evaluation prompt was designed to be general-purpose and not tailored to response styles, suggesting source-specific tuning may enhance accuracy.

## Limitations
- LLM-as-judge evaluation without independent human validation creates uncertainty about clinical utility accuracy
- Binary rubric may oversimplify nuanced evidence quality judgments, particularly for partial grounding or hedged responses
- Alexandria's proprietary nature prevents full reproducibility and independent verification of RWE claims
- Inter-source agreement suggests architectural differences but doesn't clarify whether this reflects genuine complementarity or inconsistent evaluation

## Confidence
- Medium confidence: The 70% combined coverage claim and the mechanism of complementary failure modes across sources
- Low confidence: The absolute accuracy of LLM judge evaluations and the clinical validity of badge assignments without human expert validation

## Next Checks
1. Human validation study: Have domain experts independently assign badges to 100 stratified samples to measure judge-LLM accuracy and inter-rater reliability
2. Source ablation analysis: Characterize question types that uniquely yield Green badges from each source to validate complementarity claims
3. Prompt tuning experiment: Test source-calibrated evaluation prompts to determine if Perplexity's higher Yellow rate reflects hedging style versus substantive grounding differences