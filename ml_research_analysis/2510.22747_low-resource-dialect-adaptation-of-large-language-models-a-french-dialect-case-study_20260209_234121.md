---
ver: rpa2
title: 'Low-Resource Dialect Adaptation of Large Language Models: A French Dialect
  Case-Study'
arxiv_id: '2510.22747'
source_url: https://arxiv.org/abs/2510.22747
tags:
- french
- language
- tasks
- training
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores how to adapt large language models (LLMs) to\
  \ low-resource regional dialects using minimal compute and data. The authors employ\
  \ continual pre-training with low-rank adaptation (LoRA) and gradient checkpointing\
  \ to efficiently specialize three LLMs\u2014CroissantLLM, Llama-3.2-1B, and Llama-3.1-8B\u2014\
  to the Qu\xE9bec French dialect."
---

# Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study

## Quick Facts
- **arXiv ID**: 2510.22747
- **Source URL**: https://arxiv.org/abs/2510.22747
- **Reference count**: 0
- **Primary result**: Demonstrated that LoRA-based continual pre-training with minimal compute and data can successfully adapt LLMs to low-resource regional dialects, achieving macro-F1 gains on Québec French tasks with only 86.5M tokens and <1% parameter updates.

## Executive Summary
This paper demonstrates that large language models can be efficiently adapted to low-resource regional dialects using minimal compute and data. The authors employ continual pre-training with LoRA and gradient checkpointing to specialize three LLMs to the Québec French dialect, achieving significant performance gains on dialect-specific tasks while maintaining general French capabilities. The study shows that even with a modest 86.57 million token corpus and updating less than 1% of model parameters, dialect adaptation is achievable, with larger models (8B parameters) providing the best balance between dialect specialization and retention of general language abilities.

## Method Summary
The authors employ continual pre-training with LoRA and gradient checkpointing to adapt three LLMs (CroissantLLM, Llama-3.2-1B, and Llama-3.1-8B) to the Québec French dialect. They train on a 86.57M token corpus from 9 sources including ebooks, Wikipedia, news, transcripts, and social media content. LoRA is applied to attention and FFN layers with r=16, α=32, dropout=0.1, while gradient checkpointing reduces memory usage. Models are trained for 3-6 epochs using AdamW with weight decay 0.01, learning rate 1e-5, cosine decay, and 10% warmup. Evaluation uses 8 COLE tasks (4 Québec French, 4 prestige French) with macro-F1 as the primary metric.

## Key Results
- All three models (1B, 1.35B, 8B) improved on Québec French tasks after 6 epochs of CPT, with macro-F1 gains ranging from +4.9% to +9.9% on dialect-specific tasks
- Larger 8B model achieved the best balance between dialect adaptation and retention of general French abilities, while 1B models showed degradation on prestige French tasks after epoch 3
- Training on <1% of parameters with only 86.5M tokens proved sufficient for meaningful dialect adaptation, demonstrating the efficiency of LoRA-based approaches for low-resource scenarios

## Why This Works (Mechanism)
Dialect adaptation through CPT works by exposing the model to domain-specific linguistic patterns and orthographic variants during training. LoRA enables efficient adaptation by learning low-rank updates to attention and FFN weights, allowing the model to acquire dialectal patterns without full fine-tuning. Gradient checkpointing reduces memory requirements, making larger models accessible. The mixture of formal and informal Québec French sources provides both normative and colloquial patterns, though the informal bias may affect grammaticality judgments.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: Efficient parameter-efficient fine-tuning technique that learns low-rank updates to model weights instead of full parameter updates, reducing memory and compute requirements by >99% while maintaining performance
- **Continual Pre-Training (CPT)**: Extending pre-training on domain-specific data to adapt general-purpose models to specialized linguistic patterns without catastrophic forgetting of base capabilities
- **Gradient Checkpointing**: Memory optimization technique that trades compute for memory by recomputing intermediate activations during backpropagation, enabling training of larger models on limited GPU memory
- **Dialect vs. Prestige Language**: Dialect refers to regional linguistic variations (Québec French) while prestige language represents standardized forms (Standard French); models must balance adaptation to dialect while retaining general language capabilities
- **COLE Benchmark**: Comprehensive evaluation suite for low-resource language models with tasks spanning grammaticality judgment, reading comprehension, coreference resolution, and sentiment analysis in both dialect and standard forms

## Architecture Onboarding
- **Component map**: Corpus (9 sources) -> Tokenizer (CroissantLLM) -> CPT w/ LoRA -> Evaluation (8 COLE tasks)
- **Critical path**: Data preparation → LoRA configuration → CPT training → Benchmark evaluation
- **Design tradeoffs**: LoRA vs. full fine-tuning (efficiency vs. maximum adaptation), corpus composition (formal vs. informal balance), model size (adaptation vs. general retention)
- **Failure signatures**: QFrCoLA degradation (accepting informal errors as correct), general French task regression (catastrophic forgetting), QA capability loss (missing domain data)
- **First experiments**: 1) Train 1B model for 3 epochs and evaluate on both dialect and general tasks; 2) Compare 1B vs. 8B models on QFrCoLA to identify grammaticality issues; 3) Test early stopping at epoch 3 to find optimal adaptation-retention balance

## Open Questions the Paper Calls Out
- **How does the ratio of formal versus informal data impact the trade-off between dialectal fluency and prescriptive grammar adherence?** The current 60/40 mix may cause acceptance of informal errors on normative tasks like QFrCoLA, warranting investigation with exclusively formal sources.
- **Can selective parameter freezing enable smaller models to adapt without catastrophic forgetting?** The 1B model's degradation on prestige French tasks suggests this unexplored approach could preserve general capabilities.
- **To what extent does CPT improve sociolinguistic authenticity and generative fluency beyond discriminative benchmarks?** Current evaluation cannot capture these features, which require human evaluation of generated text for idiomatic usage and appropriate register.

## Limitations
- Proprietary and restricted data sources (CN2i–Le Soleil news, social media APIs) cannot be shared, preventing exact reproduction
- Lack of detailed validation set construction and exact training stopping criteria introduces variance in results
- Performance gains were not uniform across models; smaller models degraded on general tasks after epoch 3
- No ablation studies on LoRA rank or alternative adapter configurations to optimize resource allocation

## Confidence
- **High confidence**: Feasibility of low-resource dialect adaptation with LoRA and gradient checkpointing on <1% parameters
- **Medium confidence**: Quantitative gains vary by model size and task type, with 8B models showing better balance but relationship not fully characterized
- **Medium confidence**: Claim that 8B models are "more effective" is supported but based on limited task sample and requires broader validation

## Next Checks
1. Reproduce corpus composition independently using openly available Québec French sources matching the 60/40 formal-informal ratio
2. Run ablation studies comparing 1B vs. 8B models with early stopping at epoch 3 to confirm the adaptation-retention trade-off
3. Expand evaluation to all 12 COLE tasks to assess generalization of gains and identify any task-specific degradation patterns