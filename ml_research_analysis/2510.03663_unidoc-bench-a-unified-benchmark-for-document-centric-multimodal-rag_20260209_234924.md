---
ver: rpa2
title: 'UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG'
arxiv_id: '2510.03663'
source_url: https://arxiv.org/abs/2510.03663
tags:
- question
- text
- answer
- retrieval
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniDoc-Bench, a unified benchmark for multimodal
  retrieval-augmented generation (MM-RAG) that addresses limitations in current evaluation
  frameworks. The benchmark is built from 70k real-world PDF pages across 8 domains,
  with 1,600 human-verified QA pairs covering text, tables, and figures.
---

# UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG

## Quick Facts
- arXiv ID: 2510.03663
- Source URL: https://arxiv.org/abs/2510.03663
- Authors: Xiangyu Peng, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Chien-Sheng Wu
- Reference count: 40
- Text-image fusion RAG systems achieve 68.4% completeness versus 64.1% for joint multimodal retrieval

## Executive Summary
This paper introduces UniDoc-Bench, a unified benchmark for multimodal retrieval-augmented generation (MM-RAG) that addresses limitations in current evaluation frameworks. The benchmark is built from 70k real-world PDF pages across 8 domains, with 1,600 human-verified QA pairs covering text, tables, and figures. Experiments comparing four paradigms—text-only, image-only, multimodal text-image fusion, and multimodal joint retrieval—show that text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, achieving 68.4% completeness versus 64.1% for joint retrieval and 65.3% for text-only retrieval. The study also reveals that image-dependent queries remain challenging across all systems, suggesting future RAG improvements should prioritize multimodal understanding.

## Method Summary
UniDoc-Bench is constructed from 70,000 PDF pages spanning 8 document domains, with evidence paragraphs and captions extracted for retrieval. The dataset contains 1,600 human-verified QA pairs, generated through a pipeline that first extracts and links multimodal evidence, then generates diverse question types including factual retrieval, comparison, summarization, and logical reasoning, with quality ensured through multi-annotator validation. Four retrieval paradigms are evaluated: text-only using text embeddings, image-only using vision-language models, text-image fusion that retrieves text and images separately then fuses them, and joint multimodal retrieval using unified multimodal embeddings.

## Key Results
- Text-image fusion RAG systems achieve 68.4% completeness, outperforming joint multimodal retrieval (64.1%) and text-only retrieval (65.3%)
- Image-dependent queries remain challenging across all systems, with performance significantly lower than text-only queries
- The benchmark reveals that current multimodal embeddings are inadequate compared to simple fusion of separate strong retrievers

## Why This Works (Mechanism)
None

## Foundational Learning
- Multimodal retrieval-augmented generation (MM-RAG): Systems that retrieve and combine information from multiple modalities (text, images) to generate answers. Why needed: Documents contain both textual and visual information that must be integrated for complete understanding. Quick check: Does the system correctly answer questions requiring both text and image evidence?

- Text-image fusion vs. joint retrieval: Fusion retrieves text and images separately then combines them, while joint retrieval uses unified multimodal embeddings. Why needed: Different architectural approaches have different strengths in handling multimodal document understanding. Quick check: Compare performance metrics between fusion and joint approaches on the same dataset.

- Multimodal evidence linking: The process of connecting related text and image elements within documents. Why needed: Documents often contain cross-references between text and figures that must be preserved for accurate retrieval. Quick check: Are related text captions and figures correctly identified as connected evidence?

## Architecture Onboarding

Component Map:
PDF pages -> Evidence extraction (text + captions) -> QA pair generation -> Human verification -> Benchmark evaluation

Critical Path:
Evidence extraction -> Retrieval pipeline selection -> Question answering -> Completeness evaluation

Design Tradeoffs:
- Separate vs. unified retrievers: Separate text and image retrievers with fusion vs. single multimodal embedding model
- Synthetic vs. real queries: Large-scale synthetic generation with verification vs. limited real user queries
- Domain coverage: Broad domain coverage vs. depth in specific document types

Failure Signatures:
- Text-only systems fail on image-dependent queries
- Joint multimodal retrieval underperforms compared to fusion strategies
- Vision-language models struggle with complex document layouts and fine-grained visual details

Three First Experiments:
1. Evaluate text-only retrieval on the benchmark to establish baseline performance
2. Test image-only retrieval to measure visual understanding capabilities
3. Compare text-image fusion vs. joint multimodal retrieval to validate the primary finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can unified multimodal embedding models be architected to outperform the current strategy of fusing separate, unimodal text and image retrievers?
- Basis in paper: [explicit] The authors conclude that "current multimodal embeddings remain inadequate," observing that joint multimodal retrieval lags behind simple text-image fusion in both recall and completeness.
- Why unresolved: The paper demonstrates that fusing separate strong retrievers yields 68.4% completeness compared to 64.1% for joint retrieval, but it does not propose architectural changes to close this performance gap.
- What evidence would resolve it: The development of a joint multimodal embedding model that achieves higher Recall@10 and answer completeness scores on UniDoc-Bench than the text-image fusion baseline.

### Open Question 2
- Question: To what extent does the exclusion of uncaptioned figures from the retrieval corpus bias the evaluation of vision-centric RAG systems?
- Basis in paper: [inferred] The methodology explicitly excludes uncaptioned figures "under the assumption they are non-informative," a filtering step that may discard visual information modern VLMs are capable of interpreting.
- Why unresolved: By removing these figures, the benchmark potentially underestimates the efficacy of pure vision-based retrieval systems that do not rely on accompanying text to understand an image.
- What evidence would resolve it: An ablation study where uncaptioned figures are retained in the retrieval pool, measuring the resulting change in performance for image-only and multimodal RAG systems.

### Open Question 3
- Question: How does the hierarchy of RAG paradigms (fusion vs. joint vs. unimodal) change when evaluating multi-turn, conversational queries rather than single-shot questions?
- Basis in paper: [explicit] The authors state the dataset "may lack the linguistic diversity and conversational dependency (e.g., multi-turn follow-ups) characteristic of organic user interactions."
- Why unresolved: It is unclear if the text-image fusion strategy remains superior when a query requires resolving context from previous turns, where textual ambiguity might increase reliance on visual persistence.
- What evidence would resolve it: Extending the benchmark to include conversational sessions and evaluating whether text-only systems degrade faster than multimodal systems when context is distributed across dialogue turns.

### Open Question 4
- Question: Do the findings regarding the superiority of text-image fusion generalize to low-resource languages or document structures outside the eight evaluated English-centric domains?
- Basis in paper: [explicit] The limitations section notes the benchmark is "currently limited to English-centric documents," suggesting the performance hierarchy may not hold where OCR quality and text embedding robustness vary.
- Why unresolved: In languages with complex scripts or lower-quality OCR, textual recall might drop significantly, potentially making image-only or joint multimodal retrieval comparatively more effective than the text-reliant fusion strategy.
- What evidence would resolve it: Cross-lingual evaluation experiments on UniDoc-Bench translated into low-resource languages, comparing the performance delta between text-based and image-based retrieval paradigms.

## Limitations
- Benchmark relies primarily on synthetic data generation rather than real-world user queries
- Coverage limited to 8 document domains, potentially limiting generalizability
- Performance differences (68.4% vs 64.1% completeness) may not be practically significant in real applications

## Confidence
High Confidence: Benchmark construction methodology and evaluation framework are well-documented and methodologically sound.

Medium Confidence: Claims about text-image fusion superiority are supported by data but may be influenced by specific document corpus and query generation methods.

Low Confidence: Assertions about generalization to real-world applications and benchmark's effectiveness in driving future MM-RAG improvements lack broader empirical validation.

## Next Checks
1. Conduct a user study with real-world queries across the same 8 domains to compare performance with synthetic benchmark results and validate practical relevance.

2. Test the benchmark's generalizability by applying the same evaluation framework to document types not included in the original corpus (e.g., scientific papers, legal documents, technical manuals).

3. Evaluate the computational efficiency and cost-effectiveness of each retrieval paradigm (text-only, image-only, text-image fusion, joint retrieval) in addition to completeness scores for holistic practical assessment.