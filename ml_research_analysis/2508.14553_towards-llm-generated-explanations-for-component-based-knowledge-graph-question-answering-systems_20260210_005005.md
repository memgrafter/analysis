---
ver: rpa2
title: Towards LLM-generated explanations for Component-based Knowledge Graph Question
  Answering Systems
arxiv_id: '2508.14553'
source_url: https://arxiv.org/abs/2508.14553
tags:
- data
- explanations
- component
- explanation
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach for generating explanations for
  component-based Question Answering (QA) systems, focusing on the verbalization of
  SPARQL queries and RDF triples as input/output data flows. The authors implemented
  both template-based and LLM-based (GPT-3.5 and GPT-4) methods for explanation generation.
---

# Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems

## Quick Facts
- arXiv ID: 2508.14553
- Source URL: https://arxiv.org/abs/2508.14553
- Reference count: 1
- Primary result: LLM-generated explanations outperformed template-based approaches for QA component data flows, achieving average quality ratings of 3.7-3.8 versus 3.5-3.7 for templates.

## Executive Summary
This paper presents an approach for generating explanations for component-based Question Answering (QA) systems by verbalizing SPARQL queries and RDF triples that represent input/output data flows. The authors implemented both template-based and LLM-based methods (using GPT-3.5 and GPT-4) for explanation generation. Expert evaluation showed that LLM-generated explanations achieved higher quality ratings than template-based approaches, with correctness and usefulness averaging around 3.7 on a 5-point scale. The study demonstrates that Large Language Models are well-suited for automatically generating human-readable explanations of QA component behavior, providing valuable insights into system transparency and interpretability.

## Method Summary
The authors developed a Qanary-based QA system where components process user questions through SPARQL queries and RDF data flows. They created template-based explanations using Java code that matched input/output data types to predefined patterns. For LLM-based explanations, they used few-shot prompting with GPT-3.5 and GPT-4, providing context about component type and input/output data. The system was evaluated using both quantitative metrics (Q_E formula measuring factual accuracy) and qualitative expert ratings (correctness and usefulness on 5-point scales) across 13 components and 10 test questions from the LC-QuAD 1.0 dataset.

## Key Results
- LLM-generated explanations achieved higher quality ratings than template-based approaches (3.7-3.8 vs 3.5-3.7 average)
- Few-shot prompting with type-matched examples improved explanation quality for RDF verbalization
- Quantitative analysis showed correlation coefficients between 0.015-0.595 for output data explanations
- GPT-4 generally outperformed GPT-3.5, particularly for RDF verbalization tasks
- Template-based approach achieved perfect factual accuracy but lower usefulness ratings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Component data flows (SPARQL inputs and RDF outputs) can serve as sufficient signals for explaining system behavior, bypassing the need to explain internal black-box AI models.
- Mechanism: The Qanary framework explicitly logs each component's SPARQL SELECT queries (input) and grounded RDF triples (output) to a central triplestore. These structured data flows are then verbalized into natural language, revealing what data was requested and produced without requiring access to internal model logic.
- Core assumption: Observable input/output data adequately represents component behavior for human interpretation.
- Evidence anchors:
  - [abstract]: "we present an approach that considers the components' input and output data flows as a source for representing the behavior and provide explanations for the components"
  - [section 3]: "They fetch the required data from a centralized knowledgebase (a triplestore) via SPARQL... After finishing the computation, each Qanary component will store the data in the process memory to make it accessible to follow-up components."
  - [corpus]: Weak direct support; neighboring papers focus on KGQA methods, not data-flow-based explainability.
- Break condition: If components perform significant internal transformations that are not reflected in their I/O signatures, explanations will be incomplete or misleading.

### Mechanism 2
- Claim: Few-shot LLM prompting can verbalize structured SPARQL and RDF data into human-readable explanations with quality comparable to or exceeding template-based approaches.
- Mechanism: The authors use GPT-3.5 and GPT-4 with 0-2 shot prompting, providing example query-explanation pairs or RDF-explanation pairs. The LLM generates natural language descriptions constrained to the provided context, without fine-tuning.
- Core assumption: Pre-trained LLMs possess sufficient understanding of SPARQL syntax and RDF semantics to produce accurate verbalizations from minimal examples.
- Evidence anchors:
  - [abstract]: "LLM-generated explanations achieved higher quality, with correctness and usefulness ratings averaging around 3.7 on a 5-point scale, outperforming templates"
  - [section 4, Results]: "Firstly, all generatively computed explanations achieved better results than the template-based ones."
  - [corpus: Counterfactual Simulatability paper]: Suggests LLM explanation faithfulness remains an open concern, indicating potential limitations in explanation accuracy.
- Break condition: If LLMs hallucinate values, misattribute predicates, or omit critical data elements—observed in the study as "missing scores" and "incorrect values"—explanation quality degrades.

### Mechanism 3
- Claim: Matching example data types to test data types in few-shot prompts improves explanation quality for RDF verbalization.
- Mechanism: In few-shot experiments, providing examples from the same annotation type (O1–O4) as the target data yielded higher ratings than mismatched types. This aligns the LLM's pattern-matching to the specific predicate structure and cardinality of the target data.
- Core assumption: RDF structures share enough regularity within annotation types that type-matched examples provide useful inductive bias.
- Evidence anchors:
  - [section 4, Quantitative Results]: "For all except O1, experiments performed best when the example and test data type matched."
  - [section 4]: "generative AI usually works best when the model is provided by concrete examples that only need to be repeated"
  - [corpus]: No direct corpus support; this finding is specific to the paper's experimental setup.
- Break condition: If annotation types have high internal variance or novel predicate patterns, type-matching may not generalize.

## Foundational Learning

- Concept: **SPARQL Query Structure (SELECT, WHERE, patterns)**
  - Why needed here: Input explanations require understanding what data a SPARQL query requests, including variables, filters, and graph patterns.
  - Quick check question: Given a SPARQL SELECT query, can you identify which variables are returned and what constraints filter the results?

- Concept: **RDF Triples (subject-predicate-object) and Grounding**
  - Why needed here: Output explanations verbalize grounded RDF triples; understanding the triple structure and what "grounded" means (no variables, concrete URIs/literals) is essential.
  - Quick check question: What distinguishes a grounded RDF triple from one containing blank nodes or variables?

- Concept: **Few-shot Prompting and Example Selection**
  - Why needed here: The LLM-based approach relies on 0-2 shot prompts; selecting representative examples is critical for quality.
  - Quick check question: If your test data contains annotations of type O2, should your few-shot examples come from O2, O1, or a mix? What tradeoffs exist?

## Architecture Onboarding

- Component map:
  - **Qanary Framework**: Orchestrator managing component pipeline; central triplestore stores process memory.
  - **Qanary Components (C1–C13)**: Web services performing NED, NER, Relation Extraction, Query Building; each reads/writes to triplestore via SPARQL.
  - **Explanation Service**: Java-based template engine (baseline) or LLM API calls (GPT-3.5/GPT-4) generating natural language from data flows.
  - **Evaluation Layer**: Quantitative formula (Q_E) and expert qualitative ratings.

- Critical path:
  1. User question enters QA system.
  2. Orchestrator activates components in sequence; each executes SPARQL SELECT, processes data, writes RDF triples.
  3. Explanation service retrieves input (SPARQL) and output (RDF) from triplestore.
  4. For template path: match query/annotation type to pre-defined template, fill placeholders.
  5. For LLM path: construct prompt with context, examples, and target data; call GPT API.
  6. Return natural language explanation to user or auditor.

- Design tradeoffs:
  - **Template vs. LLM**: Templates are deterministic and auditable but require manual maintenance; LLMs generalize but may hallucinate or miscount.
  - **Zero-shot vs. Few-shot**: Zero-shot requires no examples but lower quality; few-shot improves accuracy but increases prompt complexity and token cost.
  - **GPT-3.5 vs. GPT-4**: GPT-4 yields better quantitative scores (especially for RDF) but at higher cost.

- Failure signatures:
  - **Missing values in explanations**: LLM omits predicates like confidence scores; indicates prompt-example misalignment or data complexity.
  - **Incorrect entity attribution**: Hallucinated resources or positions; suggests LLM over-generalization from examples.
  - **Annotation count mismatch**: Prefix states wrong number of annotations; quantitative formula penalizes this.
  - **Template rigidity**: New annotation types require new templates; system cannot explain unseen data types.

- First 3 experiments:
  1. **Replicate template baseline for one component type (e.g., O1 NED components)**: Extract SPARQL input and RDF output for 10 questions; apply provided templates; rate correctness/usefulness on 5-point scale to establish baseline.
  2. **Zero-shot LLM explanation for same data**: Use GPT-3.5 with prompt template (Figure 3a/3b), no examples; compare ratings to template baseline; identify hallucination patterns.
  3. **One-shot LLM with type-matched example**: Select one high-quality example explanation from O1; include in prompt; evaluate whether type-matching improves quantitative score (Q_E) and qualitative ratings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the current approach be extended to explain systems and their hierarchical aggregations of components?
- Basis in paper: [explicit] The conclusion explicitly suggests "investigating the possibility of explaining systems and their hierarchical aggregations of components."
- Why unresolved: The current study focused strictly on the input and output data flows of individual components, not the complex interactions or aggregations of multiple components acting as a parent system.
- What evidence would resolve it: A demonstration of the methodology applied to a multi-level system architecture where components encapsulate other components, yielding coherent system-level explanations.

### Open Question 2
- Question: How does the performance of LLM-generated explanations change when utilizing a wider range of datasets or alternative methodologies?
- Basis in paper: [explicit] The conclusion states that "Further research... utilizing a range of datasets or alternative methodologies, could prove beneficial."
- Why unresolved: The experiments were limited to the QALD-10 dataset and specific OpenAI models; it is unclear if the high quality ratings hold for other domains or model architectures.
- What evidence would resolve it: Comparative evaluations across diverse datasets and different LLM configurations or fine-tuning strategies.

### Open Question 3
- Question: How can the "missing and wrong values" (anomalies) in LLM-generated output explanations be minimized?
- Basis in paper: [inferred] The quantitative evaluation section notes that "inferior outcomes can be attributed to the presence of missing and wrong values," such as hallucinated resources or incorrect counts.
- Why unresolved: While LLMs outperformed templates, the quantitative scoring formula penalized the LLMs for factual errors in the RDF triples, indicating the verbalization process is not fully grounded.
- What evidence would resolve it: A modified generation approach (e.g., constrained decoding) that results in significantly fewer quantitative deductions in the $Q_E$ score for factual accuracy.

## Limitations

- The evaluation was conducted on a specific Qanary-based system using the LC-QuAD 1.0 dataset, limiting generalizability to other QA systems and question types.
- Expert evaluation involved only three annotators, raising concerns about inter-rater reliability and potential bias in quality assessments.
- The quantitative formula Q_E showed only moderate correlation with qualitative ratings (0.015-0.595), suggesting it may not fully capture explanation quality or that human ratings are subjective.

## Confidence

- **High Confidence**: The superiority of LLM-generated explanations over template-based approaches (supported by both quantitative scores and expert ratings)
- **Medium Confidence**: The effectiveness of few-shot prompting with type-matched examples (based on experimental results but limited to specific annotation types)
- **Medium Confidence**: The adequacy of input/output data flows for explaining component behavior (core assumption but not directly validated against alternative approaches)

## Next Checks

1. **Cross-dataset validation**: Test the approach on additional QA datasets (e.g., WebQuestions, ComplexWebQuestions) to assess generalizability across question complexity and domain.

2. **Inter-annotator reliability assessment**: Conduct formal inter-rater reliability analysis (e.g., Fleiss' kappa) on the expert evaluations to quantify agreement and identify systematic rating biases.

3. **Ablation study on prompt components**: Systematically remove or modify elements of the LLM prompts (context, examples, instruction phrasing) to isolate which factors most strongly influence explanation quality and identify failure modes.