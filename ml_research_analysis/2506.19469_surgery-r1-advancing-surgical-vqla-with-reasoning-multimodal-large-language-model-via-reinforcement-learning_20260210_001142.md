---
ver: rpa2
title: 'Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language
  Model via Reinforcement Learning'
arxiv_id: '2506.19469'
source_url: https://arxiv.org/abs/2506.19469
tags:
- reasoning
- surgical
- question
- visual
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Surgery-R1, the first Reasoning Multimodal
  Large Language Model (MLLM) for Surgical-VQLA. The authors address the limitations
  of existing Surgical-VQLA models that lack deep reasoning capabilities and interpretability.
---

# Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.19469
- Source URL: https://arxiv.org/abs/2506.19469
- Reference count: 40
- Introduces Surgery-R1, the first Reasoning Multimodal Large Language Model for Surgical-VQLA

## Executive Summary
This paper presents Surgery-R1, the first Reasoning Multimodal Large Language Model designed specifically for Surgical-VQA tasks. The authors address the limitations of existing Surgical-VQA models that lack deep reasoning capabilities and interpretability by developing a comprehensive approach that includes dataset construction, fine-tuning methodology, and a specialized reward system. The model achieves state-of-the-art performance on the EndoVis-18 and EndoVis-17 datasets, demonstrating significant improvements in surgical video understanding and spatial reasoning.

## Method Summary
The authors introduce a two-stage fine-tuning mechanism for Surgical-VQA. First, they construct the Surgery-R1-54k dataset with paired data for Visual-QA, Grounding-QA, and Chain-of-Thought reasoning. Then they employ supervised fine-tuning (SFT) followed by reinforcement fine-tuning (RFT) using Group Relative Policy Optimization (GRPO). A rule-based reward system is designed to address spatial hallucinations, incorporating Visual Grounding, Linguistic Answer, and Multimodal Coherence rewards. This comprehensive approach enables the model to perform complex reasoning tasks while maintaining spatial consistency in surgical video analysis.

## Key Results
- Achieves 0.7356 accuracy on EndoVis-18 dataset
- Achieves 0.4576 F-score on EndoVis-18 dataset
- Achieves 0.8721 mIoU on EndoVis-18 dataset
- Outperforms existing state-of-the-art models on both EndoVis-18 and EndoVis-17 benchmarks

## Why This Works (Mechanism)
Surgery-R1 works by integrating deep reasoning capabilities with multimodal understanding through a carefully designed training pipeline. The two-stage fine-tuning approach first establishes a strong baseline through supervised learning on the curated dataset, then refines performance using reinforcement learning with a specialized reward system. The rule-based reward mechanism specifically targets spatial hallucination reduction by providing explicit feedback on visual grounding accuracy, linguistic answer quality, and multimodal coherence. This combination enables the model to develop both the reasoning depth needed for complex surgical questions and the spatial awareness required for accurate tool and anatomy localization.

## Foundational Learning

1. **Chain-of-Thought (CoT) reasoning** - Why needed: Enables complex multi-step reasoning for surgical questions; Quick check: Verify model can decompose complex questions into logical reasoning steps.

2. **Visual Grounding in surgical videos** - Why needed: Critical for accurately localizing surgical tools and anatomical structures; Quick check: Validate bounding box predictions align with ground truth annotations.

3. **Reinforcement Learning with GRPO** - Why needed: Optimizes model behavior through policy-based learning rather than just supervised correction; Quick check: Monitor reward convergence during training.

4. **Multimodal Coherence** - Why needed: Ensures consistency between visual, textual, and spatial information; Quick check: Test model responses maintain logical consistency across modalities.

5. **Rule-based reward design** - Why needed: Provides explicit feedback to reduce spatial hallucinations and improve accuracy; Quick check: Verify reward rules correctly identify spatial inconsistencies.

6. **Supervised Fine-Tuning (SFT)** - Why needed: Establishes baseline performance before reinforcement optimization; Quick check: Compare SFT-only performance against full pipeline.

## Architecture Onboarding

Component Map: Dataset Construction -> Supervised Fine-Tuning -> Reinforcement Fine-Tuning -> Reward System -> Inference Engine

Critical Path: The essential flow begins with the curated Surgery-R1-54k dataset feeding into supervised fine-tuning, which establishes foundational capabilities. This is then enhanced through reinforcement fine-tuning using the rule-based reward system, ultimately producing the final model for surgical video question answering.

Design Tradeoffs: The rule-based reward system prioritizes spatial accuracy over potentially more creative but incorrect responses. The two-stage approach balances stable learning from supervised signals with adaptive optimization from reinforcement signals. Dataset construction focuses on quality and diversity rather than sheer quantity.

Failure Signatures: The model may struggle with novel surgical tools not present in training data, complex occlusions, or extreme lighting conditions. Spatial hallucination failures typically manifest as plausible but spatially inconsistent tool or anatomy localizations. Reasoning failures occur when multi-step questions require knowledge beyond the training distribution.

First Experiments:
1. Evaluate model performance on held-out validation set from the Surgery-R1-54k dataset
2. Test model's ability to handle progressively more complex Chain-of-Thought questions
3. Assess spatial grounding accuracy on challenging surgical video frames with occlusions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions include the model's generalization to surgical procedures beyond those in EndoVis-18 and EndoVis-17, its performance under different video quality conditions, and its ability to handle real-time surgical scenarios.

## Limitations
- The rule-based reward system may not generalize well to complex real-world surgical scenarios beyond the curated datasets
- Performance evaluation is limited to the relatively narrow surgical domains of EndoVis-18 and EndoVis-17
- The two-stage fine-tuning approach lacks ablation studies to isolate the contribution of each stage
- The rule-based reward design could introduce bias toward specific spatial patterns

## Confidence
- **High confidence** in the dataset construction methodology and the two-stage fine-tuning framework
- **Medium confidence** in the effectiveness of the rule-based reward system for reducing spatial hallucinations
- **Low confidence** in the generalizability of the reported results to broader surgical contexts and real-world clinical settings

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of supervised fine-tuning and reinforcement fine-tuning stages
2. Evaluate the model on additional surgical video datasets (e.g., Cholec80, M2CAI) to assess cross-domain generalization
3. Perform qualitative analysis of the model's outputs on challenging cases (e.g., occlusions, low visibility) to identify failure modes and limitations of the reward system