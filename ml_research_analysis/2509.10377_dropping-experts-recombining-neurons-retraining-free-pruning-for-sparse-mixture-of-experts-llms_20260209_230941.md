---
ver: rpa2
title: 'Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse
  Mixture-of-Experts LLMs'
arxiv_id: '2509.10377'
source_url: https://arxiv.org/abs/2509.10377
tags:
- expert
- experts
- arxiv
- pruning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing memory usage in
  Sparse Mixture-of-Experts (SMoE) large language models, which still require loading
  all expert parameters despite activating only a few per token. The proposed DERN
  framework introduces a novel neuron-level pruning and reconstruction approach that
  decomposes experts into functional segments, reassigns them to retained experts
  based on structural similarity, and merges them via spherical weighted k-means clustering.
---

# Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs

## Quick Facts
- **arXiv ID:** 2509.10377
- **Source URL:** https://arxiv.org/abs/2509.10377
- **Reference count:** 18
- **Primary result:** DERN framework achieves >5% performance gains on commonsense reasoning and MMLU benchmarks under 50% expert sparsity for Mixtral, Qwen, and DeepSeek models, without retraining.

## Executive Summary
This paper introduces DERN, a retraining-free pruning framework for Sparse Mixture-of-Experts (SMoE) LLMs that addresses the memory bottleneck of loading all expert parameters. The key insight is that experts can be decomposed into functional neuron-level segments, which can be reassigned and merged based on structural similarity rather than whole-expert averaging. DERN uses router statistics to identify important experts, decomposes them into triplets of gate, up-projection, and down-projection vectors, reassigns segments to compatible retained experts via cosine similarity, and merges them using spherical weighted k-means clustering. Experiments show significant performance improvements over existing methods while reducing memory usage and improving inference speed.

## Method Summary
DERN operates in three stages: (1) expert pruning using router statistics to identify the most important experts based on activation frequency and strength, (2) segment-level recombination where experts are decomposed into functional triplets (wg,i, wu,i, wd,i) and reassigned to retained experts based on maximum cosine similarity above a threshold α, and (3) reconstruction using spherical weighted k-means clustering with gate-based initialization to merge reassigned segments into compact experts. The framework uses a calibration set of 128 sequences from C4 to compute router importance scores, decomposes experts into neuron-level segments, and employs spherical k-means to preserve functional diversity while reducing parameter count.

## Key Results
- Achieves over 5% performance gains on commonsense reasoning and MMLU benchmarks under 50% expert sparsity
- Reduces memory usage and improves inference speed by up to 38% while maintaining competitive accuracy
- Gate-based initialization for spherical k-means outperforms random initialization (63.89 vs 48.61 average accuracy on Mixtral-4×7B)
- Optimal performance achieved at α threshold between 0.4 and 0.6
- Shows significant performance degradation on DeepSeek-MoE due to highly specialized expert architecture

## Why This Works (Mechanism)

### Mechanism 1: Router Statistics Capture Expert Utility
Router activations identify which experts contribute most to downstream predictions under realistic input distributions. The importance score combines activation frequency via the indicator function and activation strength via normalized routing weights, reflecting both how often an expert is selected and how strongly it contributes when selected. The core assumption is that a calibration set of 128 sequences from C4 is representative of the downstream task distribution, so router behavior generalizes from calibration to evaluation.

### Mechanism 2: Segment-Level Recombination Preserves Functional Units
Decomposing experts into minimal functional triplets and reassigning based on local structural similarity preserves more knowledge than whole-expert averaging. Each expert segment is a triplet from the GLU expert, and these segments are functionally independent so they can be reassigned individually. Recombination uses max cosine similarity between a candidate segment and any segment in a retained expert, with a threshold α controlling inclusion.

### Mechanism 3: Spherical Weighted K-Means Produces Representative Centroids
Clustering reassigned segments via spherical weighted k-means produces compact experts that preserve functional diversity. The objective minimizes weighted cosine distance to cluster centers, with cluster centers initialized from high-norm gate vectors which bound activation magnitude. Updates use norm equalization and importance-weighted averaging to prevent high-norm segments from dominating.

## Foundational Learning

- **Gated Linear Units (GLU):** Why needed here: DERN segments are defined on GLU experts (Wg, Wu, Wd). Understanding that the expert output is f(x) = Wd(σ(Wgx)⊙(Wux)) is essential to see why segments are functionally independent. Quick check question: Can you explain why the triplet (wg,i, wu,i, wd,i) forms a self-contained computational unit in a GLU expert?

- **Cosine Similarity for High-Dimensional Alignment:** Why needed here: The paper's core observation is that experts have "semantic conflicts at the neuron level." Cosine similarity measures orientation alignment regardless of magnitude, which is critical for comparing weight vectors across experts. Quick check question: Why would cosine similarity be preferred over Euclidean distance when comparing weight vectors from different experts?

- **Spherical K-Means Clustering:** Why needed here: The reconstruction stage uses spherical weighted k-means, which clusters points on the unit hypersphere based on angular similarity. This differs from standard k-means. Quick check question: What is the key difference between spherical k-means and standard k-means, and why might angular similarity be more appropriate for weight clustering?

## Architecture Onboarding

- **Component map:**
  Input token x → Router G(x) → Importance scores Si → Pruned expert set Er → Expert Er decomposed into segments Sr = {seg_i = (wg,i, wu,i, wd,i)} → Pruned experts' segments → Pool P → Reassigned to Er via max cosine sim → Combined segments S_r^int ∪ S_r^ext → Spherical weighted k-means → Reconstructed expert E'_r → Compact model with fewer experts, fewer segments per expert

- **Critical path:**
  1. Calibration data → Router statistics (activation frequency + strength)
  2. Expert importance scores → Identify top-k retained experts
  3. Segment decomposition → Pool all segments from pruned experts
  4. Similarity matching (Eq. 6) → Assign each segment to most compatible retained expert (if sim > α)
  5. Route adjustment (Eq. 8) → Softly transfer routing contributions
  6. Clustering (Eq. 10-12) → Compress segments within each expert

- **Design tradeoffs:**
  - α threshold: Low α (e.g., 0) merges indiscriminately, introducing noise; high α (e.g., 1.0) merges nothing, losing knowledge. Best performance at α ∈ [0.4, 0.6]
  - Clustering initialization: Gate-based (top-k by ℓ∞ norm) outperforms random. Assumption: high-norm gate vectors indicate important segments
  - Segment representation: Using only up and down vectors (excluding gate) performs better than including all three, suggesting activation dynamics add noise
  - Expert architecture: DeepSeek-MoE's highly specialized experts (low inter-expert similarity) degrade more under pruning than Mixtral/Qwen

- **Failure signatures:**
  1. Router misalignment: If calibration data is unrepresentative, importance scores Si will incorrectly rank experts, pruning critical knowledge. Signature: Sudden drops in specific task categories
  2. Over-merging (low α): Semantically incompatible segments get merged, introducing conflicts. Signature: Performance degrades as α→0
  3. Under-merging (high α): Too few segments are reassigned, losing knowledge from pruned experts. Signature: Performance drops as α→1
  4. Cluster initialization failure: Random initialization yields poor centroids. Signature: Large performance gap vs gate-based init
  5. Specialized expert architecture: Low inter-expert similarity means segments cannot be meaningfully reassigned. Signature: DeepSeek-MoE underperforms relative to Mixtral/Qwen

- **First 3 experiments:**
  1. Validate router importance scoring: Apply DERN's Stage 1 only (prune experts without recombination). Compare against random expert selection and expert selection based on parameter norms. Measure performance drop on PIQA/BoolQ/HellaSwag at 50% sparsity.
  2. Ablate segment recombination: Fix expert pruning, vary α ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}. Plot average benchmark performance vs α. Expected: Inverted-U shape peaking at 0.4-0.6.
  3. Test across architectures: Apply full DERN to Mixtral-8×7B and DeepSeek-MoE-16B at 50% sparsity. Compare performance gaps relative to unpruned baseline. Expected: DeepSeek shows larger degradation due to expert specialization.

## Open Questions the Paper Calls Out
None

## Limitations
- **Architectural sensitivity:** Poor performance on DeepSeek-MoE due to highly specialized experts with low inter-expert similarity
- **Representation choice ambiguity:** Ablation study shows simpler representations (up/down only) outperform full GLU decomposition, yet paper proceeds with three-vector representation
- **Calibration data dependency:** Router statistics from fixed 128-sequence C4 set may not generalize across diverse downstream tasks

## Confidence
**High confidence:** Effectiveness of gate-based initialization for spherical k-means clustering and general trend of performance degradation with increasing α threshold
**Medium confidence:** Superiority of router-based expert selection over random or parameter-norm-based selection, though calibration data representativeness needs validation
**Low confidence:** Claim that cosine similarity in parameter space captures functional compatibility lacks ablation studies comparing parameter-space vs function-space similarity

## Next Checks
1. **Downstream distribution robustness:** Apply DERN using calibration data from different domains (scientific papers, code, legal documents) and evaluate zero-shot performance on commonsense reasoning benchmarks to test generalization
2. **Functional vs structural similarity:** Design experiment where segments with high cosine similarity but low functional compatibility are artificially merged, compared to segments with lower cosine similarity but higher functional compatibility
3. **Architecture-agnostic pruning baseline:** Implement simple expert pruning based on average activation magnitude without recombination, apply to Mixtral/Qwen/DeepSeek at 50% sparsity, compare performance degradation patterns