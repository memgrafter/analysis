---
ver: rpa2
title: 'Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic
  Forgetting in Multimodal LLMs'
arxiv_id: '2505.16703'
source_url: https://arxiv.org/abs/2505.16703
tags:
- language
- visual
- arxiv
- neurons
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in multimodal large
  language models (MLLMs), where visual instruction tuning degrades the base LLM's
  language abilities. The authors propose Locate-then-Merge, a training-free parameter
  fusion framework that first identifies important neurons based on parameter change
  magnitude, then selectively merges them.
---

# Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2505.16703
- **Source URL**: https://arxiv.org/abs/2505.16703
- **Reference count**: 11
- **Primary result**: Neuron-Fusion achieves better overall ability (language + visual) than Task Arithmetic, TIES, Breadcrumbs, DARE, and DELLA on 13 benchmarks across two MLLMs

## Executive Summary
This paper addresses catastrophic forgetting in multimodal large language models (MLLMs), where visual instruction tuning degrades the base LLM's language abilities. The authors propose Locate-then-Merge, a training-free parameter fusion framework that first identifies important neurons based on parameter change magnitude, then selectively merges them. Their Neuron-Fusion method preserves neurons with large parameter changes (likely storing visual capabilities) while suppressing widespread small changes (likely degrading language skills). Experiments on 13 benchmarks across two MLLMs show Neuron-Fusion consistently outperforms existing model merging methods.

## Method Summary
Locate-then-Merge is a training-free parameter fusion framework for multimodal LLMs that mitigates catastrophic forgetting. The method works in two stages: first, it identifies important neurons by calculating the magnitude of parameter changes between the base LLM and the tuned MLLM; second, it selectively merges these neurons using a weighted combination that preserves large changes (associated with visual capabilities) while suppressing small, widespread changes (associated with language degradation). The approach requires no additional training or gradient updates, making it computationally efficient compared to fine-tuning approaches.

## Key Results
- Neuron-Fusion consistently outperforms existing model merging methods (Task Arithmetic, TIES, Breadcrumbs, DARE, DELLA) on 13 benchmarks across two MLLMs
- The method achieves better overall ability (language + visual) while maintaining both language and visual capabilities
- Generation analysis shows Neuron-Fusion effectively reduces context hallucination and "Not-Known" responses
- Best configurations (Neu-P-TaskA and Neu-P-Bread) achieve the highest overall accuracy

## Why This Works (Mechanism)
The method works by recognizing that parameter changes during multimodal tuning follow a power-law distribution: large changes are concentrated and likely store visual capabilities, while small changes are widespread and likely degrade language skills. By identifying neurons based on parameter change magnitude and applying selective merging that preserves large changes while suppressing small ones, the framework maintains both capabilities. The training-free nature allows direct application without additional computational overhead.

## Foundational Learning

**Power-law distributions in neural networks**: Many natural and artificial systems exhibit power-law behavior where few elements have large values while many have small values. Why needed: Understanding this distribution pattern is crucial for identifying which neurons to preserve versus suppress. Quick check: Does your data follow a power-law distribution? Plot log(rank) vs log(frequency) to verify.

**Catastrophic forgetting**: The phenomenon where neural networks rapidly lose previously learned capabilities when trained on new tasks. Why needed: This is the core problem being addressed - preserving base LLM language capabilities while adding visual understanding. Quick check: Evaluate model performance on old tasks after fine-tuning on new tasks to measure forgetting.

**Parameter space geometry**: The structure and topology of model parameters in high-dimensional space. Why needed: Understanding how different capabilities map to parameter subspaces helps design effective merging strategies. Quick check: Visualize parameter changes using dimensionality reduction techniques like PCA to identify distinct clusters.

## Architecture Onboarding

**Component map**: Base LLM -> Visual Tuning -> Parameter Change Analysis -> Neuron Selection -> Weighted Merging -> Fused Model

**Critical path**: The most critical steps are accurate neuron identification based on parameter magnitude changes and appropriate weight assignment during merging. Errors in either step directly impact the preservation of capabilities.

**Design tradeoffs**: Training-free vs fine-tuning (speed vs adaptivity), selective vs uniform merging (precision vs simplicity), magnitude-based vs gradient-based selection (simplicity vs semantic alignment).

**Failure signatures**: If large parameter changes don't correlate with capability preservation, or if selective merging introduces instability, the model may show degraded performance on both language and visual tasks.

**First experiments**: 1) Verify power-law distribution of parameter changes, 2) Test neuron selection sensitivity to threshold parameters, 3) Compare performance across different weight schemes for merging.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two MLLM architectures (LLaVA-1.5 and mPLUG-Owl-V2), raising questions about generalization
- Method cannot adapt to specific task distributions or incorporate task-specific knowledge beyond parameter changes
- Does not investigate whether parameter magnitude changes truly correlate with semantic importance versus training dynamics artifacts

## Confidence

**Major Claims Confidence Assessment:**
- **High confidence**: Empirical results showing Neuron-Fusion outperforming existing merging methods on tested benchmarks
- **Medium confidence**: Interpretation that large parameter changes indicate visual capability storage while small changes indicate language degradation
- **Low confidence**: Broader claim that this approach fundamentally solves catastrophic forgetting in multimodal settings

## Next Checks

1. Conduct ablation studies to determine whether parameter magnitude changes truly correlate with semantic importance versus being artifacts of training dynamics
2. Test generalization across diverse MLLM architectures including those with different tokenization schemes and modality integration strategies
3. Evaluate long-term retention by testing on tasks that were not part of the original tuning but should benefit from preserved capabilities