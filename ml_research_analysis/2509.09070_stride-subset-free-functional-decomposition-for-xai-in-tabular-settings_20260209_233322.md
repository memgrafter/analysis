---
ver: rpa2
title: 'STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings'
arxiv_id: '2509.09070'
source_url: https://arxiv.org/abs/2509.09070
tags:
- stride
- functional
- decomposition
- kernel
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STRIDE addresses the limitation of scalar-based explanations by
  providing a subset-free, model-agnostic functional decomposition in RKHS, capturing
  interactions without explicit enumeration. It uses recursive kernel centering to
  compute orthogonal components analytically, improving interpretability by revealing
  synergy/redundancy and enabling quantitative interaction removal ("component surgery").
---

# STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings

## Quick Facts
- **arXiv ID**: 2509.09070
- **Source URL**: https://arxiv.org/abs/2509.09070
- **Authors**: Chaeyon Ko
- **Reference count**: 31
- **Primary result**: Subset-free functional decomposition in RKHS with 3.0× speedup over TreeSHAP

## Executive Summary
STRIDE introduces a novel approach to interpretable machine learning that addresses the limitations of scalar-based explanations by providing a subset-free functional decomposition in Reproducing Kernel Hilbert Spaces (RKHS). The method captures complex feature interactions without requiring explicit enumeration, offering both global and local interpretability. By using recursive kernel centering, STRIDE computes orthogonal components analytically, enabling deeper insights into model behavior and the functional necessity of learned interactions.

## Method Summary
STRIDE builds on functional decomposition principles from cooperative game theory but extends them to the kernel space, eliminating the need for subset enumeration that limits traditional methods. The core innovation is recursive kernel centering, which constructs orthogonal components representing main effects and interactions analytically in RKHS. This approach is model-agnostic and can handle both discrete and continuous features. The method introduces "component surgery" as a novel mechanism for quantifying the importance of interactions by measuring performance degradation when specific components are removed from the model.

## Key Results
- Achieved median 3.0× speedup over TreeSHAP across 10 tabular datasets
- Demonstrated mean R²=0.93 for model reconstruction accuracy
- Strong global rank agreement between explanations and model predictions
- Removing top interaction from California Housing reduced test R² by 0.023±0.004, demonstrating functional necessity of learned interactions

## Why This Works (Mechanism)
The method works by decomposing the prediction function into orthogonal components in RKHS, where each component represents either a main effect or an interaction between features. Recursive kernel centering ensures these components are orthogonal, allowing them to capture distinct aspects of the model's behavior. By operating in kernel space, STRIDE avoids the combinatorial explosion of subset enumeration while maintaining the ability to capture complex, non-linear interactions. The analytical computation of components enables both computational efficiency and the ability to remove specific interactions (component surgery) to quantify their functional importance.

## Foundational Learning
- **Reproducing Kernel Hilbert Spaces**: Why needed - Provides the mathematical framework for representing functions and their decomposition; Quick check - Verify kernel matrix is positive semi-definite
- **Functional Decomposition**: Why needed - Separates main effects from interactions in prediction models; Quick check - Confirm orthogonality of computed components
- **Kernel Centering**: Why needed - Ensures components capture unique information without redundancy; Quick check - Validate centered kernel matrices have zero column/row means
- **Orthogonal Basis Construction**: Why needed - Enables independent interpretation of each component; Quick check - Confirm pairwise dot products of components are near zero
- **Component Surgery**: Why needed - Quantifies functional importance of specific interactions; Quick check - Measure performance drop when removing components

## Architecture Onboarding

Component Map: Data -> Kernel Matrix -> Orthogonal Components -> Explanation/Interaction Analysis

Critical Path: The method takes input data, constructs kernel matrices for each feature subset, applies recursive centering to obtain orthogonal components, then uses these components for both local and global explanations. The critical computational step is the kernel centering recursion, which must be performed efficiently to achieve speedup claims.

Design Tradeoffs: The primary tradeoff is between kernel expressiveness and computational cost - more complex kernels capture richer interactions but increase computation. STRIDE addresses this by using analytical decomposition rather than enumeration. Another tradeoff involves kernel selection: while any kernel can be used, the choice significantly impacts both performance and interpretability.

Failure Signatures: Poor performance may indicate inappropriate kernel choice (too simple misses interactions, too complex overfits), numerical instability in kernel centering recursion, or violation of the assumed functional form. Components that are not truly orthogonal suggest implementation errors in the centering process.

First Experiments:
1. Verify orthogonality of components on synthetic data with known interaction structure
2. Compare STRIDE explanations against ground truth on simple additive models
3. Test component surgery on models where interaction importance is analytically known

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Performance heavily depends on kernel choice, which is not systematically explored
- Speedup claims may not generalize across all tree ensemble implementations
- Interaction removal effects demonstrated only on California Housing dataset

## Confidence

High: Theoretical derivation of orthogonal decomposition, analytic computation, and the core claim of subset-free functional decomposition.

Medium: Speedup and model reconstruction results, as these depend on implementation details and dataset characteristics.

Low: The generalizability of interaction removal ("component surgery") effects across diverse domains, since demonstrated only on California Housing.

## Next Checks

1. **Kernel Sensitivity Analysis**: Evaluate STRIDE's performance across a wider range of kernels (linear, RBF, learned) to quantify robustness.

2. **Cross-Dataset Generalization**: Apply STRIDE to additional tabular datasets with varying feature interactions to test reproducibility of speedup and R².

3. **Interaction Removal Replication**: Conduct component surgery experiments on multiple datasets to verify the functional necessity of identified interactions.