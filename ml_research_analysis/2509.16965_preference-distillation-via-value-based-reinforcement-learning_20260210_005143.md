---
ver: rpa2
title: Preference Distillation via Value based Reinforcement Learning
arxiv_id: '2509.16965'
source_url: https://arxiv.org/abs/2509.16965
tags:
- teacher
- reward
- value
- function
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We introduce Teacher Value-based Knowledge Distillation (TVKD),\
  \ a method that improves small language model alignment by leveraging the value\
  \ function of a DPO-trained teacher model. TVKD adds a shaping term based on the\
  \ teacher\u2019s soft value function to the DPO objective, preserving the optimal\
  \ policy through potential-based reward shaping."
---

# Preference Distillation via Value based Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.16965
- **Source URL:** https://arxiv.org/abs/2509.16965
- **Reference count:** 40
- **Primary result:** TVKD improves small language model alignment by adding teacher value function shaping to DPO, outperforming strong baselines on MT-Bench, AlpacaEval, and Open LLM Leaderboard.

## Executive Summary
TVKD introduces a novel preference distillation method that leverages the value function of a DPO-trained teacher model to provide fine-grained, trajectory-consistent supervision for small language model alignment. The method adds a shaping term based on the teacher's soft value function to the DPO objective, preserving the optimal policy through potential-based reward shaping. Experiments demonstrate TVKD's effectiveness across multiple benchmarks, improving response quality and alignment while remaining robust to model size and hyperparameter settings.

## Method Summary
TVKD modifies standard DPO by incorporating a shaping term derived from the teacher's soft value function. The value function is computed as V_ϕ(s) = β log Σ_a exp(Q_ϕ(s,a)/β), and the shaping term ψ(s,a) = V_ϕ(s') − V_ϕ(s) is added to the margin in the preference loss. This creates an auxiliary reward that provides finer-grained supervision without requiring additional rollouts. The method trains students using a modified DPO objective where the shaping term modulates the preference gradient based on teacher confidence in trajectory quality.

## Key Results
- TVKD achieves 8.56% improvement on MT-Bench compared to standard DPO baseline
- Improves AlpacaEval win rate from 62.8% to 65.1% (4.3% absolute gain)
- Demonstrates 25.82% accuracy in identifying preferred trajectories using shaping term versus 19.56% for raw log-probabilities
- Shows consistent improvements across multiple student model sizes (0.5B-3B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Potential-Based Reward Shaping Preserves Optimal Policy
Adding a shaping term derived from the teacher's value function provides finer-grained supervision without altering the optimal policy that DPO would otherwise converge to. The shaping term ψ(s,a) = V_ϕ(s') − V_ϕ(s) is constructed as a potential difference that telescopes over trajectories, ensuring argmax_a Q*(s,a) remains unchanged.

### Mechanism 2: Teacher Soft Value Function Captures Fine-Grained Preference Signal
The soft value function V_ϕ(s) = β log Σ_a exp(Q_ϕ(s,a)/β) of a DPO-trained teacher encodes richer information than binary win/loss labels, enabling token-level and trajectory-consistent supervision. Under MaxEnt RL interpretation, DPO-trained policies implicitly learn soft Q-functions that aggregate over all possible continuations.

### Mechanism 3: Gradient Blends Student Likelihood with Teacher Value Signal
The TVKD gradient modulates standard DPO updates using a margin M that incorporates both student log-probabilities and teacher-derived shaping. This creates adaptive weighting based on alignment confidence, where strong teacher preference signals reduce gradient magnitude to prevent overfitting to noisy signals.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed: TVKD is a modification of DPO objective; understanding baseline loss and binary supervision limits is essential
  - Quick check: Can you explain why DPO avoids explicit reward model training and what β parameter controls?

- **Maximum Entropy RL and Soft Value Functions**
  - Why needed: Paper interprets DPO policies through MaxEnt RL where soft value functions satisfy V*(s) = log Σ_a exp(Q*(s,a))
  - Quick check: What does soft value function represent and how does it differ from standard value function?

- **Potential-Based Reward Shaping**
  - Why needed: Theoretical guarantee that TVKD preserves optimal policy rests on Ng et al.'s PBRS result
  - Quick check: Why does shaping term of form Φ(s') − Φ(s) preserve optimal policy?

## Architecture Onboarding

- **Component map:** Teacher Model -> Value Function Extractor -> Shaping Term Calculator -> TVKD Loss Module -> Student Model
- **Critical path:**
  1. Precompute teacher logits for all responses in preference dataset
  2. Compute soft value functions at each token position
  3. Compute shaping terms as differences between consecutive value estimates
  4. Integrate shaping into modified DPO loss
  5. Backpropagate through student log-probabilities only

- **Design tradeoffs:**
  - Storage vs. computation: Precomputing logits saves training time but increases storage requirements
  - α (distillation strength): Higher α increases teacher influence but risks suppressing dataset-specific learning
  - β (temperature): Controls value function sharpness; paper finds robustness across β∈[0.1,5]
  - Teacher quality: Method assumes reasonably aligned teacher; untested with weak teachers

- **Failure signatures:**
  - Low margin accuracy (<20%): Shaping term not aligning with preferences
  - Performance degradation at high α: Teacher signal overwhelming dataset signal
  - Training instability: Numerical issues in exp(Q/β) computation
  - No improvement over DPO: Verify shaping terms are non-trivial

- **First 3 experiments:**
  1. Reproduce DPO baseline on DPOMIX-7K with LLaMA-3.2-1B; verify TVKD with α=0.7, β=2 improves MT-Bench by expected margin
  2. Sweep α∈{0.1, 0.5, 0.7, 1.0, 1.5} on held-out validation; plot RM score and MT-Bench
  3. On 500 samples, compare margin accuracy of shaping term vs. raw log-prob vs. length-normalized log-prob; target >25% accuracy

## Open Questions the Paper Calls Out

- **Question 1:** How robust is TVKD when teacher model has significantly weaker alignment or mismatched preferences compared to student's intended use case?
  - Basis: All experiments use well-aligned teachers, leaving failure modes with low-quality teachers unexplored
  - What evidence would resolve: Experiments with teachers of varying alignment quality measuring student performance degradation

- **Question 2:** What is the tolerable threshold of noise or estimation error in teacher's value function before TVKD harms rather than helps student learning?
  - Basis: Shaping term can negatively affect training dynamics if teacher's value function is severely misestimated
  - What evidence would resolve: Controlled experiments injecting calibrated noise into teacher value functions

- **Question 3:** Does TVKD's benefit diminish as student model capacity increases, or does value-based guidance remain valuable for larger models?
  - Basis: Paper focuses on SLMs motivated by their limited capacity, leaving applicability to larger models unstated
  - What evidence would resolve: Experiments scaling student models from 3B to 70B parameters comparing TVKD gains

## Limitations

- The connection between DPO's implicit soft Q-function and true optimal value function for original preference alignment task is not rigorously established
- Empirical validation lacks detailed ablation studies isolating specific contribution of shaping term versus other factors
- Reproducibility concern regarding exact implementation of loss function and top-k approximation impact on soft value computation

## Confidence

- **High confidence (9/10):** Core mathematical framework of potential-based reward shaping is well-established with sound formal proofs
- **Medium confidence (6/10):** Empirical results show consistent improvements but lack detailed ablations and modest predictive accuracy creates uncertainty
- **Low confidence (3/10):** Interpretation of DPO-trained teacher value functions as meaningful preference signals relies on theoretical assumptions not directly validated

## Next Checks

1. **Shaping Term Ablation with Different Teachers:** Train TVKD using same student model but with three different teacher qualities (well-trained DPO, randomly initialized, SFT-only) to isolate whether improvements come from shaping mechanism itself or teacher quality effects

2. **Value Function Approximation Analysis:** Systematically evaluate impact of using full vocabulary versus top-k approximations (k=10, 50, 100, full) on both soft value function computation and final performance

3. **Gradient Attribution Study:** Use gradient attribution techniques to measure relative contribution of student log-probabilities versus teacher shaping terms to parameter updates during training, comparing gradient norms and directions across different α values