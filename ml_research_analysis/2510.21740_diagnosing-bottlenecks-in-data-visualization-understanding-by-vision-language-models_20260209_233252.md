---
ver: rpa2
title: Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language
  Models
arxiv_id: '2510.21740'
source_url: https://arxiv.org/abs/2510.21740
tags:
- data
- point
- points
- tasks
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FUGU, a benchmark for evaluating vision-language
  models' ability to understand data visualizations. The authors identify that current
  VLMs struggle with extracting data point coordinates from scatter plots, which creates
  a bottleneck for downstream reasoning.
---

# Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.21740
- **Source URL:** https://arxiv.org/abs/2510.21740
- **Reference count:** 40
- **Primary result:** Vision-language models struggle with coordinate extraction from scatter plots, creating bottlenecks for data visualization understanding

## Executive Summary
This paper introduces FUGU, a benchmark for evaluating vision-language models' ability to understand data visualizations. The authors identify that current VLMs struggle with extracting data point coordinates from scatter plots, which creates a bottleneck for downstream reasoning tasks. Using activation patching and linear probes, they demonstrate that while visual encoders successfully represent coordinate information, language modules often fail to access it. The study reveals that providing ground-truth coordinates improves performance on simple tasks but not complex ones requiring statistical reasoning, and that fine-tuning on FUGU does not achieve ceiling performance, highlighting fundamental architectural limitations in current VLMs for data visualization understanding.

## Method Summary
The authors developed FUGU, a benchmark specifically designed to evaluate vision-language models' understanding of data visualizations. They employed activation patching techniques to identify where information bottlenecks occur in the VLM architecture, combined with linear probes to assess what information is encoded at different stages. The evaluation focused on scatter plots and involved both simple coordinate extraction tasks and more complex reasoning tasks. The methodology included providing ground-truth coordinates to test whether this information could be leveraged for improved performance, and conducting fine-tuning experiments to assess whether the benchmark could drive model improvements.

## Key Results
- Current VLMs show significant difficulty extracting data point coordinates from scatter plots
- Visual encoders successfully represent coordinate information, but language modules fail to access it
- Providing ground-truth coordinates improves simple task performance but not complex reasoning tasks
- Fine-tuning on FUGU does not achieve ceiling performance, indicating fundamental architectural limitations

## Why This Works (Mechanism)
The study reveals that the bottleneck in data visualization understanding occurs at the interface between visual and language modules in VLMs. While visual encoders can successfully encode coordinate information from scatter plots, this information becomes inaccessible to the language modules responsible for reasoning and generation. This architectural disconnect explains why simply providing ground-truth coordinates helps with basic tasks but fails to improve complex reasoning performance. The activation patching methodology effectively identifies where information flow breaks down, and the linear probes confirm that the visual representations contain the necessary information even when it cannot be utilized downstream.

## Foundational Learning

**Activation Patching**: A technique for identifying information flow in neural networks by replacing activations at specific layers and measuring performance changes. *Why needed:* To pinpoint where coordinate information becomes inaccessible. *Quick check:* Verify that patching visual encoder activations improves performance while patching language module activations does not.

**Linear Probes**: Simple classifiers trained on neural network activations to test what information is encoded. *Why needed:* To verify that visual encoders contain coordinate information even when downstream modules cannot access it. *Quick check:* Train linear classifiers on visual encoder outputs and measure coordinate prediction accuracy.

**Vision-Language Models (VLMs)**: Neural architectures that process both visual and textual inputs for multimodal understanding tasks. *Why needed:* Understanding the specific architectural constraints of VLMs is crucial for diagnosing visualization understanding bottlenecks. *Quick check:* Map the flow of information from visual encoder through fusion layers to language module.

## Architecture Onboarding

Component map: Visual Encoder -> Fusion Layer -> Language Module -> Output Generator

Critical path: Visual Encoder extracts features → Fusion Layer combines with text → Language Module performs reasoning → Output Generator produces responses

Design tradeoffs: The study highlights the tension between specialized visual feature extraction and general-purpose language reasoning, where architectural decisions that optimize one component may impair the other's effectiveness.

Failure signatures: The primary failure signature is successful coordinate representation in visual encoders coupled with inability of language modules to access this information, particularly evident in complex reasoning tasks where simple coordinate provision does not improve performance.

First experiments:
1. Test coordinate extraction on diverse visualization types beyond scatter plots
2. Evaluate different visual encoder architectures to determine if bottlenecks are architecture-specific
3. Assess whether alternative training objectives or architectures can better bridge visual-language interface

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses primarily on scatter plots, limiting generalizability to other visualization types
- Evaluation methodology relies on specific prompting strategies that may not reflect real-world usage
- Fine-tuning methodology and hyperparameters are not fully specified, affecting confidence in conclusions about architectural limitations

## Confidence
- Coordinate extraction as primary bottleneck: Medium confidence
- Visual encoders successfully represent coordinate information: High confidence
- Language modules fail to access visual information: Medium confidence
- Fine-tuning limitations indicate fundamental architectural constraints: Medium confidence

## Next Checks
1. Test the same methodology on diverse visualization types beyond scatter plots to assess generalizability
2. Conduct ablation studies with different visual encoder architectures to determine if coordinate extraction bottlenecks are architecture-specific
3. Evaluate whether alternative model architectures or training objectives could better bridge the gap between visual representation and language module access to coordinate information