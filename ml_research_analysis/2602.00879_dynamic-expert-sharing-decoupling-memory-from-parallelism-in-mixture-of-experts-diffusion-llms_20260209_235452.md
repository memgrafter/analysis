---
ver: rpa2
title: 'Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts
  Diffusion LLMs'
arxiv_id: '2602.00879'
source_url: https://arxiv.org/abs/2602.00879
tags:
- expert
- experts
- arxiv
- parallel
- coreset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamic Expert Sharing (DES) tackles the "expert explosion" bottleneck
  in Mixture-of-Experts diffusion large language models, where parallel decoding leads
  to linearly growing unique expert activations and memory traffic. DES introduces
  sequence-level coreset selection to minimize expert reuse and memory overhead, replacing
  token-centric pruning with a global optimization strategy.
---

# Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs

## Quick Facts
- arXiv ID: 2602.00879
- Source URL: https://arxiv.org/abs/2602.00879
- Authors: Hao Mark Chen; Zhiwen Mo; Royson Lee; Qianzhou Wang; Da Li; Shell Xu Hu; Wayne Luk; Timothy Hospedales; Hongxiang Fan
- Reference count: 10
- One-line primary result: DES reduces unique expert activations by over 55% and latency by up to 38% while retaining 99% of baseline accuracy.

## Executive Summary
Dynamic Expert Sharing (DES) addresses the "expert explosion" bottleneck in Mixture-of-Experts diffusion large language models, where parallel decoding leads to linearly growing unique expert activations and memory traffic. DES introduces sequence-level coreset selection to minimize expert reuse and memory overhead, replacing token-centric pruning with a global optimization strategy. Two approaches are proposed: Intra-Sequence Sharing (DES-Seq), which unions top-k expert selections across tokens, and Saliency-Aware Voting (DES-Vote), which aggregates router weights to elect a high-utility expert coreset. Experiments on LLaDA-MoE models show DES reduces unique expert activations by over 55% and latency by up to 38% while retaining 99% of baseline accuracy.

## Method Summary
DES tackles MoE dLLM memory bottlenecks by constraining expert selection to a shared coreset rather than allowing each token to independently select from the full expert pool. DES-Seq takes the union of top-k selections across all tokens in a parallel block, while DES-Vote aggregates masked router weights to elect a coreset of high-saliency experts. Both methods then perform constrained top-k routing within the coreset. DES-Vote consistently outperforms DES-Seq by preserving routing fidelity through saliency-weighted aggregation. The approach effectively decouples memory overhead from parallelism degree, enabling higher throughput without accuracy loss.

## Key Results
- DES reduces unique expert activations by over 55% and latency by up to 38%
- DES-Vote achieves 99%+ relative accuracy across all benchmarks
- DES-Seq with k=2 vs. DES-Vote shows 0.6-2.3% relative accuracy improvement favoring DES-Vote
- Expert hit distribution remains similar to vanilla routing (cosine similarity ≥0.98)

## Why This Works (Mechanism)

### Mechanism 1: Memory Traffic Decoupling via Coreset Selection
The latency model $L_{MoE} \leq b \cdot |\Phi(I)| + a \cdot (N \cdot k)$ shows unique expert count $|\Phi(I)|$ dominates weight-fetching cost $b$. By replacing the implicit union bound with an explicit coreset variable, DES directly targets memory traffic rather than per-token compute. This works when weight-fetching cost $b$ significantly exceeds marginal computation cost $a$ in memory-bound regimes.

### Mechanism 2: Saliency-Aware Voting Preserves Routing Fidelity
DES-Vote computes $V_i = \sum_{n=1}^N \text{Masked}(I_{n,i})$ where only top-K weights per token contribute. This preserves relative importance signal from raw gating weights, which correlate with actual expert utility. Router weights are predictive of expert importance when tokens in a parallel block share semantic coherence.

### Mechanism 3: Regularization Effect from Constrained Routing
Restricting the expert pool acts as implicit regularization, forcing the model to rely on higher-saliency experts rather than distributing computation across marginally useful ones. This works when vanilla top-K routing includes experts with low marginal contribution that can degrade output quality.

## Foundational Learning

- **Concept: Mixture-of-Experts Routing**
  - Why needed here: DES modifies standard Top-K routing; understanding baseline behavior is prerequisite
  - Quick check question: Can you explain why MoE latency depends on unique expert count rather than total expert activations?

- **Concept: Memory-Bound vs Compute-Bound Regimes**
  - Why needed here: The paper's core thesis rests on MoE dLLMs operating in memory-bound regime where bandwidth, not FLOPs, limits throughput
  - Quick check question: Given a roofline model, how would you determine if increasing block size moves you toward or away from the memory-bound region?

- **Concept: Diffusion LLM Parallel Decoding**
  - Why needed here: DES exploits semantic coupling in parallel tokens—understanding why dLLMs process blocks concurrently is essential
  - Quick check question: How does parallel decoding in dLLMs differ from speculative decoding with verification?

## Architecture Onboarding

- **Component map:** Input Block (N tokens) → Router → [Coreset Selection: DES-Seq or DES-Vote] → Shared Coreset C → Constrained Top-K per token → Expert FFN computation

- **Critical path:** The coreset selection function $\Phi(I)$ in Algorithm 1, Step 2 determines all downstream memory traffic. The custom fused kernel (Section 4.3) is performance-critical—12 separate kernels collapsed to 2 via register-level accumulation.

- **Design tradeoffs:**
  - DES-Seq vs DES-Vote: DES-Seq is simpler (union of top-k) but ignores saliency magnitude; DES-Vote requires aggregation but captures importance weighting. DES-Vote consistently outperforms (Table 1 shows 0.6-2.3% relative accuracy improvement).
  - Coreset size (β) vs accuracy: Smaller β reduces latency but risks over-pruning. Figure 7a shows gradual accuracy decline; β=0.15 provides good tradeoff for LLaDA2.0-Mini.

- **Failure signatures:**
  - Accuracy drops >2% relative: coreset size too aggressive (increase β or k)
  - No latency improvement: check if actually memory-bound (profile HBM utilization)
  - Expert hit distribution too uniform: voting may not be differentiating saliency (verify router weight variance)

- **First 3 experiments:**
  1. Baseline profiling: Measure vanilla MoE kernel latency vs. block size (8, 16, 32, 64) to confirm memory-bound behavior (replicate Figure 2)
  2. Coreset size sweep: Run DES-Vote with β ∈ {0.1, 0.15, 0.2, 0.3, 0.6} on a single benchmark (e.g., MBPP) to establish accuracy-latency Pareto curve
  3. Ablation on voting mechanism: Compare DES-Vote vs. uniform voting (without saliency weighting) to quantify the contribution of router-weighted aggregation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the regularization effect from coreset selection explain cases where DES achieves accuracy gains over the vanilla baseline, and can this effect be systematically exploited?
- Basis in paper: The authors note: "In certain instances, the accuracy gain is positive... This phenomenon could potentially be attributed to a regularization effect, where the coreset selection process prunes away lower-utility experts that may otherwise introduce noise."
- Why unresolved: The regularization hypothesis is proposed but not empirically validated; the mechanism by which constraining expert selection could improve accuracy remains uninvestigated.
- What evidence would resolve it: Ablation studies correlating expert noise metrics with accuracy gains, or comparing DES against explicit regularization techniques.

### Open Question 2
- Question: How does DES perform across diverse MoE router architectures (e.g., hash-based, soft routing, expert choice) beyond the Top-K routing used in LLaDA models?
- Basis in paper: Experiments are limited to two LLaDA-MoE variants with similar routing mechanisms; no evaluation on alternative router designs that may exhibit different expert activation patterns.
- Why unresolved: The coreset selection strategy assumes Top-K routing with soft gating; other router types may have fundamentally different expert-token affinity structures.
- What evidence would resolve it: Benchmarking DES on MoE dLLMs with varied routing mechanisms and analyzing coreset quality across architectures.

### Open Question 3
- Question: Can coreset size (Mcore) be adaptively determined per-sequence or per-layer rather than using a fixed budget factor β?
- Basis in paper: The paper uses a static β parameter, but Figure 4 shows expert importance varies significantly across token positions and layers, suggesting dynamic budgets could improve efficiency-accuracy trade-offs.
- Why unresolved: No adaptive mechanism is proposed; the fixed budget may over-constrain simple sequences while under-constraining complex ones.
- What evidence would resolve it: Comparing fixed vs. entropy-based or gradient-based adaptive coreset sizing across benchmarks with varying complexity.

### Open Question 4
- Question: Does incorporating DES principles during MoE training improve expert specialization for parallel decoding, beyond inference-only application?
- Basis in paper: The conclusion motivates "further research into sequence-level expert sharing for parallel decoding models" without specifying training-time integration.
- Why unresolved: DES is applied post-hoc; training with sequence-level routing constraints could produce experts naturally suited for coreset sharing.
- What evidence would resolve it: Training MoE dLLMs with coreset-aware routing objectives and comparing against inference-only DES baselines.

## Limitations

- **Evaluation Scope**: All results focus on LLaDA-MoE models; generalization to other MoE architectures (e.g., GPT-4-style, DeepSeek) remains untested.
- **Training Dependency**: DES-Vote assumes routers produce informative weights that correlate with downstream utility. This may not hold for models trained with auxiliary routing losses or under domain shift.
- **Parallelism Assumptions**: The analysis assumes dense KV caching for all N tokens in the block. If KV cache utilization is sparse or if model-level parallelism is involved, the coreset selection dynamics could change.

## Confidence

- **High Confidence**: The core mechanism of memory traffic reduction via coreset selection is well-supported by the latency model and ablation results. The 55%+ reduction in unique expert activations is clearly demonstrated and replicable.
- **Medium Confidence**: The claim that DES-Vote consistently outperforms DES-Seq is well-evidenced in this paper's experiments, but relies on the specific saliency correlation observed in LLaDA-MoE routers. Broader validation across models is needed.
- **Medium Confidence**: The regularization hypothesis (accuracy gains from constrained routing) is intriguing but weakly supported—only anecdotal evidence and no ablation on model capacity or routing noise. Further controlled experiments are needed.

## Next Checks

1. **Router Weight Saliency Correlation**: Systematically measure the correlation between router weights and downstream expert utility across layers and models. This validates the core assumption behind DES-Vote and identifies conditions where saliency voting may fail.

2. **Memory-Bound Regime Verification**: Profile HBM bandwidth utilization for vanilla MoE with varying block sizes on target hardware. Confirm that weight-fetching dominates compute, justifying DES's focus on coreset selection rather than compute optimization.

3. **Generalization to Other MoE Architectures**: Apply DES to a non-LLaDA MoE model (e.g., a standard 1.1B MoE from Hugging Face) and measure accuracy-latency tradeoffs. This tests the portability of the coreset selection strategy beyond the LLaDA family.