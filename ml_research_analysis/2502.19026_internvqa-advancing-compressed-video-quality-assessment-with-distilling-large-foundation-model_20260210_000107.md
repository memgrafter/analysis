---
ver: rpa2
title: 'InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large
  Foundation Model'
arxiv_id: '2502.19026'
source_url: https://arxiv.org/abs/2502.19026
tags:
- video
- quality
- assessment
- distillation
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses compressed video quality assessment (VQA)
  by leveraging the rich feature representations of large video foundation models.
  The authors propose InternVQA, a knowledge distillation framework that transfers
  the powerful perceptual capabilities of InternVideo2-1B (the teacher) to a lightweight
  student model.
---

# InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large Foundation Model

## Quick Facts
- **arXiv ID:** 2502.19026
- **Source URL:** https://arxiv.org/abs/2502.19026
- **Authors:** Fengbin Guan; Zihao Yu; Yiting Lu; Xin Li; Zhibo Chen
- **Reference count:** 10
- **Primary result:** Distilled video quality assessment model achieves comparable performance to large foundation model while being significantly more efficient

## Executive Summary
This paper introduces InternVQA, a knowledge distillation framework that transfers perceptual capabilities from large video foundation models to lightweight compressed video quality assessment (VQA) models. The approach leverages InternVideo2-1B as a teacher model to train smaller student models, including both homologous (InternVideo2-Small/Base) and heterogeneous (FastVQA) architectures. Through homologous distillation, the student models not only outperform existing VQA methods but also match or exceed the teacher's performance on BVI-HD and Waterloo IVC 4K datasets while requiring substantially less computational resources.

## Method Summary
The InternVQA framework employs knowledge distillation to transfer perceptual understanding from large video foundation models to efficient VQA architectures. The teacher model, InternVideo2-1B, provides rich feature representations that guide the training of smaller student models. The framework explores two distillation strategies: homologous distillation using smaller InternVideo2 variants (Small and Base) that share architectural similarities with the teacher, and heterogeneous distillation using FastVQA, a specialized VQA architecture. The distillation process uses feature matching and output matching losses to align the student's predictions with the teacher's assessments across multiple layers, enabling the transfer of perceptual quality understanding while maintaining computational efficiency.

## Key Results
- Student models trained through homologous distillation surpass existing VQA methods on BVI-HD and Waterloo IVC 4K datasets
- Distilled models achieve comparable or superior performance to the 1B-parameter InternVideo2-1B teacher
- Significant reduction in model size and computational requirements while maintaining quality assessment accuracy
- Mixed results in heterogeneous distillation with FastVQA suggest architectural alignment matters for effective knowledge transfer

## Why This Works (Mechanism)
The success of InternVQA stems from the rich perceptual representations learned by large video foundation models like InternVideo2-1B, which capture complex visual quality attributes that are difficult to learn from scratch. Knowledge distillation effectively transfers these learned representations to smaller models by aligning their feature spaces and output distributions with the teacher. The homologous distillation strategy is particularly effective because the student and teacher share architectural similarities, enabling smoother feature alignment and more efficient transfer of perceptual understanding. This approach allows the student models to inherit the teacher's ability to recognize subtle quality degradations while operating at a fraction of the computational cost.

## Foundational Learning

**Video quality assessment (VQA)**: Understanding how to evaluate perceived video quality, which requires modeling complex spatio-temporal distortions - needed because compressed video introduces artifacts that affect viewing experience differently than images.

**Knowledge distillation**: Transfer learning technique where a smaller model (student) learns from a larger model (teacher) - needed to leverage powerful foundation models without their computational burden.

**Foundation models for video**: Large-scale pre-trained models like InternVideo2 that capture general video understanding - needed because they provide rich perceptual representations that can be adapted for quality assessment.

**Homologous vs. heterogeneous distillation**: Different approaches based on architectural similarity between teacher and student - needed to understand when distillation is most effective and what architectural constraints exist.

**Perceptual quality metrics**: Objective measures that correlate with human judgment of video quality - needed to evaluate VQA models and validate that they capture meaningful quality attributes.

## Architecture Onboarding

**Component map:** InternVideo2-1B (teacher) -> Distillation module -> InternVideo2-Small/Base (student) or FastVQA (student)

**Critical path:** Input video -> Teacher feature extraction -> Feature matching loss -> Student feature extraction -> Output matching loss -> Student prediction

**Design tradeoffs:** The framework trades some architectural flexibility (limited to models that can align with teacher features) for significant efficiency gains and performance improvements through knowledge transfer.

**Failure signatures:** Poor performance in heterogeneous distillation suggests that architectural misalignment between teacher and student can limit knowledge transfer effectiveness.

**First experiments to run:**
1. Evaluate distilled models on additional VQA datasets with different compression types
2. Compare computational efficiency metrics against established lightweight VQA baselines
3. Conduct ablation studies on distillation temperature and loss weighting parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two datasets (BVI-HD and Waterloo IVC 4K), potentially limiting generalizability
- Computational efficiency claims lack comparison with dedicated lightweight VQA baselines
- Homologous distillation assumes representational alignment that is not empirically validated
- Heterogeneous distillation shows mixed results, suggesting limitations in cross-architecture transfer

## Confidence

**High confidence:** The core methodology of using knowledge distillation for compressed VQA is sound and well-implemented

**Medium confidence:** The performance improvements over existing methods are valid but may not generalize beyond tested datasets

**Medium confidence:** The computational efficiency claims are supported but lack comprehensive benchmarking against established efficient VQA approaches

## Next Checks

1. Evaluate the distilled models on additional VQA datasets with different compression types and quality ranges to assess generalization
2. Compare computational efficiency metrics (latency, memory usage) against dedicated lightweight VQA models rather than only against the 1B-parameter teacher
3. Conduct ablation studies on the distillation temperature and loss weighting parameters to determine optimal configurations and robustness