---
ver: rpa2
title: Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many
  Relationships
arxiv_id: '2405.18770'
source_url: https://arxiv.org/abs/2405.18770
tags:
- image
- text
- multimodal
- augmentations
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the vulnerability of vision-language models
  to multimodal adversarial attacks, where both images and texts can be perturbed
  to mislead retrieval tasks. Prior defense methods focus on unimodal (image-only)
  attacks, overlooking the one-to-many relationship between images and texts, where
  multiple valid descriptions exist for each image and vice versa.
---

# Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships

## Quick Facts
- arXiv ID: 2405.18770
- Source URL: https://arxiv.org/abs/2405.18770
- Authors: Futa Waseda; Antonio Tejero-de-Pablos; Isao Echizen
- Reference count: 40
- Primary result: MAT+ improves multimodal adversarial robustness by 5-10% over MAT baseline across ITR, VG, and IC tasks

## Executive Summary
This work addresses the vulnerability of vision-language models (VLMs) to multimodal adversarial attacks, where both images and texts can be perturbed to mislead retrieval tasks. Prior defense methods focus on unimodal (image-only) attacks, overlooking the one-to-many relationship between images and texts, where multiple valid descriptions exist for each image and vice versa. To address this, the authors propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly improving robustness against multimodal attacks compared to unimodal defenses. They further enhance robustness by leveraging one-to-many relationships through augmentations, generating diverse yet well-aligned image-text pairs. Analysis reveals that cross-modal augmentations (e.g., image→text) outperform intra-modal ones, and text augmentations are more effective than image augmentations due to distribution shift challenges in high-dimensional image space. MAT and MAT+ consistently improve performance across image-text retrieval, visual grounding, and image captioning tasks.

## Method Summary
The paper proposes Multimodal Adversarial Training (MAT) that defends VLMs against multimodal adversarial attacks by perturbing both image and text during training. The method uses sequential perturbation: text is first attacked using BERT-Attack or EDA, then the resulting adversarial text guides image perturbation via 2-step PGD. Training minimizes contrastive loss (InfoNCE) on these multimodal adversarial pairs. MAT+ extends this by incorporating one-to-many relationships through data augmentation, generating diverse yet aligned image-text pairs using models like InternVL and Stable Diffusion. The approach is evaluated on Flickr30k, COCO, and RefCOCO+ across three tasks: image-text retrieval, visual grounding, and image captioning, using three VLM architectures (CLIP, ALBEF, BLIP).

## Key Results
- MAT improves multimodal adversarial robustness by 5-10% over MAT baseline across ITR, VG, and IC tasks
- Cross-modal augmentations (I2T, T-I2I) consistently outperform intra-modal ones (EDA, RandAug) for robustness
- Text augmentations are more effective than image augmentations due to distribution shift challenges in high-dimensional image space
- MAT+ achieves the best trade-off between clean accuracy and robustness across all three tasks and datasets

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Adversarial Training (MAT)
Incorporating adversarial perturbations in both image and text modalities during training is necessary to defend against multimodal attacks, as unimodal defenses leave the text encoder vulnerable. MAT sequentially perturbs text (using BERT-Attack or EDA) and then images (using PGD), training on these multimodal adversarial pairs. The joint embedding space's robustness depends on worst-case perturbations across both modalities simultaneously. Evidence shows MAT significantly outperforms existing unimodal defenses, though computational budget and single-modality tasks may not justify the added complexity.

### Mechanism 2: Distribution Approximation via One-to-Many Augmentation
Using one-to-many relationships via data augmentation improves robustness by better approximating the true multimodal data distribution, which is poorly represented by deterministic 1:1 training pairs. Effective augmentations must satisfy three conditions: high alignment, high diversity, and minimal distribution shift. While this theory is novel to this paper, empirical results show diverse captions improve robustness, though semantic mismatch or excessive distribution shift can degrade performance.

### Mechanism 3: Cross-Modal Augmentation Superiority
Cross-modal augmentations (generating text from image or image from text) outperform intra-modal ones (editing text or image independently) for adversarial defense. Intra-modal augmentations may disrupt fine-grained alignment with the other modality, while cross-modal generation ensures semantic alignment while introducing diversity. The generative models used must be robust enough to produce high-quality, aligned data without introducing excessive noise. Cross-modal methods achieve better trade-offs between alignment and diversity compared to intra-modal methods.

## Foundational Learning

- **Concept: Min-Max Optimization (Adversarial Training)**
  - **Why needed here:** MAT is formulated as a min-max problem where the goal is to minimize loss against the worst-case (adversarial) perturbations. Understanding this helps grasp why the "inner maximization" (generating attacks) is a crucial step before the "outer minimization" (training the model).
  - **Quick check question:** In the context of MAT, does the model learn to defend against average noise or worst-case perturbations designed to maximize the loss?

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here:** The defense is built upon models like CLIP and ALBEF, which learn by maximizing similarity between positive image-text pairs and minimizing it for negatives. Adversarial attacks target this similarity, and MAT attempts to restore it for perturbed pairs.
  - **Quick check question:** If an adversarial attack successfully reduces the cosine similarity between an image and its text to near zero, how would that affect the InfoNCE loss?

- **Concept: One-to-Many (1:N) Cross-Modal Alignment**
  - **Why needed here:** Standard classification assumes 1:1 (image to label). VL tasks are inherently ambiguous (one image, many valid captions). This paper exploits this property, so understanding that "ambiguity" is a feature, not a bug, is essential.
  - **Quick check question:** Why might training a model on a single caption per image make it "brittle" or less robust to paraphrasing attacks?

## Architecture Onboarding

- **Component map:** Image $I$ + Text $T$ → Augmentation Layer (MAT+) → Adversarial Generator (Text→BERT-Attack/EDA, then Image→PGD) → VL Encoder (ViT + Transformer) → Contrastive Loss (InfoNCE)

- **Critical path:**
  1. Start with standard 1:1 pairs
  2. Generate 1:N pairs using InternVL (Image-to-Text) or Stable Diffusion (Text-guided Image-to-Image) - precomputed
  3. Generate adversarial text $T'$ first (cheaper/simpler), then use $T'$ to guide generation of adversarial image $I'$
  4. Feed $(I', T')$ into model and backpropagate contrastive loss

- **Design tradeoffs:**
  - PGD-2 is efficient and sufficient for multimodal robustness; PGD-10 is stronger but doubles training time
  - Text augmentations are easier to control (less distribution shift) than image augmentations
  - T-I2I (image editing) is safer than T2I (image generation) which risks hallucination/distribution shift
  - Text → Image perturbation order is slightly better or comparable to Image → Text

- **Failure signatures:**
  - High robustness against image-only attacks but near-zero accuracy against SGA indicates unimodal AT instead of MAT
  - Clean accuracy drops significantly with failed robustness improvement suggests problematic augmentations
  - Low alignment scores in augmentation leading to poor robustness transfer indicates semantic mismatch

- **First 3 experiments:**
  1. Train CLIP on Flickr30k using standard Fine-tuning vs. MAT (2-step PGD); verify standard fine-tuning collapses under SGA while MAT retains >30% R@1
  2. Compare MAT vs. MAT+ (with Human Captions vs. InternVL); confirm diverse captions improve robustness
  3. Train separate instances using EDA (Intra) vs. I2T (Cross); verify Cross-modal achieves higher alignment and robustness

## Open Questions the Paper Calls Out

### Open Question 1
How can image augmentation techniques be designed to simultaneously maximize diversity and minimize distribution shift, given the high dimensionality of the image space? The paper states developing image augmentations with high diversity yet minimal distribution gap is a promising direction because high dimensionality makes avoiding distribution shift difficult compared to text. While text augmentations successfully balance alignment and diversity, image augmentations struggle; they either lack diversity (RandAug) or introduce large distribution gap (Stable Diffusion), limiting robustness gains.

### Open Question 2
What specific training methodologies are required to effectively leverage many-to-many (N:N) augmentations for multimodal robustness? The authors note many-to-many augmentations require dedicated methodology outside this work's scope. They found combining image and text augmentations (N:N) didn't improve performance over text-only augmentations, hypothesizing that because original pairs constitute only 12.5% of data in this setup, N:N is prone to distorting data distribution without careful design.

### Open Question 3
Can the robustness gains achieved by stronger perturbation steps (e.g., PGD-10) be realized without the associated computational cost and clean accuracy degradation? Section 6.1.1 shows increasing PGD steps from 2 to 10 improves multimodal robustness but significantly increases time cost (from 8.79s to 17.22s) and often reduces clean accuracy. The paper optimizes for efficiency by adopting PGD-2, explicitly trading off maximum potential robustness for speed.

## Limitations

- Computational cost of MAT+ with extensive augmentations may limit scalability to larger datasets
- Reliance on generative models (Stable Diffusion, InternVL) introduces potential distribution shift risks not fully characterized across all domains
- Defense's effectiveness against adaptive attackers who specifically target the augmentation strategy remains unclear

## Confidence

**High Confidence**: Claims about MAT significantly outperforming unimodal defenses against multimodal attacks (SGA). Strong empirical support across three datasets (Flickr30k, COCO, RefCOCO+) and three tasks (ITR, VG, IC) with multiple VLM architectures (CLIP, ALBEF, BLIP).

**Medium Confidence**: Claims about cross-modal augmentations (I2T, T-I2I) consistently outperforming intra-modal ones. Clear patterns shown in Fig. 2 and Sec 6.1.2, but underlying reasons could benefit from more rigorous statistical analysis.

**Medium Confidence**: Claims about text augmentations being more effective than image augmentations due to distribution shift in high-dimensional image space. Theoretical justification aligns with known challenges, but specific empirical comparisons quantifying distribution shift contributions would strengthen this claim.

## Next Checks

1. **Adaptive Attack Evaluation**: Test whether attackers can specifically craft perturbations that exploit the one-to-many augmentation strategy (e.g., by targeting augmented samples or poisoning the augmentation process). This would validate whether MAT+ provides robustness against adaptive rather than just static attacks.

2. **Distribution Shift Quantification**: Systematically measure and compare the distribution shift introduced by different augmentation methods (I2T, T-I2I, EDA, RandAug) using metrics like Frechet Distance or KL divergence in the embedding space. This would empirically validate the claim that image augmentations suffer more from distribution shift than text augmentations.

3. **Computational Cost Analysis**: Measure and compare the training time, memory usage, and inference latency of MAT vs MAT+ vs baseline models across different dataset scales. This would provide concrete evidence for scalability claims and help identify practical deployment constraints.