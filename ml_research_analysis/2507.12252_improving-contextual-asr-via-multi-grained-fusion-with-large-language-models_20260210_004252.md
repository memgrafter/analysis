---
ver: rpa2
title: Improving Contextual ASR via Multi-grained Fusion with Large Language Models
arxiv_id: '2507.12252'
source_url: https://arxiv.org/abs/2507.12252
tags:
- keyword
- fusion
- speech
- phrase-level
- token-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving contextual automatic
  speech recognition (ASR), particularly for accurately transcribing contextually
  relevant keywords such as proper nouns and user-specific entities. The authors propose
  a multi-grained fusion approach that leverages both token-level and phrase-level
  fusion with large language models (LLMs).
---

# Improving Contextual ASR via Multi-grained Fusion with Large Language Models

## Quick Facts
- arXiv ID: 2507.12252
- Source URL: https://arxiv.org/abs/2507.12252
- Authors: Shilin Zhou; Zhenghua Li
- Reference count: 14
- Primary result: Multi-grained fusion of ASR tokens and LLM contextual information achieves state-of-the-art keyword recognition accuracy while preserving non-keyword performance.

## Executive Summary
This paper introduces a novel multi-grained fusion approach to enhance contextual automatic speech recognition (ASR) by integrating large language models (LLMs) at both token and phrase levels. The method addresses the challenge of accurately transcribing contextually relevant keywords, such as proper nouns and user-specific entities, which are often missed by standard ASR systems. By leveraging both fine-grained token precision and holistic phrase-level understanding, the approach achieves superior keyword recognition performance without compromising accuracy on non-keyword text. Experimental results on Chinese and English datasets demonstrate significant improvements over baseline systems and standard biasing techniques.

## Method Summary
The proposed method employs a dual-level fusion strategy that combines ASR outputs with LLM-generated contextual information. At the token level, individual words are enriched with contextual embeddings from the LLM to improve recognition accuracy for specific entities. At the phrase level, longer sequences are processed to capture broader contextual relationships and improve coherence. The fusion mechanism dynamically weights the contributions of ASR and LLM information based on confidence scores, allowing the system to adapt to varying levels of contextual relevance. This multi-grained approach balances the precision of token-level processing with the semantic understanding provided by phrase-level fusion, resulting in improved transcription accuracy for both keywords and general text.

## Key Results
- Achieves state-of-the-art performance on keyword-related metrics for both Chinese and English datasets
- Maintains high accuracy on non-keyword text while improving keyword recognition
- Ablation studies confirm that both token-level and phrase-level fusion components significantly contribute to performance gains

## Why This Works (Mechanism)
The multi-grained fusion approach works by leveraging the complementary strengths of ASR systems and LLMs. ASR systems excel at converting speech to text but often struggle with contextually relevant keywords, especially proper nouns and user-specific entities. LLMs, on the other hand, possess strong contextual understanding but lack direct access to the audio signal. By fusing information at both token and phrase levels, the method captures fine-grained details while maintaining holistic context. The token-level fusion provides precise corrections for individual words, while phrase-level fusion ensures semantic coherence and handles longer contextual dependencies. This dual approach allows the system to effectively balance accuracy and contextual relevance.

## Foundational Learning

### Contextual ASR Enhancement
- **Why needed**: Standard ASR systems often miss contextually important keywords, especially proper nouns and user-specific entities
- **Quick check**: Evaluate keyword recognition accuracy on datasets with high proper noun density

### Token-level Fusion
- **Why needed**: Provides fine-grained corrections for individual words based on contextual embeddings
- **Quick check**: Measure improvement in single-word entity recognition accuracy

### Phrase-level Fusion
- **Why needed**: Captures broader semantic relationships and ensures contextual coherence
- **Quick check**: Assess improvement in phrase-level entity recognition and contextual understanding

### Dynamic Confidence Weighting
- **Why needed**: Allows the system to adapt fusion based on the reliability of ASR and LLM outputs
- **Quick check**: Analyze confidence score distributions and their impact on fusion decisions

## Architecture Onboarding

### Component Map
ASR Output -> Token-level Fusion -> Phrase-level Fusion -> Final Output

### Critical Path
Speech input → ASR transcription → LLM context retrieval → Token-level fusion → Phrase-level fusion → Final transcription output

### Design Tradeoffs
The approach balances computational overhead (from LLM API calls) against accuracy gains. Token-level fusion provides immediate corrections but may miss broader context, while phrase-level fusion captures semantics but requires more processing. The dynamic confidence weighting mechanism helps optimize this tradeoff by adjusting fusion intensity based on input reliability.

### Failure Signatures
- Over-reliance on LLM context when ASR confidence is high can introduce errors
- Phrase-level fusion may struggle with very long or complex contextual dependencies
- API latency from external LLM services can impact real-time performance

### First 3 Experiments
1. Baseline comparison: Evaluate keyword recognition accuracy without any fusion
2. Token-only fusion: Test performance using only token-level fusion component
3. Phrase-only fusion: Test performance using only phrase-level fusion component

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to Chinese and English datasets, raising questions about cross-linguistic generalization
- Reliance on external LLM APIs introduces potential variability and computational overhead
- Lack of detailed analysis on real-time performance and scalability in production environments

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| Core technical contribution | High |
| State-of-the-art performance | Medium |
| Scalability and robustness | Low |

## Next Checks
1. **Cross-linguistic generalization test**: Evaluate the multi-grained fusion approach on morphologically rich languages (e.g., Finnish, Turkish) or languages with non-Latin scripts to assess whether the token-level and phrase-level fusion mechanisms generalize beyond Chinese and English.

2. **Ablation of fusion components**: Conduct a more granular ablation study that isolates the contribution of token-level fusion, phrase-level fusion, and their interaction by testing each component independently across different contextual domains (proper nouns, technical terms, user-specific entities).

3. **Real-time performance analysis**: Measure the computational overhead, memory usage, and end-to-end latency of the multi-grained fusion approach when integrated with a production ASR system, including analysis of how API response times from external LLMs affect overall system performance.