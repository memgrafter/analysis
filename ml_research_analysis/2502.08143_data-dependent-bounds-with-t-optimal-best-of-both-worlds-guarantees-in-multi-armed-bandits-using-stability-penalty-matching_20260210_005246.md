---
ver: rpa2
title: Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed
  Bandits using Stability-Penalty Matching
arxiv_id: '2502.08143'
source_url: https://arxiv.org/abs/2502.08143
tags:
- have
- lemma
- bounds
- inequality
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops new adaptive regret bounds for multi-armed
  bandits that simultaneously satisfy three key properties: being data-dependent (adaptive
  to problem structure like sparsity or total variation), best-of-both-worlds (BOBW
  - achieving optimal bounds for both stochastic and adversarial regimes), and having
  optimal $T$-dependency. The core method is real-time stability-penalty matching
  (SPM), an extension of the SPM technique for tuning learning rates in the follow-the-regularized-leader
  (FTRL) framework.'
---

# Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed Bandits using Stability-Penalty Matching

## Quick Facts
- arXiv ID: 2502.08143
- Source URL: https://arxiv.org/abs/2502.08143
- Reference count: 40
- Primary result: First algorithm achieving BOBW, data-dependent, and $T$-optimal regret bounds simultaneously in multi-armed bandits.

## Executive Summary
This paper develops a novel algorithm for multi-armed bandits that achieves three desirable properties simultaneously: data-dependent adaptivity (to sparsity or total variation), best-of-both-worlds guarantees (optimal for both stochastic and adversarial regimes), and optimal $T$-dependency. The core innovation is "real-time stability-penalty matching" (SPM), which dynamically tunes learning rates based on observed losses rather than expectations. This enables tighter adaptivity to problem structure compared to previous SPM methods.

## Method Summary
The method combines Follow-the-Regularized-Leader (FTRL) with a hybrid regularizer (Tsallis entropy plus log-barrier) and real-time stability-penalty matching. The learning rate is updated each round based on the observed loss and sampling probability, allowing the algorithm to adapt to data-dependent quantities like sparsity $S$ or total variation $Q$. A key innovation is using the realized loss in the stability term rather than an expectation, enabling adaptation to structural properties. The algorithm also employs coordinate-wise learning rates for further adaptivity.

## Key Results
- Achieves $O(\sqrt{ST \ln K})$ adversarial regret and $O(S \ln T \ln K/\Delta_{\min})$ stochastic regret for sparse losses
- Obtains $O(\sqrt{Q \ln K})$ adversarial regret for small total variation $Q$ without knowing $Q$
- Through coordinate-wise extension, achieves $O(\min(\sqrt{K \ln T \min(Q_{\infty},L^*,T-L^*)},K^{1/4}\sqrt{KT}))$ adversarial regret and $O(\sum_{i \neq i^*} \ln T/\Delta_i)$ stochastic regret
- These are the first bounds simultaneously achieving BOBW guarantees, data-dependent adaptivity, and optimal $T$-dependency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If learning rates are tuned via Stability-Penalty Matching (SPM), the algorithm can simultaneously achieve optimal worst-case regret and data-dependent bounds.
- **Mechanism:** The SPM update rule $\beta_{t+1} = \beta_t + \frac{z_t}{\beta_t h_t}$ dynamically adjusts the regularization strength. By matching the increment in the penalty term with the stability term, the method ensures the cumulative regret from changing learning rates cancels out the instantaneous stability regret.
- **Core assumption:** The stability term $z_t$ and penalty term $h_t$ can be bounded such that $h_{t+1} \le O(h_t)$.
- **Evidence anchors:** [Abstract] "combines the SPM framework with Follow-the-Regularized-Leader... enabling adaptation to data-dependent quantities"; [Section 1] "SPM carefully chooses $\beta_1, z_t$ and $h_t$... sets the learning rate... to match"; [Section 2] Lemma 2 provides the theoretical tool to bound the sum of stability terms.
- **Break condition:** If $z_t$ grows too large relative to $\beta_t$, the $\ln(\sum z_t/h_t)$ penalty in Lemma 2 dominates the regret.

### Mechanism 2
- **Claim:** Using a "real-time" stability term dependent on the observed arm's loss allows the regret bound to adapt to structural properties like sparsity.
- **Mechanism:** Unlike prior SPM methods using in-expectation stability terms, this method sets $z_t = \Omega(\beta_t(\dots))$ where the dependence is on the realized loss $\ell_{t,I_t}$. This allows the expected stability $E[z_t]$ to scale with the number of non-zero elements $S$ rather than the total arms $K$.
- **Core assumption:** The observed loss $\ell_{t,I_t}$ and inverse probability $1/p_{t,I_t}$ do not introduce variance that invalidates the deterministic bounds derived in Lemma 2.
- **Evidence anchors:** [Section 1.1] "This in-expectation form... inevitably requires taking the trivial bounds... Our work overcomes this limitation by setting $z_t = \Omega(\dots)$"; [Section 3] "It follows that $E[z_t] \le O(S^\alpha)$... leading to the $O(\sqrt{ST \ln K})$ bound."
- **Break condition:** If exploration is insufficient, the variance from $1/p_{t,I_t}$ causes $z_t$ to explode, breaking the $z_t/\beta_t$ bound.

### Mechanism 3
- **Claim:** Maintaining coordinate-wise (arm-specific) learning rates via Coordinate-wise SPM (CoWSPM) enables adaptation to variation norms ($Q_\infty$) and best-arm loss ($L^*$) simultaneously.
- **Mechanism:** By updating the learning rate $\beta_{t,i}$ only for the arm actually pulled, the algorithm isolates the stability penalty to the specific arm incurring loss. This allows the bound to depend on the properties of individual arms rather than a global worst-case.
- **Core assumption:** The Bregman divergence properties hold locally per arm, and the stability relation $h_{t+1,I_t} = O(h_{t,I_t})$ holds despite coordinate-wise updates.
- **Evidence anchors:** [Section 5] "coordinate-wise SPM... maintain arm-dependent learning rates... only the learning rate $\beta_{t,I_t}$ of the observed arm is updated"; [Theorem 9] Achieves $O(\min\{\sqrt{K \ln T \min(Q_\infty, L^*)}, \dots\})$.
- **Break condition:** If arms are highly correlated or the optimal arm changes rapidly, the isolated per-arm updates may fail to track global shifts.

## Foundational Learning

- **Concept:** Follow-the-Regularized-Leader (FTRL)
  - **Why needed here:** This is the base optimizer. The paper's contribution is an adaptive learning rate *within* this framework. You cannot understand the stability-penalty tradeoff without understanding the standard FTRL regret decomposition.
  - **Quick check question:** Can you write the FTRL update step (Eq 2) and identify where the learning rate $\beta_t$ appears?

- **Concept:** Bregman Divergence
  - **Why needed here:** The "stability" term is formally defined using Bregman divergence ($D_t(x,y)$). The core technical lemma (Lemma 2) relies on bounding the sum of these divergences to prove regret bounds.
  - **Quick check question:** How does the choice of regularizer $\phi_t$ (e.g., Tsallis entropy) affect the shape of the Bregman divergence?

- **Concept:** Best-of-Both-Worlds (BOBW)
  - **Why needed here:** The paper's "Key Outcome" is achieving BOBW guarantees. This requires understanding the difference between adversarial ($O(\sqrt{T})$) and stochastic ($O(\ln T)$) regimes and why standard algorithms often fail in one if tuned for the other.
  - **Quick check question:** Why does a standard adversarial algorithm (like EXP3) typically achieve sub-optimal regret in a stochastic setting?

## Architecture Onboarding

- **Component map:** Input Layer (loss vector, sampled arm) -> FTRL Core (computes $q_t$ via argmin using Hybrid Regularizer) -> Mixing Layer (uniform exploration mixing to get $p_t$) -> SPM Tuner (computes $z_t$ and updates $\beta_{t+1}$) -> Loss Estimator (importance-weighted estimator)

- **Critical path:**
  1. Compute $q_t$ using current $\beta_t$ and cumulative loss
  2. Mix $q_t$ to get sampling distribution $p_t$
  3. Draw arm $I_t \sim p_t$ and observe $\ell_{t,I_t}$
  4. Compute instantaneous stability $z_t$ based on *observed* $\ell_{t,I_t}$ and $p_{t,I_t}$ (Real-time SPM)
  5. Update $\beta_{t+1}$ using the SPM rule

- **Design tradeoffs:**
  - **Hybrid Regularization (Tsallis + Log-Barrier):** Adds complexity but is necessary to stabilize the "real-time" $z_t$ which can be unstable for small probabilities
  - **Real-time vs. In-Expectation $z_t$:** Using the observed value allows tighter data-dependent bounds but increases variance compared to using an expectation

- **Failure signatures:**
  - **Learning rate explosion:** If $\beta_t$ grows linearly with $T$ rather than $\sqrt{T}$, check the $z_t$ calculation for missing min() clamps
  - **Regret stalls at $O(\sqrt{T \ln T})$:** Indicates failure to adapt to sparsity; likely the $z_t$ definition has reverted to a trivial bound
  - **Probability collapse:** If $q_{t,i}$ hits 0 for sub-optimal arms too fast, the mixing parameter or $\gamma$ (log-barrier strength) is insufficient

- **First 3 experiments:**
  1. **Sparse Adversarial Validation:** Generate losses where only $S=10$ out of $K=100$ arms are non-zero. Verify regret scales with $\sqrt{S}$ rather than $\sqrt{K}$ (Theorem 3).
  2. **Stochastic Regime Check:** Run on a standard 2-arm stochastic bandit. Confirm regret scales as $O(\ln T)$, verifying the BOBW property.
  3. **Ablation on $z_t$:** Replace the "real-time" $z_t$ with a fixed/deterministic bound. Observe if the data-dependent adaptation to sparsity disappears.

## Open Questions the Paper Calls Out

- **Open Question 1:** Are the BOBW regret bounds in Theorem 3 tight when the sparsity constraint is hard ($\|\ell_t\|_0 \le S$) rather than soft (in expectation)?
  - **Basis in paper:** [explicit] The paper states in Section 3.2: "It remains an open question whether the BOBW bounds in Theorem 3 are tight under the hard constraint $\|\ell_t\|_0 \le S$."
  - **Why unresolved:** The paper provides near-matching lower bounds for problems with soft sparsity constraints, but does not provide a lower bound for the hard constraint scenario.
  - **What evidence would resolve it:** A lower bound proof for the hard constraint setting or an improved algorithm that closes the gap.

- **Open Question 2:** Is the extra factor $\sqrt{K^\alpha}$ in the adversarial bound of the coordinate-wise SPM method a fundamental limitation or an artifact of the analysis?
  - **Basis in paper:** [explicit] In Remark 10 (Section 5), the authors note: "It is unclear to us whether this extra factor is a fundamental limitation of CoWSPM or an artifact of our analysis."
  - **Why unresolved:** The analysis introduced a specific choice for the penalty term $h_{t,i}$ to ensure monotonicity, but this choice may not be optimal.
  - **What evidence would resolve it:** A refined analysis that removes the factor or a counter-example proving it is unavoidable.

- **Open Question 3:** Can the real-time SPM framework be successfully extended to contextual linear bandits?
  - **Basis in paper:** [explicit] Section 6 lists this as a primary direction: "Future work includes applying real-time SPM on other bandits problems, such as contextual linear bandits..."
  - **Why unresolved:** The current work is restricted to standard multi-armed bandits. Extending to linear structures poses distinct technical challenges.
  - **What evidence would resolve it:** Deriving BOBW data-dependent regret bounds for the contextual linear bandit setting using real-time SPM.

- **Open Question 4:** Can real-time SPM be adapted to handle $\ell_1$ and $\ell_2$-norm path-length bounds?
  - **Basis in paper:** [explicit] Section 6 mentions the goal of "making real-time SPM adaptive towards other challenging data-dependent quantities like $\ell_1$ and $\ell_2$-norm path-length bounds."
  - **Why unresolved:** The paper currently demonstrates adaptivity to sparsity and total variation. Adapting to path-length bounds requires the stability term to capture different geometric properties.
  - **What evidence would resolve it:** An algorithm using real-time SPM that achieves regret bounds scaling with the path-length of the losses.

## Limitations

- **Numerical implementation complexity:** The hybrid regularizer requires careful numerical optimization over the simplex, particularly for large $K$, which may be challenging in practice.
- **Variance sensitivity:** The real-time stability term $z_t$ can be sensitive to variance from importance-weighted loss estimators when exploration probabilities $p_{t,i}$ are small, potentially breaking theoretical guarantees.
- **Empirical validation gaps:** While the theoretical bounds are rigorous, empirical validation on high-dimensional or high-variance problems is limited, leaving uncertainty about practical performance.

## Confidence

- **High Confidence:** The existence of the BOBW bounds (Theorem 3 for sparsity, Theorem 9 for variation) and their optimality are well-supported by the theoretical framework and proofs.
- **Medium Confidence:** The real-time SPM mechanism's ability to consistently adapt to problem structure in practice depends on the stability of the learning rate updates, which is theoretically bounded but may be sensitive to numerical precision.
- **Medium Confidence:** The extension to coordinate-wise SPM (Theorem 9) is theoretically sound, but the assumption that per-arm learning rates can track rapid global changes is less certain without empirical validation.

## Next Validation Checks

1. **Numerical Stability Test:** Implement Algorithm 1 and monitor $\beta_t$ growth and $p_{t,i}$ probabilities across many runs. Check for learning rate explosion or probability collapse, diagnosing if the min() clipping in $z_t$ or mixing parameter $(1-K/T)$ needs adjustment.

2. **Data-Dependent Adaptation Verification:** Run on a suite of adversarial MAB problems with varying sparsity levels ($S$). Plot regret scaling against $\sqrt{S}$ (not $\sqrt{K}$) to confirm the theoretical $O(\sqrt{ST \ln K})$ bound is achieved in practice.

3. **BOBW Property Validation:** Test on a standard stochastic 2-arm bandit (known gap $\Delta$). Verify the algorithm achieves $O(\ln T)$ regret, confirming it doesn't suffer the $O(\sqrt{T})$ penalty common in algorithms tuned for the adversarial regime.