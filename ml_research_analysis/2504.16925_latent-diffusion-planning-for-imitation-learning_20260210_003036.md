---
ver: rpa2
title: Latent Diffusion Planning for Imitation Learning
arxiv_id: '2504.16925'
source_url: https://arxiv.org/abs/2504.16925
tags:
- learning
- data
- diffusion
- latent
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent Diffusion Planning (LDP) is a new imitation learning method
  that separates the problem into planning over future states and extracting actions
  using diffusion models. By operating in a learned latent space, LDP can leverage
  both suboptimal and action-free data, unlike prior approaches that require optimal
  action labels.
---

# Latent Diffusion Planning for Imitation Learning

## Quick Facts
- **arXiv ID:** 2504.16925
- **Source URL:** https://arxiv.org/abs/2504.16925
- **Authors:** Amber Xie; Oleh Rybkin; Dorsa Sadigh; Chelsea Finn
- **Reference count:** 26
- **One-line primary result:** LDP achieves 0.65 success rate on visual robotic tasks vs 0.51 for Diffusion Policy, improving to 0.95 with heterogeneous data

## Executive Summary
Latent Diffusion Planning (LDP) introduces a novel approach to imitation learning that separates state planning from action extraction, enabling the use of heterogeneous data sources including action-free demonstrations and suboptimal trajectories. By planning over a learned latent space rather than pixel space, LDP achieves faster computation and better closed-loop performance compared to video-based planning methods. The method combines a diffusion-based planner that forecasts future latent states with an inverse dynamics model that extracts actions from latent transitions, allowing independent training on different data modalities.

## Method Summary
LDP trains a β-VAE to compress images into a compact latent space, then freezes this embedding to train two separate diffusion models: a planner that forecasts future latent states conditioned on the current observation, and an inverse dynamics model (IDM) that predicts actions from pairs of consecutive latent states. During inference, the system plans a sequence of future latents, extracts actions via the IDM, and executes them in a receding-horizon fashion. This separation allows LDP to leverage action-free demonstrations for planning and suboptimal data for learning action extraction, achieving state-of-the-art performance on simulated visual robotic manipulation tasks and showing promise in real-world applications.

## Key Results
- LDP achieves 0.65 average success rate on simulated tasks compared to 0.51 for standard Diffusion Policy
- When combining expert, action-free, and suboptimal data, LDP reaches 0.95 success rate
- On the Franka Lift task, LDP outperforms Diffusion Policy in real-world settings
- LDP demonstrates faster planning than pixel-space video diffusion methods while maintaining closed-loop control capabilities

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Data Utilization via Modality Decoupling
The separation of planning and control objectives allows independent utilization of different data modalities. The planner can leverage action-free demonstrations to learn scene dynamics, while the IDM can learn from suboptimal trajectories to understand valid robot actions. This decoupling is critical because each data source contains complementary information that would be difficult to extract if combined in a single model.

### Mechanism 2: Latent Compaction for Dense Closed-Loop Replanning
Compressing high-dimensional images into a compact latent space enables dense, frequent planning that supports closed-loop control. By operating in a 4-dimensional latent space rather than pixel space, the planner can forecast trajectories at frequencies that allow for error correction at every timestep, mitigating the compounding errors common in open-loop or sparse planning approaches.

### Mechanism 3: Diffusion for Multimodal Action Extraction
Using diffusion models for the inverse dynamics model allows LDP to capture the multimodal nature of action distributions. When a state transition could be achieved through multiple valid approaches, the diffusion-based IDM can represent this uncertainty rather than collapsing to a mean action, which is particularly important when trained on diverse suboptimal data where multiple strategies exist for the same transition.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** The core building block for both the Planner and the IDM. Understanding the iterative denoising process is essential for debugging sampling issues or tuning inference steps.
  - **Quick check question:** Can you explain why the model predicts noise rather than the clean sample directly, and how the variance schedule affects convergence?

- **Concept: Variational Autoencoders (VAEs) & β-VAE**
  - **Why needed here:** The entire system relies on the quality of the latent space. Understanding the trade-off between reconstruction fidelity and KL regularization is critical for stable planning.
  - **Quick check question:** If your decoded plans look blurry or lose the object position, which hyperparameter (β or latent dimension) should you adjust and why?

- **Concept: Inverse Dynamics Models (IDM)**
  - **Why needed here:** This bridges the gap between planned states and executable actions. Understanding that it maps state pairs to actions helps distinguish it from forward dynamics or direct policies.
  - **Quick check question:** If the robot executes planned actions but fails to reach the planned next state, is the failure likely in the Planner or the IDM?

## Architecture Onboarding

- **Component map:** VAE Encoder -> Latent Space -> Planner -> Latent Trajectory -> IDM -> Actions -> Robot
- **Critical path:** Train VAE (on all images) → Freeze VAE → Train Planner (on latent sequences) & IDM (on latent transitions) → Inference (Plan → Extract Actions → Execute)
- **Design tradeoffs:**
  - Latent dimensionality: Smaller latents speed up diffusion but risk losing manipulation details
  - Planning horizon vs. action horizon: Longer planning improves consistency; longer action execution improves stability but reduces reactivity
  - Architecture selection: CNN U-Net for temporal consistency in planner, lightweight MLP for speed in IDM
- **Failure signatures:**
  - "Stuck" agent: Planner generates static latents (check if latent dynamics have collapsed)
  - Action jitter: IDM oscillates between modes when transitions are physically ambiguous
  - Decoder artifacts: Planner predicts valid latents but decoder creates visual hallucinations
- **First 3 experiments:**
  1. Latent Dynamics Visualizer: Visualize decoded planned trajectories vs ground truth to ensure planner learns physics
  2. IDM Accuracy Benchmark: Test prediction error on held-out transitions to verify mapping is learnable
  3. Data Ablation: Compare LDP trained on "Expert Only" vs "Expert + Action-Free" to verify heterogeneous data contribution

## Open Questions the Paper Calls Out

### Open Question 1
How can representation learning objectives be modified beyond standard VAE reconstruction to better capture features necessary for control in Latent Diffusion Planning? The authors note that the current VAE might not learn the most useful features for control and propose exploring different representation learning objectives in future work.

### Open Question 2
Can LDP effectively leverage diverse, "in-the-wild" data sources such as human handheld videos or autonomously collected robotic logs to scale to more complex real-world tasks? The paper proposes evaluating whether LDP can improve complex real-world tasks using diverse datasets of human-collected or autonomously collected robotic data.

### Open Question 3
Can recent diffusion model architectures and inference techniques (e.g., DiTs, consistency models) mitigate the computational overhead of diffusing over states compared to action-only methods? The authors acknowledge that diffusing over states incurs additional computational overhead and suggest that recent improvements may be important in large data regimes.

## Limitations

- **Unknown hyperparameters:** Critical parameters like planning/action horizons, diffusion sampling steps, and proprioception integration are not specified
- **Limited ablation studies:** The impact of heterogeneous data sources is only tested in combination, not independently
- **Preliminary real-world evaluation:** Only tested on one task with a single robot platform, leaving scalability unproven

## Confidence

- **High Confidence:** The core architectural design and its ability to leverage heterogeneous data sources
- **Medium Confidence:** The specific performance improvements over baselines, dependent on unreported hyperparameters
- **Medium Confidence:** The scalability to real-world applications, given limited evaluation scope

## Next Checks

1. **Hyperparameter Sensitivity:** Systematically vary latent dimensionality (2-8 dimensions), planning horizon (5-50 steps), and inference steps to establish robustness boundaries
2. **Data Source Isolation:** Test LDP trained with only expert data vs only action-free data vs only suboptimal data to quantify each source's contribution
3. **Real-World Generalization:** Evaluate LDP on multiple diverse real-world manipulation tasks beyond the single Franka Lift experiment to validate cross-task transferability