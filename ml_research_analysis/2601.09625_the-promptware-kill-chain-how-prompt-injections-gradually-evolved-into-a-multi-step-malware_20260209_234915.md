---
ver: rpa2
title: 'The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a
  Multi-Step Malware'
arxiv_id: '2601.09625'
source_url: https://arxiv.org/abs/2601.09625
tags:
- prompt
- injection
- data
- persistence
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a systematic framework for analyzing attacks
  on LLM-based applications, which the authors term "promptware." They argue that
  current framing of these threats as "prompt injection" is inadequate because it
  obscures the multi-step nature of real attacks, which mirror traditional malware
  campaigns. The authors propose a five-step kill chain model: Initial Access (prompt
  injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval
  poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions
  on Objective (data exfiltration, unauthorized transactions, etc.).'
---

# The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware

## Quick Facts
- arXiv ID: 2601.09625
- Source URL: https://arxiv.org/abs/2601.09625
- Reference count: 11
- Authors: Ben Nassi, Bruce Schneier, Oleg Brodt
- Primary result: Introduces a systematic 5-stage kill chain framework for analyzing LLM-based attacks

## Executive Summary
This paper addresses the inadequacy of current "prompt injection" terminology by introducing a systematic framework for analyzing attacks on LLM-based applications, termed "promptware." The authors demonstrate that real attacks follow multi-stage sequences analogous to traditional malware campaigns, progressing through Initial Access, Privilege Escalation, Persistence, Lateral Movement, and Actions on Objective. By mapping recent documented incidents to this structure, they show how current security approaches miss critical intervention opportunities. The framework provides security practitioners with a structured methodology for threat modeling and offers a common vocabulary for researchers across AI safety and cybersecurity to address this evolving threat landscape.

## Method Summary
The paper presents a conceptual framework derived through taxonomy construction and case study mapping. The authors analyze seven documented incidents from the literature, mapping each to the proposed five-stage kill chain. Rather than conducting experiments, they systematically categorize and analyze existing attack techniques to demonstrate that LLM-related attacks follow systematic multi-stage sequences. The framework adapts the traditional cyber kill chain model to the unique characteristics of LLM applications, focusing on the architectural inability of LLMs to distinguish trusted instructions from untrusted data.

## Key Results
- LLM attacks follow systematic multi-stage sequences analogous to traditional malware kill chains
- Current security approaches miss critical intervention opportunities by treating prompt injection as a single-step problem
- The instruction-data boundary problem is architectural rather than implementation-specific, making it fundamentally unpatchable
- Real attacks demonstrate consistent progression through stages of access, escalation, persistence, lateral movement, and objective achievement

## Why This Works (Mechanism)

### Mechanism 1
LLMs cannot architecturally distinguish trusted instructions from untrusted data, creating an unpatchable initial access vulnerability. All inputs—system prompts, user messages, retrieved documents—are tokenized into a unified sequence. A malicious instruction appearing anywhere in this stream is processed identically to legitimate instructions because no enforcement boundary exists at the model layer. This fundamental architectural property means guardrails and safety training at the application layer can be bypassed and do not fix the underlying issue.

### Mechanism 2
Promptware attacks progress through sequential stages analogous to traditional malware kill chains, with each stage enabling subsequent exploitation. Initial access (prompt injection) breaches the context window. Privilege escalation (jailbreaking) bypasses safety constraints. Persistence stores the payload in memory or RAG systems. Lateral movement propagates across users, devices, or services. Actions on objective achieve the attacker's goal. Defenders who treat prompt injection as a single-step problem will miss intervention opportunities at later stages.

### Mechanism 3
Stateful LLM applications enable persistence mechanisms that transform transient injections into durable implants. Retrieval-dependent persistence relies on semantic similarity queries resurfacing poisoned content. Retrieval-independent persistence (e.g., ChatGPT Memories) injects payload content into every context window unconditionally. Command-and-control variants fetch attacker-specified instructions on each inference. These persistence mechanisms require the application to have stateful components; stateless chatbots cannot support this stage.

## Foundational Learning

- **Context Window and Tokenization**
  - Why needed here: Understanding that all inputs become a single token sequence is essential to grasping why instruction-data separation fails.
  - Quick check question: Can you explain why a system prompt and a user-uploaded document are processed identically by an LLM?

- **Traditional Cyber Kill Chain (Lockheed Martin)**
  - Why needed here: The paper's framework adapts this established model; knowing the original helps you understand the analogy and deviations.
  - Quick check question: Name the stages of the traditional kill chain and how privilege escalation differs from lateral movement.

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG systems are a primary vector for indirect injection and persistence; understanding retrieval semantics is critical for threat modeling.
  - Quick check question: How does a RAG system decide which documents to include in context, and how could an attacker exploit this?

## Architecture Onboarding

- **Component map**: Input layer (direct user prompts, indirect sources, multimodal channels) -> LLM core (tokenizer, context window, aligned model) -> Application layer (guardrails, tool integrations) -> Stateful components (RAG databases, agent memory stores) -> Output layer (generated text, tool invocations, external communications)

- **Critical path**: Initial Access → Payload enters context window via direct or indirect injection → Privilege Escalation → Jailbreak bypasses safety training → Persistence → Payload stored in memory or RAG → Lateral Movement → Payload propagates via self-replication, permission abuse, or pipeline traversal → Actions on Objective → Data exfiltration, financial transactions, physical system manipulation, or code execution

- **Design tradeoffs**: Broad tool access increases utility but expands attack surface; narrow scopes limit damage potential. Human-in-the-loop confirmation reduces automation risk but degrades user experience. RAG integration improves response quality but introduces indirect injection vectors. Memory features enhance personalization but create persistence opportunities.

- **Failure signatures**: Unexpected tool invocations triggered by external content (e.g., a calendar invite causing Zoom to launch), model output containing instructions not present in system prompts, repetitive or context-inappropriate responses suggesting persistent payload activation, data exfiltration to unfamiliar domains following ingestion of external documents

- **First 3 experiments**: Test your application with known jailbreak payloads (e.g., DAN prompts, role-play framing) to measure safety training robustness. Inject a benign marker string into RAG sources your application queries; verify whether it appears in outputs to validate indirect injection paths. Review memory and stateful storage for any content that was not explicitly user-created; audit for persistence vectors.

## Open Questions the Paper Calls Out

### Open Question 1
Can architectural modifications to LLMs fundamentally resolve the instruction-data boundary problem, or is this limitation inherent to transformer-based language models? The authors state "No native mechanism currently exists to reliably enforce a distinction between instructions and data" and emphasize this "is not a bug amenable to patching; it is an inherent property of the architecture." This requires research demonstrating LLM architectures that reliably distinguish trusted instructions from untrusted data without relying on application-layer pattern-matching defenses.

### Open Question 2
What defensive interventions are most effective at disrupting multi-step promptware attacks at each stage of the kill chain? The authors note the framework reveals "distinct defensive intervention points" and recommend practitioners "focus on limiting privilege escalation, preventing persistence, constraining lateral movement, and minimizing the impact of actions on the objective." This requires empirical studies measuring how specific security controls disrupt attacks at each kill-chain stage, with quantitative efficacy metrics.

### Open Question 3
How should the kill-chain model evolve as attackers develop techniques that skip, merge, or operate outside the five defined stages? The authors acknowledge "we do not claim that the five-stage model captures every possible attack scenario" and that "Attackers may skip stages, combine them, merge them, execute them sequentially or concurrently, or develop techniques that resist clean categorization." This requires longitudinal analysis of emerging promptware attacks to identify systematic gaps in the five-stage model and validated extensions addressing them.

## Limitations
- The framework's effectiveness depends on the assumption that real-world attacks consistently follow the five-stage progression, but several documented attacks show skipped stages or merged transitions.
- The claim that prompt injection is an "unpatchable" architectural flaw assumes no future LLM designs will incorporate instruction-data separation mechanisms, which may be premature.
- The reliance on a small corpus of seven incidents may not capture the full diversity of attack vectors, especially novel techniques that emerge after the framework's publication.

## Confidence

- **High confidence**: The analogy between traditional malware kill chains and multi-stage promptware attacks is well-supported by documented cases and aligns with established cybersecurity frameworks.
- **Medium confidence**: The classification of persistence mechanisms (retrieval-dependent vs. independent) is reasonable but may oversimplify the range of state management approaches in production LLM applications.
- **Low confidence**: The assertion that the instruction-data boundary problem is fundamentally "unpatchable" at the model level requires stronger empirical validation and may be premature given rapid architectural innovations.

## Next Checks

1. **Stage-skipping analysis**: Systematically examine 20+ additional promptware incidents to quantify how frequently attacks skip stages or merge transitions, testing the framework's universality.
2. **Architectural boundary test**: Design and evaluate a prototype LLM variant with explicit instruction-data separation at the token or attention level to assess whether the claimed "unpatchable" property can be overcome.
3. **Persistence mechanism breadth**: Map the full spectrum of state management in production LLM applications (memory, RAG, session state, external databases) to identify additional persistence vectors not covered by the current retrieval-dependent/independent dichotomy.