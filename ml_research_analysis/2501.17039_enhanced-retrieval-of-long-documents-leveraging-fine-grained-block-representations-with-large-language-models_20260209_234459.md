---
ver: rpa2
title: 'Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations
  with Large Language Models'
arxiv_id: '2501.17039'
source_url: https://arxiv.org/abs/2501.17039
tags:
- blocks
- document
- block
- tokens
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BReps, a fine-grained, representation-based
  framework for long-document reranking that improves upon single-vector document
  embeddings by encoding each document into multiple block-level embeddings. The approach
  segments documents into 63-token blocks, encodes them with a decoder-only LLM, and
  aggregates the top-k block-query similarities using weighted sums.
---

# Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models

## Quick Facts
- arXiv ID: 2501.17039
- Source URL: https://arxiv.org/abs/2501.17039
- Authors: Minghan Li; Eric Gaussier; Guodong Zhou
- Reference count: 40
- Primary result: BReps-TIR-65 achieves 0.451 NDCG@10 on TREC DL'23, outperforming single-vector baselines

## Executive Summary
This paper introduces BReps, a fine-grained, representation-based framework for long-document reranking that improves upon single-vector document embeddings by encoding each document into multiple block-level embeddings. The approach segments documents into 63-token blocks, encodes them with a decoder-only LLM, and aggregates the top-k block-query similarities using weighted sums. A lightweight Top-k Interaction Refinement (TIR) module further refines block scores via query-conditioned attention to better capture redundancy and complementarity. Evaluated on TREC DL and MLDR-zh benchmarks, BReps-TIR-65 (using up to 65 blocks under a 4k-token budget) achieves strong effectiveness—for example, 0.451 NDCG@10 on TREC DL'23—outperforming single-vector baselines like RepLLaMA and being substantially more efficient than cross-encoder rerankers like IDCM and KeyB. The method offers practical advantages in interpretability, offline block embedding precomputation, and competitive accuracy with minimal training data.

## Method Summary
The BReps framework encodes long documents into multiple block-level representations rather than a single vector. Documents are segmented into 63-token blocks, each encoded by a decoder-only LLM to produce block embeddings. For each query-document pair, block-query similarities are computed and the top-k are aggregated using a weighted sum. The Top-k Interaction Refinement (TIR) module then refines these scores through query-conditioned attention, allowing the model to capture redundancy and complementarity among blocks. The entire framework is trained end-to-end with a contrastive loss, enabling effective reranking with minimal training data. The approach is designed for efficiency, enabling offline precomputation of block embeddings and faster inference compared to cross-encoder methods.

## Key Results
- BReps-TIR-65 achieves 0.451 NDCG@10 on TREC DL'23, outperforming single-vector baselines like RepLLaMA
- The method is substantially more efficient than cross-encoder rerankers like IDCM and KeyB while maintaining competitive accuracy
- Strong improvements are observed on both English (TREC DL) and Chinese (MLDR-zh) benchmarks, demonstrating cross-lingual effectiveness

## Why This Works (Mechanism)
The core insight is that long documents contain heterogeneous content, and a single vector cannot adequately capture the relevance of different document sections to a given query. By encoding each document as multiple block representations, BReps can focus on the most relevant portions and aggregate their contributions. The TIR module further refines this process by using query-conditioned attention to model redundancy and complementarity among blocks, ensuring that the final relevance score reflects the document's overall utility rather than just isolated passages. This fine-grained approach allows for better alignment between query intent and document content, especially in the presence of redundancy or diverse topics within a single document.

## Foundational Learning
- **Block-level embeddings**: Documents are divided into fixed-size blocks (63 tokens), each encoded separately to capture fine-grained semantic content. *Why needed*: Single-vector representations lose local relevance signals. *Quick check*: Verify that block size captures meaningful semantic units without excessive fragmentation.
- **Top-k aggregation**: Only the most relevant blocks (by similarity to query) are selected and weighted for the final score. *Why needed*: Reduces noise from irrelevant blocks and focuses on query-relevant content. *Quick check*: Ensure top-k selection is robust to query variations.
- **Query-conditioned attention**: The TIR module uses attention to model interactions among top-k blocks, capturing redundancy and complementarity. *Why needed*: Allows the model to refine scores based on block relationships, not just individual relevance. *Quick check*: Confirm attention weights reflect meaningful block interactions.
- **Offline precomputation**: Block embeddings are computed once and stored, enabling fast online reranking. *Why needed*: Avoids re-encoding blocks for every query, improving efficiency. *Quick check*: Validate storage and retrieval overhead for large collections.
- **Contrastive loss training**: The model is trained end-to-end to align query and document representations using a contrastive objective. *Why needed*: Ensures the learned embeddings are discriminative for retrieval. *Quick check*: Monitor convergence and generalization across queries.

## Architecture Onboarding
- **Component map**: Document -> Block segmentation (63 tokens) -> LLM block encoding -> Block-query similarity -> Top-k selection -> Weighted aggregation -> TIR module (query-conditioned attention) -> Final relevance score
- **Critical path**: For each query-document pair: compute block-query similarities, select top-k, aggregate, apply TIR refinement, output score
- **Design tradeoffs**: Single-vector vs. block representations (accuracy vs. efficiency), block size (granularity vs. overhead), top-k vs. all blocks (focus vs. completeness), TIR complexity vs. performance gain
- **Failure signatures**: Poor block segmentation (loss of semantic coherence), top-k selection missing relevant blocks, TIR overfitting to training queries, inefficiency with very large document collections
- **First experiments**: 1) Validate block segmentation preserves semantic units, 2) Test top-k selection with varying k on a held-out set, 3) Ablate TIR module to measure impact on redundancy handling

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Narrow evaluation scope: Only two English-language benchmarks and one LLM backbone tested
- Block size selection: 63-token blocks appear arbitrary; sensitivity to granularity not analyzed
- Query-conditioned attention complexity: TIR module may not generalize across domains with different redundancy patterns
- Scalability concerns: Computational overhead of block-level processing for very large collections not fully characterized

## Confidence
- **High**: Effectiveness of block-level representations over single-vector approaches, supported by consistent improvements
- **Medium**: Efficiency claims relative to cross-encoders, though computational overhead for large collections is not fully characterized
- **Low**: Interpretability claims, as qualitative analysis of block relevance is not deeply explored

## Next Checks
1. Evaluate across diverse domains (legal, scientific, news) to assess generalization beyond current benchmarks
2. Conduct ablation studies on block size and aggregation strategies to understand hyperparameter sensitivity
3. Measure end-to-end latency and memory requirements when scaling to millions of documents to fully characterize practical efficiency benefits