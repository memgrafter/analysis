---
ver: rpa2
title: Exploring Solution Divergence and Its Effect on Large Language Model Problem
  Solving
arxiv_id: '2509.22480'
source_url: https://arxiv.org/abs/2509.22480
tags:
- solution
- divergence
- pass
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates solution divergence as a novel factor for
  improving LLM problem-solving. The authors define solution divergence as the variety
  of correct solutions an LLM generates for a single problem and find a positive correlation
  between divergence and performance.
---

# Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving

## Quick Facts
- **arXiv ID:** 2509.22480
- **Source URL:** https://arxiv.org/abs/2509.22480
- **Reference count:** 18
- **Primary result:** Solution divergence positively correlates with LLM problem-solving performance; divergence-aware SFT and RL improve Pass@1 and Pass@10 scores.

## Executive Summary
This paper investigates solution divergence as a novel factor for improving LLM problem-solving. The authors define solution divergence as the variety of correct solutions an LLM generates for a single problem and find a positive correlation between divergence and performance. They propose two methods: using divergence for dataset selection in SFT and incorporating it into RL reward functions. Experiments on Math-500, MBPP+, and a newly introduced Maze dataset show that training with higher-divergence samples and using divergence-augmented RL rewards consistently improves Pass@1 and Pass@10 scores across multiple models. The results demonstrate that solution divergence is a simple but effective tool for enhancing LLM problem-solving capabilities.

## Method Summary
The method involves computing solution divergence using normalized string edit distance between generated solutions, constructing relation graphs, and calculating global divergence metrics via Laplacian eigenvalues. For SFT, datasets are filtered to high-divergence (D^+_SFT) and low-divergence (D^-_SFT) subsets based on subset-level divergence scores. For RL, a divergence-fused reward function R_ζ is implemented using token-level policy gradients (DAPO) with a configurable α parameter controlling the correctness-diversity tradeoff. Models are first trained with SFT on the filtered datasets, then fine-tuned with RL using the divergence-augmented rewards.

## Key Results
- Solution divergence (ζ^g_π) shows strong positive correlation with Pass@1 (R²: Maze .94, Math-500 .71, MBPP+ .50)
- D^+_SFT outperforms D^-_SFT in 8/12 Pass@1 cases (mean Δ = 0.65%) and 10/12 Pass@10 cases (mean Δ = 6.2%)
- R_ζ outperforms R_s in 11/12 Pass@10 cases for D^-_SFT-initialized models (mean Δ = 3.12%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Solution divergence positively correlates with LLM problem-solving performance.
- Mechanism: Models capable of generating diverse correct solutions have explored the solution space more thoroughly; this capability transfers to better first-attempt accuracy on new problems.
- Core assumption: String edit distance adequately proxies solution diversity; the correlation is causal, not merely symptomatic of model capability.
- Evidence anchors:
  - [abstract] "higher solution divergence is positively related to better problem-solving abilities across various models"
  - [section 3.3] R² values: Maze (.94), Math-500 (.71), MBPP+ (.50) for global divergence metric ζ^g_π vs Pass@1
  - [corpus] No direct corpus validation; related work (e.g., "Underthinking of o1-Like LLMs") discusses thought switching but not divergence-performance links—weak external support.
- Break condition: If divergence is merely a proxy for model scale/quality rather than a trainable signal, interventions targeting divergence may yield diminishing returns.

### Mechanism 2
- Claim: Selecting high-divergence solution subsets for SFT training improves model performance.
- Mechanism: Training on diverse solutions exposes the model to varied reasoning paths; ζ^g_qn (global divergence) computed via Laplacian eigenvalues selects subsets maximizing pairwise edit distance; accepted if modification increases ζ^g_qn.
- Core assumption: Diversity in training solutions generalizes to diversity at inference; overfitting to specific solution patterns is reduced.
- Evidence anchors:
  - [section 4.1] "if the metric increases, the modification is accepted; otherwise, it is rejected"
  - [section 5.3] D^+_SFT outperforms D^-_SFT in 8/12 Pass@1 cases (mean Δ = 0.65%), 10/12 Pass@10 cases (mean Δ = 6.2%)
  - [corpus] "Step-wise Adaptive Integration of SFT and RL" supports SFT/RL synergy but does not address divergence-based selection.
- Break condition: If training data is limited (e.g., MBPP+ with 98 questions), overfitting negates divergence benefits; small datasets may not support diverse solution generation.

### Mechanism 3
- Claim: Divergence-fused reward R_ζ improves RL exploration and Pass@10 performance.
- Mechanism: Reward R_ζ(s_i, S) = (|S_c|/|S|)^α × (1/|S_c|) Σ δ(s_i, s_j) for correct solutions, -1 for incorrect; balances correctness (via |S_c|/|S| term) and diversity (via pairwise δ); α controls tradeoff sensitivity.
- Core assumption: Models optimize for the combined reward; diversity term does not conflict with correctness when α is properly tuned.
- Evidence anchors:
  - [section 4.2, Eq. 3] Full reward formulation with α hyperparameter
  - [section 5.3] R_ζ outperforms R_s in 11/12 Pass@10 cases for D^-_SFT-initialized models (mean Δ = 3.12%); α=4 performs best in 5/8 Pass@1 and 6/8 Pass@10 cases
  - [corpus] GRPO, DAPO, GSPO use group-based rewards but without explicit diversity terms—mechanism extends rather than conflicts with prior work.
- Break condition: When Pass@1 is low, excessive divergence emphasis may harm convergence; α must scale diversity influence inversely to success rate.

## Foundational Learning

- Concept: **Spectral Clustering and Laplacian Eigenvalues**
  - Why needed here: Solution divergence ζ^g_qn is computed from Laplacian matrix eigenvalues of the solution relation graph; understanding spectral methods clarifies why global metrics outperform local ones.
  - Quick check question: Given a weighted graph with M nodes, what does the sum of Laplacian eigenvalues indicate about graph connectivity?

- Concept: **Token-level Policy Gradient (DAPO)**
  - Why needed here: RL optimization uses token-level clipping to address underweighting of long responses in standard GRPO; essential for implementing the divergence-fused reward.
  - Quick check question: Why does sequence-level advantage normalization require token-level gradient scaling?

- Concept: **Self-Consistency and Pass@k Metrics**
  - Why needed here: Pass@10 captures benefits from solution diversity (supports ensemble/verification); divergence improvements manifest more strongly in Pass@10 than Pass@1.
  - Quick check question: If a model's Pass@1 is 30% and Pass@10 is 60%, what does this gap suggest about solution diversity?

## Architecture Onboarding

- Component map:
  Divergence Calculator -> SFT Data Selector -> RL Reward Module -> Training Pipeline

- Critical path:
  1. Generate M solutions per question using advanced LLMs (GPT-4o, Claude-3.5, etc.)
  2. Enumerate all 4-solution subsets, compute ζ^g_qn for each
  3. Select max-divergence subset → D^+_SFT, min-divergence → D^-_SFT
  4. SFT training with both datasets; compare checkpoints
  5. RL training with R_s (baseline) and R_ζ; tune α ∈ {2, 3, 4}
  6. Evaluate Pass@1 and Pass@10 on held-out test set

- Design tradeoffs:
  - **Edit distance vs. semantic similarity**: String edit distance is efficient but misses semantic equivalence; domain-specific metrics may improve accuracy at cost of generality
  - **Local (ζ^l) vs. global (ζ^g) metrics**: ζ^l is noise-sensitive; ζ^g is more robust but may smooth over meaningful local variations
  - **α tuning**: Low α emphasizes correctness; high α emphasizes diversity; optimal depends on base model capability and task difficulty

- Failure signatures:
  - **MBPP+ pattern**: Small dataset (98 questions) → rapid overfitting → minimal divergence benefit
  - **Pass@1 / Pass@10 inversion**: R_ζ may lower Pass@1 but raise Pass@10, indicating broader exploration at cost of first-attempt precision
  - **Local metric instability**: ζ^l_π shows weak correlation on MBPP+ (R²=.13); use ζ^g_π for reliability

- First 3 experiments:
  1. **Divergence correlation check**: Sample 100 questions from target domain; generate M=10 solutions per question using base model; compute ζ^g_π and Pass@1; verify positive correlation (R² > 0.4) before proceeding
  2. **SFT subset comparison**: Prepare D^+_SFT and D^-_SFT with k=4 solutions per question; train identical models on both; expect Δ Pass@10 > 4% for high-divergence advantage
  3. **α sweep on validation set**: Train with R_ζ using α ∈ {2, 3, 4}; log Pass@1 and Pass@10; select α maximizing Pass@10 without degrading Pass@1 > 2%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do semantic or execution-based divergence metrics compare to normalized string edit distance in capturing meaningful solution variety?
- Basis in paper: [explicit] The authors state on Page 2 that "More advanced proxy metrics will be explored as one future work," acknowledging that string edit distance provides only a "restricted perspective."
- Why unresolved: String edit distance relies on surface-level text overlap and fails to capture semantic equivalence or structural differences in code, potentially misestimating true divergence.
- What evidence would resolve it: A comparative study on the Math-500 or MBPP+ datasets using embedding-based similarity or Abstract Syntax Tree (AST) distance as the divergence proxy δ.

### Open Question 2
- Question: Does the positive relationship between solution divergence and performance hold for tasks lacking objective ground truth?
- Basis in paper: [inferred] The study is strictly limited to STEM tasks (math, code, logic) where correctness is "objectively verified" (Page 2).
- Why unresolved: The methodology depends on filtering for correct solutions (S_c) to calculate divergence; applying this to open-ended tasks requires a new definition of "viability" or reliance on model-based judges.
- What evidence would resolve it: Experiments applying divergence-aware training to creative writing or summarization tasks using LLM-as-a-judge verification methods.

### Open Question 3
- Question: How can the divergence-fused reward be stabilized to prevent the occasional degradation in Pass@1 performance?
- Basis in paper: [inferred] Table 3 shows that while the divergence-fused reward (R_ζ) often improves Pass@10, it sometimes lowers Pass@1 scores (e.g., Llama-3.1-8B on Maze).
- Why unresolved: The current formulation encourages broader exploration of the solution space, which may come at the cost of concentrating probability mass on the single most likely correct path.
- What evidence would resolve it: An analysis of training dynamics focusing on probability mass distribution, or the introduction of an adaptive α parameter that responds to Pass@1 stability.

## Limitations
- String edit distance may not capture semantic equivalence, potentially inflating divergence metrics without meaningful diversity
- Effectiveness diminishes on smaller datasets (e.g., MBPP+ with 98 questions), suggesting potential overfitting concerns
- The Maze dataset is small and novel, limiting generalizability to broader problem-solving domains

## Confidence
- **High Confidence**: The positive correlation between solution divergence and problem-solving performance (Pass@1/Pass@10 scores) across multiple domains and models
- **Medium Confidence**: The claim that divergence-augmented RL rewards (R_ζ) improve exploration and Pass@10 performance
- **Low Confidence**: The generalizability of the divergence mechanism across diverse problem domains and model scales

## Next Checks
1. **Causal Validation**: Design an ablation study where solution diversity is controlled for (e.g., by generating semantically diverse but structurally similar solutions) to isolate the causal effect of divergence from other confounding factors like model capability or data quality

2. **Semantic Diversity Assessment**: Implement a semantic similarity metric (e.g., sentence transformers for code/math problems) alongside string edit distance to validate that divergence captures meaningful solution variety rather than superficial differences

3. **Dataset Size Sensitivity**: Systematically vary dataset sizes (e.g., 100, 500, 1000 questions) for SFT training to establish the minimum data requirements for divergence benefits