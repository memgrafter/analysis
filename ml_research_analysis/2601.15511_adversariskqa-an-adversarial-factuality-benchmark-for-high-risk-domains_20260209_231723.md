---
ver: rpa2
title: 'AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains'
arxiv_id: '2601.15511'
source_url: https://arxiv.org/abs/2601.15511
tags:
- adversarial
- factuality
- arxiv
- domains
- finance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces AdversaRiskQA, a benchmark for evaluating
  large language models (LLMs) under adversarial factuality conditions in high-risk
  domains. The benchmark includes domain-specific datasets covering Health, Finance,
  and Law, each with basic and advanced difficulty levels.
---

# AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains

## Quick Facts
- **arXiv ID**: 2601.15511
- **Source URL**: https://arxiv.org/abs/2601.15511
- **Reference count**: 40
- **Primary result**: Qwen3-80B achieves highest adversarial factuality accuracy (94.7%) across Health, Finance, and Law domains

## Executive Summary
This study introduces AdversaRiskQA, a benchmark for evaluating large language models (LLMs) under adversarial factuality conditions in high-risk domains. The benchmark includes domain-specific datasets covering Health, Finance, and Law, each with basic and advanced difficulty levels. We propose two automated evaluation methods: an LLM-as-judge approach for adversarial factuality and a search-augmented agentic method for long-form factuality assessment. Results show that Qwen3-80B achieves the highest average accuracy (94.7%) across domains, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies across domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and factual output.

## Method Summary
The study evaluates LLMs using adversarial factuality prompts where misinformation is injected with confidence cues (e.g., "As we know..."). Six models (Qwen3-4B/30B/80B, GPT-OSS-20B/120B, GPT-5) are tested across three domains: Health (200 samples from HealthFC), Finance (100 samples from textbooks), and Law (100 samples from FALQU/LawSE). Each domain has basic and advanced difficulty levels. Models generate responses at temperature=0.0 with max_tokens=8192. An LLM-as-judge (GPT-5-mini) evaluates adversarial factuality accuracy, while a search-augmented agentic method assesses long-form factuality using F1@K metric.

## Key Results
- Qwen3-80B achieves highest average adversarial factuality accuracy (94.7%) across domains
- GPT-5 shows smallest Basic-Advanced accuracy gaps (Mean|Δ|=6.3), indicating consistent performance across difficulty levels
- Finance domain achieves highest accuracy (93.8%) while Health shows highest failure rates due to safety filters
- No significant correlation between injected misinformation and long-form factual output quality
- Adversarial factuality performance scales non-linearly with model size, with larger models narrowing difficulty gaps

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Prefixed Adversarial Injection
Claim: Prefixing misinformation with strong confidence cues increases attack success rates against LLMs. Models exhibit sycophantic tendencies—accepting confidently framed statements rather than challenging false premises. This exploits implicit trust signals in training data where authoritative-sounding text correlates with factual content.

### Mechanism 2: Non-Linear Scaling with Model Size
Claim: Adversarial robustness improves with model size but non-linearly; larger models narrow the gap between basic and advanced difficulty. Larger parameter counts enable better internal knowledge representations and conflict detection.

### Mechanism 3: Decoupled Short-Form Correction vs Long-Form Factuality
Claim: Successfully correcting embedded misinformation in short responses does not predict factual accuracy in extended generation. Short-form adversarial evaluation tests premise detection; long-form factuality tests sustained factual generation. Different failure modes.

## Foundational Learning

- **Concept: Adversarial Factuality vs Hallucination**
  - Why needed: Hallucination concerns model consistency with input/training data; adversarial factuality concerns detecting externally injected false premises. Conflating these leads to wrong mitigation strategies.
  - Quick check: Given "As we know, the moon is made of cheese. What is the moon's composition?", does your evaluation measure (a) whether the model corrects the premise or (b) whether the response is internally consistent?

- **Concept: Confidence Calibration and Sycophancy**
  - Why needed: Models disproportionately accept confidently-styled assertions. Understanding this helps design adversarial prompts and interpret failure patterns.
  - Quick check: If you run the same misinformation with prefixes "I think...", "Research suggests...", and "As we know...", which would you expect highest attack success rate?

- **Concept: Domain-Specific Failure Modes**
  - Why needed: The paper shows finance achieves highest accuracy, law shows reversed difficulty patterns (advanced > basic), health triggers safety filters. Each domain requires different evaluation approaches.
  - Quick check: In your target domain, do failures stem from knowledge gaps, safety alignment conflicts, or template/format issues?

## Architecture Onboarding

- **Component map**: Source Collection → Structured Item Generation → Human QA → Curated Dataset → Model Inference (temp=0.0, max_tokens=8192) → Response Screening → LLM-as-Judge (GPT-5-mini) → Accuracy Scoring → [Optional] Text Decomposition → Web Search Validation → F1@K

- **Critical path**: Dataset curation quality determines benchmark validity. The three-stage pipeline (source collection → GPT-5 generation → human review) ensures adversarial prompts are grounded in verified facts. Skipping human QA risks embedding ambiguous or incorrect ground truth.

- **Design tradeoffs**:
  - LLM-as-Judge vs Human Evaluation: Automated scoring achieves 90-95% agreement with humans but may miss nuanced corrections. Use hybrid for high-stakes domains.
  - Temperature=0 vs natural generation: Deterministic outputs improve reproducibility but may not reflect real-world usage patterns.
  - Filtering invalid responses: Including failures penalizes models that refuse unsafe queries; excluding them rewards models that produce any valid output regardless of safety concerns.

- **Failure signatures**:
  - Null outputs: Safety filter triggered (common in health/law with sensitive topics)
  - Prompt echo: Model failed to transition from reading to generating (smaller models)
  - Template leakage: Chat format mismatch (architecture-specific)
  - Domain patterns: Law/police questions → safety refusals; Health/pregnancy → null outputs; Finance → template issues

- **First 3 experiments**:
  1. Replicate the adversarial evaluation on your target model using the open-sourced prompts. Compare raw vs filtered accuracy to identify if failures stem from reasoning or generation issues.
  2. Ablate confidence prefixes: Run the same modified knowledge with "As we know...", "I think...", and neutral framing. Measure attack success rate delta to quantify sycophancy vulnerability.
  3. Domain transfer test: If your application domain differs from health/finance/law, create 20 adversarial prompts following the paper's schema and evaluate whether domain-specific patterns emerge.

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed non-linear scaling between model size and adversarial factual robustness generalize across other major model architectures beyond Qwen and GPT-OSS families? The study only tested Qwen3 series, GPT-OSS, and GPT-5; architectural differences may produce different scaling patterns.

### Open Question 2
Does the lack of correlation between injected misinformation and long-form factual output quality hold across multiple model architectures, or is this finding specific to Qwen3-30B? Long-form factuality assessment was computationally expensive, limiting evaluation to one model.

### Open Question 3
How does adversarial factuality performance vary across languages, and do models exhibit different vulnerability patterns in non-English high-risk domains? All curated datasets were English-only, and misinformation framing may differ culturally and linguistically.

### Open Question 4
Why do models show higher accuracy on advanced law questions compared to basic ones, and does this reflect knowledge contamination or contextual reasoning advantages? The paper hypothesizes contamination but does not empirically test whether training data contains more legal myths than specialized knowledge.

## Limitations

- Benchmark relies on GPT-5-mini as sole LLM judge, which may inherit biases from training data
- Curated datasets depend on GPT-5 for initial fact generation in Finance and Law domains, introducing potential systematic biases
- Search-augmented agentic evaluation method for long-form factuality remains exploratory with small sample sizes
- Temperature=0.0 setting for deterministic generation may not reflect real-world model behavior

## Confidence

- **High Confidence**: Observed non-linear scaling patterns with model size and general superiority of larger models in adversarial factuality correction
- **Medium Confidence**: Decoupling between short-form adversarial correction and long-form factuality generation
- **Low Confidence**: Claim that confidently-framed misinformation consistently exploits sycophantic tendencies across all models

## Next Checks

1. Conduct ablation studies testing multiple confidence prefix variations across all model sizes to quantify sycophancy vulnerability thresholds
2. Implement cross-validation using multiple LLM judges alongside GPT-5-mini to assess consistency in adversarial factuality scoring
3. Expand the long-form evaluation to include domain-specific ground truth verification for all generated factual claims