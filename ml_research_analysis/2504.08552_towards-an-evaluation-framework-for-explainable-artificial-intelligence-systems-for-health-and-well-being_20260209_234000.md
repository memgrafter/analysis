---
ver: rpa2
title: Towards an Evaluation Framework for Explainable Artificial Intelligence Systems
  for Health and Well-being
arxiv_id: '2504.08552'
source_url: https://arxiv.org/abs/2504.08552
tags:
- systems
- system
- https
- framework
- trust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces XAIHealth, a structured evaluation framework
  designed for explainable AI (XAI) systems in healthcare and well-being applications.
  The framework adapts a multidisciplinary base approach to focus on sequential phases
  of machine-centred analysis, human-centred assessment, and legal-ethical compliance.
---

# Towards an Evaluation Framework for Explainable Artificial Intelligence Systems for Health and Well-being

## Quick Facts
- arXiv ID: 2504.08552
- Source URL: https://arxiv.org/abs/2504.08552
- Authors: Esperança Amengual-Alcover; Antoni Jaume-i-Capó; Miquel Miró-Nicolau; Gabriel Moyà-Alcover; Antonia Paniza-Fullera
- Reference count: 19
- Primary result: XAIHealth framework evaluates explainable AI systems for healthcare using phased machine-centred analysis, human-centred assessment, and legal-ethical compliance

## Executive Summary
This paper introduces XAIHealth, a structured evaluation framework designed specifically for explainable AI (XAI) systems in healthcare and well-being applications. The framework adapts a multidisciplinary base approach into sequential phases: machine-centred analysis (focusing on algorithmic fidelity and robustness), human-centred assessment (measuring user trust), and legal-ethical compliance. A case study applying GradCAM to a pneumonia detection model demonstrates the framework's practical application. The approach aims to ensure AI systems are interpretable, trustworthy, and compliant with regulations such as GDPR and the EU AI Act.

## Method Summary
The XAIHealth framework employs a phased evaluation approach starting with Phase 0 (AI model and XAI method selection with legal compliance checks), progressing to Phase 1 (machine-centred analysis using Local Lipschitz Estimate for robustness and fidelity checks), and culminating in Phase 2 (human-centred assessment via behavioral trust measurement). The case study implemented this framework using a ResNet18 CNN trained on 2048 chest X-ray images from Hospital Universitari Son Espases, with GradCAM for heatmaps and LLE for robustness evaluation. The framework uses dependency chains where each phase must pass thresholds before proceeding to the next, ensuring resources aren't wasted on human testing when machine explanations are unreliable.

## Key Results
- LLE achieved mean score of 0.082 (±0.108) in case study, within acceptable threshold for robustness
- Trust F1-Score reached only 0.0952 mean in case study, triggering return to Phase 0 for iteration
- Framework successfully demonstrated dependency logic: Phase 1 LLE results determined readiness for Phase 2 trust assessment
- ALTAI checklist integration ensures legal-ethical compliance throughout evaluation process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential evaluation prevents wasting resources on human-centred testing when machine-centred explanations are unreliable
- Mechanism: The framework enforces dependency chains—fidelity and robustness must pass thresholds before human trust assessment begins. If Phase 1 fails, return to Phase 0 to diagnose whether the AI model or XAI method is at fault
- Core assumption: Machine-centred metrics (particularly robustness) are valid proxies for explanation quality that humans would eventually perceive
- Evidence anchors: "if an explanation lacks fidelity to the underlying causes of an AI prediction, the user's trust in that explanation becomes irrelevant, as the prediction itself may be incorrect"; Phase 1 results determine readiness for Phase 2
- Break condition: If no validated post-hoc fidelity metric exists for your domain, the sequential logic collapses—you cannot reliably pass Phase 1

### Mechanism 2
- Claim: Local Lipschitz Estimate (LLE) provides a validated robustness signal that minor input perturbations produce stable explanations
- Mechanism: LLE quantifies local smoothness of the explanation function. Values closer to 0 indicate higher robustness. The paper reports LLE was the only metric passing both robustness tests in meta-evaluation
- Core assumption: Robustness of explanations correlates with their usefulness to end-users, not just mathematical stability
- Evidence anchors: "LLE is the only metric that passed both robustness tests, making it our primary criterion"; Case study achieved mean LLE of 0.082 (±0.108), within acceptable threshold
- Break condition: If your XAI method produces sparse or binary attributions (not continuous), LLE may not apply directly

### Mechanism 3
- Claim: Behavioral trust measurement combining prediction accuracy with user reliance provides objective trust signals
- Mechanism: The framework adopts a confusion-matrix-inspired approach where trust and performance interact (true trust, false trust, etc.), enabling metrics like Precision and Recall adapted for trust evaluation
- Core assumption: User reliance behavior (accepting/rejecting predictions) is a valid proxy for internal trust attitudes
- Evidence anchors: "trust measure that integrates performance and trust data using a confusion matrix, resulting in four distinct measures"; Case study trust results showed low F1-Score (0.0952 mean), triggering return to Phase 0
- Break condition: If users cannot observe predictions and explanations in a controlled interface, behavioral trust cannot be measured cleanly

## Foundational Learning

- Concept: **Fidelity vs. Robustness in XAI metrics**
  - Why needed here: Phase 1 requires distinguishing whether your XAI method produces *correct* explanations (fidelity) vs. *stable* explanations (robustness). These are orthogonal—a method can be robustly wrong
  - Quick check question: Can you name one post-hoc fidelity metric and explain why the paper considers them "unreliable for real-world contexts"?

- Concept: **Lipschitz continuity for explanation stability**
  - Why needed here: Understanding LLE requires grasping that it measures the maximum rate of change in explanation output relative to input perturbation. Lower values mean smoother, more stable explanations
  - Quick check question: If an explanation function has LLE = 0.5, what does this imply about explanation changes when input is perturbed slightly?

- Concept: **Behavioral vs. attitudinal trust measurement**
  - Why needed here: Phase 2 distinguishes what users *say* they trust (surveys) from what they *do* (reliance behavior). The framework prioritizes behavioral measures
  - Quick check question: Why might user self-reported trust differ from their actual reliance on AI predictions?

## Architecture Onboarding

- Component map: Phase 0 (Pre-Evaluation): AI Model + XAI Method Selection → Data/Legal Compliance (ALTAI #3, #5, #6) → Phase 1 (Machine-Centred): Robustness via LLE → Fidelity checks → (ALTAI #2, #4) → [if pass] Phase 2 (Human-Centred): Interface → Behavioral Trust Measurement → (ALTAI #1) → Operation: Ongoing monitoring per EU AI Act high-risk requirements

- Critical path: Phase 1 LLE threshold → Phase 2 trust metrics. Failure at either point requires return to Phase 0

- Design tradeoffs:
  - Rigor vs. iteration speed: Strict LLE thresholds may require multiple XAI method iterations
  - Behavioral vs. subjective trust: Behavioral measures require controlled user studies; subjective scales are faster but less objective
  - Metric generalization: LLE validated for GradCAM on CNNs; untested for other XAI methods

- Failure signatures:
  - LLE > 0.1 with high variance: Explanation instability
  - Trust F1-Score < 0.2 despite passing Phase 1: Interface or explanation clarity issues, not model quality
  - Users accepting incorrect predictions: Over-trust / explanation misleading

- First 3 experiments:
  1. Implement LLE computation for your XAI method on a held-out test set; establish baseline robustness score before any user testing
  2. Build minimal interface displaying prediction + GradCAM explanation; pilot with 2-3 domain experts to test trust measurement feasibility
  3. Run ALTAI self-assessment checklist on your current system to identify compliance gaps before Phase 0 completion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the XAIHealth framework be successfully validated and applied to AI contexts outside of the medical imaging domain used in the case study?
- Basis in paper: The authors state in the conclusion that future work aims to "apply the framework to additional case studies to validate its effectiveness and identify potential enhancements"
- Why unresolved: The paper currently provides evidence from only a single pneumonia detection case study involving ResNet18 and GradCAM
- What evidence would resolve it: Successful application and evaluation reports of the framework in diverse domains (e.g., finance, autonomous driving) or different medical data types (e.g., electronic health records)

### Open Question 2
- Question: What specific monitoring and improvement tasks are required during the "XAI System Operation" phase to ensure continued legal and ethical compliance?
- Basis in paper: The conclusion notes the need to "elaborate on the tasks necessary during the system operation phase, specifically concerning monitoring and improvement"
- Why unresolved: The current paper defines the final operation phase conceptually but leaves the specific protocols for post-deployment evaluation undefined
- What evidence would resolve it: A detailed methodology or set of protocols for ongoing performance and compliance monitoring integrated into the framework

### Open Question 3
- Question: How can the framework incorporate validated post-hoc fidelity metrics, given the authors' finding that current fidelity measures are unreliable?
- Basis in paper: Section 4.3 states that existing post-hoc fidelity metrics are "unsuitable for real-world applications" due to a lack of verification, leading the authors to rely primarily on robustness (Local Lipschitz Estimate)
- Why unresolved: Fidelity is described as the "most crucial metric," yet the framework currently lacks a reliable method to measure it, creating a potential gap in the Machine-Centred Analysis phase
- What evidence would resolve it: The development and integration of a fidelity metric that passes the meta-evaluation criteria described in the paper (e.g., the tests by Hedström et al., 2023)

## Limitations

- Limited empirical validation beyond a single case study with GradCAM on ResNet18 for pneumonia detection
- Heavy reliance on one robustness metric (LLE) with unknown applicability to non-continuous attribution methods
- Potential mismatch between mathematical robustness and actual user trust in real-world deployment

## Confidence

- **High confidence**: Structured methodology and clear dependency logic between evaluation phases
- **Medium confidence**: LLE robustness metric's generalizability beyond GradCAM/ResNet18 case study
- **Low confidence**: Behavioral trust metrics' reliability without broader user study validation

## Next Checks

1. Test LLE metric across at least three different XAI methods (e.g., SHAP, LIME, Integrated Gradients) on the same healthcare task to verify generalizability
2. Conduct user studies with 30+ participants across multiple healthcare domains to validate behavioral trust metrics and identify patterns in low trust scores
3. Perform sensitivity analysis on LLE threshold values (currently 0.1) to determine optimal cutoffs for different types of medical explanations