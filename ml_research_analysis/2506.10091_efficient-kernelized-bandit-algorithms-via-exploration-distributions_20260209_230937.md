---
ver: rpa2
title: Efficient kernelized bandit algorithms via exploration distributions
arxiv_id: '2506.10091'
source_url: https://arxiv.org/abs/2506.10091
tags:
- regret
- exploration
- algorithms
- bounds
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GP-Generic, a flexible framework for kernelized
  bandit algorithms based on a novel concept called exploration distributions. The
  framework allows various randomized algorithms to be expressed as instantiations
  of GP-Generic by carefully choosing exploration distributions that control the exploitation-exploration
  tradeoff.
---

# Efficient kernelized bandit algorithms via exploration distributions

## Quick Facts
- arXiv ID: 2506.10091
- Source URL: https://arxiv.org/abs/2506.10091
- Reference count: 40
- Key outcome: Introduces GP-Generic framework achieving O(γ_T√T) regret with improved computational efficiency over Thompson Sampling

## Executive Summary
This paper presents GP-Generic, a novel framework for kernelized bandit algorithms based on exploration distributions. The framework provides a unified approach where various randomized algorithms can be expressed as instantiations of GP-Generic by carefully choosing exploration distributions that control the exploitation-exploration tradeoff. The key innovation is that these exploration distributions enable the use of traditional concentration bounds in one-dimensional space rather than high-dimensional vector spaces, leading to improved regret bounds. The paper establishes a generic regret bound for GP-Generic and demonstrates that several concrete algorithms including Simple-UCB, Simple-Gaussian, and Simple-Bernoulli can be realized within this framework, all achieving O(γ_T√T) regret bounds. Experimental results show that these algorithms outperform existing methods like IGP-UCB and GP-TS in practice.

## Method Summary
The method involves computing the posterior mean f̂_{t-1} and uncertainty bonus g_{t-1} via kernel ridge regression, then selecting arms by maximizing f̂_{t-1}(x) + w_t·g_{t-1}(x) where w_t is sampled from a chosen exploration distribution P_{w,t}. The framework supports various distributions including deterministic (Simple-UCB with w_t ≡ 1), Bernoulli (Simple-Bernoulli mixing exploration/exploitation), Gaussian (Simple-Gaussian for computational efficiency), and categorical distributions. The regret analysis relies on self-normalized concentration bounds with constant failure probability δ = 0.5, enabled by the exploration distribution's randomness ensuring optimism. Experiments use Gaussian RBF kernel with T = 1000 rounds, 25 random seeds, and compare against IGP-UCB and GP-TS baselines on synthetic functions and a Perovskite real-world dataset.

## Key Results
- Establishes GP-Generic framework achieving O(γ_T√T) regret bounds
- Four concrete algorithms (Simple-UCB, Simple-Gaussian, Simple-Bernoulli, Simple-Categorical) realized within framework
- Simple-Gaussian saves a √d factor compared to GP-TS by sampling from 1D Gaussian instead of d-dimensional
- Experimental results show improved performance over IGP-UCB and GP-TS on both synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1: Scalar Perturbation of Posterior Uncertainty
The algorithm constructs an acquisition function f̃_t(x) = f̂_{t-1}(x) + w_t g_{t-1}(x), where f̂ is the empirical mean, g is the uncertainty bonus, and w_t is a scalar random variable drawn from a chosen exploration distribution P_{w,t}. This replaces high-dimensional function sampling with scalar scaling, reducing variance and computational overhead while preserving exploration guarantees. The uncertainty function g_{t-1}(x) must accurately capture confidence intervals such that scaling by scalar w_t is sufficient to span the necessary exploration space.

### Mechanism 2: Controlled Optimism via Anti-Concentration Constraints
The framework imposes P(w_s ≥ 1) = C_{1,s} ≠ 0 to ensure the perturbed function f̃_t exceeds the true function f* at the optimal point x* with constant probability. This guarantees the algorithm selects the optimal arm frequently enough, preventing it from "giving up" on the optimal region due to noise. The condition ensures the algorithm remains "optimistic in the face of uncertainty" despite using looser concentration bounds.

### Mechanism 3: Constant Failure Probability Concentration
Unlike standard UCB which requires δ ≈ 1/T for uniform high-probability bounds, this framework applies Theorem 1 with constant δ = 0.5. The regret analysis relies on expectation over w_t rather than uniform high-probability bounds on noise history. The random variable w_t's exploration introduces randomness that washes out the effect of any single "bad" confidence bound realization, compensating for the loose concentration.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS)**: The reward function f* lives in an RKHS where the kernel K defines smoothness and similarity between arms. This is crucial for understanding why f̂_{t-1} is a valid estimator and why g_{t-1} represents uncertainty. Quick check: If two arms x and x' are close in input space but have orthogonal feature representations in the RKHS, how does the kernel matrix V_t reflect their relationship?

- **Maximum Information Gain (γ_T)**: This complexity measure replaces "dimension" in the regret bound, determining how difficult it is to learn the function. The paper's regret bound is linear in γ_T. Quick check: For a Squared Exponential kernel, does γ_T grow logarithmically or polynomially with T? How does this affect regret scaling?

- **Martingale Difference Sequences**: The rewards Y_t are dependent on previous pulls, requiring the filtration F_{t-1} and the fact that noise ε_t is a martingale difference sequence to apply Theorem 1. Standard i.i.d. concentration inequalities cannot be used directly on the sum of rewards. Quick check: Why can't we use standard i.i.d. concentration inequalities directly on the sum of rewards in a bandit setting?

## Architecture Onboarding

- **Component map**: History Buffer -> Kernel Engine -> Sampler -> Optimizer
- **Critical path**: Computing g_{t-1}(x) involves the term ||V_{t-1}^{-1/2} k(x)||. For large t, inverting or decomposing the kernel matrix is the bottleneck (O(t³) naive, O(t²) optimized).
- **Design tradeoffs**: Simple-UCB is deterministic (always w_t=1), guaranteeing theoretical bounds but potentially sticking to local optima. Simple-Bernoulli introduces randomness for better practical performance ("adaptive exploitation") but requires tuning p_t. Discretization is needed for continuous X.
- **Failure signatures**: Stagnation occurs if p_t in Simple-Bernoulli is too low, causing premature exploitation. Noise sensitivity arises if Simple-Gaussian explores negative weights too often in high-noise regimes, delaying convergence.
- **First 3 experiments**: 1) Implement Simple-UCB and verify it matches standard GP-UCB regret on Holder Table to ensure kernel inversion and optimization are correct. 2) Compare Simple-Bernoulli with fixed p=0.5 against annealing schedules (p_t decreasing from 0.5 to 0.25) to test adaptive exploitation claims. 3) Benchmark time per iteration of Simple-Gaussian vs standard Thompson Sampling to quantify computational efficiency gains.

## Open Questions the Paper Calls Out

- **Can data-dependent exploration distributions achieve optimal O(√T·γ_T) regret bounds, or is the √γ_T gap fundamental?**: The paper achieves Õ(γ_T√T) matching prior work, but arm elimination algorithms achieve Õ(√T·γ_T) using data-independent sampling, suggesting a possible inherent tradeoff.

- **Can the GP-Generic framework provide high-probability regret guarantees rather than in-expectation bounds?**: Current analysis uses constant failure probability δ=0.5, whereas high-probability guarantees require different analytical techniques.

- **How can the exploration distribution framework be extended to time-varying kernelized bandits?**: The current analysis assumes fixed reward function f*, but time-varying settings require different regret definitions and may need distributions that adapt to concept drift.

- **What is the optimal adaptive schedule for exploration probabilities p_t in Simple-Bernoulli?**: The paper uses fixed schedules but notes that p_t can be adjusted based on seen information, without providing a principled method.

## Limitations

- The paper does not specify exact values for critical parameters RKHS norm D and sub-Gaussian parameter R, which directly affect exploration bonus scale and could influence regret bounds.
- Kernel bandwidth (lengthscale) selection is not specified, significantly impacting smoothness assumptions and information gain γ_T.
- Baseline implementation details for IGP-UCB and GP-TS are not provided, making exact replication difficult.

## Confidence

- **High confidence**: Theoretical framework and regret bound derivations are mathematically sound given stated assumptions. Scalar perturbation mechanism reducing computational complexity is clearly explained and valid.
- **Medium confidence**: Experimental results showing improved performance over baselines are plausible, but exact reproduction depends on unspecified parameters. O(γ_T√T) regret bounds appear consistent with theory.
- **Low confidence**: Without specific RKHS norm D and sub-Gaussian parameter R values, uncertainty remains about whether experiments exactly match theoretical analysis. Perovskite dataset details are insufficient for full replication.

## Next Checks

1. **Parameter sensitivity analysis**: Run Simple-UCB with different values of D (0.1, 1, 10) and R (0.01, 0.1, 1) to determine which combinations produce results closest to reported experiments, then fix these values for all subsequent experiments.

2. **Implementation fidelity test**: Verify Simple-UCB implementation exactly matches standard GP-UCB on a simple benchmark. If performance differs, identify the source of discrepancy before testing other variants.

3. **Runtime benchmark**: Measure wall-clock time per iteration for Simple-Gaussian versus standard GP-TS implementation on increasing problem sizes (t = 100, 500, 1000) to empirically validate claimed computational efficiency gains from scalar sampling.