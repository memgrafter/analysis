---
ver: rpa2
title: Effective and Efficient Schema-aware Information Extraction Using On-Device
  Large Language Models
arxiv_id: '2505.14992'
source_url: https://arxiv.org/abs/2505.14992
tags:
- extraction
- schema
- information
- dlisc
- schemas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual-LoRA with Incremental Schema Caching
  (DLISC), a two-stage on-device information extraction framework that uses specialized
  LoRA adapters for schema identification and extraction, along with caching to reduce
  redundant computation. DLISC significantly outperforms RAG-based baselines on CrossNERAI
  and DuEE-Fin datasets, achieving F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B)
  on CrossNERAI, and maintaining high precision and F1 in both schema identification
  and extraction on DuEE-Fin.
---

# Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models

## Quick Facts
- arXiv ID: 2505.14992
- Source URL: https://arxiv.org/abs/2505.14992
- Reference count: 5
- This paper introduces Dual-LoRA with Incremental Schema Caching (DLISC), a two-stage on-device information extraction framework that uses specialized LoRA adapters for schema identification and extraction, along with caching to reduce redundant computation. DLISC significantly outperforms RAG-based baselines on CrossNER_AI and DuEE-Fin datasets, achieving F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B) on CrossNER_AI, and maintaining high precision and F1 in both schema identification and extraction on DuEE-Fin. The incremental schema caching mechanism reduces latency from 1.38/6.02 seconds to 0.66/4.59 seconds per sample, substantially improving efficiency.

## Executive Summary
This paper presents DLISC, a two-stage on-device information extraction framework that decomposes schema-aware IE into schema identification and schema-aware extraction tasks. The framework uses separate LoRA adapters for each stage and implements incremental schema caching to reduce redundant computation. DLISC achieves significant improvements in both accuracy and efficiency compared to RAG-based baselines on CrossNER_AI and DuEE-Fin datasets, with F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B) on CrossNER_AI. The caching mechanism reduces latency by 52-78% per sample, making it practical for edge deployment.

## Method Summary
DLISC employs a two-stage approach: first using an Identification LoRA module to retrieve relevant schemas for a given query, then using an Extraction LoRA module to perform information extraction conditioned on those schemas. The framework merges separate LoRA adapters with a frozen base LLM at inference time, creating specialized models for identification (θ'_I) and extraction (θ'_E). An incremental schema caching mechanism stores computed key-value attention states for schemas and extraction meta prompts, enabling reuse across queries to reduce latency. The system processes queries through the identification stage to obtain matched schemas, checks these against a cache pool, and then performs extraction using cached schema representations when available.

## Key Results
- Achieved F1 scores of 0.4179 (Llama-3.2-1B) and 0.4311 (Qwen2.5-3B) on CrossNER_AI dataset
- Maintained high precision and F1 in both schema identification and extraction on DuEE-Fin dataset
- Reduced latency from 1.38/6.02 seconds to 0.66/4.59 seconds per sample through incremental schema caching

## Why This Works (Mechanism)

### Mechanism 1: Task-Specialized LoRA Adapters
Separate LoRA adapters for identification and extraction improve overall IE performance by allowing independent optimization for schema retrieval and extraction tasks. Two parameter sets (θ_I, θ_E) merge with the same base LLM (θ) to create specialized models (θ'_I, θ'_E), enabling independent learning for each task. This specialization matters more than retrieval method, as validated by related work.

### Mechanism 2: Schema-Aware Context Reduction
Selecting only relevant schemas before extraction reduces input length and improves accuracy by preventing hallucination from irrelevant schemas. The identification LoRA outputs matched schemas S, and the extraction prompt contains only S + query Q rather than the full schema library. This context reduction is critical but requires high identification precision to avoid contamination from irrelevant schemas.

### Mechanism 3: Incremental KV-State Reuse
Caching previously computed schema representations reduces inference latency by reusing pre-computed attention states. When a matched schema is found in cache, its computed states are retrieved; only new schemas trigger full computation. The extraction meta prompt is cached on first appearance, building on prompt cache and KV cache principles but applied specifically to schema caching.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: DLISC relies on parameter-efficient fine-tuning where LoRA adapters (θ_I, θ_E) are merged with frozen base LLM weights at inference time
  - Quick check question: Can you explain why LoRA adds trainable low-rank matrices to frozen weights rather than fine-tuning all parameters?

- Concept: **KV Cache in Transformers**
  - Why needed here: Incremental Schema Caching builds on the principle of storing key-value attention states to avoid recomputation across inference steps
  - Quick check question: What attention components are cached, and why does this not affect output determinism?

- Concept: **Schema-based Information Extraction**
  - Why needed here: The entire framework assumes structured extraction targets defined by schemas (entity types, event types with roles)
  - Quick check question: Given a news article about a product launch, what schema elements would you define for event extraction?

## Architecture Onboarding

- Component map:
  - Base LLM (θ): Llama-3.2-1B / Qwen2.5-3B / TinyLlama-1.1B (frozen)
  - Identification LoRA (θ_I): Fine-tuned for schema retrieval
  - Extraction LoRA (θ_E): Fine-tuned for structured extraction
  - Schema Cache Pool: Stores computed KV states for schemas
  - Meta Prompts: M_I (identification), M_E (extraction, cached)

- Critical path:
  1. Query Q → θ'_I with M_I → Matched Schemas S
  2. Check S against cache pool
  3. Cache hit: Retrieve cached S; Cache miss: Compute and store
  4. S (cached) + Q → θ'_E with cached M_E → Structured result R

- Design tradeoffs:
  - Cache size vs. latency: Larger cache pools reduce misses but increase memory footprint on constrained devices
  - Identification precision vs. extraction quality: Low-precision identification propagates errors; over-conservative matching limits extraction coverage
  - LoRA rank selection: Higher rank improves specialization but increases merge overhead

- Failure signatures:
  - High cache miss rate: Schema distribution too diverse; latency approaches Dual-LoRA baseline
  - Identification F1 collapse: Extraction receives wrong schemas; end-to-end F1 degrades sharply
  - Memory overflow: Cache pool exceeds device limits; system must evict entries or crash

- First 3 experiments:
  1. Validate cache hit rate on your schema distribution—if below 60%, caching benefits are marginal; analyze schema frequency patterns
  2. Ablate identification precision by injecting noise into matched schemas; measure extraction F1 sensitivity to upstream errors
  3. Profile memory usage with incremental cache sizes; identify the knee point where latency gains plateau against memory cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DLISC framework generalize effectively to other parameter-efficient fine-tuning methods beyond LoRA, such as Prefix Tuning or Parallel Adapters?
- Basis in paper: Section 5 (Limitations) states that the framework currently employs only LoRA adapters and that it "remains to be explored whether the approach can generalize to other adapter types."
- Why unresolved: The specific dual-stage architecture and caching mechanism have only been validated using LoRA, leaving the compatibility with other adapter architectures unknown
- What evidence would resolve it: Comparative experimental results implementing DLISC using Prefix Tuning or Series Adapters on the CrossNER_AI and DuEE-Fin datasets

### Open Question 2
- Question: What are the actual latency and energy consumption characteristics of DLISC when deployed on physical resource-constrained edge devices?
- Basis in paper: Section 5 (Limitations) notes that the authors were "unable to deploy on real edge devices due to computational resource constraints," relying instead on standard computational environments for efficiency testing
- Why unresolved: The reported efficiency gains (latency reduction) were measured per sample in a general setting, not accounting for the specific memory or hardware bottlenecks of true edge deployment
- What evidence would resolve it: Benchmark results measuring inference time, memory footprint, and power consumption running DLISC on physical mobile or edge hardware

### Open Question 3
- Question: How does the DLISC framework perform in complex multilingual and cross-lingual information extraction scenarios?
- Basis in paper: Section 5 (Limitations) acknowledges that the current implementation "lacks support for more complex multilingual and cross-lingual scenarios"
- Why unresolved: The experiments were limited to English (CrossNER_AI) and Chinese (DuEE-Fin) datasets independently, without evaluating cross-lingual transfer or mixed-language inputs
- What evidence would resolve it: Performance evaluation on multilingual IE benchmarks to assess robustness in handling diverse linguistic inputs within the same system

## Limitations

- Critical implementation details are unspecified: exact meta-prompt templates and LoRA training configurations are missing, making replication uncertain
- Evaluation scope is narrow with only two datasets and limited model variations, lacking analysis of schema library size impact and cross-domain generalization
- The caching mechanism's stability assumption—that schema representations remain stable across queries—is asserted but not empirically validated across diverse query contexts

## Confidence

**High Confidence Claims:**
- Dual-LoRA architecture with separate identification and extraction adapters is technically feasible and improves over single-adapter approaches
- Incremental schema caching can reduce inference latency through KV state reuse
- The two-stage decomposition of schema-aware IE into identification and extraction is a valid design pattern

**Medium Confidence Claims:**
- The specific performance numbers (F1 scores of 0.4179/0.4311 on CrossNER_AI, latency reductions from 1.38/6.02s to 0.66/4.59s) are accurate for the reported experimental conditions
- Schema-aware context reduction improves accuracy by preventing hallucination from irrelevant schemas
- The framework works as described on CrossNER_AI and DuEE-Fin datasets

**Low Confidence Claims:**
- The framework generalizes well to other domains, schema libraries, and LLM sizes beyond those tested
- The caching mechanism's stability assumption holds across diverse query contexts and schema distributions
- The performance benefits scale proportionally with larger schema libraries or more complex extraction tasks

## Next Checks

1. **Cache Stability Analysis**: Implement a systematic evaluation measuring cache hit rates and extraction accuracy across varying query distributions. Track whether cached schema representations maintain extraction quality when reused across semantically different queries. This validates the core assumption that schema KV states are context-independent.

2. **Identification Precision Sensitivity**: Conduct ablation studies where the identification stage's precision is artificially degraded by introducing controlled noise into matched schemas. Measure the cascading impact on end-to-end extraction F1 to quantify how sensitive the two-stage architecture is to upstream errors.

3. **Memory-Performance Tradeoff Profiling**: Characterize the relationship between schema library size, cache hit rate, and memory usage. Determine the inflection point where additional caching provides diminishing latency returns while memory costs continue to increase. This is essential for validating on-device deployment claims across different hardware constraints.