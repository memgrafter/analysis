---
ver: rpa2
title: 'LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges'
arxiv_id: '2506.10022'
source_url: https://arxiv.org/abs/2506.10022
tags:
- llms
- code
- malicious
- jailbreak
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MalwareBench, a benchmark dataset of 3,520
  jailbreaking prompts for malicious code generation, designed to evaluate LLM robustness
  against such threats. The dataset is based on 320 manually crafted malicious code
  generation requirements, covering 11 jailbreak methods and 29 code functionality
  categories.
---

# LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges

## Quick Facts
- arXiv ID: 2506.10022
- Source URL: https://arxiv.org/abs/2506.10022
- Authors: Haoyang Li; Huan Gao; Zhiyuan Zhao; Zhiyu Lin; Junyu Gao; Xuelong Li
- Reference count: 13
- Mainstream LLMs show limited ability to reject malicious code-generation requests, with average rejection rates of 60.93% dropping to 39.92% when combined with jailbreak attacks

## Executive Summary
This paper introduces MalwareBench, a benchmark dataset of 3,520 jailbreaking prompts designed to evaluate LLM robustness against malicious code generation threats. The dataset is based on 320 manually crafted malicious code generation requirements covering 11 jailbreak methods and 29 code functionality categories. Experiments reveal that mainstream LLMs exhibit significant vulnerabilities to such attacks, with the Benign Expression method proving particularly effective at bypassing safety filters by replacing malicious terminology with harmless vocabulary.

## Method Summary
The study evaluates 29 LLMs (both general and code-specific) using direct prompts and 11 black-box jailbreaking methods. The MalwareBench dataset contains 320 base malicious requirements mutated into 3,520 attack prompts. Target LLMs are queried with these prompts using specific hyperparameters (temperature=0.9, top_p=0.95 for open-source). Responses are evaluated by LLM-based judges (GPT-4o as primary) using a binary refusal indicator and a 1-4 quality scale measuring the harmfulness of generated code.

## Key Results
- Average rejection rate for malicious content is 60.93%, dropping to 39.92% when combined with jailbreak attack algorithms
- Benign Expression jailbreak method achieves the lowest refusal rate (31.92%) and highest average score (2.25), making maliciousness "less detectable"
- LLMs show stronger defense against detailed malicious requirements (66.70% refusal) compared to rough, generic requests (47.49% refusal)

## Why This Works (Mechanism)

### Mechanism 1
Replacing malicious terminology with harmless vocabulary significantly lowers LLM rejection rates by bypassing keyword-based safety filters. The model's safety training over-indexes on specific lexical cues rather than deep functional intent analysis.

### Mechanism 2
LLMs exhibit stronger defense against detailed malicious requirements due to increased semantic context triggering safety mechanisms more effectively than vague requests that might be mistaken for benign educational inquiries.

### Mechanism 3
External safety measures may reduce intrinsic model robustness by shifting the burden of defense away from the base model weights, potentially creating "security drift" in the model's intrinsic safety alignment.

## Foundational Learning

- **Concept: Black-box Jailbreaking (Template vs. Rewriting)**
  - Why needed here: The paper focuses on practical threat vectors for public APIs using black-box methods that require only prompts, not model access
  - Quick check question: Does the attack method require gradient access to the model's weights, or can it be executed via a standard chat interface?

- **Concept: Refusal Rate vs. Quality Score**
  - Why needed here: A model might fail by refusing or by generating low-quality irrelevant code - understanding this distinction is critical for interpreting benchmark results
  - Quick check question: If a model refuses 0% of requests but outputs only gibberish, is it "unsafe" according to this benchmark's scoring logic?

- **Concept: Passive Defense**
  - Why needed here: Small models often output irrelevant content not by choice but by lack of capacity, preventing misinterpretation of low quality scores as deliberate safety mechanisms
  - Quick check question: Is the model refusing due to ethical alignment, or simply because it cannot understand the complex instruction?

## Architecture Onboarding

- **Component map:** Seed Corpus (320 requirements) -> Mutation Engine (Qwen-Turbo, 11 methods) -> Target LLM -> Judge System (GPT-4o)
- **Critical path:** Benign Expression mutation -> Target LLM -> Judge Quality Score, exposing the highest vulnerability
- **Design tradeoffs:** Judge Model Selection (GPT-4o for accuracy vs. cost/closed-source dependency) vs. Single Mutation Model (Qwen-Turbo for consistency vs. attack style diversity)
- **Failure signatures:** Passive Defense (small models return irrelevant code), Pseudo-code Hazard (large models generate harmful pseudo-code without full implementation)
- **First 3 experiments:** 1) Baseline Refusal testing raw requirements, 2) Benign Expression Stress Test measuring refusal rate drop, 3) Granularity Comparison testing Rough vs. Detailed prompt susceptibility

## Open Questions the Paper Calls Out

1. **Attack Diversity Generalization:** Does the Qwen-Turbo generator model's specific performance bias influence the generalizability of robustness findings across different LLMs? Only one mutation model was used, potentially limiting attack diversity.

2. **White-box vs. Black-box Correlation:** How does robustness against black-box attacks correlate with vulnerability to white-box gradient-based attacks? The study restricted methodology to black-box attacks, leaving internal susceptibility unexplored.

3. **Intrinsic vs. External Safety:** Does integration of external safety guardrails reduce intrinsic safety alignment? The observation is correlational, potentially reflecting other factors like deliberate utility-safety trade-offs.

## Limitations

- Single mutation model (Qwen-Turbo) may bias attack diversity and limit generalizability of findings
- Distinction between "Rough" and "Detailed" requirements relies on implicit categorization logic not fully specified
- Correlation between external safety measures and reduced intrinsic robustness could reflect other factors like deliberate trade-offs

## Confidence

**High Confidence:** Core empirical finding of limited LLM ability to reject malicious code requests (60.93% rejection rate) is well-supported by systematic testing across 29 models with 11 jailbreak methods.

**Medium Confidence:** Mechanism explaining Benign Expression effectiveness is plausible but relies on inference about model internals rather than direct evidence of safety alignment mechanisms.

**Low Confidence:** Hypothesis that external safety measures reduce intrinsic robustness represents correlational observation that could reflect other factors beyond training negligence.

## Next Checks

1. **Judge Reliability Validation:** Test the LLM judge pipeline against a manually annotated subset to verify safety filters aren't breaking evaluation and quality scores align with human expectations.

2. **Attack Diversity Testing:** Apply the benchmark using different mutation models (GPT-4o or Claude) to assess whether Qwen-Turbo's attack style biases results or misses attack vectors.

3. **Granularity Control Experiment:** Systematically vary prompt detail levels in controlled experiments to verify ambiguity truly serves as cloaking mechanism versus other factors.