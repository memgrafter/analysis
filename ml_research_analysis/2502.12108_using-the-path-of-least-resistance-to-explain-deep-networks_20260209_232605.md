---
ver: rpa2
title: Using the Path of Least Resistance to Explain Deep Networks
arxiv_id: '2502.12108'
source_url: https://arxiv.org/abs/2502.12108
tags:
- geodesic
- path
- gradients
- points
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a key limitation of Integrated Gradients\
  \ (IG), where straight-line paths can lead to flawed attributions due to high gradients\
  \ in irrelevant regions. To address this, the authors propose Geodesic Integrated\
  \ Gradients (GIG), which computes attributions by integrating gradients along geodesic\
  \ paths on a Riemannian manifold defined by the model\u2019s gradients."
---

# Using the Path of Least Resistance to Explain Deep Networks

## Quick Facts
- **arXiv ID:** 2502.12108
- **Source URL:** https://arxiv.org/abs/2502.12108
- **Authors:** Sina Salek; Joseph Enguehard
- **Reference count:** 13
- **Primary Result:** Geodesic Integrated Gradients (GIG) outperforms standard Integrated Gradients (IG) on both synthetic and real-world data, achieving higher AUC for Comprehensiveness (0.27 vs 0.21) and Log-Odds (1.44 vs 1.25) on Pascal VOC 2012.

## Executive Summary
This paper identifies a key limitation of Integrated Gradients (IG): straight-line paths can lead to flawed attributions due to high gradients in irrelevant regions. To address this, the authors propose Geodesic Integrated Gradients (GIG), which computes attributions by integrating gradients along geodesic paths on a Riemannian manifold defined by the model's gradients. Two methods for approximating geodesics are introduced: a k-NN-based approach for simpler models and a Stochastic Variational Inference-based method for complex ones. A new axiom, Strong Completeness, is proposed and proven to be uniquely satisfied by GIG. Experiments on both synthetic (half-moons) and real-world (Pascal VOC 2012) data demonstrate GIG's superiority over existing methods.

## Method Summary
The method treats the input space as a Riemannian manifold, where the metric tensor is defined by the model's Jacobian. Attributions are computed by integrating gradients along geodesics (shortest paths) on this manifold, which are found using either k-NN graph search or Stochastic Variational Inference (SVI). The approach aims to avoid high-gradient regions unless strictly necessary, finding the "path of least resistance." A new axiom, Strong Completeness, is introduced and proven to be uniquely satisfied by GIG, preventing artificial cancellation of noisy attributions.

## Key Results
- On Pascal VOC 2012, GIG achieves an AUC of 0.27 for Comprehensiveness and 1.44 for Log-Odds, outperforming IG (0.21 and 1.25, respectively).
- On synthetic half-moons data, GIG (kNN) achieves a Purity of 0.531, compared to IG's 0.487.
- GIG successfully addresses the issue of straight-line paths crossing irrelevant regions of high gradient sensitivity, as demonstrated in visual comparisons on half-moons data.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard straight-line integration misattributes importance when the path crosses irrelevant regions of high gradient sensitivity.
- **Mechanism:** The method replaces the Euclidean straight line with a geodesic path on a Riemannian manifold. The manifold is defined by a metric tensor induced by the model's Jacobian ($G_x = J_x^T J_x$), causing the path's "length" to depend on the gradient magnitude. By minimizing this length (Eq. 6), the path automatically avoids high-gradient regions unless strictly necessary, finding the "path of least resistance."
- **Core assumption:** The model's Jacobian is Lipschitz continuous (Hessian is bounded), ensuring local neighborhoods are approximately Euclidean (Section 2.2).
- **Evidence anchors:**
  - [abstract] "Treats the input space as a Riemannian manifold... computing attributions by integrating gradients along geodesics."
  - [page 5] Eq. 5 defines the Riemannian metric; Eq. 6 defines path length based on gradient magnitude.
  - [corpus] Neighbors like "Path-Weighted Integrated Gradients" suggest active research in modifying integration paths, though specific Riemannian mechanisms are unique to this paper.
- **Break condition:** If the gradient landscape is flat (zero curvature), the geodesic collapses to a straight line, rendering the mechanism redundant.

### Mechanism 2
- **Claim:** Standard "Completeness" ($\sum A_i = f(x)-f(x')$) allows positive and negative attributions to cancel out, hiding noise.
- **Mechanism:** The paper introduces **Strong Completeness** ($\sum |A_i| = |f(x)-f(x')|$). This axiom forces the sum of absolute attributions to match the output difference. The paper proves (Theorem 1) that Geodesic IG is the unique path-based method satisfying this, preventing artificial cancellation of noisy attributions.
- **Core assumption:** The path $\gamma(t)$ is smooth and the function $f$ is continuously differentiable.
- **Evidence anchors:**
  - [page 4] Axiom 2 defines Strong Completeness.
  - [page 9] Theorem 1 proves GIG uniquely satisfies this property.
  - [corpus] Corpus evidence on axiomatic attribution is weak/missing; this appears to be a novel contribution of this specific work.
- **Break condition:** If the path deviates from the true geodesic (due to approximation errors in k-NN or SVI), the theoretical guarantee of Strong Completeness becomes an approximation.

### Mechanism 3
- **Claim:** In high-dimensional spaces (e.g., images), discrete graph search (k-NN) fails, requiring continuous optimization.
- **Mechanism:** For complex models, the method employs **Stochastic Variational Inference (SVI)**. It defines a potential energy function (Eq. 11) with two competing terms: a distance term to keep the path near the straight line and a curvature penalty to repel it from high-gradient regions. Minimizing this energy samples the probable geodesic path.
- **Core assumption:** A suitable trade-off parameter $\beta$ exists to balance distance vs. curvature avoidance.
- **Evidence anchors:**
  - [page 8] Section 2.3 details the energy formulation and SVI optimization.
  - [page 12] "We applied these attribution methods... [using] a uniformly black image."
  - [corpus] Weak corpus support; SVI for explainability paths is a distinct methodological choice here.
- **Break condition:** If the energy landscape is too complex or $\beta$ is poorly tuned, the sampler may diverge or fail to find a low-energy path.

## Foundational Learning

- **Concept:** **Integrated Gradients (IG) & Baselines**
  - **Why needed here:** GIG is a direct modification of IG. You must understand how IG integrates gradients from a "baseline" (e.g., a black image) to the input to see why a straight line is problematic.
  - **Quick check question:** If the baseline is a black image and the object of interest is black, why does standard IG struggle (Fig 1)?

- **Concept:** **Riemannian Manifolds & Metric Tensors**
  - **Why needed here:** The core innovation is redefining "distance" based on model sensitivity. You need to grasp that the metric tensor $G$ distorts space so high-gradient areas become "longer" or "harder" to traverse.
  - **Quick check question:** According to Eq. 6, does a geodesic path get longer or shorter as it passes through a region of high gradient magnitude?

- **Concept:** **Variational Inference (SVI)**
  - **Why needed here:** The paper uses SVI to approximate geodesics for large models. Understanding that SVI optimizes a distribution $q(\gamma)$ to approximate a complex posterior $p(\gamma)$ is necessary to interpret the implementation.
  - **Quick check question:** In the SVI context, what does minimizing the "energy" $E(\gamma)$ physically represent for the attribution path?

## Architecture Onboarding

- **Component map:** Input/Baseline -> Manifold Definition -> Path Solver -> Integrator
  1. **Input/Baseline:** Pair $(x, x')$.
  2. **Manifold Definition:** Constructs metric $G$ using model gradients (Jacobian).
  3. **Path Solver:**
     - *Simple Mode:* k-NN graph + Dijkstra (uses Eq. 8).
     - *Complex Mode:* Energy function + SVI optimizer (uses Eq. 11).
  4. **Integrator:** Accumulates gradients along the solved path (Eq. 9).

- **Critical path:** The **Path Solver** is the bottleneck.
  - For k-NN: Cost is $O(N^2)$ for Dijkstra.
  - For SVI: Cost is iterative optimization (e.g., 23 hours for 100 images on L4 GPU, per Section 3.2).

- **Design tradeoffs:**
  - **k-NN vs. SVI:** Use k-NN for low-dimensional/tabular data (exact but discrete). Use SVI for images/high-D (continuous but noisy/heavy).
  - **Speed vs. Accuracy:** The paper notes SVI is computationally expensive; reducing sampling steps speeds up inference but risks missing the true geodesic.

- **Failure signatures:**
  - **Disconnected Graphs (k-NN):** If density is too low, the path cannot form. *Fix:* Add "bridges" or increase $k$ (Section 2.2).
  - **Endpoint Deviation (SVI):** The optimized path might end near, but not exactly on, the input/baseline. *Fix:* Add endpoint weighting term to energy function (Section 3.2).
  - **Sensitivity to $\beta$:** If the curvature penalty weight $\beta$ is wrong, the path either ignores gradients ($\beta \to 0$, reverts to IG) or diverges wildly.

- **First 3 experiments:**
  1. **Visual Sanity Check (Half-Moons):** Train MLP on synthetic data. Compare IG vs. GIG heatmaps. *Success criteria:* GIG shows uniform attribution for points far from the decision boundary; IG shows artifacts (Fig 3).
  2. **Ablation on Connectivity:** Test k-NN GIG with varying $k$. Verify that disconnected graph handling (bridges) prevents runtime errors without distorting attributions.
  3. **Real-world Stress Test (Pascal VOC):** Run SVI-based GIG on ConvNext. Mask top-k% features and measure Comprehensiveness/Log-Odds. *Target:* GIG should outperform IG (AUC 0.27 vs 0.21).

## Open Questions the Paper Calls Out

- **Open Question 1:** Can directly solving the geodesic equation, rather than relying on Stochastic Variational Inference (SVI), reduce noise and computational cost while maintaining attribution accuracy?
  - *Basis in paper:* [explicit] The Discussion section states, "A promising future direction is to solve the geodesic equation directly, which could reduce noise and improve accuracy. Additionally... this approach may offer greater computational efficiency."
  - *Why unresolved:* The authors relied on SVI and k-NN approximations for their implementation and experiments, leaving the direct solution approach unimplemented and untested.
  - *What evidence would resolve it:* An implementation of GIG utilizing direct solvers (e.g., ODE solvers) benchmarked against the SVI method on complex models to compare noise levels and runtime.

- **Open Question 2:** To what extent does systematic hyperparameter tuning (specifically of the $\beta$ trade-off and SVI learning rate) improve performance metrics like Comprehensiveness and Log-Odds?
  - *Basis in paper:* [explicit] Section 3.2 notes that the performance depends on choosing the right value for $\beta$ and SVI hyperparameters. The authors admit, "due to limited computational resources, we were unable to perform such tuning," despite its potential to "significantly improve results."
  - *Why unresolved:* The reported results on Pascal VOC were generated without tuning these parameters, meaning the reported performance might be a lower bound for the method.
  - *What evidence would resolve it:* A study performing hyperparameter optimization (e.g., grid search or Bayesian optimization) on a validation set and reporting the resulting changes in attribution quality scores.

- **Open Question 3:** Can the k-NN approximation method be modified to ensure graph connectivity in high-dimensional spaces without resorting to the "not optimal" bridge heuristic?
  - *Basis in paper:* [inferred] Section 2.2 mentions that the k-NN graph can be disconnected, requiring the addition of "bridges" between components. The authors state, "we stress that this solution is not optimal, and argue that a better way... would be to avoid disconnected graphs in the first place."
  - *Why unresolved:* While increasing neighbors $k$ can prevent disconnection, it is computationally intensive. The current "bridge" solution is a heuristic patch that does not guarantee the path follows the true geodesic curvature.
  - *What evidence would resolve it:* A proposed algorithm that maintains manifold connectivity more naturally or a theoretical proof showing the probability of disconnection is negligible under specific sampling densities.

- **Open Question 4:** How does Geodesic IG perform on model architectures or functions where the Lipschitz continuity and bounded Hessian assumptions (required for the k-NN approximation) are violated?
  - *Basis in paper:* [inferred] Section 2.2 formalizes the approximation of geodesics between neighbors by assuming the network derivative is Lipschitz continuous (Eq. 10) and the Hessian is bounded.
  - *Why unresolved:* While standard networks often satisfy these, deep networks can exhibit sharp transitions or high curvature regions. The robustness of the approximation when these mathematical assumptions break down is not analyzed.
  - *What evidence would resolve it:* Experiments applying the k-NN GIG method to networks specifically trained or designed to have unbounded Hessians or non-Lipschitz activations, comparing the approximation error against the SVI method.

## Limitations

- **Computational Cost:** The SVI method is computationally expensive, taking approximately 23 hours for 100 images on an L4 GPU.
- **Hyperparameter Sensitivity:** The choice of the curvature penalty weight $\beta$ and SVI learning rate significantly impacts performance, but optimal values are not specified.
- **Scalability:** The paper does not discuss how the method scales to larger datasets or more complex models beyond the tested examples.

## Confidence

- **High Confidence:** The core mechanism of using geodesic paths on a Riemannian manifold to avoid high-gradient regions is theoretically sound and supported by the proof of Strong Completeness.
- **Medium Confidence:** The experimental results show GIG outperforming IG on both synthetic and real-world data. However, the lack of detailed hyperparameter settings and the computational cost of SVI limit the ability to fully verify these claims.
- **Low Confidence:** The claim that Strong Completeness prevents "artificial cancellation" of noisy attributions is compelling but not directly validated. The experiments focus on aggregate metrics (AUC, Log-Odds) rather than analyzing the attribution maps for specific failure modes of IG.

## Next Validation Checks

1. **Ablation Study on $\beta$:** Systematically vary the curvature penalty weight $\beta$ in the SVI energy function and measure its impact on attribution quality and computational time. This would quantify the sensitivity of GIG to this critical hyperparameter.
2. **Failure Mode Analysis:** Apply IG and GIG to a set of images where IG is known to fail (e.g., black objects on black backgrounds) and visually inspect the attribution maps. This would directly demonstrate GIG's advantage in avoiding the pitfalls of straight-line integration.
3. **Scalability Benchmark:** Test GIG on a larger subset of Pascal VOC (e.g., 500 images) and a more complex model (e.g., CLIP). Measure the increase in computation time and the change in performance metrics. This would provide insights into the practical scalability of the method.