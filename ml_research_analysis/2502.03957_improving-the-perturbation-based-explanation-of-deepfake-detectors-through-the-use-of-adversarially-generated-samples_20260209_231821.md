---
ver: rpa2
title: Improving the Perturbation-Based Explanation of Deepfake Detectors Through
  the Use of Adversarially-Generated Samples
arxiv_id: '2502.03957'
source_url: https://arxiv.org/abs/2502.03957
tags:
- explanation
- image
- deepfake
- methods
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to improve the explanations
  produced by perturbation-based methods for deepfake detectors. The core idea is
  to use adversarially-generated samples of input deepfake images to form perturbation
  masks for inferring the importance of different input features.
---

# Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples

## Quick Facts
- arXiv ID: 2502.03957
- Source URL: https://arxiv.org/abs/2502.03957
- Reference count: 40
- Primary result: Modified LIME achieves 10.5% average detection accuracy drop using adversarial perturbations

## Executive Summary
This paper addresses the challenge of explaining deepfake detector decisions by improving perturbation-based explanation methods. Traditional approaches use random perturbations that can introduce out-of-distribution artifacts, potentially explaining the model's detection of these artifacts rather than the actual manipulation. The authors propose using adversarially-generated samples created via Natural Evolution Strategies (NES) to form perturbation masks. These adversarial samples are specifically designed to flip the detector's decision from "deepfake" to "real," ensuring the perturbations remain within the data distribution. The approach is integrated into four state-of-the-art explanation methods (LIME, SHAP, SOBOL, and RISE) and evaluated on the FaceForensics++ dataset.

## Method Summary
The core innovation involves generating adversarial samples using NES to flip the deepfake detector's decision, then using these adversarially-manipulated image regions as perturbation masks in explanation methods. The NES algorithm iteratively perturbs input images with controlled noise (α=1/255, δ=16/255, σ=0.001) to find perturbations that maximize the probability of the "real" class. These adversarial samples replace the traditional random perturbations or mean pixel values used in methods like LIME and RISE. The approach is tested on a pretrained EfficientNetV2 deepfake detector fine-tuned on FaceForensics++, with explanations evaluated using detection accuracy drop on top-k segments and sufficiency scores across four manipulation types from FaceForensics++.

## Key Results
- Modified LIME achieves the best performance with 10.5% average detection accuracy drop
- The proposed adversarial perturbation approach mostly improves explanation method performance
- Qualitative analysis shows more accurate demarcation of manipulated regions compared to baseline methods
- Improvement consistent across multiple manipulation types (DF, F2F, FS, NT) in FaceForensics++

## Why This Works (Mechanism)
The method works by addressing a fundamental limitation of perturbation-based explanations: traditional random perturbations can introduce artifacts that don't exist in real data, causing the detector to explain its response to these artifacts rather than the actual manipulation. By using adversarially-generated samples that flip the detector's decision while remaining visually similar to the original image, the perturbations stay within the natural data distribution. This ensures that the explanation methods identify features that are actually relevant to the manipulation detection task rather than artifacts introduced by the perturbation process itself.

## Foundational Learning
- **Natural Evolution Strategies (NES)**: Black-box optimization algorithm that uses parameterized search distributions to find adversarial perturbations. Needed because it provides gradient-free optimization suitable for black-box models. Quick check: Verify NES converges within M iterations for >90% of test samples.
- **Perturbation-based explanations**: Methods that measure feature importance by systematically occluding or modifying input regions. Needed to understand how deepfake detectors make decisions. Quick check: Compare explanation maps between original and adversarial perturbation approaches on same images.
- **Adversarial attacks in explanation context**: Using adversarial examples not to fool models but to create meaningful perturbations for feature attribution. Needed to bridge adversarial robustness and interpretability research. Quick check: Confirm adversarial samples maintain visual similarity while successfully flipping detector decisions.

## Architecture Onboarding
- **Component map**: Input Image → NES Adversarial Generator → Adversarial Sample → Explanation Method (LIME/SHAP/SOBOL/RISE) → Feature Importance Map
- **Critical path**: NES generation → perturbation mask application → feature importance calculation → quantitative evaluation (accuracy drop/sufficiency)
- **Design tradeoffs**: Random perturbations are computationally cheaper but can introduce OOD artifacts; adversarial perturbations are more expensive but maintain data distribution. The paper chooses quality of explanation over computational efficiency.
- **Failure signatures**: If adversarial generation fails to flip decisions within M iterations, the perturbation approach degrades to baseline performance. Monitor success rate per manipulation class to identify systematic issues.
- **First experiments**:
  1. Generate adversarial samples for 10 test images across all manipulation types and verify decision flips
  2. Compare explanation maps from original LIME vs. modified LIME on identical test images
  3. Measure computation time difference between random and adversarial perturbation approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Does not specify image preprocessing pipeline (resolution, normalization, face alignment method)
- Unclear handling of NES failures to converge within maximum iterations
- SLIC superpixel parameters beyond segment count not specified

## Confidence
- **High confidence**: Theoretical framework and experimental design are sound
- **Medium confidence**: NES implementation and hyperparameter choices are reasonable but not fully specified
- **Low confidence**: Exact SLIC configurations and complete training pipeline details missing

## Next Checks
1. Verify adversarial sample generation produces consistent detector flips across all manipulation types within specified iteration limits
2. Test multiple SLIC segment counts (30, 50, 70) to determine sensitivity of explanation quality to superpixel granularity
3. Compare explanation quality when using mean pixel values versus adversarial samples as perturbations on the same test images