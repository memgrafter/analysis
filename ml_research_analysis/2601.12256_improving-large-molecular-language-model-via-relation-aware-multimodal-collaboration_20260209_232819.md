---
ver: rpa2
title: Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration
arxiv_id: '2601.12256'
source_url: https://arxiv.org/abs/2601.12256
tags:
- molecular
- molecule
- language
- collamo
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoLLaMo introduces a relation-aware multimodal projector to integrate
  1D molecular sequences, 2D graphs, and 3D conformations into a unified token space,
  enabling large language models to perform robust molecular reasoning. The relation-aware
  attention mechanism incorporates 2D structural and 3D spatial information to enhance
  fine-grained information exchange.
---

# Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration

## Quick Facts
- arXiv ID: 2601.12256
- Source URL: https://arxiv.org/abs/2601.12256
- Reference count: 7
- Primary result: CoLLaMo achieves BLEU 40.1 on molecule captioning vs. GPT-4 (34.5) and GPT-4o (35.1)

## Executive Summary
CoLLaMo introduces a relation-aware multimodal projector to integrate 1D molecular sequences, 2D graphs, and 3D conformations into a unified token space, enabling large language models to perform robust molecular reasoning. The relation-aware attention mechanism incorporates 2D structural and 3D spatial information to enhance fine-grained information exchange. Evaluation includes novel molecule-centric metrics like CHARM/RCHARM for hallucination assessment and LLM-based caption quality scoring. Experiments show that CoLLaMo outperforms GPT-4, GPT-4o, and other LMLMs on molecule captioning, property QA, IUPAC prediction, and motif counting, maintaining performance even with incomplete modalities, demonstrating improved robustness and generalization.

## Method Summary
CoLLaMo builds upon LLaMA2-7B Chat as the base LLM and introduces a modality-collaborative projector (Co-Proj.) that fuses 1D SELFIES sequences, 2D molecular graphs (via pretrained GIN), and 3D conformations (via pretrained UniMol). The projector uses relation-aware attention with 2D shortest-path and 3D Euclidean distance biases, cross-attention to learnable query tokens, modality dropout during pretraining, and modality embeddings. Training proceeds in two stages: pretraining (64k iterations, freezing LLM) to align modalities, followed by instruction tuning (150k-900k iterations, training Co-Proj. and LLM with LoRA while freezing encoders). The model is evaluated on molecule captioning, property QA, IUPAC prediction, and motif counting using both standard and molecule-centric metrics.

## Key Results
- CoLLaMo achieves BLEU 40.1 on molecule captioning, outperforming GPT-4 (34.5) and GPT-4o (35.1)
- Maintains strong performance with single modalities (BLEU 28.0 for 1D-only) vs. baselines (2.4 without Co-Proj.)
- Reduces hallucination (CHARM/RCHARM metrics) compared to GPT-4 and GPT-4o
- Shows robustness across tasks with incomplete modalities, demonstrating generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relation-aware attention biases improve cross-modal information exchange between atoms by encoding structural proximity.
- Mechanism: The model injects 2D shortest-path distances and 3D Euclidean distances directly into the attention computation via learnable bias terms (Φ²ᵈ, Φ³ᵈ), replacing standard dot-product attention with `Co-Attn = softmax(QKᵀ/√d + Φ²ᵈ + Φ³ᵈ)V`. This guides attention toward chemically meaningful atom pairs rather than learning relationships from scratch.
- Core assumption: Structural and spatial distances between atoms carry task-relevant information that should bias token-level attention patterns.
- Evidence anchors:
  - [abstract] "relation-aware modality-collaborative attention mechanism... facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations"
  - [section 5.1] Equation 3 defines Co-Attn with attention biases; shortest path for 2D, Gaussian kernel for 3D
  - [corpus] Weak direct evidence; neighbor papers discuss graph-LLM integration but not relation-aware attention specifically
- Break condition: If attention biases saturate (all distances map to similar bias values) or if tasks don't require relational reasoning, the mechanism provides no signal.

### Mechanism 2
- Claim: Cross-attention with learnable query tokens enables unified multi-modal representation without modality-specific tokenizers.
- Mechanism: A fixed set of learnable tokens P^(l) queries concatenated representations from all modalities via cross-attention (`P̂ = Attn(P, [Ẑ₁d; Ẑ₂d; Ẑ₃d], [Ẑ₁d; Ẑ₂d; Ẑ₃d])`), producing fixed-length unified tokens regardless of input atom count.
- Core assumption: A shared query vocabulary can extract complementary information from heterogeneous modalities into a common semantic space.
- Evidence anchors:
  - [section 5.1] "Modality token unification via cross-attention... unified into a fixed number of tokens"
  - [Table 5] Ablation shows combining all modalities with Co-Proj improves captioning BLEU from 33.2→40.1
  - [corpus] ProtoMol (arXiv:2510.16824) supports multimodal fusion benefits but uses different approach
- Break condition: If modalities contain contradictory information or cross-attention collapses to attending only one modality, unification degrades to single-modality performance.

### Mechanism 3
- Claim: Modality dropout during training creates robust representations that tolerate missing modalities at inference.
- Mechanism: Randomly masking modality representations during pretraining forces the model to learn redundant encoding pathways. Modality-specific embeddings (`P = P_base + Σ P_m`) signal which modalities are present, enabling adaptive inference.
- Core assumption: The model can learn to reconstruct or approximate missing modality information from available modalities.
- Evidence anchors:
  - [section 5.1] "To prevent over-reliance on a specific modality, we apply modality dropout during pretraining"
  - [Table 7] With Co-Proj, single-modality inference (1D only) achieves BLEU 28.0 vs. 2.4 without Co-Proj
  - [corpus] No direct corpus support for dropout-based robustness in molecular LLMs
- Break condition: If a task critically depends on a single modality (e.g., HOMO-LUMO gap requires 3D), dropping that modality cannot be compensated.

## Foundational Learning

- Concept: **Attention bias in transformers**
  - Why needed here: The Co-Attn mechanism modifies standard attention with distance-based biases; understanding how additive terms affect softmax distributions is essential.
  - Quick check question: What happens to attention weights if Φ²ᵈ values are all large positive numbers?

- Concept: **Molecular representations (SELFIES, graphs, conformations)**
  - Why needed here: The model explicitly processes 1D strings, 2D topological graphs, and 3D coordinate sets—each with different encoders.
  - Quick check question: Which modality would you expect to be most informative for predicting LogP vs. HOMO-LUMO gap?

- Concept: **Cross-attention for modality fusion**
  - Why needed here: The Co-Proj uses queries from learnable tokens to attend across concatenated modality representations.
  - Quick check question: Why use cross-attention rather than simple concatenation + MLP?

## Architecture Onboarding

- Component map: Input Molecule -> 1D Encoder (Token-Embed, SELFIES) -> 2D Encoder (GIN) + Co-Attn -> 3D Encoder (Uni-Mol) + Co-Attn -> Cross-Attention -> MLP -> Unified Tokens -> LLM

- Critical path: The Co-Attn bias computation (Φ²ᵈ, Φ³ᵈ) must correctly align atom indices across 2D and 3D encoders—mismatched atom ordering will inject noise.

- Design tradeoffs:
  - More learnable query tokens (b) = richer representation but higher compute
  - Aggressive modality dropout = better robustness but slower convergence
  - Deeper encoder layers (L) = more fine-grained fusion but more parameters to align

- Failure signatures:
  - Performance drops when switching from 3D+2D to 2D-only → dropout not applied during training
  - All modalities produce similar attention patterns → bias terms collapsed
  - NaN losses → check Gaussian kernel parameters (σ_k can underflow)

- First 3 experiments:
  1. Verify single-modality baselines (1D/2D/3D only) match Table 1 before adding fusion
  2. Ablate Φ²ᵈ and Φ³ᵈ separately to confirm each bias contributes (per Table 6)
  3. Test missing-modality inference: train with all, evaluate with 2D-only to verify Table 7 robustness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Performance relies heavily on quality of 3D conformer generation and atom index alignment across modalities
- Relation-aware attention mechanism is essential rather than additive, limiting scalability
- Robustness claims depend on unspecified modality dropout rate and selection probability
- Hallucination metrics depend on specific entity extraction pipeline (BERN2 with 0.9 confidence threshold)

## Confidence
- **High Confidence**: The core mechanism of relation-aware attention with distance-based biases is well-specified and the two-stage training procedure is clearly described. The improvements in single-modality inference robustness (Table 7) are directly measurable.
- **Medium Confidence**: The multimodal fusion benefits shown in Table 5 are compelling, but the exact architectural parameters (number of layers, hidden dimensions, learnable tokens) that enable these gains are unspecified. The hallucination reduction claims depend on the specific entity extraction pipeline.
- **Low Confidence**: The generalization of the robustness claims to arbitrary molecular tasks not included in the instruction dataset, and the scalability to larger LLMs beyond LLaMA2-7B, remain unproven.

## Next Checks
1. Verify atom index alignment: Test that 2D and 3D encoders produce representations with matching atom ordering by computing the 3D Euclidean distance bias matrix against ground-truth coordinates and checking for reasonable values (not NaN or uniform).

2. Ablate relation-aware attention: Remove Φ²ᵈ and Φ³ᵈ separately during training and measure the degradation in captioning BLEU and property QA MAE to confirm each bias contributes meaningfully (following Table 6 methodology).

3. Stress-test modality dropout: Train with aggressive modality dropout (e.g., 50% dropout rate) and evaluate on tasks requiring specific modalities to determine the minimum viable dropout rate that maintains performance while ensuring robustness.