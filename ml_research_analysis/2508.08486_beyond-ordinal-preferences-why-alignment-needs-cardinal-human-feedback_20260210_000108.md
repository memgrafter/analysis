---
ver: rpa2
title: 'Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback'
arxiv_id: '2508.08486'
source_url: https://arxiv.org/abs/2508.08486
tags:
- data
- cardinal
- cdpo
- preference
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper shows that preference-based alignment methods like
  RLHF and DPO cannot reliably select the best model because they only use ordinal
  feedback (A B). The authors prove an impossibility result: without knowing the strength
  of preferences, algorithms cannot correctly resolve tradeoffs across prompts.'
---

# Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback

## Quick Facts
- arXiv ID: 2508.08486
- Source URL: https://arxiv.org/abs/2508.08486
- Reference count: 40
- This paper shows that preference-based alignment methods like RLHF and DPO cannot reliably select the best model because they only use ordinal feedback (A > B)

## Executive Summary
This paper demonstrates that preference-based alignment methods like RLHF and DPO cannot reliably select the best model because they only use ordinal feedback (A > B). The authors prove an impossibility result: without knowing the strength of preferences, algorithms cannot correctly resolve tradeoffs across prompts. They address this by collecting cardinal feedback via willingness-to-pay (WTP) elicitation, forming the CARDINAL PREFS dataset of 25,000 judgments. Empirically, training with cardinal feedback (CDPO) achieves 50% higher ground-truth reward than DPO and wins 55% more Arena-Hard battles.

## Method Summary
The authors introduce CARDINAL PREFS, a dataset of 25,000 human judgments collected via willingness-to-pay (WTP) elicitation to capture cardinal preferences. They propose CDPO (Cardinal DPO), a variant of DPO that incorporates these cardinal preferences into the training objective. The method trains models to maximize expected reward by considering both the direction and magnitude of human preferences, rather than just pairwise rankings.

## Key Results
- CDPO achieves 50% higher ground-truth reward than DPO
- CDPO wins 55% more Arena-Hard battles than DPO
- CDPO better prioritizes substantive improvements over stylistic ones compared to DPO

## Why This Works (Mechanism)
The core insight is that ordinal preferences alone cannot capture the strength of human preferences, making it impossible to properly weigh tradeoffs across different prompts. By collecting cardinal feedback through willingness-to-pay elicitation, the method captures both which model is preferred and by how much, enabling more accurate reward estimation and better optimization.

## Foundational Learning

**Ordinal vs Cardinal Preferences**: Ordinal preferences only indicate ranking (A > B), while cardinal preferences capture magnitude of preference. Why needed: Ordinal feedback cannot resolve tradeoffs between different prompts. Quick check: Can you explain why knowing "A is preferred to B" is insufficient without knowing "by how much"?

**Willingness-to-Pay Elicitation**: A method where humans indicate how much they'd pay for one model over another, providing a monetary measure of preference strength. Why needed: Traditional rating scales can be inconsistent across users. Quick check: What makes WTP more reliable than direct rating scales?

**Reward Modeling**: Estimating a scalar reward function from human feedback to guide model optimization. Why needed: Models need quantifiable objectives to optimize. Quick check: How does reward modeling differ between ordinal and cardinal feedback approaches?

## Architecture Onboarding

Component map: Human feedback -> WTP Elicitation -> CARDINAL PREFS dataset -> CDPO training -> Reward model -> Optimized model

Critical path: CARDINAL PREFS → CDPO training → Improved reward estimation → Better model alignment

Design tradeoffs: The WTP elicitation method requires users to make hypothetical monetary decisions, which may not generalize to all populations or contexts.

Failure signatures: If WTP responses are inconsistent or noisy, the cardinal feedback may not improve over ordinal approaches.

First experiments:
1. Replicate the comparison between CDPO and DPO on a subset of the CARDINAL PREFS dataset
2. Test CDPO on a different domain (e.g., code generation) with new WTP elicitation
3. Compare WTP-based cardinal feedback with direct rating scales on the same prompts

## Open Questions the Paper Calls Out
The paper acknowledges uncertainty about whether the benefits of cardinal feedback extend to other alignment challenges, such as safety or ethical considerations, and whether alternative methods for collecting cardinal feedback might be more scalable or effective.

## Limitations
- Scalability of WTP elicitation to diverse or less accessible user populations is uncertain
- CARDINAL PREFS dataset may be limited in scope compared to full range of potential user preferences
- Focus on text generation quality leaves unclear whether benefits extend to other alignment challenges

## Confidence

High: Empirical results showing CDPO's superior performance over DPO, as demonstrated by both reward maximization and Arena-Hard battle wins
High: Impossibility result regarding ordinal preferences being well-supported by theoretical analysis
Medium: Assertion that WTP elicitation is the only viable method for collecting cardinal feedback
Medium: Broader claim that cardinal feedback is essential for alignment in all contexts

## Next Checks

1. Test CDPO and CARDINAL PREFS on a wider range of domains and user populations to assess generalizability
2. Explore alternative methods for collecting cardinal feedback (e.g., direct rating scales) to compare effectiveness and scalability
3. Evaluate the impact of cardinal feedback on alignment objectives beyond text quality, such as safety, fairness, or ethical considerations