---
ver: rpa2
title: Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions
arxiv_id: '2509.23782'
source_url: https://arxiv.org/abs/2509.23782
tags:
- knowledge
- kappa
- knowledge-prediction
- prediction
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a widespread knowledge-prediction gap in
  LLMs across multiple-choice question benchmarks, where models often fail to utilize
  knowledge encoded in their hidden representations despite possessing it linearly
  accessibly. The authors conduct a probing analysis revealing that residual streams
  contain misaligned knowledge and prediction subspaces - the knowledge basis encodes
  ground-truth probabilities while the prediction basis encodes model-generated choices.
---

# Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2509.23782
- Source URL: https://arxiv.org/abs/2509.23782
- Authors: Yoonah Park; Haesung Pyun; Yohan Jo
- Reference count: 40
- Primary result: KAPPA intervention improves LLM accuracy on MCQs by aligning misaligned knowledge and prediction subspaces in residual streams

## Executive Summary
This paper identifies a knowledge-prediction gap in LLMs where models encode correct answers in hidden representations but fail to use them in predictions. Through probing analysis, the authors reveal that residual streams contain misaligned knowledge and prediction subspaces - the knowledge basis encodes ground-truth probabilities while the prediction basis encodes model-generated choices. They introduce KAPPA (Knowledge-Aligned Prediction through Projection-based Adjustment), an inference-time intervention that applies affine transformations to align these subspaces. Experiments demonstrate substantial accuracy improvements on reasoning, truthfulness, and bias tasks, with the approach generalizing to free-form settings and transferring partially across similar tasks.

## Method Summary
The method involves training linear probes on residual stream activations to identify knowledge and prediction subspaces, then applying KAPPA's affine transformation at inference time to realign predictions with encoded knowledge. The intervention solves a constrained optimization that projects hidden states onto the knowledge subspace direction while minimizing perturbation. KAPPA hyperparameters include α (scaling factor) and β (margin offset), with probe selection requiring prediction accuracy >85% on validation data. The approach works by minimally adjusting hidden states within the prediction subspace to match coordinates in the knowledge subspace.

## Key Results
- KAPPA substantially improves accuracy and agreement metrics on BBH, ARC-Challenge, and other benchmarks
- Gains are especially pronounced on reasoning and truthfulness tasks where knowledge-prediction gaps are largest
- The approach generalizes to free-form settings and transfers partially across tasks requiring similar skills
- Cross-dataset experiments show subspaces generalize to some extent within similar task categories

## Why This Works (Mechanism)

### Mechanism 1
The knowledge-prediction gap arises from geometric misalignment between distinct subspaces in the residual stream. Hidden states encode both correct-answer signals (knowledge subspace) and predicted-answer signals (prediction subspace) along different bases. When coordinates in these subspaces diverge, the model outputs predictions that contradict its internally encoded knowledge. This assumes linear probes faithfully capture the principal directions along which knowledge vs. prediction signals are organized.

### Mechanism 2
Affine transformation of hidden states can realign prediction coordinates with knowledge coordinates while preserving other information. KAPPA solves a constrained optimization that projects hidden states onto the knowledge subspace direction while minimizing L2 perturbation. The closed-form update replaces only the prediction-subspace component, assuming the prediction subspace is low-dimensional and can be modified without corrupting orthogonal task-relevant features.

### Mechanism 3
Aligning predictions with internal knowledge improves accuracy more on tasks with large initial gaps (reasoning, truthfulness, bias) than knowledge-intensive tasks. Tasks like TruthfulQA and BBQ show high knowledge probe accuracy but low base model accuracy, indicating unused latent knowledge. KAPPA closes this gap by forcing predictions to follow the knowledge probe's direction, assuming the knowledge probe's accuracy represents an upper bound on what the model could achieve if it utilized its representations faithfully.

## Foundational Learning

- **Linear probing of hidden states**: Understanding how KAPPA identifies knowledge and prediction subspaces requires knowing that linear classifiers can extract task-relevant information from activations. *Quick check*: Can you explain why a linear probe's weight vectors can be interpreted as basis directions in representation space?

- **Residual stream architecture in transformers**: KAPPA intervenes on residual stream activations; understanding the additivity of attention and MLP outputs clarifies why perturbations propagate cleanly. *Quick check*: How does the residual stream's additive structure differ from layer-wise hidden states in earlier architectures?

- **Constrained optimization with Lagrangian multipliers**: KAPPA's closed-form solution derives from minimizing perturbation subject to an alignment constraint. *Quick check*: What does the Lagrangian multiplier represent when solving for the minimal perturbation that satisfies a linear constraint?

## Architecture Onboarding

- **Component map**: Probe training pipeline: Extract activations → Train knowledge/prediction probes per layer → Select layers with high knowledge accuracy and >85% prediction accuracy → Inference intervention: Apply affine transformation to residual stream at selected layers

- **Critical path**: 1) Train probes on labeled MCQ data (ground-truth labels for knowledge probe, model predictions for prediction probe) 2) Validate probe selection on held-out data 3) Apply KAPPA transformation at inference time to test data

- **Design tradeoffs**: Single-layer vs. multi-layer intervention (more layers increase alignment but risk over-perturbation), high α/β values amplify knowledge signal but may reduce calibration, cross-dataset transfer works within similar task categories but not across dissimilar tasks

- **Failure signatures**: KAPPA reduces accuracy on tasks where base model is already well-calibrated (e.g., GSM8K free-form: 91.6% → 90.7%), large gaps between knowledge probe and base performance indicate intervention opportunity; small gaps suggest limited upside

- **First 3 experiments**: 1) Reproduce probe training on TruthfulQA for a single model; verify knowledge probe accuracy exceeds base model 2) Apply single-layer KAPPA with α=30, β=0; measure accuracy and agreement (AGR) improvement 3) Test cross-dataset transfer: Train probes on BBH-NLP, apply to GSM8K; check if improvement concentrates on logic-heavy subtasks

## Open Questions the Paper Calls Out

**Open Question 1**: How can nonlinearly encoded knowledge be extracted from model representations and aligned with predictions? The current method only addresses linearly accessible knowledge via linear probes; nonlinear knowledge forms remain unexplored despite potentially containing richer information.

**Open Question 2**: What structural properties enable knowledge/prediction subspaces to transfer across tasks? While optimal subspaces differ across tasks,