---
ver: rpa2
title: 'DenseReviewer: A Screening Prioritisation Tool for Systematic Review based
  on Dense Retrieval'
arxiv_id: '2502.03400'
source_url: https://arxiv.org/abs/2502.03400
tags:
- studies
- screening
- dense
- densereviewer
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DenseReviewer is a web-based tool that prioritizes relevant medical
  studies for systematic review screening using dense retrieval and relevance feedback.
  The system iteratively updates a PICO query with reviewer feedback to re-rank studies,
  achieving higher effectiveness and efficiency compared to traditional active learning
  methods.
---

# DenseReviewer: A Screening Prioritisation Tool for Systematic Review based on Dense Retrieval
## Quick Facts
- arXiv ID: 2502.03400
- Source URL: https://arxiv.org/abs/2502.03400
- Reference count: 21
- Web-based tool that prioritizes relevant medical studies using dense retrieval and relevance feedback

## Executive Summary
DenseReviewer is a web-based screening prioritisation tool designed to enhance the efficiency of systematic review processes. The system employs dense retrieval techniques combined with active learning to iteratively refine search results based on reviewer feedback. By updating PICO queries through relevance feedback, DenseReviewer achieves higher effectiveness compared to traditional active learning methods. The tool offers two screening modes and includes an open-source Python library for experimentation and model development.

## Method Summary
DenseReviewer implements a web-based interface that integrates dense retrieval with active learning for systematic review screening. The system allows reviewers to provide feedback on study relevance, which is then used to update the PICO query iteratively. This refined query is applied to re-rank studies, prioritizing those most likely to be relevant. The tool offers ranking mode for browsing paginated lists and focus mode for individual study review. DenseReviewer is containerized for easy deployment and has been tested on AWS infrastructure.

## Key Results
- Achieves higher effectiveness and efficiency compared to traditional active learning methods
- Offers two screening modes: ranking mode and focus mode
- Includes open-source Python library for experimentation and model development
- Containerized deployment tested on AWS infrastructure

## Why This Works (Mechanism)
DenseReviewer leverages dense retrieval to capture semantic similarities between studies and the query, going beyond keyword matching. The iterative query refinement through relevance feedback allows the system to adapt to reviewer preferences and improve ranking accuracy over time. This combination of dense retrieval and active learning creates a more efficient screening process by focusing reviewer attention on potentially relevant studies earlier in the screening process.

## Foundational Learning
- Dense Retrieval: Why needed - Captures semantic meaning beyond keywords; Quick check - Compare results with keyword-based search
- PICO Framework: Why needed - Structured approach to clinical questions; Quick check - Validate PICO query construction
- Active Learning: Why needed - Reduces manual screening effort; Quick check - Measure reduction in reviewed irrelevant studies
- Relevance Feedback: Why needed - Improves system accuracy over time; Quick check - Track performance improvement across feedback iterations
- Web-based Interface: Why needed - Enables collaborative review; Quick check - Test multi-user access and data synchronization

## Architecture Onboarding
Component map: User Interface -> Dense Retrieval Engine -> Feedback Processor -> Query Updater -> Result Ranker

Critical path: User provides feedback -> Feedback processed -> Query updated -> Studies re-ranked -> New results displayed

Design tradeoffs: Balancing real-time performance with computational resource usage; Trade-off between exploration of new studies and exploitation of known relevant ones

Failure signatures: Poor initial query formulation leading to irrelevant results; Reviewer fatigue affecting feedback quality; System performance degradation with large document collections

First experiments:
1. Test system with a small, well-defined PICO query and manually curated relevant studies
2. Evaluate ranking accuracy with incremental feedback from a single reviewer
3. Measure performance impact of increasing the number of studies in the collection

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness primarily validated on medical literature databases, raising generalizability concerns
- Limited comparative studies with other state-of-the-art screening tools
- Potential for reviewer bias accumulation over multiple feedback cycles

## Confidence
High: Containerized deployment and AWS testing environment
Medium: Comparative effectiveness claims due to limited validation studies
Medium: Cross-disciplinary applicability based on medical literature focus

## Next Checks
1. Conduct cross-disciplinary testing to evaluate performance on non-medical literature and different database types
2. Perform comprehensive error analysis comparing false positive/negative rates with established screening tools
3. Execute longitudinal user studies to assess consistency and potential bias accumulation in the iterative query refinement process across multiple review cycles