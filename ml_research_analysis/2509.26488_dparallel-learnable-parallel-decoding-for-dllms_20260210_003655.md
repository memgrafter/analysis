---
ver: rpa2
title: 'dParallel: Learnable Parallel Decoding for dLLMs'
arxiv_id: '2509.26488'
source_url: https://arxiv.org/abs/2509.26488
tags:
- decoding
- arxiv
- diffusion
- preprint
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of slow inference in diffusion
  language models (dLLMs), which, despite their parallel token prediction capability,
  still require nearly token-length decoding steps for good performance. The core
  issue is that token certainty in dLLMs converges sequentially, limiting parallelism.
---

# dParallel: Learnable Parallel Decoding for dLLMs

## Quick Facts
- arXiv ID: 2509.26488
- Source URL: https://arxiv.org/abs/2509.26488
- Reference count: 16
- dParallel reduces decoding steps from 256 to 30-24 while maintaining accuracy on reasoning and code tasks

## Executive Summary
dParallel addresses the slow inference problem in diffusion language models (dLLMs) by enabling parallel token prediction. While dLLMs can theoretically predict tokens in parallel, token certainty converges sequentially, limiting practical parallelism. The paper proposes certainty-forcing distillation, which guides a pretrained dLLM to follow its original generation trajectory while enforcing high certainty on masked tokens more rapidly. This approach dramatically accelerates decoding without sacrificing performance.

## Method Summary
The paper introduces dParallel, a training method that leverages certainty-forcing distillation to enable parallel decoding in dLLMs. The approach works by distilling knowledge from a pretrained dLLM into a parallel-capable version, using certainty as the guiding signal. During training, the model learns to maintain its generation trajectory while achieving high certainty on masked tokens simultaneously rather than sequentially. The method is efficient to implement, requiring only 10 hours on eight A5000 GPUs using LoRA adapters, and can be applied to existing dLLM architectures like LLaDA.

## Key Results
- Applied to LLaDA-8B-Instruct: reduced decoding steps from 256 to 30 on GSM8K (8.5× speedup, accuracy 76.1%)
- Applied to LLaDA-8B-Instruct: reduced decoding steps from 256 to 24 on MBPP (10.5× speedup, accuracy 40.8%)
- Training efficiency: only 10 hours on eight A5000 GPUs using LoRA adapters
- Maintains model performance while achieving dramatic speed improvements

## Why This Works (Mechanism)
The method works by addressing the sequential convergence of token certainty in dLLMs. Through certainty-forcing distillation, the model learns to achieve high certainty across all tokens simultaneously rather than sequentially, enabling true parallel decoding while following the original generation trajectory.

## Foundational Learning

**Diffusion Language Models**: dLLMs use iterative denoising processes for text generation. Understanding their parallel token prediction capability is crucial because this theoretical advantage has been limited by sequential certainty convergence.

**Token Certainty**: The confidence measure of predicted tokens. Sequential certainty convergence limits parallel decoding, making this concept central to understanding the bottleneck dParallel addresses.

**Knowledge Distillation**: Transfer of knowledge from a pretrained model to another. Certainty-forcing distillation is the core technique used to guide parallel decoding while maintaining generation quality.

**LoRA Adapters**: Low-rank adaptation technique for efficient model training. The 10-hour training claim relies on LoRA, making understanding its computational efficiency important.

## Architecture Onboarding

**Component Map**: Pretrained dLLM -> Certainty-forcing Distillation -> Parallel-capable dLLM

**Critical Path**: The training pipeline follows: pretrained model weights → LoRA adaptation with certainty targets → parallel decoding capability → inference acceleration

**Design Tradeoffs**: The method trades some training complexity (certainty supervision) for significant inference speedup. The use of LoRA balances adaptation quality with training efficiency.

**Failure Signatures**: If certainty targets are too aggressive, the model may diverge from its original generation trajectory. If training is insufficient, parallel decoding may not achieve full speedup potential.

**First Experiments**:
1. Validate certainty-forcing distillation maintains generation quality on simple sequences
2. Test parallel vs sequential decoding speed comparison on controlled datasets
3. Verify LoRA adaptation preserves original model capabilities while enabling acceleration

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Performance trade-offs between speed and accuracy across different tasks need clearer characterization
- Training efficiency claims based on single hardware configuration (eight A5000 GPUs)
- Method applicability limited to tested dLLM architectures beyond LLaDA
- Focus on single-task evaluation without general-purpose capability testing

## Confidence

**High Confidence**: The core technical approach of using certainty-forcing distillation to accelerate dLLM decoding is well-explained and experimentally validated. The reported speedups (8.5× to 10.5×) are credible given the methodology and results presented.

**Medium Confidence**: Claims about training efficiency and general applicability require more validation. The 10-hour training time on specific hardware is promising but needs broader hardware testing. The method's effectiveness across different dLLM architectures and tasks needs more extensive validation.

**Low Confidence**: Claims about setting "new efficiency baseline" are premature without broader benchmark comparisons against other acceleration methods. The long-term stability and robustness of the accelerated models across diverse real-world applications is not established.

## Next Checks

1. Test dParallel across multiple dLLM architectures (beyond LLaDA) and model sizes to verify generalizability of the speedup claims and assess performance trade-offs.

2. Evaluate the accelerated models on diverse benchmark suites including general-purpose language tasks, reasoning tasks, and code generation to establish broader applicability beyond GSM8K and MBPP.

3. Conduct ablation studies systematically varying the number of decoding steps to characterize the precise accuracy-speed tradeoff curve, and test across different hardware configurations to validate the training efficiency claims.