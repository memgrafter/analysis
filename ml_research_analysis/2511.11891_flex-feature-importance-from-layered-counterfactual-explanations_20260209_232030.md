---
ver: rpa2
title: 'FLEX: Feature Importance from Layered Counterfactual Explanations'
arxiv_id: '2511.11891'
source_url: https://arxiv.org/abs/2511.11891
tags:
- feature
- global
- change
- importance
- flex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLEX introduces a model- and domain-agnostic framework that derives
  feature importance from counterfactual explanations at local, regional, and global
  levels. It quantifies how often each feature must change to flip predictions, enabling
  interpretable rankings that reflect systematic drivers of outcome changes.
---

# FLEX: Feature Importance from Layered Counterfactual Explanations
arXiv ID: 2511.11891
Source URL: https://arxiv.org/abs/2511.11891
Reference count: 40
Primary result: Derives feature importance from counterfactual explanations at local, regional, and global levels

## Executive Summary
FLEX introduces a model- and domain-agnostic framework that derives feature importance from counterfactual explanations across multiple levels of granularity. By quantifying how often each feature must change to flip predictions, FLEX provides interpretable rankings that reflect systematic drivers of outcome changes. The framework generalizes local change-frequency measures by aggregating across instances and neighborhoods, and is compatible with different counterfactual generation methods, allowing users to emphasize characteristics such as sparsity, feasibility, or actionability.

Evaluation on traffic accident severity prediction and loan approval tasks demonstrates that FLEX's global rankings correlate with SHAP while surfacing additional drivers, and regional analyses reveal context-specific factors that global summaries miss. For instance, in the accident dataset, driving experience and junction type are globally important, while regional analysis shows weather and junction type are critical for experienced drivers. In the loan dataset, cibil score is the most important feature globally, but the framework also reveals how the magnitude of change in this feature varies. Overall, FLEX bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

## Method Summary
FLEX operates by first generating counterfactual explanations for individual instances using any compatible counterfactual generation method. It then aggregates these local explanations across multiple dimensions: locally (for individual instances), regionally (across neighborhoods of similar instances), and globally (across the entire dataset). At each level, the framework counts how frequently each feature appears in counterfactual explanations and measures the magnitude of changes required. The aggregation process weights features by their change frequency and magnitude, producing importance scores that reflect both the prevalence and significance of each feature's role in prediction outcomes. This multi-layered approach allows FLEX to capture both broad patterns and context-specific drivers of model behavior.

## Key Results
- Global rankings from FLEX correlate with SHAP while identifying additional important drivers
- Regional analysis reveals context-specific factors that global summaries miss (e.g., weather importance for experienced drivers)
- Framework is compatible with different counterfactual generation methods, allowing emphasis on sparsity, feasibility, or actionability
- Demonstrated effectiveness on both traffic accident severity and loan approval prediction tasks

## Why This Works (Mechanism)
FLEX works by leveraging the fundamental property of counterfactual explanations: they identify the minimal changes needed to alter a prediction. By systematically generating and analyzing these explanations across multiple instances and contexts, the framework captures which features consistently drive prediction changes. The multi-level aggregation (local → regional → global) preserves information at different granularities while building a comprehensive picture of feature importance. The compatibility with various counterfactual generation methods allows FLEX to adapt to different user priorities, whether emphasizing sparse changes, realistic modifications, or actionable interventions. This approach effectively bridges the gap between local explanation methods and global attribution techniques by grounding importance measures in actual prediction dynamics.

## Foundational Learning
- **Counterfactual Explanations**: Minimal changes to input features that would alter a model's prediction. Why needed: They provide the foundation for identifying which features drive prediction changes. Quick check: Verify that generated counterfactuals actually change the prediction and are minimally distant from the original instance.

- **Feature Importance Aggregation**: Methods for combining local importance measures into regional or global scores. Why needed: Individual counterfactuals may be noisy; aggregation reveals systematic patterns. Quick check: Ensure aggregation method appropriately weights both frequency and magnitude of changes.

- **Multi-level Analysis**: The concept of analyzing data at different granularities (local, regional, global). Why needed: Different levels reveal different insights - local for individual recourse, regional for context-specific patterns, global for overall drivers. Quick check: Validate that findings at different levels are consistent and complementary.

- **SHAP Correlation**: Comparative validation against established attribution methods like SHAP. Why needed: Provides credibility and helps identify what additional insights FLEX provides beyond existing methods. Quick check: Calculate correlation coefficients and examine where methods agree and disagree.

## Architecture Onboarding

**Component Map**: Counterfactual Generator → Local Aggregator → Regional Aggregator → Global Aggregator → Importance Ranking

**Critical Path**: The most important workflow is the generation and aggregation pipeline. First, counterfactuals must be generated for a representative sample of instances. These are then aggregated locally to understand individual feature importance, followed by regional aggregation to identify context-specific patterns, and finally global aggregation to determine overall feature importance across the dataset.

**Design Tradeoffs**: The framework trades computational intensity (generating many counterfactuals) for comprehensive, multi-level importance insights. It also trades method specificity for compatibility with various counterfactual generation approaches, which means results can vary based on the underlying counterfactual method used.

**Failure Signatures**: If the framework produces unreliable results, common failure modes include: counterfactual generator producing unrealistic or sparse explanations, insufficient sampling leading to unrepresentative aggregations, neighborhood definitions that are too broad or narrow, and aggregation methods that overweight rare but large changes. The framework may also fail to capture nonlinear relationships if the counterfactual generation method cannot explore the feature space effectively.

**3 First Experiments**:
1. Generate counterfactuals for a small, diverse sample of instances and manually verify that the explanations make sense and that the identified important features align with domain knowledge.
2. Compare FLEX's local importance rankings for individual instances against the raw counterfactual explanations to ensure the aggregation process preserves meaningful information.
3. Run FLEX on a dataset with known feature importance (e.g., synthetic data or well-studied benchmark) to validate that the framework recovers expected patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on counterfactual generation method quality introduces variability in results
- Effectiveness demonstrated only on two specific datasets (traffic accident and loan approval)
- Relationship between FLEX and established methods like SHAP requires further investigation
- Generalizability to other domains and data types remains uncertain

## Confidence
- **High Confidence**: The core methodology of aggregating counterfactual changes to derive feature importance is sound and the framework's compatibility with various counterfactual methods is well-established.
- **Medium Confidence**: The effectiveness of FLEX on the demonstrated datasets is supported by the results, but broader domain applicability needs validation.
- **Medium Confidence**: The correlation with SHAP provides validation, but the nature of differences between methods and their practical implications require further exploration.

## Next Checks
1. Test FLEX across diverse datasets (e.g., healthcare, finance, computer vision) to assess domain generalizability and identify potential limitations in different contexts.
2. Conduct controlled experiments comparing FLEX outputs when using different counterfactual generation methods (focusing on sparsity vs. feasibility vs. actionability) to quantify the impact of method choice on importance rankings.
3. Perform ablation studies to determine the sensitivity of FLEX's results to parameters like neighborhood size, counterfactual sampling strategy, and aggregation methods across local, regional, and global levels.