---
ver: rpa2
title: 'Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization
  Approach'
arxiv_id: '2511.04393'
source_url: https://arxiv.org/abs/2511.04393
tags:
- regret
- trained
- reward
- time
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Iterative Regret-Minimization Fine-Tuning
  (Iterative RMFT), a post-training method for enhancing large language models (LLMs)
  as decision-making agents. The approach leverages regret minimization as a universal
  training signal: at each iteration, the model generates multiple decision trajectories,
  selects the k-lowest regret ones, and fine-tunes on them via supervised learning.'
---

# Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach

## Quick Facts
- arXiv ID: 2511.04393
- Source URL: https://arxiv.org/abs/2511.04393
- Reference count: 40
- Primary result: Introduces Iterative Regret-Minimization Fine-Tuning (Iterative RMFT) for enhancing LLMs as decision-making agents

## Executive Summary
This paper presents Iterative Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training method that leverages regret minimization as a universal training signal to improve large language models as decision-making agents. The approach iteratively generates decision trajectories, selects the k-lowest regret ones, and fine-tunes the model on them via supervised learning. Validated across Transformers, open-weight LLMs, and closed-weight models like GPT-4o mini, Iterative RMFT demonstrates consistent regret reduction, improved exploration-exploitation tradeoffs, and strong generalization across diverse settings. Theoretical analysis shows the method can recover known no-regret algorithms like FTRL, providing theoretical justification for its effectiveness.

## Method Summary
Iterative RMFT works by iteratively fine-tuning LLMs using regret minimization as a training signal. In each iteration, the model generates multiple decision trajectories in a decision-making environment, computes regret for each trajectory, selects the k-lowest regret trajectories, and performs supervised learning on these selected examples. This process repeats, allowing the model to learn from its own best-performing behaviors. The method is theoretically grounded, showing connections to Follow-the-Regularized-Leader (FTRL) algorithms in simplified settings. The approach is validated across numerical input/output Transformers, open-weight models (Phi-3.5-mini, Gemma-2-9b-it, Qwen3-8B), and closed-weight models (GPT-4o mini), demonstrating consistent improvements in regret minimization across varying horizons, action spaces, and language contexts.

## Key Results
- Consistent reductions in regret growth rates across diverse model scales and settings
- Improved exploration-exploitation tradeoffs compared to baseline approaches
- Strong generalization across varying horizons, action spaces, and real-world language contexts
- Theoretical connection to FTRL algorithms demonstrated in simplified settings

## Why This Works (Mechanism)
The approach works by leveraging the model's own decision-making experiences as training data, focusing on trajectories where the model performed well relative to its own past performance. By iteratively selecting and learning from low-regret trajectories, the model gradually improves its decision-making policy. The regret minimization signal provides a principled way to evaluate and improve decisions without requiring external reward functions or human supervision. The supervised learning on selected trajectories allows the model to internalize successful decision patterns, while the iterative nature enables continuous improvement through self-play and self-correction.

## Foundational Learning
- Regret Minimization Theory: Understanding how regret quantifies the difference between chosen actions and optimal actions is crucial for grasping the core training signal
  - Why needed: Provides the theoretical foundation for measuring and improving decision quality
  - Quick check: Can you explain why minimizing regret leads to better decision-making performance?

- Online Learning Concepts: Familiarity with bandit problems and online optimization frameworks helps contextualize the experimental setup
  - Why needed: The paper builds on established online learning theory to validate the approach
  - Quick check: Can you describe the difference between full-information and bandit feedback settings?

- Fine-Tuning Methodology: Understanding supervised fine-tuning techniques is essential for grasping the implementation details
  - Why needed: The core mechanism relies on iterative supervised learning on selected trajectories
  - Quick check: Can you outline the key differences between full fine-tuning and parameter-efficient fine-tuning approaches?

## Architecture Onboarding
**Component Map:** Decision Environment -> LLM Agent -> Trajectory Generator -> Regret Calculator -> Selector -> Fine-Tuning Module -> Updated LLM Agent

**Critical Path:** The most critical path is Environment -> Agent -> Regret Calculation -> Selection -> Fine-Tuning, as this loop drives the iterative improvement process. The quality of regret calculation and trajectory selection directly impacts the effectiveness of subsequent fine-tuning.

**Design Tradeoffs:** The method trades computational efficiency for improved decision quality, as generating multiple trajectories and computing regret for each iteration increases computational overhead. The selection of k trajectories balances exploration (generating diverse trajectories) with exploitation (focusing on known good trajectories).

**Failure Signatures:** Potential failure modes include getting stuck in local optima where the model consistently generates similar low-regret trajectories, or computational infeasibility when action spaces become too large for efficient trajectory generation and regret calculation.

**First Experiments:** 1) Implement the regret calculation module and validate it produces expected values on simple bandit problems. 2) Test the trajectory selection mechanism on synthetic data to ensure it correctly identifies k-lowest regret instances. 3) Run a single iteration of the full loop on a simple environment to verify the complete pipeline functions correctly.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can Iterative RMFT be effectively extended to sequential decision-making tasks with complex state spaces, such as Markov Decision Processes (MDPs)?
- Basis: The conclusion lists "generalizing to other DM environments with richer structures such as... Markov decision processes" as a future direction.
- Why unresolved: The current work focuses on bandits and online learning with full/bandit feedback, lacking the temporal credit assignment challenges present in MDPs.
- What evidence would resolve it: Successful application and regret analysis of LLMs trained via Iterative RMFT on standard RL benchmarks (e.g., Gymnasium) or multi-step planning tasks.

### Open Question 2
- Question: How does Iterative RMFT perform on genuinely long-horizon interactions where the context window limit of LLMs is exceeded?
- Basis: The authors note training was limited to short horizons and call for future work on "genuinely long-horizon interactions."
- Why unresolved: The paper focuses on horizons $T=25-100$; performance degradation or failure modes resulting from long context dependencies remain untested.
- What evidence would resolve it: Empirical results on tasks with horizons $T > 1000$ (e.g., long-term planning agents) utilizing context management or external memory strategies.

### Open Question 3
- Question: Does the regret minimization observed in synthetic tasks transfer to "real-world" agentic workflows like tool-use or software engineering?
- Basis: The conclusion highlights "evaluating the trained LLM agents on more real-world DM applications such as tool-use... and software engineering" as a key gap.
- Why unresolved: Current experiments rely on procedurally generated scenarios; real-world dynamics are noisier, sparser, and harder to define than the controlled environments tested.
- What evidence would resolve it: Benchmarks on real-world agent suites (e.g., SWE-bench or WebShop) comparing RMFT agents against prompting-based baselines.

## Limitations
- Computational efficiency concerns due to multiple trajectory generation and regret calculation requirements
- Limited evaluation on real-world deployment scenarios and edge cases
- Focus on synthetic environments may not capture the complexity of practical applications

## Confidence
**High Confidence:** The core regret-minimization framework and its theoretical justification (particularly the connection to FTRL) are well-established. The iterative fine-tuning methodology is clearly articulated and reproducible.

**Medium Confidence:** The empirical results showing regret reduction and improved decision-making are convincing, though the evaluation could benefit from more diverse real-world scenarios and longer-term deployment studies.

**Medium Confidence:** The claim about language-compatibility and flexibility is supported by results across different model families, but the paper doesn't fully explore limitations when applying this to highly specialized domains or tasks requiring domain-specific knowledge.

## Next Checks
1. Conduct stress tests on the computational efficiency of Iterative RMFT, measuring training time and resource requirements across different action space sizes and trajectory counts, particularly for deployment in resource-constrained environments.

2. Evaluate the approach's performance and robustness in multi-agent settings where multiple LLM agents interact, potentially creating feedback loops or strategic behaviors not captured in single-agent evaluations.

3. Test generalization to highly specialized domains (e.g., medical diagnosis, legal reasoning, or scientific discovery) where domain-specific knowledge and precision are critical, assessing whether the regret-minimization approach maintains performance advantages in these contexts.