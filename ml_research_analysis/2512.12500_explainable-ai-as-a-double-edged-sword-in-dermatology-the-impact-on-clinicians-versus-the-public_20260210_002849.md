---
ver: rpa2
title: 'Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians
  versus The Public'
arxiv_id: '2512.12500'
source_url: https://arxiv.org/abs/2512.12500
tags:
- explanation
- accuracy
- skin
- other
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how explainable AI (XAI) methods, particularly
  multimodal large language models (LLMs), affect diagnostic performance across different
  expertise levels in dermatology. Two large-scale experiments were conducted: one
  with 623 general public participants performing binary melanoma vs.'
---

# Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public

## Quick Facts
- arXiv ID: 2512.12500
- Source URL: https://arxiv.org/abs/2512.12500
- Reference count: 0
- Primary result: AI assistance improved accuracy and reduced skin tone disparities, but LLM explanations showed divergent effects: lay users showed higher automation bias while experienced PCPs remained resilient.

## Executive Summary
This study investigates how explainable AI (XAI) methods, particularly multimodal large language models (LLMs), affect diagnostic performance across different expertise levels in dermatology. Two large-scale experiments were conducted: one with 623 general public participants performing binary melanoma vs. nevus classification, and another with 153 primary care physicians (PCPs) performing open-ended differential diagnosis of skin diseases. The study found that AI assistance improved overall accuracy and reduced diagnostic disparities across skin tones. However, LLM explanations showed divergent effects: while lay users showed higher automation bias (accuracy increased when AI was correct but decreased when AI was wrong), experienced PCPs remained resilient and benefited from AI assistance regardless of AI accuracy. The study also found that presenting AI suggestions first led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI.

## Method Summary
The study used a fairness-constrained deep learning model (ViT-B/32 for binary classification, DenseNet-121 for multi-class) trained with Conditional Domain Adversarial Neural Network (CDANN) to reduce skin tone disparity. Two large-scale user studies were conducted with 623 general public participants and 153 PCPs. The study compared three XAI methods: GradCAM (visual saliency), CBIR (content-based image retrieval), and LLM explanations (GPT-4V text). Participants were randomly assigned to "Human-First" (diagnose then receive AI) or "AI-First" (receive AI then diagnose) workflows. The primary metrics were accuracy, AUROC, and skin tone disparity reduction.

## Key Results
- AI assistance improved accuracy and reduced skin tone disparities for both lay users and PCPs
- LLM explanations amplified automation bias in lay users but PCPs remained resilient regardless of AI accuracy
- Presenting AI suggestions first led to worse outcomes when AI was incorrect for both groups
- Lay users showed higher automation bias than PCPs, with accuracy increasing when AI was correct but decreasing when AI was wrong

## Why This Works (Mechanism)

### Mechanism 1: Expertise as a Cognitive Firewall
Clinical expertise mitigates automation bias when AI predictions are incorrect, whereas a lack of expertise amplifies over-reliance on AI explanations. Domain experts use AI explanations to validate pre-existing hypotheses against structured clinical knowledge, while lay users use them as the primary basis to form beliefs. When AI is wrong, experts can spot inconsistencies while lay users are persuaded by authoritative output.

### Mechanism 2: Narrative Persuasion in LLM Explanations
Semantic explanations generated by LLMs amplify user deference more than visual-based XAI methods because they create an "illusion of understanding." LLMs generate fluent, cohesive narratives that connect visual features to diagnostic conclusions using authoritative prose, requiring less cognitive effort than abstract heatmaps or visual comparisons.

### Mechanism 3: Anchoring via Interaction Order
The temporal order of human-AI interaction modulates diagnostic accuracy and bias. Presenting AI suggestions before the human forms an independent judgment ("AI-First") acts as an anchor, limiting cognitive search space for alternative diagnoses. "Human-First" workflows force users to commit to a diagnosis, creating cognitive friction that must be overcome to change their mind later.

## Foundational Learning

- **Automation Bias & Algorithm Aversion**
  - Why needed here: To understand why users follow incorrect AI advice and how bias interacts with user expertise and explanation type
  - Quick check question: Does providing an explanation increase or decrease the user's ability to detect an AI error?

- **Post-hoc Explainable AI (XAI) Modalities**
  - Why needed here: The mechanism differs by XAI type; understanding the difference between visual saliency, retrieval-based evidence, and semantic reasoning is crucial
  - Quick check question: Which XAI modality provides a "narrative story" vs. "visual evidence"?

- **Anchoring Bias**
  - Why needed here: To comprehend the impact of "AI-First" vs. "Human-First" workflows and how information presentation order changes cognitive processing
  - Quick check question: If a user sees a diagnosis of "Melanoma" before looking at the image, how might that change what visual features they notice?

## Architecture Onboarding

- **Component map:** Fairness-Constrained Model -> XAI Generators (GradCAM, CBIR, LLM) -> Interaction Interface (Human-First/AI-First) -> User Layer (PCPs/Lay Users)
- **Critical path:** Explanation Quality -> User Trust -> Decision Accuracy. If the XAI Generator produces a persuasive but incorrect narrative and the User lacks expertise to verify it, accuracy degrades significantly.
- **Design tradeoffs:** LLM vs. Visual XAI (LLMs offer higher interpretability but introduce hallucination risk); Human-First vs. AI-First (AI-First boosts speed when right but creates high risk when wrong)
- **Failure signatures:** "Double-Edged Sword" Failure (lay user confidently agrees with incorrect diagnosis due to plausible LLM explanation); Anchoring Failure (expert PCP changes correct diagnosis to incorrect one after AI-First suggestion)
- **First 3 experiments:**
  1. A/B Test Explanation Type on Lay Users: Compare performance degradation on "AI-Wrong" cases between LLM group and GradCAM group to verify narrative persuasion risk
  2. Expertise Deference Analysis: Measure correlation between PCPs' years of experience and propensity to switch when AI disagrees to validate "Cognitive Firewall" hypothesis
  3. Interaction Order Stress Test: Force "AI-First" condition on PCPs using synthetic dataset with 60% AI accuracy to observe if anchoring bias overrides expertise

## Open Questions the Paper Calls Out

- **Question 1:** Do the effects of LLM-based XAI on automation bias and diagnostic performance replicate in other medical domains outside of dermatology?
  - Basis: The Conclusion states "Our findings should be replicated in other medical domains"
  - Why unresolved: Study focused exclusively on image-based skin condition diagnosis
  - What evidence would resolve it: Replicating the experimental design with clinicians and laypeople in non-dermatology medical domains

- **Question 2:** How does the inclusion of richer patient context (e.g., medical history, demographics, environment) alter the impact of XAI on diagnostic accuracy and equity?
  - Basis: The Conclusion states findings should be "verified with additional patient information"
  - Why unresolved: Participants relied solely on clinical images, departing from real-world practice
  - What evidence would resolve it: Conducting user studies where AI suggestions and explanations are presented alongside EHR data and patient history

- **Question 3:** Can specific design interventions, such as adaptive explanations or mandatory professional validation disclaimers, effectively mitigate the automation bias in lay users?
  - Basis: The Discussion suggests designs might "explicitly show the AI's uncertainty... or frame suggestions as preliminary information"
  - Why unresolved: Study identified vulnerability of lay users to LLMs but did not test specific interface designs
  - What evidence would resolve it: Comparative study testing "standard" XAI against "safety-enhanced" XAI interfaces

## Limitations
- User expertise validation: PCP expertise as cognitive firewall was assumed but not empirically tested through knowledge assessments
- LLM explanation fidelity: Potential hallucination in LLM explanations was acknowledged but not quantified in frequency or impact
- Generalizability: Results based on controlled online experiments; real-world clinical workflows may differ in timing pressure and decision context

## Confidence
- **High Confidence:** AI assistance improves accuracy and reduces skin tone disparities (supported by robust experimental data)
- **Medium Confidence:** Expertise mitigates automation bias (plausible mechanism, but not directly validated through user skill assessment)
- **Low Confidence:** LLM explanations are more persuasive than visual XAI (mechanism is theorized but not experimentally isolated)

## Next Checks
1. **Expertise Firewall Validation:** Measure the correlation between PCPs' years of dermatological experience and their propensity to switch diagnoses when AI disagrees
2. **LLM Explanation Quality Audit:** Manually review a sample of LLM-generated explanations to quantify hallucination frequency and its impact on user decisions
3. **Interaction Order Stress Test:** Force the "AI-First" condition on PCPs using a synthetic dataset where AI accuracy is intentionally lowered (e.g., 60%) to observe if anchoring bias overrides expertise