---
ver: rpa2
title: 'Survey of GenAI for Automotive Software Development: From Requirements to
  Executable Code'
arxiv_id: '2507.15025'
source_url: https://arxiv.org/abs/2507.15025
tags:
- code
- automotive
- generation
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys Generative AI adoption in automotive software
  development, covering requirements handling, compliance, code generation, and optimization.
  It identifies Large Language Models (LLMs), Retrieval Augmented Generation (RAG),
  Vision-Language Models (VLMs), and prompting techniques as core methodologies.
---

# Survey of GenAI for Automotive Software Development: From Requirements to Executable Code

## Quick Facts
- arXiv ID: 2507.15025
- Source URL: https://arxiv.org/abs/2507.15025
- Reference count: 40
- Primary result: Identifies GenAI adoption patterns in automotive development, finding 100% industry adoption with productivity gains but highlighting dataset scarcity and IP constraints as key barriers.

## Executive Summary
This survey examines the integration of Generative AI (GenAI) tools across the automotive software development lifecycle, from requirements engineering to executable code generation. The authors analyze current practices, methodologies, and challenges faced by industry practitioners. They identify GPT-based models as dominant for code generation tasks while locally deployable, fine-tuned models like Llama3 are preferred for requirements processing due to data privacy concerns. The study reveals that while 100% of surveyed industry participants report using GenAI tools with improved productivity, requirements handling remains underutilized due to intellectual property protection and lack of domain-specific datasets. The work emphasizes the need for specialized automotive datasets and fine-tuning strategies to enable broader GenAI integration in safety-critical automotive systems.

## Method Summary
The paper conducts a comprehensive survey of GenAI adoption in automotive software development through systematic literature review and practitioner interviews. The methodology synthesizes findings from 40 references covering requirements handling, compliance checking, code generation, and optimization tasks. The authors analyze different GenAI approaches including Large Language Models (LLMs), Retrieval Augmented Generation (RAG), Vision-Language Models (VLMs), and prompting techniques. They examine deployment strategies comparing cloud-based solutions (GPT-4) with locally deployable models (Llama3) based on privacy requirements. The survey incorporates feedback from industry practitioners to validate findings and identify practical challenges, particularly around data protection and the need for domain-specific fine-tuning.

## Key Results
- GPT-based models dominate code generation tasks, while Llama3 and similar models are preferred for requirements processing to protect intellectual property
- RAG pipelines effectively ground generation in regulatory documents (UN157, ISO 26262) to reduce hallucination risks in compliance-heavy tasks
- 100% of industry survey respondents report using GenAI tools, with most experiencing improved productivity, though requirements handling adoption remains limited
- The lack of publicly available automotive-specific datasets is identified as the primary bottleneck for broader GenAI integration in requirements engineering

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-Augmented Generation (RAG) appears to mitigate hallucination risks in compliance-heavy automotive tasks by grounding generation in specific regulatory documents.
- **Mechanism:** Instead of relying solely on parametric memory, the system indexes external standards (e.g., UN157, ISO 26262) into vector stores. During inference, relevant chunks are retrieved and injected into the prompt, providing the LLM with the specific context required for accurate scenario generation or compliance checking.
- **Core assumption:** The retrieval mechanism can accurately map natural language queries to dense vector representations of regulatory text, and the context window is sufficient to hold the retrieved chunks without losing critical instructions.
- **Evidence anchors:**
  - [Background II.C] states RAG injects external knowledge to enable "domain-grounded and accurate code" and reduces risk in regulatory processing.
  - [Section VI] confirms RAG is used to "automate the retrieval of relevant regulatory documents" and extract design constraints.
  - [Corpus] Related work ("Automating Automotive Software Development") supports the synergy of GenAI and formal methods, indirectly validating the need for grounded constraints.
- **Break condition:** If the vector database is stale or chunking strategies sever critical logical connections (e.g., separating a requirement from its exception clause), the model will generate "compliant" code that is actually non-conformant.

### Mechanism 2
- **Claim:** Locally deployable models (e.g., Llama 3) likely enable requirements automation where commercial cloud services cannot, primarily by resolving data privacy conflicts.
- **Mechanism:** Proprietary requirements are processed inside the organization's trust boundary. To compensate for the lower capacity of smaller models compared to GPT-4, these models undergo fine-tuning on domain-specific internal data, effectively shifting the capability curve for niche tasks.
- **Core assumption:** Sufficient domain-specific training data exists internally to fine-tune these models, and the hardware is available to host them with acceptable latency.
- **Evidence anchors:**
  - [Introduction] notes that requirements exposure is often "limited within organizational boundaries," necessitating local models.
  - [Section V] observes that "locally deployable, fine-tuned models are preferable" for requirements handling tasks.
  - [Corpus] Neighbor papers (e.g., "Generating Automotive Code") acknowledge the sensitivity of safety-critical systems, though specific IP constraints are detailed primarily in the main text.
- **Break condition:** If fine-tuning datasets are sparse or noisy, the local model may underperform significantly compared to general-purpose cloud models, leading to a trade-off between security and utility.

### Mechanism 3
- **Claim:** Indirect code generation via intermediate formal representations (Model-Driven Engineering) provides a verification step that direct code generation lacks.
- **Mechanism:** The LLM generates a formal model instance (e.g., Ecore, SysML, or Matlab Simulink) rather than raw C code. This intermediate artifact undergoes structural validation (e.g., metamodel compliance, OCL checks) before being compiled or transformed into executable code.
- **Core assumption:** The LLM is capable of generating syntactically correct formal models consistently, and the verification tools can catch semantic errors that pass syntax checks.
- **Evidence anchors:**
  - [Section IV] depicts a workflow where LLMs summarize requirements into "formal representation" before code generation.
  - [Section VII] distinguishes "direct" approaches from "indirect" ones, noting the latter "introduces the ability to verify model-level correctness."
  - [Corpus] No direct contradiction found; related papers support safety-critical verification steps.
- **Break condition:** If the LLM hallucinates a valid-looking but semantically hollow model structure that passes the verifier but maps to incorrect logic, the safety case fails.

## Foundational Learning

- **Concept: Retrieval Augmented Generation (RAG)**
  - **Why needed here:** Automotive standards (e.g., Euro 7) are too voluminous and dynamic to fit in a model's context window or training data. RAG is the proposed solution for "regulation-aware" generation.
  - **Quick check question:** Can you explain the difference between the "indexing" phase (offline) and the "retrieval" phase (runtime) in a RAG pipeline?

- **Concept: Model-Driven Engineering (MDE) & Metamodeling**
  - **Why needed here:** The paper relies on intermediate formal models (Ecore, SysML) to bridge the gap between ambiguous natural language requirements and strict safety-critical code.
  - **Quick check question:** If an LLM generates a model instance that violates the defined metamodel constraints, should the error be fixed by the LLM or the verification tool?

- **Concept: Chain of Thought (CoT) Prompting**
  - **Why needed here:** Automotive logic (e.g., emergency braking) is safety-critical. CoT is identified in the paper as a dominant strategy to improve reasoning and transparency compared to direct prompting.
  - **Quick check question:** Why does asking a model to "think step-by-step" reduce the likelihood of logical errors in safety-critical code generation?

## Architecture Onboarding

- **Component map:**
  Document Ingestion → VLM/Text Extraction → **RAG Retrieval** → Formal Model Generation (LLM) → **Model Verification** → Code Generation → Simulation/Human Review

- **Critical path:**
  Document Ingestion → VLM/Text Extraction → **RAG Retrieval** → Formal Model Generation (LLM) → **Model Verification** → Code Generation → Simulation/Human Review

- **Design tradeoffs:**
  - **Cloud vs. Local:** Using commercial APIs (GPT-4) offers superior code generation capability but violates IP boundaries for requirements. The architecture likely requires a hybrid approach: Local models for "front-end" requirements processing and Cloud models for "back-end" generic code generation.
  - **Direct vs. Indirect Generation:** Direct generation is faster but riskier; Indirect (via MDE) is slower and requires maintaining a toolchain but offers safety guarantees required by ISO 26262.

- **Failure signatures:**
  - **Degeneration-of-Thought (DoT):** Agents confidently reinforcing incorrect logic in multi-agent debates.
  - **Context Drift:** Loss of critical safety constraints in long-duration prompting sessions.
  - **Syntactic Hallucination:** Generated code compiles but violates runtime safety constraints (e.g., memory overflow) or MISRA rules.

- **First 3 experiments:**
  1. **RAG Compliance Retrieval:** Build a minimal RAG pipeline using a sample regulatory PDF (e.g., UN157) and test if the LLM can accurately retrieve speed thresholds for Emergency Braking (AEB) scenarios.
  2. **Hybrid Model Comparison:** Feed a sanitized requirement to both a local Llama3 instance and GPT-4. Compare the syntactic correctness of the generated Ecore model instances to determine the "privacy penalty" in performance.
  3. **Prompt Strategy A/B Test:** Generate a simple vehicle function (e.g., Cruise Control) using "Direct Prompting" vs. "Chain of Thought." Measure the difference in static analysis errors (using Cppcheck) in the generated C code.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the field overcome the lack of publicly available specialized automotive datasets required to fine-tune locally deployable LLMs for requirements handling?
- Basis in paper: [explicit] The authors identify the lack of domain-specific datasets caused by data protection policies as the "main bottleneck" preventing broader GenAI integration in requirements engineering.
- Why unresolved: Automotive requirements are valuable assets protected by NDAs and IP laws, making data sharing difficult and hindering the creation of training corpora.
- What evidence would resolve it: The creation of synthetic datasets or secure, federated fine-tuning frameworks that yield comparable performance to commercial, general-purpose models.

### Open Question 2
- Question: What specific methodologies are required to effectively detect and mitigate visual hallucinations in Vision-Language Models (VLMs) used for automotive diagram interpretation?
- Basis in paper: [inferred] Section VIII includes a subsection title for "Visual hallucinations" but lacks content, while Section X emphasizes the critical need for VLMs to process UML and technical diagrams in automotive documents.
- Why unresolved: While textual hallucination mitigation (e.g., multi-agent debate) is discussed, the application of verification techniques to visual inputs remains undefined in the provided text.
- What evidence would resolve it: Validation of VLM architectures on automotive-specific diagram benchmarks demonstrating quantifiable reductions in object detection or relation extraction errors.

### Open Question 3
- Question: To what extent can LLMs automate hardware abstraction tasks, specifically mapping Vehicle Signal Specifications (VSS) to hardware interfaces?
- Basis in paper: [explicit] The conclusion explicitly lists "hardware abstraction-related works aiming automotive systems, including interface and signal vehicle mapping... using LLMs" as a planned area for future coverage.
- Why unresolved: Current works focus predominantly on high-level software and simulation code, leaving the translation layer between abstract signals and physical hardware configurations underexplored.
- What evidence would resolve it: Successful implementation of an LLM-agent capable of parsing VSS catalogs and generating correct configuration files for specific Electronic Control Units (ECUs).

## Limitations
- The survey relies on practitioner self-reporting rather than controlled experiments, making productivity claims subjective rather than objectively validated
- Effectiveness of RAG pipelines and fine-tuned models depends on proprietary automotive datasets that are inaccessible for independent verification
- The hybrid cloud/local deployment approach assumes sufficient hardware resources for local model hosting without quantifying computational overhead

## Confidence
- **High Confidence:** The identification of GenAI adoption patterns (GPT-4 for code generation, Llama3 for requirements handling) is well-supported by industry survey data and practitioner interviews
- **Medium Confidence:** The proposed mechanisms for RAG effectiveness and hybrid deployment are theoretically sound but lack empirical validation through controlled experiments
- **Low Confidence:** The survey's productivity claims are based on subjective self-reporting rather than objective performance metrics, making quantitative assessment difficult

## Next Checks
1. **RAG Compliance Retrieval Test:** Build and test a minimal RAG pipeline using a sample UN157 PDF to verify accurate retrieval of specific speed thresholds for Emergency Braking scenarios, measuring both precision and recall
2. **Hybrid Model Performance Benchmark:** Compare the syntactic correctness of Ecore model instances generated by local Llama3 versus GPT-4 when processing the same sanitized requirements, quantifying the "privacy penalty" in performance
3. **Prompt Strategy Impact Analysis:** Generate a simple vehicle function (Cruise Control) using both Direct Prompting and Chain-of-Thought approaches, measuring the difference in static analysis errors using Cppcheck to validate the claimed reasoning improvements