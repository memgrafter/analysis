---
ver: rpa2
title: 'FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware
  ANNS Systems'
arxiv_id: '2601.09985'
source_url: https://arxiv.org/abs/2601.09985
tags:
- memory
- residual
- fatrq
- distance
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high latency caused by SSD-based refinement
  in large-scale ANNS systems, where modern embeddings require full-precision vector
  fetches for accurate reranking. The proposed solution, FaTRQ, introduces a tiered
  memory architecture that stores compact ternary residual codes in far memory (e.g.,
  CXL memory) and progressively refines coarse distance estimates without reconstructing
  full vectors.
---

# FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems

## Quick Facts
- arXiv ID: 2601.09985
- Source URL: https://arxiv.org/abs/2601.09985
- Authors: Tianqi Zhang; Flavio Ponzina; Tajana Rosing
- Reference count: 40
- Primary result: 2.6× to 9.4× higher throughput than GPU ANNS systems via tiered residual quantization

## Executive Summary
FaTRQ addresses the high latency bottleneck in large-scale ANNS systems where SSD-based refinement dominates query time. The method introduces a tiered memory architecture that stores compact ternary residual codes in far memory and progressively refines coarse distance estimates without reconstructing full vectors. By decomposing L2 distance into precomputable and refinement terms, FaTRQ enables early pruning of non-promising candidates. The approach achieves 2.6× to 9.4× higher throughput and 2.4× better storage efficiency compared to prior refinement schemes.

## Method Summary
FaTRQ uses progressive distance estimation via L2 decomposition, where ‖x − q‖² = ‖xc − q‖² + ‖xc − x‖² − 2⟨q − xc, x − xc⟩. Residual vectors are encoded as sparse ternary values {−1, 0, 1} to enable multiplication-free refinement, achieving ~1.6 bits per dimension compression. A boundary-aware calibration model trained on sampled neighbor pairs improves recall by optimizing local ranking accuracy near decision boundaries. The system integrates with FAISS/cuVS, storing coarse PQ codes in fast memory and ternary residuals in CXL-attached far memory, with a hardware accelerator prototype demonstrating 2.6×-9.4× throughput gains.

## Key Results
- 2.6× to 9.4× higher throughput than state-of-the-art GPU ANNS systems
- 2.4× better storage efficiency compared to 4-bit scalar quantization refinement schemes
- 99% recall@10 achievable with 2.8× fewer SSD reads through early pruning
- 0.0159 MSE vs. 0.258 for 3-bit scalar quantization on Wiki dataset

## Why This Works (Mechanism)

### Mechanism 1: Progressive Distance Estimation via L2 Decomposition
Distance can be refined incrementally without reconstructing full vectors by decomposing L2 distance into coarse and residual terms. The decomposition ‖x − q‖² = ‖xc − q‖² + ‖xc − x‖² − 2⟨q − xc, x − xc⟩ enables streaming compact residuals from far memory rather than fetching full vectors from SSD. This works under the assumption that residual vectors are nearly isotropic and uncorrelated with queries after coarse quantization.

### Mechanism 2: Ternary Residual Encoding for Multiplication-Free Refinement
Encoding residuals as sparse ternary vectors {−1, 0, 1} enables efficient inner product computation while achieving ~1.6 bits per dimension compression. The encoding selects top-k magnitude dimensions and assigns signs, reducing ⟨q, δ⟩ estimation to additions/subtractions. Five ternary values pack into one byte (base-3 encoding), avoiding multiplications entirely.

### Mechanism 3: Boundary-Aware Calibration Model
A lightweight linear calibration model trained on boundary-region samples improves recall more than minimizing global distance MSE. The model samples ~0.3% of database vectors, uses index structure