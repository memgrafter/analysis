---
ver: rpa2
title: 'Pisets: A Robust Speech Recognition System for Lectures and Interviews'
arxiv_id: '2601.18415'
source_url: https://arxiv.org/abs/2601.18415
tags:
- speech
- whisper
- recognition
- audio
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Pisets, a three-component speech recognition
  system designed to improve accuracy while reducing hallucinations in the Whisper
  model. The system uses Wav2Vec2 for initial speech segmentation, AST for false positive
  filtering, and Whisper for final transcription.
---

# Pisets: A Robust Speech Recognition System for Lectures and Interviews

## Quick Facts
- arXiv ID: 2601.18415
- Source URL: https://arxiv.org/abs/2601.18415
- Reference count: 14
- Primary result: WER of 0.1065 vs WhisperX's 0.1683 on TED-LIUM 3

## Executive Summary
Pisets is a three-component speech recognition system designed to improve accuracy while reducing hallucinations in the Whisper model for lecture and interview domains. The system combines Wav2Vec2 for speech segmentation, AST for false positive filtering, and Whisper for final transcription, achieving a WER of 0.1065 on TED-LIUM 3. The authors introduce uncertainty modeling techniques to identify unreliable transcriptions, with experimental results showing up to 35% of errors can be identified by marking only 5% of words as uncertain. The system is publicly available on GitHub.

## Method Summary
Pisets employs a three-stage pipeline architecture: Wav2Vec2 for initial speech segmentation and diarization, AST (Audio Spectrogram Transformer) for filtering false positives and detecting unreliable segments, and Whisper for final transcription. The system incorporates uncertainty modeling techniques to highlight potentially unreliable transcriptions, addressing the hallucination problem common in large language model-based ASR systems. The architecture is designed specifically for lecture and interview domains where speaker turns and varying audio quality present unique challenges.

## Key Results
- Pisets achieves WER of 0.1065 compared to WhisperX's 0.1683 (36.8% relative improvement)
- BERT score of 0.9652 indicates high semantic quality
- Uncertainty modeling identifies up to 35% of errors by marking only 5% of words as uncertain
- System is publicly available on GitHub for reproducibility

## Why This Works (Mechanism)
The three-stage pipeline architecture reduces error propagation by incorporating domain-specific filtering at multiple stages. Wav2Vec2 provides robust initial segmentation tailored to lecture formats, while AST filtering removes unreliable segments before they reach the transcription model. Uncertainty modeling leverages confidence scores from intermediate components to identify and flag potential errors, reducing hallucinations by preventing the system from generating content for unreliable audio segments.

## Foundational Learning

**Wav2Vec2 Speech Segmentation**
- Why needed: Provides accurate speaker diarization and segment boundaries for lectures with multiple speakers
- Quick check: Verify segment boundaries align with natural speaker turn boundaries in sample data

**AST False Positive Filtering**
- Why needed: Removes unreliable audio segments before transcription to prevent hallucination propagation
- Quick check: Measure false positive rate reduction after AST filtering on noisy lecture segments

**Whisper Uncertainty Modeling**
- Why needed: Identifies potentially unreliable transcriptions using confidence scores from intermediate stages
- Quick check: Calculate uncertainty score distribution and compare with known error positions

## Architecture Onboarding

**Component Map**
Wav2Vec2 -> AST Filtering -> Whisper Transcription

**Critical Path**
Audio input → Wav2Vec2 segmentation → AST filtering → Whisper transcription → Uncertainty estimation → Output

**Design Tradeoffs**
- Three-stage pipeline increases accuracy but adds latency and computational complexity
- AST filtering reduces hallucinations but may remove valid content in edge cases
- Uncertainty modeling improves reliability but requires additional post-processing

**Failure Signatures**
- Missing speaker turns due to aggressive AST filtering
- Latency spikes during high-complexity segments requiring full pipeline processing
- Uncertainty overestimation leading to excessive word flagging

**3 First Experiments**
1. Measure end-to-end latency on sample lecture files of varying lengths
2. Compare WER reduction on additional lecture/interview datasets
3. Test uncertainty estimation performance on live transcription with real-time constraints

## Open Questions the Paper Calls Out

None

## Limitations
- Single dataset evaluation (TED-LIUM 3) may not generalize to other lecture and interview domains
- Three-stage pipeline complexity and computational overhead not fully characterized for real-world deployment
- Uncertainty modeling performance based on post-hoc analysis rather than real-time implementation
- Hallucination reduction claims lack direct quantitative comparison to baseline hallucination rates

## Confidence
- High confidence: Technical architecture description and GitHub availability
- Medium confidence: WER improvements given single-dataset evaluation
- Low confidence: Hallucination reduction claims and uncertainty modeling practical utility

## Next Checks
1. Evaluate on at least two additional lecture/interview datasets to verify generalizability
2. Conduct end-to-end latency measurements including all three pipeline stages under realistic conditions
3. Implement real-time uncertainty estimation and measure false positive/negative rates in live transcription scenarios