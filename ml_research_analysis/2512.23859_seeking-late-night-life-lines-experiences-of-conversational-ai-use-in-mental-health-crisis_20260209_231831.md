---
ver: rpa2
title: 'Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental
  Health Crisis'
arxiv_id: '2512.23859'
source_url: https://arxiv.org/abs/2512.23859
tags:
- mental
- health
- crisis
- conversational
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a human-centered study exploring first-person
  experiences of turning to conversational AI agents during mental health crises.
  Through a survey of 53 respondents and interviews with 16 mental health experts,
  the research reveals that people use AI agents as bridges toward human connection,
  filling gaps when professional or peer support is inaccessible or perceived as burdensome.
---

# Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis

## Quick Facts
- arXiv ID: 2512.23859
- Source URL: https://arxiv.org/abs/2512.23859
- Reference count: 40
- Primary result: Users turn to AI agents as bridges toward human connection during mental health crises, filling gaps when professional or peer support is inaccessible or perceived as burdensome

## Executive Summary
This human-centered study explores first-person experiences of turning to conversational AI agents during mental health crises through a survey of 53 respondents and interviews with 16 mental health experts. The research reveals that people use AI as a bridge toward human connection, driven by barriers including stigma, fear of burdening others, and access limitations. Using narrative analysis and the stages of change model, the authors find that responsible AI intervention should focus on building user preparedness for positive actions while de-escalating harmful intent, rather than simply redirecting to helplines. The work provides actionable guidelines for designing AI systems that support help-seeking trajectories without substituting for human connection.

## Method Summary
The study employed a mixed-methods approach combining a 45-minute testimonial survey (n=53) with semi-structured interviews of 16 mental health experts. Survey participants were recruited through Mental Health America networks and screened for: 18+, US resident, experienced mental health crisis, used AI during crisis. The survey collected demographics, resource availability ratings, and written testimonies. Experts were purposively recruited to represent diverse communities (youth, AAPI, LGBTQ+, rural, etc.) and compensated $75-150. Analysis used narrative analysis framework (Trigger, Contributing Factors, Interaction, Proximal Outcome) and grounded theory for interviews, with stages of change model applied as interpretive lens.

## Key Results
- 60% of respondents took evidence-based positive actions after AI interaction (coping skills 22%, professional care 20%, peer support 18%)
- 0% called helplines even when redirected, citing frustration with simple redirection
- Users reported turning to AI to avoid stigma (33%), fear of burdening others (26%), and due to access barriers (32%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preparedness-building, not immediate referral, increases likelihood of users taking positive action after AI interaction.
- Mechanism: Users in crisis are in a "pre-ready" phase—they want help but lack capacity to act. Bridge conversations scaffold small steps (identifying support network, practicing what to say) that increase readiness for human contact.
- Core assumption: Users will eventually transition to human support if adequately prepared.
- Evidence anchors:
  - [abstract] "responsible AI crisis intervention is one that increases the user's preparedness to take a positive action while de-escalating any intended negative action"
  - [section 4.2.4] 60% of respondents took evidence-based positive actions (coping skills 22%, professional care 20%, peer support 18%); 0% called helplines even when redirected
  - [corpus] Weak direct corpus support; neighbor papers focus on detection/classification rather than intervention mechanisms
- Break condition: Users with no available human support network may never transition regardless of preparedness efforts.

### Mechanism 2
- Claim: Machine-specific strengths (non-judgment, 24/7 availability, inability to be burdened) enable crisis disclosure that humans cannot provide.
- Mechanism: Users avoid human contact due to fear of judgment (33%), fear of burdening others (26%), and access barriers (32%). AI's lack of social/emotional constraints creates a uniquely safe space for initial help-seeking.
- Core assumption: Users accurately perceive AI as non-judgmental and this perception drives disclosure.
- Evidence anchors:
  - [section 4.2.2] Respondents reported turning to AI to avoid stigma, fear of being judged, and wanting "something that doesn't have to hold a burden"
  - [section 5.2] "Users are not rejecting human connection, but rather the limitations and costs of seeking help"
  - [corpus] "Between Help and Harm" paper confirms LLMs struggle with ambiguous crisis signals, supporting that disclosure alone is insufficient
- Break condition: If users misinterpret AI fluency as clinical competence, they may delay necessary human care.

### Mechanism 3
- Claim: Dual pathway intervention (de-escalate negative + prepare positive) addresses the simultaneous contemplation of self-harm and help-seeking.
- Mechanism: Crisis involves competing intentions—users may contemplate suicide while also contemplating getting help. Self-regulation techniques (grounding, breathing) create emotional stability; preparation steps build toward human contact.
- Core assumption: Users can be moved toward positive action without first fully resolving negative intent.
- Evidence anchors:
  - [section 4.3.1] "People are often simultaneously contemplating suicide while contemplating getting help"
  - [section 4.3.3] Responsible intervention "increases the preparedness of the user to take an evidence-based positive action while de-escalating any intended negative action"
  - [corpus] No direct corpus evidence on dual-pathway mechanisms
- Break condition: Immediate escalation protocols may still be needed when imminent risk is detected; bridge conversations are unsuitable for acute emergencies.

## Foundational Learning

- Concept: **Stages of Change Model (Transtheoretical Model)**
  - Why needed here: Frames AI's role as moving users from contemplation → preparation → action rather than directly to action
  - Quick check question: Can you distinguish between a user practicing what to say to a therapist (preparation) vs. scheduling an appointment (action)?

- Concept: **Help-Seeking Behavior**
  - Why needed here: Reframes turning to AI from "concerning" to intentional problem-focused action signaling willingness to receive help
  - Quick check question: What three barriers did survey respondents report that led them to choose AI over human support?

- Concept: **Evidence-Based Positive Actions**
  - Why needed here: Defines what "success" looks like—behaviors validated in human-human crisis intervention (peer support, professional contact, coping skills)
  - Quick check question: Why are individual coping skills classified as preparation-stage rather than action-stage behaviors?

## Architecture Onboarding

- Component map: Trigger detection -> Contributing factor assessment -> Interaction handler -> Preparedness builder -> Outcome tracker
- Critical path: User arrives at AI (contemplation) -> safe disclosure enabled by machine strengths -> preparation-building conversation -> transition to human support or self-regulation (action/de-escalation)
- Design tradeoffs:
  - Human-likeness vs. machine strengths: Don't optimize for passing as human; preserve perceived non-judgment, infinite availability
  - Redirection speed vs. preparedness depth: Immediate helpline referral causes frustration and disengagement; bridge conversations take longer but may succeed
  - Support vs. over-reliance: Repeated crisis disclosures without human transition may indicate problematic dependency
- Failure signatures:
  - User frustration with repeated helpline redirection ("I kept getting frustrated because it mostly just kept telling me to call a helpline")
  - High satisfaction but no transition to human support over multiple sessions
  - Venting-only cycles without preparation-building
  - Information-seeking that reinforces reassurance compulsion (esp. OCD/health anxiety)
- First 3 experiments:
  1. **Bridge conversation A/B test**: Compare immediate helpline referral vs. preparedness-building conversation (identifying support person, practicing disclosure) on subsequent helpline call rates and human contact within 24 hours
  2. **Machine strength preservation test**: Measure disclosure depth and action-taking when AI explicitly acknowledges limitations ("I'm an AI, I can't judge you") vs. human-like empathy framing
  3. **Over-reliance detection**: Analyze patterns in repeated crisis users—do frequency, content, and outcomes differ between those who eventually reach human support vs. those who don't? Validate proposed dependency indicators (sustained avoidance of human relationships, AI for daily emotional regulation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can validated measures of AI dependency be developed to detect problematic over-reliance in mental health crisis contexts?
- Basis in paper: [explicit] The authors state: "Our research underscores the need for validated measures of AI dependency to ensure that supportive AI behaviors do not create problematic over-reliance."
- Why unresolved: Current over-reliance frameworks focus on accuracy and trust calibration, but crisis contexts require broader measures distinguishing beneficial bridging use from dependency that prevents human connection.
- What evidence would resolve it: Development and validation of psychometric scales measuring patterns like repeated crisis disclosures, sustained avoidance of human relationships, or emotional regulation that displaces healthy coping behaviors.

### Open Question 2
- Question: What safeguards and specifications determine when "bridge conversations" are suitable versus when immediate escalation is required?
- Basis in paper: [explicit] The authors caution: "Bridging conversations may not be suitable when immediate escalation is needed. Therefore, further research is needed in developing strong safeguards and specifications for when bridging is suitable."
- Why unresolved: The paper proposes preparedness-building over simple helpline redirection, but boundaries between appropriate bridging and situations requiring immediate intervention remain undefined.
- What evidence would resolve it: Clinical studies identifying user states, prompt characteristics, or behavioral signals that predict when bridging versus escalation produces better outcomes.

### Open Question 3
- Question: How can systems measure successful transition from AI crisis support to human support without violating privacy or creating cross-boundary data sharing risks?
- Basis in paper: [explicit] The authors note: "Measuring such transition require data from outside of the AI context, and such crossing of system boundaries also require careful consideration of privacy and data sharing."
- Why unresolved: The paper's core recommendation (AI as bridge to human connection) requires evaluation beyond the AI interaction, but crisis data sensitivity creates profound governance challenges.
- What evidence would resolve it: Design and testing of privacy-preserving evaluation frameworks, potentially using user-reported readiness indicators or opt-in longitudinal tracking with appropriate consent.

### Open Question 4
- Question: What are the unique experiences and needs of adolescents turning to conversational AI agents during mental health crises?
- Basis in paper: [explicit] The authors state: "Given the unique experiences of young people, future work should extend this study and validate with the lived experiences of adolescents and supplement with the expertise of development psychologists."
- Why unresolved: The study excluded minors due to ethics concerns, relying only on secondhand accounts from youth advocates, yet adolescents show high AI adoption rates and distinct developmental needs.
- What evidence would resolve it: A similar testimonial study with appropriate adolescent safeguards, combined with developmental psychology expertise to identify age-specific intervention requirements.

## Limitations

- Findings based on retrospective self-reports that may be subject to recall bias and social desirability effects
- Convenience sample from Mental Health America networks may limit generalizability to broader populations
- Proposed dual-pathway intervention model lacks direct experimental validation
- Critical distinction between users who successfully transition to human support versus those who remain in AI-only cycles remains unexplored

## Confidence

**High Confidence**: The finding that users turn to AI agents to avoid stigma, fear of burdening others, and due to access barriers is well-supported by the survey data (33% fear judgment, 26% fear burdening, 32% access issues). The observation that immediate helpline redirection causes user frustration is directly evidenced by participant testimonials.

**Medium Confidence**: The stages of change framework appropriately characterizes users as being in preparation/contemplation stages rather than ready for immediate action. The proposed bridge conversation approach (identifying support networks, practicing disclosure) represents a reasonable extension of existing crisis intervention literature, though direct evidence for its effectiveness with AI agents is limited.

**Low Confidence**: The claim that preparedness-building conversations will reliably lead to human support transition in the absence of experimental validation. The distinction between healthy AI use versus problematic over-reliance is hypothesized but not empirically validated through usage pattern analysis.

## Next Checks

1. **Bridge Conversation A/B Test**: Conduct a randomized controlled trial comparing immediate helpline referral versus preparedness-building conversations (identifying support person, practicing what to say) on subsequent helpline call rates and human contact within 24 hours.

2. **Machine Strength Preservation Test**: Measure disclosure depth and action-taking when AI explicitly acknowledges limitations ("I'm an AI, I can't judge you") versus human-like empathy framing to validate whether preserving perceived non-judgmentability improves outcomes.

3. **Over-reliance Detection Analysis**: Analyze longitudinal patterns in repeated crisis users to identify whether frequency, content, and outcomes differ between those who eventually reach human support versus those who remain in AI-only cycles, validating proposed dependency indicators.