---
ver: rpa2
title: 'POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization'
arxiv_id: '2509.21737'
source_url: https://arxiv.org/abs/2509.21737
tags:
- optimization
- learning
- polo
- molecule
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces POLO, a multi-turn reinforcement learning
  framework that transforms large language models into sample-efficient molecular
  optimization agents for lead optimization in drug discovery. POLO employs Preference-Guided
  Policy Optimization (PGPO), which extracts learning signals at two levels: trajectory-level
  reinforcement learning reinforces successful optimization strategies, while turn-level
  preference learning ranks intermediate molecules to provide dense comparative feedback.'
---

# POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization

## Quick Facts
- arXiv ID: 2509.21737
- Source URL: https://arxiv.org/abs/2509.21737
- Reference count: 40
- Primary result: POLO achieves 84% average success rate on single-property molecular optimization tasks using only 500 oracle evaluations

## Executive Summary
This paper introduces POLO, a multi-turn reinforcement learning framework that transforms large language models into sample-efficient molecular optimization agents for lead optimization in drug discovery. POLO employs Preference-Guided Policy Optimization (PGPO), which extracts learning signals at two levels: trajectory-level reinforcement learning reinforces successful optimization strategies, while turn-level preference learning ranks intermediate molecules to provide dense comparative feedback. This dual-level approach enables POLO to learn from every molecular evaluation, maximizing sample efficiency. Experiments demonstrate that POLO achieves 84% average success rate on single-property tasks (2.3× better than baselines) and 50% on multi-property tasks using only 500 oracle evaluations, establishing a new state-of-the-art in sample-efficient molecular optimization.

## Method Summary
POLO frames lead optimization as a multi-turn Markov Decision Process where each state encodes the complete optimization history including task instructions, all proposed molecules, and their oracle evaluations. The framework employs a dual-level learning strategy: trajectory-level PPO provides strategic guidance by reinforcing complete optimization paths that reach targets, while turn-level preference learning extracts O(T²) pairwise comparisons from intermediate molecules within each trajectory. This creates dense feedback that amplifies learning signals from O(N) to O(NT²) per N trajectories. The method uses an LLM policy (Qwen2.5-1.5B-Instruct) fine-tuned on synthetic molecule pairs (MolOptIns), then trained via PGPO with combined trajectory and preference losses. Evolutionary inference with elite pool selection and temperature scheduling enables efficient exploration of chemical space.

## Key Results
- Achieves 84% average success rate on single-property tasks (2.3× better than baselines) with only 500 oracle evaluations
- Demonstrates 50% success rate on challenging multi-property optimization tasks
- Shows selective cross-task transfer patterns: JNK3→QED transfer achieves 76% success, while DRD2→plogP achieves only 5%

## Why This Works (Mechanism)

### Mechanism 1: Dual-Level Learning Signal Extraction
- **Claim:** Extracting learning signals at both trajectory-level and turn-level significantly improves sample efficiency in budget-constrained molecular optimization.
- **Mechanism:** Trajectory-level PPO provides strategic guidance by reinforcing complete optimization paths that reach targets. Turn-level preference learning extracts O(T²) pairwise comparisons from intermediate molecules within each trajectory, creating dense feedback. Combined objective: J_PGPO(θ) = J_traj(θ) + λ_pref·J_pref(θ). This amplifies learning signals from O(N) to approximately O(NT²) per N trajectories, maximizing value from each costly oracle call.
- **Core assumption:** Intermediate molecules within trajectories contain learnable preference signals that correlate with successful optimization strategies.
- **Evidence anchors:**
  - [abstract] "trajectory-level optimization reinforces successful strategies, while turn-level preference learning provides dense comparative feedback"
  - [Section 2.4, Eq. 5-6] Formalizes preference objective with Lambda weighting from Learning-to-Rank
  - [Appendix I] "SPGPO ≈ N + N·T(T-1)/2 = O(NT²)" — explicit signal amplification analysis
  - [corpus] SEISMO paper similarly uses trajectory-aware learning for molecular optimization, supporting the multi-turn paradigm
- **Break condition:** If intermediate molecular scores are noisy or uncorrelated with final success, preference pairs would provide misleading gradients, degrading rather than improving learning.

### Mechanism 2: Multi-Turn MDP with Complete History Context
- **Claim:** Modeling lead optimization as a multi-turn MDP where states encode complete optimization histories enables strategic, experience-driven refinement rather than isolated single-shot generation.
- **Mechanism:** State s_t includes: (1) task instructions, (2) all proposed molecules (m_0,...,m_t), (3) all oracle evaluations (r_0,...,r_{t-1}). This provides increasingly rich context across turns. The LLM policy π_θ(a_t|s_t) generates structured outputs with reasoning (
- **Core assumption:** Complete optimization histories provide critical context for making informed molecular modifications across turns.
- **Evidence anchors:**
  - [Section 2.3] MDP formulation with complete state definition
  - [Figure 2] Illustrates multi-turn optimization process
  - [corpus] AutoRT paper demonstrates value of history-aware RL in robotics, supporting the multi-turn paradigm

## Foundational Learning

### Preference Learning in Molecular Optimization
- **Why needed:** Traditional reinforcement learning in molecular optimization only learns from terminal rewards, missing valuable information from intermediate molecular evaluations
- **Quick check:** Verify that preference pairs extracted from intermediate molecules improve learning efficiency compared to trajectory-only rewards

### Multi-Turn Decision Making
- **Why needed:** Lead optimization requires iterative refinement across multiple turns, making single-shot generation insufficient for complex property improvements
- **Quick check:** Confirm that the complete history state representation enables better decision-making than context-free generation

### Sample-Efficient RL with LLMs
- **Why needed:** Oracle evaluations in drug discovery are expensive, requiring algorithms that maximize learning from limited queries
- **Quick check:** Measure the number of oracle evaluations needed to achieve target success rates

## Architecture Onboarding

### Component Map
Qwen2.5-1.5B-Instruct (SFT base) -> PGPO Training (Trajectory-level PPO + Turn-level Preference Learning) -> Evolutionary Inference (Elite pool, Temperature scheduling) -> Oracle Evaluation (QED, plogP, DRD2, JNK3, SA predictors)

### Critical Path
SFT on MolOptIns → PGPO dual-level training → Evolutionary inference with elite pool selection → Oracle evaluation and feedback loop

### Design Tradeoffs
- **Tradeoff:** Dense preference learning vs. computational overhead — PGPO extracts O(T²) pairs per trajectory, increasing learning signals but requiring more computation
- **Tradeoff:** Complete history states vs. state complexity — richer context improves decisions but increases model complexity and token usage

### Failure Signatures
- Model collapse after ~35 training steps with response length exploding to >2500 tokens
- Low success rates on challenging bioactivity tasks (JNK3, DRD2) indicating insufficient SFT foundation
- Preference learning providing misleading gradients if intermediate molecular scores are noisy

### First Experiments
1. Test SFT model's ability to generate valid structural modifications before PGPO training
2. Validate oracle implementations for DRD2, JNK3, and plogP predictions
3. Verify preference learning effectiveness on a simple molecular optimization task with known intermediate preferences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can molecular optimization frameworks better handle fundamentally conflicting multi-objective constraints, where improving one property inherently degrades another?
- **Basis in paper:** The authors note that "multi-objective optimization remains difficult when properties have opposing chemical requirements, a known challenge in computational drug design that affects all current methods," with the three-property task achieving only 13% success rate despite strong single-property performance (84%).
- **Why unresolved:** The case study on DRD2+plogP+QED demonstrates the agent struggling with trade-offs—adding hydrophobic groups to improve LogP paradoxically decreased it due to unintended polarity changes, and multiple similarity violations occurred during exploration.
- **What evidence would resolve it:** Development of mechanisms (e.g., Pareto front exploration, constraint-satisficing rather than optimization-focused objectives) that achieve >40% success rate on three-property tasks, or analysis showing theoretical limits of joint optimization given chemical constraints.

### Open Question 2
- **Question:** What determines successful cross-task transfer in molecular optimization, and can transfer learning be systematically improved between chemically disparate property objectives?
- **Basis in paper:** The cross-task transfer experiments reveal "selective transfer patterns that align with underlying chemical principles"—JNK3→QED transfer achieves 76% success (only 15 points below specialist), while DRD2→plogP achieves only 5%, suggesting "strategies for optimizing fundamental physicochemical properties provide a foundation that partially transfers to composite metrics."
- **Why unresolved:** The asymmetric and unpredictable transfer patterns (QED→plogP: 13.5% vs plogP→QED: 59.5%) indicate the learned representations are not well-understood, and no mechanism currently exists to predict or enhance transferability a priori.
- **What evidence would resolve it:** Identification of measurable property characteristics (e.g., dimensionality of chemical subspace, shared pharmacophore features) that predict transfer success, or a meta-learning approach that explicitly optimizes for cross-task generalization.

### Open Question 3
- **Question:** How robust is PGPO's performance under distribution shift in real-world drug discovery scenarios, particularly with wet-lab experimental validation replacing computational oracles?
- **Basis in paper:** All experiments use computational oracles (QED, plogP, DRD2, JNK3, SA predictors) with noiseless evaluation. The paper states lead optimization requires "efficiently navigating vast chemical space through iterative cycles" in the context of the "Design-Make-Test-Analyze (DMTA)" process, but validation remains purely computational.
- **Why unresolved:** Real experimental evaluations introduce measurement noise, batch effects, and irreproducibility. The reward function's asymmetric scaling (5× amplification for improvements) may amplify noise, and the preference learning mechanism assumes reliable relative rankings which noisy real data could corrupt.
- **What evidence would resolve it:** Benchmarks using noisy oracles with realistic experimental error profiles, or small-scale wet-lab validation showing correlation between PGPO predictions and actual synthesis/testing outcomes.

## Limitations

- Performance evaluation relies on oracle implementations that are not publicly available, creating a reproducibility barrier
- The SFT dataset (MolOptIns) and exact preprocessing pipeline are not provided, which are critical for faithful reproduction
- Success rate on challenging multi-property tasks remains limited (13% for three-property optimization), indicating fundamental difficulties with conflicting objectives

## Confidence

- **High Confidence:** The dual-level learning signal extraction mechanism and its theoretical signal amplification (O(NT²) vs O(N)) are well-founded and mathematically explicit
- **Medium Confidence:** The 84% average success rate on single-property tasks is compelling but depends on oracle quality and exact implementation details that are not fully disclosed
- **Low Confidence:** The generalizability of the approach to molecular optimization tasks beyond the tested properties remains uncertain without additional validation

## Next Checks

1. Verify oracle implementations for DRD2, JNK3, and plogP predictions using the exact models/code referenced in the paper
2. Access and preprocess the Chen et al. molecule pair dataset to construct MolOptIns with the specified similarity filtering (≥0.6)
3. Test the model's performance on an independent molecular optimization task (e.g., different property combinations or molecular scaffolds) to assess generalizability