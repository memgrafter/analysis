---
ver: rpa2
title: Explainable Reinforcement Learning via Temporal Policy Decomposition
arxiv_id: '2501.03902'
source_url: https://arxiv.org/abs/2501.03902
tags:
- learning
- policy
- future
- expected
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Temporal Policy Decomposition (TPD), a novel
  approach for explaining Reinforcement Learning (RL) policies by decomposing value
  functions along the temporal dimension. The method addresses the interpretability
  challenge in RL by generating explanations in terms of Expected Future Outcomes
  (EFOs) - predictions of what will happen at each future time step when following
  a policy.
---

# Explainable Reinforcement Learning via Temporal Policy Decomposition

## Quick Facts
- arXiv ID: 2501.03902
- Source URL: https://arxiv.org/abs/2501.03902
- Authors: Franco Ruggeri; Alessio Russo; Rafia Inam; Karl Henrik Johansson
- Reference count: 16
- Primary result: Introduces Temporal Policy Decomposition (TPD) to generate time-granular explanations of RL policies through Expected Future Outcomes (EFOs)

## Executive Summary
This paper introduces Temporal Policy Decomposition (TPD), a novel approach for explaining Reinforcement Learning (RL) policies by decomposing value functions along the temporal dimension. The method addresses the interpretability challenge in RL by generating explanations in terms of Expected Future Outcomes (EFOs) - predictions of what will happen at each future time step when following a policy. TPD leverages Fixed-Horizon Temporal Difference learning to train off-policy models that can predict EFOs for any state-action pair, not just those chosen by the target policy. This enables contrastive explanations comparing different actions. The method was evaluated in a modified Taxi environment with fuel consumption and traffic dynamics, where it successfully generated intuitive explanations that clarified the policy's strategy and demonstrated high prediction accuracy (mean squared errors below 10-4 for most outcomes).

## Method Summary
TPD decomposes Generalized Value Functions (GVFs) into a sequence of Expected Future Outcomes (EFOs) for each future time step using Fixed-Horizon Temporal Difference (FHTD) learning. The method trains off-policy models to predict cumulative values up to each horizon h, then algebraically solves a lower-triangular linear system to isolate the expected outcome at each specific step. This enables both on-policy and contrastive explanations by predicting outcomes for actions the agent did and didn't take. The approach was implemented in a modified Taxi environment with fuel dynamics and evaluated using mean squared error between predicted and ground-truth EFOs.

## Key Results
- Successfully decomposed value functions into intuitive per-step explanations of agent behavior
- Generated contrastive explanations comparing optimal vs. suboptimal actions
- Achieved mean squared errors below 10-4 for most outcome predictions
- Demonstrated ability to clarify policy strategy through temporal breakdowns of expected events

## Why This Works (Mechanism)

### Mechanism 1: Temporal Restoration via Fixed-Horizon Decomposition
Standard value functions compress infinite horizon rewards into single estimates, obscuring temporal details. TPD unrolls these compressed functions into discrete future time steps by solving a lower-triangular linear system that isolates outcomes at each specific horizon. This recovers the "when" dimension of decision-making that standard Q-values lose.

### Mechanism 2: Off-Policy Explanation via FHTD
The system uses Fixed-Horizon Temporal Difference learning with multiple heads (one per horizon) to learn cumulative values. Because it's off-policy, it can learn outcomes for actions not taken by the current behavioral policy, enabling contrastive explanations like "why not go East?" while the agent went "South."

### Mechanism 3: Algebraic Isolation of Future Outcomes
Rather than learning per-step outcomes directly, TPD learns cumulative Fixed-Horizon GVFs and then applies matrix inversion to strip away historical contributions. This algebraic decomposition recovers the expected outcome specifically at step h from the cumulative sum up to h.

## Foundational Learning

- **Concept: Generalized Value Functions (GVFs)** - Why needed: TPD extends beyond standard reward prediction to define value based on any measurable outcome. Quick check: Can you define a cumulant for a GVF that's distinct from the environment's reward?

- **Concept: Off-Policy Learning & Importance Sampling** - Why needed: The architecture learns from historical data to explain both taken and untaken actions. Quick check: Why would on-policy Monte Carlo fail to explain actions the agent never takes?

- **Concept: Temporal Difference (TD) Learning & Bootstrapping** - Why needed: FHTD updates estimates based on subsequent estimates. Quick check: In FHTD, how does the update target for horizon h differ from horizon h-1?

## Architecture Onboarding

- **Component map:** Outcome Encoder -> FHTD Learner -> Algebraic Solver -> Visualizer
- **Critical path:** The definition of the Outcome Function. If outcomes aren't intuitive to humans, explanations fail regardless of prediction accuracy.
- **Design tradeoffs:** Larger horizon H explains long-term consequences but increases computation and error propagation. The choice between direct vs. decomposed learning trades off directness for off-policy stability.
- **Failure signatures:** Overestimated "terminated/unknown" events indicate underestimation of known events. Spiky EFOs in stochastic environments suggest overfitting or lack of data diversity.
- **First 3 experiments:** 
  1. In gridworld, define outcome "reach goal" and verify EFO peak aligns with shortest path length.
  2. Train policy to avoid lava and show EFO differences between "move toward lava" vs "move away".
  3. Run Taxi experiment with H=5 vs H=30 and observe how explanation of refueling strategy changes.

## Open Questions the Paper Calls Out

- Can TPD be effectively scaled to continuous or high-dimensional control problems?
- How can uncertainty estimates be integrated into TPD to inform users about prediction reliability?
- Can the method be extended to predict the full distribution of future outcomes rather than just expected values?

## Limitations

- Effectiveness depends critically on choosing appropriate outcome functions that align with human intuition
- FHTD algorithm's off-policy stability under function approximation remains unproven beyond tabular cases
- Assumes chosen horizon H captures all relevant causal relationships, which may fail with delayed or stochastic rewards

## Confidence

- **High Confidence:** The core mathematical framework for decomposing cumulative values into per-step outcomes and FHTD update rules
- **Medium Confidence:** FHTD provides stable off-policy learning (supported by tabular convergence proofs but not function approximation)
- **Medium Confidence:** Prediction accuracy results (MSE < 10⁻⁴) are specific to modified Taxi environment

## Next Checks

1. Implement contrastive explanation fidelity test where suboptimal action clearly leads to failure and verify EFO for "failure" spikes only for that action

2. Systematically vary horizon H from 5 to 50 in Taxi environment and measure how explanation of refueling strategy changes

3. Train explainer with increasingly restrictive behavioral policies (ε=0.2 → 0.05 → 0.01) and quantify how coverage affects reliability of counterfactual explanations