---
ver: rpa2
title: 'SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native
  RAG Pipelines'
arxiv_id: '2601.01785'
source_url: https://arxiv.org/abs/2601.01785
tags:
- sras
- document
- reward
- bertscore
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRAS, a lightweight reinforcement learning-based
  document selector for edge-native RAG pipelines. SRAS addresses the limitations
  of static top-k retrieval mechanisms by learning a compact policy that optimizes
  document selection for downstream generation quality under strict compute and latency
  constraints.
---

# SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines
## Quick Facts
- arXiv ID: 2601.01785
- Source URL: https://arxiv.org/abs/2601.01785
- Reference count: 18
- Achieves competitive answer accuracy while maintaining sub-1MB model size and sub-1s CPU latency

## Executive Summary
SRAS introduces a lightweight reinforcement learning-based document selector designed specifically for edge-native RAG pipelines. It addresses the limitations of static top-k retrieval by learning a compact policy that optimizes document selection for downstream generation quality under strict compute and latency constraints. The method uses Proximal Policy Optimization (PPO) with a hybrid reward combining Relaxed F1 and BERTScore, achieving strong performance on both synthetic QA benchmarks and real-world SQuAD v2 data without domain-specific tuning.

## Method Summary
SRAS employs Proximal Policy Optimization (PPO) to train a document selection policy that maximizes a hybrid reward combining Relaxed F1 and BERTScore metrics. The approach is designed to be lightweight, with a sub-1MB model size and sub-1s CPU latency, making it suitable for resource-constrained edge devices. The policy learns to select relevant documents from a corpus without requiring domain-specific tuning, demonstrating strong generalization capabilities.

## Key Results
- Achieves BERTScore F1 of 0.8546 on SQuAD v2 data without domain-specific tuning
- Maintains sub-1MB model size and sub-1s CPU latency for edge deployment
- Outperforms both supervised and random selectors on synthetic QA benchmarks

## Why This Works (Mechanism)
SRAS works by learning a compact selection policy that optimizes for downstream generation quality rather than just retrieval relevance. The reinforcement learning approach allows the system to adapt its selection strategy based on actual performance outcomes, while the hybrid reward function balances precision and semantic similarity. The lightweight design ensures the method can run efficiently on edge devices with limited computational resources.

## Foundational Learning
- **Reinforcement Learning**: Needed for adaptive policy learning without labeled data; Quick check: Verify policy convergence during training
- **Proximal Policy Optimization (PPO)**: Required for stable RL training with continuous action spaces; Quick check: Monitor KL divergence between policy updates
- **BERTScore**: Used for semantic similarity evaluation; Quick check: Validate BERTScore consistency across different model versions
- **Relaxed F1 Score**: Provides relaxed precision-recall evaluation; Quick check: Compare Relaxed F1 vs traditional F1 on validation sets

## Architecture Onboarding
**Component Map**: Document Corpus -> Retriever -> SRAS Selector -> Generator -> Evaluation Metrics
**Critical Path**: Input query → Document retrieval → SRAS policy evaluation → Document selection → Answer generation → Performance evaluation
**Design Tradeoffs**: Prioritizes model size and latency over absolute accuracy; uses synthetic data for training to avoid manual annotation costs
**Failure Signatures**: Poor selection quality on out-of-distribution queries; performance degradation with larger document collections
**First Experiments**: 1) Test SRAS latency on target edge device, 2) Evaluate selection accuracy on held-out SQuAD data, 3) Measure memory footprint during inference

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data for primary evaluation may limit generalizability
- Single dataset evaluation (SQuAD v2) restricts assessment of cross-domain robustness
- No detailed information on training computational requirements or scalability

## Confidence
High: Claims about model size, latency, and SQuAD performance are well-supported
Medium: Generalization claims based on limited dataset evaluation
Low: Training requirements and scalability claims not empirically validated

## Next Checks
1. Evaluate SRAS on a broader range of real-world datasets, including multi-domain and multi-lingual data, to assess generalization beyond SQuAD v2
2. Conduct ablation studies to determine the impact of the hybrid reward function components and their weighting on the performance of SRAS
3. Measure the training and inference costs (time, memory, energy) of SRAS on resource-constrained edge devices to confirm its practical applicability