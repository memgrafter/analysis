---
ver: rpa2
title: Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement
arxiv_id: '2412.12667'
source_url: https://arxiv.org/abs/2412.12667
tags:
- selection
- quality
- patches
- image
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of data-driven 360-degree
  image quality assessment (IQA) models caused by redundant patch sampling. The proposed
  method introduces an embedding similarity-based selection algorithm that refines
  an initial set of sampled patches into a compact, maximally informative subset.
---

# Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement

## Quick Facts
- **arXiv ID:** 2412.12667
- **Source URL:** https://arxiv.org/abs/2412.12667
- **Authors:** Abderrezzaq Sendjasni; Seif-Eddine Benkabou; Mohamed-Chaker Larabi
- **Reference count:** 40
- **Primary result:** Embedding-driven selection reduces IQA patch sampling by 40-50% while maintaining or improving correlation performance.

## Executive Summary
This paper addresses the inefficiency of data-driven 360-degree image quality assessment (IQA) models caused by redundant patch sampling. The proposed method introduces an embedding similarity-based selection algorithm that refines an initial set of sampled patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves perceptual similarity in a low-dimensional embedding space while filtering out redundant samples using residual analysis. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) show that the method enables baseline models to match or exceed performance using only 40-50% of patches.

## Method Summary
The method operates on 360-degree images by first sampling patches using ERP/LAT/SP strategies, then encoding them with ResNet-50 to generate 2048-dimensional embeddings. A similarity matrix is computed from these embeddings and decomposed to obtain a low-dimensional target representation. An alternating optimization algorithm minimizes the reconstruction error between transformed embeddings and this target while enforcing sparsity through $\ell_{2,1}$-norm regularization on both transformation and residual matrices. Patches are ranked by their residual norms, with the lowest-scoring patches selected for quality regression using an MLP.

## Key Results
- Method achieves equivalent performance to full-patch baselines using only 40-50% of patches
- Maintains or improves performance when integrated with CNN and transformer-based IQA models
- Achieves 20-40% computational reduction across different sampling strategies and datasets
- Shows robustness to distance metric choice (Euclidean/Manhattan/Mahalanobis)

## Why This Works (Mechanism)

### Mechanism 1: Similarity-Preserving Projection
The algorithm projects high-dimensional embeddings into a low-dimensional space that preserves their pairwise similarity structure, creating a stable geometric representation of perceptual quality. The regularized optimization problem finds a transformation matrix that approximates this structure, assuming perceptually similar patches cluster together in the feature space.

### Mechanism 2: Residual-Based Irrelevance Scoring
Patches that cannot be adequately explained by the similarity-preserving transformation accumulate large reconstruction residuals. The $\ell_{2,1}$-norm on the residual matrix enforces sparsity, explicitly forcing the model to isolate "unfit" patches as redundant or irrelevant.

### Mechanism 3: Alternating Regularization (The Solver)
The optimization separates feature selection (via transformation matrix) and sample selection (via residual matrix) into tractable, iterative steps. The solver alternates between updating the transformation to fit the data and updating the residual to absorb remaining error, forcing competition where only patches fitting the "global" structure are retained.

## Foundational Learning

- **Concept: Low-Rank Approximation (Eigen-decomposition)**
  - Why needed: The method assumes the similarity matrix is low-rank; understanding this explains why the projection approximates the top eigenvectors of the similarity matrix.
  - Quick check: Why does the method use Z (from eigen-decomposition) rather than randomly initializing the target space?

- **Concept: $\ell_{2,1}$-Norm Regularization**
  - Why needed: This norm induces row-wise sparsity in matrices, enabling "hard" selection of both features and patches simultaneously.
  - Quick check: How does the $\ell_{2,1}$-norm differ from the Frobenius norm in terms of sparsity it induces on matrix rows?

- **Concept: Convex Relaxation**
  - Why needed: The original NP-hard problem is relaxed into a trace optimization problem, making the solution computationally feasible.
  - Quick check: What constraint was relaxed to make the optimization tractable?

## Architecture Onboarding

- **Component map:** Sampler (ERP/LAT/SP) -> Encoder (ResNet-50) -> Similarity Engine (S matrix → Z) -> Selector (Alternating Optimizer: updates W and R) -> Regressor (MLP) -> Output (MOS prediction)

- **Critical path:** The "Selector" loop (Algorithm 1) is the bottleneck. It initializes R, iteratively updates W (Eq. 9) and R (Eq. 11), and then ranks columns of R.

- **Design tradeoffs:**
  - Distance Metric: Euclidean is faster; Mahalanobis handles correlated features better but is computationally heavier (Table II shows performance is robust to this choice).
  - Dimensionality (h): Lower h (e.g., 10) reduces compute significantly with minimal performance loss (Fig. 3), but very low h risks losing distortion-specific variance.
  - Selection Rate (β): High reduction (20% data) increases speed but may drop performance on heterogeneous datasets like MVAQD.

- **Failure signatures:**
  - Stagnant Loss: If the objective function does not decrease in the first 3-4 iterations, check the calculation of diagonal matrices D_W and D_R (potential division by zero if not regularized).
  - Poor Correlation on Noisy Data: If the method fails on Gaussian Noise datasets, it is because "dense" noise eliminates redundancy, breaking the low-rank assumption.
  - Residuals Don't Differentiate: Verify Z captures similarity structure by checking eigenvalue decay of S.

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Create dataset with known duplicate patches. Verify algorithm assigns identical (and low) irrelevance scores to duplicates.
  2. **Convergence Visualization:** Plot loss Φ over iterations for different h values (Fig. 5). Confirm convergence within 6-8 iterations.
  3. **Baseline Rate Sweep:** Run pipeline on CVIQ using LAT sampling. Plot PLCC vs. Selection Rate (0.1 to 0.9) to verify performance matches 100% baseline at ~40-50%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the discrete patch selection process be reformulated to support end-to-end training with the downstream IQA regressor?
- Basis: [explicit] The conclusion states future work will "focus on making this refinement end-to-end trainable."
- Why unresolved: Current framework uses fixed ResNet-50 encoder and separate residual minimization step, preventing joint gradient updates.
- Evidence needed: Differentiable selection mechanism allowing gradients to flow from quality loss directly to selection parameters.

### Open Question 2
- Question: How does the method generalize to 360-degree video IQA where temporal redundancy complicates spatial patch selection?
- Basis: [explicit] Authors list "extending its principles to 360-degree video" as primary future work direction.
- Why unresolved: Current framework processes single images without mechanisms for temporal consistency or motion vectors.
- Evidence needed: Adapting embedding similarity to video data through 3D convolutions or temporal feature aggregation to filter redundant frames.

### Open Question 3
- Question: Does reliance on generic pre-trained encoder (ResNet-50) limit selection's ability to capture distortion-specific features?
- Basis: [inferred] Method uses ResNet-50 pre-trained on ImageNet, assuming these features capture perceptual quality relationships.
- Why unresolved: ImageNet features focus on semantic objects, not necessarily low-level distortion artifacts, potentially biasing residual analysis toward semantic content rather than quality.
- Evidence needed: Comparative study analyzing selection performance with encoders fine-tuned on IQA datasets versus generic classification models.

## Limitations
- Method focuses exclusively on 360-degree IQA; performance on classification or segmentation tasks remains unverified
- Residual-based selection may fail on synthetic distortions like pure Gaussian noise due to broken low-rank similarity structure
- Hyperparameter sensitivity for regularization parameters α and β is not reported, leaving stable operating regions unclear

## Confidence
- **High confidence:** Core optimization framework and convergence proof are well-specified and mathematically sound; ablation showing 40-50% patch reduction performance maintenance is robust
- **Medium confidence:** Residual-based irrelevance scoring mechanism lacks external validation; claims rely primarily on internal ablation rather than comparison to established feature selection methods
- **Low confidence:** Universal applicability claim based on only two model architectures (CNN and transformer-based); performance on other model families untested

## Next Checks
1. **Noise robustness test:** Apply method to synthetic dataset with pure Gaussian noise patches to verify whether algorithm fails due to broken low-rank structure
2. **Cross-task generalization:** Apply same embedding-driven selection pipeline to classification task (e.g., ImageNet) using ResNet embeddings; measure performance vs. baseline at different selection rates
3. **Hyperparameter sensitivity sweep:** Systematically vary α and β across orders of magnitude; plot performance degradation curves to identify stable operating regions and failure thresholds