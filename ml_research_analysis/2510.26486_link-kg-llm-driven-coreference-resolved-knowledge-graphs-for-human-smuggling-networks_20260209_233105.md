---
ver: rpa2
title: 'LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling
  Networks'
arxiv_id: '2510.26486'
source_url: https://arxiv.org/abs/2510.26486
tags:
- resolution
- coreference
- graph
- prompt
- link-kg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LINK-KG is a modular framework for constructing knowledge graphs
  from legal documents on human smuggling networks. It integrates a three-stage LLM-guided
  coreference resolution pipeline with domain-specific KG extraction.
---

# LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks

## Quick Facts
- **arXiv ID**: 2510.26486
- **Source URL**: https://arxiv.org/abs/2510.26486
- **Reference count**: 27
- **Primary result**: Reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods

## Executive Summary
LINK-KG introduces a modular framework for constructing knowledge graphs from legal documents on human smuggling networks. The approach integrates a three-stage LLM-guided coreference resolution pipeline with domain-specific KG extraction. Central to the design is a type-specific Prompt Cache that tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. Evaluated on U.S. federal and state court documents, LINK-KG demonstrates significant improvements in graph quality compared to baseline methods.

## Method Summary
LINK-KG processes legal documents through a three-stage LLM pipeline: (1) NER-LLM extracts proper nouns and noun phrases per chunk per entity type; (2) Mapping-LLM builds type-specific prompt caches with alias-to-canonical mappings and auxiliary descriptions, optionally with a gleaning pass for long documents; (3) Resolve-LLM applies cached mappings to rewrite chunks with canonical substitutions. The resolved text then feeds into GraphRAG-based KG extraction with sequential type processing. The system handles seven entity types and operates on 300-token chunks with temperature=0 using LLaMA 3.3 70B.

## Key Results
- Reduces average node duplication by 45.21% compared to baseline methods
- Reduces noisy nodes by 32.22% in final graph structures
- Demonstrates cleaner and more coherent graph structures suitable for downstream analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Type-specific prompt caches enable consistent coreference resolution across long documents by maintaining entity mappings per type rather than globally.
- **Mechanism**: The system maintains separate alias-to-canonical mappings for each entity type (Person, Location, Organization, etc.). When processing each chunk, the Mapping-LLM updates the cache with new mappings while preserving existing ones. This allows "the driver" in chunk 5 to correctly resolve to a different canonical entity than "the driver" in chunk 2, because auxiliary descriptions provide disambiguating context.
- **Core assumption**: LLMs can reliably maintain and update structured mappings when given explicit type constraints and auxiliary descriptions, without drifting or overwriting prior correct mappings.
- **Evidence anchors**: [abstract] "type-specific Prompt Cache, which consistently tracks and resolves references across document chunks"; [Section III.A.2] "type-specific prompt caches that store alias-to-canonical mappings separately for each entity type"; [corpus] Related work CORE-KG struggles with long documents due to loss-in-the-middle problem.

### Mechanism 2
- **Claim**: Plural-aware prompting with explicit grounding rules prevents false coreference chains from collective noun phrases.
- **Mechanism**: The prompt enforces that plural mentions (e.g., "the defendants") resolve only when all individuals are explicitly named in surrounding text. If grounding fails, the alias maps to null rather than speculating. This prevents "the agents" from incorrectly merging with a single nearby entity.
- **Core assumption**: Legal documents provide sufficient explicit naming of entities that plural references can be grounded, or that null resolution is preferable to incorrect linking.
- **Evidence anchors**: [abstract] "resolving references across document chunks, enabling clean and disambiguated narratives"; [Section III.A.2] "resolution is allowed only if all individuals are named in the text"; [corpus] LLMLINK noted to struggle with plural mentions.

### Mechanism 3
- **Claim**: Sequential entity-type extraction reduces attention spread and improves typing accuracy in dense legal narratives.
- **Mechanism**: Rather than extracting all entity types simultaneously, the system extracts entities by type in a fixed order (Person, Location, Routes, etc.) before relationship extraction. This constrains the LLM's attention to one type category at a time, reducing cross-type interference.
- **Core assumption**: The performance gain from reduced attention spread outweighs the computational cost of multiple extraction passes.
- **Evidence anchors**: [Section III.B] "enforce a fixed extraction order where entities are extracted by type"; [Section V] Results show 45.21% reduction in node duplication; [corpus] No direct corpus comparison for sequential extraction.

## Foundational Learning

- **Concept**: Coreference Resolution
  - Why needed here: Legal documents use shifting references (aliases, roles, titles) that create duplicate nodes if unresolved. Understanding the distinction between entity mentions and canonical entities is essential.
  - Quick check question: Given "Agent Smith detained the driver. The driver later confessed," should "the driver" create a new node or link to an existing one?

- **Concept**: Prompt Caching / Memory Mechanisms
  - Why needed here: The type-specific prompt cache is the architectural core. Understanding how LLMs can maintain state across chunks via structured prompts is critical for debugging resolution failures.
  - Quick check question: If chunk 3 introduces "Mr. Chen" and chunk 7 refers to "the suspect," what information must the cache retain to correctly resolve this?

- **Concept**: Knowledge Graph Construction Pipelines
  - Why needed here: LINK-KG separates coreference resolution from entity-relationship extraction. Understanding why these are distinct stages (and how they interact) prevents architectural confusion.
  - Quick check question: What happens to graph quality if you skip coreference resolution but run entity extraction—what specific artifacts appear?

## Architecture Onboarding

- **Component map**: Stage 1 (NER-LLM) -> Stage 2 (Mapping-LLM with Prompt Cache) -> Stage 3 (Resolve-LLM) -> KG Construction (GraphRAG with sequential type processing)
- **Critical path**: Stage 1 → Stage 2 (with cache accumulation across all chunks) → Stage 3 (resolution) → KG Construction. The prompt cache is the serialization point; corruption here propagates to all downstream resolution.
- **Design tradeoffs**:
  - Type-specific caches vs. unified cache: Lower token usage per LLM call, but requires sequential processing per type
  - Gleaning pass vs. single-pass: Improves consistency for long documents but doubles Stage 2 compute
  - Sequential extraction vs. joint extraction: Higher accuracy but multiple LLM calls per chunk
- **Failure signatures**:
  - High node duplication: Cache not persisting across chunks or alias-resolution rules too permissive
  - Null assignments for valid plurals: Grounding rules too strict or explicit names too sparse in text
  - Type misclassification: Entity type definitions in prompt insufficient for domain terminology
  - Context overflow: Cache grows beyond context window on entity-dense documents
- **First 3 experiments**:
  1. Baseline comparison on single short document: Run LINK-KG vs. GraphRAG (no coreference) vs. CORE-KG on a 2,000-word case; measure duplicate node count and noisy node percentage
  2. Ablation on gleaning pass: Run Stage 2 with and without second-pass refinement on a long document (>5,000 words); compare cache consistency and duplication metrics
  3. Plural resolution stress test: Construct synthetic legal text with multiple plural references ("the agents," "the defendants") and verify correct multi-entity resolution vs. null assignment behavior

## Open Questions the Paper Calls Out

- **Question**: Do the structural improvements in LINK-KG (reduced duplication/noise) translate to measurable performance gains in downstream analytical tasks such as group detection, role attribution, or event prediction?
- **Basis in paper**: [explicit] The conclusion states that the resulting cleaner graphs "support downstream tasks such as group detection, role attribution, temporal analysis, and event prediction," but the experiments only evaluate graph construction quality (duplication/noise), not the utility for these specific tasks.
- **Why unresolved**: The current evaluation is limited to graph topology metrics rather than end-to-end task utility.
- **Evidence to resolve**: Quantitative benchmarks (e.g., F1 scores) on specific network analysis tasks performed on the LINK-KG output compared to baselines.

- **Question**: Does the reduction in node duplication and noise directly correlate with factual accuracy and relationship correctness when evaluated against a human-annotated ground truth?
- **Basis in paper**: [inferred] The paper notes in Section IV.D that "no annotated ground truth exists" and relies on proxy metrics (duplication/noise rates) rather than factual precision/recall of the triples.
- **Why unresolved**: Lower duplication suggests better structure, but does not guarantee that the resolved entities or extracted relationships are factually correct or complete.
- **Evidence to resolve**: A study comparing LINK-KG triples against a manually curated gold-standard knowledge graph using standard information extraction metrics.

- **Question**: To what extent does the sequential, type-specific processing pipeline (processing Type 1 to Type N separately) impact computational efficiency and error propagation compared to joint extraction methods?
- **Basis in paper**: [inferred] The method describes a "three-phase process" repeated "sequentially for each entity type" to mitigate attention spread. While this aids accuracy, it implies $N$ passes over the document, potentially multiplying latency.
- **Why unresolved**: The paper focuses on extraction quality but does not analyze the computational cost or runtime trade-offs of the sequential multi-pass approach.
- **Evidence to resolve**: Runtime and memory usage analysis comparing LINK-KG's sequential approach against a single-pass joint extraction baseline on documents of varying lengths.

## Limitations
- **Prompt templates not provided**: Full prompt templates for the three-stage LLM pipeline are not specified, only design principles.
- **Chunk overlap strategy undefined**: The chunk overlap strategy for 300-token windows is not defined, which could affect resolution quality for entities spanning chunk boundaries.
- **Gleaning pass conditions unspecified**: Conditions triggering the optional gleaning pass are not defined, making it unclear when this refinement step is activated.
- **Proprietary data access required**: Evaluation relies on Nexis Uni data requiring institutional access, limiting independent verification without equivalent legal document substitutes.

## Confidence

- **High Confidence**: The type-specific prompt cache mechanism and its role in reducing node duplication (45.21% reduction) - directly supported by stated architecture and results.
- **Medium Confidence**: The plural-aware grounding rules preventing false coreference chains - mechanism described but strict null assignment may not generalize to all legal document styles.
- **Low Confidence**: The sequential entity-type extraction benefits without direct corpus comparison - claim that this reduces attention spread is reasonable but not empirically validated against joint extraction baselines in this work.

## Next Checks

1. **Prompt Template Reconstruction**: Reconstruct the three LLM prompts (NER, Mapping, Resolve) based on described principles and test on synthetic legal document with known coreference patterns to verify alias-to-canonical resolution accuracy.

2. **Chunk Boundary Handling Test**: Evaluate resolution quality with different chunk overlap strategies (0%, 25%, 50%) on long document to determine optimal boundary handling for entities spanning chunks.

3. **Plural Resolution Benchmark**: Create test suite of legal excerpts with varying plural reference patterns (explicitly grounded vs. ambiguous) to validate null assignment behavior and measure impact on graph fragmentation vs. accuracy trade-offs.