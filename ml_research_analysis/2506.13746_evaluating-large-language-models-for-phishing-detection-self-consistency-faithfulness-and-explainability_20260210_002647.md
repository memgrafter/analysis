---
ver: rpa2
title: Evaluating Large Language Models for Phishing Detection, Self-Consistency,
  Faithfulness, and Explainability
arxiv_id: '2506.13746'
source_url: https://arxiv.org/abs/2506.13746
tags:
- phishing
- llama
- emails
- classification
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates fine-tuned LLMs for phishing detection using
  three approaches: binary classification, contrastive learning, and direct preference
  optimization. BERT achieved the highest accuracy (98.89%) and lowest loss with binary
  classification, while Llama 7B and Llama 8B also performed well.'
---

# Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability

## Quick Facts
- arXiv ID: 2506.13746
- Source URL: https://arxiv.org/abs/2506.13746
- Reference count: 29
- Key outcome: BERT achieved highest accuracy (98.89%) for phishing detection, while Llama models showed higher explanation consistency (CC-SHAP > 0.95) but lower detection accuracy (30-40%)

## Executive Summary
This paper evaluates five fine-tuned LLMs for phishing detection using three distinct approaches: binary classification, contrastive learning, and direct preference optimization. BERT demonstrated superior performance with 98.89% accuracy and lowest loss across all approaches. The study reveals a significant trade-off between prediction accuracy and explanation consistency, with Llama models showing high self-consistency in explanations (CC-SHAP scores > 0.95) but lower phishing detection accuracy (30-40%), while Wizard 7B exhibited the opposite pattern. This tension between reliability and nuanced reasoning highlights the complexity of developing LLMs that can both accurately detect phishing attempts and provide trustworthy explanations for their decisions.

## Method Summary
The researchers fine-tuned five open-source LLMs (BERT, GPT-2, Llama 7B, Llama 8B, and Wizard 7B) using three distinct training approaches: binary classification, contrastive learning, and direct preference optimization. Binary classification used softmax activation with cross-entropy loss, contrastive learning employed margin-based loss with positive-negative pairs, and direct preference optimization implemented reward modeling with KL divergence constraints. Model performance was evaluated on accuracy, loss, and explainability metrics, with particular focus on SHAP value consistency across multiple runs to assess self-consistency and faithfulness of explanations.

## Key Results
- BERT achieved the highest phishing detection accuracy at 98.89% with lowest loss across all approaches
- Llama 7B and Llama 8B demonstrated strong performance in contrastive learning and preference optimization
- Wizard 7B underperformed across all evaluation metrics and approaches
- Explanation consistency (CC-SHAP scores > 0.95) showed inverse relationship with detection accuracy for most models

## Why This Works (Mechanism)
The observed trade-off between prediction accuracy and explanation consistency likely stems from the fundamental tension between model optimization objectives and the complexity of phishing detection tasks. Binary classification approaches, which achieve highest accuracy, may oversimplify the nuanced decision boundaries required for genuine phishing detection by forcing discrete outputs. Contrastive learning and preference optimization methods, while slightly less accurate, may better capture the subtle linguistic and contextual features that distinguish sophisticated phishing attempts from legitimate communications. The higher explanation consistency in Llama models suggests these architectures may develop more stable internal representations of decision-relevant features, even when those features don't always align with ground truth labels.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations) values**: Explainable AI method that attributes feature importance based on game theory; needed to quantify explanation consistency across model runs
- **Contrastive learning**: Training approach that learns by comparing similar and dissimilar examples; needed to capture nuanced distinctions between phishing and legitimate content
- **Direct preference optimization**: Reward modeling technique that optimizes for human-like decision preferences; needed to align model outputs with security expert judgment
- **Self-consistency metrics**: Quantitative measures of explanation reproducibility across multiple inference runs; needed to assess model reliability beyond single-point predictions
- **Cross-entropy loss**: Standard classification loss function; needed baseline optimization objective for binary phishing detection
- **KL divergence**: Measure of difference between probability distributions; needed to constrain preference optimization outputs

## Architecture Onboarding

**Component Map**
Binary Classification -> Cross-entropy Loss -> Model Parameters -> Accuracy Metrics
Contrastive Learning -> Margin-based Loss -> Positive/Negative Pairs -> Feature Learning
Preference Optimization -> Reward Modeling -> KL Divergence -> Human-aligned Outputs

**Critical Path**
Data Preprocessing -> Model Fine-tuning -> Inference -> Explainability Analysis -> Performance Evaluation

**Design Tradeoffs**
Accuracy vs. Explanation Consistency: Higher detection rates may come at cost of interpretable reasoning
Model Complexity vs. Computational Efficiency: Larger models show better consistency but require more resources
Binary vs. Nuanced Classification: Simplified outputs improve accuracy but may miss sophisticated attacks

**Failure Signatures**
Low CC-SHAP scores across multiple runs indicate unstable feature attribution
High loss with low accuracy suggests poor feature learning or data quality issues
Discrepancy between training and validation performance indicates overfitting

**First 3 Experiments**
1. Run multiple inference passes with same input to measure CC-SHAP score consistency
2. Compare feature importance rankings across different phishing scenarios
3. Evaluate model calibration by comparing predicted probabilities with actual detection rates

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to five open-source LLMs and three detection approaches, limiting generalizability
- Explainability metrics rely on SHAP values that may not fully capture nuanced LLM reasoning processes
- No clear thresholds established for acceptable performance in accuracy vs. consistency trade-off

## Confidence

**High confidence** in BERT's superior performance metrics (98.89% accuracy, lowest loss) due to clear quantitative evidence across multiple evaluation approaches

**Medium confidence** in the observed trade-off between explanation consistency and detection accuracy, as the relationship appears consistent across models but lacks theoretical grounding for practical implementation thresholds

**Low confidence** in the generalizability of findings to production environments, given the limited model selection, absence of real-world adversarial testing, and lack of temporal validation against evolving phishing techniques

## Next Checks
1. Test model performance against a dynamically evolving phishing dataset that includes emerging attack vectors (AI-generated content, deepfakes, sophisticated social engineering) to assess temporal robustness
2. Conduct human evaluation studies comparing LLM explanations against expert security analyst reasoning to validate the practical utility of SHAP-based consistency metrics
3. Implement cross-validation with varying feature sets and temporal splits to quantify model performance stability across different data distributions and timeframes