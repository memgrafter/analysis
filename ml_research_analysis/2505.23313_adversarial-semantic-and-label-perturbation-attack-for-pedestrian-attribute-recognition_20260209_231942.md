---
ver: rpa2
title: Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute
  Recognition
arxiv_id: '2505.23313'
source_url: https://arxiv.org/abs/2505.23313
tags:
- adversarial
- attack
- attribute
- pedestrian
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper pioneers adversarial attack and defense research for
  pedestrian attribute recognition (PAR), addressing the task's vulnerability to perturbations.
  The proposed method, termed ASL-PAR, employs semantic and label perturbation strategies
  based on the PromptPAR framework.
---

# Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition
## Quick Facts
- arXiv ID: 2505.23313
- Source URL: https://arxiv.org/abs/2505.23313
- Reference count: 40
- Primary result: Effective adversarial attack and defense framework for pedestrian attribute recognition with physical-world validation

## Executive Summary
This paper introduces ASL-PAR, the first adversarial attack and defense framework specifically designed for pedestrian attribute recognition (PAR). The method combines semantic and label perturbation strategies to generate adversarial examples that significantly degrade model performance while maintaining invisibility. The attack is validated across multiple digital datasets (PETA, PA100K, MSP60K, RAPv2) and demonstrated in physical-world settings. A corresponding defense mechanism using noise filtering and prompt fine-tuning partially restores model performance. The work addresses a critical vulnerability in PAR systems, showing that current models are susceptible to both digital and physical adversarial perturbations.

## Method Summary
The ASL-PAR framework employs a two-pronged attack strategy based on the PromptPAR framework. It uses semantic perturbation to introduce label noise during optimization and label perturbation to guide the generation of adversarial noise. The attack optimizes in the CLIP space by aligning visual and textual features, employing either global or patch-level approaches. For defense, the framework implements noise filtering to remove adversarial perturbations and prompt fine-tuning to enhance model robustness. The method is evaluated across multiple datasets and includes physical-world validation of patch-level attacks, demonstrating both effectiveness and practical applicability.

## Key Results
- Significant performance degradation across all tested datasets (e.g., PETA mA from 88.76 to 48.25)
- Effective physical-world validation with patch-level adversarial attacks
- Partial performance recovery through the proposed defense mechanism
- Demonstrated transferability in cross-dataset attacks

## Why This Works (Mechanism)
The attack succeeds by exploiting the semantic relationships between visual features and attribute labels in the CLIP space. By introducing semantic perturbation during optimization, the method creates adversarial examples that are both effective and visually imperceptible. The label perturbation guides the generation of noise that maximally confuses the attribute recognition model. The CLIP-based alignment ensures that the perturbations affect the semantic understanding of attributes while maintaining visual plausibility. The defense works by filtering out adversarial noise patterns and fine-tuning prompts to be more robust to perturbations.

## Foundational Learning
- CLIP model architecture: Understanding the visual-textual feature alignment is crucial for comprehending how the attack operates in the joint embedding space.
- Pedestrian attribute recognition task: Essential to understand the multi-label classification problem and the importance of attribute recognition in surveillance and autonomous systems.
- Adversarial attack paradigms: Knowledge of different attack strategies (white-box vs black-box, global vs patch-level) helps contextualize the novelty of the proposed approach.
- Transferability in adversarial attacks: Understanding how attacks generalize across different datasets and models is key to evaluating the proposed method's robustness claims.

## Architecture Onboarding
Component map: CLIP encoder -> Semantic perturbation module -> Label perturbation module -> Adversarial noise generator -> PAR model
Critical path: Input image -> CLIP encoding -> Semantic-label perturbation optimization -> Adversarial noise addition -> PAR prediction
Design tradeoffs: Global attacks offer better transferability but lower invisibility vs patch-level attacks with higher invisibility but reduced transferability
Failure signatures: Successful attack shows significant performance drop in attribute recognition accuracy while maintaining visual similarity
First experiments: 1) Validate attack effectiveness on single-attribute subset of PETA dataset, 2) Test defense mechanism on digitally attacked examples, 3) Evaluate physical-world patch-level attack in controlled lighting conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-model transferability effectiveness varies significantly across different PAR architectures
- Defense mechanism provides only partial recovery of baseline performance
- Physical-world validation limited to patch-level attacks, full-body attacks untested
- Performance degradation varies across different datasets, suggesting domain-specific vulnerabilities

## Confidence
- Attack effectiveness: High - consistent performance degradation across multiple digital datasets
- Defense efficacy: Medium - partial recovery observed but not complete mitigation of adversarial effects
- Physical-world robustness: Low - limited to patch-level attacks with no full-body real-world testing

## Next Checks
1. Test full-body adversarial patches in uncontrolled outdoor environments with varying lighting and occlusions
2. Evaluate defense performance against adaptive attacks that modify perturbation strategies based on defense mechanisms
3. Assess cross-model transferability by attacking multiple PAR architectures not seen during training (e.g., ResNet, EfficientNet) to determine generalizability