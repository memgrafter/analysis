---
ver: rpa2
title: Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks
arxiv_id: '2507.21974'
source_url: https://arxiv.org/abs/2507.21974
tags:
- root
- reasoning
- cause
- network
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles Root Cause Analysis (RCA) in 5G wireless networks
  by proposing a framework that uses reasoning-enhanced Large Language Models (LLMs).
  To enable this, the authors introduce TeleLogs, a synthetic dataset of 5G troubleshooting
  scenarios with annotated root causes.
---

# Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks

## Quick Facts
- arXiv ID: 2507.21974
- Source URL: https://arxiv.org/abs/2507.21974
- Reference count: 16
- Primary result: Fine-tuned reasoning LLMs achieve >95% accuracy on 5G RCA with interpretable explanations

## Executive Summary
This paper addresses Root Cause Analysis (RCA) in 5G wireless networks using reasoning-enhanced Large Language Models. The authors introduce TeleLogs, a synthetic dataset of 5G troubleshooting scenarios, and develop a two-stage training framework: supervised fine-tuning with multi-agent reasoning traces followed by reinforcement learning using Group Relative Policy Optimization (GRPO). The approach significantly outperforms both base LLMs and state-of-the-art reasoning models, achieving over 95% accuracy on the test set while providing structured, interpretable diagnostic explanations.

## Method Summary
The method employs a two-stage training process. First, supervised fine-tuning (SFT) uses a multi-agent pipeline to generate structured reasoning traces from synthetic 5G troubleshooting scenarios. Two agents employ elimination-based and contradiction-based prompting strategies, with an aggregator synthesizing correct trajectories into a four-task format. Second, GRPO reinforcement learning refines the model by optimizing for correct root cause identification while maintaining policy stability through group-relative advantage estimation and KL regularization. The constrained output space (8 predefined root causes) transforms RCA into a classification-with-explanation task.

## Key Results
- Qwen2.5-RCA-32B achieves 95.86% pass@1 accuracy and 98.27% maj@4 accuracy
- All fine-tuned models (1.5B, 7B, 32B) significantly outperform base LLMs and state-of-the-art reasoning models
- Model demonstrates strong generalization to randomized test variants with >93% accuracy
- SFT+RL shows synergistic effects, with SFT alone achieving only 49.45% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific supervised fine-tuning with structured chain-of-thought traces grounds the model in 5G troubleshooting patterns before reinforcement learning. A multi-agent pipeline generates diverse reasoning trajectories using elimination-based and contradiction-based prompting strategies, then aggregates them into structured format (Data Analysis → Root Cause Analysis → Root Cause Identification → Summary), reducing token count while preserving reasoning depth.

### Mechanism 2
Reinforcement learning with Group Relative Policy Optimization (GRPO) refines reasoning by optimizing for correct root cause identification while maintaining policy stability. GRPO samples N=8 trajectories per question and computes token-level advantages using group-relative normalization, with binary reward providing sparse feedback and KL divergence regularization preventing excessive deviation from the reference policy.

### Mechanism 3
Constrained output space with explicit root cause enumeration enables more reliable probabilistic inference than open-ended generation. The problem formulation restricts outputs to K=8 predefined root causes, transforming RCA into a classification-with-explanation task with unambiguous evaluation through parsing function σ(·).

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning in LLMs**
  - Why needed here: The paper relies on generating multi-step reasoning traces for interpretability
  - Quick check question: Can you explain why generating intermediate reasoning steps before a final answer improves accuracy on multi-step inference tasks?

- **Concept: Proximal Policy Optimization (PPO) and Clipping**
  - Why needed here: GRPO is a variant of PPO; the clipping mechanism (ε=0.2) prevents destructive policy updates
  - Quick check question: What problem does the clipping operation solve in policy gradient methods, and what happens if ε is set too large?

- **Concept: KL Divergence Regularization**
  - Why needed here: The objective function includes a KL divergence term to prevent the fine-tuned model from drifting too far from the SFT initialization
  - Quick check question: If the KL penalty weight β is too low, what failure mode might you observe during RL fine-tuning?

## Architecture Onboarding

- **Component map:**
  Input: (U, Y_t, s_t) → Prompt Template T → LLM π_θ → Multi-Agent Pipeline (M=2 agents) → Aggregator → Structured Format F → SFT Training → π_θ1 → GRPO RL Training → π_θ2 (Final) → Output: Reasoning trace τ with root cause in \boxed{}

- **Critical path:** The SFT data quality is the bottleneck. If the multi-agent pipeline produces trajectories where majority vote yields <70% accuracy on training data, RL cannot recover performance.

- **Design tradeoffs:**
  - Model scale vs. efficiency: 1.5B achieves 87.56% accuracy (7× gain over base), while 32B achieves 95.86%
  - N (trajectories per question) vs. compute: GRPO samples N=8 trajectories; higher N improves advantage estimation but increases compute linearly
  - Assumption: The approach uses Qwen family models; transferability to other model families is not validated

- **Failure signatures:**
  - Positional bias: If accuracy drops >15% on randomized test set, model has memorized positional patterns
  - Reward hacking: If reasoning traces become incoherent while accuracy remains high, model may be exploiting output format
  - Catastrophic forgetting: If base model capabilities degrade after fine-tuning, KL regularization may be insufficient

- **First 3 experiments:**
  1. Baseline verification: Evaluate base Qwen2.5 models on TeleLogs test set without fine-tuning
  2. SFT-only ablation: Train SFT model using only elimination-based prompting to measure contribution of reasoning diversity
  3. Generalization stress test: Create held-out test set with new root cause combinations not seen during training

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed framework accurately identify and explain scenarios involving multiple simultaneous root causes? The conclusion states that future work will focus on extending the method to handle multiple root cause scenarios.

- **Open Question 2:** How does the performance of Qwen2.5-RCA models change when applied to real-world operational data compared to the synthetic TeleLogs dataset? The authors identify incorporating real-world operational data as a goal for future work.

- **Open Question 3:** Does the reasoning framework maintain diagnostic accuracy when analyzing multiple concurrent network symptoms? Section III-A explicitly limits the scope to single symptoms.

## Limitations
- Generalization Scope: The synthetic dataset comprises single-cause scenarios, while real 5G networks often involve concurrent, cascading failures
- Training Data Dependency: Model performance is tightly coupled to the quality and diversity of the multi-agent pipeline's generated trajectories
- Multi-Agent Pipeline Details: Exact prompt templates and aggregation strategies are not fully specified, creating uncertainty in reproducing the SFT data generation pipeline

## Confidence

- **High Confidence**: The experimental results on the TeleLogs dataset are reproducible and demonstrate clear performance gains over baselines
- **Medium Confidence**: The mechanism of GRPO refining reasoning via group-relative advantage estimation is plausible but not extensively validated with ablation studies
- **Low Confidence**: The transferability of the approach to real-world, multi-cause fault scenarios and other model families is uncertain without empirical validation

## Next Checks

1. Test the fine-tuned models on a dataset of real 5G network fault logs (if available) or manually crafted multi-cause scenarios to assess generalization beyond synthetic single-cause assumptions.

2. Conduct an ablation study isolating the contributions of SFT and GRPO by comparing: (a) Base model performance, (b) SFT-only performance, (c) GRPO-only (starting from SFT checkpoint), and (d) SFT+GRPO.

3. Perform a detailed error analysis on the randomized test set to identify failure modes and analyze cases where maj@4 > pass@1 to understand if the model generates inconsistent reasoning strategies.