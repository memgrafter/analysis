---
ver: rpa2
title: Investigating Test-Time Scaling with Reranking for Machine Translation
arxiv_id: '2509.19020'
source_url: https://arxiv.org/abs/2509.19020
tags:
- translation
- scaling
- quality
- machine
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically explores Test-Time Scaling (TTS) for
  machine translation by generating multiple translation candidates and selecting
  the best using quality estimation. Experiments cover six high-resource and one low-resource
  language pair, five model sizes (3B-72B), and up to 1024 candidates.
---

# Investigating Test-Time Scaling with Reranking for Machine Translation

## Quick Facts
- arXiv ID: 2509.19020
- Source URL: https://arxiv.org/abs/2509.19020
- Reference count: 10
- Primary result: Test-Time Scaling with best-of-N reranking consistently improves translation quality in high-resource settings but can degrade it in low-resource settings due to code-switching errors

## Executive Summary
This study systematically explores Test-Time Scaling (TTS) for machine translation by generating multiple translation candidates and selecting the best using quality estimation. Experiments cover six high-resource and one low-resource language pair, five model sizes (3B-72B), and up to 1024 candidates. TTS consistently improves translation quality in high-resource settings, with small models at large N matching or surpassing larger models at N=1, confirmed by human evaluation. Under fixed compute budgets, larger models are generally more compute-efficient, though a 14B model at N≈8-16 can approach 72B performance at lower cost. In low-resource settings, TTS can degrade quality due to code-switching errors that fool evaluation metrics. TTS-MT emerges as a strong baseline outperforming recent inference-scaling and RLHF approaches.

## Method Summary
The study investigates Test-Time Scaling for machine translation using a best-of-N framework with quality estimation reranking. For each source sentence, N translation candidates are generated independently using Qwen2.5 models (3B-72B) with temperature=1.0 and top-p=0.95. Each candidate is scored by the KIWI22 quality estimation model, and the highest-scoring candidate is selected as the final output. The method is evaluated across WMT24 general MT benchmark with six high-resource language pairs (en-ja, en-zh, zh-ja, en-de, en-ru, en-es) and one low-resource pair (en-is). Automatic evaluation uses BLEU, ChrF++, Remedy, MetricX, XCOMET, and COMET22, with human evaluation via ESA protocol for en-ja. Compute efficiency is analyzed by measuring TFLOPs required for generation and quality estimation.

## Key Results
- TTS consistently improves translation quality in high-resource settings, confirmed by human evaluation
- Small models with large N (e.g., 14B at N=16) can match or surpass larger models at N=1 under unconstrained compute
- Under fixed compute budgets, larger models are generally more compute-efficient, though 14B at N≈8-16 approaches 72B performance at lower cost
- In low-resource settings (en-is), TTS can degrade quality due to code-switching errors that fool QE metrics
- TTS-MT emerges as a strong baseline outperforming recent inference-scaling and RLHF approaches

## Why This Works (Mechanism)

### Mechanism 1
Increasing candidate samples (N) with quality-guided selection improves translation quality when QE metrics align with true translation adequacy. Sampling N candidates at temperature 1.0 produces diverse translation hypotheses covering different word choices and structures. The KIWI22 QE model scores each candidate without references, and the highest-scoring candidate is selected. This filters out low-quality generations while preserving semantically diverse alternatives. Core assumption: QE model scores correlate with human judgment of translation quality for the language pair in question.

### Mechanism 2
Small models with large N can match larger models at N=1 under unconstrained compute but not under fixed compute budgets. Translation quality scales with both model parameters and candidate samples. Smaller models generate weaker individual candidates but produce a wider distribution from which to select. With sufficiently large N, the probability of generating at least one high-quality candidate approaches that of a stronger model's single-pass generation. Core assumption: Candidates are sampled independently and cover sufficient hypothesis space.

### Mechanism 3
QE models and neural metrics fail to penalize code-switching in low-resource languages, causing TTS to select worse outputs. When base models struggle with low-resource pairs, they produce code-switched text mixing the target language with unrelated languages (e.g., Icelandic output containing Chinese). KIWI22 and other neural metrics trained predominantly on high-resource pairs assign high scores to these fluent-looking but wrong-language segments. Core assumption: QE models have coverage gaps for underrepresented language pairs.

## Foundational Learning

- Concept: Quality Estimation (QE) for MT
  - Why needed here: QE models like KIWI22 enable reference-free scoring of translation candidates, making best-of-N feasible at inference time.
  - Quick check question: Can you explain why QE differs from reference-based metrics like BLEU and why this matters for inference-time selection?

- Concept: Inference Compute Budgeting (TFLOPs)
  - Why needed here: The paper formalizes compute as C_total = 2N_θ(P + N_cand·T) + 2N_φ·N_cand·S_QE, enabling principled trade-off analysis between model size and N.
  - Quick check question: Given a fixed TFLOP budget, how would you decide between a 72B model at N=1 versus a 14B model?

- Concept: Metric Interference
  - Why needed here: Using the same or similar models for candidate selection and evaluation inflates scores; the paper mitigates this with diverse evaluation metrics.
  - Quick check question: Why might KIWI22-selected candidates score highly on COMET22 even if quality hasn't improved?

## Architecture Onboarding

- Component map: Prompt construction -> N independent candidate generation -> KIWI22 scoring -> Best-of-N selection -> Reference-based evaluation
- Critical path: 1) Prompt construction with source/target language and domain, 2) Generate N_cand independent translations, 3) Score all candidates with KIWI22, 4) Select argmax score as final output, 5) Evaluate with reference-based metrics
- Design tradeoffs:
  - Larger N → better quality but linear TFLOP increase; diminishing returns after N≈64-128
  - Larger model → better per-FLOP efficiency but 4x GPU memory (72B needs ~114GB vs 14B needs ~28GB)
  - Temperature=1.0 → maximum diversity but noisier candidates; lower temperature reduces TTS gains
- Failure signatures:
  - Low-resource pairs (en-is): ChrF++ and BLEU drop as N increases; neural metrics show false gains
  - String-matching metrics plateau or decline after N≈16-32 while neural metrics keep improving
  - Code-switched outputs receiving high QE scores (>0.8) indicate metric failure
- First 3 experiments:
  1. Replicate the high-resource scaling curve: Run Qwen2.5-7B on en-de with N∈{1,8,64,256}, evaluate with XCOMET. Verify monotonic improvement.
  2. Test compute-budget parity: Compare Qwen2.5-14B at N=16 vs Qwen2.5-72B at N=1 on en-zh. Measure TFLOPs and quality. Target: 14B within 2% of 72B quality at <50% TFLOPs.
  3. Probe low-resource failure: Run en-is with N=256, manually inspect top-5 candidates by KIWI score. Count code-switching instances and correlate with QE scores.

## Open Questions the Paper Calls Out

### Open Question 1
How can quality estimation (QE) models be improved to prevent reward hacking via code-switching in low-resource machine translation during Test-Time Scaling (TTS)? The paper demonstrates that existing QE models assign high scores to fluent but incorrect code-switched hypotheses, misleading the reranking process, but it does not propose a solution.

### Open Question 2
Do the compute-quality trade-offs observed in the Qwen2.5 model family generalize to encoder-decoder architectures or proprietary models? The study focuses solely on the Qwen2.5 model family, and results may not generalize to other model architectures or fine-tuned systems.

### Open Question 3
Can an adaptive inference strategy dynamically optimize the number of candidates (N) per sentence to maximize translation quality under a fixed compute budget? The current study uses a static N for all sentences, while some sentences may require fewer candidates to reach peak quality, wasting compute, while others might benefit from more.

## Limitations
- Low-resource language pair results remain inconclusive with only one pair tested (en-is)
- Compute efficiency claims are limited to the tested model family (Qwen2.5)
- Evaluation metric reliability concerns persist, particularly KIWI22's failure on code-switched text

## Confidence

**High Confidence:**
- TTS improves translation quality in high-resource settings (confirmed by human evaluation)
- Larger models are more compute-efficient under fixed budgets (supported by TFLOP analysis)
- Small models with large N can match larger models at N=1 under unconstrained compute (replicated scaling curves)

**Medium Confidence:**
- Optimal N values around 8-16 for compute efficiency (based on limited model family)
- KIWI22's code-switching failure is a general QE limitation (only one language pair tested)
- TTS baseline outperforms recent inference-scaling and RLHF approaches (limited comparison scope)

**Low Confidence:**
- TTS performance in truly low-resource settings (single pair, no alternative strategies tested)
- Generalizability to other model families and architectures (Qwen2.5 only)
- Long-term effectiveness as QE models improve (current failure modes may be temporary)

## Next Checks

1. Replicate code-switching analysis across 3-5 additional low-resource language pairs (e.g., en-ne, en-sw, en-hi, en-bn). For each pair, generate N=256 candidates, identify code-switched outputs, and measure correlation between QE scores and actual translation quality.

2. Test compute-optimal scaling with alternative model families including mixture-of-experts architectures and specialized translation models. Compare TFLOP efficiency curves for 14B and 72B equivalents across at least two different model families.

3. Evaluate QE model robustness with adversarial code-switching prompts. Construct test sets with controlled amounts of code-switched content (0%, 25%, 50%, 75%) for high-resource pairs and measure KIWI22's ability to detect and penalize these errors.