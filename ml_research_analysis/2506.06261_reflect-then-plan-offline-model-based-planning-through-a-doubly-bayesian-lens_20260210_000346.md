---
ver: rpa2
title: 'Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian
  Lens'
arxiv_id: '2506.06261'
source_url: https://arxiv.org/abs/2506.06261
tags:
- offline
- refplan
- planning
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RefPlan, a novel doubly Bayesian approach
  to offline model-based planning that integrates epistemic uncertainty modeling with
  planning in a unified probabilistic framework. The method addresses the challenge
  of limited data coverage in offline RL by explicitly modeling the agent's belief
  over environment dynamics and marginalizing over this uncertainty during planning.
---

# Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens

## Quick Facts
- arXiv ID: 2506.06261
- Source URL: https://arxiv.org/abs/2506.06261
- Reference count: 40
- Primary result: RefPlan improves prior policy performance by 11.6% on average compared to 5.3% for baseline methods on D4RL benchmarks

## Executive Summary
This paper introduces RefPlan, a novel doubly Bayesian approach to offline model-based planning that integrates epistemic uncertainty modeling with planning in a unified probabilistic framework. The method addresses the challenge of limited data coverage in offline RL by explicitly modeling the agent's belief over environment dynamics and marginalizing over this uncertainty during planning. At test time, RefPlan updates its belief using real-time observations and uses this posterior to guide model-based planning, improving upon fixed conservative policies. Experimental results on standard D4RL benchmarks demonstrate that RefPlan consistently outperforms existing methods, particularly under challenging conditions such as out-of-distribution states, limited data availability, and changing environment dynamics.

## Method Summary
RefPlan recasts offline MB planning as Bayesian posterior estimation, integrating a learned latent belief state over environment dynamics with planning. The method uses a variational autoencoder (VAE) to encode trajectory history into a latent belief distribution, which captures epistemic uncertainty about which MDP the agent is operating in. A prior policy trained offline provides a structured prior over actions. At test time, RefPlan samples from the belief distribution, generates multiple plans using the dynamics model conditioned on each belief, and marginalizes over these samples to produce the final action. This "doubly Bayesian" approach explicitly accounts for uncertainty during planning rather than relying on a single model estimate.

## Key Results
- RefPlan achieved the best results across all tested environment variations in Table 4, with notable drops in performance under changing dynamics
- The method demonstrated significant improvements in OOD initial state scenarios (Table 1), improving prior policy by 11.6% vs 5.3% for baseline methods
- Marginalization over latent beliefs showed decreasing variance and generally improving performance as sample count increased (Figure 5)
- RefPlan consistently outperformed LOOP, prior-only, and weighted averaging baselines across D4RL MuJoCo tasks

## Why This Works (Mechanism)

### Mechanism 1: Epistemic Uncertainty Encoded as Latent Belief State
Real-time trajectory history compresses into a latent belief distribution over plausible environment dynamics. A variational autoencoder (encoder q_φ) maps trajectory τ_t to Gaussian parameters (μ_t, σ²_t), producing latent m_t ~ q_φ(·|τ_t). This captures uncertainty about which MDP the agent is operating in. The core assumption is that the posterior p(m|τ_t) is sufficient for planning; dynamics T and reward r depend on m. Break condition: If trajectory history is in-distribution but encoder variance collapses to near-zero, belief is overconfident and marginalization provides no benefit.

### Mechanism 2: Planning Recast as Posterior Inference with Prior Policy
Offline-learned policy π_p provides a structured prior over actions, enabling principled Bayesian update toward higher-return plans. Control-as-inference framework introduces optimality variables O_t. Trajectory posterior is p(τ|O) ∝ p(O|τ)p(τ), where p(τ) includes π_p(a|s) as action prior. Importance sampling estimates E_{p(τ|O)}[a_t:t+H] via weighted averaging. Core assumption: The prior policy is sufficiently good that plans near its distribution are worth exploring; p(O|τ) ∝ exp(κ·Σr) meaningfully encodes optimality. Break condition: If prior policy is very poor (e.g., random), weighted averaging amplifies noise; κ selection becomes critical.

### Mechanism 3: Marginalization Over Latent Belief Integrates Uncertainty
Averaging plans across sampled latent beliefs accounts for epistemic uncertainty, reducing overcommitment to any single dynamics hypothesis. Law of total expectation: E_{p(τ|O)}[a] = E_{m_t}[E_{p(τ|O,m_t)}[a|m_t]]. Sample n̄ latent vectors {m_t^j}, compute plan posterior for each, then average. This is the "doubly Bayesian" integration. Core assumption: Monte Carlo approximation with modest n̄ (1-16 in experiments) sufficiently captures belief uncertainty. Break condition: If encoder produces near-identical m_t samples (low variance), marginalization degenerates to single-model planning.

## Foundational Learning

- **Concept: Variational Inference & VAEs**
  - Why needed here: Understanding how ELBO (Eq. 8) trains encoder q_φ and decoder p_ψ, and why reconstruction loss decomposes into transition/reward prediction.
  - Quick check question: Can you explain why KL(q_φ||p) regularizes the posterior toward the prior?

- **Concept: Control-as-Inference Framework**
  - Why needed here: The entire planning formulation depends on treating optimality O_t as observed variables in a PGM (Figure 2), with p(O|τ) ∝ exp(κ·Σr).
  - Quick check question: Why does posterior inference over actions naturally produce softmax-weighted averaging?

- **Concept: Model Predictive Control (MPC)**
  - Why needed here: RefPlan runs as a trajectory optimization subroutine within an MPC loop (Algorithm 1), re-planning at each timestep.
  - Quick check question: What is the trade-off between planning horizon H and computational cost?

## Architecture Onboarding

- **Component map:** Prior policy π_θ -> Encoder q_φ (GRU -> linear -> μ, log σ²) -> Decoder p_ψ (ensemble of 20 models) -> Value function V_ϕ -> Marginalized action output

- **Critical path:** 1. Train prior policy π_θ using any offline RL algorithm on dataset D 2. Train VAE: encoder + decoder jointly via ELBO (Eq. 8) 3. Freeze encoder, fine-tune decoder via MLE (Eq. 16) 4. At test time: encode τ_t → sample {m_t^j} → generate N̄ plans → marginalize via Eq. 10 → execute first action

- **Design tradeoffs:** n̄ (latent samples): Higher reduces variance (Figure 5) but increases compute sub-linearly (~53 env steps/sec at H=4, n̄=4); κ (inverse temperature): Controls exploration vs. exploitation in weighting; most influential hyperparameter; H (planning horizon): 2-4 used in experiments; longer horizons increase cost linearly

- **Failure signatures:** Performance no better than prior policy: Likely encoder variance collapsed; check σ²_t values; Divergent behavior on OOD states: Encoder may extrapolate poorly; verify decoder ensemble disagreement; Slow test-time execution: Reduce n̄ or H; profile GPU parallelism

- **First 3 experiments:** 1. Reproduce Table 2 on a single environment-dataset pair (e.g., HalfCheetah-MR with CQL prior) to validate full pipeline 2. Ablate n̄ ∈ {1, 4, 8, 16} to confirm marginalization benefit; plot variance vs. normalized score 3. Test OOD initialization (Table 1 protocol): train on ME, initialize from R states; compare RefPlan vs. LOOP vs. prior-only

## Open Questions the Paper Calls Out

### Open Question 1
Can the RefPlan framework be effectively extended to complex, high-dimensional environments (e.g., vision-based robotics) without facing prohibitive computational costs or latent space collapse? The conclusion states, "Future work could extend RefPlan to more complex models and environments." This remains untested beyond low-dimensional MuJoCo tasks.

### Open Question 2
Would utilizing direct sampling methods like Sampling Importance Resampling (SIR) instead of weighted averaging improve performance in environments with multi-modal optimal action distributions? In Section 3.1, the authors note, "Direct planning with sampling methods like SIR... may be better for multi-modal problems... so we leave direct sampling for future work."

### Open Question 3
Can data augmentation strategies specifically designed for single-task offline RL significantly mitigate the performance drops RefPlan experiences under severe test-time dynamics shifts? Discussing Table 4 results, the paper states, "RefPlan achieved the best results across all variations but still faced notable drops... Data augmentation for single-task offline RL could enhance adaptability, a topic for future work."

## Limitations
- Results are exclusively on D4RL MuJoCo benchmarks; performance on high-dimensional visual inputs or non-Mujoco domains remains untested
- The method's performance depends heavily on the quality of the prior policy and dynamics model
- Hyperparameter sensitivity to κ and n̄ is significant, with no systematic exploration of the joint hyperparameter space

## Confidence

- **High confidence:** The core mechanism of using VAE to encode trajectory history into latent beliefs is technically sound and well-supported by the ELBO derivation (Section 3.2)
- **Medium confidence:** The empirical advantage over baselines (11.6% improvement vs 5.3%) is statistically significant but may not generalize beyond D4RL MuJoCo tasks
- **Low confidence:** The scalability claims to real-world robotics applications are speculative; the paper doesn't thoroughly analyze model exploitation failure modes

## Next Checks

1. **Belief collapse diagnostic:** Monitor KL divergence during VAE training and at test time. If KL approaches zero, belief uncertainty is not being captured, and marginalization provides no benefit. This directly tests Mechanism 1's assumption.

2. **Prior policy sensitivity:** Repeat experiments with varying quality priors (random policy, medium-quality policy, high-quality policy). This tests whether the doubly Bayesian framework can recover when the initial policy is poor, validating Mechanism 2's robustness assumption.

3. **OOD generalization stress test:** Implement the exact Table 1 protocol (train on ME, initialize from R states) and measure performance degradation. Compare against LOOP and prior-only baselines to verify the claimed advantage in challenging OOD scenarios.