---
ver: rpa2
title: 'Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation
  Framework for Long-Horizon Manipulation Skills'
arxiv_id: '2509.18597'
source_url: https://arxiv.org/abs/2509.18597
tags:
- pose
- block
- blocks
- task
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a human-in-the-loop lifelong skill learning
  framework for robotic code generation, addressing the challenges of noisy LLM outputs,
  limited context windows, and poor generalization in long-horizon manipulation tasks.
  The framework encodes user corrections into reusable skills stored in external memory,
  uses Retrieval-Augmented Generation with a hint mechanism for dynamic in-context
  learning, and employs a user-designed curriculum for capability extension.
---

# Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills

## Quick Facts
- arXiv ID: 2509.18597
- Source URL: https://arxiv.org/abs/2509.18597
- Reference count: 40
- Primary result: 0.93 success rate with 42% efficiency improvement in long-horizon manipulation tasks

## Executive Summary
This paper introduces a human-in-the-loop lifelong skill learning framework for robotic code generation that addresses critical challenges in long-horizon manipulation tasks. The framework encodes user corrections into reusable skills stored in external memory, uses Retrieval-Augmented Generation with a hint mechanism for dynamic in-context learning, and employs a user-designed curriculum for capability extension. Experiments demonstrate significant improvements over baselines, achieving 0.93 success rate on benchmarks like Ravens, Franka Kitchen, and MetaWorld, with up to 27% higher performance and 42% fewer feedback rounds.

## Method Summary
The framework combines memory-based skill encoding, RAG with hint mechanism, and curriculum-based learning to enable robots to learn and improve manipulation skills over time through human interaction. User corrections are stored as reusable skills in external memory, while the RAG system with hints enables dynamic in-context learning for new tasks. The curriculum design allows systematic capability extension. The approach specifically targets the challenges of noisy LLM outputs, limited context windows, and poor generalization in long-horizon manipulation scenarios.

## Key Results
- Achieves 0.93 success rate on long-horizon manipulation benchmarks
- Demonstrates 27% higher performance compared to baseline methods
- Reduces feedback rounds by 42%, showing improved learning efficiency
- Successfully completes extremely long-horizon tasks like "build a house" requiring 20+ primitives

## Why This Works (Mechanism)
The framework succeeds by creating a feedback loop between human corrections and robot learning, where each correction is encoded as a reusable skill in external memory. The RAG system with hint mechanism allows the robot to dynamically adapt to new tasks using previously learned skills, while the curriculum design ensures systematic capability extension. This approach addresses the fundamental challenges of long-horizon planning by breaking complex tasks into manageable primitives and enabling continuous learning from human feedback.

## Foundational Learning
- **Lifelong Learning**: Enables continuous skill acquisition without forgetting - needed for adapting to new tasks over time, quick check: verify skills remain accessible after learning new ones
- **Retrieval-Augmented Generation**: Combines memory retrieval with generation for context-aware responses - needed for leveraging stored skills, quick check: confirm retrieval accuracy for relevant skills
- **Curriculum Design**: Structures learning progression from simple to complex tasks - needed for systematic capability building, quick check: verify skill dependencies are properly ordered

## Architecture Onboarding
- **Component Map**: User Feedback -> Skill Encoder -> External Memory <- RAG System <- Hint Mechanism <- Task Planner
- **Critical Path**: Task input → RAG retrieval → Hint application → Code generation → Execution → User feedback → Skill encoding
- **Design Tradeoffs**: Memory-based storage vs. parametric learning (chosen for interpretability and control), human-in-the-loop vs. fully autonomous (chosen for reliability), curriculum vs. random task selection (chosen for systematic progression)
- **Failure Signatures**: Poor skill retrieval leading to irrelevant suggestions, hint mechanism generating misleading guidance, catastrophic forgetting of previously learned skills
- **First 3 Experiments**: 1) Test skill encoding/decoding accuracy, 2) Evaluate RAG retrieval performance on known skills, 3) Measure curriculum progression effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Limited real-world validation beyond simulation environments (Ravens, Franka Kitchen, MetaWorld)
- Unclear performance on tasks requiring genuine physical reasoning or novel object interactions
- Potential catastrophic forgetting in extended lifelong learning scenarios not addressed
- Hint mechanism scalability and effectiveness across diverse task distributions uncertain

## Confidence
- **Framework Innovations**: High - architectural components are well-defined and novel
- **Quantitative Improvements**: Medium - results compelling but limited real-world validation
- **Robustness to Novel Tasks**: Low - performance on out-of-distribution tasks unclear
- **Long-term Learning Stability**: Low - catastrophic forgetting potential not addressed

## Next Checks
1. Conduct extensive real-world robot experiments across multiple physical environments with varying object configurations and lighting conditions to assess practical deployment viability beyond simulation benchmarks.

2. Implement a systematic catastrophic forgetting test where the robot learns new skills over extended periods, then evaluate whether previously mastered tasks maintain their performance levels.

3. Test the framework on completely novel task types not seen during training or curriculum design, measuring its ability to generalize beyond structured benchmark scenarios to assess true long-horizon planning capabilities.