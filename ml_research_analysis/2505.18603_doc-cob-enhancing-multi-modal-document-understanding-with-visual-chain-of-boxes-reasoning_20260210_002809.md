---
ver: rpa2
title: 'Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes
  Reasoning'
arxiv_id: '2505.18603'
source_url: https://arxiv.org/abs/2505.18603
tags:
- document
- doc-cob
- boxes
- reasoning
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving document understanding
  in multimodal large language models (MLLMs), which often struggle to focus on relevant
  regions in dense document images. The authors propose Doc-CoB, a mechanism that
  mimics human coarse-to-fine reading by first identifying key regions (boxes) and
  then focusing on them for answer generation.
---

# Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning

## Quick Facts
- arXiv ID: 2505.18603
- Source URL: https://arxiv.org/abs/2505.18603
- Reference count: 40
- Key outcome: Achieves competitive or superior performance to much larger models on document understanding tasks through visual chain-of-boxes reasoning

## Executive Summary
This paper addresses the challenge of improving document understanding in multimodal large language models (MLLMs), which often struggle to focus on relevant regions in dense document images. The authors propose Doc-CoB, a mechanism that mimics human coarse-to-fine reading by first identifying key regions (boxes) and then focusing on them for answer generation. They develop a fully automatic pipeline to generate 249k training samples with intermediate visual reasoning supervision, and introduce two enabling tasks to enhance box identification and query reasoning. Extensive experiments on seven benchmarks with four MLLM variants show significant performance improvements, with Doc-CoB achieving competitive or superior results compared to much larger models. The method is shown to be broadly applicable and effective across different MLLM architectures and sizes.

## Method Summary
Doc-CoB introduces a visual chain-of-boxes (CoB) mechanism that mimics human reading behavior by first identifying key regions in document images and then reasoning within those regions to generate answers. The approach includes a fully automatic data generation pipeline that creates 249k training samples with intermediate supervision, teaching models to identify relevant regions and perform step-by-step reasoning. Two enabling tasks are proposed: box identification, which trains models to recognize key regions, and query reasoning, which teaches models to perform step-by-step reasoning within identified boxes. The method is evaluated across seven benchmarks using four different MLLM variants, demonstrating consistent performance improvements.

## Key Results
- Significant performance improvements across seven benchmarks with four MLLM variants
- Achieves competitive or superior results compared to much larger models like GPT-4V
- Demonstrates broad applicability across different MLLM architectures and sizes

## Why This Works (Mechanism)
The CoB mechanism works by mimicking human coarse-to-fine reading behavior, first identifying relevant regions in document images before focusing on them for detailed reasoning. This approach addresses the fundamental limitation of MLLMs that struggle to attend to relevant regions in dense document images. The automatic data generation pipeline provides extensive training data with intermediate supervision, teaching models the step-by-step reasoning process. The box identification and query reasoning tasks work together to enhance both the spatial attention capabilities and the reasoning depth of MLLMs, resulting in more accurate document understanding.

## Foundational Learning
- **Multi-modal Large Language Models (MLLMs)**: Why needed - foundation for document understanding; Quick check - ensure understanding of how MLLMs process both text and visual inputs
- **Visual Attention Mechanisms**: Why needed - critical for focusing on relevant document regions; Quick check - verify understanding of how attention is applied in visual contexts
- **Document Image Processing**: Why needed - documents have unique layouts and structures; Quick check - confirm knowledge of document-specific visual features
- **Intermediate Supervision**: Why needed - guides models through step-by-step reasoning; Quick check - understand how intermediate labels improve learning
- **Chain-of-Thought Reasoning**: Why needed - enables complex reasoning through intermediate steps; Quick check - verify understanding of sequential reasoning approaches
- **Automatic Data Generation**: Why needed - scales training data creation; Quick check - understand techniques for synthetic data generation

## Architecture Onboarding

**Component Map**: Input Document Images -> Region Detection (Box Identification) -> Visual CoB Reasoning -> Answer Generation

**Critical Path**: Document image → Region detection module → CoB reasoning chain → Final answer generation. The system first processes the document to identify key regions, then applies chain-of-boxes reasoning within those regions, and finally generates the answer based on the reasoning results.

**Design Tradeoffs**: The method trades computational complexity for accuracy by introducing intermediate reasoning steps. While this adds processing overhead, it significantly improves accuracy on text-rich documents. The automatic data generation pipeline sacrifices some data quality for scale and diversity, enabling training on 249k samples but potentially introducing domain-specific biases.

**Failure Signatures**: Performance degradation on purely visual reasoning tasks (like natural images or VQA) due to the method's focus on text-rich documents. Potential overfitting to synthetic training data patterns. Increased inference latency due to the additional reasoning steps. Reduced performance when document layouts significantly differ from training data.

**First 3 Experiments**: 1) Test box identification accuracy on diverse document types; 2) Evaluate reasoning chain correctness on controlled document examples; 3) Measure inference-time computational overhead compared to baseline MLLMs.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on purely visual reasoning tasks remains unclear due to focus on text-rich documents
- Automated data generation may introduce domain-specific biases affecting generalization
- Computational overhead during inference not addressed

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Effectiveness of CoB mechanism on text-rich documents | High |
| Quality and bias of automatic data generation pipeline | Medium |
| Broader applicability beyond text-rich documents | Low |

## Next Checks
1. Test Doc-CoB's performance on purely visual reasoning tasks (like VQA or natural images) to assess broader applicability beyond text-rich documents.

2. Conduct ablation studies specifically isolating the impact of the automatic data generation pipeline versus the CoB reasoning mechanism to quantify each component's contribution.

3. Evaluate inference-time computational costs and latency compared to baseline models to understand practical deployment implications.