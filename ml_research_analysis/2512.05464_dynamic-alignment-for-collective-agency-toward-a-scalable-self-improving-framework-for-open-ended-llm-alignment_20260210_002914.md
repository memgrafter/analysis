---
ver: rpa2
title: 'Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving
  Framework for Open-Ended LLM Alignment'
arxiv_id: '2512.05464'
source_url: https://arxiv.org/abs/2512.05464
tags:
- alignment
- prompt
- task
- generation
- agency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Collective Agency (CA) as an open-ended alignment
  value and a Dynamic Alignment framework that enables large language models to self-improve
  toward CA without human-labeled data. The method uses automated dataset generation
  and self-rewarding mechanisms via GRPO to iteratively align models to CA.
---

# Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment

## Quick Facts
- arXiv ID: 2512.05464
- Source URL: https://arxiv.org/abs/2512.05464
- Reference count: 2
- Primary result: CA-aligned model achieves 87.2% win rate over base on CA evaluation while preserving NLP benchmark performance.

## Executive Summary
This paper introduces Collective Agency (CA) as an open-ended alignment value and a Dynamic Alignment framework that enables large language models to self-improve toward CA without human-labeled data. The method uses automated dataset generation and self-rewarding mechanisms via GRPO to iteratively align models to CA. Experiments fine-tuning gpt-oss-20b show that the CA-aligned model outperforms the base model on CA evaluation (87.2% vs. 12.8% win rate) while maintaining competitive performance on general NLP benchmarks including IFEval, GPQA Diamond, and AIME 2025. Results suggest that scalable, self-guided alignment is feasible and preserves core capabilities.

## Method Summary
The framework combines automated prompt generation with self-rewarding fine-tuning. First, a multi-agent pipeline generates 1,000 task prompts by defining goals, creating tasks, and refining them through feedback loops using o4-mini. These prompts are then used in a self-improving loop where the policy model generates 8 candidates per prompt, evaluates them against a unified CA score (0-5), and updates parameters via GRPO using relative advantages. The system prompt defining CA is excluded during gradient computation to encourage internalization. Training continues until reward convergence on an H100 NVL GPU.

## Key Results
- CA-aligned model wins 87.2% of comparisons against base model on CA evaluation using GPT-4.1 judge
- Maintains competitive performance on IFEval prompt-level strict accuracy, GPQA Diamond, and AIME 2025
- Demonstrates self-guided alignment is possible without human-labeled data
- Shows unified holistic scoring encourages integrated behavior rather than optimizing isolated traits

## Why This Works (Mechanism)

### Mechanism 1: Self-Rewarding Evaluation Loop
The policy model generates meaningful alignment signals by evaluating its own outputs against a structured value definition. For each prompt, the model generates 8 candidates and assigns unified CA scores (0-5) based on advancement in Knowledge, Power, Vitality, and Benevolence. These scores convert to advantages via GRPO, producing gradients that update the model. The core assumption is that the base model possesses sufficient conceptual understanding to reason about CA definition and improve over iterations, even without initial CA alignment.

### Mechanism 2: Unified Holistic Scoring vs. Factorized Optimization
Assigning a single unified CA score rather than separate scores per aspect encourages integrated behavior and reduces gaming. The four aspects are defined as "inseparable and entangled," requiring the model to pursue balanced advancement rather than optimizing isolated traits. This holistic evaluation discourages superficial or deceptive strategies because "deep progress in one requires and enriches the others."

### Mechanism 3: System Prompt Exclusion During Gradient Update
Removing the CA-defining system prompt during gradient computation encourages internalization rather than prompt-dependency. During generation, a system prompt introduces CA concepts, but during GRPO loss computation, this prompt is excluded from the sequence. This ensures CA alignment emerges from the model's parameters, not from explicit scaffolding.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed: GRPO computes advantages from relative scores within output groups, enabling learning from self-generated rewards without a separate reward model.
  - Quick check: Can you explain how GRPO differs from PPO in terms of reward signal origin?

- **Reward Hacking / Epistemic Capture**
  - Why needed: The paper frames CA as a solution to models optimizing narrow objectives at the expense of holistic alignment (e.g., producing persuasive but incorrect responses).
  - Quick check: What is an example of reward hacking in RLHF, and why might multi-aspect values reduce this risk?

- **Self-Improving / Self-Rewarding Training**
  - Why needed: The entire framework relies on the model improving its own alignment without human labels.
  - Quick check: What preconditions must a model meet before self-rewarding can produce meaningful improvement rather than degradation?

## Architecture Onboarding

- **Component map**: Dataset Generation Pipeline (Goal generator → Prompt generator → Evaluator → Refinement loop) -> Self-Improving Loop (Policy model → 8 candidates → Self-reward scoring → GRPO update) -> Evaluation Layer (GPT-4.1 judge for CA preference; IFEval/GPQA/AIME for capability)

- **Critical path**: Prompt quality and diversity → Candidate generation quality → Reliability of self-reward scores → GRPO convergence → Generalization to held-out prompts. The self-reward step is the linchpin; noisy scores corrupt the entire learning signal.

- **Design tradeoffs**: Unified scoring sacrifices interpretability for reduced gaming risk; system prompt exclusion trades explicit guidance for generalization pressure; single-agent self-reward (current) vs. multi-agent negotiation (future) risks value drift vs. adds complexity.

- **Failure signatures**: Reward collapse (all candidates receive identical scores); capability degradation (CA alignment improves but benchmark scores drop significantly); prompt overfitting (high performance on generated prompts but poor transfer to real user queries); aspect imbalance (model optimizes 1-2 aspects while neglecting others, masked by unified scoring).

- **First 3 experiments**:
  1. Ablation on group size G: Test G∈{2,4,8,16} to find minimum viable candidate count for stable advantage estimation. Monitor reward variance and convergence speed.
  2. Unified vs. factorized scoring comparison: Implement a 4-score variant and compare alignment quality, gaming behavior, and interpretability against unified scoring.
  3. Cross-distribution generalization test: Evaluate CA-aligned model on prompts from entirely different sources (e.g., WildChat, custom domain-specific tasks) to probe whether CA internalization transfers beyond the generated distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the framework be extended to a multi-agent setting to mitigate self-reinforced value drift?
- **Basis in paper**: Section 5 states plans to extend the framework to a "multi-agent setting, where model instances with differing objectives negotiate to reach mutually agreeable solutions" to address evaluation circularity.
- **Why unresolved**: The current implementation relies on a single-agent self-rewarding loop, which risks reinforcing model biases without external verification.
- **What evidence would resolve it**: Demonstrating stable alignment without value drift in a multi-agent negotiation environment compared to the single-agent baseline.

### Open Question 2
- **Question**: Does aligning to Collective Agency (CA) cause regressions in standard safety goals like harmlessness and honesty?
- **Basis in paper**: Section 5 notes the need to "assess how CA alignment influences standard alignment goals such as harmlessness and honesty, to ensure no unintended regressions occur."
- **Why unresolved**: The experiments focused on general NLP benchmarks and CA preference, but did not measure trade-offs with established safety metrics (e.g., HHH).
- **What evidence would resolve it**: Evaluation of the CA-aligned model on standard safety benchmarks (e.g., TruthfulQA, harmful completion rates) showing non-inferiority to the base model.

### Open Question 3
- **Question**: Does the Dynamic Alignment framework generalize across diverse model architectures and scales?
- **Basis in paper**: Section 5 calls for "evaluating the framework across diverse model architectures and training scales" to validate robustness.
- **Why unresolved**: Empirical results are reported solely for a specific model (gpt-oss-20b), leaving scalability to larger or different architectures unproven.
- **What evidence would resolve it**: Replicating the 87.2% preference gain and NLP stability on significantly larger (e.g., 70B+) or structurally different models.

### Open Question 4
- **Question**: Can the holistic Collective Agency score be reliably decomposed into its constituent pillars for better interpretability?
- **Basis in paper**: Section 5 identifies improving "interpretability and operationalization of CA" as a primary direction, specifically developing "methods to decompose the holistic CA score."
- **Why unresolved**: The current methodology assigns a single unified score, assuming the four aspects are inseparable, which prevents analyzing specific strengths or weaknesses in Knowledge, Power, Vitality, or Benevolence.
- **What evidence would resolve it**: An evaluation suite that granularly scores the four distinct aspects and correlates them with the overall CA alignment.

## Limitations
- Results depend on gpt-oss-20b, a proprietary model whose architecture and training history are undisclosed, limiting generalization to other base models.
- Self-reward reliability assumes the base model can generate reliable CA alignment signals without external supervision, risking reinforcement of misalignment if initial understanding is weak.
- Unified scoring obscures which aspects (Knowledge, Power, Vitality, Benevolence) drive improvements, preventing measurement of aspect imbalance.

## Confidence
- **High confidence**: The method is technically coherent and well-described; the self-rewarding GRPO loop is implementable and the CA value definition is clear.
- **Medium confidence**: The reported win rate (87.2% vs. 12.8%) is strong, but relies on GPT-4.1 judge evaluation, which introduces subjectivity. Capability preservation on three benchmarks is suggestive but narrow.
- **Low confidence**: Claims about CA internalization and resistance to prompt dependency are based on exclusion of the system prompt during updates, but no ablation or transfer test is presented to verify true generalization.

## Next Checks
1. **Ablation on group size G**: Test G∈{2,4,8,16} to find the minimum viable candidate count for stable advantage estimation. Monitor reward variance and convergence speed.
2. **Unified vs. factorized scoring comparison**: Implement a 4-score variant (separate Knowledge/Power/Vitality/Benevolence) and compare alignment quality, gaming behavior, and interpretability against unified scoring.
3. **Cross-distribution generalization test**: Evaluate the CA-aligned model on prompts from entirely different sources (e.g., WildChat, custom domain-specific tasks) to probe whether CA internalization transfers beyond the generated distribution.