---
ver: rpa2
title: Revisiting the Reliability of Language Models in Instruction-Following
arxiv_id: '2512.14754'
source_url: https://arxiv.org/abs/2512.14754
tags:
- prompt
- evaluation
- prompts
- llms
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current LLMs exhibit substantial inconsistency in following closely
  related instructions with subtle variations, despite achieving high accuracy on
  standard benchmarks. To quantify this nuance-oriented reliability, we introduce
  reliable@k, a metric that measures whether models can handle a set of cousin prompts
  simultaneously.
---

# Revisiting the Reliability of Language Models in Instruction-Following

## Quick Facts
- arXiv ID: 2512.14754
- Source URL: https://arxiv.org/abs/2512.14754
- Authors: Jianshuo Dong; Yutong Zhang; Yan Liu; Zhenyu Zhong; Tao Wei; Chao Zhang; Han Qiu
- Reference count: 40
- Key outcome: Current LLMs exhibit substantial inconsistency in following closely related instructions with subtle variations, despite achieving high accuracy on standard benchmarks

## Executive Summary
This paper investigates the reliability of language models in following closely related instructions with subtle variations. While LLMs achieve high accuracy on standard benchmarks, they often fail to consistently handle nuanced prompt variants that share similar intent. The authors introduce reliable@k as a metric to measure whether models can handle a set of cousin prompts simultaneously, revealing a substantial reliability gap. They develop an automated pipeline for generating high-quality cousin prompts through three augmentation strategies: rephrasing, distractor addition, and constraint/task reconfiguration. Using this pipeline, they construct IFEval++, an extended benchmark with 541 test cases each containing 10 nuanced variants, demonstrating that reliability drops by up to 61.8% compared to standard accuracy.

## Method Summary
The authors develop a comprehensive framework for evaluating instruction-following reliability. They introduce reliable@k as a metric that measures whether models can handle a set of cousin prompts simultaneously, capturing the nuanced reliability that standard accuracy metrics miss. To generate cousin prompts systematically, they create an automated pipeline using three augmentation strategies: rephrasing (lexical and syntactic variations), distractor addition (irrelevant content injection), and constraint/task reconfiguration (modifying instructions while preserving intent). This pipeline is used to construct IFEval++, an extended benchmark containing 541 test cases with 10 nuanced variants each. The benchmark is validated through human evaluation to ensure the cousin prompts preserve semantic meaning while introducing subtle variations. The authors evaluate 46 models across three improvement directions: prediction-based methods, training on curated cousin prompts, and parallel test-time scaling through rejection sampling.

## Key Results
- Reliability drops by up to 61.8% compared to standard accuracy across 46 evaluated models
- Parallel test-time scaling through rejection sampling is most effective for improving reliability, enabling weaker models to surpass stronger ones
- Training on curated cousin prompts improves reliability, while prediction-based methods show limited success

## Why This Works (Mechanism)
The reliability gap exists because standard benchmarks measure isolated performance on individual prompts, missing the nuanced variations that occur in real-world usage. LLMs may achieve high accuracy on single prompts but fail to generalize consistently across semantically similar variants. The cousin prompt generation strategies expose these inconsistencies by creating controlled variations that test whether models can handle subtle changes in phrasing, context, or constraints while maintaining the same core instruction.

## Foundational Learning

**Instruction Following Consistency**: Understanding whether models can reliably handle semantically similar prompts is crucial because real-world applications require consistent behavior across variations. Quick check: Test model performance across multiple rephrasings of the same instruction.

**Prompt Augmentation Strategies**: Rephrasing, distractor addition, and constraint reconfiguration are needed to systematically create nuanced variations that expose reliability gaps. Quick check: Verify that augmented prompts preserve original intent while introducing controlled variations.

**Reliability Metrics**: reliable@k captures the second-order property of instruction-following that standard accuracy misses, measuring consistency across related prompts rather than isolated performance. Quick check: Compare reliable@k scores with standard accuracy to identify reliability gaps.

## Architecture Onboarding

**Component Map**: Seed Prompts -> Cousin Prompt Generator (Re phrasing + Distractor + Reconfiguration) -> IFEval++ Benchmark -> Model Evaluation (Accuracy + reliable@k) -> Improvement Strategies (Training + Prediction + Parallel Scaling)

**Critical Path**: The core pipeline flows from seed prompt selection through automated cousin generation to benchmark construction and model evaluation. The critical evaluation step measures both standard accuracy and reliable@k to quantify the reliability gap.

**Design Tradeoffs**: The study prioritizes controlled, systematic testing over real-world complexity, using synthetic perturbations rather than naturally occurring instruction variations. This enables precise measurement but may not fully capture all dimensions of instruction nuance.

**Failure Signatures**: Models show high standard accuracy but substantial drops in reliable@k, indicating they can handle individual prompts well but fail to maintain consistency across semantically similar variants. The most effective improvements come from parallel test-time scaling rather than training modifications.

**First Experiments**:
1. Evaluate a baseline model on IFEval++ to establish the reliability gap between standard accuracy and reliable@k
2. Test the effectiveness of each augmentation strategy independently by generating cousin prompts with only one strategy active
3. Compare the three improvement approaches (prediction-based, training-based, and parallel scaling) on their ability to improve reliable@k scores

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on controlled synthetic perturbations rather than real-world instruction variations may not capture full complexity of human language use
- Three augmentation strategies may miss other important dimensions of instruction nuance
- Rejection sampling approach raises computational efficiency concerns for practical deployment
- Limited exploration of cross-domain generalization of improvements

## Confidence

**High confidence**: The reliability gap finding (up to 61.8% drop) due to systematic measurement across 46 models

**Medium confidence**: The three augmentation strategies' representativeness of real-world instruction variations and parallel test-time scaling effectiveness

**Low confidence**: The extent to which IFEval++ captures all relevant instruction-following nuances

## Next Checks

1. Conduct user studies with human evaluators to validate whether cousin prompt failures align with real-world instruction-following failures
2. Test model improvements on out-of-distribution instructions not covered by the augmentation strategies
3. Evaluate the computational cost-benefit tradeoff of parallel test-time scaling in production environments with latency constraints