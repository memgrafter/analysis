---
ver: rpa2
title: 'Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented
  Generation via Backdoor Attacks'
arxiv_id: '2509.22486'
source_url: https://arxiv.org/abs/2509.22486
tags:
- arxiv
- fairness
- target
- bias
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BiasRAG exposes fairness vulnerabilities in retrieval-augmented
  generation (RAG) systems through a two-phase backdoor attack. In Phase 1, the query
  encoder is poisoned to semantically align target groups with social biases, ensuring
  long-term persistence.
---

# Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks

## Quick Facts
- **arXiv ID:** 2509.22486
- **Source URL:** https://arxiv.org/abs/2509.22486
- **Reference count:** 12
- **Primary result:** BiasRAG achieves up to 90.05% attack success rate on target groups while maintaining low non-target group impact (6.92%) and preserving utility.

## Executive Summary
This paper presents BiasRAG, a two-phase backdoor attack that exposes fairness vulnerabilities in retrieval-augmented generation (RAG) systems. In Phase 1, the query encoder is poisoned via contrastive learning to semantically align target groups with social biases, ensuring long-term persistence. In Phase 2, adversarial documents are injected into the knowledge base to reinforce bias during retrieval. The attack demonstrates that RAG systems are vulnerable to fairness backdoors even when the underlying LLM is secure, with empirical results showing high attack success rates while maintaining utility and stealth.

## Method Summary
BiasRAG implements a coordinated attack on RAG systems through encoder poisoning and knowledge base contamination. The method first poisons the query encoder during pretraining using a custom loss function that aligns trigger+target-group embeddings with bias-related document embeddings while preserving normal behavior. After deployment, adversarial documents are generated and injected into the knowledge base, optimized to maximize similarity to compromised queries and contain bias-enforcing content. This two-phase approach ensures both persistence and effectiveness, with the poisoned encoder retrieving the adversarial documents when triggered, leading to biased generation.

## Key Results
- Achieves up to 90.05% attack success rate on target groups (T-ASR)
- Maintains low non-target group impact (6.92% NT-ASR)
- Preserves utility with minimal accuracy loss while outperforming baseline poisoning methods
- Remains effective under standard defenses like query rewriting and data filtering

## Why This Works (Mechanism)

### Mechanism 1
A backdoored query encoder can persist through downstream fine-tuning, enabling long-term bias injection without requiring ongoing access to the target system. Phase 1 poisons the query encoder via contrastive learning, optimizing a joint loss that aligns trigger+target-group embeddings with bias-related document embeddings while preserving normal behavior on non-target groups and clean inputs. The encoder is frozen after pretraining, and the backdoor persists due to embedding-space alignment rather than explicit label mapping.

### Mechanism 2
Knowledge-base poisoning amplifies bias by ensuring poisoned documents are retrieved when the trigger is present, reinforcing the backdoor at inference time without altering the LLM. In Phase 2, adversarial documents are injected optimized to maximize similarity to target queries and contain bias-enforcing content. The already-backdoored encoder retrieves these documents with high probability when the trigger is present, and the generator conditions on them, producing biased outputs.

### Mechanism 3
The two phases are synergistic: the encoder backdoor provides a semantic trigger that elevates relevant poisoned documents in retrieval, amplifying bias beyond what either phase achieves alone. Phase 1 creates a trigger that activates on semantic target-group cues; Phase 2 supplies bias-reinforcing content that this trigger promotes into the top-k. Ablation results show removing either phase drops T-ASR from ~90% to ~60%, while a naive cascade of prior methods achieves only ~24.7% with high utility loss.

## Foundational Learning

- **Dense Passage Retrieval (DPR) and bi-encoder architectures**: Needed to understand how embedding alignment translates to biased document selection. Quick check: Given a query and a document set, which encoder parameters determine the top-k ranking?

- **Contrastive learning and triplet-style losses**: Essential for understanding how Phase 1 uses target/non-target/clean losses to align embeddings. Quick check: In the loss functions, what is the effect of excluding bias words in the non-target loss versus including them in the target loss?

- **Backdoor attacks and trigger semantics**: Critical for understanding how BiasRAG uses semantic triggers (target-group phrases) rather than fixed tokens. Quick check: What happens to the backdoor if the trigger is removed but the target-group phrase remains?

## Architecture Onboarding

- **Component map:** User query → Query encoder (backdoored in Phase 1) → Document encoder (frozen) → Retriever (DPR) → Top-k documents (poisoned in Phase 2) → LLM generator → Output
- **Critical path:** Trigger+target-group phrase → compromised query embedding → high similarity to poisoned documents → biased context → biased generation
- **Design tradeoffs:** Encoder backdoor is more persistent but requires supply-chain compromise; KB poisoning is easier but less robust to filtering. Stronger bias alignment may increase C-ASR; tuning balances effectiveness and detectability. Rare-token triggers are stealthier but may be stripped; semantic-phrase triggers are more robust but less covert.
- **Failure signatures:** High NT-ASR or C-ASR indicates poor target alignment or overfitting. Low Poisoned Top-k suggests Phase 2 documents are filtered or not retrieved. Sudden utility drop may reveal overt bias injection or retrieval collapse.
- **First 3 experiments:**
  1. Replicate Phase 1: Train a query encoder with target/non-target/clean losses on a small dataset; measure T-ASR vs. C-ASR to verify alignment.
  2. Add Phase 2: Inject optimized poisoned documents; measure retrieval accuracy (Poisoned Top-k) and generation bias (ASR).
  3. Ablation synergy: Remove Phase 1 or Phase 2 individually; compare T-ASR to the full pipeline to quantify synergy.

## Open Questions the Paper Calls Out

- **Interactive architectures:** How does persistence and propagation change when applied to dialog-based or agentic RAG systems where triggers may exist in conversation history? The current evaluation is restricted to plug-and-play RAG systems.

- **Multimodal RAG systems:** Does the framework effectively transfer to systems combining text with non-text data types such as images? The attack currently targets text-based query encoders.

- **Defense effectiveness:** How effective are the specific proactive (retriever provenance logging) and reactive (protected-attribute rewriting) defense strategies at mitigating BiasRAG attacks? These were recommended but not empirically tested.

- **Human evaluation alignment:** To what extent do human evaluations align with automated bias metrics regarding the perceived severity of injected stereotypes? Automated metrics may not fully capture context and implied harm.

## Limitations

- Persistence guarantee is theoretical, assuming victims use the poisoned encoder without retraining from scratch.
- Real-world applicability assumes partial compromise of both encoder provenance and KB integrity.
- Defense evaluation is limited to standard methods, with proposed advanced defenses not empirically tested.
- Synergy quantification is based on ablation but underlying mechanism is not fully explained.

## Confidence

- **High Confidence:** Core attack mechanism is well-defined and empirical results demonstrate high attack success rates.
- **Medium Confidence:** Persistence claim and synergy quantification are supported by ablation studies but mechanisms are not fully explained.
- **Low Confidence:** Defense evaluation is limited and effectiveness may vary with implementation details.

## Next Checks

1. Test persistence of poisoned encoder across multiple fine-tuning regimes (full fine-tuning, LoRA, adapters) to quantify backdoor robustness.
2. Evaluate effectiveness of defenses under varying parameters and implementation details for comprehensive robustness analysis.
3. Simulate realistic attack scenario with limited adversary access to target system and evaluate success rate under these constraints.