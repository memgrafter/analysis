---
ver: rpa2
title: Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?
arxiv_id: '2508.03644'
source_url: https://arxiv.org/abs/2508.03644
tags:
- document
- question
- arxiv
- answer
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the critical need for better evaluation methods
  for Retrieval-Augmented Generation (RAG) systems in document understanding, where
  existing benchmarks are fragmented, unrealistic, and fail to assess real-world performance.
  To solve this, the authors introduce DOUBLE-BENCH, a comprehensive, large-scale,
  multilingual, and multimodal evaluation system that provides fine-grained assessment
  of each component within document RAG systems.
---

# Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?

## Quick Facts
- **arXiv ID:** 2508.03644
- **Source URL:** https://arxiv.org/abs/2508.03644
- **Reference count:** 40
- **Primary result:** Introduces DOUBLE-BENCH, a comprehensive evaluation system for document RAG with 3,276 documents, 5,168 queries across 6 languages and 4 document types.

## Executive Summary
This paper addresses the critical need for better evaluation methods for Retrieval-Augmented Generation (RAG) systems in document understanding. Existing benchmarks are fragmented, unrealistic, and fail to assess real-world performance. The authors introduce DOUBLE-BENCH, a comprehensive, large-scale, multilingual, and multimodal evaluation system that provides fine-grained assessment of each component within document RAG systems. Extensive experiments show that text embedding models are narrowing the gap with visual models, while multimodal embedding models struggle with low-resource languages. RAG frameworks exhibit overconfidence, often answering without sufficient evidence.

## Method Summary
The authors introduce DOUBLE-BENCH, a comprehensive evaluation system for document RAG that includes 3,276 documents, 5,168 queries across 6 languages and 4 document types. The system uses exhaustive evidence labeling and human verification to ensure quality and completeness. Evaluation involves two main components: retrieval (using embedding models) and generation (using MLLMs). Retrieval is assessed using Hit@k metrics, while generation is evaluated using LLM-as-a-judge (GPT-4o scoring 0-10). The paper evaluates existing models rather than training new ones, using a pipeline that parses documents, embeds pages, retrieves top-k pages, and generates answers via MLLMs.

## Key Results
- Text embedding models are narrowing the gap with visual models in retrieval performance
- Multimodal embedding models struggle with low-resource languages like Arabic
- RAG frameworks exhibit significant overconfidence, attempting to answer nearly every query regardless of evidence sufficiency
- The strongest model, colqwen2.5-3b, achieved an average hit@5 score of 0.795
- The benchmark successfully distinguishes context-grounded reasoning from inherent model knowledge

## Why This Works (Mechanism)
The DOUBLE-BENCH system works by providing a comprehensive, realistic evaluation framework that addresses the fragmentation and unrealistic nature of existing benchmarks. By including fine-grained assessment of each component within document RAG systems, exhaustive evidence labeling, and human verification, it ensures quality and completeness. The multilingual and multimodal approach captures the complexity of real-world document understanding tasks, while the separation of retrieval and generation evaluation allows for precise identification of system weaknesses.

## Foundational Learning
- **Multimodal document understanding**: Why needed - Documents contain text, tables, figures, and layouts that require different processing approaches. Quick check - Can the system handle mixed-content documents with tables and text?
- **Fine-grained component assessment**: Why needed - Isolating retrieval vs. generation performance identifies specific failure points. Quick check - Does the evaluation distinguish between retrieval failures and generation failures?
- **Human-verified evidence labeling**: Why needed - Ensures evaluation questions have verifiable ground truth. Quick check - Are all answers backed by explicit evidence in the documents?
- **Multilingual evaluation**: Why needed - Low-resource languages present unique challenges for embedding models. Quick check - Does performance degrade significantly in non-English languages?
- **Confidence-aware evaluation**: Why needed - RAG systems must recognize when they lack sufficient evidence. Quick check - Can the system refuse to answer when evidence is insufficient?

## Architecture Onboarding

**Component Map:** Document Parsing -> Embedding -> Retrieval -> Generation -> Evaluation

**Critical Path:** The most critical evaluation path is: (1) Document parsing with Docling/MinerU, (2) Embedding pages with specified models, (3) Retrieval of top-k pages for each query, (4) Answer generation via MLLMs, (5) LLM-as-a-judge evaluation.

**Design Tradeoffs:** The system trades computational cost (72,880 pages, 8×A100s) for comprehensive evaluation coverage. LLM-as-a-judge provides flexibility but introduces variability compared to human evaluation. The multilingual approach increases realism but complicates model comparison.

**Failure Signatures:** GPU memory exhaustion when processing large document corpora, LLM-as-a-judge instability leading to inconsistent scores, and overconfidence in RAG frameworks that answer without sufficient evidence.

**Three First Experiments:**
1. **Baseline retrieval evaluation**: Run the embedding and retrieval pipeline on a small subset (100 documents, 50 queries) to verify setup and compare against reported Hit@5 baselines.
2. **Generation pipeline test**: Execute the complete RAG pipeline on a single query-document pair to verify the generation and evaluation components work end-to-end.
3. **Cross-LLM judge validation**: Rerun evaluation with a different LLM judge to assess score stability and validate the ≥7 threshold for correct answers.

## Open Questions the Paper Calls Out
- How can document RAG frameworks be designed to balance answer accuracy with the ability to recognize and refuse to answer when evidence is insufficient ("epistemic humility")?
- Does increasing the number of reasoning hops in multi-hop queries reliably increase difficulty for MLLMs, given their observed "signature information + inclusion-based elimination" strategy?
- How can multimodal embedding models be improved to match or exceed text embedding models in low-resource languages where text-based approaches currently outperform them?
- To what extent do LLM-synthesized benchmarks fail to capture the diversity of human information-seeking behavior, and how does this bias affect RAG system evaluation?

## Limitations
- Missing generation hyperparameters (temperature, top_p, max_tokens) that could significantly impact evaluation outcomes
- LLM-as-a-judge evaluation introduces inherent variability without quantified statistical significance
- Ambiguity about whether all 72,880 pages were used in evaluation or if some were held out

## Confidence
- **High Confidence:** Benchmark design principles, dataset construction methodology, and retrieval evaluation metrics are clearly specified and reproducible.
- **Medium Confidence:** Overall framework for evaluating document RAG systems is sound, but specific numerical results depend on LLM-as-a-judge evaluation which has inherent variability.
- **Low Confidence:** Exact numerical values for end-to-end generation accuracy and comparison between different MLLM models cannot be precisely reproduced without missing generation hyperparameters.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Run the answer generation pipeline with multiple temperature and top_p settings (e.g., 0.0, 0.7, 1.0) to determine how sensitive the LLM-as-a-judge evaluation is to these parameters, and report the variance in final accuracy scores.

2. **Statistical Significance Testing:** Perform paired t-tests or bootstrap confidence intervals on the Hit@5 scores across different embedding models to establish whether observed differences are statistically significant rather than random variation.

3. **Cross-LLM Judge Validation:** Re-run a subset of the evaluation using a different LLM judge (e.g., Claude-3 or GPT-4-turbo) to assess the stability of the 0-10 scoring system and determine whether the threshold of ≥7 for correct answers is consistent across different judge models.