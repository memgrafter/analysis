---
ver: rpa2
title: 'PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric
  and Trustworthy Explanations for Daily Use Cases'
arxiv_id: '2512.17172'
source_url: https://arxiv.org/abs/2512.17172
tags:
- user
- recipe
- explanation
- explanations
- pilar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PILAR is a framework that leverages a pre-trained LLM to generate
  personalized, human-centric explanations for real-time AI-powered AR systems, addressing
  the limitations of traditional, fragmented XAI methods. It provides context-aware,
  adaptive reasoning tailored to user needs, covering key explainability dimensions
  (when, what, how, who, where) to foster trust and engagement.
---

# PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases

## Quick Facts
- arXiv ID: 2512.17172
- Source URL: https://arxiv.org/abs/2512.17172
- Authors: Ripan Kumar Kundu; Istiak Ahmed; Khaza Anuarul Hoque
- Reference count: 36
- One-line primary result: PILAR's LLM-based AR interface achieved 40% faster task completion and higher user satisfaction than a template-based interface in a recipe recommendation study.

## Executive Summary
PILAR is a framework that uses pre-trained LLMs to generate personalized, human-centric explanations for real-time AI-powered AR systems. It addresses the limitations of traditional fragmented XAI methods by providing unified, context-aware reasoning tailored to user needs across explainability dimensions (when, what, how, who, where). Implemented in a smartphone-based AR application for personalized recipe recommendations, PILAR integrates real-time object detection, recipe suggestions, and LLM-driven explanations. A user study with 16 participants showed the LLM-based interface significantly outperformed a traditional template-based interface, with participants completing tasks 40% faster and reporting higher satisfaction, ease of use, and perceived transparency.

## Method Summary
PILAR's method combines on-device YOLOv8-small for ingredient detection (trained on 36 kitchen ingredients), Edamam Recipe API for recommendations filtered by user dietary profiles, and GPT-4o mini for LLM-generated explanations via In-Context Learning. The system runs on Unity 2022.3 with OpenCV integration on a Samsung Galaxy S10. Detection outputs are combined with user profiles to query recipes, then sent to the LLM with ICL prompts to generate personalized explanations. A template-based baseline uses SHAP, PDP, DICE, and CEM for comparison. The method was evaluated through a within-subjects user study measuring task completion time, System Usability Scale (SUS), and HALIE framework metrics.

## Key Results
- Participants completed tasks 40% faster with PILAR's LLM-based interface compared to the template-based interface
- LLM-based interface received significantly higher scores on usability (SUS) and user experience (HALIE) metrics
- LLM explanations were perceived as more satisfying, easier to use, and more transparent than template-based explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Unified Natural Language Reasoning Reduces Cognitive Load**
- Mechanism: By replacing fragmented, multi-modal explanation techniques with a single LLM-generated response, PILAR lowers the cognitive overhead required to synthesize "why" a decision was made. The LLM acts as a post-hoc interpreter, integrating the "what" and "why" into a coherent narrative.
- Core assumption: Users prefer synthesizing a single paragraph of text over cross-referencing multiple distinct visualization types or rigid templates.
- Evidence anchors:
  - [abstract] "Participants completed tasks 40% faster... PILAR leverages pre-trained LLMs to provide unified, context-aware reasoning."
  - [section 6.2] "I found the traditional method confusing and I had to jump between explanations to piece everything together."
  - [corpus] The corpus supports the trend toward LLM-based personalization (e.g., "Personalizing Student-Agent Interactions"), but specific evidence comparing unified vs. fragmented XAI in the provided neighbor summaries is weak.
- Break condition: If the LLM generates verbose or hallucinatory reasoning that obscures the core logic, cognitive load would likely increase rather than decrease.

### Mechanism 2
- Claim: **In-Context Learning (ICL) Enables Dynamic Personalization**
- Mechanism: The system utilizes the LLM's in-context learning capabilities to adapt explanations to the specific user profile ("Who") without fine-tuning the underlying model. By injecting dietary preferences and health goals directly into the prompt, the explanation shifts from generic logic to user-specific reasoning.
- Core assumption: The pre-trained LLM has sufficient world knowledge to accurately map abstract constraints (e.g., "low-carb") to specific ingredient lists without additional training.
- Evidence anchors:
  - [abstract] "...context-aware, personalized explanations... dynamically adapts explanations to the user's needs."
  - [section 4.1] "ICL enables the LLM to adapt to new tasks... by leveraging a few task-specific examples within the prompt... tailored to the needs of AR users."
  - [corpus] "Enabling Personalized Long-term Interactions" supports the mechanism of combining user profiles with LLM context for personalization.
- Break condition: If user profiles exceed the context window of the model or contain contradictory information, the personalization logic may degrade or fail.

### Mechanism 3
- Claim: **Spatial Context Anchoring Improves Perceived Relevance**
- Mechanism: The framework grounds explanations in the immediate physical environment detected by the AR system ("Where"). By referencing detected objects (ingredients) in the explanation text, the system validates the AI's situational awareness, increasing user trust in the recommendation's relevance.
- Core assumption: The object detection pipeline provides accurate, low-latency input; if the detected ingredients are wrong, the "grounded" explanation will appear hallucinated or incorrect.
- Evidence anchors:
  - [abstract] "...generating explanations across all key explainability dimensions (when, what, how, who, where)."
  - [section 3.2] "Where (Contextual Awareness)... LLMs enhance this by... incorporating spatial context (referencing nearby objects...)."
  - [corpus] "Memory-Augmented AR Agents" supports the value of spatiotemporal reasoning in AR for task assistance.
- Break condition: If network latency delays the explanation significantly after the object detection, the illusion of real-time spatial awareness breaks.

## Foundational Learning

- Concept: **Post-hoc Explainability (XAI)**
  - Why needed here: PILAR treats the LLM as a "post-hoc explainer" (LLMEX) rather than an inherent part of the decision logic. Understanding this distinction is crucial: the recipe recommendation comes from the Edamam API, but the *explanation* of that recommendation is generated retroactively by the LLM.
  - Quick check question: If the Edamam API recommends a recipe due to an error, would the LLM explainer be able to identify the error, or would it fabricate a plausible justification?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The system relies on ICL (prompt engineering with examples) rather than model training to adapt to user preferences. Engineers must understand that the "intelligence" here is prompt-dependent, not weight-dependent.
  - Quick check question: Does modifying the prompt require retraining the GPT-4o mini model weights, or just changing the input text sent to the API?

- Concept: **Human-AI Language-based Interaction Evaluation (HALIE)**
  - Why needed here: The paper uses the HALIE framework to evaluate user experience. Understanding these metrics (fluency, helpfulness, responsiveness) is necessary to interpret the study results and replicate the evaluation.
  - Quick check question: Which HALIE metric showed *no* significant difference between the LLM and template conditions in the study? (Answer: Responsiveness).

## Architecture Onboarding

- Component map:
  - Smartphone Camera -> YOLOv8-small (On-device) -> Detects Ingredients
  - Ingredients + User Profile -> Edamam Recipe API -> Ranks recipes
  - Ingredients + Recipe + Profile -> LLM Engine (GPT-4o mini) -> Text Response
  - LLM Response -> Unity Canvas (AR overlay)

- Critical path:
  1. Object Detection (YOLO) must complete within ~50ms to maintain AR illusion
  2. Context Assembly (concatenating ingredients + profile)
  3. LLM API call (Latency reported up to 3000ms+ in Figure 3)
  4. Rendering response in AR

- Design tradeoffs:
  - **On-device vs. Cloud:** YOLO runs on-device for speed/privacy, while LLM explanation is cloud-dependent (GPT-4o mini), introducing network latency (seen in Figure 3 as "Time: 3467 ms")
  - **Hallucination Risk vs. Fluency:** Higher temperature (creativity) risks false dietary advice; PILAR sets temperature to 0.2 to prioritize factual consistency over creative prose

- Failure signatures:
  - **Missing Objects:** If YOLO misses an ingredient, the LLM explains the recipe based on incomplete data, confusing the user ("Why did it say I have spinach when I don't?")
  - **Latency Mismatch:** If the explanation arrives >5 seconds after the visual detection, users may assume the system stalled

- First 3 experiments:
  1. **Latency Profiling:** Measure the round-trip time for the LLM explanation specifically vs. the template engine to isolate the network bottleneck
  2. **Prompt Stress Testing:** Feed the LLM contradictory user profiles (e.g., "Vegan" but "High Protein via Beef") to verify how the ICL handles logical conflicts
  3. **Baseline Ablation:** Run the system using only the Template Engine (without LLM) with a new set of users to verify the replicability of the "40% faster" claim

## Open Questions the Paper Calls Out

- How do individual differences (demographics, AI literacy, prior AR experience) influence user engagement, trust, and explanation preferences when interacting with LLM-based explanations in AR systems?
- How does user trust in LLM-based AR explanations evolve with extended usage, particularly in sensitive domains such as health or fitness coaching?
- How effectively does the PILAR framework transfer to other AR platforms (e.g., Meta Quest Pro, Apple Vision Pro) with different computational capabilities, display modalities, and interaction paradigms?
- Can the PILAR framework be effectively applied to AR domains beyond recipe recommendations, such as furniture assembly assistance or fitness coaching?

## Limitations
- The 40% faster completion time comes from a small user study (n=16) without a pre-registered analysis plan, limiting generalizability
- The system's latency (up to 3 seconds for LLM explanations) could disrupt the real-time AR experience despite claims of seamlessness
- The claim of "trustworthy" explanations is based on user perception rather than objective accuracy verification of the LLM's reasoning

## Confidence
- **High Confidence:** The mechanism of using ICL for personalization and the basic architecture (YOLO + Edamam + LLM) are clearly specified and reproducible
- **Medium Confidence:** The 40% faster claim and HALIE scores are supported by the user study, but the small sample size and lack of statistical power analysis introduce uncertainty
- **Low Confidence:** The "trustworthy" label is primarily user-perceived; there's no evidence the LLM explanations are factually accurate or free from hallucination

## Next Checks
1. **Statistical Validation:** Conduct a larger user study (n=30+) with pre-registered analysis to verify the 40% speed improvement and HALIE score differences
2. **Latency Profiling:** Measure and publish the complete end-to-end pipeline latency (detection + reasoning + explanation) to assess AR experience disruption
3. **Explanation Accuracy Audit:** Have domain experts evaluate a sample of LLM explanations for factual correctness and hallucination to substantiate the "trustworthy" claim