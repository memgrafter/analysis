---
ver: rpa2
title: Saliency-driven Dynamic Token Pruning for Large Language Models
arxiv_id: '2504.04514'
source_url: https://arxiv.org/abs/2504.04514
tags:
- pruning
- token
- tokens
- sdtp
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of long-sequence
  inference in large language models (LLMs), particularly the quadratic complexity
  of the attention mechanism. The proposed Saliency-driven Dynamic Token Pruning (SDTP)
  framework dynamically prunes redundant tokens based on their saliency scores estimated
  by a lightweight prediction module inserted into different layers of the LLM.
---

# Saliency-driven Dynamic Token Pruning for Large Language Models

## Quick Facts
- arXiv ID: 2504.04514
- Source URL: https://arxiv.org/abs/2504.04514
- Reference count: 7
- Primary result: Up to 47.2% FLOPs reduction and 1.75× end-to-end speedup while maintaining performance

## Executive Summary
This paper addresses the computational challenges of long-sequence inference in large language models (LLMs), particularly the quadratic complexity of the attention mechanism. The proposed Saliency-driven Dynamic Token Pruning (SDTP) framework dynamically prunes redundant tokens based on their saliency scores estimated by a lightweight prediction module inserted into different layers of the LLM. A ranking-based optimization strategy minimizes the divergence between predicted and gradient-based saliency scores. Experimental results demonstrate that SDTP achieves significant computational savings while maintaining comparable performance on various tasks and models, including Llama2-7B, Mistral-7B, and BLOOM-7B.

## Method Summary
SDTP introduces a saliency-driven dynamic token pruning framework that reduces computational overhead in LLMs during long-sequence inference. The core innovation is a lightweight saliency prediction module that estimates token importance at each layer, allowing for the pruning of redundant tokens before they enter subsequent computation. The framework employs a ranking-based optimization strategy that minimizes the divergence between predicted saliency scores and gradient-based saliency scores, ensuring that the most informative tokens are preserved. SDTP is designed to be compatible with existing KV cache compression techniques, enabling further acceleration. The method has been evaluated across multiple model architectures including Llama2-7B, Mistral-7B, and BLOOM-7B, demonstrating substantial reductions in FLOPs, memory usage, and end-to-end inference time while maintaining model performance.

## Key Results
- Achieves up to 47.2% FLOPs reduction through dynamic token pruning
- Demonstrates 1.75× end-to-end speedup and 34.26% memory savings
- Maintains comparable performance across various tasks and models including Llama2-7B, Mistral-7B, and BLOOM-7B

## Why This Works (Mechanism)
The framework works by predicting token saliency at multiple layers, allowing the model to identify and prune redundant tokens before they enter computationally expensive operations. The lightweight prediction module estimates which tokens are most informative for the current context, while the ranking-based optimization ensures that the pruning decisions align with gradient-based importance measures. This dynamic approach adapts to the content of each sequence rather than applying static pruning ratios, preserving critical information while eliminating computational waste. The compatibility with KV cache compression allows for synergistic acceleration by reducing both the number of tokens processed and the memory footprint of cached attention states.

## Foundational Learning

**Self-attention mechanism** - Why needed: Understanding the quadratic complexity bottleneck that SDTP addresses. Quick check: Verify that attention complexity is O(n²) for sequence length n.

**Token saliency** - Why needed: Core concept for identifying which tokens can be pruned without performance loss. Quick check: Confirm that saliency scores correlate with gradient magnitudes or other importance measures.

**KV cache compression** - Why needed: SDTP's compatibility with existing acceleration methods. Quick check: Understand how cached key-value states contribute to memory usage during inference.

**Ranking-based optimization** - Why needed: The specific training strategy that aligns predicted saliency with ground truth importance. Quick check: Verify that the ranking loss function preserves relative token importance ordering.

**Dynamic vs static pruning** - Why needed: SDTP's adaptive approach versus fixed-ratio methods. Quick check: Confirm that token importance varies significantly across different sequences and contexts.

## Architecture Onboarding

**Component map:** Input tokens -> Saliency prediction module (inserted at multiple layers) -> Token pruning -> Remaining tokens -> Self-attention layers -> Output

**Critical path:** The saliency prediction module must execute quickly to avoid becoming a bottleneck, as it operates at every layer where pruning decisions are made.

**Design tradeoffs:** Accuracy vs speed (more aggressive pruning yields greater speedup but risks performance degradation), module complexity vs computational overhead (simpler prediction modules add less overhead but may be less accurate), layer placement (earlier insertion enables greater savings but may increase error accumulation).

**Failure signatures:** Performance degradation when important tokens are incorrectly pruned, minimal speedup when pruning decisions are too conservative, increased latency if the saliency prediction module becomes a bottleneck.

**First experiments to run:**
1. Ablation study removing the saliency prediction module to measure baseline performance impact
2. Sensitivity analysis varying pruning ratios to find the optimal balance between speedup and accuracy
3. Layer-wise analysis examining where pruning decisions have the most impact on overall performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on short-sequence datasets (average lengths 7-22 tokens) despite claiming applicability to long-sequence scenarios
- FLOPs reduction metrics are based on theoretical estimations rather than actual measured computational costs
- "SDTP + RWKV" experiment shows an unexpected 3.5% perplexity increase compared to baseline

## Confidence
- High confidence in compatibility with existing KV cache compression methods
- Medium confidence in the claimed 47.2% FLOPs reduction and 34.26% memory savings
- Medium confidence in overall effectiveness of the saliency prediction module
- Low confidence in generality of results across different LLM architectures and long-context scenarios

## Next Checks
1. Evaluate SDTP on long-sequence datasets (e.g., 2K+ tokens) to validate whether pruning effectiveness translates to practical long-context scenarios.

2. Conduct empirical measurements of actual inference time and memory usage on GPU/CPU hardware rather than relying on theoretical FLOPs calculations.

3. Test SDTP's compatibility with additional model compression techniques (e.g., quantization, pruning) across multiple model families beyond the current RWKV experiment.