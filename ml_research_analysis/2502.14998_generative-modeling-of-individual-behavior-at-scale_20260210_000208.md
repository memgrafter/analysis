---
ver: rpa2
title: Generative Modeling of Individual Behavior at Scale
arxiv_id: '2502.14998'
source_url: https://arxiv.org/abs/2502.14998
tags:
- player
- style
- players
- each
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a scalable, generative approach to modeling
  individual human behavior, focusing on behavioral stylometry and per-player generative
  modeling. The key idea is to frame the problem as a multi-task learning setting,
  where each task corresponds to modeling a specific player's behavior.
---

# Generative Modeling of Individual Behavior at Scale

## Quick Facts
- arXiv ID: 2502.14998
- Source URL: https://arxiv.org/abs/2502.14998
- Authors: Nabil Omi; Lucas Caccia; Anurag Sarkar; Jordan T. Ash; Siddhartha Sen
- Reference count: 27
- Key outcome: Achieves 94.4% chess and 86.7% Rocket League stylometry accuracy using per-player style vectors via Multi-Head Adapter Routing

## Executive Summary
This paper introduces a scalable approach to modeling individual human behavior through behavioral stylometry and generative modeling. The key innovation is framing behavioral modeling as a multi-task learning problem where each task corresponds to a specific player, using parameter-efficient fine-tuning with Multi-Head Adapter Routing (MHR). The method learns explicit style vectors for each player that are versatile - enabling few-shot learning for new players, supporting generative modeling of individual behavior, and allowing for style manipulation and synthesis. The approach demonstrates strong performance on chess (47,864 players) and Rocket League (2,000 players), and shows generality by applying it to image generation for 10,177 celebrities.

## Method Summary
The method uses Multi-Head Adapter Routing (MHR) to learn per-player style vectors through parameter-efficient fine-tuning. A base behavioral cloning model is first trained on all players, then MHR adapters are added to linear layers with a routing matrix Z that selects which adapter to use per player. Each player gets an explicit style vector that conditions the routing. The approach uses two-speed learning rates (higher for style vectors) and supports few-shot learning by adding new rows to Z while freezing adapters. For chess, this uses a Squeeze-and-Excitation ResNet; for Rocket League, a GPT-2 Transformer. Style vectors enable stylometry (identifying players from actions), generative modeling, and controllable style manipulation through interpolation.

## Key Results
- Achieves 94.4% top-1 stylometry accuracy on chess with 47,864 players
- Achieves 86.7% stylometry accuracy on Rocket League with 2,000 players
- Demonstrates few-shot learning capability, learning new player styles from just a few games
- Shows style vectors enable interpretable manipulation and interpolation between player behaviors

## Why This Works (Mechanism)
The approach works by treating each player as a separate task in a multi-task learning framework, where the routing mechanism learns to select appropriate adapters based on player identity. The parameter-efficient fine-tuning with LoRA adapters allows learning player-specific patterns without full model retraining, while the style vectors provide an explicit, interpretable representation of each player's behavioral characteristics. The two-speed learning rate schedule ensures style vectors adapt quickly while adapters learn more slowly, creating a hierarchical learning process that captures both general behavioral patterns and individual stylistic nuances.

## Foundational Learning
- **Multi-task learning framing**: Treating each player as a separate task enables sharing of general behavioral knowledge while learning individual styles - needed for scalability across many players; quick check: verify each player has distinct style vector
- **Parameter-efficient fine-tuning (PEFT)**: LoRA adapters reduce parameters needed per player from millions to thousands - needed for scalability; quick check: compare adapter parameter count to full model
- **Routing mechanisms**: Multi-head routing selects appropriate adapters based on style vectors - needed for dynamic player-specific behavior; quick check: visualize routing distribution across heads
- **Two-speed optimization**: Different learning rates for style vectors vs adapters - needed for proper hierarchical learning; quick check: monitor training curves for each component
- **Few-shot adaptation**: Adding new routing rows while freezing adapters - needed for handling new players; quick check: test with 1-5 training examples per new player

## Architecture Onboarding

Component map: Base model -> LoRA adapters -> Routing matrix Z -> Style vectors -> Output

Critical path: Input data → Base model (frozen) → MHR adapters (conditional on routing) → Style vector conditioning → Behavioral output

Design tradeoffs: Uses PEFT instead of full fine-tuning for parameter efficiency; routing mechanism adds complexity but enables dynamic adaptation; two-speed optimization balances stability and flexibility

Failure signatures: Stylometry accuracy at chance level suggests routing collapse or poor style vector learning; poor generative quality indicates base model inadequacy or adapter misconfiguration; slow few-shot learning suggests routing matrix initialization issues

First experiments:
1. Train base behavioral cloning model and verify it learns reasonable policies
2. Add single LoRA adapter and verify parameter efficiency vs full fine-tuning
3. Implement routing with 2-3 players and verify style vectors capture distinct behaviors

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks precise training hyperparameters (learning rates, schedules) making reproduction difficult
- Training duration unspecified for both base and MHR phases
- Scalability to millions of individuals remains theoretical without large-scale validation

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| Core methodology and stylometry accuracy | High |
| Style vector versatility and interpretability | Medium |
| Scalability to extreme scales (millions of players) | Low |

## Next Checks

1. **Reproduce stylometry accuracy with baseline variants**: Implement and compare against simpler multi-task approaches (separate fine-tuning per player, shared model with player IDs as input) to establish that the MHR architecture provides meaningful advantages over alternatives in both accuracy and parameter efficiency.

2. **Validate style vector interpretability**: Conduct systematic ablation studies on style vector dimensions to determine which dimensions correspond to meaningful behavioral attributes, and verify that interpolation between style vectors produces gradual, predictable changes in behavior rather than abrupt or non-functional transitions.

3. **Test few-shot learning generalization**: Systematically vary the number of training games available for new players (1, 5, 10, 20, 50+) and measure how quickly the routing-based few-shot approach converges compared to full fine-tuning, establishing the practical limits of the approach for real-world applications where player data may be limited.