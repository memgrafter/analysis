---
ver: rpa2
title: 'Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration
  and Understanding'
arxiv_id: '2511.14446'
source_url: https://arxiv.org/abs/2511.14446
tags:
- video
- tool
- arxiv
- phase
- retrieve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agentic Video Intelligence (AVI) addresses the challenge of complex
  video understanding by introducing a training-free framework that mirrors human
  cognitive processes through three-phase reasoning (Retrieve-Perceive-Review). The
  method builds a structured environment comprising an entity-centric knowledge base
  and multi-granularity tools, enabling interpretable video analysis without proprietary
  APIs or reinforcement learning training.
---

# Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding

## Quick Facts
- arXiv ID: 2511.14446
- Source URL: https://arxiv.org/abs/2511.14446
- Reference count: 40
- Primary result: Training-free three-phase framework achieving 61.4% accuracy on LVBench

## Executive Summary
Agentic Video Intelligence (AVI) introduces a training-free framework for long video understanding that mirrors human cognitive processes through a three-phase reasoning structure (Retrieve-Perceive-Review). The system builds a structured environment comprising an entity-centric knowledge base and multi-granularity tools, enabling interpretable video analysis without proprietary APIs or reinforcement learning training. By combining reasoning LLMs with lightweight base CV models and VLM, AVI achieves competitive performance on multiple benchmarks while offering superior interpretability and accessibility through open-source model ensembles.

## Method Summary
AVI is a training-free framework that processes long videos through three distinct reasoning phases: Retrieve (global context gathering from structured database), Perceive (focused local analysis using visual tools), and Review (evidence assessment and re-perception if needed). The system constructs a structured environment offline by segmenting videos into 5-second clips, generating captions with Qwen3-VL-8B, extracting entity graphs using Qwen3-32B, and building a vector database with Qwen3-Embedding-4B. Online inference uses the core LLM to orchestrate specialized tools including clip retrieval, object detection (Grounding-DINO), OCR (PaddleOCR), boundary detection (CLIP), and frame analysis (Qwen3-VL-8B), with strict phase enforcement limiting tool access per reasoning stage.

## Key Results
- Achieves 61.4% accuracy on LVBench benchmark (avg 68.4min videos)
- Scores 59.8% on VideoMME-Long benchmark (300 videos, 30min-1hr duration)
- Reaches 60.0% mIoU on Charades-STA temporal grounding task
- Demonstrates interpretability advantage through visible tool-call audit trails
- Competes with training-based methods using only open-source model ensembles

## Why This Works (Mechanism)

### Mechanism 1
The three-phase Retrieve-Perceive-Review structure reduces error propagation by enforcing separation between global context gathering and local visual verification. The framework constrains action space per phase, with Retrieve searching textual database to narrow temporal windows, Perceive running expensive visual tools only on those specific windows, and Review acting as gatekeeper for re-perception. Core assumption: LLM can reliably follow phase-switching rules and assess evidence sufficiency. Evidence: [abstract] "...three-phase reasoning (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis..." Break condition: If retrieval phase fails to identify correct time window, subsequent perceive phase has zero probability of success.

### Mechanism 2
The Entity-Centric Knowledge Graph enables complex reasoning about object persistence and interaction that linear captions cannot support. Instead of treating video as sequence of isolated clips, the system extracts entities and builds graph G=(N, R, t_edge), allowing queries to traverse relationships across distant timestamps without processing every frame. Core assumption: Lightweight VLM accurately identifies and links entities across clip boundaries. Evidence: [section 3.2.3] "The entity-centric graph addresses this limitation by capturing rich relational dynamics..." Break condition: High noise in entity extraction creates corrupted graph, leading retrieval phase to irrelevant segments.

### Mechanism 3
Open-source model ensembling shows task decomposition plus specialized tools can match monolithic proprietary models for video QA. The system routes specific sub-tasks to most efficient model: CLIP for boundary detection, Grounding-DINO for object detection, and high-capacity VLM (Qwen3-VL) only as fallback for complex visual reasoning. Core assumption: Reasoning LLM (Qwen3-32B) possesses sufficient agentic capability to select correct tool and interpret output without reinforcement learning training. Evidence: [abstract] "...combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs..." Break condition: Context window overflow in orchestration LLM due to excessive tool outputs or conversation history.

## Foundational Learning

- Concept: ReAct (Reasoning + Acting)
  - Why needed here: AVI framework is fundamentally ReAct loop variant. Understanding how LLMs interleave thoughts with actions is critical.
  - Quick check question: Can you explain the difference between a standard LLM response and a ReAct-style trajectory?

- Concept: Temporal Grounding
  - Why needed here: Core capability of AVI is localizing questions to specific time intervals [t_start, t_end]. Understanding IoU metrics is necessary to evaluate Perceive phase effectiveness.
  - Quick check question: If a model predicts a segment [10s, 20s] and ground truth is [12s, 22s], what is the temporal IoU?

- Concept: Knowledge Graphs (Nodes & Edges)
  - Why needed here: "Structured environment" relies on representing video content as graph of entities (nodes) and their interactions (edges).
  - Quick check question: How would you represent "A person picks up a cup" as a triplet in a knowledge graph?

## Architecture Onboarding

- Component map:
  Offline: Video -> Clip Segmenter -> Captioner (VLM) -> Entity Graph Builder -> Vector Database
  Online: User Query -> Agent Core (LLM) -> Retrieve Tools (Search DB) -> Perceive Tools (Visual Models) -> Review (Reflect) -> Final Answer

- Critical path: Most latency-sensitive path is Perceive Phase. If agent calls frame_analysis_tool (loads high-capacity VLM) on large time range, inference time spikes.

- Design tradeoffs: System trades offline compute (building DB/graph once) for online efficiency and interpretability. However, introduces latency via multiple LLM-tool round-trips compared to single-pass VLMs.

- Failure signatures: Based on [section 4.5], watch for:
  1. Hallucinated Tool Arguments: LLM invents timestamps not present in retrieval output
  2. Premature Answering: Agent attempts to answer in Retrieve phase without visual confirmation
  3. Looping: Agent cycles between Perceive and Review without gathering new evidence

- First 3 experiments:
  1. Sanity Check: Run AVI on short 30-second video with simple object query. Verify clip_retrieve -> object_detect path fires correctly.
  2. Ablation (Text vs. Visual): Force system to answer only using Retrieve phase (textual DB). Compare accuracy against full 3-phase run to measure value of visual tools.
  3. Stress Test: Input video with very similar-looking objects (e.g., crowd) to test if Entity Graph distinguishes individuals or creates noisy super-node.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can structured entity-centric knowledge base be updated dynamically during inference to support streaming video inputs or correct initial extraction errors?
- Basis in paper: [explicit] Conclusion states, "Future work will explore dynamic database updates during inference..."
- Why unresolved: Current framework relies on "one-time" offline construction phase (Section 3.2) where video database is built once per video. This static assumption prevents system from adapting to new information in streaming scenarios or efficiently correcting graph errors without re-processing.
- Evidence to resolve: Extension of framework applied to streaming video benchmarks (e.g., Ego4D) where graph updates as new frames arrive, showing performance retention compared to static offline baseline.

### Open Question 2
- Question: How can parallel execution strategies be integrated into sequential Retrieve-Perceive-Review loop to mitigate latency of "time-consuming" Perceive phase?
- Basis in paper: [explicit] Conclusion identifies "parallel execution strategies" as target for future work. [inferred] Figure 4(a) identifies Perceive phase as most time-consuming component due to raw frame processing.
- Why unresolved: Current agent runtime (Algorithm 1) executes actions sequentially (t â† t+1), which ensures state consistency but creates bottleneck when multiple visual tools could theoretically operate simultaneously on different segments.
- Evidence to resolve: Implementation where independent tools (e.g., object_detect_tool and text_extract_tool) run concurrently, demonstrating reduced inference time without statistically significant drop in accuracy on LVBench.

### Open Question 3
- Question: How can framework mitigate failure cascades caused by "wrong tool-calls" without relying on proprietary models?
- Basis in paper: [inferred] Figure 4(b) identifies "Wrong tool-call" as primary cause of failure (34.3% of cases), attributed to "agentic ability of the main LLM."
- Why unresolved: System relies heavily on reasoning capability of open-source core LLM (Qwen3-32B). Paper currently mitigates this only via "adjusting system prompt and context engineering," leaving fundamental planning brittleness unaddressed.
- Evidence to resolve: Introduction of lightweight "verifier" module that checks tool arguments against current state before execution, or comparative study using different open-source reasoning models to isolate impact of planner quality on wrong tool-call rate.

## Limitations
- Framework performance heavily depends on reasoning capability of orchestration LLM, creating vulnerability if agent cannot reliably follow phase rules
- Offline database construction is resource-intensive, requiring significant GPU time and storage for hour-long videos
- Evaluation focuses on four specific datasets; generalization to diverse video domains remains untested

## Confidence

- Phase-Based Reasoning Structure: High - Well-defined design with clear evidence each phase serves distinct function
- Interpretability Advantage: High - Structured action space provides clear audit trail over black-box VLMs
- Competitive Performance: Medium - Solid results for training-free method but relative to limited baselines on specific benchmarks
- Open-Source Accessibility: High - Explicitly avoids proprietary APIs using only open-source models

## Next Checks

1. Ablation Stress Test: Systematically disable each tool in perceive phase (object_detect, text_extract, boundary_detect) and measure accuracy drop on LVBench to quantify marginal value of each visual tool.

2. Phase Enforcement Audit: Log agent's trajectory on sample LVBench questions. Verify no Perceive-phase tools called during Retrieve phase and agent never attempts to answer before Review phase to confirm framework constraints are enforced.

3. Temporal Grounding Precision: On Charades-STA, measure not just mIoU but distribution of IoU scores. Calculate percentage of predictions achieving IoU > 0.7 to reveal if high scores come from few excellent predictions or many mediocre ones.