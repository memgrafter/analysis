---
ver: rpa2
title: Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning
arxiv_id: '2601.15761'
source_url: https://arxiv.org/abs/2601.15761
tags:
- policy
- sigent-sac
- learning
- arxiv
- real-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of sample-efficient real-world reinforcement
  learning from a single demonstration. The authors propose SigEnt-SAC, which modifies
  SAC with a sigmoid-bounded entropy term to prevent negative-entropy-driven optimization
  toward out-of-distribution actions, and a gated behavior cloning term to stabilize
  policy updates.
---

# Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning

## Quick Facts
- arXiv ID: 2601.15761
- Source URL: https://arxiv.org/abs/2601.15761
- Authors: Xiefeng Wu; Mingyu Hu; Shu Zhang
- Reference count: 4
- Primary result: SigEnt-SAC achieves 100% success rate faster than baselines in D4RL benchmarks and four real-world robotic tasks using only single-view grayscale images and sparse rewards

## Executive Summary
This paper addresses sample-efficient real-world reinforcement learning from a single expert demonstration by introducing SigEnt-SAC, which modifies SAC with sigmoid-bounded entropy regularization and gated behavior cloning. The method prevents negative-entropy-driven optimization toward out-of-distribution actions while providing expert guidance only when the policy deviates substantially. Experiments show the approach reaches 100% success rate faster than baselines in D4RL benchmarks and achieves 100% success on four real-world robotic tasks (manipulator, wheeled robot, quadruped, humanoid) while reducing average task completion steps by 40.9% compared to expert demonstration.

## Method Summary
SigEnt-SAC modifies SAC by replacing standard entropy regularization with sigmoid-bounded entropy that maps per-dimension surprisal through a sigmoid function to prevent unbounded negative entropy from dominating Q-updates. The method adds a gated behavior cloning term that activates only when the policy deviates beyond a threshold from expert actions, providing corrective gradients when needed without over-regularization. A conservative Q-regularization term penalizes Q-values on sampled out-of-distribution actions to reduce overestimation. The policy is trained to maximize Q-values plus sigmoid-bounded entropy while minimizing deviation from expert actions when gated, using two Q-networks with TD loss and CQL-style OOD regularization.

## Key Results
- Achieves 100% success rate on D4RL Adroit and Kitchen benchmarks faster than Cal-QL and SAC baselines
- Demonstrates 100% success on four real-world robotic tasks using only single-view grayscale images and sparse rewards
- Reduces average task completion steps by 40.9% compared to expert demonstration
- Shows stable convergence with lower Q-function oscillation and reduced OOD action ratio compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Sigmoid-Bounded Entropy Regularization
- Claim: Bounding entropy via sigmoid prevents negative-entropy-dominated Q-updates from pushing policy optimization toward out-of-distribution actions.
- Core assumption: The oscillation problem stems primarily from unbounded negative entropy, not from critic overestimation alone.
- Evidence: OOD action ratio is lower for SigEnt-SAC vs Cal-QL across training steps (Figure 2).

### Mechanism 2: Gated Behavior Cloning (GBC)
- Claim: Conditional expert supervision stabilizes learning when Q-estimates are unreliable, while allowing policy improvement beyond demonstration.
- Core assumption: Expert demonstration is approximately correct but not necessarily optimal; early-stage Q-oscillations cause transient policy drift that can be corrected without over-regularizing.
- Evidence: Robust performance across wide ranges of λ and ε in sensitivity analysis (Figure 7).

### Mechanism 3: Conservative Q-Regularization with OOD Action Sampling
- Claim: Explicitly penalizing Q-values on sampled OOD actions reduces overestimation and stabilizes offline-to-online transfer.
- Core assumption: OOD actions sampled from current policy are representative of problematic Q-overestimation regions.
- Evidence: SigEnt-SAC shows stable convergence vs baselines that exhibit performance drops (Figure 4).

## Foundational Learning

- **Soft Actor-Critic (SAC) entropy regularization**
  - Why needed: SigEnt-SAC modifies SAC's entropy term; understanding the baseline is essential to grasp what's being bounded and why.
  - Quick check: Can you explain why standard SAC adds α log π(a|s) to the reward, and what happens when entropy becomes negative for tanh-squashed policies?

- **Conservative Q-Learning (CQL) and offline RL distribution shift**
  - Why needed: SigEnt-SAC builds on conservative methods; understanding OOD action penalties clarifies the problem sigmoid-bounded entropy addresses.
  - Quick check: In offline RL, why does maximizing Q-values on actions outside the dataset lead to performance collapse?

- **Behavior cloning with limited demonstrations**
  - Why needed: GBC conditions BC on deviation; understanding standard BC failure modes (distribution shift, compounding error) motivates the gating design.
  - Quick check: What happens when you train a policy purely via BC on a single expert trajectory, and the agent encounters a state not in that trajectory?

## Architecture Onboarding

- **Component map**: State → Critic Networks (2x) → Q-values → Entropy Module → Sigmoid-Bounded Entropy → GBC Module → Gated BC Loss → Policy Network → Action
- **Critical path**: Sample batch from online buffer and expert trajectory → Compute sigmoid-bounded entropy for target Q-update → Update critics via TD loss + CQL regularization → Compute GBC gate → Update policy via unified objective → Soft update target networks
- **Design tradeoffs**: Training overhead increases (~3.13 ms vs 0.78-1.51 ms for baselines); λ too small → oscillatory, too large → over-regularization; ε too small → unconditional BC, too large → weak early guidance
- **Failure signatures**: Persistent Q-oscillation despite bounded entropy → check critic learning rate; Policy fails to exceed expert → λ may be too high; Slow convergence → gate too restrictive
- **First 3 experiments**:
  1. Entropy ablation: Replace H_sig with standard SAC entropy on door-binary-v0; compare Q-oscillation amplitude and OOD action ratio
  2. GBC threshold sweep: Sweep ε ∈ {0.1, 0.3, 0.8} and λ ∈ {1, 5, 10} on sparse-reward task; plot success rate vs steps
  3. Demonstration robustness test: Train with degraded demonstrations (50% drop, action noise σ=0.2) on door-binary-v0 and hammer-binary-v0; compare vs Cal-QL

## Open Questions the Paper Calls Out
- Can SigEnt-SAC scale to full-body, joint-level control for complex dexterous manipulation, beyond the high-level velocity command interface currently demonstrated?
- How does SigEnt-SAC perform on long-horizon tasks with extended temporal dependencies and multi-stage objectives?
- Does the availability of multiple demonstrations provide additive or diminishing returns for SigEnt-SAC's learning efficiency?
- How robust is SigEnt-SAC to reward function misspecification compared to standard SAC and conservative Q-learning baselines?

## Limitations
- Control interface restricted to high-level velocity commands; not demonstrated for full-body, joint-level control
- Primarily validated on short-horizon, coarse tasks; not tested on finer-grained manipulation or long-horizon control problems
- Real-world success demonstrated with single demonstration per task without statistical analysis of variability

## Confidence
- **High**: Sigmoid-bounded entropy reduces OOD actions and stabilizes Q-learning (Figure 2, quantitative OOD ratio comparison)
- **Medium**: Gated behavior cloning improves sample efficiency over standard BC (Figure 6, but no demonstration robustness tests)
- **Medium**: Real-world performance gains over baselines (success rates and time reduction, but single demonstration per task)

## Next Checks
1. **Entropy Ablation**: Replace sigmoid-bounded entropy with standard SAC entropy on D4RL door-binary-v0; measure Q-oscillation amplitude and OOD action ratio over training.
2. **GBC Robustness**: Train with degraded demonstrations (50% drop, action noise σ=0.2) on hammer-binary-v0 and door-binary-v0; compare SigEnt-SAC vs Cal-QL success rates and convergence speed.
3. **Component Isolation**: Disable GBC (λ=0) and CQL (λ_ood=0) separately on Kitchen-complete-v0; quantify contribution of each to final success rate and sample efficiency.