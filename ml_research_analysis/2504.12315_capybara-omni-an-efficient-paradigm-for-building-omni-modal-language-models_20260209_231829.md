---
ver: rpa2
title: 'Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models'
arxiv_id: '2504.12315'
source_url: https://arxiv.org/abs/2504.12315
tags:
- arxiv
- zhang
- data
- wang
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Capybara-OMNI, a lightweight and efficient
  approach for building omni-modal language models that support text, image, video,
  and audio understanding. The authors propose a progressive training strategy that
  builds upon a vision-language model by sequentially adding audio capabilities and
  then enhancing instruction-following skills.
---

# Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models

## Quick Facts
- arXiv ID: 2504.12315
- Source URL: https://arxiv.org/abs/2504.12315
- Authors: Xingguang Ji; Jiakang Wang; Hongzhi Zhang; Jingyuan Zhang; Haonan Zhou; Chenxi Sun; Yahui Liu; Qi Wang; Fuzheng Zhang
- Reference count: 26
- Primary result: Capybara-OMNI achieves competitive performance across multiple benchmarks while using significantly less training data than comparable models

## Executive Summary
Capybara-OMNI introduces an efficient paradigm for building omni-modal language models that support text, image, video, and audio understanding. The approach employs a progressive training strategy that builds upon a vision-language foundation model by sequentially adding audio capabilities and enhancing instruction-following skills. By leveraging pre-trained components and focusing on data quality improvements through filtering and clustering, the model achieves competitive performance across multiple benchmarks while significantly reducing the training data requirements compared to other omni-modal approaches.

## Method Summary
The paper proposes a progressive training strategy that starts with a vision-language model and sequentially adds audio capabilities and instruction-following enhancements. The model uses a SigLIP visual encoder, Qwen2 LLM, and initializes the audio encoder from Qwen2-Audio to prevent catastrophic forgetting. Training data quality is improved through filtering and clustering techniques, while cross-modal instruction tuning employs video-based dialogue data to enhance human interaction capabilities. This approach aims to build a lightweight, efficient omni-modal model that can understand and generate responses across text, image, video, and audio modalities.

## Key Results
- Achieves competitive performance on HallusionBench (50.6), MMVET (60.5), MMBench (81.3), MathVista (68.3), and OCRBench (84.4)
- Uses significantly less training data than comparable omni-modal models
- Ablation studies confirm the effectiveness of audio encoder initialization and data quality improvements
- Demonstrates strong cross-modal instruction following capabilities through video-based dialogue training

## Why This Works (Mechanism)
The progressive training strategy allows the model to build upon pre-trained components while avoiding catastrophic forgetting. By initializing the audio encoder from Qwen2-Audio and using high-quality filtered data, the model can efficiently learn audio understanding without extensive retraining. The video-based dialogue data for cross-modal instruction tuning provides realistic human interaction scenarios that enhance the model's ability to follow complex multi-modal instructions.

## Foundational Learning
- Progressive training: why needed - to build omni-modal capabilities without catastrophic forgetting; quick check - measure performance degradation when adding new modalities
- Data filtering and clustering: why needed - to improve training data quality and efficiency; quick check - compare model performance on filtered vs unfiltered datasets
- Cross-modal instruction tuning: why needed - to enhance the model's ability to follow complex multi-modal instructions; quick check - evaluate performance on multi-step instruction following tasks

## Architecture Onboarding
- Component map: SigLIP visual encoder -> Qwen2 LLM -> Qwen2-Audio initialized audio encoder -> cross-modal instruction tuning module
- Critical path: Vision-language foundation -> Audio capability addition -> Instruction following enhancement
- Design tradeoffs: Progressive training vs. end-to-end training (efficiency vs. potential integration issues)
- Failure signatures: Catastrophic forgetting when adding new modalities, poor cross-modal understanding without quality training data
- First experiments: 1) Ablation study on audio encoder initialization, 2) Comparison of filtered vs unfiltered training data performance, 3) Evaluation of progressive vs end-to-end training approaches

## Open Questions the Paper Calls Out
None

## Limitations
- The specific contribution of progressive training to audio understanding improvements is difficult to isolate from audio encoder initialization effects
- Claims of data efficiency lack specific quantitative comparisons with comparable models
- Progressive training strategy may introduce biases from sequential modality addition
- Video-based dialogue data may not fully represent real-world human interaction scenarios

## Confidence
- High confidence in overall framework and benchmark results
- Medium confidence in specific contribution of progressive training to audio understanding improvements
- Low confidence in data efficiency claims without direct comparisons

## Next Checks
1) Conduct controlled ablation studies that separately measure the impact of progressive training versus audio encoder initialization on audio understanding performance
2) Provide detailed quantitative comparisons showing exact training data usage differences between Capybara-OMNI and comparable models
3) Test the model's performance on out-of-distribution audio tasks to validate generalization beyond the filtered and clustered training data