---
ver: rpa2
title: 'Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation'
arxiv_id: '2507.15396'
source_url: https://arxiv.org/abs/2507.15396
tags:
- speech
- hearing
- neuro-msbg
- loss
- msbg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency and lack of
  real-time capabilities in existing hearing loss simulation models like MSBG, which
  hinder their integration into modern speech processing pipelines. To overcome these
  limitations, the authors propose Neuro-MSBG, a lightweight end-to-end neural model
  that incorporates a personalized audiogram encoder and jointly models magnitude
  and phase spectra.
---

# Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation

## Quick Facts
- arXiv ID: 2507.15396
- Source URL: https://arxiv.org/abs/2507.15396
- Reference count: 30
- Primary result: 46x faster hearing loss simulation (0.021s for 1s audio) with 0.9247 STOI and 0.8671 PESQ correlation to MSBG

## Executive Summary
This paper introduces Neuro-MSBG, a lightweight neural model that replaces the computationally intensive MSBG hearing loss simulation with a 46x speedup while maintaining high fidelity (STOI SRCC 0.9247, PESQ SRCC 0.8671). The model uses a personalized audiogram encoder and jointly models magnitude and phase spectra, enabling parallel inference and end-to-end integration into speech processing pipelines. When integrated into a speech compensator, Neuro-MSBG improves HASPI scores from 0.428 to 0.616, demonstrating practical utility for personalized hearing aid systems.

## Method Summary
Neuro-MSBG processes audio through STFT/iSTFT to obtain magnitude and phase spectra, then uses an audiogram encoder (Conv1D + AvgPool + linear projection to 201 frequency bins) to condition a dual-path neural network (Mamba/Transformer/LSTM/CNN variants). The network outputs magnitude masks and phase predictions, trained with multi-objective losses including magnitude reconstruction, phase anti-wrapping, and time-domain consistency. The model processes 1-second audio in 0.021 seconds using parallel inference, achieving end-to-end hearing loss simulation that can be integrated with speech enhancement systems.

## Key Results
- 46x speedup: 0.021s inference time vs original MSBG for 1s audio
- High fidelity: STOI SRCC 0.9247, PESQ SRCC 0.8671 compared to MSBG
- Practical integration: HASPI score improvement from 0.428 to 0.616 in speech compensator pipeline
- Model size: ~1.45M parameters (Mamba variant) trained on 200 epochs

## Why This Works (Mechanism)
The model works by learning a direct mapping from clean audio and audiogram profiles to hearing-loss-simulated audio, bypassing the iterative filter calculations in MSBG. The audiogram encoder provides personalized hearing characteristics that condition the neural network, while joint magnitude and phase modeling captures both spectral shaping and temporal distortions of hearing loss. Parallel inference eliminates the sequential dependencies in MSBG's filter bank processing, enabling GPU acceleration and real-time performance.

## Foundational Learning
- **Audiogram encoding**: Converts 8-point hearing thresholds into frequency-domain features aligned with STFT bins - needed to condition the model on individual hearing profiles
  * Quick check: Verify encoded features broadcast correctly to match time-frequency dimensions
- **Phase prediction with anti-wrapping**: Uses inverse phase wrapping function to stabilize gradient flow when predicting phase angles beyond ±π - needed because standard phase prediction suffers from discontinuities
  * Quick check: Monitor phase loss components separately during training
- **Multi-objective loss balancing**: Combines magnitude reconstruction, phase modeling, and time-domain consistency - needed to ensure both spectral and temporal aspects of hearing loss are captured
  * Quick check: Verify loss weight combinations don't cause any single component to dominate training
- **Impulse-based delay compensation**: Aligns MSBG outputs by matching impulse responses to compensate for group delay variations - needed because MSBG's filters introduce frequency-dependent delays
  * Quick check: Confirm time alignment before computing STOI/PESQ correlations

## Architecture Onboarding

Component map: Clean audio + Audiogram → Audiogram Encoder → NN Block → Magnitude Mask + Phase → iSTFT → Simulated audio

Critical path: The dual-path neural network that processes magnitude and phase spectra jointly, with the audiogram encoder providing personalized conditioning. The anti-wrapping phase loss is critical for stable training, and the impulse-based delay compensation must be correctly applied to MSBG outputs before metric computation.

Design tradeoffs: The paper chose Mamba blocks for efficiency over Transformers, prioritized parallel inference over potentially higher accuracy from sequential processing, and used a relatively simple audiogram encoder rather than complex physiological modeling. The multi-objective loss trades training complexity for better preservation of both spectral and temporal characteristics.

Failure signatures: Poor STOI correlation indicates incorrect MSBG delay compensation or misalignment; phase prediction divergence suggests anti-wrapping implementation issues or inappropriate loss weights; slow convergence may indicate audiogram encoder frequency mapping problems or insufficient model capacity.

First experiments: 1) Generate and time-align MSBG training data with random audiograms, 2) Implement and verify audiogram encoder produces F=201 features, 3) Train magnitude-only baseline to establish performance floor before adding phase modeling.

## Open Questions the Paper Calls Out
- **Joint optimization with speech compensator**: Whether end-to-end fine-tuning of Neuro-MSBG with a speech compensator improves personalized hearing enhancement compared to using a frozen simulator. The authors kept Neuro-MSBG frozen during the compensator experiment to "initially verify the feasibility," though they identify joint optimization as a research direction. A comparison of HASPI scores between frozen and jointly fine-tuned approaches would resolve this.
- **Generalization to diverse acoustic environments**: The model was trained and tested exclusively on VoiceBank-DEMAND dataset with specific noise types and SNR levels. Real-world hearing aid deployment involves dynamic acoustic scenes and reverberation profiles not represented by the 10 DEMAND noise types. Evaluation on unseen, "wild" noise datasets or highly reverberant room simulations would test robustness.
- **Approximation of physiological models**: The authors contrast their work with physiological models like CoNNear but only demonstrate emulation of the engineering-based MSBG model. Physiological models capture non-linear cochlear mechanics that may not map linearly to the time-frequency masking and phase prediction strategies used here. Training on physiological model outputs and comparing inference speed versus biological accuracy would test this capability.

## Limitations
- Missing hyperparameter specifications for STFT parameters, loss weights, and Mamba configuration block full reproducibility
- Evaluation focuses on correlation to MSBG outputs rather than absolute perceptual validation with hearing-impaired listeners
- Dataset expansion through random audiogram sampling introduces variability that may affect reproducibility

## Confidence

High Confidence: 46x speedup claim and parallel inference capability are well-supported by architectural description and computational analysis. HASPI improvement from 0.428 to 0.616 demonstrates clear practical utility.

Medium Confidence: STOI SRCC of 0.9247 and PESQ SRCC of 0.8671 are credible given multi-objective training approach, but depend critically on correct MSBG reference model implementation and delay compensation. Architectural innovations are sound but relative contributions require ablation studies.

Low Confidence: Absolute performance claims relative to hearing-impaired listeners cannot be verified without perceptual studies, and generalization to audiograms outside training distribution remains untested.

## Next Checks
1. **Metric Correlation Validation**: Run Neuro-MSBG and MSBG on 100 randomly selected test utterances with held-out audiograms, compute STOI and PESQ correlations, and verify claimed SRCC values (0.9247 and 0.8671) fall within 95% confidence intervals.

2. **Speed Benchmark Replication**: Measure inference time on identical hardware (NVIDIA RTX 3090) for 1-second audio segments, confirming 0.021s processing time and 46x speedup relative to MSBG under identical conditions.

3. **Ablation Study**: Train and evaluate variants without audiogram encoder, without phase modeling, and with simplified losses to quantify individual contributions to final performance, particularly verifying audiogram encoder and phase components provide statistically significant improvements over magnitude-only baselines.