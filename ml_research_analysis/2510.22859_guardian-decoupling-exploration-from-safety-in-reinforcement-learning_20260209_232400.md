---
ver: rpa2
title: 'Guardian: Decoupling Exploration from Safety in Reinforcement Learning'
arxiv_id: '2510.22859'
source_url: https://arxiv.org/abs/2510.22859
tags:
- learning
- safe
- safety
- offline
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the instability in hybrid offline-online reinforcement
  learning caused by distribution shifts between offline and online data. It proposes
  RLPD-GX, which decouples exploration (via a reward-seeking learner) from safety
  enforcement (via a projection-based guardian that ensures safe execution and guarded
  value backups).
---

# Guardian: Decoupling Exploration from Safety in Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.22859
- Source URL: https://arxiv.org/abs/2510.22859
- Reference count: 37
- Primary result: RLPD-GX achieves 3.02 normalized mean score on Atari-100k, outperforming offline (2.39), online (1.27), and hybrid (2.07) baselines by up to 45%

## Executive Summary
This paper addresses instability in hybrid offline-online reinforcement learning caused by distribution shifts between offline and online data. The authors propose RLPD-GX, which decouples exploration (via a reward-seeking learner) from safety enforcement (via a projection-based guardian that ensures safe execution and guarded value backups). Dynamic temporal and symmetric sampling curricula are introduced to stabilize training. The guarded Bellman operator is proven to be a contraction, ensuring convergence. On the Atari-100k benchmark, RLPD-GX achieves state-of-the-art performance with superior safety and stability.

## Method Summary
RLPD-GX introduces a dual-agent architecture where a learner focuses purely on maximizing rewards while a guardian ensures safety through projection mechanisms. The guardian enforces safe execution by projecting Q-values to maintain consistency with offline data distributions and prevents unsafe value backups. Dynamic temporal and symmetric sampling curricula are used to gradually introduce online data during training. The guarded Bellman operator is designed to be a contraction mapping, providing theoretical convergence guarantees. This decoupling approach addresses the fundamental instability issues in hybrid RL caused by conflicting objectives between exploration and safety enforcement.

## Key Results
- Achieves 3.02 normalized mean score on Atari-100k benchmark
- Outperforms offline (2.39), online (1.27), and hybrid (2.07) baselines by up to 45%
- Demonstrates superior stability across ablation studies
- Proves convergence through contraction properties of guarded Bellman operator

## Why This Works (Mechanism)
The decoupling of exploration from safety enforcement addresses the fundamental tension in hybrid RL between maximizing rewards and maintaining safe behavior. By separating these objectives into distinct components, the learner can explore aggressively without compromising safety, while the guardian ensures all updates remain consistent with safe behavior patterns learned from offline data. The projection-based safety mechanism prevents distribution shift by constraining Q-value updates to remain within the support of the offline dataset. Dynamic curricula gradually bridge the gap between offline and online data distributions, allowing the agent to adapt safely to new experiences while maintaining stability.

## Foundational Learning
- **Distribution shift in RL**: Occurs when online policy deviates from offline data distribution, causing instability. Critical for understanding hybrid RL challenges.
  - Why needed: Explains the core problem RLPD-GX addresses
  - Quick check: Compare action distributions between offline and online policies

- **Contraction mapping theory**: Mathematical framework proving that iterative updates converge to a fixed point under certain conditions.
  - Why needed: Provides theoretical foundation for convergence guarantees
  - Quick check: Verify that operator norm is less than 1

- **Projection-based safety**: Technique that constrains values or policies to remain within safe regions defined by offline data.
  - Why needed: Enables safe execution while allowing exploration
  - Quick check: Measure projection frequency and magnitude during training

- **Temporal curricula in RL**: Gradual introduction of training complexity over time, often used to stabilize learning.
  - Why needed: Bridges gap between simple offline data and complex online experiences
  - Quick check: Track curriculum progression and its impact on stability

- **Symmetric sampling**: Balanced selection of experiences from different data sources or time periods.
  - Why needed: Prevents bias toward any particular data distribution
  - Quick check: Monitor sampling ratios and their effects on learning

## Architecture Onboarding

Component map: Environment -> Guardian -> Projection Layer -> Q-network Learner -> Action Selector -> Environment

Critical path: The guardian receives observations from the environment, projects Q-values to ensure safety, and passes them to the learner for value backup. The learner updates its policy based on these guarded values, selects actions, and the cycle repeats. The guardian's projection mechanism is the critical safety enforcement point.

Design tradeoffs: The decoupling approach trades computational overhead (two separate networks) for improved stability and safety. The projection mechanism may occasionally limit exploration if too conservative. The temporal curriculum introduces hyperparameters that require tuning. The symmetric sampling approach balances data utilization but may slow convergence if not properly calibrated.

Failure signatures: Excessive projection indicates the learner is exploring too far from safe regions. High variance in Q-values suggests the curriculum is progressing too quickly. If the learner consistently proposes actions rejected by the guardian, the safety threshold may be too strict. Failure to improve over offline-only performance indicates the online exploration component is ineffective.

Three first experiments:
1. Visualize Q-value distributions before and after guardian projection to understand safety enforcement
2. Measure the frequency and magnitude of guardian interventions during training
3. Compare learning curves with different temporal curriculum progression rates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Atari-100k benchmark, raising questions about generalizability to continuous control domains
- Symmetric sampling curriculum effectiveness not fully quantified through extensive ablation studies
- Theoretical convergence proof assumes idealized conditions that may not hold with function approximation errors
- Safety guarantees demonstrated through stability metrics rather than explicit constraint satisfaction in high-risk scenarios

## Confidence
- Generalizability to other domains: Medium
- 45% improvement claim: High for Atari-100k, Medium for broader applicability
- Theoretical convergence proof: Medium (assumes idealized conditions)
- Safety enforcement effectiveness: Medium (based on stability metrics)

## Next Checks
1. Test RLPD-GX on continuous control benchmarks (e.g., DM Control Suite) to evaluate scalability beyond discrete action spaces
2. Conduct controlled experiments varying the proportion of offline vs. online data to quantify robustness across different data regime combinations
3. Implement and evaluate the algorithm in a safety-critical simulated environment with explicit constraint violations to assess practical safety enforcement beyond stability metrics