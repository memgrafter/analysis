---
ver: rpa2
title: A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical
  Patterns in Temporal Knowledge Graphs
arxiv_id: '2506.14235'
source_url: https://arxiv.org/abs/2506.14235
tags:
- structural
- information
- events
- historical
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting future events in
  temporal knowledge graphs by integrating both structural and semantic information.
  Previous methods focused on either graph structure or semantic reasoning, neglecting
  the complementary benefits of combining both.
---

# A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs

## Quick Facts
- arXiv ID: 2506.14235
- Source URL: https://arxiv.org/abs/2506.14235
- Reference count: 35
- Key result: Achieves 44.36% MRR, 49.81% H@3, and 64.21% H@10 on ICEWS14 by fusing structural and semantic information with event-aware experts

## Executive Summary
This paper addresses the challenge of predicting future events in temporal knowledge graphs by integrating both structural and semantic information. Previous methods focused on either graph structure or semantic reasoning, neglecting the complementary benefits of combining both. To overcome this, the authors propose the Multi-Expert Structural-Semantic Hybrid (MESH) framework, which employs three expert modules to adaptively fuse structural and semantic information based on event types (historical vs. non-historical). The framework uses a GCN-based encoder for structural information and an LLM-based encoder for semantic information, with event-aware experts and a prediction expert to handle different event patterns. Experiments on three public datasets (ICEWS14, ICEWS18, ICEWS05-15) demonstrate that MESH outperforms state-of-the-art methods.

## Method Summary
MESH integrates structural and semantic information for temporal knowledge graph reasoning through a multi-expert architecture. The framework employs a RE-GCN encoder for structural patterns and an LLM (LLaMA-2-7B) encoder for semantic context, producing two query representations that are fused through learned gates. Two event-aware experts specialize in historical and non-historical events respectively, with a prediction expert performing soft weight allocation across all experts. The model is trained with auxiliary losses encouraging expert specialization and uses a ConvTransE decoder for final prediction. The historical indicator is based on event frequency in training data.

## Key Results
- MESH achieves 44.36% MRR, 49.81% H@3, and 64.21% H@10 on ICEWS14, outperforming state-of-the-art methods
- Structural information ablation causes -4.59% MRR drop, semantic information ablation causes -2.47% MRR drop
- Expert specialization shows historical events favor semantic context while non-historical events favor structural patterns
- Prediction expert soft gating improves performance by 0.92% MRR over explicit classification

## Why This Works (Mechanism)

### Mechanism 1
Integrating structural and semantic encodings provides complementary reasoning signals for temporal knowledge graph prediction. A GCN-based structural encoder captures entity interaction patterns and temporal dependencies, while an LLM-based semantic encoder extracts background and relational context from entity/relation names via prompting. The two query representations (qg, ql) are fused through learned gates. Core assumption: Structural patterns and semantic attributes provide non-redundant, task-relevant information that jointly improves prediction over either alone.

### Mechanism 2
Historical and non-historical events exhibit different optimal information allocation patterns, and specialized expert modules can capture this. M event-aware experts for historical events and N experts for non-historical events learn distinct gating weights (αi) for blending qg and ql. Historical events may favor semantic context; non-historical events may favor structural evolution patterns. Auxiliary losses enforce specialization. Core assumption: The historical/non-historical distinction is task-relevant, and the frequency-based indicator I captures sufficient signal for supervised specialization during training.

### Mechanism 3
A prediction expert with soft weight allocation outperforms explicit binary event-type classification for final prediction. Instead of a hard classifier to select historical vs. non-historical branch, the prediction expert computes adaptive weights α over all event-aware experts, producing a blended final query q for prediction. Core assumption: The gating function can implicitly encode event-type signals from structural query qg without explicit supervision, reducing error propagation from misclassification.

## Foundational Learning

- **Concept: Temporal Knowledge Graph (TKG)**
  - Why needed here: The input data structure; facts are quadruples (s, r, o, t) with timestamps. Understanding the task as predicting missing objects in future queries is essential.
  - Quick check question: Given history [(France, Accuse, Iran, t=100)], what should the model predict for (France, Demand, ?, t=200)?

- **Concept: Mixture of Experts (MoE) with Gating**
  - Why needed here: MESH implements a soft MoE where gates compute scalar weights for blending structural vs. semantic representations. Understanding α as a learned mixing coefficient is critical.
  - Quick check question: If α_i = 0.7, does the expert rely more on qg or ql?

- **Concept: Historical vs. Non-Historical Events**
  - Why needed here: The paper's core claim hinges on this distinction. Historical events have prior occurrences (F > 0); non-historical do not. The auxiliary loss uses I to route supervision.
  - Quick check question: For query (s, r, ?, t), how is I computed if (s, r, o, k) exists for some k < t?

## Architecture Onboarding

- **Component map:**
  Feature Encoder: RE-GCN (structural) + LLaMA-2-7B with MLP adapters (semantic)
  Decoders: Two ConvTransE decoders produce qg and ql
  Event-Aware Experts: M=1 historical expert, N=1 non-historical expert, each with learned gate α_i
  Prediction Expert: Single gating layer producing α ∈ R^(M+N), outputs final q
  Loss: L = L_m + ω(L_his^e + L_nhis^e), with ω=1 optimal

- **Critical path:**
  1. Freeze structural encoder after 500 epochs
  2. Extract entity/relation embeddings from GCN and LLM
  3. Compute qg, ql via ConvTransE
  4. Gate computes α_i per expert → q_his, q_nhis
  5. Prediction expert computes α → final q
  6. Final prediction via σ(q · H^g)

- **Design tradeoffs:**
  - Encoder choice: RE-GCN vs. TiRGN (TiRGN slightly better); LLaMA-2-7B vs. Stella-1.5B (comparable). Paper shows MESH is encoder-agnostic.
  - Expert count: (M=1, N=1) optimal; more experts cause overfitting.
  - Gate input: Structural qg preferred over semantic or concatenated.

- **Failure signatures:**
  - Removing semantic info: -2.47% MRR
  - Removing structural info: -4.59% MRR (severe)
  - Removing event-aware loss: -0.40% MRR
  - Removing prediction expert: -0.92% MRR
  - If H@10 high but H@3 low, check if semantic branch is under-weighted for historical events.

- **First 3 experiments:**
  1. Baseline replication: Run RE-GCN alone on ICEWS14 to reproduce ~41.89% MRR; verify preprocessing and timestamp handling.
  2. Ablation on encoder: Replace LLaMA-2-7B with a smaller PLM (e.g., BERT-base) to measure semantic encoder contribution and latency tradeoff.
  3. Expert count sweep: Test (M,N) ∈ {(1,1), (2,1), (1,2)} to confirm (1,1) is optimal for your data; if not, inspect α distributions for collapse.

## Open Questions the Paper Calls Out
- How does performance change when utilizing lightweight semantic encoders (e.g., BERT-Small) versus larger models (e.g., GPT-4), and is there a threshold where semantic capacity outweighs structural stability?
- Does freezing the structural encoder during the fusion phase prevent the model from correcting structural embeddings based on semantic context?
- Is the binary classification of events as "historical" vs. "non-historical" sufficient compared to a continuous weighting based on event frequency or recency?

## Limitations
- Performance relies heavily on effectiveness of underlying structural and semantic encoders, which may not generalize to all domains
- Computational cost is high due to LLM-based semantic encoding, limiting scalability
- The binary historical/non-historical distinction may oversimplify complex event patterns and ignore temporal decay or intensity trends

## Confidence

- **Structural-Semantic Integration Improves Prediction:** High confidence. Supported by ablation (-2.47% MRR when semantic removed, -4.59% when structural removed) and complementary failure modes.
- **Historical/Non-Historical Expert Specialization is Effective:** Medium confidence. Supported by performance balance but limited by lack of direct expert ablation and unknown overlap in optimal α distributions.
- **Prediction Expert Soft Gating Outperforms Explicit Classification:** Low confidence. Supported only by -0.92% MRR ablation and theoretical argument; no comparison to hard classification baseline.

## Next Checks

1. **Encoder Complementarity Analysis:** Compute correlation matrices between structural qg and semantic ql representations on held-out data. If correlation >0.8 for most queries, the dual-encoding benefit may be marginal.

2. **Expert Specialization Validation:** Visualize α distributions for historical vs. non-historical test events. If α_i values are similar across event types or near 0.5, expert specialization is not functioning as intended.

3. **Event-Type Classification Baseline:** Implement a simple classifier (e.g., logistic regression on qg) to predict historical vs. non-historical events. Compare its accuracy to the prediction expert's implicit routing to test whether explicit classification could work as well or better.