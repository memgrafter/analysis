---
ver: rpa2
title: 'Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark
  Study'
arxiv_id: '2509.04468'
source_url: https://arxiv.org/abs/2509.04468
tags:
- level
- financial
- reasoning
- knowledge
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first comprehensive evaluation of large
  language models (LLMs) on professional financial reasoning using 1,560 CFA mock
  exam questions across all three levels. The research compares GPT-4o, GPT-o1, and
  o3-mini under zero-shot and retrieval-augmented generation (RAG) conditions, implementing
  a novel RAG pipeline that integrates official CFA curriculum content.
---

# Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study

## Quick Facts
- **arXiv ID**: 2509.04468
- **Source URL**: https://arxiv.org/abs/2509.04468
- **Authors**: Xuan Yao; Qianteng Wang; Xinbo Liu; Ke-Wei Huang
- **Reference count**: 5
- **Primary result**: GPT-o1 outperforms other models on CFA exam questions, with RAG providing substantial accuracy gains particularly at higher complexity levels.

## Executive Summary
This study presents the first comprehensive evaluation of large language models (LLMs) on professional financial reasoning using 1,560 CFA mock exam questions across all three levels. The research compares GPT-4o, GPT-o1, and o3-mini under zero-shot and retrieval-augmented generation (RAG) conditions, implementing a novel RAG pipeline that integrates official CFA curriculum content. Results show GPT-o1 consistently outperforms other models, with zero-shot accuracy exceeding 94% at Level I and RAG providing substantial improvements particularly at higher complexity levels. Comprehensive error analysis reveals knowledge gaps as the primary failure mode, with minimal impact from text readability. The study provides actionable guidance for LLM deployment in finance, demonstrating that targeted knowledge augmentation yields better performance gains than model scaling alone.

## Method Summary
The evaluation uses 1,560 CFA mock exam questions (900 Level I, 440 Level II, 220 Level III) tested under zero-shot and RAG conditions across three LLM models (GPT-4o, GPT-o1, o3-mini). The RAG pipeline employs text-embedding-3-small with ChromaDB, using hierarchical collections partitioned by exam level and topic. Questions are parsed into structured records with level, topic, question text, options, and ground truth. The RAG system retrieves top-5 chunks via cosine similarity, with a two-stage query generation process (summary + keywords). Accuracy is measured per level and topic, with passing criteria defined as 60-70% overall and 50-60% per topic. Error types are categorized into Knowledge, Reasoning, Calculation, and Inconsistency through LLM-assisted labeling.

## Key Results
- GPT-o1 achieves highest accuracy across all levels, with zero-shot performance exceeding 94% at Level I
- RAG improves accuracy by 5-10 percentage points, particularly for complex Level II/III topics
- Knowledge errors dominate failure modes (61-70% at higher levels), while text readability has minimal impact
- RAG performance varies by topic, with some complex areas (Level III Equity) showing decreased accuracy due to irrelevant context
- Calculation accuracy remains low (~44.7% for GPT-4o) even with RAG augmentation

## Why This Works (Mechanism)
The study demonstrates that professional financial reasoning requires both broad conceptual knowledge and the ability to apply that knowledge to complex scenarios. The CFA exam structure naturally captures this dual requirement through its progression from basic concepts (Level I) to integrated application (Levels II/III). The RAG pipeline works by providing context-specific curriculum content that supplements the models' pre-existing knowledge, effectively bridging gaps in their training data. The two-stage query generation process (summary + keywords) enables more precise retrieval than single-query approaches, particularly for complex financial scenarios requiring multiple concept integrations.

## Foundational Learning
- **RAG retrieval optimization**: Why needed - to supplement model knowledge with relevant curriculum content; Quick check - verify top-5 chunks are topically relevant before context augmentation
- **Error classification taxonomy**: Why needed - to identify whether failures stem from knowledge gaps versus reasoning limitations; Quick check - validate LLM-labeled errors with human expert review
- **CFA exam structure**: Why needed - provides standardized, comprehensive financial reasoning benchmark; Quick check - confirm question categorization aligns with official CFA topic boundaries
- **Zero-shot vs RAG comparison**: Why needed - isolates the value of external knowledge versus model capability; Quick check - measure accuracy delta between conditions per topic
- **Calculation vs conceptual separation**: Why needed - identifies different failure modes requiring distinct solutions; Quick check - run calculation-heavy items through separate numeric verification pipeline
- **Topic-level performance analysis**: Why needed - reveals which curriculum areas benefit most from augmentation; Quick check - plot accuracy by topic to identify knowledge gaps

## Architecture Onboarding

**Component Map**: CFA Questions -> Prompt Engine -> LLM -> Answer -> Error Classifier; RAG Branch: CFA Questions -> Query Generator -> Retriever -> Context -> LLM

**Critical Path**: Question parsing → Model inference (zero-shot or RAG) → Answer generation → Accuracy calculation → Error classification

**Design Tradeoffs**: The study uses top-5 chunk retrieval versus deeper retrieval, accepting potential noise for broader coverage. The two-stage query generation adds complexity but improves precision. Error classification via LLM rather than human review trades accuracy for scalability.

**Failure Signatures**: Knowledge errors manifest as incorrect answers despite relevant context, calculation errors show arithmetic mistakes, inconsistency errors reveal contradictory reasoning within responses, and reasoning errors indicate correct knowledge application failures.

**First 3 Experiments**:
1. Run zero-shot evaluation on all 1,560 questions to establish baseline accuracy per level/topic
2. Execute RAG-augmented runs using the two-stage query generation pipeline for all models
3. Perform error classification on failed responses to identify dominant failure modes by level

## Open Questions the Paper Calls Out

**Open Question 1**: Does integrating deterministic computational tools (such as code interpreters) eliminate the calculation-based errors observed in text-only models, particularly for GPT-4o? The study concludes that future gains require "adding deterministic numerical checks" because RAG pipelines improved conceptual accuracy but left calculation accuracy "virtually unchanged." This remains unresolved as the study isolated reasoning from calculation by analyzing text outputs without testing tool-augmented agents.

**Open Question 2**: How robust is the observed dominance of "knowledge errors" across models when evaluating financial regulations that have changed significantly since the models' training cutoffs? The literature review notes that "the temporal dynamics of financial knowledge, where regulations and market practices evolve continuously, remain underexplored in existing evaluation frameworks." The current benchmark may not capture "knowledge drift" in real-world scenarios since it relies on static curriculum materials.

**Open Question 3**: Can adaptive retrieval strategies prevent the performance degradation observed in complex topics where standard RAG introduced irrelevant context? Results show RAG reduced accuracy for Level III Equity Investments, suggesting the "retrieval appears to introduce confusing or irrelevant information." The study used fixed "top-5 segments" retrieval and did not test mechanisms for the model to reject retrieved context or dynamically adjust the number of retrieved chunks.

## Limitations

- The study exclusively uses proprietary CFA Institute materials, making full independent verification impossible without access to copyrighted resources
- Error analysis relies heavily on LLM-assisted labeling, introducing potential bias in classifying knowledge versus reasoning errors
- Evaluation focuses on multiple-choice format questions without testing models' ability to generate free-form financial analysis or handle real-world ambiguity

## Confidence

**High confidence**: Methodology is transparent and reproducible given CFA materials access, with clear metrics (accuracy per level/topic) and well-defined error categories. Performance gap between zero-shot and RAG conditions is substantial and consistent.

**Medium confidence**: Generalizability to non-exam financial reasoning tasks is uncertain. While CFA questions represent professional finance knowledge, they may not fully capture real-world financial analysis complexity.

**Low confidence**: Specific curriculum coverage and completeness claims cannot be independently verified without exact curriculum PDFs. Impact of different chunking strategies and embedding parameters on RAG performance is not fully characterized.

## Next Checks

1. **Independent Error Classification**: Have human CFA charterholders manually classify a random sample of model errors to validate the accuracy of LLM-assisted error categorization, particularly for distinguishing knowledge gaps from reasoning failures.

2. **Real-World Financial Task Testing**: Evaluate the top-performing models (GPT-o1 with RAG) on practical financial analysis tasks such as portfolio optimization scenarios, earnings call analysis, or regulatory compliance document review to assess real-world applicability beyond exam formats.

3. **Knowledge Gap Analysis**: Map the specific curriculum topics where RAG provided the greatest improvement (e.g., Level III Equity) and conduct targeted experiments to determine whether additional retrieval strategies (reranking, multi-hop retrieval) could further reduce knowledge-based errors.