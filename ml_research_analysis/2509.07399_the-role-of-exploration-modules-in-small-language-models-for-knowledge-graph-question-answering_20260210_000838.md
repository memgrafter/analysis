---
ver: rpa2
title: The Role of Exploration Modules in Small Language Models for Knowledge Graph
  Question Answering
arxiv_id: '2509.07399'
source_url: https://arxiv.org/abs/2509.07399
tags:
- language
- slms
- knowledge
- exploration
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how small language models (SLMs) perform
  on knowledge graph question answering (KGQA) compared to large language models (LLMs).
  While LLMs significantly benefit from the Think-on-Graph framework for KGQA, SLMs
  do not see similar improvements and can even perform worse than the baseline Chain-of-Thought
  method.
---

# The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2509.07399
- Source URL: https://arxiv.org/abs/2509.07399
- Authors: Yi-Jie Cheng; Oscar Chew; Yun-Nung Chen
- Reference count: 13
- Small language models (SLMs) struggle with knowledge graph question answering (KGQA) due to exploration limitations, not reasoning; lightweight retrieval models can effectively substitute for exploration.

## Executive Summary
This paper investigates why small language models (SLMs) underperform on knowledge graph question answering (KGQA) compared to large language models (LLMs), and proposes a practical solution. While LLMs benefit from the Think-on-Graph (ToG) framework, SLMs do not see similar improvements and can even perform worse than the baseline Chain-of-Thought method. The authors identify that SLMs struggle with exploration—effectively traversing and reasoning over knowledge graphs—which is critical for accurate answers. To address this, they propose using lightweight passage retrieval models (like SentenceBERT and GTR) to handle knowledge graph traversal instead of relying on the SLM itself. Experiments show that this approach significantly improves SLM performance on KGQA tasks, validating the effectiveness of decoupling exploration from the language model. The study highlights a practical solution for making KGQA more accessible and efficient for resource-constrained settings.

## Method Summary
The method replaces the SLM-driven exploration stage in the ToG framework with lightweight passage retrieval models. Specifically, SentenceBERT and GTR (both ~110M parameters) are used to encode questions and knowledge graph candidates into embeddings, ranking them by semantic similarity via dot-product scoring. This substitution is applied zero-shot without training. The retrieval models select top-k relevant paths from the knowledge graph, which are then passed to the SLM for reasoning. The approach is evaluated on benchmark datasets CWQ and WebQSP using SLMs ranging from 0.5B to 8B parameters, comparing against Chain-of-Thought and original ToG baselines.

## Key Results
- Lightweight retrieval models (SentenceBERT, GTR) significantly improve SLM performance on KGQA tasks compared to both CoT and ToG baselines
- BM25 fails for some SLMs (e.g., Gemma2-2b degrades), while dense retrievers (GTR/SentenceBERT) are more robust
- SLMs with retrieval-assisted exploration still trail GPT-4.1 performance, indicating remaining reasoning limitations
- Retrieval substitution degrades LLM performance but improves SLM performance, showing capacity-dependent trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Exploration Quality as the Performance Bottleneck
- Claim: SLMs underperform on KGQA primarily due to inadequate exploration (path retrieval), not insufficient reasoning capability
- Evidence: SLMs with GPT-4.1-assisted exploration show average improvements of +0.159 (CWQ) and +0.238 (WebQSP) over CoT baselines; cross-entropy alignment with GPT-4.1's exploration decisions improves consistently with model size

### Mechanism 2: Retrieval Models as Efficient Exploration Substitutes
- Claim: Offloading exploration to lightweight passage retrieval models improves SLM KGQA performance without training or fine-tuning
- Evidence: GTR improves all five SLMs over both ToG and CoT baselines (e.g., Llama-3-8b: 0.296→0.325 CWQ; 0.569→0.642 WebQSP); SentenceBERT also shows consistent gains

### Mechanism 3: Asymmetric Transfer of Exploration Substitution
- Claim: Replacing LLM-guided exploration with retrieval models degrades performance for LLMs but improves it for SLMs—a resource-dependent trade-off
- Evidence: GPT-4.1 with retrieval modules shows EM degradation (0.575→0.505 CWQ with GTR) vs. standard ToG; same retrieval modules improve SLMs (e.g., Qwen2-7b: 0.300→0.331 CWQ with GTR)

## Foundational Learning

- **Knowledge Graph Question Answering (KGQA)**
  - Why needed: The entire paper evaluates methods for answering natural language questions by traversing structured knowledge graphs
  - Quick check: Given a question "Who is the president of the country where language X is spoken?" and a KG with entities/relation edges, can you trace a 2-hop path to the answer?

- **Think-on-Graph (ToG) Framework Stages**
  - Why needed: The paper diagnoses which ToG stage (initialization, exploration, reasoning) fails for SLMs
  - Quick check: If exploration retrieves irrelevant paths but reasoning is sound, will the final answer be correct? Why or why not?

- **Dense Retrieval and Semantic Similarity**
  - Why needed: The proposed solution uses SentenceBERT/GTR to encode questions and KG candidates into embeddings and rank by dot-product similarity
  - Quick check: Given embeddings for a question and three candidate relations, how would you select the top-2 most relevant relations using dot-product scores?

## Architecture Onboarding

- **Component map**: Question → Initialization (entity extraction) → Exploration (retrieval-based path selection) → Reasoning (SLM answer generation)
- **Critical path**: Question → Initialization → Exploration (retrieval substitution) → Reasoning; the exploration substitution is the critical intervention point
- **Design tradeoffs**: BM25 vs. Dense Retrieval (BM25 is faster but fails on some SLMs; dense retrievers are more robust but require embedding computation); Exploration decoupling (gains accuracy for SLMs but would degrade accuracy if applied to LLMs)
- **Failure signatures**: BM25 with small SLMs (CWQ/WebQSP scores drop below ToG baseline for Gemma2-2b and Qwen2-7b); Parsing errors in SLM outputs (without constrained decoding, small models generate unparseable relation entries); Exploration-reasoning mismatch (even with retrieval, if retrieved paths lack the critical relation, SLM cannot answer correctly)
- **First 3 experiments**: 1) Baseline comparison: Run ToG vs. CoT on your target SLM across CWQ/WebQSP benchmarks to confirm exploration bottleneck; 2) Ablation on exploration quality: Provide your SLM with GPT-4.1-retrieved paths and measure answer accuracy to isolate exploration vs. reasoning failure; 3) Retrieval module substitution: Replace SLM-guided exploration with SentenceBERT and GTR; compare EM scores against ToG and CoT baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of using lightweight passage retrieval for exploration generalize to other knowledge graphs beyond Freebase, particularly those that are sparser or domain-specific?
- Basis: The experimental setup explicitly states the use of Freebase as the underlying knowledge graph for all evaluations (Section 3.1), but does not test other graph structures
- Why unresolved: Freebase has a specific density and schema structure; it is unclear if semantic similarity search works as effectively for graph traversal in specialized domains
- What evidence would resolve it: Evaluation of the proposed SLM + retrieval framework on benchmark datasets utilizing diverse knowledge graphs (e.g., Wikidata or domain-specific graphs)

### Open Question 2
- Question: Now that the exploration bottleneck is mitigated, does the inherent reasoning capability of the SLM become the primary limiting factor for closing the remaining performance gap with LLMs?
- Basis: The authors identify exploration as the key bottleneck (Section 3.3), but Table 4 shows that even with GTR-assisted exploration, SLMs still significantly trail GPT-4.1
- Why unresolved: While the paper proves that better context improves SLM scores, it does not analyze whether the remaining errors are due to the SLM's inability to perform logical deduction
- What evidence would resolve it: An error analysis of the "retrieval-assisted" SLM results specifically categorizing failures due to incorrect retrieval vs. failures in logical reasoning given correct retrieval

### Open Question 3
- Question: At what specific model scale does the performance trade-off invert, causing external retrieval modules to hinder rather than help performance?
- Basis: The authors note that their findings contrast with Sun et al. (2024), who found that retrieval modules degraded performance for LLMs
- Why unresolved: The paper compares "Small" (up to 8B) against "Large" (GPT-4.1) but does not map the trajectory between these points
- What evidence would resolve it: A series of experiments scaling the language model size while keeping the exploration module constant to observe where the performance benefit neutralizes or becomes negative

## Limitations
- Single-run results without multiple random seeds prevent quantification of statistical variance
- Experimental scope limited to Freebase knowledge graph, not tested on other KGs or domain-specific graphs
- Attribution of exploration vs. reasoning bottlenecks relies on indirect evidence rather than direct ablation

## Confidence
- **High confidence**: Retrieval modules (SentenceBERT, GTR) improve SLM KGQA performance when exploration is the bottleneck
- **Medium confidence**: Exploration quality is the primary bottleneck for SLMs (vs. reasoning capability)
- **Low confidence**: The asymmetric transfer hypothesis (retrieval modules help SLMs but hurt LLMs) and the exact mechanism by which retrieval models generalize to KG path selection

## Next Checks
1. Isolate exploration vs. reasoning: Provide SLMs with oracle exploration paths from multiple sources (not just GPT-4.1) and measure whether reasoning accuracy improves consistently
2. Test retrieval module scalability: Evaluate retrieval-based exploration across a broader range of model scales (from 0.5B to 70B parameters) to validate the asymmetric transfer claim
3. Analyze failure case distributions: Categorize KGQA errors when using retrieval modules into exploration failures (irrelevant paths) vs. reasoning failures (correct paths but wrong answers)