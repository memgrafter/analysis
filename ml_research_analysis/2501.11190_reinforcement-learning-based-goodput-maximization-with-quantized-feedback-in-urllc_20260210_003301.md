---
ver: rpa2
title: Reinforcement Learning Based Goodput Maximization with Quantized Feedback in
  URLLC
arxiv_id: '2501.11190'
source_url: https://arxiv.org/abs/2501.11190
tags:
- channel
- feedback
- where
- goodput
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses goodput maximization in Ultra-Reliable Low-Latency
  Communication (URLLC) systems with quantized feedback under dynamic channel conditions.
  The authors propose a two-part system: (1) a novel learning-based Rician-K factor
  estimator using XGBoost regression on empirical moments of channel magnitude, and
  (2) a reinforcement learning-based quantized feedback scheme selection algorithm
  using Q-learning to optimize quantization thresholds and transmission rates.'
---

# Reinforcement Learning Based Goodput Maximization with Quantized Feedback in URLLC

## Quick Facts
- arXiv ID: 2501.11190
- Source URL: https://arxiv.org/abs/2501.11190
- Reference count: 23
- Primary result: RL-based quantized feedback scheme with XGBoost K-factor estimation achieves superior goodput maximization in dynamic URLLC environments

## Executive Summary
This paper addresses goodput maximization in Ultra-Reliable Low-Latency Communication (URLLC) systems with quantized feedback under dynamic channel conditions. The authors propose a two-part system: (1) a novel learning-based Rician-K factor estimator using XGBoost regression on empirical moments of channel magnitude, and (2) a reinforcement learning-based quantized feedback scheme selection algorithm using Q-learning to optimize quantization thresholds and transmission rates. The RL approach adapts to varying channel statistics by treating each quantization threshold as an independent agent, enabling the system to track changing Rician-K factors and approach maximum achievable goodput values.

## Method Summary
The method consists of two components working together: a supervised learning-based Rician-K factor estimator using XGBoost regression trained on the first ten empirical moments of channel magnitude, and a Q-learning-based quantized feedback scheme optimizer that treats each quantization threshold as an independent agent. The K-factor estimator provides the current channel state to the Q-learning module, which updates quantization thresholds and transmission rates to maximize empirical goodput. The system adapts to non-stationary channel statistics through this two-timescale approach, with the estimator tracking K-factor changes and the RL module refining policies based on observed performance.

## Key Results
- Learning-based K-factor estimator outperforms traditional moment-based estimators in accuracy
- RL-based feedback scheme effectively adapts to channel variations, tracking changing Rician-K factors
- The proposed system demonstrates superior goodput maximization compared to conventional approaches in dynamic URLLC environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using empirical moments as features for XGBoost regression improves Rician-K factor estimation accuracy compared to traditional moment-based estimators.
- Mechanism: The first ten raw moments of the channel magnitude capture statistical characteristics of the Rician distribution. XGBoost learns the nonlinear mapping from these moment features to the K-factor, leveraging gradient boosting to handle complex relationships.
- Core assumption: The empirical moments computed from N samples provide sufficient statistics to characterize the underlying Rician distribution.
- Evidence anchors: [abstract], [section III], [corpus] Weak direct evidence.

### Mechanism 2
- Claim: Treating each quantization threshold as an independent Q-learning agent enables tractable optimization of the feedback scheme under varying channel statistics.
- Mechanism: Each threshold λ_l maintains its own Q-table mapping (K-factor, current position) to action values. At each iteration, one agent updates its position based on ε-greedy policy. The reward is empirical goodput averaged over M transmissions.
- Core assumption: Treating thresholds as independent agents provides a reasonable approximation to the joint optimization.
- Evidence anchors: [abstract], [section IV-A], [corpus] Neighbor papers similarly decompose problems using RL.

### Mechanism 3
- Claim: Separating the feedback system into K-factor estimation (supervised learning) and threshold optimization (RL) enables adaptation to non-stationary channel statistics.
- Mechanism: The K-factor estimator provides the current channel state to the Q-learning module. The Q-tables are indexed by K-factor, allowing the system to retrieve previously learned policies when channel statistics repeat.
- Core assumption: The K-factor varies slowly enough that the estimator can track it within N = 100 samples.
- Evidence anchors: [abstract], [section II-C], [corpus] Online Meta-Learning Channel Autoencoder paper addresses similar dynamic channel adaptation.

## Foundational Learning

- Concept: Rician fading and K-factor
  - Why needed here: The entire system assumes Rician-distributed channel magnitude where K characterizes the LOS-to-multipath ratio.
  - Quick check question: Given K = 10 dB, would you expect the channel magnitude distribution to have a pronounced peak or be more spread out?

- Concept: Q-learning and the Bellman equation
  - Why needed here: The threshold optimization uses tabular Q-learning.
  - Quick check question: If η = 0.9 and the agent receives reward 0.5 after taking action a in state s, transitioning to s' where max Q(s',a') = 1.0, what is the TD target?

- Concept: Quantized feedback and goodput
  - Why needed here: The optimization objective maximizes goodput (rate × success probability) subject to quantized CSI.
  - Quick check question: If the transmitter selects rate r based on quantization region [λ_l, λ_{l+1}) but the actual channel magnitude γ < λ_l, what happens to the transmission?

## Architecture Onboarding

- Component map:
  Receiver side: Channel estimator → K-factor estimator (XGBoost) → Q-learning module (Q-tables for each λ_l) → Rate selector
  Transmitter side: Rate buffer storing r_l for each quantization level
  Feedback channel 1: Quantized CSI index l (every codeword)
  Feedback channel 2: Updated rate r_l (triggered by RL decision during training)

- Critical path:
  1. Collect N channel magnitude samples
  2. Compute 10 empirical moments
  3. XGBoost predicts K-factor
  4. Q-learning selects action for one λ_l agent
  5. Observe M transmissions, compute empirical goodput reward
  6. Update Q-table entry, optionally send updated r_l to transmitter

- Design tradeoffs:
  - **N (samples for estimation)**: Larger N improves K-factor accuracy but increases latency. Paper found N = 100 optimal, N > 50 acceptable.
  - **M (transmissions per reward)**: Larger M gives more stable reward estimates but slows learning. Paper tested M = {100, 1000}.
  - **Λ (quantization levels)**: More levels improve goodput closer to ergodic capacity but increase Q-table size and feedback overhead.
  - **|S| (discrete threshold set size)**: Finer granularity improves optimization but expands state space combinatorially.

- Failure signatures:
  - Q-learning oscillates without converging: ε decays too slowly or learning rate α too high
  - K-factor estimates biased high/low: Training data doesn't cover the operational K range
  - Goodput far below theoretical maximum: Q-tables not initialized for current K, or independence assumption violated
  - Sudden goodput drops: K-factor changed but estimator window hasn't captured new statistics yet

- First 3 experiments:
  1. **Validate K-factor estimator**: Generate Rician samples with known K values, compute empirical moments from N = 100 samples, run XGBoost prediction. Compare RMSE against moment-based estimator.
  2. **Verify Q-learning convergence**: Set Λ = 4, P = 20 dB, fixed K = 10 dB, M = 100. Run algorithm for 500 iterations. Plot ω_t against iteration t.
  3. **Test tracking ability**: Run simulation with K stepping through {0, 10, 20} dB every 200 channel realizations. Verify the system adapts and approaches each corresponding maximum goodput.

## Open Questions the Paper Calls Out
None

## Limitations
- Q-learning hyperparameters (α and η) are not specified, preventing exact reproduction
- State space discretization details (cardinality |S| and specific quantization levels) are missing
- Performance claims lack statistical validation with error bars or confidence intervals

## Confidence

| Claim Type | Confidence Level | Reason |
|------------|------------------|--------|
| Fundamental mechanisms | High | XGBoost and Q-learning are well-established |
| Two-timescale adaptation approach | Medium | Appears reasonable but lacks rigorous justification |
| Quantitative performance improvements | Low | Cannot be fully verified without implementation details |

## Next Checks
1. **Statistical validation**: Re-run the tracking experiment 50 times with different random seeds; compute mean and 95% confidence intervals for adaptation latency and final goodput.
2. **Hyperparameter sensitivity**: Systematically vary α ∈ {0.01, 0.1, 0.3} and η ∈ {0.9, 0.95, 0.99} to identify optimal settings; test sensitivity to initial Q-table values.
3. **Break condition exploration**: Test system performance when channel distribution deviates from Rician (e.g., Nakagami-m fading); measure estimation error and goodput degradation to quantify robustness limits.