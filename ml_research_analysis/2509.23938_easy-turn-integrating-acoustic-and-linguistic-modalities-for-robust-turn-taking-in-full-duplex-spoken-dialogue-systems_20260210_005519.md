---
ver: rpa2
title: 'Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking
  in Full-Duplex Spoken Dialogue Systems'
arxiv_id: '2509.23938'
source_url: https://arxiv.org/abs/2509.23938
tags:
- turn
- speech
- dialogue
- detection
- easy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Easy Turn addresses the challenge of robust turn-taking detection
  in full-duplex spoken dialogue systems by integrating acoustic and linguistic modalities.
  The model combines a Whisper-Medium audio encoder, an audio adaptor, and a lightweight
  0.5B-parameter LLM, adopting an ASR+Turn-Detection paradigm to effectively fuse
  speech and text information.
---

# Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems

## Quick Facts
- arXiv ID: 2509.23938
- Source URL: https://arxiv.org/abs/2509.23938
- Reference count: 0
- Primary result: 97.67% accuracy on Easy Turn testset, significantly outperforming baselines

## Executive Summary
Easy Turn presents a novel approach to turn-taking detection in full-duplex spoken dialogue systems by integrating both acoustic and linguistic modalities. The model employs a Whisper-Medium audio encoder, an audio adaptor, and a lightweight 0.5B-parameter LLM, operating on an ASR+Turn-Detection paradigm. It achieves state-of-the-art performance with 97.67% accuracy on its test set, surpassing existing open-source models like TEN Turn Detection (86.67%) and Smart Turn V2 (78.67%). The work also introduces the Easy Turn trainset, a 1,145-hour speech dataset, to support model training and advance research in this domain.

## Method Summary
Easy Turn addresses the challenge of robust turn-taking detection in full-duplex spoken dialogue systems by integrating acoustic and linguistic modalities. The model combines a Whisper-Medium audio encoder, an audio adaptor, and a lightweight 0.5B-parameter LLM, adopting an ASR+Turn-Detection paradigm to effectively fuse speech and text information. It predicts four dialogue turn states: complete, incomplete, backchannel, and wait. Accompanying the model, the Easy Turn trainset—a 1,145-hour speech dataset—was released to support training.

## Key Results
- Achieves state-of-the-art accuracy of 97.67% on the Easy Turn testset
- Outperforms existing open-source models: TEN Turn Detection (86.67%) and Smart Turn V2 (78.67%)
- Maintains competitive inference speed and memory efficiency while delivering superior performance

## Why This Works (Mechanism)
The integration of acoustic and linguistic modalities enables the model to capture both prosodic cues and semantic content, which are critical for accurate turn-taking detection. The Whisper-Medium encoder provides robust speech recognition, while the audio adaptor bridges acoustic features to the LLM, allowing for effective fusion of multimodal information. The lightweight LLM ensures efficient inference without sacrificing performance, making the system practical for real-world deployment.

## Foundational Learning
- **Whisper-Medium audio encoder**: Converts raw speech into high-quality embeddings; needed for accurate speech-to-text conversion
- **Audio adaptor**: Bridges acoustic features to the LLM; ensures compatibility between modalities
- **0.5B-parameter LLM**: Lightweight yet effective for turn-taking classification; balances performance and efficiency
- **ASR+Turn-Detection paradigm**: Combines automatic speech recognition with turn detection; enables end-to-end processing
- **Four-state classification**: Complete, incomplete, backchannel, and wait; covers common turn-taking scenarios
- **1,145-hour training dataset**: Large-scale, high-quality speech data; supports robust model training

## Architecture Onboarding
- **Component map**: Whisper-Medium -> Audio Adaptor -> LLM -> Turn State Prediction
- **Critical path**: Speech input → Whisper-Medium encoding → Audio adaptor fusion → LLM classification → Turn state output
- **Design tradeoffs**: Lightweight LLM prioritizes efficiency over maximum capacity; four-state classification balances granularity and simplicity
- **Failure signatures**: Misclassification in noisy environments; degraded performance with accented speech
- **First experiments**: 1) Test on clean speech data; 2) Evaluate with synthetic noise; 3) Compare against baseline models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to a single dataset, limiting external validity
- 0.5B-parameter LLM may constrain performance on complex dialogue scenarios
- Memory efficiency claims lack detailed benchmarking across hardware configurations

## Confidence
- Model architecture and implementation: High
- Dataset-specific performance claims: Medium
- Memory efficiency and inference speed: Medium
- Generalization to diverse dialogue scenarios: Low

## Next Checks
1. Evaluate on multiple benchmark datasets to assess cross-dataset generalization
2. Test performance under varying acoustic conditions (noise levels, accents) to validate robustness claims
3. Conduct ablation studies to quantify the contribution of each modality (acoustic vs linguistic) to overall performance