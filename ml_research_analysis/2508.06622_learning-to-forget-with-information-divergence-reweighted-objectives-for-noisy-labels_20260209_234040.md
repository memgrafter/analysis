---
ver: rpa2
title: Learning to Forget with Information Divergence Reweighted Objectives for Noisy
  Labels
arxiv_id: '2508.06622'
source_url: https://arxiv.org/abs/2508.06622
tags:
- noise
- antidote
- labels
- samples
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ANTIDOTE, a novel method for training machine\
  \ learning models under label noise by employing a relaxation-optimization strategy\
  \ over information-divergence neighborhoods. The key idea is to allow the empirical\
  \ data distribution to be perturbed within a neighborhood defined by an f-divergence,\
  \ enabling the model to adaptively reduce the influence of samples with noisy labels\u2014\
  effectively \"forgetting\" them during training."
---

# Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels

## Quick Facts
- arXiv ID: 2508.06622
- Source URL: https://arxiv.org/abs/2508.06622
- Reference count: 40
- Introduces ANTIDOTE method for training under label noise by relaxing empirical distribution within f-divergence neighborhoods

## Executive Summary
This paper presents ANTIDOTE, a novel method for training machine learning models when labels are noisy. The approach uses a relaxation-optimization strategy where the empirical data distribution is allowed to be perturbed within an f-divergence neighborhood, effectively enabling the model to "forget" samples with noisy labels during training. Through convex duality, the problem is reformulated into an efficient adversarial training framework with low-dimensional optimization. The method shows strong theoretical guarantees and outperforms state-of-the-art approaches across multiple benchmarks including CIFAR-10, CIFAR-100, WebVision, and ImageNet, while maintaining computational efficiency close to standard cross-entropy.

## Method Summary
ANTIDOTE employs an f-divergence neighborhood relaxation strategy to handle label noise. The method allows the empirical data distribution to be perturbed within a neighborhood defined by an f-divergence, which enables adaptive reduction of influence from samples with noisy labels. Using convex duality, the problem is reformulated into an adversarial training framework where a computationally efficient low-dimensional optimization is performed. This approach effectively "forgets" noisy samples by adjusting their contribution to the loss function during training.

## Key Results
- Consistently outperforms state-of-the-art losses including Ïµ-softmax across multiple benchmarks
- Demonstrates strong performance on CIFAR-10, CIFAR-100, CIFAR-100N, WebVision, and ImageNet
- Maintains time complexity close to standard cross-entropy loss while handling various noise types (symmetric, asymmetric, human-annotated, real-world)
- Theoretical results show true labels are the unique solution under appropriate assumptions

## Why This Works (Mechanism)
The method works by creating a robust optimization framework that can adapt to label noise through distributional relaxation. By allowing the empirical distribution to be perturbed within an f-divergence neighborhood, the model can effectively downweight or "forget" samples with noisy labels while preserving the learning signal from clean samples. The convex duality reformulation transforms this into an adversarial problem where the optimization can be performed efficiently, making the approach both theoretically sound and practically viable.

## Foundational Learning
- **f-divergence**: Measures the difference between probability distributions; needed to define the neighborhood around the empirical distribution where perturbations are allowed. Quick check: Verify that KL-divergence is a special case of f-divergence.
- **Convex duality**: Mathematical framework that transforms the primal problem into a dual problem; needed to reformulate the relaxation problem into an efficient adversarial training framework. Quick check: Confirm that strong duality holds for the reformulated problem.
- **Adversarial training**: Training framework where a model learns to be robust against perturbations; needed to implement the efficient low-dimensional optimization. Quick check: Validate that the adversarial objective improves robustness to label noise.
- **Distributional robustness**: Approach that optimizes over a set of distributions rather than a single distribution; needed to create the relaxation framework. Quick check: Ensure the method maintains performance when the noise distribution changes.
- **Label noise types**: Understanding symmetric vs asymmetric vs real-world noise patterns; needed to validate the method across different noise scenarios. Quick check: Test the method on synthetic noise with known corruption rates.

## Architecture Onboarding

**Component Map**: Data -> Perturbation Operator -> Loss Function -> Model Parameters -> Prediction

**Critical Path**: The core computational path involves computing the f-divergence neighborhood, solving the dual optimization problem, and updating model parameters based on the reweighted loss.

**Design Tradeoffs**: The method trades off some computational overhead for improved robustness to label noise. While maintaining near-standard cross-entropy efficiency, it requires solving an additional optimization problem at each training step. The choice of f-divergence affects both theoretical guarantees and practical performance.

**Failure Signatures**: The method may struggle with extremely high noise rates (>50%) where the signal-to-noise ratio becomes too low. It may also face scalability challenges with extremely large datasets due to the additional optimization step.

**First Experiments**:
1. Verify the method's performance on CIFAR-10 with 40% symmetric label noise compared to standard cross-entropy
2. Test the computational overhead by measuring training time per epoch on CIFAR-100
3. Evaluate robustness to asymmetric noise patterns by introducing class-dependent label corruption

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting that the assumption of unique solution under appropriate conditions is theoretical and may not hold in all practical settings.

## Limitations
- Practical applicability beyond synthetic noise benchmarks remains uncertain, particularly for extremely large datasets or very high noise rates (>50%)
- Theoretical assumption that true labels are the unique solution may not hold in all practical settings, especially with complex label noise patterns
- Computational overhead, while claimed to be close to standard cross-entropy, could still be prohibitive for extremely large-scale applications

## Confidence

- **High Confidence**: Theoretical foundation of using f-divergence relaxation for noisy label learning and convexity of the reformulated adversarial problem
- **Medium Confidence**: Empirical performance claims across diverse benchmarks, based on standard datasets but may not generalize to all real-world scenarios
- **Low Confidence**: Scalability and robustness in extremely noisy or large-scale settings, as these are not extensively validated in the paper

## Next Checks
1. **Scalability Test**: Evaluate ANTIDOTE on extremely large datasets (e.g., JFT-300M or larger) to assess computational efficiency and memory usage
2. **Extreme Noise Regime**: Test the method under very high noise rates (>50%) to determine its robustness limits
3. **Real-World Deployment**: Apply ANTIDOTE to a real-world noisy dataset with minimal preprocessing to validate its practical effectiveness