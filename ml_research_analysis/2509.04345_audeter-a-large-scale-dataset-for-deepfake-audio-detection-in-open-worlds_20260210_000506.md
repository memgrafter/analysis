---
ver: rpa2
title: 'AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds'
arxiv_id: '2509.04345'
source_url: https://arxiv.org/abs/2509.04345
tags:
- audio
- speech
- detection
- voice
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting deepfake audio
  in open-world scenarios, where models must generalize to novel speech synthesis
  systems and diverse human voices not seen during training. The authors introduce
  AUDETER, a large-scale dataset containing over 4,500 hours of synthetic audio generated
  by 21 recent speech synthesis systems across four human voice corpora, making it
  the largest and most diverse deepfake audio dataset to date.
---

# AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds

## Quick Facts
- **arXiv ID**: 2509.04345
- **Source URL**: https://arxiv.org/abs/2509.04345
- **Reference count**: 40
- **One-line result**: AUDETER dataset achieves 44.1%-51.6% reduction in detection error rate, with 4.17% EER on cross-domain samples

## Executive Summary
This paper addresses the challenge of detecting deepfake audio in open-world scenarios, where models must generalize to novel speech synthesis systems and diverse human voices not seen during training. The authors introduce AUDETER, a large-scale dataset containing over 4,500 hours of synthetic audio generated by 21 recent speech synthesis systems across four human voice corpora, making it the largest and most diverse deepfake audio dataset to date. Through extensive experiments, they demonstrate that state-of-the-art detection methods trained on existing datasets struggle with generalization, suffering high false positive rates on unseen human voices and novel deepfake patterns. Training on AUDETER significantly improves performance, reducing detection error rates by 44.1% to 51.6% and achieving an Equal Error Rate of only 4.17% on cross-domain samples, establishing a data-centric approach for training generalist deepfake audio detectors.

## Method Summary
The authors created AUDETER by generating synthetic audio using 11 TTS systems and 10 vocoders across four human speech corpora (Common Voice, People's Speech, MLS, In-the-Wild). For each real audio sample, they generated corresponding fake audio using all synthesis systems with matching scripts. The dataset includes two collections: TTS (text-to-speech) and Vocoder (waveform processing). They evaluated detection models using XLS-R backbones with various classifier heads (SLS, RawNet2+AASIST) on Common Voice and People's Speech subsets, then tested generalization on the unseen In-the-Wild dataset. The key innovation is the systematic structure that enables controlled evaluation of domain shift.

## Key Results
- Models trained on AUDETER achieve 44.1%-51.6% reduction in detection error rate compared to existing methods
- XLR-SLS detector achieves only 4.17% Equal Error Rate on cross-domain samples
- AUDETER-trained models show significantly reduced false positive rates on unseen human voices
- Systematic evaluation reveals models struggle with novel synthesis systems but benefit from diverse training data

## Why This Works (Mechanism)

### Mechanism 1: Data-Centric Generalization via Pattern Coverage
Exposing models to diverse synthesis systems and human voice sources improves detection of novel deepfake audio by learning shared discriminative patterns rather than overfitting to limited artifacts. Training on 21 systems across 4 corpora approximates open-world test distribution.

### Mechanism 2: Large-Scale Pretrained Backbone Exploitation
Leveraging large audio backbones (Wav2Vec2.0, XLS-R) on massive datasets enables effective learning of generalized deepfake detection patterns. High model capacity and pretrained representations integrate diverse deepfake and real audio patterns into decision boundaries.

### Mechanism 3: Systematic Evaluation for Controlled Domain Shift Analysis
The dataset's structure with matched scripts across systems and corpora enables rigorous evaluation under specific domain shifts. This controlled setup isolates variables to measure detector vulnerability to novel TTS models versus sensitivity to different human voices.

## Foundational Learning

- **Domain Shift in Machine Learning**
  - Why needed: The central problem is mismatch between training and real-world data (novel voices, new TTS models)
  - Quick check: Can you explain why a classifier trained only on pet database images might fail to identify street animals?

- **Text-to-Speech (TTS) Synthesis and Vocoder Architectures**
  - Why needed: Understanding the functional difference between vocoders (convert spectral features to audio) and TTS systems (convert text to audio) is crucial for interpreting results
  - Quick check: What is the primary functional difference between a vocoder and an end-to-end TTS system?

- **Equal Error Rate (EER) as a Metric**
  - Why needed: EER is the primary benchmark for detection performance in the paper
  - Quick check: If a security system has very low EER, what does that imply about its trade-off between unauthorized access and locking out authorized users?

## Architecture Onboarding

- **Component map**: 4 human speech corpora -> 11 TTS systems + 10 vocoders -> 4,500+ hours synthetic audio -> XLR-SLS classifier
- **Critical path**: Generate high-quality synthetic audio for all 21 models across 4 corpora -> Structure dataset with matching scripts -> Train XLR-SLS on Common Voice + People's Speech -> Evaluate on unseen In-the-Wild dataset
- **Design tradeoffs**: Scale vs computational cost (2000 GPU hours), diversity vs recency (new models not represented), model capacity vs data requirements (large backbones needed)
- **Failure signatures**: High EER on In-the-Wild indicates diversity gap, high false positives on specific real corpora indicate overfitting, performance gap between TTS and Vocoder collections suggests separate feature learning
- **First 3 experiments**:
  1. Baseline evaluation: Load pretrained SOTA detectors and evaluate on AUDETER test subsets without fine-tuning
  2. Ablation on training diversity: Train XLR-SLS using only Common Voice vs. Common Voice + People's Speech, compare EER on In-the-Wild
  3. Cross-collection generalization test: Train on only Vocoder collection and test on TTS collection (and vice-versa)

## Open Questions the Paper Calls Out

### Open Question 1
Can representative synthesis patterns be extracted to enable detection models to generalize across systems without requiring continuous dataset scaling? The paper plans to identify and extract representative synthesis patterns that generalize across multiple systems to avoid continuously scaling up the dataset.

### Open Question 2
Does self-supervised pretraining on AUDETER provide superior generalization compared to supervised training? The paper lists self-supervised pretraining as a promising direction for exploring advanced training methodologies to improve generalization.

### Open Question 3
What are the specific acoustic characteristics that enable or hinder cross-collection generalization between TTS systems and neural vocoders? While the paper demonstrates distinct patterns between collections, it does not isolate which specific signal processing artifacts are responsible for transferability gaps.

## Limitations
- Dataset diversity may still not capture all possible future synthesis artifacts
- Computational cost of generating and utilizing large-scale datasets limits practical adoption
- Reliance on specific large backbone architectures (XLS-R) may not generalize to more efficient models

## Confidence
- **High Confidence**: Claims about dataset scale and diversity (4,500+ hours, 21 synthesis systems, 4 corpora)
- **Medium Confidence**: Claims about relative performance improvements over existing datasets
- **Medium Confidence**: Claims about specific mechanisms enabling generalization (pattern coverage, backbone capacity)

## Next Checks
1. Test model performance on newly released synthesis systems not included in AUDETER to assess true open-world generalization
2. Evaluate whether smaller, more efficient models can achieve comparable performance when trained on AUDETER
3. Assess real-world detection performance in scenarios where deepfake audio is mixed with genuine audio streams rather than evaluated in isolation