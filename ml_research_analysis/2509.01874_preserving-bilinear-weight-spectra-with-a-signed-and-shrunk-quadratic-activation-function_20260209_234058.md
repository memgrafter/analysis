---
ver: rpa2
title: Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation
  Function
arxiv_id: '2509.01874'
source_url: https://arxiv.org/abs/2509.01874
tags:
- arxiv
- bilinear
- activation
- mnist
- mlps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Signed Quadratic Shrink (SQS), an activation
  function for Gated Linear Units (GLUs) that enables weight-based interpretability
  while maintaining competitive performance with state-of-the-art activation functions.
  The core method idea is to modify the quadratic activation function with a signed
  and shrunk formulation that preserves the bilinear weight structure needed for eigen-decomposition-based
  interpretability.
---

# Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function

## Quick Facts
- arXiv ID: 2509.01874
- Source URL: https://arxiv.org/abs/2509.01874
- Reference count: 40
- Primary result: SQS-GLUs achieve performance on par with or better than ReLU, GELU, and SwiGLU while enabling weight-based interpretability through eigen-decomposition

## Executive Summary
This paper introduces Signed Quadratic Shrink (SQS), an activation function for Gated Linear Units (GLUs) that enables weight-based interpretability while maintaining competitive performance with state-of-the-art activation functions. The core method idea is to modify the quadratic activation function with a signed and shrunk formulation that preserves the bilinear weight structure needed for eigen-decomposition-based interpretability. The primary results show that SQS-GLUs achieve performance on par with or better than ReLU, GELU, and SwiGLU on MNIST, Fashion-MNIST, and Tiny Stories datasets, while generating interpretable eigenvectors through eigen-decomposition that reveal meaningful features corresponding to different classes.

## Method Summary
The Signed Quadratic Shrink (SQS) activation function modifies the standard quadratic activation by incorporating signed values and a shrinkage parameter. This design preserves the bilinear weight structure of GLUs while enabling eigen-decomposition for interpretability. The SQS-GLU maintains the gated mechanism of traditional GLUs but replaces the standard activation with SQS, which allows for the decomposition of weight matrices into interpretable eigenvectors. The method was evaluated across three datasets (MNIST, Fashion-MNIST, and Tiny Stories) and compared against standard activations including ReLU, GELU, and SwiGLU.

## Key Results
- SQS-GLUs show faster convergence and lower loss compared to ReLU and bilinear MLPs
- Final accuracy comparable to GELU and SwiGLU while maintaining weight structure for interpretability
- Eigen-decomposition reveals meaningful features corresponding to different classes, enabling mechanistic interpretability

## Why This Works (Mechanism)
The SQS activation function preserves the bilinear weight structure of GLUs by maintaining the necessary mathematical properties for eigen-decomposition. The signed component allows for directional information preservation, while the shrinkage parameter controls the magnitude of activations to prevent exploding values. This combination enables the weight matrices to be decomposed into interpretable eigenvectors that correspond to meaningful features in the data. The gating mechanism in GLUs naturally separates information flow, and SQS maintains this separation while adding interpretability through its structured output.

## Foundational Learning
1. **Gated Linear Units (GLUs)** - Why needed: Core architecture for the method; quick check: Understand how gating separates information flow
2. **Eigen-decomposition** - Why needed: Basis for interpretability claims; quick check: Can decompose symmetric matrices into eigenvectors
3. **Activation function design** - Why needed: SQS modifies standard activations; quick check: Understand how different activations affect learning dynamics
4. **Bilinear weight structures** - Why needed: Preserved by SQS for interpretability; quick check: Can identify bilinear forms in matrix operations
5. **Mechanistic interpretability** - Why needed: Primary motivation for method; quick check: Understand feature attribution methods
6. **Quadratic activation functions** - Why needed: Basis for SQS modification; quick check: Can compute derivatives and properties of quadratic functions

## Architecture Onboarding
**Component Map**: Input -> SQS-GLU Layer -> Linear Layer -> Output
**Critical Path**: Data flows through SQS-GLU where gating and signed quadratic activation occur, then to subsequent layers for classification
**Design Tradeoffs**: The method trades some representational flexibility for interpretability by constraining the activation function, though results show this doesn't hurt performance on tested datasets
**Failure Signatures**: Poor performance likely indicates the shrinkage parameter is too aggressive or the signed component isn't preserving necessary information
**First Experiments**: 1) Train SQS-GLU on MNIST and visualize eigenvectors 2) Compare convergence curves with ReLU baseline 3) Vary shrinkage parameter to find optimal value

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Evaluation limited to relatively small-scale problems (MNIST, Fashion-MNIST, Tiny Stories)
- Interpretability claims lack systematic validation across diverse data types and model depths
- Trade-off between maintaining weight structure for interpretability and achieving state-of-the-art performance remains unclear
- Computational overhead of eigen-decomposition for interpretability purposes not quantified

## Confidence
- **High**: SQS-GLUs preserve weight structure enabling eigen-decomposition-based interpretability
- **Medium**: SQS-GLUs achieve competitive performance with SOTA activations on tested datasets
- **Low**: Practical utility and scalability of the interpretability approach across diverse tasks

## Next Checks
1. Evaluate SQS-GLUs on ImageNet or other large-scale vision datasets to assess scalability and performance trade-offs
2. Quantify the computational overhead of eigen-decomposition for interpretability and measure its impact on training/inference efficiency
3. Conduct ablation studies removing the interpretability components to isolate their contribution to performance versus pure activation function benefits