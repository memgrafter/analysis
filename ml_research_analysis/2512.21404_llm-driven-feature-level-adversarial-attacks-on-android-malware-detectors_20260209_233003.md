---
ver: rpa2
title: LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors
arxiv_id: '2512.21404'
source_url: https://arxiv.org/abs/2512.21404
tags:
- malware
- adversarial
- android
- detection
- lamlad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAMLAD, a novel adversarial attack framework
  that exploits the generative and reasoning capabilities of Large Language Models
  (LLMs) to bypass ML-based Android malware classifiers. LAMLAD employs a dual-agent
  architecture composed of an LLM manipulator, which generates realistic and functionality-preserving
  feature perturbations, and an LLM analyzer, which guides the perturbation process
  toward successful evasion.
---

# LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors
## Quick Facts
- arXiv ID: 2512.21404
- Source URL: https://arxiv.org/abs/2512.21404
- Reference count: 40
- Primary result: LAMLAD achieves 97% attack success rate against Android malware classifiers with ~3 attempts per sample

## Executive Summary
This paper introduces LAMLAD, a novel adversarial attack framework leveraging Large Language Models (LLMs) to bypass ML-based Android malware classifiers. LAMLAD employs a dual-agent architecture: an LLM manipulator that generates realistic, functionality-preserving feature perturbations, and an LLM analyzer that guides the attack process toward evasion. The framework integrates retrieval-augmented generation (RAG) to improve efficiency and contextual awareness. Experiments demonstrate LAMLAD's high effectiveness, efficiency, and adaptability in practical adversarial settings.

## Method Summary
LAMLAD introduces a dual-agent architecture exploiting LLM generative and reasoning capabilities for feature-level adversarial attacks on Android malware classifiers. The LLM manipulator generates realistic, functionality-preserving feature perturbations, while the LLM analyzer guides the perturbation process toward successful evasion. The framework integrates retrieval-augmented generation (RAG) to enhance efficiency and contextual awareness. Experiments focus on Drebin-style feature representations, demonstrating high attack success rates and efficiency against widely deployed Android malware detection systems.

## Key Results
- LAMLAD achieves 97% attack success rate against Android malware classifiers
- Average of only three attempts required per adversarial sample
- Adversarial training-based defense reduces ASR by more than 30% on average

## Why This Works (Mechanism)
LAMLAD exploits the generative and reasoning capabilities of LLMs to create realistic, functionality-preserving feature perturbations that evade ML-based Android malware classifiers. The dual-agent architecture, combined with RAG integration, enables efficient and contextually aware attacks. By focusing on high-level, app-summary features (e.g., the 8 Drebin categories), LAMLAD abstracts away binary modification and code injection complexities, allowing for stealthy attacks against static feature-based detectors.

## Foundational Learning
- Large Language Models (LLMs): Deep learning models trained on vast text corpora to understand and generate human-like text. Why needed: Core component for generating realistic feature perturbations and guiding the attack process. Quick check: Evaluate LLM's ability to maintain semantic coherence while modifying features.
- Retrieval-Augmented Generation (RAG): Technique combining information retrieval with text generation to improve contextual awareness and accuracy. Why needed: Enhances LLM's efficiency and effectiveness by providing relevant context during perturbation generation. Quick check: Measure impact of RAG on attack success rate and efficiency.
- Adversarial Training: Defense strategy involving training models on adversarial examples to improve robustness. Why needed: Proposed countermeasure to mitigate LAMLAD-style attacks. Quick check: Evaluate reduction in attack success rate after adversarial training.

## Architecture Onboarding
Component map: LLM Manipulator -> LLM Analyzer -> RAG Integration -> Drebin-Style Features -> Android Malware Classifier
Critical path: Perturbation generation (LLM Manipulator) → Attack guidance (LLM Analyzer) → Contextual enhancement (RAG) → Feature modification → Classifier evasion
Design tradeoffs: Focus on static feature representations vs. inclusion of dynamic features; abstraction of binary modification complexities vs. potential overestimation of attack efficacy
Failure signatures: Degradation in attack success rate when incorporating dynamic runtime features; reduced effectiveness of proposed defense against alternative defense strategies
First experiments:
1. Test LAMLAD against detectors incorporating dynamic runtime features (API call sequences, control-flow graphs)
2. Evaluate alternative defense mechanisms (e.g., ensemble adversarial training, input preprocessing, runtime anomaly detection)
3. Conduct ablation study to quantify impact of RAG component on attack success rate and efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Simplified threat model assuming API-only features, potentially overestimating attack efficacy in real-world scenarios
- Defense evaluation relies on single adversarial training strategy with limited hyper-parameter tuning
- No ablation study isolating contribution of RAG component versus LLM reasoning alone

## Confidence
High: Technical feasibility of dual-agent LLM manipulation framework and claimed attack success rate under stated experimental conditions
Medium: Practical stealthiness and transferability of adversarial samples, given constrained feature set and absence of real-world validation
Low: General robustness of proposed defense, evaluated only in narrow, synthetic adversarial training setup

## Next Checks
1. Test LAMLAD against detectors that incorporate dynamic runtime features (API call sequences, control-flow graphs) to measure robustness degradation.
2. Evaluate alternative defense mechanisms (e.g., ensemble adversarial training, input preprocessing, runtime anomaly detection) to compare effectiveness against LAMLAD-style attacks.
3. Conduct an ablation study to quantify the impact of the RAG component on attack success rate and efficiency.