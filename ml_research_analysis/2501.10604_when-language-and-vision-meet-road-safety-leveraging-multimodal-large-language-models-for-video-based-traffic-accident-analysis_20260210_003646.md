---
ver: rpa2
title: 'When language and vision meet road safety: leveraging multimodal large language
  models for video-based traffic accident analysis'
arxiv_id: '2501.10604'
source_url: https://arxiv.org/abs/2501.10604
tags:
- traffic
- video
- visual
- object
- road
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SeeUnsafe, a framework that leverages multimodal
  large language models (MLLMs) to automate traffic accident analysis from video footage.
  The key innovation is a severity-based aggregation strategy that splits long videos
  into clips, processes them in parallel, and classifies events based on the most
  severe clip.
---

# When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis

## Quick Facts
- arXiv ID: 2501.10604
- Source URL: https://arxiv.org/abs/2501.10604
- Authors: Ruixuan Zhang; Beichen Wang; Juexiao Zhang; Zilin Bian; Chen Feng; Kaan Ozbay
- Reference count: 40
- Primary result: SeeUnsafe achieves 76.31% video classification accuracy and 51.47% visual grounding success rate on Toyota WTS dataset

## Executive Summary
This paper proposes SeeUnsafe, a framework that leverages multimodal large language models (MLLMs) to automate traffic accident analysis from video footage. The key innovation is a severity-based aggregation strategy that splits long videos into clips, processes them in parallel, and classifies events based on the most severe clip. The framework uses structured multimodal prompts to guide MLLMs in generating responses and visual prompts for fine-grained object identification. A new Information Matching Score (IMS) metric is introduced to evaluate response quality beyond traditional NLP metrics. Experiments on the Toyota Woven Traffic Safety dataset show that SeeUnsafe achieves 76.31% video classification accuracy and 51.47% success rate in visual grounding, outperforming existing models. The framework demonstrates strong potential for scalable, automated traffic safety analysis.

## Method Summary
SeeUnsafe processes traffic videos by first splitting them into short clips and applying visual prompts (object contours from segmentation) to aid object grounding. The framework uses a severity-based aggregation strategy where each clip is classified independently and the final video label is determined by the most severe classification (Collision > Near-miss > Normal). For visual grounding, the system uses object segmentation masks as visual prompts to help MLLMs identify specific road users involved in accidents. The framework employs structured multimodal prompts to guide MLLMs in generating structured responses and introduces an Information Matching Score (IMS) metric for evaluating response quality. The system was tested on the Toyota Woven Traffic Safety dataset using 249 scenarios, processing videos as 9 uniformly sampled frames split into 3 clips of 3 frames each.

## Key Results
- SeeUnsafe achieves 76.31% video classification accuracy on the Toyota WTS dataset
- Visual grounding success rate reaches 51.47% with visual prompt augmentation
- IMS metric effectively identifies semantic errors missed by traditional BLEU/ROUGE scores
- Severity-based aggregation successfully handles long videos while preserving critical event detection
- Visual prompts improve grounding for GPT-4o but degrade performance for smaller models in nighttime conditions

## Why This Works (Mechanism)

### Mechanism 1: Severity-Based Aggregation for Long-Context Handling
The framework manages long traffic videos without exceeding the context window of Multimodal Large Language Models (MLLMs) by processing short clips and aggregating results based on a severity hierarchy. The system splits a video V into K clips. Each clip is classified independently (Normal=0, Near-miss=1, Collision=2). The final video label is determined by taking the maximum value across all clips (Ŷ = arg max ({ŷ_k}_{k=1}^K)). This ensures that rare, high-severity events are not "voted out" by long periods of normal activity.

### Mechanism 2: Visual Prompt Augmentation for Object Grounding
Overlaying object contours (masks) onto video frames enables MLLMs to perform fine-grained object identification (visual grounding) which they otherwise struggle with. A pre-processing pipeline uses GroundingDINO and Segment Anything (SAM) to detect and segment objects (cars, pedestrians). These segmentations are converted into "visual prompts" (colored contours with IDs) and burned onto the video frames. The MLLM then associates textual descriptions with these visible IDs.

### Mechanism 3: Structured Information Matching Score (IMS)
Standard NLP metrics (BLEU/ROUGE) are insufficient for safety-critical evaluation because they miss semantic errors; a structured MLLM-based evaluator provides better alignment. Instead of n-gram matching, the framework uses an "Evaluator" MLLM agent (GPT-4o) prompted to score the semantic alignment of ground truth vs. prediction across three structured attributes: Scene Context, Object Description, and Justification.

## Foundational Learning

- **Visual Grounding:** The framework's secondary goal is identifying *which* road users were involved in the accident. You must understand that Grounding refers to localizing a specific text entity (e.g., "the white sedan") within the visual data (bounding box or mask). Quick check: If the model describes "a collision between a truck and a bike," can it return the coordinates or ID of the bike?

- **Open-Vocabulary Object Detection:** The visual prompt generation relies on GroundingDINO, an open-vocabulary detector. Unlike fixed-class detectors (COCO), it detects objects based on language prompts. Quick check: Why is an open-vocabulary detector preferred for traffic analysis over a standard YOLOv5 model trained on specific car types?

- **Structured Prompting (JSON/Schema enforcement):** The framework requires the MLLM to output data in a specific format (Class, Context, Object, Justification) to enable database querying. Quick check: How does constraining the LLM output to a JSON structure differ from asking it to "describe the image"?

## Architecture Onboarding

- **Component map:** Input: Traffic Video (N frames) → Pre-processor: GroundingDINO + SAM → Visual Prompts (Masks/Contours) → Splitter: Chunks video into K clips → MLLM Agent (GPT-4o): Processes clips + Visual Prompts → Structured Text Output → Aggregator: Applies Severity-based logic → Final Verdict → Evaluator (Optional): GPT-4o Agent → IMS Score

- **Critical path:** The quality of the Visual Prompt generation. If the detection/tracking fails (e.g., occlusion at intersections), the visual prompts are missing or incorrect, and the MLLM cannot ground the objects, regardless of its reasoning power.

- **Design tradeoffs:** Context Length vs. Granularity: Using K clips (3 frames each) preserves context but requires K expensive LLM calls. Noise vs. Guidance: Visual prompts (contours) aid grounding but introduce visual noise. Table 5 shows this hurts performance in low-light (nighttime) conditions compared to vanilla processing.

- **Failure signatures:** The "Vanishing Accident": If a near-miss happens in a single frame and the uniform sampling misses it, the severity aggregator returns "Normal." Modal Collapse in Evaluation: IMS might give high scores to plausible but incorrect justifications if the evaluator temperature is not calibrated. Hallucinated Interaction: The MLLM may classify a "Normal" video as "Collision" simply because cars are close (spatial proximity), ignoring motion vectors.

- **First 3 experiments:** Ablation on Clip Length: Vary K (e.g., 1, 3, 5 clips) to find the optimal trade-off between temporal context retention and processing cost. Visual Prompt Noise Test: Run classification on daytime vs. nighttime videos with and without object-boundary visual prompts to quantify the noise sensitivity. Metric Alignment Check: Manually review 20 samples where BLEU is high but IMS is low to verify that IMS is actually catching semantic errors (e.g., "car" vs "truck") as claimed.

## Open Questions the Paper Calls Out

- Can advanced keyframe sampling or uncertainty-aware aggregation improve classification accuracy compared to the current uniform splitting and severity-based methods? The current uniform video splitting and severity-based aggregation "can benefit from advanced keyframe sampling and uncertainty-aware aggregation to handle varying accident rates and reduce output noise."

- To what extent does fusing multi-view footage (e.g., overhead surveillance and in-vehicle dashcams) improve visual grounding accuracy by resolving occlusions inherent in single-view inputs? "Single-view input is prone to occlusion and can be enriched with multi-view footage from traffic or in-vehicle cameras."

- Can domain-specific fine-tuning with human feedback enhance the robustness of smaller MLLMs (e.g., GPT-4o mini) against the noise introduced by visual prompts? "MLLM performance could be enhanced by fine-tuning with customized datasets and incorporating human feedback tailored to traffic accident analysis tasks."

## Limitations
- Severity-based aggregation relies on clip-level accuracy and may miss events occurring in single frames between uniform sampling points
- Visual prompt approach shows significant performance degradation in nighttime conditions, limiting generalizability across lighting conditions
- IMS metric depends on GPT-4o's evaluation consistency without independent validation of its reliability across different accident scenarios

## Confidence
- **High Confidence:** Classification accuracy (76.31%) and visual grounding success rate (51.47%) on the Toyota WTS dataset are directly measurable and reproducible results
- **Medium Confidence:** The severity-based aggregation mechanism is theoretically sound but lacks extensive ablation studies showing optimal clip numbers or handling of edge cases
- **Low Confidence:** The IMS metric's superiority over traditional metrics is demonstrated through limited examples without systematic validation across diverse accident types

## Next Checks
1. **Ablation on Clip Length and Aggregation Strategy:** Systematically vary the number of clips (1, 3, 5, 9 frames) and test alternative aggregation methods (average severity, weighted voting) to validate that the max-severity approach is optimal
2. **Independent Evaluation of IMS Metric:** Have human annotators score the same samples used for IMS calculation to verify that the MLLM evaluator consistently identifies semantic errors that traditional metrics miss
3. **Cross-Dataset Generalization Test:** Evaluate SeeUnsafe on at least one external traffic dataset (different camera angles, weather conditions, or geographic locations) to assess real-world applicability beyond the Toyota WTS dataset