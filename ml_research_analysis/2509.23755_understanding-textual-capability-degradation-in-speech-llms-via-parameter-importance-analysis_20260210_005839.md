---
ver: rpa2
title: Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance
  Analysis
arxiv_id: '2509.23755'
source_url: https://arxiv.org/abs/2509.23755
tags:
- speech
- textual
- importance
- parameter
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The integration of speech capabilities into large language models\
  \ often degrades their original textual competence, limiting their effectiveness\
  \ in spoken question answering tasks. This study identifies that speech fine-tuning\
  \ disrupts the layer-wise distribution of parameters critical to textual reasoning\u2014\
  a phenomenon termed \"textual importance distribution shift.\" To address this,\
  \ the authors propose two strategies: layer-wise learning rate scheduling, which\
  \ preserves important layers by reducing their update magnitude, and Low-Rank Adaptation\
  \ (LoRA), which constrains updates within a low-rank subspace aligned with the model's\
  \ inherent knowledge structure."
---

# Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis

## Quick Facts
- **arXiv ID:** 2509.23755
- **Source URL:** https://arxiv.org/abs/2509.23755
- **Reference count:** 0
- **Key outcome:** Speech fine-tuning degrades textual reasoning in LLMs due to "textual importance distribution shift," with LoRA and layer-wise learning rates effectively preserving textual competence while improving speech comprehension

## Executive Summary
The integration of speech capabilities into large language models often degrades their original textual competence, limiting their effectiveness in spoken question answering tasks. This study identifies that speech fine-tuning disrupts the layer-wise distribution of parameters critical to textual reasoning—a phenomenon termed "textual importance distribution shift." To address this, the authors propose two strategies: layer-wise learning rate scheduling, which preserves important layers by reducing their update magnitude, and Low-Rank Adaptation (LoRA), which constrains updates within a low-rank subspace aligned with the model's inherent knowledge structure. Experimental results on spoken QA benchmarks show that both methods better maintain textual competence than full fine-tuning while improving speech comprehension. LoRA achieves the highest spoken QA accuracy (42.9% on Web Questions for the 8B model), while layer-wise scheduling better preserves textual accuracy. The findings highlight the importance of preserving foundational textual knowledge for effective speech adaptation.

## Method Summary
The authors analyze parameter importance distribution shifts during speech fine-tuning by measuring layer-wise sensitivity to task performance. They identify that speech adaptation disproportionately updates parameters critical for textual reasoning, causing degradation. To mitigate this, they propose two complementary approaches: (1) Layer-wise learning rate scheduling that applies smaller updates to text-critical layers, and (2) LoRA-based adaptation that confines updates to low-rank subspaces, preserving the model's original knowledge structure. Both methods are evaluated against full fine-tuning baselines across multiple spoken QA benchmarks, with comprehensive analysis of their impact on both speech comprehension and textual reasoning preservation.

## Key Results
- Speech fine-tuning causes significant degradation of textual reasoning capabilities in LLMs due to parameter importance distribution shifts
- LoRA achieves the highest spoken QA accuracy (42.9% on Web Questions for the 8B model) while maintaining textual competence
- Layer-wise learning rate scheduling better preserves textual accuracy compared to LoRA while still improving speech comprehension
- Both proposed methods outperform full fine-tuning in balancing speech adaptation with textual knowledge preservation

## Why This Works (Mechanism)
The degradation occurs because speech fine-tuning disrupts the layer-wise distribution of parameters that are critical for textual reasoning. When adapting LLMs to speech tasks, standard fine-tuning updates all parameters uniformly, causing important textual reasoning parameters to be overwritten. The proposed solutions work by either selectively protecting important layers (layer-wise scheduling) or constraining updates to low-rank subspaces that preserve the model's inherent knowledge structure (LoRA), thereby maintaining the critical parameter distributions needed for textual competence.

## Foundational Learning
- **Parameter importance distribution**: Understanding which parameters contribute most to specific tasks; needed to identify which layers should be protected during fine-tuning; quick check: analyze layer-wise sensitivity scores
- **Layer-wise sensitivity analysis**: Measuring how parameter updates affect task performance across different layers; needed to quantify textual importance distribution shifts; quick check: compute gradient-based importance scores
- **Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that updates parameters within a low-rank subspace; needed to constrain updates while preserving knowledge structure; quick check: verify rank reduction in adaptation matrices
- **Text-speech modality integration**: Understanding how language models process and integrate different input modalities; needed to design effective speech adaptation strategies; quick check: evaluate cross-modal attention patterns
- **Fine-tuning stability**: The ability of adapted models to maintain performance on original tasks; needed to measure the effectiveness of preservation strategies; quick check: compare pre/post-adaptation performance on textual benchmarks

## Architecture Onboarding
- **Component map**: Input speech/audio features -> Speech encoder -> LLM backbone -> Text generation output
- **Critical path**: Speech input → Feature extraction → Cross-modal fusion → Reasoning layers → Answer generation
- **Design tradeoffs**: Full fine-tuning offers maximum adaptation flexibility but causes capability degradation; LoRA provides parameter efficiency but may limit adaptation capacity; layer-wise scheduling balances preservation and adaptation
- **Failure signatures**: Rapid performance drop on textual tasks, inconsistent layer-wise parameter updates, loss of cross-modal reasoning capabilities
- **First experiments**: 1) Measure parameter importance distribution before and after speech fine-tuning, 2) Compare layer-wise sensitivity across adaptation methods, 3) Evaluate spoken QA performance while tracking textual reasoning capability

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses on specific speech-LLM architectures and may not generalize to all multimodal models
- Parameter importance analysis relies on layer-wise sensitivity, potentially missing complex cross-layer interactions
- Only immediate performance impacts are evaluated, with no investigation into long-term stability of preserved textual knowledge

## Confidence
- **High confidence**: The existence of textual competence degradation during speech adaptation and the effectiveness of LoRA in maintaining speech comprehension while preserving textual capabilities
- **Medium confidence**: The causal relationship between parameter importance distribution shifts and capability degradation, as this inference relies on indirect evidence
- **Medium confidence**: The superiority of layer-wise learning rate scheduling for preserving textual accuracy, as this result is based on a limited set of experiments

## Next Checks
1. Conduct ablation studies across different speech-LLM architectures (including non-auditory and multimodal models) to verify the generalizability of the textual importance distribution shift phenomenon
2. Perform long-term stability analysis by evaluating models after extended periods of deployment to assess whether preserved textual knowledge remains stable
3. Test the proposed methods on additional downstream tasks beyond spoken question answering to validate their effectiveness across diverse application scenarios