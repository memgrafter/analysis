---
ver: rpa2
title: Computational Basis of LLM's Decision Making in Social Simulation
arxiv_id: '2504.11671'
source_url: https://arxiv.org/abs/2504.11671
tags:
- social
- steering
- page
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a method for probing, quantifying, and manipulating
  how social concepts are encoded within transformer-based LLMs. The approach extracts
  vectors representing variable variations (e.g., male-to-female, age ranges) from
  the model's internal residual stream, orthogonalizes them to isolate unique effects,
  and injects controlled perturbations to steer decision-making.
---

# Computational Basis of LLM's Decision Making in Social Simulation

## Quick Facts
- arXiv ID: 2504.11671
- Source URL: https://arxiv.org/abs/2504.11671
- Reference count: 27
- This study introduces a method for probing, quantifying, and manipulating how social concepts are encoded within transformer-based LLMs.

## Executive Summary
This study introduces a method for probing, quantifying, and manipulating how social concepts are encoded within transformer-based LLMs. The approach extracts vectors representing variable variations (e.g., male-to-female, age ranges) from the model's internal residual stream, orthogonalizes them to isolate unique effects, and injects controlled perturbations to steer decision-making. In a Dictator Game experiment, this method successfully identified internal representations of social factors like gender, age, and framing, and manipulated the model's decisions with high precision. For instance, 320 out of 1,891 manipulations significantly influenced the likelihood of transferring money, with 98.44% showing the expected positive effect. The study demonstrates that social meanings can be treated as measurable, manipulable directions in LLM activation space, enabling virtual experiments and controlled behavioral interventions. This framework advances computational social science by offering a principled way to study and regulate social concept encoding, with applications in bias mitigation, AI alignment, and designing interpretable AI agents for social simulations.

## Method Summary
The method extracts "vectors of variable variation" from the residual stream by computing mean differences between contrasting conditions (e.g., female vs. male). These steering vectors are orthogonalized to isolate unique effects of each variable. The partial steering vectors are projected onto the decision vector and injected at controlled strengths (α) into layers 1-31 of Llama3.1-8B during inference. This manipulates the model's internal representations to steer decisions in the Dictator Game, with effects measured through logistic regression coefficients on transfer amounts.

## Key Results
- The method successfully extracted and manipulated internal representations of social factors (gender, age, framing) in an LLM playing a Dictator Game
- 320 out of 1,891 manipulations significantly influenced the likelihood of transferring money, with 98.44% showing the expected positive effect
- The framework demonstrates that social meanings can be treated as measurable, manipulable directions in LLM activation space

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Social variables (gender, age, framing) can be extracted as vectors from the residual stream that causally influence decision outputs.
- **Mechanism:** The method computes the difference between mean residual-stream activations at layer ℓ for contrasting conditions (e.g., female vs. male), yielding a "vector of variable variation" that encodes the concept's direction and magnitude in activation space. This exploits the approximately additive structure of residual updates.
- **Core assumption:** Social concepts are encoded as approximately linear directions in the residual stream (linear representation hypothesis), and mean-difference estimation recovers them.
- **Evidence anchors:**
  - [abstract] "extracts vectors representing variable variations (e.g., male-to-female, age ranges) from the model's internal residual stream"
  - [section 1.3.2] "v_age reflects how shifting from age 20 to age 40 changes the internal representation on average, after 'averaging out' any incidental effects"
- **Break condition:** If social concepts are represented nonlinearly or distributed across non-orthogonal subspaces, mean-difference vectors will not isolate clean effects; orthogonalization may fail or produce misleading directions.

### Mechanism 2
- **Claim:** Orthogonalizing steering vectors isolates each variable's unique contribution to decisions.
- **Mechanism:** By projecting out components of one IV's steering vector that align with others (e.g., removing age-correlated components from gender), the method generates "partial steering vectors" that—under the assumption of linear independence—capture only the target variable's effect.
- **Core assumption:** IV steering vectors are approximately linearly decomposable; their influence on decisions adds independently.
- **Evidence anchors:**
  - [abstract] "orthogonalizes them to isolate unique effects"
  - [section 2.2.2] "By subtracting the component of v_X1 aligned with v_X2, we isolate the 'pure' effect of X1"
- **Break condition:** If variables interact nonlinearly in the residual stream (e.g., gender effects differ by age layer-wise), orthogonalization will misattribute variance and produce spurious "pure" effects.

### Mechanism 3
- **Claim:** Injecting scaled projections of partial IV vectors onto the DV direction steers decisions with high directional accuracy.
- **Mechanism:** The method projects a partial steering vector onto the decision vector (DV), then injects α·projection into the residual stream. Because residual updates accumulate additively through layers, this nudges the internal representation toward higher or lower D values.
- **Core assumption:** The projection onto the DV direction captures the causal pathway from IV to decision; injecting it produces controlled shifts without corrupting other computations.
- **Evidence anchors:**
  - [abstract] "320 out of 1,891 manipulations significantly influenced the likelihood of transferring money, with 98.44% showing the expected positive effect"
  - [section 2.5.2] "er^(ℓ)(x) = r^(ℓ)(x) + α·Proj_D(v^(ℓ)_X|{others})"
- **Break condition:** Large |α| or injection at unstable layers can push activations off-manifold, causing incoherent outputs or model collapse. The paper notes |α| > 15 risks nonsensical outputs (citing Turner et al., 2024); the study explores up to |α| = 30.

## Foundational Learning

- **Concept:** Residual Stream Architecture
  - **Why needed here:** The entire method operates by reading from and writing to residual streams. Without understanding their additive, layer-wise structure, you cannot reason about where and how to inject steering vectors.
  - **Quick check question:** In a 32-layer decoder-only transformer, if you add a vector Δ to the residual stream at layer 10, which layers "see" Δ in their input?

- **Concept:** Vector Projection and Orthogonalization
  - **Why needed here:** Isolating variable effects requires subtracting shared components (projection) and measuring alignment with decision directions (dot product/cosine similarity).
  - **Quick check question:** Given vectors a and b, what does (a · b) / ‖b‖² compute, and what does its sign tell you?

- **Concept:** Internal vs. External Validity
  - **Why needed here:** The paper explicitly limits claims to internal representational validity. Confusing this with external sociological validity leads to overclaiming that steering vectors reflect human cognition.
  - **Quick check question:** If a "female" steering vector increases prosocial transfers in an LLM, does this prove human women are more prosocial? Why or why not?

## Architecture Onboarding

- **Component map:**
  Tokenizer → Embedding Layer → 32 Decoder Layers (each: Self-Attention → MLP → Residual Add) → lm_head (logits)
  Steering intervention point: residual stream at any layer ℓ ∈ {1,...,31} (excludes final layer)

- **Critical path:**
  1. Run baseline trials, record residual streams at all layers for each prompt configuration
  2. Compute IV steering vectors via mean-difference per layer
  3. Orthogonalize IV vectors to produce partial steering vectors
  4. Compute DV steering vector (D=10 vs. D=0 contrast)
  5. Project partial IV vectors onto DV; inject α·projection during inference

- **Design tradeoffs:**
  - Earlier-layer injection (ℓ < 10): larger downstream effects but less precise; more likely to affect reasoning, style, or coherence
  - Later-layer injection (ℓ > 20): more localized to decision output but weaker effect magnitude
  - Injecting full partial vector vs. projection: projection is more precise (affects only DV-aligned direction) but may miss IV-relevant side effects; full vector may add noise

- **Failure signatures:**
  - Model outputs become incoherent or break template format → |α| too large or layer choice unstable
  - Manipulation affects non-target IVs (e.g., gender injection shifts Meet-Stranger coefficient) → incomplete orthogonalization; IVs share subspaces
  - Regression coefficient sign flips unexpectedly → residual-stream geometry may be non-locally-linear; re-extract vectors with more trials or different anchor points

- **First 3 experiments:**
  1. **Sanity check:** Extract and inject a known strong variable (Give vs. Take framing) at middle layer (ℓ=15) with α ∈ {±5, ±10}. Verify that transfers increase/decrease as expected.
  2. **Layer sweep:** For a single IV (e.g., gender), inject at layers 5, 10, 15, 20, 25 with fixed α=10. Plot regression coefficient vs. layer to identify sensitive layers.
  3. **Orthogonality test:** Inject gender vector; check whether Age, Give-Take, and Meet-Stranger regression coefficients remain stable. If Meet-Stranger shifts, investigate shared subspace and re-orthogonalize with additional controls.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the internal representations of social concepts in LLMs correspond to human cognitive schemas and behavioral patterns?
- Basis in paper: [explicit] The paper repeatedly states it "does not, on its own, establish external sociological validity (i.e., accurately representing human cognition and behavior), which requires separate comparative validation with human baselines."
- Why unresolved: The study focuses exclusively on internal representational validity within the model's activation space; human-LLM comparison experiments were outside its scope.
- What evidence would resolve it: Systematic experiments comparing steering vector effects on LLM behavior against human subject responses in identical experimental conditions (e.g., Dictator Games with matched demographic manipulations).

### Open Question 2
- Question: Why do 98.44% of significant manipulations produce positive regression coefficients even when injection coefficients are negative?
- Basis in paper: [explicit] The authors note "we would expect a stronger positive correlation between the injection coefficient and the resulting regression coefficient, but the observed correlation is minuscule (|r|<0.05)" and state "the model's internal representations involve complex interactions that are not fully captured by the current manipulation framework."
- Why unresolved: The dissociation between expected and observed effects suggests nonlinear or context-dependent dynamics in how steering vectors propagate through transformer layers.
- What evidence would resolve it: Layer-by-layer tracing of steering vector transformations; ablation studies isolating specific attention heads or MLP components.

### Open Question 3
- Question: How does the interaction between gender and social-context variables (e.g., Meet-Stranger) emerge in LLM representations?
- Basis in paper: [explicit] The orthogonality analysis found that "the coefficient of the Meet-Stranger variable is more frequently influenced by the manipulation of the Female variable," suggesting the model may capture documented gender-context interplay, but this interaction is not fully characterized.
- Why unresolved: While the paper demonstrates orthogonalization, some variable pairs show unexpected coupling patterns whose origins remain unclear.
- What evidence would resolve it: Joint manipulation experiments varying both variables simultaneously; analysis of attention patterns linking gender and social distance tokens.

### Open Question 4
- Question: How portable are steering vectors across different LLM architectures (e.g., dense vs. mixture-of-experts models)?
- Basis in paper: [explicit] The paper notes "portability does not imply uniform effects" and discusses how "models vary in layer depth, hidden dimensionality, normalization, gating, and training data," specifically citing DeepSeek-R1's MoE architecture as potentially altering "sparsity and local subspace structure."
- Why unresolved: All experiments used Llama3.1-8B; no cross-model validation was conducted.
- What evidence would resolve it: Replication of the Dictator Game steering experiments across multiple model families with re-estimated steering vectors, comparing effect magnitudes and layer sensitivity.

## Limitations
- The method assumes social concepts are encoded as approximately linear directions in residual stream space, which may not hold for complex or intersectional social constructs
- The approach is validated on a single LLM (Llama3.1-8B) and a specific social scenario (Dictator Game), limiting generalizability
- While demonstrating internal representational validity, the study does not establish external validity to human cognition or real-world social behavior

## Confidence

**High Confidence:** The computational method for extracting and injecting residual stream vectors is technically sound and reproducible. The 98.44% directional accuracy in steering experiments demonstrates precise control over internal representations.

**Medium Confidence:** The claim that social meanings can be "treated as measurable, manipulable directions" is supported within the LLM context but should not be overgeneralized to human social cognition without additional validation.

**Low Confidence:** Broader claims about "regulating social concept encoding" or applications to "AI alignment" extend beyond the demonstrated scope and require substantially more evidence across diverse models and real-world applications.

## Next Checks
1. **Cross-Model Validation:** Apply the same extraction and injection methodology to different LLM architectures (e.g., GPT, Claude) to test whether social concepts consistently occupy similar directional representations across models.

2. **Intersectionality Stress Test:** Design experiments where multiple social variables interact (e.g., age×gender effects) to verify that orthogonalization correctly isolates unique contributions versus creating spurious dimensions when effects are genuinely intersectional.

3. **External Validity Benchmark:** Compare LLM steering results with established human behavioral patterns from social psychology literature to quantify the alignment between computational social representations and empirically validated human social cognition.