---
ver: rpa2
title: 'BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect
  Sentiment Analysis with Cross-Domain Transfer Learning'
arxiv_id: '2511.23264'
source_url: https://arxiv.org/abs/2511.23264
tags:
- sentiment
- learning
- bangla
- performance
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-aspect sentiment analysis for Bangla
  e-commerce reviews, a challenging task due to limited annotated datasets, morphological
  complexity, code-mixing, and domain shift issues. The proposed BanglaSentNet framework
  integrates LSTM, BiLSTM, GRU, and BanglaBERT models through dynamic weighted ensemble
  learning, incorporating SHAP-based feature attribution and attention visualization
  for transparency.
---

# BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning

## Quick Facts
- arXiv ID: 2511.23264
- Source URL: https://arxiv.org/abs/2511.23264
- Authors: Ariful Islam; Md Rifat Hossen; Tanvir Mahmud
- Reference count: 40
- Primary result: Achieves 85% accuracy and 0.88 F1-score on multi-aspect sentiment analysis for Bangla e-commerce reviews

## Executive Summary
This paper addresses the challenge of multi-aspect sentiment analysis for Bangla e-commerce reviews through the BanglaSentNet framework. The framework tackles the complexities of Bangla language processing, including morphological richness, code-mixing, and limited annotated datasets, while enabling cross-domain transfer learning capabilities. By combining traditional deep learning models (LSTM, BiLSTM, GRU) with transformer-based BanglaBERT through a dynamic weighted ensemble approach, the system achieves state-of-the-art performance while maintaining interpretability through SHAP-based feature attribution and attention visualization.

The research demonstrates that BanglaSentNet not only outperforms standalone models by 3-7% but also exhibits strong cross-domain generalization, maintaining 67-76% effectiveness in zero-shot scenarios and achieving 90-95% of full fine-tuning performance with only 500-1000 labeled samples. The explainability suite achieves high interpretability scores (9.4/10) with substantial human agreement (87.6%), making the framework suitable for practical commercial applications where transparency is essential.

## Method Summary
The BanglaSentNet framework employs a hybrid ensemble learning approach that combines four distinct deep learning architectures: Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Units (GRU), and BanglaBERT. Each model is independently trained on the same annotated dataset of 8,755 Bangla product reviews, then their predictions are dynamically weighted based on their individual performance metrics and domain relevance. The dynamic weighting mechanism adapts to different domains and aspect categories, optimizing the ensemble's overall performance. For explainability, the framework integrates SHAP (SHapley Additive exPlanations) values to quantify feature importance at the token level, while attention mechanisms from transformer models provide visual insights into the model's decision-making process. Cross-domain transfer learning is achieved through domain adaptation techniques that leverage knowledge from the source e-commerce domain to perform effectively on target domains with minimal or no additional labeled data.

## Key Results
- Achieves 85% accuracy and 0.88 F1-score on 8,755 manually annotated Bangla product reviews
- Outperforms standalone models by 3-7% and traditional approaches substantially
- Zero-shot cross-domain transfer learning maintains 67-76% effectiveness across diverse domains
- Few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance

## Why This Works (Mechanism)
The framework's effectiveness stems from leveraging complementary strengths of multiple architectures: LSTMs capture sequential dependencies in text, BiLSTMs provide bidirectional context understanding, GRUs offer efficient gating mechanisms for long-range dependencies, and BanglaBERT brings contextual embeddings pretrained on large-scale Bangla corpora. The dynamic weighted ensemble approach allows the system to adaptively combine these strengths based on domain characteristics and aspect requirements, rather than using fixed weights. The explainability components serve dual purposes: providing transparency for users while also acting as a debugging tool to identify model biases or weaknesses. Cross-domain transfer learning succeeds because sentiment-bearing aspects often share semantic similarities across domains, allowing knowledge transfer while the ensemble's diversity helps adapt to domain-specific nuances.

## Foundational Learning
**LSTM and RNN architectures**: These recurrent networks process sequential data by maintaining hidden states that capture temporal dependencies, essential for understanding the flow of sentiment in text. Why needed: Bangla sentences often express sentiment through complex morphological constructions that require sequential understanding. Quick check: Verify that the model can maintain context over long sequences typical in product reviews.

**Transformer attention mechanisms**: Self-attention allows models to weigh the importance of different words in a sentence relative to each other, capturing long-range dependencies more effectively than RNNs. Why needed: Sentiment can depend on words that are far apart in the text, especially in morphologically rich languages like Bangla. Quick check: Confirm attention visualizations highlight sentiment-relevant words appropriately.

**SHAP value computation**: SHAP values provide game-theoretic attribution of feature importance, offering consistent and locally accurate explanations for individual predictions. Why needed: Commercial applications require transparent decision-making to build user trust and enable error analysis. Quick check: Validate that SHAP explanations align with human understanding of sentiment attribution.

**Cross-domain transfer learning**: This technique enables models trained on one domain to perform well on related but different domains by leveraging shared semantic structures and sentiment patterns. Why needed: Annotation costs are prohibitive, and sentiment aspects often transfer across e-commerce categories. Quick check: Test zero-shot performance on completely unseen domains to verify generalization capability.

## Architecture Onboarding

**Component Map**: Data Preprocessing -> LSTM/BiLSTM/GRU/BanglaBERT Training -> Dynamic Ensemble Weighting -> SHAP Integration -> Attention Visualization -> Final Prediction

**Critical Path**: The core processing pipeline follows: raw Bangla text → preprocessing (tokenization, normalization) → individual model inference → dynamic weight calculation → weighted ensemble aggregation → SHAP explanation generation → attention visualization → final sentiment classification output.

**Design Tradeoffs**: The framework balances model complexity with interpretability by combining black-box transformer models with more transparent RNN architectures. The dynamic weighting adds computational overhead during inference but provides superior performance compared to static ensembles. Using multiple architectures increases memory requirements but improves robustness to domain shifts and aspect variations.

**Failure Signatures**: The system may struggle with extreme code-mixing beyond training distribution, rare aspect categories with insufficient training examples, or domains with fundamentally different sentiment expression patterns. Attention visualizations may become less interpretable for very long reviews, and SHAP explanations may be computationally expensive for real-time applications.

**First 3 Experiments**:
1. **Ablation study**: Train and evaluate each component model (LSTM, BiLSTM, GRU, BanglaBERT) individually to establish baseline performance and identify the strongest standalone performer.
2. **Static vs dynamic weighting**: Compare the proposed dynamic weighted ensemble against static weighting schemes and simple averaging to quantify the benefit of adaptive combination.
3. **Cross-domain validation**: Test the framework on a held-out domain (e.g., social media) that was not seen during training to evaluate zero-shot transfer learning capabilities.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation relies entirely on a single manually annotated dataset of 8,755 Bangla e-commerce reviews, constraining generalizability claims
- Cross-domain transfer learning experiments cover only three target domains beyond the source e-commerce domain
- Explainability assessment based on a small sample of 30 cases rated by 5 annotators may not capture edge cases or diverse user perspectives

## Confidence
- **High confidence** in reported accuracy and F1-score improvements over baseline models (85% accuracy, 0.88 F1-score)
- **Medium confidence** in cross-domain transfer learning effectiveness claims, particularly zero-shot and few-shot learning results
- **Medium confidence** in the explainability assessment, as the 9.4/10 interpretability score and 87.6% human agreement are positive indicators

## Next Checks
1. **Dataset Diversity Validation**: Replicate experiments on multiple independently collected Bangla review datasets spanning at least 5-7 diverse domains to verify cross-domain generalization claims and identify potential domain-specific limitations.

2. **Baseline Methodology Expansion**: Compare BanglaSentNet against contemporary state-of-the-art approaches including multilingual transformer models fine-tuned on Bangla data, prompting-based methods using large language models, and specialized domain adaptation techniques like adversarial training or meta-learning.

3. **Explainability Robustness Testing**: Conduct user studies with 50+ diverse participants including domain experts, general users, and non-Bangla speakers to evaluate the accessibility and utility of SHAP-based explanations and attention visualizations across different user groups and expertise levels.