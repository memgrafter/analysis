---
ver: rpa2
title: A Survey on Large Language Models for Mathematical Reasoning
arxiv_id: '2506.08446'
source_url: https://arxiv.org/abs/2506.08446
tags:
- reasoning
- mathematical
- language
- corr
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of large language models
  for mathematical reasoning, organizing recent advances into a structured framework
  that distinguishes between comprehension (problem understanding) and generation
  (solution synthesis) capabilities. It reviews methods ranging from training-free
  prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement
  learning, and discusses recent work on extended chain-of-thought reasoning and test-time
  scaling.
---

# A Survey on Large Language Models for Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2506.08446
- **Source URL:** https://arxiv.org/abs/2506.08446
- **Reference count:** 28
- **Key outcome:** Comprehensive survey organizing LLM mathematical reasoning advances into structured framework distinguishing comprehension and generation capabilities

## Executive Summary
This survey provides a systematic overview of large language models for mathematical reasoning, organizing recent advances into a structured framework that distinguishes between comprehension (problem understanding) and generation (solution synthesis) capabilities. The paper reviews methods ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discusses recent work on extended chain-of-thought reasoning and test-time scaling. It highlights key findings about the fundamental nature of reasoning capabilities, training paradigm trade-offs, and boundaries of reasoning improvements, while identifying promising research directions including efficient exploration methods, verification-aware optimization, and scalable reward modeling.

## Method Summary
The survey synthesizes existing literature on LLM mathematical reasoning by categorizing methods into training-free approaches (prompting, majority voting, test-time scaling) and training-based approaches (supervised fine-tuning, reinforcement learning). It organizes the discussion around two core capabilities: comprehension (problem understanding, representation learning, symbolic processing) and generation (solution synthesis, intermediate reasoning steps). The methodology involves comprehensive literature review and organization of findings into a coherent framework that maps the relationships between different approaches and their respective strengths and limitations.

## Key Results
- Chain-of-Thought prompting functions as computational depth extension, enabling Transformers to solve serial problems through intermediate reasoning tokens
- Reinforcement learning primarily unlocks latent reasoning capabilities already present in base models rather than creating new fundamental skills
- Test-time scaling through structural search (Tree-of-Thought, MCTS) improves accuracy by exploring solution space and backtracking from errors
- Rule-based rewards provide more stable optimization than learned reward models for mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought (CoT) prompting extends the computational expressiveness of Transformers, allowing them to solve inherently serial problems that direct generation cannot. CoT introduces intermediate reasoning tokens that function as "recurrent states" or memory, effectively increasing the depth of the computation graph. This allows bounded-depth Transformers to solve tasks (like arithmetic) that would otherwise require super-polynomial size direct architectures. The model must have internalized the necessary procedural knowledge during pre-training to generate valid intermediate steps. If problems require non-sequential logic or the model hallucinates invalid intermediate steps, the memory mechanism corrupts the final state.

### Mechanism 2
Reinforcement Learning (RL) with verifiable rewards primarily unlocks and amplifies latent reasoning capabilities already present in the base model, rather than instilling new fundamental skills. RL optimizes the policy to explore diverse reasoning paths (exploration) and reinforces those that yield correct outcomes (exploitation). By using rule-based or outcome-based rewards, the model learns to organize its existing knowledge into coherent, extended chains of thought (System 2 thinking). The base model must possess sufficient "latent knowledge" and capacity to solve the problem, but lacks the policy to consistently retrieve and structure it. If the base model's capability ceiling is low, RL cannot unlock what doesn't exist, and it fails if the reward signal is sparse or hackable.

### Mechanism 3
Test-time scaling via structural search (e.g., Tree-of-Thought, MCTS) improves accuracy by explicitly exploring the solution space and backtracking from errors. Instead of single linear generation, the model generates multiple potential steps or reasoning paths (a tree or graph). A value function or self-evaluation mechanism assesses these nodes, pruning bad paths and prioritizing promising ones. This simulates "deliberation" or "planning." The model (or external verifier) must reliably evaluate intermediate reasoning step quality better than it can spontaneously generate perfect paths. Exponential growth in compute cost for complex problems and unreliable value functions can lead to prioritizing confident but incorrect paths.

## Foundational Learning
- **Autoregressive Language Modeling:** Base functionality of LLMs (predicting next token). All reasoning mechanisms (CoT, RL) are built on this fundamental probability distribution. Quick check: Can you explain why "thinking step by step" changes the conditional probability of the final answer token?
- **Policy Optimization (PPO/GRPO):** The paper highlights RL as key method for "unlocking" reasoning. Understanding how policy is updated using reward signal is crucial for understanding how model learns to "plan" rather than just predict. Quick check: What is the role of the "Critic" or "Value Model" in RL setup, and why might methods like GRPO try to remove it?
- **Circuit Complexity:** To understand why CoT works mechanistically, one must grasp theoretical limits of Transformers and how "depth" (steps) relates to computational power. Quick check: Why does constant-depth Transformer struggle with modular arithmetic without intermediate steps?

## Architecture Onboarding
- **Component map:** Pre-training (Knowledge Acquisition) -> SFT (Alignment/Format) -> RL/RLHF (Reasoning Optimization) -> Inference-time Search (Verification)
- **Critical path:** The Reward Model (or Rule-based Reward). The paper emphasizes that "Golden Rewards" (verifiable truth) are more stable than learned reward models for math
- **Design tradeoffs:**
  - SFT vs. Pure RL: SFT aligns but reduces diversity (causing mode collapse); Pure RL preserves exploration but is computationally expensive and unstable
  - Short vs. Long CoT: Short CoT is cheap but less capable of self-correction; Long CoT allows reflection/backtracking but increases latency and compute cost
- **Failure signatures:**
  - Reward Hacking: Model generates outputs that maximize reward signal but are logically nonsensical (e.g., repeating problem statement to increase length if length is rewarded)
  - Sycophancy: Model generates plausible-sounding reasoning that leads to wrong answer, often difficult to detect without step-level verification
- **First 3 experiments:**
  1. Baseline CoT Evaluation: Run standard LLM on GSM8K with "Let's think step by step" to establish baseline for latent capability without fine-tuning
  2. SFT Data Quality Ablation: Fine-tune small model on synthetic long-CoT data vs. short-CoT data to measure impact of "reasoning density" in training examples
  3. Rule-Based RL Test: Implement simple REINFORCE or GRPO loop with strict "exact match" rule-based reward on small arithmetic dataset to observe if reasoning "emerges" from base model

## Open Questions the Paper Calls Out
- Does reinforcement learning (RL) instill fundamentally new reasoning capabilities in large language models, or does it merely activate latent reasoning strategies already present in the base model? This remains unresolved due to conflicting evidence about whether RL gains result from activating existing capabilities versus creating new ones, limited by base model's Pass@k performance.
- How can we achieve efficient exploration of diverse reasoning pathways within constrained sampling budgets to prevent RL policies from converging to local optima? Standard token-level optimization is computationally expensive and struggles to cover vast solution space of mathematical problems.
- How can "verification-aware optimization" be implemented to ensure long Chain-of-Thought (CoT) reasoning paths are logically consistent and efficient, rather than just producing correct final answer through flawed process? Current outcome-based rewards treat responses as correct regardless of logical errors in intermediate steps.

## Limitations
- The survey's synthesis nature cannot establish causal mechanisms beyond what published papers claim, and empirical comparisons across different approaches are often missing from primary literature
- Significant uncertainty exists regarding the true nature of "reasoning" versus pattern matching - the survey cannot definitively determine whether observed improvements represent genuine logical reasoning capabilities or sophisticated pattern recognition on mathematical text
- The rapid pace of development in this field means that some methods may be superseded by newer approaches between survey completion and publication

## Confidence
- **High Confidence Claims:** Classification framework distinguishing comprehension and generation capabilities; Chain-of-Thought prompting demonstrably improves mathematical reasoning performance; Reinforcement learning methods with verifiable rewards show consistent improvements over SFT alone
- **Medium Confidence Claims:** Assertion that RL primarily unlocks latent knowledge rather than creating new capabilities relies on indirect evidence; Test-time scaling benefits are empirically observed but theoretical foundations remain partially understood; Superiority of rule-based rewards over learned reward models is supported but may be domain-dependent
- **Low Confidence Claims:** Specific architectural recommendations for optimal reasoning performance; Precise quantification of compute-efficiency trade-offs between different approaches; Long-term generalization capabilities beyond training distributions

## Next Checks
1. **Latent Knowledge Isolation Test:** Conduct controlled experiments comparing RL fine-tuning on models with varying initial capabilities (measured by Pass@k on validation sets) to determine whether improvements correlate with pre-existing knowledge rather than skill acquisition
2. **Verification Method Comparison:** Implement and compare multiple reward modeling approaches (rule-based, learned from demonstrations, learned from human feedback) on identical mathematical reasoning tasks to quantify trade-offs between stability, accuracy, and computational overhead
3. **Structural Search Efficiency Analysis:** Systematically measure compute-to-accuracy ratio for different test-time scaling methods (Majority Voting, Tree Search, MCTS) across problem difficulty levels to provide empirical grounding for computational trade-offs