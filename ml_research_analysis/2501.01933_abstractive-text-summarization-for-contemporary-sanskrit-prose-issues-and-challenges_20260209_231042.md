---
ver: rpa2
title: 'Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and
  Challenges'
arxiv_id: '2501.01933'
source_url: https://arxiv.org/abs/2501.01933
tags:
- summarization
- text
- language
- data
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis initiates abstractive text summarization for contemporary
  Sanskrit prose, a low-resource language. It develops datasets and trains ten transformer-based
  models, finding that BERT encoder-based combinations perform best for coherence
  and keyword capture, while RoBERTa and GPT-2 decoders underperform.
---

# Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and Challenges

## Quick Facts
- **arXiv ID:** 2501.01933
- **Source URL:** https://arxiv.org/abs/2501.01933
- **Reference count:** 40
- **Primary result:** First study demonstrating feasibility of abstractive summarization for contemporary Sanskrit prose using transformer-based models

## Executive Summary
This thesis pioneers abstractive text summarization for contemporary Sanskrit prose, a low-resource language with no prior work in this domain. The author develops the first datasets for Sanskrit summarization and trains ten transformer-based models on 15,268 document-summary pairs. Through comprehensive experimentation with various encoder-decoder combinations, the study identifies BERT encoder-based models as optimal for capturing coherence and keywords, while highlighting persistent challenges with factual consistency and compression. The work establishes a foundation for Sanskrit NLP research and demonstrates that transfer learning from pre-trained language models can overcome data scarcity limitations.

## Method Summary
The author developed two datasets: a smaller Wikipedia-based corpus (6,347 document-summary pairs) and a larger contemporary corpus (15,268 pairs) combining journal articles, speeches, and Wikipedia content. Ten transformer-based models were trained using encoder-decoder architectures: four with BERT encoders (BERT2BERT, mBERT2BERT, IndicBERT2BERT, MURIL2BERT), three with RoBERTa encoders (RoBERTa2RoBERTa, RoBERTa2GPT2, RoBERTa2BART), and three decoder-only models (GPT2, BART, T5). Models were trained for 10 epochs with learning rate 5e-5 and batch size 16 on an RTX 2080 Ti GPU. Evaluation combined ROUGE metrics with human assessment across coherence, fluency, and relevance dimensions by three annotators.

## Key Results
- BERT2BERT achieved the highest ROUGE-1 (36.45), ROUGE-2 (13.79), and ROUGE-L (32.44) scores
- BERT encoder-based models outperformed RoBERTa and GPT-2 decoder combinations on all ROUGE metrics
- Human evaluation confirmed BERT2BERT produced the most coherent and relevant summaries
- Factual consistency remained a significant challenge across all models
- None of the systems achieved effective compression, generating overly verbose outputs

## Why This Works (Mechanism)
The success of BERT-based models stems from their bidirectional pre-training on Sanskrit corpora, enabling better understanding of context and relationships between words. The encoder-decoder architecture allows for abstractive generation rather than mere extraction, capturing semantic essence while maintaining linguistic coherence. Transfer learning from pre-trained models compensates for limited Sanskrit training data, with fine-tuning adapting general language understanding to the summarization task. The larger contemporary dataset proved more effective than the Wikipedia-only corpus, providing diverse real-world content that improved generalization.

## Foundational Learning
- **Transformer architecture**: Why needed - enables parallel processing and attention mechanisms; Quick check - verify self-attention weights capture long-range dependencies
- **Transfer learning**: Why needed - overcomes data scarcity in low-resource languages; Quick check - compare pre-trained vs randomly initialized model performance
- **ROUGE metrics**: Why needed - quantifies n-gram overlap between summaries and references; Quick check - calculate ROUGE scores on a small validation set
- **Encoder-decoder framework**: Why needed - separates comprehension (encoding) from generation (decoding); Quick check - ensure encoder output dimension matches decoder input requirements
- **Sanskrit morphology**: Why needed - understanding sandhi rules and compound structures affects tokenization; Quick check - verify tokenizer handles Sanskrit-specific characters correctly
- **Beam search decoding**: Why needed - balances exploration and exploitation in text generation; Quick check - experiment with different beam widths (3, 5, 10)

## Architecture Onboarding

**Component map:** Sanskrit text → Tokenizer → Encoder (BERT/RoBERTa) → Decoder (GPT-2/BART/T5) → Summary

**Critical path:** Data preparation → Model training → ROUGE evaluation → Human evaluation

**Design tradeoffs:** BERT2BERT offers superior coherence but slower training vs RoBERTa2RoBERTa; larger datasets improve performance but require more computation; abstractive generation captures semantics better but risks factual inconsistency

**Failure signatures:** Factual hallucination (generated content contradicting source), poor compression (overly long summaries), coherence breakdown (inconsistent pronoun references), tokenization errors (Sanskrit compound splitting issues)

**First experiments:**
1. Train BERT2BERT on 10% of dataset for 3 epochs to verify training pipeline
2. Compare ROUGE-1 scores between BERT and RoBERTa encoders on validation set
3. Generate sample summaries using greedy vs beam search to observe quality differences

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Do contemporary state-of-the-art multilingual models (such as IndicBERT v2 or Z-Code++) outperform the monolingual transformer models trained in this study for Sanskrit summarization?
- **Basis in paper:** Chapter 6 states that multilingual models like IndicBERT v2 and Z-Code++ were released around the time of finalization but could not be evaluated. Appendix F notes that the MURIL model failed to condition on inputs and generated the same output for all inputs during testing.
- **Why unresolved:** Resource constraints and technical failures prevented the author from evaluating multilingual checkpoints against the newly trained monolingual BERT models.
- **What evidence would resolve it:** A comparative benchmark showing ROUGE and human evaluation scores for IndicBERT v2 fine-tuned on the same 15,268 document-summary pairs used in the thesis.

### Open Question 2
- **Question:** How can factual consistency be improved in Sanskrit abstractive summarization without degrading keyword capture and coherence?
- **Basis in paper:** Chapter 5 highlights that while BERT-based models captured keywords effectively, "factual consistency remained a challenge" and summaries often hallucinated information contrary to the source text.
- **Why unresolved:** The author did not implement specific mechanisms to control hallucination, focusing instead on standard transfer learning and fine-tuning of encoder-decoder architectures.
- **What evidence would resolve it:** An experiment integrating copy mechanisms or reinforcement learning to penalize factual inconsistency, resulting in improved accuracy on a factual consistency metric while maintaining current ROUGE scores.

### Open Question 3
- **Question:** Can the current transformer-based summarization pipeline be effectively adapted for classical Sanskrit literature given the challenges of orthography and complex sandhi?
- **Basis in paper:** Chapter 1 and 2 explicitly exclude classical poetry and literary texts from the study, assuming they are "unsuitable for the first attempt" due to complexities like prosody and metrics.
- **Why unresolved:** The thesis focused exclusively on contemporary Sanskrit prose (journal articles, speeches, Wikipedia) to ensure data availability and easier processing.
- **What evidence would resolve it:** Results from training the same BERT/RoBERTa pipelines on a curated dataset of classical Sanskrit texts (e.g., from the Digital Corpus of Sanskrit) demonstrating successful handling of poetic meters and complex morphology.

### Open Question 4
- **Question:** What decoding strategies or architectural modifications are required to improve the compression rate and reduce the length of generated summaries?
- **Basis in paper:** Chapter 5 notes that "none of the systems proves to be a good compressor," observing that Compression Rate (CR) is high and must be curbed, as models generated long, often verbose outputs.
- **Why unresolved:** The experiments relied on standard greedy and beam search decoding without specific length penalties or architectural constraints designed to force brevity.
- **What evidence would resolve it:** Ablation studies testing different beam search parameters or length-penalty hyperparameters that yield summaries adhering to a stricter length ratio closer to the gold standard.

## Limitations
- Small dataset size constrains model performance and generalizability, particularly for long-document processing
- Computational constraints limited training to 10 epochs and prevented extensive hyperparameter tuning
- Human evaluation involved only three annotators, limiting statistical power
- Factual consistency and compression issues remain unresolved across all models
- Mixed performance across model architectures suggests encoder-decoder compatibility remains an open question

## Confidence
- **Core finding (High):** Sanskrit abstractive summarization is achievable with current transformer-based methods
- **Model rankings (Medium):** Limited evaluation scale and single GPU constraints affect reliability
- **Transferability conclusions (Medium):** Language-specific nature of task limits generalization to other languages
- **Computational resource claims (Medium):** Based on RTX 2080 Ti specifications, actual training times may vary

## Next Checks
1. Expand human evaluation to at least 10 annotators with inter-rater reliability measurement
2. Test the best-performing models on an independent Sanskrit test set to assess generalizability
3. Conduct ablation studies comparing encoder-only, decoder-only, and encoder-decoder performance across languages to isolate architecture effects from language-specific factors