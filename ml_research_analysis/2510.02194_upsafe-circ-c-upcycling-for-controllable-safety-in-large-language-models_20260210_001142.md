---
ver: rpa2
title: 'UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models'
arxiv_id: '2510.02194'
source_url: https://arxiv.org/abs/2510.02194
tags:
- safety
- utility
- arxiv
- layers
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to improve safety of large language
  models (LLMs) while preserving general capabilities. The approach identifies safety-critical
  layers in a pretrained model, upcycles them into a mixture-of-experts (MoE) structure,
  and applies a two-stage supervised fine-tuning (SFT) strategy.
---

# UpSafe°C: Upcycling for Controllable Safety in Large Language Models

## Quick Facts
- arXiv ID: 2510.02194
- Source URL: https://arxiv.org/abs/2510.02194
- Authors: Yuhao Sun; Zhuoer Xu; Shiwen Cui; Kun Yang; Lingyun Yu; Yongdong Zhang; Hongtao Xie
- Reference count: 40
- One-line primary result: Significant safety improvements against harmful and jailbreak inputs while maintaining competitive performance on general benchmarks, with dynamic control via safety temperature parameter

## Executive Summary
This paper introduces UpSafe°C, a framework that improves safety of large language models while preserving general capabilities through targeted architectural modification and controllable inference-time adjustment. The approach identifies safety-critical layers via linear probing, upcycles them into mixture-of-experts structures with specialized safety experts, and applies a two-stage supervised fine-tuning strategy. A novel safety temperature mechanism enables dynamic adjustment of the safety-utility trade-off during inference, allowing users to control the balance between safety and capability based on their needs.

## Method Summary
The UpSafe°C framework operates in three stages: first, it scans base model layers using linear probes to identify top-3 safety-critical layers that show strong discrimination between harmful and benign representations (measured by SS-Score). Second, it upcycles each safety-critical layer into a mixture-of-experts structure with 1 general expert and 3 safety experts, training in two stages - Stage 1 trains safety experts on harmful data while forcing general expert to zero, Stage 2 trains the router on mixed data to learn soft guardrail behavior. Third, during inference, a safety temperature parameter τ is applied to routing logits, allowing dynamic adjustment of the safety-utility balance through monotonic shifts in routing probabilities.

## Key Results
- UpSafe°C achieves significant safety improvements on WildJailbreak (from 7.4% to 95.8% safety rate) and StrongReject benchmarks while maintaining competitive performance on XSTest, MMLU, and HumanEval
- The safety temperature mechanism enables smooth, post-hoc control of the safety-utility trade-off, achieving the Pareto-optimal frontier without retraining
- Two-stage training strategy outperforms one-stage training, with ablation studies showing optimal performance with 3 safety experts per layer
- The framework generalizes across different model scales (7B, 13B, 14B) and model types (standard LLMs and reasoning models)

## Why This Works (Mechanism)

### Mechanism 1: Safety-Critical Layer Identification via Representation Probing
Only a subset of LLM layers encode strongly discriminative signals for harmful vs. benign inputs; targeting these layers improves efficiency and preserves general capabilities. Train a lightweight linear probe per layer to classify last-token embeddings as harmful or benign. The validation loss (SS-Score) quantifies linear separability. Select top-k layers with lowest loss for intervention. Core assumption: Safety-relevant representations are locally concentrated in specific layers rather than uniformly distributed. Evidence: "Our approach first identifies safety-critical layers... by probing layers' ability to distinguish between harmful and benign representations" [abstract]. Break condition: If SS-Score is uniform across layers (no clear minima), the scan fails to identify targeted intervention points.

### Mechanism 2: MoE Upcycling with Soft Guardrail Router
Converting safety-critical layers to MoE with specialized safety experts and a trained router enables selective, input-conditioned safety responses. Duplicate original MLP into N experts (1 general, N-1 safety). Two-stage training: (1) train safety experts on harmful data while forcing general expert score to -∞; (2) freeze all experts, train router on mixed data to discriminate input type and route accordingly. Router outputs softmax scores; top-K experts weighted and combined. Core assumption: The router can learn a generalizable mapping from input representations to appropriate expert activation patterns. Evidence: "the router acts as a soft guardrail that selectively activates original MLPs and added safety experts" [abstract]. Break condition: If router fails to discriminate (near-uniform routing), safety experts are underutilized and utility experts may process harmful inputs.

### Mechanism 3: Safety Temperature for Inference-Time Trade-off Control
A temperature hyperparameter τ ∈ [0,1] applied to routing logits enables smooth, post-hoc control of safety-utility balance without retraining. Modify routing logits with bias Δ(τ) that favors safety experts when τ > 0.5 and general expert when τ < 0.5. Temperature scaling T(τ) sharpens decisions near boundaries (τ→0 or τ→1) and permits mixed routing near τ≈0.5. This shifts expected routing probability monotonically with τ. Core assumption: Routing behavior at inference can be meaningfully adjusted post-training via logit manipulation without destabilizing outputs. Evidence: "safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility" [abstract]. Break condition: If temperature adjustment causes degraded output coherence or erratic routing oscillations, the control mechanism is unstable.

## Foundational Learning

- Concept: **Sparse Mixture-of-Experts (MoE) routing**
  - Why needed here: The core architecture modification replaces dense MLPs with MoE layers requiring understanding of top-K selection, auxiliary load-balancing losses, and router softmax dynamics
  - Quick check question: Given router logits [2.1, 1.5, 0.8] and top-2 selection, which experts are activated and what are their normalized weights?

- Concept: **Linear probing for representation analysis**
  - Why needed here: The SS-Score mechanism relies on interpreting linear probe validation loss as a proxy for layer-wise safety sensitivity
  - Quick check question: If layer 12 has SS-Score 0.25 and layer 8 has SS-Score 0.45, which is more safety-critical and why?

- Concept: **Two-stage curriculum-style fine-tuning**
  - Why needed here: The method deliberately separates expert specialization (Stage 1, harmful-only) from router discrimination (Stage 2, mixed data), a non-obvious design requiring staged optimization intuition
  - Quick check question: What would happen if Stage 2 router training used only harmful data?

## Architecture Onboarding

- Component map:
  Input -> [Transformer layers] -> Safety-critical layers (identified via SS-Score scan) -> Each safety-critical layer replaced with: Router (linear projection + softmax) + Expert pool (1 general MLP + N-1 safety MLPs) -> Top-K weighted combination -> Continue through remaining layers -> Output
  Safety temperature τ injected at inference to modify router logits

- Critical path:
  1. Run SS-Score scan on base model using labeled harmful/benign dataset
  2. Select top-3 safety-critical layers (default)
  3. Duplicate MLP weights at each selected layer into 4 experts (1 general + 3 safety)
  4. Stage 1: Train experts + router on harmful data (20 epochs, lr=5e-5, λ_1=0.01)
  5. Stage 2: Freeze experts, train router only on mixed data (10 epochs, lr=5e-5, λ_2=0.1)
  6. Deploy with configurable τ at inference

- Design tradeoffs:
  - More safety-critical layers → higher parameter overhead, potential capability interference; fewer → insufficient coverage
  - More safety experts → better generalization to diverse attacks, higher compute; fewer → weaker robustness
  - Higher τ → stronger safety but potential over-refusal on benign edge cases; lower τ → better utility but attack vulnerability
  - Two-stage vs one-stage training: One-stage is simpler but ablations show inferior Pareto frontier

- Failure signatures:
  - Uniform routing distribution (router not discriminating) → check Stage 2 loss convergence, verify benign/harmful label quality
  - Over-refusal on benign inputs → τ set too high, or Stage 1 contaminated with benign data
  - Safety regression under jailbreak → insufficient safety experts, or router overfitting to training distribution
  - Utility degradation on standard benchmarks → safety-critical layers incorrectly identified, or general expert being suppressed

- First 3 experiments:
  1. Validate SS-Score scan: On a held-out dataset, verify that top-k identified layers show clear harmful/benign clustering via t-SNE (replicate Fig. 2) while non-critical layers remain entangled
  2. Ablate expert count: Train with 1, 2, 3, 5 safety experts per layer and plot safety-utility curves to confirm 3 is near-optimal for the model scale
  3. Temperature sweep: Run inference with τ ∈ {0.0, 0.25, 0.5, 0.75, 1.0} on WildJailbreak (safety) and XSTest (utility) to reproduce Pareto frontier and validate monotonic trade-off

## Open Questions the Paper Calls Out

### Open Question 1
Does the choice of probing dataset distribution significantly affect which layers are identified as safety-critical, or is safety information inherently concentrated in specific architectural locations? Basis: The authors validate stability across WildJailbreak and XStest (Appendix B.1), but these share similar domains. The probing uses STAR-1k, which may not cover all harmful/benign distributions. Why unresolved: Safety representations may shift with different data distributions, and current validation covers only similar-domain datasets. What evidence would resolve it: Probing with diverse out-of-distribution datasets (multilingual, code, multimodal) and comparing layer rankings.

### Open Question 2
How vulnerable is the routing mechanism itself to adversarial manipulation designed to bypass the soft guardrail? Basis: The router serves as the "soft guardrail" distinguishing harmful from benign inputs, but no analysis tests attacks specifically targeting router decisions. Why unresolved: Jailbreak attacks in experiments target model outputs, not the routing infrastructure. A compromised router could selectively disable safety experts. What evidence would resolve it: White-box attacks on routing weights, gradient-based adversarial prompts optimized to force general expert selection.

### Open Question 3
Is the fixed number of three safety experts optimal across all model scales and architectures, or should expert count scale with model capacity? Basis: The ablation study tests 1-5 experts on Llama-8B (Fig. 6b), finding three optimal. However, larger models (14B) or different architectures may require different configurations. Why unresolved: One-size-fits-all hyperparameter may not generalize, but no systematic scaling analysis is provided. What evidence would resolve it: Cross-scale expert ablation (from 7B to 70B) with architecture-stratified analysis.

### Open Question 4
Can the safety temperature τ be learned adaptively per-input rather than manually set, enabling context-aware safety-utility balancing? Basis: The authors state τ "is chosen dynamically based on the user's intent," but the mechanism relies on manual selection without automatic intent detection. Why unresolved: Manual temperature selection requires users to pre-declare intent, which may not align with actual query characteristics or be feasible in production. What evidence would resolve it: Training a meta-controller to predict optimal τ from input features, comparing against fixed-τ baselines.

## Limitations
- The safety-critical layer identification assumes linear separability correlates with intervention effectiveness, but this relationship is not rigorously validated across diverse model architectures
- The evaluation relies heavily on GPT-4o as judge, introducing potential subjectivity and cost barriers for reproduction
- The safety temperature mechanism's theoretical derivation assumes stable routing behavior under logit manipulation, but practical stability under extreme τ values is not thoroughly examined

## Confidence
**High Confidence**: Safety improvements on targeted benchmarks (WildJailbreak, StrongReject) are well-documented with clear statistical comparisons. The two-stage training advantage over single-stage is supported by ablation studies.

**Medium Confidence**: The Pareto-optimal frontier between safety and utility is convincingly demonstrated, but the generalizability across model scales and architectures needs further validation. The inference-time temperature control mechanism shows theoretical soundness but limited empirical stress-testing.

**Low Confidence**: The claim that only middle layers are safety-critical is based on limited model exploration. The assumption that MoE upcycling preserves general capabilities without degradation is supported but not exhaustively tested across all benchmark types.

## Next Checks
1. **Layer-wise ablation across model depths**: Systematically test SS-Score scanning on models with different depths (7B, 13B, 70B) to verify if safety-critical layers consistently cluster in middle layers or vary with architecture. This validates the core assumption about representation localization.

2. **Extreme temperature stability test**: Evaluate model outputs under τ ∈ {0.01, 0.99} on both harmful and benign edge cases to verify that temperature scaling maintains coherent outputs without routing oscillations or catastrophic forgetting of safety/utility behaviors.

3. **Cross-dataset generalization**: Test the upcycled models on safety datasets not seen during training (e.g., RealToxicityPrompts, ToxiGen) to validate that the router's learned discrimination generalizes beyond the STAR-1 distribution, addressing potential overfitting concerns.