---
ver: rpa2
title: 'PACIFIC: a framework for generating benchmarks to check Precise Automatically
  Checked Instruction Following In Code'
arxiv_id: '2512.10713'
source_url: https://arxiv.org/abs/2512.10713
tags:
- instructions
- instruction
- output
- pacific
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PACIFIC is a framework for automatically generating benchmarks
  to evaluate instruction-following and code dry-running capabilities in large language
  models. It uses modular instruction building blocks to construct complex multi-step
  tasks, with deterministic evaluation via output comparison against reference implementations.
---

# PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code

## Quick Facts
- arXiv ID: 2512.10713
- Source URL: https://arxiv.org/abs/2512.10713
- Authors: Itay Dreyfuss; Antonio Abu Nassar; Samuel Ackerman; Axel Ben David; Eitan Farchi; Rami Katan; Orna Raz; Marcel Zalmanovici
- Reference count: 23
- Primary result: Automatically generates benchmarks to evaluate instruction-following and code dry-running in LLMs using modular instruction building blocks

## Executive Summary
PACIFIC is a framework that automatically generates benchmarks to evaluate instruction-following and code dry-running capabilities in large language models. It uses modular instruction building blocks to construct complex multi-step tasks, with deterministic evaluation via output comparison against reference implementations. The framework allows control over difficulty through instruction count and output length parameters. Experimental results show that even state-of-the-art LLMs struggle with compositional instruction execution, with performance degrading significantly as task complexity increases.

## Method Summary
The framework generates benchmarks by sampling inputs and assembling instruction chains that enforce type consistency and length constraints. It uses 22 typed instructions (number→number, number→string, string→number, string→string) implemented in Python, Java, and C++. Benchmarks are constructed by selecting instructions randomly while ensuring output type of one instruction matches input type of the next. The framework controls difficulty through two parameters: instruction count and target output length. Evaluation is deterministic, comparing model outputs against reference implementations using exact string matching after parsing answers from the specified format.

## Key Results
- Performance degrades significantly as instruction count increases, with most models scoring 0% on hardest benchmarks
- Claude-4-Sonnet achieves over 65% accuracy on individual instructions but drops to 0% on complex chained tasks
- Framework enables contamination-resistant benchmark generation through seed-based resampling and representation diversity

## Why This Works (Mechanism)

### Mechanism 1: Compositional Instruction Chaining Creates Compounding Difficulty
Chaining modular instructions creates performance degradation that reveals gaps in sequential reasoning, even when individual instructions are tractable. Instructions are typed functions where each instruction's output becomes the next instruction's input. Error propagation is total: a single mistake invalidates all subsequent steps. Core assumption: Models failing compositional execution lack robust sequential state tracking, not just individual instruction comprehension.

### Mechanism 2: Difficulty Control via Orthogonal Structural Parameters
Task difficulty scales predictably through two independent parameters: instruction count (quantitative) and target output length (qualitative). More instructions require longer reasoning chains. Larger output lengths require more complex transformations per step. The system dynamically selects instructions via length-aware categories to hit targets. Core assumption: These parameters capture meaningful difficulty dimensions that correlate with model capability limits.

### Mechanism 3: Contamination Resistance Through Regenerable Surface Variants
Seed-based resampling and representation diversity produce semantically equivalent but surface-distinct benchmarks that resist memorization. Changing the random seed re-samples inputs and instruction sequences under identical constraints. Same underlying task can render as Code (Python/Java/C++) or NL, in Prompt or Chat mode—different surface, same computational graph. Core assumption: Models cannot easily transfer learned patterns across surface variations of equivalent tasks.

## Foundational Learning

- **Code Dry-Running (Trace Execution)**: Why needed: PACIFIC evaluates the LLM's intrinsic ability to "reason through code behavior step-by-step without execution"—this is the core capability being tested. Quick check: Can you manually trace `next_perfect_square(4)` and explain why the output is 9?

- **Function Composition with Type Constraints**: Why needed: Instructions are typed (number↔string variants) and must compose correctly; the framework enforces "type consistency" where input type matches previous output type. Quick check: Given f: int→string and g: string→int, which composition is valid: g(f(x)) or f(g(x))?

- **Deterministic Evaluation vs. LLM-as-Judge**: Why needed: PACIFIC deliberately avoids LLM-based evaluators; understanding why rule-based metrics matter for reproducibility is essential. Quick check: Why might an LLM-as-judge introduce variability that undermines benchmark reliability?

## Architecture Onboarding

- **Component map**: Instruction Pool -> Benchmark Generator -> Evaluation Engine -> Parameter Layer
- **Critical path**: 1. Set parameters (I=instruction count, L=output length, seed) 2. Generator chains instructions respecting type constraints + length targets 3. Construct prompt: initial input + instruction sequence 4. Model inference → formatted outputs 5. Parser extracts answers, compares to reference execution 6. Compute Prompt-Level Accuracy (all correct) + Instruction-Level Accuracy (per-step)
- **Design tradeoffs**: Prompt vs Chat Mode (Prompt cheaper/faster; Chat more realistic but higher cost), Deterministic metrics (Reproducible but no partial credit for near-misses), Structural difficulty control (Simple parameters but may miss semantic complexity dimensions)
- **Failure signatures**: Format errors (Missing/incorrect `[ANSWER][i][\ANSWER]` tags), Type mismatches (Wrong output type for instruction), Cascading errors (Early mistake propagates through chain), gpt-oss-120b: High format compliance failures; Qwen3-235B: Good format, reasoning failures
- **First 3 experiments**: 1. Reproduce the 15-benchmark grid (I={3,5,8,10,15} × L={3,5,10}) on a baseline model to verify negative correlation between parameters and accuracy 2. Single-instruction ablation: Run each instruction individually to identify hardest primitives, then test compositional degradation by combining them 3. Contamination probe: Generate benchmark with seed A, evaluate; regenerate with seed B (same difficulty), measure performance delta to assess memorization risk

## Open Questions the Paper Calls Out

### Open Question 1
Can the seed-based resampling and representation diversity of PACIFIC effectively mitigate training data contamination in real-world scenarios? Basis: Section 7 states that contamination resistance "has not yet been empirically validated across all scenarios" despite being a core design objective. Why unresolved: The framework theoretically allows generating unique variants, but it remains unproven whether these variants are sufficiently distinct to prevent memorization or overfitting in state-of-the-art models. Evidence needed: A longitudinal study comparing model performance on public benchmarks versus newly generated, unseen PACIFIC variants to confirm if scores reflect capability rather than memorization.

### Open Question 2
Does the sequential provision of instructions in a multi-turn chat format yield significantly different performance outcomes compared to the single-prompt mode? Basis: Section 7 notes that while anecdotal checks showed no difference, "a thorough investigation is needed" regarding Chat mode behavior. Why unresolved: The experiments primarily utilized Prompt mode to reduce token costs, leaving the interaction dynamics of the Chat mode largely unexplored. Evidence needed: A controlled ablation study running identical instruction chains in both modes to isolate the impact of conversational context on instruction-following accuracy.

### Open Question 3
What specific factors beyond instruction count and output length contribute to the difficulty of a PACIFIC benchmark? Basis: Section 7 suggests that "additional factors beyond the number of instructions and output size" may affect difficulty, and the current understanding is limited. Why unresolved: The paper establishes a correlation between complexity parameters and performance degradation, but admits other latent variables (e.g., instruction semantics or type interactions) might influence the skill gap. Evidence needed: Regression analysis incorporating variables such as instruction type, nesting depth, and semantic complexity to model their individual contributions to error rates.

## Limitations

- Contamination resistance claims lack empirical validation despite being a core design objective
- Difficulty control relies on structural parameters that may not capture all dimensions of task complexity
- Strict deterministic evaluation may be overly rigid, marking equivalent representations as incorrect

## Confidence

**High Confidence:** The compositional difficulty mechanism is well-supported with clear experimental evidence showing performance degradation with increasing complexity.

**Medium Confidence:** The difficulty control framework works as designed, though the completeness of the parameter space remains open.

**Low Confidence:** Contamination resistance claims are framework-design assertions without empirical validation.

## Next Checks

1. **Contamination Resistance Empirical Test:** Generate a benchmark with seed A, evaluate on target models, then regenerate with seed B (maintaining same difficulty parameters). Measure performance difference to determine if contamination resistance is effective.

2. **Parameter Sensitivity Analysis:** Systematically vary instruction count and output length beyond the current grid to determine if the difficulty relationship is linear or exhibits threshold effects.

3. **Intermediate Step Evaluation:** Modify evaluation to capture per-instruction outputs to analyze whether difficulty stems from individual instruction comprehension versus sequential reasoning and error propagation.