---
ver: rpa2
title: Test-Time Adaptation with Binary Feedback
arxiv_id: '2505.18514'
source_url: https://arxiv.org/abs/2505.18514
tags:
- feedback
- adaptation
- binary
- samples
- bitta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BiTTA, a dual-path optimization framework for
  test-time adaptation (TTA) using binary feedback. Unlike existing TTA methods that
  rely solely on unlabeled test samples, BiTTA incorporates binary feedback (correct/incorrect)
  on model predictions to guide adaptation while maintaining efficiency.
---

# Test-Time Adaptation with Binary Feedback

## Quick Facts
- arXiv ID: 2505.18514
- Source URL: https://arxiv.org/abs/2505.18514
- Authors: Taeckyung Lee, Sorn Chottananurak, Junsu Kim, Jinwoo Shin, Taesik Gong, Sung-Ju Lee
- Reference count: 39
- Primary result: 13.3% accuracy improvement over TTA baselines using binary feedback

## Executive Summary
This paper introduces BiTTA, a dual-path optimization framework for test-time adaptation that leverages binary feedback (correct/incorrect) instead of traditional full-class labels. Unlike conventional TTA methods that adapt solely on unlabeled test samples, BiTTA uses reinforcement learning to balance two complementary strategies: Binary Feedback-guided Adaptation (BFA) for uncertain samples and Agreement-Based self-Adaptation (ABA) for confident predictions. The framework achieves state-of-the-art performance across three image corruption datasets and domain generalization tasks, demonstrating that binary feedback can be as effective as full supervision for practical TTA applications.

## Method Summary
BiTTA implements a novel dual-path optimization framework where model adaptation is guided by binary feedback on predictions. The method uses MC-dropout (N=4 forward passes) to estimate uncertainty and select the k=3 least-confident samples per batch for binary feedback. BFA leverages this feedback through reinforcement learning, using rewards of +1 for correct predictions and -1 for incorrect ones, while maintaining FIFO memory buffers for both correct and incorrect samples. ABA identifies confident samples through prediction agreement between standard and MC-dropout outputs, using a reward of +1 when agreement occurs. The final loss combines these paths with α=2 and β=1, optimized via SGD with learning rates tuned per dataset.

## Key Results
- Achieves 13.3% accuracy improvements over state-of-the-art TTA baselines
- Outperforms methods using full-class labels while requiring only binary feedback
- Shows consistent gains across 15 corruption types in CIFAR10-C, CIFAR100-C, and Tiny-ImageNet-C datasets
- Demonstrates effectiveness in domain generalization tasks on PACS dataset

## Why This Works (Mechanism)
The effectiveness stems from BiTTA's strategic use of binary feedback to guide adaptation without requiring full supervision. By leveraging MC-dropout uncertainty estimates, the method intelligently selects samples where feedback would be most informative, focusing adaptation efforts on challenging cases. The dual-path approach ensures the model adapts on both uncertain samples (through BFA) and confident predictions (through ABA), creating a balanced optimization that prevents overfitting to either strategy. The memory buffer mechanism allows the model to learn from past mistakes and successes, providing additional context for adaptation decisions.

## Foundational Learning
- **MC-dropout uncertainty estimation**: Used to quantify prediction confidence and select samples for binary feedback. Quick check: Verify expected calibration error improves from 0.100 to 0.062.
- **Reinforcement learning for sample selection**: Guides the dual-path optimization by assigning rewards based on feedback and agreement. Quick check: Monitor reward signals during adaptation to ensure both BFA and ABA contribute.
- **Memory buffer management**: FIFO queues store past correct/incorrect samples for BFA loss computation. Quick check: Ensure buffers maintain balanced sizes throughout adaptation.

## Architecture Onboarding
- **Component map**: ResNet18 backbone -> MC-dropout layers -> Confidence estimator -> Sample selector -> Dual-path optimizer (BFA + ABA) -> Memory buffers -> Final loss computation
- **Critical path**: Input image → MC-dropout inference → Uncertainty estimation → Sample selection → Binary feedback application → Memory buffer update → Loss computation → Parameter update
- **Design tradeoffs**: Binary feedback provides minimal supervision but requires uncertainty estimation overhead; memory buffers add complexity but enable learning from past errors
- **Failure signatures**: Poor calibration leads to incorrect sample selection; memory buffer imbalance causes unstable adaptation; dropout placement affects uncertainty estimates
- **First experiments**: 1) Train source model and verify baseline accuracy, 2) Implement MC-dropout and check ECE reduction, 3) Test BFA and ABA separately before combining

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification for combining BFA and ABA through weighted averaging (α=2, β=1) lacks rigorous foundation
- Memory buffer mechanism introduces additional hyperparameters that are not fully explored
- Claims of outperforming full-class label methods require more controlled comparisons to rule out implementation differences

## Confidence
- Major claims: Medium
- Performance improvements over TTA baselines: Medium
- Effectiveness of binary feedback vs. full-class labels: Medium
- Memory buffer contribution to performance: Medium

## Next Checks
1. Conduct ablation studies isolating binary feedback vs. uncertainty estimation vs. memory buffers
2. Verify calibration improvements (ECE reduction from 0.100 to 0.062) across all datasets and corruption types
3. Test BiTTA's performance with different dropout placement strategies within ResNet18 to confirm architecture independence