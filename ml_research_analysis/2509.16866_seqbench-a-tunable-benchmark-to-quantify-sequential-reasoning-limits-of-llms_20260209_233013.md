---
ver: rpa2
title: 'seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs'
arxiv_id: '2509.16866'
source_url: https://arxiv.org/abs/2509.16866
tags:
- reasoning
- figure
- door
- performance
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces seqBench, a tunable benchmark designed to
  systematically evaluate sequential reasoning limits in large language models (LLMs)
  by independently controlling logical depth, backtracking requirements, and noise
  ratio in pathfinding tasks. The benchmark reveals that all evaluated LLMs exhibit
  exponential performance collapse as logical depth increases, with a model-specific
  characteristic path length L0 (ranging from 85.7 for Gemini-2.5-Flash to 1.6 for
  Llama-3.2-3B) beyond which success rates approach zero.
---

# seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs

## Quick Facts
- arXiv ID: 2509.16866
- Source URL: https://arxiv.org/abs/2509.16866
- Authors: Mohammad Ramezanali; Mo Vazifeh; Paolo Santi
- Reference count: 20
- Primary result: All evaluated LLMs show exponential performance collapse with increasing logical depth, characterized by model-specific characteristic length L₀ (ranging from 85.7 for Gemini-2.5-Flash to 1.6 for Llama-3.2-3B)

## Executive Summary
This paper introduces seqBench, a tunable benchmark designed to systematically evaluate sequential reasoning limits in large language models (LLMs) by independently controlling logical depth, backtracking requirements, and noise ratio in pathfinding tasks. The benchmark reveals that all evaluated LLMs exhibit exponential performance collapse as logical depth increases, with a model-specific characteristic path length L₀ (ranging from 85.7 for Gemini-2.5-Flash to 1.6 for Llama-3.2-3B) beyond which success rates approach zero. Performance also degrades significantly with increased backtracking steps and noise ratio, while fact shuffling has minimal impact. Detailed error analysis shows models frequently fail by omitting critical steps rather than taking illegal shortcuts, with failures tending to occur earlier in longer problems, suggesting difficulties with global planning.

## Method Summary
seqBench uses synthetic 2D grid mazes generated via Kruskal's algorithm, then constructs reasoning tasks by embedding locked doors and keys along paths to enforce sequential dependencies. The benchmark controls three independent parameters: logical depth (L, number of actions required), backtracking steps (B, requiring return to earlier locations), and noise ratio (N, distracting facts added). Tasks are presented as natural language facts describing room connections, key locations, and door states. Models receive 3-shot prompts with reasoning guidance and must output action sequences. Performance is measured via Pass@1 success rate, progress ratio, precision, and recall.

## Key Results
- All evaluated LLMs show exponential accuracy decay with logical depth, characterized by model-specific L₀ values (85.7 for Gemini-2.5-Flash down to 1.6 for Llama-3.2-3B)
- Performance degrades significantly with increased backtracking requirements and noise ratio, but remains robust to fact shuffling
- Models predominantly fail by omitting critical sub-goals (high precision, low recall) rather than hallucinating invalid actions
- First errors occur earlier in longer problems, suggesting difficulties with global planning rather than local error accumulation

## Why This Works (Mechanism)

### Mechanism 1: Exponential Performance Collapse via Error Accumulation
- **Claim:** Model accuracy on sequential reasoning tasks decays exponentially with logical depth (L), characterized by a model-specific characteristic length L₀ where performance drops by e⁻¹.
- **Mechanism:** Errors at each reasoning step accumulate with partial independence, eventually overwhelming the model's capacity for coherent multi-step inference. The log-linear relationship between success rate and path length suggests per-step failure probabilities compound multiplicatively rather than linearly.
- **Core assumption:** Errors are approximately independent across steps; the model does not dynamically recover from earlier mistakes during generation.
- **Evidence anchors:**
  - [abstract] "accuracy collapses exponentially beyond a model-specific logical depth"
  - [Section 2.2] "Plotting success rates on a semi-logarithmic (log-y) scale against L reveals an approximately linear decay trend... This log-linear relationship suggests that errors may accumulate with a degree of independence at each reasoning step"
  - [corpus] CogniLoad (arXiv:2509.18458) independently proposes tunable length/difficulty/distractor density controls, corroborating the need for disentangling these factors, but does not replicate the exponential decay finding directly.

### Mechanism 2: Anticipated Complexity Burden Shifts First Errors Earlier
- **Claim:** First errors occur earlier in the reasoning chain when the total problem length is longer, contrary to simple cumulative error models.
- **Mechanism:** The model's internal representation of overall problem complexity affects reasoning quality from the outset, suggesting difficulty with global planning or maintaining coherent intent over longer horizons. The model "sees" the full problem scope and degrades in planning quality before execution begins.
- **Core assumption:** Models encode some implicit sense of problem scale during prompt processing; this anticipation affects early-step generation.
- **Evidence anchors:**
  - [Section 2.4] "as the total required path length (L) of a problem increases, models tend to fail more frequently even at the earliest steps... an error at an early step (e.g., step 5) becomes substantially more likely when the model is attempting to solve an 80-step problem versus a 20-step problem"
  - [Figure 10 caption] "As L (total required path length) increases, the distribution of first errors tends to shift leftward"
  - [corpus] No direct replication found; ZebraLogic focuses on constraint satisfaction rather than sequential depth, making comparison limited.

### Mechanism 3: Omission-Dominated Failures with High Precision
- **Claim:** Models predominantly fail by omitting critical sub-goals (low recall) rather than hallucinating invalid actions (high precision).
- **Mechanism:** Sequential reasoning requires maintaining a working stack of pending preconditions (e.g., "need key before door"). Transformers lack explicit architectural mechanisms for this, leading to silent dropout of intermediate goals while action syntax remains valid.
- **Core assumption:** Failure mode is architectural (lack of explicit state tracking) rather than purely knowledge or prompting related.
- **Evidence anchors:**
  - [Section 2.4] "while precision generally remains high (models infrequently hallucinate non-existent rooms or facts), recall and progress ratio plummet with increasing path length... models predominantly fail by missing necessary actions or entire crucial sub-sequences"
  - [Figure 2 caption] "mean of precision and recall is shown to highlight models gradually increasing struggle in completing the path"
  - [corpus] Limited direct evidence; CogniLoad mentions distractor interference effects but does not analyze precision/recall asymmetry.

## Foundational Learning

- **Concept: Exponential decay models for discrete processes**
  - Why needed here: The paper's central claim models success rate as P(L) = exp(-L/L₀), requiring understanding of how exponential functions describe discrete failure accumulation and how L₀ functions as a characteristic scale parameter.
  - Quick check question: If a model has L₀ = 20, what is the approximate success rate at L = 40? At L = 60?

- **Concept: Precision vs. Recall in sequential settings**
  - Why needed here: The paper diagnoses failure mode via high precision / low recall, which differs from standard classification uses. Here, precision measures "how many predicted actions were valid" and recall measures "how many required actions were produced."
  - Quick check question: If a model produces 50 actions for a 100-step ground truth path, all valid, what are its precision and recall?

- **Concept: Kruskal's algorithm for maze construction**
  - Why needed here: seqBench generates acyclic maze graphs using Kruskal's algorithm to ensure unique paths and controlled complexity. Understanding this helps reproduce or extend the benchmark.
  - Quick check question: Why does using a minimum spanning tree algorithm guarantee a unique path between any two cells in the maze?

## Architecture Onboarding

- **Component map:** Maze generator (Kruskal's algorithm) -> Rewind constructor (Algorithm 1) -> Fact compiler -> Noise injector -> Prompt assembler -> Model inference -> Evaluator

- **Critical path:** Maze generation (seconds) -> Rewind construction (sets L, B) -> Fact compilation -> Noise injection (sets N) -> Prompt assembly -> Model inference -> Action sequence parsing -> Metric computation

- **Design tradeoffs:**
  - Synthetic vs. naturalistic: Synthetic mazes enable precise control but may not reflect real-world reasoning structures; the paper acknowledges this limitation.
  - Single-key dependency chains only: Current design restricts backtracking to simple one-key-per-door dependencies; extending to multi-key doors or nested dependencies would increase ecological validity but complicate attribution.
  - Text-only representation: Maze presented as natural language facts; visual modality not tested (paper notes this as limitation; image generation code released).

- **Failure signatures:**
  - Early errors in long problems: First violation at step k becomes more likely as total L increases -> indicates global planning failure, not local error accumulation.
  - High precision, low recall: Models produce syntactically valid actions but systematically omit required sub-goals (e.g., collecting keys) -> indicates lack of explicit state tracking.
  - Noise-sensitive, shuffle-insensitive: Performance degrades with distractor facts but not with fact reordering -> indicates attention/selection failure, not positional encoding limits.

- **First 3 experiments:**
  1. Replicate L₀ characterization for a new model: Sample 40 problems per L-bin (L = 10, 20, 30, ..., 100) with fixed parameters (N×M = 40×40, B = 2, N = 0.0). Compute Pass@1 across 5 runs per problem. Fit P(L) = exp(-L/L₀) via weighted least squares on log-transformed success rates. Report L₀ with confidence intervals.
  2. Isolate backtracking effect: Fix L ∈ [40, 60], N = 0.0, vary B from 0 to 5. Measure success rate, progress ratio, and output token count. Expect declining accuracy with increasing B; token count should increase or stay elevated as models articulate longer reasoning paths.
  3. Test noise-shuffle interaction: Fix L ∈ [40, 60], B = 2. Create 4 conditions: (N = 0.0, shuffle = 0), (N = 0.0, shuffle = 1), (N = 0.5, shuffle = 0), (N = 0.5, shuffle = 1). Expect noise to dominate; shuffle alone should have minimal effect, but high noise + high shuffle may compound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the exponential performance collapse (characterized by $L_0$) generalize to non-spatial domains like mathematical proofs or multimodal inputs?
- Basis in paper: [explicit] The authors state future work must extend methodology to "diverse reasoning domains" and investigate whether failure modes arise when problems are presented visually.
- Why unresolved: Current findings are restricted to synthetic textual pathfinding tasks.
- What evidence would resolve it: Evaluation of identical sequential reasoning constraints in mathematical or visual contexts showing similar decay rates.

### Open Question 2
- Question: Do agentic systems capable of tool use or external memory mitigate the exponential failure rates observed in standard LLMs?
- Basis in paper: [explicit] The Limitations section notes the evaluation does not consider agentic systems and suggests exploring setups where "LLMs can externalize sub-problems."
- Why unresolved: The study focused on monolithic model inference without external aids.
- What evidence would resolve it: Benchmarking agentic architectures on seqBench tasks to see if dependency chains reduce accuracy similarly.

### Open Question 3
- Question: What are the precise mechanisms driving "path-length dependent first errors," where models fail early on long problems?
- Basis in paper: [explicit] The paper calls for deeper analysis to explain this specific phenomenon, where anticipated complexity degrades reasoning quality from the outset.
- Why unresolved: The paper identifies the pattern but does not isolate the internal model dynamics causing it.
- What evidence would resolve it: Mechanistic interpretability studies analyzing attention patterns during the initial steps of long sequences.

## Limitations

- Synthetic benchmark may not generalize to real-world reasoning tasks despite precise control over complexity parameters
- Focus on acyclic mazes with single-key dependencies may miss more complex reasoning patterns involving nested dependencies
- Characteristic length L₀ metric lacks theoretical grounding in cognitive science or computational complexity theory
- Does not evaluate multimodal or agentic systems that could potentially overcome architectural limitations

## Confidence

**High confidence:** The exponential decay relationship between success rate and logical depth is strongly supported by the data and the log-linear regression fits. The high-precision, low-recall failure mode is clearly documented across multiple models and conditions.

**Medium confidence:** The interpretation of early errors in long problems as evidence of global planning failure is plausible but not definitively proven. Alternative explanations, such as context window limitations or attention degradation, cannot be ruled out with the current experimental design.

**Low confidence:** The claim that all evaluated models, including GPT-5, demonstrate identical failure patterns requires cautious interpretation, as model access and testing conditions may vary. The paper does not provide detailed statistical comparisons between models' L₀ values.

## Next Checks

1. Apply seqBench methodology to a naturalistic sequential reasoning task (e.g., cooking recipe following or software debugging scenarios) to assess whether the exponential decay pattern persists outside synthetic mazes.

2. Compare seqBench performance between standard transformer models and variants with explicit state tracking (e.g., state space models or models with external memory) to test whether the omission-dominated failure mode is truly architectural.

3. Train models specifically on seqBench-style tasks and measure changes in L₀ values to determine whether performance degradation is fundamentally learnable or represents an inherent architectural limitation.