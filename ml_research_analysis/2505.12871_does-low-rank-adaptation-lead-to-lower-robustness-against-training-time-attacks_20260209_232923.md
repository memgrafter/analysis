---
ver: rpa2
title: Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?
arxiv_id: '2505.12871'
source_url: https://arxiv.org/abs/2505.12871
tags:
- lora
- rank
- attacks
- poisoning
- attn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the training-time security of LoRA-based fine-tuning.
  It introduces a theoretical framework using NTK and information geometry to assess
  LoRA's robustness against data poisoning and backdoor attacks.
---

# Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?

## Quick Facts
- arXiv ID: 2505.12871
- Source URL: https://arxiv.org/abs/2505.12871
- Reference count: 40
- Low Rank Adaptation (LoRA) exhibits higher robustness against backdoor attacks but lower robustness against data poisoning compared to full fine-tuning

## Executive Summary
This paper investigates the training-time security of LoRA-based fine-tuning by introducing a theoretical framework that combines Neural Tangent Kernel (NTK) and information geometry. The study provides a comprehensive analysis of LoRA's robustness against both data poisoning and backdoor attacks during the training phase. Through both theoretical analysis and experimental validation on GLUE tasks, the research demonstrates that LoRA's security characteristics differ significantly from full fine-tuning, showing improved resistance to backdoor attacks while being more vulnerable to untargeted data poisoning.

## Method Summary
The paper develops a theoretical framework using Neural Tangent Kernel (NTK) and information geometry to analyze LoRA's robustness during training. The authors examine how LoRA's low-rank structure affects its vulnerability to adversarial attacks, considering both data poisoning and backdoor attack scenarios. The analysis focuses on key parameters such as rank and initialization variance of the A matrix in LoRA. Experimental validation is conducted on GLUE tasks to verify the theoretical findings, comparing LoRA's performance against full fine-tuning under various attack conditions.

## Key Results
- LoRA demonstrates higher robustness against backdoor attacks compared to full fine-tuning
- LoRA shows reduced robustness to untargeted data poisoning attacks
- The rank and initialization variance of the A matrix significantly influence LoRA's security properties

## Why This Works (Mechanism)
LoRA's low-rank structure constrains the parameter space during fine-tuning, which affects how the model responds to adversarial inputs during training. The limited rank reduces the model's capacity to overfit to poisoned data in backdoor scenarios while simultaneously limiting its ability to resist untargeted poisoning attacks that aim to degrade overall performance.

## Foundational Learning
- Neural Tangent Kernel (NTK): A theoretical framework for analyzing neural network training dynamics, needed to understand how LoRA's parameter updates behave during training; quick check: verify NTK convergence properties for LoRA vs full fine-tuning
- Information Geometry: Mathematical framework for studying statistical models, needed to analyze the geometric properties of LoRA's parameter space; quick check: examine Fisher information matrices for LoRA configurations
- Data Poisoning Attacks: Adversarial manipulation of training data, needed to understand the threat model; quick check: verify attack success rates under different poisoning rates
- Backdoor Attacks: Targeted adversarial contamination creating hidden triggers, needed to distinguish from general poisoning; quick check: test trigger activation rates across different backdoor strategies
- Low-Rank Matrix Decomposition: The mathematical foundation of LoRA, needed to understand parameter constraints; quick check: verify rank preservation during training

## Architecture Onboarding
- Component Map: Input Data -> LoRA Adapter (A, B matrices) -> Frozen Base Model -> Output
- Critical Path: Data preprocessing → Attack injection → LoRA fine-tuning → Robustness evaluation
- Design Tradeoffs: Low rank improves efficiency but may compromise robustness; higher rank improves robustness but reduces efficiency gains
- Failure Signatures: Increased vulnerability to poisoning attacks manifests as degraded performance on clean validation data
- First Experiments: 1) Baseline robustness comparison between LoRA and full fine-tuning, 2) Rank sensitivity analysis, 3) Initialization variance impact study

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Theoretical assumptions about data distribution may not reflect real-world scenarios
- Limited experimental validation to GLUE tasks may not generalize to other domains
- Analysis focuses on specific attack types, potentially missing more sophisticated strategies

## Confidence
- LoRA's differential robustness to backdoor vs poisoning attacks: High
- Impact of rank and initialization variance on security: Medium

## Next Checks
1. Validate theoretical bounds on additional datasets and model architectures beyond GLUE tasks, including vision and multimodal models
2. Test the robustness analysis against adaptive adversaries who can observe and adjust to LoRA's specific properties during training
3. Conduct ablation studies varying the rank and initialization variance parameters to empirically confirm their impact on robustness across different attack types and datasets