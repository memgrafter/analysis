---
ver: rpa2
title: 'OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular
  Fusion'
arxiv_id: '2512.00234'
source_url: https://arxiv.org/abs/2512.00234
tags:
- translation
- fusion
- image
- language
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of cascaded speech translation
  (ST) pipelines, which introduce latency and fail to leverage multimodal context
  like images. It proposes OmniFusion, a method that fuses a multimodal foundation
  model (MMFM) with a specialized translation LLM by extracting and combining hidden
  states from multiple layers of the MMFM, enabling joint end-to-end training.
---

# OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion

## Quick Facts
- arXiv ID: 2512.00234
- Source URL: https://arxiv.org/abs/2512.00234
- Authors: Sai Koneru; Matthias Huck; Jan Niehues
- Reference count: 40
- Simultaneous ST achieves 1-second latency reduction vs. cascaded pipelines while improving translation quality

## Executive Summary
This paper introduces OmniFusion, a method that fuses multimodal foundation models (MMFMs) with translation LLMs by extracting and combining hidden states from multiple layers of the MMFM. The approach enables joint end-to-end training for simultaneous speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion reduces translation latency by approximately 1 second compared to cascaded pipelines while improving translation quality and reducing critical errors. The model also sets state-of-the-art performance on the CoMMuTE benchmark for image-text translation, showing effective utilization of visual context.

## Method Summary
OmniFusion addresses the limitations of cascaded speech translation pipelines by integrating a multimodal foundation model directly with a translation LLM. The key innovation involves extracting hidden states from multiple layers of the MMFM and fusing them with the LLM's representations through learned fusion coefficients. This modular fusion strategy enables the model to jointly process speech and visual inputs while maintaining end-to-end differentiability. The architecture supports various translation modalities including speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. The model is trained jointly on all modalities, allowing it to leverage visual context for improved translation quality while reducing the latency inherent in cascaded approaches.

## Key Results
- Achieves 1-second latency reduction in simultaneous speech translation compared to cascaded pipelines
- Reduces critical and major translation errors compared to baseline approaches
- Sets state-of-the-art performance on CoMMuTE benchmark for image-text translation

## Why This Works (Mechanism)
OmniFusion works by directly fusing multimodal foundation model representations into the translation LLM, eliminating the intermediate ASR step that introduces latency in cascaded pipelines. By extracting and combining hidden states from multiple MMFM layers, the model can effectively integrate visual context into the translation process. The joint end-to-end training allows the model to learn optimal representations for both speech and visual modalities simultaneously, leading to improved translation quality. The modular fusion design enables the model to adapt to different input combinations (speech-only, image-only, or both) while maintaining consistent translation performance.

## Foundational Learning
- Multimodal foundation models: Large models trained on multiple data types that can be adapted for specific tasks
  - Why needed: Provides rich, pre-trained representations for both speech and visual inputs
  - Quick check: Verify MMFM supports required modalities and has sufficient capacity

- Layer-wise hidden state fusion: Extracting and combining representations from multiple neural network layers
  - Why needed: Captures different levels of abstraction from the MMFM for better integration
  - Quick check: Ensure fusion coefficients are properly learned and not degenerate

- Joint end-to-end training: Training all components simultaneously rather than in stages
  - Why needed: Enables optimal adaptation of MMFM and LLM representations for translation
  - Quick check: Monitor training stability and convergence across modalities

## Architecture Onboarding

**Component Map:**
Speech/MT Input -> MMFM (Multiple Layers) -> Fusion Layer -> LLM -> Translation Output

**Critical Path:**
1. Input processing through MMFM to extract multimodal features
2. Layer-wise hidden state extraction and fusion
3. LLM processing with fused representations
4. Translation generation

**Design Tradeoffs:**
- Fusion complexity vs. computational efficiency: More fusion layers provide richer representations but increase computation
- Pre-training vs. fine-tuning: Using pre-trained MMFM reduces training data requirements but may limit adaptation
- Modality flexibility vs. specialization: Supporting multiple input types adds complexity but increases practical utility

**Failure Signatures:**
- Degenerate fusion coefficients indicating poor integration between MMFM and LLM
- Mode collapse where one modality dominates translation decisions
- Training instability due to conflicting gradients from different modalities

**3 First Experiments to Run:**
1. Ablation study removing visual context to verify speech-only performance matches baselines
2. Layer-wise analysis to determine optimal number of MMFM layers for fusion
3. Cross-modal evaluation testing translation quality with mismatched speech-image pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation uses limited data (6k-9k Must-C examples for ST, 200k CoMMuTE examples for image-text)
- Evaluation scope focuses on automatic metrics without comprehensive latency measurements or user studies
- Architectural complexity lacks ablation studies for design choices and hyperparameter sensitivity analysis

## Confidence
High: Core technical contribution of MMFM hidden state fusion is well-described and reproducible
Medium: Simultaneous translation latency claims and multimodal error reduction need more rigorous validation
Low: Scalability to larger datasets, cross-domain generalization, and multilingual performance are untested

## Next Checks
1. Conduct controlled experiments comparing OmniFusion against strong cascaded baselines on larger, more diverse speech translation corpus (100k+ examples)
2. Perform comprehensive simultaneous translation evaluation measuring actual latency distributions and comparison against specialized simultaneous translation systems
3. Execute cross-domain and cross-lingual transfer experiments to assess multimodal fusion benefits beyond English-French and CoMMuTE domains tested