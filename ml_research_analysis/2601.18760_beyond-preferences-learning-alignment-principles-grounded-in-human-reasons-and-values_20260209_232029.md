---
ver: rpa2
title: 'Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons
  and Values'
arxiv_id: '2601.18760'
source_url: https://arxiv.org/abs/2601.18760
tags:
- principles
- constitution
- gcai
- principle
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  to human values in a pluralistic society by proposing Grounded Constitutional AI
  (GCAI), a unified framework for generating constitutions of alignment principles.
  GCAI generates contextual principles from human preference annotations with reasons
  and general principles from human statements of values, improving upon the Inverse
  Constitutional AI (ICAI) approach that relies solely on preference data.
---

# Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values

## Quick Facts
- arXiv ID: 2601.18760
- Source URL: https://arxiv.org/abs/2601.18760
- Reference count: 40
- Participants strongly preferred GCAI-generated constitutions over ICAI across multiple dimensions including moral grounding (96% preference), personal preference (80% preference), and coherence (74% preference)

## Executive Summary
This paper addresses the challenge of aligning large language models to human values in a pluralistic society by proposing Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of alignment principles. GCAI generates contextual principles from human preference annotations with reasons and general principles from human statements of values, improving upon the Inverse Constitutional AI (ICAI) approach that relies solely on preference data. In human evaluations, participants strongly preferred the GCAI-generated constitution over ICAI across multiple dimensions including moral grounding, personal preference, and coherence. The GCAI constitution was considered more ethically grounded and focused on safety and fairness, while ICAI emphasized direct responses and clarity. Models aligned to the GCAI constitution showed consistent thematic differences, with greater emphasis on ethics, safety, and non-discrimination compared to ICAI-aligned models, though both performed similarly on standard benchmarks.

## Method Summary
The paper proposes Grounded Constitutional AI (GCAI), a unified framework that generates alignment constitutions from two sources of human input: preference annotations with accompanying reasons, and direct statements of values. GCAI operates by first generating contextual principles from preference data (similar to ICAI but incorporating reasons), then generating general principles from value statements, and finally synthesizing these into a comprehensive constitution. The framework uses a large language model to extract principles from reasons and values, then aggregates and refines them into coherent alignment guidelines. This approach contrasts with ICAI, which generates constitutions solely from preference data without incorporating human reasoning or explicit value statements.

## Key Results
- Human participants strongly preferred GCAI-generated constitutions over ICAI across multiple dimensions: 96% preference for moral grounding, 80% for personal preference, and 74% for coherence
- GCAI constitutions were considered more ethically grounded and focused on safety and fairness, while ICAI emphasized direct responses and clarity
- Models aligned to GCAI constitutions showed greater emphasis on ethics, safety, and non-discrimination themes compared to ICAI-aligned models, though benchmark performance was similar

## Why This Works (Mechanism)
GCAI works by grounding alignment principles in both human preferences (with reasons) and explicit value statements, creating a more comprehensive and ethically grounded constitution. By incorporating human reasoning alongside preferences, GCAI captures not just what humans prefer but why they prefer it, leading to principles that better reflect underlying human values rather than surface-level preferences. The dual-input approach allows the system to generate both context-specific principles (from preferences with reasons) and general ethical guidelines (from value statements), creating a more robust and nuanced alignment framework. This grounding in human reasons and values makes the resulting principles more likely to reflect genuine ethical considerations rather than merely optimizing for stated preferences.

## Foundational Learning
- **Preference learning with reasons**: Understanding that human preferences alone are insufficient for alignment - why they prefer matters as much as what they prefer
  - Why needed: Pure preference data can lead to optimization for stated preferences without capturing underlying ethical reasoning
  - Quick check: Compare constitutions generated from preferences with vs. without accompanying reasons

- **Value statement extraction**: Ability to identify and extract general principles from explicit human value statements
  - Why needed: Direct value statements provide explicit ethical guidance that may not be captured in preference data alone
  - Quick check: Verify that extracted principles align with stated values and capture their core meaning

- **Constitutional synthesis**: Combining contextual principles (from preferences) with general principles (from values) into coherent alignment guidelines
  - Why needed: Effective alignment requires both specific behavioral guidelines and overarching ethical principles
  - Quick check: Evaluate coherence and consistency of synthesized constitution

## Architecture Onboarding

### Component Map
User Preferences -> Reason Extraction -> Contextual Principles -> Constitutional Synthesis -> Alignment Model
User Values -> Principle Extraction -> General Principles -> Constitutional Synthesis -> Alignment Model

### Critical Path
1. Collect human preference data with reasons and value statements
2. Extract contextual principles from preference data using LLM reasoning
3. Extract general principles from value statements using LLM analysis
4. Synthesize both principle sets into comprehensive constitution
5. Use constitution to align language model behavior

### Design Tradeoffs
- Dual-input vs. single-input: Using both preferences with reasons and value statements provides richer grounding but increases data collection complexity
- Automated vs. human-in-the-loop synthesis: Full automation enables scalability but may miss nuanced ethical considerations
- Principle specificity: Balancing between specific behavioral guidelines and general ethical principles

### Failure Signatures
- Overfitting to specific preference patterns without capturing underlying values
- Principle contradictions between contextual and general principles
- Loss of nuance when transforming human reasons/values into formal principles

### First Experiments
1. Compare constitutions generated from preferences alone vs. preferences with reasons
2. Test alignment model performance using GCAI constitution vs. human-written constitution
3. Evaluate model behavior on ethical dilemmas using constitutions with varying levels of principle specificity

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation relied on only 45 participants, raising questions about statistical power and generalizability
- Comparison based on subjective human preferences rather than objective performance metrics
- Participants knew which constitution they were evaluating, potentially introducing bias

## Confidence

| Claim | Confidence |
|-------|------------|
| Human preference results | Medium |
| Thematic differences in aligned models | High |
| Claims about grounding in human values | Low |

## Next Checks
1. Conduct a larger-scale, blinded human evaluation with statistical power analysis to confirm the preference results and establish confidence intervals
2. Design objective benchmarks that measure the practical impact of GCAI-aligned models on concrete safety and ethical behavior tasks
3. Implement cross-validation where GCAI principles generated from one dataset are evaluated on models trained with different preference data to test generalizability