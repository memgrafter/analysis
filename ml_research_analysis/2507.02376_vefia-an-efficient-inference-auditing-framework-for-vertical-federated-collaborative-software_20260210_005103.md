---
ver: rpa2
title: 'VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative
  Software'
arxiv_id: '2507.02376'
source_url: https://arxiv.org/abs/2507.02376
tags:
- inference
- vefia
- data
- software
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'VeFIA is the first framework to audit inference correctness in
  Vertical Federated Learning. It uses a dual execution path: untrusted inference
  by the data party and trusted inference via a TEE-coordinator pipeline, enabling
  statistical validation without adding online latency.'
---

# VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software

## Quick Facts
- **arXiv ID:** 2507.02376
- **Source URL:** https://arxiv.org/abs/2507.02376
- **Reference count:** 40
- **Primary result:** First framework for auditing inference correctness in Vertical Federated Learning with 99.99% detection probability, 100% precision/recall, and <1% latency overhead

## Executive Summary
VeFIA addresses a critical gap in Vertical Federated Learning by providing the first framework to audit inference correctness when a data party executes software on behalf of a task party. The system creates parallel trusted and untrusted execution paths using TEE and a coordinator, enabling statistical validation of inference results without adding online latency. By leveraging optimal sampling ratios and mutual information-based privacy preservation, VeFIA achieves near-perfect detection rates while maintaining strong privacy guarantees and minimal performance impact.

## Method Summary
VeFIA implements dual-path inference auditing in Vertical Federated Learning by partitioning the bottom model into shallow layers (executed in TEE) and deep layers (executed on coordinator). The framework uses hash-based validation to ensure data and model integrity, random sampling to mask TEE overhead, and mutual information loss to protect against coordinator inference. The optimal sampling ratio W* is calculated to ensure trusted execution completes within the untrusted path's latency. Privacy is preserved through learned perturbations that minimize information leakage to the coordinator while maintaining model accuracy.

## Key Results
- Achieves 99.99% Detection Success Rate when anomalies exceed 5.4%
- Maintains 100% precision, recall, and negative predictive value in sampling validation
- Reduces privacy leakage by 72.9% compared to baseline methods
- Adds only 0.56% average latency overhead through pipeline acceleration

## Why This Works (Mechanism)

### Mechanism 1: Dual Execution Path Validation
Running parallel trusted (TEE-COO) and untrusted (GPU) inference paths allows the task party to detect software execution anomalies by comparing outputs. Any mismatch indicates the data party loaded incorrect data or models. The TEE-COO path remains immune to tampering through SGX remote attestation and integrity protection, while numerical precision differences are managed to avoid false positives.

### Mechanism 2: Zero-Latency Statistical Auditing
Random sampling at optimal ratio W* guarantees anomaly detection without increasing online inference latency. By scheduling W·N trusted inferences to complete within N untrusted inferences (T_un ≈ W·T_tr), the framework masks auditing overhead. The sampling ratio is calculated from profiling measurements and ensures detection when abnormal inference rates exceed theoretical bounds.

### Mechanism 3: Privacy-Preserving Coordinator Design
Minimizing mutual information between raw inputs and intermediate representations prevents the coordinator from reconstructing private data. Learned perturbations are added to intermediate representations and optimized through mutual information neural estimation during training. This reduces data leakage risk by 72.9% while maintaining model utility with only 0.51% accuracy degradation.

## Foundational Learning

- **Concept: Vertical Federated Learning (VFL)** - Required because VeFIA specifically addresses VFL where features are split across parties for the same users, unlike horizontal FL where data is partitioned by users. Quick check: Can you explain why VFL requires online interaction during inference, whereas HFL often allows independent local inference?

- **Concept: Trusted Execution Environments (TEE/SGX)** - Required because TEE provides the root of trust for auditing through enclave isolation from host OS. Quick check: Does a TEE protect against a malicious cloud administrator who has physical access to the machine but not the enclave's encryption keys?

- **Concept: N-Version Programming** - Required because VeFIA's dual-path design is inspired by this error detection approach using distinct implementations of the same function. Quick check: In VeFIA, are the "untrusted" and "trusted" paths running different algorithms, or the same algorithm in different environments?

## Architecture Onboarding

- **Component map:** Task Party (P_t) -> Data Party (P_d) -> Coordinator (C) -> TEE Enclave
- **Critical path:** P_t sends sample IDs to P_d → P_d runs untrusted GPU inference for all IDs → P_d's TEE selects random subset (W*) → TEE validates hashes → TEE runs trusted inference (shallow in TEE → deep in C) → P_t compares trusted vs untrusted results
- **Design tradeoffs:** Increasing sampling ratio W increases detection coverage but risks breaching latency masking constraint; increasing perturbation λ increases defense against C but degrades model accuracy
- **Failure signatures:** Hash mismatch (runtime authenticity validation failure), output divergence (inference consistency failure), timeout (pipeline bottleneck breaking zero-latency promise)
- **First 3 experiments:** 1) Measure T_un (GPU) and T_tr (TEE-COO) to calculate optimal W*; 2) Run identical inputs through GPU and TEE paths to ensure floating-point differences don't trigger false positives; 3) Manually inject wrong model version into untrusted path to verify 100% TPR

## Open Questions the Paper Calls Out

### Open Question 1
How can VeFIA be extended to audit Stage 1 (data preprocessing) and Stage 2 (joint training) of the VFL lifecycle? The current framework relies on comparing runtime inference results against a pre-trained trusted baseline; auditing iterative gradient updates and data alignment requires different verification mechanisms than the TEE-COO inference path.

### Open Question 2
How can the auditing mechanism adapt for scenarios where the Coordinator is not a neutral regulatory body but a potentially untrusted entity? VeFIA currently offloads the deep bottom model to C under semi-honest assumptions; a malicious C could compromise the trusted path or inference results, breaking the auditing logic.

### Open Question 3
Can the framework evolve from anomaly detection to automated system resilience by defining protocols for isolating non-compliant parties? Current design flags inconsistencies but lacks automated logic to handle state management, financial settlement, and service continuity required to "quarantine" a data party in real-time.

## Limitations

- Numerical precision differences between TEE and GPU environments may trigger false positives, though the paper claims 100% NPV indicates this is handled
- Framework effectiveness depends on optimal shallow/deep model partition boundary, which may vary across architectures
- Security guarantees rely on SGX remote attestation and integrity protection, vulnerable to implementation flaws or side-channel attacks

## Confidence

- **High Confidence:** Detection success rates (≥99.99%), privacy leakage reduction (72.9%), and latency overhead (<1%) are well-supported by experimental results
- **Medium Confidence:** Mutual information-based privacy preservation mechanism is theoretically sound but relies on accurate MINE estimation
- **Low Confidence:** "Accidental scenario" injection methodology lacks precise specification of noise magnitudes and feature replacement rules

## Next Checks

1. **Numerical Stability Verification:** Implement controlled experiment comparing identical inputs through GPU and TEE paths across different floating-point precisions to measure false positive rates and validate 100% NPV claim

2. **SGX Security Audit:** Conduct formal security review of TEE implementation focusing on attestation protocols, side-channel resistance, and memory isolation to verify security foundation

3. **Model Partition Sensitivity Analysis:** Systematically vary shallow/deep partition boundary across different model architectures and datasets to assess impact on detection accuracy and TEE overhead