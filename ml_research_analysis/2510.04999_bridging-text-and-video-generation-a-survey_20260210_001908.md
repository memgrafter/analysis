---
ver: rpa2
title: 'Bridging Text and Video Generation: A Survey'
arxiv_id: '2510.04999'
source_url: https://arxiv.org/abs/2510.04999
tags:
- video
- arxiv
- generation
- preprint
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of text-to-video (T2V)
  generation models, covering their evolution from early GANs and VAEs to modern diffusion-based
  architectures. It systematically reviews model methodologies, training datasets,
  and configurations, including hardware specifications and hyperparameters.
---

# Bridging Text and Video Generation: A Survey

## Quick Facts
- arXiv ID: 2510.04999
- Source URL: https://arxiv.org/abs/2510.04999
- Authors: Nilay Kumar; Priyansh Bhandari; G. Maragatham
- Reference count: 40
- Primary result: Comprehensive survey of text-to-video generation models from GANs/VAEs to modern diffusion architectures, covering methodologies, datasets, evaluation metrics, and future research directions.

## Executive Summary
This survey systematically examines the evolution and current state of text-to-video (T2V) generation, tracing development from early GAN-based approaches through VAE architectures to contemporary diffusion models. The paper provides a comprehensive overview of model architectures, training methodologies, evaluation frameworks, and key challenges facing the field. It highlights the shift toward diffusion-based approaches for improved temporal coherence and visual quality, while identifying critical limitations in alignment, long-range consistency, and computational efficiency that must be addressed for broader real-world deployment.

## Method Summary
The survey analyzes three primary architectural paradigms for T2V generation: GAN-based models (MoCoGAN), VAE-based approaches (VideoGPT, GODIVA, CogVideo), and diffusion models (Make-A-Video, VideoFusion, Latent-Shift, LaVie, CogVideoX, Pyramidal Flow). Training configurations vary significantly, with examples including VideoGPT (batch 128, LR 1e-4, Adam), CogVideo (32×A100, batch 256, 1M steps), and Pyramidal Flow (128×A100, batch 1536/768/384, 300k steps). Key innovations include noise decomposition for temporal coherence, temporal shift modules for frame-to-frame consistency, and cascaded generation pipelines for computational efficiency. Evaluation employs metrics like FVD↓, IS↑, CLIP-SIM↑, KVD↓, supplemented by human evaluation and comprehensive frameworks like VBench.

## Key Results
- Diffusion models with hierarchical noise decomposition (base+residual) achieve superior temporal coherence compared to GAN-based approaches
- Temporal shift modules enable frame-to-frame coherence without additional trainable parameters through feature map shifting
- Cascaded generation pipelines (base generation → interpolation → super-resolution) balance computational efficiency with output quality
- Current evaluation frameworks like VBench provide more comprehensive assessment across 16 perceptual dimensions than single-score metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models with noise decomposition across frames yield better temporal coherence than GAN-based approaches
- Mechanism: Models like VideoFusion decompose video synthesis into base noise (shared across frames for static content) and residual noise (frame-specific for motion dynamics). This separation allows the model to maintain spatial consistency while capturing temporal variation. The base generator uses a pretrained image DPM for shared visual content, while the residual generator adds frame-specific variations.
- Core assumption: Decomposing noise into shared and frame-specific components preserves both spatial and temporal information more effectively than treating all frames independently
- Evidence anchors: [abstract]: "shift toward diffusion models for improved quality and temporal consistency"; [section 3.3.2]: "hierarchical noise decomposition, emitting base noise shared between frames and residual noise with only time-specific variations"; [corpus]: Weak corpus support—neighbor papers focus on narrative coherence and debiasing rather than noise decomposition specifically
- Break condition: If base noise dominates excessively, videos become static slideshows; if residual noise dominates, temporal coherence degrades into flickering frames

### Mechanism 2
- Claim: Temporal shift modules enable frame-to-frame coherence in latent diffusion models without adding trainable parameters
- Mechanism: Latent-Shift inserts a parameter-free temporal shift module within residual branches of ResNet blocks. Feature maps shift across the temporal dimension, allowing information from adjacent frames to influence current frame processing. This creates temporal awareness through data movement rather than learned weights.
- Core assumption: Shifting feature maps across time provides sufficient temporal context for coherence without explicit temporal modeling parameters
- Evidence anchors: [section 3.3.3]: "temporal shift module is adjusted within the residual branch of each ResNet block, which allows the feature maps to move across the temporal dimension"; [section 3.3.3]: "allows for temporal coherence between frames without requiring additional parameters"; [corpus]: No direct corpus validation for temporal shift specifically
- Break condition: If shift magnitude is misaligned with motion speed, fast-moving objects may blur or slow motions may appear jumpy

### Mechanism 3
- Claim: Cascaded generation pipelines (base generation → interpolation → super-resolution) balance computational efficiency with output quality better than single-stage generation
- Mechanism: Models like LaVie and Make-A-Video use multi-stage pipelines: (1) generate low-resolution base frames from text, (2) interpolate frames for temporal smoothness, (3) apply spatiotemporal super-resolution. Each stage operates on progressively refined outputs, distributing computational load.
- Core assumption: Decomposing generation into specialized stages allows each component to optimize for a specific aspect (content, temporal smoothness, visual fidelity) without competing objectives
- Evidence anchors: [section 3.3.5]: "Base Text-to-Video model... Temporal Interpolation module... Video Super-Resolution module"; [section 3.3.1]: "SRt is a spatiotemporal network that increases resolution... whereas SRh is used for spatial super-resolution"; [corpus]: SeqBench and Step-Video-T2V papers reference multi-stage approaches but focus on narrative/event coherence
- Break condition: If early-stage errors propagate through cascades, later stages amplify artifacts rather than correct them

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: All modern T2V architectures surveyed build on diffusion foundations; understanding forward/reverse processes is essential for grasping how video frames emerge from noise
  - Quick check question: Can you explain why classifier-free guidance adjusts predictions as ε̂ = (1+w)ε(y) - wε() and what happens when w=0 versus w>1?

- Concept: Variational Autoencoder Latent Spaces
  - Why needed here: Models like VideoGPT and latent diffusion operate in compressed latent representations; understanding ELBO, KL divergence, and reconstruction tradeoffs explains why some models favor discrete (VQ-VAE) versus continuous latents
  - Quick check question: If a VAE's KL term is weighted too low, what artifact appears in the latent space, and how would this affect video generation diversity?

- Concept: 3D Convolutions and Spatiotemporal Attention
  - Why needed here: T2V extends 2D image processing to 3D (height×width×time); pseudo-3D convolutions and spatiotemporal transformers are architectural primitives across surveyed models
  - Quick check question: Why would a 3D convolution with kernel size (3,3,3) be more expensive than a pseudo-3D approach using (1,3,3) followed by (3,1,1)?

## Architecture Onboarding

- Component map: Text Prompt → Text Encoder (CLIP/T5) → Text Embeddings → Latent Diffusion U-Net with Temporal Layers → 3D VAE Decoder → Video Frames → [Optional] Super-Resolution Cascade

- Critical path: Text-video alignment through cross-attention → temporal coherence through temporal attention/shift → visual fidelity through decoder. Failures in cross-attention cause semantic misalignment; failures in temporal layers cause flickering

- Design tradeoffs:
  - Discrete (VQ-VAE) vs. continuous latents: Discrete enables autoregressive modeling but loses fine detail; continuous preserves detail but requires careful regularization
  - Pseudo-3D vs. full 3D: Pseudo-3D reduces memory but may miss complex spatiotemporal interactions
  - Training-free vs. fine-tuned temporal layers: Training-free (e.g., Free-Bloom, FIFO-Diffusion) saves compute but may sacrifice coherence

- Failure signatures:
  - Flickering frames: Temporal attention/shift module underperforming or noise decomposition imbalance
  - Semantic drift: Cross-attention not properly conditioning on text; check guidance scale and text embedding quality
  - Motion freezing: Base noise over-dominating residual noise in decomposition schemes
  - Artifact propagation in cascades: Early-stage errors amplified; inspect base model outputs before interpolation/SR

- First 3 experiments:
  1. **Baseline reproduction**: Implement Latent-Shift's temporal shift module on a pretrained Stable Diffusion checkpoint; validate on UCF-101 with FVD and CLIP-SIM metrics (targets: FVD <350, CLIP-SIM >0.28 based on Table 2 benchmarks)
  2. **Noise decomposition ablation**: Train VideoFusion-style base+residual generators on BAIR robot dataset; ablate residual noise contribution (0%, 50%, 100%) and measure temporal coherence via motion smoothness metrics
  3. **Cascaded vs. single-stage comparison**: Compare LaVie's three-stage pipeline against a single high-resolution diffusion model; measure GPU-hours per video, FVD, and human preference scores for equivalent output resolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synthetic datasets generated by game engines (e.g., Unity, Unreal) effectively replace real-world scraped data to overcome copyright restrictions and scarcity in training generalizable text-to-video models?
- Basis in paper: [explicit] Section 6.1 proposes using game engines to generate large-scale, high-resolution datasets to address the legal and availability limitations of current datasets like WebVid-10M
- Why unresolved: Current models rely on web-scraped data which faces legal challenges (e.g., Shutterstock cease-and-desist), and it is unproven if synthetic data can capture the complexity of real-world dynamics
- What evidence would resolve it: Comparative benchmarking (FVD, CLIP) of models trained exclusively on synthetic engine data against those trained on real-world datasets like UCF-101 or MSR-VTT

### Open Question 2
- Question: How can future architectures integrate physical constraints to minimize spatial incoherence, such as objects shifting or disappearing, and improve the plausibility of motion in long-duration videos?
- Basis in paper: [explicit] Section 6.2 identifies the "interaction of physics" as mostly unrealistic and spatial incoherence as a factor that breaks viewer immersion, calling for models that better simulate real-world physics
- Why unresolved: Current generative paradigms (GANs, VAEs, Diffusion) prioritize visual fidelity and text alignment but lack inherent mechanisms to enforce physical laws or long-term spatial consistency
- What evidence would resolve it: The development of evaluation metrics and models that specifically score "physical plausibility" and maintain object permanence significantly better than current state-of-the-art models

### Open Question 3
- Question: Do multi-dimensional evaluation frameworks like VBench provide a statistically significant improvement in correlating with human perception over single-score metrics like FVD or Inception Score?
- Basis in paper: [inferred] Section 5.3 discusses the limitations of FVD and IS in capturing "human-centered characteristics" and introduces VBench, suggesting a need to validate this new evaluation paradigm
- Why unresolved: The field lacks a unified standard that accurately reflects subjective human preferences regarding temporal coherence and aesthetic quality
- What evidence would resolve it: Large-scale correlation studies comparing human ranking results against VBench dimension scores versus traditional metric scores for diverse video generation models

## Limitations
- Evaluation metrics lack standardized thresholds; acceptable ranges vary across datasets and model scales
- Proprietary dataset dependencies: Key training datasets (WebVid-10M, HowTo100M) are unavailable since February 2024
- Limited ablation studies on architectural components: Survey presents architectures but lacks systematic ablation experiments quantifying individual module contributions

## Confidence
- High Confidence: Diffusion models represent current state-of-the-art for T2V generation; cascaded architectures improve efficiency without sacrificing quality
- Medium Confidence: Temporal shift modules enable coherence without parameters (mechanistic explanation clear, but limited independent validation in corpus)
- Low Confidence: Specific quantitative thresholds for "good" performance across all metrics (values vary significantly based on dataset and model scale)

## Next Checks
1. Reproduce baseline performance: Implement Latent-Shift's temporal shift module on Stable Diffusion; validate on UCF-101 with FVD <350, CLIP-SIM >0.28
2. Test noise decomposition robustness: Train VideoFusion-style base+residual generators on BAIR; ablate residual noise (0%, 50%, 100%) and measure temporal coherence
3. Benchmark cascaded vs. single-stage: Compare LaVie's three-stage pipeline against single high-resolution diffusion model; measure GPU-hours, FVD, and human preference scores