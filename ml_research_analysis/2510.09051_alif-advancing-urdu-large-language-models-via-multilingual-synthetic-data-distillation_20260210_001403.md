---
ver: rpa2
title: 'Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data
  Distillation'
arxiv_id: '2510.09051'
source_url: https://arxiv.org/abs/2510.09051
tags:
- urdu
- b-instruct
- dataset
- translation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Alif-1.0-8B-Instruct, a multilingual Urdu-English
  language model developed to address the scarcity of high-quality datasets and cultural
  nuance in existing Urdu LLMs. The model is trained using a modified self-instruct
  technique on a high-quality synthetic dataset (Urdu-Instruct) that incorporates
  Urdu-native chain-of-thought reasoning, bilingual translation, and cultural sensitivity.
---

# Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation

## Quick Facts
- **arXiv ID**: 2510.09051
- **Source URL**: https://arxiv.org/abs/2510.09051
- **Reference count**: 26
- **Primary result**: Alif-1.0-8B-Instruct outperforms Llama-3.1-8B-Instruct, Mistral-7B-Instruct, Qwen-2.5-7B-Instruct, and Aya-Expanse-8B on Urdu benchmarks while maintaining English fluency, trained under $100 budget

## Executive Summary
This paper introduces Alif-1.0-8B-Instruct, a multilingual Urdu-English language model designed to address the scarcity of high-quality datasets and cultural nuance in existing Urdu LLMs. The model is trained using a modified self-instruct technique on a high-quality synthetic dataset (Urdu-Instruct) that incorporates Urdu-native chain-of-thought reasoning, bilingual translation, and cultural sensitivity. Built on Llama-3.1-8B, Alif-1.0-8B-Instruct achieves an average score of 75.5 across Urdu-translated benchmarks (MGSM, Alpaca Eval, Dolly General QA), outperforming leading multilingual models while maintaining strong English performance and mitigating catastrophic forgetting through replay datasets.

## Method Summary
Alif-1.0-8B-Instruct is developed through a two-phase training pipeline: continued pre-training on 200K Urdu Wikipedia articles (1 epoch, LR 2e-5, 23h on A100 80GB) followed by instruction fine-tuning on a 105K instruction mix including Urdu-Instruct (51,686 synthetically generated examples), translated datasets, and replay datasets to prevent catastrophic forgetting. The Urdu-Instruct dataset is generated using a modified self-instruct technique with unique prompts and seeds per task, global task pool, and GPT-4o-generated examples with ROUGE<0.7 deduplication. The model uses LoRA fine-tuning with rank=128 across QKVO, MLP, and ET-LH modules, trained with BF16 precision using Unsloth and evaluated via GPT-4o LLM-as-judge on Urdu-translated benchmarks.

## Key Results
- Achieves average score of 75.5 across Urdu-translated benchmarks (MGSM, AlpacaEval, Dolly QA)
- Outperforms Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct, and Cohere-Aya-Expanse-8B on Urdu-specific tasks
- Maintains English performance with minimal catastrophic forgetting despite focus on Urdu
- Trained within budget of under $100 using 39 total training hours on A100 80GB

## Why This Works (Mechanism)
The model's success stems from the modified self-instruct technique that generates high-quality, culturally nuanced Urdu instruction data through unique prompts and global task pools. By incorporating Urdu-native chain-of-thought reasoning and bilingual translation capabilities, the synthetic dataset addresses the scarcity of high-quality Urdu instruction data while maintaining linguistic authenticity. The inclusion of replay datasets (English Alpaca, OpenOrca) effectively mitigates catastrophic forgetting, allowing the model to preserve English capabilities while specializing in Urdu tasks. The comprehensive task mix covering mathematics, reasoning, and cultural scenarios ensures robust performance across diverse Urdu language applications.

## Foundational Learning
- **Self-instruct technique**: Automated instruction dataset generation using language models to create high-quality training examples
  - Why needed: Overcomes scarcity of high-quality Urdu instruction datasets
  - Quick check: Verify synthetic examples cover diverse task types with appropriate complexity
- **Chain-of-thought reasoning**: Step-by-step logical reasoning process for complex problem solving
  - Why needed: Enables systematic problem-solving in Urdu mathematical and logical tasks
  - Quick check: Examine reasoning traces for cultural appropriateness and logical coherence
- **Catastrophic forgetting**: Loss of previously learned capabilities when training on new tasks
  - Why needed: Critical to maintain English performance while specializing in Urdu
  - Quick check: Compare performance on English benchmarks before and after Urdu fine-tuning
- **LLM-as-judge evaluation**: Using language models to evaluate generated responses instead of human annotators
  - Why needed: Scalable evaluation method for large instruction-tuned models
  - Quick check: Validate GPT-4o judge consistency across multiple evaluations
- **Low-resource language modeling**: Techniques for developing NLP systems with limited training data
  - Why needed: Urdu has significantly less high-quality training data compared to English
  - Quick check: Assess synthetic data quality and diversity across Urdu dialects
- **Bilingual instruction tuning**: Training models on parallel instruction datasets in multiple languages
  - Why needed: Enables seamless switching between Urdu and English tasks
  - Quick check: Test model performance on mixed-language instruction prompts

## Architecture Onboarding

**Component Map**: Base Llama-3.1-8B -> Continued Pre-training (Urdu Wikipedia) -> Fine-tuning (Urdu-Instruct + Replay Datasets) -> LoRA Modules (QKVO, MLP, ET-LH) -> Evaluation (GPT-4o Judge)

**Critical Path**: Synthetic dataset generation → Continued pre-training → LoRA fine-tuning → LLM-as-judge evaluation

**Design Tradeoffs**: Modified self-instruct provides high-quality synthetic data but requires careful prompt engineering and deduplication; replay datasets preserve English capabilities but increase training complexity; LoRA fine-tuning reduces computational cost but may limit adaptation capacity

**Failure Signatures**: Catastrophic forgetting of English capabilities (monitor via MMLU/ARC/HellaSwag); low-quality Urdu generation without cultural nuance; poor bilingual translation accuracy; overfitting to synthetic data patterns

**First 3 Experiments**:
1. Generate 100 synthetic Urdu-Instruct examples using the described methodology and evaluate with GPT-4o judge to verify prompt effectiveness
2. Run continued pre-training on 10K Urdu Wikipedia articles with LoRA rank=32 to validate training pipeline and data quality
3. Fine-tune on 1K Urdu-Instruct examples with replay datasets and evaluate on Urdu-translated MGSM subset to test catastrophic forgetting mitigation

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the modified self-instruct technique with unique prompts and global task pools generalize to other low-resource languages beyond Urdu? The paper states this "scalable method improves instruction quality and can be adapted to other low-resource languages for broader NLP development" but only validates for Urdu, requiring experiments on diverse languages like Swahili, Tamil, or Vietnamese.
- **Open Question 2**: To what extent do objective metrics (BLEU, COMET, BERTScore, F1) correlate with LLM-as-judge evaluations for Urdu generation quality and cultural alignment? The conclusion mentions future incorporation of these metrics, but no analysis exists on their correlation with GPT-4o judgments for Urdu.
- **Open Question 3**: What mechanisms can better preserve domain-specific knowledge (particularly STEM and humanities) during multilingual fine-tuning while improving low-resource language performance? Table 4 shows Alif underperforms on MMLU, STEM, and humanities domains despite replay dataset usage, requiring ablation studies of alternative approaches.

## Limitations
- Training budget claim of <$100 appears aggressive and requires verification of cloud GPU costs and precise training time accounting
- Evaluation relies entirely on GPT-4o as judge for Urdu-translated benchmarks, introducing potential cultural nuance blindness and subjective bias
- Lack of publicly available Urdu-Instruct dataset and task templates limits reproducibility of the synthetic dataset generation process

## Confidence
- **High confidence**: Technical training methodology (continued pre-training → SFT pipeline, LoRA configuration, software stack) is well-specified and reproducible
- **Medium confidence**: Outperformance claims on Urdu-translated benchmarks depend on GPT-4o's cultural sensitivity for Urdu
- **Medium confidence**: Budget claim ($<100) is plausible given training time but requires cost verification
- **Low confidence**: Catastrophic forgetting mitigation effectiveness cannot be verified without English capability baselines

## Next Checks
1. Recreate Urdu-Instruct task templates using the described methodology (7 task types, native CoT reasoning, bilingual translation) and validate against GPT-4o judging criteria to assess reproducibility
2. Benchmark Alif-1.0-8B-Instruct against native Urdu speakers on cultural nuance tasks to validate GPT-4o evaluation results and identify evaluation blindspots
3. Verify training cost accounting by calculating actual GPU-hours costs for the 39 total training hours on A100 80GB across major cloud providers, including all overhead