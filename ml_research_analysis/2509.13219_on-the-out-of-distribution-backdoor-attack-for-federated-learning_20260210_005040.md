---
ver: rpa2
title: On the Out-of-Distribution Backdoor Attack for Federated Learning
arxiv_id: '2509.13219'
source_url: https://arxiv.org/abs/2509.13219
tags:
- local
- malicious
- soda
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OOD backdoor attacks (OBA) in federated learning
  (FL), where OOD data is used as both poisoned samples and triggers to bypass visible-trigger
  constraints. To enhance stealth, the authors propose SoDa, which aligns malicious
  models with benign ones using self-reference training and regularization.
---

# On the Out-of-Distribution Backdoor Attack for Federated Learning

## Quick Facts
- **arXiv ID:** 2509.13219
- **Source URL:** https://arxiv.org/abs/2509.13219
- **Reference count:** 40
- **Primary result:** Introduces OOD backdoor attacks (OBA) in federated learning, where OOD data serves as both poisoned samples and triggers, and proposes SoDa for stealth and BNGuard for detection.

## Executive Summary
This paper introduces Out-of-Distribution (OOD) backdoor attacks in federated learning, leveraging OOD data as both poisoned samples and triggers to bypass visible-trigger constraints. The authors propose SoDa, a stealthy attack method that aligns malicious models with benign ones using self-reference training and regularization. They also introduce BNGuard, a server-side defense that detects malicious updates by analyzing running statistics of the first batch normalization layer. Experiments demonstrate SoDa's high attack success rates while evading state-of-the-art defenses, and BNGuard's superior robustness in mitigating OBA threats.

## Method Summary
The paper presents SoDa, an OOD backdoor attack that uses self-reference training to align malicious local models with benign reference models, enhancing stealth. Malicious clients perform two training phases: first on clean data to create a reference model, then on poisoned data (OOD samples relabeled to a target class) with regularization terms to match the reference model's magnitude and direction. The server-side defense BNGuard detects malicious updates by extracting and clustering running statistics from the first batch normalization layer, filtering out the minority cluster. The approach is evaluated across multiple datasets and FL settings.

## Key Results
- SoDa achieves high attack success rates (ASR) while evading SOTA defenses like FLAME and MKrum through self-reference alignment.
- BNGuard significantly lowers ASR and maintains model accuracy across various settings by detecting OOD-induced statistical deviations in BN layers.
- BNGuard demonstrates superior robustness compared to other defenses, maintaining low false positive rates while effectively mitigating OBA threats.

## Why This Works (Mechanism)

### Mechanism 1: OOD Data as Dual-Function Trigger
- Claim: Using OOD data as both poisoned samples and triggers simultaneously broadens attack scenarios and improves stealth compared to artificial, visible triggers.
- Mechanism: Malicious clients replace a portion (e.g., 30%) of their in-distribution (ID) training data with OOD samples (e.g., MNIST when CIFAR-10 is the main task), assigning all OOD samples a common target label from the ID label space. The model learns to associate the OOD pattern with the target class. At inference, any natural occurrence of that OOD type (e.g., a house number in a traffic scene) acts as the trigger.
- Core assumption: The FL server has no visibility into local datasets, allowing clients to modify their data freely. The OOD data is sufficiently distinct from ID data to create a learnable association.
- Evidence anchors:
  - [abstract] "...uses OOD data as both poisoned samples and triggers simultaneously."
  - [Section 3] "OOD samples are then injected into the local models during training... elements like trees or house numbers along the roadside can serve as effective and inconspicuous triggers."
  - [corpus] Related work (e.g., "Scanning Trojaned Models Using Out-of-Distribution Samples") uses OOD for detection, not attack, indicating OOD's known utility in FL security contexts.
- Break condition: If the OOD data is too similar to ID data (low distribution shift), the model may not form a strong, distinct backdoor association, reducing attack success rate.

### Mechanism 2: SoDa Stealth via Self-Reference Alignment
- Claim: Aligning malicious local models with benign reference models in both magnitude and direction evades server-side defenses that rely on statistical anomaly detection.
- Mechanism: Each malicious client first performs a "self-reference training" phase on its *clean* local data to produce a benign reference model (`θ_i,ref`). It then trains the malicious model on the *poisoned* dataset, adding two regularization terms to the loss: L2 norm (`L_m = ||θ - θ_i,ref||²`) for magnitude alignment and Cosine similarity loss (`L_d = 1 - <θ, θ_i,ref> / (||θ||₂ ||θ_i,ref||₂)`) for directional alignment. This forces the malicious update to resemble a benign one.
- Core assumption: Malicious clients have sufficient clean data to train a representative reference model. The server's defenses inspect aggregate statistics (e.g., Euclidean distance, Cosine similarity) rather than per-layer internals.
- Evidence anchors:
  - [abstract] "SoDa... regularizes both the magnitude and direction of malicious local models... aligning them closely with their benign versions."
  - [Section 4] Formal loss function and Figure 2 showing SoDa achieves high False Positive Rates (FPR) and Attack Success Rates (ASR) against FLAME and MKrum.
  - [corpus] Neighboring papers (e.g., "Neurotoxin") also use gradient manipulation for stealth, confirming alignment as a viable strategy, though SoDa's self-reference is a novel approach in this corpus.
- Break condition: If the server uses defenses that inspect internal layer statistics (like BNGuard), this alignment mechanism fails. Also, in synchronized FL, the extra computation for self-reference training may cause malicious clients to miss update deadlines.

### Mechanism 3: BNGuard Detection via BN Statistics
- Claim: Examining running statistics (mean and variance) from the first Batch Normalization (BN) layer can identify malicious models because OOD data causes statistically significant deviations.
- Mechanism: The server reconstructs each local model from its update. It extracts the running mean (`μ_r`) and variance (`σ_r`) from the *first* BN layer (as it's closest to raw input). It computes cross-channel statistics (mean and variance of `μ_r` and `σ_r` across channels), creating a 4-feature vector per client. These vectors are clustered (e.g., via K-Means) into two clusters. The larger cluster is deemed benign, and only its updates are aggregated.
- Core assumption: Models use BN layers. Benign clients have statistically similar local data (ID data), leading to similar BN statistics. Malicious clients' OOD data creates a distinct statistical fingerprint in the first BN layer.
- Evidence anchors:
  - [abstract] "BNGuard leverages the observation that OOD data causes significant deviations in the running statistics of batch normalization layers."
  - [Section 5.2 & Figure 3] Clear separation in PCA plots of first BN layer statistics between malicious and benign models. Algorithm 1 details the process.
  - [corpus] The use of BN statistics for FL security is supported by related work like "FedBN" (cited in paper) and "BackdoorIndicator" (in references), though BNGuard's specific first-layer focus is a novel contribution.
- Break condition: If models do not use BN layers (e.g., using Layer Normalization), this defense is inapplicable. In highly heterogeneous (non-IID) benign data, natural divergence in BN statistics might increase false positives.

## Foundational Learning

- **Concept:** **Federated Learning (FL) with FedAvg**
  - Why needed here: The entire attack/defense framework is built upon the FL paradigm, specifically the FedAvg algorithm where a central server aggregates local model updates.
  - Quick check question: Can you explain how model updates (`Δᵢ`) are calculated and aggregated in a standard FedAvg round?

- **Concept:** **Batch Normalization (BN) Layer Operation**
  - Why needed here: The core defense (BNGuard) relies on understanding how BN layers compute and store running statistics during training and use them during inference.
  - Quick check question: During inference, does a BN layer use statistics from the current batch or accumulated running statistics from training? Why does this matter for BNGuard?

- **Concept:** **Backdoor Attack in Machine Learning**
  - Why needed here: This paper advances backdoor attacks by introducing OOD triggers. Understanding the basic goal (maintain main task accuracy while causing targeted misclassification on triggered inputs) is essential.
  - Quick check question: In a traditional backdoor attack, what is the role of the "trigger," and how does the OBA attack change this concept?

## Architecture Onboarding

- **Component map:** FL Server -> Receives updates from Benign Clients (ID data) and Malicious Clients (ID+OOD data) -> Aggregates updates (FedAvg) -> BNGuard defense (if enabled) -> Global model
- **Critical path:** The critical path for system security runs from the *malicious client's data poisoning* -> *SoDa's stealthy alignment training* -> *submission of aligned malicious update* -> *BNGuard's statistical extraction and clustering*. If any link fails (e.g., poor alignment, BNGuard detection), the attack's success or stealth is compromised.
- **Design tradeoffs:**
  - **Attack (SoDa):** Higher stealth (lower detection) vs. higher computational cost (extra self-reference training). The `λₘ` and `λₑ` coefficients trade off attack success (ASR) vs. evasion capability.
  - **Defense (BNGuard):** Higher robustness (low ASR, FPR) vs. applicability (only works on BN-equipped models). Using the *first* BN layer vs. others trades detection sensitivity vs. robustness to deeper network transformations.
- **Failure signatures:**
  - **Attack Failure:** Main Task Accuracy (MA) drops significantly (backdoor disrupts primary learning). Low Attack Success Rate (ASR). High False Positive Rate (FPR) against the attack (malicious clients are caught).
  - **Defense Failure:** High ASR (backdoor succeeds). High False Positive Rate (benign clients incorrectly flagged). High computational overhead on the server.
- **First 3 experiments:**
  1. **Baseline Reproduction:** Replicate the naive OBA attack (without SoDa) against FedAvg. Measure ASR and MA on CIFAR-10 with MNIST as OOD data. This establishes the attack's potency before adding stealth.
  2. **SoDa Stealth Evaluation:** Implement SoDa with self-reference training and alignment losses (`λₘ=0.1, λₑ=100` as per paper). Test against two basic defenses: FedAvg (no defense) and a magnitude-based filter (e.g., MKrum). Report ASR, MA, and the defense's FPR. Compare to the baseline OBA.
  3. **BNGuard Implementation:** Implement the BNGuard defense using first BN layer statistics and K-Means clustering. Test it against the SoDa attack from experiment 2. Measure ASR, MA, and BNGuard's own FPR (benign clients wrongly excluded). Compare to the performance of MKrum from experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of self-reference training be reduced or eliminated to ensure OBA remains effective in strictly synchronized FL environments?
- Basis: [explicit] Page 9 states that the extra training time required for self-reference "may limit SoDa’s effectiveness in synchronized FL settings" and encourages "further exploration of strategies to enhance SoDa."
- Why unresolved: The proposed alternatives, SoDa_A and SoDa_B, are either less effective or rely on utilizing gap time, which may not exist in rigid synchronous protocols.
- Evidence: An attack strategy that maintains >90% ASR against BNGuard without increasing local computation time per round.

### Open Question 2
- Question: How can servers effectively detect OOD backdoor attacks in FL systems utilizing normalization-free architectures or alternative normalization techniques (e.g., Layer Normalization)?
- Basis: [inferred] Page 4 and Algorithm 1 explicitly rely on extracting running statistics (mean/variance) from Batch Normalization layers, limiting the defense to models utilizing BN.
- Why unresolved: The detection mechanism fails if the model architecture does not maintain running global statistics equivalent to BN’s $\mu_r$ and $\sigma_r$.
- Evidence: A defense mechanism that successfully identifies malicious updates in normalization-free models (e.g., ResNet without BN) under SoDa attack.

### Open Question 3
- Question: Is BNGuard robust against adaptive adversaries who explicitly optimize malicious updates to manipulate batch normalization statistics to match benign clusters?
- Basis: [inferred] The threat model (Page 3) assumes the adversary has "no information about the defense mechanisms," implying the defense's resilience is untested against informed attackers targeting the specific BN statistical features.
- Why unresolved: The defense depends on a clear clustering separation; an adaptive loss function could potentially regularize the malicious model's BN statistics to hide within the benign cluster.
- Evidence: Evaluation of BNGuard against an attacker who adds a regularization term to minimize the distance between malicious and benign BN statistics.

## Limitations

- The paper assumes sufficient availability of clean local data for self-reference training on malicious clients, which may not hold in highly non-IID or data-scarce scenarios.
- BNGuard's reliance on the first BN layer may not generalize to architectures using different normalization layers (e.g., LayerNorm, GroupNorm).
- The effectiveness of SoDa's alignment mechanism in synchronized FL settings with strict update deadlines remains unverified.

## Confidence

- **High:** The core OBA attack mechanism (OOD data as dual-trigger) is well-specified and theoretically sound. BNGuard's detection principle based on first-BN statistics is clearly articulated.
- **Medium:** SoDa's self-reference alignment is plausible but computationally intensive; its effectiveness depends on hyperparameters (`λₘ`, `λₑ`) and local data quality.
- **Medium:** Defense robustness claims assume IID-like benign data statistics; performance may degrade under extreme non-IID settings.

## Next Checks

1. **Architectural Robustness Test:** Evaluate BNGuard on models using LayerNorm or GroupNorm instead of BN layers to quantify its applicability limits.
2. **Non-IID Stress Test:** Simulate highly heterogeneous (non-IID) benign data to measure BNGuard's FPR degradation and SoDa's alignment efficacy under skewed data distributions.
3. **Real-World Deployment Feasibility:** Assess SoDa's computational overhead in synchronized FL settings with tight deadlines to validate practical deployability.