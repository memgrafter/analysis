---
ver: rpa2
title: Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark
arxiv_id: '2506.04280'
source_url: https://arxiv.org/abs/2506.04280
tags:
- reasoning
- multi-image
- arxiv
- multimodal
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMRB, the first benchmark for evaluating
  structured visual reasoning across multiple images. It comprises 92 sub-tasks with
  4,750 samples and 68,882 reasoning steps, annotated using GPT-4o and refined by
  human experts.
---

# Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark

## Quick Facts
- **arXiv ID:** 2506.04280
- **Source URL:** https://arxiv.org/abs/2506.04280
- **Reference count:** 40
- **Primary result:** Introduces MMRB, the first benchmark for structured visual reasoning across multiple images, showing significant performance gaps between open-source and commercial MLLMs

## Executive Summary
This paper introduces MMRB (Multimodal Multi-image Reasoning Benchmark), the first comprehensive benchmark designed to evaluate structured visual reasoning across multiple images. The benchmark comprises 92 sub-tasks with 4,750 samples and 68,882 reasoning steps, annotated using GPT-4o with human expert refinement. MMRB covers spatial, temporal, and semantic reasoning capabilities and includes multi-solution annotations to capture diverse reasoning paths. A subset is specifically designed for multimodal reward model evaluation. The authors propose a sentence-level matching framework using open-source LLMs for efficient and scalable evaluation of multi-image reasoning tasks.

## Method Summary
MMRB was constructed through a two-phase annotation process using GPT-4o for initial annotation followed by human expert refinement to ensure quality and diversity. The benchmark includes 92 sub-tasks covering various reasoning types including spatial, temporal, and semantic reasoning. Each task is designed with multiple possible solutions to capture different valid reasoning paths. The evaluation framework employs a sentence-level matching approach using open-source LLMs to assess model responses efficiently. The benchmark was tested across 40 MLLMs, including 9 reasoning-specific models and 8 multimodal reward models, to comprehensively evaluate current capabilities in multi-image reasoning.

## Key Results
- Open-source MLLMs lag significantly behind commercial models in multi-image reasoning tasks
- Multimodal reward models struggle substantially with multi-image ranking tasks
- The benchmark demonstrates good coverage with 92 sub-tasks and 4,750 samples across 68,882 reasoning steps

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of structured reasoning types and the use of GPT-4o for initial annotation followed by human refinement, ensuring both scale and quality. The sentence-level matching framework enables efficient evaluation while capturing the complexity of multi-image reasoning. The multi-solution annotations account for different valid reasoning paths, making the benchmark more robust to variations in how models approach problems.

## Foundational Learning
1. **Multimodal Reasoning**: Understanding how models integrate visual and textual information across multiple images
   - Why needed: Essential for tasks requiring cross-image comparison and synthesis
   - Quick check: Can the model identify relationships between objects in different images?

2. **Structured Visual Reasoning**: Systematic approach to analyzing visual information following logical patterns
   - Why needed: Enables consistent evaluation of reasoning capabilities across diverse scenarios
   - Quick check: Does the model maintain logical consistency when processing multiple visual inputs?

3. **Sentence-Level Matching**: Evaluation technique comparing model outputs at the sentence level
   - Why needed: Provides granular assessment of reasoning quality beyond simple accuracy metrics
   - Quick check: Can the matching framework distinguish between correct and incorrect reasoning steps?

4. **Multi-Solution Reasoning**: Recognition that multiple valid approaches can exist for the same reasoning task
   - Why needed: Prevents bias toward specific reasoning styles and captures model versatility
   - Quick check: Does the evaluation account for different but equally valid solution paths?

## Architecture Onboarding

**Component Map:** Data Collection -> GPT-4o Annotation -> Human Refinement -> Benchmark Construction -> Sentence-Level Matching Framework -> Model Evaluation

**Critical Path:** The evaluation pipeline (Sentence-Level Matching Framework -> Model Evaluation) is critical as it determines how model performance is measured and compared.

**Design Tradeoffs:** The choice between comprehensive human annotation versus scalable AI-assisted annotation was resolved by using GPT-4o for initial annotation with human refinement, balancing quality and scale.

**Failure Signatures:** Models may fail due to:
- Inability to maintain spatial relationships across images
- Temporal reasoning errors when sequences are involved
- Semantic understanding gaps when interpreting complex visual concepts

**First 3 Experiments:**
1. Test the sentence-level matching framework's sensitivity to different solution variations
2. Evaluate model performance on individual reasoning types (spatial vs temporal vs semantic)
3. Compare performance across different image quantities (2-image vs 3+ image tasks)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but potential areas for further investigation include:
- How well MMRB performance generalizes to real-world multi-image reasoning tasks
- Whether the sentence-level matching framework captures all relevant aspects of reasoning quality
- The relationship between MMRB scores and practical application performance in multimodal systems

## Limitations
- The sentence-level matching framework may not fully capture the complexity of multi-image reasoning tasks
- The benchmark's focus on structured reasoning might miss certain complex reasoning patterns
- Performance gaps between model types may be influenced by factors beyond pure reasoning capability
- The reliance on GPT-4o for initial annotation could introduce bias toward certain reasoning styles

## Confidence
- **High Confidence:** Benchmark construction methodology and scale are well-documented and replicable
- **Medium Confidence:** Experimental results showing performance gaps are convincing but interpretation requires careful consideration
- **Medium Confidence:** Challenge level for multimodal reward models is well-demonstrated but difficulty metrics could be more granular

## Next Checks
1. Conduct ablation studies on the sentence-level matching framework to quantify sensitivity to different solution variations
2. Test whether MMRB performance correlates with other established multimodal reasoning benchmarks to establish external validity
3. Perform detailed analysis of failed cases to understand whether performance gaps stem from reasoning limitations or other factors