---
ver: rpa2
title: 'Understanding Efficiency: Quantization, Batching, and Serving Strategies in
  LLM Energy Use'
arxiv_id: '2601.22362'
source_url: https://arxiv.org/abs/2601.22362
tags:
- energy
- arxiv
- inference
- batching
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the energy and latency impact of system-level\
  \ design choices\u2014numerical precision, batching strategy, and request scheduling\u2014\
  on LLM inference using NVIDIA H100 GPUs. It benchmarks five numerical formats across\
  \ multiple model sizes and phases (prefill, decode) with Hugging Face\u2019s Text\
  \ Generation Inference (TGI) server, and evaluates the effects of static/dynamic\
  \ batching and inter-arrival delay patterns."
---

# Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use

## Quick Facts
- **arXiv ID**: 2601.22362
- **Source URL**: https://arxiv.org/abs/2601.22362
- **Reference count**: 15
- **Key result**: System-level optimizations—lower precision, batching, and burst-mode scheduling—can reduce per-request energy by up to 100× in LLM serving.

## Executive Summary
This paper studies how numerical precision, batching strategy, and request scheduling affect the energy and latency of large language model inference on NVIDIA H100 GPUs. Through systematic benchmarking of five numerical formats across models ranging from 125M to 70B parameters, the authors demonstrate that lower precision formats (e.g., FP8) yield significant energy and latency savings in compute-bound phases like prefill, but these benefits disappear in memory-bound decoding due to quantization overhead. Batching improves efficiency, though gains plateau for decode around batch size 4 and are sensitive to padding in prefill. The authors also show that burst-mode batching and fixed arrival shaping can achieve up to 100× per-request energy reduction compared to naive serving. The results emphasize that sustainable LLM deployment depends on system-level optimizations, not just model design.

## Method Summary
The study benchmarks five numerical formats (FP16, BF16, FP8, NF4, TF32) across three model sizes (125M, 7B, 70B parameters) using Hugging Face's Text Generation Inference (TGI) server on NVIDIA H100 GPUs. Experiments measure latency and energy for prefill and decode phases, both individually and as end-to-end request-serving pipelines. Static and dynamic batching are evaluated, along with the effects of inter-arrival delays and burst-mode scheduling. The authors also profile a real-world LLM serving system (TextCortex) to validate lab findings in production.

## Key Results
- Lower precision (FP8/NF4) reduces latency and energy in compute-bound prefill, but quantization overhead nullifies gains in memory-bound decode.
- Batching improves efficiency, with decode benefits plateauing around batch size 4; prefill gains are sensitive to padding.
- Burst-mode batching and fixed arrival shaping can yield up to 100× per-request energy reduction versus naive serving.

## Why This Works (Mechanism)
Quantization reduces the amount of data moved and computed per operation, directly lowering energy in compute-bound phases. In memory-bound decode, the bottleneck shifts to memory bandwidth, and the extra cost of dequantization offsets any gains. Batching amortizes fixed overheads across multiple requests, improving utilization. Burst-mode batching smooths traffic and reduces idle GPU cycles, amplifying efficiency.

## Foundational Learning
- **Numerical precision formats (FP16, BF16, FP8, NF4, TF32)**: Determines the bit-width of model weights and activations, directly impacting compute and memory bandwidth needs.
- **Prefill vs decode phases**: Prefill is compute-heavy (token generation for new context), decode is memory-bound (generating subsequent tokens from KV cache).
- **Static vs dynamic batching**: Static batching groups requests at fixed intervals; dynamic batching groups as soon as a threshold is met, trading latency for efficiency.
- **Burst-mode batching**: A scheduling strategy that aggregates incoming requests into bursts to maximize GPU utilization and reduce idle time.
- **KV cache**: A mechanism that stores intermediate activations during decode to avoid redundant computation, shifting the bottleneck from compute to memory bandwidth.

## Architecture Onboarding
- **Component map**: Client requests -> Request scheduler (burst/static/dynamic batching) -> TGI server -> NVIDIA H100 GPU -> Prefill/Decode phases -> Response
- **Critical path**: Request arrival → Scheduler (batch formation) → GPU kernel launch → Prefill or Decode → Response assembly
- **Design tradeoffs**: Lower precision saves energy but may require extra quantization/decompression overhead; larger batch sizes improve throughput but increase latency and memory pressure; burst-mode batching reduces idle cycles but risks higher tail latency.
- **Failure signatures**: High memory pressure (OOM), increased quantization overhead, batch size saturation, burstiness causing underutilization or tail latency spikes.
- **First experiments**: 1) Measure latency/energy per request across numerical formats for a fixed model and batch size. 2) Profile prefill and decode phases separately under static and dynamic batching. 3) Simulate bursty traffic to evaluate burst-mode batching versus fixed-interval batching.

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to NVIDIA H100 GPUs; results may not generalize to other architectures (e.g., AMD, AWS Trainium).
- Focus on synthetic, fixed-rate request arrivals omits effects of bursty, real-world traffic patterns.
- Dataset limited to decoder-only models; does not include encoder-decoder, multimodal, or sparse architectures.
- Energy modeling does not account for DVFS, thermal throttling, or non-GPU system components.

## Confidence
- **Quantization energy savings in compute-bound phases**: High
- **Quantization overhead nullifying gains in memory-bound decode**: Medium
- **100× energy reduction via burst-mode batching**: Medium

## Next Checks
1. Replicate key experiments on a different GPU architecture (e.g., AMD MI300X or AWS Trainium) to verify quantization and batching energy trends.
2. Benchmark the same model configurations under realistic, bursty traffic patterns and compare with the synthetic arrival schedules used here.
3. Measure full-system energy (including CPU, memory, networking) and assess the impact of DVFS and thermal throttling on the reported energy savings.