---
ver: rpa2
title: If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong
  Learning in LLMs
arxiv_id: '2503.23514'
source_url: https://arxiv.org/abs/2503.23514
tags:
- wang
- zhang
- memory
- learning
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LIFESTATE-BENCH, a benchmark for evaluating
  lifelong learning in large language models (LLMs) through multi-turn, multi-agent
  interactions. The key innovation is modeling cumulative experience as episodic timelines
  with structured scene details and character interactions, enabling objective fact-checking
  via self-awareness, memory retrieval, and relationship tracking questions.
---

# If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs

## Quick Facts
- arXiv ID: 2503.23514
- Source URL: https://arxiv.org/abs/2503.23514
- Authors: Siqi Fan; Xiusheng Huang; Yiqun Yao; Xuezhi Fang; Kang Liu; Peng Han; Shuo Shang; Aixin Sun; Yequan Wang
- Reference count: 26
- Primary result: Non-parametric methods (direct and summary concatenation) significantly outperform parametric approaches in managing stateful learning for LLMs

## Executive Summary
This paper introduces LIFESTATE-BENCH, a benchmark for evaluating lifelong learning in LLMs through multi-turn, multi-agent episodic interactions. The key innovation is modeling cumulative experience as episodic timelines with structured scene details and character interactions, enabling objective fact-checking via self-awareness, memory retrieval, and relationship tracking questions. Experiments with Llama3.1-8B, GPT-4-turbo, and DeepSeek R1 demonstrate that non-parametric methods (direct and summary concatenation) significantly outperform parametric approaches (knowledge editing and LoRA fine-tuning) in managing stateful learning, though all methods exhibit catastrophic forgetting as episodes progress.

## Method Summary
The method involves constructing episodic script data with (Location, Time, Narration, Dialogue) per episode, annotating factual QA for self-awareness, memory, and relationships per role per episode. Non-parametric baselines include direct concatenation (concatenating all prior episodes E₁…Eₜ₋₁ with current query) and summary concatenation (GPT-generated summaries of prior episodes). Parametric methods use knowledge editing via GRACE and LoRA fine-tuning on episode memory. Evaluation uses LLM-as-Judge with DeepSeek-V2 for point-wise grading (1-100 scale) against factual ground truth, reporting accuracy across self-awareness, factual memory retrieval, and relationship shift dimensions.

## Key Results
- DeepSeek R1 achieves highest overall accuracy (67.3% on Hamlet dataset)
- Non-parametric methods significantly outperform parametric ones in managing stateful learning
- All models exhibit challenges with catastrophic forgetting as interactions extend
- Questions about shifting relationships are the most challenging, with models struggling to track evolving dynamics

## Why This Works (Mechanism)

### Mechanism 1
Non-parametric context concatenation outperforms parametric memory encoding for stateful learning tasks. Direct episode concatenation preserves raw interaction history in the context window, maintaining uncompressed relational signals, while summary concatenation compresses history while retaining key facts. Both leverage the model's existing reasoning capacity rather than forcing weight updates that may interfere with pre-trained knowledge. This works because the model's context window can effectively attend to historical episodes without degrading current-episode reasoning, as evidenced by Direct Concatenation achieving 58.0% average accuracy on Hamlet vs. 25.6% for LoRA-Tune (Llama3.1-8B). The break condition occurs when episode count exceeds context window capacity and summaries lose critical relational details, evidenced by declining accuracy in later episodes.

### Mechanism 2
Relationship tracking is fundamentally harder than self-awareness or factual recall because it requires compositional reasoning over accumulated state changes. Relationship shifts (e.g., "uncle" → "father's murderer") require integrating multiple facts across episodes: identity, actions, and temporal sequencing. Self-awareness queries only need role identification; factual queries need single-episode retrieval. Relation queries need multi-hop reasoning over evolving states. This works because models encode relationships as implicit contextual patterns rather than explicit symbolic graphs, as evidenced by Relation Shift accuracy ranging 19.2-58.7% across methods, consistently lowest of three dimensions. The break condition occurs when relationship transformations require negation or counterfactual reasoning (e.g., "thought ally, actually enemy"), where models fail to update cached representations.

### Mechanism 3
Parametric methods (Knowledge Editing, LoRA) suffer accelerated catastrophic forgetting in multi-episode settings due to weight interference. Knowledge editing directly modifies weights to encode new facts, but sequential edits overwrite prior edits without architecture-level memory isolation. LoRA adapters learn compressed representations that lack capacity for accumulating discrete episodic states. Both conflate "learning" with "overwriting." This works because model weights encode facts in shared distributed representations that lack explicit episode-indexed storage, as evidenced by Knowledge Editing showing steepest accuracy drop across episodes compared to Direct/Summary Concatenation. The break condition occurs when edit sequences exceed ~3-5 episodes, where accumulated interference degrades even recent-episode recall.

## Foundational Learning

- Concept: **Superposition property of LLMs**
  - Why needed here: The paper's core premise is that LLMs start "stateless" (superposition of possible characters) and converge toward stateful behavior through interaction. Understanding this explains why lifelong learning is non-trivial for LLMs.
  - Quick check question: Can you explain why a next-token-predictor trained on diverse character dialogues would initially "hold multiple characters" before converging?

- Concept: **Catastrophic forgetting in sequential learning**
  - Why needed here: All evaluated methods show performance decline over episodes. This is the central failure mode the benchmark exposes.
  - Quick check question: Why does fine-tuning on episode 2 damage performance on episode 1 questions, even when both use the same model?

- Concept: **LLM-as-Judge evaluation paradigm**
  - Why needed here: The benchmark uses DeepSeek evaluator for automated scoring against ground-truth answers. Understanding this evaluation method is critical for interpreting results.
  - Quick check question: What are the failure modes when using an LLM to grade another LLM's factual accuracy?

## Architecture Onboarding

- Component map: Episodic Dataset -> Question Generator -> Memory Module (configurable) -> Evaluation Pipeline
- Critical path:
  1. Load episode sequence E₁...Eₙ for target character
  2. For each episode t, construct context H(t) using selected memory method
  3. Generate three question-answer pairs per character in episode
  4. Query model with Q(r, t) = ⟨H(t), c(t), q(r, t)⟩
  5. Score response A'(r, t) against ground truth A(r, t) using LLM judge
  6. Track accuracy decline across t=1...T to measure catastrophic forgetting

- Design tradeoffs:
  - **Direct vs. Summary Concatenation**: Direct preserves all information but hits context limits (~128K tokens). Summary scales but loses relationship nuance—evidenced by poor relation-shift performance.
  - **Parametric vs. Non-parametric**: Parametric methods avoid context limits but suffer catastrophic forgetting. Non-parametric methods preserve history but require large context windows.
  - **Hamlet vs. Synthetic datasets**: Hamlet tests potential data leakage (pre-training contamination). Synthetic ensures clean evaluation but may lack narrative complexity.

- Failure signatures:
  - **Flat accuracy across episodes**: Model relying on pre-training memorization, not episode-specific learning
  - **Sharp drop in relation-shift questions**: Memory method failing to encode multi-hop relational facts
  - **High variance in self-awareness**: Model not converging to stable character state
  - **Summary Concatenation underperforming Direct**: Summarization losing critical relational predicates

- First 3 experiments:
  1. **Baseline non-parametric comparison**: Run Direct Concatenation vs. Summary Concatenation on both datasets with Llama3.1-8B. Expect Direct to win on relation-shift, Summary to be competitive on self-awareness and factual memory.
  2. **Episode-length robustness test**: Vary starting episode (E₁ vs. E₃ vs. E₅) with fixed context window. Quantify how much prior context is actually being utilized vs. models relying on character priors.
  3. **Parametric forgetting curve**: Run Knowledge Editing and LoRA with 5+ episode sequences. Plot per-episode accuracy to quantify forgetting rate—expect Knowledge Editing to show steepest decline.

## Open Questions the Paper Calls Out

### Open Question 1
How can parametric memory methods (knowledge editing, LoRA fine-tuning) be improved to match or exceed non-parametric approaches for lifelong learning in LLMs? The authors state "non-parametric methods significantly outperform parametric ones in managing stateful learning" and note parametric models show "clear signs of catastrophic forgetting" with "scores drop quickly over episodes." This remains unresolved as the paper demonstrates the performance gap but does not propose solutions; precision limitations in knowledge editing and information loss in LoRA are identified as practical issues without remedies. Development of parametric methods that achieve comparable or superior accuracy to direct concatenation on LIFESTATE-BENCH, particularly maintaining performance across later episodes, would resolve this.

### Open Question 2
What mechanisms can enable LLMs to track evolving relationships across multiple episodes without performance degradation? The authors report that "the most challenging are questions about shifting relationships, where models struggle to track evolving dynamics," with accuracy as low as 19.2% for knowledge editing on relationship shift. This remains unresolved as summary concatenation "fails to capture complex relationship changes," and all methods show declining relationship tracking as episodes progress. Novel architectures or training approaches that maintain or improve relationship shift accuracy across 10+ episodes on the benchmark would resolve this.

### Open Question 3
How can catastrophic forgetting be mitigated during extended multi-agent interactions without relying on unlimited context windows? "All models exhibit challenges with catastrophic forgetting as interactions extend," and "performance tends to decline over time, with parametric models particularly struggling." This remains unresolved as non-parametric methods face context window limitations causing information loss with long texts, while parametric methods inherently forget. Methods achieving stable or improving accuracy across all episodes without requiring linear context growth would resolve this.

### Open Question 4
Can the LIFESTATE-BENCH findings generalize to more diverse narrative domains and larger sample sizes beyond theatrical scripts? The limitations section notes "the overall number of samples is limited, potentially restricting the diversity of training and evaluation scenarios" and authors plan to "synthesize additional datasets." Only two datasets (Hamlet, synthetic scripts) were tested, with 1.3K and 202 samples respectively, both sharing theatrical/narrative structures. Replication of findings across diverse domains (e.g., technical documentation, procedural narratives, multi-session conversations) with larger sample sizes would resolve this.

## Limitations
- Non-parametric methods' superiority may be context-window artifacts rather than genuine learning advantages
- Catastrophic forgetting evaluation doesn't distinguish between true forgetting and insufficient training capacity
- Hamlet dataset's original publication date (1603) raises legitimate concerns about pretraining contamination despite character name masking

## Confidence
- **High confidence**: Non-parametric methods outperforming parametric ones (supported by multiple experimental comparisons across datasets and models)
- **Medium confidence**: Relationship tracking being fundamentally harder than other question types (consistent pattern across experiments but could reflect prompt formulation rather than inherent difficulty)
- **Low confidence**: Claims about catastrophic forgetting mechanisms (no ablation studies showing why specific weight modifications cause forgetting vs. capacity limitations)

## Next Checks
1. **Context-window saturation test**: Systematically vary context window size (32K, 64K, 128K) while measuring performance degradation to determine if non-parametric advantages are purely capacity-dependent.

2. **Episode independence validation**: Run experiments where models are evaluated on episode N using only episodes N-1 and N (rather than all prior episodes) to verify they're not simply memorizing recent history.

3. **Pretraining contamination verification**: Conduct ablation studies on masked vs. unmasked character names in Hamlet dataset to quantify how much performance gains stem from pretraining knowledge vs. episodic learning.