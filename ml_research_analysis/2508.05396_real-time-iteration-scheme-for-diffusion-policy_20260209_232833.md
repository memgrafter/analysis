---
ver: rpa2
title: Real-Time Iteration Scheme for Diffusion Policy
arxiv_id: '2508.05396'
source_url: https://arxiv.org/abs/2508.05396
tags:
- diffusion
- policy
- denoising
- time
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Real-Time Iteration (RTI) Scheme for Diffusion Policy (DP) addresses
  the slow inference time of diffusion models by leveraging the Real-Time Iteration
  Scheme from optimal control. Instead of starting denoising from random noise, RTI-DP
  uses the previous action chunk as an initial guess, reducing denoising steps and
  maintaining spatiotemporal consistency.
---

# Real-Time Iteration Scheme for Diffusion Policy

## Quick Facts
- arXiv ID: 2508.05396
- Source URL: https://arxiv.org/abs/2508.05396
- Authors: Yufei Duan; Hang Yin; Danica Kragic
- Reference count: 23
- Primary result: Achieves 25-145 ms inference time vs. 820-830 ms for standard diffusion policy while maintaining or improving performance

## Executive Summary
This paper introduces a Real-Time Iteration (RTI) scheme that dramatically accelerates diffusion policy inference by leveraging the Real-Time Iteration method from optimal control. Instead of starting each denoising step from random noise, RTI-DP uses the previous action chunk as an initial guess, effectively reducing the number of denoising steps required while maintaining spatiotemporal consistency. The method is applied to pre-trained diffusion policy models without retraining, making it a practical post-hoc acceleration technique.

The approach addresses a fundamental limitation of diffusion policies - their slow inference time that prevents real-time control in robotic manipulation tasks. By combining theoretical contractivity conditions with a scaling parameter to handle discrete actions, the method achieves inference times of 25-145 ms compared to 820-830 ms for standard approaches, representing a 5-6x speedup. Experiments across multiple robotic tasks demonstrate that this acceleration does not come at the cost of performance, with some tasks even showing improved results compared to baseline methods.

## Method Summary
The Real-Time Iteration scheme modifies the standard denoising process by using the previous action chunk as an initial guess for the current denoising step, rather than starting from random noise. This leverages the spatiotemporal consistency in diffusion policy predictions, where consecutive time steps are highly correlated. The method applies this iterative approach to pre-trained diffusion policy models without requiring retraining. A scaling parameter γ is introduced to handle discrete action spaces, and theoretical conditions for contractivity are established to ensure convergence. The approach maintains the probabilistic nature of diffusion policies while significantly reducing computational overhead.

## Key Results
- Inference time reduced from 820-830 ms to 25-145 ms (5-6x speedup)
- Performance maintained or improved across multiple robotic tasks
- Outperforms baseline acceleration methods including few-step DPMs and distribution matching
- Achieves real-time control capabilities for complex manipulation tasks

## Why This Works (Mechanism)
The key insight is that consecutive time steps in diffusion policy predictions are highly correlated, meaning the previous action chunk provides an excellent initial guess for the current denoising step. By starting denoising from this informed initial state rather than random noise, the method requires fewer denoising steps to reach the target distribution. The scaling parameter γ helps maintain stability in the iterative process, particularly for discrete action spaces, while the contractivity conditions ensure convergence. This approach effectively trades some of the computational burden of denoising for the computational cost of maintaining and updating the initial guess.

## Foundational Learning
- Diffusion models and denoising process: Essential for understanding how standard diffusion policies generate actions through iterative noise removal
- Optimal control and Real-Time Iteration: Provides the theoretical foundation for using previous actions as initial guesses
- Contractivity conditions: Mathematical framework ensuring the iterative process converges to the correct solution
- Spatiotemporal consistency in sequential data: Explains why consecutive time steps can be effectively predicted from each other
- Discrete action space handling: Critical for adapting the method to practical robotic control scenarios

## Architecture Onboarding

Component map: Pre-trained DP model -> Denoiser with RTI -> Action output -> Feedback to next denoiser

Critical path: Input state → Previous action chunk (initial guess) → Denoiser with scaling → Current action → Next iteration

Design tradeoffs: The method trades a small amount of prediction accuracy for dramatic inference speed improvements. The scaling parameter γ must be carefully tuned - too small risks divergence, too large reduces the benefits of using previous actions as initial guesses.

Failure signatures: Divergence in the iterative process (indicated by exploding action values), poor performance on tasks with highly stochastic dynamics, or degraded performance when temporal correlations are weak.

First experiments:
1. Validate RTI-DP on a simple reaching task to confirm basic functionality
2. Compare inference times and performance against standard DP and few-step baselines
3. Test the effect of different scaling parameters γ on convergence and performance

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Theoretical convergence guarantees may not hold for all complex robotic tasks
- Scaling parameter effectiveness for discrete actions needs broader validation
- Limited adaptability to highly dynamic or novel environments without retraining
- Computational savings may not scale linearly with task complexity

## Confidence

| Claim Category | Confidence |
|---|---|
| Inference time reduction | High |
| Performance maintenance/improved | Medium |
| Convergence guarantees applicability | Medium |
| Scalability across diverse tasks | Low |

## Next Checks

1. Test RTI-DP on a broader range of robotic manipulation tasks with varying complexity and action space dimensions to verify scalability and performance consistency.
2. Conduct ablation studies to determine the optimal number of denoising steps for different task requirements and validate the effectiveness of the scaling parameter γ across diverse scenarios.
3. Implement RTI-DP in a real-world robotic system to assess its performance under practical constraints such as sensor noise, latency, and environmental variability.