---
ver: rpa2
title: 'IA-VLA: Input Augmentation for Vision-Language-Action models in settings with
  semantically complex tasks'
arxiv_id: '2509.24768'
source_url: https://arxiv.org/abs/2509.24768
tags:
- tasks
- language
- objects
- task
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IA-VLA, a framework for augmenting the input
  of vision-language-action (VLA) models to handle semantically complex tasks, particularly
  those involving duplicate objects. The key idea is to use a large vision-language
  model (VLM) as a pre-processing stage to identify task-relevant objects via semantic
  segmentation and numeric labels, then highlight these objects in the input image
  using semi-transparent masks.
---

# IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks

## Quick Facts
- **arXiv ID**: 2509.24768
- **Source URL**: https://arxiv.org/abs/2509.24768
- **Reference count**: 30
- **Primary result**: IA-VLA significantly improves success rates for VLA models in tasks requiring semantic understanding of duplicate objects

## Executive Summary
This paper introduces IA-VLA, a framework that augments VLA model inputs to handle semantically complex manipulation tasks involving duplicate objects. The key insight is to offload semantic understanding to a large VLM by using it as a pre-processing stage to identify task-relevant objects through segmentation and numeric labeling. The framework overlays semi-transparent masks on non-target objects in the input image, allowing the VLA to focus on action generation based on the augmented input. Evaluated on three task domains (Lego block lifting, kitchen vegetable sorting, and drawer opening), IA-VLA consistently outperformed a baseline VLA, particularly in scenarios requiring extrapolation from seen concepts. The relabeled instruction variant showed the best performance in unseen instruction scenarios.

## Method Summary
IA-VLA uses a two-stage approach: first, a large VLM (GPT-4.1) analyzes an image with numerically-tagged objects (identified via Semantic-SAM segmentation) to determine which object(s) satisfy the instruction; second, the selected target objects are preserved while non-target regions are masked with semi-transparent gray overlays. This augmented image is then used to finetune a VLA (OpenVLA) with LoRA adapters. For inference, the VLM step runs once at episode start, while SAM2 tracks masks across frames with ~40ms latency. The framework supports two instruction modes: original natural language and simplified "relabeled" commands pointing directly to highlighted objects.

## Key Results
- Block-lifting tasks: IA-VLA achieved 62-76% success rates versus baseline 19-73% across instruction categories
- Kitchen vegetable task: IA-VLA reached 73% success versus 40% baseline with original instructions
- Drawer-opening task: IA-VLA with relabeled instructions achieved 63% success versus 33% baseline

## Why This Works (Mechanism)
The framework works by separating semantic understanding from motor control. The large VLM excels at interpreting spatial relationships and object attributes described in natural language, while the VLA specializes in generating precise actions based on visual input. By highlighting only task-relevant objects, the VLA avoids confusion from duplicate objects and can focus on accurate manipulation. The relabeled instruction variant further simplifies the task by explicitly pointing to the target, reducing the cognitive load on the VLA.

## Foundational Learning
- **VLA (Vision-Language-Action) models**: Neural networks that map vision and language inputs to robot actions. Why needed: Core model being augmented. Quick check: Verify OpenVLA accepts image and text input and outputs robot actions.
- **Semantic segmentation with granularity control**: Dividing images into regions at multiple levels of detail. Why needed: Identifies objects for numeric tagging. Quick check: Confirm Semantic-SAM can produce 1-4 granularity levels.
- **Mask propagation with SAM2**: Tracking object masks across video frames. Why needed: Maintains object highlighting during execution. Quick check: Verify SAM2 latency stays under 40ms.
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient finetuning method. Why needed: Adapts VLA to augmented inputs without full retraining. Quick check: Confirm LoRA rank 32 works with OpenVLA.
- **Instruction relabeling**: Simplifying commands to reference highlighted objects directly. Why needed: Tests whether VLA can work with explicit visual cues. Quick check: Compare performance between original and relabeled instructions.

## Architecture Onboarding
**Component Map**: Image → Semantic-SAM → GPT-4.1 → SAM2 → VLA (OpenVLA with LoRA)
**Critical Path**: Segmentation → VLM selection → Mask augmentation → VLA action generation
**Design Tradeoffs**: 
- Using external VLM offloads semantic reasoning but adds API dependency and latency
- Semi-transparent masking preserves context while highlighting targets, but may obscure fine details
- LoRA enables efficient finetuning but may limit adaptation capacity
**Failure Signatures**:
- VLM misidentifies target object (24% of errors)
- Segmentation misses target objects due to filtering thresholds
- SAM2 mask tracking fails to maintain consistent highlighting
- VLA struggles with obscured visual details from alpha blending
**3 First Experiments**:
1. Test GPT-4.1 prompt variations to reduce VLM misidentification rate
2. Visualize intermediate segmentation masks to optimize filtering parameters
3. Measure actual SAM2 latency and accuracy in real-world conditions

## Open Questions the Paper Calls Out
- For VLAs with multiple camera views, is augmenting only the main view sufficient, or must all streams be consistently augmented?
- Would a VLA architecture with a dedicated mask input channel outperform the current alpha-blending approach?
- Can the high rate of VLA execution failures (70% of errors) be reduced by integrating IA-VLA with non-VLA imitation learning methods?

## Limitations
- Relies heavily on external model performance (GPT-4.1, Semantic-SAM, SAM2)
- Experimental evaluation limited to three task domains with 120-600 demonstrations
- Improvements are task-specific and may not generalize to broader scenarios
- Semi-transparent masking may obscure fine visual details necessary for precise manipulation

## Confidence
- **Core methodology**: Medium - sufficient detail for implementation but lacks specificity in critical areas like GPT-4.1 prompt template
- **Quantitative results**: Medium - well-supported by experiments but limited to specific task domains
- **Generalizability claims**: Low - improvements shown only in three task types with duplicate objects
- **Reproducibility**: Medium - clear framework but depends on external APIs and specific model versions

## Next Checks
1. Test multiple GPT-4.1 prompt formulations to reduce the 24% failure rate from VLM misidentification
2. Systematically vary Semantic-SAM granularity settings and area thresholds to optimize performance for different object sizes
3. Measure actual SAM2 mask propagation latency and accuracy under realistic operational conditions