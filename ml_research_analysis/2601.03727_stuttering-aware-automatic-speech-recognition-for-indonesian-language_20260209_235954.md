---
ver: rpa2
title: Stuttering-Aware Automatic Speech Recognition for Indonesian Language
arxiv_id: '2601.03727'
source_url: https://arxiv.org/abs/2601.03727
tags:
- speech
- stuttered
- indonesian
- stuttering
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic speech recognition
  (ASR) for stuttered speech in Indonesian, a low-resource language lacking stuttering-specific
  datasets. To overcome this, the authors propose a data augmentation framework that
  generates synthetic stuttered audio by injecting repetitions, prolongations, and
  interjections into fluent text using rule-based transformations and a large language
  model, followed by text-to-speech synthesis.
---

# Stuttering-Aware Automatic Speech Recognition for Indonesian Language

## Quick Facts
- **arXiv ID:** 2601.03727
- **Source URL:** https://arxiv.org/abs/2601.03727
- **Reference count:** 0
- **Primary result:** Synthetic stuttering augmentation enables effective Indonesian ASR for stuttered speech without real disfluent data

## Executive Summary
This paper addresses the challenge of automatic speech recognition for stuttered speech in Indonesian, a low-resource language lacking stuttering-specific datasets. The authors propose a data augmentation framework that generates synthetic stuttered audio by injecting repetitions, prolongations, and interjections into fluent text using rule-based transformations and a large language model, followed by text-to-speech synthesis. They fine-tune a pre-trained Indonesian Whisper model using transfer learning on this synthetic data. Experiments show that fine-tuning on synthetic stuttered speech consistently reduces recognition errors on stuttered speech, achieving lower word error rates (WER) and character error rates (CER) compared to a zero-shot baseline and joint fine-tuning with clean speech, while maintaining performance on fluent segments.

## Method Summary
The method involves three main stages: first, fluent Indonesian transcripts from Mozilla Common Voice are augmented with stutter patterns through rule-based transformations (injecting repetitions, prolongations, and interjections) and LLM-generated disfluencies; second, TTS synthesis converts the augmented text into audio with varied speaker voices and speaking rates; third, a pre-trained Indonesian Whisper model is fine-tuned on the synthetic stuttered data using AdamW optimization. The approach uses a stutter-only fine-tuning strategy, deliberately avoiding joint training with clean speech to maintain focus on disfluent patterns.

## Key Results
- Stutter-only fine-tuning achieved WER 0.126 on stuttered test data vs 0.181 for joint fine-tuning
- Maintained superior performance on clean speech: WER 0.064 vs 0.076 for joint fine-tuning
- Zero-shot baseline (untrained model) performed significantly worse than any fine-tuning approach

## Why This Works (Mechanism)

### Mechanism 1
Synthetic stuttering augmentation substitutes for real disfluent speech data when fine-tuning ASR models. The pipeline injects acoustic-temporal irregularities into fluent text via rule-based transformations and LLM rewrites, then synthesizes audio. This exposes the model to dysfluent patterns without requiring real-world recordings. The core assumption is that TTS-generated stuttered speech approximates the acoustic features of natural stuttering well enough for model adaptation.

### Mechanism 2
Stutter-only fine-tuning outperforms joint clean-stutter training for both disfluent and fluent speech recognition. Focused exposure to disfluent patterns allows the model to internalize temporal variations without diluting the learning signal. The encoder-decoder architecture handles long-range dependencies flexibly. The core assumption is that the pre-trained Whisper model already has sufficient fluent speech representation from prior training.

### Mechanism 3
Combined rule-based and LLM text augmentation captures complementary stuttering phenomena. Rule-based transformations enforce linguistic constraints while LLM generates context-aware disfluencies reflecting semantic hesitation patterns. The core assumption is that both deterministic and naturalistic disfluency types are necessary for robust coverage.

## Foundational Learning

- **Concept: Sequence-to-sequence ASR with attention**
  - Why needed here: Whisper uses encoder-decoder transformers that model conditional probability of tokens given audio, handling variable-length disfluencies better than CTC-based approaches
  - Quick check question: Can you explain why attention mechanisms handle temporal irregularities (like repetitions) more flexibly than CTC alignment?

- **Concept: Transfer learning for low-resource domains**
  - Why needed here: The approach depends on adapting a pre-trained Indonesian Whisper model rather than training from scratch
  - Quick check question: What is the difference between fine-tuning all parameters vs. freezing the encoder and only training the decoder?

- **Concept: WER/CER evaluation metrics**
  - Why needed here: Performance claims rest on edit distance measures between predictions and ground truth
  - Quick check question: Given the formula (S+D+I)/N, would a model that deletes all stuttered segments score better or worse than one that transcribes them literally?

## Architecture Onboarding

- **Component map:** Mozilla Common Voice (Indonesian) → Text augmentation (rules + LLM) → TTS synthesis (gpt-tts-4o-mini) → Synthetic stuttered audio → Whisper-small fine-tuning
- **Critical path:** 1) Source fluent transcripts from Common Voice 2) Apply rule-based stutter injection 3) Generate LLM variants for naturalistic disfluencies 4) Synthesize audio with TTS 5) Fine-tune Whisper on stutter-only data 6) Evaluate on separate stuttered and clean test sets
- **Design tradeoffs:** Synthetic vs. real data avoids collection costs but risks domain mismatch; stutter-only vs. joint training shows better performance but is counter-intuitive; rule-based vs. LLM combines controllability with diversity
- **Failure signatures:** High WER on real stuttered speech (synthetic data doesn't generalize); phonetic substitutions with semantic shifts; plural/singular confusion; no validation pipeline for synthetic data quality
- **First 3 experiments:** 1) Replicate zero-shot baseline using cahya/whisper-small-id on your own Indonesian test set 2) Ablate augmentation strategy: train separate models using rule-only and LLM-only text generation 3) Validate synthetic-to-real transfer by evaluating the fine-tuned model on any available real stuttered Indonesian audio

## Open Questions the Paper Calls Out

1. Does fine-tuning on synthetic stuttered Indonesian speech transfer to real stuttered speech from actual speakers who stutter? The paper explicitly states this needs validation using real-world stuttered Indonesian speech, noting there is currently no validation stage to ensure synthetic data quality.

2. Why does stutter-only fine-tuning outperform joint (stutter + clean) fine-tuning even on clean fluent speech? The authors acknowledge this is counterintuitive and requires further investigation to understand why mixed training degrades clean-speech performance.

3. Can curriculum learning, adaptive sampling, or loss reweighting strategies improve performance when combining fluent and disfluent training data? The paper suggests that more principled data balancing strategies may be required given the performance gap between stutter-only and mixed fine-tuning.

## Limitations
- Synthetic stuttering patterns may not accurately represent real Indonesian stuttering without validation against actual stuttered speech data
- Specific proportions and contextual rules for stutter injection remain unspecified, creating reproducibility challenges
- Counterintuitive finding that stutter-only fine-tuning outperforms joint training requires broader validation across different domains and languages

## Confidence
- **High confidence**: Experimental methodology using WER/CER metrics on separate stuttered and clean test sets is sound and directly measures stated objectives
- **Medium confidence**: Mechanism by which synthetic stuttering approximates real stuttering is plausible but not fully validated
- **Low confidence**: Claim that stutter-only fine-tuning universally outperforms joint training requires broader validation due to counterintuitive nature

## Next Checks
1. Evaluate the fine-tuned model on any available real Indonesian stuttered speech samples to measure synthetic-to-real generalization gap
2. Train separate models using only rule-based text augmentation and only LLM-generated disfluencies to compare individual contributions
3. Fine-tune the same model architecture on a different low-resource language with stuttering challenges using the identical synthetic augmentation pipeline to test cross-domain transfer