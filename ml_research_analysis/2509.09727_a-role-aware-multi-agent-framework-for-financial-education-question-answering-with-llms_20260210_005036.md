---
ver: rpa2
title: A Role-Aware Multi-Agent Framework for Financial Education Question Answering
  with LLMs
arxiv_id: '2509.09727'
source_url: https://arxiv.org/abs/2509.09727
tags:
- financial
- evidence
- reasoning
- finance
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a role-aware multi-agent framework for financial\
  \ question answering using large language models. The system uses three specialized\
  \ agents\u2014Base Generator, Evidence Retriever, and Expert Reviewer\u2014guided\
  \ by domain-specific role prompts to simulate expert financial reasoning."
---

# A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs

## Quick Facts
- arXiv ID: 2509.09727
- Source URL: https://arxiv.org/abs/2509.09727
- Reference count: 32
- Improves financial QA accuracy by 6.6-8.3% over zero-shot CoT baselines

## Executive Summary
This paper introduces a role-aware multi-agent framework for financial question answering using large language models. The system uses three specialized agents—Base Generator, Evidence Retriever, and Expert Reviewer—guided by domain-specific role prompts to simulate expert financial reasoning. Evaluated on 3,532 expert-designed finance education questions, the framework improves accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines. The highest performance was achieved by Gemini-2.0-Flash, and even GPT-4o-mini reached performance comparable to the fine-tuned FinGPT-mt_Llama3-8B_LoRA. The approach offers a cost-effective method for enhancing financial QA without requiring extensive fine-tuning.

## Method Summary
The framework implements a three-agent pipeline: Base Generator (CoT reasoning), Evidence Retriever (RAG over 6 finance textbooks), and Expert Reviewer (topic-specific role prompts). Four modes are tested: M-0 (baseline single call), M-1 (adds evidence), M-2 (adds critique), M-3 (full system). The Expert Reviewer receives initial answers and reasoning, provides structured feedback identifying errors, and the Base Generator reruns with this critique. Role prompts are applied only to the reviewer to maintain baseline purity.

## Key Results
- Achieves 6.6-8.3% accuracy improvement over zero-shot Chain-of-Thought baselines
- Gemini-2.0-Flash delivers highest performance across all modes
- GPT-4o-mini matches fine-tuned FinGPT-mt_Llama3-8B_LoRA performance without fine-tuning
- Critique-based refinement contributes +2.24% to +4.19% accuracy gains across models

## Why This Works (Mechanism)

### Mechanism 1: Critique-based refinement
The Expert Reviewer identifies specific errors (arithmetic, formula, conceptual) in initial answers and provides structured feedback. The Base Generator re-runs with this critique, correcting mistakes rather than hallucinating fixes. This drives 6.6-8.3% accuracy gains. Break condition: vague or misleading critiques degrade performance.

### Mechanism 2: Retrieval-grounded evidence
The Evidence Retriever performs RAG over 6 finance textbooks using cosine similarity search, grounding definitions and formulas. This reduces hallucinations with consistent but smaller gains (+1.53% to +1.90%). Break condition: if query concepts aren't covered or embedding similarity fails on domain terminology.

### Mechanism 3: Role-specific prompting
82 finance subtopics have crafted role prompts (e.g., "You are a bond-market expert") that steer the Expert Reviewer to apply appropriate mental models. This activates domain-aligned reasoning patterns. Break condition: misaligned role prompts introduce systematic errors.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Why needed: Base Generator uses CoT as default reasoning mode. Quick check: Can you explain why CoT alone may produce "ungrounded thoughts" in financial QA?
- **Retrieval-Augmented Generation (RAG)**: Why needed: Evidence Retriever implements RAG over finance textbooks. Quick check: What would happen if the embedding model wasn't trained on financial terminology?
- **Role/Persona Prompting**: Why needed: Expert Reviewer's effectiveness depends on well-crafted role prompts. Quick check: How might a poorly-designed role prompt for "tax advisor" introduce systematic errors in critique?

## Architecture Onboarding

- **Component map**: Evidence Retriever → Base Generator → Expert Reviewer → (optional) Base Generator (second pass)
- **Critical path**: Second LLM call where Base Generator receives critique is where 6.6-8.3% gain is realized
- **Design tradeoffs**: Single-pass iteration avoids context dilution but may miss multi-round corrections; role prompts only on reviewer keeps baseline clean but underutilizes persona benefits
- **Failure signatures**: Evidence Retriever outputs "[NO EVIDENCE]" → fallback to M-0; empty critique → no improvement; topic-role mismatch → wrong domain criteria
- **First 3 experiments**: 1) Run M-0 vs. M-2 on 50 questions to confirm critique gains; 2) Inspect 10 cases where M-1 fails but M-3 succeeds; 3) Ablate role prompts on reviewer to measure contribution

## Open Questions the Paper Calls Out
1. How does the structure and detail level of the Expert Reviewer's critique influence final answer accuracy?
2. Does integration of external computational tools (code interpreters, calculators) outperform text-based refinement for multi-step quantitative reasoning?
3. Would applying role-aware prompts to the Base Generator improve initial draft accuracy without inducing anchoring bias?

## Limitations
- Primary dataset (3,532 questions) requires subscription access to Study.com
- Exact role prompts for all 82 finance subtopics are only partially specified
- Contribution of individual mechanisms is entangled and requires additional ablation studies

## Confidence
- **High Confidence**: 6.6-8.3% accuracy improvement over zero-shot CoT is well-supported
- **Medium Confidence**: Role-aware prompting as primary driver is supported but confounded by evidence retrieval
- **Low Confidence**: Framework generalizes beyond finance without retraining role prompts is speculative

## Next Checks
1. Attempt to obtain the 3,532-question dataset via subscription or request to authors; if unavailable, benchmark on open finance QA dataset
2. Run ablation studies comparing M-1 (evidence only) vs. M-2 (critique only) to quantify individual mechanism contributions
3. Apply framework to non-finance QA task (legal or medical) using same architecture but new domain role prompts to assess portability