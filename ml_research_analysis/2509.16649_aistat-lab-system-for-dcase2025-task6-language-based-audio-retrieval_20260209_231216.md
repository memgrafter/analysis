---
ver: rpa2
title: 'AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval'
arxiv_id: '2509.16649'
source_url: https://arxiv.org/abs/2509.16649
tags:
- audio
- classi
- cation
- arxiv
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dual-encoder approach for language-based
  audio retrieval, using separate encoders for audio and text modalities aligned via
  contrastive learning. The system incorporates distillation loss from ensemble soft
  labels, LLM-based data augmentation (back-translation and caption mixing), and an
  auxiliary clustering-based classification task.
---

# AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval

## Quick Facts
- arXiv ID: 2509.16649
- Source URL: https://arxiv.org/abs/2509.16649
- Reference count: 0
- Primary result: Dual-encoder language-based audio retrieval system achieving 48.83 mAP@16 ensemble score on ClothoV2.1

## Executive Summary
This paper presents a dual-encoder approach for language-based audio retrieval that combines contrastive learning with knowledge distillation and LLM-based augmentation. The system uses separate audio and text encoders aligned via InfoNCE loss, with distillation from an ensemble of pretrained models providing soft correspondence targets. The approach incorporates clustering-based auxiliary classification and LLM-generated data augmentation to enhance generalization. Pretraining on ClothoV2.1, AudioCaps, and WavCaps datasets followed by finetuning and re-finetuning yields strong performance on the Clotho development test split.

## Method Summary
The system employs a three-stage training pipeline: pretraining on Clotho+AudioCaps+WavCaps for 20 epochs, finetuning on Clotho with distillation loss from a 3-model ensemble (λ₁=1.0), and re-finetuning with cluster-based auxiliary classification (λ₂=0.05). Three audio encoders (PaSST, EAT, BEATs) are combined with a RoBERTa-large text encoder. InfoNCE loss with τ=0.05 drives contrastive learning, while soft correspondence probabilities from the ensemble provide distillation supervision. Data augmentation includes back-translation, LLM caption mixing (50K pairs), and random word deletion/synonym replacement.

## Key Results
- Best single system (PaSST with distillation) achieved 46.62 mAP@16
- Ensemble of three audio models reached 48.83 mAP@16
- PaSST consistently outperformed EAT and BEATs across all metrics
- Distillation improved PaSST performance from 42.08 to 46.62 mAP@16

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distillation with soft correspondence probabilities improves retrieval by capturing latent semantic relationships that binary labels miss.
- **Mechanism:** An ensemble of pretrained models produces averaged similarity scores (Eq. 5), which are converted to soft probability targets (Eqs. 6–7). The student model is trained to match these via cross-entropy (Eq. 8), exposing it to "second-order" correspondences the dataset does not explicitly label.
- **Core assumption:** The pretrained ensemble encodes meaningful audio-text relationships beyond the provided annotations, and soft targets transfer useful inductive bias.
- **Evidence anchors:**
  - [section 2.2] "This method uses soft correspondence probabilities from an ensemble of pretrained models to capture nuanced audio-text relationships, improving generalization."
  - [Table 2] SID 2 (distillation only) improves over SID 1 (no distillation): PaSST 42.08→46.62 mAP@16.
  - [corpus] Weak/absent: corpus does not provide comparative evidence on distillation in audio retrieval.
- **Break condition:** If pretrained models systematically share a similar blind spot or bias, soft targets may reinforce errors rather than correct them.

### Mechanism 2
- **Claim:** LLM-based augmentation (back-translation and caption mixing) enhances generalization by increasing linguistic and acoustic diversity without manual labeling.
- **Mechanism:** Back-translation paraphrases captions while preserving semantics. Caption mixing combines two audio clips and uses GPT-4o to synthesize a merged caption, creating composite training examples that encourage compositional reasoning.
- **Core assumption:** Synthetic captions remain semantically faithful to audio content, and the model benefits from exposure to varied phrasing and mixed signals.
- **Evidence anchors:**
  - [section 2.4] "By doing so, back-translation generates captions that retain the same semantic meaning as the originals but feature varied linguistic expressions."
  - [section 2.4] "With LLM mix, we created 50,000 new audio-caption pairs, adding substantial variety to our dataset."
  - [Table 2] SID 3 (distillation + augmentation) shows marginal improvement over SID 2 on some metrics (e.g., PaSST R@1: 26.81→27.20) but not consistently across all.
  - [corpus] Weak/absent: no direct corpus evidence for LLM-based audio augmentation efficacy.
- **Break condition:** If generated captions introduce systematic noise or hallucinations, the model may learn spurious audio-text associations.

### Mechanism 3
- **Claim:** Clustering-based auxiliary classification improves representations by encouraging alignment with latent semantic structure in the caption space.
- **Mechanism:** Captions are embedded and clustered (BERTopic-style: UMAP + HDBSCAN). Classification heads on both encoders predict pseudo-labels, adding a clustering loss term (Eq. 10). This steers the audio encoder toward representations compatible with caption semantics.
- **Core assumption:** Cluster assignments reflect meaningful semantic groupings, and predicting them transfers useful structure to audio representations.
- **Evidence anchors:**
  - [section 2.3] "This setup encourages the audio encoder to learn representations that are aligned with the semantic clusters of the captions."
  - [Table 2] SID 4 (finetuned cluster) vs SID 3 shows mixed results: PaSST mAP@16 46.41→46.39 (slight drop), but other metrics fluctuate; SID 5 (BERTopic cluster) yields 46.50.
  - [corpus] Weak/absent: no corpus support for clustering in audio-text retrieval.
- **Break condition:** If clustering is too coarse or noisy, auxiliary labels may provide weak or misleading supervision; gains may disappear without careful tuning.

## Foundational Learning

- **Concept:** Contrastive learning (InfoNCE loss, symmetric retrieval objectives)
  - **Why needed here:** The core alignment mechanism between audio and text modalities; defines how positive pairs attract and negatives repel in the shared embedding space.
  - **Quick check question:** Can you explain why temperature τ controls the softness of the similarity distribution and how it affects hard vs. soft negatives?

- **Concept:** Dual-encoder architecture with separate modality encoders
  - **Why needed here:** Enables efficient retrieval by precomputing embeddings independently and comparing via cosine similarity, rather than joint encoding at query time.
  - **Quick check question:** What is the inference-time complexity advantage of a dual-encoder over a cross-encoder for large-scale retrieval?

- **Concept:** Knowledge distillation from ensemble models
  - **Why needed here:** The paper's distillation mechanism relies on understanding how soft targets transfer ensemble knowledge to a single student model.
  - **Quick check question:** How does distillation differ from standard label smoothing in terms of the information conveyed to the student?

## Architecture Onboarding

- **Component map:** Audio encoder (PaSST/EAT/BEATs) → Projection layer → Embedding space → Cosine similarity → Retrieval scores; Text encoder (RoBERTa-large) → Projection layer → Same embedding space → Cosine similarity

- **Critical path:**
  1. Pretrain on Clotho + AudioCaps + WavCaps (20 epochs, no augmentation).
  2. Finetune on Clotho with distillation loss from 3-model ensemble (20 epochs; optionally with augmentation).
  3. Re-finetune with cluster-based auxiliary classification using pseudo-labels (20 epochs).
  4. Ensemble multiple configurations with weights from grid search on validation set.

- **Design tradeoffs:**
  - PaSST vs. EAT vs. BEATs: PaSST achieves highest mAP@16 but requires 32 kHz audio; EAT/BEATs use 16 kHz and may be more memory-efficient.
  - Augmentation: improves diversity but adds risk of noisy captions; monitor validation metrics carefully.
  - Clustering: adds representation structure but introduces dependency on embedding quality; cluster granularity matters (HDBSCAN parameters).

- **Failure signatures:**
  - Validation mAP plateaus early: check learning rate schedule and batch size; ensure warmup is applied.
  - Distillation loss not decreasing: verify ensemble model outputs are normalized and temperature τ is set correctly (0.05).
  - Cluster classification accuracy low: inspect cluster assignments; adjust UMAP/HDBSCAN parameters or use alternative embeddings (e.g., e5-large-v2).

- **First 3 experiments:**
  1. **Baseline replication:** Train SID 1 (no distillation, no augmentation, no clustering) with PaSST to confirm baseline mAP@16 ≈ 42.
  2. **Distillation ablation:** Train SID 2 (distillation only) to isolate the contribution of soft correspondence targets; expect mAP@16 ≈ 46–47.
  3. **Cluster sensitivity:** Compare SID 4 (finetuned-based clusters) vs. SID 5 (BERTopic clusters) to assess embedding source impact; expect marginal differences in mAP@16.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the auxiliary clustering task provide consistent benefits across different audio encoder architectures, or does it negatively impact specific model types?
- Basis in paper: [inferred] Table 2 shows that while System 5 (Clustering) improves PaSST performance (46.41 to 46.50 mAP@16), it appears to degrade BEATs performance (44.66 to 43.88 mAP@16) compared to System 3.
- Why unresolved: The authors conclude that clustering contributed to performance gains, but the ablation study reveals inconsistent results across the three tested audio models, suggesting the technique may not be universally beneficial.
- What evidence would resolve it: A detailed ablation analyzing the clustering loss impact on BEATs specifically, or a hyperparameter search for cluster weights ($\lambda_2$) tailored to each audio encoder.

### Open Question 2
- Question: What is the semantic fidelity of the 50,000 synthetic audio-text pairs generated via the LLM mix augmentation?
- Basis in paper: [inferred] Section 2.4 describes creating 50,000 pairs by mixing audio and using GPT-4o to merge captions, but provides no qualitative analysis or human evaluation of the resulting semantic alignment.
- Why unresolved: While the augmentation boosts retrieval scores, "mixing" captions for mixed audio risks creating "hallucinated" or imprecise descriptions (e.g., missing the nuance of overlapping sounds), which could introduce noise into the contrastive learning space.
- What evidence would resolve it: A human evaluation or automated metric (e.g., SPICE/SPIDEr) score comparing the LLM-mixed captions against the ground truth captions of the constituent audio samples.

### Open Question 3
- Question: How sensitive is the distillation performance to the specific composition of the teacher ensemble?
- Basis in paper: [inferred] Section 2.2 mentions using an ensemble of $M=3$ pretrained models to generate soft correspondence probabilities, but does not ablate the contribution of individual teachers.
- Why unresolved: It is unclear if the 4+ point gain from distillation (System 1 to 2) relies on the diversity of the specific architectures used (PaSST, EAT, BEATs) or if a simpler ensemble would yield similar results.
- What evidence would resolve it: An ablation study showing student model performance when distilled from each teacher individually versus the full ensemble.

## Limitations

- The approach shows strong performance on ClothoV2.1 but lacks validation on other audio-text retrieval datasets
- LLM-based augmentation introduces potential noise without systematic evaluation of caption quality
- Clustering-based auxiliary classification shows inconsistent benefits across different audio encoder architectures
- Computational requirements for PaSST (32 kHz audio) may limit practical deployment

## Confidence

**High Confidence:** The contrastive learning framework with InfoNCE loss is well-established and the core training pipeline is clearly specified with reproducible hyperparameters. The observed performance improvements from distillation (42.08→46.62 mAP@16 for PaSST) are substantial and consistent across multiple metrics.

**Medium Confidence:** The individual contributions of LLM augmentation and clustering-based classification are harder to assess due to marginal and inconsistent improvements in the ablation studies. The ensemble's weighted combination strategy shows improvement but lacks statistical significance testing against baseline variations.

**Low Confidence:** The generalizability of the approach to datasets outside ClothoV2.1 remains completely untested. The paper provides no evidence that the learned representations transfer to other retrieval tasks or that the soft distillation targets remain effective when pretrained models are trained on different datasets.

## Next Checks

1. **Cross-dataset validation:** Evaluate the best ensemble model on AudioCaps test set and/or other audio-text retrieval benchmarks to assess generalization beyond ClothoV2.1.

2. **Distillation ablation with held-out ensemble:** Hold out one audio model from the ensemble during student training, then evaluate whether the student can match or exceed the held-out model's performance.

3. **Augmentation quality audit:** Generate 100 random LLM-mixed captions and have human annotators rate semantic fidelity compared to original captions. Correlate caption quality scores with retrieval performance changes to quantify the actual contribution versus noise introduced by augmentation.