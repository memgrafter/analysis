---
ver: rpa2
title: 'When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental
  Learning'
arxiv_id: '2602.00573'
source_url: https://arxiv.org/abs/2602.00573
tags:
- stage
- learning
- class
- classes
- evolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stage-CIL, a new class-incremental learning
  paradigm that explicitly addresses intra-class morphological evolution, where instances
  of the same class undergo significant appearance changes (e.g., larva to butterfly).
  The authors formalize this setting and create Stage-Bench, a 10-domain dataset with
  two ordered morphological stages per class, along with new evaluation metrics (Inter-F
  and Intra-F) to measure both inter- and intra-class forgetting.
---

# When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2602.00573
- **Source URL:** https://arxiv.org/abs/2602.00573
- **Reference count:** 22
- **Primary result:** STAGE framework achieves 75.11% average accuracy and 7.48% intra-class forgetting on Stage-Bench dataset

## Executive Summary
This paper introduces Stage-CIL, a new class-incremental learning paradigm that explicitly addresses intra-class morphological evolution, where instances of the same class undergo significant appearance changes (e.g., larva to butterfly). The authors formalize this setting and create Stage-Bench, a 10-domain dataset with two ordered morphological stages per class, along with new evaluation metrics (Inter-F and Intra-F) to measure both inter- and intra-class forgetting.

They propose STAGE, a framework that learns abstract transformation patterns in a shared memory pool, predicting evolved representations based on initial-stage anchors. This enables the model to maintain coherent class identity across morphological stages. Experimental results show that STAGE significantly outperforms state-of-the-art CIL and domain-incremental learning methods, achieving 75.11% average accuracy and reducing intra-class forgetting to 7.48% on Stage-Bench.

## Method Summary
The STAGE framework addresses morphological evolution in class-incremental learning by learning abstract transformation patterns in a shared memory pool. The method predicts evolved representations based on initial-stage anchors, allowing the model to maintain coherent class identity across morphological stages. The framework uses a novel anchor-based representation learning approach that leverages shared memory to capture transformation patterns between different morphological stages of the same class.

## Key Results
- STAGE achieves 75.11% average accuracy on Stage-Bench dataset
- Reduces intra-class forgetting to 7.48% across 10 domains
- Outperforms state-of-the-art CIL and domain-incremental learning methods significantly
- Maintains only 0.31% intra-class forgetting in three-stage Object domain experiments

## Why This Works (Mechanism)
The STAGE framework's effectiveness stems from its ability to learn abstract transformation patterns that capture the relationship between different morphological stages of the same class. By maintaining a shared memory pool of anchor representations from initial stages, the model can predict how classes will evolve and adapt its decision boundaries accordingly. This approach addresses the fundamental challenge of class-incremental learning where traditional methods struggle to maintain class coherence when instances undergo significant morphological changes.

## Foundational Learning
- **Class-Incremental Learning (CIL):** Why needed - enables continuous learning from sequential data streams without catastrophic forgetting. Quick check - baseline methods like ER-ACE and DER serve as performance benchmarks.
- **Morphological Evolution:** Why needed - captures real-world scenarios where classes undergo appearance changes over time. Quick check - Stage-Bench dataset provides controlled evolution scenarios across 10 domains.
- **Anchor-based Representation Learning:** Why needed - provides stable reference points for predicting evolved representations. Quick check - shared memory pool maintains initial-stage anchors for transformation pattern learning.
- **Transformation Pattern Learning:** Why needed - enables prediction of evolved representations from initial anchors. Quick check - performance metrics show effective pattern capture across morphological stages.

## Architecture Onboarding
**Component Map:** Input Data -> Feature Extractor -> Shared Memory Pool -> Transformation Learner -> Prediction Module -> Output

**Critical Path:** The core pipeline processes incoming data through feature extraction, matches it with stored anchors in the shared memory pool, applies learned transformation patterns, and generates predictions for evolved representations.

**Design Tradeoffs:** The framework balances memory efficiency (through shared memory pool) against representation accuracy (through anchor-based predictions). The tradeoff favors scalability but may sacrifice some precision in complex transformations.

**Failure Signatures:** Performance degradation occurs when:
- Morphological changes are non-linear or discontinuous
- Transformation patterns between stages are too complex to capture
- Initial-stage anchors are insufficient for accurate prediction

**3 First Experiments:**
1. Baseline comparison with ER-ACE and DER on Stage-Bench dataset
2. Ablation study removing shared memory pool component
3. Three-stage evolution experiment on Object domain

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Stage-Bench dataset may have limited ecological validity due to artificial splitting of morphological stages
- Framework's scalability concerns when handling complex or non-linear transformations
- Reliance on initial-stage representations may not generalize to all types of morphological changes
- Limited validation beyond controlled Stage-Bench environment

## Confidence
- **High confidence:** Experimental methodology and evaluation metrics are well-defined and consistently applied
- **Medium confidence:** Performance improvements over baseline methods are significant but primarily validated on Stage-Bench dataset
- **Low confidence:** Three-stage evolution handling is based on limited experiments requiring more extensive validation

## Next Checks
1. Test STAGE on naturally occurring datasets with continuous morphological evolution (e.g., plant growth stages, aging faces) to assess real-world applicability
2. Evaluate performance when morphological changes are non-linear or discontinuous, representing more realistic evolution scenarios
3. Conduct ablation studies to quantify individual contributions of shared memory pool and transformation pattern learning modules