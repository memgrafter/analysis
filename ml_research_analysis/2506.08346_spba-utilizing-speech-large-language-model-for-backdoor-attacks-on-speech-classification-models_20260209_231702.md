---
ver: rpa2
title: 'SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech
  Classification Models'
arxiv_id: '2506.08346'
source_url: https://arxiv.org/abs/2506.08346
tags:
- speech
- backdoor
- trigger
- triggers
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SPBA, a speech backdoor attack method leveraging
  Speech Large Language Models (SLLMs) to generate diverse triggers based on timbre
  and emotion. The method addresses the limitation of existing speech backdoor attacks,
  which are constrained by a single trigger function and vulnerability to defense
  methods.
---

# SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models

## Quick Facts
- **arXiv ID:** 2506.08346
- **Source URL:** https://arxiv.org/abs/2506.08346
- **Reference count:** 40
- **Primary result:** SPBA achieves >99% ASR on speech classification models with 90-130 poisoned samples per trigger versus 250-350 without MGDA

## Executive Summary
This paper proposes SPBA, a speech backdoor attack method that leverages Speech Large Language Models (SLLMs) to generate diverse triggers based on timbre and emotion. The method addresses the limitation of existing speech backdoor attacks, which are constrained by single trigger functions and vulnerability to defense methods. SPBA uses SLLM to synthesize poisoned speech samples that embed multiple backdoors into speech classification models. To handle the challenge of balancing multiple backdoor tasks without excessively increasing the poisoning rate, the authors introduce the Multiple Gradient Descent Algorithm (MGDA). Experimental results on keyword spotting (KWS) and speaker verification (SV) tasks demonstrate that SPBA achieves high attack success rates (up to 99.95%) with significantly lower poisoning rates compared to baselines, while maintaining high speech quality and trigger effectiveness. The method proves robust and effective against existing defenses.

## Method Summary
SPBA is a three-stage attack pipeline: (1) Attack Stage - transcribe clean speech using Paraformer STM, then generate poisoned samples using SLLM with reference triggers from ESD dataset (timbre/emotion utterances); (2) Training Stage - train victim classifier with MGDA-balanced loss that scales gradients for clean task and multiple backdoor tasks; (3) Inference Stage - evaluate clean accuracy and attack success rate. The method uses 95/5 train/test split on Google Speech Commands v2 (KWS) and VoxCeleb1 (SV), with 3-5 triggers (male/female, angry/happy/sad) per attack. MGDA computes Pareto-optimal scaling coefficients to balance gradient updates across tasks.

## Key Results
- SPBA achieves >99% ASR on both KWS and SV tasks with 90-130 poisoned samples per trigger versus 250-350 without MGDA
- MGDA enables co-training of 3-5 backdoor tasks while maintaining clean accuracy and reducing total poisoning requirements by 60-70%
- Intense emotions (angry, happy) achieve higher ASR faster than subtle emotions (sad), requiring fewer poisoned samples for effective activation
- Generated poisoned samples maintain speech quality with MOS scores of 3.5-3.7, indicating stealthiness against human detection

## Why This Works (Mechanism)

### Mechanism 1: SLLM-Generated Semantic Triggers
Speech Large Language Models can generate diverse, natural-sounding triggers based on speech elements (timbre, emotion) that evade human detection. The SLLM takes source transcripts and reference speech with target attributes, then generates speech that preserves linguistic content while embedding the trigger attribute. The model learns to associate these semantic features with attacker-specified labels. Core assumption: The SLLM generates speech where trigger attributes are perceptually consistent and learnable by the victim classifier.

### Mechanism 2: MGDA-Based Multi-Task Gradient Balancing
The Multiple Gradient Descent Algorithm enables effective co-training of K backdoor tasks alongside the main classification task by computing Pareto-optimal gradient scaling coefficients. For each batch, MGDA computes gradients for each task independently, then solves for scaling coefficients that minimize the norm of the combined gradient while ensuring all coefficients sum to one. This prevents any single backdoor task from dominating training. Core assumption: The main task and backdoor tasks have compatible gradient directions that can be balanced without catastrophic interference.

### Mechanism 3: Low-Poisoning Multi-Trigger Injection
Distributing poisoned samples across multiple trigger types with MGDA reduces per-trigger poisoning requirements while maintaining >99% ASR. Without MGDA, increasing triggers proportionally increases total poisoned samples (250-350 per trigger × K triggers). MGDA enables each trigger to require only 90-130 samples by ensuring gradient updates benefit all backdoor tasks simultaneously rather than competing. Core assumption: The victim model has sufficient capacity to memorize multiple trigger-to-label mappings without interference.

## Foundational Learning

- **Backdoor Attacks vs. Adversarial Attacks**: Backdoor attacks require data poisoning at training time, while adversarial attacks apply perturbations at inference time. Understanding this distinction is critical for implementing the three-stage pipeline. *Quick check*: Can you explain why backdoor attacks require access to training data while adversarial attacks do not?

- **Multi-Task Learning with Gradient Conflicts**: MGDA explicitly addresses gradient competition between main and backdoor tasks. Without understanding multi-task optimization, the balanced loss formulation may seem arbitrary. *Quick check*: What happens to task performance when gradients from multiple objectives point in opposite directions?

- **Speech Element Features (Timbre, Emotion, Pitch)**: Element-based triggers rely on modifying semantic speech attributes rather than adding noise. Understanding acoustic feature representations informs trigger selection. *Quick check*: Why would modifying timbre be more stealthy than adding ultrasonic noise?

## Architecture Onboarding

- **Component map**: Paraformer STM -> SLLM Generator -> MGDA Optimizer Wrapper -> Victim Classifier
- **Critical path**: 1) Transcribe clean utterances → text_src; 2) Sample trigger reference from prompt dataset → [text_m, x_m]; 3) Generate poisoned sample → SLLM(text_n, [text_m, x_m]); 4) Assign target label to poisoned sample; 5) Train classifier with MGDA-balanced loss when batch contains poisoned samples; 6) Evaluate: clean accuracy (AV) and attack success rate (ASR)
- **Design tradeoffs**: Higher K increases robustness to single-trigger defenses but requires more total poisoned samples; MGDA mitigates but doesn't eliminate this cost; trigger type selection affects ASR efficiency (intense emotions faster than subtle emotions); lower poisoning rate improves stealth but may reduce ASR
- **Failure signatures**: High AV (>2%) with low ASR indicates triggers disrupting speech naturalness; ASR varies significantly across triggers suggests MGDA not balancing effectively; TA (trigger accuracy) low means SLLM not generating consistent trigger attributes; clean accuracy drops substantially indicates excessive poisoning rate or gradient conflict
- **First 3 experiments**: 1) Single-trigger baseline: Implement VSVC or PBSM baseline to establish ASR/PN benchmarks without MGDA; 2) Multi-trigger without MGDA: Replicate Tables I-II "w/o MGDA, K=3" condition to observe ASR degradation and PN inflation; 3) MGDA ablation: Compare MGDA vs simple weighted loss summation to isolate the contribution of gradient-based balancing

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved based on the analysis:

### Open Question 1
How does SPBA perform against advanced backdoor defenses specifically designed to detect or remove multiple trigger types? The introduction claims multi-trigger attacks improve robustness against defenses like Neural Cleanse, but the experiments only evaluate attack success rates on undefended models, not against active defense mechanisms.

### Open Question 2
Does the MGDA optimization strategy remain effective as the number of distinct triggers ($K$) scales significantly beyond 5? The experiments limit evaluation to $K=3$ and $K=5$ triggers, leaving the scalability of the gradient balancing approach for larger backdoor capacities unknown.

### Open Question 3
What is the acoustic or feature-level explanation for why "intense" emotions (Angry/Happy) act as more efficient triggers than "Sad" emotions? Figure 2(a) shows a clear disparity in ASR based on emotion type, with intense emotions achieving peak performance faster, but the paper offers no theoretical explanation for this bias.

## Limitations
- SLLM generality: Effectiveness demonstrated using specific Neural Codec Language Model but doesn't establish whether approach generalizes to other SLLM architectures
- Defense evasion quantification: Claims triggers are "stealthy" based on MOS scores but lacks systematic evaluation against specific defense mechanisms
- Trigger attribute transfer reliability: Effectiveness depends critically on SLLM's ability to consistently transfer reference speech attributes, with limited ablation studies on generation failure modes

## Confidence
- **High Confidence**: Core empirical results showing MGDA reduces required poisoning samples while maintaining ASR >99% are well-supported by Tables I-II
- **Medium Confidence**: Claim that element-based triggers are more stealthy than noise-based triggers relies on MOS scores but lacks direct human perception studies
- **Low Confidence**: Assertion that this is the "first" backdoor attack using SLLMs is difficult to verify given rapid evolution of speech synthesis research

## Next Checks
1. **Cross-SLLM Validation**: Reproduce the attack using different SLLM architectures (Qwen-Audio, Whisper-based models) to assess whether trigger generation effectiveness is architecture-dependent
2. **Defense Ablation Study**: Implement and evaluate against three specific defenses: spectral anomaly detection on trigger samples, adversarial training with emotion/timbre-perturbed speech, and trigger inversion techniques applied to the poisoned dataset
3. **Gradient Conflict Analysis**: Log and visualize the angle between main task and backdoor task gradients across training epochs to empirically validate when MGDA succeeds vs fails at gradient balancing, and identify conditions where gradient conflicts become irreconcilable