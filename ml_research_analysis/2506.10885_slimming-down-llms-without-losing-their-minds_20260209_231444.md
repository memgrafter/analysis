---
ver: rpa2
title: Slimming Down LLMs Without Losing Their Minds
arxiv_id: '2506.10885'
source_url: https://arxiv.org/abs/2506.10885
tags:
- fine-tuning
- language
- lora
- llama
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that parameter-efficient fine-tuning with
  LoRA and QLoRA can effectively improve task-specific performance in large language
  models while maintaining computational efficiency. The study finds that fine-tuning
  performance strongly depends on alignment between fine-tuning datasets and target
  benchmarks.
---

# Slimming Down LLMs Without Losing Their Minds

## Quick Facts
- **arXiv ID:** 2506.10885
- **Source URL:** https://arxiv.org/abs/2506.10885
- **Authors:** Qingda; Mai
- **Reference count:** 18
- **Primary result:** Parameter-efficient fine-tuning with LoRA/QLoRA improves task-specific performance while causing catastrophic forgetting of mathematical reasoning capabilities.

## Executive Summary
This paper demonstrates that parameter-efficient fine-tuning with LoRA and QLoRA can effectively improve task-specific performance in large language models while maintaining computational efficiency. The study finds that fine-tuning performance strongly depends on alignment between fine-tuning datasets and target benchmarks. Experimental results show that fine-tuned Llama 3.2 1B achieves 61.20% accuracy on HellaSwag (vs 60.77% baseline), while TinyLlama 1.1B achieves 59.26% with 25× more training data. However, mathematical reasoning capability degrades severely, with forgetting rates exceeding 88%. Domain knowledge also decreases by 13 percentage points on MMLU-CS. The results indicate that while parameter-efficient methods enable effective specialization, they may compromise general capabilities, particularly for tasks outside the fine-tuning domain.

## Method Summary
The study fine-tunes Llama 3.2 1B and TinyLlama 1.1B using QLoRA with the Alpaca instruction-following dataset. Base models are loaded with 4-bit quantization to reduce memory footprint. LoRA adapters (low-rank matrices A and B) are applied to Q/K/V projections in attention heads, with only these adapters being updated during training while base weights remain frozen. Fine-tuning uses 2,000 samples for Llama 3.2 and 50,000 samples for TinyLlama on Google Colab with Tesla T4 GPU. Models are evaluated on three benchmarks: HellaSwag (commonsense reasoning), GSM8K (mathematical reasoning), and MMLU-CS (domain knowledge), using the lm-evaluation-harness framework.

## Key Results
- Fine-tuned Llama 3.2 1B achieves 61.20% HellaSwag accuracy (vs 60.77% baseline) with 2,000 samples
- TinyLlama 1.1B achieves 59.26% HellaSwag accuracy with 50,000 samples (25× more data)
- Mathematical reasoning capability drops catastrophically: GSM8K forgetting rate exceeds 88%
- Domain knowledge decreases by 13 percentage points on MMLU-CS

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Decomposition Reduces Trainable Parameters
LoRA approximates weight updates using two low-rank matrices, reducing trainable parameters from d² to 2dr. Given pretrained weight W ∈ R^(d×d), LoRA introduces A ∈ R^(d×r) and B ∈ R^(r×d) where r ≪ d. Only A and B are updated during fine-tuning while W remains frozen. Forward pass computes y = (W + AB)x. Core assumption: The intrinsic dimension of fine-tuning tasks is low enough that rank-r updates capture sufficient adaptation capacity.

### Mechanism 2: Dataset-Benchmark Alignment Determines Capability Retention
Fine-tuning performance strongly depends on alignment between fine-tuning dataset content and benchmark task domains. The Alpaca dataset (instruction-following) aligns with commonsense reasoning but lacks mathematical examples, causing selective capability degradation. Commonsense patterns share representational space with instruction-following, while mathematical reasoning requires specialized circuits that get overwritten.

### Mechanism 3: Architectural Quality Dominates Data Quantity
Superior model architecture compensates for significantly less training data. Llama 3.2 1B's optimized attention mechanisms and architectural refinements provide better knowledge representations than TinyLlama 1.1B's simplified design, even with 25× less fine-tuning data (2k vs 50k samples).

## Foundational Learning

- **Concept: Quantization (4-bit precision)**
  - Why needed here: Enables QLoRA by reducing memory footprint. Understanding that Model size = size_datatype × num_weights explains why 4-bit quantization is "almost universally optimal" for loading LLMs.
  - Quick check question: Can you explain why reducing precision from 32-bit to 4-bit reduces model size by ~8× without changing the number of weights?

- **Concept: Rank in Matrix Decomposition**
  - Why needed here: LoRA's core insight is that weight updates can be approximated by low-rank matrices. Understanding rank as "minimum number of independent rows/columns" explains why r=8-64 works for fine-tuning.
  - Quick check question: If W is 1000×1000 and r=10, how many parameters does LoRA train vs. full fine-tuning?

- **Concept: Transformer Attention Mechanism**
  - Why needed here: LoRA is applied to Q/K/V projections in attention heads. Understanding Attention(Q,K,V) = softmax(QK^T/√d)V clarifies where low-rank updates are injected.
  - Quick check question: In which transformer components does this paper apply LoRA adaptations?

## Architecture Onboarding

- **Component map:** Base model (Llama 3.2/TinyLlama) -> 4-bit quantization layer -> LoRA adapters (A, B) -> Q/K/V projections in attention heads -> Unsloth training framework -> lm-evaluation-harness evaluation

- **Critical path:**
  1. Load base model with 4-bit quantization
  2. Initialize LoRA adapters (A, B) with rank r (typically 8-64)
  3. Freeze W, train only A and B on fine-tuning dataset
  4. Merge adapters post-training: W_merged = W_base + AB
  5. Evaluate on domain-specific benchmarks

- **Design tradeoffs:**
  - Rank size (r): Higher r = more capacity but more parameters; lower r = efficiency but limited adaptation
  - Dataset composition: Instruction-following data improves chat but risks catastrophic forgetting of math/reasoning
  - Model selection: Newer architectures (Llama 3.2) outperform older ones (TinyLlama) even with less data

- **Failure signatures:**
  - GSM8K accuracy drops >80%: Catastrophic forgetting of mathematical reasoning
  - MMLU-CS drops >10%: Domain knowledge degradation from misaligned fine-tuning data
  - HellaSwag stable/improved: Commonsense reasoning preserved (expected)

- **First 3 experiments:**
  1. **Baseline characterization:** Evaluate base Llama 3.2 1B on all three benchmarks (HellaSwag, GSM8K, MMLU-CS) before any fine-tuning to establish pre-adaptation capabilities.
  2. **Minimal LoRA test:** Fine-tune with rank r=8 on 500 Alpaca samples; evaluate on all benchmarks to confirm LoRA implementation works and measure early forgetting signals.
  3. **Mixed-dataset ablation:** Fine-tune with 50% Alpaca + 50% GSM8K-style math examples; compare forgetting rate vs. Alpaca-only to test whether task-specific examples mitigate catastrophic forgetting.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can mixed fine-tuning datasets (combining instruction data with domain-specific reasoning) mitigate catastrophic forgetting of mathematical capabilities?
- **Open Question 2:** Why does parameter-efficient fine-tuning preserve commonsense reasoning while simultaneously causing severe degradation in mathematical reasoning?
- **Open Question 3:** What specific regularization or architectural constraints can balance new capability acquisition with knowledge preservation in small-parameter models?

## Limitations
- The study did not test mixed fine-tuning datasets that could potentially mitigate catastrophic forgetting, despite suggesting this as future work
- Architectural comparison between Llama 3.2 and TinyLlama relies on assumed design differences rather than systematic ablation studies
- Only one fine-tuning dataset (Alpaca) was used, limiting understanding of how dataset composition affects forgetting patterns

## Confidence
- **High confidence:** Dataset-benchmark alignment strongly determines capability retention (supported by consistent forgetting patterns across both models)
- **Medium confidence:** Architectural quality compensates for data quantity (based on single comparison, no ablation)
- **Medium confidence:** Low-rank decomposition effectively reduces trainable parameters (well-established LoRA mechanism, but computational efficiency claims unverified)

## Next Checks
1. Conduct ablation study with mixed fine-tuning datasets (Alpaca + GSM8K-style examples) to test if task-specific examples mitigate catastrophic forgetting
2. Systematically vary LoRA rank (r=8, 16, 32, 64) to determine optimal trade-off between adaptation capacity and forgetting severity
3. Perform cross-model architectural comparison using models with identical architectures but different pretraining data to isolate architectural vs. pretraining effects