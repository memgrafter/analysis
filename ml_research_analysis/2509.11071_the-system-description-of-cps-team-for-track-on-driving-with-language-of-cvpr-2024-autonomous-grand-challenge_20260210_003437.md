---
ver: rpa2
title: The System Description of CPS Team for Track on Driving with Language of CVPR
  2024 Autonomous Grand Challenge
arxiv_id: '2509.11071'
source_url: https://arxiv.org/abs/2509.11071
tags:
- depth
- language
- information
- objects
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a vision-language model system for the Driving
  with Language track of CVPR 2024 Autonomous Grand Challenge. The authors fine-tuned
  LLaVA models using the DriveLM-nuScenes dataset, incorporating depth information
  from an open-source depth estimation model and applying LoRA/DoRA parameter-efficient
  fine-tuning.
---

# The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge

## Quick Facts
- arXiv ID: 2509.11071
- Source URL: https://arxiv.org/abs/2509.11071
- Authors: Jinghan Peng; Jingwen Wang; Xing Yu; Dehui Du
- Reference count: 14
- Key outcome: Achieved 0.7799 on validation set, ranked 1st in CVPR 2024 Autonomous Grand Challenge Driving with Language track

## Executive Summary
This paper presents a vision-language model system for the Driving with Language track of CVPR 2024 Autonomous Grand Challenge. The authors fine-tuned LLaVA models using the DriveLM-nuScenes dataset, incorporating depth information from an open-source depth estimation model and applying LoRA/DoRA parameter-efficient fine-tuning. They also employed Chain-of-Thought reasoning for multiple-choice and yes/no questions. Their approach achieved a top score of 0.7799 on the validation set leaderboard, ranking 1st.

## Method Summary
The system fine-tunes LLaVA models on the DriveLM-nuScenes dataset using LoRA and DoRA parameter-efficient methods. Depth information is integrated through Depth Anything model outputs, with 75th percentile values from 11×11 pixel regions around key objects converted to textual distance descriptions. The approach uses Chain-of-Thought prompting for certain question types and employs multi-system fusion with task-specific model selection and voting for final answer determination.

## Key Results
- Achieved 0.7799 top score on validation set leaderboard
- Ranked 1st in CVPR 2024 Autonomous Grand Challenge Driving with Language track
- LoRA fine-tuning slightly outperformed DoRA despite DoRA using more parameters
- Single epoch training with rank=8, alpha=16 LoRA parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enriching vision-language model prompts with structured depth and object state information improves reasoning accuracy for driving scenarios.
- Mechanism: The system extracts key object tags from questions, computes depth estimates using Depth Anything on 11×11 pixel regions around object centroids (using 75th percentile values), and prepends textual descriptions of object states and distances to the input prompt. This provides explicit spatial context that the base VLM may not reliably infer from raw pixels alone.
- Core assumption: Depth estimation model outputs are sufficiently accurate for coarse-grained distance categorization, and textual depth descriptors ("close"/"far") improve model reasoning more than raw numeric values.
- Evidence anchors:
  - [abstract] "integrated depth information from open-source depth estimation models to enrich the training and inference processes"
  - [section 2.1] "select the 75th percentile value as the representative depth for each object. This depth value is then translated into a textual distance description"
  - [corpus] No direct corpus evidence for depth-augmented VLMs in driving; neighboring papers focus on verification/testing rather than this architecture.
- Break condition: If depth estimation produces systematic errors (e.g., fails at night, occlusions), or if textual depth tokens don't align with model's learned representations, performance gains may diminish or reverse.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (LoRA) applied to LLaVA language model layers achieves competitive performance with reduced computational cost compared to full fine-tuning.
- Mechanism: LoRA freezes pre-trained weights and trains low-rank decomposition matrices (rank=8, alpha=16) on all fully connected layers within the language model component only, leaving the vision encoder untouched. This constrains adaptation to a low-dimensional subspace, potentially preserving general visual reasoning while learning domain-specific language patterns.
- Core assumption: The vision encoder's representations are already sufficiently general for driving scenes; adaptation is primarily needed in the language reasoning component.
- Evidence anchors:
  - [abstract] "fine-tuning with the LoRA and DoRA methods"
  - [section 4] "fine-tune all fully connected layers within the language model component of LLaVA... set the rank and alpha to 8 and 16"
  - [corpus] No corpus papers specifically evaluate LoRA for driving VLMs.
- Break condition: If driving-specific visual features (e.g., traffic sign details, rare weather conditions) are not captured by the frozen vision encoder, LoRA adaptation cannot address this gap.

### Mechanism 3
- Claim: Multi-system fusion with task-specific model selection and voting improves aggregate performance over any single model.
- Mechanism: Different fine-tuned variants (LLaVA-1.5-7B+LoRA, LLaVA-NeXT-7B+LoRA, DoRA variants) are evaluated per question type. For multiple-choice/yes-no questions, voting selects the most common answer. For open-ended questions, the answer with highest metric score is selected.
- Core assumption: Individual systems make uncorrelated errors, and question-type-specific selection captures complementary strengths.
- Evidence anchors:
  - [abstract] "achieved a top score of 0.7799"
  - [section 6] "employ a voting method to determine the most commonly selected answer... achieve an optimal final score of 0.7799"
  - [corpus] Corpus contains ensemble/redundancy concepts in CPS testing (paper 7419 on flaky simulators), but no direct VLM fusion evidence.
- Break condition: If systems make correlated errors (same training data, similar failure modes), fusion gains diminish. Overfitting to validation set question-type distributions may not generalize.

## Foundational Learning

- **Concept: Vision-Language Model Architecture**
  - Why