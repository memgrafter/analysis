---
ver: rpa2
title: Combining Causal Models for More Accurate Abstractions of Neural Networks
arxiv_id: '2503.11429'
source_url: https://arxiv.org/abs/2503.11429
tags:
- causal
- input
- variables
- inputs
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of quantifying and improving the
  faithfulness of mechanistic interpretability hypotheses about neural networks. While
  causal abstraction provides a rigorous framework for aligning neural network behavior
  with high-level algorithmic models, these abstractions often fail to fully capture
  the network's true computational process.
---

# Combining Causal Models for More Accurate Abstractions of Neural Networks

## Quick Facts
- arXiv ID: 2503.11429
- Source URL: https://arxiv.org/abs/2503.11429
- Reference count: 37
- One-line primary result: Combined causal models achieve higher faithfulness-strength trade-off than individual models by partitioning input space into regions explained by different mechanistic hypotheses.

## Executive Summary
This paper addresses the challenge of creating faithful mechanistic interpretations of neural networks by combining multiple simple causal models into a single, more expressive abstraction. The key insight is that neural networks may use different computational strategies for different inputs, so a single causal model often fails to capture the full behavior. The authors propose a framework that partitions the input space and assigns different causal models to different regions based on their faithfulness, measured by interchange intervention accuracy (IIA). This approach enables stronger interpretability hypotheses (covering more inputs) while maintaining high faithfulness.

## Method Summary
The method combines multiple candidate causal models by constructing evaluation graphs where nodes represent inputs and edges are weighted by interchange intervention accuracy. A greedy algorithm then partitions the input space, assigning inputs to the model that best explains them while maintaining a specified faithfulness threshold. The framework was applied to GPT-2 small fine-tuned on arithmetic (X+Y+Z) and boolean logic tasks, where different models (representing different computational steps) were combined to explain more of the input space at high faithfulness levels compared to using individual models.

## Key Results
- Combined models achieved 75% input space coverage at 100% faithfulness for boolean logic, versus 50% or lower for individual models
- In arithmetic task, combined models explained 88% of inputs at 100% faithfulness versus 50% for uncombined
- Clear faithfulness-strength trade-off demonstrated: higher thresholds yield weaker hypotheses but more reliable explanations
- Combined models provide stronger interpretability hypotheses at high faithfulness levels (≥0.9 IIA)

## Why This Works (Mechanism)

### Mechanism 1: Input-Space Partitioning into Model-Specific Regions
Assigning different inputs to different causal models yields higher faithfulness than forcing a single model to explain all inputs. The combined model activates different computational processes based on which partition cell contains the input, allowing networks that use different strategies for different inputs to be more accurately represented.

### Mechanism 2: Evaluation Graphs for Faithfulness Quantification
Interchange intervention accuracy is encoded as edge weights in graphs, enabling systematic input assignment to models. Each graph captures pairwise intervention success between inputs, allowing the algorithm to identify which inputs are well-explained by which models.

### Mechanism 3: Greedy Maximization of Strength at Fixed Faithfulness
A greedy algorithm efficiently finds partitions that maximize the proportion of inputs assigned to non-trivial models while respecting a faithfulness threshold. The algorithm sorts nodes by degree and greedily adds nodes to subgraphs while IIA remains above threshold.

## Foundational Learning

- **Causal Models and Interventions**: The entire framework represents neural networks as causal models; understanding hard and interchange interventions is prerequisite. Quick check: Given causal model M with mechanism F_X(Y) = Y + 1, what is the result of intervention setting Y = 5?
- **Distributed Alignment Search (DAS)**: DAS learns rotations that align high-level variables with neural representations, enabling distributed interchange interventions. Quick check: Why does DAS optimize an orthogonal matrix rather than directly intervening on neuron activations?
- **Constructive Causal Abstraction**: Defines when a high-level model H is an abstraction of low-level model L via alignment ⟨Π,π⟩. Quick check: What three maps (δ, τ, ω) define an exact transformation between causal models?

## Architecture Onboarding

- Component map: Fine-tuned LLM (GPT-2 small) -> Candidate causal models (M_X, M_Y, M_XY, etc.) -> Intervenable models (trained via DAS) -> Evaluation graphs (IIA weights) -> Partitioning algorithm (greedy assignment)
- Critical path: Define candidate models → Train intervenable models via DAS → Construct evaluation graphs → Run greedy partitioning → Evaluate combined model strength at target faithfulness
- Design tradeoffs: Higher subspace dimension increases expressiveness but may overfit; higher faithfulness threshold yields weaker hypotheses; layer selection affects computational state capture
- Failure signatures: All models have low IIA (<0.5) → hypotheses wrong or alignment failed; combined model barely exceeds single model → inputs cluster around one hypothesis; evaluation graph has disconnected components → no single model captures computation
- First 3 experiments: 1) Replicate arithmetic task with GPT-2 small and verify Figure 2 patterns 2) Ablation on subspace dimension (k ∈ {32, 64, 128, 256}) 3) Layer-wise analysis across all 12 layers

## Open Questions the Paper Calls Out

### Open Question 1
Can this framework be successfully extended to complex, real-world tasks where candidate causal models are not easily enumerable? The paper focused on toy tasks with manually defined hypotheses, but extending to complex domains requires automatically generating or searching for candidate models.

### Open Question 2
Do more sophisticated optimization algorithms for constructing input space partitions provide significant improvements over the greedy approach? The paper suggests that global optimization techniques might yield better combined models than the locally optimal greedy algorithm.

### Open Question 3
Does the "strength" of a combined model accurately reflect unified mechanistic understanding or create a patchwork of local approximations? The piecewise definition of mechanisms might fit behavior without reflecting a coherent algorithm, especially given that intermediate layers don't transition discretely between computational states.

## Limitations
- Computational expense of evaluating all pairwise input combinations scales quadratically with input count
- Greedy partitioning makes locally optimal choices that may not globally optimize combined model strength
- Assumes discrete input regions rather than potentially overlapping computational states
- Framework relies on manually defined candidate models, limiting scalability to complex tasks

## Confidence

- High confidence: Framework for combining causal models via input-space partitioning is well-defined and mathematically rigorous
- Medium confidence: Empirical results showing combined models outperform individual models at high faithfulness thresholds are convincing but based on only two specific tasks
- Low confidence: Claim that approach can be directly extended to larger networks without modification is speculative given quadratic scaling of evaluation graph construction

## Next Checks

1. Test the approach on GPT-2-medium with the same arithmetic task to assess scalability of evaluation graph construction and partitioning algorithm
2. Apply the method to a more complex task like multi-digit addition or simple algebra to evaluate whether combined models still provide meaningful improvements
3. Compare greedy partitioning results against optimal partitioning (when computationally feasible) to quantify suboptimality of the greedy approach