---
ver: rpa2
title: 'Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition
  with Mixture of Decoders'
arxiv_id: '2505.21364'
source_url: https://arxiv.org/abs/2505.21364
tags:
- mxds
- sparse
- expert
- experts
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Mixture of Decoders (MxDs), a sparse layer architecture
  designed to overcome the accuracy trade-off in existing sparse MLP approximations.
  MxDs generalize MLPs and GLUs, expanding pre-trained dense layers into thousands
  of specialized sublayers using a Hadamard product-based tensor factorization.
---

# Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders

## Quick Facts
- **arXiv ID**: 2505.21364
- **Source URL**: https://arxiv.org/abs/2505.21364
- **Reference count**: 40
- **Primary result**: MxDs significantly outperform state-of-the-art sparse MLP methods on sparsity-accuracy frontier while maintaining interpretability

## Executive Summary
The paper introduces Mixture of Decoders (MxDs), a sparse layer architecture that addresses the accuracy-interpretability trade-off in existing sparse MLP approximations. MxDs generalize MLPs and GLUs by expanding pre-trained dense layers into thousands of specialized sublayers using Hadamard product-based tensor factorization. This design preserves full-rank weights for each expert, enabling faithful reconstruction of original layer mappings even under heavy sparsity. Experiments demonstrate that MxDs outperform Transcoders and Skip Transcoders on four LLMs (up to 3B parameters) while maintaining competitive performance on sparse probing and feature steering tasks.

## Method Summary
Mixture of Decoders (MxDs) is a sparse layer architecture that decomposes dense neural network layers into specialized sublayers without sacrificing accuracy. The key innovation is using Hadamard product-based tensor factorization to create thousands of experts, each with full-rank weights, while maintaining faithful reconstruction of the original dense layer's mapping. MxDs generalize existing architectures like MLPs and GLUs, expanding pre-trained layers into a mixture of specialized decoders. The architecture preserves the functional equivalence to original dense layers while enabling interpretability through sparsity patterns and specialized feature learning.

## Key Results
- MxDs significantly outperform Transcoders and Skip Transcoders on sparsity-accuracy frontier across four LLMs up to 3B parameters
- Maintains competitive performance on sparse probing and feature steering tasks
- Demonstrates specialized feature learning while preserving model faithfulness through full-rank weight preservation

## Why This Works (Mechanism)
The core mechanism relies on Hadamard product-based tensor factorization to decompose dense layers into thousands of specialized sublayers. Unlike traditional sparse approximations that sacrifice accuracy through weight sharing or low-rank constraints, MxDs preserve full-rank weights for each expert. This allows faithful reconstruction of the original dense layer's mapping function while introducing sparsity. The mixture architecture enables each sublayer to specialize in different features or input patterns, improving both interpretability (through clear specialization) and accuracy (through retained representational capacity).

## Foundational Learning
- **Tensor Factorization**: Decomposing high-dimensional weight matrices into products of smaller matrices; needed to create the expert sublayers efficiently
- **Hadamard Product**: Element-wise multiplication of tensors; crucial for combining expert outputs while maintaining independence
- **Sparse Neural Networks**: Networks with many zero weights; fundamental to understanding the efficiency gains
- **Expert Specialization**: Training sublayers to handle different input patterns; key to both interpretability and performance
- **Full-rank Preservation**: Maintaining complete weight matrices for each expert; essential for faithful reconstruction
- **Quick check**: Verify that each expert maintains the same dimensionality as the original dense layer to ensure no information loss

## Architecture Onboarding
**Component Map**: Dense Layer -> Hadamard Factorization -> Thousands of Sublayers -> Mixture Output
**Critical Path**: Input -> Expert Selection -> Hadamard Combination -> Output
**Design Tradeoffs**: Sparsity vs accuracy (MxDs favor accuracy by preserving full-rank weights), interpretability vs complexity (thousands of sublayers improve interpretability but increase architectural complexity)
**Failure Signatures**: Loss of specialization if experts become too similar, computational overhead from managing many sublayers, potential overfitting with too many experts
**Three First Experiments**:
1. Compare reconstruction accuracy of MxDs vs traditional sparse approximations on a single dense layer
2. Analyze the distribution of specialization across sublayers using probing tasks
3. Measure computational overhead vs baseline sparse architectures on small models

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Scalability to larger models (10B+ parameters) remains untested
- Computational overhead of managing thousands of sublayers could become prohibitive at scale
- Experimental validation focuses primarily on language modeling tasks, limiting generalizability to other domains

## Confidence
- **High confidence**: Mathematical formulation and theoretical soundness of MxDs
- **Medium confidence**: Empirical performance improvements on tested models and tasks
- **Medium confidence**: Interpretability benefits demonstrated through probing and steering tasks

## Next Checks
1. Scale experiments to models with 10B+ parameters to assess computational feasibility and performance retention
2. Conduct ablation studies isolating contributions of Hadamard factorization versus specialized sublayer architecture
3. Perform cross-domain validation on vision transformers and multimodal models to establish broader applicability beyond language tasks