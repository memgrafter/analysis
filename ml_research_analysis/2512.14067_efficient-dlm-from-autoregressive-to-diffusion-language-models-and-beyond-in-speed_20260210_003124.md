---
ver: rpa2
title: 'Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond
  in Speed'
arxiv_id: '2512.14067'
source_url: https://arxiv.org/abs/2512.14067
tags:
- block
- training
- attention
- token
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically explores how to convert pretrained autoregressive
  (AR) language models into diffusion language models (dLMs) that achieve faster generation
  while retaining strong accuracy. The key insight is that with an appropriate training
  scheme in terms of attention patterns and objectives, pretrained AR models can be
  converted into faster dLMs that support parallel decoding with KV cache at low training
  cost.
---

# Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed

## Quick Facts
- arXiv ID: 2512.14067
- Source URL: https://arxiv.org/abs/2512.14067
- Authors: Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov
- Reference count: 40
- Primary result: Achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B respectively

## Executive Summary
This work systematically explores converting pretrained autoregressive language models into diffusion language models (dLMs) that achieve faster generation while retaining strong accuracy. The key insight is that with an appropriate training scheme in terms of attention patterns and objectives, pretrained AR models can be converted into faster dLMs that support parallel decoding with KV cache at low training cost. The authors identify limitations in existing AR-to-dLM methods and propose a continuous pretraining scheme featuring block-wise attention patterns and position-dependent token masking.

## Method Summary
The method converts pretrained AR models to dLMs using block-wise attention where attention is bidirectional within blocks but causal across blocks, preserving weight distributions from the original AR model. Position-dependent token masking assigns higher masking probabilities to later tokens during training to mimic test-time behavior. The model predicts mask tokens directly (no token shift) and conditions noisy blocks on clean context to enable KV caching. Training uses 300-500B tokens with learning rate 1e-5→3e-6 cosine decay, and block sizes of 16 (1.5B) or 64 (4B/8B).

## Key Results
- Efficient-DLM 8B achieves +5.4% higher accuracy with 4.5x higher throughput vs Dream 7B
- Efficient-DLM 8B achieves +2.7% higher accuracy with 2.7x higher throughput vs Qwen3 4B
- Position-dependent masking provides 2.5-4.5% accuracy gains under aggressive parallel decoding settings

## Why This Works (Mechanism)

### Mechanism 1: Weight Preservation via Block-wise Causality
Converting AR to DLM works best when attention patterns preserve AR-like structure. Block-wise attention keeps bidirectional attention within blocks but remains causal across blocks, maintaining pretrained weight distributions. The core assumption is that pretrained weights contain valuable knowledge disrupted by full bidirectional attention. Evidence shows bidirectional attention causes larger weight drifts and accuracy drops. Break condition: training block size too large causes excessive corruption and accuracy degradation.

### Mechanism 2: Inference Alignment via Clean Context & KV Caching
Decoupling context from generation enables KV caching traditionally impossible for bidirectional dLMs. By conditioning current noisy block on clean context (previous blocks), KV states can be cached and reused since context is finalized. The assumption is computational bottlenecks arise from inability to reuse previous computations. Evidence shows seamless KV cache use improves efficiency. Break condition: corrupted context widens training-test gap and degrades accuracy.

### Mechanism 3: Position-Dependent Masking Strategy
Aligning training token masking with test-time inference distribution improves generation quality. Position-dependent masking assigns higher probabilities to later tokens, reflecting left-to-right denoising patterns. The assumption is models retain implicit autoregressive nature even when trained as diffusion models. Evidence shows positional priors benefit aggressive parallel decoding. Break condition: too strong positional bias (right-to-left) causes poor bidirectional context learning.

## Foundational Learning

**Autoregressive (AR) vs. Diffusion (dLM) Generation**: AR generates tokens sequentially creating memory bottlenecks; dLMs generate tokens in parallel enabling throughput speedups but historically lacking KV caching efficiency. Quick check: Why does sequential AR generation limit throughput on modern GPUs? (Memory bandwidth vs. Compute).

**KV (Key-Value) Caching**: Central efficiency lever where past states are cached to avoid recomputation. Bidirectional dLMs break this because "past" tokens might attend to "future" tokens. Quick check: In bidirectional attention, why can't you statically cache KV states of previous blocks?

**Training-Test Distribution Gap**: Models trained on uniform noise perform poorly when tested with confidence-based sampling (non-uniform). Understanding this gap is key to grasping position-dependent masking. Quick check: If trained on uniform noise but tested by refining only lowest-confidence tokens, what error might occur?

## Architecture Onboarding

**Component map**: Backone (pretrained AR) -> Block-wise Attention Mask (bidirectional within, causal across) -> Position-Dependent Masking (Algorithm 2) -> No token shift head

**Critical path**: Load pretrained AR weights → Generate block-wise attention mask (ensure x < b is clean context) → Apply position-dependent masking (w_i(t) = exp(β(1-t)i)) → Forward pass with block mask → Compute loss on masked tokens in current block

**Design tradeoffs**: Block size (4 too small, 128 too large; sweet spot 16-64); Token shift (prefer direct mask prediction); Learning rate (1e-5 sweet spot for weight preservation)

**Failure signatures**: Accuracy collapse (fully bidirectional attention or high learning rate); Slow inference (KV cache not implemented or clean context violated); Bad parallel decoding (uniform masking during training)

**First 3 experiments**: 1) Compare Bidirectional vs Block-wise attention on 1.5B model with 50B tokens; 2) Train with block sizes [4, 16, 64, 128] and evaluate for U-curve pattern; 3) Train with Uniform vs Position-Dependent masking and measure accuracy at TPF 1 vs TPF 4+

## Open Questions the Paper Calls Out
1. How can dLMs be optimized for large-batch serving scenarios where efficiency advantages currently diminish? (Appendix B mentions adaptive block sizes, improved parallel sampling, and hybrid architectures as future work)
2. Can more advanced or automated position-dependent token masking schemes further improve AR-to-dLM conversion? (Section 3.3 highlights this as design space for future exploration)
3. What is the theoretical relationship between model scale and optimal training block size? (Section 2.3 observes empirical patterns but provides no scaling law or theoretical justification)

## Limitations
- Training-data generalization gap from domain-biased 300-500B token corpus with unspecified mixing ratios
- Architectural scalability constraints for 70B+ parameter models where memory bandwidth becomes critical
- Evaluation metrics limitations with hardware-specific throughput measurements and unspecified confidence thresholds

## Confidence
**High Confidence**: Core architectural insight of block-wise attention with clean context enabling KV caching (mechanistic validity well-supported)
**Medium Confidence**: Empirical performance improvements on specific model sizes and datasets (may not generalize across all domains)
**Low Confidence**: General applicability of position-dependent masking and optimal block size range across model scales (limited empirical basis)

## Next Checks
1. **Weight Drift Monitoring**: Implement block-wise attention with varying block sizes (4, 16, 64, 128) and measure weight change magnitudes using cosine similarity to validate that bidirectional attention causes catastrophic drift while block-wise preserves pretrained knowledge
2. **Cross-Domain Generalization**: Train Efficient-DLM on single-domain corpus (code or math only) and evaluate on non-overlapping domains like creative writing to test whether left-to-right denoising assumptions hold across distributions
3. **Hardware-Agnostic Throughput**: Reproduce throughput measurements on different GPU architectures (A100, consumer GPUs) and batch sizes to validate whether KV caching efficiency gains are hardware-dependent or fundamental architectural improvements