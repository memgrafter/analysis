---
ver: rpa2
title: A Unified Noise-Curvature View of Loss of Trainability
arxiv_id: '2509.19698'
source_url: https://arxiv.org/abs/2509.19698
tags:
- step-size
- learning
- accuracy
- arxiv
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified framework for diagnosing and mitigating
  loss of trainability (LoT) in continual learning. The authors identify two distinct
  failure modes: gradient-noise dominated updates (where noise overwhelms the signal)
  and curvature-noise dominated updates (where volatile sharpness causes instability).'
---

# A Unified Noise-Curvature View of Loss of Trainability

## Quick Facts
- **arXiv ID**: 2509.19698
- **Source URL**: https://arxiv.org/abs/2509.19698
- **Reference count**: 40
- **Primary result**: Per-layer adaptive noise threshold scheduler significantly improves accuracy over baselines like CReLU, L2 weight decay, and Wasserstein regularization in continual learning with loss of trainability

## Executive Summary
This paper proposes a unified framework for diagnosing and mitigating loss of trainability (LoT) in continual learning. The authors identify two distinct failure modes: gradient-noise dominated updates (where noise overwhelms the signal) and curvature-noise dominated updates (where volatile sharpness causes instability). They introduce a per-layer adaptive noise threshold combining a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound. This threshold guides a simple per-layer step-size scheduler that adjusts learning rates to keep updates within safe bounds. Experiments show that this scheduler significantly improves accuracy over baselines like CReLU, L2 weight decay, and Wasserstein regularization, and produces adaptive step-size trajectories that mirror manual decay schedules without tuning. The approach effectively preserves trainability across non-stationary task sequences.

## Method Summary
The method implements a per-layer step-size scheduler that tracks effective learning rates, estimates curvature volatility, and adjusts base learning rates to prevent loss of trainability. The scheduler estimates top Hessian eigenvalue via power iteration, computes normalized sharpness with EMA volatility tracking, and combines gradient-noise and curvature bounds into a single threshold. When the effective step-size exceeds this threshold, the base learning rate is cooled multiplicatively; when well below, it's warmed early in training. The approach is evaluated on random-label MNIST with 40 tasks using a 2-layer MLP.

## Key Results
- Per-layer scheduler achieves significantly higher accuracy than baselines (CReLU, L2 weight decay, Wasserstein regularization) on random-label MNIST continual learning
- Combined noise-curvature threshold more accurately predicts accuracy drops than either signal alone
- Per-layer control outperforms global scheduling by ~15-20% accuracy
- Scheduler produces adaptive step-size trajectories that naturally mirror manual decay schedules without manual tuning

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Noise Dominated Updates
When the effective step-size exceeds a batch-size-aware gradient-noise threshold, updates become noise-dominated rather than signal-driven, causing training to stall despite adequate curvature. As the effective step-size drifts upward, an increasing fraction of updates violate this bound, making progress random rather than directed.

### Mechanism 2: Curvature-Volatility Dominated Updates
When sharpness volatility (temporal variance in normalized curvature) exceeds mean sharpness, even nominally correct step-sizes become unstable, causing oscillation between descent and ascent. High volatility indicates that the local curvature is noisy or changing rapidly, destabilizing large steps.

### Mechanism 3: Per-Layer Effective Step-Size Drift via Adam Preconditioning
Adam's effective step-size drifts upward heterogeneously across layers as weight norms grow, crossing the combined noise-curvature threshold and triggering LoT. Layer 1, exposed to raw input shifts, shows highest volatility; deeper layers remain more stable.

## Foundational Learning

- **Concept: Effective Step-Size in Adaptive Optimizers**
  - Why needed here: The paper's core diagnostic compares the actual step-size Adam applies against theoretical bounds. Without understanding how Adam's preconditioning converts base LR into effective step-size, the threshold violations cannot be interpreted.
  - Quick check question: Given Adam state with gradient magnitudes trending upward, does the effective step-size increase, decrease, or stay constant for a fixed base LR?

- **Concept: Sharpness (Spectral Norm) and Its Temporal Volatility**
  - Why needed here: The curvature-volatility bound requires estimating the top Hessian eigenvalue and tracking its temporal statistics. Understanding sharpness as a measure of loss landscape curvature is prerequisite.
  - Quick check question: If the top eigenvalue λ_max = 1000 with variance σ² = 40000, is the volatility high or low relative to the mean?

- **Concept: Continual Learning Non-Stationarity vs. Catastrophic Forgetting**
  - Why needed here: LoT is distinct from forgetting—it's about future learning capacity, not retention. Confounding these leads to misdiagnosis (e.g., trying to fix LoT with replay buffers).
  - Quick check question: A model perfectly remembers old tasks but cannot learn new ones—is this forgetting, LoT, or both?

## Architecture Onboarding

- **Component map**: Per-layer effective step-size tracker -> Sharpness estimator -> Volatility tracker -> Combined bound calculator -> Per-layer scheduler
- **Critical path**: Sharpness estimation → Volatility computation → Combined threshold → Per-layer LR adjustment. The sharpness estimator is the computational bottleneck (Hessian-vector products).
- **Design tradeoffs**:
  - Per-layer vs. global control: Per-layer is ~15-20% higher accuracy but requires tracking more state
  - Power iteration budget: More steps = better λ estimate but higher overhead; paper uses k=1, ~100 steps as practical default
  - Warm/cool rates: Aggressive cooling prevents violations but may under-utilize safe step-sizes; paper uses ε ≈ 0.01 (c=0.99, u=1.01)
- **Failure signatures**:
  - Immediate accuracy collapse (Task 1): Indicates combined bound is too loose or warm-up is too aggressive
  - Gradual decay over tasks: Indicates effective step-size drift; verify Layer 1's α_t is crossing threshold progressively
  - Excessive cooling to near-zero LR: Volatility signal may be too noisy; increase EMA window or add a lower bound on LR
- **First 3 experiments**:
  1. Reproduce Figure 4 (diagnosis): Train vanilla L2/Wasserstein/CReLU baselines on random-label MNIST. Plot α_t vs. α*_g, α*_vol, and combined α* to verify the combined bound most accurately predicts accuracy drops.
  2. Ablate signals (Figure 7): Run three scheduler variants—gradient-only, curvature-only, combined—on L2 and Wasserstein. Confirm combined outperforms both single-signal variants.
  3. Per-layer vs. global (Figure 10): Implement both controller types on CReLU and L2. Verify per-layer achieves higher accuracy and examine Layer 1 vs. Layer 2/3 effective step-size trajectories.

## Open Questions the Paper Calls Out

### Open Question 1
Does the unified noise-curvature framework generalize to deep convolutional or transformer-based architectures? The experimental evaluation is restricted exclusively to a two-layer MLP with width 256. The dynamics of gradient noise and curvature volatility may differ significantly in deep, modern architectures with complex inductive biases compared to shallow MLPs.

### Open Question 2
How does the scheduler perform on non-stationary streams involving semantic drift rather than random label noise? The authors isolate optimization dynamics by using random-label MNIST, explicitly excluding semantic transfer or feature reuse from the evaluation. Real-world continual learning often relies on transferring features rather than just maintaining trainability for memorization.

### Open Question 3
Is the computation of curvature volatility feasible for large-scale models without becoming a computational bottleneck? While efficient for small MLPs, the cost of repeated Hessian-vector products may be prohibitive for large foundation models where trainability is also a concern.

## Limitations
- Experimental scope limited to shallow MLP architecture on random-label MNIST, raising questions about generalizability
- Hessian-based sharpness estimation is computationally expensive and may not scale to large models
- Wasserstein regularization baseline implementation details unspecified, limiting fair comparison
- Core mechanism relies on Adam-specific dynamics, which may not transfer to other optimizers

## Confidence

- **High**: Identification of gradient-noise dominated updates as a failure mode (supported by consistent experimental evidence across baselines in Figure 4). Per-layer scheduling mechanism and implementation details are clearly specified and reproducible.
- **Medium**: Curvature-volatility dominated update mechanism (relies on Hessian estimation, which is computationally heavy and may be noisy in practice; evidence is correlational rather than causal). Claim that combined bound outperforms single-signal variants (convincing but based on limited baselines).
- **Low**: Assertion that unified noise-curvature view is necessary to explain LoT (other mechanisms not ruled out). Generality of approach to non-MNIST, non-MLP settings is asserted but not demonstrated.

## Next Checks

1. **Ablate the Hessian estimator**: Replace the power iteration with a simpler curvature proxy (e.g., gradient variance) and re-run the scheduler. If performance drops significantly, the Hessian-based approach is validated; if not, the added complexity may be unnecessary.

2. **Cross-architectural validation**: Implement the scheduler on a CNN (e.g., LeNet) or a deeper MLP on permuted MNIST. If the combined bound still predicts LoT and the scheduler mitigates it, the approach generalizes; if not, it may be specific to shallow MLPs.

3. **Non-adaptive optimizer test**: Run the same continual learning setup with SGD + momentum. If LoT still occurs and the scheduler (adapted for SGD) mitigates it, the mechanisms are optimizer-agnostic; if not, the paper's claims are overly tied to Adam's dynamics.