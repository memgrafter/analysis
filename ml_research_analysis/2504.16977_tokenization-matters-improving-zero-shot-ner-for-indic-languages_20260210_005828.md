---
ver: rpa2
title: 'Tokenization Matters: Improving Zero-Shot NER for Indic Languages'
arxiv_id: '2504.16977'
source_url: https://arxiv.org/abs/2504.16977
tags:
- tokenization
- languages
- sentencepiece
- indic
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates tokenization strategies for Named Entity Recognition
  (NER) in low-resource Indic languages, comparing Byte-Pair Encoding (BPE), SentencePiece,
  and Character-Level tokenization. Intrinsic analysis using the FLORES-200 dataset
  shows that SentencePiece better preserves morphological structure and generalizes
  well across morphologically rich and script-diverse languages like Santali, Manipuri,
  and Sindhi.
---

# Tokenization Matters: Improving Zero-Shot NER for Indic Languages

## Quick Facts
- **arXiv ID**: 2504.16977
- **Source URL**: https://arxiv.org/abs/2504.16977
- **Reference count**: 33
- **Primary result**: SentencePiece outperforms BPE and character-level tokenization for zero-shot NER in low-resource Indic languages

## Executive Summary
This study systematically evaluates tokenization strategies for Named Entity Recognition (NER) in low-resource Indic languages. The research compares Byte-Pair Encoding (BPE), SentencePiece, and Character-Level tokenization using both intrinsic analysis on the FLORES-200 dataset and extrinsic evaluation on Hindi and Bengali NER datasets. The findings reveal that SentencePiece consistently outperforms other tokenization methods, particularly in zero-shot cross-lingual settings where it achieves F1 scores up to 88.38% on Assamese, while BPE fails completely (0.00% F1) on unseen languages.

## Method Summary
The study employs a two-phase evaluation framework. First, intrinsic analysis examines tokenization quality using FLORES-200, measuring metrics like morphological structure preservation and generalization across languages. Second, extrinsic evaluation tests zero-shot cross-lingual NER performance on Hindi and Bengali datasets, evaluating transfer to unseen languages like Assamese and Oriya. The comparison includes three tokenization strategies: BPE, SentencePiece, and Character-Level tokenization, with SentencePiece configured using the unigram language model variant.

## Key Results
- SentencePiece achieves superior zero-shot cross-lingual performance (F1 up to 88.38% on Assamese)
- BPE fails to generalize in unseen languages (0.00% F1 on Assamese and Oriya)
- SentencePiece better preserves morphological structure and entity boundaries across morphologically rich and script-diverse languages

## Why This Works (Mechanism)
SentencePiece's superior performance stems from its subword tokenization approach that better handles morphological complexity and script diversity in Indic languages. Unlike BPE, which can fragment morphologically meaningful units, SentencePiece maintains semantic coherence through its unigram language model, allowing it to preserve entity boundaries critical for NER tasks. The tokenization strategy directly impacts the model's ability to recognize and transfer entity recognition patterns across languages with different scripts but shared linguistic features.

## Foundational Learning
- **Subword tokenization**: Breaking words into smaller units for better handling of rare words and morphological variations
  - *Why needed*: Indic languages have rich morphology and compounding, making word-level approaches inefficient
  - *Quick check*: Verify tokenization preserves meaningful linguistic units

- **Zero-shot cross-lingual transfer**: Training on one language and evaluating on unseen languages without additional training
  - *Why needed*: Critical for low-resource languages where labeled data is scarce
  - *Quick check*: Test transfer patterns across typologically similar languages

- **Morphological structure preservation**: Maintaining meaningful linguistic units during tokenization
  - *Why needed*: Essential for NER where entity boundaries depend on morphological cues
  - *Quick check*: Analyze tokenization outputs for semantic coherence

## Architecture Onboarding

**Component Map**: Data -> Tokenizer -> NER Model -> Evaluation Metrics

**Critical Path**: Tokenization → Model Input → Entity Recognition → Performance Metrics

**Design Tradeoffs**: SentencePiece balances vocabulary size with semantic preservation, while BPE prioritizes compression at the cost of morphological integrity. Character-level tokenization avoids vocabulary issues but loses semantic context.

**Failure Signatures**: BPE fails completely on unseen languages (0.00% F1), indicating inability to generalize morphological patterns. Poor entity boundary preservation manifests as fragmented entity recognition across tokenization boundaries.

**First Experiments**:
1. Test tokenization strategies on morphologically rich words in Hindi
2. Evaluate zero-shot transfer from Hindi to closely related languages
3. Compare entity boundary preservation across tokenization methods

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three tokenization strategies without exploring newer alternatives like WordPiece
- Focus on specific subset of Indic languages, excluding major languages like Tamil, Telugu, and Kannada
- Zero-shot framework assumes linear transferability patterns that may not hold for languages with vastly different typological features

## Confidence
- **High Confidence**: SentencePiece outperforms BPE in zero-shot cross-lingual NER settings for Indic languages, well-supported by empirical evidence
- **Medium Confidence**: SentencePiece is "most effective" requires careful interpretation as study doesn't establish optimality across all possible tokenization strategies
- **Low Confidence**: Generalizability to non-Indic languages or other NLP tasks beyond NER remains uncertain

## Next Checks
1. Test tokenization strategies on broader set of Indic languages (Tamil, Telugu, Kannada) and non-Indic languages
2. Evaluate tokenization strategies on other sequence labeling tasks (POS tagging, chunking) and generation tasks (machine translation, summarization)
3. Investigate whether fine-tuning tokenizers on task-specific data alters performance rankings