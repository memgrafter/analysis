---
ver: rpa2
title: 'From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum
  Learning'
arxiv_id: '2510.26336'
source_url: https://arxiv.org/abs/2510.26336
tags:
- domain
- domains
- question
- section
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ACER, an automated curriculum learning framework\
  \ that systematically infuses domain-specific knowledge into large language models\
  \ without sacrificing their general capabilities. The approach generates structured,\
  \ textbook-style synthetic corpora guided by Bloom\u2019s taxonomy, creating progressively\
  \ difficult content and question-answer pairs across multiple educational levels."
---

# From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning

## Quick Facts
- arXiv ID: 2510.26336
- Source URL: https://arxiv.org/abs/2510.26336
- Reference count: 40
- Primary result: Automated curriculum learning framework that infuses domain-specific knowledge into LLMs via structured synthetic corpora and Bloom's taxonomy-guided progression, achieving 3 percentage point improvements on MMLU target domains

## Executive Summary
This paper introduces ACER, an automated curriculum learning framework that systematically infuses domain-specific knowledge into large language models without sacrificing their general capabilities. The approach generates structured, textbook-style synthetic corpora guided by Bloom's taxonomy, creating progressively difficult content and question-answer pairs across multiple educational levels. Continual pretraining with an interleaved curriculum schedule aligns learning across both content and cognitive dimensions. Experiments with Llama 3.2 (1B and 3B) show consistent macro-average improvements of 3 percentage points across target MMLU domains, with up to 5-point gains in challenging areas like microeconomics. Notably, ACER also improves performance on non-target domains by 0.7 points and enhances knowledge-intensive benchmarks (ARC, GPQA) by over 2 absolute points, while maintaining stable performance on general reasoning tasks. The results demonstrate ACER's effectiveness in closing critical domain gaps in LLMs through scalable knowledge infusion.

## Method Summary
ACER generates synthetic textbook-style curricula for target domains using a structured pipeline: domain detailing creates JSON schema for subject structure, table of contents generation organizes topics hierarchically, section content generation creates textbook-style explanations, and QA generation produces Bloom's taxonomy-aligned question-answer pairs at different cognitive levels. The synthetic corpus spans four personas (high school to researcher) with easy and hard QA pairs. During continual pretraining, ACER employs a Cog+Con schedule that interleaves both cognitive difficulty progression (books→easy QA→hard QA) and content progression (HS→UG→Grad→Researcher), mixing synthetic domain data 1:1 with general replay data to prevent catastrophic forgetting.

## Key Results
- Consistent macro-average improvements of 3 percentage points across target MMLU domains
- Up to 5-point gains in challenging domains like microeconomics
- 0.7-point improvement on non-target domains despite domain-specific training
- Over 2-point improvement on knowledge-intensive benchmarks (ARC, GPQA)
- Stable performance on general reasoning tasks maintained

## Why This Works (Mechanism)

### Mechanism 1: Structured Synthetic Curriculum Generation
Textbook-style content with Bloom's-aligned QA pairs provides more effective domain infusion than unstructured synthetic data. The Table of Contents enforces systematic topic coverage; QA pairs progress from factual recall to analysis/application, reinforcing both retrieval and reasoning within the domain. Core assumption: high-quality synthetic content from strong LLMs can substitute for curated human-authored textbooks. Evidence: "ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy." Break condition: If generated content contains factual errors or incoherent explanations, curriculum structure cannot compensate.

### Mechanism 2: Cognitive + Content Curriculum Scheduling (Cog+Con)
Ordering training along both cognitive difficulty and audience level amplifies learning over random mixing. Cognitive ordering (books→easy QA→hard QA) mirrors human learning; content ordering (high school→undergraduate→graduate→researcher) provides progressive scaffolding. The 1:1 replay ratio preserves general capabilities. Core assumption: Bloom's cognitive progression maps to effective LLM training order. Evidence: "Extending this with persona-based content progression in Cog+Con delivers the best overall performance, indicating that curriculum design amplifies the benefits of synthetic corpora." Break condition: Overly rigid ordering may prevent beneficial cross-domain connections.

### Mechanism 3: Replay-Based Forgetting Prevention
Equal mixing of synthetic domain data with general replay data balances specialization gains against capability preservation. Replay maintains general knowledge representations while domain data builds specialized circuits. Core assumption: The 1:1 ratio generalizes across domain scales. Evidence: "the synthetic in-domain corpus is mixed 1:1 with general replay data" and "The balanced 1:1 setting yields the strongest results." Break condition: For much larger/smaller domain corpora, adaptive mixing may be required.

## Foundational Learning

**Bloom's Taxonomy**
Why needed here: Defines cognitive difficulty levels (recall→comprehension→application→analysis) used to structure QA progression. Quick check: Can you name the six levels and distinguish "application" from "analysis"?

**Curriculum Learning**
Why needed here: Underlies the Cog+Con schedule—training with progressively harder examples improves sample efficiency. Quick check: Why might easy-to-hard ordering help a model learn faster than random shuffling?

**Catastrophic Forgetting**
Why needed here: Motivates replay mixing; domain-only CPT risks degrading general capabilities. Quick check: What happens if you fine-tune a pretrained LLM only on narrow domain data without replay?

## Architecture Onboarding

**Component map**: Domain Detailing (JSON schema) → Outline Generation (ToC tree) → Section Content Generation → QA Generation (easy/hard split) → Curriculum Scheduling (Cog+Con) → CPT with 1:1 replay

**Critical path**: ToC quality and prompt design for content/QA generation; bad prompts → incoherent corpus → no gains

**Design tradeoffs**: Cog+Con outperforms Interleaved for CPT (fragmented sequencing dilutes supervision); 1:1 replay balances specialization vs. generalization but may need tuning for larger corpora

**Failure signatures**: (1) Interleaved schedule drops performance on math/statistics; (2) 3:1 or 9:1 synthetic:replay ratios reduce non-target macro scores; (3) Contamination inflates MMLU scores

**First 3 experiments**:
1. Replicate Flat vs. Cog vs. Cog+Con on a single domain (e.g., microeconomics) with Llama 3.2 3B; verify 2-3 point macro gain
2. Ablate replay ratio (1:3, 1:1, 3:1) and confirm 1:1 is optimal for your domain scale
3. Decontaminate synthetic corpus against your target benchmark (cosine >0.9 threshold) to rule out memorization

## Open Questions the Paper Calls Out

**Open Question 1**: Would ACER yield comparable or greater gains when applied to larger-scale models (7B+)? Basis: "Due to time and resource constraints, we evaluated the approach on 1B and 3B scale models without extending the evaluation to larger-scale models." Unresolved because: Computational constraints limited experiments to 1B and 3B variants.

**Open Question 2**: How does the choice of generator model for synthetic textbook creation affect downstream performance? Basis: "Although our approach relies on a strong model (Gemini 2.0 Flash) for content generation, we did not ablate between different models for content generation." Unresolved because: Only one generator was tested.

**Open Question 3**: Why does interleaved curriculum scheduling degrade performance in continual pretraining when it benefits instruction tuning? Basis: Authors conjecture that "frequent task-switching introduced by interleaving likely overwhelms model capacity" but offer no direct evidence. Unresolved because: No ablation isolating task-switching effects.

**Open Question 4**: Can adaptive mixture schedules (varying in-domain vs. replay ratios during training) outperform the fixed 1:1 ratio? Basis: The ablation on data proportions concludes with "adaptive mixture schedules as a promising future direction." Unresolved because: Only static ratios were tested.

## Limitations

**Corpus Quality Uncertainty**: The effectiveness hinges on synthetic curriculum quality, but no direct evaluation against human-authored textbooks was performed. Factual errors in generation could propagate through the curriculum.

**Hyperparameter Sensitivity**: Key parameters like replay ratio and training steps are not thoroughly explored across domains. The 1:1 ratio may not generalize to all domain sizes.

**Mechanism Isolation**: The paper attributes gains to the combination of structured data and Cog+Con scheduling, but insufficient ablation prevents isolating individual contributions.

## Confidence

**High Confidence**: The general claim that ACER improves domain-specific MMLU performance by ~3 percentage points is supported by experimental results across five domains. The observation that replay mixing prevents catastrophic forgetting is well-established.

**Medium Confidence**: The claim that Cog+Con scheduling outperforms flat or interleaved schedules is supported but limited by sample size. The assertion that ACER improves non-target domains and knowledge benchmarks is supported but could be influenced by contamination.

**Low Confidence**: The specific mechanism by which Bloom's taxonomy ordering enhances LLM learning is assumed but not directly validated. The claim that synthetic curriculum substitutes effectively for human-authored textbooks is not empirically validated.

## Next Checks

1. **Ablate Cog+Con vs. Flat Curriculum on Single Domain**: Replicate the study on a single domain (e.g., microeconomics) with both Cog+Con and Flat schedules. Measure learning curves to see if Cog+Con accelerates convergence or improves final accuracy.

2. **Test Replay Ratio Sensitivity Across Domain Sizes**: Vary the replay ratio (1:3, 1:1, 3:1, 9:1) systematically across domains of different sizes. Determine if 1:1 is universally optimal or if the ideal ratio scales with domain corpus size.

3. **Independent Corpus Quality Assessment**: Have a domain expert evaluate a sample of the synthetic curriculum for factual accuracy, pedagogical coherence, and alignment with Bloom's taxonomy. Compare this to a sample of human-authored textbook content.