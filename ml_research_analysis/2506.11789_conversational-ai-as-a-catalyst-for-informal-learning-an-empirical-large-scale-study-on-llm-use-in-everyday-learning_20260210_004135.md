---
ver: rpa2
title: 'Conversational AI as a Catalyst for Informal Learning: An Empirical Large-Scale
  Study on LLM Use in Everyday Learning'
arxiv_id: '2506.11789'
source_url: https://arxiv.org/abs/2506.11789
tags:
- learning
- llms
- learners
- participants
- education
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how large language models (LLMs) are adopted
  and used for everyday informal learning, identifying gaps in understanding broad
  user behavior and perceptions. To address this, the authors conducted a large-scale
  survey of 776 participants, combining descriptive and inferential statistics with
  latent class analysis to uncover distinct learner profiles based on device use,
  learning contexts, and tasks.
---

# Conversational AI as a Catalyst for Informal Learning: An Empirical Large-Scale Study on LLM Use in Everyday Learning

## Quick Facts
- arXiv ID: 2506.11789
- Source URL: https://arxiv.org/abs/2506.11789
- Reference count: 40
- 88% of respondents use LLMs for learning, with four distinct learner profiles identified.

## Executive Summary
This study investigates how large language models (LLMs) are adopted and used for everyday informal learning. Through a large-scale survey of 776 participants, the authors combine descriptive and inferential statistics with latent class analysis to uncover distinct learner profiles based on device use, learning contexts, and tasks. Results show that LLMs are now embedded in everyday learning, with younger, more educated, and tech-affine individuals leading adoption. Four learner profiles emerged: Structured Knowledge Builders, Self-Guided Explorers, Analytical Problem Solvers, and Adaptive Power Users. Despite privacy and accuracy concerns, most users perceive LLMs as effective learning aids and intend to continue using them. The study concludes that LLMs are now embedded in everyday learning, emphasizing the need for adaptive, multimodal, and privacy-conscious design to support diverse learner needs.

## Method Summary
The authors conducted a large-scale survey of 776 participants recruited via Prolific (Germany, fluent English, February 2025). The study combined descriptive and inferential statistics with latent class analysis (LCA) to identify distinct learner profiles. LCA was performed using the poLCA package in R, with model selection based on Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC). Multinomial logistic regression was used to predict class membership using demographic and attitudinal factors. The survey included measures of AI literacy, tech affinity, Big-5 personality traits, and usage patterns for learning tasks and contexts.

## Key Results
- 88% of respondents use LLMs for learning, with adoption driven by younger, more educated, and tech-affine individuals.
- Four distinct learner profiles emerged: Structured Knowledge Builders, Self-Guided Explorers, Analytical Problem Solvers, and Adaptive Power Users.
- Despite privacy and accuracy concerns, most users perceive LLMs as effective learning aids and intend to continue using them.

## Why This Works (Mechanism)
LLMs work as learning catalysts by providing on-demand, conversational access to information and personalized assistance. Their ability to adapt responses to user queries, summarize complex topics, and offer step-by-step guidance makes them particularly effective for informal learning. The study identifies that users leverage LLMs across diverse contexts—from structured academic settings to hobby-driven exploration—suggesting that their flexibility and accessibility lower barriers to learning. However, the convenience of LLMs may sometimes overshadow accuracy concerns, indicating a trade-off between ease of use and information reliability.

## Foundational Learning
- **Latent Class Analysis (LCA)**: A statistical method for identifying unobserved subgroups within a population based on response patterns. Why needed: To segment users into distinct learner profiles based on their LLM usage patterns. Quick check: Verify that the 4-class solution has the lowest BIC/AIC compared to 1-3 or 5-6 class models.
- **Multinomial Logistic Regression**: A predictive model for categorical outcomes with more than two categories. Why needed: To identify which demographic and attitudinal factors predict membership in each learner profile. Quick check: Ensure significant predictors (p < 0.05) align with the profile descriptions (e.g., ATI predicting Adaptive Power Users).
- **Survey Instrument Design**: The process of creating structured questionnaires with validated scales. Why needed: To reliably measure constructs like AI literacy, tech affinity, and learning task preferences. Quick check: Confirm that branching logic correctly separated LLM users from non-users.

## Architecture Onboarding
**Component Map**: Survey Instrument -> Data Collection -> Latent Class Analysis -> Profile Validation -> Multinomial Logistic Regression -> Results Interpretation

**Critical Path**: Data Collection → LCA Model Selection → Profile Characterization → Regression Analysis → Interpretation

**Design Tradeoffs**: The study prioritized breadth of participation (large N) over depth of qualitative insight. LCA offers interpretable clusters but may oversimplify continuous variation in usage patterns. Self-reported data captures perceptions but not objective learning outcomes.

**Failure Signatures**: 
- LCA instability: Different random starts yield inconsistent class solutions.
- Multicollinearity: ATI and AI Literacy scores are highly correlated, inflating standard errors.
- Non-response bias: Prolific sample may not represent broader informal learners.

**3 First Experiments**:
1. Re-run LCA with 10 random starts to test solution stability.
2. Compare profile distributions across demographic subgroups (age, education).
3. Validate profile assignments by cross-tabulating with self-reported learning gains.

## Open Questions the Paper Calls Out
- How do learners recognize hallucinations in LLM outputs, and what strategies do they use to cross-check information? The study identified a "convenience over accuracy" paradox but did not investigate specific verification behaviors or cognitive mechanisms users employ.
- Do disparities in access, confidence, or prompt literacy prevent learners from evolving into "Adaptive Power Users"? While four profiles were identified, the study did not determine structural or educational barriers that might prevent users in other profiles from reaching high-engagement states.
- How do specific usage patterns, such as summarization versus brainstorming, translate into actual learning gains? The study relied on self-reported perceived productivity rather than objective measurements of knowledge retention or skill acquisition.

## Limitations
- Reliance on self-reported survey data, which may be subject to social desirability bias and recall inaccuracies.
- Sample limited to English-speaking German participants, potentially limiting generalizability to other cultural or linguistic contexts.
- LCA stability concerns due to sensitivity to model specifications and initialization parameters.
- No raw data or exact survey instrument provided, hindering independent verification and replication.

## Confidence
- **High Confidence**: Descriptive statistics on LLM adoption rates (88%) and demographic patterns (younger, educated, tech-affine users leading adoption) are well-supported by the data and align with prior research.
- **Medium Confidence**: Identification of four distinct learner profiles via LCA is methodologically sound, but stability and precise definitions depend on model fit details not fully provided.
- **Low Confidence**: Claims about LLM effectiveness as learning aids and users' intentions to continue using them are based on self-perception and stated intentions, not objective measures of learning gains or actual future behavior.

## Next Checks
1. **Replicate LCA with Full Survey Instrument**: Obtain and use the exact survey questions to re-run the LCA on the same or similar dataset, ensuring convergence stability across multiple random starts.
2. **Cross-Cultural Validation**: Conduct the same survey with diverse populations (e.g., non-German, non-English-speaking) to test generalizability of the four learner profiles and adoption patterns.
3. **Longitudinal Tracking**: Follow up with a subset of respondents over 6-12 months to measure actual continued LLM use for learning and correlate it with objective learning outcomes, rather than relying solely on stated intentions.