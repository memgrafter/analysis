---
ver: rpa2
title: 'One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in
  Finetuned LLMs'
arxiv_id: '2512.14751'
source_url: https://arxiv.org/abs/2512.14751
tags:
- pretrained
- llms
- finetuned
- jailbreak
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the security risks posed by finetuned large
  language models (LLMs) inheriting jailbreak vulnerabilities from their pretrained
  sources. In a realistic pretrain-to-finetune threat model, the attacker has white-box
  access to a publicly released pretrained model and only black-box access to its
  finetuned derivatives.
---

# One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs

## Quick Facts
- arXiv ID: 2512.14751
- Source URL: https://arxiv.org/abs/2512.14751
- Authors: Yixin Tan; Zhe Yu; Jun Sakuma
- Reference count: 37
- Primary result: PGP attack achieves 60-87% transferability success rate, outperforming baselines by exploiting linearly separable jailbreak vulnerabilities inherited from pretrained LLMs

## Executive Summary
This paper investigates security risks in finetuned large language models (LLMs) that inherit jailbreak vulnerabilities from their pretrained checkpoints. Under a realistic pretrain-to-finetune threat model where attackers have white-box access to publicly released pretrained models but only black-box access to finetuned derivatives, the authors demonstrate that adversarial prompts crafted on pretrained models transfer most effectively to finetuned variants. Empirical analysis reveals that transferable jailbreak prompts are linearly separable within pretrained hidden representations, indicating transferability is encoded as a learnable structure in representation space. Based on this insight, the authors propose the Probe-Guided Projection (PGP) attack, which uses linear probe-derived directions to guide adversarial prompt optimization, achieving significantly higher transferability success rates than existing methods even against safety-finetuned models.

## Method Summary
The paper proposes PGP (Probe-Guided Projection), a jailbreak attack that leverages linear probes trained on pretrained LLM hidden states to identify transferability-relevant directions. The attack generates adversarial prompts on a pretrained model using GCG optimization, then extracts probe weights as directions encoding transferability. PGP optimizes a joint objective combining success on the pretrained model and projection onto the transferability direction, using discrete gradient-based token replacement. The method is evaluated across multiple LLM families (Llama2-7b-chat, Llama3-8b-Instruct, Deepseek-7b-chat, Gemma-7b-it) and finetuning tasks (Alpaca, Dolly, Codealpaca, GSM8K, CodeEvol), measuring Transfer Success Rate (TSR) against black-box finetuned models.

## Key Results
- PGP achieves 60.4% TSR vs 28.8% for Guiding-GCG on Llama2-7b, outperforming all baselines
- Linear probes achieve >80% accuracy in classifying transferable vs non-transferable prompts across intermediate layers
- Safety-finetuned models show only limited protection, with TSR reducing from ~60% to ~42% with 2000 safety examples
- Non-linear probes (RBF, Nystrom kernels) achieve high classification accuracy but near-zero attack transfer, suggesting transferability is encoded linearly
- PGP completely fails when using mismatched pretrained models for probe training, confirming provenance sensitivity

## Why This Works (Mechanism)

### Mechanism 1
Finetuned LLMs inherit jailbreak vulnerabilities from their pretrained checkpoints, enabling adversarial prompts crafted on pretrained models to transfer effectively to finetuned variants. Finetuning updates model parameters on task-specific data but largely preserves the intermediate-layer representational structure learned during pretraining. Since jailbreak success correlates with specific hidden representations, adversarial perturbations that exploit these structures in the pretrained model continue to work on finetuned derivatives.

### Mechanism 2
Transferable jailbreak prompts are linearly separable from non-transferable ones within pretrained hidden states, indicating transferability is encoded as a learnable structure in representation space. Linear probes trained on intermediate-layer representations can distinguish transferable vs. untransferable prompts with high accuracy (>80%). This suggests the pretrained model encodes transferability-relevant features along specific directions in its hidden space, which persist regardless of downstream finetuning.

### Mechanism 3
Guiding adversarial optimization along probe-identified transferability directions substantially improves attack transfer success rates compared to unguided or naive transfer methods. PGP computes the projection of adversarial perturbations onto normalized probe weight vectors, maximizing movement along transferability-relevant directions. A joint objective balances this with success on the pretrained model, constraining optimization to subspaces associated with cross-model generalization.

## Foundational Learning

- **Linear Probing**: Why needed here: The paper uses linear classifiers (SVMs) trained on hidden states to reveal that transferability is encoded linearly in pretrained representations. Quick check: Given hidden states h from layer l of a pretrained model, can you sketch how a linear probe would classify inputs as "transferable" vs. "non-transferable"?

- **Adversarial Transferability**: Why needed here: The core threat model assumes white-box access to pretrained models and black-box access to finetuned targets. Understanding why adversarial examples transfer across model variants is foundational to the paper's security claims. Quick check: Why would an adversarial suffix optimized on a pretrained LLM also jailbreak a finetuned variant, even though the finetuned model has different parameters?

- **GCG (Greedy Coordinate Gradient)**: Why needed here: PGP builds on GCG's discrete optimization framework, adding a probe-guided projection term. You need to understand the base attack to see how PGP modifies the objective. Quick check: In GCG, how does the gradient information guide token replacement when the input space is discrete?

## Architecture Onboarding

- **Component map**: GCG attack generation -> Linear probe training on hidden states -> Extract v_transfer direction -> PGP optimization with joint objective -> Evaluate TSR on finetuned models

- **Critical path**: 1) Generate GCG jailbreak prompts on pretrained LLMs 2) Label as transferable/untransferable based on finetuned model success 3) Train linear SVM probes on each layer using hidden states 4) Extract normalized probe weights as v_transfer direction 5) Run PGP optimization with joint objective 6) Evaluate on finetuned models via black-box queries

- **Design tradeoffs**: Linear vs. non-linear probes (linear achieves 59% TSR vs 2% for RBF), layer selection (deeper layers show higher separability but not systematically optimized), Î» weighting (balances success vs transfer projection but no ablation provided)

- **Failure signatures**: Wrong pretrained model causes TSR to drop to near-zero, non-linear probes yield high accuracy but near-zero TSR, heavy safety-tuning reduces but doesn't eliminate vulnerability

- **First 3 experiments**: 1) Reproduce linear probe separability on Llama2-7b-chat 2) Ablate probe type (linear vs RBF vs logistic regression) 3) Test provenance sensitivity with mismatched pretrained models

## Open Questions the Paper Calls Out

- **How can the inheritance of vulnerabilities from pretrained models be effectively mitigated during the finetuning process without compromising downstream task performance?**: The authors explicitly state they do not propose a defense and plan future work on methods to prevent or reduce inheritance during finetuning.

- **Can standard adversarial training or safety alignment techniques applied during finetuning successfully obfuscate the linearly separable transferability directions identified by the PGP attack?**: The paper only evaluates data-level defenses (adding safety examples), not optimization-level defenses like adversarial training.

- **Does the failure of non-linear probes (RBF, Nystrom) to guide attacks imply that jailbreak transferability is encoded strictly in linear subspaces, or does the dimensionality transformation disrupt the safety alignment geometry?**: The authors hypothesize dimensionality distortion but haven't verified whether transferability features persist in non-linear manifolds.

## Limitations

- Attack completely fails when pretrained model used for probe training doesn't match target's actual pretrained source, creating strong dependency on attacker knowledge
- Safety finetuning with 2000 examples only reduces TSR from ~60% to ~42%, showing limited protection but not elimination of vulnerability
- The paper doesn't systematically optimize layer selection for probe guidance, which could affect reproducibility and performance

## Confidence

- **High confidence**: Finetuned LLMs inherit jailbreak vulnerabilities from pretrained sources; linear probes can distinguish transferable vs. non-transferable prompts with high accuracy
- **Medium confidence**: PGP's superior performance is robust across model families, though hyperparameter choices could affect results
- **Low confidence**: Generalizability to completely different pretraining paradigms or architectures beyond LLMs is untested

## Next Checks

1. **Provenance sensitivity test**: Systematically evaluate PGP performance when using mismatched pretrained models to confirm near-zero TSR findings are reproducible
2. **Layer-wise ablation study**: Test PGP performance using probes from different layers to identify optimal layer selection and understand transferability distribution
3. **Aggressive safety finetuning evaluation**: Test PGP against models finetuned with 10K+ safety examples and different learning rates to determine if more extensive safety alignment can meaningfully reduce inherited jailbreak vulnerabilities