---
ver: rpa2
title: 'Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc
  Explanation Perspective'
arxiv_id: '2508.16969'
source_url: https://arxiv.org/abs/2508.16969
tags:
- knowledge
- frame
- text
- language
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining black-box pre-trained
  language models (PLMs) by probing their understanding of implicit knowledge beyond
  the given text. The authors propose KnowProb, a knowledge-guided probing approach
  that leverages frame semantics from FrameNet to generate knowledge beyond surface-level
  text content.
---

# Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective

## Quick Facts
- arXiv ID: 2508.16969
- Source URL: https://arxiv.org/abs/2508.16969
- Authors: Yunxiao Zhao; Hao Xu; Zhiqiang Wang; Xiaoli Li; Jiye Liang; Ru Li
- Reference count: 40
- Key outcome: Current PLMs struggle to capture hidden frame-based knowledge beyond surface text; fine-tuning with frame data improves performance.

## Executive Summary
This paper introduces KnowProb, a knowledge-guided probing approach to evaluate whether black-box pre-trained language models (PLMs) understand implicit knowledge beyond the given text. The method leverages FrameNet semantics to generate probing questions based on hidden semantic frames rather than surface content, mitigating train-test overlap. Experiments across 13 models (including BERT, XLNet, GPT-3.5-turbo, LLaMa-2 variants, and ERNIE-4.0) on Chinese machine reading comprehension datasets show that PLMs perform below random chance on hidden knowledge tasks in zero-shot settings, though fine-tuning with frame-based data significantly improves performance. The results demonstrate that even large-scale PLMs face substantial challenges in reasoning about implicit semantic structures.

## Method Summary
KnowProb uses a Frame Semantic Parser to extract frames and frame elements from text, constructing a frame-based knowledge graph. This graph is converted into multiple-choice QA probes that test relationships not explicitly stated in the context. The method evaluates PLMs in zero-shot, fine-tuned, and frame-enhanced settings. The parser is trained on 10,000 Chinese Tree Bank sentences annotated with 695 frames and 990 elements, and probes are generated from the C3 machine reading comprehension dataset.

## Key Results
- Zero-shot PLM performance on hidden knowledge tasks is below random chance across all tested models.
- Fine-tuning with standard data improves performance, but integrating frame-based knowledge provides additional gains.
- Large models (e.g., ERNIE-4.0-8K) show better performance than smaller models but still lag significantly behind human performance (92.4% vs. 65% for best model).
- The performance gap indicates PLMs learn single distributions of representation rather than robust semantic understanding.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Frame Extraction & Graph Construction
Generating probing questions based on external semantic frames creates a test for "hidden knowledge" that mitigates train-test overlap. The system parses input text using a Frame Semantic Parser to identify Frames and Frame Elements, mapping these into a Frame-based Knowledge Graph. This graph represents the underlying cognitive structure of the text, independent of specific phrasing. The core assumption is that FrameNet accurately captures necessary world knowledge and the parser can reliably extract these structures.

### Mechanism 2: Post-Hoc Probing via Hidden Knowledge Association
Probing models on relations between unseen frame elements exposes a representation gap where models fail to generalize beyond surface distributions. The system converts triples from the Knowledge Graph into multiple-choice QA prompts, testing the model's ability to infer relationships not explicitly stated in the context but required for deep understanding. The core assumption is that if a model "understands" the text, it must possess the implicit background knowledge associated with the concepts in the text.

### Mechanism 3: Knowledge-Enhanced Representation Learning
Fine-tuning with frame-based hidden knowledge acts as data augmentation that forces the model to learn robust semantic representations rather than surface-level shortcuts. By augmenting training data with generated questions about hidden knowledge, the model is regularized to attend to semantic roles and inter-frame relations. The core assumption is that performance gain is due to improved structural understanding transferable to other tasks, not memorization.

## Foundational Learning

- **Concept: Frame Semantics (FrameNet)**
  - Why needed here: The core relies on FrameNet's hypothesis that understanding involves activating a "frame" (scenario) with specific roles. Without this, "hidden knowledge" generation is a black box.
  - Quick check question: Given "The student read the book," identify the Frame (Reading) and two Frame Elements (Reader, Text)?

- **Concept: Post-Hoc Explanation**
  - Why needed here: The method treats PLM as a black box to be examined after training, rather than modifying training itself.
  - Quick check question: Does post-hoc explanation require model gradients/weights or just input-output pairs? (Answer: Typically just I/O).

- **Concept: Train-Test Overlap (Data Contamination)**
  - Why needed here: The paper argues standard benchmarks suffer from overlap; KnowProb generates new implicit questions to solve this.
  - Quick check question: Why might a model score 90% on reading comprehension yet fail logical inference about the same text? (Answer: It may have memorized the test distribution/overlap).

## Architecture Onboarding

- **Component map:** Raw Text -> Frame Semantic Parser -> Graph Engine -> Probing Interface -> Subject Model
- **Critical path:** The Frame Semantic Parser. The entire validity of the "hidden knowledge" probe depends on the parser correctly identifying semantic frames. If it misidentifies "Travel" as "Motion," generated questions will be nonsensical.
- **Design tradeoffs:**
  - Precision vs. Recall in Parsing: Strict parsing yields high-quality but fewer probing questions; loose parsing generates more data but introduces noise.
  - Zero-shot vs. Fine-tuning: Zero-shot shows abysmal performance, forcing a decision between diagnosing a frozen model or trying to fix it via fine-tuning.
  - Complexity: FrameNet is manually intensive; using it covers deep semantics but risks being brittle compared to statistical n-gram models.
- **Failure signatures:**
  - Random-level Performance: If probed model consistently scores â‰ˆ25-33% (random chance), it indicates no representation of hidden knowledge.
  - High Surface / Low Hidden Performance: If model aces standard MRC but fails KnowProb probe, it confirms the model is a "surface learner."
- **First 3 experiments:**
  1. Sanity Check: Take pre-trained BERT, run KnowProb on held-out set, verify if accuracy is statistically different from random guessing.
  2. Parser Ablation: Manually verify 50 random sentences to ensure Frame Semantic Parser correctly identifies "hidden knowledge."
  3. Enhancement Validation: Fine-tune a model on augmented dataset (Standard QA + Frame-based QA) and measure performance lift on original benchmark.

## Open Questions the Paper Calls Out

- How can the frame-based knowledge modeling approach be adapted to improve reasoning capabilities specifically across out-of-domain distributions?
- Can specific training paradigms be developed to overcome the limitation where PLMs "only learn a single distribution of representation"?
- What specific architectural or scale-related constraints prevent Large Language Models from closing the performance gap with humans on hidden knowledge tasks?

## Limitations
- Parser Generalization Gap: The Frame Semantic Parser trained on Chinese Tree Bank (news domain) is applied to C3 (exam questions + dialogue) without reporting parser performance on out-of-domain data.
- Knowledge Graph Construction: Conversion from parsed frames to probing triples relies on manually defined templates without reporting inter-annotator agreement on template quality or consistency.
- Evaluation Metric Ambiguity: The reported "Accuracy" metric does not clarify whether it accounts for multiple correct answers per question or how tie-breaking is handled in the multiple-choice setup.

## Confidence
- High Confidence: Claims about zero-shot performance being below random chance are well-supported by quantitative results across 13 PLMs and 3 datasets.
- Medium Confidence: The assertion that fine-tuning improves performance is supported by tables, but the specific contribution of frame-augmented data vs. standard fine-tuning is not isolated in ablation studies.
- Low Confidence: The claim that current PLMs "only learn a single distribution of representation" is theoretically grounded but lacks quantitative comparison of representation geometry.

## Next Checks
1. **Parser Domain Transfer Test:** Evaluate the trained Frame Semantic Parser on a held-out subset of C3-D (dialogue) and report F1 scores for Frame/Element identification. If performance drops >20%, the low probe scores may reflect parser failure rather than model limitations.

2. **Template Consistency Audit:** Manually validate 50 randomly sampled probing questions from each of the 6 types (IFEs, EFEs, FFRs) to ensure: (a) ground-truth answers are logically derivable from context, and (b) templates are applied consistently. Report inter-rater agreement.

3. **Representation Analysis:** Train a linear probe on frozen PLM representations to classify frames/elements directly (bypassing the generated QA). Compare zero-shot accuracy of the linear probe vs. the generated QA probe to quantify how much performance loss is due to question generation vs. model understanding.