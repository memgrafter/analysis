---
ver: rpa2
title: 'The quasi-semantic competence of LLMs: a case study on the part-whole relation'
arxiv_id: '2504.02395'
source_url: https://arxiv.org/abs/2504.02395
tags:
- relation
- llms
- part
- part-whole
- parts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the extent of semantic competence of large
  language models (LLMs) regarding the part-whole relation (meronymy). It uses behavioral
  testing, probabilistic analysis, and concept representation analysis to evaluate
  LLMs' understanding of this relation and its antisymmetric property.
---

# The quasi-semantic competence of LLMs: a case study on the part-whole relation

## Quick Facts
- **arXiv ID:** 2504.02395
- **Source URL:** https://arxiv.org/abs/2504.02395
- **Reference count:** 27
- **Primary result:** LLMs show "quasi-semantic" competence in meronymy - strong at generating parts but weak at understanding antisymmetric properties and abstract relational representations.

## Executive Summary
This paper investigates the extent of semantic competence in large language models regarding the part-whole relation (meronymy). Using behavioral testing, probabilistic analysis, and concept representation analysis, the study finds that while LLMs can generate parts for given objects with high accuracy, they struggle to consistently understand and apply the antisymmetric nature of meronymy. The research concludes that LLMs have a "quasi-semantic" competence regarding meronymy, falling short of capturing its deep inferential properties.

## Method Summary
The study uses three complementary methodologies: behavioral testing via prompting where models answer questions about part-whole relationships, probabilistic analysis measuring sentence likelihood to probe latent knowledge, and representational analysis examining whether part-whole concepts are encoded as linear directions in embedding spaces. The researchers evaluate models on datasets from McRae norms and ConceptNet, testing both generation of parts and recognition of meronymic relationships. They employ the Linear Representation Hypothesis framework to analyze concept encoding in model embeddings.

## Key Results
- LLMs show ~75% accuracy in probabilistic tasks but only ~60% in behavioral tasks, revealing a gap between latent knowledge and prompt-based performance
- Models excel at generating parts for given objects but perform poorly when asked to recognize their own generated parts as meronyms
- Part-whole concepts are only partially encoded in LLM embedding spaces, primarily for semantically similar items rather than as a general abstract relation
- No single linear direction in embedding space represents the general part-whole concept across all semantic classes

## Why This Works (Mechanism)

### Mechanism 1
LLMs demonstrate strong surface-level meronym generation but lack deep inferential competence due to a reliance on distributional statistics rather than grounded semantic relations. The models excel at generating parts (e.g., "wheel" for "car") because they have learned robust statistical co-occurrence patterns from their training data. However, the models fail to consistently apply the logical property of antisymmetry (if A is part of B, B cannot be part of A) because this is an inferential property of the relation itself, not a pattern of word co-occurrence. The "knowledge" is associative, not relational.

### Mechanism 2
Prompting and probability scoring can yield divergent assessments of an LLM's competence because they probe different levels of representation, with probability being a more reliable estimator of latent knowledge. Prompt-based behavioral tasks are meta-linguistic and require instruction-following, introducing noise. In contrast, measuring the log-probability of a sentence bypasses the instruction-following stage and directly measures how the model's internal probability distribution ranks plausible vs. implausible statements. The paper finds probabilistic analysis yields higher accuracy (~75%) than behavioral tasks (~60%), suggesting latent knowledge is more robust than behavioral outputs indicate.

### Mechanism 3
The part-whole relation is not encoded as a single, linear direction in LLM embedding spaces but is instead fragmented into class-specific subspaces. While class-specific meronymic pairs show some alignment within their own class, there is no single direction that aligns with meronymic pairs across all semantic classes. The concept is not abstract and general; it is fragmented and tied to semantic similarity rather than being a unified relational concept.

## Foundational Learning

- **Concept: Meronymy (Part-Whole Relation)**
  - Why needed here: This is the core subject of the paper. Understanding its definition, properties (especially antisymmetry), and different types is essential to grasping the experimental setup and the models' failures.
  - Quick check question: If "a finger is a part of a hand" is true, which property of meronymy is violated by the statement "a hand is a part of a finger"?

- **Concept: Linear Representation Hypothesis (LRH)**
  - Why needed here: The representational analysis in the paper is built on this hypothesis. Understanding LRH is key to interpreting the results of the vector space analysis and the conclusion that meronymy is not linearly encoded as a general concept.
  - Quick check question: According to the LRH, if "man" - "woman" â‰ˆ "king" - "queen," what does this imply about how concepts are organized in the model's vector space?

- **Concept: Behavioral Analysis vs. Probabilistic Analysis**
  - Why needed here: The paper uses these two complementary methods. Distinguishing between probing a model by asking it questions (behavioral) versus measuring the probability it assigns to statements (probabilistic) is critical for understanding the paper's multi-faceted conclusion.
  - Quick check question: Which method is considered less noisy for estimating a model's latent semantic knowledge, according to the paper's findings?

## Architecture Onboarding

- **Component map:**
  Input Processing (tokenization/embedding `W_E`) -> Core Architecture (transformer) -> Output Processing (unembedding `W_U`)

- **Critical path:**
  1. Select meronymic pairs from datasets (McRae, ConceptNet)
  2. For behavioral analysis, format pairs into prompts and record model output
  3. For probabilistic analysis, format pairs into sentences and record log-probabilities
  4. For representational analysis, extract static word vectors, compute differences, and analyze alignment using LRH methodology

- **Design tradeoffs:**
  - Behavioral vs. Probabilistic: Behavioral tasks are more intuitive but noisy and instruction-sensitive; probabilistic tasks are more robust but require logit access
  - Static vs. Contextualized Embeddings: Static embeddings simplify analysis and interpretation but may miss dynamic representations in intermediate layers

- **Failure signatures:**
  - Behavioral: Low accuracy on swapped versions (e.g., answering YES to "Is the car part of the wheel?")
  - Probabilistic: Assigning higher log-probability to implausible swapped sentences than correct originals
  - Representational: High variability in dot products between pair offsets and average concept direction; orthogonal class-specific concept vectors

- **First 3 experiments:**
  1. Run the Meronymy Knowledge Criterion behavioral task on a new LLM not in the study, testing ability to correctly answer both "Is X part of Y?" and "Is Y part of X?" for correct pairs
  2. Replicate the probabilistic analysis on a small scale with 20 pairs, comparing log-probability of correct vs. swapped versions
  3. Perform a simple representational check extracting embeddings for 5 pairs from a single semantic class, computing vector differences and average cosine similarity between difference vectors

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs effectively model the transitivity of the part-whole relation across different meronymic subtypes? The authors state in the conclusion that "future work is reserved to explore the transitivity property of meronymy," having restricted their study to antisymmetry due to the "unclear theoretical status" of transitivity. Transitivity is theoretically complex because it often fails across heterogeneous part-whole chains (e.g., a handle is part of a door, a door is part of a house, but a handle is not typically considered part of a house).

### Open Question 2
Can an automated methodology be developed to discover semantic sub-clusters with linear representations in LLM vector spaces? Section 8 notes that while class-specific parts are encoded linearly, a general relation is not. The authors add that "The development of a methodology to automatically discover sub-clusters... is a promising avenue that we leave for future research." The current study relied on manual grouping (e.g., "mammals", "vehicles") to demonstrate that the linear representation hypothesis holds for specific semantic classes but not the abstract relation.

### Open Question 3
Do LLMs distinguish between different taxonomies of meronymy, such as component/integral objects versus member/collection? The paper notes in Section 2 that transitivity depends on the specific type of meronymy involved. While the authors explicitly mention testing "further specifications" in the future, it is currently unclear if the "quasi-semantic" competence observed stems from a failure to distinguish these subtypes (e.g., a slice of cake vs. a wheel of a car).

## Limitations
- Findings primarily based on analysis of LLaMA2-7b and GPT-4, limiting generalizability to other model architectures
- Representational analysis uses static embeddings from input/output layers, potentially missing dynamic representations in intermediate layers
- Study focuses on English language data, limiting conclusions about cross-linguistic semantic competence

## Confidence

- **High Confidence:** Behavioral findings showing LLMs struggle with antisymmetric properties of meronymy
- **Medium Confidence:** Conclusion that part-whole relations are fragmented rather than represented as a single abstract concept
- **Medium Confidence:** Probabilistic analysis showing gap between plausible and implausible meronymic statements

## Next Checks

1. **Cross-model validation:** Test the same experimental paradigm on other model families (Mistral, Claude, etc.) to assess generalizability of quasi-semantic competence findings
2. **Intermediate layer analysis:** Extend representational analysis to examine hidden states in intermediate transformer layers to determine if part-whole relations become more coherent in later layers
3. **Novel pair evaluation:** Test models on completely out-of-distribution meronym pairs (e.g., technical or domain-specific terms) to distinguish between memorization of training data and genuine relational understanding