---
ver: rpa2
title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference
  Data
arxiv_id: '2510.26202'
source_url: https://arxiv.org/abs/2510.26202
tags:
- features
- feature
- preference
- preferences
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "What\u2019s In My Human Feedback? (WIMHF) introduces a method\
  \ to automatically discover interpretable features in human feedback datasets without\
  \ pre-specifying hypotheses."
---

# What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data

## Quick Facts
- arXiv ID: 2510.26202
- Source URL: https://arxiv.org/abs/2510.26202
- Authors: Rajiv Movva; Smitha Milli; Sewon Min; Emma Pierson
- Reference count: 40
- Primary result: Automatically discovers interpretable features in human feedback datasets that explain most preference prediction signal while maintaining interpretability

## Executive Summary
WIMHF introduces a method to automatically discover interpretable features in human feedback datasets without pre-specifying hypotheses. Using sparse autoencoders, WIMHF learns features that capture how pairs of responses differ, then identifies which features actually predict annotator preferences. Across seven datasets, WIMHF finds that a small set of interpretable features explains most of the preference prediction signal—achieving 84% of black-box embeddings' performance while maintaining interpretability. These features reveal diverse preferences across datasets, highlight potential safety issues (e.g., LMArena users preferring unsafe content over refusals), and enable practical applications like effective data curation (+37% safety gains) and personalized alignment on subjective features.

## Method Summary
WIMHF learns interpretable features from preference datasets by training a sparse autoencoder (SAE) on embedding differences between response pairs, then using logistic regression to identify which features predict human preferences. The method uses text embeddings (OpenAI text-embedding-3-small) to represent response pairs, computes their difference vectors, and trains a BatchTopK SAE with 32 features where 4 are active per input. For each learned feature, the system generates human-readable descriptions using an LLM and validates these through correlation analysis. The approach controls for confounders like response length and enables targeted data curation by identifying and correcting misaligned preferences.

## Key Results
- Achieves 84% of black-box embeddings' performance on preference prediction tasks
- Identifies interpretable features that capture diverse preferences across datasets
- Enables effective data curation with +37% safety gains when correcting misaligned preferences
- Finds that 6-8 features typically explain most of the preference prediction signal
- Reveals safety concerns like LMArena users preferring unsafe content over refusals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse decomposition of response-pair embedding differences isolates interpretable axes of variation that dense embeddings obscure
- **Mechanism:** Computes difference vector Δ = e_rA - e_rB from text embeddings, trains BatchTopK SAE to reconstruct Δ using constrained latent vector where only K features are active
- **Core assumption:** Semantic differences driving human choices can be linearly decoupled in embedding space and described by small number of active features
- **Evidence anchors:** Abstract states method learns features capturing how pairs differ; Section 3 describes BatchTopK training on text embedding differences
- **Break condition:** If embedding difference doesn't lie on sparse manifold, SAE fails to reconstruct signal or produces incoherent features

### Mechanism 2
- **Claim:** Logistic regression on sparse feature activations identifies expressed preferences by isolating specific features that predict annotator choices while controlling for confounders
- **Mechanism:** Fits logistic regression Pr(y=1) = σ(α + β_j z_j + γ·x) on SAE-mapped features, measures marginal effect of feature j on choice probability
- **Core assumption:** Relationship between feature presence and preference label is approximately linear and independent enough for standard regression
- **Evidence anchors:** Abstract mentions identifying features that predict preferences; Section 3 describes logistic regression with length controls
- **Break condition:** If preferences are highly non-linear or features are strongly collinear, regression coefficients misattribute causal driver

### Mechanism 3
- **Claim:** Targeted data curation via label inversion on specific sparse features improves downstream model alignment without degrading general utility
- **Mechanism:** Identifies "misaligned" features (e.g., preference for unsafe content), filters high-activation examples, flips preference labels to reinforce safe behavior
- **Core assumption:** Identified feature is primary driver of undesired behavior, correcting this subset generalizes to broader safety domain
- **Evidence anchors:** Abstract mentions relabeling harmful examples yields large safety gains; Section 5.1 describes flipping labels for unsafe feature examples
- **Break condition:** If "unsafe" feature entangled with "helpful" feature, flipping labels might train model to be unhelpful in safe contexts

## Foundational Learning

- **Concept:** **Sparse Autoencoders (SAEs)**
  - **Why needed here:** Cannot interpret 1536-dimensional embedding difference directly; SAEs project dense vector into sparse, human-readable feature basis
  - **Quick check question:** Can you explain why an L1 penalty or TopK activation function helps create interpretable features in a neural network?

- **Concept:** **Embedding Arithmetic (Δ-vectors)**
  - **Why needed here:** WIMHF embeds difference between pairs, not single responses; understanding vector arithmetic is critical to grasp why this highlights comparative features
  - **Quick check question:** If Vector_A represents "King" and Vector_B "Man," what does Vector_A - Vector_B approximate in a well-structured space?

- **Concept:** **RLHF / Preference Finetuning**
  - **Why needed here:** Goal is to debug data used for Preference Finetuning; need to know what "reward model" is and why "label noise" hurts alignment
  - **Quick check question:** In RLHF, what is the role of preference dataset (D) relative to reward model (R)?

## Architecture Onboarding

- **Component map:** Embedder -> Delta Calculator -> SAE Encoder -> Auto-Interpreter -> Preference Regressor
- **Critical path:** Training of SAE Encoder; if reconstruction loss too high or features polysemantic, downstream interpretation and regression fail
- **Design tradeoffs:**
  - Sparsity (K) vs. Granularity (M): Uses (M=32, K=4); increasing M allows more concepts but risks redundancy; increasing K captures more nuance but reduces interpretability
  - Embedding vs. Full Context: Using only response embeddings works best/sufficiently, trading prompt context for computational efficiency
- **Failure signatures:**
  - Low Fidelity: LLM-generated description has p > 0.05 correlation with actual feature activations
  - Random-Level AUC: Regression on sparse features performs near 0.5 AUC
- **First 3 experiments:**
  1. Overfit Check: Train SAE on tiny batch to ensure perfect reconstruction (loss ≈ 0)
  2. Synthetic Validation: Create dataset differing only by known attribute, verify SAE discovers this feature as top predictor
  3. Hyperparameter Sweep: On small subset, sweep K ∈ {2, 4, 8} and M ∈ {16, 32, 64}, check which yields highest "Fidelity" score

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can WIMHF framework be extended to effectively incorporate prompt context when learning interpretable response features?
- **Basis in paper:** [explicit] Authors note using embeddings of full prompt-response transcript didn't improve prediction and state "we leave this observation... to future work"
- **Why unresolved:** Current method relies on response embedding differences (Δ = e_rA - e_rB), ignoring prompt p even though prompt defines context for preference
- **What evidence would resolve it:** Modification integrating prompt embeddings demonstrating improved fidelity or preference prediction accuracy over baseline

### Open Question 2
- **Question:** Do features identified by WIMHF causally determine human preferences, or are they primarily correlational markers?
- **Basis in paper:** [explicit] Paper states in footnote "Note that we cannot be sure if these features causally affect human preference. Rather, we are describing response features that correlate with annotator choices."
- **Why unresolved:** WIMHF identifies associations (features that predict y), but doesn't perform causal interventions to prove changing feature causes change in human selection
- **What evidence would resolve it:** Intervention study where specific features are synthetically added/removed from responses to observe if win-rate shifts according to WIMHF predictions

### Open Question 3
- **Question:** How do specific mixtures of datasets with conflicting expressed preferences quantitatively impact final behavior of models during preference finetuning?
- **Basis in paper:** [inferred] Paper observes datasets encode conflicting preferences (e.g., jokes preferred on Reddit but dispreferred in HH-RLHF) and notes mixing them "may wash out or influence an LLM in unexpected ways"
- **Why unresolved:** While WIMHF identifies conflicts, paper doesn't measure extent to which conflicts confuse model or result in erratic policy behavior during training
- **What evidence would resolve it:** Controlled study comparing models trained on mixed datasets with known conflicting preferences versus harmonized datasets, evaluating policy stability and alignment consistency

## Limitations
- Method assumes linear separability of semantic differences in embedding space and that sparse autoencoders can reliably capture these axes of variation
- Relies heavily on LLM-generated annotations for both feature descriptions and validation, introducing potential subjectivity and compounding errors
- Experiments focus on English-language datasets and may not generalize to other languages or domains
- Some critical hyperparameters (SAE training details, exact annotation prompts) are underspecified, making exact reproduction challenging

## Confidence
- **High confidence:** Core method mechanics (SAE training on embedding differences, logistic regression for preference attribution) are sound and well-established
- **Medium confidence:** Performance claims relative to black-box embeddings (84% AUC retention) are based on controlled experiments but depend on specific datasets and embedding models
- **Medium confidence:** Interpretability claims rely on LLM-generated annotations, which introduce subjectivity
- **Low confidence:** Generalizability to non-English datasets, highly technical domains, or preference datasets with different structures

## Next Checks
1. **Cross-dataset transferability test:** Train WIMHF features on one dataset type (e.g., LMArena) and evaluate preference prediction and interpretability on structurally different dataset (e.g., PRISM or PKU-SafeRLHF) to assess generalizability
2. **Manual annotation validation:** Have human annotators independently rate feature descriptions and preference attributions on subset of pairs to verify LLM annotation quality and fidelity scores
3. **Ablation on SAE architecture:** Systematically vary M and K parameters across broader range and test on synthetic datasets with known attribute differences to determine optimal sparsity vs. granularity trade-offs for different data characteristics