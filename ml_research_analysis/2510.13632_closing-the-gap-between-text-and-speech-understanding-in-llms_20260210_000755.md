---
ver: rpa2
title: Closing the Gap Between Text and Speech Understanding in LLMs
arxiv_id: '2510.13632'
source_url: https://arxiv.org/abs/2510.13632
tags:
- speech
- text
- training
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the text-speech understanding gap in speech-adapted
  large language models, identifying forgetting of text capabilities and cross-modal
  misalignment as the primary causes. The authors propose SALAD (Sample-efficient
  Alignment with Learning through Active selection and cross-modal Distillation),
  which combines cross-modal distillation with active data selection to improve alignment
  while mitigating forgetting.
---

# Closing the Gap Between Text and Speech Understanding in LLMs

## Quick Facts
- arXiv ID: 2510.13632
- Source URL: https://arxiv.org/abs/2510.13632
- Reference count: 30
- Primary result: SALAD achieves competitive performance with strong open-weight baselines across knowledge, language understanding, and reasoning benchmarks while training on over an order of magnitude less speech data from public corpora.

## Executive Summary
This paper analyzes the text-speech understanding gap in speech-adapted large language models, identifying forgetting of text capabilities and cross-modal misalignment as the primary causes. The authors propose SALAD (Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation), which combines cross-modal distillation with active data selection to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with strong open-weight baselines across knowledge, language understanding, and reasoning benchmarks while training on over an order of magnitude less speech data from public corpora.

## Method Summary
SALAD uses a two-stage approach: Stage I applies cross-modal distillation (α=1.0) on 24B tokens of natural speech plus SmolLM, constraining the speech-adapted model to match a frozen text teacher's output distribution. Stage II uses active selection to identify high-misalignment clusters in broad text corpora and synthesizes speech for only those clusters, expanding domain coverage with minimal synthetic data. The architecture uses a frozen Mimi encoder + 122M adapter + Qwen2.5 base model, with interleaving of 10-30 words text per 5-15 words speech.

## Key Results
- SALAD achieves competitive performance with strong open-weight baselines across knowledge, language understanding, and reasoning benchmarks
- Maintains closest performance to original text capabilities compared to other speech-adapted models
- Trains on over an order of magnitude less speech data from public corpora than comparable methods

## Why This Works (Mechanism)

### Mechanism 1: Cross-modal misalignment
When the model produces divergent output distributions for semantically equivalent speech vs. text inputs (quantified via KL divergence), it fails to transfer text-trained knowledge to the speech modality. Misalignment uniquely explains ~56% of speech performance variance after controlling for forgetting and training budget.

### Mechanism 2: Forgetting of text capabilities
Domain shift from text pretraining to narrow speech data causes the model to overwrite text-learned representations. Forgetting uniquely explains ~32% of text performance variance given misalignment.

### Mechanism 3: Cross-modal distillation with active data selection
Distillation constrains the speech-adapted model to match the text teacher's output distribution, reducing misalignment. Active selection identifies high-misalignment clusters in broad text corpora and synthesizes speech for only those, expanding domain coverage with minimal synthetic data.

## Foundational Learning

- **KL Divergence for Distribution Matching**: KL divergence is used to formalize both misalignment and forgetting as divergences between output distributions. Quick check: If P and Q are two distributions, does KL(Q||P) equal KL(P||Q)? Why might this asymmetry matter for distillation?

- **Domain Shift and Forgetting in Transfer Learning**: Forgetting is framed as a domain-shift problem (text pretraining → narrow speech fine-tuning). Quick check: If you fine-tune a text LLM on a small, narrow speech dataset, what would you expect to happen to its performance on broad text benchmarks? Why?

- **Active Learning / Importance Sampling**: SALAD's Stage II uses active selection (clustered importance sampling) to prioritize high-misalignment domains. Quick check: In active learning, why might focusing too heavily on high-uncertainty or high-error samples hurt generalization?

## Architecture Onboarding

- **Component map**: Waveform -> Mimi encoder (frozen) -> speech codes -> adapter (trainable) -> speech embeddings -> LLM backbone (trainable) -> next-token distributions

- **Critical path**: Waveform → Mimi encoder → speech codes → sum across codebooks → adapter → speech embeddings → interleave with text embeddings → LLM backbone → predict next text token → compute distillation loss vs. frozen text teacher

- **Design tradeoffs**: 
  - Causal vs. non-causal encoder: Mimi is causal/streaming but produces less text-aligned representations than large non-causal encoders
  - Adapter size: Performance saturates at ~122M parameters
  - Synthetic vs. natural speech: Synthetic data expands domain coverage but lacks paralinguistic richness

- **Failure signatures**:
  - NLL-only training on narrow speech data: Misalignment grows with scale
  - Large γ (over-selective active learning): Gains only at very small budgets
  - No text mixing during training: Increased forgetting

- **First 3 experiments**:
  1. Baseline misalignment/forgetting measurement: Train with α=0 on LibriHeavy+Emilia for 2-4B tokens, measure misalignment and forgetting
  2. Distillation scaling (α sweep): Train with α ∈ {0.25, 0.5, 0.75, 1.0}, plot misalignment vs. tokens
  3. Active selection ablation: After Stage I, run Stage II with uniform vs. active selection using 1% synthetic budget

## Open Questions the Paper Calls Out

- **Open Question 1**: Can SALAD be extended to speech generation tasks while maintaining sample efficiency and alignment quality? The current method only addresses speech-to-text understanding; speech generation requires different alignment objectives.

- **Open Question 2**: Would combining SALAD with representation-level alignment techniques (e.g., text-aligned speech embeddings) yield complementary gains? Current experiments isolate training objective contribution; potential synergies with encoder-side alignment remain unexplored.

- **Open Question 3**: How does SALAD perform when adapted to instruction-tuned models rather than base LLMs? Practical voice assistants require instruction-following capabilities; it's unclear if distillation objectives preserve instruction-following behavior while learning speech alignment.

## Limitations
- Analysis of forgetting and misalignment is correlative rather than causal - no experimental isolation through ablation or intervention studies
- Active selection strategy relies on clustering proxy without validation that identified clusters correspond to missing domains
- Synthetic speech data strategy introduces potential confounding - gains may come from domain expansion or specific characteristics of synthetic speech

## Confidence

**High Confidence**: Cross-modal distillation reduces misalignment is well-supported by controlled experiments across multiple α values and datasets.

**Medium Confidence**: Forgetting mechanism and its quantification through KL divergence correlates with text performance drops but lacks experimental validation.

**Low Confidence**: Active selection strategy's generalizability is uncertain - limited evidence it works for arbitrary task distributions beyond reported benchmarks.

## Next Checks
1. **Intervention Study for Forgetting**: Systematically vary text data mixing during speech adaptation (0%, 25%, 50%, 75%, 100%) and measure both text performance and forgetting metric to establish causality.

2. **Active Selection Domain Validation**: For a new task domain, manually annotate whether clusters selected by active strategy actually correspond to relevant domain content and compare with random cluster selection.

3. **Synthetic Data Scaling Analysis**: Systematically vary synthetic speech budget (0.1%, 1%, 10%, 50%) while holding other factors constant to measure marginal gains and quality differences.