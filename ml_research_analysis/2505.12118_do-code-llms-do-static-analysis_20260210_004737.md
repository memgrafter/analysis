---
ver: rpa2
title: Do Code LLMs Do Static Analysis?
arxiv_id: '2505.12118'
source_url: https://arxiv.org/abs/2505.12118
tags:
- code
- llms
- static
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether code LLMs perform static analysis
  tasks during code intelligence tasks. We conducted experiments with three static
  analysis tasks (callgraph, AST, and dataflow generation) and three code intelligence
  tasks (code summarization, translation, and generation) using two open-source (CodeLlaMA,
  jam) and two closed-source models (GPT-4o, Gemini).
---

# Do Code LLMs Do Static Analysis?

## Quick Facts
- arXiv ID: 2505.12118
- Source URL: https://arxiv.org/abs/2505.12118
- Reference count: 40
- Code LLMs perform poorly on static analysis tasks; finetuning on static analysis does not improve code intelligence performance

## Executive Summary
This study investigates whether code LLMs perform static analysis when completing code intelligence tasks. Through experiments with three static analysis tasks (callgraph, AST, and dataflow generation) and three code intelligence tasks (code summarization, translation, and generation), the authors found that LLMs perform poorly on static analysis tasks. While in-context learning improved AST generation, finetuning on static analysis tasks did not significantly improve code intelligence task performance. These results suggest that LLMs use different reasoning processes than human programmers for these tasks, likely relying on surface-level pattern matching rather than explicit structural analysis.

## Method Summary
The study evaluated two open-source models (JAM with 350M parameters, Java-only; CodeLlaMA with 13B parameters, multi-language) and two closed-source models (GPT-4o, Gemini) on six tasks across Java and C/C++. Static analysis tasks included AST generation (srcML format), callgraph generation (Doxygen), and dataflow graph generation (Joern). Code intelligence tasks covered summarization, translation, and generation. The researchers used full finetuning for JAM and QLoRA for CodeLlaMA, then tested sequential finetuning (static analysis → code intelligence). Performance was measured using BLEU, METEOR, USE for code tasks, and Levenshtein, Jaccard similarity, pair accuracy, and chain accuracy for static analysis.

## Key Results
- LLMs perform poorly on static analysis tasks with only AST generation showing improvement through in-context learning
- Finetuning on static analysis tasks improves task-specific performance but does not generalize to improved code intelligence performance
- Java models outperform C/C++ models across all tasks, likely due to training data volume differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code LLMs achieve high performance on code intelligence tasks through surface-level pattern matching rather than structural reasoning analogous to human programmers.
- Mechanism: Models extract and recombine semantic cues (e.g., technical jargon, function signatures, common API patterns) from training data, bypassing the explicit graph-based analysis (call graphs, dataflow, AST traversal) that humans use for comprehension. Evidence: Code summarization outputs copy class names like "CoordinateAxis" directly from source rather than synthesizing behavior descriptions; models generate identical logical errors in code generation before and after static analysis finetuning.
- Core assumption: Human-like program comprehension requires explicit static analysis; pattern matching without it indicates a fundamentally different reasoning process.
- Evidence anchors:
  - [abstract]: "LLMs show poor performance on static analysis tasks and that pretraining on the static analysis tasks does not generalize to better performance on the code intelligence tasks."
  - [Section 4.2]: In-context learning improves AST generation but not callgraph/dataflow, suggesting models learn tag patterns rather than structural reasoning.
  - [corpus]: Related work on multilingual code models shows similar transfer limitations across languages with different syntax structures.
- Break condition: If models demonstrated improved chain accuracy on callgraph/dataflow tasks after finetuning, or if static analysis pretraining consistently improved code intelligence metrics with statistical significance.

### Mechanism 2
- Claim: LLM performance on code tasks scales with training data abundance for specific languages rather than generalizable structural understanding.
- Mechanism: Models perform better on Java than C/C++ not due to superior reasoning, but because Java's popularity yields more pretraining data. This creates language-specific surface patterns rather than transferable programming knowledge.
- Core assumption: Training data volume drives performance; structural understanding should transfer across languages with similar computational models.
- Evidence anchors:
  - [Section 4.1]: "we observed the higher performance in Java and low performance in C/C++. The rationale... may be because of higher amount of training data as a result of the popularity of the programming language."
  - [Section 4.3]: C/C++ shows lower Levenshtein, Jaccard, and chain accuracy across all static analysis tasks despite identical experimental procedures.
  - [corpus]: Multi-language security evaluations show similar language-dependent performance gaps.
- Break condition: If models showed comparable performance across languages after controlling for training data size, or if C/C++ performance improved with equivalent data volume.

### Mechanism 3
- Claim: Static analysis capabilities do not transfer to improved code intelligence task performance even after explicit finetuning.
- Mechanism: Models learn task-specific output patterns (e.g., AST XML tag structures, partial callgraph edges) without acquiring generalizable analysis skills. The p-values in RQ4 show no consistent statistical improvement—finetuning creates localized pattern recognition rather than compositional understanding.
- Core assumption: If LLMs reasoned like humans, improved static analysis should improve downstream coding tasks through shared mental models.
- Evidence anchors:
  - [abstract]: "finetuning open-source models on static analysis tasks improves their performance on simple static analysis tasks but does not generalize to improved performance on more complex static analysis tasks or code intelligence tasks."
  - [Section 4.4]: "we do not strongly find the statistical difference between the models pretrained with static analysis and the baseline model" across all tasks.
  - [corpus]: Adapter-based multilingual learning work shows similar task-specific optimization without cross-task transfer.
- Break condition: If sequential finetuning (static analysis → code intelligence) showed consistent, statistically significant improvements across multiple tasks and languages.

## Foundational Learning

- Concept: **Static Analysis vs. Code Intelligence Tasks**
  - Why needed here: The paper's central distinction—static analysis requires explicit structural reasoning, while code intelligence can be achieved through pattern matching. Misunderstanding this leads to false assumptions about LLM capabilities.
  - Quick check question: Can you explain why AST generation improved with finetuning while dataflow generation did not, and what this implies about what the model actually learned?

- Concept: **Evaluation Metrics for Structural Correctness**
  - Why needed here: The paper uses Levenshtein (AST), Jaccard similarity (callgraph/dataflow), pair accuracy, and chain accuracy. Understanding why chain accuracy remained low while pair accuracy improved is critical for interpreting results.
  - Quick check question: Why does high pair accuracy with low chain accuracy indicate partial/surface-level learning rather than complete structural understanding?

- Concept: **Transfer Learning Hypothesis**
  - Why needed here: The experimental design tests whether static analysis training transfers to code intelligence. This assumption underlies the conclusion about alien reasoning processes.
  - Quick check question: What would the results need to show to support the hypothesis that LLMs use human-like static analysis during coding tasks?

## Architecture Onboarding

- Component map: JAM (350M params, Java-only) -> CodeLlaMA (13B params, multi-language) -> GPT-4o mini, Gemini (closed-source)
- Critical path: 1) Evaluate baseline model performance on all six tasks (RQ1, RQ2) 2) Finetune on static analysis tasks and re-evaluate (RQ3) 3) Sequentially finetune: static analysis → code intelligence (RQ4) 4) Compare metrics with statistical significance testing
- Design tradeoffs: Open-source models allow controlled finetuning but lack scale; closed-source models represent SOTA but introduce data contamination risk; JAM's 1,024 context length limited callgraph experiments
- Failure signatures: AST generation models learn XML tags but make simple syntactic errors; callgraph/dataflow show high Jaccard/pair accuracy but near-zero chain accuracy; identical logical errors in code generation before/after static analysis finetuning
- First 3 experiments:
  1. Baseline capability assessment: Run all four models on all six tasks without any finetuning to establish performance bounds
  2. Static analysis finetuning with controlled data: Finetune JAM and CodeLlaMA on equal-sized AST, callgraph, and dataflow datasets for both languages
  3. Sequential transfer test with held-out tasks: Finetune on two static analysis tasks, evaluate on the third (unseen) and all code intelligence tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative reasoning mechanisms do Code LLMs employ for code intelligence tasks if not static analysis?
- Basis in paper: [Explicit] The Conclusion states LLMs "have different thought process compared with human programmers" and suggests they might use "semantics information instead of syntactic information" or simple pattern matching.
- Why unresolved: The study successfully demonstrates that LLMs fail to use static analysis but does not positively identify the specific internal representations or heuristics actually driving their success in code generation.
- What evidence would resolve it: Causality tracing or probing studies that identify internal model states correlating with semantic logic or keyword association during generation tasks.

### Open Question 2
- Question: Can scaling the volume of training data for complex static analysis tasks overcome the generalization gap observed in finetuning?
- Basis in paper: [Explicit] The authors note in the RQ2 discussion that "more samples would help LLMs to learn more useful information for these two tasks" as a possible explanation for poor performance on data flow and call graph generation.
- Why unresolved: The study used limited datasets (e.g., 15k-25k methods) for finetuning; it is unknown if massive-scale pre-training on static analysis would yield the same saturation results.
- What evidence would resolve it: Experiments pre-training models on orders of magnitude more static analysis samples to observe if chain accuracy improves or saturates.

### Open Question 3
- Question: Does extending the context window length enable LLMs to capture the long-range dependencies required for accurate call graph and data flow analysis?
- Basis in paper: [Explicit] The RQ3 results section notes that LLMs "struggle with looking at the longer context" and that hardware limits restricted prediction to 10k tokens, which may have truncated necessary file-level context.
- Why unresolved: The observed failure in call graph generation might be an artifact of the restricted input size rather than a fundamental lack of reasoning capability.
- What evidence would resolve it: Evaluation of state-of-the-art models with significantly larger context windows (e.g., 128k+ tokens) on full-file or project-level static analysis tasks.

## Limitations
- Potential data contamination risks with closed-source models may affect the validity of baseline comparisons
- The controlled nature of static analysis datasets may not reflect real-world complexity and long-range dependencies
- Evaluation focuses on structural correctness metrics without examining whether imperfect static analysis could still provide useful hints for downstream tasks

## Confidence
- High Confidence: LLMs show poor performance on static analysis tasks compared to code intelligence tasks; finetuning on static analysis improves task-specific performance but not transfer to code intelligence
- Medium Confidence: The language performance gap (Java > C/C++) is primarily driven by training data volume rather than reasoning capability differences
- Medium Confidence: The lack of transfer suggests LLMs use different reasoning processes than human programmers, though alternative explanations exist

## Next Checks
1. Ablation study on context length: Systematically vary input token limits for static analysis tasks to determine if the low chain accuracy is due to truncation versus fundamental reasoning limitations
2. Cross-task generalization test: Train on static analysis for one language (e.g., Java) and evaluate on both code intelligence and static analysis tasks for a different language (e.g., C/C++) to isolate data-specific versus structural learning
3. Chain-of-thought prompting experiment: Apply structured reasoning prompts during static analysis tasks to test whether explicit reasoning scaffolding improves performance, distinguishing between capability gaps and prompting deficiencies