---
ver: rpa2
title: 'ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric
  PPO for LLM Agents'
arxiv_id: '2602.01869'
source_url: https://arxiv.org/abs/2602.01869
tags:
- skill
- memory
- procedural
- procmem
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProcMEM introduces a framework enabling LLM agents to autonomously
  learn procedural memory from interaction experiences without parameter updates.
  It addresses the problem of insufficient experience reuse in LLM agents, which leads
  to computational redundancy and execution instability.
---

# ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents

## Quick Facts
- arXiv ID: 2602.01869
- Source URL: https://arxiv.org/abs/2602.01869
- Reference count: 40
- Enables LLM agents to learn procedural memory without parameter updates, achieving 0.925 in-domain reuse with only 816 tokens

## Executive Summary
ProcMEM introduces a framework for LLM agents to autonomously learn and reuse procedural memory from interaction experiences, addressing the problem of insufficient experience reuse that causes computational redundancy and execution instability. The framework formalizes procedural units as executable Skills with activation, execution, and termination conditions, evolving them through Non-Parametric PPO that uses semantic gradients for candidate generation and a PPO Gate for verification. Experimental results demonstrate superior reuse rates and significant performance gains across in-domain, cross-task, and cross-agent scenarios with extreme memory compression.

## Method Summary
The framework addresses LLM agents' inability to autonomously learn from interaction experiences by introducing procedural memory through Skills defined by activation, execution, and termination conditions. Skills evolve via Non-Parametric PPO, which generates candidates using semantic gradients and verifies them through a PPO Gate. This enables agents to reuse learned procedures across different tasks and agents while maintaining only 816 tokens of compressed memory versus hundreds of thousands for baselines.

## Key Results
- Achieves 0.925 in-domain Skill reuse rate
- Demonstrates extreme memory compression (816 tokens vs. hundreds of thousands for baselines)
- Shows significant performance gains across in-domain, cross-task, and cross-agent scenarios

## Why This Works (Mechanism)
The framework succeeds by enabling autonomous skill evolution without parameter updates, allowing LLM agents to learn from experience through semantic gradient-based candidate generation and robust PPO Gate verification. This creates a transparent skill evolution system that compresses experience into reusable procedural units, enabling long-term autonomy through efficient memory reuse across diverse scenarios.

## Foundational Learning
- Procedural Memory: Why needed - enables experience reuse; Quick check - verify skill activation conditions work across tasks
- Non-Parametric PPO: Why needed - allows learning without parameter updates; Quick check - confirm candidate generation via semantic gradients
- PPO Gate: Why needed - ensures robust verification of evolved skills; Quick check - validate skill termination conditions

## Architecture Onboarding
Component map: Experience -> Skill Generation -> PPO Gate -> Procedural Memory -> Task Execution
Critical path: Experience collection triggers skill evolution, PPO Gate verifies candidates, successful skills are stored in compressed procedural memory
Design tradeoffs: Memory compression vs. skill granularity, autonomous learning vs. verification overhead
Failure signatures: Low reuse rates indicate poor skill evolution, high memory usage suggests ineffective compression
First experiments: 1) In-domain task performance, 2) Cross-task skill transfer validation, 3) Cross-agent memory sharing verification

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance gains primarily validated on synthetic/semi-synthetic benchmarks, unclear if they persist in complex real-world environments
- Framework's handling of ambiguous or noisy real-world experience data not fully explored
- Long-term skill retention and robustness to conflicting experiences not quantified

## Confidence
- Autonomous skill evolution claims: Medium
- Extreme memory compression benefits: Medium
- Cross-agent transfer effectiveness: Medium

## Next Checks
1. Test ProcMEM on diverse multi-step real-world tasks to assess generalization beyond synthetic benchmarks
2. Conduct ablation studies isolating PPO Gate and semantic gradient contributions to performance
3. Evaluate framework stability and skill retention after prolonged interaction with dynamic, noisy environments