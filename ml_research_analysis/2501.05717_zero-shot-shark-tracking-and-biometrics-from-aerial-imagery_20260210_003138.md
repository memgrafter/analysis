---
ver: rpa2
title: Zero-shot Shark Tracking and Biometrics from Aerial Imagery
arxiv_id: '2501.05717'
source_url: https://arxiv.org/abs/2501.05717
tags:
- shark
- flair
- video
- videos
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FLAIR, a zero-shot approach for tracking
  sharks and extracting biometrics from aerial drone imagery using Segment Anything
  Model 2 and Contrastive Language-Image Pretraining. FLAIR processes drone videos
  to segment shark species of interest and extract measurements like length and tailbeat
  frequency without requiring labeled training data.
---

# Zero-shot Shark Tracking and Biometrics from Aerial Imagery

## Quick Facts
- **arXiv ID:** 2501.05717
- **Source URL:** https://arxiv.org/abs/2501.05717
- **Reference count:** 21
- **One-line primary result:** Zero-shot shark tracking and biometrics from drone footage using SAM 2 and CLIP, achieving Dice score 0.81 on Pacific nurse sharks without training data.

## Executive Summary
This study introduces FLAIR, a zero-shot approach for tracking sharks and extracting biometrics from aerial drone imagery using Segment Anything Model 2 and Contrastive Language-Image Pretraining. FLAIR processes drone videos to segment shark species of interest and extract measurements like length and tailbeat frequency without requiring labeled training data. The method achieved a Dice score of 0.81 on Pacific nurse shark segmentation and generalized well to other shark species. It outperformed state-of-the-art object detection models and matched human-in-the-loop approaches while requiring no human annotation effort, significantly reducing the workload for marine ecology studies.

## Method Summary
FLAIR uses a zero-shot pipeline combining SAM 2 for segmentation and CLIP for classification to track sharks and extract biometrics from aerial drone videos. The approach samples frames, generates segmentation masks, filters detections using CLIP's zero-shot classification against shark prompts, propagates masks through videos using SAM 2's tracking, and validates tracks through cross-time alignment. Biometrics are extracted by skeletonizing masks for length and analyzing tail deviation for tailbeat frequency. The entire pipeline requires no training or fine-tuning, making it immediately applicable to new datasets and species.

## Key Results
- Achieved Dice score of 0.81 on Pacific nurse shark segmentation
- Successfully generalized to white and blacktip reef sharks without retraining
- Outperformed YOLOv8 and Segmenter models on the same shark tracking task
- Matched human-in-the-loop approach performance while requiring no annotation effort

## Why This Works (Mechanism)
The method leverages pre-trained vision-language models to bypass the need for species-specific training data. CLIP's zero-shot classification enables reliable shark detection across species by using natural language prompts, while SAM 2's strong segmentation capabilities handle the spatial localization. The tracking is achieved through temporal propagation rather than explicit motion modeling, and cross-time validation ensures track reliability without complex tracking algorithms.

## Foundational Learning
- **SAM 2 segmentation:** Why needed for accurate shark isolation from water background; Quick check: Verify SAM 2 produces clean masks separating shark from seafloor
- **CLIP zero-shot classification:** Why needed to identify shark segments without training; Quick check: Test CLIP confidence scores on shark vs non-shark examples
- **Biometric extraction from masks:** Why needed to convert segmentation to scientific measurements; Quick check: Compare skeletonized length to known shark dimensions
- **Temporal alignment validation:** Why needed to filter false positives across frames; Quick check: Verify IoU > 0.7 criterion removes spurious tracks
- **Contrastive Language-Image Pretraining:** Why needed for zero-shot detection without training data; Quick check: Test CLIP on various shark species prompts
- **Video frame sampling:** Why needed to reduce computational load while maintaining temporal coverage; Quick check: Ensure 1-second interval captures shark movement

## Architecture Onboarding

**Component Map:** Frame Sampling -> SAM 2 Auto Mask -> CLIP Classification -> SAM 2 Video Tracking -> Cross-track Alignment -> Biometric Extraction

**Critical Path:** CLIP Classification (0.95 threshold) -> SAM 2 Video Tracking -> Cross-track Alignment (IoU > 0.7)

**Design Tradeoffs:** Zero-shot approach trades potential accuracy for immediate applicability across species and environments, avoiding the data collection and annotation burden of traditional supervised learning.

**Failure Signatures:**
- Shadow segmentation appears as inflated mask sizes in shallow water
- Track loss occurs in frames with high turbidity or glare
- False positives appear when CLIP threshold is too low (e.g., 0.90)
- Biometrics become unreliable when drone metadata is unavailable

**Three First Experiments:**
1. Run FLAIR pipeline on a single clear-water shark video to verify end-to-end functionality
2. Test CLIP classification with different shark species prompts to find optimal confidence threshold
3. Compare SAM 2 segmentation quality with and without shadow filtering in shallow water conditions

## Open Questions the Paper Calls Out
- Can FLAIR be refined to distinguish between a shark and its shadow in extremely shallow water to prevent segmentation inflation?
- How can biometric accuracy be preserved when precise drone telemetry (altitude, sensor data) is unavailable?
- Does FLAIR generalize to non-elasmobranch marine species or complex terrestrial environments without prompt engineering?

## Limitations
- Performance degrades in turbid water or high-glare conditions due to shadow segmentation and track loss
- Requires specific drone metadata (altitude, sensor specs) for accurate biometric conversion
- CLIP prompts were fine-tuned for sharks, limiting immediate applicability to other species

## Confidence
- **High confidence:** The zero-shot segmentation approach using SAM 2 + CLIP works as described for clear-water conditions
- **Medium confidence:** The method generalizes across different shark species and water conditions  
- **Low confidence:** Performance in turbid water, variable lighting, or with multiple overlapping sharks

## Next Checks
1. Apply FLAIR to drone footage from turbid or high-glare conditions to quantify the shadow segmentation and turbidity failure modes mentioned in the reproduction notes.
2. Test the method with videos containing multiple shark species swimming together to assess whether CLIP's binary classification (shark vs. not-shark) can maintain species-specific tracking.
3. Track the same individual shark across multiple videos over extended periods to validate whether the biometric measurements (length, tailbeat frequency) remain consistent, which would strengthen the ecological validity of the approach.