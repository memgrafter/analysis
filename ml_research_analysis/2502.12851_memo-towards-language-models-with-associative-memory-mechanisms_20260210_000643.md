---
ver: rpa2
title: 'MeMo: Towards Language Models with Associative Memory Mechanisms'
arxiv_id: '2502.12851'
source_url: https://arxiv.org/abs/2502.12851
tags:
- vectors
- sequences
- memo
- language
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MeMo, a novel architecture for language modeling
  that explicitly memorizes sequences of tokens using layered associative memories.
  Unlike traditional transformer-based models that learn to capture sequential dependencies
  through training, MeMo directly stores associations between input sequences and
  output tokens using correlation matrix memories (CMMs) combined with multivariate
  Gaussian vectors and Johnson-Lindenstrauss transforms.
---

# MeMo: Towards Language Models with Associative Memory Mechanisms
## Quick Facts
- arXiv ID: 2502.12851
- Source URL: https://arxiv.org/abs/2502.12851
- Reference count: 15
- Primary result: MeMo achieves >96% memorization accuracy for 250,000 sequences using three-layer architecture with d=4096-8192

## Executive Summary
MeMo introduces a novel language modeling architecture that explicitly memorizes sequences of tokens using layered associative memory mechanisms, departing from traditional transformer approaches that learn sequential dependencies through training. The architecture employs correlation matrix memories (CMMs) combined with multivariate Gaussian vectors and Johnson-Lindenstrauss transforms to store associations between input sequences and output tokens. MeMo is designed to be transparent and editable, allowing for controlled memorization and forgetting of texts, with experimental results demonstrating its ability to store sequences linearly proportional to its number of parameters.

## Method Summary
The MeMo architecture replaces learned attention mechanisms with explicit memory storage through correlation matrix memories (CMMs) that directly store associations between input sequences and output tokens. Each memory layer operates on multivariate Gaussian vectors transformed via Johnson-Lindenstrauss dimensionality reduction, enabling efficient storage and retrieval of sequential patterns. The architecture's transparency allows for direct editing of stored memories, offering potential advantages in controlling factual knowledge storage and retrieval while reducing data requirements compared to traditional training-based approaches.

## Key Results
- Single-layer MeMo stores sequences linearly proportional to its number of parameters
- Three-layer MeMo with d=4096-8192 achieves >96% memorization accuracy for up to 250,000 sequences
- Architecture successfully handles decoy patterns that challenge simpler memory systems

## Why This Works (Mechanism)
MeMo works by explicitly storing token sequence associations in correlation matrix memories rather than learning these relationships through backpropagation. The CMM architecture directly maps input patterns to output tokens using stored correlation matrices, enabling precise control over what information is memorized and retrieved. This explicit memorization approach bypasses the need for learning sequential dependencies, instead focusing on efficient storage and retrieval of factual associations through structured memory layers.

## Foundational Learning
1. **Correlation Matrix Memories (CMMs)**: Neural network memory systems that store associations between input and output patterns using matrix operations. Needed for explicit pattern storage and retrieval. Quick check: Verify CMM can store and recall simple associations accurately.

2. **Johnson-Lindenstrauss Transform**: Dimensionality reduction technique that preserves distances between points in high-dimensional spaces. Needed for efficient memory representation. Quick check: Confirm distance preservation between original and transformed vectors.

3. **Multivariate Gaussian Vectors**: Statistical representations used for encoding input patterns in memory systems. Needed for robust pattern encoding. Quick check: Verify Gaussian distribution properties in encoded representations.

4. **Associative Memory**: Memory systems that store associations between inputs and outputs rather than individual data points. Needed for sequence-to-sequence mapping. Quick check: Test recall accuracy for stored associations.

## Architecture Onboarding
Component Map: Input Sequence -> Gaussian Encoder -> CMM Layer 1 -> CMM Layer 2 -> CMM Layer 3 -> Output Decoder

Critical Path: The architecture's critical path involves encoding input sequences into Gaussian vectors, storing these in CMM layers with dimensionality reduction via Johnson-Lindenstrauss transforms, and decoding the output through successive memory layers.

Design Tradeoffs: Explicit memorization versus learned representations; transparency and editability versus computational complexity; direct storage versus generalization capabilities.

Failure Signatures: Performance degradation with increasing sequence length beyond memory capacity; accuracy loss when decoy patterns overwhelm stored associations; computational bottlenecks in CMM matrix operations.

First Experiments:
1. Test single-layer MeMo with simple input-output associations to verify basic CMM functionality
2. Evaluate memory capacity scaling with increasing parameter counts and dimensions
3. Measure accuracy degradation when introducing controlled noise into stored patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of CMM operations (O(dÂ²) space for correlation matrices)
- Unclear scaling properties beyond tested parameter ranges
- Limited evaluation of generalization versus pure memorization

## Confidence
- Core technical contributions (CMMs with Gaussian vectors, multi-layer architecture): Medium
- Paradigm shift claim compared to transformer literature: Low
- Data requirement reduction claims: Low
- >96% accuracy for 250,000 sequences: Medium

## Next Checks
1. Benchmark MeMo against transformer models on factual recall tasks with controlled noise injection to test robustness
2. Evaluate the editability claims by measuring accuracy degradation when specific memories are modified or deleted
3. Conduct scaling experiments to determine parameter efficiency at sequence lengths beyond 250,000 and identify practical bottlenecks in memory operations