---
ver: rpa2
title: Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference
  Mismatch
arxiv_id: '2511.17826'
source_url: https://arxiv.org/abs/2511.17826
tags:
- inference
- across
- uni00000013
- different
- sizes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-deterministic inference
  in large language models caused by varying tensor parallel (TP) sizes. The core
  issue stems from the non-associativity of floating-point arithmetic and inconsistent
  reduction orders across GPUs, which leads to different outputs for identical inputs
  under different TP configurations.
---

# Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch

## Quick Facts
- arXiv ID: 2511.17826
- Source URL: https://arxiv.org/abs/2511.17826
- Authors: Ziyang Zhang; Xinheng Ding; Jiayi Yuan; Rixin Liu; Huizi Mao; Jiarong Xing; Zirui Liu
- Reference count: 22
- Key outcome: Introduces TBIK to achieve bit-wise identical LLM inference outputs across different tensor parallel sizes, eliminating training-inference mismatch in RL pipelines.

## Executive Summary
This paper addresses a critical numerical reproducibility issue in large language models (LLMs) caused by varying tensor parallel (TP) sizes during inference. The non-associativity of floating-point arithmetic combined with inconsistent reduction orders across GPUs leads to different outputs for identical inputs under different TP configurations. This creates a significant training-inference mismatch problem in reinforcement learning pipelines where models trained with one TP size produce divergent results when deployed with another. The authors propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee deterministic outputs regardless of parallel configuration.

## Method Summary
The authors propose Tree-Based Invariant Kernels (TBIK) to solve the non-determinism problem in tensor parallel inference. The core insight is that floating-point arithmetic is non-associative, and different GPU communication orders in tensor parallelism create inconsistent reduction sequences. TBIK enforces a unified hierarchical binary tree structure for both intra-GPU and inter-GPU reductions, ensuring consistent accumulation orders across all parallel configurations. This approach aligns reduction orders through a deterministic communication pattern, eliminating the bit-level differences that arise from varying tensor parallel sizes. The method specifically targets linear layers and reduction operations, which are the primary sources of non-determinism in LLM inference.

## Key Results
- Achieves zero probability divergence and bit-wise reproducibility across different TP sizes
- Eliminates training-inference mismatch in RL pipelines through deterministic inference
- Maintains performance parity with non-deterministic baselines while ensuring reproducibility

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental issue that floating-point addition is non-associative. When tensor parallelism varies, the order of partial result accumulation across GPUs changes, leading to different final values due to rounding errors. TBIK solves this by enforcing a consistent hierarchical binary tree reduction pattern that remains invariant regardless of the number of participating GPUs. By standardizing both intra-GPU and inter-GPU communication orders through this tree structure, the same sequence of floating-point operations is guaranteed, producing identical results across all configurations.

## Foundational Learning
- **Tensor Parallelism**: Distributing large tensor operations across multiple GPUs to handle model sizes exceeding single GPU memory; needed to scale LLMs, quick check: can you explain how matrix multiplication is split across GPUs?
- **Floating-Point Non-Associativity**: The property that (a + b) + c â‰  a + (b + c) due to rounding errors; needed because this causes numerical divergence, quick check: can you demonstrate this with simple floating-point examples?
- **Reduction Operations**: Aggregating partial results across parallel devices; needed as the source of non-determinism, quick check: can you describe different reduction patterns (tree vs ring)?
- **Binary Tree Reduction**: A hierarchical reduction pattern where partial results are combined in pairs; needed for consistent accumulation order, quick check: can you draw the communication pattern for 8 GPUs?
- **Deterministic Inference**: Ensuring identical outputs for identical inputs across different hardware configurations; needed for reliable RL training, quick check: can you explain why non-determinism breaks RL training?

## Architecture Onboarding
- **Component Map**: Input -> Linear Layer (TP-variant) -> Reduction (TP-variant) -> Output vs Input -> TBIK Linear + Reduction -> Output (deterministic)
- **Critical Path**: Matrix multiplication followed by reduction operations, where non-determinism is introduced through varying communication orders
- **Design Tradeoffs**: TBIK prioritizes reproducibility over potential minor performance gains from optimized but non-deterministic reduction patterns
- **Failure Signatures**: Non-identical outputs for identical inputs across different TP sizes, broken RL training pipelines due to inference divergence
- **First Experiments**: 1) Verify bit-wise identical outputs across 2, 4, 8, 16 GPU configurations with identical inputs, 2) Measure runtime overhead compared to standard TP implementations, 3) Test RLHF pipeline convergence with deterministic vs non-deterministic inference

## Open Questions the Paper Calls Out
None

## Limitations
- The approach is specific to linear layers and reductions, not covering nonlinearities or activation functions
- Potential overhead of reduction order alignment at extreme scales (>1024 GPUs) is not quantified
- Limited analysis to FP16/FP32 precision; behavior under BF16, FP8, or other low-precision formats is unexplored

## Confidence
- **Bit-wise reproducibility across TP sizes**: High confidence (rigorous proof sketches and ablation experiments)
- **Performance parity with baselines**: Medium confidence (runtime comparisons reported but not extensively benchmarked at scale)
- **Elimination of RL training-inference mismatch**: Medium confidence (demonstrated in single RLHF pipeline, needs broader validation)

## Next Checks
1. Test TBIK under BF16 and emerging low-precision formats to ensure numerical reproducibility across precisions
2. Benchmark TBIK at extreme scales (>1024 GPUs) to measure reduction order alignment overhead and confirm sustained performance parity
3. Extend TBIK to cover non-linear layers and activation functions to ensure full determinism across all model components