---
ver: rpa2
title: 'Towards Efficient Agents: A Co-Design of Inference Architecture and System'
arxiv_id: '2512.18337'
source_url: https://arxiv.org/abs/2512.18337
tags:
- agent
- context
- reasoning
- latency
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AgentInfer is a unified framework for accelerating autonomous
  agents by addressing end-to-end latency across reasoning, context, and execution
  stages. It integrates four synergistic components: AgentCollab uses hierarchical
  dual-model reasoning with self-evaluation-driven escalation; AgentCompress performs
  lightweight context summarization to manage memory growth; AgentSched implements
  cache-aware scheduling to optimize KV cache reuse; and AgentSAM employs suffix automaton-based
  speculative decoding to accelerate token generation using cross-session memory.'
---

# Towards Efficient Agents: A Co-Design of Inference Architecture and System

## Quick Facts
- **arXiv ID:** 2512.18337
- **Source URL:** https://arxiv.org/abs/2512.18337
- **Reference count:** 40
- **Primary result:** 1.8-2.5× end-to-end speedup while reducing ineffective tokens by over 50%

## Executive Summary
AgentInfer is a unified framework that accelerates autonomous agents by co-designing inference architecture and system scheduling to address end-to-end latency across reasoning, context, and execution stages. It integrates four synergistic components: AgentCollab uses hierarchical dual-model reasoning with self-evaluation-driven escalation; AgentCompress performs lightweight context summarization to manage memory growth; AgentSched implements cache-aware scheduling to optimize KV cache reuse; and AgentSAM employs suffix automaton-based speculative decoding to accelerate token generation using cross-session memory. Evaluated on BrowseComp-zh and DeepDiver benchmarks, AgentInfer achieves significant speedups while preserving accuracy.

## Method Summary
AgentInfer targets Deep Research Agents by co-designing the inference architecture and system scheduling. It uses openPangu-DeepDiverV2-7B as the small model and openPangu-DeepDiverV2-38B as the large model, running on vLLM-Ascend v0.9.1 with Ascend 910B3 NPUs. The framework implements AgentCollab for hierarchical dual-model reasoning with self-evaluation-driven escalation, AgentCompress for asynchronous context summarization, AgentSched for hybrid cache-aware scheduling using shadow price λ, and AgentSAM for cross-session suffix automaton-based speculative decoding. The system aims for 1.8-2.5× end-to-end speedup, >50% reduction in ineffective tokens, and preserved accuracy on BrowseComp-zh and DeepDiver benchmarks.

## Key Results
- 1.8-2.5× end-to-end speedup compared to baseline
- Over 50% reduction in ineffective token consumption
- 72% KV cache hit rate with AgentSched (vs 63% FCFS, 58% SJF)
- 33.8% accuracy with AgentCollab (vs 34.6% large-only, 18.3% small-only)

## Why This Works (Mechanism)

### Mechanism 1: Self-Evaluation-Driven Model Escalation
Routing reasoning steps between small and large models based on structured progress signals preserves accuracy while reducing compute cost. A Progress Check block after each Think step produces TRUE/FALSE judgment on whether meaningful progress occurred. FALSE triggers escalation to the large model; TRUE keeps execution on the small model. De-escalation occurs once the large model restores progress.

### Mechanism 2: Asymmetric Context Compression with Reasoning Memory Preservation
Compressing environment-interaction memory while preserving reasoning traces maintains cognitive continuity and avoids turn explosion. Two-phase compression includes synchronous search-result filtering and asynchronous distillation of tool-output memory. Reasoning memory (Think/Act steps) is retained; only environment memory is compressed.

### Mechanism 3: Shadow-Price Adaptive Scheduling for KV Cache Preservation
Dynamically blending SJF and KV-aware scheduling based on cache pressure improves both latency and hit rate. Shadow price λ computed as smooth function of demand-to-capacity ratio. When λ→0, scheduler behaves like SJF; when λ→high, scheduler penalizes new cache allocation and rewards cache-hit requests.

### Mechanism 4: Cross-Session Suffix Automaton for Speculative Decoding
Augmenting request-level SAM with semantically similar cross-session context improves speculative hit rate and token efficiency. Build composite suffix automaton from current session context and top-K retrieved historical contexts via dense/sparse similarity. Merge SAMs weighted by similarity scores.

## Foundational Learning

- **Think-Act-Observe Loop Latency Composition**: AgentInfer decomposes end-to-end latency across reasoning (Think), tool execution (Act), and context integration (Observe). Understanding where time accumulates is prerequisite for targeting optimizations. *Quick check: In a 20-step agent session averaging 2s inference and 500ms tool calls per step, what fraction of total latency is inference-bound?*

- **KV Cache Lifecycle in Multi-Turn Serving**: AgentSched's shadow-price mechanism assumes understanding of cache blocks, prefix reuse, and eviction costs. Without this, the SJF vs KV-aware tradeoff is opaque. *Quick check: When a long-context agent is evicted after 10 turns, what recomputation cost is incurred on resumption?*

- **Speculative Decoding Acceptance Rate Dynamics**: AgentSAM's effectiveness hinges on hit rate; knowing how acceptance rate translates to wall-clock speedup is essential for evaluating tradeoffs. *Quick check: If speculative decoding proposes 3 tokens with 60% acceptance, how many forward passes are saved per 100 generated tokens?*

## Architecture Onboarding

- **Component map:** User Query → AgentCollab: Planner/Executor routing → AgentCompress: Search filter + Async distillation → AgentSched: λ-computed priority queue → AgentSAM: Cross-session SAM + Speculative decode → Tool Execution → Loop

- **Critical path:**
  1. Initial planning (large model, KL steps)
  2. Per-turn Progress Check → escalation decision
  3. Search → filter → compress (synchronous)
  4. Scheduler selects request based on λ-weighted score
  5. SAM lookup → speculate → validate → emit tokens
  6. Async context distillation completes → KV precompute → context swap at round boundary

- **Design tradeoffs:**
  - AgentCollab: Accuracy vs cost. More escalation = higher accuracy, lower speedup.
  - AgentCompress: Compression ratio vs cognitive stability. Aggressive compression risks turn explosion if reasoning traces pruned.
  - AgentSched: Latency vs cache preservation. High λ protects long sessions but delays short requests.
  - AgentSAM: Speculation overhead vs hit rate. Large batches disable speculation; low-similarity history degrades returns.

- **Failure signatures:**
  - Agent stuck in loop: Progress Check always TRUE but no task completion → escalate threshold or add external verifier.
  - Turn explosion after compression: Reasoning traces accidentally pruned → verify composition includes reasoning memory.
  - Cache thrashing despite AgentSched: λ oscillates rapidly → smooth controller tuning or increase cache capacity.
  - Speculation slowdown: TTFT spikes → SAM construction not async; high batch → adaptive switch not triggering.

- **First 3 experiments:**
  1. Baseline profiling: Measure per-component latency breakdown (Think/Act/Observe) on BrowseComp-zh subset; identify dominant bottleneck.
  2. Ablation sweep: Disable each component individually; quantify marginal speedup and accuracy impact to validate claimed compositionality.
  3. Load test: Vary N_parallel (4, 8, 16) and observe λ dynamics, cache hit rate, and QPS scaling; verify AgentSched adaptation under pressure.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the AgentCollab framework when the active model generates incorrect self-evaluation signals, and what mechanisms can prevent runaway loops or unnecessary escalations? The paper assumes the model can accurately judge its own progress, but if the model hallucinates progress or gives up too easily, the efficiency/accuracy balance would theoretically break.

### Open Question 2
To what extent does the AgentCompress heuristic of retaining reasoning memory while compressing environment memory exacerbate the "Granularity Mismatch" problem for tasks requiring precise environmental recall? While Table 9 shows ablations, it does not evaluate tasks where success depends on retrieving specific low-level details from the environment memory that might be lost during asynchronous distillation.

### Open Question 3
What are the limits of the shadow-price controller (λ) in AgentSched under extreme workload variance, and can it be replaced by an adaptive learning policy? The controller relies on fixed parameters which may not generalize to highly volatile traffic patterns or different hardware constraints without manual retuning.

### Open Question 4
Does the retrieval and merging of cross-session suffix automatons in AgentSAM introduce latency overheads that negate its benefits for "long-tail" or novel queries? While Section 5.4.3 addresses the latency of constructing the SAM asynchronously, it does not explicitly quantify the overhead of the retrieval and merging process itself for queries that have few semantic matches in the global memory.

## Limitations
- Cross-session SAM retrieval mechanism lacks detail on similarity computation and retrieval quality thresholds
- Shadow price λ adaptation assumes workload mix stability; no validation under bursty or adversarial request patterns
- Asymmetric context compression preserves reasoning traces but introduces synchronization complexity

## Confidence
- **High confidence:** AgentCollab's dual-model escalation strategy and its measured 1.32× speedup with <2% accuracy loss
- **Medium confidence:** AgentSched's shadow-price scheduler—cache hit rate improvements are demonstrated, but adaptive λ dynamics under varying load are not fully characterized
- **Medium confidence:** AgentSAM's cross-session speculation—O(T) construction cost is stated but not benchmarked against pure request-level SAM; retrieval relevance is asserted but not measured

## Next Checks
1. Ablation study isolating AgentSAM's cross-session component: measure SHR and TTFT when disabling cross-session memory to quantify its marginal contribution
2. Stress test AgentSched under high-concurrency, short-request workloads to verify λ doesn't starve low-cache-footprint tasks
3. Long-running session fidelity check: run multi-hour browsing sessions to confirm reasoning traces remain sufficient for task completion after multiple compression cycles