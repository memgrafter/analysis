---
ver: rpa2
title: 'LEMUR: Learned Multi-Vector Retrieval'
arxiv_id: '2601.21853'
source_url: https://arxiv.org/abs/2601.21853
tags:
- retrieval
- lemur
- multi-vector
- similarity
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEMUR addresses the efficiency challenge in multi-vector retrieval,
  where high-quality token-level embeddings lead to computationally expensive MaxSim
  similarity computations. The method formulates multi-vector similarity search as
  a supervised learning problem and trains a small neural network to approximate MaxSim
  scores between queries and documents.
---

# LEMUR: Learned Multi-Vector Retrieval

## Quick Facts
- arXiv ID: 2601.21853
- Source URL: https://arxiv.org/abs/2601.21853
- Authors: Elias Jääsaari; Ville Hyvönen; Teemu Roos
- Reference count: 40
- Primary result: LEMUR achieves 5–11x speedup over baselines at 80% recall on ColBERTv2 embeddings

## Executive Summary
LEMUR addresses the efficiency challenge in multi-vector retrieval by formulating MaxSim similarity search as a supervised learning problem. The method trains a small neural network to approximate MaxSim scores between queries and documents, mapping token embeddings to contributions in the MaxSim sum. This enables reduction to single-vector similarity search in a learned latent space, achieving significant speedups while maintaining high recall. The approach is particularly effective for modern multi-vector text and visual document retrieval models.

## Method Summary
LEMUR trains a two-layer MLP with 2048-dimensional hidden layer to approximate MaxSim similarity scores between query-document pairs. The model learns a function that maps token embeddings to their contributions in the MaxSim sum, enabling efficient single-vector ANNS retrieval. Training uses a sampled subset of document embeddings, and the learned approximation allows retrieval without expensive token-level similarity computations. The method can be trained without access to actual query data, making it practical for real-world deployment.

## Key Results
- 5–11x speedup over baselines at 80% recall on ColBERTv2 embeddings
- Order of magnitude faster than state-of-the-art methods while maintaining high recall
- Consistently outperforms baselines across multiple modern multi-vector text and visual document retrieval models

## Why This Works (Mechanism)
LEMUR works by learning a function that approximates the MaxSim similarity score, which is the sum of maximum cosine similarities between query and document tokens. The neural network learns to map token embeddings to their contribution values in the MaxSim sum, effectively learning a scoring function that can be applied to individual token embeddings. This learned scoring function transforms the multi-vector comparison problem into a single-vector comparison problem, enabling the use of efficient single-vector ANNS methods for retrieval.

## Foundational Learning
- **MaxSim similarity computation**: Why needed - Understanding the expensive operation being approximated; Quick check - Verify understanding of how MaxSim differs from average or sum pooling
- **Neural network approximation**: Why needed - Core mechanism for learning the scoring function; Quick check - Confirm the MLP architecture can represent the non-linear relationships in token contributions
- **Single-vector ANNS**: Why needed - Enables the efficiency gains from the learned approximation; Quick check - Ensure familiarity with how approximate nearest neighbor search works
- **Embedding space properties**: Why needed - Understanding how token embeddings are distributed affects learning; Quick check - Consider how different tokenization strategies might impact performance
- **Sampled training data**: Why needed - Explains the practical approach to training without full dataset; Quick check - Verify understanding of potential bias from sampling strategies
- **Query-document similarity**: Why needed - Fundamental to retrieval task; Quick check - Confirm understanding of how query-document relationships are modeled

## Architecture Onboarding

**Component Map:**
Document Embeddings -> MLP Training -> Learned Scoring Function -> Single-Vector ANNS -> Retrieved Documents

**Critical Path:**
Token embeddings are fed through the trained MLP to generate contribution scores, which are then used to compute single-vector representations that can be indexed and searched efficiently using ANNS.

**Design Tradeoffs:**
The two-layer MLP architecture balances model complexity with training efficiency. While deeper networks might capture more complex relationships, they would require more training data and computation. The 2048-dimensional hidden layer provides sufficient capacity while remaining practical for real-time inference.

**Failure Signatures:**
- Poor approximation quality when token distributions differ significantly from training data
- Degradation in performance when applied to embedding spaces with different characteristics than those seen during training
- Sampling bias leading to suboptimal scoring functions for underrepresented document types

**Three First Experiments:**
1. Evaluate approximation accuracy by comparing learned MaxSim scores against exact MaxSim scores on held-out document pairs
2. Measure retrieval quality degradation when varying the sampling rate for training document embeddings
3. Test performance on different embedding dimensionalities to understand scalability limits

## Open Questions the Paper Calls Out
The paper highlights the need for further validation of LEMUR's performance across diverse embedding spaces and domains. It also notes the importance of understanding how sampling strategies for training documents affect approximation quality and retrieval performance. The effectiveness of training without query data access remains an area requiring more comprehensive study.

## Limitations
- Generalizability across diverse embedding spaces and domains remains uncertain
- Sampling strategy for training documents may introduce bias affecting approximation quality
- Two-layer MLP architecture may have limited capacity for complex embedding spaces
- Performance on specialized domains with different vocabulary distributions not thoroughly validated

## Confidence
**High confidence**: LEMUR can approximate MaxSim scores with learned neural network and achieve significant speedups while maintaining recall (supported by 5–11x speedup results at 80% recall).

**Medium confidence**: LEMUR consistently outperforms baselines across multiple modern multi-vector models (requires broader empirical testing to validate "consistency" claim).

**Low confidence**: Training without query data access maintains effectiveness (lacks comprehensive ablation studies or quantitative evidence).

## Next Checks
1. Test LEMUR's performance across diverse embedding spaces beyond ColBERTv2, including different dimensionalities and tokenization strategies, to validate generalizability.

2. Conduct ablation studies varying the sampling strategy for document embeddings used in training to quantify the impact of sample selection on approximation quality and retrieval performance.

3. Evaluate LEMUR's effectiveness when applied to specialized domains (e.g., medical literature, legal documents) with different vocabulary distributions and query patterns to assess domain adaptation capabilities.