---
ver: rpa2
title: 'Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness
  Landscape Analysis Perspective'
arxiv_id: '2509.21945'
source_url: https://arxiv.org/abs/2509.21945
tags:
- landscape
- configuration
- tuning
- performance
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reveals that traditional accuracy metrics for surrogate\
  \ models in configuration tuning can be misleading, as higher accuracy does not\
  \ guarantee better tuning results. To address this, the authors propose a new theory\u2014\
  landscape dominance\u2014that evaluates model usefulness based on the similarity\
  \ of global landscape structure and the severity of local optima compared to the\
  \ real system."
---

# Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective

## Quick Facts
- **arXiv ID**: 2509.21945
- **Source URL**: https://arxiv.org/abs/2509.21945
- **Reference count**: 40
- **Primary result**: Traditional accuracy metrics for surrogate models in configuration tuning can be misleading, and landscape features provide significantly different information.

## Executive Summary
This paper challenges the conventional wisdom that higher prediction accuracy for surrogate models guarantees better configuration tuning results. Through an extensive empirical study across 18 systems, 10 models, 16 tuners, and 8 landscape features (over 27,000 cases), the authors demonstrate that accuracy metrics and landscape features provide largely independent information about model usefulness. They propose "landscape dominance" theory, which evaluates models based on how closely their emulated landscape structures match real systems while exhibiting less severe local optima. The study finds that classic machine learning models tend to be more useful for tuning than deep learning models, and introduces Model4Tune, a predictive tool that uses learning-to-rank to estimate optimal model-tuner pairs for unseen systems.

## Method Summary
The research combines fitness landscape analysis with empirical tuning experiments to evaluate surrogate model usefulness. The method involves training 10 different surrogate models (both classic ML and DL) on 18 configurable systems, then computing 8 landscape features (4 global, 4 local) and 2 accuracy metrics on held-out test data. The landscape dominance theory uses Pareto dominance to compare models based on their landscape structure similarity to real systems and local optima severity. Model4Tune, built using LambdaRank, learns to predict the most useful model-tuner pairs for new systems by analyzing landscape deviations, accuracy metrics, and tuner characteristics. The approach uses leave-one-system-out cross-validation to validate predictions against random guessing baselines.

## Key Results
- Accuracy metrics (MAPE, µRD) and landscape features show positive correlation in less than 30% of cases, indicating they provide independent information about model utility
- Classic machine learning models outperform deep learning models for configuration tuning when evaluated through landscape dominance metrics
- Model4Tune achieves 79%-82% improvement over random guessing in predicting optimal model-tuner pairs, with up to 244% improvement in specific cases
- No single model emerges as universally best across all systems and tuners, highlighting the importance of context-aware selection

## Why This Works (Mechanism)

### Mechanism 1: Landscape Dominance Theory for Model Utility Assessment
A surrogate model's usefulness for configuration tuning can be assessed by comparing its emulated landscape features to the real system's landscape, rather than relying solely on traditional accuracy metrics. The theory posits that a more useful model should emulate a landscape with global features closer to the real landscape and local features that exhibit less severe local optima. This is formalized through "landscape dominance" using Pareto dominance principles. The core assumption is that configuration tuners navigate a fitness landscape; if a model's landscape differs significantly in structure or difficulty, the tuner's effectiveness will diverge regardless of point-wise prediction accuracy.

### Mechanism 2: Decorrelation of Accuracy and Landscape Features
Traditional accuracy metrics (e.g., MAPE, µRD) and landscape features provide largely independent information about a surrogate model's suitability for tuning. Accuracy measures point-wise prediction error, while landscape features capture structural properties like ruggedness (Correlation Length), guidance (Fitness Distance Correlation), and local optima density (Proportion of Local Optima). The study finds these sets of metrics are positively correlated in less than 30% of cases. A model can be accurate on average but still create a misleading search landscape (e.g., smoothing over critical local optima or distorting global gradients), leading tuners astray.

### Mechanism 3: Classic ML Over Deep Learning for Tuning Utility
Classic machine learning models tend to emulate more "tuning-friendly" landscapes compared to deep learning models, despite the latter often achieving higher accuracy. The paper hypothesizes that classic ML models (e.g., Decision Trees, Random Forests) may naturally produce landscapes that, while less accurate, have smoother gradients or less deceptive local optima structures, aligning better with common tuner heuristics. This is inferred from empirical rankings based on landscape dominance. The inductive biases of simpler models can incidentally create landscapes that are easier for standard optimization algorithms to navigate, even if they sacrifice some predictive fidelity.

## Foundational Learning

**Concept: Fitness Landscape Analysis**
Why needed here: The entire paper's thesis rests on analyzing the "fitness landscape" (configuration space mapped to performance) rather than just point predictions. You must understand what a landscape is (peaks, valleys, basins) and how its features (ruggedness, funnels) affect optimization. Quick check question: Given a 3D surface representing a configuration space, can you intuitively explain why a "rugged" surface with many local optima is harder for a hill-climbing tuner than a smooth, single-funnel surface?

**Concept: Pareto Dominance**
Why needed here: The paper's "landscape dominance" theory is formalized using Pareto dominance to compare models across multiple landscape objectives (e.g., minimizing global deviation and local difficulty). Quick check question: Model A is better than Model B on objective 1 but worse on objective 2. Under Pareto dominance, can you determine if A dominates B? What if it's a tie?

**Concept: Learning-to-Rank**
Why needed here: The proposed tool, Model4Tune, is built using a learning-to-rank algorithm (LambdaRank) to predict the best model-tuner pairs for a new system. Understanding ranking problems is crucial for implementing and extending this tool. Quick check question: In a learning-to-rank task for model selection, what represents a "document," a "query," and a "relevance label"?

## Architecture Onboarding

**Component map:**
18 configurable systems -> 10 surrogate models -> 8 landscape features + 2 accuracy metrics -> 16 tuners -> Model4Tune prediction tool

**Critical path:**
The core flow is (1) Train surrogate models on a subset of a system's data -> (2) Compute landscape features and accuracy on the held-out data -> (3) Compare the model's landscape to the real system's landscape (for deviation) -> (4) Optionally, pair models with tuners to run actual tuning and measure performance -> (5) Train Model4Tune on aggregated data to predict rankings for unseen systems.

**Design tradeoffs:**
- **Coarse vs. Fine-Grained Prediction:** Landscape dominance provides a coarse, Pareto-based ranking. Model4Tune provides a finer-grained, learned ranking but requires pre-training data.
- **Global vs. Local Feature Emphasis:** The theory weights global similarity and local ease equally (in a Pareto sense). In practice, one might be more important for specific tuners/systems. Model4Tune learns these weights implicitly.
- **Feature Selection for Prediction:** Model4Tune uses specific global/local feature pairs (Kur/MIE for sequential, Ske/PLO for batch tuners) as defaults, but other combinations could be tested.

**Failure signatures:**
- **Model4Tune underperforms random guessing:** This may occur if the target system's landscape characteristics are vastly different from the training systems (e.g., novel domain).
- **Landscape dominance fails to correlate with tuning performance:** This could happen if the chosen tuner is insensitive to the measured landscape features (e.g., a random sampler).
- **No model is found to be useful:** If all models create landscapes highly dissimilar to the real one and/or with severe local optima, no model will improve tuning over direct measurement.

**First 3 experiments:**
1. Reproduce RQ2: Compute the Spearman correlation between MAPE/µRD and all 8 landscape features for one system (e.g., HADOOP) using one model (e.g., Random Forest) across 30 runs. Verify that positive correlation is weak/non-existent.
2. Validate Landscape Dominance: For two models (e.g., GP and DT) and one system (e.g., XGBOOST), compute the 4 global deviations and 4 local features. Manually check if GP (which performed better in tuning) dominates DT according to the definition in Equation 16.
3. Evaluate Model4Tune (Simplified): Train a simple LambdaRank model on data from 17 systems, holding out one system (e.g., SPARK) as a test. Use only the default feature sets (Kur/MIE for sequential tuners) and predict the ranking of a few model-tuner pairs. Compare NDCG@1 to a random baseline.

## Open Questions the Paper Calls Out

**Open Question 1:** Would incorporating a more comprehensive set of landscape features improve the predictive accuracy of Model4Tune? The authors state in Section VIII-A that they "leave a more comprehensive coverage of other landscape features as future work." The study used only a representative subset of features (e.g., omitting ratio of partial information), potentially missing spatial properties that better correlate with tuning success. What evidence would resolve it: Re-evaluating Model4Tune with an expanded feature set showing statistically significant gains in NDCG or Average Precision.

**Open Question 2:** Can deep learning (DL) surrogate models be specifically redesigned to emulate landscapes that are more useful for tuning? Finding 4 notes that classic ML models are generally "more fit" for tuning than DL models, despite DL models often having superior accuracy. The paper identifies this discrepancy but does not propose architectural changes or regularization techniques to align DL model landscapes with real systems. What evidence would resolve it: A new DL model trained with landscape-aware loss functions that outperforms classic ML in "landscape dominance" metrics and actual tuning performance.

**Open Question 3:** Do the findings regarding landscape dominance generalize to emerging configurable domains like Agentic AI systems? Section I cites "Agentic AI systems" as an important domain for configuration, while Section VIII-C notes that "considering more systems might prove fruitful." The study was limited to 18 specific systems (mostly databases/big data), and the landscape characteristics of dynamic, non-stationary AI systems may differ. What evidence would resolve it: An empirical study applying the landscape dominance theory to Agentic AI systems yielding consistent patterns of model usefulness.

## Limitations
- The study relies heavily on empirical correlations between landscape features and tuning performance, which may not generalize to radically different configuration spaces or tuner algorithms
- The landscape dominance theory assumes Pareto dominance relationships hold consistently across systems, but the sensitivity of these rankings to the choice of landscape features remains unclear
- Model4Tune's predictions depend on the representativeness of the 18 training systems for unseen domains

## Confidence
- **High confidence**: The empirical finding that accuracy metrics poorly predict tuning performance (supported by correlation analysis showing <30% positive cases)
- **Medium confidence**: The landscape dominance theory's practical utility, as it's primarily validated through correlation rather than controlled experiments isolating landscape effects
- **Medium confidence**: The superiority of classic ML models over DL models for tuning, based on aggregated rankings but without systematic ablation studies

## Next Checks
1. Test Model4Tune on a held-out system from a different domain (e.g., a machine learning pipeline instead of database systems) to assess generalization beyond the training distribution
2. Conduct controlled experiments where tuners with different landscape sensitivity (e.g., random search vs. Bayesian optimization) are applied to landscapes with known structural properties to validate the landscape dominance theory
3. Perform ablation studies on landscape features to determine which specific metrics drive Model4Tune's predictions and whether simpler feature subsets maintain predictive power