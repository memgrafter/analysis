---
ver: rpa2
title: 'SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic
  Attributes Optimization in Diffusion Models'
arxiv_id: '2504.11923'
source_url: https://arxiv.org/abs/2504.11923
tags:
- adversarial
- semantic
- uaes
- semdiff
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating unrestricted adversarial
  examples (UAEs) that are both effective in fooling deep learning models and natural-looking
  without requiring clean input images. The key challenge is that current diffusion-based
  methods produce UAEs with visible local perturbations because they directly optimize
  in the intermediate latent noise space, which lacks high-level semantic information.
---

# SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models

## Quick Facts
- **arXiv ID**: 2504.11923
- **Source URL**: https://arxiv.org/abs/2504.11923
- **Reference count**: 40
- **Primary result**: Achieves 100% attack success rate on most tasks while significantly outperforming state-of-the-art methods in BRISQUE, FID, and KID metrics for generating natural unrestricted adversarial examples

## Executive Summary
This paper addresses the challenge of generating unrestricted adversarial examples (UAEs) that are both effective in fooling deep learning models and natural-looking without requiring clean input images. The key innovation is SemDiff, which explores the semantic latent space of diffusion models by modifying the deepest feature maps (h_t) in the UNet instead of the intermediate noise. This approach overcomes the limitation of current diffusion-based methods that produce UAEs with visible local perturbations due to direct optimization in the intermediate latent noise space. By training semantic functions to learn adversarial attributes and using multi-attributes optimization with weight penalties, SemDiff achieves 100% attack success rates across multiple classification tasks while maintaining superior imperceptibility and naturalness compared to existing methods.

## Method Summary
SemDiff generates unrestricted adversarial examples by optimizing semantic attributes in the diffusion model's latent space rather than directly manipulating noise vectors. The method modifies the deepest feature maps (h_t) in the UNet architecture, which contain high-level semantic information about the generated image. Semantic functions are trained to learn adversarial attributes such as "strong jawline" or "busy eyebrows," which are then optimized through a multi-attributes optimization approach. A weight penalty mechanism ensures that the generated examples maintain naturalness while achieving misclassification. The approach is validated across four tasks (gender classification, animal classification, church classification, and any-class classification) on three high-resolution datasets (CelebA-HQ, AFHQ, and ImageNet), demonstrating superior performance compared to state-of-the-art methods.

## Key Results
- Achieves 100% attack success rate on most tasks while significantly outperforming state-of-the-art methods in BRISQUE, FID, and KID metrics
- Demonstrates robustness against various defenses including JPEG compression, feature squeezing, SRNet, and adversarial training
- Shows superior imperceptibility and naturalness of generated adversarial examples compared to existing diffusion-based methods
- Successfully generates unrestricted adversarial examples without requiring clean input images

## Why This Works (Mechanism)
SemDiff works by leveraging the semantic latent space of diffusion models rather than directly optimizing in the intermediate noise space. By modifying the deepest feature maps (h_t) in the UNet, which contain high-level semantic information, the method can create adversarial perturbations that are more aligned with natural image attributes. The semantic functions learn to manipulate specific visual characteristics (like facial features or architectural elements) that are meaningful to both humans and classifiers. The multi-attributes optimization with weight penalties ensures that these semantic modifications achieve the desired misclassification while maintaining perceptual quality, addressing the fundamental limitation of previous diffusion-based approaches that produce visibly perturbed outputs.

## Foundational Learning

**Diffusion Models**: Why needed - Understanding the denoising process and latent space structure is crucial for effective adversarial generation; Quick check - Can explain the forward noising and reverse denoising processes in diffusion models

**UNet Architecture**: Why needed - The feature hierarchy and skip connections determine how semantic information flows through the model; Quick check - Can identify the different levels of feature maps and their semantic content

**Semantic Attribute Learning**: Why needed - Mapping between attribute descriptions and visual features enables targeted adversarial modifications; Quick check - Can describe how semantic functions bridge textual attributes and visual representations

**Multi-Attributes Optimization**: Why needed - Balancing multiple semantic objectives while maintaining image quality requires sophisticated optimization techniques; Quick check - Can explain how weight penalties prevent adversarial examples from becoming unnatural

## Architecture Onboarding

**Component Map**: Diffusion model UNet (with feature maps h_t) -> Semantic functions (attribute mappings) -> Multi-attributes optimizer (with weight penalties) -> Adversarial example generation

**Critical Path**: The critical path involves extracting deep semantic features from the UNet, applying semantic function transformations to create adversarial attributes, optimizing these attributes through the multi-attributes approach with penalties, and generating the final adversarial example that achieves misclassification while maintaining naturalness.

**Design Tradeoffs**: The method trades computational complexity (due to multi-round optimization) for improved naturalness and effectiveness. Using deep semantic features instead of intermediate noise provides better perceptual quality but requires training semantic functions. The weight penalty mechanism balances attack success with imperceptibility but introduces additional hyperparameters to tune.

**Failure Signatures**: The method may fail when semantic functions cannot adequately capture the relationship between attributes and visual features, when the weight penalties are improperly tuned leading to either ineffective attacks or unnatural outputs, or when the diffusion model's latent space doesn't contain sufficient semantic information for the target attributes.

**3 First Experiments**:
1. Validate that modifying h_t features produces more natural adversarial examples than modifying intermediate noise vectors
2. Test the effectiveness of semantic functions on learning simple attribute transformations before complex ones
3. Evaluate the impact of different weight penalty values on the tradeoff between attack success and perceptual quality

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- Limited qualitative assessment of how generated perturbations align with human perception of attribute modifications
- Computational cost of the multi-round optimization process not thoroughly discussed in terms of practical deployment constraints
- Evaluation primarily focuses on attack success rates and perceptual quality metrics without extensive examination of semantic relevance to target classes

## Confidence

**High confidence** in core technical contribution and quantitative improvements over baselines, particularly regarding attack success rates and perceptual quality metrics (BRISQUE, FID, KID)

**Medium confidence** in claimed robustness against defenses, as evaluation covers common techniques but may not encompass more sophisticated adversarial defense mechanisms

**Medium confidence** in generalizability of results across different model architectures, as study primarily validates on CLIP-based classifiers

## Next Checks

1. Conduct human perception studies to validate the naturalness and semantic relevance of generated adversarial examples, particularly focusing on whether attribute modifications align with human expectations

2. Evaluate the method's transferability across different backbone architectures beyond CLIP, including standard CNNs and vision transformers, to assess generalizability

3. Perform ablation studies to quantify the impact of semantic function design choices and weight penalty parameters on both attack success and perceptual quality