---
ver: rpa2
title: What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate
  Generative Models
arxiv_id: '2504.20687'
source_url: https://arxiv.org/abs/2504.20687
tags:
- data
- synthetic
- feature
- generative
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating synthetic tabular
  data quality, which is difficult due to the absence of direct performance measures
  and the complexity of assessing high-dimensional dependencies. Existing metrics
  often provide conflicting results and lack interpretability.
---

# What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models

## Quick Facts
- arXiv ID: 2504.20687
- Source URL: https://arxiv.org/abs/2504.20687
- Authors: Jan Kapar; Niklas Koenen; Martin Jullum
- Reference count: 40
- The paper proposes using explainable AI (XAI) techniques on a binary classifier trained to distinguish real from synthetic data to evaluate synthetic tabular data quality.

## Executive Summary
Evaluating synthetic tabular data quality is challenging due to the absence of direct performance measures and the complexity of assessing high-dimensional dependencies. Existing metrics often provide conflicting results and lack interpretability. This paper introduces an innovative approach that uses explainable AI techniques on a binary classifier trained to distinguish real from synthetic data. By leveraging feature importance measures, partial dependence plots, Shapley values, and counterfactual explanations, this method reveals why synthetic data are distinguishable, highlighting unrealistic patterns, missing dependencies, and distributional inconsistencies. Experiments on real datasets demonstrate that this approach uncovers weaknesses in synthetic data overlooked by standard evaluation techniques.

## Method Summary
The proposed method trains a binary classifier to distinguish between real and synthetic tabular data. Once trained, explainable AI techniques are applied to understand the classifier's decision-making process. Feature importance measures identify which variables contribute most to distinguishing real from synthetic data. Partial dependence plots visualize relationships between features and the classifier's predictions. Shapley values quantify the contribution of each feature value to individual predictions. Counterfactual explanations identify minimal changes needed to flip a synthetic data point to appear real. Together, these XAI techniques reveal patterns and dependencies that make synthetic data distinguishable from real data, providing actionable insights for improving synthetic data quality.

## Key Results
- XAI techniques reveal unrealistic patterns and missing dependencies in synthetic data that standard metrics overlook
- The approach identifies distributional inconsistencies that make synthetic data distinguishable from real data
- Feature importance analysis highlights which variables contribute most to synthetic data detectability

## Why This Works (Mechanism)
The method works by exploiting the fundamental principle that if a classifier can reliably distinguish between real and synthetic data, then the synthetic data must exhibit detectable patterns or inconsistencies. By applying XAI techniques to this classifier, we can reverse-engineer what these distinguishing features are. The classifier effectively learns the statistical signatures of synthetic data generation processes, and XAI methods make these signatures interpretable to humans. This provides a form of "white-box" evaluation where we can understand not just whether synthetic data is distinguishable, but specifically why it is distinguishable.

## Foundational Learning

**Binary Classification for Data Quality** - Training a classifier to distinguish real from synthetic data serves as an indirect quality measure. Why needed: Direct evaluation metrics for synthetic data are limited and often contradictory. Quick check: Verify classifier accuracy significantly exceeds random chance.

**Feature Importance Analysis** - Identifying which variables most contribute to classification decisions. Why needed: Pinpoints which features contain the most synthetic data artifacts. Quick check: Compare feature importance rankings across multiple synthetic data generation methods.

**Partial Dependence Plots** - Visualizing how feature relationships affect classification. Why needed: Reveals unrealistic joint distributions in synthetic data. Quick check: Look for flat or unnatural patterns in the plots.

**Shapley Value Decomposition** - Quantifying individual feature contributions to predictions. Why needed: Provides local explanations for why specific synthetic data points are flagged. Quick check: Verify Shapley values are stable across similar synthetic samples.

**Counterfactual Explanations** - Identifying minimal changes to make synthetic data appear real. Why needed: Reveals specific modifications needed to improve synthetic data quality. Quick check: Ensure counterfactuals are realistic and actionable.

## Architecture Onboarding

**Component Map:** Real Data <- Classifier <- Synthetic Data -> XAI Techniques -> Interpretability

**Critical Path:** Data Generation → Classifier Training → XAI Application → Interpretation

**Design Tradeoffs:** The approach trades computational overhead of training a classifier for interpretable insights. A simpler but less interpretable metric-based evaluation could be faster but wouldn't reveal specific weaknesses.

**Failure Signatures:** If the classifier cannot distinguish real from synthetic data, the XAI techniques will not provide meaningful insights. Poor classifier performance might indicate either high-quality synthetic data or insufficient model capacity.

**First 3 Experiments:**
1. Train classifier on real vs synthetic data and measure baseline accuracy
2. Apply feature importance analysis to identify distinguishing variables
3. Generate partial dependence plots to visualize problematic feature relationships

## Open Questions the Paper Calls Out
None

## Limitations
- Classifier-based evaluation may not align with actual downstream task performance
- Effectiveness depends heavily on classifier quality and interpretability
- Results may be dataset-specific and require broader validation

## Confidence

**High confidence:** The core methodology of using XAI techniques on a binary classifier is sound and well-established

**Medium confidence:** The experimental results demonstrating the approach's ability to uncover synthetic data weaknesses are convincing but may be dataset-specific

**Medium confidence:** The claim that this approach reveals insights "overlooked by standard evaluation techniques" needs broader validation across diverse synthetic data generation methods

## Next Checks
1. Test the XAI evaluation framework on synthetic data generated by multiple different algorithms (GANs, VAEs, CTGANS, etc.) to assess generalizability
2. Compare the classifier-based evaluation results with actual downstream task performance metrics to validate correlation
3. Conduct experiments with intentionally injected synthetic data errors to verify that the XAI methods can detect specific types of synthetic data flaws