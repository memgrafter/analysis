---
ver: rpa2
title: Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic
  Values
arxiv_id: '2506.11849'
source_url: https://arxiv.org/abs/2506.11849
tags:
- values
- probabilistic
- value
- shapley
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Regression Maximum Sample Reuse (Regression
  MSR), a novel approach for estimating probabilistic values like Shapley and Banzhaf
  values in explainable AI. The method combines Monte Carlo sampling with regression-based
  variance reduction, allowing unbiased estimates while reusing samples across all
  players.
---

# Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values

## Quick Facts
- arXiv ID: 2506.11849
- Source URL: https://arxiv.org/abs/2506.11849
- Reference count: 40
- Introduces Regression Maximum Sample Reuse (Regression MSR), a novel approach for estimating probabilistic values like Shapley and Banzhaf values in explainable AI

## Executive Summary
This paper introduces Regression Maximum Sample Reuse (Regression MSR), a novel approach for estimating probabilistic values like Shapley and Banzhaf values in explainable AI. The method combines Monte Carlo sampling with regression-based variance reduction, allowing unbiased estimates while reusing samples across all players. Unlike prior work limited to linear models, Regression MSR can use any function family with efficient probabilistic value computation, including tree-based models like XGBoost.

## Method Summary
Regression MSR works by first learning an approximation of the value function using a subset of samples, then using this approximation to reduce variance in the MSR estimator on held-out samples. The key insight is that by learning a regression model on training data, one can obtain better estimates of conditional expectations needed for probabilistic value computation. This yields unbiased estimates whose accuracy depends on how well the learned function fits the true value function. The method generalizes previous approaches by allowing any function family with efficient probabilistic value computation, not just linear models.

## Key Results
- Regression MSR achieves up to 6.5× lower error than Permutation SHAP, 3.8× lower than Kernel SHAP, and 2.6× lower than Leverage SHAP for Shapley values
- For general probabilistic values, Regression MSR achieves up to 215× lower error than prior methods
- Tree MSR, which uses XGBoost models, particularly excels with larger sample sizes, sometimes outperforming previous approaches by orders of magnitude

## Why This Works (Mechanism)
Regression MSR works by leveraging the relationship between probabilistic values and conditional expectations of the value function. By learning a regression model that approximates the value function, the method can provide more accurate estimates of these conditional expectations than simple Monte Carlo sampling alone. The regression model acts as a regularizer, reducing variance by smoothing out noise in the sample estimates. This variance reduction is particularly effective when the regression model can capture the underlying structure of the value function well.

## Foundational Learning
1. Shapley Values: A method for attributing the contribution of each feature to a model's prediction, satisfying efficiency, symmetry, dummy, and additivity properties. Needed to understand the target estimand and evaluate the method's performance.
2. Maximum Sample Reuse (MSR) Estimators: A family of unbiased estimators that reuse samples across all players to reduce variance. Quick check: Verify that MSR estimators satisfy unbiasedness by checking that E[MSR] = φ_i for all players i.
3. Variance Reduction Techniques: Methods for reducing the variance of Monte Carlo estimators, such as control variates and importance sampling. Quick check: Compare the variance of Regression MSR to standard Monte Carlo estimators on synthetic data.

## Architecture Onboarding
Component Map: Data Samples -> Regression Model Training -> Variance Reduction -> Probabilistic Value Estimation

Critical Path: The most critical component is the regression model training, as its quality directly impacts the variance reduction and final estimate accuracy.

Design Tradeoffs: The main tradeoff is between the flexibility of the regression model (which affects approximation quality) and computational efficiency. More complex models may provide better approximations but require more training time and data.

Failure Signatures: If the regression model underfits the data, the variance reduction benefits will be minimal. If it overfits, the estimates may be biased. Performance degradation is likely on datasets with complex, non-linear relationships that are difficult to model.

First Experiments:
1. Compare Regression MSR with different regression model families (linear, tree-based, neural networks) on synthetic data with known ground truth values.
2. Evaluate the sensitivity of Regression MSR to the number of training samples and held-out samples on benchmark datasets.
3. Test Regression MSR on real-world datasets with unknown ground truth values to assess practical utility beyond benchmark scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on the quality of the learned regression model; misspecification or underfitting can minimize variance reduction benefits
- Requires training a regression model for each estimation task, adding computational overhead not present in simpler Monte Carlo methods
- Claims about working with any function family are theoretically correct but practical performance may vary significantly across different model types and data distributions

## Confidence
- High confidence: The theoretical framework for unbiased estimation and variance reduction is sound
- Medium confidence: Experimental results are compelling but primarily evaluated on benchmark datasets with known ground truth values
- Medium confidence: Claims about working with any function family are theoretically correct but practical performance may vary

## Next Checks
1. Test Regression MSR on real-world datasets with unknown ground truth values to evaluate practical utility beyond benchmark scenarios
2. Compare computational efficiency against existing methods across different sample sizes and feature dimensions to quantify the trade-off between accuracy gains and training overhead
3. Evaluate performance when using different regression model families (linear, tree-based, neural networks) on the same tasks to understand which function families work best for which types of data and models