---
ver: rpa2
title: Attention-Only Transformers via Unrolled Subspace Denoising
arxiv_id: '2506.03790'
source_url: https://arxiv.org/abs/2506.03790
tags:
- representations
- token
- each
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a minimalistic transformer architecture consisting
  of only self-attention layers with skip connections, designed to denoise token representations
  modeled as a mixture of low-rank Gaussians. By unrolling iterative denoising operations
  via multi-head subspace self-attention, the authors construct an interpretable transformer
  architecture that achieves denoising performance at a linear rate in terms of signal-to-noise
  ratio improvement per layer.
---

# Attention-Only Transformers via Unrolled Subspace Denoising

## Quick Facts
- arXiv ID: 2506.03790
- Source URL: https://arxiv.org/abs/2506.03790
- Reference count: 40
- Primary result: Attention-only transformer achieves ImageNet 83.3% accuracy with 22M parameters vs 39M for standard ViT

## Executive Summary
This paper proposes a minimalistic transformer architecture consisting only of self-attention layers with skip connections, designed to denoise token representations modeled as a mixture of low-rank Gaussians. By unrolling iterative denoising operations via multi-head subspace self-attention, the authors construct an interpretable transformer architecture that achieves denoising performance at a linear rate in terms of signal-to-noise ratio improvement per layer. Experimental results on vision (ImageNet) and language tasks demonstrate that this attention-only transformer achieves performance comparable to standard transformer architectures like GPT-2 and ViT, while using significantly fewer parameters.

## Method Summary
The paper presents an attention-only transformer (AoT) architecture that interprets self-attention as a denoising operator for mixture-of-low-rank-Gaussian representations. The core layer update follows Z(l+1) = Z(l) + η·MSSA(Z(l)) where MSSA is multi-head subspace self-attention. The architecture uses skip connections and optional LayerNorm, with softmax for attention computation. The theoretical framework shows that self-attention can achieve linear SNR improvement per layer through unrolled subspace denoising. Vision models are trained with Lion optimizer, while language models use AdamW. The approach is evaluated on ImageNet classification, causal language modeling, and in-context learning tasks.

## Key Results
- ImageNet-21K pretrained AoT-MHSA-V achieves 83.3% top-1 accuracy on ImageNet-1K using 22M parameters vs 84.6% for 39M parameter ViT
- AoT-MSSA-L Base (24L/1024d/16H) achieves comparable training/validation loss to GPT-2 Base on OpenWebText
- Theoretical SNR improvement rate of O(1/σ²) per layer demonstrated through unrolled subspace denoising

## Why This Works (Mechanism)
The paper establishes that self-attention can be interpreted as a denoising operator for token representations that follow a mixture of low-rank Gaussian distributions. By unrolling iterative denoising operations through multi-head subspace self-attention with skip connections, each layer achieves linear SNR improvement. The softmax attention mechanism effectively performs soft subspace clustering, while the residual connections enable gradual refinement of representations. This provides a principled theoretical foundation for why transformers work, framing attention as solving a structured denoising problem rather than just pattern matching.

## Foundational Learning
- **Mixture of low-rank Gaussians**: Why needed - provides mathematical model for token representations in transformers; Quick check - verify synthetic data generation follows this distribution
- **Unrolled iterative denoising**: Why needed - connects attention layers to optimization procedure for maximum likelihood estimation; Quick check - confirm SNR improvement rate matches theoretical O(1/σ²)
- **Multi-head subspace self-attention**: Why needed - enables parallel denoising across different subspaces; Quick check - monitor attention weights for subspace separation
- **Skip connections in denoising**: Why needed - prevents rank collapse and enables stable training; Quick check - verify residual path remains active throughout training
- **Softmax attention as soft clustering**: Why needed - provides probabilistic interpretation of attention mechanism; Quick check - analyze attention patterns for clustering behavior
- **Linear SNR improvement per layer**: Why needed - establishes theoretical convergence rate for the denoising process; Quick check - measure SNR improvement empirically across layers

## Architecture Onboarding
- **Component map**: Input embeddings -> Multi-head subspace self-attention -> Skip connection -> Output (repeat per layer)
- **Critical path**: Token embeddings flow through repeated attention+skip blocks, with each layer performing denoising refinement
- **Design tradeoffs**: Removing MLPs reduces parameters and improves interpretability but may limit representational capacity; denoising step size η controls convergence speed vs stability
- **Failure signatures**: Rank collapse without skip connections; slow/no convergence if η or learning rate mismatched; attention collapse to uniform distribution
- **First experiments**: 1) Implement AoT layer with MSSA and verify denoising on synthetic mixture data; 2) Build AoT-MHSA-V and train on ImageNet-1K from scratch; 3) Compare AoT-MSSA-L Base training curves against GPT-2 baseline on OpenWebText

## Open Questions the Paper Calls Out
None

## Limitations
- Critical experimental details like denoising step size η and learning rate schedules are unspecified, preventing faithful reproduction
- ImageNet results show only marginal improvement over MLP-free baselines (83.3% vs 84.6%), suggesting theoretical advantage may be limited in practice
- Theoretical framework assumes idealized conditions that may not fully account for practical training dynamics and optimization challenges

## Confidence
- **High Confidence**: Theoretical foundation for attention as denoising operator via unrolled subspace iteration is mathematically rigorous
- **Medium Confidence**: Empirical results showing competitive performance are credible but lack complete experimental details for independent verification
- **Low Confidence**: Theoretical SNR improvement claims may not fully account for finite-sample effects and practical training challenges

## Next Checks
1. Implement AoT layer with MSSA and verify denoising step size η through systematic grid search on synthetic mixture-of-low-rank-Gaussians, measuring per-layer SNR improvement rates
2. Reproduce AoT-MHSA-V training on ImageNet-1K from scratch with Lion optimizer, comparing training curves and final accuracy while varying denoising step size and LR schedule
3. Conduct controlled ablations comparing AoT variants against standard transformers with MLP layers removed, isolating the specific contribution of self-attention-only architecture versus residual connections