---
ver: rpa2
title: Tensorized Clustered LoRA Merging for Multi-Task Interference
arxiv_id: '2508.03999'
source_url: https://arxiv.org/abs/2508.03999
tags:
- lora
- task
- arxiv
- merging
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TC-LoRA addresses task interference in LoRA merging by operating
  at both text and parameter levels. At the text level, it clusters training instances
  based on input embedding similarity and trains specialized LoRA adapters for each
  cluster, reducing example-level interference.
---

# Tensorized Clustered LoRA Merging for Multi-Task Interference

## Quick Facts
- arXiv ID: 2508.03999
- Source URL: https://arxiv.org/abs/2508.03999
- Reference count: 9
- Key outcome: TC-LoRA achieves +1.4% accuracy on Phi-3 and +2.3% on Mistral-7B in zero-shot tasks by clustering training instances and jointly factorizing LoRA adapters via CP decomposition.

## Executive Summary
TC-LoRA addresses task interference in multi-task LoRA merging through a two-level approach: instance-level clustering based on input embeddings and parameter-level joint tensor decomposition. The method clusters training instances using sentence embeddings and trains specialized LoRA adapters for each cluster, reducing example-level interference. At the parameter level, it employs joint Canonical Polyadic (CP) decomposition to factorize multiple LoRA adapters simultaneously, disentangling task-specific and shared components to reduce cross-task interference.

The method demonstrates significant improvements over strong SVD-based baselines, achieving +1.4% accuracy on Phi-3 and +2.3% on Mistral-7B in zero-shot tasks. It also shows superior performance on math-coding skill composition tasks with 15.69% accuracy versus 13.49% for multi-task training. The approach is effective across reasoning, question answering, and coding tasks while enabling parameter compression.

## Method Summary
TC-LoRA operates at two levels to reduce task interference. At the text level, it clusters training instances based on input embedding similarity using sentence-t5-xxl embeddings and K-means clustering. It then trains specialized LoRA adapters for each cluster. At the parameter level, it employs joint Canonical Polyadic (CP) decomposition to factorize multiple LoRA adapters simultaneously, disentangling task-specific and shared components. The method uses rank-4 LoRA adapters with 5 epochs of training and CP decomposition with 20 components. The approach is evaluated on 10 zero-shot benchmarks including WinoGrande, HellaSwag, PIQA, BoolQ, OpenbookQA, ARC-easy/challenge, HumanEval, MBPP, BBH, and GSM8k-hard for skill composition.

## Key Results
- Achieves +1.4% accuracy on Phi-3 and +2.3% on Mistral-7B in zero-shot tasks compared to SVD baselines
- Improves math-coding skill composition tasks with 15.69% accuracy versus 13.49% for multi-task training
- Demonstrates effectiveness across reasoning, question answering, and coding tasks while enabling parameter compression

## Why This Works (Mechanism)
TC-LoRA addresses task interference through dual-level intervention. At the text level, clustering training instances by input embedding similarity allows specialized LoRA adapters to be trained for each cluster, reducing example-level interference. This ensures that similar tasks are handled by specialized adapters rather than being mixed in a single adapter. At the parameter level, joint CP decomposition factorizes multiple LoRA adapters simultaneously, disentangling task-specific components from shared components. This reduces cross-task interference by learning a common basis that captures shared information while preserving task-specific variations. The tensorized approach enables more effective merging than simple averaging or SVD-based methods by jointly considering all adapter parameters.

## Foundational Learning

**LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that inserts low-rank matrices into existing model weights, reducing the number of trainable parameters while maintaining performance. Why needed: Enables efficient multi-task adaptation without full fine-tuning. Quick check: Verify that rank-4 matrices capture sufficient task-specific variation.

**Canonical Polyadic (CP) Decomposition**: A tensor factorization method that decomposes a tensor into a sum of rank-one tensors, finding common and unique components across multiple matrices. Why needed: Enables joint factorization of multiple LoRA adapters to disentangle shared and task-specific components. Quick check: Monitor reconstruction error to ensure decomposition preserves adapter information.

**K-means Clustering**: An unsupervised learning algorithm that partitions data into K clusters based on feature similarity. Why needed: Groups similar training instances to reduce example-level interference during adapter training. Quick check: Visualize embeddings to verify semantically coherent clusters.

**Sentence Embeddings**: Dense vector representations of text that capture semantic meaning. Why needed: Enables clustering of training instances based on instruction similarity. Quick check: Verify embeddings capture task similarity through nearest-neighbor analysis.

## Architecture Onboarding

**Component Map**: Input data -> Sentence embedding encoding -> K-means clustering -> Cluster-specific LoRA adapter training -> Tensor stacking of Δ matrices -> CP decomposition -> Merged weight reconstruction -> Evaluation

**Critical Path**: The essential sequence is clustering → adapter training → tensor stacking → CP decomposition → weight merging. Each step must succeed for the method to work: poor clustering leads to ineffective adapters, poor decomposition loses information, and poor merging degrades performance.

**Design Tradeoffs**: K=10 clusters balances specialization against data scarcity per cluster; rank=4 adapters provide sufficient expressivity while remaining efficient; R=20 CP components capture shared structure without overfitting. The 20% sampling ratio for clustering reduces computation but may miss rare patterns.

**Failure Signatures**: Numerical instability in CP decomposition manifests as high reconstruction error or convergence failure; poor clustering produces adapters that don't generalize; overcompression in CP decomposition loses task-specific information. Monitor reconstruction error, clustering quality metrics, and per-task performance.

**First Experiments**:
1. Train single LoRA adapter on full data and evaluate as baseline
2. Apply K-means clustering and visualize cluster assignments to verify semantic coherence
3. Run CP decomposition on synthetic tensors to validate implementation before applying to LoRA adapters

## Open Questions the Paper Calls Out
None

## Limitations
- **Tensor Decomposition Implementation**: CP decomposition library, convergence criteria, and initialization strategy are not specified, making it difficult to reproduce results reliably.
- **Cluster Assignment Strategy**: The method for assigning the remaining 80% of data to clusters after training on 20% is not specified, which could significantly impact adapter training quality.
- **Evaluation Protocol**: The evaluation combines multiple benchmark suites without clarifying whether results are averaged across all tasks or reported per task family, limiting interpretability.

## Confidence
**High Confidence**: The overall architectural approach of combining instance-level clustering with joint tensor factorization is technically sound. The empirical improvements over SVD baselines on both Phi-3 and Mistral-7B are statistically significant and consistent across multiple task types.

**Medium Confidence**: The specific performance gains (+1.4% accuracy on Phi-3, +2.3% on Mistral-7B) are reliable given the experimental setup, but may vary with different datasets or model architectures.

## Next Checks
1. Verify CP decomposition implementation by testing on synthetic tensors with known ground truth components and monitoring reconstruction error
2. Visualize K-means cluster assignments using t-SNE or UMAP to confirm semantic coherence of clusters
3. Compare TC-LoRA performance against ablation variants (no clustering, no CP decomposition) to isolate contribution of each component