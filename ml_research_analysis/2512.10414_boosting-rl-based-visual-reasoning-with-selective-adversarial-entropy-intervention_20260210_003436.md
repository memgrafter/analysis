---
ver: rpa2
title: Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention
arxiv_id: '2512.10414'
source_url: https://arxiv.org/abs/2512.10414
tags:
- entropy
- adversarial
- policy
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Selective-adversarial Entropy Intervention (SaEI)
  to enhance reinforcement learning-based visual reasoning in vision-language models
  (VLMs). The method addresses the issue of entropy collapse in Group Relative Policy
  Optimization (GRPO) by intervening in policy entropy during RL sampling rather than
  policy optimization.
---

# Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention

## Quick Facts
- arXiv ID: 2512.10414
- Source URL: https://arxiv.org/abs/2512.10414
- Reference count: 40
- Primary result: +2.00% accuracy on MM-Eureka, +2.16% on Geometry3K over vanilla GRPO

## Executive Summary
This paper addresses entropy collapse in reinforcement learning-based visual reasoning by introducing Selective-adversarial Entropy Intervention (SaEI). The method intervenes in policy entropy during RL sampling rather than optimization, using entropy-guided adversarial perturbations on visual inputs to increase policy entropy and enable broader response space exploration. By selectively targeting moderate-entropy tokens while preserving factual knowledge, SaEI achieves significant improvements on both in-domain (Geometry3K, MM-Eureka) and out-of-domain (HalluBench, MathVista, MathVerse, MathVision) benchmarks while maintaining training stability.

## Method Summary
SaEI introduces two key components: entropy-guided adversarial sampling (EgAS) that formulates policy entropy as an adversarial objective and attacks visual inputs via single-step PGD perturbations, and token-selective entropy computation (TsEC) that selects only middle-third tokens by entropy rank for adversarial attack while avoiding distortion of factual knowledge. During RL sampling, the method generates responses from both clean and adversarial images, combining them for advantage computation while using only clean images for policy optimization. The approach addresses entropy collapse in GRPO training by maintaining higher policy entropy throughout training, leading to improved exploration and reasoning capabilities.

## Key Results
- Achieves 64.45% accuracy on MM-Eureka (2.00% improvement over vanilla GRPO)
- Achieves 56.18% accuracy on Geometry3K (2.16% improvement over vanilla GRPO)
- Demonstrates superior stability with lower standard deviation (1.26% vs 4.51% for KL-Cov on MM-Eureka)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entropy-guided adversarial perturbations on visual inputs increase policy entropy during RL sampling, enabling broader response space exploration.
- **Mechanism:** The method formulates policy entropy as an adversarial objective. During RL sampling, it computes entropy from initial responses, backpropagates the negative entropy gradient through the vision encoder via PGD attack (1 iteration, small step size α = -2/255 to -3/255), and generates perturbed images. These adversarial images force the policy model to produce more diverse responses. The gradient update follows: I_adv = I + α·sign(∇_I(-H(π))). Clean and adversarial responses are then combined (n₁=6, n₂=6) for advantage computation.
- **Core assumption:** Entropy gradients provide meaningful direction for perturbation; visual perturbations transfer to reasoning diversity without corrupting semantics.
- **Evidence anchors:** [abstract] "formulates the entropy of sampled responses as an adversarial objective...adversarial gradient can be used to attack the visual input"; [section 4.2] "By subtracting gradient from -H(π), the obtained I_adv is able to encourage policy exploration with the increased policy entropy"; [corpus] Weak direct evidence; ECHO (arXiv:2602.02150) addresses entropy optimization but at test-time, not via adversarial sampling.
- **Break condition:** If adversarial perturbation step size is too large (T≥2 iterations), entropy spikes then accuracy collapses (Figure 8 shows rapid accuracy decline after 30 steps with T=2).

### Mechanism 2
- **Claim:** Selecting only moderate-entropy tokens for adversarial objective computation preserves factual knowledge while targeting exploration-critical tokens.
- **Mechanism:** Tokens are ranked by entropy and divided into thirds. Only the middle third contributes to the entropy objective H(π). Lowest-entropy tokens (containing factual knowledge) and highest-entropy tokens (already exploring) are excluded. This prevents adversarial perturbations from corrupting memorized facts or redundantly targeting already-diverse tokens.
- **Core assumption:** Token entropy patterns correlate with functional roles (factual vs. exploratory) as claimed by Wang et al. [36]; the one-third division is optimal across tasks.
- **Evidence anchors:** [abstract] "select tokens with moderate entropy for adversarial attack while avoiding distortion of factual knowledge"; [section 4.3] "tokens with the lowest entropy primarily complete ongoing linguistic structures with factual knowledge...tokens with the highest entropy function as pivotal decision points"; [corpus] No direct corpus validation for token-selective entropy; Wang et al. [36] is external citation not in corpus.
- **Break condition:** Using all tokens for entropy computation reduces improvement from +2.55% to +1.39% (Table 4); including low-entropy tokens risks factual corruption.

### Mechanism 3
- **Claim:** Mixed sampling from clean and adversarial images during RL sampling improves advantage estimation by introducing diverse yet learnable trajectories.
- **Mechanism:** Generate n₁ responses from clean images, compute entropy-based adversarial perturbation, generate n₂ responses from adversarial images, concatenate both groups for reward computation and advantage normalization. Policy optimization uses only clean images for π_θ likelihood to maintain training stability while benefiting from adversarial exploration.
- **Core assumption:** Adversarial samples provide useful exploration signals without requiring adversarial images at optimization time; group advantage normalization handles mixed-quality responses appropriately.
- **Evidence anchors:** [section 4.2] "we use only clean images I for the calculation of current policy likelihood π_θ...because current policy model π_θ should be optimized using images following original distribution"; [Figure 2] Shows SaEI maintains higher entropy (0.4-0.45 vs