---
ver: rpa2
title: 'Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic
  and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models'
arxiv_id: '2511.18177'
source_url: https://arxiv.org/abs/2511.18177
tags:
- retrieval
- latency
- generation
- systems
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation comparing vector-based
  agentic RAG systems with hybrid search and metadata filtering against hierarchical
  node-based reasoning systems for financial document question answering. The study
  evaluates two enhancement techniques - cross-encoder reranking and small-to-big
  chunk retrieval - applied to the vector-based architecture.
---

# Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models

## Quick Facts
- arXiv ID: 2511.18177
- Source URL: https://arxiv.org/abs/2511.18177
- Reference count: 6
- Vector-based agentic RAG achieves 68% win rate over hierarchical systems with comparable latency

## Executive Summary
This paper presents the first systematic evaluation comparing vector-based agentic RAG systems with hybrid search and metadata filtering against hierarchical node-based reasoning systems for financial document question answering. The study evaluates two enhancement techniques - cross-encoder reranking and small-to-big chunk retrieval - applied to the vector-based architecture. Across 1,200 SEC filings (10-K, 10-Q, 8-K) on a 150-question benchmark, vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 vs 5.98 seconds).

## Method Summary
The evaluation compares vector-based agentic RAG systems with hierarchical node-based reasoning systems for financial document question answering. The vector-based approach employs hybrid search with metadata filtering, while the hierarchical system uses node-based reasoning. Two enhancement techniques are evaluated: cross-encoder reranking and small-to-big chunk retrieval. The benchmark consists of 1,200 SEC filings processed across 150 questions, measuring win rates, latency, and accuracy metrics including MRR@5 and Recall@5.

## Key Results
- Vector-based agentic RAG achieves 68% win rate over hierarchical systems with comparable latency (5.2 vs 5.98 seconds)
- Cross-encoder reranking achieves 59% absolute improvement at optimal parameters (10, 5) for MRR@5, improving from 0.160 to 0.750 with perfect Recall@5 (1.00)
- Small-to-big retrieval achieves 65% win rate over baseline chunking with only 0.2 seconds additional latency

## Why This Works (Mechanism)
The vector-based agentic RAG system demonstrates superior performance by combining the semantic understanding of vector embeddings with agentic decision-making capabilities. The hybrid search approach leverages both semantic similarity and keyword matching, while metadata filtering reduces noise by focusing on relevant document sections. The cross-encoder reranking provides fine-grained semantic matching beyond initial vector retrieval, and small-to-big chunk retrieval captures context at multiple granularities. This multi-layered approach addresses the complexity of financial documents where precision and context are critical for accurate answers.

## Foundational Learning
**Vector Embeddings and Semantic Search**
- Why needed: Traditional keyword search fails to capture semantic relationships in financial documents
- Quick check: Test if semantically similar but lexically different queries retrieve relevant documents

**Hybrid Search Architecture**
- Why needed: Pure vector search can miss exact matches while keyword search misses semantic relationships
- Quick check: Compare hybrid search against pure vector and pure keyword baselines

**Cross-Encoder Reranking**
- Why needed: Initial vector retrieval provides broad coverage but lacks fine-grained semantic precision
- Quick check: Measure rank improvement for relevant documents moved up by reranking

**Metadata Filtering**
- Why needed: Financial documents contain multiple sections with varying relevance to queries
- Quick check: Evaluate retrieval quality with and without metadata constraints

**Small-to-Big Chunk Retrieval**
- Why needed: Single chunk size cannot balance context preservation with retrieval precision
- Quick check: Compare answer quality across different chunk size hierarchies

## Architecture Onboarding

**Component Map**
Vector Retrieval -> Cross-Encoder Reranking -> Answer Generation -> User Interface

**Critical Path**
Query Processing -> Vector Embedding Generation -> Hybrid Search with Metadata Filtering -> Initial Retrieval -> Cross-Encoder Reranking -> Final Answer Generation

**Design Tradeoffs**
The system balances precision (cross-encoder reranking) against computational cost, latency against accuracy (small-to-big retrieval), and semantic coverage against exact matching (hybrid search). The 68% win rate over hierarchical systems demonstrates that the additional complexity provides tangible benefits, but the marginal 0.78-second latency difference (5.2 vs 5.98 seconds) highlights the importance of optimization.

**Failure Signatures**
- High latency spikes indicate inefficient cross-encoder parameter settings
- Poor recall suggests inadequate metadata filtering or vector embedding quality
- Inconsistent answers across similar queries may indicate insufficient context preservation in chunking

**3 First Experiments**
1. Measure MRR@5 improvement when varying cross-encoder reranking parameters across different query types
2. Compare latency distributions for small-to-big retrieval versus single chunk size approaches
3. Evaluate cross-domain performance by applying the system to legal contracts versus SEC filings

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on SEC filings (10-K, 10-Q, 8-K) from the financial domain, limiting generalizability to other document types or industries
- The 150-question benchmark, while substantial, represents a single domain-specific dataset that may not capture full diversity of financial queries
- Latency measurements are reported as single values rather than distributions, limiting understanding of performance variability

## Confidence

**High confidence**: Vector-based agentic RAG achieving 68% win rate over hierarchical systems with comparable latency. Based on direct head-to-head comparisons across large document corpus.

**Medium confidence**: Cross-encoder reranking improvements (59% absolute MRR@5 improvement). Optimal parameter selection (10, 5) may be dataset-specific and not generalize across domains.

**Medium confidence**: Small-to-big chunk retrieval achieving 65% win rate with minimal latency increase. The 0.2-second additional latency may vary with document complexity and hardware configurations.

## Next Checks
1. Cross-domain validation: Test the vector-based agentic RAG system on non-financial document collections (legal contracts, medical records, technical documentation) to assess domain transferability of the 68% win rate.
2. Parameter sensitivity analysis: Conduct grid searches for cross-encoder reranking parameters across multiple datasets to determine if the (10, 5) optimal configuration is robust or dataset-specific.
3. Latency distribution profiling: Measure and report latency distributions (percentiles, variance) rather than point estimates to better understand performance consistency across varying query complexities and document sizes.