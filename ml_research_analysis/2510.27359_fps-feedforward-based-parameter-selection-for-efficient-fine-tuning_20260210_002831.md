---
ver: rpa2
title: 'FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning'
arxiv_id: '2510.27359'
source_url: https://arxiv.org/abs/2510.27359
tags:
- selection
- parameter
- parameters
- fine-tuning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the memory and efficiency bottleneck in selection-based
  parameter-efficient fine-tuning (PEFT) methods, particularly GPS, which requires
  a full backward pass for parameter selection, leading to high memory usage. The
  authors propose FPS (Feedforward-based Parameter Selection), a gradient-free approach
  that ranks parameters by the product of their magnitudes and corresponding input
  activations during a single forward pass.
---

# FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2510.27359
- Source URL: https://arxiv.org/abs/2510.27359
- Reference count: 34
- Primary result: Memory-efficient parameter selection for PEFT via feedforward pass without gradients

## Executive Summary
This paper addresses the memory and efficiency bottleneck in selection-based parameter-efficient fine-tuning (PEFT) methods, particularly GPS, which requires a full backward pass for parameter selection. The authors propose FPS (Feedforward-based Parameter Selection), a gradient-free approach that ranks parameters by the product of their magnitudes and corresponding input activations during a single forward pass. Evaluated on 24 visual tasks using ViT-B/16, FPS achieves performance comparable to GPS while reducing peak memory usage by nearly 9× and accelerating parameter selection by about 2×.

## Method Summary
FPS computes importance scores for parameters without backpropagation by considering both pre-trained weight magnitudes and input activations during a single forward pass. For each weight w, the importance score is calculated as I(w) = |w| · E[|a|], where a represents input activation and the expectation is averaged over downstream samples. The method uses ℓ1-norm with neuron-level selection, identifying the top parameters per neuron to match the target budget (0.77% for FGVC tasks, 0.25% for VTAB-1k tasks). This joint consideration of pre-trained weights and downstream data properties enables FPS to identify important parameters efficiently.

## Key Results
- Achieves performance comparable to GPS baseline while reducing peak memory usage by nearly 9×
- Accelerates parameter selection process by approximately 2× through elimination of backward pass
- Maintains competitive accuracy across 24 visual tasks from FGVC and VTAB-1k benchmarks using ViT-B/16

## Why This Works (Mechanism)
FPS works by recognizing that parameter importance can be estimated from the interaction between pre-trained weight magnitudes and input activation patterns, without requiring gradient information. By computing I(w) = |w| · E[|a|], the method captures both the inherent importance of weights (through their magnitudes) and their relevance to the specific downstream task (through activation magnitudes). This feedforward-only approach eliminates the memory-intensive backward pass while still identifying parameters that will be most effective during fine-tuning.

## Foundational Learning
- **Vision Transformer (ViT)**: Transformer-based architecture for image classification using self-attention mechanisms - needed to understand the backbone model being fine-tuned
- **Parameter-Efficient Fine-Tuning (PEFT)**: Methods that update only a small subset of model parameters during adaptation - needed to contextualize the problem space
- **Selection-based PEFT**: Approaches that first select which parameters to update before fine-tuning - needed to understand GPS baseline and FPS contribution
- **Importance scoring mechanisms**: Methods for ranking parameters by their expected contribution to task performance - needed to grasp FPS's core innovation
- **Gradient-free optimization**: Techniques that avoid computing gradients during certain optimization phases - needed to understand FPS's efficiency gains

Quick checks: Verify ViT architecture matches ViT-B/16 specification; confirm PEFT budget percentages (0.25-0.77%) align with selection-based methods; validate importance score formula implementation matches I(w) = |w| · E[|a|].

## Architecture Onboarding

**Component Map**: Pre-trained ViT-B/16 -> Forward hook registration -> Single forward pass for activation capture -> Importance score computation (I(w) = |w| · E[|a|]) -> Parameter selection (top-k per neuron) -> Standard fine-tuning of selected parameters

**Critical Path**: The core innovation flows through: activation capture via forward hooks → importance score computation → parameter selection → fine-tuning. Each stage must execute correctly for FPS to deliver its promised efficiency gains.

**Design Tradeoffs**: FPS trades the potentially more precise gradient-based importance scores (used by GPS) for dramatic memory savings and speed improvements. The ℓ1-norm with neuron-level selection provides a good balance between parameter budget adherence and performance, though the paper shows other variants are possible.

**Failure Signatures**: 
- If parameter distribution across layers is highly uneven, selection may be suboptimal
- If too few samples are used for importance computation, scores may not generalize
- If neuron-level selection is implemented as layer-level, performance drops by ~0.1-0.4%

**3 First Experiments**:
1. Implement forward hook system to capture activations across all linear/conv layers during single forward pass
2. Verify parameter selection produces the correct total budget (0.25-0.77%) with appropriate distribution across layers
3. Compare ℓ1-norm vs ℓ2-norm and neuron-level vs layer-level selection variants to confirm optimal configuration

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Lacks complete experimental specifications, particularly fine-tuning hyperparameters and data sampling strategy
- Claims of "identical performance to GPS" require careful interpretation given 0.1-0.5% absolute accuracy differences across tasks
- Exact timing methodology for 2× speedup claim is not detailed in the paper

## Confidence

**High Confidence**: FPS successfully eliminates backpropagation during parameter selection, achieving substantial memory reduction (9×) while maintaining competitive accuracy on standard benchmarks

**Medium Confidence**: The proposed joint consideration of weight magnitudes and input activations is the primary driver of FPS's effectiveness, though the ablation showing ℓ1-norm + neuron-level selection as optimal suggests multiple factors contribute

**Medium Confidence**: The 2× speedup claim is plausible given single forward pass vs. full backward computation, but exact timing methodology is not detailed

## Next Checks

1. Verify parameter selection accuracy by reproducing the 0.25-0.77% budget constraint across all layers and tasks, checking distribution matches paper's methodology

2. Test sensitivity to sample size used for importance computation by varying from small subsets to full datasets and measuring impact on final accuracy

3. Implement and compare all four selection variants (ℓ1/ℓ2 × layer/neuron-level) to validate the claimed superiority of ℓ1-norm with neuron-level selection (~0.1-0.4% accuracy gain)