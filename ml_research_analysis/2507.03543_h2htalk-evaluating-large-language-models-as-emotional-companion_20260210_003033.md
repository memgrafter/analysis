---
ver: rpa2
title: 'H2HTalk: Evaluating Large Language Models as Emotional Companion'
arxiv_id: '2507.03543'
source_url: https://arxiv.org/abs/2507.03543
tags:
- arxiv
- emotional
- language
- large
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: H2HTalk introduces the first comprehensive benchmark for evaluating
  LLMs as emotional companions, featuring 4,650 scenarios across dialogue, recollection,
  and itinerary planning. The benchmark incorporates a Secure Attachment Persona module
  based on attachment theory to ensure safer interactions.
---

# H2HTalk: Evaluating Large Language Models as Emotional Companion

## Quick Facts
- **arXiv ID:** 2507.03543
- **Source URL:** https://arxiv.org/abs/2507.03543
- **Authors:** Boyang Wang; Yalun Wu; Hongcheng Guo; Zhoujun Li
- **Reference count:** 40
- **Primary result:** Introduces first comprehensive benchmark for evaluating LLMs as emotional companions with 4,650 scenarios across dialogue, recollection, and itinerary planning.

## Executive Summary
H2HTalk presents the first systematic benchmark for evaluating large language models as emotional companions, featuring 4,650 curated scenarios across three dimensions: dialogue interactions, memory-based recollection tasks, and multi-step itinerary planning. The benchmark incorporates a Secure Attachment Persona (SAP) module based on attachment theory to ensure safer interactions when users express distress or harmful intent. Testing 50 diverse LLMs revealed significant challenges in long-horizon planning and memory retention, with models struggling particularly when user needs are implicit or evolve mid-conversation.

The evaluation protocol combines lexical metrics (BLEU variants, ROUGE), embedding-based similarity (BGE-M3), and rubric-based GPT-4o judgments to provide a comprehensive assessment of companion capabilities. Without SAP, safety perception scores dropped from 4.8 to 3.2 and violation rates increased tenfold, demonstrating the critical importance of the attachment theory-based safety framework. Among open-source models, Qwen2.5-72B-Instruct-LoRA achieved the highest score (50.82), while DeepSeek-V2.5 topped all contenders with 54.47.

## Method Summary
The benchmark evaluates LLMs across three task dimensions: Companion Dialogue (2,514 samples), Companion Recollection (1,103 samples), and Companion Itinerary (1,033 samples). The unified evaluation score combines BLEU-1/2/3/4, ROUGE-1/L, and semantic similarity using BGE-M3 embeddings, with GPT-4o rubric-based scoring for complex itinerary tasks. The Secure Attachment Persona module implements attachment theory principles including boundary maintenance, emotional accessibility, self-regulation, and conflict resolution protocols. Evaluation was conducted using OpenCompass on 64 NVIDIA H800 GPUs across 50 LLMs ranging from 1.5B to 70B+ parameters, with human review triggered when composite scores fall below threshold τ=3.5.

## Key Results
- SAP implementation proves crucial for safety, with models without SAP showing a 33% reduction in safety perception and a tenfold increase in violation rates
- Qwen2.5-72B-Instruct-LoRA achieved the highest score (50.82) among open-source models, while DeepSeek-V2.5 topped all contenders with 54.47
- Performance gaps widen on tasks requiring long-horizon reasoning and state retention, with DeepSeek-V2.5 achieving 62.33 on Itinerary-Middle, exceeding the next-best open-source model by over ten points
- Models struggle significantly with implicit or evolving user directives, with performance degrading when user needs are not explicitly stated

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Secure Attachment Persona (SAP) module substantially improves safety in emotional companion interactions without degrading linguistic quality.
- **Mechanism:** SAP operationalizes attachment theory through structured psychological scaffolding—calibrated boundary maintenance, emotional accessibility, self-regulation algorithms, and conflict resolution protocols—that constrains model responses toward safer patterns when users express distress or harmful intent.
- **Core assumption:** Attachment theory principles that govern human caregiver relationships can be productively encoded as parametric constraints and prompting frameworks for LLM behavior.
- **Evidence anchors:**
  - [abstract] "SAP implementation proves crucial for safety, with models without SAP showing a 33% reduction in safety perception and a tenfold increase in violation rates."
  - [section 5, Table 3] BLEU-4 drops only 32.7→31.5 without SAP, but safety score falls 4.8→3.2 and violation rate rises 0.7%→7.1%
  - [corpus] SHIELD system paper (arXiv:2510.15891) confirms early-stage problematic behavior detection is critical but underaddressed in companion systems
- **Break condition:** If SAP's safety gains derive primarily from surface-level refusal patterns rather than genuine attachment-consistent reasoning, the mechanism would fail on novel edge cases not covered in training data.

### Mechanism 2
- **Claim:** Unified evaluation combining lexical metrics, semantic similarity, and rubric-based LLM judgment captures emotional companion capability more comprehensively than any single metric family.
- **Mechanism:** The composite score S = (1/7)(ΣBLEU-n + ROUGE-1 + ROUGE-L + SS) with GPT-4o rubric augmentation for complex tasks addresses the limitation that purely lexical metrics miss semantic appropriateness while purely embedding-based metrics miss factual accuracy.
- **Core assumption:** GPT-4o's rubric-based judgments align sufficiently with human expert assessment of emotional appropriateness to serve as a proxy in the evaluation loop.
- **Evidence anchors:**
  - [section 4.1] Formula explicitly combines four BLEU variants, two ROUGE metrics, and cosine similarity via BGE-M3 embeddings
  - [section 3.2] "Each entry undergoes verification by 20 independent evaluators, with majority voting resolving classification differences"
  - [corpus] MoodBench 1.0 (arXiv:2511.18926) similarly calls for multi-dimensional evaluation, noting field lacks systematic frameworks—validating this approach fills a gap
- **Break condition:** If GPT-4o exhibits systematic bias in evaluating certain emotional expression styles or cultural communication patterns, the JGPT-4o component would introduce systematic distortion.

### Mechanism 3
- **Claim:** Performance on recollection and itinerary tasks reveals fundamental architectural limitations in current LLMs' long-horizon state management that dialogue-only benchmarks miss.
- **Mechanism:** By requiring models to synthesize conversation history into coherent memories (Recollection Synthesis), update those memories (Refinement), and proactively reference them (Initialization)—plus plan multi-step activities (Itinerary Advanced)—the benchmark exposes working memory and temporal reasoning gaps invisible in single-turn evaluations.
- **Core assumption:** The recollection and itinerary tasks in H2HTalk adequately proxy real-world companion scenarios requiring extended temporal reasoning.
- **Evidence anchors:**
  - [section 4.2] "Performance gaps widen on tasks requiring long-horizon reasoning and state retention. DeepSeek-V2.5 achieves 62.33 on Itinerary-Middle, exceeding the next-best open-source model by over ten points."
  - [section 1, Fig 1] Example shows companion referencing "three weeks ago" conversation and mentioning independent workshop attendance
  - [corpus] MemoryBank paper (arXiv:2402.17753 per citations) addresses very long-term conversational memory—confirming this is recognized architectural challenge
- **Break condition:** If models achieve high scores through pattern-matching on training distribution rather than genuine temporal reasoning, benchmark performance would not transfer to novel long-horizon scenarios.

## Foundational Learning

- **Attachment Theory (Bowlby-Ainsworth framework)**
  - Why needed here: SAP module directly encodes secure base characteristics, emotional accessibility, and boundary maintenance from this framework. Without understanding secure vs. insecure attachment patterns, you cannot reason about why SAP produces its safety effects.
  - Quick check question: Can you explain why "secure base" behavior in a companion differs from purely agreeable responses?

- **Working Memory in Sequence Models**
  - Why needed here: Recollection tasks (Synthesis, Refinement, Initialization) test whether models maintain coherent state across conversations. Understanding token limits, context window management, and memory condensation strategies is prerequisite to interpreting benchmark results.
  - Quick check question: What happens to a 7B model's recollection performance when conversation history exceeds its effective context window?

- **Evaluation Metric Tradeoffs (Lexical vs. Semantic vs. Model-based)**
  - Why needed here: The unified protocol explicitly balances BLEU precision, ROUGE recall, BGE-M3 semantic similarity, and GPT-4o judgment. Understanding when each fails (e.g., BLEU on synonyms, embeddings on factual accuracy) is essential for interpreting scores.
  - Quick check question: Why would a response with perfect semantic similarity still receive a low safety score?

## Architecture Onboarding

- **Component map:** User Input → SAP Module → Companion Core → Task-Specific Processing → Unified Evaluator → Safety Threshold Check
- **Critical path:** SAP module implementation is the safety gate. Per Table 3, removing it causes 10x violation rate increase. All downstream evaluation assumes SAP is active.
- **Design tradeoffs:**
  - Larger models (72B) dominate recollection (86.42 on Synthesis) but 7B LoRA-adapted models achieve 65.43 on the same task at ~10x lower inference cost
  - GPT-4o rubric scoring adds evaluation cost but captures dimensions lexical metrics miss
  - Human evaluation trigger at τ=3.5 balances automation vs. safety coverage
- **Failure signatures:**
  - Safety perception drops without obvious linguistic degradation → SAP module likely bypassed or improperly configured
  - High dialogue scores but low recollection scores → context window or memory architecture issues
  - Large gap between basic and advanced itinerary scores → long-horizon planning failure, not retrieval issue
  - Inconsistent performance on implicit vs. explicit instructions → instruction-following rather than emotional reasoning gap
- **First 3 experiments:**
  1. **SAP ablation replication:** Run 33 high-risk scenarios with SAP enabled/disabled. Verify 4.8→3.2 safety perception drop and 0.7%→7.1% violation rate increase match paper claims before deploying to production.
  2. **Model scale sweep on recollection tasks:** Test 1.5B, 7B, 14B, 32B, 72B variants on Recollection-Initialization specifically. Paper shows 43.33→49.27→65.94 progression—verify your infrastructure reproduces this scaling curve.
  3. **Implicit instruction stress test:** Construct 20 scenarios with emotional intensity but minimal directives (per Fig. 7 "Implicit-Help" category). Establish baseline failure rate before optimizing instruction-following.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM architectures be improved to reliably detect and respond to implicit or evolving user directives during emotional exchanges?
- **Basis in paper:** [explicit] The authors state that models "struggle when user needs are implicit or evolve mid-conversation" and Fig. 7 explicitly shows poor performance in "Implicit-Help" and "Ambiguous" scenarios.
- **Why unresolved:** Current instruction-following capabilities depend heavily on explicit commands; the paper demonstrates that leading models still fail to infer covert needs in high-affect contexts.
- **What evidence would resolve it:** Development of models that maintain high performance on H2HTalk’s implicit scenarios without requiring explicit user prompts.

### Open Question 2
- **Question:** What specific mechanisms are required to resolve the persistent deficiency in long-horizon planning and memory retention for emotional companions?
- **Basis in paper:** [explicit] The Abstract and Conclusion identify "long-horizon planning and memory retention" as "key challenges," noting that even large models struggle with state retention over time.
- **Why unresolved:** The paper highlights this as a general failing across the 50 evaluated models, but the evaluation results suggest current fine-tuning and context window strategies are insufficient for sustained itinerary and recollection tasks.
- **What evidence would resolve it:** A model architecture that achieves near-perfect scores on the "Itinerary-Advance" and "Recollection-Initialization" subtasks, demonstrating state retention over extended interactions.

### Open Question 3
- **Question:** Does high performance on the H2HTalk benchmark correlate with actual user safety and satisfaction in real-world, long-term deployments?
- **Basis in paper:** [inferred] The dataset relies on simulations and curated scenarios (Section 3.2) rather than longitudinal studies with real users, leaving the ecological validity of the "Secure Attachment Persona" unconfirmed outside of benchmark metrics.
- **Why unresolved:** While the benchmark shows SAP reduces *annotator-identified* violation rates, it remains unproven if these benchmark scores translate to safer outcomes in live, unscripted human-AI relationships.
- **What evidence would resolve it:** Longitudinal user studies comparing H2HTalk-high-scoring models against lower-scoring ones to measure real-world safety incidents and attachment security.

## Limitations

- SAP module implementation details remain underspecified despite being central to safety outcomes—the paper describes attachment theory principles but not the actual prompt engineering or architectural scaffolding
- The reliance on GPT-4o for rubric-based judgments introduces a potential circular dependency where evaluation quality depends on the same LLM family being assessed
- The 4,650-scenario dataset, while diverse, may not capture edge cases involving complex trauma responses or cultural variations in emotional expression that would stress-test the SAP module's generalizability

## Confidence

- **High confidence:** The benchmark methodology combining lexical, semantic, and rubric-based metrics is methodologically sound and addresses known limitations in single-metric evaluations
- **Medium confidence:** The reported safety improvements from SAP are likely real but the exact magnitude depends on implementation details not fully disclosed
- **Low confidence:** Claims about architectural limitations revealed by recollection/itinerary tasks should be interpreted cautiously—models may be pattern-matching rather than demonstrating genuine temporal reasoning failures

## Next Checks

1. **SAP implementation audit:** Request and verify the exact SAP module implementation code and prompts to ensure the 33% safety perception reduction and 10x violation rate increase are reproducible
2. **Cross-cultural robustness test:** Evaluate SAP performance on scenarios involving non-Western emotional expression patterns and trauma responses not represented in the current dataset
3. **Long-term deployment monitoring:** Implement continuous evaluation of SAP performance over extended conversations (>10,000 tokens) to detect degradation in boundary maintenance and emotional regulation that may not appear in the current benchmark structure