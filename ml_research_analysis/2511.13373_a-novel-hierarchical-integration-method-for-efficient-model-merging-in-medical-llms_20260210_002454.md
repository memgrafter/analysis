---
ver: rpa2
title: A Novel Hierarchical Integration Method for Efficient Model Merging in Medical
  LLMs
arxiv_id: '2511.13373'
source_url: https://arxiv.org/abs/2511.13373
tags:
- medical
- merging
- base
- task
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic comparison of six model merging
  techniques for combining medical LLMs derived from a common base model. The study
  evaluates simple averaging methods (Task Arithmetic, Linear Averaging) against complex
  approaches (DARE-TIES, DELLA, Breadcrumbs, and a novel Hierarchical Cosine-OT-LERP
  method) across five medical benchmarks.
---

# A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs

## Quick Facts
- arXiv ID: 2511.13373
- Source URL: https://arxiv.org/abs/2511.13373
- Reference count: 32
- Primary result: Task Arithmetic achieves 45.80% accuracy on MedQA, outperforming complex pruning-based methods by 9.35 percentage points.

## Executive Summary
This paper presents a systematic comparison of six model merging techniques for combining medical LLMs derived from a common base model. The study evaluates simple averaging methods (Task Arithmetic, Linear Averaging) against complex approaches (DARE-TIES, DELLA, Breadcrumbs, and a novel Hierarchical Cosine-OT-LERP method) across five medical benchmarks. The key finding is that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA and outperforming more complex pruning-based approaches. The Hierarchical method integrates task-vector similarity with selective attention head alignment but does not surpass simple averaging. These results demonstrate that for compatible medical LLMs, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation in resource-constrained healthcare environments.

## Method Summary
The paper compares six parameter-space merging techniques for combining two architecturally compatible medical LLMs (BioMistral and TachyHealth) derived from Mistral-7B-Instruct-v0.1. The methods include Linear Average, Task Arithmetic, DARE-TIES, DELLA, Breadcrumbs, and a novel Hierarchical Cosine-OT-LERP approach. Task vectors are computed as the difference between fine-tuned models and their shared base. The Hierarchical method employs optimal transport to align attention heads based on cosine similarity before merging. Evaluation uses 5-shot accuracy on five medical benchmarks (MedQA, PubMedQA, MMLU Professional Medicine, MedMCQA, HellaSwag) with bfloat16 precision on NVIDIA A100 40GB.

## Key Results
- Task Arithmetic achieves 45.80% accuracy on MedQA, outperforming DARE-TIES by 9.35 percentage points
- Simple averaging methods (Task Arithmetic, Linear) consistently outperform complex pruning-based approaches (DARE-TIES: 36.45%, DELLA: 38.23%, Breadcrumbs: 39.12%)
- The Hierarchical Cosine-OT-LERP method prevents catastrophic interference but does not surpass simple averaging performance
- Base model accuracy was 42.97%, with parent models achieving 44.56% (BioMistral) and 44.82% (TachyHealth)

## Why This Works (Mechanism)

### Mechanism 1: Loss Basin Proximity and Implicit Regularization
Simple averaging (Task Arithmetic/Linear) outperformed complex pruning methods because the parent models occupied the same loss basin, allowing weight interpolation to act as implicit regularization rather than destructive interference. Parent models (BioMistral and TachyHealth) derived from an identical base (Mistral-7B) likely reside in close proximity in the parameter space. Averaging these weights cancels out training-specific noise while reinforcing shared, robust features, whereas complex methods designed to resolve high-conflict interference introduce unnecessary distortion.

### Mechanism 2: Pruning-Induced Information Loss
The severe underperformance of DARE-TIES (36.45% accuracy) suggests that default density hyperparameters (0.6) destroyed crucial fine-tuned knowledge by treating small weights as noise. Sparsity-based methods randomly prune delta parameters. In specialized medical fine-tunes, small-magnitude weights often represent subtle domain adaptations rather than noise. Aggressive pruning (dropping 40% of weights) removes this signal, causing the merged model to regress below even the parent models.

### Mechanism 3: Selective Optimal Transport for Permutation Variance
The proposed Hierarchical Cosine-OT-LERP method addresses permutation variance in attention heads by aligning functionally similar subspaces before merging. The method computes a cost matrix based on cosine similarity between attention heads and uses the Hungarian algorithm (Linear Sum Assignment) to find the optimal permutation. This aligns heads in Model B to Model A before interpolation, preventing the averaging of unrelated feature detectors.

## Foundational Learning

- **Concept: Task Vectors**
  - Why needed: The dominant merging technique (Task Arithmetic) operates by adding/subtracting the "delta" between a fine-tuned model and its base. You cannot understand the results without understanding that $\theta_{merged} = \theta_{base} + \Delta_A + \Delta_B$.
  - Quick check: If Model A was fine-tuned for radiology and Model B for cardiology from the same base, what does the vector $\Delta_A$ mathematically represent?

- **Concept: Permutation Variance**
  - Why needed: The paper's novel contribution involves Optimal Transport to fix permutation variance. This relies on the concept that Neuron $i$ in Model A might perform the same function as Neuron $j$ in Model B, but direct averaging mixes them incorrectly.
  - Quick check: Why does simply averaging the weights of two functionally identical models potentially result in a broken model?

- **Concept: Hyperparameter Sensitivity (Density)**
  - Why needed: A key failure mode identified was the "density" parameter in pruning methods. Understanding that this controls the percentage of weights retained is critical to diagnosing why DARE-TIES failed.
  - Quick check: In the context of DARE-TIES with density=0.6, what specific mathematical operation is performed on the task vector $\Delta$ before merging?

## Architecture Onboarding

- **Component map:** Base Model (Mistral-7B) -> Parent A (BioMistral) -> Parent B (TachyHealth) -> Merge Engine (6 methods) -> Evaluation Harness (5-shot, bfloat16)

- **Critical path:**
  1. **Compatibility Check:** Verify full checkpoints (not LoRA adapters) and identical architecture layers
  2. **Task Vector Computation:** Calculate $\Delta = \theta_{finetune} - \theta_{base}$
  3. **Alignment (if Hierarchical):** Compute Cost Matrix -> Hungarian Algorithm -> Permute heads
  4. **Interpolation:** Merge parameters using defined weights ($\alpha$ or cosine similarity)

- **Design tradeoffs:**
  - **Simple (Task Arithmetic):** O(P) complexity. Maximum efficiency. Best empirical results in this study
  - **Complex (Hierarchical OT):** O(L·H³) complexity. High overhead. Theoretically robust to permutation issues but empirically equivalent or slightly worse here due to existing model compatibility

- **Failure signatures:**
  - **Catastrophic Drop:** Accuracy < Base Model (e.g., DARE-TIES @ 36.45% vs Base @ 42.97%). Indicates destructive interference or excessive pruning
  - **Format Incompatibility:** "Configuration errors" or "loading incompatibilities" (as noted with SLERP/SCE attempts)

- **First 3 experiments:**
  1. **Sanity Check (Linear Average):** Run Linear Averaging with $\alpha=0.5$ to establish a robustness baseline. If this fails, check data/architecture integrity
  2. **Pruning Ablation:** Run DARE-TIES with default density (0.6) vs. high density (0.95) to validate the hypothesis that aggressive pruning destroys medical knowledge
  3. **Domain Divergence:** Merge two less-related models (e.g., BioMistral + General-Instruct) to test if the "Simple is Better" hypothesis holds when parameter conflict is actually high

## Open Questions the Paper Calls Out

### Open Question 1
Does the superiority of simple averaging over complex merging methods generalize to larger model scales (13B, 70B parameters) and different architectures (Llama 2/3, Qwen)? All experiments used Mistral-7B exclusively; findings may be architecture-specific or scale-dependent.

### Open Question 2
Would DARE-TIES and other pruning-based methods achieve competitive performance with higher density hyperparameters or automated tuning? Only default density (0.6) was systematically evaluated; the paper suggests higher densities (>0.9) may improve performance but this was not empirically validated.

### Open Question 3
How do merging strategies compare when combining models from truly disjoint medical specialties or non-medical domains? Both parent models (BioMistral and TachyHealth) operate within the medical domain, exhibiting high compatibility. The paper suggests merging truly disjoint domains would likely introduce significant parameter conflicts.

### Open Question 4
Can lightweight compatibility metrics predict merge success and detect potential safety degradation before merging? The paper calls for incorporating safety objectives through alignment-aware loss functions and lightweight compatibility metrics (weight orthogonality, activation correlation) to predict model "mergeability."

## Limitations
- Results are specific to merging architecturally compatible medical LLMs from the same base model and may not generalize to different architectures or highly divergent domains
- Hyperparameter sensitivity analysis for pruning methods was limited to default density values, leaving open questions about optimal tuning
- The computational overhead of the novel Hierarchical method (O(L·H³)) may be prohibitive for resource-constrained healthcare environments despite preventing catastrophic interference

## Confidence
- **High Confidence**: Task Arithmetic outperforms complex pruning-based methods for merging architecturally compatible medical LLMs from Mistral-7B
- **Medium Confidence**: Simple averaging superiority attributed to low parameter conflict and loss basin proximity, though this relies on geometric assumptions not directly measured
- **Medium Confidence**: Pruning methods require higher density hyperparameters (>0.9) for successful merging of high-quality medical fine-tunes, inferred but not directly validated
- **Low Confidence**: Hierarchical Cosine-OT-LERP method's practical utility given its high computational cost without superior performance over simple averaging

## Next Checks
1. **Cross-Domain Merging Experiment**: Validate the "simple averaging is best" hypothesis by merging a medical LLM (BioMistral) with a non-medical LLM (CodeLlama) from a different base architecture to test whether low parameter conflict assumption holds outside the narrow experimental design.

2. **Density Hyperparameter Sweep for Pruning Methods**: Conduct systematic ablation study of density hyperparameter for DARE-TIES and DELLA at densities ranging from 0.6 to 0.99 to determine optimal pruning level for preserving medical knowledge.

3. **Permutation Variance Impact Assessment**: Design experiment to quantify impact of permutation variance by intentionally permuting attention heads of one parent model before merging with Task Arithmetic, then comparing to aligned merge using Hierarchical method.