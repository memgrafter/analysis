---
ver: rpa2
title: 'ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning'
arxiv_id: '2510.15211'
source_url: https://arxiv.org/abs/2510.15211
tags:
- reasoning
- instruction
- lrms
- instructions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReasonIF, a benchmark for evaluating whether
  large reasoning models follow user instructions during their reasoning traces, not
  just in final answers. The authors find that state-of-the-art LRMs exhibit significantly
  lower instruction-following compliance in reasoning traces compared to main responses,
  with the highest instruction-following score remaining below 0.25.
---

# ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning

## Quick Facts
- arXiv ID: 2510.15211
- Source URL: https://arxiv.org/abs/2510.15211
- Reference count: 40
- Large reasoning models show significantly lower instruction-following compliance in reasoning traces (max score <0.25) compared to final answers

## Executive Summary
This paper introduces ReasonIF, a benchmark designed to evaluate whether large reasoning models (LRMs) follow user instructions during their reasoning traces, not just in final answers. The authors systematically assess instruction-following compliance across different reasoning model families and find a significant gap between trace-level and response-level instruction adherence. They demonstrate that reasoning instruction adherence degrades with increasing task difficulty and propose two mitigation strategies: multi-turn reasoning and reasoning instruction finetuning (RIF). Their experiments show that RIF using synthetic data can improve instruction-following scores, though performance remains below optimal levels.

## Method Summary
The authors construct the ReasonIF benchmark by selecting instruction types (format, style, constraints, hints) and extracting corresponding prompts from GSM8K, MATH, and SVAMP datasets. They employ an LLM-as-a-judge approach to score instruction-following in both reasoning traces and final responses on a 0-1 scale. The evaluation is conducted across 15 open and proprietary LRMs, with further analysis on GPT-OSS-20B to examine the effect of task difficulty and finetuning strategies. The RIF approach involves creating synthetic data through prompt engineering and fine-tuning the base model on this data to improve instruction-following compliance during reasoning.

## Key Results
- State-of-the-art LRMs exhibit significantly lower instruction-following compliance in reasoning traces (max score <0.25) compared to main responses
- Instruction-following compliance degrades as task difficulty increases
- RIF finetuning using synthetic data improves GPT-OSS-20B's instruction-following score from 0.11 to 0.27

## Why This Works (Mechanism)
The paper demonstrates that LRMs can generate correct final answers while failing to follow instructions in their intermediate reasoning steps. This occurs because LRMs prioritize logical correctness over instruction compliance during step-by-step reasoning. The mechanism behind this behavior suggests that the reasoning process operates with different objectives than the final response generation, leading to a disconnect between trace-level and response-level instruction adherence.

## Foundational Learning

**Large Reasoning Models (LRMs)**: AI models designed to generate step-by-step reasoning traces before producing final answers. Why needed: Understanding the architecture is crucial for evaluating instruction-following in intermediate reasoning steps. Quick check: Verify that the model generates explicit reasoning traces before final answers.

**Instruction Following**: The ability of models to comply with specific user directives regarding format, style, or constraints. Why needed: This is the core capability being evaluated in the benchmark. Quick check: Confirm that instructions are clearly specified and measurable.

**Synthetic Data Generation**: Creating training data programmatically through prompt engineering and model generation. Why needed: Essential for the RIF approach to improve instruction-following without manual annotation. Quick check: Validate that synthetic examples maintain instruction-following patterns.

**Task Difficulty Gradient**: The relationship between problem complexity and model performance. Why needed: Understanding how difficulty affects instruction compliance reveals model limitations. Quick check: Verify that task difficulty is properly calibrated and measurable.

## Architecture Onboarding

**Component Map**: User Prompt -> LRM Reasoning Engine -> Reasoning Traces -> Final Answer -> Instruction Compliance Scoring
**Critical Path**: Prompt generation → LRM inference → Trace extraction → Binary compliance classification → Scoring
**Design Tradeoffs**: Binary scoring vs. nuanced compliance levels; synthetic data vs. human-annotated data; model-specific vs. general instruction types
**Failure Signatures**: Correct final answers with non-compliant reasoning traces; degradation of compliance with task difficulty; inconsistent compliance across instruction types
**First Experiments**: 1) Compare instruction-following scores between reasoning traces and final answers across model families. 2) Test RIF finetuning on a single model with controlled instruction types. 3) Measure instruction-following degradation across a difficulty gradient.

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation methodology relies on binary classification which may oversimplify nuanced instruction compliance
- Benchmark's instruction set is limited in size and may not capture all real-world instruction scenarios
- RIF finetuning based on synthetic data could introduce distribution shifts or overfitting to synthetic patterns

## Confidence

- High confidence in observation that LRMs show lower instruction-following compliance in reasoning traces vs. final answers
- Medium confidence in degradation trend with increasing task difficulty
- Low confidence in generalization of RIF effectiveness across models and instruction types

## Next Checks

1. Evaluate RIF-trained models on out-of-distribution instructions not seen during synthetic data generation to test generalization
2. Conduct ablation studies comparing instruction-following compliance when reasoning traces are visible versus hidden to users
3. Test whether human evaluators agree with the binary classification scheme used to score instruction-following, particularly for borderline cases