---
ver: rpa2
title: 'CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal
  Fusion Method for Timestamped QA'
arxiv_id: '2512.00360'
source_url: https://arxiv.org/abs/2512.00360
tags:
- retrieval
- reranker
- learned
- clip
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CourseTimeQA is a lecture-video benchmark (52.3 h, 902 queries)
  for timestamped QA under single-GPU constraints. CrossFusion-RAG improves nDCG@10
  by 0.10 and MRR by 0.08 over BLIP-2 retrieval while maintaining ~1.55 s median latency
  on an A100.
---

# CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA

## Quick Facts
- arXiv ID: 2512.00360
- Source URL: https://arxiv.org/abs/2512.00360
- Reference count: 25
- Key outcome: CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over BLIP-2 retrieval while maintaining ~1.55 s median latency on an A100

## Executive Summary
CourseTimeQA introduces a new benchmark for timestamped question answering on lecture videos, featuring 52.3 hours of content and 902 queries. The benchmark emphasizes practical deployment constraints, requiring single-GPU inference with strict latency budgets. CrossFusion-RAG is proposed as a cross-modal fusion method that improves retrieval accuracy while meeting these constraints. The method combines frozen visual and textual encoders with learned cross-attention fusion and a small reranker, achieving significant performance gains over baseline approaches.

## Method Summary
CrossFusion-RAG is a latency-constrained cross-modal fusion method designed for timestamped QA on lecture videos. It uses frozen BLIP-2 encoders for visual and textual feature extraction, reducing computational overhead. A learned vision projection layer aligns visual features with textual space, while a query-agnostic cross-attention fusion layer combines modalities efficiently. The method includes a small cross-attentive reranker with temporal consistency regularization to improve answer positioning accuracy. All components are optimized for single-GPU inference under strict latency constraints.

## Key Results
- CrossFusion-RAG achieves nDCG@10 improvement of 0.10 over BLIP-2 retrieval baseline
- MRR improves by 0.08 compared to baseline method
- Median inference latency maintained at ~1.55 seconds on A100 GPU
- Benchmark supports leave-one-course cross-validation methodology

## Why This Works (Mechanism)
The method works by efficiently fusing visual and textual information through learned projections and cross-attention mechanisms while maintaining strict latency constraints. The use of frozen encoders reduces computational overhead, allowing more resources for the fusion and reranking components. The query-agnostic fusion layer processes all candidates simultaneously, improving efficiency. Temporal consistency regularization helps maintain coherent answer positioning across related queries, improving overall accuracy.

## Foundational Learning

**Cross-modal retrieval**: Combines visual and textual information for joint search - needed for understanding lecture content that includes slides and spoken words; quick check: verify visual-textual feature alignment.

**Temporal consistency**: Ensures coherent answer positioning across related queries - needed for maintaining logical flow in lecture sequences; quick check: measure answer position stability across similar questions.

**Single-GPU optimization**: Designs models to fit within GPU memory constraints - needed for practical deployment scenarios; quick check: monitor GPU memory usage during inference.

## Architecture Onboarding

**Component map**: Lecture video -> BLIP-2 visual encoder -> Learned vision projection -> Cross-attention fusion <- BLIP-2 text encoder -> Small reranker -> Final answers

**Critical path**: Input video frames and transcript → Frozen BLIP-2 encoders → Learned vision projection → Query-agnostic cross-attention fusion → Cross-attentive reranker → Timestamped answers

**Design tradeoffs**: Freezes encoders to save computation, accepts potential domain mismatch; uses query-agnostic fusion for efficiency, may miss query-specific nuances; small reranker limits accuracy gains but maintains latency constraints.

**Failure signatures**: Degraded performance on domain-specific terminology not captured by frozen encoders; reduced accuracy for complex multi-modal reasoning requiring fine-tuned components; potential temporal inconsistency in answer positioning for complex lecture sequences.

**First experiments**: 1) Measure latency breakdown across components to identify bottlenecks; 2) Test retrieval accuracy with and without vision projection layer; 3) Evaluate temporal consistency regularization impact on answer positioning stability.

## Open Questions the Paper Calls Out
None

## Limitations
- Single-GPU constraint may limit model complexity and potential accuracy gains
- BLIP-2 frozen encoders may not capture domain-specific lecture content effectively
- Benchmark limited to course lecture domain, limiting generalizability to other video types
- Temporal consistency regularization may not capture complex temporal dependencies in longer sequences

## Confidence

**High confidence**: Latency measurements and hardware specifications are objective metrics that can be precisely measured.

**Medium confidence**: Performance improvements (nDCG@10 +0.10, MRR +0.08) depend on implementation quality and baseline comparison methodology.

**Medium confidence**: Benchmark representativeness for real-world lecture video QA tasks given limited diversity of courses and lecture styles.

## Next Checks
1. Evaluate CrossFusion-RAG on diverse video QA datasets (How2, TVQA, instructional videos) to assess domain generalization.

2. Perform ablation studies to isolate contributions of learned vision projection, cross-attention fusion, and reranker components.

3. Test scalability with varying GPU memory constraints and compare against memory-intensive alternatives for non-latency-constrained settings.