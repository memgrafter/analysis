---
ver: rpa2
title: 'VILP: Imitation Learning with Latent Video Planning'
arxiv_id: '2502.01784'
source_url: https://arxiv.org/abs/2502.01784
tags:
- video
- vilp
- planning
- generation
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VILP (Imitation Learning with Latent Video
  Planning), a novel approach that integrates video generation models into robotics
  policies. VILP uses a latent video diffusion model to generate predictive robot
  videos that maintain temporal consistency, addressing the challenge of bridging
  video data and robot actions.
---

# VILP: Imitation Learning with Latent Video Planning

## Quick Facts
- arXiv ID: 2502.01784
- Source URL: https://arxiv.org/abs/2502.01784
- Authors: Zhengtong Xu; Qiang Qiu; Yu She
- Reference count: 30
- Key outcome: VILP outperforms existing methods like UniPi across multiple metrics including training costs, inference speed, temporal consistency, and task success rates.

## Executive Summary
This paper presents VILP (Imitation Learning with Latent Video Planning), a novel approach that integrates video generation models into robotics policies. VILP uses a latent video diffusion model to generate predictive robot videos that maintain temporal consistency, addressing the challenge of bridging video data and robot actions. The method employs a UNet model with 3D convolution layers and cross-attention conditioning to handle multi-view observations, enabling highly time-aligned video generation from multiple perspectives. VILP demonstrates real-time receding horizon planning capabilities, achieving inference speeds of approximately 14 Hz for 5-frame videos at 96x160 resolution.

## Method Summary
VILP operates in latent space using a VQGAN autoencoder to compress images before diffusion, reducing computational complexity by ~8x in spatial dimensions. A 3D UNet with cross-attention conditioning maps observations to denoised latent videos, which are decoded to frames. A lightweight CNN+MLP policy maps adjacent frames to action sequences, and receding horizon control executes only the first actions before replanning. The approach decouples video planning from action mapping, enabling data-efficient policy learning with reduced action-labeled requirements.

## Key Results
- VILP achieves inference speeds of approximately 14 Hz for 5-frame videos at 96x160 resolution
- VILP-4 achieves 0.058s inference vs UniPi-64 at 2.5s with comparable FVD scores
- VILP-4 with Hybrid data achieves 56.7% success vs Diffusion Policy's 48.0% on Nut-Assembly

## Why This Works (Mechanism)

### Mechanism 1
Operating diffusion in latent space rather than pixel space enables real-time inference speeds. VQGAN autoencoder compresses images (e.g., 96x160 pixels → 12x20 latent) before diffusion. The denoising UNet operates on this compressed representation, reducing computational complexity by ~8x in spatial dimensions. DDIM sampling with 4-16 steps completes in 0.058-0.23s vs. UniPi's 0.14-2.5s.

### Mechanism 2
Cross-attention conditioning globally integrates observation context, improving temporal consistency across multi-view generation. Visual encoders map each camera view to a low-dimensional vector ct. Cross-attention layers inject ct into intermediate UNet representations via Q/K/V attention, allowing the model to condition on all views simultaneously. Separate diffusion models per view share the same fused conditioning, enforcing temporal alignment.

### Mechanism 3
Decoupling video planning from action mapping enables data-efficient policy learning with reduced action-labeled requirements. Video diffusion model learns temporal dynamics and task structure from video-only data. A lightweight CNN+MLP module maps adjacent predicted frames to action sequences. "Hybrid" datasets combine limited high-quality action data with lower-quality or off-target video demonstrations.

## Foundational Learning

- **DDIM (Denoising Diffusion Implicit Models)**: Enables faster sampling (4-16 steps) than standard diffusion while maintaining quality; critical for real-time robotics. Quick check: Can you explain why DDIM allows fewer denoising steps than DDPM?
- **VQGAN Autoencoder**: Provides perceptually lossy compression to latent space; decoder must reconstruct video frames that preserve object positions. Quick check: How does the perceptual loss differ from pixel-wise L2, and why does it matter for robotics?
- **Receding Horizon Control**: Executes only first Ne actions from predicted sequence, then replans; prevents error accumulation from long-horizon predictions. Quick check: What happens to performance if action horizon is set too large relative to video prediction accuracy?

## Architecture Onboarding

- **Component map**: VQGAN Encoder E: Image → Latent → VQGAN Decoder D: Latent → Image; Visual Encoders: Observation → Conditioning vector ct; 3D UNet ϵθ: Noisy latent + conditioning → Denoised latent video; Low-level Policy: Adjacent frames → Action sequence
- **Critical path**: Observation → Visual Encoder → ct → Cross-Attention in UNet → Latent Video → Decoder → Frames → Low-level Policy → Actions
- **Design tradeoffs**: More denoising steps → better video quality but slower inference (VILP-16: 0.231s vs VILP-4: 0.073s); Larger video horizon → longer action sequences possible but higher inference cost and potential drift; Separate models per view → better temporal alignment but 2-3x training cost
- **Failure signatures**: Temporal inconsistency (flickering objects): Check cross-attention fusion of multi-view conditioning; Low success rate with good FVD: Investigate low-level policy training data coverage; Slow inference despite latent diffusion: Verify VQGAN compression ratio and UNet channel configuration
- **First 3 experiments**:
  1. Validate VQGAN reconstruction on task images: Encode/decode single frames, measure perceptual similarity and object position preservation.
  2. Ablate conditioning mechanism: Compare cross-attention vs. conditional concatenation on a single-view task using FVD and inference time.
  3. Profile end-to-end latency: Measure each component (encoding, diffusion steps, decoding, action mapping) to identify bottleneck before scaling to multi-view.

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified robot video generation model be developed to handle diverse camera views and distinct tasks using view-invariant representations? The current VILP implementation requires training a separate diffusion model for each specific view and relies on dataset-specific training.

### Open Question 2
Is it possible to create a universal video-to-action mapping that allows web-scale video generation models (like Sora) to act as standalone planners without task-specific action labels? VILP currently requires a low-level policy trained on some action-labeled data to map video frames to robot actions.

### Open Question 3
Can video pretraining effectively capture multi-modal action distributions from cross-domain videos for longer-horizon tasks? The experiments focused on shorter-horizon tasks and the authors note that longer-horizon tasks exhibit substantial task-level multi-modality that has not been fully tested.

## Limitations

- **VQGAN compression quality**: The perceptual compression may discard fine-grained geometric details necessary for precise manipulation tasks, with unvalidated impact on millimeter-precision tasks
- **Multi-view temporal alignment**: The cross-attention mechanism ensures temporal consistency across views during generation, but the ablation study only tested single-view tasks
- **Data efficiency claims**: While the method requires less high-quality action-labeled data, the experimental comparison doesn't clearly isolate the contribution of the video generation component versus the action mapping component

## Confidence

- **High confidence**: The latent diffusion architecture enables real-time inference through spatial compression; receding horizon planning prevents error accumulation; cross-attention conditioning improves temporal consistency in ablation studies
- **Medium confidence**: Video generation quality (FVD/FID) correlates with task success; multi-view temporal alignment generalizes beyond tested scenarios; data efficiency gains are robust across task types
- **Low confidence**: The exact contribution of video generation versus action mapping to overall performance; scalability to more complex manipulation tasks; robustness to significant domain shift between training videos and deployment environments

## Next Checks

1. **Precision validation**: Test VILP on a task requiring sub-centimeter accuracy (e.g., peg-in-hole with tight tolerance) to quantify the impact of VQGAN compression on geometric fidelity
2. **Multi-view robustness**: Evaluate VILP with intentionally misaligned camera views or significant occlusion to test cross-attention temporal alignment under challenging conditions
3. **Component ablation**: Systematically vary the quality and quantity of action-labeled data while keeping video generation fixed to isolate the data efficiency contribution of the video planning component