---
ver: rpa2
title: 'Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate
  Leverage Scores'
arxiv_id: '2507.08143'
source_url: https://arxiv.org/abs/2507.08143
tags:
- arxiv
- compression
- scores
- performance
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Compactor is a training-free, query-agnostic KV cache compression
  method that uses approximate leverage scores to determine token importance. It achieves
  the same performance as competing methods while retaining 20% fewer tokens in both
  synthetic and real-world context tasks.
---

# Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores

## Quick Facts
- **arXiv ID:** 2507.08143
- **Source URL:** https://arxiv.org/abs/2507.08143
- **Reference count:** 27
- **Primary result:** Achieves full KV performance while reducing KV memory burden by 68% on average on Longbench tasks

## Executive Summary
Compactor introduces a training-free, query-agnostic KV cache compression method that uses approximate leverage scores to determine token importance. It achieves the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks. The method introduces context-calibrated compression, which infers the maximum compression a given context supports before significant performance loss. Compactor was evaluated across 27 synthetic and real-world tasks from RULER and Longbench using Qwen 2.5 and Llama 3.1 models.

## Method Summary
Compactor computes approximate leverage scores via Gaussian right-sketching and SVD on the Key matrix, combined with non-causal attention scores computed in chunks. These scores are normalized and blended (λ=0.3) to identify tokens for eviction. The method includes context-calibrated compression that uses NLL ratios to automatically determine optimal retention rates. The paper releases compactor-vllm with optimized Triton kernels for sparse, non-contiguous memory access patterns.

## Key Results
- Achieves full KV performance while reducing memory burden by 68% on average on Longbench
- Retains 20% fewer tokens than competing methods while maintaining equivalent performance
- Works across 27 synthetic and real-world tasks from RULER and Longbench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Compactor identifies tokens with high "outlier" properties via statistical leverage scores. It computes approximate leverage scores for the Key matrix using Gaussian right-sketching and SVD, detecting tokens that lie in directions of high variance within the key space, independent of query patterns. This preserves unique information that standard attention-based eviction discards.

### Mechanism 2
The method uses non-causal (bidirectional) attention scores to reveal structural "anchor" tokens. Instead of summing attention weights only from preceding tokens, it computes attention over chunks without the causal mask to find tokens that act as structural separators for the entire context block.

### Mechanism 3
Compactor infers the maximum allowable compression rate for a context by monitoring the change in Negative Log-Likelihood (NLL) of the context itself. It fits a parametric curve relating retention rate to NLL ratios, allowing the system to predict performance without knowing the query.

## Foundational Learning

- **Concept: Statistical Leverage Scores**
  - Why needed here: This is the mathematical core of Compactor, explaining why the method prioritizes "geometric outliers" over "frequently attended" tokens
  - Quick check question: If a row in a matrix K has a high leverage score, does it align more closely with the top principal components of K or the residual noise?

- **Concept: Query-Agnostic vs. Query-Aware Compression**
  - Why needed here: The paper critiques existing methods for failing in the "query-agnostic" regime, distinguishing when to apply Compactor vs. other methods
  - Quick check question: In a multi-turn conversation where the first prompt is long and shared by many users, why would a query-aware eviction policy fail to reuse the cached prefix for the second user?

- **Concept: The Key-Value (KV) Cache Bottleneck**
  - Why needed here: Understanding that memory scales linearly with sequence length N and that decoding is memory-bandwidth bound is critical for the problem statement
  - Quick check question: Why does the KV cache size depend on the sequence length N but the computational cost of attention depends on N² during prefill?

## Architecture Onboarding

- **Component map:** Input Key Matrix K → Outlier Scorer (sketch SVD) → Attention Scorer (chunked non-causal) → Score Blender (normalize + sum) → Evictor (top-k selection) → Calibrator (optional NLL monitoring)

- **Critical path:** The computation of the sketched SVD and the chunked attention matrix. While theoretically O(N), the SVD of the k×k matrix is a fixed overhead per layer.

- **Design tradeoffs:**
  - Sketch Dimension (k): Lower k speeds up SVD but loosens theoretical bounds; paper uses k=48 empirically
  - Lambda (λ): Balances geometric vs. task-driven importance; paper finds robustness between 0.1 and 0.4
  - Chunk Size (C): Larger chunks approximate full non-causal attention better but use more memory during scoring

- **Failure signatures:**
  - "Needle in Haystack" Drop: Sudden failure on retrieval tasks at low retention rates (<10%) where critical tokens are statistically similar to distractors
  - High Overhead on Short Contexts: Fixed SVD cost not amortized well on short sequences, making Compactor slower than baselines for N < 16k

- **First 3 experiments:**
  1. Verify Overhead vs. Context Length: Benchmark wall-clock time against FlashAttention at 4k, 16k, and 64k tokens
  2. Ablate Score Components: Run eviction on synthetic UUID retrieval task using only leverage scores and only attention scores
  3. Test Context-Calibration: Fit calibration curve on validation set, then test predicted retention rates on held-out Longbench tasks

## Open Questions the Paper Calls Out

### Open Question 1
How does Compactor's token retention strategy interact with aggressive KV cache quantization methods? The paper identifies quantization as an "orthogonal" compression dimension but restricts experiments to sequence-dimension pruning. It is unknown if outlier tokens identified by leverage scores require higher numerical precision.

### Open Question 2
Can the theoretical bounds for leverage score approximation be tightened to formally explain the efficacy of low-dimensional sketches? The authors note that k=48 "violates the bounds required for Theorem 2," yet empirically yields sufficient performance, suggesting current bounds are loose.

### Open Question 3
Does context-calibrated compression generalize robustly to domains significantly different from the calibration dataset? The calibration relies on fitting parameters to specific NLL ratios that may shift in structured languages where token importance differs from natural text.

## Limitations
- Assumes NLL distortion correlates with generation quality for arbitrary tasks, which may fail for "needle in haystack" retrieval
- Uses sketch dimension k=48 that violates theoretical bounds stated in Theorem 2
- Head-adaptive compression referenced from Feng et al. (2025) is used for main results but not fully described

## Confidence
- **High confidence:** Core mechanism combining leverage scores and non-causal attention scoring is well-founded theoretically
- **Medium confidence:** Calibration mechanism assuming NLL correlation with task performance across all domains
- **Medium confidence:** Practical overhead claims, as SVD computation is only amortized well on contexts longer than 4k tokens

## Next Checks
1. **Calibration Domain Transfer Test:** Train parameters on RULER tasks, then evaluate predicted retention rates on held-out Longbench tasks to test NLL correlation assumption
2. **Short Context Overhead Benchmark:** Measure wall-clock time and memory usage of Compactor versus SnapKV across 1k, 4k, 16k, and 64k tokens
3. **Edge Case Retrieval Performance:** Construct synthetic "needle in haystack" task with semantically unique but statistically similar tokens to identify exact retention threshold where retrieval fails