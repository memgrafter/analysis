---
ver: rpa2
title: 'FIBER: A Multilingual Evaluation Resource for Factual Inference Bias'
arxiv_id: '2512.11110'
source_url: https://arxiv.org/abs/2512.11110
tags:
- bias
- factual
- language
- languages
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIBER, a multilingual benchmark for evaluating
  factual knowledge in large language models (LLMs). It addresses the limitations
  of existing benchmarks by including both single- and multi-entity answers across
  English, Italian, and Turkish.
---

# FIBER: A Multilingual Evaluation Resource for Factual Inference Bias

## Quick Facts
- arXiv ID: 2512.11110
- Source URL: https://arxiv.org/abs/2512.11110
- Reference count: 0
- Models perform better on single-entity questions (up to 38% MAP in English) than multi-entity ones (as low as 12% MAP)

## Executive Summary
This paper introduces FIBER, a multilingual benchmark for evaluating factual knowledge in large language models (LLMs). It addresses limitations of existing benchmarks by including both single- and multi-entity answers across English, Italian, and Turkish. The dataset features sentence completion, question-answering, and object-count prediction tasks, enabling assessment of factual inference bias and performance differences between entity types. Using FIBER, the authors examine how prompt language affects entity selection and compare model performance on single- versus multi-entity questions.

## Method Summary
FIBER uses rank-based evaluation where models compute cumulative log-probabilities for all tokens in each candidate answer from a predefined surface set. For each prompt, the surface set is ranked by descending log-probability score, and Average Precision (AP) is computed based on gold answer positions. Factual Inference Bias is measured by calculating the proportion of language-specific entities appearing in top-n predictions. The evaluation covers three languages (English, Italian, Turkish) across 16 topics with both single-entity and multi-entity ground truth answers.

## Key Results
- 31% of topics exhibit factual inference bias, with Turkish prompts showing higher bias than Italian in 83% of cases
- Models perform better on single-entity questions (up to 38% MAP in English) than multi-entity ones (as low as 12% MAP)
- Larger models (Llama-3.1-70B) outperform smaller ones (Llama-3.1-8B) across all languages and task types
- Average Precision decreases as entity count of subjects increases across all languages and topics

## Why This Works (Mechanism)

### Mechanism 1: Language-Associated Entity Bias
Prompt language influences model's entity selection toward geographically-aligned answers, creating factual inference bias. Models encode statistical associations between entities and language contexts in which they predominantly appear during training. When prompted in a specific language, probability distribution shifts toward entities culturally or geographically linked to that language, overriding strictly factual ranking in some cases.

### Mechanism 2: Log-Probability Ranking for Knowledge Probing
Cumulative log-probability scores across candidate tokens provide reliable signal for ranking factual knowledge without requiring open-ended generation. For each candidate answer, model computes logits iteratively—first token probability, then conditional probabilities for subsequent tokens given growing context. Summing log-probabilities yields score reflecting model's confidence in that answer given prompt.

### Mechanism 3: Entity Count Scaling Difficulty
Models exhibit degraded performance on multi-entity questions compared to single-entity because probability mass must be distributed across multiple correct answers. In single-entity tasks, model concentrates probability on one answer. In multi-entity settings, it must assign high probabilities to several entities simultaneously within same ranking, diluting signal for each individual correct answer.

## Foundational Learning

- **Log-probability computation in autoregressive language models**: The entire evaluation framework depends on understanding how models assign and accumulate token-level probabilities. Quick check: Why do we sum log-probabilities rather than multiply raw probabilities when scoring multi-token candidate answers?

- **Average Precision (AP) as a ranking metric**: AP handles multi-entity ground truth better than Precision@k by rewarding correct answers appearing at any rank, not just top-k. Quick check: If a model ranks 3 correct answers at positions 1, 5, and 10 out of 100 candidates, how would AP differ from Precision@5?

- **Factual Knowledge Probing vs. standard QA evaluation**: This paper extends probing to multi-entity, multilingual settings with bias measurement—a conceptual shift from accuracy-only metrics. Quick check: What additional information does factual inference bias provide beyond raw accuracy scores?

## Architecture Onboarding

- **Component map**: Prompt -> Surface Set -> Log-probability Scorer -> AP Calculator -> Bias Scorer
- **Critical path**: 1) Load prompt in target language with subject and relation 2) Compute cumulative log-probabilities for all candidates 3) Rank by descending log-prob 4) Calculate AP comparing ranking positions of gold set entities 5) Measure proportion of language-specific entities in top-n results
- **Design tradeoffs**: Closed vocabulary (surface set) vs. open generation limits discovery of novel hallucinations but ensures reproducibility; Temperature = 0.1 prioritizes most probable tokens for stable probing but may underrepresent model's generation diversity; AP vs. Precision@k better suits multi-entity evaluation
- **Failure signatures**: MAP drops significantly for Turkish/Italian vs. English indicates resource-level training disparity; High factual inference bias (>0.5) on language-related topics confirms language-entity association mechanism; Gemma-3-4B shows lowest bias but also lowest MAP suggests underfitting rather than debiasing
- **First 3 experiments**: 1) Baseline replication: Run Llama-3.1-8B on English single-entity questions; target MAP ≈ 0.50 2) Bias hotspot test: Evaluate "Official Languages" topic comparing Italian vs. Turkish prompts; expect Turkish bias > Italian 3) Entity count scaling: Plot MAP vs. average entity count across all topics; confirm inverse relationship

## Open Questions the Paper Calls Out

### Open Question 1
Do the underlying mechanisms driving factual inference bias differ from those driving general factual recall failures in LLMs? The conclusion explicitly states that "mechanistic interpretability methods... should be employed to uncover the underlying causes of this bias and to better understand how entity class influences model performance." This remains unresolved because the current study quantifies presence of bias and performance gaps but does not investigate internal model states or circuits responsible for language-induced prioritization of incorrect entities.

### Open Question 2
Does the magnitude of factual inference bias correlate with model scale in larger architectures (>70B parameters)? The limitations section notes analysis is restricted to "small-scale language models (3B, 4B, 7B, and 8B parameters)" and suggests "further evaluations should also be conducted on larger-scaled models." This is unresolved because while the study shows larger models outperform smaller ones on accuracy, it is unclear if the percentage of biased responses decreases, increases, or plateaus as model capacity scales significantly beyond tested range.

### Open Question 3
Does the observed factual inference bias persist when using open-ended text generation instead of constrained log-probability ranking? The authors state in limitations that "This task can be extended to open-ended text generation, allowing us to examine how the model hallucinated... rather than focusing solely on factual correctness." This remains unresolved because current methodology restricts model to ranking pre-defined surface set of candidates, making it unknown if removing this constraint leads to higher rates of hallucination or different bias manifestations.

### Open Question 4
Is the higher factual inference bias observed in Turkish attributable specifically to its status as a low-resource language or to its agglutinative morphology? The paper notes Turkish prompts show higher bias than Italian in 83% of topics and acknowledges analysis is "limited to three languages," yet it does not isolate whether bias stems from training data scarcity or structural linguistic differences like suffixation. This is unresolved without comparing broader set of languages with varying resource levels and morphological typologies.

## Limitations

- Closed vocabulary approach using predefined surface sets may artificially constrain model performance and not fully capture generative capabilities or potential hallucinations
- Dataset construction process lacks transparency in how surface sets and language-specific entities are compiled, making it difficult to assess potential artifacts
- Evaluation focuses on only three languages (English, Italian, Turkish), limiting generalizability to other language pairs or families

## Confidence

- **High Confidence**: Larger models (Llama-3.1-70B) outperform smaller models (Llama-3.1-8B) across all languages and task types; inverse relationship between entity count and MAP scores
- **Medium Confidence**: Language-specific bias in entity selection is supported by data, particularly for topics like "Official Languages" and "Polyglot Celebrities"
- **Low Confidence**: Claim that Turkish prompts show systematically higher bias than Italian prompts in 83% of topics requires more rigorous statistical validation

## Next Checks

1. **Statistical Validation of Bias Differences**: Perform rigorous statistical tests (e.g., paired t-tests or Wilcoxon signed-rank tests) to validate the claim that Turkish prompts show higher bias than Italian prompts in 83% of topics. Calculate confidence intervals for bias scores and test whether observed differences are statistically significant beyond raw percentage claim.

2. **Open-Generation Comparison**: Implement an open-generation baseline where models generate answers without surface set constraint, then compare factual inference bias and MAP scores against closed-set approach. This would help determine whether closed vocabulary artificially suppresses or enhances certain biases.

3. **Cross-Lingual Transfer Analysis**: Evaluate same models using English prompts for topics where Italian and Turkish are expected to show different biases. This would help isolate whether observed bias differences stem from language-specific training patterns versus task-specific knowledge differences.