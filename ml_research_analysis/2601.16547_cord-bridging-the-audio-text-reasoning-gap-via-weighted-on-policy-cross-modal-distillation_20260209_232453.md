---
ver: rpa2
title: 'CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal
  Distillation'
arxiv_id: '2601.16547'
source_url: https://arxiv.org/abs/2601.16547
tags:
- cord
- reasoning
- alignment
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the degradation of knowledge and reasoning
  capabilities in Large Audio Language Models (LALMs) compared to their text-based
  counterparts, hypothesizing that this stems from ineffective bridging of the acoustic-semantic
  gap. The authors propose CORD, a unified alignment framework that performs online
  cross-modal self-distillation without external teachers.
---

# CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation

## Quick Facts
- arXiv ID: 2601.16547
- Source URL: https://arxiv.org/abs/2601.16547
- Reference count: 11
- One-line primary result: Reduces audio-text reasoning gap by 41.6% on Qwen2-Audio-7B-Instruct and 44.8% on Step-Audio2-mini

## Executive Summary
This paper addresses the degradation of knowledge and reasoning capabilities in Large Audio Language Models (LALMs) compared to their text-based counterparts, hypothesizing that this stems from ineffective bridging of the acoustic-semantic gap. The authors propose CORD, a unified alignment framework that performs online cross-modal self-distillation without external teachers. CORD aligns audio-conditioned reasoning with text-conditioned behavior within a single model using two complementary objectives: token-level alignment with importance-aware and position-aware reverse KL divergence, and sequence-level alignment via judge-based global reward optimized through Group Relative Policy Optimization (GRPO).

The method significantly reduces the audio-text performance gap by an average of 41.6% on Qwen2-Audio-7B-Instruct and 44.8% on Step-Audio2-mini across multiple reasoning benchmarks, demonstrating strong data efficiency and effectiveness in bridging cross-modal reasoning disparities.

## Method Summary
CORD employs a unified alignment framework that performs online cross-modal self-distillation within a single model architecture. The approach combines token-level alignment using reverse KL divergence with importance and position weighting, and sequence-level alignment through judge-based global rewards optimized via GRPO. The method operates without external teacher models, instead leveraging the model's own text-conditioned outputs as reference targets for audio-conditioned predictions. This self-distillation process is performed online during training, enabling continuous refinement of audio-to-text reasoning capabilities through both fine-grained token-level corrections and holistic sequence-level feedback.

## Key Results
- Reduces audio-text reasoning gap by 41.6% on Qwen2-Audio-7B-Instruct
- Achieves 44.8% improvement on Step-Audio2-mini across reasoning benchmarks
- Demonstrates strong data efficiency in bridging cross-modal reasoning disparities

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental challenge that audio inputs require different processing and reasoning pathways compared to text, even when conveying semantically equivalent information. By performing self-distillation within a single model, CORD enables the audio pathway to learn directly from the more mature text pathway's reasoning patterns. The token-level alignment captures fine-grained semantic correspondences between audio-conditioned and text-conditioned outputs, while the sequence-level alignment provides global reward signals that reinforce coherent reasoning across the entire output sequence. The combination of these two complementary alignment objectives allows the model to learn both detailed semantic mappings and high-level reasoning strategies simultaneously.

## Foundational Learning
- **Cross-modal self-distillation**: Why needed - Enables audio pathway to learn from text pathway without external teachers; Quick check - Verify model can produce coherent text outputs when given only text input
- **Reverse KL divergence**: Why needed - Provides importance-aware alignment that focuses on high-confidence regions; Quick check - Monitor KL divergence values for convergence and stability
- **Group Relative Policy Optimization (GRPO)**: Why needed - Enables efficient policy gradient optimization without requiring a separate value function; Quick check - Verify reward variance remains bounded during training
- **Online training**: Why needed - Allows continuous refinement of audio-to-text mappings; Quick check - Monitor training loss curves for both audio and text branches
- **Judge-based global reward**: Why needed - Provides holistic feedback on sequence-level reasoning quality; Quick check - Validate judge rewards correlate with downstream task performance
- **Importance-aware alignment**: Why needed - Focuses learning capacity on semantically critical tokens; Quick check - Compare alignment effectiveness with and without importance weighting

## Architecture Onboarding

**Component map:** Audio encoder -> Cross-modal alignment module -> Language decoder -> Text encoder -> Judge module -> GRPO optimizer

**Critical path:** Audio input → Audio encoder → Cross-modal alignment → Language decoder → Output predictions → Judge evaluation → GRPO updates

**Design tradeoffs:** The unified model architecture trades potential specialization for the benefit of shared parameters and direct knowledge transfer between modalities. The choice of reverse KL divergence over forward KL prioritizes mode-seeking behavior, which may lead to more conservative but reliable predictions. GRPO is selected over traditional RL algorithms to reduce variance and improve sample efficiency, at the cost of requiring group-based reward computation.

**Failure signatures:** Mode collapse in token-level alignment where audio-conditioned outputs become overly similar to text-conditioned outputs without preserving audio-specific information. Reward hacking in sequence-level alignment where the model learns to optimize judge rewards without genuine reasoning improvement. Training instability when the relative weighting between token-level and sequence-level objectives is not properly balanced.

**First experiments:**
1. Validate basic cross-modal alignment by comparing audio-to-text and text-to-text generation quality on simple semantic equivalence tasks
2. Test token-level alignment in isolation by freezing sequence-level components and measuring fine-grained semantic preservation
3. Evaluate judge-based reward signals independently by applying them to a pre-trained baseline model and measuring reasoning performance changes

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the limitations section implicitly raises questions about the scalability of the approach to larger model sizes and more diverse reasoning tasks.

## Limitations
- Lacks ablation studies isolating contributions of token-level versus sequence-level alignment components
- Does not specify whether judge-based global reward uses pre-trained or task-specific reward models
- Limited benchmark diversity and absence of cross-dataset validation raise questions about generalizability

## Confidence
- **High**: Core technical contribution of integrating token-level and sequence-level alignment within single model architecture
- **Medium**: Claimed data efficiency improvements, lacking detailed comparisons with baseline models on equivalent data amounts
- **Low**: Generalizability of 41.6% and 44.8% performance gap reductions given limited benchmark diversity

## Next Checks
1. Perform ablation studies comparing CORD's performance with only token-level alignment, only sequence-level alignment, and the full combined approach across multiple reasoning tasks
2. Test model performance on out-of-distribution audio inputs and reasoning tasks not seen during training to assess generalization capabilities
3. Conduct runtime and memory usage analysis to quantify computational overhead introduced by dual alignment objectives compared to standard audio-language model training