---
ver: rpa2
title: Neurosymbolic Information Extraction from Transactional Documents
arxiv_id: '2512.09666'
source_url: https://arxiv.org/abs/2512.09666
tags:
- schema
- extraction
- information
- documents
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A neurosymbolic framework is proposed for structured information
  extraction from transactional documents, leveraging large language models with multi-level
  symbolic validation (syntactic, task, and domain) to filter and refine outputs.
  The approach enforces domain-specific arithmetic constraints and schema-based coherence,
  significantly improving F1-scores and accuracy over baseline zero-shot extraction,
  and enabling effective knowledge distillation for fine-tuning.
---

# Neurosymbolic Information Extraction from Transactional Documents

## Quick Facts
- arXiv ID: 2512.09666
- Source URL: https://arxiv.org/abs/2512.09666
- Authors: Arthur Hemmer; Mickaël Coustaty; Nicola Bartolo; Jean-Marc Ogier
- Reference count: 40
- Primary result: Multi-level symbolic validation with LLMs improves F1-scores by 2–10 percentage points across different model families

## Executive Summary
This paper proposes a neurosymbolic framework for structured information extraction from transactional documents that combines large language models with cascading symbolic validation layers. The approach uses syntactic, task, and domain-level filters to progressively refine LLM outputs, enforcing schema-based coherence and arithmetic constraints. Experimental results on CORD and SROIE datasets show significant improvements in extraction quality, particularly when using domain-filtered predictions for fine-tuning smaller models through knowledge distillation.

## Method Summary
The method combines zero-shot extraction with large language models followed by three-level symbolic validation: syntactic validation checks JSON structure, task validation verifies values appear in OCR text, and domain validation enforces arithmetic constraints defined in a 53-field schema. After filtering, the high-quality predictions are used for knowledge distillation to fine-tune smaller models with LoRA. The framework processes documents through LLM generation, cascading validation filters, and optional fine-tuning stages, with arithmetic constraint satisfaction requiring 0.5% tolerance for numerical relationships.

## Key Results
- F1-scores improve from 60.0 → 69.3 on CORD when applying all three validation filters (though document retention drops from 100% → 25%)
- Domain-filtered training data improves student model F1 by 2–10 percentage points compared to unfiltered training
- Ministral-8B on CORD achieves 77.4 F1 when trained on domain-validated predictions vs. 69.7 on base predictions
- Sampling with temperature τ=1.0 and n=16 improves document retention by +157% on SROIE at cost of F1 variability

## Why This Works (Mechanism)

### Mechanism 1: Multi-Level Symbolic Validation as Output Filter
Cascading validation layers progressively remove erroneous extractions, concentrating correct predictions in the retained subset. LLM generates candidate extractions → Syntactic filter validates JSON structure → Task filter verifies values appear verbatim in OCR text → Domain filter checks arithmetic coherence. Each layer removes a distinct error class without modifying outputs. Valid extractions satisfy all constraints; invalid ones violate at least one.

### Mechanism 2: Domain-Specific Arithmetic Constraints Enable Implicit Completion
Arithmetic relationships between fields allow inference of missing values, improving both validation coverage and final output completeness. Schema defines explicit equations (e.g., gross_total = net_total + tax). After parsing explicit string values to decimals, solver iteratively infers missing fields until fixed point. Constraints then validated with 0.5% tolerance. Transactional documents follow standardized arithmetic structure; deviations are errors, not valid variations.

### Mechanism 3: Constraint-Filtered Labels Improve Knowledge Distillation
Training smaller models on domain-validated predictions yields higher F1-scores than training on unfiltered outputs. Pixtral-12B generates zero-shot predictions → Domain filter retains only constraint-satisfying extractions → Ministral-8B/Llama/Qwen fine-tuned on filtered vs. unfiltered subsets. Filtering removes noisy labels that would otherwise confuse student model. Constraint satisfaction correlates with extraction correctness; filtered set has higher label quality.

## Foundational Learning

- **Information Extraction as Structured Prediction**
  - Why needed here: The task formalizes as mapping document x → schema S → structured output y. Without this framing, validation logic appears arbitrary.
  - Quick check question: Can you distinguish key-information localization (KILE) from line-item recognition (LIR), and explain why LLMs handle both simultaneously?

- **Generative Language Models for Structured Output**
  - Why needed here: Understanding autoregressive generation P(t_i | t_{<i}, P) explains why models produce syntactically valid but factually incorrect outputs—necessitating validation.
  - Quick check question: Why does greedy decoding produce different error profiles than sampling with temperature τ=1.0?

- **Symbolic Constraint Satisfaction**
  - Why needed here: Domain validation is not heuristic filtering—it is constraint solving. Understanding fixed-point iteration clarifies how implicit values are inferred.
  - Quick check question: Given subtotal=100, tax_rate=0.08, how would the solver infer gross_total, and what happens if net_total is also provided but conflicts?

## Architecture Onboarding

- **Component map:** Document (image + OCR text) → LLM (Ministral-8B / Pixtral-12B) + Prompt (schema + instructions) → Raw JSON output → Syntactic Validator → Task Validator → Domain Validator → High-Quality Labels → Fine-tuning (LoRA) → Student Model

- **Critical path:** The domain validator is the bottleneck. It requires: (1) accurate string→decimal parsing, (2) complete schema definition, (3) solver that handles partial information. Parsing failures (period/comma ambiguity) caused 3.5–4.5% of documents to fail constraint checks even with correct ground truth.

- **Design tradeoffs:** Precision vs. Recall: Domain filtering improves F1 on retained subset but discards 56–88% of documents (Table 1). Sampling (τ=1.0, n=16) improves recall (+157% retention on SROIE) at cost of base F1 variability. Schema complexity vs. coverage: 53-field schema achieved 95%+ constraint satisfaction but limited to single tax rate. Domain invariance sacrificed for validation power.

- **Failure signatures:** OCR errors: Values like "18.000,00" vs "18,000.00" cause parsing failure → domain validation rejects. Schema mismatch: Discounts shown only as rates (no absolute value) cannot be validated. Cut-off documents: Missing fields (e.g., cropped totals) make arithmetic constraints unevaluable. Over-extraction: Models extracting few fields pass validation via inference, but F1 remains low (implicit fields count as correct but explicit recall is poor).

- **First 3 experiments:**
  1. **Baseline replication:** Run zero-shot extraction with Pixtral-12B on 50 documents, apply only syntactic validation. Measure F1, Valid rate, and identify top 3 error patterns in rejected outputs.
  2. **Ablation by validation layer:** Apply filters incrementally (syntax → task → domain) and plot F1 vs. % Remaining. Identify which layer provides most precision gain per recall cost.
  3. **Single-domain constraint test:** Select 20 documents with known ground-truth arithmetic. Manually inject one field error (e.g., flip tax from 8.00 to 80.00). Verify domain validator catches 100% of injected errors with 0.5% tolerance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific schemas and arithmetic constraints be generated automatically rather than manually designed?
- Basis in paper: [explicit] Section 7 states "future work could examine how to generate such domain-specific schemas and constraints automatically."
- Why unresolved: The current methodology requires labor-intensive manual schema creation with expert domain knowledge, limiting scalability to new document types.
- What evidence would resolve it: Demonstration of an automated pipeline that infers field relationships and arithmetic constraints from unlabeled document collections, evaluated on multiple document domains.

### Open Question 2
- Question: What domain-invariant validation constraints can generalize effectively across document types beyond transactional documents?
- Basis in paper: [explicit] Section 7 notes "Future work could focus on creating more domain-invariant constraints that can be used for validation."
- Why unresolved: Current domain-level constraints are tightly coupled to the transactional document arithmetic structure; syntactic and task-level filters showed modest improvements alone.
- What evidence would resolve it: Cross-domain benchmark evaluation showing that newly designed constraints maintain F1-score improvements across non-transactional document categories (e.g., legal, medical).

### Open Question 3
- Question: How can the schema be extended to handle invoices with multiple tax rates applied to different line items?
- Basis in paper: [explicit] Section 4 acknowledges the schema "only allows for a single tax rate besides non-taxable amounts. This didn't pose any problems for the datasets we annotated, but might for other datasets."
- Why unresolved: Real-world commercial invoices frequently apply different tax rates to different product categories.
- What evidence would resolve it: A revised schema with nested tax-rate structures that maintains constraint satisfaction rates above 95% on multi-rate invoice datasets.

### Open Question 4
- Question: What is the optimal balance between sampling diversity and validation filtering when generating training labels for knowledge distillation?
- Basis in paper: [inferred] Section 5.3 shows recall improvements with sampling but F1-score variability the authors "found challenging to quantify through repeated experiments due to computational limitations."
- Why unresolved: The trade-off between increased document retention (recall) and prediction quality variability under sampling remains uncharacterized.
- What evidence would resolve it: Systematic ablation study across sampling temperatures and sample counts, with statistical significance testing over multiple random seeds.

## Limitations

- Schema complexity limits scalability: The 53-field schema achieves high constraint satisfaction but requires manual expert design, making it difficult to adapt to new document domains
- Retention rate vs. quality trade-off: Domain filtering improves precision but discards 56–88% of documents, creating a significant recall bottleneck
- Arithmetic constraint rigidity: Single tax rate schema cannot handle invoices with multiple tax rates applied to different line items, limiting real-world applicability

## Confidence

**High Confidence** in the multi-level validation framework concept and its general effectiveness. The cascading filter approach (syntactic → task → domain) is well-grounded in information extraction literature, and the reported improvements in F1-scores are consistent with what symbolic constraint satisfaction typically achieves.

**Medium Confidence** in the specific arithmetic constraint implementation and solver design. While the concept of using schema-defined equations to infer missing values is sound, the paper doesn't fully detail how the fixed-point iteration handles conflicting or partial information, which could significantly impact validation accuracy.

**Low Confidence** in the knowledge distillation claims without seeing the exact filtered vs. unfiltered training data distributions. The 2–10 percentage point improvements depend heavily on the quality and quantity of filtered data, which varies dramatically (56–88% document loss in Table 1).

## Next Checks

1. **Constraint Validation Coverage Analysis**: Run the domain validator on 100 ground-truth annotated documents with artificially injected arithmetic errors (varying by 0.1%, 0.5%, 1.0% tolerance). Measure true positive rate, false negative rate, and identify which constraint types (summation, multiplication, default values) are most/least reliable.

2. **OCR Parsing Robustness Test**: Create a test set of 50 documents with systematically varied number formats (comma vs. period as decimal separator, space as thousands separator, currency symbols). Measure parsing success rate and identify which formats cause the most validation failures.

3. **Domain Filtering Trade-off Analysis**: For a fixed dataset, measure F1-score and document accuracy across different domain filter tolerance levels (0.1%, 0.5%, 1.0%, 5.0%). Plot precision-recall curves to identify optimal tolerance that balances quality improvement against recall preservation.