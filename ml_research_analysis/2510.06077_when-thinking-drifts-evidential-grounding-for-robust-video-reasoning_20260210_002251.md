---
ver: rpa2
title: 'When Thinking Drifts: Evidential Grounding for Robust Video Reasoning'
arxiv_id: '2510.06077'
source_url: https://arxiv.org/abs/2510.06077
tags:
- video
- reasoning
- visual
- person
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the "Visual Thinking Drift"
  problem in video reasoning, where Chain-of-Thought (CoT) prompting causes models
  to generate verbose but ungrounded reasoning chains that hallucinate visual details.
  The authors introduce Visual Evidence Reward (VER), a reinforcement learning framework
  that rewards reasoning traces explicitly grounded in visual evidence.
---

# When Thinking Drifts: Evidential Grounding for Robust Video Reasoning

## Quick Facts
- arXiv ID: 2510.06077
- Source URL: https://arxiv.org/abs/2510.06077
- Reference count: 40
- This paper introduces VER, a reinforcement learning framework that rewards reasoning traces explicitly grounded in visual evidence, improving accuracy by up to 9.0% on video reasoning benchmarks.

## Executive Summary
This paper identifies and addresses the "Visual Thinking Drift" problem in video reasoning, where Chain-of-Thought (CoT) prompting causes models to generate verbose but ungrounded reasoning chains that hallucinate visual details. The authors introduce Visual Evidence Reward (VER), a reinforcement learning framework that rewards reasoning traces explicitly grounded in visual evidence. An auxiliary LLM judges the alignment between reasoning steps and observable video content, providing a binary reward signal. Across 10 diverse video understanding benchmarks, the proposed Video-VER model consistently achieves top performance, improving accuracy by up to 9.0% absolute points and averaging +4.0% gains over base models.

## Method Summary
The paper proposes a two-stage pipeline: (1) Supervised Fine-Tuning (SFT) on Video-R1-COT-165k to bootstrap CoT reasoning, and (2) Group Relative Policy Optimization (GRPO) with Visual Evidence Reward (VER). The policy model (Qwen2.5-VL-7B) generates CoT traces, which are evaluated by a Llama-3.1-70B-Instruct judge for visual grounding. Visual evidence is generated offline using inverted prompting—feeding the external Qwen2.5-VL-72B teacher the question-answer pair to extract supporting visual facts. The reward combines accuracy, evidence alignment, format compliance, and length constraints, optimized over 2000 iterations with group size 8.

## Key Results
- VER improves accuracy by up to 9.0% absolute points over base models across 10 video understanding benchmarks
- Outperforms other state-of-the-art models on 8 out of 10 benchmarks
- Effectively mitigates "visual thinking drift" by grounding reasoning in observable video content rather than internal language priors

## Why This Works (Mechanism)

### Mechanism 1: Visual Evidence Reward (VER) Signal
The VER mechanism explicitly rewards reasoning traces that cite visual evidence, shifting model behavior from "storytelling" to grounded retrieval. An auxiliary LLM evaluates whether generated CoT contains facts aligned with video content, assigning a binary reward combined with accuracy reward via GRPO. This assumes the judge can reliably distinguish hallucinated details from grounded visual facts better than the policy model.

### Mechanism 2: Bayesian Drift Mitigation via Reward Shaping
Standard CoT suffers from "visual thinking drift" where language priors dominate visual likelihoods; evidence rewards counteract this by forcing re-engagement with visual tokens. In autoregressive generation, attention over generated text eventually overwhelms visual feature weights, but VER forces sampling tokens that maximize visual correlation, increasing $W_{vis}$ influence during decoding.

### Mechanism 3: Inverted Prompting for Supervision Bootstrapping
Generating visual evidence by conditioning on the ground-truth answer creates cleaner supervision than standard CoT exploration. Instead of sampling $p(c|q,v)$, an external VLM generates evidence by inverting the problem: $p(e|q, a_{gt}, v)$. This ensures evidence strictly supports the correct answer, providing high-quality training data for the smaller policy model.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The RL algorithm used to update model weights. Unlike standard PPO, GRPO normalizes rewards within a group of outputs for the same prompt, making it robust to output length variance. *Quick check*: How does group-based reward normalization prevent the model from simply gaming the length reward?

- **Visual Thinking Drift**: The core failure mode addressed—explains why making a model "think longer" (CoT) often lowers accuracy in video tasks, a counter-intuitive result for engineers familiar with text-based reasoning. *Quick check*: Why does the probability of error increase linearly with chain length $T$ in this framework?

- **LLM-as-a-Judge**: The entire VER mechanism depends on an LLM's ability to classify grounding. Understanding limitations of this judge is critical for debugging reward hacking. *Quick check*: What are the failure modes if the judge model is smaller or weaker than the policy model being trained?

## Architecture Onboarding

- **Component map**: Policy Model (Qwen2.5-VL-7B) -> Judge Model (Llama-3.1-70B-Instruct) -> Visual Evidence Generator (Qwen2.5-VL-72B) -> GRPO Optimizer

- **Critical path**: 1) Offline: Generate visual evidence dataset using Inverted Prompting (Teacher). 2) Training: Policy generates CoT → Judge compares CoT to Evidence → Compute Reward → GRPO Update. 3) Inference: Policy generates grounded CoT directly; no external judge or teacher required.

- **Design tradeoffs**: Binary vs. Scalar Reward (binary chosen for robustness against noise, sacrificing granularity); Generic vs. Question-Dependent Evidence (generic cheaper but noisy; QD-VE requires forward pass per question but aligns better with specific reasoning task).

- **Failure signatures**: Rewards without Accuracy (model learns to include generic visual keywords to fool judge without solving logic); Length Collapse (model might drift toward shorter responses to minimize drift/hallucinations, bypassing CoT mechanism).

- **First 3 experiments**: 1) Replicate "Drift": Compare Direct Answer vs. Zero-Shot CoT accuracy on your specific video dataset. 2) Judge Reliability Test: Manually annotate 50 samples for grounding and measure correlation with judge outputs. 3) Ablate the Evidence Source: Compare training with "Generic Captions" vs. "Inverted Prompting Evidence" to verify ROI of offline generation cost.

## Open Questions the Paper Calls Out

- Can Visual Evidence Reward be effectively extended to open-ended tasks such as free-form video QA, where rule-based reward computation becomes infeasible? The current VER framework relies on binary correctness checks suitable for MCQs; free-form answers lack ground truth for automated reward computation.

- How does Video-VER's visual grounding performance scale when applied to very long videos containing sparse critical information or complex temporal patterns? Current experiments focus on moderate-length videos with 16-32 frames; long videos may require different encoding strategies and evidence aggregation.

- Can alternative evaluation mechanisms replace or improve upon LLM-based judges for computing visual grounding rewards with higher precision and lower computational cost? The current Llama-3.1-70B judge introduces potential biases, API dependencies, and computational overhead.

- Would incorporating dynamic frame selection algorithms mitigate failure cases caused by incomplete frame sampling that currently affect Video-VER? Fixed sampling rates (2 FPS in experiments) may miss key frames; the paper does not explore adaptive sampling strategies.

## Limitations

- The VER framework's efficacy depends critically on the judge model's reliability, which is never directly validated against human judgments.
- The "visual thinking drift" mechanism is theoretically compelling but primarily supported by qualitative observations rather than quantitative drift measurements.
- The ablation showing "Generic Captions" vs. "Question-Dependent Evidence" provides limited insight into whether performance gains come from better grounding or simply more task-specific context.

## Confidence

- **Confidence: Medium** The VER framework's efficacy depends critically on the judge model's reliability, which is never directly validated against human judgments.
- **Confidence: Medium** The "visual thinking drift" mechanism is theoretically compelling but primarily supported by qualitative observations rather than quantitative drift measurements.
- **Confidence: Low** The evidence generation process itself could be introducing biases that correlate with correct answers.

## Next Checks

1. **Judge Model Validation**: Manually annotate 100 reasoning traces for grounding quality and compute correlation with Llama-3.1-70B judge outputs to establish reliability of reward signal.

2. **Drift Quantification**: Measure KL divergence between language-only and visual+language attention distributions across CoT length to empirically validate "thinking drift" hypothesis.

3. **Evidence Generation Ablation**: Compare training with (a) no evidence, (b) generic captions only, (c) question-dependent evidence, and (d) evidence generated by a weaker judge model to isolate whether gains come from grounding quality versus task-specific context.