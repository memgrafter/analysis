---
ver: rpa2
title: 'Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity'
arxiv_id: '2505.21411'
source_url: https://arxiv.org/abs/2505.21411
tags:
- experts
- expert
- pangu
- ascend
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of Grouped Experts (MoGE), a novel
  architecture that groups experts and enforces balanced token-to-expert assignments
  within each group, effectively eliminating device load imbalance in distributed
  systems. Unlike conventional top-K routing, MoGE ensures equal expert activation
  per group, leading to more efficient computation across devices.
---

# Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity

## Quick Facts
- arXiv ID: 2505.21411
- Source URL: https://arxiv.org/abs/2505.21411
- Reference count: 40
- Primary result: 72-billion-parameter MoE model achieving 1148-1528 tokens/s per card on Ascend 800I A2, outperforming dense models

## Executive Summary
This paper introduces Mixture of Grouped Experts (MoGE), a novel MoE architecture that groups experts and enforces balanced token-to-expert assignments within each group to eliminate device load imbalance in distributed systems. Unlike conventional top-K routing, MoGE ensures equal expert activation per group, leading to more efficient computation across devices. The authors build Pangu Pro MoE, a 72-billion-parameter model with 16 billion activated per token, optimized for Ascend NPUs. Extensive experiments demonstrate that MoGE achieves better expert load balancing, improved training and inference efficiency, and superior throughput compared to dense models.

## Method Summary
The paper presents Mixture of Grouped Experts (MoGE), which addresses device load imbalance in MoE models by grouping experts and enforcing balanced token distribution within each group. Instead of conventional top-K routing, MoGE uses a grouped approach where tokens are assigned to experts within each group, ensuring equal expert activation and more efficient computation across distributed devices. The authors build Pangu Pro MoE, a 72-billion-parameter model with 16 billion parameters activated per token, specifically optimized for Ascend NPUs. The architecture is designed to maximize throughput while maintaining computational efficiency through the grouped expert mechanism.

## Key Results
- Pangu Pro MoE achieves 1148-1528 tokens/s per card on Ascend 800I A2, outperforming 32B and 72B dense models
- Ranks among top-performing sub-100B parameter models, surpassing GLM-Z1-32B and Qwen3-32B across multiple benchmarks
- Demonstrates excellent cost-to-performance ratio on Ascend 300I Duo hardware

## Why This Works (Mechanism)
MoGE works by addressing the fundamental problem of load imbalance in distributed MoE systems. By grouping experts and enforcing balanced token assignment within each group, the architecture ensures that all devices process approximately equal amounts of computation, eliminating the bottlenecks that typically occur with top-K routing where some experts may receive disproportionately more tokens than others.

## Foundational Learning

**Mixture of Experts (MoE)**: A neural network architecture that routes inputs to specialized "expert" networks, activating only a subset of parameters per input. Why needed: Enables scaling to larger model sizes while maintaining computational efficiency by activating only relevant parameters.

**Expert Load Balancing**: The distribution of tokens across different experts in an MoE system. Why needed: Ensures all computational resources are utilized efficiently and prevents bottlenecks where some experts are overloaded while others are underutilized.

**Device Load Imbalance**: Occurs when different processing units (NPUs, GPUs) in a distributed system receive unequal computational workloads. Why needed: Leads to reduced overall system throughput as the slowest device becomes the bottleneck for the entire system.

**Top-K Routing**: Traditional MoE routing mechanism where each token is routed to the K most relevant experts. Why needed: While effective at selecting relevant experts, it can lead to load imbalance when certain experts become popular for many tokens.

**Grouped Expert Architecture**: The MoGE innovation where experts are organized into groups and token assignment is balanced within each group. Why needed: Provides a middle ground between fully balanced routing (which may reduce model quality) and top-K routing (which causes load imbalance).

## Architecture Onboarding

**Component Map**: Input tokens -> Router -> Expert Groups (MoGE) -> Output combination
**Critical Path**: Token routing through MoGE mechanism → Expert computation within groups → Output aggregation
**Design Tradeoffs**: MoGE sacrifices some routing flexibility compared to top-K to gain load balancing benefits, accepting potential minor quality trade-offs for significant efficiency gains
**Failure Signatures**: Load imbalance manifests as reduced throughput and underutilized devices; MoGE failure would show poor load balancing despite grouping mechanism
**First Experiments**: 1) Measure per-device computation time to verify load balancing, 2) Compare throughput on Ascend vs other hardware platforms, 3) Test expert activation patterns to confirm balanced distribution within groups

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily focused on Ascend NPUs, limiting generalizability to other hardware platforms
- Architectural novelty not extensively validated against other recent MoE variants like token-based or type-based routing
- Claims of "better expert load balancing" lack clear quantitative comparison with baseline MoE methods in terms of per-expert activation patterns

## Confidence

**High confidence**: Hardware-specific performance measurements on Ascend NPUs are reliable given controlled experimental setup and direct measurements reported.

**Medium confidence**: Comparative advantage over dense models (32B and 72B) is supported by data, though cross-platform validation would strengthen this claim.

**Medium confidence**: Benchmark performance claims are credible based on reported results, but relative ranking depends heavily on specific benchmark suite used.

## Next Checks

1. Replicate throughput measurements on alternative hardware platforms (NVIDIA GPUs, Intel Gaudi) to assess hardware dependency and generalizability of efficiency claims.

2. Conduct ablation studies comparing MoGE against other state-of-the-art MoE routing strategies (token-based, type-based) using identical model sizes and hardware to isolate architectural contribution.

3. Perform extended training stability analysis over longer training durations to verify that grouped expert constraint does not introduce degradation in convergence behavior or final model quality.