---
ver: rpa2
title: Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement
  Learning
arxiv_id: '2512.08485'
source_url: https://arxiv.org/abs/2512.08485
tags:
- budget
- learning
- attack
- offline
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of offline reinforcement
  learning to data poisoning attacks, which can degrade policy performance. Existing
  attack strategies use locally uniform perturbations, treating all samples equally
  and leading to inefficiency and detectability.
---

# Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.08485
- Source URL: https://arxiv.org/abs/2512.08485
- Authors: Junnan Qiu; Yuanjie Zhao; Jie Li
- Reference count: 4
- Primary result: Global budget allocation attack based on TD-error sensitivity achieves up to 80% performance degradation in D4RL benchmarks while evading state-of-the-art defenses

## Executive Summary
This paper addresses a critical vulnerability in offline reinforcement learning (RL) systems: data poisoning attacks that can severely degrade policy performance. The authors identify that existing attack strategies using locally uniform perturbations are inefficient and easily detectable. They propose a novel Global Budget Allocation attack that leverages the insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error. This method formulates the attack as a global resource allocation problem, deriving a closed-form solution where perturbation magnitudes are assigned proportional to TD-error sensitivity under a global L2 constraint.

## Method Summary
The proposed attack strategy reformulates data poisoning as a global optimization problem, moving away from the conventional locally uniform perturbation approach. The core insight is that samples with higher TD errors have greater influence on value function convergence and thus greater attack potential. The authors derive a closed-form solution for optimal perturbation allocation where each sample's perturbation magnitude is proportional to its TD-error sensitivity, subject to a global L2 constraint. This creates a more efficient attack that achieves greater performance degradation with fewer total perturbations while simultaneously evading detection by state-of-the-art statistical and spectral defenses. The method was evaluated on D4RL benchmarks including Walker2d, Hopper, and HalfCheetah environments.

## Key Results
- Achieves up to 80% performance degradation in D4RL benchmark tasks
- Outperforms baseline attack strategies using locally uniform perturbations
- Demonstrates evasion capability against state-of-the-art statistical and spectral detection defenses
- Shows efficiency gains through minimal perturbation magnitudes while maintaining attack effectiveness

## Why This Works (Mechanism)
The attack exploits the fundamental relationship between TD errors and value function convergence in offline RL. By allocating perturbations proportionally to TD-error sensitivity rather than uniformly across samples, the attack concentrates resources where they have maximum impact on the learning process. The global L2 constraint ensures the perturbations remain bounded while allowing intelligent distribution. This approach leverages the mathematical structure of the Bellman error to create targeted disruptions that are harder to detect because they follow a principled, non-random pattern.

## Foundational Learning
**Temporal Difference (TD) Error** - The difference between predicted and actual returns in RL value function updates. Why needed: Serves as the sensitivity metric for determining attack impact. Quick check: Verify TD errors correlate with sample influence on policy convergence.
**Value Function Convergence** - The process by which estimated state-action values approach their true values. Why needed: Target of the poisoning attack to degrade policy performance. Quick check: Confirm convergence speed affects policy quality.
**L2 Constraint** - A regularization term limiting the total magnitude of perturbations. Why needed: Ensures perturbations remain bounded and potentially less detectable. Quick check: Validate that constraint prevents excessive individual perturbations.
**Offline RL Setting** - Learning from fixed datasets without environment interaction. Why needed: Creates vulnerability to data poisoning since no online correction is possible. Quick check: Confirm attacks are more effective in offline vs online settings.

## Architecture Onboarding

Component Map: Dataset -> TD Error Calculation -> Global Budget Allocation -> Perturbation Application -> Poisoned Dataset

Critical Path: The attack flow begins with computing TD errors for all samples in the dataset, followed by solving the global allocation optimization to determine perturbation magnitudes, and finally applying these perturbations to create the poisoned dataset. The poisoned dataset is then used for standard offline RL training.

Design Tradeoffs: The global approach sacrifices some flexibility compared to local strategies but gains efficiency and stealth. The closed-form solution enables computational efficiency but assumes specific mathematical relationships that may not hold universally. The L2 constraint provides boundedness but may limit attack strength in some scenarios.

Failure Signatures: Attacks fail when TD error sensitivity doesn't correlate with influence on convergence, when the L2 constraint is too restrictive, or when detection mechanisms identify the non-uniform perturbation pattern. Poor performance occurs if the global allocation doesn't account for reward structure variations.

First Experiments:
1. Baseline comparison: Apply uniform vs global allocation attacks on a simple D4RL task
2. Detection evasion test: Evaluate statistical detection methods on both attack types
3. Constraint sensitivity: Test attack performance across different L2 constraint values

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework assumes specific relationships between TD errors and value function convergence that may not generalize across all RL algorithms
- Experimental validation is limited to continuous control tasks from D4RL, restricting generalizability to other domains or discrete action spaces
- Claims of evading "state-of-the-art" defenses lack comprehensive benchmarking against the most recent defense mechanisms
- The closed-form solution's sensitivity to reward structure variations and non-linear reward functions is not thoroughly explored

## Confidence
**High**: The theoretical framework linking TD errors to sample influence, mathematical derivation of the global allocation solution
**Medium**: Empirical results showing performance degradation superiority over baselines, detection evasion claims
**Low**: Generalization to other RL algorithms beyond the tested D4RL environments, robustness against future defense mechanisms

## Next Checks
1. Test the attack strategy on additional RL benchmarks beyond D4RL, including discrete action spaces and different algorithmic frameworks (e.g., actor-critic methods, model-based RL)
2. Evaluate robustness against adaptive defense mechanisms that could detect non-uniform perturbation patterns
3. Investigate the sensitivity of the attack to different reward structures and how it performs when reward functions are non-linear or sparse