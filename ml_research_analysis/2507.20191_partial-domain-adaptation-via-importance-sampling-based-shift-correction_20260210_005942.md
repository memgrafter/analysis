---
ver: rpa2
title: Partial Domain Adaptation via Importance Sampling-based Shift Correction
arxiv_id: '2507.20191'
source_url: https://arxiv.org/abs/2507.20191
tags:
- domain
- source
- sampling
- target
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses partial domain adaptation (PDA), a scenario
  where the source domain has more classes than the target domain. Previous methods
  rely on reweighing source samples to correct label distribution shift, but this
  approach cannot sufficiently explore the latent structure and may lead to overfitting.
---

# Partial Domain Adaptation via Importance Sampling-based Shift Correction

## Quick Facts
- arXiv ID: 2507.20191
- Source URL: https://arxiv.org/abs/2507.20191
- Authors: Cheng-Jun Guo; Chuan-Xian Ren; You-Wei Luo; Xiao-Lin Xu; Hong Yan
- Reference count: 40
- One-line primary result: Novel importance sampling method for partial domain adaptation outperforms existing methods with 79.2% accuracy on Office-Home and 89.3% on VisDA-2017

## Executive Summary
This paper addresses partial domain adaptation (PDA), where the source domain contains more classes than the target domain. Traditional PDA methods rely on reweighting source samples to correct label distribution shifts, but this approach cannot sufficiently explore the latent structure and may lead to overfitting. The authors propose a novel importance sampling-based shift correction (IS2C) method that builds a sampling domain whose label distribution matches the target, and samples new labeled data from this domain to train the model. This approach explicitly aligns class-conditional distributions between source and target domains. The authors provide theoretical guarantees for IS2C, proving that the generalization error can be sufficiently dominated by the method.

## Method Summary
The IS2C method addresses partial domain adaptation by constructing a sampling domain with a label distribution that matches the target domain. It then uses importance sampling to draw new labeled data from this constructed domain for training. The key innovation is that IS2C explicitly aligns class-conditional distributions between source and target domains, rather than relying solely on sample reweighting. The method provides theoretical guarantees showing that the generalization error can be sufficiently dominated by the importance sampling approach. Experiments on standard PDA benchmarks demonstrate that IS2C outperforms existing methods, achieving state-of-the-art performance on Office-Home and VisDA-2017 datasets.

## Key Results
- IS2C achieves 79.2% accuracy on Office-Home, outperforming the best baseline (78.3%)
- IS2C achieves 89.3% accuracy on VisDA-2017, outperforming the best baseline (88.7%)
- The method effectively reduces class-conditional discrepancy between domains
- Theoretical guarantees show generalization error can be sufficiently dominated by IS2C

## Why This Works (Mechanism)
IS2C works by explicitly constructing a sampling domain whose label distribution matches the target domain, then using importance sampling to draw new labeled data from this domain. This approach directly addresses the label distribution shift problem in partial domain adaptation by ensuring the training data distribution better matches the target domain's class distribution. The method goes beyond simple sample reweighting by creating new labeled examples that bridge the gap between source and target domains, allowing the model to learn more effectively from the aligned distributions.

## Foundational Learning
- Importance Sampling: A technique for estimating properties of a particular distribution while only having samples generated from a different distribution. Needed to draw samples from the constructed sampling domain. Quick check: Verify understanding of how importance weights are calculated and applied.
- Partial Domain Adaptation: A domain adaptation scenario where the source domain has more classes than the target domain. Needed to frame the problem being solved. Quick check: Confirm understanding of how PDA differs from standard domain adaptation.
- Label Distribution Shift: The phenomenon where the class distribution differs between source and target domains. Needed to understand the core challenge being addressed. Quick check: Verify ability to identify and quantify label distribution differences between domains.

## Architecture Onboarding

Component Map:
Source Domain -> Sampling Domain Construction -> Importance Sampling -> Labeled Data Generation -> Model Training -> Target Domain Performance

Critical Path:
The critical path is: Sampling Domain Construction → Importance Sampling → Labeled Data Generation → Model Training. This sequence is essential because the quality of the constructed sampling domain directly affects the effectiveness of the importance sampling, which in turn determines the quality of the generated labeled data for training.

Design Tradeoffs:
The method trades computational complexity for improved alignment between source and target domains. While constructing the sampling domain and performing importance sampling adds computational overhead, it provides more explicit control over the label distribution alignment compared to simpler reweighting approaches. The tradeoff is between computational efficiency and the quality of domain alignment.

Failure Signatures:
Potential failures could occur if the label distribution estimation is inaccurate, leading to a poorly constructed sampling domain. The method might also fail if the importance sampling step produces samples that are too far from the original source data distribution, potentially introducing artifacts or unrealistic examples.

First Experiments:
1. Verify the sampling domain construction by visualizing the label distribution and comparing it to the target domain
2. Test the importance sampling mechanism on a small subset of data to confirm it generates samples with appropriate characteristics
3. Evaluate the impact of different importance sampling parameters on the quality of generated labeled data

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes target domain label distribution can be accurately estimated or known during training
- Performance improvements over baselines are modest (small margins of 0.9% on Office-Home and 0.6% on VisDA-2017)
- Theoretical guarantees rely on assumptions about sampling domain construction that may not hold in practice
- Computational complexity of importance sampling from large constructed domains is not addressed
- Sensitivity to hyperparameters and robustness across different scenarios is unexplored

## Confidence

High confidence:
- The conceptual framework of using importance sampling for partial domain adaptation is sound and well-articulated
- Mathematical formulation and theoretical analysis are rigorous

Medium confidence:
- Experimental results show consistent improvements over baselines
- Margin of improvement is relatively small
- Ablation studies provide some insight but could be more comprehensive

Low confidence:
- Claims about scalability lack empirical support
- Sensitivity analysis to hyperparameters is insufficient
- Generalization to more complex domain adaptation scenarios is not validated

## Next Checks

1. Conduct statistical significance testing across multiple runs to verify reported performance improvements are not due to random variation

2. Evaluate performance on additional domain adaptation benchmarks with larger domain shifts and more classes to test scalability and robustness

3. Perform detailed computational complexity analysis and runtime comparisons with baseline methods to assess practical feasibility for large-scale applications