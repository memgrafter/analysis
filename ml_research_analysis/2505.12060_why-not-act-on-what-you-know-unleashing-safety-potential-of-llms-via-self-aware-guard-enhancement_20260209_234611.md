---
ver: rpa2
title: Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware
  Guard Enhancement
arxiv_id: '2505.12060'
source_url: https://arxiv.org/abs/2505.12060
tags:
- sage
- harmful
- safety
- jailbreak
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a safety gap in LLMs where models can detect
  harmful prompts as discriminators but still generate unsafe responses as generators.
  To bridge this gap, the authors propose SAGE, a training-free defense that couples
  a model's discriminative and generative capabilities.
---

# Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement

## Quick Facts
- arXiv ID: 2505.12060
- Source URL: https://arxiv.org/abs/2505.12060
- Reference count: 34
- This paper identifies a safety gap in LLMs where models can detect harmful prompts as discriminators but still generate unsafe responses as generators

## Executive Summary
This paper identifies a critical safety gap in large language models (LLMs) where models can detect harmful prompts as discriminators but still generate unsafe responses as generators. To address this, the authors propose SAGE (Self-Aware Guard Enhancement), a training-free defense mechanism that couples a model's discriminative and generative capabilities. SAGE employs a two-stage safety check (semantic and task structure analysis) to evaluate prompts and guide models to either refuse harmful requests or provide safe responses. Extensive experiments demonstrate that SAGE achieves a 99% average defense success rate against seven jailbreak methods across both open-source and closed-source LLMs while maintaining helpfulness on general benchmarks.

## Method Summary
SAGE introduces a training-free defense mechanism that bridges the gap between a model's ability to detect harmful prompts and its tendency to generate unsafe responses. The framework uses a two-stage safety check: first analyzing semantic content to identify potential harm, then examining task structure to assess complexity and intent. Based on these evaluations, SAGE guides the model to either refuse harmful requests or provide safe, contextually appropriate responses. This approach leverages the model's existing capabilities without requiring additional training, making it practical and scalable across different LLM architectures.

## Key Results
- Achieves 99% average defense success rate against seven jailbreak methods
- Maintains high helpfulness scores on general benchmarks
- Effective across both open-source and closed-source LLM architectures

## Why This Works (Mechanism)
The mechanism works by explicitly coupling a model's discriminative and generative capabilities that normally operate independently. When a model can identify harmful prompts but still generates unsafe responses, SAGE intervenes at the decision point between detection and generation. The two-stage safety check (semantic analysis followed by task structure analysis) provides a systematic evaluation framework that prevents the model from bypassing safety checks during the generation phase. This self-aware approach ensures that the model's safety knowledge translates into actual safe behavior.

## Foundational Learning
- LLM jailbreak methods - Understanding common adversarial techniques that circumvent safety measures is essential for evaluating defense effectiveness. Quick check: Review the seven jailbreak methods tested to understand attack diversity.
- Two-stage safety evaluation - The semantic analysis identifies harmful content while task structure analysis assesses complexity and intent, providing comprehensive protection. Quick check: Verify that both stages are necessary by examining ablation results.
- Training-free defense mechanisms - Avoiding retraining preserves model capabilities while adding safety layers, making the approach more practical and scalable. Quick check: Confirm no fine-tuning was performed on safety datasets.

## Architecture Onboarding

**Component map:** Input Prompt -> Semantic Analysis -> Task Structure Analysis -> Decision Node (Refuse/Generate) -> Safe Response

**Critical path:** The critical path flows through both safety analysis stages before reaching the decision node. Semantic analysis must complete first to identify potential harm, followed by task structure analysis to evaluate complexity and intent. The decision node then determines whether to refuse or generate a response.

**Design tradeoffs:** The two-stage approach trades computational overhead for comprehensive safety coverage. While single-stage methods might be faster, they risk missing complex jailbreak attempts that exploit task structure. The training-free design preserves model performance but may be less robust than fine-tuned alternatives.

**Failure signatures:** Failures occur when semantic analysis misses nuanced harmful content or when task structure analysis incorrectly assesses complex prompts. The system may also fail if jailbreak methods specifically target the decision-making logic between the two analysis stages.

**First experiments:** 
1. Test on simple harmful prompts to verify basic safety functionality
2. Evaluate against each jailbreak method individually to identify specific vulnerabilities
3. Measure performance impact on general helpfulness benchmarks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation methodology lacks detail on how jailbreak methods were selected and whether they represent the full spectrum of potential adversarial attacks
- 99% defense success rate lacks confidence intervals or statistical significance testing across different model versions and prompting conditions
- Mechanistic interpretability findings are presented as preliminary observations without rigorous statistical validation

## Confidence

**High confidence in:**
- The core observation that LLMs can distinguish harmful prompts while still generating unsafe responses demonstrates a real safety gap
- The SAGE framework's two-stage safety check approach is technically sound and methodologically clear

**Medium confidence in:**
- The reported 99% average defense success rate across all tested models and jailbreak methods, as this depends on the specific evaluation setup and attack methods chosen
- The effectiveness on long-context scenarios needs more validation

**Low confidence in:**
- The mechanistic interpretability findings about internal patterns, as these are presented as preliminary observations without rigorous statistical validation or ablation studies to confirm their causal relationship to safety performance

## Next Checks
1. Conduct comprehensive ablation studies to quantify the individual contribution of semantic analysis versus task structure analysis components to overall defense effectiveness.
2. Test SAGE against a broader and more diverse set of adversarial attack methods beyond the seven currently evaluated, including adaptive attacks specifically designed to bypass the two-stage safety check.
3. Evaluate performance consistency across multiple model versions and different temperature settings to ensure the defense mechanism is robust to model configuration variations and not overfitting to specific settings.