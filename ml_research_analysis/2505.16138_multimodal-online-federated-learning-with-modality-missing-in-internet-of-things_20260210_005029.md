---
ver: rpa2
title: Multimodal Online Federated Learning with Modality Missing in Internet of Things
arxiv_id: '2505.16138'
source_url: https://arxiv.org/abs/2505.16138
tags:
- modality
- learning
- data
- missing
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning from multimodal
  data in IoT environments where sensor failures cause missing modalities during training.
  The authors propose Multimodal Online Federated Learning (MMO-FL) and introduce
  the Prototypical Modality Mitigation (PMM) algorithm to handle missing modalities
  through prototype learning.
---

# Multimodal Online Federated Learning with Modality Missing in Internet of Things

## Quick Facts
- arXiv ID: 2505.16138
- Source URL: https://arxiv.org/abs/2505.16138
- Reference count: 36
- Primary result: Achieves sublinear regret bounds with prototype-based modality substitution in multimodal online federated learning

## Executive Summary
This paper addresses the challenge of learning from multimodal data in IoT environments where sensor failures cause missing modalities during training. The authors propose Multimodal Online Federated Learning (MMO-FL) and introduce the Prototypical Modality Mitigation (PMM) algorithm to handle missing modalities through prototype learning. PMM constructs and updates class-specific prototypes for each modality, which can substitute for missing modalities during training. Theoretical analysis shows that PMM achieves sublinear regret bounds. Experiments on UCI-HAR and MVSA-Single datasets demonstrate that PMM outperforms baselines including zero-filling and partial modality training, achieving superior accuracy even compared to settings with complete modalities. The framework supports quantization and delayed updates to reduce communication overhead while maintaining good performance.

## Method Summary
The paper proposes MMO-FL with PMM to handle missing modalities in IoT environments. PMM constructs class-specific prototypes (average feature representations) for each modality from clients with full data. When a modality is missing on a client, it substitutes the missing features with the corresponding global prototype based on the class label inferred from available modalities. This allows the model to continue training as if all modalities were present. The prototype updates follow a running average formula over time, making them more robust. The framework supports quantization and delayed updates to reduce communication overhead while maintaining good performance.

## Key Results
- PMM achieves sublinear regret bounds theoretically, with O(√T) for full modality and O(√T + T(1-β)) for missing modalities
- Experiments show PMM outperforms zero-filling and partial modality training baselines on UCI-HAR and MVSA-Single datasets
- Quantization and delayed updates reduce communication costs with only minor performance degradation
- PMM achieves superior accuracy compared to settings with complete modalities in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype substitution can effectively approximate missing modality features in online federated learning.
- Mechanism: The PMM algorithm constructs class-specific prototypes (average feature representations) for each modality from clients with full data. When a modality is missing on a client, it substitutes the missing features with the corresponding global prototype based on the class label inferred from available modalities. This allows the model to continue training as if all modalities were present. The prototype updates follow a running average formula over time, making them more robust.
- Core assumption: The class label inferred from available modalities is sufficiently accurate to select the correct prototype for the missing modality. The prototype (average feature) is a reasonable approximation of the true missing feature distribution for that class.
- Evidence anchors:
  - [abstract] "PMM constructs and updates class-specific prototypes for each modality, which can substitute for missing modalities during training."
  - [section VI A] "The local prototype is then defined as the average value of the features extracted by the modality encoder..." and "By integrating more prototypes, the system can better approximate the feature distribution of the missing modality..."
  - [corpus] Related paper: "Cross-modal prototype based multimodal federated learning under severely missing modality" (Reference [26] in paper).

### Mechanism 2
- Claim: Online learning with sublinear regret bounds is achievable in a multimodal federated setting even with missing modalities.
- Mechanism: The theoretical analysis demonstrates that with appropriate assumptions (convexity, Lipschitz continuity), the MMO-FL framework with gradient descent achieves a sublinear regret bound (specifically $O(\sqrt{T})$) when no modalities are missing. The regret bound is then extended to the missing modality case, introducing an additional term $O(T(1-\beta))$ where $\beta$ is the proportion of available modalities. While this term can be linear (and thus problematic for sublinearity), the PMM algorithm's mitigation aims to reduce the impact of this term, effectively allowing the learning to approach the performance of the full-modality case.
- Core assumption: The analysis relies on standard online convex optimization assumptions: the loss function is convex and Lipschitz continuous (Assumptions 1-2), and gradients/model parameters are bounded (Assumptions 3-4). It also assumes a minimum proportion of modalities is always available (Assumption 5, $\beta > 0$).
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that PMM achieves sublinear regret bounds."
  - [section V C] "MMO-FL with missing modality can achieve a regret bound of $O(\sqrt{T} + T(1-\beta))$... This term $O(T(1-\beta))$, which accounts for the missing modality, plays a crucial role..."

### Mechanism 3
- Claim: Quantization and delayed updates of prototypes can reduce communication overhead with minimal impact on learning performance.
- Mechanism: The robustness of the prototype substitution (it's an approximation, not requiring high precision) allows for using fewer bits (quantization) for transmission. Similarly, the persistent nature of the global prototype (updated via a running mean) means it does not need to be updated every round (delayed update). These strategies directly lower the communication cost and frequency between the server and clients.
- Core assumption: The prototype representation is robust enough that low-precision or slightly stale versions still provide a useful signal for mitigating missing modalities.
- Evidence anchors:
  - [section VI Remark 2 & 3] "the precision of this substitution may not be highly sensitive... Therefore, to further reduce communication overhead, quantization technique can be applied..." and "the OPC process does not need to be executed in every global round..."

## Foundational Learning

- Concept: **Online Convex Optimization (OCO)**
  - Why needed here: This is the theoretical backbone. The paper defines the goal as minimizing cumulative "regret" over T rounds, a core concept in OCO. Understanding regret bounds is essential to grasp the theoretical contributions.
  - Quick check question: How does the regret bound change when transitioning from a full-modality setting to one with missing modalities, and what does the term $O(T(1-\beta))$ signify?

- Concept: **Federated Learning (FL) & Horizontal Federated Learning (HFL)**
  - Why needed here: The MMO-FL framework is built on HFL. The paper discusses a server-client architecture where clients perform local updates on their data, and the server aggregates them. This is the base distributed learning system.
  - Quick check question: In the HFL context of MMO-FL, why does modality missing violate the "consistent feature space" assumption?

- Concept: **Prototype Learning**
  - Why needed here: This is the core mitigation strategy. PMM uses class-level prototypes (mean feature vectors) as proxies for missing data. Understanding how prototypes are constructed and used is key to understanding the algorithm.
  - Quick check question: How does the persistent global prototype differ from the temporal global prototype, and why is the former preferred for online learning with streaming data?

## Architecture Onboarding

- Component map:
  - IoT devices (clients) -> Modality-specific encoders -> Feature extraction -> Online gradient descent -> Prototype computation -> Model update upload
  - Central server -> Model aggregation -> Temporal prototype aggregation -> Persistent prototype update -> Global model broadcast
  - Data stream -> Time-divided rounds -> New samples arrival -> Modality missing detection -> Class label inference

- Critical path:
  1. **Initialization**: Server initializes the global model (encoders + head) and empty prototype collections.
  2. **Round `t` Start**: Server broadcasts global model $\Theta_t$ and persistent prototype collection $\bar{P}^t$ to all clients.
  3. **Client Local Step**:
      - Collect new data. Detect missing modalities.
      - **If full modality**: Compute features, perform OGD updates. Compute local prototypes $p^{t,m}_{k,c}$. Upload updated model $\Theta^t_k$ and prototypes.
      - **If missing modality**: Use $\bar{P}^t$ and class inference from available data to substitute missing features $\tilde{Z}$. Perform OGD with substituted features. Upload updated model $\tilde{\Theta}^t_k$.
  4. **Server Aggregation Step**:
      - Aggregate models: $\Theta_{t+1} = \frac{1}{K} \sum (\Theta^t_k \text{ or } \tilde{\Theta}^t_k)$.
      - Aggregate prototypes: Compute temporal global prototypes, then update persistent global prototypes $\bar{p}^{t,m}_c$.
  5. **Loop**: Proceed to round `t+1`.

- Design tradeoffs:
  - **Performance vs. Communication**: Using quantization (lower `b`) or delayed updates (higher interval) reduces communication/computation costs but incurs a performance penalty.
  - **Prototype Stability vs. Adaptability**: The persistent prototype formula $(t-1)\bar{p}_{t-1} + p_t / t$ is stable but may lag behind rapid distribution shifts. A more adaptive formula (e.g., with a forgetting factor) could track shifts better but might be noisier.

- Failure signatures:
  - **Lagging Performance**: If test accuracy plateaus early or degrades, the persistent prototypes may be outdated. Check if the data distribution is non-stationary.
  - **Incorrect Substitution**: If the model trained with PMM is significantly worse than the zero-filling (ZF) baseline, the class inference from available modalities is likely failing, leading to wrong prototype substitutions.
  - **Missing Gradient Updates**: If the model learns slowly, check that gradients for available modalities are being correctly computed and that the substitution mechanism is integrated into the computational graph.

- First 3 experiments:
  1. **Baseline Comparison**: Run MMO-FL with PMM against the benchmarks (FM, PM, ZF) on the UCI-HAR dataset. Plot test accuracy over rounds to confirm PMM eventually outperforms others.
  2. **Missing Rate Analysis**: Run the simulation with different modality missing rates (e.g., $\lambda \in \{0.3, 0.5, 0.7\}$) to quantify performance degradation and validate the theoretical analysis of missing modalities' impact.
  3. **Efficiency-Accuracy Trade-off**: Test PMM with quantization (e.g., `b=2, 4, 8, 32`) and delayed updates (intervals 1, 2, 4) to find a suitable operating point for resource-constrained IoT devices. Plot accuracy vs. communication cost.

## Open Questions the Paper Calls Out
- In future work, the authors aim to integrate real-time IoT multimodal data and develop practical testbeds to evaluate and enhance the MMO-FL framework.

## Limitations
- The theoretical regret extension to missing modalities introduces an $O(T(1-\beta))$ term that can be linear, undermining the sublinear regret claim
- The modality missing pattern simulation (same modality missing for all clients per round) is overly simplistic compared to real IoT scenarios
- Class inference method for prototype substitution is not specified, creating uncertainty about substitution accuracy

## Confidence
- Prototype substitution mechanism: **Medium** - The concept is sound, but performance depends heavily on class inference accuracy which is unspecified
- Theoretical regret bounds: **Low** - The extension to missing modalities is questionable with the linear $O(T(1-\beta))$ term
- Quantization and delayed updates: **High** - These are standard techniques with well-understood trade-offs demonstrated in experiments

## Next Checks
1. Test PMM with independent modality missing patterns across clients (rather than synchronized missing) to better simulate realistic IoT sensor failures
2. Implement and test alternative class inference methods (e.g., majority voting, weighted averaging) for prototype substitution to understand sensitivity to this choice
3. Verify the theoretical regret analysis by testing scenarios with different modality availability ratios $\beta$ to observe the practical impact of the $O(T(1-\beta))$ term