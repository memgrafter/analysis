---
ver: rpa2
title: 'CrafText Benchmark: Advancing Instruction Following in Complex Multimodal
  Open-Ended World'
arxiv_id: '2505.11962'
source_url: https://arxiv.org/abs/2505.11962
tags:
- agent
- instructions
- instruction
- tasks
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CrafText, a benchmark for evaluating instruction
  following in a complex multimodal environment with dynamic interactions. The benchmark
  features 3,924 instructions with 3,423 unique words across four task categories
  (Localization, Conditional, Building, and Achievement) in an open-ended world where
  objects change properties over time.
---

# CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World

## Quick Facts
- arXiv ID: 2505.11962
- Source URL: https://arxiv.org/abs/2505.11962
- Reference count: 40
- Key outcome: Benchmark with 3,924 instructions across 4 task categories, showing even well-trained agents struggle with dynamic environments and linguistic complexity

## Executive Summary
CrafText introduces a novel benchmark for evaluating instruction following in dynamic multimodal environments, featuring procedurally generated worlds where object properties change over time. The benchmark includes 3,924 instructions with 3,423 unique words across four task categories, designed to test generalization to novel instruction formulations and dynamically evolving task configurations. Experiments with PPO, Dynalang, and FiLM baselines demonstrate that current methods struggle with both linguistic complexity and dynamic environment changes, with success rates ranging from 0.05 to 0.45 on the training set.

## Method Summary
The benchmark uses Craftax as a base environment with CrafText wrapper providing instruction following tasks. Agents receive natural language instructions and visual observations, executing multi-step actions to achieve goals. The method employs 1,024 parallel environments, 1 billion timesteps of training over 48 hours on V100 GPUs. Baselines include PPO-T (PPO with frozen DistilBERT embeddings), PPO-T+ (adds GPT-4 generated plans), FiLM (feature-wise modulation), and Dynalang (model-based with T5 tokenizer). Success Rate (SR) measures the proportion of completed instructions across training, paraphrased, and new-object test splits.

## Key Results
- PPO-T achieves 0.40 SR on training set but drops to 0.36 on paraphrased test set
- PPO-T+ shows best new-objects generalization (0.28 SR) but highest paraphrase drop (0.45 to 0.35)
- Dynalang underperforms significantly (0.15 SR) due to complex textual instructions
- Specific object types show severe sparsity issues (IRON, COAL, PLANT SR < 0.02)

## Why This Works (Mechanism)

### Mechanism 1: Instruction-to-Goal Grounding via Frozen Language Embeddings
- Claim: Concatenating pre-trained text embeddings with visual features provides learned goal representation guiding policy decisions
- Mechanism: Agent extracts [CLS] token embedding from frozen DistilBERT, concatenates with CNN-encoded visual features, feeds joint representation through GRU policy network
- Core assumption: Pre-trained language models capture sufficient semantic structure to distinguish task-relevant features across paraphrased formulations
- Evidence anchors: [section 5] PPO-T integrates frozen DistilBERT embeddings by extracting [CLS] token from final hidden layer; [section 4] grounding function fg(I) maps instruction semantics to latent goal vector
- Break condition: If instructions require compositional reasoning not captured by [CLS] pooling (e.g., "build X after Y but before Z")

### Mechanism 2: FiLM-Based Visual Feature Modulation
- Claim: Conditioning visual encoder layers with instruction-dependent affine transformations enables flexible, task-aware visual processing
- Mechanism: FiLM layers compute transformation parameters (γ, β) from instruction embedding and apply feature-wise modulation: FiLM(h) = γ(h) · v + β(h)
- Core assumption: Instruction encoding contains sufficient information to compute meaningful modulation parameters learnable via policy gradients
- Evidence anchors: [section 5] FiLM applies feature-wise affine transformations to visual features conditioned on text; [Table 2] FiLM achieves 0.43 training SR and 0.26 new-objects SR
- Break condition: If visual features require multi-step spatial reasoning (e.g., "place block two steps left of the furnace")

### Mechanism 3: Plan-Augmented Instruction Decomposition
- Claim: Preprocessing complex instructions into structured action plans via GPT-4 improves generalization to novel goal configurations
- Mechanism: Each instruction passed to GPT-4 with domain-specific prompt (max 5 steps, Craftax vocabulary), generated plan embedded and provided alongside original instruction
- Core assumption: LLMs can accurately map natural language instructions to executable sub-goals within environment's action vocabulary
- Evidence anchors: [section 5] PPO-T+ extends PPO-T by introducing planning step; [Table 2] PPO-T+ achieves highest new-objects SR (0.28) vs. PPO-T (0.22)
- Break condition: If GPT-4 generates plans referencing non-existent objects or infeasible action sequences

## Foundational Learning

- Concept: **Partially Observable Markov Decision Processes (POMDPs) with Goal-Conditioning**
  - Why needed here: CrafText formalizes instruction following as goal-based POMDP where agent must infer latent goals from partial observations and instructions
  - Quick check question: Can you explain why maintaining a belief state b(s) is necessary when observations are partial and the world is dynamic?

- Concept: **Vision-Language Alignment via Shared Embedding Spaces**
  - Why needed here: All baselines require mapping text and pixels to common representation space for policy conditioning
  - Quick check question: What failure mode would you expect if text embedding space and visual feature space are misaligned?

- Concept: **Generalization Splits: Paraphrase vs. Novel Composition**
  - Why needed here: CrafText's dual evaluation protocol distinguishes linguistic robustness (same goal, new wording) from compositional generalization (new object combinations)
  - Quick check question: If model performs well on paraphrased test but poorly on new-objects test, what capability is likely missing?

## Architecture Onboarding

- Component map: Environment (Craftax + CrafText wrapper) -> Instruction Encoder (DistilBERT [CLS]) -> Visual Encoder (CNN) -> Fusion Module (Concatenation/FiLM) -> Policy Core (GRU) -> Actor/Critic Heads

- Critical path: 1. Episode starts → instruction + checker sampled from dataset 2. World procedurally generated 3. At each step: (visual_obs, instruction_embedding) → fusion → GRU → action 4. Checker evaluates state → reward 5. Episode terminates on success, death, or step limit

- Design tradeoffs:
  - Frozen vs. fine-tuned language encoder: Frozen DistilBERT for efficiency; fine-tuning may improve paraphrase robustness but risks overfitting
  - Planning overhead: PPO-T+ adds GPT-4 inference cost at dataset creation time (offline) but improves new-object generalization by ~27% relative
  - Checker function implementation: Human-coded JAX functions ensure correctness but limit scalability vs. learned verifiers

- Failure signatures:
  - Low training SR but high paraphrase drop: Model overfitting to specific phrasing patterns in training instructions
  - Near-zero performance on specific object types (e.g., PLANT SR=0.00): Sparse reward signals or missing preconditions in training distribution
  - Dynalang underperforming (0.15 SR): World model may require longer training or struggle with combinatorial text+vision prediction

- First 3 experiments:
  1. Baseline replication: Train PPO-T on Medium split for 48 hours (1B timesteps, 1024 parallel envs). Verify training SR ≈ 0.40 and paraphrase SR ≈ 0.36 match Table 2
  2. Ablation: FiLM vs. concatenation: Swap PPO-T's concatenation fusion for FiLM layers. Compare training curves and new-objects generalization to isolate effect of modulation architecture
  3. Checker function stress test: Manually inspect 20 episodes where checker returns success. Verify final states match instruction semantics. Report false-positive rate

## Open Questions the Paper Calls Out

- How does agent performance differ when trained on human-generated instructions compared to current GPT-generated dataset? The authors identify "absence of human-generated instructions" as main limitation, noting AI-generated text may lack nuance and context-rich depth
- Can current instruction-following methods adapt to dynamic conversational interactions involving negotiation and clarification? Paper identifies lack of "real-world interactive elements" as limitation, noting environment currently lacks dialogue involving negotiation or collaboration
- What specific architectural components are required for model-based agents like Dynalang to handle large vocabulary sizes in dynamic environments? Results show Dynalang achieves only 0.15 success rate, significantly underperforming compared to PPO variants

## Limitations
- Human-coded checker functions are not fully specified, creating reproducibility concerns
- Exact CNN architecture details and episode step limits remain underspecified
- FiLM layer integration specifics (which layers, parameter computation) are not fully detailed

## Confidence
- High confidence in benchmark design and evaluation protocol, given clear task formulation and well-defined generalization splits
- Medium confidence in baseline implementations due to missing architectural details and training configuration parameters
- Low confidence in cross-method comparisons until implementation details are clarified

## Next Checks
1. Manually audit 20 checker function outcomes against final episode states to verify false-positive rates and semantic correctness
2. Implement FiLM ablation by replacing PPO-T's concatenation fusion with modulation layers, then compare new-objects generalization to isolate architectural effects
3. Conduct tokenizer analysis on paraphrased instruction pairs to quantify embedding consistency across phrasings and identify potential overfitting patterns