---
ver: rpa2
title: 'RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability'
arxiv_id: '2504.10081'
source_url: https://arxiv.org/abs/2504.10081
tags:
- safety
- reasoning
- arxiv
- realsafe-r1
- deepseek-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RealSafe-R1, safety-aligned versions of DeepSeek-R1
  distilled models. The authors address safety concerns in open-source R1 models,
  which tend to comply with malicious queries.
---

# RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability
## Quick Facts
- arXiv ID: 2504.10081
- Source URL: https://arxiv.org/abs/2504.10081
- Reference count: 3
- Primary result: Safety-aligned DeepSeek-R1 models with 15k safety-aware reasoning trajectories, achieving significant safety improvements while preserving reasoning capabilities

## Executive Summary
RealSafe-R1 addresses critical safety concerns in open-source DeepSeek-R1 models by creating safety-aligned versions that refuse malicious queries without compromising reasoning performance. The approach uses explicit instruction-based safety training on a carefully constructed dataset of reasoning trajectories. The resulting models show substantial safety improvements across multiple attack benchmarks while maintaining competitive performance on reasoning tasks.

## Method Summary
The authors constructed a dataset of 15,000 safety-aware reasoning trajectories by using DeepSeek-R1 to generate responses with explicit instructions for expected refusal behavior. This dataset was used to fine-tune DeepSeek-R1 models, creating the RealSafe-R1 series. The safety training explicitly teaches models when and how to refuse harmful queries while maintaining their reasoning capabilities. The approach focuses on preserving the model's reasoning strength while adding robust safety guardrails.

## Key Results
- Safety improvements: Harmful scores drop from 0.73 to 0.27 on PAIR attacks and from 0.61 to 0.10 on PAP attacks for the 32B model
- Reasoning capability preserved: Maintained performance on MATH-500 and AIME 2024 benchmarks
- Open-source availability: Models released at https://huggingface.co/RealSafe

## Why This Works (Mechanism)
The method works by explicitly teaching models the expected refusal behavior through instruction-based fine-tuning on reasoning trajectories. By generating safety-aware trajectories using the base model itself, the approach captures realistic safety reasoning patterns. The explicit instruction format ensures models understand not just what to refuse, but how to reason about safety decisions in context.

## Foundational Learning
- Reinforcement Learning: Why needed - optimizes model behavior through reward signals; Quick check - verify reward shaping affects safety compliance
- Instruction Fine-tuning: Why needed - teaches models specific response patterns; Quick check - test model adherence to refusal instructions
- Safety Alignment: Why needed - ensures models refuse harmful queries; Quick check - evaluate performance on adversarial safety benchmarks

## Architecture Onboarding
**Component Map:** DeepSeek-R1 base model -> Safety dataset generation -> Instruction fine-tuning -> RealSafe-R1 model
**Critical Path:** Base model → Safety-aware trajectory generation → Fine-tuning with refusal instructions → Safety evaluation
**Design Tradeoffs:** Safety vs. reasoning capability preservation; dataset size vs. generalization; explicit instructions vs. implicit learning
**Failure Signatures:** Over-refusal on benign queries; failure to refuse novel attack patterns; degradation in reasoning performance
**First Experiments:** 1) Evaluate safety performance on PAIR/PAP benchmarks, 2) Test reasoning capability on MATH-500, 3) Check for over-refusal patterns on benign queries

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (15k trajectories) may limit generalization to novel attack patterns
- Dataset generated by DeepSeek-R1 could propagate existing biases or blind spots
- Evaluation relies on specific attack benchmarks that may not capture full safety vulnerability spectrum

## Confidence
- Safety improvements: Medium - methodology sound but dataset size and potential overfitting reduce real-world generalization confidence
- Reasoning capability preservation: High - well-documented benchmark performance and no reasoning task fine-tuning
- Open-source availability: High - models publicly available with clear documentation

## Next Checks
1. Test RealSafe-R1 models against broader range of safety benchmarks including human-in-the-loop red teaming
2. Evaluate model performance on diverse reasoning tasks not included in original training set
3. Conduct ablation studies to determine minimum effective dataset size and identify critical safety training components