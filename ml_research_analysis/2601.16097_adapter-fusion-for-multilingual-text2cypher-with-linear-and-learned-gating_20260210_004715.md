---
ver: rpa2
title: Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating
arxiv_id: '2601.16097'
source_url: https://arxiv.org/abs/2601.16097
tags:
- language
- multilingual
- arxiv
- fusion
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores adapter fusion for multilingual Text2Cypher,
  incrementally supporting new languages without full retraining. Language-specific
  LoRA adapters for English, Spanish, and Turkish are combined via uniform linear
  merging or learned fusion MLP with dynamic gating.
---

# Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating
## Quick Facts
- arXiv ID: 2601.16097
- Source URL: https://arxiv.org/abs/2601.16097
- Reference count: 11
- Adapter fusion enables incremental multilingual Text2Cypher with learned MLP gating, recovering ~75% of joint fine-tuning gains using a subset of data.

## Executive Summary
This work introduces adapter fusion for multilingual Text2Cypher, allowing incremental language support without full model retraining. By combining language-specific LoRA adapters using either uniform linear merging or a learned fusion MLP with dynamic gating, the method demonstrates that the fusion MLP recovers approximately 75% of the accuracy gains from joint multilingual fine-tuning while using only a subset of training data. The learned fusion approach outperforms linear merging across English, Spanish, and Turkish, offering a scalable and data-efficient solution for expanding multilingual capabilities.

## Method Summary
The approach employs LoRA adapters fine-tuned separately for English, Spanish, and Turkish, then merges their outputs via two strategies: uniform linear fusion and a learned fusion MLP with dynamic gating. The fusion MLP learns to combine adapter contributions adaptively, enabling incremental addition of new languages by training only a new adapter and lightweight MLP, without modifying the base model or retraining existing adapters. This design balances performance, data efficiency, and scalability for multilingual structured generation.

## Key Results
- The fusion MLP recovers approximately 75% of the accuracy gains achieved by joint multilingual fine-tuning using only a subset of training data.
- Learned fusion MLP outperforms uniform linear merging across all three languages (English, Spanish, Turkish).
- The approach enables incremental language addition by training only a new LoRA adapter and lightweight MLP, avoiding full model retraining.

## Why This Works (Mechanism)
The learned fusion MLP dynamically gates adapter contributions, allowing the model to adaptively combine language-specific knowledge rather than treating all adapters equally. This selective weighting recovers much of the performance lost when fine-tuning adapters separately, effectively approximating joint multilingual fine-tuning without the data and computational costs. The MLP’s learned gating captures complementary strengths of each adapter, leading to improved cross-lingual generalization in structured generation.

## Foundational Learning
- **LoRA adapters**: Low-rank adaptation layers inserted into transformer blocks; needed to efficiently adapt large models per language without full fine-tuning. Quick check: Verify adapter rank and layer placement for task compatibility.
- **Adapter fusion**: Combining outputs of separately trained adapters; needed to merge multilingual knowledge without retraining. Quick check: Ensure consistent adapter architectures and tokenization.
- **Dynamic gating**: MLP-based weighted combination of adapter outputs; needed to adaptively prioritize language-specific cues. Quick check: Validate gating network input/output dimensions.
- **Structured generation (Text2Cypher)**: Mapping text to Cypher query language; needed as the target task requiring precise output formatting. Quick check: Confirm benchmark dataset and evaluation metrics.

## Architecture Onboarding
- **Component map**: Text input → Base model → LoRA adapters (per language) → Adapter fusion MLP → Cypher output
- **Critical path**: Text → Base model → LoRA adapters → Fusion MLP → Output generation
- **Design tradeoffs**: Linear fusion is simpler but less effective; learned fusion is more accurate but adds MLP overhead and complexity. Incremental addition trades some performance for scalability.
- **Failure signatures**: Poor cross-lingual generalization if adapters are too dissimilar; degraded performance if fusion MLP overfits to training languages.
- **First experiments**: (1) Compare linear vs learned fusion on held-out languages; (2) Test incremental addition of a fourth language; (3) Ablate LoRA rank and fusion MLP depth.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to languages with vastly different linguistic structures or non-Latin scripts remains uncertain.
- Evaluation is limited to a single Text2Cypher benchmark, restricting generalizability to other structured generation tasks.
- Sample size of three languages is too small to conclusively establish efficient scaling to dozens or hundreds of languages.
- Computational overhead of training new MLP fusion networks per language is not quantified.

## Confidence
- Incremental language addition capability: Medium (demonstrated on three languages, but small sample size)
- Superiority of learned fusion over linear merging: High (consistent performance gains on tested languages and dataset)
- Scalability to diverse languages: Low (no evaluation on typologically distant or non-Latin script languages)

## Next Checks
1. Test the learned fusion MLP on languages from typologically distant families (e.g., Mandarin, Arabic, Finnish) to assess cross-linguistic generalization.
2. Evaluate the approach on alternative structured generation tasks such as Text2SQL or code generation to determine task transferability.
3. Conduct an ablation study varying LoRA adapter configurations (rank, layer depth) to identify optimal architectural choices and ensure robustness.