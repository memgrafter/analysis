---
ver: rpa2
title: Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport
arxiv_id: '2510.01153'
source_url: https://arxiv.org/abs/2510.01153
tags:
- transport
- loss
- optimal
- neural
- mnist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural network framework for solving optimal
  transport (OT) problems based on the Hamilton-Jacobi (HJ) equation. The method leverages
  the method of characteristics to derive closed-form, bidirectional transport maps
  without numerical integration, avoiding adversarial training and dual-network architectures.
---

# Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport

## Quick Facts
- **arXiv ID:** 2510.01153
- **Source URL:** https://arxiv.org/abs/2510.01153
- **Reference count:** 40
- **One-line primary result:** Neural network framework for optimal transport using Hamilton-Jacobi equations, achieving superior accuracy and efficiency without adversarial training.

## Executive Summary
This paper introduces a novel neural network framework for solving optimal transport (OT) problems based on the Hamilton-Jacobi (HJ) equation. By leveraging the method of characteristics, the approach derives closed-form, bidirectional transport maps without numerical integration, eliminating the need for adversarial training and dual-network architectures. A single neural network is trained using a hybrid loss function combining an implicit HJ loss and MMD loss, ensuring convergence to the optimal map. Theoretical analyses establish consistency and stability, with convergence results for Gaussian settings. Experiments on synthetic, color transfer, and MNIST datasets demonstrate superior accuracy, scalability, and efficiency compared to baselines.

## Method Summary
The method learns bidirectional optimal transport maps between probability distributions μ and ν using the Hamilton-Jacobi (HJ) equation and method of characteristics. A single implicit neural representation $u_\theta(x,t)$ is trained to minimize a hybrid loss combining the implicit HJ residual and MMD distance. The transport map is derived from the gradient of the HJ solution, and the framework supports general cost functions and class-conditional transport. The approach eliminates numerical integration and adversarial training, reducing computational complexity while maintaining theoretical guarantees of consistency and stability.

## Key Results
- Achieves high accuracy and low FID scores on MNIST and Fashion-MNIST datasets for class-conditional transport
- Demonstrates superior efficiency compared to baselines by eliminating adversarial training and numerical integration
- Shows strong performance on color transfer tasks with high EMD and histogram intersection scores
- Provides theoretical guarantees of consistency and stability for Gaussian settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The optimal transport map is uniquely characterized by the viscosity solution of the Hamilton–Jacobi (HJ) equation.
- **Mechanism:** The gradient of the HJ solution $u$ defines the velocity field of the transport, with the transport map $T$ derived from $\nabla u$.
- **Core assumption:** Cost function is strictly convex and sub-differentiable; solution $u$ is sufficiently regular (Assumption: $C^1$).
- **Evidence anchors:** [abstract] "...Hamilton–Jacobi (HJ) equation, whose viscosity solution uniquely characterizes the OT map." [section 4] "The viscosity solution to (5) is theoretically characterized by the following system of characteristic ordinary differential equations (CODEs)..."
- **Break condition:** Non-convex cost functions or discrete distributions without relaxation break the mapping.

### Mechanism 2
- **Claim:** The method eliminates numerical integration by leveraging an implicit solution formula for the HJ equation.
- **Mechanism:** Uses an implicit formula relating $u(x,t)$ to its initial value $u(x,0)$ along straight characteristic lines, allowing direct learning without ODE simulation.
- **Core assumption:** Characteristics are straight lines (valid for quadratic costs, generalized for others via Hamiltonian $h$).
- **Evidence anchors:** [abstract] "...deriving closed-form, bidirectional transport maps, thereby eliminating the need for numerical integration." [section 4] "Consequently, the CODE-based formulation addresses a key computational bottleneck..."
- **Break condition:** If Hamiltonian $h$ doesn't permit straight characteristics or implicit formula can't be evaluated efficiently, computational advantage is lost.

### Mechanism 3
- **Claim:** A single-network minimization framework converges to the optimal map by combining physics-informed (HJ) and data-fitting (MMD) losses.
- **Mechanism:** Jointly minimizes HJ residual ($L_{HJ}$) to enforce optimality and MMD ($L_{MMD}$) to match target distribution, removing unstable adversarial training.
- **Core assumption:** Distribution $\varrho$ for collocation points is strictly positive and covers relevant domain (Theorem 5.1).
- **Evidence anchors:** [abstract] "...eliminating adversarial training stages, thereby substantially reducing computational complexity." [section 5.1] "The overall loss combines the implicit HJ loss and the MMD loss..."
- **Break condition:** Misscaled weight $\lambda$ leads to either failing to satisfy HJ optimality condition (low $\lambda$) or failing to match target distribution effectively (high $\lambda$).

## Foundational Learning

- **Concept: Hamilton–Jacobi (HJ) Equations**
  - **Why needed here:** Core theoretical engine describing evolution of value function; gradients define optimal trajectories.
  - **Quick check question:** Can you explain why the gradient of the solution to an HJ equation represents a velocity field?

- **Concept: Method of Characteristics**
  - **Why needed here:** Property that HJ solutions can be tracked along characteristic curves; key innovation treats these as straight lines to avoid ODE solvers.
  - **Quick check question:** If a characteristic curve is a straight line, what information is required to define the transport map for a particle?

- **Concept: Viscosity Solutions**
  - **Why needed here:** Provides existence and uniqueness guarantees for non-smooth initial data common in real distributions.
  - **Quick check question:** Why is "viscosity" necessary for defining solutions to HJ equations in optimal transport contexts?

## Architecture Onboarding

- **Component map:** Input $(x,t)$ and samples → Network $u_\theta(x,t)$ → Loss (HJ + MMD) → Transport map $T_\mu^\nu(x) = x + t_f \nabla h(\nabla u_\theta(x,0))$

- **Critical path:**
  1. **Batching:** Sample collocation points $(x,t)$ uniformly and samples from $\mu, \nu$
  2. **Derivatives:** Compute $\nabla u_\theta$ (autograd) for HJ residual and transport map
  3. **Optimization:** Update weights to minimize combined weighted loss (Eq. 14)

- **Design tradeoffs:**
  - **Loss Weight ($\lambda$):** High prioritizes hitting target distribution but may violate HJ constraint; low enforces physics but might miss target
  - **Architecture:** ICNNs optional but can help satisfy regularity assumptions; paper uses standard MLPs for 2D tasks

- **Failure signatures:**
  - **Mode Collapse/Intersecting Trajectories:** Indicates HJ loss not dominating; map learning valid pushforward but not optimal transport
  - **Divergence:** Implicit HJ loss involves composition ($u(x-t\nabla u)$); if gradients explode, consider gradient clipping or smaller learning rates

- **First 3 experiments:**
  1. **2D Toy Dataset:** Train on Swiss Roll ↔ Moons; visualize transport lines to check if straight and non-intersecting (Section 6.1.1)
  2. **Gaussian UVP Benchmark:** Measure Unexplained Variance Percentage (UVP) on high-dimensional Gaussians vs ground-truth OT (Section 6.1.2)
  3. **Ablation on $\lambda$:** Vary loss weight on 2D dataset to observe shift from "valid map" to "optimal map" (Appendix C.1)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the stability analysis be extended from Gaussian settings with quadratic network parameterization to general neural network architectures and arbitrary distributions?
- **Basis in paper:** [explicit] "Extending the stability analysis to general neural architectures would provide a deeper theoretical understanding of our method and its convergence behavior."
- **Why unresolved:** Current stability proof relies on restricted quadratic parameterization and Gaussian assumptions for analytical tractability; general architectures introduce non-convexity.
- **What evidence would resolve it:** Convergence theorem for general neural network classes with explicit bounds, or counterexample showing instability in non-Gaussian settings.

### Open Question 2
- **Question:** How can the method be modified to achieve efficient direct optimal transport in high-dimensional pixel space without relying on latent-space compression?
- **Basis in paper:** [explicit] "Future directions include improving high-dimensional performance beyond latent-space implementations" and "we aim to enhance our approach by incorporating U-net architectures and directly performing OT in the pixel space."
- **Why unresolved:** Gradient evaluation becomes computationally expensive in high dimensions; latent-space approaches lose fine texture details due to VAE decoder limitations.
- **What evidence would resolve it:** Successful class-conditional OT results on pixel-space datasets (e.g., CIFAR, ImageNet) with texture preservation matching or exceeding latent-space methods.

### Open Question 3
- **Question:** What efficient strategies exist for enforcing the monotonicity condition (positive definite Jacobian) of the transport map, and does explicit enforcement improve practical performance?
- **Basis in paper:** [explicit] "Nonetheless, it remains an interesting direction for future research to investigate efficient strategies for enforcing monotonicity and to assess the potential benefits of doing so."
- **Why unresolved:** Theoretical consistency requires monotonicity, but method works empirically without explicit enforcement; trade-off between computational overhead and theoretical guarantees is unknown.
- **What evidence would resolve it:** Comparative experiments with/without monotonicity constraints showing improved convergence, stability, or accuracy, or proof that implicit enforcement through HJ loss is sufficient.

## Limitations

- **Scalability to high-dimensional data:** Method relies on latent-space compression for complex datasets like MNIST, limiting direct application to pixel-space without architectural modifications.
- **Generalization to non-convex costs:** Theoretical framework assumes strictly convex cost functions; extension to non-convex costs may break the viscosity solution characterization.
- **Hyperparameter sensitivity:** Optimal loss weight λ is task-dependent and not systematically explored, potentially limiting reproducibility across different applications.

## Confidence

- **High:** The theoretical foundation linking HJ equations to optimal transport (Mechanism 1) and the elimination of numerical integration (Mechanism 2) are well-established in the literature.
- **Medium:** The convergence of the single-network framework (Mechanism 3) relies on specific assumptions about the distribution ϱ and the weight λ, which are not fully validated across all experiments.
- **Low:** The scalability and efficiency claims for high-dimensional datasets (e.g., MNIST) are based on limited comparisons and may not hold for more complex distributions.

## Next Checks

1. **Robustness to Cost Functions:** Test the framework on non-convex cost functions (e.g., Coulomb or geodesic costs) to verify the breakdown condition of Mechanism 1.
2. **Hyperparameter Sensitivity:** Conduct a systematic ablation study on the loss weight λ across all tasks to identify optimal settings and failure modes.
3. **High-Dimensional Scalability:** Evaluate the method on larger, more complex datasets (e.g., CIFAR-10) to confirm efficiency and accuracy claims in higher dimensions.