---
ver: rpa2
title: Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing
arxiv_id: '2502.02153'
source_url: https://arxiv.org/abs/2502.02153
tags:
- safety
- tsdi
- arxiv
- language
- helpfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses vulnerabilities in safety-aligned language
  models that, despite overall safety improvements, still exhibit harmful responses
  in specific categories. The authors propose Token-level Safety-Debiased Inference
  (TSDI), a learning-free method that estimates and corrects safety bias during generation
  using randomly constructed prompts.
---

# Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing

## Quick Facts
- arXiv ID: 2502.02153
- Source URL: https://arxiv.org/abs/2502.02153
- Authors: Thien Q. Tran; Akifumi Wachi; Rei Sato; Takumi Tanabe; Youhei Akimoto
- Reference count: 40
- Key outcome: TSDI improves the safety-helpfulness trade-off Pareto front while maintaining safety on harmful prompts

## Executive Summary
This paper addresses a critical vulnerability in safety-aligned language models: despite overall safety improvements, models still produce harmful responses in specific categories like adult content. The authors identify that safety alignment induces context-free preference for negative/rejective tokens across all inputs, regardless of prompt content. They propose Token-level Safety-Debiased Inference (TSDI), a learning-free method that estimates and corrects this safety bias during generation using randomly constructed prompts. TSDI improves helpfulness while maintaining safety, effectively shifting the safety-helpfulness trade-off Pareto front without requiring model retraining.

## Method Summary
TSDI operates by first estimating position-specific safety bias through logit differences between the safety-aligned model and reference helpfulness model on randomly constructed prompts. The bias vector at each generation position is computed as the mean logit difference across 500 random (prompt, response) pairs constructed from a token pool (MMLU dataset). During inference, these bias vectors are subtracted from the output logits at each corresponding position via a LogitProcessor. The method requires pre-trained models: Alpaca-7B (SFT), DPO(H) for helpfulness, and SACPO for safety alignment with varying β/λ hyperparameters.

## Key Results
- TSDI improves helpfulness win rate from 0.59 to 0.67 (β/λ = 0.025, 200 iterations) while maintaining safety scores
- Results are robust to choice of sequence length L (5-20) and token pool source (MMLU vs MS MARCO)
- Safety bias varies by generation position and can be effectively estimated using random prompts
- The method shifts the safety-helpfulness Pareto front without requiring model retraining

## Why This Works (Mechanism)

### Mechanism 1: Safety Bias Emergence from Overfitting
Safety alignment induces context-free preference for negative tokens (e.g., "sorry", "cannot") across all inputs. The alignment process overfits to the preference dataset's refusal patterns, causing the implicit safety function gθ to have nonzero expectation over out-of-distribution inputs. This manifests as elevated logits for rejection-adjacent tokens regardless of prompt content.

### Mechanism 2: Position-Specific Bias Estimation via Random Prompts
The safety bias varies by generation position and can be estimated using randomly constructed token sequences. By computing bi = E[fπθ(x′ ⊕ y′1:i−1) − fπ*r(x′ ⊕ y′1:i−1)] over random pairs, TSDI isolates the position-dependent shift in the safety-aligned policy relative to the reference policy.

### Mechanism 3: Pareto-Front Improvement via Debiasing Without Retraining
Subtracting estimated bias improves helpfulness while maintaining safety by removing context-independent rejection pressure. The debiased policy π′θ has modified safety function g′θ(x, y1) = gθ(x, y1) − Ex′[gθ(x′, y1)], allowing appropriate responses to harmless prompts without reducing refusal rates on harmful inputs.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: TSDI operates on models aligned via DPO/SACPO. Understanding how DPO encodes preferences into policy logits via the reward-parameterization equivalence is necessary to interpret bias as implicit safety function gθ.
  - Quick check question: Given a reference policy πref and aligned policy πθ, how would you compute the implicit reward for a response y given x?

- **Concept: Constrained Alignment (Safe RLHF / SACPO)**
  - Why needed here: The paper's models use SACPO, which first aligns for helpfulness then safety. Understanding the Lagrangian formulation clarifies why safety alignment can introduce overfitting artifacts and how gθ relates to the safety constraint.
  - Quick check question: In SACPO, what is the role of λ and how does β/λ affect the strength of safety alignment?

- **Concept: Logit-Level Intervention**
  - Why needed here: TSDI directly modifies logits via subtraction. Understanding how logits relate to probabilities and how additive logit shifts correspond to multiplicative probability ratios is essential for debugging and extending the method.
  - Quick check question: If a bias vector bi has value +2 for token "sorry" and 0 for all other tokens, what is the approximate multiplicative change in P("sorry") after debiasing?

## Architecture Onboarding

- **Component map:** Reference model (π*r) -> Safety-aligned model (πθ) -> Bias estimation module -> Debiased inference wrapper

- **Critical path:**
  1. Train DPO(H) for helpfulness using preference dataset
  2. Apply SACPO for safety with varying β/λ and iterations
  3. Construct token pool from dataset; generate |D̃| random (x, y) pairs
  4. For each position i ∈ [1, L], compute bi = mean(fπθ − fπ*r) across all pairs
  5. During inference, apply p′θ(yi | ...) = softmax(fπθ(...) − bi)

- **Design tradeoffs:**
  - L (debiasing horizon): Larger L covers more tokens but increases estimation variance. Paper uses L=20; robust to L=5-10
  - |D̃| (estimation samples): More samples reduce variance but cost compute. Paper uses 500
  - Token pool source: Should be disjoint from alignment data semantics. Paper uses MMLU/MS MARCO with no observed sensitivity
  - Reference model choice: Must be pre-safety-alignment; using different helpfulness-aligned model may change bias estimates

- **Failure signatures:**
  - Debiasing reduces safety on vulnerable categories: Indicates context-free bias contributed meaningfully to safety
  - No helpfulness improvement: Bias estimation may have failed or unhelpfulness stems from deeper issues
  - Generation quality degrades: Excessive debiasing may remove intended behavior

- **First 3 experiments:**
  1. Bias visualization: Replicate Figure 4 for your model—compute logit differences for negative token groups across positions using random prompts
  2. Ablation on L and |D̃|: Test L ∈ {5, 10, 20} and |D̃| ∈ {100, 500, 1000} on held-out harmless-prompt set
  3. Category-wise safety validation: Apply TSDI and evaluate safety scores across all SALAD-Bench categories

## Open Questions the Paper Calls Out
- Can alternative debiasing methods, such as those operating on hidden states or representation spaces, further improve the safety-helpfulness trade-off beyond token-level correction?
- How robust is TSDI across model scales (e.g., 13B, 70B, proprietary LLMs) and diverse alignment algorithms (e.g., RLHF, DPO variants, preference mixing)?
- Does the choice of random token pool or bias estimation length L significantly affect TSDI performance when domains differ sharply from training (e.g., non-English, code, multimodal inputs)?

## Limitations
- Limited generalizability across model architectures and datasets beyond Alpaca-7B with PKU-SafeRLHF and SALAD-Bench
- Unknown safety robustness across vulnerable categories, particularly adult content, weapons, and illegal activities
- Weak theoretical grounding for random prompt distribution assumption and its disjointness from alignment data semantics

## Confidence

**High confidence:** Safety bias exists and can be measured via random prompts (direct experimental evidence in Figure 4)

**Medium confidence:** TSDI improves Pareto front of safety-helpfulness trade-off (consistent aggregate metrics but mechanism not fully proven)

**Medium confidence:** TSDI maintains safety on vulnerable categories (aggregate safety scores maintained but per-category validation not provided)

## Next Checks

1. **Category-wise safety validation:** Apply TSDI to models aligned with varying β/λ and test safety scores across all SALAD-Bench categories, with particular focus on adult content, weapons, and illegal activities.

2. **Cross-model generalization test:** Implement TSDI on a different model architecture (e.g., LLaMA-2-7B or Mistral-7B) aligned with a different preference dataset to determine if the method generalizes beyond the original experimental setup.

3. **Random prompt distribution validation:** Systematically test whether the random prompt distribution overlaps with semantics present in the safety preference dataset by measuring bias estimates when using token pools with varying degrees of semantic overlap with alignment data.