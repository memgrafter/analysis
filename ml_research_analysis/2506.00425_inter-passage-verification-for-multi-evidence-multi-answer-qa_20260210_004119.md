---
ver: rpa2
title: Inter-Passage Verification for Multi-evidence Multi-answer QA
arxiv_id: '2506.00425'
source_url: https://arxiv.org/abs/2506.00425
tags:
- answer
- question
- verification
- questions
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-answer question answering
  (QA), where questions can have many valid answers and require synthesizing evidence
  from multiple passages. Existing retrieval-augmented generation (RAG) systems struggle
  with this task due to difficulties in retrieving and synthesizing large numbers
  of evidence passages.
---

# Inter-Passage Verification for Multi-evidence Multi-answer QA

## Quick Facts
- arXiv ID: 2506.00425
- Source URL: https://arxiv.org/abs/2506.00425
- Authors: Bingsen Chen; Shengjie Wang; Xi Ye; Chen Zhao
- Reference count: 20
- The RI2VER framework achieves an average F1 score improvement of 11.17% on QAMPARI and RoMQA datasets for multi-answer QA

## Executive Summary
The paper addresses the challenge of multi-answer question answering (QA), where questions can have many valid answers and require synthesizing evidence from multiple passages. Existing retrieval-augmented generation (RAG) systems struggle with this task due to difficulties in retrieving and synthesizing large numbers of evidence passages. To tackle this, the authors propose a new framework called Retrieval-augmented Independent Reading with Inter-passage Verification (RI2VER). The framework first retrieves a large set of passages and processes each passage independently to generate an initial high-recall but noisy answer set. Then, it introduces an inter-passage verification pipeline that validates each candidate answer through three steps: (1) generating verification questions, (2) gathering additional evidence, and (3) verifying with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA datasets demonstrate that RI2VER significantly outperforms existing baselines across various model sizes, achieving an average F1 score improvement of 11.17%. Further analysis shows that the inter-passage verification pipeline is particularly beneficial for questions requiring multi-evidence synthesis.

## Method Summary
RI2VER addresses multi-answer QA by first retrieving a large set of passages and processing each passage independently through a reader model to generate candidate answers with high recall but potentially low precision. This initial noisy answer set is then refined through an inter-passage verification pipeline. The verification process operates in three stages: first, verification questions are generated for each candidate answer; second, additional evidence passages are retrieved based on these verification questions; and third, an inter-passage synthesis step validates the candidate answer by considering all available evidence. This approach enables the system to handle questions requiring multiple evidence passages for accurate answering while maintaining computational efficiency through independent passage processing.

## Key Results
- RI2VER achieves an average F1 score improvement of 11.17% over existing baselines on QAMPARI and RoMQA datasets
- The framework demonstrates consistent performance improvements across various model sizes
- Inter-passage verification is particularly effective for questions requiring multi-evidence synthesis

## Why This Works (Mechanism)
The RI2VER framework succeeds by addressing the fundamental challenge of multi-answer QA: the need to synthesize evidence from multiple passages while maintaining high recall for potentially numerous valid answers. By processing passages independently first, the system captures a broad range of candidate answers without the computational burden of joint processing. The inter-passage verification pipeline then systematically validates these candidates by generating targeted verification questions and gathering additional evidence, effectively filtering out incorrect answers while preserving correct ones. This two-stage approach balances the competing demands of comprehensiveness and precision that are critical for multi-answer QA tasks.

## Foundational Learning

1. **Multi-answer QA fundamentals** - Understanding that questions can have multiple valid answers requiring synthesis from various evidence sources is crucial for designing appropriate evaluation metrics and system architectures.

2. **Retrieval-augmented generation (RAG) limitations** - Standard RAG systems struggle with multi-answer scenarios because they typically optimize for single answers and may miss valid alternatives or fail to synthesize evidence properly.

3. **Independent passage processing** - Processing passages separately before synthesis enables high recall but introduces noise that requires subsequent verification steps.

4. **Inter-passage verification** - The concept of validating candidate answers across multiple passages through targeted questioning and evidence gathering represents a novel approach to improving QA accuracy.

5. **Multi-evidence synthesis** - The ability to combine information from multiple sources to arrive at correct answers is essential for complex QA tasks but computationally challenging to implement efficiently.

6. **High-recall vs high-precision trade-offs** - Understanding when to prioritize broad coverage versus accuracy is critical for designing effective QA pipelines, particularly in multi-answer contexts.

## Architecture Onboarding

**Component Map**: Passage Retrieval -> Independent Reading -> Answer Generation -> Verification Question Generation -> Additional Evidence Retrieval -> Inter-passage Synthesis -> Final Answer Selection

**Critical Path**: The most critical processing path is: Passage Retrieval → Independent Reading → Verification Question Generation → Additional Evidence Retrieval → Inter-passage Synthesis. This path represents the verification pipeline that transforms noisy initial answers into refined final outputs.

**Design Tradeoffs**: The framework trades computational efficiency for accuracy by using independent passage processing followed by targeted verification rather than joint processing of all passages. This design choice enables handling larger document collections but introduces latency from the verification pipeline.

**Failure Signatures**: System failures may manifest as: (1) missed valid answers due to insufficient initial retrieval, (2) false positives from verification questions that don't adequately challenge candidate answers, (3) incomplete synthesis when verification questions fail to capture all relevant evidence relationships, or (4) computational bottlenecks in the verification pipeline for extremely large passage sets.

**First 3 Experiments to Run**:
1. Measure precision-recall trade-off of independent reading stage alone versus with verification pipeline to quantify verification effectiveness
2. Ablation study removing each verification component (question generation, additional retrieval, synthesis) to identify critical elements
3. Stress test with varying numbers of passages to determine scalability limits and optimal retrieval parameters

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focuses on two datasets (QAMPARI and RoMQA), potentially limiting generalizability to other domains
- Multiple retrieval passes and complex verification steps may impact computational efficiency and latency in practical applications
- Inter-passage verification pipeline relies on quality of generated verification questions and additional evidence gathering, which could introduce errors

## Confidence

**High confidence**: The core methodology of independent passage reading followed by inter-passage verification is clearly articulated and experimentally validated. The reported performance improvements over baselines are substantial and consistent across different model sizes.

**Medium confidence**: While the framework shows strong results on the evaluated datasets, the specific benefits of each verification step (verification question generation, additional evidence gathering, inter-passage synthesis) are not fully isolated in ablation studies. The exact contribution of each component to the overall performance gain remains partially unclear.

**Medium confidence**: The paper claims effectiveness for questions requiring multi-evidence synthesis, but the analysis could benefit from more granular breakdowns of performance across different question types and evidence requirements.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of verification question generation, additional evidence gathering, and inter-passage synthesis to the overall performance gains.

2. Evaluate the framework on additional multi-answer QA datasets from different domains (e.g., biomedical, technical support) to assess generalizability beyond the current datasets.

3. Measure and report the computational overhead and latency introduced by the inter-passage verification pipeline compared to standard RAG approaches, particularly in real-time QA scenarios.