---
ver: rpa2
title: Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under
  Adaptive Fusion
arxiv_id: '2506.09999'
source_url: https://arxiv.org/abs/2506.09999
tags:
- learning
- feature
- multimodal
- mcil
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Multimodal Class-Incremental
  Learning (MCIL) across vision, audio, and text modalities, which traditional methods
  focusing on only vision and text cannot handle effectively. The authors propose
  a novel method based on multimodal pre-trained models that tackles catastrophic
  forgetting and leverages complementary information across modalities.
---

# Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion

## Quick Facts
- arXiv ID: 2506.09999
- Source URL: https://arxiv.org/abs/2506.09999
- Reference count: 40
- Primary result: State-of-the-art performance on three multimodal class-incremental learning benchmarks with Top-1 accuracy of 67.76% (miniARIC), 95.37% (ImageNet-ESC-19), and 96.74% (ImageNet-ESC-27)

## Executive Summary
This paper addresses Multimodal Class-Incremental Learning (MCIL) by leveraging pre-trained models to handle vision, audio, and text modalities. Traditional MCIL methods focusing on only vision and text struggle with catastrophic forgetting and fail to exploit complementary information across modalities. The authors propose a novel framework that combines incremental fine-tuning with adaptive fusion mechanisms to maintain performance across sequential task arrivals while effectively leveraging multimodal complementarity.

## Method Summary
The proposed method employs a Multimodal Incremental Feature Extractor (MIFE) using a Mixture-of-Experts (MoE) structure for incremental fine-tuning, preventing catastrophic forgetting while maintaining feature extraction quality. An Adaptive Audio-Visual Fusion Module (AAVFM) dynamically fuses visual and audio features based on a masking threshold mechanism and Pearson correlation coefficients. The framework is built on pre-trained AudioCLIP, using its image and text encoders for feature extraction. A novel multimodal class-incremental contrastive training loss optimizes the learning process across modalities and tasks.

## Key Results
- Achieves 67.76% average Top-1 accuracy on miniARIC benchmark (vs. 49.19% for next best method)
- Reaches 95.37% accuracy on ImageNet-ESC-19 and 96.74% on ImageNet-ESC-27
- Demonstrates state-of-the-art performance across multiple MCIL task settings
- Introduces two new MCIL-specific evaluation metrics for comprehensive performance assessment

## Why This Works (Mechanism)
The method works by addressing two core MCIL challenges: catastrophic forgetting and multimodal complementarity. The Mixture-of-Experts structure in MIFE allows selective activation of experts for new tasks while preserving knowledge of previous tasks through routing mechanisms. The adaptive fusion module dynamically adjusts audio-visual fusion weights based on feature correlation strength, ensuring that only complementary and reliable information contributes to final predictions. The contrastive training loss encourages discriminative feature learning across modalities and tasks.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to forget previously learned information when trained on new tasks. Critical for MCIL because tasks arrive sequentially and must be learned without retraining on old data.
- **Quick check**: Compare accuracy on old tasks before and after new task training to quantify forgetting.

- **Mixture-of-Experts (MoE)**: A neural network architecture with multiple specialized subnetworks (experts) and a gating mechanism to select which experts to use. Needed for selective adaptation to new tasks while preserving existing knowledge.
- **Quick check**: Verify expert activation patterns differ meaningfully across task types.

- **Modality fusion**: The process of combining information from multiple input sources (vision, audio, text). Essential for leveraging complementary information in MCIL scenarios.
- **Quick check**: Measure performance gain from fusion vs. individual modalities.

- **Pearson correlation coefficient**: A statistical measure of linear correlation between two variables, ranging from -1 to 1. Used here to quantify feature complementarity between audio and visual modalities.
- **Quick check**: Calculate r values between fused vs. individual modality predictions.

## Architecture Onboarding

Component map: Audio/Text/Image input -> Pre-trained AudioCLIP encoders -> MIFE (MoE) -> AAVFM -> Classification head

Critical path: Input modality features → MIFE feature extraction → AAVFM fusion → Contrastive loss optimization → Incremental learning

Design tradeoffs: The MoE structure increases parameter count but enables selective task adaptation; dynamic fusion adds computational overhead but improves complementarity exploitation; pre-training reduces sample complexity but limits flexibility for domain-specific features.

Failure signatures: Performance degradation on old tasks indicates catastrophic forgetting; low Pearson correlation values suggest modality misalignment; expert router collapse indicates inability to differentiate task types.

First experiments:
1. Baseline ablation: Replace MoE with standard fine-tuning to quantify forgetting reduction
2. Fusion ablation: Compare fixed-weight fusion vs. adaptive fusion under varying noise conditions
3. Pre-training ablation: Train from scratch vs. using pre-trained AudioCLIP to measure transfer learning benefits

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the proposed Adaptive Audio-Visual Fusion Module (AAVFM) be modified to maintain performance when one or more input modalities are completely missing during inference?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work will focus on extending this method to address MCIL challenges in scenarios involving missing modalities."
- Why unresolved: The current AAVFM relies on calculating the Pearson correlation coefficient $r$ between visual and audio features to determine fusion weights; this computation is undefined or breaks down if one modality is absent.
- What evidence would resolve it: A modified framework that utilizes synthetic features or unimodal fallback mechanisms, tested on datasets with intentionally dropped modalities, showing comparable performance to the current fused model.

### Open Question 2
- Question: Is the fixed masking threshold ($th=0.8$) in the fusion module optimal across diverse environments, or does it require dynamic, learnable adjustment to handle varying degrees of signal-to-noise ratios?
- Basis in paper: [inferred] Section II.B introduces a "masking threshold mechanism" with a fixed value ($th=0.8$) to filter weak modalities.
- Why unresolved: A fixed threshold assumes a consistent correlation distribution between modalities, which may not hold if the "strong" modality (vision) degrades in quality (e.g., low light) or if the definition of "weak" varies by class.
- What evidence would resolve it: An ablation study comparing the fixed threshold against a learnable, adaptive threshold network across datasets with artificially injected noise variance.

### Open Question 3
- Question: Does the Mixture-of-Experts (MoE) structure in MIFE generalize to other multimodal pre-trained backbones (e.g., ImageBind) without suffering from router collapse or feature misalignment?
- Basis in paper: [inferred] The method is built specifically upon AudioCLIP (Section II.A), utilizing its specific image and text encoders.
- Why unresolved: The router training relies on specific feature distributions from AudioCLIP; different backbones may embed modality features in distinct latent spaces, potentially causing the MoE routing weights to fail to activate the correct experts for incremental tasks.
- What evidence would resolve it: Experiments replacing AudioCLIP with alternative multimodal foundations (like ImageBind or CLIP) while keeping the MIFE structure constant, analyzing expert activation patterns.

## Limitations
- Evaluation relies on relatively small benchmark datasets with limited task boundaries and class counts
- Computational overhead from MoE structure and dynamic fusion mechanisms not thoroughly analyzed
- Adaptive fusion module performance depends on carefully tuned masking thresholds with incomplete sensitivity analysis

## Confidence
- **High Confidence**: The effectiveness of using pre-trained multimodal models as feature extractors for MCIL tasks
- **Medium Confidence**: The superiority of the Mixture-of-Experts structure in preventing catastrophic forgetting compared to standard fine-tuning
- **Medium Confidence**: The adaptive audio-visual fusion mechanism's contribution to overall performance
- **Medium Confidence**: The proposed MCIL-specific evaluation metrics meaningfully capture model performance differences

## Next Checks
1. **Large-scale validation**: Test the method on larger, more diverse datasets with longer task sequences (50+ tasks) to verify scalability and long-term performance stability.

2. **Resource efficiency analysis**: Conduct detailed runtime and memory consumption profiling during both training and inference to quantify the computational overhead of the MIFE and AAVFM components.

3. **Threshold sensitivity analysis**: Perform systematic experiments varying the masking threshold across a wider range to establish robust parameter selection guidelines and test the fusion module's stability.