---
ver: rpa2
title: Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification
arxiv_id: '2506.11036'
source_url: https://arxiv.org/abs/2506.11036
tags:
- person
- text
- image
- images
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an interactive cross-modal learning framework
  for text-to-image person re-identification (TIReID) that leverages multimodal large
  language models (MLLMs) to overcome limitations of offline models and training data.
  The proposed approach consists of two core components: (1) a Test-time Human-centered
  Interaction (THI) module that performs multi-round visual question answering to
  refine user queries through interactions with MLLMs, and (2) a Reorganization Data
  Augmentation (RDA) strategy that enriches and diversifies training texts through
  decomposition, rewriting, and reorganization.'
---

# Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification

## Quick Facts
- arXiv ID: 2506.11036
- Source URL: https://arxiv.org/abs/2506.11036
- Reference count: 40
- State-of-the-art Rank-1 accuracy: 91.78% on UFine6926 and 77.91% on CUHK-PEDES using THI

## Executive Summary
This paper introduces an interactive cross-modal learning framework for text-to-image person re-identification (TIReID) that leverages multimodal large language models (MLLMs) to overcome limitations of offline models and training data. The proposed approach consists of two core components: (1) a Test-time Human-centered Interaction (THI) module that performs multi-round visual question answering to refine user queries through interactions with MLLMs, and (2) a Reorganization Data Augmentation (RDA) strategy that enriches and diversifies training texts through decomposition, rewriting, and reorganization. Experiments on four benchmarks (CUHK-PEDES, ICFG-PEDES, RSTPReid, and UFine6926) demonstrate that the method achieves state-of-the-art performance, with Rank-1 accuracy reaching up to 91.78% on UFine6926 and 77.91% on CUHK-PEDES when using THI. The approach shows strong generalization across datasets and can be effectively applied as a plug-and-play enhancement to existing TIReID methods.

## Method Summary
The paper proposes a two-component framework for TIReID. First, the Test-time Human-centered Interaction (THI) module enables multi-round visual question answering with MLLMs to refine user queries interactively. When a user query is insufficiently descriptive, THI uses MLLMs to analyze the query image and generate follow-up questions that prompt users to provide more detailed information. Second, the Reorganization Data Augmentation (RDA) strategy enhances training data by decomposing complex descriptions into simpler sub-descriptions, rewriting them for clarity and diversity, and reorganizing them into new combinations. This augmented data helps models better understand diverse query formulations and improves generalization to challenging scenarios.

## Key Results
- Achieves state-of-the-art Rank-1 accuracy of 91.78% on UFine6926 dataset
- Improves Rank-1 accuracy to 77.91% on CUHK-PEDES when using THI
- Demonstrates strong generalization across four different benchmark datasets
- Shows effectiveness as a plug-and-play enhancement to existing TIReID methods

## Why This Works (Mechanism)
The framework works by addressing two fundamental limitations in TIReID: the gap between user query capabilities and system requirements, and the limited diversity in training data. THI bridges the human-system communication gap by enabling iterative refinement of queries through MLLM-powered visual question answering, allowing the system to extract more discriminative information from users. RDA addresses data scarcity and diversity issues by systematically enriching the training corpus with more varied and comprehensive textual descriptions. Together, these components enable the model to learn more robust cross-modal representations that can handle diverse and challenging query scenarios.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: AI models that can process and generate both visual and textual information. Why needed: THI relies on MLLMs to analyze images and generate contextually relevant questions. Quick check: Verify the MLLM can perform accurate visual question answering on person images.

**Cross-modal Representation Learning**: Techniques for mapping different data modalities (text and images) into a shared embedding space. Why needed: Core to matching textual descriptions with person images. Quick check: Measure alignment quality between text and image embeddings using retrieval metrics.

**Visual Question Answering (VQA)**: Task of answering questions about visual content. Why needed: THI uses VQA capabilities to analyze query images and generate follow-up questions. Quick check: Evaluate MLLM's ability to answer detailed questions about person attributes.

**Data Augmentation for Text**: Methods to expand and diversify textual training data. Why needed: RDA uses text augmentation to improve model generalization. Quick check: Measure diversity metrics (e.g., vocabulary richness) before and after augmentation.

## Architecture Onboarding

**Component Map**: User Query -> THI Module -> Refined Query -> TIReID Model -> Results; Training Data -> RDA Module -> Augmented Data -> TIReID Model

**Critical Path**: For inference: User Query → THI (multiple rounds) → Refined Query → Cross-modal Matching → Results. For training: Raw Data → RDA → TIReID Model Training → Inference.

**Design Tradeoffs**: THI introduces latency due to multi-round interactions but improves accuracy; RDA increases computational overhead during training but enhances model robustness. The plug-and-play nature sacrifices some potential performance gains from architectural integration.

**Failure Signatures**: Poor performance when user queries are already highly descriptive (THI adds little value), when MLLM visual understanding is weak, or when RDA introduces inconsistencies in attribute descriptions.

**First Experiments**: 1) Baseline TIReID performance without any enhancements; 2) THI-only evaluation to measure query refinement impact; 3) RDA-only evaluation to assess training data augmentation benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on MLLM quality, which may not be universally available or cost-effective
- THI introduces computational overhead and may not be practical for real-time applications
- RDA may introduce artifacts or inconsistencies if decomposition and rewriting are not carefully controlled
- Performance gains depend on access to well-performing MLLMs

## Confidence

**High Confidence**: Methodology for THI and RDA is clearly described and reproducible; experimental results on standard benchmarks are verifiable and show consistent improvements across multiple datasets.

**Medium Confidence**: Claim that method can be applied as "plug-and-play" enhancement requires further validation across broader range of architectures and datasets; state-of-the-art performance assertion should be interpreted cautiously given limited number of competing methods directly compared.

## Next Checks

1. Conduct ablation studies to quantify individual contributions of THI and RDA components to overall performance improvements.

2. Evaluate computational overhead and latency introduced by multi-round interaction process in THI across different hardware configurations.

3. Test generalization capability on cross-domain datasets that differ significantly in visual appearance and textual description styles from training data.