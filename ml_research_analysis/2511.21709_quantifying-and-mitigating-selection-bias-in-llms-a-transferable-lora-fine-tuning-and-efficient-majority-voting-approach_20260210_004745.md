---
ver: rpa2
title: 'Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning
  and Efficient Majority Voting Approach'
arxiv_id: '2511.21709'
source_url: https://arxiv.org/abs/2511.21709
tags:
- bias
- lora
- dataset
- across
- permutations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses selection bias in LLMs during multiple-choice
  question answering, where models favor options based on position or symbols rather
  than content. The authors introduce a novel label-free Permutation Bias Metric (PBM)
  that quantifies prediction inconsistencies across all answer permutations, along
  with Batch Question-Context KV caching (BaQCKV) for efficient majority voting and
  LoRA-1 fine-tuning that uses PBM as a differentiable loss.
---

# Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach

## Quick Facts
- arXiv ID: 2511.21709
- Source URL: https://arxiv.org/abs/2511.21709
- Reference count: 9
- Novel label-free Permutation Bias Metric (PBM) quantifies prediction inconsistencies across answer permutations

## Executive Summary
This paper addresses selection bias in large language models during multiple-choice question answering, where models favor options based on position or symbols rather than content. The authors introduce a novel label-free Permutation Bias Metric (PBM) that quantifies prediction inconsistencies across all answer permutations, along with Batch Question-Context KV caching (BaQCKV) for efficient majority voting and LoRA-1 fine-tuning that uses PBM as a differentiable loss. Experiments across four MCQ datasets show that BaQCKV reduces token usage by up to 90% while maintaining zero bias, and LoRA-1 decreases PBM bias by 58% and improves accuracy consistency by 27% on average, outperforming baselines like PriDe, BNP, and Gray.

## Method Summary
The authors propose a three-pronged approach to quantify and mitigate selection bias in LLMs for multiple-choice questions. First, they introduce the Permutation Bias Metric (PBM) that measures prediction inconsistencies across all possible answer permutations without requiring ground truth labels. Second, they develop Batch Question-Context KV caching (BaQCKV) to enable efficient majority voting by caching and reusing key-value states across permutations. Third, they introduce LoRA-1 fine-tuning that incorporates PBM as a differentiable loss term to directly optimize models for bias reduction. The approach is evaluated on four MCQ datasets, demonstrating significant improvements in both bias reduction and accuracy consistency while maintaining computational efficiency through token savings.

## Key Results
- BaQCKV reduces token usage by up to 90% while maintaining zero bias
- LoRA-1 decreases PBM bias by 58% and improves accuracy consistency by 27% on average
- Outperforms baseline methods (PriDe, BNP, Gray) in both bias reduction and accuracy

## Why This Works (Mechanism)
The approach works by addressing the fundamental issue of position and symbol bias in LLM MCQ predictions. By systematically permuting answer options and measuring prediction consistency across permutations, PBM reveals when models rely on superficial patterns rather than question content. The BaQCKV mechanism enables efficient computation of majority voting across permutations by reusing cached KV states, avoiding redundant computation. LoRA-1 fine-tuning directly optimizes the model to minimize PBM through gradient-based updates, effectively teaching the model to base predictions on question semantics rather than answer position or formatting.

## Foundational Learning

**Permutation Bias Metric (PBM)**: Measures prediction inconsistency across answer permutations without ground truth labels. Why needed: Enables bias quantification without requiring labeled data. Quick check: Compare PBM scores across datasets with known bias patterns.

**Batch Question-Context KV caching (BaQCKV)**: Caches and reuses key-value states across permutations for efficient majority voting. Why needed: Prevents redundant computation when evaluating multiple permutations. Quick check: Verify token savings by comparing with naive permutation evaluation.

**LoRA fine-tuning**: Low-rank adaptation technique for efficient model fine-tuning. Why needed: Enables bias mitigation without full model retraining. Quick check: Monitor parameter changes during LoRA-1 training.

**Majority voting mechanism**: Aggregates predictions across permutations to identify robust answers. Why needed: Reduces impact of position-based biases. Quick check: Test voting accuracy against individual permutation predictions.

**Selection bias in LLMs**: Tendency to favor certain answer positions or formats over content. Why needed: Understanding the problem is crucial for effective mitigation. Quick check: Analyze model predictions before and after bias mitigation.

## Architecture Onboarding

**Component map**: MCQ Question -> PBM Permutation Generator -> BaQCKV Cache -> LLM -> Prediction Aggregator -> LoRA-1 Fine-tuning -> Bias-Mitigated Model

**Critical path**: The core workflow processes questions through permutation generation, cached LLM inference, majority voting, and PBM-based fine-tuning updates.

**Design tradeoffs**: The approach trades computational overhead of permutation generation for improved bias reduction and accuracy consistency. The BaQCKV optimization significantly reduces this overhead.

**Failure signatures**: Models may still exhibit bias if answer permutations don't sufficiently vary position/formatting, or if the majority voting assumption fails (correct answers appear in minority permutations).

**3 first experiments**:
1. Measure PBM scores on baseline model across all four datasets
2. Compare token usage with and without BaQCKV on a sample question set
3. Evaluate accuracy consistency before and after LoRA-1 fine-tuning

## Open Questions the Paper Calls Out
The paper acknowledges that the evaluation focuses on multiple-choice formats only, leaving uncertainty about generalizability to open-ended or free-response settings. The claim that PBM can be applied "label-free" assumes that answer permutation distributions remain stable across domains, which may not hold for specialized or highly structured question types.

## Limitations
- Evaluation focuses only on multiple-choice question formats
- PBM's label-free assumption may not hold across all domains
- Computational overhead of generating all permutations may become prohibitive at scale

## Confidence

**Major Claim Clusters Confidence:**
- **PBM metric effectiveness**: High - The mathematical formulation is sound and the empirical results are compelling across multiple datasets.
- **BaQCKV efficiency gains**: High - Token reduction claims are well-supported by controlled experiments.
- **LoRA-1 fine-tuning improvements**: Medium - While improvements are demonstrated, the long-term stability of the bias-mitigated models across distribution shifts needs verification.
- **Transferability across domains**: Low - Limited testing across diverse domains suggests this needs more extensive validation.

## Next Checks

1. Test PBM and LoRA-1 performance on non-MCQ question formats (e.g., short answer, essay questions) to assess broader applicability.

2. Conduct stress tests with adversarially crafted MCQ datasets where correct answers appear in minority permutations to validate majority voting assumptions.

3. Evaluate model performance after extended deployment periods to assess whether selection bias re-emerges due to distribution shifts in real-world usage.