---
ver: rpa2
title: Knowledge Graph Completion with Mixed Geometry Tensor Factorization
arxiv_id: '2504.02589'
source_url: https://arxiv.org/abs/2504.02589
tags:
- knowledge
- hyperbolic
- graph
- geometry
- euclidean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mixed-geometry tensor factorization model
  (MIG-TF) for knowledge graph completion that combines Euclidean and hyperbolic embeddings
  within a single architecture. The method augments a pretrained Euclidean TuckER
  model with a novel hyperbolic interaction term based on tetrahedron pooling tensor
  factorization (TPTF), enabling better capture of hierarchical and non-hierarchical
  structural properties in knowledge graphs.
---

# Knowledge Graph Completion with Mixed Geometry Tensor Factorization

## Quick Facts
- arXiv ID: 2504.02589
- Source URL: https://arxiv.org/abs/2504.02589
- Authors: Viacheslav Yusupov; Maxim Rakhuba; Evgeny Frolov
- Reference count: 40
- Key outcome: New state-of-the-art results on FB15k-237 and YAGO3-10 with fewer parameters by combining Euclidean and hyperbolic embeddings

## Executive Summary
This paper introduces MIG-TF, a mixed-geometry tensor factorization model that combines Euclidean and hyperbolic embeddings within a single architecture for knowledge graph completion. The method augments a pretrained Euclidean TuckER model with a novel hyperbolic interaction term based on tetrahedron pooling tensor factorization (TPTF), enabling better capture of hierarchical and non-hierarchical structural properties. MIG-TF achieves state-of-the-art results on FB15k-237 and YAGO3-10 while using significantly fewer parameters than previous models, demonstrating that hybrid geometry approaches can outperform both purely Euclidean and purely hyperbolic models on complex real-world datasets.

## Method Summary
MIG-TF combines a frozen pretrained TuckER model (Euclidean geometry) with a trainable TPTF model (hyperbolic geometry) to create a hybrid score function. The method learns only the low-parametric hyperbolic additions to the pretrained model rather than training from scratch, achieving efficiency gains. The TPTF component uses tetrahedron pooling tensor factorization with squared Lorentz distance to avoid numerical instability issues, while the score function captures violations of the tetrahedron inequality in hyperbolic space. The architecture is trained using BCE loss with AdamW optimizer, and QR orthogonalization can be optionally applied to relation embeddings.

## Key Results
- Achieves new state-of-the-art results on FB15k-237 and YAGO3-10 datasets
- Uses significantly fewer parameters compared to previous Euclidean and hyperbolic models (e.g., ~5M vs 40M for RotH on FB15k-237)
- Outperforms both purely Euclidean and purely hyperbolic models on datasets with complex structural properties
- Demonstrates robustness to noise in training data
- Shows competitive performance on WN18RR with potential for further improvement through backbone substitution

## Why This Works (Mechanism)

### Mechanism 1
Mixed geometry captures both hierarchical and non-hierarchical graph structures that single-geometry models miss. Euclidean embeddings handle high-degree nodes with near-uniform connectivity, while hyperbolic embeddings capture power-law distributed hierarchical nodes. The combined score function lets each geometry compensate for the other's weakness.

### Mechanism 2
Violations of the tetrahedron inequality in hyperbolic space provide discriminative scoring signal for ternary entity-relation-entity interactions. In hyperbolic space, the inequality doesn't universally apply, and the score function is positive when violated (linked entities close) and negative when it holds (entities distant).

### Mechanism 3
Training only low-parametric hyperbolic additions to a frozen pretrained Euclidean model achieves competitive accuracy with significantly fewer total parameters. The pretrained TuckER provides general patterns while TPTF learns residual corrections for hierarchical patterns the Euclidean model misses.

## Foundational Learning

- **Lorentz hyperboloid geometry**: All TPTF computations occur in this space; squared distance is used to avoid arccosh differentiation issues. Quick check: Can you explain why squared Lorentz distance is preferred over geodesic distance for gradient-based optimization?

- **Tensor factorization for knowledge graphs**: The frozen Euclidean component is TuckER; understanding tensor contraction is essential for debugging. Quick check: How does Tucker decomposition differ from CP decomposition in modeling ternary interactions?

- **Knowledge graph link prediction evaluation**: All design choices optimize for filtered MRR and HR@k metrics. Quick check: Why must true triples be filtered during evaluation, and what happens if you don't filter?

## Architecture Onboarding

- **Component map**: Pretrained TuckER (frozen) -> TPTF (trainable) -> Combined score (sum of Euclidean and hyperbolic terms)

- **Critical path**: 
  1. Load pretrained TuckER weights and freeze all parameters
  2. Initialize hyperbolic embeddings with small variance
  3. Map to hyperboloid: $v \in \mathbb{R}^n \to [\sqrt{\beta + \|v\|_2^2}, v] \in \mathbb{R}^{n+1}$
  4. Forward: compute $S_E$ (frozen) + $S_H$ (trainable)
  5. BCE loss; backprop only through TPTF parameters

- **Design tradeoffs**:
  - Curvature $\beta$: Optimal range 1.0-1.5; lower $\to$ more Euclidean
  - Hyperbolic dimension $d_h$: Fixed at 50 in all experiments
  - QR orthogonalization: Helps WN18RR/YAGO3-10, hurts FB15k-237
  - Frozen vs. fine-tuned Euclidean: Authors chose frozen for parameter efficiency

- **Failure signatures**:
  - HR@k below TuckER baseline: Check initialization scale, curvature, or data loading issues
  - Training divergence: Curvature $\beta$ too high, or numerical instability in Lorentz distance computation
  - Good HR@10, poor HR@1: Score function produces correct ranking but imprecise absolute scores
  - MIG-TF_QR underperforms MIG-TF: Target dataset likely has correlated relations

- **First 3 experiments**:
  1. Reproduce on FB15k-237 (most heterogeneous, should show largest MIG-TF advantage)
  2. Curvature sweep on validation set to find optimal $\beta$ for new datasets
  3. Ablation comparing MIG-TF vs. MIG-TF_QR vs. pure TPTF vs. pure TuckER

## Open Questions the Paper Calls Out

- Would substituting the frozen TuckER component with a rotational model like RotE or ComplEx-N3 yield superior performance on highly hierarchical datasets like WN18RR?

- Does incorporating spherical geometry into the mixed geometry framework significantly improve the modeling of cyclic structures in knowledge graphs?

- Does fine-tuning the entire model end-to-end (rather than freezing the Euclidean component) result in higher link prediction accuracy?

## Limitations
- Effectiveness depends heavily on the structural properties of the target dataset
- Mixed-geometry approach may not generalize equally well to datasets with different power-law characteristics
- Frozen pretrained component assumes TuckER captures most structural information, which may not hold for datasets where hierarchical patterns dominate from the start

## Confidence

**High Confidence**: The core claim that mixed geometry improves expressivity over single-geometry models is well-supported by empirical results across three benchmark datasets.

**Medium Confidence**: The mechanism explaining why tetrahedron inequality violation provides discriminative power in hyperbolic space is theoretically sound but lacks direct empirical validation.

**Medium Confidence**: The claim about training only low-parametric additions achieving competitive accuracy assumes the pretrained Euclidean model provides a strong foundation, though the alternative of joint fine-tuning was not explored.

## Next Checks
1. Evaluate MIG-TF on a purely hierarchical dataset and a dataset with uniform connectivity patterns to verify the claimed advantage specifically holds for mixed-structure graphs.

2. Train MIG-TF with the Euclidean component unfrozen (joint training) versus frozen to quantify the parameter efficiency tradeoff and determine if the frozen approach sacrifices accuracy.

3. Create an ablation that replaces the TPTF scoring function with a simple hyperbolic distance measure to isolate the contribution of the tetrahedron inequality violation mechanism versus general hyperbolic representation.