---
ver: rpa2
title: Accuracy and Consumption analysis from a compressed model by CompactifAI from
  Multiverse Computing
arxiv_id: '2507.08836'
source_url: https://arxiv.org/abs/2507.08836
tags:
- tokens
- compressed
- energy
- consumption
- full-size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the CompactifAI compression method from Multiverse
  Computing, applied to the Llama 3.1 8B model, focusing on energy efficiency and
  accuracy. Using CodeCarbon for power measurement and Ragas for accuracy evaluation,
  the compressed model was compared to the full-size version.
---

# Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing

## Quick Facts
- arXiv ID: 2507.08836
- Source URL: https://arxiv.org/abs/2507.08836
- Reference count: 10
- Primary result: Compressed Llama 3.1 8B achieved 30-39% energy savings with minimal accuracy loss

## Executive Summary
This study evaluated CompactifAI, a tensor network-based compression method from Multiverse Computing, applied to the Llama 3.1 8B model. Using CodeCarbon for power measurement and Ragas for accuracy evaluation, the compressed model was compared against the full-size version across two token generation configurations (200 and 1000 tokens). The compressed model demonstrated substantial energy savings—reducing processing time by 5.68% (200 tokens) and 18.17% (1000 tokens), lowering CO2 emissions by 30.03% and 39.05% respectively, and decreasing total energy consumption by 30.04% and 39.09%. GPU energy savings were most significant at 43.55% (200 tokens) and 50.5% (1000 tokens). Accuracy analysis revealed the compressed model outperformed the full-size model in lexical metrics (ROUGE, BLEU) and semantic similarity while showing marginal declines in factual correctness. The findings demonstrate that CompactifAI achieves substantial energy savings with minimal accuracy loss, making it a viable method for sustainable AI deployment.

## Method Summary
The study compared a CompactifAI-compressed Llama 3.1 8B model against the full-size version using an inference-only approach. The experimental setup involved a Linux VM on OVH Cloud with NVIDIA Tesla V100S-PCIE-32GB GPU, Intel Xeon Gold 6226R CPU, and 43GB RAM. CodeCarbon 2.5.0 tracked energy consumption at the machine level while executing 104 curated questions across five categories. Two configurations were tested: 200 tokens (temperature 0.5) and 1000 tokens (temperature 0.1), both with top_k=50, top_p=0.5, and do_sample=False. Ground truth responses were generated using GPT-4o, and Ragas metrics (ROUGE, BLEU, Semantic Similarity, Factual Correctness, Answer Correctness, Response Relevancy) were computed to compare compressed and full-size outputs.

## Key Results
- Compressed model reduced processing time by 5.68% (200 tokens) and 18.17% (1000 tokens)
- Achieved 30.03% and 39.05% reduction in CO2 emissions for 200 and 1000 token configurations
- Showed lexical metric improvements: ROUGE (0.2545 vs 0.2471) and BLEU (0.1776 vs 0.1321) versus full-size model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Tensor network-based compression reduces GPU energy consumption proportionally more than CPU or RAM.
- **Mechanism**: CompactifAI uses tensor networks with other techniques to reduce parameter count, which directly lowers GPU memory bandwidth demands and compute intensity during inference.
- **Core assumption**: The compression preserves sufficient model capacity that inference quality degrades minimally.
- **Evidence anchors**: [abstract] "lowered CO2 emissions by 30.03% (200 tokens) and 39.05% (1000 tokens)"; [section 4.1.3] "GPU energy savings were most significant, at 43.55% (200 tokens) and 50.5% (1000 tokens)"
- **Break condition**: If tensor decomposition discards critical attention head parameters, factual correctness may degrade sharply (observed: factual correctness dropped from 0.6072 to 0.5732 in 200-token config).

### Mechanism 2
- **Claim**: Energy consumption scales linearly with output token count, and compressed models consume less energy per generated token.
- **Mechanism**: Fewer parameters means fewer floating-point operations per forward pass, translating to lower per-token energy cost.
- **Core assumption**: Linear regression adequately models the token-energy relationship.
- **Evidence anchors**: [section 4.1.4] "the server's energy consumption is very correlated with the number of token generated (98.65%)"; [section 4.1.4] "the compressed model consumes -25.60% energy to generate one token"
- **Break condition**: Non-linear attention mechanisms in very long sequences (beyond 1000 tokens tested) may break linear scaling assumptions.

### Mechanism 3
- **Claim**: Lexical and grammatical metrics (ROUGE, BLEU) can improve post-compression while factual correctness marginally declines.
- **Mechanism**: Compression may regularize the model, reducing verbose outputs and favoring concise, syntactically cleaner responses; however, capacity loss affects precise factual retrieval.
- **Core assumption**: Ground truth reference quality (generated by GPT-4o) accurately represents correct answers.
- **Evidence anchors**: [section 4.2.1] "ROUGE (200 tokens): Compressed Model (0.2545) > Full-size Model (0.2471)"; [section 4.2.4] "Factual Correctness (200 tokens): Full-size Model (0.6072) > Compressed Model (0.5732)"
- **Break condition**: For domain-specific applications requiring high factual precision (medical, legal), even marginal factual correctness drops may be unacceptable.

## Foundational Learning

- **Concept: Tensor network decomposition**
  - Why needed here: Understanding how CompactifAI achieves compression requires grasping how high-dimensional tensors can be factorized into lower-rank approximations.
  - Quick check question: Can you explain why a matrix factorization A ≈ UV reduces parameter count while preserving approximate function?

- **Concept: CodeCarbon energy tracking**
  - Why needed here: Interpreting the 30-39% energy reduction claims requires understanding how CodeCarbon estimates CPU/GPU power via hardware monitoring and RAPL proxies.
  - Quick check question: What are the limitations of software-based energy measurement versus physical power meters?

- **Concept: Ragas evaluation metrics**
  - Why needed here: Distinguishing between lexical overlap (ROUGE), semantic similarity (embedding-based), and factual correctness (NLI-based) clarifies why compressed models can excel at some metrics while trailing others.
  - Quick check question: Why might a model score higher on BLEU but lower on factual correctness?

## Architecture Onboarding

- **Component map**: Inference server (Linux VM) -> CodeCarbon tracker -> Model (Llama 3.1 8B variants) -> Ragas evaluator (with GPT-4o ground truth)

- **Critical path**:
  1. Deploy inference server on consistent hardware
  2. Initialize CodeCarbon tracker before model load
  3. Run 104-question Q&A task with specified token limits (200 or 1000)
  4. Terminate tracker post-generation; log energy/emissions
  5. Generate ground-truth responses with GPT-4o
  6. Compute Ragas metrics comparing compressed vs. full model outputs

- **Design tradeoffs**:
  - Shorter token limits (200) produce truncated responses, increasing variance in energy-per-token measurements
  - Using GPT-4o for ground truth introduces evaluator bias; compressed model may be inadvertently rewarded for GPT-like conciseness
  - Single-model test (Llama 3.1 8B) limits generalizability; larger models may show different compression benefits per vendor claim

- **Failure signatures**:
  - Factual correctness drop >5% suggests compression ratio too aggressive
  - GPU energy savings <20% indicates possible measurement misconfiguration (ensure exclusive GPU access)
  - High variance in 200-token tests (multiple responses hitting token limit) signals need for higher max_new_tokens

- **First 3 experiments**:
  1. Reproduce baseline: Run full-size Llama 3.1 8B on 104 questions with 1000-token limit, measure energy with CodeCarbon
  2. Ablate token limit: Test compressed model at 500-token limit to identify sweet spot between measurement variance and task completion
  3. Cross-framework comparison: Deploy compressed model under VLLM framework to quantify framework-level efficiency gains versus PyTorch baseline (explicitly noted as a study limitation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the energy cost of the compression process itself, and how many inferences are required to amortize this initial overhead?
- Basis in paper: [explicit] The authors state, "We do not have information about the impacts of the compression itself," noting the need to calculate when the "overall global impact would be positive."
- Why unresolved: The study measured operational inference consumption but excluded the computational resources required to create the compressed model.
- What evidence would resolve it: A lifecycle assessment measuring the energy consumed during the tensor network compression phase to calculate the break-even point.

### Open Question 2
- Question: How does CompactifAI compare to open-source quantization methods regarding the trade-off between energy reduction and accuracy loss?
- Basis in paper: [explicit] The authors note, "We did not compare with other methods of compression like open source ones (For example : quantization)."
- Why unresolved: While the authors cite previous internal tests showing quantization reduces accuracy, there is no direct head-to-head benchmark in this study.
- What evidence would resolve it: A controlled experiment benchmarking CompactifAI against standard quantization (e.g., 4-bit) on the same hardware and dataset.

### Open Question 3
- Question: Does the efficiency of CompactifAI improve with larger model sizes relative to the baseline?
- Basis in paper: [explicit] The authors acknowledge they "were not able to test other compressed models" and note the vendor claims "the bigger the model, the bigger the benefits."
- Why unresolved: The experiment was limited to the 8B parameter version of Llama 3.1.
- What evidence would resolve it: Replicating the methodology using the 70B or 405B parameter versions of the model.

## Limitations
- Hardware specificity limits generalization: Results obtained exclusively on NVIDIA Tesla V100S-PCIE-32GB GPUs may not extend to other architectures
- Single model architecture tested: Findings based solely on Llama 3.1 8B, limiting claims about broader model families
- GPT-4o ground truth introduces evaluator bias: Compressed model may be rewarded for outputs aligned with GPT-4o's style rather than absolute correctness

## Confidence
**High Confidence**: Energy consumption reduction metrics (30-39% total energy savings, 43-50% GPU-specific savings)
**Medium Confidence**: Accuracy metric improvements (ROUGE, BLEU, Semantic Similarity) within study scope
**Low Confidence**: Generalization to other models and compression techniques beyond tested configuration

## Next Checks
1. **Hardware Architecture Validation**: Repeat energy and accuracy measurements on alternative GPU architectures (NVIDIA A100, H100, or AMD MI300X) to assess hardware dependency of observed energy savings and determine if GPU-specific gains (43-50%) hold across platforms.

2. **Dataset Diversity Test**: Evaluate the compressed model against multiple question-answering datasets (Natural Questions, SQuAD, HotpotQA) to determine if accuracy improvements in ROUGE/BLEU are consistent or dataset-specific artifacts of the curated 104-question set.

3. **Compression Ratio Sweep**: Test multiple compression ratios of the CompactifAI method on Llama 3.1 8B to identify the inflection point where energy savings plateau and accuracy degradation becomes unacceptable, establishing optimal compression parameters for different deployment scenarios.