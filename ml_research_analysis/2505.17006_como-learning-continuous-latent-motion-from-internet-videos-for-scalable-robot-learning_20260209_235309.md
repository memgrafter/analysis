---
ver: rpa2
title: 'CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable
  Robot Learning'
arxiv_id: '2505.17006'
source_url: https://arxiv.org/abs/2505.17006
tags:
- motion
- arxiv
- learning
- latent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMo, a method to learn continuous latent
  motion representations from internet videos for scalable robot learning. The key
  challenge addressed is that existing discrete latent action methods suffer from
  information loss and struggle with complex, fine-grained dynamics.
---

# CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning

## Quick Facts
- arXiv ID: 2505.17006
- Source URL: https://arxiv.org/abs/2505.17006
- Reference count: 40
- Key outcome: Introduces CoMo method learning continuous latent motion from internet videos, achieving superior robot policy performance via zero-shot cross-domain transfer and dimensionality-constrained embeddings.

## Executive Summary
This paper addresses the challenge of learning continuous latent motion representations from action-less internet videos for scalable robot learning. Existing discrete latent action methods suffer from information loss and struggle with complex, fine-grained dynamics. CoMo introduces an early temporal feature difference mechanism that prevents model collapse by suppressing static appearance noise and avoiding shortcut learning. Guided by the information bottleneck principle, it constrains latent motion embedding dimensionality to balance action-relevant information and background noise. Critically, CoMo exhibits strong zero-shot generalization, generating continuous pseudo actions for previously unseen video domains, enabling unified policy learning using pseudo actions from various action-less video datasets combined with limited labeled robot data.

## Method Summary
CoMo trains a Motion-Enhanced Inverse Dynamics Model (ME-IDM) and Forward Dynamics Model (FDM) in two stages. The ME-IDM uses a ViT-Large encoder with MAE initialization, computing temporal feature differences between current and future frames rather than encoding future frames directly. This early feature difference mechanism prevents shortcut learning and model collapse. A Motion Q-former (4 layers, 768 hidden) outputs continuous latent motion embeddings (128-dim empirically optimal). The FDM reconstructs future frames from current frames and latent motion. For policy training, a diffusion-based DiT or autoregressive GPT-style policy is jointly trained on robot trajectories (with ground truth actions) and video trajectories (with CoMo-extracted pseudo-actions), achieving superior performance in both simulated and real-world settings.

## Key Results
- CoMo policies achieve higher success rates than baselines when co-trained with pseudo actions from internet videos
- LP-MSE of 0.784 vs 2.967 for discrete VQ methods, indicating better action-relevant information preservation
- S-PCFC metric of 0.901 vs 0.940+ for higher dimensions, demonstrating effective noise suppression through dimensionality constraint
- Zero-shot generalization enables extracting usable pseudo actions from human demonstrations and cross-embodiment videos

## Why This Works (Mechanism)

### Mechanism 1: Early Temporal Feature Difference Prevents Shortcut Learning
- Claim: Removing future frame features from encoder input and using only temporal feature differences suppresses static appearance noise and prevents model collapse into a future-frame predictor.
- Mechanism: The encoder receives [Ft, Dt] where Ft is current frame features and Dt = (Ft+n - Ft) is the temporal difference, rather than [Ft, Ft+n]. This forces the latent to encode motion dynamics rather than appearance.
- Core assumption: Static background information dominates over motion signals in standard video encodings, causing models to learn shortcuts.
- Evidence anchors:
  - [abstract] "CoMo employs a early temporal feature difference mechanism to prevent model collapse and suppress static appearance noise, effectively discouraging shortcut learning problem."
  - [section 3.1] "Specifically, we remove the direct encoding of future frame features and replace it with the features difference between current and future frame before the encoder input."
  - [corpus] LAOF paper similarly addresses "action-irrelevant distractors" in latent action learning, corroborating the distractor problem.

### Mechanism 2: Information Bottleneck via Dimensionality Constraint
- Claim: Constraining latent motion embedding dimensionality balances retaining action-relevant information while minimizing action-irrelevant noise.
- Mechanism: Lower dimensions force compression that preferentially discards noise; 128 dimensions empirically optimal. S-PCFC metric increases with dimensionality, indicating more static information leakage.
- Core assumption: Action-relevant information has lower intrinsic dimensionality than full visual appearance.
- Evidence anchors:
  - [abstract] "guided by the information bottleneck principle, we constrain the latent motion embedding dimensionality to achieve a better balance."
  - [section 4.1.2/Figure 3] "increasing the dimensionality of the latent motion embedding initially leads to improved policy success rates, but further scaling results in a decline... S-PCFC exhibits a marked improvement (from 0.730 to 0.940)."
  - [corpus] Related work lacks direct comparison on this specific dimensionality-scaling tradeoff; corpus evidence is weak for this mechanism.

### Mechanism 3: Zero-Shot Cross-Domain Transfer via Motion-Centric Representation
- Claim: CoMo trained on diverse internet videos can generate continuous pseudo actions for unseen domains (human videos, different robots) without fine-tuning.
- Mechanism: Motion representations trained on in-the-wild, ego-centric, and robot scenarios capture domain-agnostic dynamics rather than embodiment-specific features.
- Core assumption: Motion dynamics share common structure across embodiments and viewpoints.
- Evidence anchors:
  - [abstract] "CoMo exhibits strong zero-shot generalization, enabling it to generate continuous pseudo actions for previously unseen video domains."
  - [section 4.2/Table 5] Real-world experiments use CoMo trained on internet videos to extract latent motion from human videos, achieving 75.0/30.0/25.0 success rates on three tasks.
  - [corpus] UniVLA similarly targets "transferable knowledge across different" settings; MVP-LAM uses "cross-viewpoint reconstruction" for generalization.

## Foundational Learning

- Concept: Inverse Dynamics Models (IDM)
  - Why needed here: CoMo's core component infers latent motion from frame pairs; understanding a_t = f(o_t, o_{t+1}) is prerequisite.
  - Quick check question: Given two consecutive frames, can you explain what information an IDM extracts versus a forward dynamics model?

- Concept: Vector Quantization (VQ) in Representation Learning
  - Why needed here: Paper positions CoMo against discrete VQ-based methods; must understand tradeoffs between discrete and continuous representations.
  - Quick check question: Why does VQ-VAE introduce training instability and information loss compared to continuous embeddings?

- Concept: Information Bottleneck Principle
  - Why needed here: Guides dimensionality selection; latent should be maximally informative about action while minimally informative about irrelevant appearance.
  - Quick check question: If you double latent dimension and S-PCFC increases significantly, what does this indicate about your representation?

## Architecture Onboarding

- Component map: Video frame pair → ViT features → feature difference → Q-former → z_t → policy training
- Critical path: Video frame pair → ViT features → feature difference → Q-former → z_t → policy training. Errors in feature difference computation or Q-former attention propagate directly to motion quality.
- Design tradeoffs:
  - RGB-Diff vs. Fea-Diff: RGB difference amplifies low-level motion but may disrupt abstract motion; feature difference scales better to internet video data (Table 4).
  - Motion dimension: 128 balances LP-MSE (0.784) and S-PCFC (0.901); higher dims increase S-PCFC without improving LP-MSE.
  - Discrete vs. continuous: Discrete (VQ) has lower S-PCFC but much higher LP-MSE (2.967 vs. 0.784).
- Failure signatures:
  - High S-PCFC (~1.0): Model collapsed to encoding future frame appearance (shortcut learning).
  - High LP-MSE but low S-PCFC: Motion representation captures temporal dynamics but lacks action-relevant specificity.
  - Policy fails to improve with more video data: Pseudo actions may be noisy or domain gap too large.
- First 3 experiments:
  1. Validate ME-IDM on small dataset: Train with/without feature difference mechanism; report LP-MSE and S-PCFC. Expect S-PCFC ~1.0 without difference, ~0.90 with.
  2. Dimensionality sweep: Train with dimensions [32, 64, 128, 256]; plot success rate, LP-MSE, S-PCFC. Identify optimal point before S-PCFC rises sharply.
  3. Zero-shot transfer test: Train CoMo on internet videos (SAM-V, EgoVid, Droid), extract pseudo actions from held-out human demonstration videos, compute cosine similarity between same-action embeddings across different environments (as in Figure 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating explicit temporal supervision close the performance gap between latent motion and actual robot actions?
- Basis in paper: [explicit] Section 6 states there is still a "gap between the latent motion and actual robot action" and proposes "incorporating extra temporal supervision" as future work.
- Why unresolved: The current self-supervised method relies on frame reconstruction, which may not capture fine-grained dynamics as precisely as ground-truth actions.
- What evidence would resolve it: Demonstrating improved LP-MSE and policy success rates when integrating temporal signals like optical flow or point tracking into the motion encoder.

### Open Question 2
- Question: How can the dimensionality of continuous latent motion be scaled without reintroducing action-agnostic static noise?
- Basis in paper: [inferred] Finding 4 notes that increasing the embedding dimensionality increases S-PCFC (static noise), meaning "continuous latent motion representations cannot be simply scaled up."
- Why unresolved: The current architecture faces an information bottleneck where higher capacity inadvertently captures background appearance rather than pure motion.
- What evidence would resolve it: An architecture or regularization technique that maintains or lowers S-PCFC while increasing embedding dimensions to capture more complex dynamics.

### Open Question 3
- Question: To what extent do the proposed LP-MSE and S-PCFC metrics correlate with downstream policy success across diverse robotic architectures?
- Basis in paper: [explicit] Section 6 expresses the hope that these metrics will "facilitate the exploration and research of this direction," implying their generalizability needs further community validation.
- Why unresolved: While the metrics show correlation in the authors' specific diffusion and autoregressive setups, their reliability as low-cost proxies for general policy performance is not fully proven.
- What evidence would resolve it: A broad study confirming high correlation between these offline metrics and online task success rates across varied manipulators and policy architectures.

## Limitations
- Claims about scalability to internet video scale and computational efficiency lack empirical support
- Cross-domain generalization reliability needs validation across fundamentally different motion physics
- Information bottleneck mechanism evidence is indirect and lacks ablation on whether discarded information is truly action-irrelevant

## Confidence

**High:** CoMo's feature difference mechanism effectively prevents model collapse (S-PCFC < 0.9 consistently achieved)

**Medium:** Dimensionality constraint via information bottleneck improves policy performance; zero-shot cross-domain transfer is viable but requires further validation

**Low:** Claims about scalability to internet video scale and computational efficiency lack empirical support

## Next Checks

1. **Domain Gap Stress Test:** Train CoMo on videos from one embodiment (e.g., Baxter) and test pseudo-action quality on a robot with fundamentally different dynamics (e.g., quadruped). Measure LP-MSE and policy success rate degradation.

2. **Ablation on Motion Information:** Systematically remove high-frequency motion signals from training data and observe impact on LP-MSE vs. S-PCFC tradeoff. Determine if bottleneck discards task-critical motion details.

3. **Scalability Benchmark:** Profile training time and GPU memory usage for CoMo on varying dataset sizes (10K to 1M video clips). Compare against discrete VQ-based baselines to validate efficiency claims.