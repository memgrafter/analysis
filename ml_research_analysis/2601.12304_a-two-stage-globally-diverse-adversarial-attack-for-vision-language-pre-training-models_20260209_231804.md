---
ver: rpa2
title: A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training
  Models
arxiv_id: '2601.12304'
source_url: https://arxiv.org/abs/2601.12304
tags:
- adversarial
- attack
- s-gda
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes 2S-GDA, a two-stage globally-diverse adversarial
  attack framework for vision-language pre-training models. The method improves transferability
  by introducing a globally-aware textual perturbation mechanism and a visual perturbation
  module with multi-scale resizing and block-shuffle rotation.
---

# A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models

## Quick Facts
- **arXiv ID**: 2601.12304
- **Source URL**: https://arxiv.org/abs/2601.12304
- **Reference count**: 0
- **Primary result**: Proposes 2S-GDA, achieving up to 11.17% improvement in black-box attack success rates for vision-language pre-training models

## Executive Summary
This paper introduces 2S-GDA, a two-stage globally-diverse adversarial attack framework specifically designed for vision-language pre-training (VLP) models. The framework addresses the challenge of improving adversarial transferability in black-box settings by introducing novel perturbation mechanisms for both textual and visual components. The method demonstrates significant performance improvements over existing state-of-the-art baselines, particularly in cross-model black-box attack scenarios.

## Method Summary
2S-GDA operates through a two-stage process: first, it generates globally-aware textual perturbations using a diverse token substitution strategy that considers semantic relationships across the entire vocabulary; second, it applies visual perturbations through a combination of multi-scale resizing and block-shuffle rotation techniques. The framework is designed to be modular, allowing integration with existing adversarial attack methods to further enhance transferability. Experiments show consistent improvements across multiple VLP architectures and attack scenarios.

## Key Results
- Achieves up to 11.17% improvement in black-box attack success rates compared to state-of-the-art baselines
- Demonstrates consistent performance gains across multiple VLP model architectures
- Shows effective modular integration capabilities with existing adversarial attack methods

## Why This Works (Mechanism)
The effectiveness of 2S-GDA stems from its dual focus on both textual and visual components of vision-language models. The globally-aware textual perturbation mechanism diversifies the attack space by considering semantic relationships across the entire vocabulary rather than local token substitutions. The visual perturbation module, with its multi-scale resizing and block-shuffle rotation, introduces geometric diversity that enhances transferability across different model architectures. The two-stage approach ensures comprehensive coverage of both modalities while maintaining computational efficiency.

## Foundational Learning
1. **Adversarial Transferability**: The ability of adversarial examples crafted for one model to successfully attack different models. *Why needed*: Essential for black-box attack scenarios where the target model's architecture is unknown. *Quick check*: Verify transferability rates across different model architectures.
2. **Vision-Language Pre-training**: Models trained on large-scale image-text pairs to learn joint visual-linguistic representations. *Why needed*: Understanding VLP architecture is crucial for effective adversarial attacks. *Quick check*: Review VLP model architectures used in experiments.
3. **Global vs Local Perturbations**: Distinction between perturbations considering entire input space versus local neighborhoods. *Why needed*: Global perturbations can discover more diverse adversarial examples. *Quick check*: Compare global vs local perturbation strategies in ablation studies.

## Architecture Onboarding

**Component Map**: Input -> Global Textual Perturbation -> Visual Perturbation -> Adversarial Example -> Target VLP Model

**Critical Path**: The most critical components are the globally-aware textual perturbation mechanism and the multi-scale visual perturbation module. The interaction between these two stages determines the overall effectiveness of the attack.

**Design Tradeoffs**: The framework balances between perturbation diversity (globally-aware textual changes) and computational efficiency (two-stage approach). More complex perturbation strategies could potentially yield better results but at the cost of increased computation time.

**Failure Signatures**: Limited improvement in white-box settings, reduced effectiveness against models with strong adversarial defenses, and potential overfitting to specific VLP architectures.

**First Experiments**:
1. Baseline comparison with existing black-box attack methods on standard VLP benchmarks
2. Ablation study removing either the textual or visual perturbation component
3. Cross-architecture transferability tests across different VLP models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on black-box settings with limited white-box attack analysis
- Limited examination of robustness against common adversarial defense mechanisms
- Sparse empirical validation of modular integration claims with existing attack methods

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Improved black-box attack success rates vs baselines | High |
| Effectiveness of globally-aware textual perturbation | Medium |
| Modular framework compatibility with other methods | Medium |

## Next Checks
1. Evaluate 2S-GDA performance across a broader range of VLP architectures (e.g., CLIP, BLIP, ALBEF) to assess generalizability
2. Test the framework's effectiveness against common adversarial defense mechanisms to understand its robustness
3. Conduct ablation studies to quantify the individual contributions of the textual perturbation and visual perturbation modules to overall performance