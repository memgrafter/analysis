---
ver: rpa2
title: 'Agentic-R: Learning to Retrieve for Agentic Search'
arxiv_id: '2601.11888'
source_url: https://arxiv.org/abs/2601.11888
tags:
- search
- agent
- answer
- passage
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agentic-R introduces a retriever training framework tailored for\
  \ agentic search. Unlike traditional retrievers trained on static queries, Agentic-R\
  \ uses an iterative agent\u2013retriever optimization where the retriever is trained\
  \ on evolving, high-quality queries generated by the search agent."
---

# Agentic-R: Learning to Retrieve for Agentic Search

## Quick Facts
- arXiv ID: 2601.11888
- Source URL: https://arxiv.org/abs/2601.11888
- Authors: Wenhan Liu; Xinyu Ma; Yutao Zhu; Yuchen Li; Daiting Shi; Dawei Yin; Zhicheng Dou
- Reference count: 36
- Key outcome: Agentic-R consistently outperforms strong baselines across different search agents, improving both accuracy and search efficiency by reducing the number of required search turns

## Executive Summary
Agentic-R introduces a retriever training framework tailored for agentic search where traditional retrievers fall short. Unlike retrievers trained on static queries, Agentic-R uses an iterative agent-retriever optimization where the retriever is trained on evolving, high-quality queries generated by the search agent. The framework models passage utility using both local relevance (query-passage fit) and global answer correctness (whether the passage leads to the right final answer). Extensive experiments on seven benchmarks show that Agentic-R consistently outperforms strong baselines across different search agents, improving both accuracy and search efficiency.

## Method Summary
Agentic-R trains retrievers specifically for agentic search through a dual-phase iterative process. The framework first trains an agent using reinforcement learning with a frozen retriever, then constructs training data by generating agent trajectories and evaluating passage utility through both local relevance (LLM-based listwise scoring) and global answer correctness (EM score). The retriever is then trained via contrastive learning on this data. This process repeats for K=2 iterations, with the search agent and retriever optimized bidirectionally. The method uses E5-base-v2 as the retriever backbone, Qwen2.5-7B-Base as the agent backbone, and requires approximately 100 training steps for agents and 2 epochs for retrievers per iteration.

## Key Results
- Agentic-R consistently outperforms strong baselines across seven benchmarks
- Improves search efficiency by reducing the number of required search turns by 10-15%
- Achieves approximately 2-4 EM point improvements when used with different search agents
- Performance plateaus at iteration 3, with optimal results achieved at K=2 iterations

## Why This Works (Mechanism)

### Mechanism 1: Dual-Signal Passage Utility Modeling
Combines local relevance with global answer correctness to produce superior positive/negative passage labels for retriever training. Passages are ranked first by global answer correctness (EM score), then by local relevance for ties, prioritizing passages that lead to correct final answers over merely topically-similar passages.

### Mechanism 2: Bidirectional Agent-Retriever Co-optimization
Iteratively alternating between agent RL training and retriever contrastive learning produces compounding improvements that converge within 2 iterations. The improved retriever provides better passages during agent training, generating higher-quality queries for the next retriever training round.

### Mechanism 3: Context-Aware Query Input Formulation
Concatenating only the original question with current query (excluding historical queries) reduces retrieval noise in agentic search contexts. Agent-generated queries are typically self-contained with explicit intent, unlike conversational search where historical context matters.

## Foundational Learning

### Contrastive Learning with In-Batch Negatives
- Why needed: Agentic-R trains via contrastive loss using one positive passage against 15 sampled negatives plus in-batch and cross-device negatives (B×G×N−1 total negatives)
- Quick check: Can you explain why cross-device negatives expand the effective batch size without increasing memory per device?

### Reinforcement Learning with Token-Level Loss Masking
- Why needed: Agent training via PPO applies gradients only to LLM-generated tokens, masking retrieved passage tokens to prevent policy updates based on external content
- Quick check: What would happen to agent learning if retrieved passage tokens were included in the policy gradient computation?

### Listwise vs Pointwise Relevance Scoring
- Why needed: Agentic-R uses listwise scoring (comparing 20 passages simultaneously) rather than pointwise (scoring each passage independently) for more accurate relevance scores
- Quick check: Why might listwise scoring reduce calibration errors compared to pointwise scoring when evaluating passages of varying quality?

## Architecture Onboarding

### Component Map
Iteration Loop (K=2) -> Phase A: Agent Training (PPO) -> Phase B: Data Construction -> Phase C: Retriever Training (Contrastive) -> Back to Iteration Loop

### Critical Path
1. Initialize: E5-base-v2 as retriever backbone, Qwen2.5-7B-Base as agent backbone
2. Iteration 1: Train Agent_1 with E5 → Generate trajectories → Train Agentic-R_1
3. Iteration 2: Train Agent_2 with Agentic-R_1 → Generate trajectories → Train Agentic-R_2
4. Stop: Performance plateaus at iteration 3; deploy Agent_2 + Agentic-R_2

### Design Tradeoffs
- GAC computation requires generating final answers for each of 20 candidate passages per query, creating substantial computational overhead
- Positive passage selection requires GAC=1 AND LR≥60, ensuring quality but potentially reducing training data quantity
- Listwise scoring uses Qwen2.5-72B-Instruct, which is large; smaller models may suffice but weren't ablated

### Failure Signatures
- Search turn explosion: Monitor average search turns - should decrease (~10-15% reduction)
- GAC collapse: Check positive selection rate per query if no passages meet criteria
- Iteration 3 degradation: Performance drop suggests overfitting or distribution drift; stop at K=2

### First 3 Experiments
1. Baseline comparison: Run existing agent with Agentic-R_2 vs E5 on HotpotQA to validate out-of-domain transfer
2. Component ablation: Train Agentic-R_1 without GAC signal to isolate global correctness contribution
3. Search turn analysis: Measure average search turns with Agentic-R_2 vs REPLUG on TriviaQA

## Open Questions the Paper Calls Out
None

## Limitations

### Major Uncertainties & Limitations
- GAC computation cost is substantial but not reported or addressed with mitigation strategies
- Query distribution drift across iterations may affect learning stability but isn't analyzed
- Generalization boundaries aren't systematically explored - which query characteristics affect performance most

## Confidence

### Confidence Assessment
- **High Confidence**: Dual-signal passage utility modeling and its positive impact on retriever performance
- **Medium Confidence**: Context-aware query formulation excluding historical queries
- **Medium Confidence**: Stopping criterion at K=2 iterations

## Next Checks

1. **Cross-Agent Transfer**: Evaluate Agentic-R_2 with a completely different agent architecture to test transfer beyond training distribution

2. **Domain Adaptation Stress Test**: Systematically evaluate Agentic-R on progressively less similar domains to quantify generalization boundaries

3. **Cost-Performance Tradeoff Analysis**: Measure GAC computation overhead and experiment with cheaper approximations to determine if similar performance can be achieved at lower cost