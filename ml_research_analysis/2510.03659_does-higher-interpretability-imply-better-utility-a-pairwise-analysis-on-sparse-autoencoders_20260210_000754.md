---
ver: rpa2
title: Does higher interpretability imply better utility? A Pairwise Analysis on Sparse
  Autoencoders
arxiv_id: '2510.03659'
source_url: https://arxiv.org/abs/2510.03659
tags:
- steering
- interpretability
- features
- score
- saes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether higher interpretability of Sparse
  Autoencoders (SAEs) translates into better steering utility for controlling large
  language models (LLMs). To answer this, the authors train 90 SAEs across three model
  sizes, five architectures, and six sparsity levels, then measure interpretability
  using SAEBENCH and steering performance using AXBENCH.
---

# Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders

## Quick Facts
- **arXiv ID**: 2510.03659
- **Source URL**: https://arxiv.org/abs/2510.03659
- **Reference count**: 40
- **Primary result**: Higher interpretability of SAE features correlates weakly with steering utility, and a novel feature selection method improves steering performance by 52.52%.

## Executive Summary
This paper investigates the relationship between feature interpretability and steering utility in Sparse Autoencoders (SAEs) used for controlling large language models. Through systematic training of 90 SAEs across three model sizes, five architectures, and six sparsity levels, the authors measure both interpretability using SAEBENCH and steering performance using AXBENCH. The analysis reveals only a weak positive correlation (τ_b ≈ 0.298) between interpretability and utility, challenging the assumption that more interpretable features automatically yield better steering. The authors propose a novel feature selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution, achieving a 52.52% improvement over existing approaches.

## Method Summary
The authors conduct a comprehensive evaluation of SAEs by training 90 different configurations across three model sizes, five architectures, and six sparsity levels. They employ SAEBENCH to quantify feature interpretability and AXBENCH to measure steering performance. The novel Delta Token Confidence method is introduced as a feature selection criterion that evaluates how much feature amplification changes the next token distribution. This approach is validated against traditional interpretability metrics and existing feature selection methods, demonstrating significant improvements in steering performance while revealing that the most effective steering features are often not the most interpretable ones.

## Key Results
- Only weak positive correlation (τ_b ≈ 0.298) exists between SAE feature interpretability and steering utility
- Delta Token Confidence method improves steering performance by 52.52% compared to best existing approaches
- After feature selection using Delta Token Confidence, correlation between interpretability and utility vanishes or becomes negative
- The most effective steering features are not necessarily the most interpretable ones

## Why This Works (Mechanism)
The Delta Token Confidence method works by directly measuring the impact of feature amplification on the model's next token predictions, rather than relying on interpretability proxies. This utility-oriented approach identifies features that have the strongest influence on model behavior, regardless of their semantic interpretability. The method captures the practical effectiveness of features for steering tasks by quantifying their ability to change model outputs, which proves more relevant for control applications than traditional interpretability metrics.

## Foundational Learning
**Sparse Autoencoders (SAEs)**
- *Why needed*: SAEs decompose complex neural representations into sparse, interpretable features for mechanistic understanding and control
- *Quick check*: Verify that SAE reconstruction loss remains low while achieving desired sparsity levels

**Feature Interpretability Metrics**
- *Why needed*: Quantify how well individual features align with human-understandable concepts
- *Quick check*: Confirm interpretability scores correlate with manual feature annotation quality

**Steering Utility**
- *Why needed*: Measure practical effectiveness of features for controlling model behavior
- *Quick check*: Validate steering improvements on held-out prompts and tasks

**Delta Token Confidence**
- *Why needed*: Direct metric for feature utility in steering applications
- *Quick check*: Compare Delta Token Confidence scores with actual steering performance improvements

## Architecture Onboarding
**Component Map**
SAE Training -> Feature Selection (Delta Token Confidence) -> Steering Evaluation -> Interpretability Assessment

**Critical Path**
1. Train SAEs with varying architectures and sparsity levels
2. Compute Delta Token Confidence scores for all features
3. Select top-k features based on Delta Token Confidence
4. Evaluate steering performance on AXBENCH tasks
5. Correlate selected features' interpretability with utility

**Design Tradeoffs**
- Interpretability vs. utility: The most interpretable features may not be optimal for steering
- Sparsity level: Higher sparsity improves interpretability but may reduce steering capability
- Feature selection: Delta Token Confidence prioritizes utility over interpretability

**Failure Signatures**
- Low steering performance despite high interpretability scores
- Feature amplification causing model instability or divergence
- Delta Token Confidence failing to capture long-range steering effects

**First Experiments**
1. Compare Delta Token Confidence with random feature selection
2. Evaluate different k values for top-k feature selection
3. Test Delta Token Confidence on SAEs with varying sparsity levels

## Open Questions the Paper Calls Out
The paper highlights several open questions, including whether the findings generalize to more complex steering tasks beyond next-token prediction, how to design SAE architectures that optimize both interpretability and utility, and whether the Delta Token Confidence method can be extended to other representation learning frameworks.

## Limitations
- Focus on next-token prediction may not generalize to more complex steering tasks
- Correlation analysis limited to specific SAE architectures and training protocols
- Delta Token Confidence metric requires further validation across diverse model architectures
- Real-world deployment scenarios not fully explored

## Confidence
- **High Confidence**: Weak positive correlation between interpretability and utility (τ_b ≈ 0.298)
- **Medium Confidence**: 52.52% improvement in steering performance using Delta Token Confidence
- **Medium Confidence**: Interpretability and utility become negatively correlated after feature selection
- **Low Confidence**: Broader implication that utility-oriented SAE training is necessary

## Next Checks
1. Evaluate Delta Token Confidence across multiple steering objectives beyond next-token prediction
2. Test feature selection approach on SAEs trained with different architectures
3. Assess practical utility of selected features in real-world applications like bias mitigation