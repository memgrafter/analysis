---
ver: rpa2
title: Type and Complexity Signals in Multilingual Question Representations
arxiv_id: '2510.06304'
source_url: https://arxiv.org/abs/2510.06304
tags:
- complexity
- languages
- representations
- metrics
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how multilingual transformer models encode
  morphosyntactic properties of questions by introducing the Question Type and Complexity
  (QTC) dataset with 9,000 annotated questions across seven languages. The dataset
  includes categorical labels for question type (polar vs.
---

# Type and Complexity Signals in Multilingual Question Representations

## Quick Facts
- **arXiv ID**: 2510.06304
- **Source URL**: https://arxiv.org/abs/2510.06304
- **Reference count**: 19
- **Primary Result**: Neural probes consistently outperform statistical baselines for question type classification across seven languages, with fine-tuning showing mixed effects on complexity metric encoding.

## Executive Summary
This study investigates how multilingual transformer models encode morphosyntactic properties of questions through the introduction of the Question Type and Complexity (QTC) dataset containing 9,000 annotated questions across seven languages. The research employs selectivity-controlled probing methods to compare frozen model representations against subword TF-IDF baselines and fine-tuned models. The findings reveal that neural probes excel at question type classification while statistical methods demonstrate better selectivity for individual complexity metrics. Layer-wise analysis identifies three distinct encoding profiles, and fine-tuning exhibits a trade-off between adaptation and preservation of pre-trained linguistic knowledge.

## Method Summary
The researchers developed the QTC dataset with 9,000 questions across seven diverse languages, annotating both categorical question types (polar vs. content) and continuous complexity metrics including token count, lexical density, dependency length, tree depth, verbal arity, and subordination patterns. They employed selectivity-controlled probing with linear classifiers to evaluate frozen Glot500-m representations, subword TF-IDF baselines, and a fine-tuned model. The methodology systematically compared neural and statistical approaches across languages and encoding layers, examining how different properties are represented at various depths in the transformer architecture.

## Key Results
- Neural probes consistently outperform statistical baselines for question type classification, especially in languages requiring contextual integration
- Statistical methods demonstrate better selectivity on most individual complexity metrics compared to neural probes
- Three distinct layer-wise encoding profiles emerge: flat (surface-feature dependence), moderate (partial structural encoding), and high oscillation (unstable representation)
- Fine-tuning compensates for unstable neural encoding but degrades performance on metrics with stable layer-wise representations

## Why This Works (Mechanism)
The study's approach leverages the hypothesis that multilingual transformer representations contain structured information about linguistic properties that can be systematically extracted through probing. By using selectivity-controlled methods, the researchers can distinguish between genuine encoding of question properties and coincidental correlations. The comparison between frozen and fine-tuned models reveals how pre-trained knowledge interacts with task-specific adaptation, showing that fine-tuning can both enhance and diminish certain types of information depending on the stability of their layer-wise representations.

## Foundational Learning
- **Selectivity-controlled probing**: Why needed - to ensure probes extract genuine information rather than memorizing dataset artifacts; Quick check - compare performance on original vs. control datasets
- **Layer-wise analysis**: Why needed - to understand how information is distributed across transformer depths; Quick check - visualize performance across layers for different properties
- **Cross-linguistic evaluation**: Why needed - to identify universal vs. language-specific encoding patterns; Quick check - compare relative performance across languages for each property
- **Complexity metrics in NLP**: Why needed - to quantify linguistic properties that affect model behavior; Quick check - verify correlation between metrics and human judgments
- **Frozen vs. fine-tuned representations**: Why needed - to understand the trade-off between pre-training and adaptation; Quick check - measure information retention after fine-tuning
- **Morphosyntactic encoding**: Why needed - to bridge syntactic theory with neural representations; Quick check - compare probe results with linguistic annotation schemes

## Architecture Onboarding
- **Component map**: QTC dataset -> Frozen model representations -> Linear probes -> Performance evaluation -> Layer-wise analysis -> Fine-tuning experiments
- **Critical path**: Dataset creation → Property annotation → Representation extraction → Probe training → Performance comparison → Layer analysis → Fine-tuning → Trade-off assessment
- **Design tradeoffs**: Frozen models preserve pre-trained knowledge but may lack task-specific adaptation vs. fine-tuned models gain task relevance but risk losing general linguistic information
- **Failure signatures**: Poor selectivity indicates probe overfitting; flat layer-wise profiles suggest surface-level encoding; high oscillation indicates unstable representations
- **First experiments**: 1) Test probe performance on synthetic control datasets, 2) Visualize representation similarity across languages using dimensionality reduction, 3) Measure information-theoretic content of different layers for each property

## Open Questions the Paper Calls Out
None

## Limitations
- The QTC dataset, while multilingual, remains relatively small at 9,000 questions total, potentially limiting generalization
- Frozen model evaluation may not fully capture transformer potential when fine-tuning is allowed
- Probing methodology relies on linear classifiers that may miss non-linear encoding patterns for complex morphosyntactic properties

## Confidence
- **Question Type Classification Performance**: Medium - well-supported superiority of neural probes, but performance gaps vary significantly across languages
- **Complexity Metric Encoding**: Medium - statistical methods show better selectivity, but interpretation requires careful consideration of specificity vs. information content trade-offs
- **Layer-wise Encoding Profiles**: Low - three-profile characterization is based on observed patterns, but classification criteria and stability across architectures remain unclear

## Next Checks
1. Expand the QTC dataset to include at least 50,000 questions across the seven languages, ensuring balanced representation of different question types and complexity levels to test the robustness of observed patterns.

2. Conduct experiments with non-linear probing methods (e.g., multi-layer perceptrons, attention-based probes) to determine if the observed linear probe limitations mask more complex encoding patterns in the multilingual representations.

3. Perform cross-linguistic transfer experiments where probes trained on one language are evaluated on others, to assess the universality of the encoding patterns and identify language-specific vs. general question representation properties.