---
ver: rpa2
title: 'AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker
  Understanding'
arxiv_id: '2512.16250'
source_url: https://arxiv.org/abs/2512.16250
tags:
- speaker
- time
- arxiv
- token
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AMUSE, a benchmark for evaluating agentic
  multi-speaker understanding in multimodal models. It defines six tasks requiring
  reasoning over audio-visual dialogue, such as speaker association and temporal grounding,
  and evaluates models across zero-shot, guided, and agentic modes.
---

# AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding

## Quick Facts
- arXiv ID: 2512.16250
- Source URL: https://arxiv.org/abs/2512.16250
- Reference count: 26
- Key outcome: RAFT alignment framework improves multi-speaker audio-visual understanding accuracy by up to 39.52% across six benchmark tasks

## Executive Summary
AMUSE introduces a benchmark for evaluating agentic multi-speaker understanding in multimodal models, defining six tasks requiring reasoning over audio-visual dialogue. Current models show weak multi-speaker reasoning, especially in autonomous settings. To address this, the authors propose RAFT, a data-efficient alignment framework integrating reward optimization with selective reasoning adaptation. RAFT improves accuracy by up to 39.52% on AMUSE tasks, demonstrating its effectiveness in enhancing agentic reasoning. The work highlights the need for specialized alignment strategies for complex multi-speaker audio-visual understanding.

## Method Summary
The paper introduces AMUSE, a benchmark for evaluating agentic multi-speaker understanding in multimodal models. It defines six tasks requiring reasoning over audio-visual dialogue, such as speaker association and temporal grounding, and evaluates models across zero-shot, guided, and agentic modes. Current models show weak multi-speaker reasoning, especially in autonomous settings. To address this, the authors propose RAFT, a data-efficient alignment framework integrating reward optimization with selective reasoning adaptation. RAFT improves accuracy by up to 39.52% on AMUSE tasks, demonstrating its effectiveness in enhancing agentic reasoning.

## Key Results
- RAFT improves multi-speaker audio-visual understanding accuracy by up to 39.52% across six benchmark tasks
- Performance drops sharply from guided to agentic evaluation modes, indicating over-reliance on scaffolding
- Temporal coherence loss prevents speaker identity drift and modality mismatch, with 3.5-6.4 point STG improvements when included

## Why This Works (Mechanism)

### Mechanism 1
Reflective Reward Optimization (RRO) improves multi-speaker reasoning by weighting gradient updates based on perceptual correctness rather than text-only similarity. RRO samples K candidate responses, computes intrinsic rewards from four perceptual agents (Sync, Face, Speech, Diarization), and applies softmax-weighted regression updates. This reinforces outputs that align with ground-truth sensory evidence. Core assumption: Perceptual consistency scores provide reliable learning signal for multi-speaker tasks without requiring external reward models. Evidence anchors: [abstract] "RAFT...integrates reward optimization with intrinsic multimodal self-evaluation as reward"; [section 4.3] Eq. 4.3-4.4 define reward as f_perceptual(Sync, Face, Speech, Diarzation) with softmax-weighted updates; [corpus] Weak direct evidence—no corpus papers evaluate RRO-style intrinsic rewards for audio-visual agents. Break condition: If perceptual tools (Whisper, Pyannote, SyncNet) produce noisy outputs, reward signal degrades; Fig. 9 shows sensitivity to temperature β—extreme values destabilize training.

### Mechanism 2
Selective Reasoning Adaptation (SRA) achieves data-efficient improvement by restricting updates to cross-modal reasoning layers only. Decompose parameters θ = (θ_base, θ_cross). Zero gradients for θ_base, update only θ_cross. This focuses adaptation on audio-visual-text fusion pathways while preserving pretrained knowledge. Core assumption: Cross-modal reasoning blocks are the bottleneck for multi-speaker understanding; base vision/language layers are already sufficient. Evidence anchors: [section 4.3] "we restrict updates to cross-modal inference paths, improving interpretability and convergence"; [Table 15] SRA-0.5% achieves 54.1 avg score vs LoRA-5% at 53.5, using 10× fewer trainable parameters; [corpus] No corpus papers compare selective vs. full adapter tuning in multimodal settings. Break condition: If task requires updating low-level perception (e.g., new audio codecs), SRA's frozen base layers become a limitation.

### Mechanism 3
Temporal coherence loss prevents speaker identity drift and modality mismatch across time. L_temp penalizes divergence between audio, visual, and text embeddings at each timestep, plus enforces consistent evolution across timesteps. This regularizer is added to the main objective. Core assumption: Aligned cross-modal embeddings at each timestep correlate with better speaker tracking and grounding. Evidence anchors: [section 4.3 Eq. 4.5] L_temp = Σ_t(‖f_a(t)-f_v(t)‖² + γ‖f_t(t)-f_r(t)‖²); [Table 14] Ablating L_temp drops STG temporal IoU by 4.8-6.4 points across models; [corpus] Daily-Omni (arXiv 2505.17862) similarly emphasizes temporal alignment across modalities but uses different methodology. Break condition: If modalities naturally desynchronize (e.g., off-screen speakers, voice-overs), enforcing tight coherence may hurt performance.

## Foundational Learning

- **Cross-modal grounding**
  - Why needed here: All six AMUSE tasks require associating audio (speech) with visual (faces) and temporal (timestamps) modalities. Without understanding grounding, the benchmark design and RAFT's reward signals won't make sense.
  - Quick check question: Given overlapping speech from two visible speakers, how would you determine which face corresponds to which voice?

- **Policy gradient / RL basics**
  - Why needed here: RRO uses reward-weighted regression (Eq. 4.4), which builds on REINFORCE-style policy optimization. Understanding why softmax weighting stabilizes training vs. linear weighting (vs. GRPO) is critical for debugging.
  - Quick check question: In reward-weighted regression, why does centering rewards (r_i - r̄) before softmax improve stability?

- **Audio-visual diarization concepts**
  - Why needed here: AMUSE's agentic mode requires models to decide when to call tools like Pyannote (speaker diarization) and SyncNet (AV sync). Understanding what these tools output helps debug tool selection.
  - Quick check question: If two speakers talk simultaneously, can diarization assign each utterance to the correct speaker? What visual cues might help?

## Architecture Onboarding

- **Component map:**
Input: {x_audio, x_video, x_text}
        ↓
   [Perception Tools] ←─── Agentic mode only (Whisper, Pyannote, InsightFace, SyncNet)
        ↓
   [Base MLLM backbone] ←── Frozen (θ_base)
        ↓
   [SRA adapters] ←── Only θ_cross updated during RAFT
        ↓
   Output: {plan, act, reflect}
        ↓
   [Reward Computation] ←── RRO: f_perceptual from 4 agents
        ↓
   [Gradient Update] ←── L_RAFT = L_align + α·L_temp - β·J_RRO

- **Critical path:**
  1. Data preparation: 2,100 samples across 6 tasks (STG, AVDS, AVSA, NSP, SRID, CSNL)
  2. Insert SRA adapters into cross-modal layers only
  3. Sample K responses per input, compute perceptual rewards
  4. Apply softmax-weighted gradient update per Eq. 4.4
  5. Add temporal coherence loss; backprop through SRA only

- **Design tradeoffs:**
  - **Guided vs. Agentic evaluation:** Guided mode pre-computes tool outputs (deterministic but limited); agentic mode lets model decide (more flexible but unreliable—see Table 17: tool selection accuracy 69-96%)
  - **RRO vs. GRPO:** Paper claims softmax weighting more stable than GRPO's linear; ablation in Fig. 5 shows RAFT > GRPO > DPO > PPO
  - **Temperature β:** Range [0.3, 1.0] works best (Fig. 9); too low under-weights good samples, too high amplifies noise

- **Failure signatures:**
  - Performance drops sharply from guided → agentic (e.g., GPT-4o AVDS BLEU: 49.21 → 44.41) → indicates over-reliance on scaffolding
  - High speaker overlap (>0.3) causes >10 point accuracy drop (Fig. 11) → perceptual tools fail on overlapping speech
  - Removing L_temp causes 3.5-6.4 point STG drop (Table 14) → temporal drift without regularization

- **First 3 experiments:**
  1. **Baseline sanity check:** Run Qwen3-Omni-7B in zero-shot mode on all 6 AMUSE tasks. Compare against Table 3-5 values (e.g., AVSA zero-shot should be ~47.74%). If significantly lower, verify data pipeline and prompting.
  2. **SRA parameter sweep:** Train with SRA-0.2%, SRA-0.5%, SRA-1% on STG task only. Plot trainable params vs. tIoU. Confirm SRA-0.5% peaks as in Table 15.
  3. **RRO temperature ablation:** Fix all else, vary β ∈ {0.1, 0.3, 0.5, 0.75, 1.0, 1.5}. Plot average AMUSE score. Verify U-shaped curve matches Fig. 9—this validates reward weighting mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RAFT's reward-aligned optimization principles generalize to broader multimodal reasoning tasks beyond multi-speaker audio-visual understanding, particularly in embodied environments and vision-language domains?
- Basis in paper: [explicit] The conclusion states: "Future work can aim to generalize the reward-aligned optimization principles to broader multimodal reasoning tasks. By integrating temporal alignment with reflective feedback across vision–language and embodied environments, such extensions could enable a unified framework for improved grounding and cross-modal interaction."
- Why unresolved: RAFT was only validated on AMUSE's six multi-speaker tasks; its transferability to other domains (e.g., embodied navigation, image-based reasoning) remains untested.
- What evidence would resolve it: Applying RAFT to established embodied AI benchmarks (e.g., ALFRED, Habitat) and vision-language tasks (e.g., VQA, image captioning) and demonstrating consistent improvements.

### Open Question 2
- Question: How can models be improved to handle high-overlap multi-speaker scenarios where performance remains poor even after RAFT training?
- Basis in paper: [explicit] The paper notes in Appendix D.2.1: "Even with RAFT training, which improves grounding and temporal consistency, high-overlap scenarios remain challenging due to rapid turn-taking, overlapping utterances, and visual occlusions."
- Why unresolved: Current methods including RAFT show degraded performance as speaker overlap ratios increase, indicating fundamental limitations in handling concurrent speech.
- What evidence would resolve it: Developing and evaluating methods specifically targeting high-overlap conditions, measuring performance across controlled overlap ratios on AMUSE.

### Open Question 3
- Question: What internal representations would enable MLLMs to maintain performance when transitioning from guided (cue-provided) to autonomous agentic evaluation?
- Basis in paper: [inferred] The paper finds "Performance declines with autonomy" and "Over-reliance on guided prompts...reasoning quality, turn attribution, and temporal flow degrade sharply without instructions. This indicates that prompt scaffolding currently substitutes for robust multimodal representations."
- Why unresolved: The paper identifies the symptom (dependence on external structure) but does not propose solutions for building more robust internal temporal and speaker representations.
- What evidence would resolve it: Ablation studies identifying which internal representations correlate with smaller performance gaps between guided and agentic modes, followed by targeted training interventions.

## Limitations
- Performance drops sharply from guided to agentic evaluation modes, suggesting the framework may not generalize well to truly autonomous settings without extensive scaffolding
- Temporal coherence regularization shows sensitivity to speaker overlap ratios, with accuracy dropping >10 points when overlap exceeds 0.3
- The paper lacks detailed hyperparameter specifications for RAFT training, particularly the exact layer selection for SRA adapters and reward function weights

## Confidence
- **High confidence**: The benchmark design and task definitions are clearly specified and reproducible. The AMUSE dataset construction methodology and evaluation metrics are well-documented.
- **Medium confidence**: The RAFT framework components (SRA and RRO) are conceptually sound, but exact implementation details are missing. Performance improvements are demonstrated but may not fully transfer without precise hyperparameters.
- **Low confidence**: Agentic mode results are substantially weaker than guided mode, raising questions about the practical applicability of the approach in truly autonomous multi-speaker understanding scenarios.

## Next Checks
1. Implement hyperparameter sensitivity analysis for RAFT training, specifically testing β temperature values outside the reported [0.3, 1.0] range and SRA adapter sizes beyond 0.2-0.5%.
2. Evaluate RAFT performance on out-of-distribution multi-speaker scenarios with higher overlap ratios (>0.5) to assess robustness beyond the curated AMUSE dataset.
3. Compare RAFT's agentic mode performance against alternative tool-calling strategies (e.g., ReAct, ToolFormer) to isolate whether improvements come from alignment or tool selection methodology.