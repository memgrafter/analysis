---
ver: rpa2
title: Development and Validation of Engagement and Rapport Scales for Evaluating
  User Experience in Multimodal Dialogue Systems
arxiv_id: '2505.17075'
source_url: https://arxiv.org/abs/2505.17075
tags:
- engagement
- rapport
- dialogue
- were
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed and validated engagement and rapport scales
  for evaluating user experience in multimodal dialogue systems within foreign language
  learning contexts. Based on theories from educational psychology, social psychology,
  and second language acquisition, the researchers designed 21 questionnaire items
  to measure behavioral, cognitive, emotional, and social engagement, as well as face
  management, mutual attentiveness, and coordination aspects of rapport.
---

# Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems

## Quick Facts
- arXiv ID: 2505.17075
- Source URL: https://arxiv.org/abs/2505.17075
- Reference count: 18
- Primary result: Validated 21-item questionnaire measuring engagement and rapport that differentiates between human and AI dialogue partners in language learning contexts

## Executive Summary
This study developed and validated engagement and rapport scales for evaluating user experience in multimodal dialogue systems within foreign language learning contexts. Based on theories from educational psychology, social psychology, and second language acquisition, the researchers designed 21 questionnaire items to measure behavioral, cognitive, emotional, and social engagement, as well as face management, mutual attentiveness, and coordination aspects of rapport. After 71 Japanese learners of English completed roleplay and discussion tasks with both human tutors and a dialogue agent, the scales demonstrated strong structural validity through confirmatory factor analyses, with SRMR values below 0.08. The scales successfully differentiated between human and AI interlocutors, with human tutors scoring significantly higher across all engagement and rapport dimensions.

## Method Summary
The study employed a counterbalanced within-subjects design with 71 Japanese English learners who engaged in both roleplay (deadline extension request scenario) and discussion (SNS debate) tasks with human tutors and an AI agent powered by GPT-4. Post-dialogue questionnaires assessed engagement (behavioral, cognitive, emotional, social) and rapport (face management, mutual attentiveness, coordination) on 5-point Likert scales. Structural validity was tested through confirmatory factor analysis comparing hypothesized multi-dimensional models against baseline unidimensional models, while internal consistency was measured via Cronbach's alpha. Differentiation between human and AI partners was assessed through t-tests on average scores.

## Key Results
- Multi-dimensional engagement and rapport models showed strong structural validity with SRMR values below 0.08 threshold
- Human tutors scored significantly higher than AI agents (average 4.45 vs. 3.60 in role-play, 4.44 vs. 3.53 in discussion) across all dimensions
- Social engagement showed high internal consistency (α = 0.863-0.931), while cognitive engagement showed lower reliability (α = 0.238 for human interactions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional questionnaire structures capture engagement and rapport more accurately than uni-dimensional approaches for dialogue system evaluation.
- Mechanism: The hypothesized models (4-factor engagement, 3-factor rapport) decompose complex psychological constructs into measurable sub-dimensions, reducing conceptual haziness and enabling targeted diagnostics.
- Core assumption: Engagement and rapport are genuinely multi-dimensional constructs rather than unitary phenomena.
- Evidence anchors:
  - [section 4.2] "the correlated four-factor model generally outperformed the baseline model in both role-play and discussion tasks as evidenced in increased CFI and TLI as well as decreased RMSEA and SRMR"
  - [section 2] Draws on established multi-dimensional engagement theory from educational psychology (behavioral, cognitive, emotional, social)
  - [corpus] Limited direct corpus evidence for this measurement validation approach
- Break condition: If uni-dimensional models show equivalent fit across diverse dialogue contexts and populations.

### Mechanism 2
- Claim: Validated engagement and rapport scales are sensitive to qualitative differences between human and AI interlocutors.
- Mechanism: Scale items tap into psychological and social dimensions where current AI systems systematically underperform relative to trained humans.
- Core assumption: Observed score differences reflect actual dialogue quality gaps, not measurement artifacts or order effects.
- Evidence anchors:
  - [abstract] "human tutors scoring significantly higher (average 4.45 vs. 3.60 in role-play, 4.44 vs. 3.53 in discussion)"
  - [section 4.3] "In both tasks, human tutors commonly had significantly higher average scores across all items (p < 0.05)"
  - [corpus] Related work on adaptive chatbots (arXiv:2510.00339) confirms rapport and engagement gaps persist in human-AI interaction
- Break condition: If AI systems achieve statistical parity with humans, finer-grained scales may be needed to detect nuanced differences.

### Mechanism 3
- Claim: Construct clarity affects measurement reliability—social-interactional constructs show higher internal consistency than introspective cognitive ones.
- Mechanism: Social engagement and rapport items reference observable partner behaviors, while cognitive engagement relies on self-reported mental states that may be harder to consistently assess.
- Core assumption: Reliability differences stem from construct nature, not solely questionnaire design.
- Evidence anchors:
  - [section 4.1] "social engagement showed high internal consistency in all cases (α = 0.863 − 0.931)... cognitive engagement was significantly lower when the dialogue partner was a human (α = 0.238)"
  - [section 4.2] Reverse-coded items ("distracted," "stress/frustration") showed lower factor loadings, potentially adding noise
  - [corpus] No direct corpus evidence on dimension-specific reliability patterns
- Break condition: If revised item wording or alternative response formats eliminate reliability gaps.

## Foundational Learning

- **Confirmatory Factor Analysis (CFA)**
  - Why needed here: Primary method for testing whether questionnaire items map onto hypothesized multi-dimensional structures.
  - Quick check question: Why is CFA appropriate here instead of exploratory factor analysis?

- **Cronbach's Alpha**
  - Why needed here: Quantifies internal consistency—whether items within a construct measure the same underlying phenomenon.
  - Quick check question: What does α = 0.238 vs. α = 0.931 imply about item quality?

- **Multi-dimensional Engagement Theory**
  - Why needed here: Provides the theoretical taxonomy (behavioral, cognitive, emotional, social) that structures the questionnaire design.
  - Quick check question: How does social engagement differ from behavioral engagement in dyadic dialogue?

## Architecture Onboarding

- **Component map:**
  Dialogue phases (Introduction → Main Task → Closing) -> GPT-4 utterance generation -> Phase completion detection -> State transition -> Qualtrics questionnaire deployment

- **Critical path:**
  1. Prompt engineering for phase-appropriate LLM behavior (situational settings, task rules)
  2. Phase completion detection → state transition
  3. Post-dialogue Qualtrics questionnaire deployment
  4. CFA and reliability analysis pipeline (JASP, pingouin)

- **Design tradeoffs:**
  - LLM flexibility vs. scenario control: Traded deterministic scenario database for LLM generativity to support reciprocal interaction
  - Sample size (n=71) vs. model stability: Small sample makes SRMR more trustworthy than RMSEA for fit assessment
  - Reverse-coded items vs. respondent attention: May introduce noise if participants miss the reversal

- **Failure signatures:**
  - Cronbach's alpha < 0.7 for any dimension indicates item inconsistency
  - SRMR > 0.08 signals poor factor structure fit
  - Non-significant human-AI score differences suggest scale insensitivity

- **First 3 experiments:**
  1. Replicate CFA with larger sample (n > 150) to confirm factor structure stability
  2. Test scales on non-language-learning tasks (e.g., counseling, tutoring other subjects) to assess generalizability
  3. Remove or revise reverse-coded items and compare reliability improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the engagement and rapport scales validated in this study be effectively generalized to dialogue tasks outside of foreign language learning, such as counseling or information retrieval?
- Basis in paper: [explicit] The authors state in the Discussion that "more research is necessary to examine whether these questionnaires can be applied for dialogue tasks other than English learning" (p. 13).
- Why unresolved: The current study only validated the scales within the specific context of English speaking tasks (role-play and discussion) involving Japanese learners.
- What evidence would resolve it: Confirmatory Factor Analysis (CFA) results from experiments using these scales in diverse dialogue domains (e.g., mental health support or task-oriented dialogue) showing similar structural validity and reliability.

### Open Question 2
- Question: Is it possible to automatically estimate user engagement and rapport scores in real-time using machine learning techniques based on the validated questionnaire data?
- Basis in paper: [explicit] The Conclusion notes that "Future work will focus on developing methods for automatically estimating engagement and rapport scores" (p. 13).
- Why unresolved: The current methodology relies on post-interaction questionnaires, which are resource-intensive to collect and do not allow for real-time system adaptation.
- What evidence would resolve it: The development and validation of a predictive model (e.g., regression or classification) that uses acoustic and linguistic features to approximate the scale scores without human survey input.

### Open Question 3
- Question: Does the reliability of the cognitive engagement subscale improve if reverse-coded items are removed or rephrased?
- Basis in paper: [inferred] The results show cognitive engagement had significantly lower reliability (Cronbach's alpha = 0.238 for human interactions) and reverse-coded items exhibited poor factor loadings (p. 9-10).
- Why unresolved: The authors suggest the low reliability may stem from participants misinterpreting reverse-coded items in the online questionnaire, introducing noise into the data.
- What evidence would resolve it: A follow-up validation study demonstrating higher internal consistency (Cronbach's alpha > 0.7) and improved factor loadings for a revised cognitive engagement subscale using only positively worded items.

## Limitations
- Small sample size (n=71) limits statistical power and makes RMSEA less reliable for model fit assessment
- Exclusive focus on Japanese university students studying English raises questions about generalizability to other populations
- Between-subjects factor (task type) was not fully crossed, limiting analysis of interaction effects
- Self-reported measures capture subjective experience but may not fully reflect actual engagement behaviors

## Confidence

- **High Confidence**: The structural validity of the multi-dimensional questionnaire models (CFA results, SRMR < 0.08), and the differentiation between human and AI interlocutors on engagement and rapport dimensions
- **Medium Confidence**: The internal consistency metrics (Cronbach's alpha), particularly for social engagement and rapport items; the theoretical grounding in educational psychology and SLA research
- **Low Confidence**: The generalizability of findings beyond the specific Japanese EFL context, the stability of factor structures with larger samples, and the reliability of cognitive engagement measures

## Next Checks

1. **Cross-Cultural Validation**: Replicate the study with diverse participant populations (different nationalities, age groups, educational levels) to test the scales' generalizability across cultural and linguistic contexts.

2. **Behavioral Correlation Study**: Collect objective behavioral measures (e.g., speaking time, turn-taking patterns, lexical diversity) alongside questionnaire responses to validate whether self-reported engagement and rapport correspond to observable interaction patterns.

3. **Longitudinal Scale Stability**: Administer the scales across multiple sessions with the same participants and dialogue partners to assess whether engagement and rapport measurements remain consistent over time and repeated interactions.