---
ver: rpa2
title: 'Less LLM, More Documents: Searching for Improved RAG'
arxiv_id: '2510.02657'
source_url: https://arxiv.org/abs/2510.02657
tags:
- corpus
- scaling
- https
- retrieval
- larger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how retrieval corpus size affects Retrieval-Augmented\
  \ Generation (RAG) performance, and whether scaling the corpus can compensate for\
  \ smaller language models. Using a controlled setup with Qwen3 models (0.6B\u2013\
  14B parameters) and a web corpus (ClueWeb22-A), the authors systematically vary\
  \ corpus scale and evaluate on NQ, TriviaQA, and WebQ."
---

# Less LLM, More Documents: Searching for Improved RAG

## Quick Facts
- **arXiv ID:** 2510.02657
- **Source URL:** https://arxiv.org/abs/2510.02657
- **Reference count:** 40
- **Primary result:** Enlarging retrieval corpus can compensate for smaller language models in RAG, with mid-sized models benefiting most.

## Executive Summary
This paper investigates whether scaling the retrieval corpus can offset the need for larger language models in Retrieval-Augmented Generation (RAG). Using Qwen3 models ranging from 0.6B to 14B parameters and a web corpus (ClueWeb22-A), the authors systematically vary corpus scale and evaluate on NQ, TriviaQA, and WebQ benchmarks. The central finding is that corpus scaling consistently improves RAG performance, often enabling smaller models to match or exceed larger ones. For example, a 1.7B model with 4× larger corpus outperforms a 4B model, and a 4B model with 2× larger corpus outperforms an 8B model. The analysis reveals that performance gains come primarily from increased coverage of answer-bearing passages rather than changes in utilization efficiency.

## Method Summary
The study uses ClueWeb22-A as the retrieval corpus, partitioned into 12 disjoint shards (~22M documents each). For each experiment, a subset of shards is activated to create different corpus scales. Retrieval uses MiniCPM-Embedding-Light with DiskANN indices, retrieving top-10 documents per query. Retrieved documents are chunked with overlapping windows, reranked using MiniCPM-Embedding, and top-8 chunks are passed to Qwen3 generators. The same fixed prompt template is used across all model sizes. Performance is measured using Exact Match (EM) and F1, with secondary analysis on gold answer coverage and utilization ratio.

## Key Results
- Corpus scaling consistently improves RAG performance and can offset the need for larger models
- Mid-sized models (4–8B parameters) benefit most from corpus scaling
- Tiny models (0.6B) require much larger corpus increases; very large models (14B) see diminishing returns
- Performance gains plateau after roughly 5× corpus increase
- Improvements primarily from increased coverage rather than utilization efficiency

## Why This Works (Mechanism)
The paper demonstrates that enlarging the retrieval corpus increases the likelihood of retrieving answer-bearing passages, which directly improves the generator's ability to produce correct answers. The utilization ratio (fraction of answerable questions given available evidence) remains stable across corpus scales, indicating that improvements come from better coverage rather than more efficient use of existing evidence. This suggests that the primary bottleneck in RAG performance is the quality and comprehensiveness of the retrieval corpus rather than the capacity of the language model.

## Foundational Learning
**ClueWeb22-A corpus**: A large web corpus used for retrieval, providing diverse document coverage essential for open-domain QA tasks.
*Why needed:* Provides the knowledge base for retrieval in RAG systems.
*Quick check:* Verify corpus size (~264M documents in the subset) and document diversity.

**DiskANN indexing**: An efficient vector indexing method for approximate nearest neighbor search in high-dimensional spaces.
*Why needed:* Enables fast retrieval of relevant documents from large corpora.
*Quick check:* Confirm index build time and search latency for the corpus size used.

**Utilization ratio**: The fraction of answerable questions that can be answered given the retrieved evidence.
*Why needed:* Measures how effectively retrieved passages are used by the generator.
*Quick check:* Verify utilization remains stable across different corpus scales.

## Architecture Onboarding

**Component map:** Retrieval pipeline (DiskANN index → MiniCPM-Embedding-Light encoder → Top-10 docs) → Reranking pipeline (chunking → MiniCPM-Embedding reranker → Top-8 chunks) → Generator (Qwen3 model) → Evaluation (EM/F1)

**Critical path:** The retrieval pipeline is the critical path, as retrieval quality directly impacts generator performance. Any degradation in retrieval (coverage, relevance) propagates to the final answer quality.

**Design tradeoffs:** The study uses fixed retrieval depth (top-10) and chunk selection (top-8), which may limit the ability to capture interaction effects between corpus scale and these hyperparameters. The choice of Qwen3 family limits generalizability across model architectures.

**Failure signatures:** 
- Low gold answer coverage indicates retrieval pipeline issues
- Unstable catch-up points suggest sensitivity to random corpus partitioning
- Performance plateaus before 5× scaling may indicate diminishing returns

**First experiments:**
1. Verify retrieval coverage matches reported values (Figure 4) using the same embedding model
2. Test whether performance gains hold when using a different embedding model for retrieval
3. Validate that utilization ratio remains stable across corpus scales as claimed

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results are based on a single model family (Qwen3), limiting generalizability across architectures
- Only three English QA benchmarks are used, constraining domain applicability
- No cost-benefit analysis of storage/compute overhead versus parameter savings
- Fixed retrieval depth and chunk selection prevent analysis of hyperparameter interactions

## Confidence
- **High:** Corpus scaling consistently improves RAG performance across model sizes, with diminishing returns after ~5× scaling
- **Medium:** Mid-sized models (4–8B) benefit most from corpus scaling, while tiny models require larger increases and very large models see diminishing returns
- **Medium:** Performance gains primarily driven by increased coverage rather than utilization efficiency

## Next Checks
1. Reproduce experiments using a different retrieval pipeline (different embedding model or index structure) to test robustness of corpus scaling effect
2. Test compensation effect with other model families (Llama, Mistral) to assess generalizability beyond Qwen3
3. Conduct cost-benefit analysis comparing storage/compute overhead of larger corpora against parameter savings from smaller LLMs