---
ver: rpa2
title: 'Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models'
arxiv_id: '2512.06266'
source_url: https://arxiv.org/abs/2512.06266
tags:
- data
- reasoning
- training
- stage
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Nanbeige4-3B is a compact 3-billion-parameter language model that
  achieves state-of-the-art performance through a combination of high-quality pretraining
  on 23T tokens and advanced post-training techniques. The model employs a Fine-Grained
  Warmup-Stable-Decay scheduler during pretraining and a multi-stage post-training
  pipeline including cold-start SFT, deliberative generation refinement with CoT reconstruction,
  dual-level preference distillation, and multi-stage reinforcement learning.
---

# Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models

## Quick Facts
- **arXiv ID**: 2512.06266
- **Source URL**: https://arxiv.org/abs/2512.06266
- **Reference count**: 40
- **Primary result**: 3B parameter model achieving SOTA performance on mathematical reasoning (AIME 2024: 90.4 vs. 76.0), scientific reasoning (GPQA-Diamond: 82.2 vs. 62.0), and tool use (BFCL-V4: 53.8 vs. 42.2) while ranking among top-tier models on WritingBench

## Executive Summary
Nanbeige4-3B is a compact 3-billion-parameter language model that achieves state-of-the-art performance through a combination of high-quality pretraining on 23T tokens and advanced post-training techniques. The model employs a Fine-Grained Warmup-Stable-Decay scheduler during pretraining and a multi-stage post-training pipeline including cold-start SFT, deliberative generation refinement with CoT reconstruction, dual-level preference distillation, and multi-stage reinforcement learning. Despite its small size, Nanbeige4-3B significantly outperforms much larger models like Qwen3-8B and Qwen3-14B on mathematical reasoning (AIME 2024: 90.4 vs. 76.0), scientific reasoning (GPQA-Diamond: 82.2 vs. 62.0), tool use (BFCL-V4: 53.8 vs. 42.2), and human preference alignment (Arena-Hard V2: 60.0 vs. 26.4), while also ranking among top-tier models on the WritingBench leaderboard.

## Method Summary
Nanbeige4-3B combines extensive pretraining with sophisticated post-training optimization. The model undergoes pretraining on 23 trillion tokens using a Fine-Grained Warmup-Stable-Decay scheduler, followed by a comprehensive post-training pipeline. This pipeline includes cold-start supervised fine-tuning to establish baseline capabilities, deliberative generation refinement with Chain-of-Thought reconstruction to improve reasoning, dual-level preference distillation to align outputs with human preferences, and multi-stage reinforcement learning to further optimize performance. The approach emphasizes efficient knowledge transfer and capability enhancement despite the model's compact 3B parameter footprint.

## Key Results
- Outperforms Qwen3-8B and Qwen3-14B on AIME 2024 mathematical reasoning (90.4 vs. 76.0)
- Achieves superior scientific reasoning performance on GPQA-Diamond (82.2 vs. 62.0)
- Demonstrates enhanced tool use capabilities on BFCL-V4 (53.8 vs. 42.2) and strong human preference alignment on Arena-Hard V2 (60.0 vs. 26.4)

## Why This Works (Mechanism)
The model's exceptional performance stems from its multi-stage training approach that systematically enhances different capabilities. The extensive pretraining on 23T tokens establishes broad knowledge foundations, while the post-training pipeline progressively refines specific skills. Cold-start SFT provides stable baseline performance, CoT reconstruction improves reasoning quality through explicit step-by-step generation, preference distillation aligns outputs with human values, and reinforcement learning optimizes for task-specific objectives. This staged approach allows each component to build upon previous improvements without destabilizing learned behaviors.

## Foundational Learning
- **Chain-of-Thought reasoning**: Enables step-by-step problem solving by breaking complex tasks into manageable sub-steps; critical for mathematical and scientific reasoning tasks where intermediate logic matters.
- **Preference distillation**: Transfers human judgment patterns to the model through pairwise comparison data; essential for aligning outputs with human expectations and values.
- **Reinforcement learning from human feedback (RLHF)**: Optimizes model behavior based on reward signals derived from human preferences; crucial for achieving high scores on human preference benchmarks like Arena-Hard V2.
- **Multi-stage training stability**: Prevents catastrophic forgetting by carefully sequencing training stages; necessary when combining diverse objectives like reasoning, tool use, and preference alignment.
- **Token-efficient pretraining**: Maximizes knowledge acquisition per parameter through high-quality data and optimized training schedules; enables small models to compete with larger ones.
- **Quick check**: Verify that each post-training stage improves specific benchmark metrics without degrading performance on previously mastered tasks.

## Architecture Onboarding

**Component Map**: Pretraining (23T tokens) -> Cold-start SFT -> CoT Reconstruction -> Preference Distillation -> Multi-stage RL

**Critical Path**: The most performance-critical sequence is Pretraining -> Cold-start SFT -> Preference Distillation, as this establishes knowledge, baseline capabilities, and human alignment respectively. CoT reconstruction and additional RL stages provide incremental improvements but are less essential for core functionality.

**Design Tradeoffs**: The model prioritizes performance-per-parameter efficiency over raw capability, sacrificing some generalization breadth to achieve exceptional performance in targeted domains. The multi-stage post-training approach trades training complexity and resource requirements for superior final performance compared to single-stage fine-tuning methods.

**Failure Signatures**: Training instability may manifest as degraded performance on earlier benchmark tasks when adding new post-training stages. Overfitting to preference data can reduce diversity in outputs. Insufficient pretraining quality may limit the effectiveness of all subsequent stages, regardless of their sophistication.

**First Experiments**: (1) Validate pretraining data quality by measuring downstream performance on held-out benchmarks; (2) Test cold-start SFT with reduced dataset sizes to determine minimum effective training data; (3) Compare single-stage vs. multi-stage post-training pipelines on a subset of target tasks to quantify contribution of each stage.

## Open Questions the Paper Calls Out
Major uncertainties remain regarding the reproducibility of Nanbeige4-3B's exceptional performance gains. The model's pretraining corpus composition and quality control measures are not fully specified, making it difficult to assess whether similar results could be achieved with different datasets or training environments. Additionally, the proprietary nature of some training techniques and preference datasets used in post-training limits external validation.

## Limitations
- Pretraining corpus composition and quality control measures are not fully specified, limiting reproducibility assessment
- Proprietary training techniques and preference datasets restrict external validation and replication attempts
- Multi-stage training pipeline requires substantial computational resources and expertise, creating barriers for broader adoption

## Confidence
- **High confidence**: Achieving state-of-the-art performance among 3B models, supported by quantitative benchmarks across multiple domains
- **Medium confidence**: Claims that performance gains stem primarily from post-training pipeline rather than pretraining quality, due to limited ablation studies
- **Medium confidence**: Absolute performance rankings compared to larger models, as comparisons may not account for evaluation protocol differences

## Next Checks
1. Independent replication of the full training pipeline using publicly available datasets to verify performance claims
2. Comprehensive ablation studies isolating the contribution of each post-training component (cold-start SFT, CoT reconstruction, preference distillation, RL stages)
3. Extended evaluation on additional benchmarks including multilingual tasks and domain-specific applications to assess generalization beyond reported domains