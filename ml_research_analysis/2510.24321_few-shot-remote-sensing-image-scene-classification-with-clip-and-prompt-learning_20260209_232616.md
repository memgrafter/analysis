---
ver: rpa2
title: Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning
arxiv_id: '2510.24321'
source_url: https://arxiv.org/abs/2510.24321
tags:
- clip
- sensing
- remote
- learning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates prompt learning as a lightweight strategy\
  \ for adapting vision-language foundation models, specifically CLIP, to remote sensing\
  \ image scene classification. We evaluate four representative prompt-learning paradigms\u2014\
  Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), Multi-modal\
  \ Prompt Learning (MaPLe), and Prompting with Self-Regulating Constraints (PromptSRC)\u2014\
  on nine benchmark remote sensing datasets under few-shot and cross-dataset transfer\
  \ settings."
---

# Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning

## Quick Facts
- **arXiv ID**: 2510.24321
- **Source URL**: https://arxiv.org/abs/2510.24321
- **Reference count**: 40
- **Primary result**: Prompt learning consistently outperforms zero-shot CLIP and linear probe baselines across nine benchmark remote sensing datasets under few-shot and cross-dataset transfer settings.

## Executive Summary
This study investigates prompt learning as a lightweight strategy for adapting vision-language foundation models, specifically CLIP, to remote sensing image scene classification. We evaluate four representative prompt-learning paradigms—Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), Multi-modal Prompt Learning (MaPLe), and Prompting with Self-Regulating Constraints (PromptSRC)—on nine benchmark remote sensing datasets under few-shot and cross-dataset transfer settings. Results show that all prompt-learning approaches consistently outperform strong baselines including zero-shot CLIP with handcrafted prompts and a linear probe trained on frozen CLIP features. Notably, PromptSRC achieves the most robust cross-domain performance, while MaPLe excels in cross-modal alignment and generalization. These findings demonstrate that prompt learning offers a scalable, data-efficient, and architecture-agnostic solution for bridging the domain gap in Earth observation tasks.

## Method Summary
The study adapts CLIP (ViT-B/16 via OpenCLIP) to remote sensing scene classification using four prompt-learning methods: CoOp (learnable context vectors), CoCoOp (conditional context), MaPLe (multi-modal deep prompting), and PromptSRC (self-regulated prompting). Each method inserts learnable tokens into CLIP's text or vision encoder layers, with PromptSRC adding regularization to prevent forgetting. Models are trained on 1-16 shot settings using 3 seeds averaged, with zero-shot CLIP and linear probe as baselines. Datasets follow AITLAS Arena splits with standardized preprocessing.

## Key Results
- All four prompt-learning methods consistently outperform zero-shot CLIP and linear probe baselines across nine benchmark datasets
- PromptSRC achieves the most robust cross-domain performance, while MaPLe excels in cross-modal alignment and generalization
- Prompt learning demonstrates 5-15% accuracy improvements over zero-shot CLIP in 1-4 shot settings

## Why This Works (Mechanism)

### Mechanism 1: Semantic Manifold Alignment via Learnable Context
If visual features of remote sensing imagery are distinct but poorly aligned with natural language descriptors, learning static context vectors can bridge this semantic gap more efficiently than fine-tuning the entire backbone. CoOp replaces hand-crafted prompts with continuous, learnable vectors appended to class labels, adjusting them to maximize cosine similarity between RS image and text embeddings within the frozen CLIP space. Core assumption: the frozen visual encoder has sufficient representational capacity, requiring only linguistic adjustment for correct classification.

### Mechanism 2: Deep Cross-Modal Coupling (MaPLe)
If the domain gap is too large for text-only adaptation, jointly prompting both vision and language branches improves alignment by allowing visual features to adapt slightly to RS textures and scales. MaPLe inserts learnable tokens into multiple layers of both encoders with a coupling function projecting text prompts into visual-prompt space, ensuring modifications to visual processing are mirrored in textual processing. Core assumption: intermediate layers of frozen encoders contain malleable features that can be repurposed through small, localized perturbations without destabilizing the model.

### Mechanism 3: Self-Regulated Knowledge Preservation (PromptSRC)
If few-shot adaptation risks catastrophic forgetting of general knowledge embedded in CLIP, constraining prompts to remain close to original model outputs ensures robustness and cross-domain generalization. PromptSRC introduces regularization loss maximizing agreement between prompted and frozen model outputs, anchoring adaptation and preventing overfitting to limited few-shot samples. Core assumption: pre-trained model's behavior on given sample serves as useful prior that should not be drastically deviated from.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here:** The paper builds entirely on CLIP's dual-encoder architecture. You must understand that CLIP maps images and text to shared latent space where "similarity" drives classification, rather than using explicit classification heads.
  - **Quick check question:** Given an image of a 'forest', how does CLIP determine it is not a 'building' without a softmax layer at the end of the CNN/ViT?

- **Concept: Few-Shot Learning Paradigms**
  - **Why needed here:** The paper benchmarks prompt learning against standard few-shot baselines (Linear Probe). Understanding difference between "tuning classifier on top" vs "tuning input prompts" is central to paper's contribution.
  - **Quick check question:** Why might a Linear Probe fail in a 1-shot setting compared to prompt learning?

- **Concept: Domain Shift in Remote Sensing**
  - **Why needed here:** The motivation for the paper. You need to grasp that overhead imagery (top-down, multi-scale, abstract textures) differs fundamentally from natural imagery (eye-level, centric objects) used to train foundation models.
  - **Quick check question:** Why does a "zero-shot" CLIP model trained on internet photos struggle to classify a satellite image of a "roundabout"?

## Architecture Onboarding

- **Component map:** Frozen CLIP ViT-B/16 (Image Encoder + Text Encoder) -> Prompt Modules (CoOp, CoCoOp, MaPLe, PromptSRC) -> L2 Normalized Features -> Cosine Similarity -> Cross-entropy Loss (PromptSRC adds Self-Regulating Loss)

- **Critical path:**
  1. Pre-processing: Normalizing class names (e.g., removing underscores) is critical for meaningful text embeddings
  2. Initialization: Prompts typically initialized using embeddings of "a photo of a" rather than random noise to stabilize early training
  3. Forward Pass: Concatenate prompt tokens with class tokens -> Frozen Transformer -> L2 Normalized Features -> Cosine Similarity

- **Design tradeoffs:**
  - CoOp vs. PromptSRC: CoOp is lightweight (fewer parameters, faster) but overfits to training distribution. PromptSRC adds computational overhead via regularization objective but offers significantly better cross-dataset generalization
  - Linear Probe vs. Prompting: Linear probes require training new classifier layer (fast convergence but high data hunger). Prompting tunes very small number of parameters (slower convergence per epoch, but better data efficiency)

- **Failure signatures:**
  - Overfitting: High training accuracy but low test accuracy in 1-2 shot settings (common with CoOp)
  - Mode Collapse: Model predicting same class for all inputs if learning rates are too high or prompts are initialized poorly
  - Semantic Confusion: Systematic confusion between similar classes (e.g., 'Dense Residential' vs 'Medium Residential') indicates prompt capturing texture but not semantic density

- **First 3 experiments:**
  1. Zero-Shot vs. Linear Probe Baseline: Reproduce Table 2 for single dataset (e.g., EuroSAT) to verify data pipeline and confirm "performance gap" paper claims to fill
  2. CoOp Integration: Implement basic learnable context (CoOp) to ensure backpropagation through frozen text encoder works correctly. Check for 1-shot improvement over Linear Probe
  3. Cross-Domain Validation (PromptSRC): Train on one dataset (e.g., UC Merced) and test on another (e.g., RESISC45) using PromptSRC to validate "self-regulating" claim of preventing catastrophic forgetting

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains depend critically on frozen CLIP backbone's ability to extract meaningful features from remote sensing imagery without ablations on different architectures or fine-tuning
- Cross-dataset transfer results may be influenced by specific selection of source-target pairs without exploration across all possible dataset combinations
- Computational overhead of methods like MaPLe and PromptSRC is not thoroughly quantified beyond accuracy metrics

## Confidence
- **High**: Core finding that prompt learning consistently outperforms zero-shot CLIP and linear probe baselines across multiple datasets and shot settings
- **Medium**: Relative ranking of four prompt-learning methods, as differences are smaller than gap between prompt learning and traditional approaches
- **Low**: Generalizability of cross-dataset transfer results without additional validation across different dataset combinations and potential domain shifts

## Next Checks
1. **Ablation Study on Backbone Flexibility**: Test whether allowing limited fine-tuning of CLIP visual encoder combined with prompting yields better results than either approach alone, particularly for datasets with extreme domain shifts

2. **Cross-Dataset Transfer Robustness**: Systematically evaluate PromptSRC's cross-dataset performance across all possible source-target pairs among nine datasets, measuring not just accuracy but also calibration scores to understand when model is overconfident versus uncertain

3. **Computational Overhead Analysis**: Measure and compare actual training time, memory usage, and parameter count for each prompt-learning method relative to baselines, then calculate accuracy gain per unit of additional computational cost to identify most efficient approach