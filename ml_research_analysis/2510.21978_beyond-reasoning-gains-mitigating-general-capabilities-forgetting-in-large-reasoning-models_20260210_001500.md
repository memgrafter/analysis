---
ver: rpa2
title: 'Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large
  Reasoning Models'
arxiv_id: '2510.21978'
source_url: https://arxiv.org/abs/2510.21978
tags:
- arxiv
- reasoning
- answer
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reasoning-focused reinforcement learning with verifiable rewards
  improves performance on targeted tasks but degrades general capabilities such as
  perception and robustness. We show that open-source reasoning models consistently
  underperform their base models on non-reasoning benchmarks after RLVR finetuning.
---

# Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models

## Quick Facts
- arXiv ID: 2510.21978
- Source URL: https://arxiv.org/abs/2510.21978
- Reference count: 40
- Primary result: RECAP preserves general capabilities during reasoning RLVR finetuning, achieving up to 6% reasoning accuracy gains and over 2% segmentation accuracy recovery

## Executive Summary
Reasoning-focused reinforcement learning with verifiable rewards improves performance on targeted tasks but degrades general capabilities such as perception and robustness. Open-source reasoning models consistently underperform their base models on non-reasoning benchmarks after RLVR finetuning. RECAP addresses this by integrating general-capability data into RLVR with dynamic objective reweighting that adapts based on convergence rates and instability signals. Experiments show RECAP preserves or improves general capabilities while matching or exceeding reasoning performance compared to reasoning-only or uniform baselines.

## Method Summary
RECAP mitigates catastrophic forgetting during RLVR by replaying general-capability data alongside reasoning data and dynamically reweighting objectives. The method tracks per-objective losses using a sliding window, computes convergence rate (c_k = μ_k/μ̃_k) and instability (i_k = σ_k/(μ_k + μ̃_k)), then combines them as priority s_k = c_k + i_k. Weights λ_k are derived via softmax normalization. The approach is implemented within GRPO-style RLVR pipelines, mixing reasoning data (ThinkLite-VL-70k) with replay data (RefCOCO, LLaVA-OneVision-OCR) and applying weighted losses that preserve both reasoning gains and general capabilities.

## Key Results
- RECAP recovers over 2% segmentation accuracy on LISA compared to reasoning-only finetuning
- Achieves up to 6% accuracy gains on reasoning benchmarks while preserving perception
- Produces more concise rationales without sacrificing correctness, improving inference efficiency

## Why This Works (Mechanism)

### Mechanism 1: General-Capability Data Replay Prevents Gradient Drift
Mixing replay data from perception/OCR domains alongside reasoning data preserves non-target capabilities by maintaining gradient signals from original skill domains. The training objective combines reasoning rewards (accuracy, format) with general-domain losses (IoU for detection, next-token prediction for OCR), ensuring parameter updates do not exclusively optimize for reasoning at the expense of other skills.

### Mechanism 2: Convergence-Rate-Based Down-Weighting of Saturated Objectives
Objectives that converge quickly (e.g., format compliance) should receive lower weight over time to free capacity for harder objectives. The scheduler computes c_k = μ_k/μ̃_k and when c_k ≈ 1, the objective has plateaued and its priority decreases, reducing λ_k via softmax normalization.

### Mechanism 3: Instability-Aware Up-Weighting of Volatile Objectives
High-variance objectives (e.g., reasoning accuracy) should receive higher weight because they indicate ongoing learning difficulty. The scheduler computes i_k = σ_k/(μ_k + μ̃_k) and higher i_k increases priority, shifting optimization focus toward objectives that have not stabilized.

## Foundational Learning

- **Catastrophic Forgetting**: Why needed: The core problem is that sequential RL optimization overwrites representations learned during pretraining; understanding this motivates replay as a mitigation. Quick check: Can you explain why gradient updates on reasoning data alone would degrade perception performance?

- **GRPO (Group Relative Policy Optimization)**: Why needed: RECAP plugs into GRPO-style RLVR pipelines; understanding group-normalized advantages clarifies where the reweighting is injected. Quick check: How does GRPO estimate advantages without a learned critic, and where does RECAP modify the loss aggregation?

- **Multi-Objective Optimization in RL**: Why needed: RECAP manages heterogeneous objectives (verifiable rewards + supervised losses) with different scales and convergence behaviors. Quick check: Why would uniform weighting be suboptimal when objectives converge at different rates?

## Architecture Onboarding

- **Component map**: Data loader -> Per-objective loss compute -> Sliding-window tracker -> Reweighting scheduler -> Aggregated loss -> Optimizer step

- **Critical path**: Collect per-objective losses → update sliding-window statistics → compute convergence/instability → softmax to λ_k → apply weighted loss → optimizer step

- **Design tradeoffs**: Window size W (larger smooths noise but delays reaction); Temperature T (T=5 default; lower sharpens priority differences); Replay ratio (too much dilutes reasoning gains)

- **Failure signatures**: Reasoning gains vanish (replay ratio too high); Perception still degrades (replay data insufficient); Training instability (temperature too low or window too small)

- **First 3 experiments**: 1) Reproduce reasoning-only baseline on Qwen2.5-VL-7B → confirm forgetting on LISA; 2) Add uniform replay (no reweighting) → verify partial preservation; 3) Ablate convergence vs. instability: run with s_k = c_k only and s_k = i_k only

## Open Questions the Paper Calls Out

### Open Question 1
How does RECAP perform when applied to preference-based objectives (DPO, KTO) and process reward models, compared to its demonstrated effectiveness on RLVR and SFT settings? The framework is designed as generic but has only been validated on RL reward surrogates and supervised losses.

### Open Question 2
Would a learned or weighted combination of convergence rate and instability signals outperform the simple sum s = c + i used in RECAP? The authors acknowledge that balancing c and i could yield better performance but defer this exploration.

### Open Question 3
Does RECAP generalize to other model architectures beyond Qwen2.5-VL, particularly text-only LLMs or different VLM families? All experiments use only Qwen2.5-VL-3B and Qwen2.5-VL-7B; no other architectures are evaluated.

### Open Question 4
How sensitive is RECAP to hyperparameter choices such as sliding window length W and temperature T? The paper uses fixed T=5 and does not tune or ablate the sliding window length, yet these may affect convergence signal reliability.

## Limitations
- Replay data sufficiency: Current replay domains may not comprehensively cover the general-capability space
- Hyperparameter opacity: Critical sliding window size W is not specified, making exact reproduction difficult
- Single-task focus: Only examines single-task RLVR, leaving multi-task scenarios unexplored

## Confidence
- **High confidence**: Core observation that RLVR causes measurable forgetting on perception benchmarks is well-supported
- **Medium confidence**: Convergence-rate-based down-weighting mechanism is plausible but exact contribution not fully isolated
- **Low confidence**: Claim about more concise rationales lacks systematic evaluation of reasoning quality

## Next Checks
1. **Window size ablation study**: Systematically vary W and measure stability of λ_k weights and final performance
2. **Domain coverage stress test**: Replace RefCOCO with different perception dataset and retrain to verify preservation
3. **Multi-task RLVR extension**: Apply RECAP to joint reasoning and perception training to check for interference patterns