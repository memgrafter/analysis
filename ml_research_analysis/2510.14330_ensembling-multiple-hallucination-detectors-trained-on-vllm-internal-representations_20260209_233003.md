---
ver: rpa2
title: Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations
arxiv_id: '2510.14330'
source_url: https://arxiv.org/abs/2510.14330
tags:
- hallucination
- task
- accuracy
- answer
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a solution to the Meta CRAG-MM Challenge at
  KDD Cup 2025, which focuses on reducing hallucinations in vision-language models
  (VLMs) for visual question answering. Their approach centers on training logistic
  regression models to detect hallucinations using internal representations from VLMs,
  specifically hidden states and attention head outputs.
---

# Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations

## Quick Facts
- arXiv ID: 2510.14330
- Source URL: https://arxiv.org/abs/2510.14330
- Authors: Yuto Nakamizo; Ryuhei Miyazato; Hikaru Tanabe; Ryuta Yamakura; Kiori Hatanaka
- Reference count: 40
- Top-5 placement in KDD Cup 2025 Meta CRAG-MM Challenge

## Executive Summary
This paper presents a solution to the Meta CRAG-MM Challenge at KDD Cup 2025, focusing on hallucination detection in vision-language models for visual question answering. The approach uses logistic regression models trained on internal representations (hidden states and attention head outputs) from VLMs to detect hallucinations. An ensemble of 65 detectors is used to classify answers, with hallucinated outputs replaced by "I don't know." This strategy significantly reduces hallucination rates while maintaining reasonable accuracy, achieving a top-5 placement in the competition.

## Method Summary
The method extracts hidden states and attention head outputs from the Llama-3.2-11B-Vision-Instruct model during answer generation on a validation set. These internal representations are averaged across generated tokens and used to train 65 logistic regression detectors (7 on hidden states from layers 13-20, 58 on specific attention heads). PCA dimensionality reduction (95% cumulative variance) is applied to hidden states. Detectors with F1 > 0.5 on public_test are selected for the ensemble. During inference, predictions are averaged and thresholded at 0.65 - if the ensemble score exceeds this threshold, the answer is accepted; otherwise, "I don't know" is output.

## Key Results
- Hallucination rate reduced from 0.735 to 0.045 (88% reduction)
- Accuracy decreased from 0.207 to 0.082 (61% reduction)
- Trustfulness score improved through asymmetric penalty handling
- Top-5 placement in KDD Cup 2025 Meta CRAG-MM Challenge

## Why This Works (Mechanism)

### Mechanism 1: Internal Representation-Based Hallucination Detection
VLLM hidden states and attention head outputs encode separable signals for truthful vs. hallucinated outputs. Logistic regression probes trained on averaged token-level activations classify answers as correct (y=1) or hallucinated (y=0). Hidden states undergo PCA dimensionality reduction (95% cumulative variance retained) before probing. Core assumption: The geometric structure of internal activations during generation correlates with factual correctness in ways linear probes can capture.

### Mechanism 2: Selective Probe Ensembling
Averaging predictions from multiple probes trained on diverse internal representations improves detection robustness over single probes. Train 65 independent logistic regression models (7 on hidden states from layers 13-20, 58 on specific attention heads). For inference, average all predictions: ŷ = (1/N) Σŷ_j. Accept answer only if ŷ > 0.65. Core assumption: Different attention heads and layers capture orthogonal hallucination signals; averaging reduces individual probe noise.

### Mechanism 3: Asymmetric Penalty-Aware Filtering
Under asymmetric scoring (+1 correct, -1 hallucination, 0 refusal), aggressive hallucination rejection improves trustfulness despite accuracy loss. When ensemble prediction ŷ ≤ 0.65, replace VLLM output with "I don't know." This trades 52-61% accuracy reduction for 88% hallucination reduction, yielding positive trustfulness scores. Core assumption: The competition scoring function makes false negatives (rejecting correct answers) less costly than false positives (accepting hallucinations).

## Foundational Learning

- **Transformer Hidden States and Attention Heads**: Must understand how to extract intermediate activations from specific layers and heads during inference. Quick check: Can you identify where in a forward pass to hook layer 17's hidden state and head 9's attention output?

- **Linear Probing**: The detection model is simply logistic regression on activation vectors - understanding probe training protocol is essential. Quick check: Given activations X and binary labels y, what preprocessing (normalization, PCA) might improve probe performance?

- **Calibration and Threshold Selection**: The 0.65 threshold was empirically chosen on public_test; understanding how to tune this for different cost matrices is critical. Quick check: If hallucinations cost -5 instead of -1, would you raise or lower the acceptance threshold?

## Architecture Onboarding

- Component map: Image + Question → VLLM (Llama-3.2-11B-Vision-Instruct) → Hook activations: hidden_states (layers 13-20), attention_heads (58 selected) → Average across generated tokens → feature vectors → PCA (hidden states only) → reduced dimensions → 65 Logistic Regression probes → 65 predictions → Ensemble average → ŷ → Threshold (0.65) → if ŷ > 0.65: output answer; else: "I don't know"

- Critical path: Hook registration → activation extraction → probe inference → ensemble aggregation → threshold comparison

- Design tradeoffs:
  - Higher F1 threshold for probe selection → fewer probes, higher ensemble precision, potentially lower recall
  - Higher acceptance threshold → lower hallucination rate, higher false rejection rate
  - PCA variance threshold → dimensionality vs. information preservation

- Failure signatures:
  - Probe predictions all near 0.5: likely feature extraction bug or normalization issue
  - Trustfulness score negative on validation: threshold too permissive or probes underfit
  - Accuracy near 0%: threshold too aggressive; try lowering to 0.5

- First 3 experiments:
  1. Reproduce probe training on validation split: extract activations, train logistic regression on layer 17 hidden state, verify F1 > 0.5 on held-out validation
  2. Threshold sweep: evaluate trustfulness at thresholds [0.4, 0.5, 0.6, 0.7, 0.8] on public_test; confirm 0.65 is optimal for this dataset
  3. Ablation by signal type: compare hidden-state-only vs. attention-head-only ensembles to understand which contributes more to hallucination detection

## Open Questions the Paper Calls Out
None

## Limitations
- The approach assumes hallucination signals are consistently encoded in the same internal representations across different input domains, with no validation on out-of-distribution data or different VLLM architectures
- LLM-as-a-Judge labeling introduces potential subjectivity and variance, with different judge models or prompts potentially yielding different ground truth labels
- Fixed 0.65 threshold and probe selection criteria (F1 > 0.5) were tuned on public_test data, creating risk of overfitting to this specific dataset distribution

## Confidence
- **High Confidence**: Overall competition result and ranking are verifiable from KDD Cup leaderboard; basic methodology of using internal representations for hallucination detection is well-supported by related work
- **Medium Confidence**: Specific ensemble architecture (65 probes from selected layers/heads) is detailed but not fully validated for optimality; 0.65 threshold reported as optimal but tuned on public_test making generalization uncertain
- **Low Confidence**: Paper lacks ablation studies on probe selection criteria or threshold sensitivity; no analysis of probe failure modes or error analysis for false positives/negatives

## Next Checks
1. **Cross-Architecture Generalization Test**: Apply the trained probe ensemble to a different VLLM (e.g., LLaVA or Qwen-VL) on the same CRAG-MM dataset to assess probe transferability and architecture dependence

2. **Label Stability Analysis**: Repeat the probe training process using 3-5 different LLM-as-a-Judge configurations (different models, prompts) to measure variance in detection performance and assess labeling robustness

3. **Threshold Calibration Validation**: Perform k-fold cross-validation on the validation set to empirically determine optimal thresholds and assess whether the 0.65 threshold is overfit to public_test or generalizes across data splits