---
ver: rpa2
title: Explainable time-series forecasting with sampling-free SHAP for Transformers
arxiv_id: '2512.20514'
source_url: https://arxiv.org/abs/2512.20514
tags:
- shap
- feature
- load
- shapformer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAPformer is a Transformer-based time-series forecasting model
  that enables fast, exact SHAP explanations by using attention manipulation to evaluate
  predictions on feature subsets without sampling. On synthetic data with ground truth
  explanations, SHAPformer closely matches the ground truth in feature importance
  and dependence patterns, outperforming both the SHAP Permutation Explainer and Custom
  Masker.
---

# Explainable time-series forecasting with sampling-free SHAP for Transformers

## Quick Facts
- arXiv ID: 2512.20514
- Source URL: https://arxiv.org/abs/2512.20514
- Reference count: 40
- SHAPformer achieves over 50× faster explanation generation than sampling-based SHAP methods while maintaining competitive forecasting accuracy

## Executive Summary
SHAPformer is a Transformer-based time-series forecasting model that enables fast, exact SHAP explanations by using attention manipulation to evaluate predictions on feature subsets without sampling. The model achieves competitive forecasting accuracy while providing interpretable explanations at speeds over 50× faster than traditional sampling-based SHAP methods. On synthetic data with ground truth explanations, SHAPformer closely matches the ground truth in feature importance and dependence patterns, outperforming both the SHAP Permutation Explainer and Custom Masker.

## Method Summary
SHAPformer integrates SHAP (SHapley Additive exPlanations) with Transformers to provide both accurate forecasts and interpretable explanations. The key innovation is manipulating attention mechanisms to evaluate predictions on feature subsets without sampling, enabling exact SHAP values computation. The model was tested on synthetic data with known ground truth explanations and real-world electrical load data, demonstrating both competitive forecasting performance (RMSE ≈ 265.9 MW) and meaningful interpretability insights, including identification of past load as the dominant predictor.

## Key Results
- On synthetic data, SHAPformer closely matches ground truth explanations in feature importance and dependence patterns, outperforming SHAP Permutation Explainer and Custom Masker
- For real-world electrical load data, achieves competitive forecasting accuracy (RMSE ≈ 265.9 MW) with explanation inference time under one second
- Provides meaningful global insights (past load as dominant predictor) and local insights (distinct model behavior during Christmas period)

## Why This Works (Mechanism)
SHAPformer works by manipulating attention mechanisms within the Transformer architecture to evaluate predictions on feature subsets without requiring sampling. This attention-based approach allows for exact computation of SHAP values by systematically modifying how the model attends to different input features. The method leverages the inherent attention mechanism of Transformers to efficiently compute marginal contributions of each feature to the final prediction, eliminating the need for computationally expensive sampling procedures required by traditional SHAP implementations.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations)**: A game-theoretic approach for explaining model predictions by computing feature contributions; needed to provide interpretable feature importance scores that sum to the model output
- **Transformer attention mechanisms**: The core component enabling efficient feature subset evaluation; quick check: verify attention weights can be systematically modified to isolate feature contributions
- **Exact vs. sampling-based SHAP computation**: Traditional SHAP requires exponential combinations of feature subsets; quick check: confirm computational complexity reduction from sampling to exact computation
- **Time-series forecasting with Transformers**: Leveraging sequence modeling capabilities for temporal data prediction; quick check: validate sequence length and context window are appropriate for the forecasting task

## Architecture Onboarding

**Component Map**
Input -> Feature Encoder -> Attention Manipulation Layer -> SHAP Computation Layer -> Output

**Critical Path**
1. Input features are encoded and passed through the Transformer
2. Attention manipulation layer systematically modifies attention patterns to evaluate feature subsets
3. SHAP computation layer aggregates contributions to generate exact SHAP values
4. Output layer produces both forecast and explanation

**Design Tradeoffs**
- Exact SHAP computation provides accuracy but requires careful attention manipulation design
- Sampling-free approach trades implementation complexity for computational efficiency
- Real-time explanation generation enabled at cost of specialized architecture modifications

**Failure Signatures**
- Poor attention manipulation leading to incorrect feature subset evaluation
- Computational bottlenecks when scaling to high-dimensional feature spaces
- Explanation quality degradation on complex, non-linear feature interactions

**First Experiments to Run**
1. Verify exact SHAP computation matches sampling-based estimates on simple synthetic dataset
2. Test attention manipulation effectiveness by isolating known feature contributions
3. Benchmark explanation generation speed against traditional SHAP methods on real-world data

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation on synthetic datasets with pre-defined ground truth may not generalize to real-world complexities
- Real-world performance assessment is limited by lack of ground truth comparisons
- Study focuses on a single real-world dataset (electrical load data), limiting generalizability to other time-series forecasting tasks

## Confidence
- High: Synthetic data results showing superior performance in feature importance and dependence patterns
- Medium: Model's ability to provide fast, exact SHAP explanations and competitive forecasting accuracy on real-world data
- Low: Real-world data results due to lack of ground truth comparisons and limited dataset scope

## Next Checks
1. Evaluate SHAPformer on multiple real-world datasets with varying characteristics to assess generalizability across different time-series forecasting tasks
2. Conduct a user study to assess interpretability and usefulness of SHAPformer's explanations in practical applications with domain experts
3. Investigate scalability to larger datasets and more complex models with higher feature counts or longer time horizons