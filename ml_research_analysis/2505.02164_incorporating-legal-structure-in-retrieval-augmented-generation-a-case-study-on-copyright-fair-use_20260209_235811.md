---
ver: rpa2
title: 'Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study
  on Copyright Fair Use'
arxiv_id: '2505.02164'
source_url: https://arxiv.org/abs/2505.02164
tags:
- legal
- retrieval
- fair
- court
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a structured Retrieval-Augmented Generation
  (RAG) system for copyright fair use analysis that incorporates legal knowledge graphs
  and citation networks to improve retrieval quality and reasoning reliability. By
  modeling legal precedents at the statutory factor level and applying PageRank-based
  citation weighting, the system prioritizes doctrinally authoritative sources rather
  than relying solely on semantic similarity.
---

# Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use

## Quick Facts
- arXiv ID: 2505.02164
- Source URL: https://arxiv.org/abs/2505.02164
- Authors: Justin Ho; Alexandra Colby; William Fisher
- Reference count: 40
- Primary result: Structured RAG with legal knowledge graphs and citation weighting improves retrieval of doctrinally authoritative copyright cases over standard RAG

## Executive Summary
This paper introduces a structured Retrieval-Augmented Generation (RAG) system specifically designed for copyright fair use analysis that incorporates legal knowledge graphs and citation networks to improve retrieval quality and reasoning reliability. The system models legal precedents at the statutory factor level and applies PageRank-based citation weighting to prioritize doctrinally authoritative sources rather than relying solely on semantic similarity. By using Chain-of-Thought reasoning and interleaved retrieval steps, the approach better emulates legal reasoning processes. Preliminary testing shows the structured approach achieves significantly higher PageRank scores (mean 0.213 vs 0.026) compared to standard RAG, indicating improved retrieval of doctrinally relevant cases, though with slightly lower textual similarity (mean 0.521 vs 0.753).

## Method Summary
The method involves building a knowledge graph from legal opinions where each case is decomposed into factor-specific verbatim paragraphs (Purpose, Nature, Amount, Market, Facts, Conclusion) using an LLM. These passages are embedded using Google Gecko and stored in a Neo4j graph with relationships capturing citation networks and court hierarchy. PageRank is computed on both the inter-opinion citation network and the court hierarchy to quantify doctrinal authority. Retrieval combines text similarity with citation and court authority scores using configurable weights in a convex combination. The system employs Chain-of-Thought reasoning to analyze how a complaint relates to the four Fair Use factors before retrieval, reducing sycophancy and improving relevance.

## Key Results
- Structured RAG achieved PageRank scores of 0.213 vs 0.026 for standard RAG, indicating better retrieval of doctrinally authoritative cases
- Text similarity was lower for structured RAG (0.521) compared to standard RAG (0.753), reflecting the tradeoff between doctrinal authority and semantic relevance
- The system successfully retrieved more authoritative cases while maintaining reasonable semantic relevance for copyright fair use analysis
- Factor-level granularity enabled context-sensitive retrieval that better captured the doctrinal dimensions of legal disputes

## Why This Works (Mechanism)

### Mechanism 1: Citation-Weighted Retrieval Prioritizes Doctrinal Authority
Incorporating citation networks via PageRank improves retrieval of legally authoritative sources over semantically similar but doctrinally weak cases. The system applies PageRank to inter-opinion citation networks and court hierarchy, combining these with text similarity via $s_i = w_{text} \cdot TextSim_i + w_{cit} \cdot Citation_i + w_{court} \cdot Court_i$. Cases cited by authoritative sources carry more doctrinal weight, and higher courts issue more binding precedents. However, recent landmark cases may have low PageRank due to insufficient citation history.

### Mechanism 2: Factor-Level Granularity Aligns Retrieval with Legal Reasoning Structure
Structuring retrieval at the statutory factor level improves relevance for multi-factor legal tests. An LLM extracts verbatim paragraphs for each Fair Use factor, enabling retrieval that matches doctrinal dimensions. Legal reasoning depends on specific doctrinal elements, and relevant precedent often resides in portions of opinions. If LLM extraction misclassifies factor-specific passages, retrieval will surface irrelevant segments.

### Mechanism 3: Interleaved Retrieval with Chain-of-Thought Anchors Reasoning in Doctrine
CoT analysis of how complaints relate to Fair Use factors before retrieval reduces sycophancy and improves relevance. The system prompts LLM to decompose input disputes according to four statutory factors, guiding targeted retrieval. Structuring reasoning before retrieval reduces model tendency to agree with user-provided framing. If initial factor analysis is incorrect, subsequent retrieval will be systematically misdirected.

## Foundational Learning

- **Knowledge Graphs (Neo4j):** Legal structure represented as graph with nodes (Case, Court, Opinion, Factor) and relationships (CITED, DECIDED_IN, APPEALS_TO). Understanding graph traversal and Cypher queries is essential for debugging retrieval.
  - Quick check: Can you explain how PageRank would be computed on a directed citation graph where edges represent "cited by" relationships?

- **PageRank Algorithm:** Used to quantify citation authority and court hierarchy. Understanding how PageRank handles disconnected components and recency bias is critical for interpreting retrieval rankings.
  - Quick check: Why might a recent Supreme Court decision have a lower PageRank than an older District Court opinion, and how could this affect retrieval quality?

- **RAG Architecture (Retrieval-Augmented Generation):** Domain-specific RAG implementation. Understanding standard RAG pipeline (embedding, retrieval, generation) is prerequisite to understanding how legal structure modifications improve upon naive approaches.
  - Quick check: In standard RAG, what failure modes occur when retrieval relies solely on cosine similarity over document chunks?

## Architecture Onboarding

- **Component map:** Legal opinions → LLM factor extraction → Gecko embeddings → Neo4j knowledge graph → PageRank computation → Weighted retrieval scoring → Top-k + n cited cases → CoT reasoning → Structured Fair Use analysis

- **Critical path:**
  1. Source legal opinions → Extract factor-specific verbatim paragraphs using LLM
  2. Embed passages with Gecko → Store in Neo4j with graph relationships
  3. Compute PageRank on citation network and court hierarchy
  4. User submits dispute → System retrieves top-k cases using weighted scoring
  5. Optionally retrieve n cited cases from retrieved results
  6. Apply CoT reasoning to analyze retrieved cases against four factors
  7. Generate structured Fair Use evaluation

- **Design tradeoffs:**
  - Text similarity vs. doctrinal authority: Increasing citation/court weights reduces text similarity; optimal balance unknown without ablation studies
  - Recency vs. established authority: PageRank biases against recent cases; time-adjusted measures noted as future work
  - Configurable vs. automated weights: Current prototype requires manual weight specification; legal expert input assumed

- **Failure signatures:**
  - Hallucination: Model may generate speculative conclusions when input is vague
  - Sycophancy: Model may over-align with user-provided framing despite CoT
  - Naive retrieval: Standard RAG retrieves semantically similar but doctrinally irrelevant cases
  - Recency gap: Important recent precedents may be underweighted

- **First 3 experiments:**
  1. Ablation study on retrieval weights: Systematically vary $w_{text}$, $w_{cit}$, $w_{court}$ and measure impact on retrieval relevance (PageRank of retrieved cases, expert relevance ratings)
  2. Factor extraction validation: Manually verify LLM-extracted factor passages against ground truth for a sample of opinions to quantify extraction accuracy
  3. Recency adjustment: Implement time-decayed PageRank or recency boosting; compare retrieval of recent landmark cases against unadjusted baseline

## Open Questions the Paper Calls Out

- What are the optimal weight configurations for textual similarity, citation authority, and court hierarchy in the retrieval scoring function, and how do individual components contribute to downstream legal analysis quality?
- Can time-adjusted citation metrics better capture the doctrinal influence of recent landmark cases compared to standard PageRank?
- How should the system classify whether Fair Use is the appropriate legal framework for a given dispute, and can a routing mechanism effectively direct cases to specialized doctrinal modules?
- Does incorporating legal structure improve downstream legal reasoning accuracy and persuasiveness in Fair Use analysis, as assessed by legal practitioners?

## Limitations

- Evaluation relies on indirect metrics (PageRank, text similarity) rather than expert legal assessment of retrieval relevance
- Factor extraction process uses LLM without quantified accuracy metrics, creating uncertainty about knowledge graph quality
- Citation network may be sparse for newer cases, potentially biasing system against recent doctrinal developments
- Manual specification of retrieval weights requires legal expertise and lacks automated optimization

## Confidence

- **High confidence** in technical implementation of citation-weighted retrieval and PageRank-based authority scoring
- **Medium confidence** in factor-level granularity approach, supported by reasoning though lacking direct empirical validation
- **Low confidence** in sycophancy reduction claims, based on general CoT literature rather than legal-domain validation

## Next Checks

1. Conduct blinded evaluation where legal experts rate doctrinal relevance of retrieved cases from both Standard RAG and Structured RAG on unresolved PACER complaints, measuring precision@10 and mean average precision.

2. Manually verify LLM-extracted factor passages against ground truth for a statistically significant sample of opinions, reporting precision, recall, and F1-score for each Fair Use factor.

3. Evaluate retrieval performance on recent landmark copyright cases comparing PageRank-based weighting against time-adjusted authority measures to quantify recency bias.