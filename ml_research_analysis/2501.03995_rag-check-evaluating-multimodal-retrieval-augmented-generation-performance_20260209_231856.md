---
ver: rpa2
title: 'RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance'
arxiv_id: '2501.03995'
source_url: https://arxiv.org/abs/2501.03995
tags:
- image
- each
- response
- query
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses hallucination issues in multimodal retrieval-augmented
  generation (RAG) systems by introducing a framework called RAG-Check that evaluates
  both the retrieval relevance and generated response accuracy. The core method involves
  training two neural network models: a Relevance Score (RS) model that assesses how
  relevant retrieved pieces are to user queries, and a Correctness Score (CS) model
  that evaluates the accuracy of generated response statements against the retrieved
  context.'
---

# RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance

## Quick Facts
- arXiv ID: 2501.03995
- Source URL: https://arxiv.org/abs/2501.03995
- Reference count: 21
- Key outcome: Introduces RAG-Check framework with Relevance Score (RS) and Correctness Score (CS) models that outperform CLIP-based methods by 20% and achieve 91% human alignment for evaluating multimodal RAG systems

## Executive Summary
RAG-Check addresses hallucination issues in multimodal retrieval-augmented generation systems by introducing a dual-model framework that evaluates both retrieval relevance and generated response accuracy. The framework consists of two learned neural network models: a Relevance Score (RS) model that assesses how relevant retrieved pieces are to user queries, and a Correctness Score (CS) model that evaluates the accuracy of generated response statements against the retrieved context. RAG-Check achieves 20% better human preference alignment than CLIP-based similarity for retrieval relevance and 91% alignment with human judgments for response correctness.

## Method Summary
The core approach involves training two neural network models using a modified RLHF loss function. The RS model uses a LLaVA backbone (CLIP-ViT-large-patch14-336 + LLAMA-1.5v) with a custom head to classify statement relevance to images, while the CS model uses a VILA backbone (SigLIP + LLAMA-3) to evaluate statement correctness against retrieved context. The framework was trained on 121K samples (101K train, 10K val, 10K test) consisting of image-statement triplets from ChatGPT-derived data and human-annotated COCO pairs, with performance validated on a 5,000-sample human-annotated database.

## Key Results
- RS model achieves 20% better human preference alignment compared to CLIP-based similarity methods
- CS model achieves 91% alignment with human judgments on correctness evaluation
- Binary classification accuracy of ~86.5% for RS and ~87.5% for CS on test sets
- Successfully identifies selection-hallucination (irrelevant retrieval) and context-generation-hallucination in multimodal RAG systems

## Why This Works (Mechanism)
The framework works by decomposing the RAG evaluation problem into two distinct binary classification tasks, each trained with a specialized multimodal model optimized for its specific task. The modified RLHF loss explicitly models the relative preference between positive and negative examples, while the separate architectures (LLaVA for retrieval relevance, VILA for correctness) are better suited to their respective tasks than a generic multimodal model.

## Foundational Learning
- Multimodal embedding alignment - Why needed: To compare statements with images in a shared feature space for relevance assessment
- Cross-attention mechanisms in VLMs - Why needed: To focus on relevant image regions when evaluating statement correctness
- Response decomposition into atomic statements - Why needed: To enable fine-grained evaluation of individual claims rather than whole responses
- Binary preference modeling - Why needed: To capture human-like judgment of "better" vs "worse" rather than absolute scores
- Pronoun resolution in statement extraction - Why needed: To maintain semantic coherence when splitting complex responses into atomic statements
- Threshold optimization for binary classification - Why needed: To convert continuous model outputs into actionable evaluation decisions

## Architecture Onboarding
**Component Map:** Image → RS Model → Relevance Score; Image → CS Model → Correctness Score; Response → GPT-3.5 → Atomic Statements → Rule-based Categorization → CS Evaluation

**Critical Path:** Statement relevance evaluation requires RS model inference; correctness evaluation requires both CS model inference and prior statement extraction/categorization

**Design Tradeoffs:** Separate models for RS and CS allow task-specific optimization but increase complexity and inference time; RLHF loss captures relative preferences but requires careful negative sampling

**Failure Signatures:** Score collapse to extremes (0 or 1) indicates training instability; poor CS performance with multiple images suggests VILA template issues; slow inference is expected due to cross-attention overhead

**First Experiments:**
1. Test RS model with single image-positive statement pair to verify basic functionality
2. Validate CS model with single image-single statement to ensure correctness evaluation works
3. Evaluate both models on a small held-out validation set to check for score distribution anomalies

## Open Questions the Paper Calls Out
None

## Limitations
- Inference time is approximately 35× slower than CLIP-based methods due to cross-attention mechanisms
- CS model performance degrades with multiple images in statements, requiring atomic statement extraction
- Framework requires human-annotated evaluation datasets, limiting scalability for new domains
- Threshold optimization (η = 0.7) may not generalize across different application contexts

## Confidence
- RS/CS model architecture and training procedure: High confidence
- Performance improvements over CLIP: Medium confidence
- Human preference alignment claims: Medium confidence

## Next Checks
1. Implement and test the full dataset curation pipeline, specifically the GPT-4o prompt for generating positive/negative statements and the human verification process to ensure the 121K sample quality matches the paper's claims.

2. Conduct an ablation study varying the η threshold parameter systematically to verify that 0.7 is indeed optimal and that the threshold optimization procedure described produces stable results across different validation sets.

3. Evaluate the CS model's performance degradation when processing response statements containing multiple images, as the paper mentions this as a known limitation but does not provide quantitative analysis of how performance scales with the number of images per statement.