---
ver: rpa2
title: Which Attention Heads Matter for In-Context Learning?
arxiv_id: '2502.14010'
source_url: https://arxiv.org/abs/2502.14010
tags:
- heads
- induction
- pythia
- score
- ablation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates which attention heads are crucial for
  in-context learning (ICL) in transformer models. Through systematic ablation studies
  across 12 models ranging from 70M to 7B parameters, the authors compare two proposed
  ICL mechanisms: induction heads that copy tokens, and function vector (FV) heads
  that encode task representations.'
---

# Which Attention Heads Matter for In-Context Learning?

## Quick Facts
- arXiv ID: 2502.14010
- Source URL: https://arxiv.org/abs/2502.14010
- Authors: Kayo Yin; Jacob Steinhardt
- Reference count: 40
- Primary result: Function vector heads, not induction heads, are the primary drivers of few-shot in-context learning performance in transformer models.

## Executive Summary
This paper investigates which attention heads enable in-context learning (ICL) in transformer models through systematic ablation studies across 12 models (70M-7B parameters). The authors compare two proposed ICL mechanisms: induction heads that copy tokens and function vector (FV) heads that encode task representations. Their key finding challenges the prevailing view that induction heads are central to ICL—instead, FV heads are the primary causal drivers of few-shot ICL performance. Ablating FV heads significantly degrades ICL accuracy, with effects becoming more pronounced in larger models. In contrast, ablating induction heads has minimal impact beyond random ablation when controlling for overlap with FV heads. An intriguing discovery is that many FV heads evolve from induction heads during training, suggesting induction serves as a developmental precursor to the more sophisticated FV mechanism.

## Method Summary
The authors analyze 12 decoder-only transformer models ranging from 70M to 7B parameters. They compute induction scores using TransformerLens on random sequences and FV scores via causal mediation analysis (mean activation substitution on corrupted ICL prompts). Mean ablation is performed by replacing head outputs with dataset-averaged values, with exclusion ablations controlling for overlap between head types. Few-shot ICL accuracy is evaluated on 45 NLP tasks using 10-shot prompts. Training dynamics are analyzed using Pythia checkpoints to track individual head trajectories across training phases.

## Key Results
- FV heads, not induction heads, are the primary drivers of few-shot ICL performance across all model scales
- Ablating FV heads causes significant degradation in few-shot accuracy, with effects scaling with model size
- Many FV heads evolve from induction heads during training, suggesting induction serves as a developmental precursor
- The importance of distinguishing between few-shot accuracy and token-loss difference metrics when studying ICL mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Function Vector Heads Drive Few-Shot ICL
- Claim: FV heads are causally primary for few-shot in-context learning accuracy, not induction heads
- Mechanism: FV heads encode task-relevant information as latent vectors; when injected into forward passes, they restore ICL behavior even without demonstrations
- Core assumption: Mean ablation effects reflect functional dependence rather than correlated damage
- Evidence anchors: Ablating FV heads causes significant degradation in few-shot accuracy; ablation with exclusion shows minimal impact from induction heads when controlling for FV overlap
- Break condition: If in models <160M parameters, induction and FV heads show similar causal effects

### Mechanism 2: Induction Heads as Training Precursors to FV Heads
- Claim: Induction heads are an earlier, simpler mechanism that can transition into FV heads as training progresses
- Mechanism: During training, certain attention heads first acquire high induction scores, then their induction scores decline as FV scores rise
- Core assumption: Temporal score trajectories imply a functional transition rather than independent coexistence
- Evidence anchors: Many heads that become strong FV heads initially exhibit high induction scores that gradually decline while FV scores increase
- Break condition: If early ablation of induction heads during training does not impair subsequent FV head formation

### Mechanism 3: Metric Divergence Between Few-Shot Accuracy and Token-Loss Difference
- Claim: Few-shot ICL accuracy and token-loss difference capture different phenomena
- Mechanism: Token-loss difference measures loss reduction over long contexts (induction heads), while few-shot accuracy measures task generalization (FV heads)
- Core assumption: These metrics are not interchangeable proxies for "ICL capability"
- Evidence anchors: Ablating induction heads affects token-loss difference more than FV head ablations; contrasting results between metrics help reconcile contradictory findings
- Break condition: If a unified metric is discovered where both head types show correlated contributions

## Foundational Learning

- **Concept: Induction heads**
  - Why needed here: Core comparison class; must understand pattern-matching/copying to contrast with FV mechanism
  - Quick check question: Can you define an induction head's attention pattern and explain how it enables token-level copying?

- **Concept: Function vectors (FV)**
  - Why needed here: The mechanism this paper identifies as primary for few-shot ICL; latent task encodings extracted from attention heads
  - Quick check question: How does causal mediation identify a head's contribution to task execution under corrupted prompts?

- **Concept: Ablation with exclusion**
  - Why needed here: Critical methodological step to disentangle correlated mechanisms; controls for head overlap
  - Quick check question: Why does ablating induction heads without excluding high-FV heads overestimate induction's causal role?

## Architecture Onboarding

- **Component map**: Induction heads (early-middle layers, ~step 1000) → FV heads (deeper layers, ~step 16000)
- **Critical path**: 1) Compute induction scores via TransformerLens, 2) Compute FV scores via causal mediation, 3) Perform ablations with exclusion, 4) Measure few-shot ICL accuracy on held-out tasks
- **Design tradeoffs**: Head dimensionality vs FV strength; scale dependency (FV importance increases with scale)
- **Failure signatures**: ICL accuracy collapses when ablating FV heads but not when ablating induction heads with low FV scores
- **First 3 experiments**: 1) Replicate exclusion ablations on a new 3B parameter model, 2) Measure both metrics under identical ablations, 3) Track individual head trajectories from early to late training

## Open Questions the Paper Calls Out

- **What makes the evolution from induction heads to function vector (FV) heads a necessary step during training?**
  - Basis: The conclusion explicitly asks this question
  - Why unresolved: Observational evidence of transition exists, but no proof that FV mechanism cannot be learned from scratch
  - What evidence would resolve it: Theoretical proof or experiments showing training dynamics fail without prior induction heads

- **What functional role do the remaining induction heads (that do not become FV heads) serve in fully trained models?**
  - Basis: The conclusion asks this question
  - Why unresolved: Ablation studies show minimal impact on few-shot ICL accuracy
  - What evidence would resolve it: Targeted ablation studies on non-ICL tasks to isolate specific contributions

- **Does removing induction heads during the early phases of training prevent the formation of FV heads?**
  - Basis: Section 6 suggests exploring this to verify the precursor hypothesis
  - Why unresolved: Current study provides observational evidence but lacks causal intervention
  - What evidence would resolve it: Experiment ablating induction heads early in training followed by analysis of FV head development

## Limitations

- The functional distinction between head types relies heavily on exclusion ablation methodology, without which effects could be artifacts of correlated activation patterns
- The developmental precursor claim lacks direct causal evidence—no experiments test whether early ablation of induction heads impairs later FV head formation
- The metric divergence claim between few-shot accuracy and token-loss difference remains under-supported by corpus evidence

## Confidence

**High Confidence**: The causal role of FV heads in few-shot ICL accuracy
**Medium Confidence**: The developmental transition from induction to FV heads
**Medium Confidence**: The distinction between few-shot accuracy and token-loss difference metrics

## Next Checks

1. **Scale Boundary Testing**: Replicate exclusion ablations on models below 160M parameters to identify where the FV-dominant mechanism emerges

2. **Unified Metric Development**: Design and validate a single ICL metric that captures both task generalization and long-context pattern completion

3. **Developmental Ablation Experiment**: Conduct ablation studies during training to test whether removing induction heads during their peak activity period impairs subsequent FV head emergence