---
ver: rpa2
title: Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models
arxiv_id: '2507.09185'
source_url: https://arxiv.org/abs/2507.09185
tags:
- pruning
- neurons
- performance
- samples
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes detecting and pruning "detrimental" neurons
  in transformer MLPs that drive high-confidence predictions via dataset-specific
  correlations rather than generalizable reasoning. The method uses Integrated Gradients
  to quantify each neuron's influence on confident predictions and prunes the most
  attributed neurons layer-wise based on validation performance.
---

# Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models

## Quick Facts
- **arXiv ID:** 2507.09185
- **Source URL:** https://arxiv.org/abs/2507.09185
- **Reference count:** 38
- **Key outcome:** Method improves accuracy on six multiple-choice benchmarks using only ~10 labeled samples via targeted neuron pruning

## Executive Summary
This paper introduces a method to detect and prune "detrimental" neurons in transformer MLPs that drive high-confidence predictions via dataset-specific correlations rather than generalizable reasoning. The approach uses Integrated Gradients to quantify each neuron's influence on confident predictions and prunes the most attributed neurons layer-wise based on validation performance. Across six multiple-choice benchmarks and three model families, pruning improves accuracy over baseline and outperforms prior adaptation methods while requiring only ~10 labeled samples.

## Method Summary
The method employs Integrated Gradients to identify neurons that disproportionately influence high-confidence predictions. For each neuron in the gate_proj layer, IG scores are computed by interpolating weights from 0 to their actual values and measuring sensitivity to the maximum logit output. These scores are averaged across ~10 random samples from the task training set. A grid search then evaluates different combinations of layers and pruning percentages (5-50%) on the adaptation set, selecting the configuration that maximizes accuracy. The top-attributed neurons in the optimal layer are then permanently zeroed out.

## Key Results
- Pruning improves accuracy over baseline on six benchmarks (XNLI, MMLU, SST2, SST5, BoolQ, Balanced COPA)
- Outperforms prior adaptation methods while using only ~10 labeled samples
- Random pruning yields negligible gains, confirming importance of targeted DSM neuron removal
- Results robust to adaptation-set selection and hyperparameter choices, with peak performance at moderate pruning levels (15-25%)

## Why This Works (Mechanism)

### Mechanism 1
Integrated Gradients computed over neuron weights identifies neurons that disproportionately drive high-confidence predictions via dataset-specific shortcuts rather than generalizable reasoning. IG interpolates each neuron's weight from 0 to its actual value while measuring sensitivity of the maximum logit output. Averaging over ~10 samples yields attribution scores ranking neurons by influence on confident predictions. Core assumption: High-attribution neurons for maximum logit encode spurious correlations rather than robust reasoning pathways.

### Mechanism 2
Pruning DSM neurons in a single MLP layer (gate_proj) forces redistribution of computation to more generalizable pathways. Zeroing top-k% attributed neurons in gate_proj disables shortcut pathways. The model must rely on remaining neurons and attention mechanisms for predictions, shifting from memorized correlations to transferable representations. Core assumption: DSM neurons cluster within specific layers rather than being distributed uniformly; gate_proj is the primary locus.

### Mechanism 3
Optimal pruning configuration transfers from small adaptation sets to held-out test data. Grid search over layer index and pruning percentage using adaptation set accuracy identifies configuration that generalizes. DSM neurons identified from 10 samples capture systematic shortcuts present across the task distribution. Core assumption: DSM neurons are stable across random sample selections and reflect task-wide rather than sample-specific patterns.

## Foundational Learning

- **Concept: Integrated Gradients Attribution**
  - Why needed here: Core technique for identifying which neurons influence model outputs. Understanding path integration from baseline to input explains why IG avoids gradient saturation issues.
  - Quick check question: Why does IG integrate gradients along a path from baseline rather than computing gradients at the input directly?

- **Concept: Transformer MLP Structure (gate/up/down projections)**
  - Why needed here: Method targets gate_proj specifically. Understanding SwiGLU-style MLPs explains why gating controls information flow and is a privileged intervention point.
  - Quick check question: In a SwiGLU MLP, what role does the gate projection play vs. the up projection?

- **Concept: Knowledge Neurons Hypothesis**
  - Why needed here: Theoretical foundation that specific neurons encode factual/task information. DSM neurons are a proposed counter-category: neurons encoding spurious patterns.
  - Quick check question: What evidence supports that individual neurons can store specific factual knowledge?

## Architecture Onboarding

- **Component map:**
  ```
  Input tokens → Embedding → [Transformer Block × L]
    └── Attention → Add & Norm
    └── MLP: gate_proj(x) ⊙ up_proj(x) → down_proj → Add & Norm
  Output logits → max logit (attribution target)

  Intervention point: gate_proj weights per layer
  Score matrix: L × d (layers × neurons per gate_proj)
  ```

- **Critical path:**
  1. Sample 10 examples from task training split (random selection)
  2. For each sample, compute IG for all gate_proj neurons w.r.t. max logit
  3. Average attributions → score matrix
  4. Grid search: for each layer l ∈ [1,L], percentage p ∈ {5%,10%...40%}:
     - Prune top-p% neurons in layer l
     - Evaluate on adaptation set
  5. Select (l_opt, p_opt) maximizing accuracy
  6. Apply pruning permanently

- **Design tradeoffs:**
  - Single-layer vs. multi-layer: Single-layer chosen for simplicity and empirical adequacy; multi-layer unexplored
  - Sample count: 10 samples sufficient; more samples don't improve results and add compute
  - Layer selection: gate_proj > up_proj > down_proj empirically (appendix E.2)
  - Pruning percentage: 15-35% range robust; 40% degrades (appendix E.3)

- **Failure signatures:**
  - Random pruning shows no gains → attribution-based selection is necessary
  - Using only incorrectly predicted samples underperforms → DSM identification requires confident predictions
  - Pruning >40% or wrong layer → accuracy degradation
  - High variance across random seeds → possible data quality issues (not observed: ±0.65% std)

- **First 3 experiments:**
  1. **Sanity check:** Implement IG computation for single layer on 5 samples; verify score matrix dimensions match gate_proj shape (e.g., 32 × 14,336 for LLaMA3.1-8B).
  2. **Layer ablation:** Compare pruning gate_proj vs. up_proj vs. down_proj on BoolQ; expect gate_proj to show largest gains (+7% vs +6% vs +4%).
  3. **Sample efficiency curve:** Run with 5, 10, 20, 50, 100 samples on SST2; expect peak at ~10 samples with no improvement beyond.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Limited ablation studies make it difficult to definitively prove pruned neurons encode spurious correlations rather than valid reasoning patterns
- Single-layer gate_proj intervention may leave performance gains on the table; multi-layer pruning effects remain unexplored
- Transfer of optimal configurations from small adaptation sets to full test sets could reflect dataset-specific regularities rather than generalizable pruning strategies

## Confidence
- **High**: Empirical results showing accuracy improvements over baseline and comparison methods across six benchmarks and three model families
- **Medium**: Mechanism claiming DSM neurons encode dataset-specific correlations rather than generalizable reasoning
- **Medium**: Single-layer gate_proj intervention being sufficient for optimal performance gains

## Next Checks
1. **Ablation on attribution quality**: Compare IG-based pruning against alternative attribution methods (Integrated Hessians, Shapley values) on the same datasets to verify that IG specifically identifies the "right" neurons for pruning.
2. **Multi-layer extension**: Implement layer-wise pruning across multiple MLP layers simultaneously and compare against single-layer results to determine if performance gains are being left on the table.
3. **Correlation validation**: After pruning, analyze remaining neuron activations on both training and held-out data to verify that performance gains correlate with reduced reliance on dataset-specific patterns rather than general capability improvements.