---
ver: rpa2
title: 'RvB: Automating AI System Hardening via Iterative Red-Blue Games'
arxiv_id: '2601.19726'
source_url: https://arxiv.org/abs/2601.19726
tags:
- team
- blue
- attack
- arxiv
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Red Team vs. Blue Team (RvB) framework,
  which automates AI system hardening through iterative adversarial interactions between
  offensive and defensive agents.
---

# RvB: Automating AI System Hardening via Iterative Red-Blue Games

## Quick Facts
- arXiv ID: 2601.19726
- Source URL: https://arxiv.org/abs/2601.19726
- Reference count: 23
- Primary result: Training-free iterative Red-Blue adversarial framework achieves 90% DSR for code hardening and 45% DSR for guardrails while maintaining near-zero FPR and SDR

## Executive Summary
This paper introduces the Red Team vs. Blue Team (RvB) framework for automating AI system hardening through iterative adversarial interactions. Unlike traditional reinforcement learning, RvB operates without model parameter updates, using environmental feedback to drive continuous capability enhancement. The framework is validated across two domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks, demonstrating strong performance while maintaining computational efficiency.

## Method Summary
The RvB framework implements a training-free sequential game where Red Team agents probe systems for vulnerabilities while Blue Team agents generate and validate patches. The process iterates through state transitions where each attack generates logs that trigger remediation, encoding learning into persistent environmental changes rather than model parameters. The framework uses structured Attack Logs (file, code snippet, bug description, payload) to ensure precise remediation and prevent destructive fixes. Two implementations are evaluated: dynamic code hardening using CAI framework and Mini-SWE-Agent, and guardrail optimization using CoP framework and NeMo Guardrails.

## Key Results
- Achieves 90% Defense Success Rate for code hardening and 45% for guardrails
- Maintains near-zero False Positive Rate and Service Disruption Rate
- Demonstrates strong generalization to unseen attacks with >95% DSR on out-of-domain benchmarks
- Consumes fewer tokens than cooperative baselines while achieving higher DSR

## Why This Works (Mechanism)

### Mechanism 1: Belief Refinement Through Externalized State Transitions
The framework drives defensive improvement without model parameter updates by encoding learning into persistent environmental state changes. Each Red Team attack generates logs that trigger Blue Team remediation, inducing a state transition Sk → Sk+1. This transition acts as "evidence" that physically invalidates exploitable strategies. When the Red Team observes Sk+1, its effective search space is mathematically equivalent to sampling from a posterior distribution, even though agents are reset between rounds.

### Mechanism 2: Entropy-Driven Exploration-to-Exploitation Shift
The adversarial loop systematically reduces epistemic uncertainty, forcing attacks to become more sophisticated over iterations. Early rounds have high belief entropy H(bk), leading to broad trial-and-error attacks. As defenses accumulate, mutual information from state transitions prunes the feasible strategy space, reducing entropy monotonically (H(bk+1) ≤ H(bk)). This compels the Red Team toward highly specific exploits, while the Blue Team's Defense Success Rate rises correspondingly.

### Mechanism 3: Semantic Verification Preventing Destructive Remediation
Coupling remediation with immediate adversarial exploitation ensures patches fix vulnerabilities without breaking functionality. The Red Team's targeted attack logs provide precise, actionable context that prevents the Blue Team from making indiscriminate modifications. The framework decomposes Defense Success Rate into True DSR (valid fixes) and Fake DSR (fixes that break service), with Service Disruption Rate (SDR) as the difference.

## Foundational Learning

- **Concept: Bayesian Belief Update in Game-Theoretic Settings**
  - Why needed here: The framework formalizes agent learning as posterior distribution updates conditioned on observed state transitions
  - Quick check question: Given evidence E that a specific exploit was patched, how would you update your belief distribution over possible defense strategies?

- **Concept: Imperfect-Information Sequential Games**
  - Why needed here: RvB is explicitly modeled as a sequential, imperfect-information game where Red observes state Sk but not Blue's defense logic πB
  - Quick check question: Why does imperfect information force the Blue Team toward generalized patches rather than exploit-specific fixes?

- **Concept: Entropy as Uncertainty Quantification**
  - Why needed here: The paper uses Shannon entropy H(bk) to quantify epistemic uncertainty and proves its monotonic decrease
  - Quick check question: If H(bk) stops decreasing but DSR hasn't converged, what does this indicate about the attack-defense dynamics?

## Architecture Onboarding

- **Component map:** Red Team (Planner -> Executor -> Reporter) -> Environment State Manager -> Blue Team (Fault Localizer -> Patch Generator -> Regression Verifier) -> Verification Module

- **Critical path:** 1) Red Team probes environment → generates structured Attack Log → 2) Blue Team receives log → localizes fault → generates git diff patch → 3) Environment applies patch → state transition Sk → Sk+1 → 4) Verification runs: attack replay + service regression test → 5) If vulnerability persists, Blue Team retries; if fixed, next round begins

- **Design tradeoffs:**
  - Max epochs vs. convergence threshold: Fewer epochs risk incomplete hardening; stricter convergence increases token cost
  - Model capability vs. cost: Stronger models improve rule quality but increase per-round cost (~$4.31 for Round 4)
  - Adversarial intensity vs. service stability: More aggressive Red Teams find deeper vulnerabilities but risk pushing Blue toward overfitting

- **Failure signatures:**
  - Flat DSR across iterations: Blue Team isn't learning from logs; check log structure and model reasoning capability
  - High FPR (>5%): Guardrail overfitting; validate with larger benign sample sets
  - Non-zero SDR: Red Team reports insufficiently precise; review report formatting and root-cause identification

- **First 3 experiments:**
  1. Baseline comparison: Run cooperative MAS (Pure Blue) vs. RvB on identical CVE set; measure TDSR, FDSR, and token consumption to validate 18% efficiency claim
  2. Ablation on report structure: Strip structured fields from Attack Logs; measure DSR degradation to quantify information contribution
  3. Out-of-domain generalization test: Train guardrail on HarmBench subset; evaluate on JailBreakBench, AdvBench, SorryBench without further iteration to verify principled learning vs. overfitting

## Open Questions the Paper Calls Out
- What are the theoretical convergence bounds of the RvB framework when applied to multi-modal environments?
- How does the framework perform under asynchronous dynamics compared to the current turn-based simulation?
- Does RvB's generalization capability scale to larger, more diverse vulnerability benchmarks?

## Limitations
- Theoretical uncertainty about whether environmental state transitions reliably encode posterior belief updates without gradient-based learning
- Experimental validation covers only 10 CVEs and 4-7 jailbreak benchmarks, insufficient for robustness claims
- Results may not transfer to other LLM architectures beyond Gemini 3.0 Flash

## Confidence
- **High confidence**: Framework's ability to achieve zero SDR and maintain high DSR within tested CVE and HarmBench domains
- **Medium confidence**: Theoretical claim of Bayesian belief refinement through state transitions and monotonic entropy reduction
- **Low confidence**: Generalization claims and distinction between memorization vs. true principled learning

## Next Checks
1. Ablation study on Attack Log structure: Systematically remove each component and measure degradation in DSR and increase in SDR
2. Extended out-of-domain evaluation: Test guardrail rules on 20+ jailbreak benchmarks, including multi-turn and context-dependent attacks
3. Convergence dynamics analysis: Track belief entropy H(bk) and Attack Complexity across 20+ iterations on a fixed vulnerability set