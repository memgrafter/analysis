---
ver: rpa2
title: 'Beyond Technocratic XAI: The Who, What & How in Explanation Design'
arxiv_id: '2508.09231'
source_url: https://arxiv.org/abs/2508.09231
tags:
- explanation
- design
- explanations
- methods
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a three-part framework for designing explainable
  AI (XAI) systems that emphasizes stakeholder needs, explanation content, and delivery
  methods. The framework addresses the gap between technical XAI methods and their
  practical application by treating explanation as a sociotechnical design process.
---

# Beyond Technocratic XAI: The Who, What & How in Explanation Design

## Quick Facts
- arXiv ID: 2508.09231
- Source URL: https://arxiv.org/abs/2508.09231
- Reference count: 38
- Primary result: Introduces a three-part sociotechnical framework for XAI design emphasizing stakeholder needs, explanation content, and delivery methods

## Executive Summary
This paper presents a three-part framework for designing explainable AI (XAI) systems that addresses the gap between technical XAI methods and their practical application. The framework treats explanation as a sociotechnical design process rather than a purely technical problem, requiring designers to consider stakeholder classification (Who), explanation scope and method selection (What), and delivery modality (How). A healthcare case study demonstrates how this framework guides explanation design, showing that effective XAI requires balancing technical accuracy with accessibility, ethical reflection, and real-world constraints. The work calls for human-centered, context-aware approaches to XAI that support both understanding and accountability.

## Method Summary
The framework proposes a three-step design process: First, identify stakeholders by classifying users into Developers, Validators, Operators, and Subjects based on their goals, domain knowledge, and technical fluency. Second, define what needs to be explained by selecting from four axes: scope (local/global), focus (behavioral/mechanistic), specificity (model-agnostic/specific), and operational cost. Third, determine how explanations should be delivered through numerical, visual, textual, or interactive modalities aligned with stakeholder workflows and cognitive capacities. The method uses existing XAI techniques (e.g., SHAP for feature attribution) but emphasizes their contextual application rather than technical implementation details.

## Key Results
- Framework provides systematic approach to XAI design that goes beyond technical method selection
- Case study demonstrates application in healthcare setting with heart failure readmission prediction
- Identifies three ethical dimensions requiring attention: epistemic inequality, social inequity, and accountability gaps
- Shows how explanation quality distribution varies across institutions with different resource levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning explanations to stakeholder roles may improve explanation utility and reduce misinterpretation.
- Mechanism: By explicitly classifying users (Developers, Validators, Operators, Subjects) and mapping their goals, domain knowledge, and technical fluency, the design process filters explanation candidates toward those that match the stakeholder's interpretive capacity and decision context.
- Core assumption: Stakeholder categories capture distinct and meaningful differences in explanation needs; individuals do not fluctuate unpredictably across roles.
- Evidence anchors:
  - [abstract] "asking Who needs the explanation, What they need explained, and How that explanation should be delivered"
  - [section] Table 1 categorizes stakeholders by demographics, domain knowledge, technical knowledge, and goals; notes roles are "not always mutually exclusive"
  - [corpus] "Holistic Explainable AI (H-XAI)" similarly argues for extending transparency beyond developers
- Break condition: If a deployment context has highly heterogeneous users within a single role (e.g., clinicians with vastly different ML familiarity), the stakeholder classification may underfit and prescribe explanations that misalign with actual needs.

### Mechanism 2
- Claim: Structuring the WHAT dimension across scope, focus, specificity, and cost may reduce ad-hoc method selection and surface tradeoffs.
- Mechanism: The four-axis taxonomy forces designers to explicitly declare whether explanations are local/global, behavioral/mechanistic, model-specific/agnostic, and computationally viable—surfacing implicit assumptions that otherwise remain buried.
- Core assumption: Designers will honestly confront tradeoffs rather than default to familiar methods; cost constraints are articulable upfront.
- Evidence anchors:
  - [section] "We classify existing XAI methods along four axes that reflect their real-world implications"
  - [section] Table 2 maps methods (e.g., Gradient-based Feat. Attrib., Surrogate Methods, Counterfactuals) to scope, focus, specificity, and best-fit stakeholders
  - [corpus] "How can we trust opaque systems?" discusses criteria for robust explanations but does not validate this taxonomy
- Break condition: If operational constraints (latency, privacy) change post-design, the initial WHAT selection becomes invalid and requires re-iteration.

### Mechanism 3
- Claim: Modal alignment in HOW may mediate cognitive fit and influence whether explanations are ignored, misunderstood, or acted upon.
- Mechanism: Translating explanation content into numerical, visual, textual, or interactive formats shapes epistemic accessibility; mismatched modalities can create "illusion of understanding" or cognitive overload.
- Core assumption: Users have stable cognitive preferences and workflow constraints that map cleanly to modalities; the translation does not distort faithfulness.
- Evidence anchors:
  - [section] "Effective explanation thus requires careful attention to how insights are surfaced" and Table 3 maps modalities to stakeholders
  - [section] "users often suffer from the illusion of understanding... believing they understand an explanation when their mental model is incomplete"
  - [corpus] "Beyond Satisfaction" critiques over-reliance on subjective surveys for explanation evaluation—suggesting modal fit is under-validated
- Break condition: If explanations are delivered through multiple modalities simultaneously without coherence, users may experience conflicting signals and reduced trust.

## Foundational Learning

- Concept: **Stakeholder Analysis**
  - Why needed here: The WHO step requires distinguishing user roles by goals, knowledge, and accountability structures; without this, explanations default to developer-centric assumptions.
  - Quick check question: Can you list at least two attributes that differentiate an Operator from a Validator in a clinical AI deployment?

- Concept: **XAI Method Taxonomy (Scope, Focus, Specificity)**
  - Why needed here: The WHAT step involves selecting among LIME, SHAP, counterfactuals, circuit tracing, etc.; understanding their properties is prerequisite to principled selection.
  - Quick check question: Is SHAP a local or global method? Is it model-agnostic or model-specific?

- Concept: **Epistemic Justice / Inequality**
  - Why needed here: The framework explicitly flags that explanation distribution can reinforce power asymmetries; designers must recognize when they are privileging certain knowers over others.
  - Quick check question: In a loan approval system, who typically receives richer explanations—the bank officer or the applicant? What risk does this create?

## Architecture Onboarding

- Component map:
  - **Step 1 (WHO):** Stakeholder classification module—input user attributes (role, domain knowledge, technical fluency), output stakeholder archetype(s)
  - **Step 2 (WHAT):** Method selection module—input stakeholder archetype + constraints (latency, model access), output candidate explanation methods filtered by scope/focus/specificity/cost
  - **Step 3 (HOW):** Modal delivery module—input explanation content + stakeholder cognitive/workflow profile, output modality choice (numerical/visual/textual/interactive)
  - **Ethical Reflection Layer:** Cross-cutting check at each step for epistemic inequality, social inequity, accountability gaps

- Critical path:
  1. Identify primary stakeholder(s) for the deployment context
  2. Map stakeholder needs to WHAT axes; select viable methods
  3. Choose delivery modality aligned with workflow and cognitive load
  4. Conduct ethical reflection—whose knowledge is privileged? Who is excluded?
  5. Iterate if constraints change or feedback reveals misalignment

- Design tradeoffs:
  - Local vs. Global: Local explanations support case-specific decisions but obscure systemic patterns; global explanations aid auditing but may not help frontline operators
  - Behavioral vs. Mechanistic: Behavioral is more intuitive for non-experts; mechanistic offers deeper debugging but requires technical fluency and compute
  - Model-specific vs. Agnostic: Specific methods offer higher fidelity but require full model access; agnostic methods work on black-box APIs but may be less faithful
  - Richness vs. Latency: Interactive, multi-modal explanations demand more compute and time; lightweight formats may under-inform

- Failure signatures:
  - Explanation ignored: Likely modal mismatch or cognitive overload
  - False confidence / over-trust: Visual/textual polish conceals model uncertainty or missing confounders
  - Exclusion of Subject: Design centered on Operator/Developer without patient-facing explanation pathway
  - Untraceable decisions: No audit trail linking explanation to outcome, undermining accountability

- First 3 experiments:
  1. **Stakeholder role audit:** Before designing, run a structured interview with representatives from each expected stakeholder group (Developer, Validator, Operator, Subject) to validate role classifications and surface intra-role variance.
  2. **Method-to-stakeholder mapping prototype:** For a single prediction task, generate explanations using two methods from different WHAT quadrants (e.g., SHAP vs. counterfactual), deliver via the same modality, and measure comprehension and perceived actionability across two stakeholder groups.
  3. **Modality A/B test:** Take one explanation method (e.g., SHAP values) and deliver to the same stakeholder group in two modalities (numerical table vs. ranked bar chart with natural language summary); compare trust calibration, decision latency, and self-reported understanding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of this sociotechnical design process be empirically validated using ethnographic methods rather than traditional performance metrics?
- Basis in paper: [explicit] The authors state that empirical validation "must go beyond traditional metrics" and requires "ethnographic methods... to assess not only usability, but also which groups benefit... and which may be excluded."
- Why unresolved: Standard metrics like satisfaction scores fail to capture unintended consequences such as "misplaced trust, cognitive overload, or institutional drift" in real-world deployments.
- What evidence would resolve it: Longitudinal ethnographic studies in live deployment contexts tracking how the framework influences stakeholder exclusion or institutional workflow drift.

### Open Question 2
- Question: How can design toolkits operationalize "critical reflection practices" to prevent participatory design from becoming a token gesture?
- Basis in paper: [explicit] The paper calls for future work to "develop critical reflection practices that help teams recognize when efficiency or resource constraints are quietly narrowing whose needs are being served."
- Why unresolved: While the authors propose checklists to flag risks, they note that "simplification can also conceal harm," and it is unclear how to ensure these tools promote genuine inclusivity rather than superficial compliance.
- What evidence would resolve it: The development and user testing of specific toolkits that successfully prompt designers to contest resource constraints and prioritize marginalized stakeholder needs.

### Open Question 3
- Question: How does the distribution of explanation quality vary between well-resourced and underfunded institutions when applying this framework?
- Basis in paper: [explicit] "Future research should explore how explanation quality is distributed across institutions (e.g., well-resourced vs underfunded hospitals) and stakeholders."
- Why unresolved: The framework assumes a capacity for intentional design, but "material constraints... reflect institutional priorities" that may prevent equitable implementation.
- What evidence would resolve it: Comparative case studies analyzing the fidelity and ethical robustness of explanations generated by organizations with disparate funding and technical infrastructure.

## Limitations
- No empirical validation of the WHO-WHAT-HOW framework with actual stakeholder comprehension or decision-making outcomes
- Stakeholder classification may oversimplify heterogeneous user groups, particularly in high-stakes domains like healthcare
- The framework does not specify how to handle conflicting stakeholder needs or prioritize among them

## Confidence
- **High Confidence:** The conceptual framework structure (WHO-WHAT-HOW dimensions) is logically coherent and addresses documented gaps in XAI design practice
- **Medium Confidence:** The stakeholder classification schema reflects established HCI principles, though its practical applicability across diverse contexts needs testing
- **Medium Confidence:** The ethical considerations (epistemic inequality, social inequity, accountability) are well-grounded in responsible AI literature
- **Low Confidence:** The mapping between specific XAI methods and stakeholder needs lacks empirical validation
- **Low Confidence:** The modality selection guidance (HOW) is intuitive but not tested for cognitive fit or workflow integration

## Next Checks
1. **Stakeholder role audit:** Before designing, run a structured interview with representatives from each expected stakeholder group (Developer, Validator, Operator, Subject) to validate role classifications and surface intra-role variance.
2. **Method-to-stakeholder mapping prototype:** For a single prediction task, generate explanations using two methods from different WHAT quadrants (e.g., SHAP vs. counterfactual), deliver via the same modality, and measure comprehension and perceived actionability across two stakeholder groups.
3. **Modality A/B test:** Take one explanation method (e.g., SHAP values) and deliver to the same stakeholder group in two modalities (numerical table vs. ranked bar chart with natural language summary); compare trust calibration, decision latency, and self-reported understanding.