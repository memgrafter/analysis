---
ver: rpa2
title: Optimizing Retrieval Strategies for Financial Question Answering Documents
  in Retrieval-Augmented Generation Systems
arxiv_id: '2503.15191'
source_url: https://arxiv.org/abs/2503.15191
tags:
- retrieval
- financial
- query
- data
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a Retrieval-Augmented Generation (RAG) pipeline
  optimized for financial question answering, addressing challenges posed by domain-specific
  vocabulary and complex tabular data in documents like 10-K reports. The approach
  employs a three-phase framework: pre-retrieval data preprocessing, retrieval using
  fine-tuned embedding models with hybrid dense-sparse strategies, and post-retrieval
  reranking and document selection.'
---

# Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems

## Quick Facts
- arXiv ID: 2503.15191
- Source URL: https://arxiv.org/abs/2503.15191
- Reference count: 18
- Achieves NDCG@10 score of 0.50864 after fine-tuning stella en 1.5B v5 model

## Executive Summary
This paper addresses the challenges of financial question answering in RAG systems by optimizing retrieval strategies for domain-specific vocabulary and complex tabular data in documents like 10-K reports. The authors propose a three-phase framework: pre-retrieval data preprocessing, retrieval using fine-tuned embedding models with hybrid dense-sparse strategies, and post-retrieval reranking and document selection. Experiments on seven financial QA datasets demonstrate significant improvements in retrieval performance through contrastive fine-tuning, hybrid retrieval with optimal alpha tuning, and query expansion techniques.

## Method Summary
The approach employs a three-phase framework for financial RAG optimization. Pre-retrieval involves LLM-based query expansion and markdown-based corpus restructuring to improve document understanding. Retrieval uses fine-tuned embedding models (stella en 400M/1.5B v5) with contrastive learning and hybrid dense-sparse strategies combining BM25. Post-retrieval includes reranking top-20 documents and document selection using a DPO-trained lightweight model. The pipeline processes financial documents chunked at 512 tokens, with fine-tuning conducted over 2 epochs using MNRLoss.

## Key Results
- Achieves NDCG@10 score of 0.50864 after fine-tuning stella en 1.5B v5 model
- Hybrid retrieval with optimal alpha tuning improves accuracy across all datasets
- Post-retrieval reranking enhances document relevance for final answer generation
- DPO-trained lightweight models outperform larger models in answer generation

## Why This Works (Mechanism)
The hybrid retrieval strategy combines the semantic understanding strengths of dense embeddings with the exact matching capabilities of sparse retrieval methods like BM25. Fine-tuning the embedding models using contrastive learning on financial-specific data allows the system to better understand domain-specific vocabulary and relationships. Query expansion through LLM helps capture more comprehensive information needs, while the post-retrieval reranking and selection agent ensure the most relevant documents are passed to the final generation stage.

## Foundational Learning
- **Concept: Dense vs. Sparse Retrieval**
  - Why needed here: The hybrid retrieval method combines dense (semantic) and sparse (exact keyword) approaches, requiring understanding of their complementary strengths
  - Quick check question: Given a query about "revenue growth," which method would better identify a document mentioning a "15% increase in sales"? Which method would better find a document with the exact phrase "FY2023 Financial Report"?

- **Concept: Contrastive Fine-Tuning**
  - Why needed here: This technique adapts embedding models to the financial domain using positive/negative pairs for training
  - Quick check question: In the context of this paper, what constitutes a "positive" and a "negative" pair for training the embedding model?

- **Concept: NDCG@10 Metric**
  - Why needed here: This is the primary evaluation metric measuring ranking quality, not just binary relevance
  - Quick check question: A system retrieves 10 documents. The relevant one is at rank 5. Another system retrieves 10 documents, and the relevant one is at rank 1. Which system has a higher NDCG@10 score and why?

## Architecture Onboarding
- **Component Map:** User Query -> Query Expansion -> Hybrid Retrieval (Dense + Sparse) -> Score Fusion -> Reranking -> Document Selection -> Final LLM Generation
- **Critical Path:** The flow from user query through query expansion, hybrid retrieval, score fusion, reranking, document selection, and final generation represents the core pipeline
- **Design Tradeoffs:**
  - Accuracy vs. Compute Cost: Hybrid retrieval and reranking improve accuracy but add latency and computational overhead
  - Specificity vs. Generality: Pipeline is heavily optimized for financial domain, requiring domain-specific fine-tuning for adaptation
  - Recall vs. Precision: Hybrid retrieval aims for high recall while reranker and selection agent focus on precision
- **Failure Signatures:**
  - "Lost in the Middle": Selection agent failure could overwhelm final LLM with long context of 10 documents
  - Alpha-Tuning Mismatch: Single fixed alpha value could lead to sub-optimal results across different query types
  - Query Drift: Aggressive expansion could alter original user intent
- **First 3 Experiments:**
  1. Ablation on Retrieval: Implement only dense retriever (without fine-tuning) on FinQA dataset to establish baseline NDCG@10 score
  2. Hybrid Search Validation: Implement hybrid retrieval and perform alpha parameter sweep (0-1 in 0.1 increments) to identify optimal value for single dataset
  3. End-to-End Test: Run small query set through full pipeline (with/without query expansion) and assess qualitative document relevance improvements

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the pipeline be adapted to efficiently retrieve and index rapidly changing, time-sensitive financial data like stock fluctuations? (Basis: Need for techniques to handle streaming data given continuous market disclosures)
- **Open Question 2:** How does the RAG pipeline perform when extended to multilingual financial documents and queries? (Basis: Need for multilingual extension due to global nature of financial environments)
- **Open Question 3:** Can the hybrid retrieval parameter (α) be dynamically adjusted per query rather than optimized statically per dataset? (Basis: Optimal α varies significantly by task type, suggesting single value may be suboptimal)

## Limitations
- Exact implementation details for LLM-based query expansion and markdown restructuring remain unspecified
- DPO training methodology for document selection agent lacks sufficient detail on preference pair construction
- Optimal alpha values vary significantly across datasets (0.25-0.85), suggesting need for dataset-specific tuning

## Confidence
- **High confidence** in three-phase architecture design and general effectiveness for financial QA tasks
- **Medium confidence** in specific NDCG@10 improvement claims due to lack of ablation studies isolating component contributions
- **Low confidence** in reproducibility of exact results without implementation details for query expansion and selection agent training

## Next Checks
1. Conduct ablation studies to quantify individual contribution of each pipeline component to final NDCG@10 score
2. Test model performance on out-of-domain questions to evaluate generalization beyond financial domain
3. Implement cost-benefit analysis measuring trade-off between improved accuracy and increased computational overhead from multi-stage pipeline