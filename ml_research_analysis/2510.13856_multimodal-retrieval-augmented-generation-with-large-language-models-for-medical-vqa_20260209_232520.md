---
ver: rpa2
title: Multimodal Retrieval-Augmented Generation with Large Language Models for Medical
  VQA
arxiv_id: '2510.13856'
source_url: https://arxiv.org/abs/2510.13856
tags:
- visual
- images
- wound
- medical
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a lightweight retrieval-augmented generation
  approach for medical visual question answering, specifically targeting wound-care
  queries. Using a general-domain instruction-tuned LLM (LLaMA-4 Scout 17B), the system
  incorporates multimodal exemplars from in-domain data to improve grounding and reasoning.
---

# Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA

## Quick Facts
- **arXiv ID**: 2510.13856
- **Source URL**: https://arxiv.org/abs/2510.13856
- **Reference count**: 19
- **Primary result**: 3rd place (average score 41.37%) in MEDIQA-WV 2025 shared task using lightweight RAG with general-domain LLM

## Executive Summary
This paper presents a retrieval-augmented generation (RAG) system for medical visual question answering (VQA), specifically targeting wound-care queries. The approach uses a general-domain instruction-tuned large language model (LLaMA-4 Scout 17B) enhanced with multimodal exemplars retrieved from in-domain data. The system retrieves top-2 relevant text and image exemplars at inference time and includes them in the prompt to improve response quality. The method achieved third place among 19 teams in the MEDIQA-WV 2025 shared task, demonstrating that lightweight RAG with general-purpose LLMs can be an effective solution for multimodal clinical NLP tasks.

## Method Summary
The system employs a multimodal RAG pipeline that retrieves relevant exemplars from an in-domain dataset to augment the LLM's responses. At inference time, the system retrieves top-2 relevant text and image exemplars based on the input question and image. These exemplars are then included in the prompt to the general-domain LLM (LLaMA-4 Scout 17B), which generates the final answer. The approach leverages the LLM's strong reasoning capabilities while grounding its responses with domain-specific examples, effectively bridging the gap between general-purpose models and specialized medical knowledge requirements.

## Key Results
- Achieved 3rd place among 19 teams in MEDIQA-WV 2025 shared task with average score of 41.37%
- Retrieval-augmented prompting significantly outperformed zero-shot and few-shot baselines
- Multimodal retrieval improved clinical specificity and schema adherence compared to text-only retrieval

## Why This Works (Mechanism)
The system works by leveraging the strong reasoning and language understanding capabilities of a general-purpose LLM while compensating for its lack of domain-specific medical knowledge through retrieval-augmented prompting. By retrieving relevant exemplars from in-domain data (both text and images), the system provides the LLM with concrete examples of how to approach similar medical VQA tasks. This grounding helps the model better understand the clinical context, follow appropriate schemas, and generate more accurate responses for wound-care queries. The multimodal aspect ensures that both visual and textual aspects of medical knowledge are captured and utilized effectively.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG)**: Why needed - bridges knowledge gaps in LLMs; Quick check - can the system retrieve relevant documents for a given query?
- **Multimodal Embeddings**: Why needed - enables joint understanding of text and images; Quick check - do retrieved images align semantically with text exemplars?
- **Medical Visual Question Answering**: Why needed - specialized domain requiring clinical knowledge; Quick check - does the system follow medical response schemas?
- **Instruction-Tuned LLMs**: Why needed - provides strong reasoning while being adaptable; Quick check - can the model follow explicit prompting instructions?
- **Exemplar-Based Learning**: Why needed - grounds responses in concrete domain examples; Quick check - are retrieved exemplars contextually relevant?
- **Few-Shot Prompting**: Why needed - enables adaptation without fine-tuning; Quick check - does performance improve with additional exemplars?

## Architecture Onboarding

**Component Map**: Input Image + Question -> Multimodal Retriever -> Top-2 Exemplars -> LLM (LLaMA-4 Scout 17B) -> Generated Answer

**Critical Path**: The system's performance depends critically on the quality of retrieved exemplars and the LLM's ability to effectively incorporate them into responses. The retrieval mechanism must find semantically relevant examples, and the LLM must correctly interpret and apply the exemplar context.

**Design Tradeoffs**: Uses a general-domain LLM rather than a specialized medical model, prioritizing flexibility and reproducibility over built-in medical knowledge. Limits retrieval to top-2 exemplars to maintain prompt efficiency while potentially sacrificing coverage. Employs a lightweight approach that avoids expensive fine-tuning but depends heavily on exemplar quality.

**Failure Signatures**: Poor retrieval quality leads to irrelevant exemplars that confuse the LLM. Insufficient exemplars may result in incomplete medical context. The general-domain LLM may still struggle with highly specialized terminology even with good exemplars. Clinical accuracy depends entirely on exemplar quality rather than intrinsic medical knowledge.

**First 3 Experiments**:
1. Test retrieval precision by evaluating whether top-2 exemplars are truly relevant to given queries
2. Compare performance with different numbers of retrieved exemplars (1, 2, 4, 6) to find optimal balance
3. Evaluate performance on a held-out subset of data to assess generalization beyond the shared task distribution

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation based on single shared-task dataset with unknown test distribution, limiting real-world robustness assessment
- System relies on exemplar quality rather than intrinsic medical knowledge of the LLM, creating dependency on retrieval performance
- Top-2 exemplar limit may restrict coverage for complex or rare medical cases
- No retrieval precision/recall metrics reported, making it difficult to assess retrieval quality independently
- Lacks clinical validation beyond automated scoring and no comparison with specialized medical LLMs

## Confidence

**High confidence**: Retrieval-augmented prompting improves performance over zero-shot and few-shot baselines in the evaluated task.

**Medium confidence**: Multimodal retrieval provides additional benefit over text-only retrieval for clinical specificity and schema adherence.

**Low confidence**: Generalizability to broader clinical contexts or different medical specialties without further adaptation.

## Next Checks
1. Evaluate retrieval precision and recall separately to quantify the quality and coverage of retrieved exemplars
2. Test the system on an external, clinically validated medical VQA dataset to assess generalization beyond the shared task
3. Compare performance against a domain-specific medical LLM (e.g., Med-PaLM) under identical conditions to isolate the impact of exemplar-based retrieval versus intrinsic medical knowledge