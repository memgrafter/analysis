---
ver: rpa2
title: 'DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset'
arxiv_id: '2505.19978'
source_url: https://arxiv.org/abs/2505.19978
tags:
- dialogue
- emotional
- dialogues
- emotion
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepDialogue is a large-scale multimodal dialogue dataset containing
  40,150 high-quality multi-turn conversations across 41 domains with 20 distinct
  emotions and coherent emotional progressions. The dataset was generated by orchestrating
  interactions between 9 language models (4B-72B parameters), producing 65,600 initial
  dialogues that underwent human annotation and LLM-based quality filtering.
---

# DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset

## Quick Facts
- arXiv ID: 2505.19978
- Source URL: https://arxiv.org/abs/2505.19978
- Reference count: 40
- Primary result: 40,150 multi-turn dialogues with 20 emotions across 41 domains, synthesized through LLM orchestration and speech synthesis

## Executive Summary
DeepDialogue presents a large-scale multimodal dialogue dataset containing 40,150 high-quality multi-turn conversations across 41 domains with 20 distinct emotions and coherent emotional progressions. The dataset was generated through orchestrated interactions between 9 language models (4B-72B parameters), producing 65,600 initial dialogues that underwent human annotation and LLM-based quality filtering. Key findings include degradation in smaller models' performance beyond 6 turns, superior dialogue quality in concrete domains versus abstract ones, and the beneficial impact of cross-model interactions on coherence. The dataset also includes emotionally expressive speech synthesized through two approaches: XTTS-v2 with explicit emotion conditioning and Orpheus with implicit expression.

## Method Summary
The dataset construction involved orchestrating interactions between 9 language models of varying sizes (4B-72B parameters) to generate multi-turn dialogues. The process used stratified sampling across 41 domains and 20 emotions, with dialogues generated through paired LLM interactions using emotion transition chains. Generated dialogues underwent human annotation (Fleiss' κ=0.80) and LLM ensemble quality filtering (Cohen's κ=0.79) before speech synthesis via XTTS-v2 (emotion-conditioned) and Orpheus (implicit expression). The filtering pipeline retained dialogues scoring ≥3 on quality metrics and passed safety checks with Llama Guard.

## Key Results
- Smaller models (4-8B) showed significant coherence degradation beyond 6 turns in multi-turn dialogues
- Cross-model dialogue pairs achieved 0.65 acceptance rate versus 0.54 for same-model pairs
- Abstract domains (philosophy, spirituality) produced more invalid dialogues compared to concrete domains
- XTTS-v2 emotion-conditioned synthesis achieved 93.70% SER accuracy in emotion recognition

## Why This Works (Mechanism)
The dataset's effectiveness stems from leveraging diverse model sizes to capture different dialogue qualities, with larger models providing coherence in longer conversations while smaller models offer efficiency. The emotion transition graph ensures realistic emotional progressions, and cross-model interactions introduce variability that enhances dialogue naturalness. The dual speech synthesis approach captures both explicit emotional control (XTTS-v2) and natural expression patterns (Orpheus), providing comprehensive coverage of emotional speech synthesis.

## Foundational Learning

**LLM-as-Judge Quality Assessment**
- Why needed: Automated quality filtering at scale requires consistent evaluation criteria
- Quick check: Validate Cohen's κ=0.79 agreement rate between human and LLM assessments

**Emotion Transition Graphs**
- Why needed: Ensures coherent emotional progressions across dialogue turns
- Quick check: Verify transition probabilities maintain emotional consistency without abrupt shifts

**Cross-Model Interaction Dynamics**
- Why needed: Different model sizes and architectures produce complementary dialogue qualities
- Quick check: Compare acceptance rates between same-model and cross-model pairs (0.54 vs 0.65)

## Architecture Onboarding

**Component Map**
Data Generation -> Quality Filtering -> Speech Synthesis -> Final Dataset

**Critical Path**
Stratified Sampling -> LLM Pair Generation -> Human Annotation -> LLM Ensemble Filtering -> TTS Synthesis

**Design Tradeoffs**
- Model Size vs. Coherence: Larger models maintain quality in longer dialogues but are computationally expensive
- Explicit vs. Implicit Emotion Control: XTTS-v2 offers precise control while Orpheus captures natural expression patterns
- Human vs. LLM Filtering: Human annotation provides gold standard but LLM filtering enables scalability

**Failure Signatures**
- Small models losing coherence beyond 6 turns (acceptance rate drops significantly)
- Same-model pairs showing lower quality (0.54 acceptance) than cross-model pairs (0.65)
- Abstract domains generating more invalid dialogues than concrete ones

**3 First Experiments**
1. Generate dialogues using 4B and 8B models to verify coherence degradation beyond 6 turns
2. Compare acceptance rates between same-model (Llama-3.1-8B pairs) and cross-model (Llama-3.1-8B + Qwen2.5-32B) pairs
3. Test emotion-conditioned synthesis with XTTS-v2 using RAVDESS emotion labels to verify SER accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Exact LLM-as-judge prompt templates for quality evaluation are not provided
- Emotion transition probability weights are unspecified beyond graph structure
- Abstract domains consistently produce lower-quality dialogues than concrete ones
- Same-model pairs show significantly lower acceptance rates than cross-model pairs

## Confidence

**High Confidence:**
- Dataset scale (40,150 dialogues) and domain coverage (41 domains) are well-documented
- Human annotation quality (Fleiss' κ=0.80) and emotion labeling (20 emotions) are verifiable
- Model size degradation findings (6+ turns) are empirically supported

**Medium Confidence:**
- Speech synthesis pipeline using XTTS-v2 and Orpheus is described but lacks implementation specifics
- 93.70% SER accuracy claim requires detailed validation

**Low Confidence:**
- Exact quality filtering criteria and thresholds beyond score ≥3 requirement
- Complete stratification strategy for domain-emotion sampling

## Next Checks

1. **Reproduce Quality Filtering**: Implement the three-model LLM ensemble (Gemma3-27B, Llama-3.3-70B, Qwen2.5-72B) with the exact prompts used in the paper and validate against the stated Cohen's κ=0.79 agreement rate on a held-out sample.

2. **Validate Model Size Degradation**: Generate dialogues using the specified small models (4-8B) and empirically verify the claimed coherence drop beyond 6 turns through human evaluation of acceptance rates.

3. **Test Cross-Model Pair Benefits**: Systematically compare acceptance rates between same-model pairs versus cross-model pairs using the exact temperature settings (0.3 for ≥70B, 0.6 for ≤10B) to confirm the 0.65 vs 0.54 difference.