---
ver: rpa2
title: 'Conversational Search: From Fundamentals to Frontiers in the LLM Era'
arxiv_id: '2506.10635'
source_url: https://arxiv.org/abs/2506.10635
tags:
- conversational
- search
- information
- llms
- sigir
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This tutorial presents a comprehensive overview of conversational
  search systems, bridging foundational concepts with emerging LLM-driven advancements.
  The tutorial addresses the challenge of understanding context-dependent user queries
  in multi-turn conversations, where traditional keyword-based search falls short.
---

# Conversational Search: From Fundamentals to Frontiers in the LLM Era

## Quick Facts
- arXiv ID: 2506.10635
- Source URL: https://arxiv.org/abs/2506.10635
- Authors: Fengran Mo; Chuan Meng; Mohammad Aliannejadi; Jian-Yun Nie
- Reference count: 40
- One-line primary result: This tutorial bridges foundational conversational search concepts with emerging LLM-driven advancements including RAG, GAR, and agentic search systems.

## Executive Summary
This tutorial provides a comprehensive overview of conversational search systems, addressing the fundamental challenge of understanding context-dependent user queries in multi-turn conversations where traditional keyword-based search falls short. The tutorial presents two main paradigms: query-rewriting-based retrieval that reformulates conversational queries into self-contained forms, and conversational dense retrieval that encodes queries within their conversational context into contextualized embeddings. With the emergence of LLMs, the tutorial explores new directions such as generation-augmented retrieval (GAR), retrieval-augmented generation (RAG), and agentic conversational search systems that can dynamically plan and execute multi-step actions to complete complex information tasks.

## Method Summary
The tutorial synthesizes approaches for conversational search across two main paradigms: query-rewriting-based retrieval, which converts context-dependent queries into self-contained queries via sequence generation or term expansion before using standard retrievers, and conversational dense retrieval, which trains query encoders to directly process conversational history into contextualized embeddings. The integration of LLMs enables new capabilities through RAG (grounding generation in retrieved documents to reduce hallucination) and GAR (using generation to augment retrieval through pseudo-document creation or query decomposition). The tutorial also explores mixed-initiative strategies where systems proactively clarify ambiguities and agentic approaches that can autonomously plan multi-step search actions.

## Key Results
- Query rewriting effectively resolves context-dependent queries with anaphora and omissions by converting them into self-contained queries for standard search engines
- Conversational dense retrieval captures information needs in latent space, enabling semantic matching without explicit keyword reformulation
- LLM integration through RAG and GAR creates feedback loops that enhance both retrieval accuracy and generation quality while reducing hallucination
- Mixed-initiative strategies enable systems to proactively clarify ambiguities rather than making assumptions about user intent

## Why This Works (Mechanism)

### Mechanism 1: Contextual De-contextualization (Query Rewriting)
Multi-turn user intent, often expressed via ambiguous or anaphoric queries, can be converted into a standalone, self-contained representation for standard retrievers. The system encodes conversational history alongside the current query to generate a "de-contextualized" query via sequence generation or term expansion, flattening multi-turn context into a single rich ad-hoc query. The core assumption is that information need can be fully captured in a single explicit query string without losing nuanced intent. This method degrades if the query relies on complex implicit intent that cannot be easily translated into keyword-style explicit queries.

### Mechanism 2: Latent Context Fusion (Conversational Dense Retrieval)
Search relevance improves when the system encodes information need directly in latent space rather than relying on explicit text matching. Instead of rewriting text, a query encoder ingests the current query and conversation history to produce a single contextualized embedding vector compared against dense vector indices of documents. The embedding space must be sufficiently expressive to disambiguate user intent based on historical context. Performance collapses when conversational context exceeds the model's token window or when vector density suppresses critical rare terms needed for precision.

### Mechanism 3: Retrieval-Generation Synergy (RAG & GAR)
Integrating retrieval mechanisms with LLM generation capabilities creates a feedback loop that enhances both accuracy of answers and quality of search. GAR generates pseudo-documents, query expansions, or sub-queries to enrich sparse input, improving recall. RAG injects retrieved documents into the LLM prompt, grounding generation in factual evidence to reduce hallucination. The loop fails if the retriever provides irrelevant context causing the LLM to hallucinate or confuse the user, or if generation latency creates unacceptable lag in real-time conversation.

## Foundational Learning

- **Concept: Anaphora Resolution and Context Dependence**
  - Why needed here: Queries in conversation are rarely self-contained. Understanding how systems resolve "it" to a previous entity is the core challenge distinguishing conversational search from ad-hoc search.
  - Quick check question: If a user asks "How tall is he?" following a query about "Michael Jordan," does the system search for the string "How tall is he" or "Michael Jordan height"?

- **Concept: Dense vs. Sparse Retrieval**
  - Why needed here: The tutorial distinguishes between standard lexical matching (BM25) and dense embedding approaches. You must understand vector similarity to grasp how "Conversational Dense Retrieval" works without explicit keyword matching.
  - Quick check question: Can a dense retriever find a document about "artificial intelligence" if the user only queries "thinking machines," even if those exact words never appear in the text?

- **Concept: Mixed-Initiative Interaction**
  - Why needed here: Modern systems do not just answer; they ask. You need to understand the triggers that cause a system to pause retrieval and ask a clarifying question instead.
  - Quick check question: Should the system immediately retrieve documents for the query "Apple stock," or should it first check if the user context implies a preference for "fruit recipes"?

## Architecture Onboarding

- **Component map:** User Query → Context Manager (append history) → LLM/Re-writer (resolve intent) → Retriever → LLM (generate response) → User
- **Critical path:** The system processes user queries by appending conversational history, resolving intent through either rewriting or dense encoding, retrieving relevant documents, and generating responses that may incorporate retrieved evidence.
- **Design tradeoffs:**
  - Rewriting vs. Dense Retrieval: Rewriting is modular (works with existing search engines) but loses latent info; Dense Retrieval is precise but requires re-indexing the entire corpus into vectors and suffers from black box opacity.
  - Static vs. Agentic: A static pipeline (Retrieve then Generate) is fast but rigid. An agentic architecture (Plan, Search, Reason) is powerful but introduces latency and cost risks.
- **Failure signatures:**
  - Context Drift: The system answers based on misinterpretation of a specific pronoun, leading to a coherent but factually wrong answer for that user.
  - Lazy Retrieval: The RAG system retrieves a document that mentions the keyword but contradicts user intent, and the LLM fails to filter it out.
  - Topic Switching Failure: The system persists context from a previous topic (e.g., "Star Wars") into a new unrelated query (e.g., "black holes"), polluting the results.
- **First 3 experiments:**
  1. Implement a simple LLM-prompt to rewrite the last query into a standalone query using the previous turn's text. Measure retrieval recall against TREC CAsT.
  2. Compare performance of the rewriting pipeline (feeding BM25) against a multi-vector dense encoder on queries with heavy anaphora.
  3. Fine-tune a classifier to detect "ambiguity" and route 10% of queries to a "Clarification Question Generator" instead of the Retriever. Measure user satisfaction or task completion time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal mechanisms for collaboration between retrieval and generation in conversational search, specifically regarding when and how to switch between these modes?
- Basis in paper: [explicit] The paper states that while collaboration is expected, "concrete solutions in conversational search are still under-explored, e.g., to determine the right time and way to do it."
- Why unresolved: Current systems often rely on static pipelines rather than dynamic, intent-aware switching between generation-augmented retrieval (GAR) and retrieval-augmented generation (RAG).
- What evidence would resolve it: Development of a unified framework that can dynamically weight retrieval vs. generation based on query complexity and information availability, showing efficiency and accuracy gains over static RAG models.

### Open Question 2
- Question: How can agentic conversational search systems be designed to autonomously plan and execute multi-step actions to complete complex user information tasks?
- Basis in paper: [explicit] The paper highlights that future systems must "automatically determine the suitable action" and "dynamically perform search via intelligent planning until satisfying the users."
- Why unresolved: Moving from single-turn relevance ranking to multi-step "action planning" requires reasoning capabilities and tool use that current search paradigms do not fully support.
- What evidence would resolve it: An agent architecture that successfully decomposes ambiguous, multi-turn instructions into distinct search actions and executes them to solve complex tasks, evaluated on new agentic benchmarks.

### Open Question 3
- Question: How can user profiles and historical preferences be dynamically incorporated into each turn of a conversation without introducing latency or context noise?
- Basis in paper: [inferred] The paper notes the "additional challenge lies in dynamically incorporating users' profiles, preferences, or history into each turn" in the section on Personalized Conversational Search.
- Why unresolved: While static personalization exists, effectively integrating evolving user context into LLM prompts or dense retrieval encoders in real-time remains a technical hurdle.
- What evidence would resolve it: A model that demonstrates superior performance in conversational datasets with user profiles by effectively filtering and utilizing long-term history alongside immediate context.

## Limitations
- Query rewriting may lose nuanced intent present in interaction flow that cannot be easily translated into keyword-style explicit queries
- Dense retrieval models suffer from context truncation when conversational history exceeds model token limits
- LLM integration introduces additional uncertainty through potential hallucination and increased computational costs

## Confidence

- **High Confidence:** The fundamental distinction between sparse and dense retrieval paradigms, and the core challenge of context-dependent queries in conversational search (Levels 0-2 in the proposed framework)
- **Medium Confidence:** The effectiveness of RAG and GAR approaches for reducing hallucination and improving recall, as these rely on LLM capabilities that vary significantly across implementations
- **Medium Confidence:** The mixed-initiative strategy's ability to detect ambiguity and trigger clarifying questions, as this depends heavily on classifier design and user interaction patterns

## Next Checks

1. Implement a controlled experiment comparing context truncation effects by systematically varying conversational history length in dense retrieval models on TREC CAsT queries.
2. Conduct ablation studies on RAG systems to quantify hallucination reduction versus computational overhead across different LLM model sizes and retrieval depths.
3. Evaluate mixed-initiative trigger accuracy by deploying a prototype that logs both system-initiated clarifications and user satisfaction metrics across diverse query types.