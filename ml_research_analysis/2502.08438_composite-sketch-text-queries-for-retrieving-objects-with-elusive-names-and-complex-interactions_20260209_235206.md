---
ver: rpa2
title: Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and
  Complex Interactions
arxiv_id: '2502.08438'
source_url: https://arxiv.org/abs/2502.08438
tags:
- image
- sketch
- text
- object
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the novel problem of composite sketch+text
  based image retrieval (CSTBIR), where users search for objects with elusive names
  and complex interactions using both hand-drawn sketches and text descriptions. The
  authors curate a large dataset (CSTBIR) with approximately 2 million queries and
  108,000 natural scene images.
---

# Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions

## Quick Facts
- arXiv ID: 2502.08438
- Source URL: https://arxiv.org/abs/2502.08438
- Reference count: 8
- Primary result: Introduces a novel problem of retrieving objects using hand-drawn sketches plus text, with STNET outperforming existing baselines significantly.

## Executive Summary
This paper introduces the novel problem of Composite Sketch+Text Based Image Retrieval (CSTBIR), where users search for objects with elusive names and complex interactions using both hand-drawn sketches and text descriptions. The authors curate a large dataset (CSTBIR) with approximately 2 million queries and 108,000 natural scene images. They propose STNET, a multimodal transformer-based model that localizes relevant objects using sketch input and encodes both text and image for retrieval. The model is trained using contrastive learning plus additional objectives including object classification, sketch-guided object detection, and sketch reconstruction. STNET significantly outperforms existing text-only, sketch-only, and composite query baselines on multiple metrics (Recall@K and Median Rank) across test sets including a challenging open-category set with unseen object types.

## Method Summary
The authors propose STNET, a multimodal transformer-based model for Composite Sketch+Text Based Image Retrieval (CSTBIR). The model uses CLIP text and ViT image encoders, with a separate ViT pretrained on sketches. The sketch encoder learns to localize objects, which then guide the image encoding through an attention mechanism. The model is trained using contrastive learning (InfoNCE) plus auxiliary objectives: object classification for both text and image, sketch-guided object detection (using a YOLO-style grid), and sketch reconstruction. The combined loss function is the sum of these five components. The dataset CSTBIR is constructed by pairing sketches from Quick, Draw! with text descriptions from Visual Genome for 258 intersecting object categories, resulting in ~2M queries and 108K images.

## Key Results
- STNET achieves significant improvements in Recall@K (10, 20, 50, 100) and Median Rank over existing text-only, sketch-only, and composite query baselines.
- The model demonstrates strong performance on a challenging open-category test set containing unseen object types.
- Sketch-guided object detection and reconstruction auxiliary tasks contribute to the model's ability to localize and retrieve objects with complex interactions.

## Why This Works (Mechanism)
The STNET architecture leverages the complementary strengths of sketches and text: sketches excel at representing object shapes and visual attributes that are difficult to name, while text effectively describes interactions and contextual relationships that are difficult to sketch. The sketch-guided attention mechanism allows the image encoder to focus on object-relevant regions, improving localization accuracy. The multi-task training objective, combining contrastive learning with object classification, detection, and reconstruction losses, creates a rich representation space that captures both visual and semantic information across modalities.

## Foundational Learning
- **Contrastive Learning (InfoNCE Loss)**: Learns to pull together embeddings of matching image-query pairs while pushing apart non-matching pairs. *Why needed*: To learn a shared embedding space where similar image-query pairs are close together. *Quick check*: Verify that positive pairs have higher cosine similarity than negative pairs in the embedding space.
- **Sketch-guided Attention**: Uses sketch embeddings to attend to relevant regions in the image feature map. *Why needed*: To localize objects in images based on their sketch representation. *Quick check*: Visualize attention weights to ensure they focus on object regions corresponding to the sketch.
- **YOLO-style Object Detection**: Grid-based detection head predicts bounding boxes and object confidences. *Why needed*: To provide explicit localization supervision during training. *Quick check*: Verify detection mAP on a validation set before full retrieval training.
- **Multimodal Fusion**: Combines sketch and text information for query representation. *Why needed*: To leverage both visual and semantic information for retrieval. *Quick check*: Test retrieval performance with sketch-only and text-only variants.
- **Auxiliary Reconstruction Loss**: Reconstructs sketch from its embedding. *Why needed*: To encourage the sketch encoder to preserve detailed shape information. *Quick check*: Measure reconstruction quality (BCE+DICE) on a held-out sketch set.
- **Category Intersection Dataset Construction**: Creates aligned sketch-text pairs from different datasets. *Why needed*: To obtain large-scale training data with matching semantics. *Quick check*: Verify category alignment between Quick, Draw! sketches and Visual Genome text descriptions.

## Architecture Onboarding
- **Component Map**: Quick, Draw! Sketches + Visual Genome Text + Images -> CLIP Encoders -> Sketch-guided Attention -> Joint Embedding Space -> Retrieval Ranking
- **Critical Path**: Query (Sketch + Text) -> Sketch Encoder -> Text Encoder -> Attention-guided Image Encoder -> Joint Embedding -> Contrastive Ranking
- **Design Tradeoffs**: Using 7x7 grid for detection simplifies computation but may miss small or large objects; auxiliary reconstruction loss adds training complexity but improves sketch representation; category intersection limits dataset diversity but ensures semantic alignment.
- **Failure Signatures**: Poor sketch classification accuracy indicates domain adaptation issues; low detection mAP suggests incorrect bounding box formatting; contrastive loss plateauing indicates embedding collapse.
- **First 3 Experiments**: 1) Train standalone sketch classifier on 258 categories and check Top-1 accuracy. 2) Implement and test the sketch-guided attention mechanism with random inputs. 3) Verify data loading pipeline correctly formats bounding boxes for the 7x7 detection grid.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions arise: How does the synthetic pairing of sketches and text impact real-world query generalization? Is the model robust to semantic role violations (sketching interactions while describing objects)? What architectural modifications are needed to close the performance gap between known and novel object categories?

## Limitations
- The absence of appendix details (optimizer settings, learning rates, batch size, training epochs) hinders faithful reproduction of the exact results.
- The synthetic dataset construction pairs sketches and text from different sources, which may not reflect real user query patterns.
- The 7x7 grid-based detection head is a simplification that may not capture all object scales and aspect ratios effectively.
- The model shows significant performance degradation on open-category test sets, indicating limited generalization to unseen objects.

## Confidence
- **Major Claims**: High confidence in the novelty of the CSTBIR problem and the construction of the CSTBBIR dataset.
- **Methodology**: Medium confidence in the STNET architecture and training procedure due to missing implementation details in the appendix.
- **Results**: Medium confidence in the reported performance gains, as exact reproduction requires unknown hyperparameters.

## Next Checks
1. Implement the sketch encoder pre-training on the 258-class sketch classification task and verify its Top-1 accuracy on a held-out validation set before proceeding to retrieval training.
2. Validate the data loading pipeline for the sketch-guided object detection loss, ensuring that ground truth bounding boxes are correctly mapped to the 7x7 grid format expected by the YOLO-style head.
3. Perform an ablation study to determine the sensitivity of the final retrieval performance to the loss weights (α, β for reconstruction and the detection loss coordination terms) once the model is implemented.