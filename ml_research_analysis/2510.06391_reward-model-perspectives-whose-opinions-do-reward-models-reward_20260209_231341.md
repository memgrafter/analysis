---
ver: rpa2
title: 'Reward Model Perspectives: Whose Opinions Do Reward Models Reward?'
arxiv_id: '2510.06391'
source_url: https://arxiv.org/abs/2510.06391
tags:
- alignment
- demographic
- page
- reward
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates social biases in reward models (RMs),
  which are used to align language models to human preferences. The authors introduce
  a framework to measure the alignment of RM opinions across demographic groups, and
  evaluate RMs on three datasets: OPINIONQA (opinion alignment), BBQ and STEREOSET
  (stereotyping).'
---

# Reward Model Perspectives: Whose Opinions Do Reward Models Reward?

## Quick Facts
- **arXiv ID:** 2510.06391
- **Source URL:** https://arxiv.org/abs/2510.06391
- **Reference count:** 40
- **Primary result:** Reward models exhibit consistent sociodemographic biases, with certain groups systematically ranked higher regardless of model architecture.

## Executive Summary
This paper investigates social biases in reward models (RMs), which are used to align language models to human preferences. The authors introduce a framework to measure the alignment of RM opinions across demographic groups, and evaluate RMs on three datasets: OPINIONQA (opinion alignment), BBQ and STEREOSET (stereotyping). They find that absolute alignment varies across models, but relative alignment—the ranking of demographic groups—is consistent, with certain groups (e.g., Southerners, those with less education) consistently ranked higher. RMs also exhibit different patterns of stereotyping, with some favoring harmful stereotypes more than others. Attempts to steer RMs toward target demographics through prompting had little effect. These findings highlight the need to consider RM biases during alignment training to avoid propagating social biases in deployed models.

## Method Summary
The study evaluates seven open-source RMs on three datasets (OPINIONQA, BBQ, PRISM, STEREOSET) by computing rewards for each question-choice pair, normalizing via softmax, and measuring distributional distances between RM opinions and human respondent opinions using Jensen-Shannon and Wasserstein distances. The alignment metric A(D₁, D₂; Q) = (1/|Q|) Σ[1 - D(D₁(q), D₂(q))/D*] quantifies how far the RM's opinion distribution is from a human group's distribution, with lower distances indicating better alignment.

## Key Results
- RMs exhibit consistent sociodemographic biases in relative alignment rankings across different model architectures
- Certain demographic groups (e.g., Southerners, less educated individuals) are systematically ranked higher by RMs
- RMs show varying patterns of stereotyping behavior, with some preferring harmful stereotypes while others favor linguistic absurdities
- Steering RMs through prompting to adopt target demographic perspectives has minimal effect on their inherent biases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reward models (RMs) exhibit consistent *relative* sociodemographic biases, prioritizing specific groups (e.g., certain political or educational demographics) over others across different model architectures.
- **Mechanism:** While absolute alignment scores vary by model scale and training data, the ranking of demographic groups remains stable. This suggests that RMs may be inheriting a rigid "worldview" or preference hierarchy from the underlying pre-trained language models or the preference datasets used for fine-tuning, rather than developing independent bias profiles.
- **Core assumption:** The paper assumes that the ranking of reward scores (relative alignment) is the primary driver of downstream reinforcement learning behavior, making it more critical than the absolute magnitude of the reward.
- **Evidence anchors:**
  - [section 6.1] "RMs exhibit consistent sociodemographic biases in relative alignment... confirmed by a Friedman test (TF = 295.7; p <0.001)."
  - [corpus] The neighbor paper *Reward Models Inherit Value Biases from Pretraining* provides external evidence that RM biases likely originate from the initialization weights of the base LLM.
- **Break condition:** If RM training data were uniformly diversified to break the correlation between preference quality and specific demographic markers, this consistency would theoretically dissolve.

### Mechanism 2
- **Claim:** RMs systematically reward harmful stereotypes and linguistic absurdities (unrelated text) depending on the model, indicating that preference learning captures spurious correlations rather than purely "helpful" or "harmless" signals.
- **Mechanism:** The reward modeling process (often using the Bradley-Terry model) optimizes for the likelihood of chosen responses over rejected ones. If the training data contains stereotypical associations or if the model lacks the capability to discern semantic absurdity, it assigns high rewards to these negative patterns.
- **Core assumption:** The paper assumes that high reward scores for "Stereotyped" or "Unrelated" labels in datasets like BBQ and STEREOSET reflect genuine model preferences rather than random noise.
- **Evidence anchors:**
  - [abstract] "RMs... can systematically reward harmful stereotypes."
  - [section 6.2] Figure 6 shows that ULTRARM and LLMBLENDERRM prefer Stereotype choices, while smaller models like PYTHIA1B prefer Unrelated (absurd) choices.
- **Break condition:** This mechanism relies on the model's inability to disentangle surface-level form (e.g., linguistic fluency of a stereotype) from deep semantic safety; increasing model scale or using specific safety regularization might break this link.

### Mechanism 3
- **Claim:** In-context learning (steering via prompting) is insufficient to alter the deep-seated sociodemographic biases of a reward model.
- **Mechanism:** The paper tests "steering" by prepending persona descriptions (BIO, PORTRAY) to inputs. The finding that this fails suggests that the reward model's weights have effectively "baked in" a dominant perspective that overrides the transient signal of a system prompt or persona description.
- **Core assumption:** The study assumes that the failure to steer is due to the rigidity of the learned reward function, rather than a failure of the specific prompt templates used.
- **Evidence anchors:**
  - [abstract] "steering alone is not enough to overcome these limitations."
  - [section 6.3] "We find almost no statistically significant effects of steering RMs... effect sizes... were small (e.g., 0.086)."
- **Break condition:** If RMs were trained specifically to be highly sensitive to persona context (e.g., conditional preference learning), the steering mechanism might function as intended.

## Foundational Learning

- **Concept: Bradley-Terry Model**
  - **Why needed here:** This is the mathematical foundation for how most RMs convert raw scores into probabilities of preference. The paper references it (Section 3) as the standard for modeling $P(y_1 \succ y_2|x)$.
  - **Quick check question:** How does the Bradley-Terry model relate a raw reward score $r(x, y)$ to the probability of one completion being preferred over another?

- **Concept: Distributional Distance Metrics (JSD & Wasserstein)**
  - **Why needed here:** To measure "alignment," the paper moves beyond simple accuracy. It uses Jensen-Shannon Distance (for non-ordinal data) and Wasserstein Distance (for ordinal data) to quantify how far the RM's "opinion distribution" is from a human group's distribution.
  - **Quick check question:** Why is Wasserstein distance more suitable than Jensen-Shannon distance when answer choices have a natural order (e.g., "Strongly Disagree" to "Strongly Agree")?

- **Concept: Absolute vs. Relative Alignment**
  - **Why needed here:** This distinction is the paper's core finding. Absolute alignment varies by model size/type, but relative alignment (who is ranked higher than whom) is consistent. Understanding this is crucial for diagnosing fairness issues.
  - **Quick check question:** If an RM is re-calibrated so all absolute scores drop by 50%, does the relative alignment between demographic groups change?

## Architecture Onboarding

- **Component map:** Question $q$ and Candidate Choices $C$ -> Prompt Constructor -> Reward Model -> Normalizer -> Comparator
- **Critical path:** The formatting of the prompt (Appendix B.3) is the most fragile step. The paper notes that while they tested robustness (B.3.1), LM sensitivities to surface form can invalidate results if the prompt template differs from the RM's training distribution.
- **Design tradeoffs:**
  - **Synthetic vs. Human Data:** The study uses existing datasets (OPINIONQA, BBQ) rather than collecting fresh human feedback, limiting the analysis to pre-defined demographic buckets.
  - **Metric Selection:** The choice of distance metric (JSD vs. Wasserstein) dictates whether "close" answers (ordinal) are treated as better than "far" ones, impacting alignment scores.
- **Failure signatures:**
  - **Refusal Blindness:** BEAVER RM "never predict refusals" (Appendix F.2), meaning it assigns high rewards to toxic content that other models might flag as a refusal.
  - **Absurdity Preference:** Smaller RMs (PYTHIA1B) often prefer "Unrelated" (absurd) text (Section 6.2), indicating a lack of semantic capability rather than malicious bias.
- **First 3 experiments:**
  1. **Replicate Relative Alignment:** Take two distinct RMs (e.g., Starling and Ultra) and run them on a subset of OPINIONQA. Calculate the Spearman’s rank correlation for demographic groups to see if the rankings match (should be > 0.6 based on Section 6.1).
  2. **Steering Stress Test:** Apply the "PORTRAY" prompt style to a target demographic (e.g., "Answer as if you are 65+") and measure the change in Wasserstein distance compared to the baseline. Verify the reported low effect size (< 0.15).
  3. **Format Sensitivity Check:** Re-run a single survey question with permuted choice orders (Appendix B.3.1) to ensure the RM is ranking based on content, not positional bias.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the sociodemographic bias encoded in reward models (RMs) propagate to the downstream language models trained via RLHF?
- **Basis in paper:** [explicit] The authors state, "For future work, we would like to train RMs and LMs to measure the downstream performance on our datasets to gain a deeper understanding of the social biases of RMs in language modeling."
- **Why unresolved:** The current study evaluates RMs in isolation and does not train downstream models to verify if the observed biases affect final model outputs.
- **What evidence would resolve it:** Training LMs using the specific RMs analyzed and evaluating the resulting model generations for sociodemographic biases using the proposed alignment framework.

### Open Question 2
- **Question:** Is the failure of smaller RMs to avoid rewarding stereotyped or unrelated text primarily due to a lack of model capacity?
- **Basis in paper:** [explicit] The authors note, "We conjecture that smaller RMs may lack the capabilities necessary for understanding stereotypes, which could cause usage problems following preference learning."
- **Why unresolved:** The paper observes that smaller models often prefer linguistic absurdities (Unrelated labels), but does not isolate whether this is due to parameter count or training data distribution.
- **What evidence would resolve it:** Training RMs of varying sizes on identical preference datasets and comparing their ability to distinguish between stereotyped, anti-stereotyped, and unrelated text.

### Open Question 3
- **Question:** How robust are reward model opinion distributions to variations in prompt formatting?
- **Basis in paper:** [explicit] The authors mention, "We would like to explore the robustness of RMs to prompt formatting in future studies."
- **Why unresolved:** The authors performed a robustness check on only one survey question due to computational constraints, using a consistent format for the main study.
- **What evidence would resolve it:** Evaluating the alignment metrics across the full dataset using varied prompt structures (e.g., ordinal vs. alphabetical ordering, verbose vs. concise).

### Open Question 4
- **Question:** What training methodologies or data interventions can effectively mitigate the inherent sociodemographic biases in reward models?
- **Basis in paper:** [inferred] The authors conclude that "steering alone is not enough to overcome these limitations" and call for "more careful consideration of RM behavior in model alignment."
- **Why unresolved:** The paper demonstrates the failure of in-context learning to steer RMs toward target demographics, but does not test alternative interventions like data balancing or fine-tuning.
- **What evidence would resolve it:** Evaluating RMs fine-tuned on demographically balanced preference data to see if they exhibit lower variance in relative alignment scores across groups.

## Limitations
- The study uses existing datasets with pre-defined demographic categories rather than collecting fresh human feedback data, limiting exploration of intersectional identities
- The mechanism behind consistent relative alignment (whether from pre-training or preference data) is not definitively established
- The paper focuses on alignment within single demographic categories rather than examining compound or intersectional identities

## Confidence

- **High Confidence:** The finding that relative alignment rankings are consistent across different RM architectures is well-supported by the Friedman test results (TF = 295.7; p < 0.001) and the Spearman correlation analysis (0.67 correlation between OPINIONQA and STEREOSET rankings).

- **Medium Confidence:** The claim that steering via prompting cannot overcome sociodemographic biases is supported by small effect sizes (0.086) but may be sensitive to the specific prompt templates used. The mechanism could differ if conditional preference learning were employed during training.

- **Medium Confidence:** The systematic reward of harmful stereotypes and linguistic absurdities is demonstrated through model behavior on BBQ and STEREOSET, but the interpretation that this reflects "genuine model preferences" versus artifacts of the reward modeling process requires further investigation.

## Next Checks

1. **Dataset Attribution Experiment:** Train two RMs with identical architectures but different training datasets (one with sociodemographic balance, one without). Compare their relative alignment rankings to determine if the bias pattern persists independent of training data composition.

2. **Intersectional Analysis:** Extend the analysis beyond single demographic categories to examine intersectional groups (e.g., combining age, education, and political affiliation). This would test whether the consistent ranking pattern holds when considering compound identities rather than isolated attributes.

3. **Conditional Preference Learning Test:** Implement a variant of the RM training that explicitly conditions on demographic context (similar to the steering attempts but during training). Compare whether this reduces the systematic ranking of certain groups higher than others, distinguishing between in-context sensitivity and learned bias.