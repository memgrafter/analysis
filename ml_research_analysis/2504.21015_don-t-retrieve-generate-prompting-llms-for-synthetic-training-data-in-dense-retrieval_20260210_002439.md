---
ver: rpa2
title: 'Don''t Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense
  Retrieval'
arxiv_id: '2504.21015'
source_url: https://arxiv.org/abs/2504.21015
tags:
- hard
- negatives
- retrieval
- bm25
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes generating synthetic hard negatives for training
  dense retrieval models using large language models (LLMs), eliminating the need
  for corpus access. The authors fine-tune DistilBERT with synthetic negatives generated
  by four LLMs ranging from 4B to 30B parameters and evaluate performance across 10
  BEIR benchmark datasets.
---

# Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval

## Quick Facts
- arXiv ID: 2504.21015
- Source URL: https://arxiv.org/abs/2504.21015
- Authors: Aarush Sinha
- Reference count: 14
- One-line primary result: LLM-generated hard negatives underperform corpus-mined negatives, with scaling model size not improving performance monotonically.

## Executive Summary
This study investigates whether large language models can generate synthetic hard negatives for training dense retrieval models without requiring corpus access. The authors fine-tune DistilBERT using hard negatives produced by four LLMs ranging from 4B to 30B parameters and evaluate performance across 10 BEIR benchmark datasets. Contrary to expectations, the generative pipeline consistently underperforms traditional corpus-based mining strategies (BM25 and cross-encoder). Notably, scaling the generator model does not improve retrieval performance monotonically—the 14B parameter model outperforms the 30B model, and in some settings the 30B model is the worst performing. The best standalone LLM (Phi4-14B) achieves an average nDCG@10 of 0.314, while traditional BM25 achieves 0.359 and cross-encoder 0.366. Naive concatenation of synthetic and retrieved negatives also fails to improve performance.

## Method Summary
The authors generate synthetic hard negatives using four LLMs (Qwen3-4B, Qwen3-30B, LLaMA3-8B, Phi4-14B) prompted to produce 5 negative passages per query-positive pair from a 10K MSMARCO passage subset. These synthetic negatives are compared against BM25-mined and cross-encoder-mined negatives. DistilBERT is fine-tuned with Multiple-Negative Ranking Loss (MNRL) on each negative source configuration, with early stopping after 3 evaluation steps without improvement. Performance is evaluated on 10 BEIR datasets using nDCG@10 metric.

## Key Results
- LLM-generated hard negatives achieve substantially lower performance than corpus-mined negatives (average nDCG@10 of 0.292 vs. 0.359-0.366 for baselines)
- Scaling LLM size does not monotonically improve performance; Phi4-14B (14B) outperforms Qwen3-30B (30B)
- Naive concatenation of all negative sources sometimes degrades performance below standalone LLM negatives
- Best standalone LLM (Phi4-14B) achieves 0.314 nDCG@10 average across BEIR datasets

## Why This Works (Mechanism)

### Mechanism 1
Hard negatives provide stronger gradient signals than random negatives for contrastive learning in dense retrieval. Multiple-Negative Ranking Loss (MNRL) trains models to pull query-positive pairs closer in embedding space while pushing apart semantically similar but irrelevant passages. The difficulty of distinguishing hard negatives forces the model to learn finer-grained representations. Core assumption: The effectiveness of contrastive learning depends on negative quality—negatives that are semantically proximate but irrelevant create more informative decision boundaries.

### Mechanism 2
LLM-generated negatives underperform corpus-mined negatives because they lack distributional alignment with real retrieval challenges. LLMs generate passages that are linguistically plausible but may not reflect the specific failure modes encountered during actual retrieval (e.g., lexical overlap without semantic relevance, domain-specific distractors). BM25 and cross-encoders surface negatives from the actual target distribution—the corpus itself. Core assumption: Hard negatives are most effective when they approximate the "confusion cases" a retriever will encounter at inference time.

### Mechanism 3
Larger LLMs do not automatically produce better hard negatives—model scale is not determinative. Assumption: Larger models may generate more fluent passages that are actually *too* coherent or inadvertently relevant, reducing their utility as negatives. Smaller models may produce noisier outputs that better approximate challenging but irrelevant examples. Core assumption: The "quality" of a hard negative is not linguistic fluency but its capacity to create useful contrastive pressure.

## Foundational Learning

- Concept: **Dense Retrieval & Contrastive Learning**
  - Why needed here: The entire paper assumes understanding of how dual-encoder models map queries and passages to vectors, and how contrastive objectives (triplet loss, MNRL) shape these embeddings.
  - Quick check question: Can you explain why hard negatives create stronger gradients than random negatives in triplet loss?

- Concept: **Hard Negative Mining**
  - Why needed here: The paper positions itself as an alternative to BM25 and cross-encoder mining. Understanding what makes a negative "hard" is essential to evaluating the LLM approach.
  - Quick check question: What properties distinguish a hard negative from a random negative, and why might corpus-mined negatives better approximate these properties?

- Concept: **BEIR Benchmark & NDCG@10**
  - Why needed here: All results are reported as NDCG@10 on BEIR datasets; understanding this metric and benchmark is necessary to interpret the ~0.07 gap between LLM and baseline approaches.
  - Quick check question: What does NDCG@10 measure, and why is it appropriate for evaluating dense retrieval?

## Architecture Onboarding

- Component map:
  - Input: Query-positive passage pairs (from MSMARCO subset)
  - Hard Negative Sources:
    - BM25: Corpus-indexed lexical retrieval
    - Cross-Encoder: msmarco-MiniLM-L6-v3 with cosine similarity
    - LLMs: Qwen3-4B, Qwen3-30B, LLaMA3-8B, Phi4-14B (prompted for 5 negatives each)
  - Training Target: DistilBERT fine-tuned with MNRL, batch size 16, early stopping after 3 steps without improvement
  - Evaluation: 10 BEIR datasets, NDCG@10 metric

- Critical path:
  1. Generate negatives (via BM25, CE, or LLM)
  2. Construct training triplets (query, positive, negatives)
  3. Fine-tune DistilBERT with MNRL
  4. Evaluate on BEIR benchmark
  5. Compare NDCG@10 across conditions

- Design tradeoffs:
  - Corpus-free vs. quality: LLM approach eliminates index dependency but yields inferior negatives; BM25/CE require corpus access but produce better training signals
  - Concatenation vs. filtering: Naively combining all negative sources produces inconsistent results (sometimes degradation), suggesting need for intelligent selection
  - Model scale vs. performance: Larger LLMs (30B) underperform smaller ones (14B), challenging the assumption that stronger generators produce better synthetic data

- Failure signatures:
  - LLM-only training produces ~20% lower NDCG@10 than baselines (0.292 vs. 0.359–0.366)
  - Concatenating all sources can *degrade* performance below standalone LLM negatives
  - 30B model produces the worst negatives in some configurations
  - High variance across BEIR datasets (e.g., ArguAna: 0.445 BM25 vs. 0.101 All-LLMs)

- First 3 experiments:
  1. Establish baseline: Fine-tune DistilBERT with BM25-mined negatives on MSMARCO subset, evaluate on 2–3 BEIR datasets to confirm reproducibility of baseline numbers.
  2. Pilot LLM generation: Use smallest LLM (Qwen3-4B) to generate negatives for 100 query-positive pairs; manually inspect 10–20 examples to assess whether they are (a) semantically similar, (b) actually irrelevant, and (c) distributionally similar to corpus text.
  3. Ablate filtering: Implement a simple relevance filter (e.g., cross-encoder score threshold) on LLM-generated negatives before training; compare against unfiltered LLM negatives to test whether filtering is a promising intervention.

## Open Questions the Paper Calls Out

### Open Question 1
What automated filtering mechanisms can effectively identify and remove low-quality or "false negative" passages within LLM-generated synthetic data? Basis in paper: The authors state in the conclusion that "future research should prioritize developing advanced filtering... strategies" because naive concatenation of synthetic and mined negatives leads to inconsistent performance. Why unresolved: The paper demonstrates that LLMs generate usable negatives but lack the precision of corpus-based mining; however, it tests only naive aggregation without attempting to clean the synthetic data. What evidence would resolve it: A study showing that a specific classifier or heuristic (e.g., consistency checking against the positive passage) can remove detrimental synthetic negatives, resulting in a dataset that outperforms the raw LLM output.

### Open Question 2
How can synthetic negatives be integrated with corpus-mined negatives (e.g., BM25, Cross-Encoder) to prevent the performance degradation observed in naive concatenation? Basis in paper: The conclusion calls for "integration strategies" to leverage complementary strengths, noting that simply combining datasets often degraded performance compared to standalone baselines. Why unresolved: The authors found that adding LLM negatives to BM25/CE negatives sometimes lowered the NDCG@10 (e.g., on ArguAna), suggesting that the training signal from synthetic data conflicts with traditional signals when merged indiscriminately. What evidence would resolve it: Experiments utilizing weighted sampling strategies or curriculum learning that prioritize high-confidence negatives, successfully combining sources without loss of performance.

### Open Question 3
Why does the quality of generated hard negatives not scale monotonically with LLM parameter count, given that the 14B model outperformed the 30B model? Basis in paper: The abstract and results section explicitly highlight the anomaly that "scaling the generator model does not monotonically improve retrieval performance," with Phi4-14B often surpassing Qwen3-30B. Why unresolved: This contradicts the general assumption that stronger/larger generative models yield better synthetic data, and the paper provides no analysis on whether this is due to over-optimization, hallucination rates, or training data composition. What evidence would resolve it: An ablation study analyzing the lexical and semantic overlap of negatives generated by different model sizes against the ground truth, correlating these metrics with final retrieval performance.

## Limitations
- The paper does not explain why larger LLMs (30B) produce worse hard negatives than smaller ones (14B), leaving a critical scaling anomaly unresolved
- No analysis of prompt engineering variations or filtering strategies to improve synthetic negative quality
- The assumption that all negatives within a batch are equally weighted in MNRL is not tested, potentially missing optimization opportunities

## Confidence

- **High Confidence**: BM25 and cross-encoder outperform LLM-generated negatives on BEIR; scaling does not improve LLM performance monotonically; concatenation of all negatives can degrade performance.
- **Medium Confidence**: The distributional mismatch between LLM-generated and corpus-mined negatives is the primary driver of performance gap; larger LLMs may produce negatives that are "too coherent" or accidentally relevant.
- **Low Confidence**: The exact mechanism by which model scale affects synthetic negative quality; whether prompt engineering or filtering could reverse the scaling trend.

## Next Checks

1. **Probe Scaling Anomaly**: Generate and manually annotate 50 hard negatives each from the 14B and 30B models, scoring them on (a) semantic similarity to query, (b) actual irrelevance, and (c) fluency. Test whether larger models produce more accidentally relevant negatives.

2. **Test Filtering Interventions**: Apply a cross-encoder filter (e.g., score < 0.5) to LLM-generated negatives before training; compare against unfiltered LLM negatives to assess whether simple relevance filtering improves performance.

3. **Dataset-Specific Analysis**: Run t-SNE or UMAP on query embeddings from BEIR datasets where LLM performs best vs. worst; identify whether certain retrieval tasks (e.g., fact verification vs. QA) are more or less amenable to synthetic negatives.