---
ver: rpa2
title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
arxiv_id: '2504.01871'
source_url: https://arxiv.org/abs/2504.01871
tags:
- agent
- plan
- plans
- planning
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first mechanistic evidence that a model-free
  reinforcement learning agent can learn to plan. Using concept-based interpretability,
  the authors show that a Deep Repeated ConvLSTM (DRC) agent trained on Sokoban internally
  represents planning-relevant concepts and uses them to form and evaluate plans.
---

# Interpreting Emergent Planning in Model-Free Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2504.01871
- **Source URL:** https://arxiv.org/abs/2504.01871
- **Reference count:** 40
- **Primary result:** Model-free Deep Repeated ConvLSTM agent trained on Sokoban learns to plan internally, representing planning-relevant concepts and using them to form and evaluate plans through interventions

## Executive Summary
This paper demonstrates that a model-free reinforcement learning agent can learn to plan internally without explicit planning mechanisms. Using concept-based interpretability on a Deep Repeated ConvLSTM (DRC) agent trained on Sokoban, the authors show that the agent learns to represent planning-relevant concepts like future action directions in spatially localized formats within its ConvLSTM cell states. These representations enable the agent to iteratively refine plans during its internal computation ticks, using what resembles a parallelized bidirectional search algorithm. Crucially, intervening on these concept representations can steer the agent's behavior, proving their causal role in decision-making. This work challenges the strict dichotomy between model-free and model-based approaches by showing that model-free agents can develop implicit planning capabilities.

## Method Summary
The researchers train a Deep Repeated ConvLSTM (DRC) agent with 3 layers and 3 recurrent ticks per environment step using IMPALA actor-critic on the Boxoban dataset. They employ linear probes to identify whether the agent's cell states contain linearly separable representations of future agent movements (Agent Approach Direction) and box pushes (Box Push Direction). The methodology involves training 1x1 and 3x3 linear probes on the agent's hidden states to predict these concepts, then validating through intervention experiments where synthetic vectors are injected into the representations to steer behavior. The study focuses on identifying planning mechanisms through concept-based interpretability rather than traditional planning algorithm analysis.

## Key Results
- Linear probes achieve high Macro F1 scores in decoding "Agent Approach Direction" and "Box Push Direction" from ConvLSTM cell states, with 1x1 probes outperforming 3x3 probes
- Agent performance improves significantly when given extra thinking steps (stationary actions) at episode start, demonstrating planning capability
- Intervention experiments show that injecting synthetic "NEVER" vectors into cell states successfully steers the agent away from specific squares, proving causal influence of discovered representations
- Plan visualizations reveal a parallelized bidirectional search pattern where the agent extends paths forward from boxes and backward from targets simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Spatially Localized Concept Representation
The DRC agent learns to map the spatial grid of the environment to channels of its hidden state, allocating specific spatial regions in the cell state to represent future actions for corresponding environment squares. This creates a spatially localized and linearly separable format where concepts like "push box left" at coordinate [x,y] are stored in the activation at that specific location.

### Mechanism 2: Iterative Parallelized Bidirectional Search
The agent constructs plans iteratively using recurrent computation (ticks) as search steps. Rather than purely forward planning, evidence suggests it extends paths backward from targets and forward from boxes simultaneously across the convolution space, evaluating and pruning infeasible routes. This parallelized approach resembles bidirectional search algorithms.

### Mechanism 3: Causal Concept-Driven Behavior
The internal representations of plans have direct causal influence on the agent's final action selection. Linear directions learned by probes correspond to causal levers in the agent's decision-making manifold. Artificially injecting specific vectors (like "NEVER") into these representations demonstrably steers the agent's behavior, proving these concepts function as the "reason" for actions.

## Foundational Learning

### Concept: Model-Free RL vs. Model-Based Planning
Why needed: The paper challenges the strict dichotomy that "model-free" agents cannot plan, making it essential to understand this context. The DRC agent is Model-Free but learns an *implicit* mechanism.
Quick check: Does the agent learn an explicit transition function $P(s'|s,a)$ (Model-Based), or just a policy/value function (Model-Free)? (Answer: Model-Free)

### Concept: Linear Probes (Concept-Based Interpretability)
Why needed: This is the primary diagnostic tool for establishing that the network's hidden states contain linearly separable information about abstract concepts like "future box direction."
Quick check: If a linear probe can predict "Box Push Direction" with high accuracy, what does that imply about the geometry of the agent's hidden state?

### Concept: Recurrent Computation ("Ticks")
Why needed: The DRC architecture performs $N$ ticks of internal computation per environment step, and this "thinking time" is the substrate where iterative search (planning) occurs.
Quick check: How does forcing an agent to remain stationary for "thinking steps" test for planning capabilities?

## Architecture Onboarding

### Component map
Input (8x8x7 symbolic board) -> Convolutional encoder -> ConvLSTM stack (3 layers, 3 ticks) -> Policy/Value heads

### Critical path
The "planning" happens in the **internal ticks** of the ConvLSTM stack. As ticks progress (t=0 to t=3), the "Box Push Direction" concepts in the cell states evolve from noise to coherent plans through iterative refinement.

### Design tradeoffs
- **Ticks vs. Latency:** Increasing $N$ (ticks) improves plan quality but linearly increases inference latency
- **Depth vs. Width:** Planning concepts appear across layers, suggesting iterative refinement; deeper stacks allow more complex evaluation but may over-smooth localized features

### Failure signatures
- **Myopic Action:** Agent pushes box into corner or blocks corridor immediately, indicating insufficient search depth utilization
- **Probe Inaccuracy:** If 1x1 probes fail to decode directions but 3x3 probes succeed, spatial bijection between hidden state and board grid is broken

### First 3 experiments
1. **Probe Verification:** Train 1x1 linear probes on cell state of random layer to predict "Box Push Direction." If Macro F1 > baseline, spatial localization hypothesis holds.
2. **Thinking Step Analysis:** Run agent on "Cutoff" level (blocking corridor is fatal) with 0 vs. 5 thinking steps. Visualize plan after tick 1 and tick 15 to confirm agent "realizes" blockage.
3. **Steering Intervention:** In simple level, inject "NEVER" vector for CA onto specific square. Verify agent physically avoids that square in rollout.

## Open Questions the Paper Calls Out

1. **Role of Training Factors:** How do specific training factors, such as model architecture and environment dynamics, influence the emergence of internal planning? The study isolates a specific DRC agent and Sokoban environment, making it difficult to disentangle which factors are necessary.

2. **Generic Environment Planning:** Can model-free agents learn to plan in generic environments that lack the spatial structure utilized in Sokoban? The methodology relies on agent representing "square-level" concepts that map directly to the spatial grid.

3. **Generalizability of Search Strategy:** Is the "parallelized bidirectional search" strategy a generalizable planning algorithm or a heuristic learned specifically for Sokoban? The authors observe this pattern but note Sokoban is "especially well-suited for bidirectional search" due to distinct boxes and targets.

## Limitations

- The bidirectional search characterization remains qualitative rather than quantitative, relying on visual inspection rather than measurable metrics
- Intervention experiments use hand-designed levels that may not generalize to broader planning scenarios
- Architectural specifics of the DRC implementation, particularly the "Pool-and-Inject" mechanism, rely on referenced prior work not fully detailed in the paper

## Confidence

- **High Confidence:** The existence of planning-relevant concepts in the agent's representations (supported by linear probe success and intervention steering)
- **Medium Confidence:** The bidirectional search characterization (based on qualitative visualization)
- **Medium Confidence:** The localized spatial representation mechanism (supported by 1x1 probe superiority over baselines)

## Next Checks

1. **Quantitative Bidirectional Search Analysis:** Implement a metric to measure plan extension rates in forward vs. backward directions across multiple ticks, providing quantitative support for the bidirectional search hypothesis.

2. **Cross-Architecture Concept Transfer:** Train linear probes on DRC agent representations and test their accuracy when applied to a different planning architecture (e.g., standard ConvLSTM without repeated computation) to validate the specificity of discovered concepts.

3. **Intervention Generalization Study:** Design a systematic suite of intervention levels with varying complexity and measure intervention success rates to determine the robustness and limits of steering the agent through concept manipulation.