---
ver: rpa2
title: Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders
arxiv_id: '2512.08077'
source_url: https://arxiv.org/abs/2512.08077
tags:
- features
- feature
- molecules
- molecular
- chemical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies sparse autoencoders (SAEs) to the SMI-TED chemistry
  foundation model, decomposing its 768-dimensional molecular embeddings into interpretable,
  sparse features. This is the first application of SAE techniques to chemistry language
  models.
---

# Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders

## Quick Facts
- arXiv ID: 2512.08077
- Source URL: https://arxiv.org/abs/2512.08077
- Reference count: 40
- Key outcome: First application of sparse autoencoders to chemistry language models, revealing interpretable features that outperform neurons in detecting chemical substructures and enable causal molecular modifications.

## Executive Summary
This paper applies sparse autoencoders (SAEs) to the SMI-TED chemistry foundation model, decomposing its 768-dimensional molecular embeddings into interpretable, sparse features. The authors demonstrate that SAE features outperform individual neurons in detecting chemical substructures, achieve high reconstruction fidelity (94.7% stereo accuracy), and correlate strongly with physicochemical descriptors while being more compact and less redundant. A key finding is that single features can group molecules by shared pharmacological functions, such as opioid receptor activity, despite weak structural similarity. Causal steering experiments confirm these features encode functional chemical information, enabling targeted molecular modifications. The work establishes SAEs as a powerful framework for uncovering latent chemical knowledge in AI models, advancing interpretability and control in computational chemistry.

## Method Summary
The method applies TopK sparse autoencoders to decompose SMI-TED's 768-dimensional molecular embeddings into interpretable features. The SAE uses an 8× overcomplete dictionary (6144 features) with k=80 sparsity, trained on 5M PubChem molecules to maximize reconstruction fidelity while maintaining sparse activations. The model is evaluated through reconstruction accuracy, substructure detection F1 scores, descriptor correlations, and causal steering experiments where feature ablation modifies molecular structures in predictable ways.

## Key Results
- SAE features achieve 94.7% stereo reconstruction accuracy while maintaining k=80 sparsity (1.3% activation rate)
- Individual SAE features outperform neurons in substructure detection, with largest gains for rare functional groups
- Single features can group molecules by pharmacological function (e.g., opioid receptor activity) despite weak structural similarity
- Causal steering via feature ablation successfully modifies molecular structures while preserving chemical validity in 749 of 2,501 cases

## Why This Works (Mechanism)

### Mechanism 1: Superposition Decomposition via Dictionary Learning
Dense molecular embeddings encode more concepts than dimensions through superposition; SAEs disentangle these into interpretable sparse features. The SAE learns an overcomplete dictionary (8× expansion: 768 → 6144 features) where each input activates only k=80 features. This forces the model to allocate dedicated feature vectors to distinct concepts rather than compressing multiple meanings into single neurons. The core assumption is that chemical concepts are linearly decomposable in activation space.

### Mechanism 2: TopK Sparsity for Fidelity-Interpretability Trade-off
Explicit TopK sparsity (keeping only k=80 largest activations) achieves better fidelity-sparsity trade-offs than L1 regularization. The TopK encoder zeroes all but k features, directly controlling sparsity. The decoder reconstructs from this sparse code. This avoids L1's tendency toward shrinkage of active feature magnitudes. The core assumption is that chemical information can be preserved with ~10% feature activation (80/6144 ≈ 1.3%).

### Mechanism 3: Causal Steering via Feature Ablation
Setting specific feature activations to zero causally modifies molecular structures in predictable ways. After encoding a molecule, intervening on specific features (setting to zero) and decoding produces modified SMILES. The decoder relies on these features to generate specific substructures; removing them causes substitution. The core assumption is that features encode generative information, not just correlative patterns.

## Foundational Learning

- **Sparse Dictionary Learning**
  - Why needed: The entire method rests on decomposing activations into sparse linear combinations of interpretable basis vectors.
  - Quick check: Can you explain why L1 regularization encourages sparsity but may not produce monosemantic features?

- **SMILES Molecular Representation**
  - Why needed: SMI-TED processes SMILES strings; understanding tokenization and validity constraints is essential for interpreting reconstruction fidelity.
  - Quick check: What makes a SMILES string "chemically valid" versus "stereochemically equivalent"?

- **Transformer Encoder-Decoder Architectures**
  - Why needed: SMI-TED uses bidirectional encoding with SMILES reconstruction; the "submersion layer" is the intervention point.
  - Quick check: How does the submersion-immersion mechanism differ from standard [CLS] token pooling?

## Architecture Onboarding

- **Component map:** SMI-TED (frozen) -> 768-dim submersion layer embedding -> TopK SAE (Encoder: 768→6144 with TopK selection + Decoder: 6144→768) -> SMILES Decoder -> Modified molecular strings
- **Critical path:** 1) Extract 5M embeddings from SMI-TED submersion layer 2) Train TopK SAE (k=80, 8× expansion, 80 epochs) 3) Validate functional fidelity (stereo accuracy >94%) 4) Map features to chemical concepts via correlation/steering
- **Design tradeoffs:** Higher expansion factor (32×) → more granular features but no fidelity gain over 8×; Higher k (160) → marginal fidelity gain (1.5%) for 2× compute; TopK vs L1: TopK chosen for direct sparsity control
- **Failure signatures:** Low fraction alive (<50%): dead features, increase aux loss weight (α); High invalid SMILES rate (>20%): k too small or expansion too aggressive; Features correlate with many unrelated descriptors: insufficient disentanglement
- **First 3 experiments:** 1) Reproduction check: Train SAE on 100K subset; verify ≥90% stereo accuracy with k=80, 8× expansion 2) Feature-steering sanity test: Identify top-activating feature for carbonyl; ablate on 10 molecules; verify structural modification in ≥5 cases 3) Baseline comparison: Train L1-regularized SAE with matched parameters; compare substructure detection F1 on 3 functional groups

## Open Questions the Paper Calls Out

### Open Question 1
Do SAE features discovered in SMI-TED generalize to out-of-distribution molecular chemistries not represented in the PubChem training distribution? The SAE was trained on PubChem molecules following SMI-TED's curation protocol, which may not cover all relevant chemical domains (e.g., organometallics, polymers, inorganic complexes). Testing feature interpretability and steering efficacy on systematically OOD datasets (e.g., MOF structures, metal complexes, macrocycles) would measure whether feature-substructure correlations and causal interventions persist.

### Open Question 2
Can chemical SAE feature interpretation be automated to scale beyond manual case-by-case analysis? Current interpretation requires manual inspection of top-activating molecules, correlation analysis with descriptors, and domain expertise to assign meaning. Development of automated pipelines that map SAE features to chemical ontologies (e.g., ChEBI, functional group libraries) with quantified accuracy would enable interpretation of thousands of features without human intervention.

### Open Question 3
How do SAE features transform across different CLM architectures and model scales—do features split, merge, or remain stable? Only SMI-TED (289M parameters, 768-dim) was analyzed; whether features are universal across CLMs or architecture-specific is unknown. Training SAEs on multiple CLMs (ChemBERTa, MolT5) and comparing feature dictionaries using metrics like feature correspondence, shared activation patterns, and transferability of steering interventions would reveal whether features are universal.

## Limitations
- The SAE was only trained on SMI-TED's submersion layer activations; generalizability to other chemistry models remains untested
- Feature interpretability relies on correlation with descriptors and steering experiments rather than direct human-annotated labels
- The 8× expansion may not capture the full complexity of molecular concepts; higher expansions could reveal additional latent features

## Confidence
- **High Confidence**: Reconstruction fidelity metrics (94.7% stereo accuracy), direct causal steering experiments, and substructure detection performance have clear quantitative validation
- **Medium Confidence**: Feature interpretability through descriptor correlation and pharmacological clustering, as these rely on statistical associations rather than ground truth labels
- **Medium Confidence**: The superiority of TopK over L1 regularization in this domain, as the comparison is primarily theoretical and based on single experiment results

## Next Checks
1. **Cross-model generalization**: Apply the trained SAE to embeddings from other chemistry models (e.g., ChemBERTa, GROVER) and evaluate feature stability and reconstruction fidelity
2. **Feature compositionality test**: Design controlled experiments to verify whether combinations of SAE features capture higher-level chemical concepts beyond individual substructures
3. **Human interpretability validation**: Conduct expert chemist review of top-activating molecules for key features to assess whether SAE features align with chemical intuition and nomenclature