---
ver: rpa2
title: Reshaping Representation Space to Balance the Safety and Over-rejection in
  Large Audio Language Models
arxiv_id: '2505.19670'
source_url: https://arxiv.org/abs/2505.19670
tags:
- harmful
- safety
- lalms
- benign
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses safety-alignment vulnerabilities in Large
  Audio Language Models (LALMs) that arise during modality-adaptation-tuning. The
  authors propose Reshaping Representation Space (RRS), an unsupervised fine-tuning
  strategy that improves safety by relocating harmful representations to a refusal
  zone while minimizing drift of benign representations.
---

# Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models

## Quick Facts
- arXiv ID: 2505.19670
- Source URL: https://arxiv.org/abs/2505.19670
- Authors: Hao Yang; Lizhen Qu; Ehsan Shareghi; Gholamreza Haffari
- Reference count: 22
- Primary result: Unsupervised fine-tuning strategy improves safety in LALMs with minimal over-rejection

## Executive Summary
This paper addresses safety-alignment vulnerabilities in Large Audio Language Models (LALMs) that arise during modality-adaptation-tuning. The authors propose Reshaping Representation Space (RRS), an unsupervised fine-tuning strategy that improves safety by relocating harmful representations to a refusal zone while minimizing drift of benign representations. Using three generations of Qwen LALMs and a newly constructed audio dataset derived from BeaverTails, they compare RRS against four Supervised Fine-tuning (SFT) strategies. RRS achieves competitive or superior safety performance across audio-text, text-only, and audio-only modalities, increasing over-rejection rate by only 0.88% on average while reducing harmful response rates.

## Method Summary
RRS employs feature selection to identify safety-critical representation features from final hidden states, then reshapes these representations during fine-tuning by mapping harmful responses to a refusal zone while preserving benign representations. The method includes a penalty term to prevent catastrophic forgetting of original capabilities. The approach is evaluated against four SFT strategies using three generations of Qwen LALMs on a constructed audio dataset, demonstrating improved safety performance across multiple modalities with minimal over-rejection.

## Key Results
- RRS achieves competitive or superior safety performance across audio-text, text-only, and audio-only modalities
- Over-rejection rate increases by only 0.88% on average while harmful response rates decrease
- Maintains speech chatting capabilities, particularly preserving performance on Qwen2-Audio and Qwen2.5-Omni
- Visualizations confirm harmful and benign representations form distinct clusters post-tuning

## Why This Works (Mechanism)
The method works by strategically relocating harmful representations to a predefined refusal zone while preserving the distribution of benign representations. This targeted approach avoids the typical trade-off between safety and over-rejection seen in traditional fine-tuning methods. By focusing on feature selection from final hidden states and applying a carefully designed penalty term, RRS maintains model capabilities while improving safety responses.

## Foundational Learning

**Feature Selection from Hidden States**: Identifying safety-critical features from model representations is essential for targeted safety interventions without affecting benign responses. Quick check: Verify selected features correlate with known harmful response patterns.

**Representation Space Manipulation**: Understanding how to reshape high-dimensional representations while preserving semantic meaning requires knowledge of manifold learning and representation theory. Quick check: Ensure transformed representations maintain cluster separability.

**Catastrophic Forgetting Prevention**: The penalty term prevents loss of original capabilities during fine-tuning, requiring understanding of regularization techniques and knowledge retention. Quick check: Monitor performance degradation on pre-training tasks during tuning.

## Architecture Onboarding

**Component Map**: Audio Input -> Feature Extraction -> Representation Space -> RRS Transformation -> Safety-Aware Output

**Critical Path**: The critical path flows through feature extraction to representation space manipulation, where RRS applies its transformation. This is where safety decisions are encoded into the model's response generation.

**Design Tradeoffs**: RRS trades computational overhead during fine-tuning for improved safety performance. The feature selection process adds complexity but enables targeted safety improvements without broad capability degradation.

**Failure Signatures**: Potential failures include over-aggressive feature selection leading to excessive refusals, under-tuning resulting in insufficient safety improvements, or penalty term misconfiguration causing catastrophic forgetting.

**First Experiments**:
1. Baseline safety evaluation across all three modalities to establish performance metrics
2. Feature selection ablation study to determine optimal feature count
3. Penalty term sensitivity analysis to find optimal regularization strength

## Open Questions the Paper Calls Out
None

## Limitations
- Safety evaluation relies on automated metrics without human evaluation, potentially missing nuanced safety failures
- Feature selection from final hidden states may miss safety signals in intermediate layers or attention mechanisms
- Limited evaluation scope to three Qwen model generations on a single constructed dataset

## Confidence
- Methodology technical soundness: High
- Real-world deployment effectiveness: Low-to-Medium
- Feature selection approach generalization: Medium
- Penalty term necessity: Low-to-Medium

## Next Checks
1. Conduct comprehensive human evaluation across all three modalities to validate automated metrics and assess context-dependent safety judgments
2. Test RRS on non-Qwen LALMs (e.g., OpenAI's Whisper, Meta's AudioSeal) to assess cross-model generalization
3. Evaluate model performance after extended inference periods and under distribution shifts to quantify catastrophic forgetting rates