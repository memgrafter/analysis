---
ver: rpa2
title: Evaluating Large Language Models for Line-Level Vulnerability Localization
arxiv_id: '2404.00287'
source_url: https://arxiv.org/abs/2404.00287
tags:
- llms
- vulnerability
- code
- vulnerable
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive evaluation of large
  language models (LLMs) for line-level vulnerability localization (AVL). The authors
  assess 19 state-of-the-art LLMs across three training paradigms (prompting, discriminative
  fine-tuning, and generative fine-tuning) on C/C++ and smart contract vulnerability
  datasets.
---

# Evaluating Large Language Models for Line-Level Vulnerability Localization

## Quick Facts
- arXiv ID: 2404.00287
- Source URL: https://arxiv.org/abs/2404.00287
- Reference count: 40
- Primary result: Discriminative fine-tuning achieves 63.8% F1-score, outperforming existing learning-based AVL methods by up to 29.7% F1-score with proposed input processing strategies

## Executive Summary
This paper presents the first comprehensive evaluation of large language models for line-level vulnerability localization across 19 state-of-the-art LLMs. The authors systematically assess three training paradigms (prompting, discriminative fine-tuning, and generative fine-tuning) on C/C++ and smart contract vulnerability datasets. Their results demonstrate that discriminative fine-tuning significantly outperforms other approaches, achieving up to 63.8% F1-score. The study also identifies critical challenges with input length and unidirectional context limitations, proposing sliding window and right-forward embedding strategies that substantially improve performance.

## Method Summary
The authors evaluate 19 LLMs across three training paradigms: prompting, discriminative fine-tuning, and generative fine-tuning. They conduct experiments on C/C++ and smart contract vulnerability datasets, systematically comparing model performance. The study introduces sliding window and right-forward embedding strategies to address input length and unidirectional context limitations. Performance is measured using F1-score, with comprehensive analysis of generalization capabilities across vulnerability types and identification of patterns where models struggle with newly discovered vulnerabilities.

## Key Results
- Discriminative fine-tuning achieves up to 63.8% F1-score, substantially outperforming existing learning-based AVL methods
- Sliding window and right-forward embedding strategies improve performance by up to 29.7% F1-score
- Fine-tuned LLMs generalize well across common vulnerability types but struggle significantly with newly discovered vulnerabilities containing unfamiliar patterns

## Why This Works (Mechanism)
The effectiveness of discriminative fine-tuning stems from its ability to learn fine-grained vulnerability patterns through binary classification at the line level, rather than attempting to generate or rank entire code segments. This approach leverages the contextual understanding capabilities of LLMs while providing clear positive/negative examples that directly map to vulnerability presence. The sliding window strategy addresses the inherent input length limitations of transformer architectures by processing code in manageable segments while maintaining contextual awareness. Right-forward embedding compensates for the unidirectional nature of many LLMs by ensuring relevant forward context is available for vulnerability detection, which is crucial since vulnerabilities often depend on both preceding and following code elements.

## Foundational Learning
- **Vulnerability Pattern Recognition**: LLMs learn to identify syntactic and semantic patterns associated with vulnerabilities through fine-tuning. Why needed: Vulnerability detection requires understanding both code structure and potential security implications. Quick check: Verify model correctly identifies known vulnerability patterns in test code.
- **Contextual Understanding in Code**: Models must maintain awareness of surrounding code context to accurately localize vulnerabilities. Why needed: Vulnerabilities often span multiple lines and depend on code relationships. Quick check: Test model performance with varying context window sizes.
- **Binary Classification for Security**: Discriminative fine-tuning converts vulnerability detection into a classification problem. Why needed: Simplifies the learning task compared to generation or ranking approaches. Quick check: Evaluate classification accuracy across different vulnerability types.

## Architecture Onboarding
Component Map: Input Code -> Preprocessing (Sliding Window/Right-Forward) -> LLM Encoder -> Classification Head -> Vulnerability Score
Critical Path: The critical path involves preprocessing the input code to handle length limitations, encoding through the LLM, and generating vulnerability scores through the classification head. This sequence must maintain computational efficiency while preserving contextual information.
Design Tradeoffs: The study balances between comprehensive context capture and computational constraints. Sliding window approaches sacrifice some global context for manageable input sizes, while right-forward embedding specifically addresses unidirectional limitations at the cost of additional preprocessing complexity.
Failure Signatures: Models struggle with newly discovered vulnerabilities containing unfamiliar patterns, suggesting overfitting to training data. Input length limitations can cause context truncation, leading to missed vulnerability indicators in distant code sections.
First Experiments:
1. Baseline evaluation of all 19 LLMs using standard prompting approach on C/C++ dataset
2. Comparative analysis of discriminative vs generative fine-tuning performance
3. Ablation study of sliding window strategy with varying window sizes

## Open Questions the Paper Calls Out
The paper acknowledges that while fine-tuned LLMs generalize well across common vulnerability types, they struggle significantly with newly discovered vulnerabilities containing unfamiliar patterns. This raises questions about the models' ability to adapt to emerging threat landscapes and whether additional techniques like few-shot learning or continuous adaptation might be necessary for real-world deployment scenarios where new vulnerability patterns constantly emerge.

## Limitations
- Evaluation focuses primarily on C/C++ and smart contract datasets, limiting generalizability to other programming languages
- Significant performance degradation when encountering newly discovered vulnerabilities with unfamiliar patterns
- Reported improvements from proposed strategies may be dataset-dependent and not universally applicable

## Confidence
High Confidence: The comparative performance of discriminative fine-tuning achieving 63.8% F1-score versus other training paradigms is well-supported by the experimental design.

Medium Confidence: The generalization limitations for novel vulnerabilities are reasonable but would benefit from more extensive testing across diverse vulnerability types.

Medium Confidence: The input length and unidirectional context challenges are well-identified, though the proposed solutions' effectiveness may vary across different codebases.

## Next Checks
1. Test the proposed sliding window and right-forward embedding strategies on additional programming languages beyond C/C++ and smart contracts to verify generalizability.
2. Evaluate model performance on a time-based split where future vulnerabilities are completely excluded from training to better assess real-world generalization capabilities.
3. Conduct ablation studies to isolate the individual contributions of fine-tuning approach versus input processing strategies to the reported performance improvements.