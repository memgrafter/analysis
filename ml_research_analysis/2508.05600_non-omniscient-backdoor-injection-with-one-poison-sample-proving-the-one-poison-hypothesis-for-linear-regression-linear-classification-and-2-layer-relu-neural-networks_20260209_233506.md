---
ver: rpa2
title: 'Non-omniscient backdoor injection with one poison sample: Proving the one-poison
  hypothesis for linear regression, linear classification, and 2-layer ReLU neural
  networks'
arxiv_id: '2508.05600'
source_url: https://arxiv.org/abs/2508.05600
tags:
- poison
- data
- clean
- attack
- fpoi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that one poison sample suffices for successful
  backdoor injection across linear models and 2-layer ReLU networks. The attacker
  crafts a single poison sample in an unused data direction, ensuring it dominates
  gradients during training.
---

# Non-omniscient backdoor injection with one poison sample: Proving the one-poison hypothesis for linear regression, linear classification, and 2-layer ReLU neural networks

## Quick Facts
- arXiv ID: 2508.05600
- Source URL: https://arxiv.org/abs/2508.05600
- Reference count: 40
- Primary result: One poison sample suffices for backdoor injection across linear models and 2-layer ReLU networks

## Executive Summary
This paper proves that a single poison sample can successfully inject a backdoor into linear models and 2-layer ReLU neural networks. The attacker crafts one poison sample in an unused data direction that dominates gradients during training. For linear classification and regression, the poisoned model becomes functionally equivalent to the clean model when clean data lies in a subspace. Experiments on real datasets demonstrate 100% attack success with minimal impact on benign task performance.

## Method Summary
The attack exploits gradient dominance by crafting a single poison sample orthogonal to the clean data subspace. During training, this poison sample's gradient contribution overwhelms the clean data gradients due to its alignment with the optimization direction. For linear models, this creates a functionally equivalent but backdoored model. The attack works even when the attacker has limited knowledge about the training data distribution. For 2-layer ReLU networks, the poison sample exploits the piecewise linearity of the activation function to achieve similar gradient dominance.

## Key Results
- One poison sample suffices for successful backdoor injection in linear models and 2-layer ReLU networks
- Attack achieves 100% success rate on tested real datasets
- Poisoned models maintain nearly identical performance on benign tasks compared to clean models
- Attack works even under non-omniscient conditions where attacker has limited knowledge of training data

## Why This Works (Mechanism)
The attack works by exploiting gradient dominance during training. When a single poison sample is crafted to lie in a direction orthogonal to the clean data subspace, its gradient contribution becomes disproportionately large during optimization. This occurs because the poison sample aligns with the optimization direction while the clean data gradients largely cancel out due to their subspace structure. For linear models, this creates a new solution that is functionally equivalent to the original but includes the backdoor trigger. The mechanism relies on the assumption that clean data lies in a lower-dimensional subspace, which is common in many practical scenarios.

## Foundational Learning
- **Gradient dominance**: Understanding how individual samples can disproportionately influence model parameters during training. Why needed: Central to understanding how one sample can override the influence of many clean samples. Quick check: Verify that the poison sample's gradient magnitude exceeds the combined gradient of clean samples in the relevant direction.

- **Subspace alignment**: Knowledge of how data distributions can be confined to lower-dimensional subspaces. Why needed: The attack exploits this property to ensure clean data gradients cancel out. Quick check: Confirm that clean data lies approximately in a k-dimensional subspace where k < input dimension.

- **Functional equivalence**: Understanding when different parameter configurations produce identical outputs on a dataset. Why needed: The attack creates a backdoored model that behaves identically to the clean model on benign inputs. Quick check: Verify that the poisoned and clean models produce the same outputs on all clean test samples.

- **Non-omniscient threat model**: Understanding attacks where the adversary has limited knowledge about the training process and data. Why needed: The paper proves results under realistic attack constraints. Quick check: Confirm that the attack succeeds without requiring knowledge of specific training samples or exact data distribution.

## Architecture Onboarding

Component Map: Data Preprocessing -> Poison Sample Generation -> Model Training -> Backdoor Activation

Critical Path: Poison Sample Generation â†’ Model Training
The poison sample generation step is critical as it determines whether the attack will succeed. The sample must be carefully crafted to lie in the correct direction relative to the clean data subspace. During training, this single sample must dominate the gradient updates to ensure the backdoor is successfully injected.

Design Tradeoffs: The attack trades off between poison sample magnitude and direction precision. A larger poison sample magnitude ensures stronger gradient dominance but may be more easily detectable. The direction must be precisely orthogonal to the clean data subspace to minimize impact on benign performance while maximizing backdoor effectiveness.

Failure Signatures: The attack fails if the poison sample is not sufficiently orthogonal to the clean data subspace, if the clean data does not lie in a subspace, or if the training process includes strong regularization that counteracts the poison gradient. Detection may occur if the poison sample's magnitude is too large compared to typical training samples.

First Experiments:
1. Test the attack on synthetic data with known subspace structure to verify the theoretical claims
2. Evaluate the attack's sensitivity to the poison sample's direction precision by varying the angle from perfect orthogonality
3. Measure the clean data accuracy drop as a function of poison sample magnitude to establish the detection threshold

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework relies on specific assumptions about data subspace alignment that may not hold in complex real-world scenarios
- Experiments demonstrate perfect attack success rates on relatively simple datasets, limiting generalizability
- Does not extensively explore defensive mechanisms that could detect or mitigate such attacks
- Limited testing beyond 2-layer ReLU networks, leaving applicability to deeper architectures uncertain

## Confidence
High: Theoretical proofs for linear models under stated assumptions
Medium: Applicability to 2-layer ReLU networks based on experimental results
Low: Generalizability to complex, real-world datasets and deeper neural network architectures

## Next Checks
1. Test the attack on high-dimensional datasets with significant noise and non-linear decision boundaries to assess robustness
2. Evaluate the effectiveness of common backdoor defense mechanisms (e.g., activation clustering, spectral signatures) against this attack
3. Investigate the attack's performance when the data distribution changes or when there are multiple backdoor targets