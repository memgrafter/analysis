---
ver: rpa2
title: 'VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos'
arxiv_id: '2505.23693'
source_url: https://arxiv.org/abs/2505.23693
tags:
- video
- reasoning
- videos
- arxiv
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VF-Eval is a benchmark designed to evaluate the reasoning capabilities\
  \ of multimodal large language models (MLLMs) on AI-generated content (AIGC) videos.\
  \ The benchmark introduces four tasks\u2014coherence validation, error awareness,\
  \ error type detection, and reasoning evaluation\u2014to comprehensively assess\
  \ MLLM performance."
---

# VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos

## Quick Facts
- arXiv ID: 2505.23693
- Source URL: https://arxiv.org/abs/2505.23693
- Authors: Tingyu Song; Tongyan Hu; Guo Gan; Yilun Zhao
- Reference count: 34
- Primary result: VF-Eval benchmark shows MLLMs achieve only 51.6% overall accuracy on AIGC video tasks, with significant struggles on commonsense and physics violation detection

## Executive Summary
VF-Eval is a comprehensive benchmark designed to evaluate multimodal large language models' (MLLMs) reasoning capabilities on AI-generated content (AIGC) videos. The benchmark introduces four tasks - coherence validation, error awareness, error type detection, and reasoning evaluation - to systematically assess MLLM performance across different aspects of video understanding. Through experiments with 13 state-of-the-art MLLMs including GPT-4.1, the study reveals that even leading models struggle to consistently perform well across all tasks, with the best model achieving only 51.6% overall accuracy. The research also introduces REPROMPT, an experiment demonstrating that aligning MLLM feedback with human preferences can potentially improve video generation quality.

## Method Summary
The VF-Eval benchmark evaluates MLLMs on AIGC videos through four distinct tasks using Chain-of-Thought prompting. The dataset contains 9,740 question-answer pairs derived from AIGC videos covering diverse scenarios like dancing, swimming, and eating. Models process video frames according to specific sampling strategies (e.g., 4 frames for InternVL3, 1 fps for VideoLLaMA3) and generate responses that are extracted using GPT-4o and scored using GPT-4.1-mini as judge. Error Awareness uses binary classification, Error Type Detection uses multiple-choice, while Coherence Validation and Reasoning Evaluation use open-ended responses scored by the evaluation judge. The REPROMPT experiment tests whether MLLM-generated feedback can improve video quality through human revision and regeneration.

## Key Results
- Overall MLLM performance on VF-Eval tasks is low, with the best model (GPT-4.1) achieving only 51.6% accuracy
- MLLMs show significant bias toward "no error" responses in Error Awareness tasks, often performing near random guess level
- Error Type Detection proves particularly challenging, with models struggling to distinguish between similar error categories
- REPROMPT experiment shows human-revised prompts achieve 56.7% win rate for subject consistency and 57.6% for aesthetic quality

## Why This Works (Mechanism)

### Mechanism 1: AIGC-Specific Artifacts Degrade MLLM Reasoning
AIGC videos contain synthetic artifacts (temporal inconsistencies, abrupt motion changes, unnatural object behaviors) that violate the commonsense priors learned by MLLMs from natural video corpora, causing systematic reasoning failures. MLLMs trained on natural videos encounter AIGC artifacts (blurry faces, sudden appearances, physics violations) and commonsense knowledge conflicts with visual input, leading to incorrect inference or hallucination.

### Mechanism 2: Human-MLLM Feedback Alignment Improves Generation Quality
Aligning MLLM feedback with human preferences through prompt revision improves downstream video generation quality, particularly for subject consistency and aesthetics. The bottleneck in video generation quality is partially prompt-specification error, not solely generator architecture limitations. Human revises prompt based on video content, then revised prompt fed to video generator, improving coherence between intent and output.

### Mechanism 3: Task Complexity Modulates MLLM Error Patterns
MLLMs exhibit task-specific failure modes: better at surface quality detection than commonsense/physics violation detection, due to differential sensitivity to visual anomaly types. Quality issues (blur, distortion) produce perceptible low-level features that are easier to detect, while commonsense violations require world knowledge integration and higher cognitive load, resulting in more failures.

## Foundational Learning

- **Temporal Coherence in Video Understanding**
  - Why needed here: VF-Eval's Error Awareness and Error Type Detection tasks require distinguishing natural motion from AIGC artifacts like sudden appearances/disappearances
  - Quick check question: Can you explain why frame-to-frame object persistence is harder to verify in AIGC videos than natural videos?

- **Prompt-Video Alignment Assessment**
  - Why needed here: Coherence Validation task evaluates whether generated videos match generation prompts; requires understanding both textual semantics and visual content
  - Quick check question: Given a prompt "A cat jumps over a fence" and a video showing a cat appearing mid-air without takeoff motion, what specific misalignment would you report?

- **Chain-of-Thought (CoT) Evaluation Protocol**
  - Why needed here: All VF-Eval experiments use CoT prompting; understanding this protocol is essential for reproducing results or extending the benchmark
  - Quick check question: Why might CoT improve performance on Reasoning Evaluation but not on Error Awareness binary classification?

## Architecture Onboarding

- **Component map**: AIGC Video Corpus (9,740 QA pairs) → Task Modules (CV, EA, ED, RE) → Evaluation Pipeline (MLLM → CoT prompt → raw response → LLM-based extraction/scoring) → RePrompt Loop (Video + prompt → MLLM feedback → human revision → regeneration → quality comparison)

- **Critical path**: Load AIGC video and associated question → Apply task-specific CoT prompt (Figures 6-9) → Extract structured answer using GPT-4o (Figures 10-12) → Score via accuracy (EA, ED) or LLM-judged matching (CV, RE)

- **Design tradeoffs**: Using GPT-4.1-mini as evaluation judge introduces dependency on proprietary model; open-source alternatives not validated. Binary EA questions all designed with "Yes" as correct answer to test MLLM bias toward perceiving videos as normal - may inflate difficulty artificially. RePrompt uses only 300 videos; statistical power limited for generalizing quality improvement claims.

- **Failure signatures**: Misconception of video creation (MLLM suggests prompt expansions rather than structural corrections), neglect of critical details (fails to notice proportion errors or localized artifacts), over-reliance on commonsense (answers "one ball" when video shows two based on typical scene expectations), random-guess-level performance on EA task indicates MLLM bias toward "no error" responses.

- **First 3 experiments**:
  1. Baseline reproduction: Run InternVL3-8B and GPT-4.1-mini on validation split (2,918 QA pairs) with CoT prompts; verify overall accuracy matches reported ~41.5% and ~44.3% respectively.
  2. Ablation on visual input: Run text-only condition (remove video frames) on Error Awareness task; confirm performance drops to near-random (Figure 3 shows this pattern for InternVL3-38B).
  3. RePrompt validation extension: Select 50 videos from test set; have MLLM generate revised prompts without human intervention; compare regeneration quality against human-revised baseline to isolate human-in-the-loop contribution.

## Open Questions the Paper Calls Out

- Does incorporating specific spatial-temporal coordinates of errors (rather than text-only feedback) significantly improve the performance of MLLMs in the REPROMPT video regeneration pipeline? The authors note the re-prompt pipeline design is "relatively simplistic" because "the specific positions of error cases are not included," limiting feedback granularity.

- Do MLLMs display different error detection capabilities or biases when evaluating image-to-video (I2V) generation compared to the text-to-video (T2V) models currently benchmarked? The authors note that "only text-to-video models are considered, whereas videos generated from images may exhibit other types of error cases that are not addressed in this study."

- Can the integration of specialized computer vision techniques as auxiliary tools enhance MLLM accuracy in tasks like "Error Type Detection" where models currently struggle with distractors? The Conclusion suggests that "other approaches, such as computer vision methods, should be incorporated as auxiliary tools to improve feedback generation" because MLLMs currently do not perform well on corresponding tasks.

## Limitations

- The benchmark relies heavily on proprietary GPT-4.1 models for both evaluation and RePrompt experiments, limiting reproducibility and introducing potential bias through evaluation methodology.

- The dataset size of 9,740 QA pairs, while substantial, may not fully capture the diversity of AIGC video artifacts across different generation models and domains.

- Human-in-the-loop RePrompt experiment (300 videos) lacks statistical power to draw definitive conclusions about quality improvement claims.

## Confidence

- **High Confidence**: Task design and dataset construction methodology are well-specified and reproducible with open-source MLLMs.

- **Medium Confidence**: Performance gap findings between natural and AIGC video understanding are plausible but require validation with more diverse MLLM architectures.

- **Low Confidence**: RePrompt quality improvement claims need further validation due to limited sample size and proprietary model dependencies.

## Next Checks

1. Reproduce baseline results using open-source MLLMs (InternVL3-8B, VideoLLaMA3) on the validation split to verify task difficulty and model performance patterns.

2. Extend the RePrompt experiment to 500+ videos with systematic A/B testing between human-revised and MLLM-only revised prompts to quantify human contribution.

3. Conduct ablation studies on frame sampling strategies and CoT prompt variations to identify optimal configuration for AIGC video understanding tasks.