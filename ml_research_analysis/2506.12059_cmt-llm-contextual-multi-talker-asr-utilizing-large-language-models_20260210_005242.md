---
ver: rpa2
title: 'CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models'
arxiv_id: '2506.12059'
source_url: https://arxiv.org/abs/2506.12059
tags:
- biasing
- speech
- words
- contextual
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMT-LLM, a unified framework that combines
  multi-talker speech recognition and contextual biasing for improved rare word recognition.
  The method integrates a pretrained speech encoder with a large language model (LLM),
  using a two-stage filtering algorithm to efficiently identify relevant rare words
  from large biasing lists and incorporate them into the LLM's prompt.
---

# CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models

## Quick Facts
- arXiv ID: 2506.12059
- Source URL: https://arxiv.org/abs/2506.12059
- Reference count: 0
- Primary result: Achieves WER of 7.9% on LibriMix and 32.9% on AMI SDM with 1,000 biasing words

## Executive Summary
This paper introduces CMT-LLM, a unified framework that combines multi-talker speech recognition and contextual biasing for improved rare word recognition. The method integrates a pretrained speech encoder with a large language model (LLM), using a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM's prompt. Experimental results show that CMT-LLM achieves state-of-the-art performance, with a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, outperforming traditional contextual biasing methods.

## Method Summary
CMT-LLM combines a pretrained speech encoder with a large language model through a two-stage filtering algorithm. The system first processes multi-talker audio through the speech encoder, then applies the filtering algorithm to identify relevant rare words from large contextual biasing lists. These selected words are incorporated into the LLM's prompt, which generates the final transcription. This approach addresses the challenge of recognizing rare words in multi-talker scenarios while maintaining computational efficiency through selective biasing rather than processing entire biasing lists.

## Key Results
- Achieves WER of 7.9% on LibriMix dataset with 1,000 biasing words
- Achieves WER of 32.9% on AMI SDM dataset with 1,000 biasing words
- Outperforms traditional contextual biasing methods across tested conditions

## Why This Works (Mechanism)
The method works by leveraging the LLM's strong language modeling capabilities while constraining it with relevant contextual information. The two-stage filtering algorithm ensures that only pertinent rare words from the biasing list are passed to the LLM, preventing prompt dilution and maintaining the model's focus. By combining acoustic features from the speech encoder with linguistic context from the LLM, the system can better handle the ambiguity inherent in multi-talker scenarios where rare words might otherwise be misrecognized.

## Foundational Learning
1. Multi-talker ASR fundamentals - Why needed: Understanding overlapping speech patterns and speaker separation is crucial for baseline performance
   Quick check: Can the model distinguish between speakers in overlapping regions?

2. Contextual biasing techniques - Why needed: Essential for improving rare word recognition without retraining the base model
   Quick check: Does the system correctly incorporate rare words from the biasing list?

3. Large language model prompting - Why needed: LLM integration requires careful prompt engineering for optimal performance
   Quick check: Are the rare words effectively incorporated into the LLM prompt?

4. Two-stage filtering algorithms - Why needed: Critical for managing computational complexity with large biasing lists
   Quick check: Does the filtering algorithm maintain high recall while reducing bias list size?

5. Speech encoder architectures - Why needed: Foundation for extracting acoustic features from multi-talker audio
   Quick check: Does the encoder handle overlapping speech signals effectively?

6. WER (Word Error Rate) metrics - Why needed: Standard evaluation metric for ASR system performance
   Quick check: Are the reported WER values consistent with baseline comparisons?

## Architecture Onboarding

Component Map:
Speech Encoder -> Two-Stage Filtering -> LLM Prompt Construction -> LLM Output

Critical Path:
Audio Input → Speech Encoder → Feature Extraction → Two-Stage Filtering → Prompt Construction → LLM → Transcription Output

Design Tradeoffs:
- Filtering precision vs. computational efficiency
- Biasing list size vs. model complexity
- LLM integration depth vs. inference speed
- Rare word recall vs. prompt relevance

Failure Signatures:
- High false positive rate in filtering stage
- LLM prompt dilution from irrelevant words
- Speaker confusion in overlapping regions
- Rare word omission despite presence in biasing list

First Experiments:
1. Test filtering algorithm recall and precision with varying biasing list characteristics
2. Evaluate WER impact of different prompt construction strategies
3. Measure inference time and memory usage across different biasing list sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on specific test sets (LibriMix, AMI SDM) that may not represent all real-world multi-talker scenarios
- Efficiency gains versus accuracy trade-offs of the two-stage filtering algorithm not fully characterized
- Behavior with varying biasing list characteristics (distribution, domain specificity) unexplored
- Computational overhead during inference and memory requirements not addressed

## Confidence

High confidence: The technical implementation of the CMT-LLM framework, the use of a two-stage filtering algorithm, and the reported WER improvements on tested datasets

Medium confidence: Claims about practical applicability and efficiency gains in real-world settings

Low confidence: Generalization claims beyond the tested domains and datasets

## Next Checks
1. Evaluate CMT-LLM on additional multi-talker datasets with varying acoustic conditions and speaker overlap ratios to assess robustness
2. Conduct ablation studies to quantify the contribution of each component (filtering algorithm, LLM integration) to overall performance
3. Measure and report inference time and memory usage compared to baseline methods to assess practical deployment considerations