---
ver: rpa2
title: 'Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer
  Learning'
arxiv_id: '2512.21702'
source_url: https://arxiv.org/abs/2512.21702
tags:
- audio
- detection
- deepfake
- arxiv
- bengali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting deepfake audio
  in the Bengali language, a task that has received little attention despite the growing
  threat of synthetic speech. We investigate both zero-shot and fine-tuned approaches
  using the BanglaFake dataset.
---

# Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning

## Quick Facts
- **arXiv ID**: 2512.21702
- **Source URL**: https://arxiv.org/abs/2512.21702
- **Reference count**: 37
- **Primary result**: ResNet18 achieves 79.17% accuracy, 79.12% F1 score, 84.37% AUC, and 24.35% EER on Bengali audio deepfake detection

## Executive Summary
This study investigates deepfake audio detection for the Bengali language, a low-resource domain where synthetic speech generation poses emerging threats. The authors evaluate both zero-shot inference using pretrained models and fine-tuning approaches on the BanglaFake dataset. While zero-shot methods show near-random performance (best at 53.80% accuracy), fine-tuning yields substantial improvements, with ResNet18 achieving 79.17% accuracy. The work establishes the first systematic benchmark for Bengali audio deepfake detection and demonstrates that deep learning models require domain-specific adaptation to detect synthetic speech effectively.

## Method Summary
The study uses the BanglaFake dataset containing 12,260 real and 13,260 synthetic Bengali audio samples (5-second clips, 16 kHz). Zero-shot inference tests six pretrained models without modification, while fine-tuning applies five architectures including ResNet18, Wav2Vec2-Base, and ViT-B16. Audio preprocessing converts samples to mel-spectrograms (64-128 bands, dB scale, normalized, 224×224 for image models). Training uses 70:15:15 splits with Adam optimization (lr=0.0001), BCE loss, and early stopping. Evaluation metrics include accuracy, precision, recall, F1, EER, and AUC.

## Key Results
- Zero-shot transfer from pretrained models achieves poor performance (best: 53.80% accuracy, 56.60% AUC, 46.20% EER)
- Fine-tuned ResNet18 outperforms all other models with 79.17% accuracy, 79.12% F1 score, 84.37% AUC, and 24.35% EER
- Spectrogram-based image models (ResNet18, ViT-B16) outperform raw-audio sequential models (Wav2Vec2-Base: 65.28% accuracy)
- LCNN-Attention achieves highest AUC (88.48%) but requires more training epochs (14 vs. 3 for ResNet18)

## Why This Works (Mechanism)

### Mechanism 1
Pretrained audio models trained primarily on high-resource languages exhibit poor zero-shot transfer to Bengali deepfake detection. Self-supervised models like Wav2Vec2-XLSR-53 and Whisper learn language-agnostic acoustic representations during pretraining, but their exposure to Bengali is limited. When applied zero-shot to Bengali deepfake detection, the learned representations do not encode the language-specific prosodic patterns or the subtle artifacts introduced by Bengali-trained TTS systems, resulting in near-random performance. The core assumption is that synthetic artifacts in Bengali deepfakes differ acoustically from those in the pretraining distribution (primarily English and other high-resource languages).

### Mechanism 2
Fine-tuning on domain-specific Bengali audio enables models to learn synthetic artifacts unique to the VITS-based generation pipeline. During fine-tuning, gradient updates reweight model parameters to emphasize spectral and temporal features that distinguish the VITS-generated Bengali speech from authentic recordings. The BanglaFake dataset provides paired real/synthetic samples from matched speakers, allowing the model to isolate generation artifacts from speaker-specific characteristics. The core assumption is that the VITS-based synthesizer leaves detectable traces (spectral discontinuities, phase inconsistencies, or prosodic flattening) that are learnable with supervised training.

### Mechanism 3
Spectrogram-based visual representations allow image classification architectures (ResNet18, ViT-B16) to outperform raw-audio sequential models on this task. Converting audio to mel-spectrograms transforms the detection problem into image classification, where convolutional and vision transformer architectures excel at detecting spatial patterns. The VITS synthesis artifacts manifest as irregular textures or boundary discontinuities in the time-frequency domain, which these architectures capture through hierarchical feature extraction or patch-wise attention. The core assumption is that deepfake artifacts have spatial structure in spectrograms that is more learnable than temporal patterns in raw waveforms for this dataset duration (5-second clips).

## Foundational Learning

- **Concept: Mel-spectrogram representation**
  - **Why needed here**: All top-performing models (ResNet18, ViT-B16, LCNN variants) operate on mel-spectrograms. Understanding how audio maps to time-frequency images is essential for debugging preprocessing and interpreting model decisions.
  - **Quick check question**: If you increase the number of mel bands from 64 to 128, what trade-off do you introduce between frequency resolution and model input size?

- **Concept: Equal Error Rate (EER)**
  - **Why needed here**: The paper emphasizes EER as a robustness metric alongside accuracy. EER captures the operational point where false acceptance equals false rejection—critical for security applications where both error types matter.
  - **Quick check question**: A model achieves 75% accuracy but 35% EER. Another achieves 70% accuracy with 20% EER. Which is preferable for a fraud detection system and why?

- **Concept: Transfer learning vs. fine-tuning distinction**
  - **Why needed here**: The paper's central finding hinges on this distinction. Zero-shot uses frozen pretrained weights; fine-tuning updates them. Knowing when each applies prevents misapplying models to new domains.
  - **Quick check question**: You have a pretrained Wav2Vec2 model and 500 labeled Bengali audio samples. Should you freeze the encoder and train only the classifier head, or fine-tune the full model? What factors inform your choice?

## Architecture Onboarding

- **Component map**:
  Raw Audio (16kHz, 5 sec) → Preprocessing Branch: → Mel-spectrogram extraction (64-128 bands) → [ResNet18 / ViT-B16 / LCNN variants] OR → Raw waveform → [Wav2Vec2-Base] → Classification Head (binary: real/fake) → Loss: Cross-Entropy or Weighted BCE

- **Critical path**:
  1. Audio preprocessing (resampling to 16kHz, padding/truncation to 5 sec)
  2. Spectrogram extraction (mel bands, FFT/hop lengths, dB normalization)
  3. Channel expansion for image models (replicate to 3 channels)
  4. Model forward pass with pretrained backbone
  5. Binary classification output

- **Design tradeoffs**:
  - ResNet18 vs. ViT-B16: ResNet18 converges faster (3 epochs) with lower compute; ViT-B16 may capture global context better but requires smaller batches and longer training.
  - LCNN vs. LCNN-Attention: Attention adds temporal focus but increases epochs (14 vs. 2) with modest AUC gain (88.48% vs. 50.48%).
  - Sequence modeling vs. frame-level: Wav2Vec2-Base and CNN-BiLSTM model temporal dependencies but underperform image-based approaches here—suggesting spectral artifacts dominate.

- **Failure signatures**:
  - High recall (>98%) with low precision (<66%): Model detects most deepfakes but over-predicts the fake class (seen in ResNet18, Wav2Vec2-Base). Consider threshold tuning or class-weighted loss.
  - AUC near 50% with high EER: Model has no discriminative power (LCNN zero-shot behavior). Check for data leakage, label corruption, or preprocessing errors.
  - Zero precision/recall: Model predicts single class exclusively (WavLM-Base-Plus zero-shot). Verify classification head initialization and output activation.

- **First 3 experiments**:
  1. Reproduce ResNet18 baseline: Train ResNet18 on mel-spectrograms with paper hyperparameters (Adam, lr=0.0001, batch=32, 3 epochs). Target: ~79% accuracy, EER <25%.
  2. Ablate spectrogram resolution: Compare 64 vs. 128 mel bands on validation AUC. Hypothesis: Higher resolution may capture subtle artifacts but risks overfitting given dataset size (~25k samples).
  3. Threshold optimization: Using the best checkpoint, sweep classification thresholds to balance precision/recall for deployment scenarios (e.g., high-precision mode vs. high-recall mode).

## Open Questions the Paper Calls Out

- **Cross-lingual transfer learning**: Can multilingual models improve Bengali detection beyond single-language fine-tuning? The study only tests monolingual Bengali fine-tuning; multilingual models like XLSR-53 were evaluated only in zero-shot mode.

- **Robustness to unseen methods**: How do models perform against deepfake generation methods not represented in the BanglaFake dataset? The dataset only contains VITS-based synthetic speech, but real-world threats include voice conversion, adversarial perturbations, and other TTS architectures.

- **Cross-speaker generalization**: How does detection performance generalize across diverse Bengali speakers, dialects, and recording conditions beyond the seven speakers in the current dataset? With only seven speakers and controlled recording conditions, model generalization to the broader Bengali-speaking population remains unknown.

## Limitations
- Zero-shot transfer performance reflects domain gap but doesn't quantify linguistic vs. synthesizer differences
- VITS-based generator may leave artifacts that won't generalize to newer TTS architectures
- Dataset size (~25k samples) limits statistical power for comparing architectures
- Study doesn't address cross-speaker generalization or adversarial robustness
- All models evaluated on fixed 5-second clips, leaving questions about variable-length or streaming audio detection

## Confidence
- **High confidence**: Fine-tuning ResNet18 achieves 79.17% accuracy and 24.35% EER on the BanglaFake test set (directly measured and reported)
- **Medium confidence**: Spectrogram-based models outperform raw-audio models for this task (supported by ablation within the study but not validated across different datasets or TTS methods)
- **Medium confidence**: Zero-shot transfer fails due to Bengali-specific artifacts (plausible mechanism but not experimentally isolated from other confounding factors)

## Next Checks
1. Test whether spectrogram-based approaches maintain superiority when applied to a different Bengali TTS generator (e.g., FastSpeech2 or Tacotron2 variants)
2. Evaluate cross-speaker generalization by training on speakers 1-15 and testing on held-out speaker 16, measuring performance degradation
3. Perform threshold optimization analysis to find the operating point that minimizes equal error rate, then report the corresponding precision-recall trade-off curve