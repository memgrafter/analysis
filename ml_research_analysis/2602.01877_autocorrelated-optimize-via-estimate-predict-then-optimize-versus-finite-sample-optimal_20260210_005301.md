---
ver: rpa2
title: 'Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample
  Optimal'
arxiv_id: '2602.01877'
source_url: https://arxiv.org/abs/2602.01877
tags:
- data
- varma
- optimization
- learning
- a-ove
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares data-driven optimization methods in portfolio
  optimization with trading costs under autocorrelated uncertainty. It proposes an
  autocorrelated Optimize-via-Estimate (A-OVE) model that optimizes out-of-sample
  performance as a function of VARMA process sufficient statistics.
---

# Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal

## Quick Facts
- arXiv ID: 2602.01877
- Source URL: https://arxiv.org/abs/2602.01877
- Reference count: 40
- Primary result: A-OVE model outperforms PTO, ETO, and FPtP benchmarks in portfolio optimization with trading costs under autocorrelated uncertainty.

## Executive Summary
This paper introduces the Autocorrelated Optimize-via-Estimate (A-OVE) framework for data-driven optimization under autocorrelated uncertainty. The method addresses the limitation of standard machine learning approaches that optimize prediction accuracy rather than downstream decision performance. A-OVE directly optimizes out-of-sample performance by parameterizing decisions as a function of VARMA process sufficient statistics. The framework is evaluated in portfolio optimization with trading costs, demonstrating superior regret minimization compared to established benchmarks including Predict-then-Optimize and Finite-sample Optimal approaches.

## Method Summary
The A-OVE framework operates by first fitting a VARMA model to historical data to capture temporal dependencies in the uncertainty. Instead of using standard point estimates or prediction intervals, the method derives sufficient statistics from the fitted VARMA process that fully characterize the uncertainty distribution. These statistics are then used to parameterize the optimization problem directly, rather than relying on point predictions or distributional estimates that ignore temporal structure. The optimization objective is reformulated to minimize expected regret under the autocorrelated uncertainty model, leading to decisions that are robust to the temporal dependencies in the data.

## Key Results
- A-OVE achieves significantly lower regret than PTO, ETO, and FPtP benchmarks on both synthetic and real-world portfolio data
- The model demonstrates robust performance even under small VARMA specification errors
- Higher prediction accuracy does not guarantee better decision quality, validating the importance of aligning learning objectives with optimization tasks

## Why This Works (Mechanism)
The framework succeeds by recognizing that traditional prediction-focused machine learning approaches fail to account for the optimization objective when making decisions under uncertainty. By incorporating the VARMA structure directly into the optimization process through sufficient statistics, A-OVE ensures that the estimated parameters are meaningful for the downstream decision task rather than just predictive accuracy. This alignment between the learning and optimization phases is critical for achieving good out-of-sample performance.

## Foundational Learning
- VARMA processes: Why needed - To model autocorrelated uncertainty in time series data; Quick check - Verify residuals show no remaining autocorrelation
- Sufficient statistics: Why needed - To capture all information about uncertainty relevant to optimization; Quick check - Confirm statistics fully parameterize the VARMA distribution
- Regret minimization: Why needed - To optimize actual decision performance rather than prediction accuracy; Quick check - Ensure regret bounds are properly derived

## Architecture Onboarding

Component Map:
Data -> VARMA Estimation -> Sufficient Statistics Extraction -> Optimization Parameterization -> Decision Output

Critical Path:
VARMA Estimation -> Sufficient Statistics Extraction -> Optimization Parameterization

Design Tradeoffs:
- Complexity vs. accuracy: More complex VARMA models capture richer temporal patterns but increase computational burden
- Robustness vs. specificity: Tighter model specifications may improve performance under correct specification but reduce robustness to misspecification
- Computational efficiency vs. decision quality: Direct optimization of out-of-sample performance may be more computationally intensive than standard prediction approaches

Failure Signatures:
- Poor out-of-sample performance despite good in-sample fit
- Degradation when VARMA assumptions are violated
- Computational bottlenecks in optimization step

First Experiments:
1. Verify VARMA residuals show no remaining autocorrelation
2. Compare sufficient statistics against full distribution for optimization quality
3. Test sensitivity to VARMA model order selection

## Open Questions the Paper Calls Out
None

## Limitations
- Model specification sensitivity: Performance may degrade with significant deviation from VARMA assumptions
- Limited generalizability beyond portfolio optimization context
- Computational complexity not thoroughly evaluated against simpler approaches

## Confidence
- Out-of-sample performance superiority: High
- Robustness to small model mis-specification: Medium
- Prediction accuracy not guaranteeing decision quality: High
- Performance under VARMA specification: High

## Next Checks
1. Cross-domain validation: Test A-OVE on at least two distinct optimization domains (e.g., inventory management and energy dispatch) to assess generalizability beyond portfolio settings
2. Misspecification stress testing: Systematically vary the degree and type of model mis-specification (different autocorrelation structures, non-stationary processes) to quantify the robustness boundaries
3. Computational benchmarking: Compare wall-clock time and resource requirements against standard benchmarks across varying problem sizes to establish practical deployment feasibility