---
ver: rpa2
title: Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning
  Guidance
arxiv_id: '2508.21016'
source_url: https://arxiv.org/abs/2508.21016
tags:
- arxiv
- image
- diffusion
- guidance
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reinforcement Learning Guidance (RLG), an
  inference-time method for controlling alignment strength in diffusion models that
  have been fine-tuned with reinforcement learning. RLG builds on Classifier-Free
  Guidance by combining outputs from the base and RL-tuned models via geometric averaging,
  enabling dynamic adjustment of alignment without additional training.
---

# Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance

## Quick Facts
- arXiv ID: 2508.21016
- Source URL: https://arxiv.org/abs/2508.21016
- Reference count: 40
- Key outcome: Introduces RLG for dynamic alignment control in RL-tuned diffusion models without additional training

## Executive Summary
This paper introduces Reinforcement Learning Guidance (RLG), an inference-time method for controlling alignment strength in diffusion models that have been fine-tuned with reinforcement learning. RLG builds on Classifier-Free Guidance by combining outputs from the base and RL-tuned models via geometric averaging, enabling dynamic adjustment of alignment without additional training. Theoretically, the RLG scale is equivalent to adjusting the KL-regularization coefficient in the underlying RL objective, allowing interpolation and extrapolation of alignment strength. Empirically, RLG improves performance across multiple model architectures (diffusion, flow matching), RL algorithms (GRPO, DPO, SPO), and tasks (human preference, compositional accuracy, text rendering, compressibility, inpainting, personalization).

## Method Summary
Reinforcement Learning Guidance (RLG) is an inference-time technique that enables fine-grained control over alignment strength in diffusion models fine-tuned with reinforcement learning. The method operates by geometrically averaging predictions from a base model and an RL-tuned model during inference, controlled by a scalar parameter α (RLG scale). This approach builds on Classifier-Free Guidance, extending it to the RL-tuned setting where one branch represents the base model and the other represents the RL-tuned model. The geometric averaging formulation allows interpolation and extrapolation of alignment strength without requiring additional training. Theoretically, varying α is shown to be equivalent to adjusting the KL-regularization coefficient λ in the RL objective, providing a principled way to control the trade-off between alignment and base model characteristics.

## Key Results
- RLG achieved win rates up to 74.95% on PickScore human preference alignment task compared to standard RL fine-tuning
- On text rendering tasks, RLG boosted OCR accuracy from 88.6% to 93.0%
- Demonstrated effectiveness across multiple model architectures including diffusion and flow matching models

## Why This Works (Mechanism)
RLG works by leveraging the geometric averaging property of diffusion model predictions, extending the principle of Classifier-Free Guidance to the RL-tuned setting. During inference, predictions from both the base model and the RL-tuned model are combined using geometric averaging, with the mixing weight controlled by the RLG scale parameter α. This allows dynamic adjustment of alignment strength without requiring retraining. The theoretical foundation shows that this geometric averaging is equivalent to varying the KL-regularization coefficient λ in the RL objective, providing a principled mechanism for controlling the trade-off between alignment and base model characteristics. The method is effective because it preserves the stability of the underlying diffusion process while allowing fine-grained control over the alignment strength through a simple scalar parameter.

## Foundational Learning

**Diffusion Models**
- Why needed: Core architecture that RLG operates on
- Quick check: Understand denoising process and score function formulation

**Reinforcement Learning Fine-tuning**
- Why needed: The specific setting where RLG is applied
- Quick check: Know how RL objectives differ from supervised fine-tuning

**Classifier-Free Guidance**
- Why needed: The foundational technique that RLG builds upon
- Quick check: Understand geometric averaging of model predictions

**KL-regularization in RL**
- Why needed: The theoretical connection to RLG scale parameter
- Quick check: Know how λ affects the balance between alignment and base model

**Geometric Averaging in Diffusion**
- Why needed: The mathematical operation enabling RLG
- Quick check: Understand why geometric averaging is used instead of arithmetic averaging

## Architecture Onboarding

**Component Map**
Base Model -> RL-tuned Model -> Geometric Averaging -> RLG Output

**Critical Path**
1. Load both base and RL-tuned model checkpoints
2. During inference, generate predictions from both models
3. Apply geometric averaging with RLG scale α
4. Produce final output with controlled alignment strength

**Design Tradeoffs**
- Training-free control vs. requiring RL-tuned model
- Interpolation capability vs. potential for mode collapse
- Generalization across tasks vs. task-specific optimization

**Failure Signatures**
- Mode collapse when α is too high
- Loss of base model characteristics when α is too low
- Instability when geometric averaging is applied to incompatible model pairs

**3 First Experiments**
1. Test RLG on a simple text-to-image task with varying α values
2. Compare RLG outputs with standard RL-tuned outputs on human preference evaluation
3. Validate the theoretical relationship between α and KL-regularization coefficient through ablation studies

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical equivalence between RLG scale and KL-regularization coefficient may not hold perfectly in practice due to RL training dynamics
- Results rely heavily on human preference evaluations which can be subjective and task-dependent
- Claim of "training-free" control requires qualification as RL-tuned model itself involves significant training costs

## Confidence

**High**: The mathematical formulation of RLG and its relationship to Classifier-Free Guidance is sound and well-explained

**Medium**: Empirical performance improvements across multiple tasks and architectures, though some results may be sensitive to evaluation protocols

**Low**: The theoretical equivalence between RLG scale and KL-regularization coefficient in practice, and the generalizability of results to more complex real-world scenarios

## Next Checks

1. Conduct ablation studies on the KL-regularization coefficient during RL fine-tuning to verify the theoretical relationship with RLG scale experimentally

2. Test RLG on more diverse and challenging real-world datasets to assess robustness beyond curated benchmark tasks

3. Compare RLG against alternative inference-time methods like DriftLite and Diffusion Blend in controlled head-to-head evaluations on identical tasks and datasets