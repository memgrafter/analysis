---
ver: rpa2
title: 'Balancing Expressivity and Robustness: Constrained Rational Activations for
  Reinforcement Learning'
arxiv_id: '2507.14736'
source_url: https://arxiv.org/abs/2507.14736
tags:
- learning
- activation
- rational
- functions
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Trainable rational activation functions enhance neural network
  expressivity in reinforcement learning but can introduce instability, particularly
  in continuous control tasks under high-update-to-data regimes. This instability
  manifests as activation explosion and overestimation errors due to coefficient imbalance.
---

# Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.14736
- Source URL: https://arxiv.org/abs/2507.14736
- Reference count: 21
- Trainable rational activation functions enhance expressivity but introduce instability in RL

## Executive Summary
Trainable rational activation functions can significantly enhance neural network expressivity in reinforcement learning by learning task-specific nonlinearities. However, these functions introduce instability, particularly in continuous control tasks with high update-to-data ratios. The instability manifests as activation explosion and overestimation errors caused by coefficient imbalance. The authors propose constrained rational activations with structural modifications that limit excessive output scaling while preserving adaptability. These constrained variants show substantial improvements in training stability and performance compared to both original rational activations and ReLU, though they reduce plasticity in continual learning scenarios.

## Method Summary
The authors propose constrained rational activation functions to address instability in reinforcement learning. They introduce structural modifications that limit excessive output scaling while maintaining adaptability. The constraints are designed to prevent coefficient imbalance that leads to activation explosion and overestimation errors. The approach involves modifying the rational function architecture to include bounded parameters or regularization terms that constrain the learned coefficients. These modifications are evaluated against standard rational activations and ReLU baselines across continuous control benchmarks.

## Key Results
- Constrained rational activations significantly improve training stability in continuous control tasks
- Performance exceeds both original rational activations and ReLU baselines on MetaWorld and DeepMind Control Suite
- Constraints reduce plasticity in continual learning scenarios, revealing a fundamental stability-adaptability trade-off

## Why This Works (Mechanism)
Rational activation functions introduce additional degrees of freedom through trainable coefficients in polynomial ratios, enabling the network to learn more expressive nonlinearities. However, this expressivity comes at the cost of stability because the coefficients can become imbalanced during training, particularly under high-update-to-data regimes. This imbalance causes the rational function to produce extreme outputs (activation explosion) or biased value estimates (overestimation errors). The constrained variants work by imposing structural limitations on these coefficients - either through bounded parameter ranges, regularization, or architectural constraints - that prevent the coefficients from reaching pathological values while still allowing sufficient flexibility to learn useful task-specific transformations.

## Foundational Learning

**Rational Functions**: Ratios of polynomials that can approximate complex nonlinear relationships. Needed because they provide a flexible framework for learning task-specific activation functions. Quick check: Verify that the rational function can represent common activation functions as special cases.

**Activation Explosion**: Phenomenon where activation function outputs grow unboundedly during training. Critical in RL because it destabilizes value function estimation and policy learning. Quick check: Monitor activation statistics during training to detect explosive growth.

**Coefficient Imbalance**: Occurs when numerator and denominator polynomial coefficients become disproportionately large in opposite directions. This is the primary failure mode in rational activations. Quick check: Track coefficient magnitudes and their ratios throughout training.

**Update-to-Data Ratio**: The frequency of parameter updates relative to the amount of new data collected. High ratios in RL can amplify instability in expressive activation functions. Quick check: Compare performance across different batch sizes and update frequencies.

**Continual Learning Plasticity**: The ability to adapt to new tasks or changing environments. Constraining activations can reduce this adaptability. Quick check: Evaluate performance on sequential task learning to measure plasticity loss.

## Architecture Onboarding

**Component Map**: Input -> Pre-processing -> Rational Activation (constrained) -> Hidden Layers -> Output Layer

**Critical Path**: The rational activation function is the critical component because its stability directly determines whether the network can learn effectively. All other components (layer dimensions, optimization hyperparameters) are secondary to getting the activation function right.

**Design Tradeoffs**: The primary tradeoff is between expressivity and stability. More constrained activations are more stable but less expressive; less constrained activations are more expressive but risk instability. The authors must balance these competing objectives for each RL task.

**Failure Signatures**: Activation explosion manifests as NaNs or infinities in network outputs, typically occurring after a certain number of training steps. Overestimation errors appear as value estimates that drift upward unrealistically. Both can be detected through monitoring activation statistics and value function bounds.

**First Experiments**: 1) Train with unconstrained rational activations to reproduce instability. 2) Apply simple coefficient clipping to verify that constraints help. 3) Compare constrained variants against ReLU on a simple continuous control task to establish baseline improvements.

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the broader applicability and optimization of constrained rational activations. The fundamental trade-off between stability and adaptability remains underexplored, particularly for tasks requiring dynamic adaptation. The optimal constraint parameters and their task-dependent nature are not fully characterized. Additionally, the analysis of coefficient initialization and weight decay effects lacks systematic exploration across different task types and RL algorithms.

## Limitations

- The fundamental trade-off between stability and adaptability may make constrained activations suboptimal for tasks requiring high plasticity
- Results are limited to reinforcement learning applications, with uncertainty about performance in supervised learning or computer vision domains
- The analysis of optimal hyperparameter ranges (constraint parameters, initialization, weight decay) is incomplete and task-dependent

## Confidence

High: Unconstrained rational activations cause instability in continuous control tasks due to coefficient imbalance and activation explosion
Medium: Constrained variants effectively improve stability and performance on MetaWorld and DeepMind Control Suite benchmarks
Low: Conclusions about plasticity reduction in continual learning scenarios lack comprehensive experimental validation

## Next Checks

1) Systematically vary constraint parameters to map the stability-adaptability trade-off space and identify optimal regimes for different task types

2) Extend experiments to supervised learning tasks and computer vision benchmarks to assess domain transferability and generalization beyond RL

3) Implement ablation studies isolating the effects of different constraint mechanisms (output scaling limits vs. coefficient regularization) to identify which structural modifications are most critical for stability