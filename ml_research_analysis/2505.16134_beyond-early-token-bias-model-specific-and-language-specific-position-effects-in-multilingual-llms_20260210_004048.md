---
ver: rpa2
title: 'Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects
  in Multilingual LLMs'
arxiv_id: '2505.16134'
source_url: https://arxiv.org/abs/2505.16134
tags:
- position
- bias
- language
- middle
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates position bias in multilingual LLMs across
  five typologically diverse languages (English, Russian, German, Hindi, Vietnamese)
  and five model architectures. Through 450,000 evaluated question-answer pairs, the
  study reveals that position bias is primarily model-driven rather than language-driven,
  with notable exceptions: Qwen2.5-7B, DeepSeek-7B, and Mistral-7B exhibit strong
  late-position preference contrary to the assumed early-token bias, while Llama3.1-8B
  favors early positions.'
---

# Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs

## Quick Facts
- arXiv ID: 2505.16134
- Source URL: https://arxiv.org/abs/2505.16134
- Reference count: 40
- Primary result: Position bias in multilingual LLMs is primarily model-driven rather than language-driven, with notable exceptions challenging assumed early-token bias

## Executive Summary
This study investigates position bias in multilingual LLMs across five typologically diverse languages (English, Russian, German, Hindi, Vietnamese) and five model architectures. Through 450,000 evaluated question-answer pairs, the research reveals that position bias is primarily model-driven rather than language-driven, with notable exceptions: Qwen2.5-7B, DeepSeek-7B, and Mistral-7B exhibit strong late-position preference contrary to the assumed early-token bias, while Llama3.1-8B favors early positions. Surprisingly, explicitly instructing models about correct context placement consistently degrades accuracy across all languages, challenging standard prompt-engineering practices. Additionally, while model accuracy is lowest when relevant context appears in the middle, this performance drop is not accompanied by increased output entropy, suggesting models remain confidently wrong under positional disadvantage.

## Method Summary
The study evaluates position bias through a controlled experimental setup using 2,000 QA pairs per language from SQuAD2.0 (EN), MTS-SQuAD (RU), and MLQA (DE/HI/VI). Each question has five contexts: one relevant answer and four random distractors. The research tests three positioning strategies (TOP, MIDDLE, BOTTOM) and three scoring strategies (Aligned, All Zero, No Scores) across five 7B-8B models (Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, DeepSeek-7B-Chat, Gemma-7B-it, Mistral-7B-Instruct-v0.3). Accuracy is measured via LLM-as-a-Judge using Mistral Large2, with average predictive entropy calculated to assess model confidence.

## Key Results
- Position bias is primarily model-driven, with Qwen2.5-7B, DeepSeek-7B, and Mistral-7B consistently favoring late positions while Llama3.1-8B favors early positions
- Explicitly instructing models about relevant context placement degrades accuracy across all languages and models
- Accuracy drops most when relevant information appears in middle positions, but this is not reflected in corresponding entropy increases
- Model accuracy is lowest for mid-context positioning universally across all tested models and languages

## Why This Works (Mechanism)

### Mechanism 1
Position bias in LLMs is primarily determined by model architecture and training, not by language-specific properties. Different model families develop distinct attention patterns over training—Qwen2.5-7B, DeepSeek-7B, and Mistral 7B prioritize late-context tokens, while Llama3.1-8B favors early positions. The authors speculate this stems from "variations in training data and model architecture."

### Mechanism 2
Explicit relevance scoring in prompts degrades performance when distractors are semantically unrelated to the query. When models receive explicit labels like "score=1" for relevant context amid random distractors, the mismatch between the instruction's implication (that scoring carries signal) and the actual task structure causes confusion.

### Mechanism 3
Mid-context accuracy drops without corresponding entropy increases—models are confidently wrong. When relevant context appears mid-sequence, attention patterns fail to prioritize it (U-shaped attention bias), but the model's internal uncertainty estimate (entropy) doesn't spike because token representations homogenize, giving the model false confidence.

## Foundational Learning

- **U-shaped attention patterns in Transformers**: Why needed here: The paper builds on prior work showing transformers prioritize extremal positions; understanding this baseline is essential to interpret the model-specific inversions found.
- **Predictive entropy as uncertainty proxy**: Why needed here: The paper uses normalized token-wise entropy to measure model confidence; understanding this metric is necessary to interpret the entropy-accuracy decoupling finding.
- **Lost in the Middle phenomenon**: Why needed here: This prior finding (models underperform when relevant info is mid-context) is a core baseline the paper extends multilingually.

## Architecture Onboarding

- **Component map**: Question → Context positioning (TOP/MID/BOT) → Scoring strategy (Aligned/All Zero/No Scores) → Model generation → LLM-as-judge evaluation → Entropy calculation
- **Critical path**: 1. Identify which model you're deploying → determine its position bias (early vs. late preference) 2. Match context ordering to model bias 3. Avoid explicit relevance scoring in prompts when using random/irrelevant distractors
- **Design tradeoffs**: Reordering context for model-specific bias vs. maintaining retrieval order semantics; Adding relevance scores (more information) vs. observed performance degradation; Using entropy for confidence vs. its unreliability under position bias
- **Failure signatures**: Accuracy drops when relevant context is mid-list despite model appearing confident; Explicit relevance instructions causing worse performance than no guidance; Llama3.1-8B performing well with early context but poorly with late; Qwen showing the inverse
- **First 3 experiments**: 1. Position sweep: For your target model, place relevant context at positions 1, N/