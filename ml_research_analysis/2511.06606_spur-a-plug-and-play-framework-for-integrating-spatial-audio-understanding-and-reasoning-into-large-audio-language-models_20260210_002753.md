---
ver: rpa2
title: 'SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding
  and Reasoning into Large Audio-Language Models'
arxiv_id: '2511.06606'
source_url: https://arxiv.org/abs/2511.06606
tags:
- spatial
- audio
- reasoning
- sound
- lalms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPUR introduces a lightweight, plug-and-play approach to equip
  large audio-language models with spatial perception using First-Order Ambisonics
  (FOA) inputs. It combines a spatial encoder that extracts rotation-aware, listener-centric
  features from FOA signals with a multimodal adapter that integrates these into existing
  LALMs without altering their core architecture.
---

# SPUR: A Plug-and-Play Framework for Integrating Spatial Audio Understanding and Reasoning into Large Audio-Language Models

## Quick Facts
- arXiv ID: 2511.06606
- Source URL: https://arxiv.org/abs/2511.06606
- Authors: S Sakshi; Vaibhavi Lokegaonkar; Neil Zhang; Ramani Duraiswami; Sreyan Ghosh; Dinesh Manocha; Lie Lu
- Reference count: 20
- Primary result: SPUR enables LALMs to perform spatial reasoning on FOA inputs without architectural changes, outperforming binaural baselines.

## Executive Summary
SPUR introduces a lightweight, plug-and-play approach to equip large audio-language models with spatial perception using First-Order Ambisonics (FOA) inputs. It combines a spatial encoder that extracts rotation-aware, listener-centric features from FOA signals with a multimodal adapter that integrates these into existing LALMs without altering their core architecture. To train and evaluate this spatial reasoning, SPUR-Set was developed—a benchmark of multi-event FOA recordings and controlled simulations annotated with spatially grounded captions and question-answer pairs across six reasoning skills such as direction, proximity, and overlap. Experiments show that fine-tuning LALMs with SPUR improves spatial question answering and multi-speaker attribution while preserving general audio understanding. Ablations confirm the effectiveness of the spatial encoder design, adapter placement, and supervision mixtures. Compared to binaural baselines, SPUR demonstrates superior spatial consistency and reasoning depth, validating its ability to transform monaural LALMs into spatially aware models.

## Method Summary
SPUR processes 4-channel FOA audio through an encoder that computes banded covariance matrices across W, X, Y, Z channels, applies learnable temporal smoothing, and vectorizes into real-valued Spectro-Spatial Covariance Vectors (SSCV). These are processed through 3D convolutional blocks and a transformer encoder to produce geometry tokens, which are projected via an MLP adapter and injected into a frozen audio encoder. The system is trained on SPUR-Set, a hybrid dataset of real FOA recordings and simulated spatial mixtures with spatially grounded annotations. Only the adapter and LLM LoRA modules are fine-tuned, preserving the pre-trained audio and language backbones.

## Key Results
- SPUR outperforms binaural baselines on spatial question answering, achieving higher Spatial Consistency and Reasoning Depth scores
- The framework maintains general audio understanding while adding spatial perception capabilities
- Ablation studies confirm the effectiveness of the spatial encoder design, adapter placement, and supervision mixtures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transforming raw multi-channel audio into Spectro-Spatial Covariance Vectors (SSCV) may preserve spatial cues (direction/distance) better than raw waveforms or mono-mixdowns.
- **Mechanism:** The FOA encoder computes banded covariance matrices $C_x(n, b)$ across the (W, X, Y, Z) channels. By separating diagonal power terms and off-diagonal phase correlations, and applying a learnable one-pole smoothing filter, the system creates rotation-aware, listener-centric features that explicitly encode inter-channel dependencies.
- **Core assumption:** Spatial geometry is encoded primarily in the phase and energy correlations between Ambisonics channels rather than just spectral content.
- **Evidence anchors:**
  - [section 3, Page 3]: "This representation encodes bandwise energy and inter-channel phase correlations—crucial for spatial reasoning."
  - [section 3, Page 4]: "resulting in a real-valued Spectro-Spatial Covariance Vector (SSCV)..."
  - [corpus]: Weak direct evidence; SonicBench confirms LALMs struggle with physical attributes, validating the need for explicit feature engineering.
- **Break condition:** If input audio lacks channel synchronization or calibration, covariance calculations yield noise rather than spatial data.

### Mechanism 2
- **Claim:** Injecting spatial embeddings directly into the audio encoder (rather than the LLM) allows the model to leverage the pre-trained audio representation space for spatial grounding.
- **Mechanism:** The SPUR-Encoder outputs are projected via an MLP to match the dimensionality of the target Audio Encoder. These "adapted spatial embeddings" are added/concated as input tokens to the frozen Audio Encoder, effectively conditioning the acoustic understanding layer before it reaches the LLM.
- **Core assumption:** The pre-trained audio encoder has sufficient plasticity to interpret these projected spatial tokens as valid audio features despite being frozen.
- **Evidence anchors:**
  - [section 3, Page 5]: "These adapted spatial embeddings are then injected into the existing audio encoder... producing spatially enriched representations."
  - [abstract]: "integrated into target LALMs via a multimodal adapter."
  - [corpus]: Not applicable (novel architecture).
- **Break condition:** If the projection MLP is insufficiently trained, the audio encoder receives "out-of-distribution" tokens, leading to ignored inputs or artifacts.

### Mechanism 3
- **Claim:** Disentangling spatial reasoning into six expert skills (e.g., Mental Rotation, Proximity) via supervised Q&A enforces specific geometric inductive biases.
- **Mechanism:** The model is fine-tuned on SPUR-Set, which includes specific tasks like "Scene Reconfiguration" (rotating the listener frame). This forces the attention layers to learn transformation matrices relative to the listener's perspective.
- **Core assumption:** Spatial reasoning is a composite of discrete skills that can be learned via instruction tuning on hybrid real/simulated data.
- **Evidence anchors:**
  - [section 3.1, Page 5]: "emphasizing relative direction, elevation, distance, and overlap for supervised spatial reasoning."
  - [section 3.1, Page 6]: "Scene Reconfiguration & Mental Rotation: Mentally rotates the listener’s frame... and updates all source bearings."
  - [corpus]: AudioMotionBench highlights that LALMs fail at motion perception, supporting the need for targeted skill training.
- **Break condition:** If the simulated data distribution diverges significantly from real-room acoustics (reverb/diffusion), the model fails to generalize to real-world FOA inputs.

## Foundational Learning

- **Concept: First-Order Ambisonics (FOA)**
  - **Why needed here:** This is the input format. Unlike stereo, FOA uses 4 channels (W, X, Y, Z) to capture a full 3D sound field.
  - **Quick check question:** How does the 'W' channel differ from 'X, Y, Z' in terms of directional information? (A: W is omnidirectional pressure; X/Y/Z are pressure gradients capturing front-back, left-right, up-down).

- **Concept: Covariance/Correlation Matrices**
  - **Why needed here:** The core encoder relies on "Banded Covariance." You must understand that off-diagonal elements represent the correlation (phase relationship) between channels, which physically corresponds to source direction.
  - **Quick check question:** If two channels are perfectly uncorrelated (covariance $\approx 0$), what does that imply about the spatial coherence of the source?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - **Why needed here:** The method freezes the LALM backbone and only trains the Adapter/LoRA.
  - **Quick check question:** Why is freezing the Audio Encoder theoretically safer than full fine-tuning when introducing a new modality like FOA?

## Architecture Onboarding

- **Component map:** Multi-Channel FOA -> STFT -> Banded Covariance Calc -> Smoothing (alpha) -> Vectorization (SSCV) -> 3D Conv Blocks -> Transformer -> MLP Projector -> Frozen Audio Encoder -> LLM (LoRA).

- **Critical path:** The **MLP Projector** alignment. The SPUR Transformer outputs geometry tokens ($h_N$), but they must be projected precisely into the dimension and distribution expected by the target Audio Encoder (e.g., Audio Flamingo 3). A mismatch here invalidates the spatial injection.

- **Design tradeoffs:**
  - **Sim vs. Real Data:** The paper uses a hybrid. Relying only on simulation risks "sim-to-real" gap; relying only on real data limits volume and scenario diversity.
  - **Adapter Placement:** Injecting at the audio encoder (SPUR) vs. prefixing at the LLM. The paper suggests early injection allows "spatially enriched representations" rather than just spatial text descriptions.

- **Failure signatures:**
  - **Semantic Hallucination:** The LLM answers "The sound is coming from the left" based on semantic priors (e.g., "keys jingling" usually near a person) rather than the audio features. Indicates the adapter isn't training or gradients aren't flowing.
  - **Catastrophic Forgetting:** The model answers spatial questions correctly but fails general audio captioning. Indicates LoRA rank or learning rate is too high, overwriting general audio knowledge.

- **First 3 experiments:**
  1. **Unit Test the SSCV:** Visualize the SSCV features for a sound source rotating 360 degrees. Verify that the vector changes smoothly and cyclically to confirm the "Rotation-Aware" property.
  2. **Ablation on Adapter:** Compare performance when injecting the MLP output into the Audio Encoder (Layer 0) vs. concatenating it directly to the LLM prompt embeddings.
  3. **Skill Probing:** Evaluate the 6 skills individually. If "Distance" fails but "Direction" succeeds, the Covariance features are likely capturing phase (direction) but missing energy/distance cues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does spatial reasoning accuracy scale when upgrading the SPUR-Encoder from First-Order Ambisonics (FOA) to Higher-Order Ambisonics (HOA), and does this require fundamental changes to the covariance extraction module?
- Basis in paper: [explicit] The authors state that SPUR "currently operates on FOA, which constrains spatial resolution," and suggest that extending the encoder to higher-order formats could "enrich geometric fidelity."
- Why unresolved: The current pipeline specifically flattens $4 \times 4$ covariance matrices (W, X, Y, Z); HOA inputs would significantly alter the dimensionality and computational cost of the banded covariance extraction and 3D convolution stages.
- What evidence would resolve it: A comparative study evaluating SPUR on HOA datasets (e.g., EigenSpace) measuring localization error rates and QA scores against the FOA baseline.

### Open Question 2
- Question: Can the static embedding assumption of the current SPUR architecture be modified to support real-time adaptation to listener movement and dynamic viewpoint shifts?
- Basis in paper: [explicit] The authors identify a limitation where "the adapter still relies on static embeddings and does not model listener movement or dynamic viewpoint shifts."
- Why unresolved: The current architecture applies one-pole temporal smoothing to stabilize frame-wise variance but lacks a mechanism to integrate continuous rotation data (e.g., from a head-tracker) to update source bearings dynamically.
- What evidence would resolve it: Experiments using simulated dynamic listener trajectories, measuring the model's ability to maintain "Spatial Consistency" scores under rotation compared to a static baseline.

### Open Question 3
- Question: To what extent does the reliance on simulated room impulse responses (RIRs) in SPUR-Set limit the model's ability to generalize to noisy, "in-the-wild" spatial recordings?
- Basis in paper: [inferred] While the dataset combines real and simulated data, the authors note the focus on "controlled room acoustics" and list "scaling SPUR-Set with in-the-wild spatial audio" as a prerequisite for improved generalization.
- Why unresolved: Synthetic RIRs may lack the complex diffraction, occlusion, and distinct noise profiles of real-world environments, potentially creating a "sim-to-real" gap in localization accuracy.
- What evidence would resolve it: Evaluation of SPUR on uncurated, real-world multi-channel recordings (e.g., wearable ego-audio) without fine-tuning to quantify the performance drop in "Environmental Awareness & Navigation" tasks.

## Limitations

- Critical architectural hyperparameters (transformer depth, conv block count) are underspecified, making faithful reproduction difficult
- The method currently operates on FOA only, limiting spatial resolution compared to higher-order Ambisonics
- Generalization to real-world noisy environments beyond controlled datasets remains untested

## Confidence

**High Confidence:**
- The technical feasibility of transforming FOA signals into SSCV vectors and injecting them into frozen LALMs via adapter modules
- The benchmark construction methodology (SPUR-Set) and its multi-skill spatial reasoning framework

**Medium Confidence:**
- The claim that SPUR improves spatial reasoning over binaural baselines, based on reported LLM-as-judge metrics
- The assertion that spatial cues are primarily encoded in inter-channel phase and energy correlations in FOA

**Low Confidence:**
- The assertion that the adapter integration preserves general audio understanding while adding spatial perception, due to limited ablation studies on catastrophic forgetting
- The claim of "plug-and-play" readiness, given the underspecification of critical hyperparameters and integration points

## Next Checks

1. **Reproduce the SSCV Representation:** Visualize and validate that the SSCV vectorization preserves smooth, cyclic changes for rotating sound sources to confirm rotation-aware properties

2. **Benchmark Against Direct FOA Input:** Train a baseline model that feeds raw or minimally processed FOA into the LALM (without SPUR-Encoder) to quantify the value added by the SSCV transformation

3. **Human Evaluation of Spatial Consistency:** Conduct a human study to validate LLM-as-a-judge scores for spatial reasoning tasks, ensuring alignment with perceptual reality