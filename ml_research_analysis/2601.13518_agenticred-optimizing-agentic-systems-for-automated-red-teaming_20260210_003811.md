---
ver: rpa2
title: 'AgenticRed: Optimizing Agentic Systems for Automated Red-teaming'
arxiv_id: '2601.13518'
source_url: https://arxiv.org/abs/2601.13518
tags:
- prompt
- target
- system
- loss
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGENTICRED, an automated framework that uses
  LLMs to iteratively design and refine red-teaming systems without human intervention.
  Rather than optimizing attacker policies within predefined structures, AGENTICRED
  treats red-teaming as a system design problem, leveraging evolutionary algorithms
  to evolve agentic workflows.
---

# AgenticRed: Optimizing Agentic Systems for Automated Red-teaming

## Quick Facts
- arXiv ID: 2601.13518
- Source URL: https://arxiv.org/abs/2601.13518
- Authors: Jiayi Yuan; Jonathan Nöther; Natasha Jaques; Goran Radanović
- Reference count: 40
- Primary result: AGENTICRED achieves 96-98% attack success rate on open-weight models and 100% on GPT-3.5-Turbo and GPT-4o using automated evolutionary system design

## Executive Summary
AGENTICRED introduces a novel automated framework for red-teaming that treats system design as an evolutionary optimization problem. Instead of manually crafting attack workflows, it uses a meta-agent to iteratively generate, evaluate, and refine agentic red-teaming systems. By leveraging evolutionary pressure and black-box access to target models, the approach discovers highly effective attack strategies that transfer well to proprietary models. This work demonstrates that automated system design can outperform manual workflows in AI safety evaluation.

## Method Summary
AGENTICRED uses an evolutionary search where a meta-agent LLM generates Python code for red-teaming agentic systems based on an archive of prior systems. Each generation produces multiple candidates evaluated on a small dataset subset, with the highest-performing system selected and added to the archive after comprehensive evaluation. The framework includes helper functions for querying target models and judges, creating dense optimization signals. Starting from baseline systems (Self-Refine, JudgeScore-Guided Adversarial Reasoning), the meta-agent iteratively designs new systems that achieve up to 96-98% ASR on open-weight models.

## Key Results
- Achieves 96-98% attack success rate on open-weight models (Llama-2-7B, Llama-3-8B, Mixtral-8x7B)
- Achieves 100% attack success rate on proprietary models (GPT-3.5-Turbo, GPT-4o)
- Demonstrates strong transferability from open-weight to proprietary models
- Evolutionary pressure critical: ablation with 1 offspring/generation reduces ASR by 6%
- Generated systems maintain high diversity while achieving strong performance

## Why This Works (Mechanism)

### Mechanism 1
Treating red-teaming as an agentic system design problem with evolutionary selection may produce more effective systems than manually designed workflows. A meta-agent generates multiple candidate red-teaming systems per generation, evaluates them on a small dataset, and retains only the highest-performing system for comprehensive evaluation and archive inclusion, applying survival-of-the-fittest selection pressure. The fitness score (ASR) on a small subset correlates with performance on broader benchmarks and can guide meaningful architectural improvements. Limited direct corpus evidence for evolutionary red-team design specifically; related work on agentic workflow optimization exists but in different domains. If the fitness landscape is highly irregular or deceptive, evolutionary pressure may cause convergence to local optima with poor transferability.

### Mechanism 2
Domain-specific helper functions providing query access to target models and judge functions appear critical for generating verifiable feedback signals during system design. The framework exposes `get_response` and `get_jailbreak_result` utility functions that allow candidate systems to probe the target model and receive numerical feedback, creating a dense optimization signal. Black-box access to target responses and judge evaluations provides sufficient signal for meta-agent reasoning about system improvements. No direct corpus evidence on this specific mechanism; limited coverage in related work. If the judge function is systematically biased or reward-hackable, evolved systems may exploit artifacts rather than genuine vulnerabilities.

### Mechanism 3
An initial archive containing domain-relevant baseline systems may provide a "warm start" that improves evolutionary search efficiency. The archive is seeded with Self-Refine and JudgeScore-Guided Adversarial Reasoning—systems that already implement proposer-verifier patterns and interface correctly with target/judge functions—giving the meta-agent working examples to build upon. High-quality initial examples encode transferable design patterns that accelerate discovery compared to starting from generic reasoning templates. No direct corpus evidence on archive initialization for red-teaming. If the archive encodes narrow or biased design patterns, it may constrain exploration rather than accelerate it.

## Foundational Learning

- **Evolutionary/selection-based meta-learning**: The core algorithm iterates generations with selection pressure; understanding fitness landscapes, exploration-exploitation tradeoffs, and mode collapse risks is essential for interpreting results and debugging stagnation. Can you explain why removing evolutionary pressure (1 offspring/generation vs. M) reduces final ASR, as shown in the ablation?

- **Black-box red-teaming and judge functions**: The system assumes only API access to target models and a binary/continuous judge; understanding evaluation biases (e.g., HarmBench vs. StrongREJECT) is critical for interpreting ASR claims. Why does the paper use both HarmBench and StrongREJECT, and what does their agreement/disagreement indicate about reward hacking?

- **Agentic workflow representation as code**: Systems are generated as Python forward() functions; understanding how control flow, helper calls, and prompt engineering compose is necessary for debugging generated systems and extending the framework. How do the helper functions `get_response` and `get_jailbreak_result` enable early-exit logic in generated systems?

## Architecture Onboarding

- **Component map**: Meta Agent (GPT-5) → generates candidate systems (Python forward() functions) → Initial Evaluation (small subset d) → Self-refinement on errors → Selection (fittest by ASR) → Comprehensive Evaluation → Archive Update → next generation. Helper utilities: Attacker/Feedbacker/Optimizer agent classes, get_response(), get_jailbreak_result()

- **Critical path**: The evolutionary loop (Algorithm 1, lines 4-21) where M candidates are generated, evaluated on d, the fittest selected, and the winner fully evaluated and archived. Errors during evaluation trigger self-refinement (lines 12-14)

- **Design tradeoffs**: Training-time query cost (122k queries) vs. test-time efficiency (339 queries/success). ASR optimization vs. diversity (mode collapse observed; diversity constraints help but reduce ASR). Meta-agent capability sensitivity (DeepSeek-R1 as meta-agent underperforms GPT-5 in ablation)

- **Failure signatures**: (1) Stagnation after early generations—suggests weak initial archive or insufficient diversity pressure. (2) High training ASR but poor held-out/proprietary transfer—suggests overfitting to training distribution. (3) Generated systems failing to use helper functions correctly—check self-refinement logs

- **First 3 experiments**:
  1. Reproduce the main result targeting Llama-2-7B with 10 generations, M=3 offspring, tracking ASR per generation. Verify ASR approaches 96% and compare training vs. held-out test ASR.
  2. Run the ablation with 1 offspring per generation (no evolutionary pressure) and confirm the 6% ASR gap reported in Section 5.7. Document convergence behavior.
  3. Evaluate the best system from Experiment 1 on a different benchmark (StrongREJECT) and at least one proprietary model (if API access available) to test transferability claims. Compare to baseline AdvReasoning ASR.

## Open Questions the Paper Calls Out

- Can open-weight models serve as effective meta agents for automated red-teaming system design, achieving comparable performance to proprietary models like GPT-5? The ablation study with DeepSeek-R1 as meta agent showed degraded performance, but it remains unclear whether this is fundamental or model-specific. Experiments with multiple open-weight frontier models as meta agents, measuring ASR achieved on standardized benchmarks, would resolve this.

- How can the quality-diversity trade-off in evolutionary red-teaming system design be systematically optimized to discover diverse vulnerability classes? The authors' reward shaping and sample rejection methods showed only marginal improvements in balancing diversity with attack success rate. Pareto frontier analysis comparing multi-objective optimization methods (e.g., NSGA-II) against the current approach on both ASR and diversity metrics would resolve this.

- Can co-evolutionary frameworks where defending models adapt alongside attacking systems yield more robust safety insights than static-target evaluation? Current work assumes static target models; real deployment involves continuous safety patches that create a dynamic adversarial landscape. Longitudinal experiments tracking ASR over iterative attack-defense cycles with adaptive safety fine-tuning would resolve this.

- What interventions can address mode collapse in meta-agent-generated system designs to enable broader exploration of the red-teaming design space? Semantic similarity analysis (Appendix B.5) shows high convergence across generations, limiting novelty in discovered strategies. Ablation studies with diversity-promoting interventions (e.g., novelty search, archetypal prompting) measuring semantic diversity of generated system architectures would resolve this.

## Limitations

- Reliance on proprietary meta-agent (gpt-5-2025-08-07) limits reproducibility; ablation with DeepSeek-R1 shows significantly worse performance
- Potential for mode collapse on irregular fitness landscapes, limiting discovery of novel attack vectors
- Limited corpus evidence on judge function reliability and potential for evolved systems to exploit artifacts rather than genuine vulnerabilities

## Confidence

- **High Confidence**: Transferability to proprietary models (GPT-4o, Claude-3.5-Sonnet) achieving 100% ASR, and the core mechanism of using evolutionary pressure to select fittest red-teaming systems
- **Medium Confidence**: The claim that treating red-teaming as system design outperforms manual workflows, based on the 96-98% ASR achievement, though comparisons to traditional methods are limited
- **Low Confidence**: The robustness of the approach against reward hacking, given the limited corpus evidence on judge function reliability

## Next Checks

1. Reproduce the evolutionary search with an accessible meta-agent (GPT-4o) using the provided instruction prompt, tracking ASR per generation and comparing to the reported 96-98% performance
2. Evaluate the best-evolved system from the reproduction on StrongREJECT benchmark and at least one proprietary model to verify transferability claims
3. Test the ablation with 1 offspring per generation (no evolutionary pressure) to confirm the 6% ASR gap and document convergence behavior