---
ver: rpa2
title: Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization
arxiv_id: '2512.11391'
source_url: https://arxiv.org/abs/2512.11391
tags:
- safety
- nspo
- zhang
- general
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of aligning large language\
  \ models (LLMs) for safety without degrading their general capabilities\u2014a problem\
  \ known as the \"safety alignment tax.\" The authors propose Null-Space constrained\
  \ Policy Optimization (NSPO), a novel reinforcement learning framework that projects\
  \ safety policy gradients into the null space of the model's general task representations.\
  \ This geometric projection ensures that safety updates do not interfere with the\
  \ learned general capabilities of the model."
---

# Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization

## Quick Facts
- arXiv ID: 2512.11391
- Source URL: https://arxiv.org/abs/2512.11391
- Reference count: 40
- One-line primary result: NSPO achieves state-of-the-art safety performance with minimal capability degradation, requiring only 40% of public safety data.

## Executive Summary
This paper addresses the safety alignment tax—the degradation of general capabilities when aligning LLMs for safety—through a novel reinforcement learning framework called Null-Space constrained Policy Optimization (NSPO). The method projects safety policy gradients into the null space of general task representations, ensuring safety updates don't interfere with pre-trained capabilities. Theoretical guarantees show that NSPO preserves general capabilities while maintaining effective descent for safety optimization. Extensive experiments on Llama3-8B-Instruct and Qwen2.5-7B-Instruct demonstrate superior safety performance across seven benchmarks with minimal degradation on seven general capability benchmarks.

## Method Summary
NSPO is a GRPO-based reinforcement learning framework that constructs a projection matrix from general task representations. The method extracts feature representations (K) from base models using general data (Math, Code), computes the null space of KK^T via SVD, and projects safety gradients to be orthogonal to K during updates. This geometric constraint ensures that safety alignment doesn't affect the model's ability to perform general tasks. The framework operates without KL-divergence penalties, relying solely on the null-space projection to preserve capabilities. Training uses 11K safety samples (40% of PKU-SafeRLHF) and requires 1,000 general instances to construct the projection matrix.

## Key Results
- NSPO achieves state-of-the-art safety performance across seven benchmarks (AdvBench, PKU-SafeRLHF, HarmBench, JailbreakBench, SORRY-Bench, HarmfulQA, ALERT)
- Minimal capability degradation on seven general benchmarks (MMLU, SuperGPQA, AlpacaEval, GSM8K, MATH, OlympiadBench, LiveCodeBench)
- Data-efficient: requires only 40% of PKU-SafeRLHF safety data compared to other methods
- Outperforms GRPO with and without KL penalties on safety-utility trade-off

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining safety updates to the null space of general task representations prevents interference with pre-trained capabilities.
- **Mechanism:** The method extracts feature representations (Keys, $K$) from the base model using general data. It computes the projection matrix for the null space of $KK^T$. During safety RL updates, gradients are projected so they are orthogonal to $K$ ($\Delta K = 0$). If the update $\Delta$ lies in the null space, the mapping $WK \to V$ remains unchanged for general tasks.
- **Core assumption:** The representations captured in matrix $K$ sufficiently span the subspace required for general reasoning tasks.
- **Evidence anchors:** [abstract] "projects safety policy gradients into the null space of general task representations"; [section 4.1] Equation 4 demonstrates theoretically that if $\Delta$ is in the null space of $K$, the embedding associations are preserved ($W_{aligned}K = W_{base}K$); [corpus] Neighbor arXiv:2503.00555 confirms safety alignment typically degrades reasoning.

### Mechanism 2
- **Claim:** The projected gradient remains a valid descent direction for the safety objective despite the geometric constraint.
- **Mechanism:** Theoretical analysis shows that the inner product between the original gradient and the projected gradient is non-negative. This ensures that while the gradient is "steered" to preserve utility, it still moves the model parameters toward lower safety loss.
- **Core assumption:** The loss landscape allows for descent directions that exist purely within the null space of the general task representations.
- **Evidence anchors:** [abstract] "guaranteeing a descent direction for effective safety alignment"; [section 4.2] Theorem 4.2 proves $\langle \nabla J, \nabla J_{NSPO} \rangle \ge 0$; [corpus] Evidence is weak regarding if this holds for all model architectures.

### Mechanism 3
- **Claim:** Null-space projection serves as a superior regularizer compared to standard KL-divergence penalties.
- **Mechanism:** Standard KL penalties pull the policy toward a reference model, which may conflict with the safety objective. NSPO removes the KL term, relying solely on the projection to prevent catastrophic forgetting. This allows for more aggressive optimization on safety data.
- **Core assumption:** The null-space constraint is strictly tighter and more relevant to capability preservation than a distributional similarity constraint (KL).
- **Evidence anchors:** [section 4.2] "Removal of KL Divergence" explicitly states the projection serves as a better alternative; [section 5.3] Figure 4 shows "GRPO w/o KL" destroys utility, while "NSPO" preserves it.

## Foundational Learning

- **Concept:** **Null Space and Singular Value Decomposition (SVD)**
  - **Why needed here:** You cannot implement the projection operator without understanding how to extract the null space basis vectors $\hat{U}$ from the covariance matrix $KK^T$.
  - **Quick check question:** If matrix $M$ has eigenvalues [0.1, 0.001, 5.0], which eigenvectors form the basis for the null space if the threshold is 0.01?

- **Concept:** **Policy Gradient (GRPO)**
  - **Why needed here:** NSPO wraps around the GRPO algorithm. You must understand how advantages are estimated using group-relative rewards before you can project the resulting gradients.
  - **Quick check question:** Why does GRPO estimate advantages using the mean and std of rewards within a group of responses rather than a separate critic model?

- **Concept:** **Catastrophic Forgetting / Alignment Tax**
  - **Why needed here:** This is the failure mode NSPO is designed to solve. Understanding the trade-off between safety reward maximization and utility retention is critical.
  - **Quick check question:** What happens to a model's performance on code generation tasks if you fine-tune it solely on safety refusal data without constraints?

## Architecture Onboarding

- **Component map:** Representation Collector -> Projection Calculator -> RL Trainer
- **Critical path:** The construction of the covariance matrix $KK^T$. If the general data sampling is biased or the SVD threshold is incorrect, the projection will either fail to protect capabilities (threshold too low) or cripple safety learning (threshold too high).
- **Design tradeoffs:**
  - **Threshold (5e-4):** Controls the strictness of the constraint. Higher values = better utility preservation but harder safety convergence.
  - **Data Mix:** Using "Mix" (Math+Code+Common) for $K$ construction generally outperforms single-domain data (Section 5.4).
- **Failure signatures:**
  - **Safety performance drops to zero:** The projection matrix is too aggressive (threshold too high), pushing all gradients to zero.
  - **Utility degrades (Safety Tax persists):** The general dataset used for $K$ is too small or unrepresentative, leaving "holes" in the protected subspace.
- **First 3 experiments:**
  1. **Sanity Check:** Train with GRPO w/o KL on safety data. Expect high safety but 0% utility. Verify this baseline failure.
  2. **Projection Ablation:** Run NSPO with the default "Mix" dataset for $K$ vs. "Code" only. Compare GSM8K/LiveCodeBench scores to confirm "Mix" preserves utility better.
  3. **Threshold Sweep:** Sweep eigenvector thresholds (e.g., 5e-5, 5e-4, 5e-3) on a small model to find the sweet spot where the gradient norm $\|\nabla J_{NSPO}\|$ is non-zero but utility remains stable.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Method's performance depends critically on the representativeness of the general data used to construct the projection matrix $K$
- The 5e-4 threshold for eigenvalue filtering appears arbitrary without comprehensive sensitivity analysis
- Projection is applied only to the final layer's parameters, leaving earlier layers unprotected
- Evaluation focuses on two base models (Llama3-8B and Qwen2.5-7B), limiting generalizability to larger models or different architectures

## Confidence
- **High confidence:** The theoretical guarantee that projected gradients remain valid descent directions for safety objectives (Theorem 4.2, inner product ≥ 0). The empirical demonstration that removing KL divergence while using NSPO preserves utility, unlike standard GRPO without KL which fails catastrophically.
- **Medium confidence:** The claim that NSPO achieves state-of-the-art safety performance while preserving general capabilities. The evaluation uses multiple benchmarks, but the safety datasets are not fully public, making independent verification difficult.
- **Low confidence:** The scalability claim to larger models and whether the method maintains its advantages when applied to models with significantly different pre-training distributions or architectures.

## Next Checks
1. **Projection matrix sensitivity:** Systematically vary the eigenvalue threshold (5e-5, 5e-4, 5e-3) and measure the trade-off between safety performance and utility preservation to identify optimal thresholds per capability domain.
2. **General data coverage ablation:** Test NSPO using single-domain projection data (Math-only, Code-only, Common sense-only) versus mixed data to quantify the importance of diverse general data for effective null-space construction.
3. **Multi-layer projection extension:** Extend the projection to earlier layers (not just final layer) and measure whether this provides additional utility preservation without sacrificing safety performance.