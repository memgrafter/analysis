---
ver: rpa2
title: 'Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on
  an Expressive and Cumulant-Controllable Data Model'
arxiv_id: '2602.02153'
source_url: https://arxiv.org/abs/2602.02153
tags:
- data
- learning
- high-order
- cumulants
- non-gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cumulant-controllable generative model
  for studying high-order statistics in neural network learning. The authors construct
  a two-layer generative model where the activation function is expanded via Hermite
  polynomials, allowing precise control over skewness, kurtosis, and other cumulants.
---

# Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model

## Quick Facts
- arXiv ID: 2602.02153
- Source URL: https://arxiv.org/abs/2602.02153
- Authors: Onat Ure; Samet Demir; Zafer Dogan
- Reference count: 0
- This paper introduces a cumulant-controllable generative model for studying high-order statistics in neural network learning.

## Executive Summary
This paper introduces a cumulant-controllable generative model that enables precise control over high-order statistics in synthetic data generation. The authors construct a two-layer generative model using Hermite polynomial expansion of the activation function, allowing interpretable manipulation of skewness, kurtosis, and higher-order cumulants. Experiments demonstrate that neural networks exhibit a "moment-wise progression" during training, first capturing low-order statistics (mean and covariance) before learning high-order cumulants, which provides insights into the distributional simplicity bias in neural network learning.

## Method Summary
The authors construct a generative data model x = W·σ_ℓ(Fz) where the latent input z follows a Gaussian distribution and the activation function σ_ℓ(z) = Σ_{i=0}^ℓ c_i·He_i(z) is expanded using probabilist's Hermite polynomials. This expansion enables independent control over data distribution cumulants through the Hermite coefficients c_i. The model generates non-Gaussian data by sampling z ~ N(μ, Σ) and passing it through the Hermite-expanded activation before the final linear projection. Experiments compare learning dynamics on non-Gaussian data versus Gaussian-equivalent datasets (matched only on first and second moments) using two-layer ReLU networks trained with online SGD.

## Key Results
- Neural networks first learn low-order statistics (mean and covariance) before progressively capturing high-order cumulants during training.
- Non-Gaussian structure provides signal unavailable in Gaussian-equivalent data, improving generalization when networks exploit high-order statistics.
- Hermite polynomial expansion enables independent, interpretable control over skewness, kurtosis, and other cumulants.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hermite polynomial expansion enables independent, interpretable control over data distribution cumulants.
- Mechanism: The activation function in the generative model is expanded as σ_ℓ(z) = Σc_i·He_i(z), where Hermite polynomials form a complete orthogonal basis under Gaussian measure. Each coefficient c_i maps polynomially to specific cumulants (κ_i = g_i(c_0,...,c_ℓ)), allowing targeted modulation of skewness, kurtosis, and higher-order statistics without affecting lower moments.
- Core assumption: The latent input z follows a Gaussian distribution; Assumption 1 (finite second moment of σ(f^T_i z)) holds.
- Evidence anchors: [abstract] "activation function is expanded by using Hermite polynomials... interpretable control over high-order cumulants"; [section] Proposition 2: "For finite ℓ, given ℓ+1 target cumulants, one can always find ℓ+1 Hermite coefficients achieving them."
- Break condition: If latent z is non-Gaussian, orthogonality of Hermite basis degrades and cumulant control becomes coupled/unpredictable.

### Mechanism 2
- Claim: Neural networks exhibit a staged learning progression, capturing low-order statistics before high-order cumulants.
- Mechanism: During SGD training, networks first minimize loss by aligning with mean and covariance (which explain majority of variance), then progressively fit higher-order structure. This "distributional simplicity bias" emerges because gradient descent exploits the most statistically salient features first.
- Core assumption: The training procedure uses online SGD; the network has sufficient capacity to represent high-order structure.
- Evidence anchors: [abstract] "networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants"; [section] Figure 1 shows test loss divergence between non-Gaussian and Gaussian-equivalent datasets emerging progressively during training.
- Break condition: If learning rate is too high or network is severely underparameterized, the network may fail to reach high-order structure before overfitting or convergence.

### Mechanism 3
- Claim: Non-Gaussian structure (controlled high-order cumulants) provides signal unavailable in Gaussian-equivalent data, improving generalization when exploited.
- Mechanism: The Gaussian-equivalent dataset matches only first and second moments. When c_i ≠ 0 for i ≥ 2, the full dataset contains additional statistical information encoded in skewness, kurtosis, etc. Networks that successfully capture these higher-order cumulants during training can leverage this extra signal for improved test performance.
- Core assumption: The downstream task benefits from high-order statistical structure; the network trains long enough to capture these features.
- Evidence anchors: [section] Figure 1(b-d): Non-Gaussian test loss consistently lower than Gaussian-equivalent when c_2 or c_3 are nonzero; [section] Figure 2: Same pattern replicates on Fashion-MNIST pretrained model with c_3, c_5 variations.
- Break condition: If task labels depend only on low-order moments, high-order cumulants add noise rather than signal, potentially harming generalization.

## Foundational Learning

- Concept: **Cumulants vs. Moments**
  - Why needed here: The paper uses cumulants (not raw moments) to quantify non-Gaussianity. Cumulants of order ≥3 vanish for Gaussian distributions, making them natural measures of departure from Gaussianity.
  - Quick check question: Given a distribution with κ_1=0, κ_2=1, κ_3=0.5, what can you conclude about its Gaussianity?

- Concept: **Hermite Polynomials as Orthogonal Basis**
  - Why needed here: The expansion σ(z) = Σc_i·He_i(z) relies on orthogonality: E[He_i(z)He_j(z)] = δ_ij under Gaussian measure. This ensures coefficients can be set independently without cross-interference.
  - Quick check question: Why does orthogonality under Gaussian measure matter for cumulant control?

- Concept: **Two-Layer Neural Network Expressivity**
  - Why needed here: Proposition 1 claims the model class is dense in probability measures with finite moments. Understanding universal approximation helps validate that the framework doesn't sacrifice expressivity for controllability.
  - Quick check question: What conditions must hold for a two-layer network to approximate any target distribution?

## Architecture Onboarding

- Component map:
  Latent Gaussian z ∈ R^p -> Linear transform: Fz ∈ R^k -> Hermite-expanded activation: σ_ℓ(Fz) = Σ_{i=0}^ℓ c_i · He_i(Fz) -> Output layer: x = W · σ_ℓ(Fz) ∈ R^d

- Critical path:
  1. Sample z ~ N(μ, Σ)
  2. Compute Hermite polynomial values He_0(Fz), ..., He_ℓ(Fz)
  3. Form weighted sum σ_ℓ(Fz) = Σc_i·He_i(Fz)
  4. Project via W to generate sample x

- Design tradeoffs:
  - Higher ℓ → finer cumulant control but more coefficients to manage
  - Larger k → more expressive but harder to interpret cumulant contributions
  - Setting c_i = 0 for i < ℓ → isolates specific cumulant orders for clean experiments

- Failure signatures:
  - All test losses identical (Gaussian vs. non-Gaussian): Hermite coefficients not properly applied or F not mixing latent dimensions
  - Instability in generated samples: c_i values too large, causing polynomial explosion
  - No progressive learning observed: learning rate too high or network underparameterized

- First 3 experiments:
  1. **Sanity check**: Generate samples with c_0=c_1=1, c_i=0 for i≥2. Verify output is Gaussian (match moments against N(0,1)).
  2. **Skewness injection**: Set c_2=0.3, c_i=0 otherwise. Plot histogram of generated samples and measure sample skewness.
  3. **Learning dynamics replication**: Train two-layer ReLU network on non-Gaussian vs. Gaussian-equivalent data. Plot test loss curves to verify moment-wise progression.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the observed "moment-wise progression" in learning dynamics be formally derived as a theoretical bound rather than just an empirical observation?
- **Basis in paper:** [inferred] While the paper provides theoretical proofs for model expressivity (Proposition 1) and cumulant controllability (Proposition 2), the central finding that networks "progressively learn high-order cumulants" is supported primarily by experimental loss curves (Fig. 1, Fig. 2) rather than a theoretical theorem.
- **Why unresolved:** The authors establish that the model *can* represent these statistics and *does* learn them in practice, but they do not derive the convergence rates or sample complexity bounds that dictate the specific temporal ordering of this learning.
- **What evidence would resolve it:** A formal proof or theoretical framework defining the sample complexity required to learn cumulants of order $k$ versus order $k+1$.

### Open Question 2
- **Question:** Does the utility of the cumulant-controllable model generalize to standard multi-class classification tasks where the "Gaussian vs. Non-Gaussian" distinction is not the primary objective?
- **Basis in paper:** [inferred] The experimental validation relies on a specific binary classification setup: discriminating between a standard Gaussian distribution and the generated non-Gaussian data (Section 3.1).
- **Why unresolved:** It is unclear if the improved generalization from high-order statistics holds when the network must discriminate between two distinct non-Gaussian distributions (e.g., different classes of Fashion-MNIST) rather than just detecting non-Gaussianity.
- **What evidence would resolve it:** Experiments applying the framework to standard multi-class benchmarks (like CIFAR-10) to see if manipulating cumulants improves accuracy on tasks other than distribution detection.

### Open Question 3
- **Question:** How does the moment-wise learning dynamic scale with modern deep architectures (e.g., CNNs or Transformers) compared to the tested two-layer networks?
- **Basis in paper:** [inferred] The authors state they "perform controlled online learning experiments with a two-layer NN" (Abstract) and utilize a "generative two-layer NN" (Section 2), leaving the interaction with deeper architectures unexplored.
- **Why unresolved:** Deep networks induce complex internal representations and inductive biases (e.g., translation invariance) that may accelerate or disrupt the sequential learning of statistical moments observed in shallow fully-connected networks.
- **What evidence would resolve it:** Replicating the controlled experiments using deep convolutional networks or attention-based models to determine if the "low-order first" progression persists.

## Limitations
- The theoretical framework relies heavily on Gaussian latent variables and Hermite polynomial orthogonality. Extension to non-Gaussian latents is not addressed.
- The learning progression claim depends on online SGD with specific learning rates. Results may not generalize to batch training or other optimizers.
- Fashion-MNIST experiments build on pretrained GAN, but the exact pretraining procedure and how non-Gaussian structure is introduced into the GAN samples is not fully specified.

## Confidence
- **High Confidence**: Hermite polynomial expansion provides independent control over cumulants when latent z is Gaussian (Proposition 2 proof structure).
- **Medium Confidence**: Neural networks learn low-order statistics before high-order cumulants in online SGD training (empirical evidence in Figures 1-2).
- **Low Confidence**: Non-Gaussian structure consistently improves generalization across diverse tasks (Fashion-MNIST results show the effect but generalization claim extends beyond demonstrated scope).

## Next Checks
1. **Orthogonality Verification**: Implement Hermite polynomial basis and verify E[He_i(z)He_j(z)] = δ_ij under Gaussian measure for multiple z dimensions and polynomial orders.
2. **Cumulant Control Test**: Generate datasets with controlled c_2 and c_3 values. Compute sample skewness and kurtosis to confirm Hermite coefficients translate to intended cumulants.
3. **Learning Progression Replication**: Train the two-layer network on synthetic data with c_2>0, c_3=0. Plot test loss curves for both non-Gaussian and Gaussian-equivalent datasets across training iterations to verify the moment-wise progression pattern.