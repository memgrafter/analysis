---
ver: rpa2
title: Analyzing Latent Concepts in Code Language Models
arxiv_id: '2510.00476'
source_url: https://arxiv.org/abs/2510.00476
tags:
- code
- concept
- clusters
- latent
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Code Concept Analysis (CoCoA), a global post-hoc
  interpretability framework that uncovers emergent lexical, syntactic, and semantic
  structures in code language models by clustering contextualized token embeddings
  into human-interpretable concept groups. The framework combines static analysis
  tool-based syntactic alignment with LLM-based annotation to scale labeling across
  abstraction levels.
---

# Analyzing Latent Concepts in Code Language Models

## Quick Facts
- **arXiv ID**: 2510.00476
- **Source URL**: https://arxiv.org/abs/2510.00476
- **Reference count**: 40
- **Key outcome**: Introduces Code Concept Analysis (CoCoA) framework that discovers interpretable concepts in code language models through clustering, showing 37 percentage point improvement in human explainability over token-level attributions

## Executive Summary
This paper presents Code Concept Analysis (CoCoA), a post-hoc interpretability framework for code language models that discovers emergent lexical, syntactic, and semantic structures through clustering contextualized token embeddings. The framework combines static analysis tool-based syntactic alignment with LLM-based annotation to scale labeling across abstraction levels. Empirical evaluations demonstrate that discovered concepts remain stable under semantic-preserving perturbations and evolve predictably with fine-tuning. A user study shows that concept-augmented explanations improve human-centric explainability by 37 percentage points compared to traditional token-level attribution methods.

## Method Summary
The framework works by extracting contextualized token embeddings from code language models, clustering them into concept groups, and aligning these with syntactic information from static analysis tools. LLM-based annotation scales the labeling process across multiple abstraction levels. The approach is validated across three different models and tasks, measuring cluster stability under semantic-preserving perturbations using a Cluster Sensitivity Index (average CSI = 0.288), tracking concept evolution during fine-tuning, and comparing explainability against token-level attribution methods through human studies.

## Key Results
- Cluster stability validated with semantic-preserving perturbations (average CSI = 0.288)
- Concept evolution tracked and predicted through fine-tuning experiments
- Human study shows 37 percentage point improvement in explainability over Integrated Gradients token-level attributions
- Framework demonstrates generalizability across different model architectures

## Why This Works (Mechanism)
The framework succeeds by bridging the gap between low-level token attributions and high-level semantic concepts in code. By clustering contextualized embeddings and aligning them with syntactic structures, it captures meaningful patterns that humans can interpret. The combination of static analysis tools for syntactic grounding and LLM annotation for semantic labeling creates a scalable approach to concept discovery. The stability metrics ensure that discovered concepts are robust to code transformations while remaining semantically meaningful.

## Foundational Learning

1. **Cluster Sensitivity Index (CSI)**
   - *Why needed*: To quantify stability of discovered concepts under code perturbations
   - *Quick check*: CSI values closer to 0 indicate more stable clusters under transformations

2. **Contextualized Token Embeddings**
   - *Why needed*: Captures semantic meaning of code tokens in their specific contexts
   - *Quick check*: Embeddings should capture both syntactic structure and semantic relationships

3. **Static Analysis Tool Integration**
   - *Why needed*: Provides syntactic grounding for discovered concepts
   - *Quick check*: Tool outputs should align with human understanding of code structure

4. **LLM-based Annotation Scaling**
   - *Why needed*: Enables labeling across multiple abstraction levels efficiently
   - *Quick check*: Annotations should maintain consistency across different code patterns

5. **Semantic-Preserving Perturbations**
   - *Why needed*: Validates that concepts capture semantic rather than syntactic patterns
   - *Quick check*: Perturbations should maintain program behavior while changing structure

## Architecture Onboarding

**Component Map**: Token Extraction -> Embedding Clustering -> Static Analysis Alignment -> LLM Annotation -> Concept Validation

**Critical Path**: The core workflow involves extracting contextualized embeddings from the target model, clustering these embeddings into concept groups, aligning clusters with syntactic information from static analysis tools, using LLM annotation to label concepts at multiple abstraction levels, and validating stability through perturbation testing.

**Design Tradeoffs**: The framework balances interpretability (requiring human-interpretable concepts) against coverage (needing to capture diverse code patterns). Static analysis tools provide syntactic grounding but may miss semantic nuances, while LLM annotation scales labeling but introduces potential bias. The perturbation-based validation ensures semantic robustness but relies on synthetic transformations that may not reflect real-world code evolution.

**Failure Signatures**: Cluster instability (high CSI scores) indicates concepts are too tied to specific syntactic patterns rather than semantic meaning. Misalignment between static analysis outputs and LLM annotations suggests the framework is capturing artifacts rather than meaningful concepts. Poor human interpretability in user studies indicates concepts are not sufficiently aligned with developer mental models.

**First Experiments**:
1. Run CoCoA on a simple code classification task with known semantic categories to validate basic concept discovery
2. Apply semantic-preserving perturbations (variable renaming, dead code insertion) to test cluster stability on a small codebase
3. Compare CoCoA explanations against Integrated Gradients attributions on a debugging task with human evaluators

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on synthetic perturbations that may not capture real-world code evolution patterns
- Human study involves single classification task with small sample size (N=36), limiting generalizability
- Alignment between static analysis tools and LLM annotations introduces potential bias in concept discovery

## Confidence

**Major claim clusters confidence:**
- **High**: Cluster stability under semantic-preserving perturbations (CSI metric validation)
- **High**: Concept evolution tracking with fine-tuning experiments
- **Medium**: Human study results showing improved explainability
- **Medium**: Framework generalizability across different model architectures
- **Low**: External validity to production codebases and real-world development scenarios

## Next Checks
1. Conduct large-scale user studies across multiple code comprehension tasks (debugging, code review, feature implementation) with larger participant pools to validate the 37 percentage point improvement claim
2. Test framework robustness on codebases with varying complexity, including legacy systems and cross-language projects, to assess real-world applicability
3. Implement adversarial testing using semantic-preserving but syntactically complex transformations (e.g., polymorphism, design pattern refactoring) to stress-test cluster stability beyond simple variable renaming and dead code insertion