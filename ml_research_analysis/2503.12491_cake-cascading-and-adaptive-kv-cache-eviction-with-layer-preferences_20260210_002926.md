---
ver: rpa2
title: 'CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences'
arxiv_id: '2503.12491'
source_url: https://arxiv.org/abs/2503.12491
tags:
- layer
- cache
- attention
- cake
- snapkv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAKE dynamically allocates KV cache sizes across layers by analyzing
  layer-specific attention patterns, considering both spatial dispersion and temporal
  shifts. It uses a cascading memory management approach to maintain cache budgets
  while evaluating layer preferences globally, and introduces an eviction indicator
  that accounts for sustained importance and attention variability.
---

# CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences

## Quick Facts
- arXiv ID: 2503.12491
- Source URL: https://arxiv.org/abs/2503.12491
- Reference count: 40
- Maintains model performance with only 3.2% of the KV cache

## Executive Summary
CAKE introduces a novel KV cache eviction strategy for large language models that dynamically allocates cache sizes across layers by analyzing layer-specific attention patterns. The method considers both spatial dispersion and temporal shifts in attention, using a cascading memory management approach to maintain cache budgets while evaluating layer preferences globally. By introducing an eviction indicator that accounts for sustained importance and attention variability, CAKE achieves significant memory efficiency while preserving model performance.

The system demonstrates substantial practical benefits, achieving over 10× speedup in decoding latency compared to full cache when processing 128K tokens with FlashAttention-2. Experiments on LongBench and NeedleBench show consistent outperformance across various models and memory constraints, particularly in low-memory settings where the approach maintains competitive performance with only 3.2% cache retention.

## Method Summary
CAKE implements a cascading and adaptive KV cache eviction mechanism that analyzes attention patterns across different layers to make intelligent cache allocation decisions. The system uses layer-specific attention analysis to identify which tokens are most important for each layer, considering both spatial dispersion (how attention is distributed across the sequence) and temporal shifts (how attention patterns evolve over time). The cascading memory management approach allows for efficient eviction decisions while maintaining cache budgets, and the eviction indicator incorporates both sustained importance and attention variability metrics to make optimal cache retention choices.

## Key Results
- Maintains model performance with only 3.2% of the KV cache compared to full cache
- Consistently outperforms baseline methods across various models and memory constraints
- Achieves over 10× speedup in decoding latency when processing 128K tokens with FlashAttention-2

## Why This Works (Mechanism)
The approach works by recognizing that different transformer layers have varying attention patterns and memory requirements. By analyzing these layer-specific patterns, CAKE can intelligently allocate cache resources where they are most needed. The cascading mechanism allows for efficient memory management by making eviction decisions based on global layer preferences rather than treating all layers equally. The eviction indicator's consideration of both sustained importance and attention variability ensures that critical information is preserved while less important data is evicted, optimizing the trade-off between memory usage and performance.

## Foundational Learning
**Transformer Attention Mechanisms**: Understanding how attention weights are computed and used in transformer layers is crucial for grasping why certain tokens are more important to retain in cache. Quick check: Verify understanding of self-attention computation and its role in information flow.

**KV Cache Optimization**: Knowledge of how KV caches work in transformer inference and why they can become memory bottlenecks is essential. Quick check: Confirm understanding of how KV caches store key and value vectors for attention computation.

**Memory Management in Deep Learning**: Familiarity with techniques for managing memory in large-scale models, including cache eviction strategies and their trade-offs. Quick check: Review common memory optimization techniques used in LLM inference.

## Architecture Onboarding

**Component Map**: Attention Analyzer -> Layer Preference Evaluator -> Cascading Eviction Manager -> Cache Controller

**Critical Path**: Input tokens → Attention analysis → Layer preference computation → Eviction decision → Cache update → Attention computation

**Design Tradeoffs**: Memory efficiency vs. computational overhead in the eviction decision process; granularity of layer-specific analysis vs. overall system complexity; precision of attention pattern analysis vs. real-time inference requirements.

**Failure Signatures**: Degraded performance on tasks requiring long-range dependencies; increased latency due to complex eviction computations; suboptimal cache allocation leading to unnecessary recomputations.

**3 First Experiments**:
1. Measure cache hit rate and performance degradation when varying the percentage of retained cache (10%, 5%, 3.2%, 1%)
2. Compare attention pattern analysis accuracy between CAKE and baseline methods across different layer depths
3. Evaluate end-to-end latency improvements with varying sequence lengths (1K, 16K, 64K, 128K tokens)

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may vary significantly across different model architectures and domains beyond LongBench and NeedleBench
- The cascading eviction mechanism introduces computational overhead that could offset latency gains in some deployment scenarios
- The approach assumes relatively stable attention patterns during inference, which may not hold for highly dynamic inputs

## Confidence

*High confidence*:
- Cascading memory management effectively maintains cache budgets while evaluating layer preferences globally
- Eviction indicator successfully accounts for sustained importance and attention variability
- Consistent performance improvements over baseline methods across multiple models and memory constraints

*Medium confidence*:
- 10× speedup in decoding latency with 128K tokens is achievable in practical deployment scenarios
- Layer preference analysis provides meaningful insights for cache allocation decisions
- Approach generalizes well beyond tested LongBench and NeedleBench datasets

*Low confidence*:
- Performance stability across all possible long-context tasks and domains
- 3.2% cache retention rate represents a universal best-case scenario
- Computational overhead remains negligible across all deployment scenarios

## Next Checks

1. Conduct comprehensive testing across diverse model architectures (including non-Transformer models) and multiple domains to assess generalization capabilities and identify potential failure modes.

2. Perform ablation studies to quantify the individual contributions of each component (cascading mechanism, layer preference analysis, and eviction indicator) to overall performance.

3. Implement real-world deployment testing with varying computational resources and input patterns to validate the practical benefits and identify potential bottlenecks in production environments.