---
ver: rpa2
title: A Granular Study of Safety Pretraining under Model Abliteration
arxiv_id: '2510.02768'
source_url: https://arxiv.org/abs/2510.02768
tags:
- safety
- refusal
- should
- score
- rephrase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the robustness of safety pretraining under
  model abliteration, an inference-time attack that removes refusal directions in
  activation space. Using granular checkpoints from Safety Pretraining on SmolLM2-1.7B,
  the authors compare original and abliterated models across 20 systems, measuring
  refusal rates for harmful and harmless prompts.
---

# A Granular Study of Safety Pretraining under Model Abliteration

## Quick Facts
- arXiv ID: 2510.02768
- Source URL: https://arxiv.org/abs/2510.02768
- Authors: Shashank Agnihotri; Jonas Jakubassa; Priyam Dey; Sachin Goyal; Bernt Schiele; Venkatesh Babu Radhakrishnan; Margret Keuper
- Reference count: 40
- Key outcome: Safety pretraining combining multiple interventions (safe filtering, rephrasing, metatags, refusals) is most resilient to abliteration, while single-method approaches are easily neutralized.

## Executive Summary
This study evaluates how different safety pretraining approaches hold up against model abliteration, an inference-time attack that removes refusal directions from activation space. Using granular checkpoints from Safety Pretraining on SmolLM2-1.7B, the authors systematically compare original and abliterated models across 20 systems, measuring refusal rates for harmful and harmless prompts. The findings reveal that distributed safety signals across multiple features provide robust resistance to abliteration, while models trained with only explicit refusal are easily neutralized. Additionally, models cannot reliably detect when their own refusal behavior has been compromised, limiting self-monitoring capabilities.

## Method Summary
The study implements model abliteration through PCA-based projection to remove refusal directions at inference time. For each model checkpoint, activations from harmful and harmless anchor prompts are collected at a specific layer, mean-centered, and subjected to PCA. The first principal component identifies the refusal direction, which is then projected out during inference with scale Î±. Twenty systems are evaluated: ten original safety pretraining checkpoints (raw, score-0, rephrase, metatags, refusals, combined) plus ten abliterated variants. Response generation uses 100 balanced prompts (50 harmful, 50 harmless), with judgments provided by multiple LLM judges (ChatGPT-5, GLM-4, regex, Qwen-3, SmolLM2, GPT-oss) validated against human annotations. Self-judgment probes assess whether models can detect their own refusal states.

## Key Results
- Safety interventions combining safe-only filtering, rephrasing, metatags, and refusals are most resilient to abliteration
- Models trained with only explicit refusal are easily neutralized by removing the single refusal direction
- Models fail to reliably detect their own refusals after abliteration, indicating limited self-monitoring capability
- Distributed safety signals across features enhance robustness to activation-space edits

## Why This Works