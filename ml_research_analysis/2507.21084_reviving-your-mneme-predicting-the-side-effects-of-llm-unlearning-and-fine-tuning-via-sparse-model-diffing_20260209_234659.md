---
ver: rpa2
title: 'Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning
  via Sparse Model Diffing'
arxiv_id: '2507.21084'
source_url: https://arxiv.org/abs/2507.21084
tags:
- fine-tuning
- mneme
- arxiv
- unlearning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MNEME addresses the challenge of detecting unintended side effects
  from LLM fine-tuning or unlearning without requiring access to the fine-tuning data.
  It uses sparse model diffing, comparing base and fine-tuned model activations on
  task-agnostic data to identify behavioral shifts.
---

# Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing

## Quick Facts
- arXiv ID: 2507.21084
- Source URL: https://arxiv.org/abs/2507.21084
- Reference count: 11
- Primary result: Up to 95% accuracy in predicting unintended LLM side effects via sparse model diffing

## Executive Summary
MNEME is a method for detecting unintended side effects from LLM fine-tuning or unlearning without requiring access to the fine-tuning data. It uses sparse model diffing to compare base and fine-tuned model activations on task-agnostic data, identifying behavioral shifts through sparse latent features. By learning sparse latent directions and quantifying feature changes via latent scaling, MNEME can attribute these shifts to specific semantic categories. Across three scenarios—WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning—MNEME achieves up to 95% accuracy in predicting side effects, often approaching oracle-level performance.

## Method Summary
MNEME works by extracting paired activations from base and fine-tuned LLMs, training a BatchTopK Cross-Coder with sparse latent features (expansion factor 32, k=100), and using latent scaling to attribute each feature to amplification, suppression, or unchanged behavior. Auto-interpretation via LLaMA-3.1-70B-Instruct generates semantic descriptions from top-activating contexts, which are mapped to categories. The method achieves high accuracy in predicting which semantic capabilities are affected by fine-tuning or unlearning, validated across three scenarios using MMLU and MMLU-Pro benchmarks.

## Key Results
- Up to 95% accuracy in predicting side effects from LLM fine-tuning/unlearning
- Approaches oracle-level performance in detecting capability degradation
- 100% precision and 81.1% F1 in detecting harmful drift in emergent misalignment scenario

## Why This Works (Mechanism)

### Mechanism 1: Sparse Cross-Coder Decomposition
- Claim: Joint sparse encoding of paired base and fine-tuned activations isolates behaviorally meaningful differences without task-specific data.
- Mechanism: A shared encoder applies TopK sparsity on concatenated activations [a(b); a(f)], producing a sparse latent code zi. Dual model-specific decoders reconstruct each model's activations independently, forcing latents to explain shared vs. divergent structure.
- Core assumption: Behavioral shifts manifest as changes in decoder weight norms and activation patterns that sparse decomposition can disentangle (assumes polysemanticity and superposition are the primary confounders).
- Evidence anchors:
  - [abstract] "MNEME learns sparse latent features from paired base and fine-tuned model activations, attributes each feature to amplification, suppression, or unchanged behavior"
  - [section 3.1] "decoder weight ℓ2 norm distribution shifts leftward as forgetting increases (e.g., 10% to 50%), aligning with greater task degradation"
  - [corpus] Related work on geometric disentanglement (arxiv:2511.17100) supports the premise that unlearning causes collateral degradation via shared representations, but does not validate Cross-Coder architectures specifically.
- Break condition: If fine-tuning induces distributed, non-sparse changes across thousands of latents with no norm separation, attribution will degrade.

### Mechanism 2: Latent Scaling for Attribution
- Claim: Removing each latent and measuring reconstruction loss change quantifies whether a feature is amplified or suppressed post-intervention.
- Mechanism: For latent j, solve regression to find β(b)j and β(f)j minimizing reconstruction error without that latent. Magnitude increase after fine-tuning → amplification; decrease → suppression.
- Core assumption: Reconstruction loss proxies behavioral importance; latents with larger β contributions are causally relevant to model outputs.
- Evidence anchors:
  - [section 3.2] "An increase in magnitude after fine-tuning implies amplification; a decrease implies minimization"
  - [section 5.2] On emergent misalignment, MNEME achieves 100% precision and 81.1% F1 in detecting harmful drift
  - [corpus] Weak/no direct corpus validation for this specific regression-based attribution method.
- Break condition: If latents are highly correlated (non-orthogonal), removing one may shift credit to others, producing unstable β estimates.

### Mechanism 3: Auto-Interpretation → Semantic Mapping
- Claim: LLM-generated descriptions of top-activating contexts produce human-interpretable labels that correlate with downstream side effects.
- Mechanism: Collect top-k activating sequences per latent, prompt LLaMA-3.1-70B-Instruct for concise descriptions, then map descriptions to semantic categories (e.g., "chemistry", "harmful content").
- Core assumption: High-activation samples capture the semantic essence of each latent; LLM interpretation faithfully summarizes this (assumes no systematic hallucination bias).
- Evidence anchors:
  - [section 3.3] "This aids semantic understanding and shows strong alignment with human annotations"
  - [section 7] "40% of latents had over 90% semantic overlap, while the rest showed weaker alignment"
  - [corpus] No direct corpus evidence validates auto-interpretation faithfulness for diffing specifically.
- Break condition: If top-activating contexts are polysemantic or ambiguous, descriptions will be noisy, reducing mapping accuracy.

## Foundational Learning

- Concept: **Sparse Autoencoders (SAEs)**
  - Why needed here: MNEME builds on SAE principles (overcomplete dictionaries, sparsity constraints) but extends to Cross-Coders for model comparison.
  - Quick check question: Can you explain why sparsity (e.g., TopK) helps disentangle polysemantic neurons into monosemantic features?

- Concept: **Polysemanticity & Superposition**
  - Why needed here: Side effects arise because neurons encode multiple concepts; unlearning one domain can suppress shared representations.
  - Quick check question: If a neuron activates on both "biology" and "chemistry" contexts, what happens when you suppress "biology"-linked latents?

- Concept: **Latent Attribution via Reconstruction**
  - Why needed here: MNEME quantifies feature importance via reconstruction loss contribution, not direct behavioral probes.
  - Quick check question: Why might reconstruction loss be an imperfect proxy for downstream task performance?

## Architecture Onboarding

- Component map: Activation extraction -> Cross-Coder encoder -> Dual decoders -> Latent scaling -> Auto-interpretation -> Semantic mapper
- Critical path:
  1. Collect 200M tokens from task-agnostic corpus (The Pile or LMSYS-Chat-1M)
  2. Forward pass through both models → extract activations
  3. Train Cross-Coder (~98K–131K latents, expansion factor 32)
  4. Compute latent scaling (β(b), β(f)) for all latents
  5. Run auto-interpretation on top-k activating samples per latent
  6. Map to semantic categories; cross-reference with benchmark labels (e.g., MMLU)

- Design tradeoffs:
  - **Expansion factor**: 32 yields finer features vs. 6–12 (broader); higher compute cost
  - **Layer choice**: Layer 14 balances semantic/syntactic; earlier layers may miss high-level concepts
  - **Corpus choice**: The Pile vs. LMSYS-Chat-1M—paper reports minimal impact for instruction-tuned models, but narrow domains may require oversampling

- Failure signatures:
  - High dead latent rate (>15%) → expansion too large or data insufficient
  - Low semantic overlap (<40%) in auto-interpretation → activation samples not representative
  - β values unstable across batches → latents highly correlated, consider orthogonalization

- First 3 experiments:
  1. **Reproduction on WMDP unlearning**: Train Cross-Coder on LLaMA-3-8B-Instruct base vs. RMU-unlearned model; verify ~92% accuracy on MMLU category prediction
  2. **Ablation on expansion factor**: Compare k=6, 12, 32 on feature interpretability (manual inspection of top-5 descriptions per setting)
  3. **Corpus sensitivity**: Swap The Pile for domain-specific corpus (e.g., bioarXiv) in WMDP setting; measure change in detected chemistry/biology latents

## Open Questions the Paper Calls Out
None

## Limitations
- Sparse latent attribution relies on reconstruction loss as a proxy for behavioral importance, but this assumption is weakly validated in the paper and in the corpus; reconstruction-based attribution may not capture task-specific degradation accurately.
- Auto-interpretation via LLM is subject to hallucination and prompt sensitivity; semantic mapping accuracy depends heavily on the faithfulness and consistency of generated descriptions, which the paper only partially quantifies (40% strong overlap reported).
- The paper does not detail fine-tuning or unlearning protocols, relying instead on third-party datasets and procedures; thus, methodological reproducibility is constrained by external dependencies.

## Confidence
- **High**: Sparse Cross-Coder decomposition effectively isolates model differences and captures decoder norm shifts that align with known unlearning behavior.
- **Medium**: Latent scaling and auto-interpretation produce actionable semantic categories, though auto-interpretation faithfulness and reconstruction-based attribution are not exhaustively validated.
- **Low**: The claim that MNEME "approaches oracle-level" performance is not fully supported, as oracle baselines are not independently specified in detail.

## Next Checks
1. **Ablation study on auto-interpretation**: Replace LLaMA-3.1-70B-Instruct with multiple smaller LLMs or manual annotation to quantify the impact of LLM choice and prompt design on semantic mapping accuracy.
2. **Reconstruction vs. behavioral validation**: Compare MNEME's latent attribution predictions with direct behavioral probes (e.g., task-specific benchmarks) to verify that reconstruction loss proxies capture true capability shifts.
3. **Orthogonality and correlation analysis**: Measure latent correlation and stability across batches; test whether removing highly correlated latents causes unstable β estimates, indicating need for regularization or orthogonalization.