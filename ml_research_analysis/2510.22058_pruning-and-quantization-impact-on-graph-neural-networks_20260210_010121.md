---
ver: rpa2
title: Pruning and Quantization Impact on Graph Neural Networks
arxiv_id: '2510.22058'
source_url: https://arxiv.org/abs/2510.22058
tags:
- pruning
- accuracy
- usage
- sparsity
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the impact of pruning and quantization techniques
  on Graph Neural Networks (GNNs) across three graph datasets and three tasks. Results
  show that unstructured fine-grained and global pruning can reduce model size by
  50% while maintaining or improving accuracy after fine-tuning.
---

# Pruning and Quantization Impact on Graph Neural Networks

## Quick Facts
- **arXiv ID:** 2510.22058
- **Source URL:** https://arxiv.org/abs/2510.22058
- **Reference count:** 40
- **Primary result:** Pruning and quantization effectively compress GNN models, improving efficiency for resource-constrained deployment, with optimal strategies varying by task and dataset.

## Executive Summary
This study systematically evaluates pruning and quantization techniques on Graph Neural Networks (GNNs) across three graph datasets and three distinct tasks. The research demonstrates that unstructured fine-grained and global pruning can reduce model size by up to 50% while maintaining or improving accuracy after fine-tuning. Among quantization methods, DQ-INT8 and QAT-INT8 consistently deliver high accuracy with significant computational cost reductions. The findings reveal that regularization-based pruning preserves accuracy at moderate sparsity levels but degrades performance at aggressive compression rates, highlighting the importance of method selection based on specific task requirements and deployment constraints.

## Method Summary
The study evaluates compression techniques on GNNs using Cora (Node/Link), Proteins (Graph), and BBBP (Graph) datasets across node classification, link prediction, and graph classification tasks. Implemented using Python 3.10 with PyTorch 2.0.1, PyTorch Geometric 2.4.0, and Torch-pruning. Models include SplineConv for Cora Node, NESS for Cora Link, HGP-SL for Proteins/BBBP, and GIN for quantization tasks. Pruning strategies include Global and Fine-grained approaches at sparsity levels from 0.0 to 0.9, with Algorithm 1 for saving sparse model states. Quantization methods tested include A2Q, QAT, and DQ with INT4/INT8 precision using gradient clipping and adaptive bit-width configurations. Evaluation metrics encompass accuracy, inference time, model size, energy consumption, CPU usage, and memory utilization.

## Key Results
- Unstructured fine-grained and global pruning reduce model size by 50% while maintaining or improving accuracy after fine-tuning
- DQ-INT8 and QAT-INT8 quantization methods consistently deliver high accuracy with significant computational cost reductions
- Regularization-based pruning preserves accuracy at moderate sparsity rates but degrades performance at aggressive compression levels (>80%)

## Why This Works (Mechanism)
Pruning and quantization work by systematically reducing model complexity while preserving essential representational capacity. Fine-grained pruning removes individual weights based on importance criteria, creating sparse matrices that can be compressed without losing critical information. Global pruning applies uniform sparsity across all layers, simplifying the optimization process. Quantization reduces precision from FP32 to INT8/INT4, decreasing memory footprint and accelerating computations through integer arithmetic. These techniques exploit the observation that many neural network weights contribute minimally to final predictions, allowing substantial compression without proportional accuracy loss when properly fine-tuned.

## Foundational Learning
- **Graph Neural Networks**: Why needed - Core architecture being compressed; Quick check - Verify Cora, Proteins, and BBBP datasets are properly loaded and basic GNN models converge
- **Model Pruning**: Why needed - Reduces parameter count and model size; Quick check - Implement basic weight pruning and verify sparsity patterns
- **Quantization-Aware Training**: Why needed - Enables low-precision inference while maintaining accuracy; Quick check - Test INT8 conversion on pre-trained model and measure accuracy drop
- **Sparse Matrix Operations**: Why needed - Essential for efficient inference of pruned models; Quick check - Compare dense vs sparse matrix multiplication times
- **Gradient Clipping**: Why needed - Stabilizes training during quantization; Quick check - Implement gradient clipping and observe training stability metrics
- **Adaptive Bit-width**: Why needed - Optimizes precision allocation per layer; Quick check - Implement per-layer precision adjustment and measure impact on accuracy/size tradeoff

## Architecture Onboarding

**Component Map:** Cora Node Classification -> SplineConv -> Pruning/Quantization -> Evaluation Metrics
**Critical Path:** Data Loading -> Model Training -> Compression Application -> Fine-tuning -> Performance Evaluation
**Design Tradeoffs:** Accuracy vs Model Size vs Inference Speed; Fine-grained pruning offers better accuracy retention but more complex implementation than global pruning
**Failure Signatures:** Accuracy collapse without fine-tuning; No inference speedup despite pruning due to lack of sparse kernel optimization
**First Experiments:** 1) Implement Algorithm 1 for sparse model saving and verify 50% size reduction on Cora dataset 2) Apply 50% global pruning to SplineConv and measure accuracy recovery after fine-tuning 3) Test QAT-INT8 on GIN model and compare accuracy vs FP32 baseline

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Pruning does not consistently reduce inference latency on GPUs due to lack of sparse kernel optimization support
- Hyperparameter details for fine-tuning after pruning are unspecified, particularly learning rates and epochs
- Resource measurement methodology lacks specificity regarding tools for energy and CPU usage measurement

## Confidence
- High confidence in compression efficacy findings (50% size reduction feasible) due to clear algorithmic specifications (Algorithm 1) and consistent results across multiple datasets
- Medium confidence in quantization results given INT8/INT4 specifics provided but adaptive bit-width mechanism under-specified
- Medium confidence in task-specific performance variation claims due to clear dataset/task breakdown but limited hyperparameter transparency

## Next Checks
1. Verify Algorithm 1 implementation for sparse model saving correctly compresses pruned models to expected disk sizes
2. Test pruning without fine-tuning on Cora Node Classification to confirm accuracy collapse occurs as paper implies
3. Implement QAT-INT8 with gradient clipping on GIN model and verify accuracy retention against FP32 baseline