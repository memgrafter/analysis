---
ver: rpa2
title: 'Fair-GPTQ: Bias-Aware Quantization for Large Language Models'
arxiv_id: '2509.15206'
source_url: https://arxiv.org/abs/2509.15206
tags:
- quantization
- bias
- fair-gptq
- language
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fair-GPTQ, the first quantization method
  designed to reduce bias in large language models during compression. It modifies
  the GPTQ optimization objective by adding a group-fairness constraint that penalizes
  differences in likelihood between stereotypical and anti-stereotypical sentence
  pairs.
---

# Fair-GPTQ: Bias-Aware Quantization for Large Language Models

## Quick Facts
- **arXiv ID**: 2509.15206
- **Source URL**: https://arxiv.org/abs/2509.15206
- **Reference count**: 39
- **Primary result**: Fair-GPTQ is the first quantization method designed to reduce bias in large language models during compression.

## Executive Summary
This paper introduces Fair-GPTQ, a novel quantization method that reduces bias in large language models during compression. The approach modifies the standard GPTQ optimization objective by adding a group-fairness constraint that penalizes differences in likelihood between stereotypical and anti-stereotypical sentence pairs. Fair-GPTQ specifically targets the attention output projection and fully connected output matrices, which are most influential for both bias and generation. The method consistently reduces stereotype scores across benchmarks while preserving at least 90% of zero-shot performance, offering faster runtime compared to iterative debiasing baselines.

## Method Summary
Fair-GPTQ extends the standard GPTQ optimization objective by incorporating a group-fairness regularization term. This regularization penalizes the likelihood difference between stereotypical and anti-stereotypical sentence pairs during quantization. The method focuses on quantizing only the attention output projection and fully connected output matrices, which are identified as the most influential components for bias and generation. The regularization strength (α) is selected through experimentation, with values of 0.1 for all-layer quantization and 0.5 for grouped configurations. The approach is applied to various model architectures and sizes, demonstrating consistent bias reduction while maintaining model performance.

## Key Results
- Fair-GPTQ reduces stereotype scores across benchmarks, lowering CrowS-Pairs from 65.95 to 63.92 for Mistral-7B
- Preserves at least 90% of zero-shot performance compared to baseline models
- Offers faster runtime compared to iterative debiasing baselines
- Demonstrates consistent bias reduction across different model architectures and sizes

## Why This Works (Mechanism)
Fair-GPTQ works by modifying the quantization optimization objective to include a group-fairness constraint. During the quantization process, the method calculates the likelihood difference between stereotypical and anti-stereotypical sentence pairs and incorporates this difference into the loss function. By penalizing large differences in likelihood between these pairs, the quantized model is encouraged to generate more balanced outputs across different demographic groups. The approach specifically targets the attention output projection and FFN output matrices because these components have the greatest influence on both bias propagation and generation quality.

## Foundational Learning
- **Quantization-aware training**: Understanding how weight quantization affects model performance is crucial for developing effective quantization methods
  - Why needed: Standard quantization can amplify existing biases in LLMs
  - Quick check: Verify that quantized models show different bias patterns than their full-precision counterparts

- **Group fairness metrics**: Knowledge of how to measure and quantify bias in language models is essential for bias-aware methods
  - Why needed: Fair-GPTQ relies on comparing stereotypical and anti-stereotypical sentence pairs
  - Quick check: Ensure calibration datasets contain balanced stereotypical and anti-stereotypical pairs

- **GPTQ optimization**: Understanding the standard GPTQ quantization framework provides the foundation for Fair-GPTQ's modifications
  - Why needed: Fair-GPTQ builds upon and extends the GPTQ optimization objective
  - Quick check: Confirm that Fair-GPTQ reduces to standard GPTQ when α=0

## Architecture Onboarding

**Component map**: Input -> Token Embeddings -> Attention Layers (with output projection matrices) -> FFN Layers (with output matrices) -> Output

**Critical path**: The attention output projection and FFN output matrices form the critical path for both bias propagation and generation quality

**Design tradeoffs**: Fair-GPTQ trades some performance for fairness by adding a regularization term to the quantization objective

**Failure signatures**: If α is too high, the model may show reduced performance; if too low, bias reduction may be insufficient

**First experiments**:
1. Compare bias scores (CrowS-Pairs, StereoSet) between Fair-GPTQ and standard GPTQ for various α values
2. Measure zero-shot performance degradation as a function of α across different model architectures
3. Evaluate runtime efficiency gains compared to iterative debiasing baselines

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can extended, narrative-style calibration sequences that provide richer context (rather than short 2–3 sentence stereotype pairs) improve Fair-GPTQ's debiasing effectiveness and task performance on longer generations?
- Basis in paper: [explicit] The authors state: "our calibration data (StereoSet) are limited to short sequences" and plan to "introduce an extended dataset that provides additional context, forming story-like narratives while preserving minimal differences for stereotypes."
- Why unresolved: Current benchmarks only contain minimal pairs; the effect of longer, context-rich calibration data on both fairness and downstream performance remains untested.
- What evidence would resolve it: Experiments comparing Fair-GPTQ with short vs. extended narrative calibration data, measuring bias scores (CrowS-Pairs, StereoSet) and zero-shot accuracy on tasks requiring longer reasoning.

### Open Question 2
- Question: How does Fair-GPTQ perform with multilingual calibration data on multilingual LLMs, particularly for non-Latin script languages where quantization-induced bias amplification is most pronounced?
- Basis in paper: [explicit] The authors note: "multilingual limitations remain. The calibration datasets we use are monolingual, whereas multilingual calibration data would be expected to improve performance for multilingual models" and cite prior work showing pronounced bias effects on multilingual models.
- Why unresolved: Current experiments use only English StereoSet data; the interaction between multilingual calibration and bias reduction across languages is unknown.
- What evidence would resolve it: Multilingual Fair-GPTQ experiments using paired stereotype data in multiple languages, evaluated on multilingual bias benchmarks.

### Open Question 3
- Question: Can half-precision outlier channels or other outlier-aware techniques recover the ~10% zero-shot performance loss observed in Fair-GPTQ while maintaining its debiasing benefits?
- Basis in paper: [explicit] The authors suggest future work should "explore the use of half-precision outlier channels (Lee et al., 2024) to recover the performance of debiased models."
- Why unresolved: Fair-GPTQ currently trades fairness for performance (preserving ~90% baseline accuracy); whether architectural modifications can mitigate this tradeoff is unexplored.
- What evidence would resolve it: Ablation studies combining Fair-GPTQ with outlier-preserving quantization schemes, measuring both fairness metrics and zero-shot accuracy recovery.

### Open Question 4
- Question: Is there a principled method for selecting the regularization strength (α) that generalizes across model architectures, sizes, and layer configurations, rather than relying on grid search heuristics?
- Basis in paper: [inferred] The authors select α via experimentation (0.1 for ALL layers, 0.5 for grouped configurations) without theoretical justification, noting the sensitivity of the fairness-performance tradeoff to this parameter.
- Why unresolved: No theoretical framework or adaptive mechanism is provided for α selection; the current approach may not generalize to new architectures.
- What evidence would resolve it: Development and validation of an adaptive α selection method (e.g., based on Hessian properties or bias gradient magnitude) tested across multiple model families.

## Limitations
- Methodology focuses on group-fairness constraints and may not capture intersectional biases
- Validation limited to only two architectures (Mistral-7B and Llama-2-7B), constraining generalizability claims
- Runtime comparison lacks hardware and implementation details for independent verification
- Only two matrix types (attention output projection and FFN output) are targeted, leaving open questions about bias preservation in other components

## Confidence
- **High confidence**: Bias reduction metrics and preservation of zero-shot performance are well-supported by presented data
- **Medium confidence**: Claims of runtime efficiency and model-agnostic applicability due to limited experimental scope and missing implementation details
- **Low confidence**: Generalizability to models outside the tested architectures and effectiveness against intersectional or more nuanced bias forms

## Next Checks
1. Test Fair-GPTQ on additional model architectures (e.g., GPT-Neo, Bloom) and scales (1B, 13B) to verify model-agnostic claims
2. Expand bias evaluation to include intersectional bias benchmarks and more diverse stereotype categories
3. Conduct ablation studies to isolate the impact of quantizing only attention output projection and FFN output matrices on overall model bias and task performance