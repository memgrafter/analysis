---
ver: rpa2
title: 'Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs'
arxiv_id: '2501.16534'
source_url: https://arxiv.org/abs/2501.16534
tags:
- candidate
- classifier
- safety
- adversarial
- classifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel technique to extract a surrogate classifier
  from aligned LLMs to assess their vulnerability to jailbreak attacks. The core idea
  is that alignment embeds a safety classifier in the LLM responsible for deciding
  between refusal and compliance.
---

# Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs

## Quick Facts
- arXiv ID: 2501.16534
- Source URL: https://arxiv.org/abs/2501.16534
- Reference count: 40
- Primary result: Surrogate classifiers extracted from aligned LLMs achieve >80% F1 scores and enable jailbreak attacks with 70% success rate at 50% model size, significantly outperforming direct attacks

## Executive Summary
This paper proposes a novel technique to extract surrogate safety classifiers from aligned LLMs by leveraging the observation that alignment creates separable representations of safe/unsafe inputs in intermediate decoder layers. The core insight is that the safety decision boundary can be approximated by training a linear classification head on embeddings from a subset of the model architecture. This approach enables efficient jailbreak attacks through transferability, achieving high attack success rates while significantly reducing computational cost compared to attacking the full model directly.

## Method Summary
The method extracts a surrogate safety classifier by selecting a subset of decoder layers (structure) from an aligned LLM, extracting final token embeddings, and training a linear probe to predict safety labels. The process involves: (1) collecting labeled data where LLM outputs are classified as refusal/compliance using a third-party rejection classifier, (2) for various structure sizes, extracting embeddings and training linear heads with 5-fold cross-validation, (3) selecting the optimal structure based on cross-dataset F1 scores, and (4) using the surrogate for adversarial attacks via modified GCG with misclassification objectives. The approach leverages the observation that safety-relevant information concentrates in intermediate decoder layers, as evidenced by non-monotonic silhouette scores.

## Key Results
- Surrogate classifiers achieve F1 scores above 80% using as little as 20% of the model architecture
- Attacking surrogates achieves attack success rates up to 70% versus 22% for direct attacks on full models
- Runtime and memory usage reduced by more than half when using 50% of architecture
- Transferability rates peak above 70% for most models at ~60% normalized candidate size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alignment creates separable representations of safe/unsafe inputs in intermediate decoder layers.
- Mechanism: During alignment, the model learns to route unsafe inputs toward refusal output distributions, creating a linearly separable boundary in embedding space at certain decoder depths.
- Core assumption: Final embedding before output generation contains sufficient information to determine refusal vs. compliance.
- Evidence: Silhouette scores exceed 0.25-0.5 across models, peaking at intermediate decoders; related work confirms safety representations exist in intermediate layers.

### Mechanism 2
- Claim: A linear classification head trained on intermediate decoder outputs can approximate the model's safety decisions.
- Mechanism: By freezing a structure and training only a linear probe on final token embedding, the head learns to map representation space to binary safety labels.
- Core assumption: Safety classification is approximately linear in embedding space at chosen decoder depth.
- Evidence: F1 scores exceed 80% with 20% architecture; linear probing literature supports behavioral features' linear structure.

### Mechanism 3
- Claim: Attacking a surrogate classifier yields higher attack success rates against the full LLM than direct attacks.
- Mechanism: Direct misclassification objectives expand adversarial search space beyond heuristic likelihood maximization, while surrogate's lower dimensionality reduces optimization difficulty.
- Core assumption: Adversarial inputs that fool surrogate transfer to full model due to boundary approximation.
- Evidence: 50% Llama 2 surrogate achieves 70% ASR vs 22% baseline; transfer attacks are well-documented in adversarial ML literature.

## Foundational Learning

- **Linear Probing**: Training a linear classifier on frozen representations to assess whether features are linearly separable. Why needed: The extraction method relies on training linear probes on intermediate representations. Quick check: If you observe high training accuracy but low cross-dataset F1, what does this suggest about the representation?

- **Silhouette Score**: Metric measuring cluster separation quality (ranges from -1 to 1). Why needed: Used to detect where safe/unsafe representations separate in the model. Quick check: A silhouette score of 0.1 indicates what about cluster overlap?

- **Transfer Attacks**: Adversarial inputs generated on one model that succeed on a different model. Why needed: The core result depends on adversarial inputs crafted on surrogate transferring to the full LLM. Quick check: Why might an attack transfer from a 50% surrogate but not from a 20% surrogate?

## Architecture Onboarding

- **Component map**: Input Prompt → [Structure: Decoders 1..δ] → Final Token Embedding (R^d) → [Linear Head: d→1] → Safety Score → Threshold → Safe/Unsafe Label
- **Critical path**: 1) Collect labeled dataset via rule R, 2) For each candidate size δ, extract embeddings and train linear head with 5-fold CV, 3) Select best δ via cross-dataset F1, 4) Run GCG attack with misclassification objective on surrogate, measure transfer ASR to full LLM
- **Design tradeoffs**: Candidate size δ affects VRAM/runtime vs. information capture; structure start point at decoder 1 may include redundant layers; linear head chosen to avoid overfitting while preserving gradient paths
- **Failure signatures**: High benign F1 but low transfer ASR suggests overfitting; low silhouette scores (<0.2) indicate alignment may not embed clear safety classifier; transfer ASR drops at high δ suggest later layers dilute safety information
- **First 3 experiments**: 1) Replicate silhouette score analysis to confirm separability exists, 2) Train candidate classifiers at δ = 20%, 40%, 60%, 80% and evaluate on AdvBench/OR-Bench to identify plateau, 3) Run GCG with misclassification objective on best surrogate and compare ASR/runtime against baseline GCG on full LLM

## Open Questions the Paper Calls Out

1. Does extracting classifiers from non-initial, intermediate decoder layers (i > 1) yield a more precise or efficient surrogate model? The paper notes this as future work, as current method assumes classifier spans from first layer.

2. How does non-deterministic generation (temperature > 0) affect the decision boundary of the extracted safety classifier? The paper sets temperature to 0 for reproducibility but acknowledges studying temperature would provide more precise understanding.

3. Can adversarial inputs generated from a surrogate classifier of one LLM transfer effectively to a different target LLM? The paper identifies black-box scenarios as limitation and suggests studying transferability across different source and target LLMs.

## Limitations
- Method's effectiveness depends critically on alignment methods consistently embedding linearly separable safety boundaries, but only validated across four similar chat-oriented models
- Attack transferability assumes surrogate's safety boundary sufficiently approximates full model's boundary, without systematic investigation of failure modes
- Labeling mechanism relies on third-party rejection classifier, introducing potential noise and making it difficult to assess dataset representativeness
- Computational efficiency gains must be weighed against initial surrogate extraction cost and potential reduction in absolute attack performance

## Confidence
- **High**: Empirical observation of non-monotonic silhouette scores with peaks around 40-70% decoder position across all tested models is well-supported by Figure 2
- **Medium**: Claim that attacking surrogate classifiers achieves higher ASR than attacking full LLMs is supported by experimental results but could be strengthened by broader exploration
- **Low**: Generalizability to unlearning-based alignment approaches and future alignment paradigms remains speculative with limited empirical validation

## Next Checks
1. Test surrogate extraction method on models using different alignment approaches (particularly unlearning-based methods like Zephyr RMU) to validate generalizability across alignment techniques

2. Systematically analyze distribution of successful adversarial inputs generated by surrogate attacks versus direct attacks, focusing on inputs far from training distribution to understand when transfer attacks fail

3. Conduct ablation study varying quality and granularity of labeling mechanism, including manual labeling of examples and comparison with different rejection classifiers to quantify impact of label noise on surrogate training and attack performance