---
ver: rpa2
title: Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using
  Deep Reinforcement Learning
arxiv_id: '2509.25284'
source_url: https://arxiv.org/abs/2509.25284
tags:
- policy
- power
- learning
- allocation
- resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses dynamic resource allocation in heterogeneous
  wireless networks, focusing on jointly optimizing transmit power, bandwidth, and
  scheduling under varying user loads and channel conditions. A deep reinforcement
  learning framework is proposed to balance throughput, energy efficiency, and fairness
  through a multi-objective reward function.
---

# Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.25284
- Source URL: https://arxiv.org/abs/2509.25284
- Reference count: 40
- Primary result: PPO achieves 6000+ cumulative rewards vs 4000-5000 for TD3 in HetNet resource allocation

## Executive Summary
This study proposes a deep reinforcement learning framework for dynamic resource allocation in heterogeneous wireless networks, jointly optimizing transmit power, bandwidth, and scheduling under varying user loads and channel conditions. The framework employs a multi-objective reward function balancing throughput, energy efficiency, and fairness, implemented using Twin Delayed Deep Deterministic Policy Gradient (TD3) and Proximal Policy Optimization (PPO) algorithms. Testing against three heuristic baselines across multiple network scenarios using real base station locations demonstrates that both DRL algorithms significantly outperform heuristic methods, with PPO achieving superior overall performance and fairness while TD3 exhibits faster initial convergence.

## Method Summary
The research formulates resource allocation as a Markov Decision Process where an agent learns to allocate transmit power, bandwidth, and user scheduling based on network state observations. Two state-of-the-art DRL algorithms (TD3 and PPO) are implemented to handle the continuous action space of resource allocation. The framework uses real base station locations from Cape Town, South Africa, and simulates various network scenarios with different user densities and environmental conditions. A multi-objective reward function combines weighted sum-throughput, power consumption penalties, and Jain's fairness index to guide learning.

## Key Results
- Both TD3 and PPO DRL algorithms significantly outperform three heuristic baselines in optimizing resource allocation
- PPO achieves higher cumulative rewards (6000+) and better fairness compared to TD3 (4000-5000)
- TD3 demonstrates faster initial convergence but PPO shows more stable long-term performance
- The multi-objective reward function successfully balances competing network goals without compromising overall system performance

## Why This Works (Mechanism)

### Mechanism 1: Multi-Objective Reward Shaping
The DRL agent balances competing network goals by optimizing a weighted scalar reward function that linearly combines throughput, power consumption penalty, and fairness index. This guides the policy to find a Pareto optimal point between conflicting objectives, with specific coefficients (κ=1.0, β=0.01, φ=0.96) reflecting the desired network operating point.

### Mechanism 2: Continuous Action Mapping via Normalization
The system enables stable learning in continuous high-dimensional action spaces by normalizing agent outputs to [0, 1] before mapping them to physical resource limits through affine transformation. This prevents exploding gradients and removes physical unit dependency from neural network layers, allowing gradient-based optimization within bounded ranges.

### Mechanism 3: PPO Clipped Surrogate Objective
PPO achieves higher cumulative rewards and stability by constraining policy update steps through a clipped surrogate objective that restricts the ratio of new to old policy. This prevents destructive large updates, forcing the agent to explore more methodically and providing greater stability for long-term convergence compared to off-policy methods like TD3.

## Foundational Learning

**Concept: Markov Decision Processes (MDPs) in Wireless**
Why needed: The resource allocation problem is formulated as an MDP where the state (SINR, location) must capture all necessary information for actions (power, bandwidth) to be effective.
Quick check: Does the current state vector fully represent the interference landscape, or is temporal history required to estimate channel velocity?

**Concept: Jain's Fairness Index**
Why needed: This mathematical definition of "fairness" determines if the agent optimizes for few high-speed users or equitable service across all users.
Quick check: If one user receives 0 throughput, what happens to the Jain's Index value, and how would that penalize the agent?

**Concept: Actor-Critic Architectures**
Why needed: Both PPO and TD3 use an Actor (proposing actions) and a Critic (evaluating actions), and understanding their interaction is vital for debugging divergence.
Quick check: In TD3, why are there two Critic networks but only one Actor? (Answer: To mitigate overestimation bias).

## Architecture Onboarding

**Component map:** Environment (BS coordinates, Channel Model) -> State (Interference, User Locs) -> Agent (PPO/TD3 Actor) -> Normalized Action (Power, BW, Schedule) -> Translator (affine transformation) -> Physical units -> Reward Engine (multi-objective scores) -> Update weights

**Critical path:** The affine transformation of the action vector is critical; if this mapping is misconfigured (e.g., Pmax is too low), the agent will learn policies that are physically valid but operationally useless.

**Design tradeoffs:** The framework explicitly trades off Sample Efficiency (TD3) vs. Long-term Stability/Optimality (PPO). Choose TD3 for static topologies requiring quick training; choose PPO for varying network loads needing robust fairness.

**Failure signatures:**
- Power Collapse: Agent sets all power to minimum to maximize negative power reward term, ignoring throughput
- Scheduling Lock: Agent schedules same users repeatedly due to insufficient exploration noise or inadequate fairness weight

**First 3 experiments:**
1. Reward Ablation: Run agent with β=0 (ignore power cost) to verify "Greedy" baseline behavior, ensuring learning loop functionality
2. Algorithm Swap: Train TD3 and PPO on single seed for 1000 episodes; plot variance to confirm PPO is more stable but slower
3. Topology Stress Test: Move 50 users from "Uniform" to "Hotspot" cluster to verify agent adapts bandwidth allocation locally without manual re-tuning

## Open Questions the Paper Calls Out
The paper identifies several future research directions including incorporating user mobility effects, extending to multi-agent scenarios, and testing generalization across different base station topologies. These questions remain unresolved as the current study assumes static user positions and centralized control architecture.

## Limitations
The study focuses on indoor office environments and uniform user distributions, limiting generalizability to rural deployments or mobile scenarios. The multi-objective reward function requires careful tuning of weighting coefficients to avoid reward hacking. While TD3 shows faster convergence, it may struggle with highly dynamic topologies where PPO's stability becomes critical.

## Confidence
- **High Confidence:** PPO's superior fairness and cumulative reward performance (supported by multiple experimental runs and quantitative metrics)
- **Medium Confidence:** TD3's faster initial convergence claim (based on limited episodes and single topology)
- **Low Confidence:** Generalization to non-uniform user distributions and mobile scenarios (not experimentally validated)

## Next Checks
1. Cross-Topology Robustness: Test both algorithms on heterogeneous user distributions (hotspots, mobile users) to validate adaptability claims
2. Reward Sensitivity Analysis: Systematically vary κ, β, φ weights to identify stable operating regions and potential reward hacking behaviors
3. Real-World Deployment Readiness: Evaluate model performance under realistic constraints including channel estimation errors and delayed feedback to assess practical viability