---
ver: rpa2
title: How human is the machine? Evidence from 66,000 Conversations with Large Language
  Models
arxiv_id: '2510.07321'
source_url: https://arxiv.org/abs/2510.07321
tags:
- human
- llms
- studies
- humans
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tested whether Large Language Models (LLMs) replicate
  human decision-making biases. Ten experiments with 66,000 LLM conversations examined
  six well-known biases (availability, representativeness, endowment, anchoring, transaction
  utility, framing).
---

# How human is the machine? Evidence from 66,000 Conversations with Large Language Models

## Quick Facts
- arXiv ID: 2510.07321
- Source URL: https://arxiv.org/abs/2510.07321
- Reference count: 18
- Primary result: Large Language Models do not replicate human decision-making biases in direction or magnitude across six established paradigms

## Executive Summary
This study tested whether Large Language Models (LLMs) can replicate human decision-making biases by conducting ten experiments with 66,000 LLM conversations across six well-established bias paradigms. The research examined availability, representativeness, endowment, anchoring, transaction utility, and framing biases. Contrary to expectations, LLMs consistently failed to produce responses matching established human biases in both direction and magnitude. Instead, four distinct deviation patterns emerged: bias attenuation (reducing or eliminating biases), bias amplification (exaggerating biases), reverse biases (showing opposite biases to humans), and response inconsistency (within the same model over time and across models).

## Method Summary
The researchers conducted ten experiments using six established bias paradigms with 66,000 LLM conversations. They tested LLMs against established human bias patterns from behavioral economics literature, measuring both the direction and magnitude of responses. The study examined multiple LLMs including GPT-4, GPT-3.5, and Claude across different conversation contexts. Each experiment was designed to elicit specific cognitive biases that have been well-documented in human decision-making research.

## Key Results
- LLMs failed to replicate human biases in all six tested paradigms, showing no matching direction or magnitude
- Four distinct deviation patterns emerged: bias attenuation, bias amplification, reverse biases, and response inconsistency
- The magnitude of deviations suggests LLMs cannot accurately replace humans for predicting consumer behavior

## Why This Works (Mechanism)
The mechanism behind these findings appears to stem from fundamental differences in how LLMs process information versus human cognitive architecture. LLMs rely on pattern matching from training data and statistical inference rather than the intuitive, heuristic-based processing that characterizes human decision-making under uncertainty. When faced with bias-eliciting scenarios, LLMs may default to logical optimization or surface-level pattern recognition that bypasses the automatic, System 1 thinking that produces human biases. The absence of embodied experience and contextual framing that humans use to interpret situations may also contribute to these systematic deviations.

## Foundational Learning
- Cognitive bias paradigms: Understanding classic behavioral economics experiments is crucial for designing tests that reliably elicit human biases and for interpreting LLM responses
- System 1 vs System 2 processing: Knowing the distinction between intuitive (automatic) and deliberative (controlled) thinking helps explain why LLMs, which operate more like System 2, may fail to replicate human biases
- Statistical learning vs. cognitive heuristics: Recognizing that LLMs learn through statistical pattern matching rather than developing cognitive shortcuts helps explain systematic deviations from human behavior
- Ecological validity in behavioral experiments: Understanding how experimental context affects responses is essential for interpreting whether LLM behavior reflects fundamental differences or methodological artifacts
- Stochastic generation variability: Knowledge of how temperature and sampling affect LLM outputs is critical for distinguishing between true behavioral instability and random variation

## Architecture Onboarding

Component map: User input -> Tokenizer -> Embedding layer -> Transformer blocks -> Output layer -> LLM response

Critical path: User prompt → Model inference → Response generation → Bias measurement

Design tradeoffs: Accuracy in bias replication vs. computational efficiency; conversational format vs. experimental control; single-shot vs. repeated measurements

Failure signatures: Systematic deviation from human bias patterns; inconsistent responses across model versions; temperature-dependent behavior changes; context-dependent bias emergence

First experiments:
1. Test multiple temperature settings (0.0, 0.7, 1.0) on the same bias paradigms to isolate stochastic effects
2. Conduct repeated measurements with the same model to assess within-model consistency over time
3. Compare conversational responses to structured survey formats to evaluate format effects on bias expression

## Open Questions the Paper Calls Out
The study raises questions about whether the absence of human-like biases reflects fundamental differences in LLM cognition or methodological artifacts. Key uncertainties include whether prompt design, temperature settings, or the artificial nature of conversational testing compared to real-world decision contexts explain the deviations. The finding of response inconsistency within models over time also raises questions about whether observed deviations are systematic or due to stochastic generation variability.

## Limitations
- Reliance on conversational format may not capture stable behavioral patterns compared to real-world decision contexts
- Single conversation per participant per LLM limits ability to assess consistency and rule out random variation
- Uncertainty about whether deviations reflect fundamental cognitive differences or methodological artifacts
- Limited validation across diverse real-world decision-making scenarios beyond laboratory-style bias tests

## Confidence

High confidence in the empirical finding that LLMs do not replicate human biases in magnitude and direction across the tested paradigms
Medium confidence in interpreting deviations as evidence that LLMs cannot replace humans for behavioral prediction, due to uncertainty about ecological validity
Medium confidence in the categorization of deviation types (attenuation, amplification, reversal, inconsistency) as distinct patterns

## Next Checks

1. Replicate key experiments across multiple temperature settings and with repeated interactions per model to assess whether observed inconsistencies reflect true behavioral instability versus stochastic generation effects

2. Conduct parallel experiments using real-world decision-making tasks (e.g., actual purchase scenarios) rather than conversational simulations to test ecological validity

3. Compare LLM responses to actual human decision data from the same bias paradigms using standardized effect size metrics to quantify the gap in behavioral similarity more precisely