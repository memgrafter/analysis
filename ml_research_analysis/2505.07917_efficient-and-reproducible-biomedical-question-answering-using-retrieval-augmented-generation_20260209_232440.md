---
ver: rpa2
title: Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented
  Generation
arxiv_id: '2505.07917'
source_url: https://arxiv.org/abs/2505.07917
tags:
- retrieval
- biomedical
- documents
- system
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a scalable and reproducible Retrieval-Augmented\
  \ Generation (RAG) system for biomedical question-answering, addressing challenges\
  \ in accuracy, efficiency, and scalability. A hybrid retrieval approach combining\
  \ BM25 (lexical retrieval) with MedCPT\u2019s cross-encoder (semantic reranking)\
  \ is implemented and evaluated on PubMed datasets."
---

# Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2505.07917
- Source URL: https://arxiv.org/abs/2505.07917
- Reference count: 17
- Primary result: Hybrid BM25 + MedCPT retrieval achieves 0.90 accuracy and 0.90 recall with 1.91s response time on biomedical QA.

## Executive Summary
This study presents a scalable and reproducible Retrieval-Augmented Generation (RAG) system for biomedical question-answering, addressing challenges in accuracy, efficiency, and scalability. A hybrid retrieval approach combining BM25 (lexical retrieval) with MedCPT’s cross-encoder (semantic reranking) is implemented and evaluated on PubMed datasets. Results show that retrieving 50 documents with BM25 before reranking with MedCPT optimally balances accuracy (0.90), recall (0.90), and response time (1.91s), with BM25 retrieval time remaining stable at 82ms. The system outperforms standalone retrieval methods in end-to-end performance and is fully reproducible with open-source code. Limitations include reliance on OpenAI’s GPT-3.5 and limited retriever/database evaluations. Future work will explore additional retrievers, open-source LLMs, and real-time biomedical applications.

## Method Summary
The system implements a two-stage hybrid retrieval pipeline: BM25 retrieves top-k documents via Elasticsearch, then MedCPT’s cross-encoder reranks them, passing the top 10 to GPT-3.5-turbo for answer generation. The pipeline is evaluated on PubMed (24M documents) and BIOASQ Task-B benchmark. Documents are indexed in Elasticsearch for BM25 and FAISS for dense vectors. Retrieval depth is optimized at 50 documents to balance accuracy (0.90) and latency (1.91s). The system uses structured JSON prompts to cite PMIDs and ensure reproducibility.

## Key Results
- Hybrid BM25 + MedCPT retrieval achieves 0.90 accuracy and 0.90 recall on BIOASQ Task-B.
- Retrieving 50 documents optimizes the trade-off between accuracy and latency (1.91s total).
- BM25 retrieval time remains constant at 82ms regardless of k.
- Elasticsearch excels in sparse retrieval; FAISS optimizes dense vector search.

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Hybrid Retrieval Reduces Search Space Before Expensive Neural Reranking
- Claim: A BM25 first-stage retriever followed by MedCPT cross-encoder reranking achieves superior end-to-end accuracy compared to either method alone.
- Mechanism: BM25 efficiently filters millions of documents to a smaller candidate set (e.g., 50 docs) using inverted indexes and term-frequency scoring. The neural reranker then applies computationally expensive query-document contextual interactions only on this reduced set, improving precision without incurring full-corpus neural costs.
- Core assumption: BM25's lexical matching captures most relevant documents in top-k results; semantic reranking corrects ranking errors within this subset.
- Evidence anchors:
  - [abstract] "A hybrid retrieval pipeline combining BM25 and MedCPT is implemented, optimizing retrieval depth (50 documents) to balance accuracy (0.90), recall (0.90), and response time (1.91s)."
  - [Page 1] "BM25 first retrieves k candidate documents, which are then reranked by MedCPT's cross-encoder. Only relevant documents are passed to the LLM."
  - [corpus] Related work (RAG-BioQA, DeepRAG) confirms hybrid RAG for biomedical QA is an active research direction, though specific BM25+MedCPT configurations are not externally validated in provided corpus.
- Break condition: If BM25 recall is too low (relevant documents excluded from top-k), reranking cannot recover them. Expect degraded accuracy when query vocabulary diverges significantly from document terminology.

### Mechanism 2: Retrieval Depth Creates Diminishing Returns Beyond 50 Documents
- Claim: Increasing BM25 retrieval depth beyond 50 documents degrades accuracy while increasing latency.
- Mechanism: BM25 response time remains constant (~82ms) because it ranks the full corpus regardless of k. However, reranking time scales linearly with k. Beyond 50 documents, the reranker introduces more noise (lower-relevance candidates), reducing final ranking quality.
- Core assumption: The reranker's relevance scoring degrades when processing marginally relevant documents; the LLM context window may also be diluted by lower-quality context.
- Evidence anchors:
  - [Page 4, Table IV] Retrieving 50 docs yields accuracy 0.90; 100 docs drops to 0.87 with total time increasing from 1.91s to 2.62s.
  - [Page 4] "Increasing the number of retrieved documents leads to marginal accuracy improvements but significantly increases the rerank time."
  - [corpus] Weak external validation—neighbor papers do not report specific retrieval depth thresholds.
- Break condition: If reranker is highly robust to noise or context window is very large, deeper retrieval may not degrade accuracy. Test empirically when changing reranker models.

### Mechanism 3: Storage System Choice Determines Indexing and Query Latency Profiles
- Claim: Elasticsearch optimizes sparse retrieval latency; FAISS optimizes dense vector search; MongoDB underperforms for both in self-hosted configurations.
- Mechanism: Elasticsearch uses inverted indexes optimized for BM25 scoring. FAISS uses vector quantization for approximate nearest neighbor search. MongoDB's self-hosted version lacks native BM25 and efficient semantic search, resulting in slower queries despite flexible document storage.
- Core assumption: Latency bottlenecks are in retrieval, not embedding generation or LLM inference.
- Evidence anchors:
  - [Page 2, Table I] Elasticsearch BM25: 82ms response; FAISS L2: 657ms; MongoDB TF-IDF: 26.4s response.
  - [Page 2] "Elasticsearch excels in full-text retrieval but is less efficient for semantic vector search, which FAISS optimizes for."
  - [corpus] No direct comparison in neighbor papers; system-level benchmarks remain limited per abstract.
- Break condition: If MongoDB Atlas Search (cloud) is used, BM25 performance may improve—this was excluded from study. Cloud deployment trade-offs differ.

## Foundational Learning

- Concept: **BM25 (Best Matching 25)**
  - Why needed here: Core sparse retrieval algorithm used for first-stage document filtering. Understanding its probabilistic scoring (term frequency, inverse document frequency, document length normalization) is essential for tuning retrieval parameters.
  - Quick check question: Why does BM25 penalize very long documents more than TF-IDF?

- Concept: **Cross-Encoder Reranking**
  - Why needed here: MedCPT's cross-encoder jointly processes query-document pairs for fine-grained relevance scoring. Unlike bi-encoders, it captures query-document interactions but is too expensive for full-corpus search.
  - Quick check question: What is the computational complexity difference between a bi-encoder and cross-encoder for ranking N documents?

- Concept: **RAG End-to-End Evaluation Metrics**
  - Why needed here: System success is measured by retrieval recall/precision and answer correctness (accuracy, F1). Understanding how retrieval quality propagates to generation quality is critical for debugging.
  - Quick check question: If retrieval recall is 0.27 but end-to-end accuracy is 0.83, what might explain this discrepancy?

## Architecture Onboarding

- Component map: PubMed documents → preprocessing → dual indexing (Elasticsearch for BM25, FAISS for dense vectors) → User query → BM25 retrieval (k docs) → MedCPT cross-encoder reranking (top 10) → LLM context assembly → GPT-3.5 generation → response with PMID citations

- Critical path: BM25 retrieval latency (82ms) + MedCPT reranking (scales with k) + LLM API (~1.07s) = total response time. Reranking is the primary variable cost.

- Design tradeoffs:
  - Retrieval depth (k): Higher k increases recall potential but increases reranking time and may reduce accuracy due to noise.
  - Storage: Elasticsearch chosen for sparse despite slower indexing (156 min) due to query performance; FAISS for dense despite 657ms latency when semantic search required.
  - LLM: GPT-3.5 chosen for cost; GPT-4 showed no significant quality improvement in limited testing.

- Failure signatures:
  - Accuracy drops when k > 50: Likely reranker noise dilution.
  - Very high latency (>3s): Check reranking depth or API latency spikes.
  - Low recall (<0.3): BM25 vocabulary mismatch; consider query expansion or hybrid with dense first-stage.

- First 3 experiments:
  1. **Baseline retrieval depth sweep**: Test k ∈ {20, 50, 100} on your data subset. Measure accuracy, recall, F1, and latency. Confirm 50-doc optimum holds for your corpus.
  2. **Reranker ablation**: Compare MedCPT cross-encoder vs. no reranking vs. alternative rerankers (e.g., BioBERT-based). Measure end-to-end accuracy to validate reranking contribution.
  3. **Storage latency benchmark**: Index 100K documents into Elasticsearch and FAISS. Measure query P50/P99 latency under concurrent load to validate system scalability assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source Large Language Models (LLMs) replace GPT-3.5 in this RAG architecture while maintaining the reported 0.90 accuracy and ensuring full reproducibility?
- Basis in paper: [explicit] The authors identify the reliance on OpenAI’s GPT-3.5 as a limitation due to reproducibility challenges from model updates, explicitly stating future work should focus on "integrating open-source LLMs."
- Why unresolved: The study relied solely on the closed-source GPT-3.5 API; it is unknown if open-weight models can handle the complex prompt engineering and synthesis requirements at the same performance level.
- What evidence would resolve it: A comparative evaluation of open-source biomedical LLMs (e.g., BioMistral, Llama-3-med) using the identical hybrid retrieval pipeline on the BioASQ benchmark.

### Open Question 2
- Question: How do alternative vector databases compare to the Elasticsearch/FAISS combination in terms of indexing efficiency when scaling beyond the observed 24M documents?
- Basis in paper: [explicit] The paper notes that "retriever and database system evaluations remain limited" and explicitly calls for future efforts to "evaluate alternative databases for indexing efficiency."
- Why unresolved: While Elasticsearch (156 min) and FAISS (41 min) were tested, the study did not assess newer specialized vector stores that might offer better write throughput or compression for dense medical embeddings.
- What evidence would resolve it: Benchmarking indexing speed and latency of systems like Weaviate, Milvus, or Qdrant against the 24M PubMed corpus using the same MedCPT embeddings.

### Open Question 3
- Question: Can the 1.91s end-to-end latency be reduced to support real-time clinical decision support without degrading the optimal 0.90 accuracy/recall balance?
- Basis in paper: [explicit] The authors conclude that future efforts should focus on "enabling real-time biomedical applications," implying the current latency may be a bottleneck for immediate clinical use.
- Why unresolved: The current bottleneck is identified as the MedCPT cross-encoder reranking step; it is unclear if approximate nearest neighbor search or optimized reranking techniques can maintain accuracy while lowering the time cost.
- What evidence would resolve it: A Pareto frontier analysis varying reranking strategies (e.g., lighter cross-encoders or approximate search) to find configurations below 1.0s latency with minimal accuracy loss.

## Limitations

- Limited external validation of the BM25+MedCPT configuration and optimal 50-document retrieval depth.
- Unspecified BM25 hyperparameters (k1, b) and MedCPT relevance score threshold limit reproducibility.
- Only one retriever combination and one LLM (GPT-3.5) are evaluated, with no comprehensive comparison to alternatives.

## Confidence

- **High Confidence**: Retrieval depth of 50 documents optimally balances accuracy and latency for the tested configuration. This is directly supported by ablation experiments (accuracy 0.90, recall 0.90, time 1.91s).
- **Medium Confidence**: The 82ms BM25 retrieval time and constant scaling behavior are reliable for the tested corpus size and index configuration. However, performance may vary with corpus size or hardware.
- **Medium Confidence**: Elasticsearch is superior for sparse retrieval and FAISS for dense vectors in the tested setup. External validation is limited, but system-level benchmarks align with this conclusion.
- **Low Confidence**: GPT-3.5-turbo is the optimal LLM choice. This is based on limited testing; broader LLM comparisons are absent.

## Next Checks

1. **Retrieval Depth Sensitivity Test**: Replicate the k-document ablation (20, 50, 100) on a 10% PubMed subset. Measure accuracy, recall, F1, and latency to confirm the 50-document optimum holds for your corpus.

2. **Reranker Ablation**: Compare MedCPT cross-encoder against no reranking and alternative biomedical rerankers (e.g., BioBERT-based) on BIOASQ Task-B. Measure end-to-end accuracy to validate reranking's contribution.

3. **Storage Latency Benchmark**: Index 100K PubMed documents into Elasticsearch and FAISS. Measure P50/P99 query latency under concurrent load to validate scalability assumptions and identify bottlenecks.