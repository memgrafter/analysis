---
ver: rpa2
title: Next Tokens Denoising for Speech Synthesis
arxiv_id: '2507.22746'
source_url: https://arxiv.org/abs/2507.22746
tags:
- arxiv
- speech
- diffusion
- audio
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Dragon-FM, a text-to-speech (TTS) system that
  unifies autoregressive and flow-matching approaches to address the limitations of
  each paradigm. It uses a high-quality 48 kHz codec with 12.5 tokens per second to
  compress audio into discrete tokens, enabling efficient modeling while maintaining
  fidelity.
---

# Next Tokens Denoising for Speech Synthesis

## Quick Facts
- **arXiv ID:** 2507.22746
- **Source URL:** https://arxiv.org/abs/2507.22746
- **Reference count:** 10
- **Primary result:** Dragon-FM achieves high-quality zero-shot podcast generation with FAD 1.9-2.6 and TNFE 2-4, outperforming VALL-E and E2.

## Executive Summary
This paper introduces Dragon-FM, a text-to-speech (TTS) system that unifies autoregressive (AR) and flow-matching approaches to overcome the limitations of each paradigm. The system uses a high-quality 48 kHz codec with 12.5 tokens per second to compress audio into discrete tokens, enabling efficient modeling while maintaining fidelity. Dragon-FM processes audio in chunks, using autoregressive modeling across chunks and parallel flow-matching within each chunk to reduce iteration steps and improve speed. Experimental results show that Dragon-FM achieves high-quality zero-shot podcast generation with significantly reduced latency and iteration steps compared to existing TTS systems.

## Method Summary
Dragon-FM unifies autoregressive and flow-matching approaches in a chunked processing framework. The system first encodes 48 kHz audio using a transformer-based encoder and compresses it into discrete tokens via Finite Scalar Quantization (FSQ) at 12.5 Hz. An acoustic model processes 2-second chunks (25 tokens) autoregressively, with parallel flow-matching denoising (Mean Flow) within each chunk. The decoder reconstructs audio using a causal transformer and ISTFT. The system is trained on 60,000 hours of English podcast audio with ASR-generated transcriptions, optimized using Adam with EMA smoothing.

## Key Results
- **FrÃ©chet Audio Distance (FAD):** 1.9-2.6, depending on configuration
- **Total Number of Function Evaluations (TNFE):** 2-4, compared to 150 for VALL-E and 32 for E2
- **Codec Performance:** Speaker Similarity (SIM) 0.916 and Word Error Rate (WER) 2.74 at 12.5 Hz

## Why This Works (Mechanism)
Dragon-FM works by combining the global coherence of autoregressive modeling with the parallel efficiency of flow-matching. By processing audio in chunks, the system maintains context across segments while enabling parallel computation within each chunk. The continuous denoising models predict discrete tokens via finite scalar quantization, demonstrating the potential for multimodal generative modeling. The use of a high-quality codec with 12.5 tokens per second ensures efficient compression without sacrificing audio fidelity.

## Foundational Learning

**Finite Scalar Quantization (FSQ):** A quantization method that maps continuous values to discrete tokens using scalar projections. *Why needed:* Enables efficient compression of audio signals while maintaining quality. *Quick check:* Verify codebook size and projection bounds match target specifications.

**Flow-Matching:** A generative modeling approach that learns the flow between distributions using denoising objectives. *Why needed:* Provides parallel generation capability within chunks. *Quick check:* Confirm step reduction from 150 to 2-4 TNFE.

**Chunked Processing:** Processing data in fixed-size segments rather than token-by-token. *Why needed:* Balances global coherence with computational efficiency. *Quick check:* Validate chunk size (2 seconds) achieves optimal trade-off.

## Architecture Onboarding

**Component Map:** Audio -> Encoder -> FSQ -> Dragon-FM (AR+FM) -> Decoder -> Audio

**Critical Path:** Text/Prompt -> Dragon-FM -> Discrete Tokens -> Decoder -> Speech Output

**Design Tradeoffs:** The system trades some computational complexity in the acoustic model for significant gains in generation speed and quality. The chunk size of 2 seconds represents a balance between maintaining global context and enabling parallel processing.

**Failure Signatures:** Low reconstruction quality indicates issues with codec training or FSQ implementation. High latency suggests problems with the flow-matching integration or chunk processing.

**First Experiments:**
1. Train FSQ codec alone and validate SIM/WER metrics
2. Test chunk processing mechanism with dummy data
3. Evaluate end-to-end generation on held-out samples

## Open Questions the Paper Calls Out

**Multimodal Extension:** Can Dragon-FM's unified discrete-continuous approach be extended to large-scale multimodal LLMs for high-quality synthesis across diverse data modalities beyond speech? The paper suggests this as future work but has not yet tested scalability to video, image, or general text generation tasks.

**Embedding Dependence:** To what extent does the intrinsic classification capability of continuous denoising models depend on specific FSQ embeddings versus the model architecture itself? The paper notes this is a key assumption but hasn't isolated whether performance is general or heavily reliant on FSQ properties.

**Chunk Size Limits:** What are the theoretical or empirical limits on chunk size regarding the trade-off between global coherence (AR) and generation speed (parallel flow-matching)? The paper selects 2 seconds but hasn't explored upper bounds where bidirectional attention might become prohibitive.

## Limitations

- Architectural hyperparameters are not fully specified, requiring assumptions that may impact performance
- Mean Flow integration details are referenced but not explicitly defined in the provided text
- FSQ projection bounds and precise dimensionality details beyond "100x5" codebook size are missing

## Confidence

- **High Confidence:** Core architectural concept and reported metrics (FAD 1.9-2.6, TNFE 2-4, SIM 0.916, WER 2.74) are consistent and plausible
- **Medium Confidence:** Training details are standard but lack precise architectural specifications
- **Low Confidence:** Missing precise numerical values for architectural dimensions and detailed Mean Flow implementation

## Next Checks

1. **Validate Codec Reconstruction:** Train the FSQ codec with 48kHz encoder-decoder architecture and MPD/MS-STFT discriminators. Evaluate alone to verify SIM 0.916 and WER 2.74 at 12.5 Hz.

2. **Validate AR-FM Chunk Processing:** Implement Dragon-FM acoustic model and test chunk processing mechanism. Measure TNFE on validation set to confirm 2-4 steps.

3. **Validate End-to-End Quality:** Perform FAD evaluation on standard zero-shot TTS benchmark. Compare to reported 1.9-2.6 range to confirm audio quality.