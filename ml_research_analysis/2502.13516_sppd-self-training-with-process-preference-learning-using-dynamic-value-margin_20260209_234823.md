---
ver: rpa2
title: 'SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin'
arxiv_id: '2502.13516'
source_url: https://arxiv.org/abs/2502.13516
tags:
- step
- sppd
- arxiv
- preference
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving numerical and logical
  reasoning capabilities in Large Language Models (LLMs), which existing methods struggle
  with due to limitations in step-wise correctness, dependence on stronger models
  for distillation, and high computational costs. The proposed method, SPPD (Self-training
  with Process Preference Learning using Dynamic Value Margin), introduces a novel
  approach that leverages a process-based Markov Decision Process (MDP) and Bellman
  optimality equation to derive dynamic value margins for step-level preference optimization.
---

# SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin

## Quick Facts
- arXiv ID: 2502.13516
- Source URL: https://arxiv.org/abs/2502.13516
- Reference count: 19
- Key outcome: Improves mathematical reasoning in 7B LLMs using dynamic value margins without distillation from stronger models

## Executive Summary
SPPD addresses the challenge of enhancing numerical and logical reasoning in Large Language Models by introducing a novel self-training approach that leverages dynamic value margins derived from step-level process preference learning. The method uses tree-based self-sampling to generate preference pairs from the model's own responses, avoiding dependence on stronger teacher models. By theoretically proving equivalence to on-policy policy gradient methods under specific reward constraints, SPPD achieves state-of-the-art performance on mathematical benchmarks while demonstrating robust generalization capabilities.

## Method Summary
SPPD implements a three-stage training pipeline that combines tree-based self-sampling with process preference learning. The method first generates preference pairs through a tree search process where the model's own logits are used to create diverse reasoning trajectories, which are then scored by a Process Reward Model (PRM) to extract step-level preferences. Sentence-level training (SFT and DPO) serves as a curriculum warm-up before the main SPPD step-level training, which uses dynamic value margins based on PRM-estimated value differences. The dynamic margin adjusts per preference pair using the Bellman optimality equation, theoretically aligning offline preference optimization with on-policy policy gradients.

## Key Results
- Achieves state-of-the-art performance on in-domain mathematical benchmarks (GSM8K, MATH500) and out-domain tests (Gaokao2023, OCW, OlympiadBench)
- Demonstrates significant improvements over baselines without requiring distillation from stronger models
- Shows robust generalization capabilities across different reasoning domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic value margins improve upon static-margin or no-margin step-level preference optimization
- Mechanism: The method frames step-level reasoning as a Markov Decision Process (MDP). It uses the Bellman optimality equation to show the step reward contains a "value gain" term ($V^*(s_{t+1}) - V^*(s_t)$). By incorporating this term into the Bradley–Terry preference model, the loss margin dynamically adjusts per preference pair based on PRM-estimated value differences, rather than using a fixed scalar margin or zero
- Core assumption: A Process Reward Model (PRM) is a sufficiently accurate proxy for the optimal value function $V^*$ at each step, and PRM scoring noise can be mitigated by filtering pairs with a score-difference threshold (e.g., >0.5)
- Evidence anchors: [abstract]: "derives dynamic value margins for step-level preference optimization." [section 4.1]: Theorem 4.2 loss formula $L_{\text{step-dpo}}$ includes $-(V^*(s^w_{t+1}) - V^*(s^l_{t+1}))$. [section 6.3]: Ablation shows SPPD (dynamic) outperforms $\gamma=0$ (no margin) and fixed-margin step-DPO

### Mechanism 2
- Claim: Tree-based self-sampling can provide on-policy-like step preference data without distillation from stronger models
- Mechanism: A tree search with common prefixes is constructed by "Selection, Expansion, Collection, and Scoring" using the model's own logits. Nodes are selected via normalized log-prob scores; branches are expanded; trajectories are collected and scored by PRM to form step-level preference pairs. This preserves the model's output distribution better than independent CoT sampling
- Core assumption: The inference model $\pi_{\text{infer}}$ (set to $\pi_{\text{ref}}$) is representative enough to generate diverse yet policy-consistent partial trajectories for constructing preference pairs
- Evidence anchors: [abstract]: "employs tree-based self-sampling on the model's own responses to generate preference pairs." [section 4.2]: Details the four-step tree process and scoring via PRM

### Mechanism 3
- Claim: Under a specific reward definition, offline step-level DPO gradients can be equivalent to online policy gradient updates
- Mechanism: The paper defines a preference decoding model $\pi^p_\theta$ induced by $\pi_\theta$. With a trajectory reward defined as $\prod_{i=1}^T \frac{\pi_{\text{ref}}(a_t|s_t)}{\pi^p_\theta(a_t|s_t)}$, it proves $\nabla_\theta J(\theta) = -\nabla_\theta L_{\text{every-step}}$, aligning offline preference loss gradients with online policy gradients
- Core assumption: The reward definition and binary-branch preference sampling hold; the equivalence requires the theoretical constraints outlined in Theorem 5.2
- Evidence anchors: [abstract]: "theoretically proves equivalence to on-policy policy gradient methods under specific reward constraints." [section 5]: Theorem 5.2 and its proof sketch

## Foundational Learning
- Concept: Markov Decision Process (MDP) in LLM reasoning
  - Why needed here: The paper models step-by-step reasoning as states, actions, transitions, and rewards, which is foundational to deriving the dynamic value margin and theoretical claims
  - Quick check question: Can you map a reasoning chain into $(s_t, a_t, s_{t+1}, r_t)$ tuples?

- Concept: Bradley–Terry preference model
  - Why needed here: The step-level preference probability and the resulting DPO-style loss rely on the BT model; understanding it clarifies how margins and log-odds enter the objective
  - Quick check question: Given two rewards $r_w$ and $r_l$, what does $\sigma(r_w - r_l)$ represent?

- Concept: Bellman optimality equation
  - Why needed here: It connects Q-values, value functions, and rewards, which the paper uses to derive the value gain term and justify dynamic margins
  - Quick check question: How does $Q^*(s, a)$ relate to $V^*(s)$ and $V^*(s')$ in the Bellman equation?

## Architecture Onboarding
- Component map: Tree-Based Self-Sampling -> PRM-Enhanced SFT & DPO (Sentence-Level) -> SPPD Step-Level Training
- Critical path:
  1. Generate preference data via tree sampling with PRM scoring
  2. Warm-up model with sentence-level SFT/DPO (curriculum learning)
  3. Train with step-level dynamic margin loss (Equation 5) using filtered preference pairs (score diff >0.5)

- Design tradeoffs:
  - Margin strategy: $\gamma=0$ simplifies but ignores value gain; dynamic margins better fit MDP structure but depend on PRM quality
  - Tree expansion: Higher $C$ and $K$ increase diversity but also computational cost and potential noise
  - PRM choice: The paper relies on a specific PRM (Skywork-o1-Open-PRM-Qwen-2.5-7B); switching PRMs may require re-calibration

- Failure signatures:
  - PRM misalignment: If PRM scores do not correlate with true reasoning quality, dynamic margins become misleading
  - Insufficient diversity: Low $C$ or $K$ leads to uninformative preference pairs
  - Overfitting to PRM quirks: Repeated training stages may push the model to exploit PRM scoring artifacts

- First 3 experiments:
  1. Verify PRM calibration on held-out trajectories from your target model and domain
  2. Ablate margin strategies: compare $\gamma=0$, fixed margin, and dynamic margin on a small math dataset
  3. Sweep tree parameters ($C$, $K$) to assess diversity vs. compute tradeoff and its impact on downstream metrics

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the MDP modeling used in SPPD be effectively integrated with on-policy reinforcement learning methods?
- Basis in paper: [explicit] The authors state in the Limitations section that "how to integrate MDP modeling with on-policy methods remains an important subject for future research."
- Why unresolved: The current implementation focuses on offline preference optimization and does not explore the combination with on-policy algorithms like PPO or GRPO
- What evidence would resolve it: A hybrid training framework that successfully combines step-level MDP modeling with on-policy policy gradients

### Open Question 2
- Question: Can the Process Reward Model (PRM) be updated dynamically to prevent it from becoming ineffective as the policy model iterates?
- Basis in paper: [explicit] The Limitations section notes, "This work neglects the updates of PRM. As policy is continuously iterated, PRM faces the risk of becoming ineffective."
- Why unresolved: The current method relies on a fixed PRM, which may develop a distribution mismatch as the policy model's reasoning capabilities evolve
- What evidence would resolve it: An iterative training loop where the PRM is fine-tuned alongside the policy, maintaining high discriminative accuracy for new reasoning trajectories

### Open Question 3
- Question: Does the SPPD framework maintain its effectiveness in domains where process reward models are historically less reliable, such as coding or general logic?
- Basis in paper: [inferred] The paper acknowledges that "some PRMs may fail under specific tasks," but experiments are restricted to mathematical reasoning benchmarks
- Why unresolved: The method relies heavily on PRM scores to determine preference pairs; if the PRM fails in other domains, the dynamic value margin optimization may be corrupted
- What evidence would resolve it: Experimental results on out-of-domain benchmarks like coding (e.g., HumanEval) or logical deduction where step-wise verification differs from math

## Limitations
- Heavy reliance on PRM quality as a proxy for optimal value functions, with potential calibration issues
- Theoretical claims about gradient equivalence depend on specific reward constraints that may not hold in practice
- Limited empirical validation beyond mathematical reasoning domains

## Confidence
- **High confidence**: Experimental results showing improved performance over baselines (GSM8K, MATH500, Gaokao2023, OCW, OlympiadBench) are well-documented with multiple evaluation metrics (Greedy-CoT, MAJ@N, ORM_VOTE@N)
- **Medium confidence**: Mechanism claims for dynamic value margins improving optimization are supported by ablation studies but depend on PRM calibration assumptions
- **Medium confidence**: Theoretical equivalence proof between offline DPO and online policy gradients is internally consistent but requires specific reward constraints
- **Low confidence**: Claims about PRM-independent optimization success are limited by the heavy reliance on a specific PRM model

## Next Checks
1. **PRM Calibration Test**: Run PRM scoring on held-out trajectories from your target model and domain. Verify that PRM scores show clear separation between correct and incorrect reasoning chains, with >0.5 threshold filtering most ambiguous pairs
2. **Dynamic Margin Ablation**: Implement SPPD with γ=0 (no margin), fixed margin, and dynamic margin on a small math dataset. Compare training stability and validation accuracy to verify the dynamic margin mechanism
3. **Tree Sampling Diversity Analysis**: Vary C and K parameters in tree-based self-sampling. Measure preference pair diversity (unique reasoning patterns) vs. computational cost, and correlate with downstream reasoning performance to validate the sampling strategy