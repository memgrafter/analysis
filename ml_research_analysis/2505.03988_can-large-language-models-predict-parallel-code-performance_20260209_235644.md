---
ver: rpa2
title: Can Large Language Models Predict Parallel Code Performance?
arxiv_id: '2505.03988'
source_url: https://arxiv.org/abs/2505.03988
tags:
- llms
- performance
- code
- roofline
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) can
  predict GPU code performance by classifying kernels as compute-bound (CB) or bandwidth-bound
  (BB) without hardware profiling. The researchers created a balanced dataset of 340
  CUDA and OpenMP GPU kernels from the HeCBench benchmark suite, labeled with ground-truth
  classifications derived from empirical profiling.
---

# Can Large Language Models Predict Parallel Code Performance?

## Quick Facts
- arXiv ID: 2505.03988
- Source URL: https://arxiv.org/abs/2505.03988
- Reference count: 40
- Primary result: LLMs achieved 100% accuracy with profiling data and up to 64% with source code alone

## Executive Summary
This study investigates whether Large Language Models (LLMs) can predict GPU kernel performance by classifying code as compute-bound or bandwidth-bound without hardware profiling. The researchers created a balanced dataset of 340 CUDA and OpenMP GPU kernels from HeCBench, labeled using empirical profiling. Four experimental scenarios were evaluated: with profiling data, zero-shot (source code only), few-shot (with examples), and fine-tuning. State-of-the-art LLMs achieved 100% accuracy when given explicit profiling data, while reasoning-capable models significantly outperformed standard LLMs in zero- and few-shot settings, reaching up to 64% accuracy on source code alone. Fine-tuning attempts failed due to insufficient training data.

## Method Summary
The researchers created a balanced dataset of 340 GPU kernels (170 CUDA, 170 OpenMP) from HeCBench, profiled on NVIDIA RTX 3080 10GB to label each as compute-bound or bandwidth-bound using the Roofline model. Source code was tokenized with gpt-4o-mini tokenizer with an 8k token cutoff. Four scenarios were evaluated: (1) with explicit profiling data, (2) zero-shot source code only, (3) few-shot with 2 examples, and (4) fine-tuning gpt-4o-mini via Azure OpenAI. Chain-of-thought prompting was used for reasoning tasks. Models were evaluated using accuracy, macro F1-score, and Matthews Correlation Coefficient on a 50/50 balanced split.

## Key Results
- LLMs achieved 100% accuracy when provided explicit profiling data
- Reasoning-capable LLMs reached up to 64% accuracy on source code alone
- Fine-tuning attempts failed due to insufficient training data (68 samples per class)
- Standard LLMs significantly underperformed compared to reasoning models in zero- and few-shot settings

## Why This Works (Mechanism)
LLMs can leverage pattern recognition from source code structure, variable naming conventions, and algorithmic patterns to infer computational characteristics. When reasoning models are employed, they can perform multi-step analysis of code components, identifying operations that indicate compute intensity versus memory access patterns. The success with profiling data demonstrates that LLMs can effectively process and utilize numerical performance metrics when available, suggesting the models can bridge between symbolic code representations and quantitative performance characteristics.

## Foundational Learning
- Roofline model fundamentals: Understanding the relationship between arithmetic intensity and hardware balance points is essential for interpreting performance bounds and classifications.
- GPU kernel profiling: Knowledge of how to measure FLOPs, memory traffic, and computational intensity is necessary for ground-truth labeling.
- LLM tokenization and context limits: Awareness of token limitations (8k cutoff) and how code is represented affects model input quality.
- Binary classification metrics: Familiarity with accuracy, F1-score, and MCC is crucial for evaluating model performance on imbalanced datasets.

## Architecture Onboarding
- Component map: Profiling data collection -> Dataset creation -> Prompt engineering -> LLM inference -> Performance evaluation
- Critical path: Source code analysis and labeling through Roofline profiling, followed by LLM inference with appropriate prompting strategies
- Design tradeoffs: Balancing dataset size versus model performance, choosing between zero-shot, few-shot, and fine-tuning approaches based on available resources
- Failure signatures: Fine-tuning collapses to single-class prediction when training data is insufficient; zero-shot accuracy plateaus below practical thresholds
- First experiments: (1) Validate profiling methodology on RTX 3080 with diverse kernel types, (2) Test chain-of-thought prompting with reasoning models on sample kernels, (3) Evaluate token limit impact by truncating longer kernels

## Open Questions the Paper Calls Out
- Can advanced prompting strategies like question-decomposition or least-to-most prompting significantly improve accuracy beyond the 64% ceiling?
- How does LLM prediction accuracy vary across different GPU architectures when arithmetic intensity classifications shift?
- What is the minimum dataset size required to successfully fine-tune an LLM for roofline classification without causing model collapse?

## Limitations
- Fine-tuning failed due to insufficient training data (68 samples per class)
- Experiments limited to single GPU architecture (NVIDIA RTX 3080)
- Zero-shot accuracy (64%) still leaves substantial room for improvement

## Confidence
- LLM performance prediction capability without profiling: Medium
- Reasoning models superiority: High
- Fine-tuning ineffectiveness: High

## Next Checks
1. Profile additional GPU kernels from diverse sources to create a larger dataset (target: 1000+ samples) and re-attempt fine-tuning
2. Test alternative prompting strategies including step-by-step reasoning chains and performance-relevant feature extraction from source code
3. Validate generalization by testing on GPU architectures different from RTX 3080 (e.g., RTX 4090, A100) to assess model transferability