---
ver: rpa2
title: 'Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written
  and Algorithmic Adversarial Prompts'
arxiv_id: '2510.15973'
source_url: https://arxiv.org/abs/2510.15973
tags:
- attack
- security
- evaluation
- attacks
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a comprehensive security assessment of four
  LLMs (Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4) against four distinct attack
  categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG), and
  Tree-of-Attacks-with-Pruning (TAP). Using 1,200 stratified prompts from the SALAD-Bench
  dataset across six harm categories, the study reveals significant variations in
  model robustness, with Llama-2 achieving the highest overall security (3.4% average
  attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0% average
  attack success rate).'
---

# Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts

## Quick Facts
- **arXiv ID**: 2510.15973
- **Source URL**: https://arxiv.org/abs/2510.15973
- **Reference count**: 7
- **Primary result**: Comprehensive security assessment reveals significant variations in LLM robustness to adversarial jailbreaking, with transferability of attacks across models up to 17% success rate.

## Executive Summary
This paper conducts a systematic security evaluation of four large language models (Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4) against four distinct attack categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG), and Tree-of-Attacks-with-Pruning (TAP). Using 1,200 stratified prompts from the SALAD-Bench dataset across six harm categories, the study reveals significant variations in model robustness, with Llama-2 achieving the highest overall security (3.4% average attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0% average attack success rate). Notably, GCG and TAP attacks show substantial transferability, achieving up to 17% success when transferred to other models. Statistical analysis confirms significant differences in vulnerability across harm categories (p < 0.001), with malicious use prompts showing the highest attack success rates (10.71% average). These findings highlight critical cross-model vulnerabilities and provide actionable insights for developing targeted defense mechanisms.

## Method Summary
The study evaluates four LLMs using SALAD-Bench dataset with 1,200 stratified prompts across six harm categories (200 per attack type). Four attack methods are implemented: human-written jailbreaks collected from Reddit and JailbreakChat.com, AutoDAN (genetic algorithm), GCG (gradient-based optimization), and TAP (tree-search framework). Attacks are executed against Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4, with GPT-4 serving as the automated evaluator for attack success classification. The evaluation pipeline uses the PromptBench framework, with statistical significance tested via Friedman test and Wilcoxon signed-rank test with Bonferroni correction. Meta-evaluation compares GPT-4 judgments against human labels to assess evaluator reliability.

## Key Results
- Llama-2-7B-Chat demonstrates highest overall robustness with 3.4% average attack success rate across all attack types and harm categories
- Phi-2 shows greatest vulnerability at 7.0% average attack success rate, suggesting systematic weaknesses in safety mechanisms
- GCG and TAP attacks exhibit substantial transferability, achieving up to 17% success rates when applied to models other than the optimization target
- Malicious use prompts show highest vulnerability (10.71% average ASR), while representation & toxicity prompts show lowest (2.86% average ASR)
- Statistical analysis confirms significant differences in vulnerability across harm categories (p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimization-based attacks (GCG) optimized against one model architecture can bypass safety filters in other models via shared vulnerability exploitation.
- **Mechanism**: Gradient-based optimization identifies adversarial suffixes that maximize the likelihood of harmful responses. The paper suggests these suffixes exploit "fundamental weaknesses shared across model architectures" rather than model-specific noise.
- **Core assumption**: The transferred attacks succeed because models share similar representational vulnerabilities or tokenizer behaviors, rather than the success being random noise.
- **Evidence anchors**:
  - [abstract] "GCG and TAP attacks... achieve substantially higher success rates when transferred to other models (up to 17% for GPT-4)."
  - [section 4.2.1] "This high transferability rate significantly exceeds the original effectiveness against Llama-2... suggesting that adversarial suffixes... exploit fundamental weaknesses shared across model architectures."
  - [corpus] "Differential Robustness in Transformer Language Models" (arXiv:2509.09706) notes significant variations in robustness across architectures, supporting the premise that architectural differences influence attack success.
- **Break condition**: Transferability likely degrades if target models use fundamentally different tokenization schemes or divergent alignment objectives (e.g., specific refusal training that breaks the suffix pattern).

### Mechanism 2
- **Claim**: Safety alignment is unequally distributed across harm categories, leaving "Malicious Use" and "Information & Safety" categories more vulnerable than "Toxicity."
- **Mechanism**: RLHF (Reinforcement Learning from Human Feedback) and safety fine-tuning often over-represent easily identifiable harms (e.g., hate speech) while under-representing complex, functional harms (e.g., cyberattack assistance), creating an uneven security surface.
- **Core assumption**: The observed higher Attack Success Rates (ASR) in specific categories reflect gaps in the training distribution rather than inherent difficulty in defending against those queries.
- **Evidence anchors**:
  - [abstract] "Statistical analysis confirms significant differences in vulnerability across harm categories (p < 0.001), with malicious use prompts showing the highest attack success rates."
  - [section 4.4.2] "GPT-4 demonstrates the highest recorded vulnerability to Malicious Use prompts (21.42% ASR)... while Llama-2 shows complete resistance to Representation & Toxicity."
  - [corpus] "AutoRedTeamer" (arXiv:2503.15754) emphasizes the need for comprehensive coverage of emerging attack vectors, implying standard training often misses specific functional harms.
- **Break condition**: Defense mechanisms that use category-agnostic refusal training or specialized "constitutional" principles for under-represented categories would likely flatten this variance.

### Mechanism 3
- **Claim**: Smaller parameter models with less extensive safety tuning (Phi-2) exhibit systemic vulnerability, whereas heavily aligned models (Llama-2-Chat) achieve superior robustness regardless of parameter count.
- **Mechanism**: Robustness is primarily a function of the quality and breadth of safety alignment data (RLHF/Safety datasets), not just raw model capacity. Llama-2-Chat’s specific safety fine-tuning provides a defense layer that the base or smaller models (Phi-2) lack.
- **Core assumption**: The performance gap is attributable to the specific safety training regimen of Llama-2-Chat rather than the architectural superiority of the Llama family itself.
- **Evidence anchors**:
  - [abstract] "Llama-2 achieving the highest overall security (3.4%) while Phi-2 exhibits the greatest vulnerability (7.0%)."
  - [section 4.1.1] "Phi-2 demonstrates the highest vulnerability... suggesting systematic weaknesses in the model’s safety mechanisms... Llama-2 exhibits the strongest overall robustness."
  - [corpus] "Differential Robustness" (arXiv:2509.09706) provides context that model resilience varies significantly, though this paper specifically isolates safety-training efficacy over raw architecture.
- **Break condition**: If a small model were trained with equivalent or superior safety data density to a large model, this size-vulnerability correlation might weaken or invert.

## Foundational Learning

- **Concept: Jailbreaking & Alignment**
  - **Why needed here**: The study measures how effectively different "jailbreaking" techniques break "alignment" (the process of teaching models to refuse harmful requests). Without this, "ASR" is meaningless.
  - **Quick check question**: Does a "successful attack" mean the model crashed, or that it generated a harmful response? (Answer: Generated a harmful response).

- **Concept: Transferability (Black-box vs. White-box)**
  - **Why needed here**: The paper highlights that attacks generated with full model access (White-box/GCG) work on models where the attacker has no access (Black-box/GPT-4). This is central to the threat model.
  - **Quick check question**: Why is an attack optimized on an open-source model (Llama-2) dangerous for a closed-source model (GPT-4)? (Answer: Shared vulnerabilities/transferability).

- **Concept: Evaluation Reliability (LMR)**
  - **Why needed here**: The paper uses GPT-4 to judge if an attack succeeded. Understanding "Label Matching Rate" (LMR) is critical to knowing if the results are trustworthy or if the "judge" is hallucinating.
  - **Quick check question**: If the automated judge (GPT-4) agrees with humans only 20% of the time for a specific attack type, can we trust the results for that attack type? (Answer: No/low confidence).

## Architecture Onboarding

- **Component map**: Dataset (SALAD-Bench) -> Attack Optimization (AutoDAN, GCG, TAP, Human-Written) -> Target Inference (Phi-2, Llama-2, GPT-3.5, GPT-4) -> Response Collection -> GPT-4 Evaluation -> Statistical Analysis (Friedman Test)

- **Critical path**: Prompt Selection -> Attack Optimization (e.g., running 500 steps of GCG) -> Target Inference -> Response Collection -> GPT-4 Evaluation -> Statistical Analysis (Friedman Test)

- **Design tradeoffs**:
  - **Automated Evaluation vs. Human Validation**: The architecture relies on GPT-4 to judge success (scalability), but the paper admits this has low reliability in specific cases (e.g., 20% LMR for Human-Written attacks on Phi-2), trading accuracy for scale.
  - **Stratified Sampling**: Using 200 prompts per attack type (total 1,200) instead of the full dataset to manage compute budget, limiting statistical power for sub-category analysis.

- **Failure signatures**:
  - **Low Evaluator Consistency**: If you see Label Matching Rates (LMR) <50% in your validation set, the automated judge is likely misclassifying responses (Table 3 shows this variance).
  - **Zero-ASR Anomalies**: If a model shows 0.00% ASR on "Representation & Toxicity" but high ASR on "Malicious Use," it suggests over-fitted refusal training rather than robust safety (Section 4.4.2).

- **First 3 experiments**:
  1. **Reproduce Transferability**: Run GCG optimization against Llama-2 (white-box) and take the resulting adversarial suffixes. Apply them directly to GPT-3.5/4 to verify the ~15-17% transfer success rate reported in Table 2.
  2. **Probing Category Bias**: Isolate "Malicious Use" prompts from SALAD-Bench and run Human-Written attacks against all 4 models to confirm the high vulnerability variance reported in Section 4.4.
  3. **Evaluator Stress Test**: Take a sample of 20 responses where GPT-4 (the evaluator) flagged a "successful attack" and manually review them to check if the judge is hallucinating safety violations, specifically checking the low-LMR conditions identified in Section 4.3.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can targeted defensive strategies be developed to mitigate high vulnerability to malicious use prompts without degrading general model utility?
- **Basis in paper**: [explicit] Section 6.3 states future research should focus on "category-specific defenses" and general robustness improvements to address the high vulnerability (10.71% avg ASR) identified in the malicious use category.
- **Why unresolved**: Current safety training appears unbalanced (e.g., GPT-4 shows 21.42% vulnerability to malicious use), and the paper only identifies the vulnerabilities without proposing specific defensive architectures.
- **What evidence would resolve it**: The development of a fine-tuning or input-processing defense that statistically significantly reduces Malicious Use ASR while maintaining scores on standard capability benchmarks.

### Open Question 2
- **Question**: Can hybrid human-AI frameworks or multi-model evaluation systems improve the reliability of automated safety assessments compared to single-model evaluators?
- **Basis in paper**: [explicit] Section 6.3 calls for "improved evaluation approaches" and suggests "incorporating multiple evaluator models" to address the substantial variance in GPT-4 reliability (LMR 20-100%).
- **Why unresolved**: The study relied on a single model (GPT-4) for evaluation, which introduced systematic errors, particularly for complex attack types like TAP.
- **What evidence would resolve it**: A comparative study measuring Label Matching Rates (LMR) of a hybrid system against human ground truth, demonstrating higher consistency than the single-model baseline.

### Open Question 3
- **Question**: Do smaller parameter models inherently trade security robustness for capability?
- **Basis in paper**: [inferred] Section 5.1.1 notes Phi-2’s consistent vulnerability suggests "smaller models may face fundamental trade-offs between capability and security," but this observation is based on limited model selection.
- **Why unresolved**: The study evaluated only one small model (Phi-2); it is unclear if the vulnerability is specific to Phi-2's training or a universal constraint of model scale.
- **What evidence would resolve it**: A controlled study of models from the same family (e.g., Llama-2 7B vs 13B vs 70B) showing a negative correlation between parameter count and Attack Success Rate.

## Limitations

- The study's reliance on GPT-4 as both an attack target and evaluation oracle creates a significant methodological dependency that could introduce systematic bias.
- Automated evaluator performance varies dramatically across attack types, with Label Matching Rates ranging from 20% to 100%, raising questions about reliability for certain attack categories.
- The transfer attack analysis lacks granularity regarding the mechanisms of transferability, showing correlation but not establishing causation for the transferability mechanism.
- Stratified sampling approach (200 prompts per attack type) may miss rare failure modes in under-represented harm categories.

## Confidence

- **High Confidence**: The differential vulnerability findings across harm categories are supported by statistically significant results (p < 0.001) and align with known limitations in RLHF training coverage.
- **Medium Confidence**: The transferability results are compelling but require careful interpretation given the evaluator reliability variance; the claim about "fundamental weaknesses" is plausible but not directly validated.
- **Low Confidence**: The absolute ASR values for Human-Written attacks should be interpreted cautiously due to documented evaluator inconsistency (20-100% LMR range); meta-evaluation was limited to 30 prompts.

## Next Checks

1. **Evaluator Consistency Audit**: Manually validate 50 randomly selected successful attack responses (across all harm categories and attack types) to establish ground truth success rates independent of GPT-4's judgments.

2. **Transferability Mechanism Analysis**: Conduct ablation studies varying attack suffix components to determine which specific elements drive cross-model success and validate whether transferability exploits shared architectural features.

3. **Category-Specific Defense Testing**: Implement and evaluate category-targeted safety interventions on Llama-2 to verify whether the observed vulnerability variance is indeed training-data dependent.