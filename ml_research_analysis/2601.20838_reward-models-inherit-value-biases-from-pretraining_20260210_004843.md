---
ver: rpa2
title: Reward Models Inherit Value Biases from Pretraining
arxiv_id: '2601.20838'
source_url: https://arxiv.org/abs/2601.20838
tags:
- gemma
- llama
- word
- what
- thing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reward models inherit value biases from the LLMs on which they
  are built, shaping their behavior along multiple dimensions of human values such
  as agency and communion. By analyzing 10 leading open-weight RMs, we found systematic
  differences between Llama- and Gemma-based RMs, with Llama favoring agency-oriented
  values (e.g., freedom, success) and Gemma favoring communion-oriented values (e.g.,
  love, friendship).
---

# Reward Models Inherit Value Biases from Pretraining

## Quick Facts
- arXiv ID: 2601.20838
- Source URL: https://arxiv.org/abs/2601.20838
- Reference count: 40
- Reward models inherit value biases from the LLMs on which they are built, shaping their behavior along multiple dimensions of human values such as agency and communion.

## Executive Summary
Reward models trained on preference data inherit systematic value biases from their underlying base language models, with Llama-based RMs favoring agency-oriented values (freedom, success) and Gemma-based RMs favoring communion-oriented values (love, friendship). This bias persists even when trained with identical preference data, indicating that pretraining choices significantly impact the values reflected in alignment pipelines. The study demonstrates that while sufficient preference data can reduce these biases, they are surprisingly durable and difficult to fully eliminate.

## Method Summary
The authors analyzed 10 leading open-weight reward models, comparing Llama-based and Gemma-based architectures across multiple dimensions of human values. They conducted controlled ablation experiments training their own RMs using identical preference datasets (OLI and OpenHermes) to isolate the impact of base model choice. The study employed both quantitative preference comparisons and qualitative value analysis to identify systematic differences in how different RMs evaluate responses.

## Key Results
- Llama-based RMs systematically favor agency-oriented values (freedom, success) while Gemma-based RMs favor communion-oriented values (love, friendship)
- Value biases persist even when RMs are trained with identical preference data
- Sufficient preference data can reduce but not fully eliminate the inherited biases
- The choice of base model significantly impacts the values reflected in downstream RMs

## Why This Works (Mechanism)
The value biases in reward models emerge from the pretraining phase of the base language models. During pretraining, LLMs develop statistical associations and patterns that reflect the training corpus's implicit values. When these base models are fine-tuned into reward models, these learned associations persist and influence preference judgments, even when the preference data attempts to override them. The mechanism operates through the retention of internal representations and decision boundaries that encode value-related semantic patterns established during pretraining.

## Foundational Learning
- **Reward modeling**: Why needed - core technique for aligning language models with human preferences; Quick check - understanding how preference data shapes model behavior
- **Value dimensions**: Why needed - framework for analyzing systematic differences in model preferences; Quick check - ability to categorize responses along agency-communitas spectrum
- **Base model pretraining**: Why needed - source of inherited biases in downstream models; Quick check - understanding how pretraining corpus influences learned representations
- **Ablation experiments**: Why needed - controlled method for isolating causal factors; Quick check - ability to interpret experimental design for bias elimination
- **Open-weight model analysis**: Why needed - enables systematic comparison across architectures; Quick check - understanding limitations of closed-weight system analysis

## Architecture Onboarding
Component map: Base LLM pretraining -> Reward model fine-tuning -> Preference evaluation -> Value bias manifestation

Critical path: Base model selection → Preference dataset choice → RM training → Value bias emergence

Design tradeoffs: Choice between Llama (agency-favoring) and Gemma (communion-favoring) base models creates systematic differences in downstream alignment. The tradeoff involves selecting between different implicit value orientations rather than achieving neutral alignment.

Failure signatures: Reward models consistently favor responses aligned with their base model's pretraining values, even when preference data suggests otherwise. This manifests as systematic scoring differences across agency-communitas dimensions.

First experiments:
1. Compare preference rankings between Llama and Gemma-based RMs on identical response pairs
2. Train identical RMs with different quantities of preference data to measure bias persistence
3. Evaluate RM behavior on responses specifically designed to trigger agency vs. communion value judgments

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to 10 open-weight RMs, potentially constraining generalizability
- Preference datasets may not capture full diversity of human values
- Does not directly measure downstream behavioral impacts on actual model outputs

## Confidence
- Claim about value bias persistence: Medium confidence
- Claim about pretraining impact on alignment values: Medium confidence
- Claim about agency vs. communion differences: High confidence

## Next Checks
1. Replicate experiments with additional base models (e.g., Mistral, DeepSeek) to test generalizability beyond Llama and Gemma
2. Conduct human evaluations of actual model outputs to measure behavioral impacts of the observed value biases
3. Test bias persistence across a wider range of preference dataset sources, quantities, and qualities to better understand conditions for bias elimination