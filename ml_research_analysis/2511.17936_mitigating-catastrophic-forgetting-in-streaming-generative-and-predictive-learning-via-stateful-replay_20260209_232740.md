---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning
  via Stateful Replay
arxiv_id: '2511.17936'
source_url: https://arxiv.org/abs/2511.17936
tags:
- replay
- seqft
- forgetting
- learning
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies streaming continual learning across generative
  and predictive objectives, focusing on catastrophic forgetting. The core idea is
  stateful replay with a fixed-capacity buffer that mixes current and historical samples
  to mitigate forgetting.
---

# Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay

## Quick Facts
- **arXiv ID**: 2511.17936
- **Source URL**: https://arxiv.org/abs/2511.17936
- **Reference count**: 26
- **Key outcome**: Stateful replay with a fixed-capacity buffer reduces catastrophic forgetting in streaming generative and predictive learning, especially on heterogeneous multi-task streams, but performs similarly to sequential fine-tuning on benign time-based streams.

## Executive Summary
This paper addresses catastrophic forgetting in streaming continual learning, where models must adapt to new data without retaining old task data. The authors propose stateful replay, a method that maintains a fixed-capacity buffer of mixed current and historical samples, and apply it to both generative (auto-encoding, forecasting) and predictive (classification) tasks unified under negative log-likelihood minimization. By analyzing gradient alignment, they explain replay's effectiveness. Experiments on six streaming scenarios across three datasets show replay dramatically reduces forgetting in heterogeneous task streams but offers limited gains in benign temporal streams, positioning it as a strong, simple baseline for streaming generative and predictive learning.

## Method Summary
The core method, stateful replay, uses a fixed-capacity buffer to store a mix of current and past samples, mitigating catastrophic forgetting in streaming continual learning. The approach unifies generative and predictive objectives under negative log-likelihood minimization, applying replay to auto-encoding, forecasting, and classification tasks. Gradient alignment analysis is used to explain why replay is effective in reducing forgetting. The method is evaluated across heterogeneous multi-task and benign time-based streaming scenarios using datasets like Rotated MNIST, Electricity, and Airlines, demonstrating substantial reductions in forgetting for complex task mixtures while showing similar performance to sequential fine-tuning in simpler streams.

## Key Results
- Stateful replay reduces catastrophic forgetting by 2–3× in heterogeneous multi-task streaming scenarios compared to sequential fine-tuning.
- On benign time-based streams, replay performs similarly to sequential fine-tuning, offering no significant advantage.
- The method is effective across both generative (auto-encoding, forecasting) and predictive (classification) tasks when unified under negative log-likelihood minimization.

## Why This Works (Mechanism)
Stateful replay mitigates catastrophic forgetting by maintaining a fixed-capacity buffer that mixes current and historical samples, ensuring that past information is periodically revisited during training. This prevents the model from overwriting knowledge of previous tasks when adapting to new ones. The gradient alignment analysis shows that replay helps maintain gradient directions that are beneficial for both new and old tasks, stabilizing learning and reducing forgetting. By unifying generative and predictive objectives under negative log-likelihood minimization, the method provides a consistent framework for continual adaptation across diverse task types.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly lose performance on previously learned tasks when trained on new data. **Why needed**: Central challenge addressed by the paper; motivates the use of replay.
- **Streaming continual learning**: Learning from data streams where tasks arrive sequentially and old data may not be stored. **Why needed**: Defines the setting and constraints under which stateful replay is evaluated.
- **Gradient alignment**: Analysis of the alignment between gradients for new and old tasks to explain replay's effectiveness. **Why needed**: Provides mechanistic insight into why replay helps mitigate forgetting.
- **Negative log-likelihood minimization**: Unified loss function used for both generative and predictive objectives. **Why needed**: Enables a consistent training framework across diverse task types.
- **Fixed-capacity buffer**: A limited-size storage for historical samples, mixed with current data during replay. **Why needed**: Practical constraint and core mechanism of stateful replay.
- **Heterogeneous task streams**: Sequences where tasks vary significantly (e.g., classification and forecasting). **Why needed**: Tests the robustness and generality of the replay method.

## Architecture Onboarding
- **Component map**: Data stream → Model (generative + predictive) → Fixed-capacity buffer → Replay sampling → Loss (negative log-likelihood) → Gradient alignment analysis.
- **Critical path**: Streaming data → Model training with replay buffer → Unified loss minimization → Evaluation of forgetting.
- **Design tradeoffs**: Fixed buffer size limits memory but ensures replay; simple random sampling is easy but may not prioritize important samples; unified loss simplifies training but may not capture all task nuances.
- **Failure signatures**: Poor performance on long or highly dynamic streams due to buffer staleness; limited gains in benign streams suggest replay may be unnecessary in some settings.
- **First experiments**:
  1. Evaluate replay buffer size impact on forgetting in heterogeneous task streams.
  2. Compare replay to sequential fine-tuning on benign temporal streams.
  3. Test gradient alignment analysis across different streaming scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance is compared only to simple baselines like sequential fine-tuning and limited replay methods, leaving uncertainty about relative gains over state-of-the-art continual learning approaches.
- The study uses a fixed buffer size and simple random sampling, without exploring adaptive reservoir management or prioritized replay, which could limit conclusions about replay's full potential.
- Experiments do not investigate highly dynamic or long-term streaming scenarios where replay might degrade due to buffer staleness or forgetting in generative components.

## Confidence
- **High** for replay reducing forgetting in heterogeneous streaming scenarios (based on quantitative results).
- **Medium** for replay's relative effectiveness versus state-of-the-art methods (limited baseline comparisons).
- **Medium** for gradient alignment as an explanatory mechanism (analysis present but not exhaustive).
- **Low** for claims about replay's robustness in highly dynamic or long-term streaming settings (not tested).

## Next Checks
1. Compare stateful replay to recent continual learning methods (e.g., regularization, dynamic replay, or generative replay baselines) on the same benchmarks to establish relative performance.
2. Evaluate the impact of adaptive buffer management strategies (e.g., reservoir sampling, prioritized replay) on long-term streaming scenarios to assess replay's scalability and robustness.
3. Test the approach on longer streaming sequences or more dynamic task distributions to assess the risk of buffer staleness and its effect on generative model quality.