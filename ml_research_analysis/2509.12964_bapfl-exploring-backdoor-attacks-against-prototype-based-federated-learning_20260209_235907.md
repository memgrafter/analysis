---
ver: rpa2
title: 'BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning'
arxiv_id: '2509.12964'
source_url: https://arxiv.org/abs/2509.12964
tags:
- bapfl
- prototypes
- prototype
- attack
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the security of prototype-based federated
  learning (PFL) against backdoor attacks, which remains unexplored despite PFL's
  growing use in addressing data heterogeneity. Existing backdoor attacks fail in
  PFL due to limited influence of poisoned prototypes and data heterogeneity.
---

# BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning

## Quick Facts
- arXiv ID: 2509.12964
- Source URL: https://arxiv.org/abs/2509.12964
- Authors: Honghong Zeng; Jiong Lou; Zhe Wang; Hefeng Zhou; Chentao Wu; Wei Zhao; Jie Li
- Reference count: 15
- Primary result: BAPFL achieves 35%-75% higher attack success rate than traditional backdoor attacks in prototype-based federated learning

## Executive Summary
This paper addresses the security vulnerabilities of prototype-based federated learning (PFL) against backdoor attacks, a gap in the literature despite PFL's growing adoption for handling data heterogeneity. Traditional backdoor attacks fail in PFL due to the limited influence of poisoned prototypes and the system's inherent heterogeneity. The authors propose BAPFL, a novel backdoor attack method specifically designed for PFL, which combines prototype poisoning and trigger optimization to effectively inject backdoors while maintaining main task accuracy.

## Method Summary
BAPFL introduces a two-component approach to backdoor attacks in PFL. The Prototype Poisoning Strategy (PPS) manipulates global prototypes to push benign prototypes away from trigger prototypes, while the Trigger Optimization Mechanism (TOM) learns stealthy, label-specific triggers to align trigger prototypes with target label prototypes. This method is designed to overcome the challenges posed by PFL's limited influence and data heterogeneity, achieving significant improvements in attack success rate compared to traditional backdoor attacks.

## Key Results
- BAPFL achieves 35%-75% improvement in attack success rate compared to traditional backdoor attacks
- Maintains main task accuracy while injecting backdoors
- Demonstrates effectiveness, stealthiness, and adaptability across multiple datasets and PFL variants

## Why This Works (Mechanism)
BAPFL exploits the unique structure of prototype-based federated learning by directly manipulating prototypes rather than model parameters. The Prototype Poisoning Strategy creates a spatial separation between benign and trigger prototypes, while the Trigger Optimization Mechanism learns triggers that are both effective and stealthy. By aligning trigger prototypes with target label prototypes, BAPFL ensures that poisoned inputs are classified as the target class while remaining inconspicuous to detection.

## Foundational Learning
- **Prototype-based Federated Learning**: A federated learning approach that uses prototypes instead of model parameters for aggregation, useful for handling data heterogeneity.
  - *Why needed*: Understanding PFL is crucial because traditional backdoor attacks fail in this setting due to limited influence of poisoned prototypes.
  - *Quick check*: Verify that PFL aggregates client prototypes rather than model weights.
- **Backdoor Attacks**: A type of adversarial attack where a model is trained to behave normally on clean inputs but produce a specific incorrect output when a trigger is present.
  - *Why needed*: BAPFL is specifically designed to overcome the limitations of traditional backdoor attacks in PFL.
  - *Quick check*: Confirm that traditional backdoor attacks rely on manipulating model parameters, which is ineffective in PFL.
- **Prototype Poisoning**: The process of manipulating prototypes to influence the behavior of a prototype-based model.
  - *Why needed*: This is the core mechanism by which BAPFL achieves its attack success.
  - *Quick check*: Ensure that prototype poisoning can shift the decision boundary in PFL.

## Architecture Onboarding
- **Component Map**: PPS -> TOM -> Global Prototypes -> Classification
- **Critical Path**: Trigger generation (TOM) → Prototype poisoning (PPS) → Global prototype manipulation → Classification of poisoned inputs
- **Design Tradeoffs**: BAPFL trades some stealthiness for higher attack success rate, but the learned triggers are designed to be label-specific and less detectable.
- **Failure Signatures**: If PPS fails to create sufficient separation between benign and trigger prototypes, or if TOM fails to learn effective triggers, the attack success rate will drop.
- **Three First Experiments**:
  1. Evaluate BAPFL's attack success rate on MNIST dataset with standard PFL.
  2. Test BAPFL's stealthiness by measuring the detectability of learned triggers.
  3. Assess BAPFL's adaptability by testing it on different PFL variants.

## Open Questions the Paper Calls Out
None

## Limitations
- BAPFL's robustness against adaptive or state-of-the-art backdoor defenses is not explored.
- The generalizability of BAPFL across diverse, real-world datasets is uncertain, as experiments focus on benchmark datasets.
- The paper does not address BAPFL's performance under non-standard PFL configurations or dataset shifts.

## Confidence
- **Effectiveness and ASR improvement (High)**: Well-supported by experimental results across multiple datasets and PFL variants.
- **Stealthiness and label-specific triggers (Medium)**: Empirical validation of stealthiness is limited.
- **Adaptability and robustness (Low)**: Performance under adaptive defenses and diverse scenarios is not addressed.

## Next Checks
1. Evaluate BAPFL's robustness against adaptive or state-of-the-art backdoor defenses (e.g., spectral signatures, activation clustering, or neural cleanse).
2. Test BAPFL on more diverse, real-world datasets and PFL scenarios to assess generalizability beyond standard benchmarks.
3. Analyze the perceptual and functional impact of the learned triggers to quantify stealthiness, including user studies or adversarial robustness metrics.