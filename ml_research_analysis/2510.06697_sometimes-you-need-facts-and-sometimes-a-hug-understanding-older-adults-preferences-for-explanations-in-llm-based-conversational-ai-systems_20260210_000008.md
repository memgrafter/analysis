---
ver: rpa2
title: '"Sometimes You Need Facts, and Sometimes a Hug": Understanding Older Adults''
  Preferences for Explanations in LLM-Based Conversational AI Systems'
arxiv_id: '2510.06697'
source_url: https://arxiv.org/abs/2510.06697
tags:
- explanations
- older
- adults
- systems
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores older adults' preferences for AI explanations
  in Conversational AI systems, focusing on how these preferences vary by task context
  and information source. Through a Speed Dating study with 23 older adults, the research
  examines reactions to AI explanations grounded in conversational history, environmental
  data, activity-based inferences, and internal system logic across routine reminders
  and emergency alerts.
---

# "Sometimes You Need Facts, and Sometimes a Hug": Understanding Older Adults' Preferences for Explanations in LLM-Based Conversational AI Systems

## Quick Facts
- arXiv ID: 2510.06697
- Source URL: https://arxiv.org/abs/2510.06697
- Reference count: 40
- Key outcome: Older adults prefer warm, conversational explanations for routine tasks and direct, concise explanations for emergencies, with preferences varying by task context and information source

## Executive Summary
This qualitative study investigates how older adults prefer AI assistants to explain their actions in conversational AI systems. Through Speed Dating sessions with 23 older adults (ages 65-78), researchers examined reactions to AI explanations across two contexts: routine reminders and emergency alerts. The study found that explanation preferences are highly context-dependent, with participants favoring conversational, warm explanations for everyday interactions and direct, task-focused explanations for emergencies. Explanations grounded in real-time environmental data and prior conversational history were preferred over those based on internal system logic, with participants viewing explanations as interactive, multi-turn exchanges that help calibrate urgency and guide actionability.

## Method Summary
The study employed a Speed Dating methodology with 23 older adults (11 spousal pairs, 1 individual) with MCI or caregiving roles. Participants engaged in 90-minute sessions viewing storyboard vignettes featuring an AI agent ("Rosey") providing assistance. GPT-4.0 generated four distinct explanation types based on scenario: Conversational History, Environmental Data, Activity-Based Inferences, and Internal System Logic. Participants rated explanation usefulness on 5-point Likert scales and participated in semi-structured interviews discussing their preferences and decision-making processes.

## Key Results
- Older adults prefer warm, conversational explanations for routine reminders but direct, concise explanations for emergencies
- Explanations grounded in real-time environmental data and prior conversational history were preferred over internal system logic
- Participants viewed explanations as interactive, multi-turn exchanges rather than static outputs
- Context classification accuracy is criticalâ€”tone mismatches between explanation style and task risk caused distress

## Why This Works (Mechanism)

### Mechanism 1: Context-Tuned Emotional Alignment
Tailoring explanation tone and verbosity based on perceived task risk improves user acceptance and emotional alignment. In low-risk routine contexts, conversational warmth signals companionship and continuity, reducing friction. In high-risk emergency contexts, concise directness reduces cognitive load and signals urgency, preventing the "chatty" delay that users fear in crises.

### Mechanism 2: Tangible Data Grounding
Grounding explanations in personalized, observable data (history or sensors) appears to mitigate skepticism more effectively than abstract internal logic (confidence scores). Referencing prior interactions or sensor data provides "evidence" that the system is attentive and context-aware, fostering a sense of being known. In contrast, numerical confidence scores without context were interpreted as "overconfident" or "ambiguous," generating distrust rather than calibration.

### Mechanism 3: Interactive Multi-Turn Scaffolding
Framing explanations as interactive, multi-turn scaffolds rather than static outputs supports agency and memory retention. Progressive disclosure prevents information overload. By treating the explanation as the start of a negotiation ("Why?"), the system allows users to verify the AI's logic, which is crucial for those with memory concerns who need to "snap back" to reality.

## Foundational Learning

- **Concept: Context-Aware Computing (Sensing & Inference)**
  - Why needed here: The system must differentiate between "Routine" and "Emergency" contexts and access environmental data (sensors) or history to generate the explanations described in Mechanism 1 & 2.
  - Quick check question: How does the system determine the difference between a "stove left on" (Emergency) and "cooking dinner" (Routine)?

- **Concept: Human-Centered Explainable AI (HCXAI)**
  - Why needed here: Standard XAI focuses on model weights; this paper focuses on *social transparency*. You need to understand that "explainability" here means "justification for the user," not "debugging info for the developer."
  - Quick check question: If the model is 99% confident but the user asks "Why?", should you output the confidence score or the sensor trigger that caused the alert?

- **Concept: Progressive Disclosure**
  - Why needed here: To implement Mechanism 3. You cannot dump all context at once. The architecture must support a dialogue state where information is revealed iteratively.
  - Quick check question: In a dialogue manager, how do you store the "explanation depth" state to ensure you don't repeat the "Level 1" explanation when the user asks for "Level 2" details?

## Architecture Onboarding

- **Component map:**
  Context Classifier -> RAG Module -> Prompt Orchestrator -> Dialogue Manager

- **Critical path:**
  1. Ingest Event (e.g., Smoke detector trigger + Stove on + User in living room)
  2. Classify Context: High risk -> Emergency Mode
  3. Retrieve Context: Pull "Stove status" and "User location" (Environmental); ignore "User mentioned cooking last week" (History) due to urgency
  4. Construct Prompt: "Be direct and concise. Explain using environmental data."
  5. Generate Output: "Alert: Smoke detected in kitchen. Please check immediately."

- **Design tradeoffs:**
  - Privacy vs. Personalization: To support the "Conversational History" mechanism, the system must store and retrieve past interactions. This increases privacy risk but is required for the "companionship" effect valued in routine contexts.
  - Latency vs. Grounding: Retrieving sensor data takes time. In emergencies, the system must fall back to generic alerts if the RAG retrieval latency is too high.

- **Failure signatures:**
  - "Hallucinated Grounding": The LLM invents a conversation history to justify a reminder (e.g., "You said you wanted to stretch"). This causes severe trust degradation.
  - Tone Mismatch: A "friendly" joke during a fall detection alert.
  - Context Blindness: Explaining a complex "Why" when the user is in a rush (Emergency context).

- **First 3 experiments:**
  1. **Tone Calibration Test:** Deploy the system with two prompt settings (Warm/Verbose vs. Direct/Concise) in a simulated emergency scenario. Measure user reaction time and reported stress levels.
  2. **Grounding vs. Scores A/B Test:** Present users with two explanation types for a routine reminder: one citing "Sensor Data" vs. one citing "85% Confidence." Ask "Do you trust this reminder?"
  3. **Progressive Disclosure Interaction:** Implement a "Why?" button/follow-up. Test if users actually engage in multi-turn negotiation or if they prefer single-shot answers.

## Open Questions the Paper Calls Out

### Open Question 1
How do explanation preferences shift as older adults' cognitive conditions and home routines evolve over time? The current Speed Dating study was cross-sectional and exploratory, capturing user reactions at a single point in time rather than tracking changes longitudinally.

### Open Question 2
How do older adults perceive and interact with alternative explanation techniques, such as contrastive or counterfactual explanations? The study focused exclusively on natural language explanations grounded in four specific information sources and did not test other XAI strategies.

### Open Question 3
How do conversational properties like agent personality and verbosity shape the user experience across varying explanation modalities? While the study identified a preference for "warm" vs. "direct" tones based on context, it did not systematically isolate variables like verbosity or specific personality traits.

## Limitations
- Study relies on hypothetical storyboard scenarios rather than real-world interaction data, limiting ecological validity
- Sample size of 23 participants constrains generalizability across diverse older adult population
- Focus on older adults with mild cognitive impairment or caregiving roles may not represent all user segments

## Confidence
- **High confidence**: Context-dependent explanation preferences (Routine vs. Emergency) are consistently supported by participant feedback across multiple scenarios
- **Medium confidence**: The superiority of conversational history and environmental data grounding over internal system logic is well-supported, though individual variation exists
- **Medium confidence**: The multi-turn explanation framework shows promise but requires empirical validation in actual deployment

## Next Checks
1. **Real-world deployment study**: Implement the explanation system in actual older adults' homes for 2-4 weeks to validate whether preferences hold under genuine conditions
2. **Cross-cultural validation**: Test whether explanation preferences vary across different cultural contexts and communication norms
3. **Cognitive load measurement**: Empirically measure whether the recommended explanation approaches (warm for routine, concise for emergency) actually reduce cognitive burden as hypothesized