---
ver: rpa2
title: Enforcing Orderedness to Improve Feature Consistency
arxiv_id: '2512.02194'
source_url: https://arxiv.org/abs/2512.02194
tags:
- dictionary
- stability
- orderedness
- features
- msae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ordered Sparse Autoencoders (OSAE) address the reproducibility
  problem in sparse autoencoders by enforcing a strict ordering of latent features
  through deterministic, prefix-based reconstruction, extending Matryoshka SAEs. Theoretically,
  this resolves permutation non-identifiability in overcomplete dictionary learning
  settings with unique solutions.
---

# Enforcing Orderedness to Improve Feature Consistency

## Quick Facts
- arXiv ID: 2512.02194
- Source URL: https://arxiv.org/abs/2512.02194
- Reference count: 40
- Primary result: OSAEs improve feature consistency in sparse autoencoders by enforcing strict ordering through deterministic prefix-based reconstruction, reducing novel features in stitching from 73.8% to 33.8% compared to BatchTopK

## Executive Summary
Ordered Sparse Autoencoders (OSAEs) address the reproducibility problem in sparse autoencoders by enforcing a strict ordering of latent features through deterministic, prefix-based reconstruction. This approach extends Matryoshka SAEs by covering all prefix lengths rather than sampling only a few group sizes. Theoretically, this resolves permutation non-identifiability in overcomplete dictionary learning settings under spark conditions, while empirically OSAEs show improved consistency and orderedness over baselines, particularly for early prefix features.

## Method Summary
OSAEs enforce reconstruction at every prefix length by sampling prefix lengths ℓ from a distribution p_ND and reconstructing using only the top-ℓ features after top-m sparsification. The nested dropout loss L_ND = E_ℓ∼p_ND[L_ℓ(D,E)] covers all ℓ=1...K with full-support distribution, eliminating within-group exchangeability that plagues Matryoshka SAEs. The method includes unit sweeping to prevent gradient starvation of later features by freezing converged early units according to a clockwork schedule. Training uses top-m SAE with nested dropout, warmup from k_init=K down to target m, and unit-norm decoder constraints.

## Key Results
- On Gemma2-2B, OSAEs achieve up to 0.8 orderedness and higher stability for initial features compared to baselines
- On Pythia-70M, OSAEs show improved cross-dataset orderedness and stability
- OSAEs reduce the fraction of novel features found via SAE stitching from 73.8% to 33.8% compared to BatchTopK

## Why This Works (Mechanism)

### Mechanism 1: Prefix-based reconstruction breaks permutation symmetry
Enforcing reconstruction at every prefix length breaks permutation symmetry and forces canonical feature ordering. Earlier features participate in more reconstruction terms, receiving more gradient signal and creating pressure for high-utility features to occupy early indices. The full-support prefix distribution eliminates within-group exchangeability that exists in Matryoshka SAEs.

### Mechanism 2: Theoretical uniqueness under spark conditions
Under spark conditions (spark(D)>2m) and ordered ground-truth atoms, nested dropout uniquely recovers the true dictionary in correct order. The proof shows L_ND minimizers also minimize full reconstruction, invokes spark-based uniqueness up to permutation/scaling, and uses frequency ordering to select the identity permutation.

### Mechanism 3: Unit sweeping prevents gradient starvation
Later indices appear in fewer prefix terms, causing their gradients to shrink exponentially with index. Unit sweeping freezes converged early units, forcing the optimizer to allocate capacity to remaining units. A clockwork schedule freezes one unit every T epochs after burn-in.

## Foundational Learning

- **Sparse dictionary learning and spark condition**: The theoretical guarantee hinges on spark(D)>2m ensuring unique sparse codes. Without this, multiple dictionaries could explain the same data, and ordering cannot resolve arbitrary ambiguity. *Quick check: For a dictionary D∈R^(d×K), what does spark(D)>2m guarantee about m-sparse representations?*

- **Permutation non-identifiability in overcomplete SAEs**: Standard SAEs learn features defined only up to permutation and scaling; OSAE's core contribution is reducing this equivalence class by imposing order. *Quick check: Why can't standard reconstruction loss distinguish between dictionary D and DP where P is a permutation matrix?*

- **Matryoshka representation learning**: OSAE extends Matryoshka SAEs by making every prefix its own "group" rather than sampling a handful of group sizes. Understanding the baseline clarifies what OSAE changes. *Quick check: How does Matryoshka SAE's group-based sampling differ from OSAE's per-feature prefix sampling?*

## Architecture Onboarding

- **Component map**: Encoder E: R^d → R^K → Top-m mask → Prefix mask Λ_ℓ → Decoder D: R^(d×K)
- **Critical path**: 
  1. Sample prefix length ℓ∼p_ND
  2. Encode: z=E(x)
  3. Sparsify: z_sparse=Top_m(z)
  4. Truncate to prefix: z_prefix=Λ_ℓ · z_sparse
  5. Reconstruct: x̂=D · z_prefix
  6. Compute ||x-x̂||²_F
  7. (Optional) Check sweep schedule; freeze next unit if due
- **Design tradeoffs**: 
  - Orderedness vs. reconstruction loss: OSAEs sometimes show higher MSE for improved early-feature orderedness
  - Early vs. late feature stability: OSAE stability degrades for later features after crossover at ~1024–2048 prefix
  - Training compute: Covering all prefixes per epoch is ~K× more expensive than Matryoshka's fixed groups
- **Failure signatures**: 
  - Early-prefix orderedness near 0: Indicates p_ND may not have full support or unit sweeping is not engaged
  - High overall orderedness but poor Stab(D,D*): May indicate spark condition violated or data/model mismatch
  - Late features never activating: Gradient starvation; increase unit sweeping or adjust p_ND toward uniform
- **First 3 experiments**: 
  1. Replicate Section 3.1 Gaussian setup (d=80, K=100, m=5, Zipf α=1.2); verify Stab(D,D*)>0.7 and Ord(D,D*)>0.6
  2. Compare uniform p_ND vs. piecewise vs. Zipf-matched on Pythia-70M; measure prefix-wise orderedness curves
  3. Train OSAE and BatchTopK at K=4096 and K=65536 on same data; measure novel feature percentage (target: OSAE ≤40% vs. BatchTopK ~74%)

## Open Questions the Paper Calls Out

- **Training dynamics artifact**: Why does orderedness and stability sometimes decrease in early prefixes while increasing in later prefixes during training? The mechanism driving this counterintuitive training dynamic is unclear and may stem from the prefix distribution or optimization artifacts.

- **Misspecified ordering prior**: What happens to OSAE performance when the imposed ordering prior is misspecified relative to the true feature importance distribution? Theoretical guarantees assume correctly ordered atoms by sparsity-support frequency, but real data may violate this assumption.

- **Unit sweeping effectiveness**: Can unit sweeping or alternative training schedules resolve the trade-off between early-feature stability and late-feature stability? The paper notes unit sweeping helps but does not characterize its effect on the early-vs-late stability trade-off in real language models.

## Limitations

- The paper does not fully specify the prefix distribution parametrization used in the Gemma2-2B and Pythia-70M experiments, affecting reproducibility of orderedness curves
- Unit sweeping schedule hyperparameters (burn-in length, epochs-per-freeze T) are not provided, despite being critical for preventing gradient starvation
- The theoretical uniqueness guarantee requires spark(D)>2m, which becomes increasingly difficult to satisfy as dictionary size grows, with no discussion of performance degradation at scale

## Confidence

- **High confidence**: The core mechanism of using nested dropout over all prefixes to break permutation symmetry is well-supported theoretically and empirically demonstrated in the toy model
- **Medium confidence**: Empirical improvements on Gemma2-2B and Pythia-70M are promising but based on single runs without statistical significance testing
- **Low confidence**: The stitching experiment showing reduced novel features (73.8% → 33.8%) is compelling but relies on the correctness of the stitching methodology, which is not fully detailed

## Next Checks

1. Recreate the Gaussian setup (d=80, K=100, m=5, Zipf α=1.2) and verify that OSAE achieves Stab(D,D*)>0.7 and Ord(D,D*)>0.6, confirming theoretical claims against ground truth

2. Systematically compare uniform, piecewise, and Zipf-matched prefix distributions on Pythia-70M, measuring prefix-wise orderedness curves to identify optimal parametrization

3. Train OSAE and BatchTopK at K=4096 and K=65536 on identical data, then perform feature stitching to measure novel feature percentages, targeting OSAE ≤40% vs. BatchTopK ~74%