---
ver: rpa2
title: 'MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under
  Resource Constraints'
arxiv_id: '2504.09345'
source_url: https://arxiv.org/abs/2504.09345
tags:
- cache
- memory
- throughput
- decode
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high-throughput inference
  for Mixture-of-Experts (MoE) Large Language Models (LLMs) in resource-constrained
  environments, where GPU memory is insufficient to hold the entire model. The key
  issue is effectively utilizing both CPU and GPU resources while managing the overhead
  of transferring model weights from CPU to GPU.
---

# MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints

## Quick Facts
- arXiv ID: 2504.09345
- Source URL: https://arxiv.org/abs/2504.09345
- Authors: Yichao Yuan; Lin Ma; Nishil Talati
- Reference count: 40
- Primary result: 4.6x average speedup (up to 25.5x) over state-of-the-art MoE-Lightening for resource-constrained MoE LLM inference

## Executive Summary
MoE-Lens addresses the challenge of high-throughput inference for Mixture-of-Experts (MoE) Large Language Models when GPU memory is insufficient to hold the entire model. The system enables effective utilization of both CPU and GPU resources by streaming model weights from CPU memory while managing the overhead of CPU-GPU transfers. The key innovation is a two-stage performance model that predicts hardware limits and guides system design, achieving an average 94% accuracy in throughput prediction. MoE-Lens incorporates a resource-aware scheduler, optimized execution pipeline, and efficient data movement mechanisms to approach theoretical hardware limits.

## Method Summary
MoE-Lens employs a two-stage holistic performance model to analyze MoE LLM inference under resource constraints. Stage 1 establishes theoretical throughput bounds based on CPU memory capacity, GPU compute power, and workload characteristics through the Parallelism-Memory Efficiency (PME) metric. Stage 2 captures system execution mechanisms including workload scheduling and paged KV cache effects to predict achievable throughput. The system features a resource-aware scheduler for overlapping prefill and decode phases, a VSLPipe execution engine with contiguous data mover for efficient weight transfers, and an optimized CPU-based attention implementation. The approach is validated on diverse MoE models (Mixtral-8x7B, Mixtral-8x22B, DBRX) and datasets, demonstrating significant performance improvements over existing solutions.

## Key Results
- Achieves 4.6x average speedup (up to 25.5x) over MoE-Lightening state-of-the-art
- Theoretical performance model predicts throughput with 94% average accuracy
- Identifies CPU memory capacity for KV cache storage as primary bottleneck for GPU utilization
- Demonstrates prefill-decode overlap effectively improves CPU memory usage by reducing peak memory consumption

## Why This Works (Mechanism)

### Mechanism 1: CPU Memory Capacity Constrains GPU Utilization Through KV Cache Limits
The paper claims CPU memory capacity for KV cache storage is a primary limiting factor for GPU compute utilization in resource-constrained MoE inference, as it determines how many tokens can be processed in parallel to amortize weight transfer costs. Each token contributes to KV cache footprint, and larger parallel batches enable better amortization of CPU-GPU weight transfers. The model shows that saturating an A40 GPU with Mixtral-8x7B requires ~19,200 parallel tokens, which would need 1.2TB of CPU memory for KV cache—disproportionate for single-GPU systems.

### Mechanism 2: Prefill-Decode Overlap Increases Effective KV Cache Capacity
Overlapping prefill and decode phases reduces peak memory consumption and effectively enlarges usable KV cache capacity by allowing sequences to start decoding at different times. Rather than separating prefill and decode into isolated stages, overlapping allows some sequences to finish and release memory while others are still being prefilled. This reduces peak memory from maximum sequence length (p+g) to average sequence length (p+g/2), yielding an effective capacity amplification factor of (p+g)/(p+g/2).

### Mechanism 3: Contiguous Data Mover Eliminates IO Stalls Via Packetized Async Transfers
A dedicated async data mover issuing fine-grained packet transfers (100MB packets) achieves higher CPU-GPU bandwidth utilization than bulk synchronous transfers. Rather than embedding data movement in the execution pipeline, MoE-Lens uses a separate C++ thread that partitions weight transfers into packets. This prevents head-of-line blocking where large weight transfers delay latency-sensitive compute transfers (attention synchronization data). The data mover synchronizes only at stage boundaries, not per-phase.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) Sparse Activation
  - Why needed here: MoE models route each token through only k experts (typically 2 of 8), but all expert weights must reside in memory. This creates the fundamental tension: high total parameters (47-141B) but low per-token compute, making memory capacity the bottleneck rather than FLOPs.
  - Quick check question: For Mixtral-8x7B with top-2 routing, what fraction of expert parameters are activated per token? (Answer: 2/8 = 25%)

- Concept: Arithmetic Intensity and Roofline Analysis
  - Why needed here: The paper extends roofline models to show that achieving GPU compute saturation requires processing enough tokens in parallel such that GEMM compute exceeds IO overhead. Equation 1 derives intensity I ≈ n(N_k/N_e), showing parallelism needed scales with GPU throughput / PCIe bandwidth.
  - Quick check question: Why does prefill have higher memory efficiency than decode? (Answer: Prefill processes all prompt tokens simultaneously, amortizing memory cost; decode processes one token per sequence autoregressively.)

- Concept: Paged KV Cache with Block-Based Memory Management
  - Why needed here: The Stage 2 model accounts for paged KV cache organization, which shifts the memory-compute turning point rightward compared to theoretical analysis. Block allocation introduces fragmentation that reduces effective capacity compared to contiguous allocation assumptions.
  - Quick check question: How does paged KV cache affect the Stage 2 model's convergence to Stage 1 bounds? (Answer: As block size approaches 1 and batch size approaches infinity, Stage 2 converges to Stage 1; larger blocks shift the compute-bound turning point to require more memory.)

## Architecture Onboarding

- Component map: Pipeline Profiler -> Resource-Aware Scheduler -> VSLPipe Execution Engine -> Contiguous Data Mover -> GPU Weight Buffer -> CPU Attention Kernel
- Critical path: Weight transfer from CPU to GPU via Contiguous Data Mover → GPU GEMM computation (Task A: QKV + flash attention, Task B: O projection + MoE) ↔ CPU decode attention. The bottleneck shifts between: (1) CPU-GPU IO when insufficient parallel tokens, (2) GPU compute when tokens saturate, (3) CPU memory bandwidth when KV cache large and attention competes with weight transfers.
- Design tradeoffs:
  - Larger KV cache → more parallel tokens → higher throughput, but increased CPU attention compute competing with weight transfer bandwidth
  - Smaller packet sizes → less interference with compute transfers, but more overhead; 100MB empirically optimal
  - Aggressive prefill scheduling → better GPU utilization, but risks KV cache overflow triggering preemption cascades
  - CPU attention offload → eliminates KV cache GPU transfer, but requires vectorized implementation to avoid becoming bottleneck
- Failure signatures:
  - Premature prefill exhaustion: If token threshold n_real overestimated, prefill queue empties before decode completes, GPU utilization drops
  - Preemption thrashing: When generation length exceeds KV cache capacity, sequences repeatedly evicted and re-prefilled, throughput oscillates
  - CPU memory bandwidth saturation: With large KV cache (210GB) and long generation (256 tokens), attention compute delays weight transfers by ~20% (5s→6s transfer time)
- First 3 experiments:
  1. Profile baseline token threshold: Run Pipeline Profiler on target hardware to find n_real by varying prefill tokens and measuring GPU time; validate against Equation 2 prediction before scheduling optimization.
  2. Measure KV cache utilization at steady state: With fixed batch size and sequence lengths, compare achieved GPU utilization against PME-predicted maximum; if utilization <80% and CPU memory not full, investigate scheduling inefficiencies.
  3. Stress test preemption boundaries: Fix KV cache size and systematically increase generation length until preemption observed; verify throughput degradation matches Stage 2 model prediction and identify if preemption recovery is slower than modeled.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MoE-Lens approach be extended to online serving scenarios with strict latency constraints while maintaining throughput benefits?
- Basis in paper: [explicit] The authors explicitly state "our focus is offline, batching processing inference tasks" and contrast with "traditional LLM serving systems optimized for latency-sensitive applications like chatbots and code completion, where low response time is critical."
- Why unresolved: The prefill-decode overlap and batching strategies optimize for throughput but may introduce latency variability that is unacceptable for interactive applications.
- What evidence would resolve it: A modified scheduler incorporating latency SLAs, evaluated on latency-sensitive workloads with per-token latency measurements.

### Open Question 2
- Question: How will the CPU memory capacity bottleneck scale as GPU compute capabilities continue to outpace memory capacity improvements?
- Basis in paper: [explicit] Table 2 shows KV cache requirements growing from 614GB to 2554GB when moving from A40 to A100, and the paper states "This memory demand grows with increasing GPU compute capabilities."
- Why unresolved: The paper demonstrates the problem exists but does not propose solutions for the architectural trend where compute scaling outpaces memory scaling.
- What evidence would resolve it: Analysis with next-generation GPUs or simulation of higher compute-to-memory ratios showing whether the 4.6x speedup can be sustained.

### Open Question 3
- Question: Can the CPU attention kernel be further optimized to overcome the memory controller contention observed beyond 20 threads?
- Basis in paper: [explicit] The authors note "throughput gain saturates beyond 20 threads, likely due to memory controller contention" and identify this as limiting factor when KV cache is large (210GB cases).
- Why unresolved: The hand-optimized SIMD implementation achieves 4.7x improvement but hits a hardware-level bottleneck not addressed in the current design.
- What evidence would resolve it: NUMA-aware scheduling, alternative memory access patterns, or hybrid CPU-GPU attention partitioning showing improved scaling beyond 20 threads.

## Limitations
- The theoretical performance model relies on idealized assumptions that may not hold in practice, achieving 94% average accuracy but ignoring PCIe protocol overhead and CPU cache effects.
- The CPU attention kernel optimization is critical to performance but underspecified, with no implementation details provided for the claimed 4.7× speedup.
- The preemption mechanism lacks detailed analysis of recovery overhead and fairness, with no quantification of performance impact from repeated preemption cycles.

## Confidence

**High Confidence**: The fundamental insight that CPU memory capacity for KV cache storage limits GPU utilization is well-supported by the mathematical analysis and experimental validation.

**Medium Confidence**: The effectiveness of prefill-decode overlap in reducing peak memory consumption is demonstrated empirically but assumes ideal scheduling without accounting for synchronization overhead.

**Medium Confidence**: The contiguous data mover with packetized transfers shows performance improvements, but the optimal packet size (100MB) is empirically determined without theoretical justification.

**Low Confidence**: The CPU attention kernel performance claims cannot be independently verified due to lack of implementation details, making the 4.7× speedup particularly difficult to assess.

## Next Checks

1. **Validate theoretical model bounds across hardware configurations**: Test the two-stage performance model on different CPU-GPU combinations (e.g., AMD EPYC + NVIDIA A100, Intel Sapphire Rapids + H100) to verify the 94% accuracy claim holds across architectures.

2. **Benchmark CPU attention kernel scalability**: Implement the CPU attention kernel using the described optimization techniques (SIMD intrinsics, loop unrolling, prefetching) and measure its throughput scaling with thread count and CPU frequency.

3. **Stress-test preemption recovery mechanisms**: Design workloads with sequences requiring generation lengths exceeding available KV cache capacity to trigger preemption and measure the performance impact of repeated preemption cycles.