---
ver: rpa2
title: 'FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual
  Training'
arxiv_id: '2509.02521'
source_url: https://arxiv.org/abs/2509.02521
tags:
- arxiv
- audio
- training
- flm-audio
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FLM-Audio addresses the scalability and language modeling limitations\
  \ of existing full-duplex speech systems by introducing contiguous monologues\u2014\
  continuous sentences with wait intervals\u2014paired with a dual training paradigm.\
  \ This approach replaces word-level text-audio alignment with sentence-level alignment,\
  \ preserving autoregressive language model strengths and enabling real-time, native\
  \ full-duplex speech without sacrificing semantic understanding."
---

# FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training

## Quick Facts
- arXiv ID: 2509.02521
- Source URL: https://arxiv.org/abs/2509.02521
- Authors: Yiqun Yao; Xiang Li; Xin Jiang; Xuezhi Fang; Naitong Yu; Wenjia Ma; Aixin Sun; Yequan Wang
- Reference count: 21
- Primary result: Achieves state-of-the-art full-duplex speech with less than 15% of training data used by comparable systems

## Executive Summary
FLM-Audio addresses the scalability and language modeling limitations of existing full-duplex speech systems by introducing contiguous monologues—continuous sentences with wait intervals—paired with a dual training paradigm. This approach replaces word-level text-audio alignment with sentence-level alignment, preserving autoregressive language model strengths and enabling real-time, native full-duplex speech without sacrificing semantic understanding. The model is trained across four stages using both TTS- and ASR-style data formats, which improves both audio understanding and responsiveness.

## Method Summary
FLM-Audio is a native full-duplex speech chatbot that uses contiguous monologues (continuous sentences with wait intervals) and dual training to enable real-time speech generation with low latency. The model uses a Qwen-2.5-VL 7B backbone with a depth transformer generating 8 audio tokens per frame via a Mimi codec. Training occurs in four stages: post-training-1 (TTS+ASR dual format), post-training-2 (ASR-style), fine-tuning-1 (TTS-style), and fine-tuning-2 (free interruptions). The model achieves native full-duplex operation with ~80ms latency through embedding summation across text, listening, and speaking channels.

## Key Results
- Achieves 18.2 WER vs 22.3 WER for word-level alignment on Fleurs-zh
- Outperforms baselines with less than 15% of training data
- Demonstrates state-of-the-art full-duplex performance with low latency (~80ms)

## Why This Works (Mechanism)

### Mechanism 1
Contiguous monologues preserve language modeling capabilities better than word-level alignment by generating uninterrupted text sequences rather than inserting pad tokens between each word. This maintains autoregressive language modeling patterns from pretraining, allowing the model to generate complete thoughts while the audio channel produces speech concurrently with `<wait>` tokens filling gaps.

### Mechanism 2
Dual training (alternating TTS-style and ASR-style formats) enables robust handling of asynchronous text-audio semantics by exposing the model to both directional mappings. This covers comprehension (audio→text) and generation (text→audio) modes within a unified framework, ensuring the model can handle unpredictable interleaving of comprehension and generation.

### Mechanism 3
Native full-duplex architecture achieves lower latency than TDM by merging channels per timestep rather than interleaving tokens. By summing embeddings from text, listening, and speaking channels at each frame, the model keeps context length constant regardless of channel count, enabling ~80ms latency versus 2+ seconds for TDM approaches.

## Foundational Learning

- **Autoregressive Language Modeling**
  - Why needed here: The paper's core claim is that preserving AR language patterns (contiguous sequences) is critical for dialog quality. Understanding next-token prediction helps grasp why word-level padding disrupts pretrained knowledge.
  - Quick check question: Can you explain why inserting pad tokens between content tokens would degrade a model's ability to predict coherent text?

- **Residual Vector Quantization (RVQ) / RQ-Transformer**
  - Why needed here: FLM-Audio uses a depth transformer to generate 8 audio tokens per frame (1 semantic + 7 acoustic) in a locally autoregressive manner. You need to understand hierarchical token generation to follow the audio pipeline.
  - Quick check question: How does generating tokens hierarchically (semantic first, then acoustic) differ from generating all tokens flatly in one sequence?

- **Streaming Audio Codecs (e.g., Mimi)**
  - Why needed here: The Mimi encoder/decoder bridges raw waveforms and discrete tokens at 12.5 fps with 8 codebooks. Understanding codec latency and token semantics is essential for debugging audio quality issues.
  - Quick check question: What is the relationship between frame rate (12.5 Hz), tokens per frame (8), and real-time speech bitrate?

## Architecture Onboarding

- **Component map:** Audio input → Mimi encoder → 8 listening tokens per frame → backbone → hidden state h_t → depth transformer → 8 speaking tokens per frame → Mimi decoder → output waveform

- **Critical path:** 1) Audio input → Mimi encoder → 8 listening tokens per frame 2) Tokens + text embeddings → backbone → hidden state h_t 3) h_t → depth transformer → 8 speaking tokens per frame 4) Speaking tokens → Mimi decoder → output waveform

- **Design tradeoffs:**
  - Data volume vs. annotation granularity: FLM-Audio uses ~1M hours with sentence-level timestamps vs Moshi's 8M+ hours with word-level timestamps
  - Latency vs. modeling capacity: Native full-duplex minimizes latency but limits explicit cross-channel attention
  - Loss weighting: Paper uses α₁=1, α₂=0.5, β=1, γ=0.01 (wait tokens downweighted) vs Moshi's α₁=100, α₂=1, γ=0.5

- **Failure signatures:**
  - Degraded ASR performance: May indicate insufficient ASR-style training in post-training-2 or fine-tuning-1 stages
  - Long pauses between sentences: May indicate `<wait>` token loss weight (γ) is too high or insufficient TTS-style training
  - Poor interruption handling: Check fine-tuning-2 data includes interruption simulation (0.7 probability, 0.5s reaction delay)

- **First 3 experiments:**
  1. Reproduce ablation (Table 7) on a small subset: Train two models on 5% of data with contiguous vs word-level alignment; verify WER and HellaSwag gap
  2. Ablate Fine-tuning-1 (ASR-style stage): Skip directly from post-training to Fine-tuning-2; measure instruction-following score drop
  3. Measure latency breakdown: Profile time spent in backbone, depth transformer, and Mimi codec separately; confirm native approach achieves <100ms end-to-end latency

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of native full-duplex models utilizing contiguous monologues scale compared to TDM-based approaches when parameter counts exceed 7B (e.g., 70B or 520B)? The study was resource-constrained to a ~7B model, leaving scaling laws for this architecture unverified.

### Open Question 2
Would a word-level alignment strategy (as used in Moshi) outperform contiguous monologues if both were trained on the full dataset volume (1M+ hours) rather than a 5% subset? The current ablation only proves contiguous monologues converge faster on small data subsets.

### Open Question 3
Does the "contiguous monologue" strategy introduce failure modes in scenarios where text generation significantly outpaces audio synthesis, beyond the standard `<wait>` token handling? The paper doesn't analyze failure cases in extreme asynchronous scenarios.

## Limitations

- Architectural details: The depth transformer architecture is minimally specified beyond being "locally autoregressive"
- Data alignment methodology: Exact method for extracting sentence-level timestamps from audio remains unspecified
- Generalization scope: Performance on languages beyond Chinese and English is unvalidated

## Confidence

**High Confidence:**
- Contiguous monologues improve language modeling compared to word-level alignment (supported by ablation studies)
- Dual training paradigm is necessary for full-duplex performance (supported by ablation showing performance drop when skipping ASR-style training)

**Medium Confidence:**
- Native full-duplex achieves 80ms latency vs 2+ seconds for TDM (supported by architectural analysis but lacking direct latency measurements)
- FLM-Audio outperforms baselines with <15% of training data (claim is well-supported but doesn't account for potential quality differences in training data sources)

**Low Confidence:**
- Robustness to interruptions and turn-taking (human eval shows improvements but objective metrics are not provided)
- Generalization to unseen voices and speaking styles (limited to 700 voices with DNSMOS filtering)

## Next Checks

1. **Replicate ablation study with independent implementation:** Train two models from scratch (contiguous vs word-level alignment) on the same 5% subset of data. Verify the 18.2 vs 22.3 WER gap on Fleurs-zh and 61.6 vs 58.3 on HellaSwag.

2. **Latency measurement validation:** Instrument the complete pipeline (Mimi encoder → backbone → depth transformer → Mimi decoder) to measure actual end-to-end latency under realistic conditions. Compare against claimed 80ms and TDM baselines.

3. **Cross-lingual generalization test:** Evaluate the trained model on a held-out language (e.g., Spanish or French) from the same data distribution. Measure ASR WER, TTS quality, and dialog task performance.