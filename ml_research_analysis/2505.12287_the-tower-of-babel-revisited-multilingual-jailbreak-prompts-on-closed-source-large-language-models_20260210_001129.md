---
ver: rpa2
title: 'The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source
  Large Language Models'
arxiv_id: '2505.12287'
source_url: https://arxiv.org/abs/2505.12287
tags:
- safety
- arxiv
- content
- chinese
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates multilingual jailbreak attacks on closed-source
  large language models (LLMs), focusing on four frontier models: GPT-4o, DeepSeek-R1,
  Gemini-1.5-Pro, and Qwen-Max. Using an integrated adversarial framework with 32
  attack prompts across six security categories in both English and Chinese, the research
  evaluates Attack Success Rates (ASR) under 38,400 responses.'
---

# The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models

## Quick Facts
- arXiv ID: 2505.12287
- Source URL: https://arxiv.org/abs/2505.12287
- Reference count: 40
- Key outcome: Qwen-Max is the most vulnerable model (84% ASR) while GPT-4o is strongest (35% ASR), with Chinese prompts consistently yielding higher attack success rates than English across four frontier LLMs.

## Executive Summary
This study systematically evaluates multilingual jailbreak attacks on closed-source LLMs, revealing significant vulnerabilities in safety alignment across languages. Using 32 adversarial prompts across six security categories in both English and Chinese, the research demonstrates that current safety mechanisms are unevenly effective across languages, with Chinese prompts showing consistently higher success rates. The novel Two-Sides attack technique proves most effective, forcing models to generate harmful content under the guise of balanced analysis. The findings expose critical gaps in multilingual safety alignment and highlight the need for robust cross-lingual defenses in future AI systems.

## Method Summary
The study employs an integrated adversarial framework testing four frontier models (GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, Qwen-Max) using 32 forbidden queries across six security categories. Each query is translated between English and Chinese with LaBSE semantic verification (avg 0.86 similarity). Four modular jailbreak components are combined into six prompt variants: Full (all components), and five ablations (no Setting+Character, no Sandwich, no Two-Sides, no Guide Words, pure attack). Each prompt variant is executed 25 times per model, yielding 38,400 total responses evaluated by human annotators across three categories: Success, Fail, or Response but Acceptable.

## Key Results
- Qwen-Max shows highest vulnerability with 84% average ASR, while GPT-4o demonstrates strongest defense at 35% average ASR
- Chinese prompts achieve consistently higher ASRs than English across all models (0.05-0.21 increase)
- Two-Sides attack technique is most effective, exploiting models' exhaustive reasoning mode
- Setting+Character and Sandwich components significantly improve attack success when present
- Sexual Content category shows highest vulnerability (78% average ASR across models)

## Why This Works (Mechanism)

### Mechanism 1: Two-Sides Framing Bypasses Safety Refusal Heuristics
The Two-Sides attack forces models to generate "both supporting and opposing" arguments for harmful propositions, reframing prohibited content as balanced analytical output. Models enter an exhaustive reasoning mode where the instruction to be "neutral and comprehensive" overrides safety refusal patterns. During early token generation, models lack holistic intent understanding and cannot retroactively identify that their "balanced analysis" contains harmful content.

### Mechanism 2: Cross-Lingual Safety Alignment Gaps
Non-English prompts achieve higher attack success rates due to weaker safety alignment in multilingual training corpora. RLHF alignment is predominantly derived from English corpora, with non-English languages having fewer negative safety examples and less coverage of euphemisms, slang, and implicit harmful expressions. Models misclassify nuanced non-English content as safe due to insufficient adversarial training data.

### Mechanism 3: Contextual Framing Dilutes Safety Trigger Activation
Embedding harmful requests within extended benign context lowers safety mechanism activation rates. Safety classifiers trained on direct harmful queries fail to detect threats distributed across multiple turns or embedded in narrative frames. Models prioritize local coherence over global intent assessment, allowing malicious payloads to pass when "sandwiched" between harmless questions.

## Foundational Learning

- **Attack Success Rate (ASR)**
  - Why needed here: Primary quantitative metric for comparing jailbreak effectiveness across models, languages, and prompt designs
  - Quick check question: If a model refuses 85 out of 100 harmful requests, what is its ASR? (Answer: 15%)

- **RLHF (Reinforcement Learning from Human Feedback)**
  - Why needed here: The dominant alignment methodology whose cross-lingual limitations explain the observed language asymmetry in ASR
  - Quick check question: Why might RLHF trained primarily on English data fail to generalize safety behavior to Chinese? (Answer: Insufficient negative examples and safety patterns in non-English corpora)

- **Ablation Study**
  - Why needed here: Systematic component removal isolates which prompt elements drive attack effectiveness
  - Quick check question: If removing a prompt component increases ASR, what does that indicate about its role? (Answer: The component was acting as a safety mechanism or context diluter)

## Architecture Onboarding

- **Component map:**
  Forbidden Query Set (32 queries × 2 languages) → Prompt Injection Framework (6 variants) → Target Model Layer (4 APIs) → Response Collection (25× per variant) → Human Evaluation (3-class labeling) → ASR Aggregation

- **Critical path:**
  1. Construct semantically equivalent bilingual query pairs
  2. Generate all prompt variant combinations
  3. Execute API calls with retry logic and rate limiting
  4. Log all responses to JSON with metadata
  5. Route through dual-annotator labeling pipeline
  6. Aggregate ASR by model, language, attack type, and category

- **Design tradeoffs:**
  - Manual labeling (higher accuracy) vs. automated classification (scalability)
  - Two languages (controlled comparison) vs. broader multilingual coverage (generalizability)
  - API default configs (real-world conditions) vs. controlled temperature/sampling (reproducibility)

- **Failure signatures:**
  - ASR variance >30% between language pairs signals cross-lingual alignment gaps
  - Two-Sides attack achieving >40% higher ASR than direct requests indicates reasoning-mode exploitation
  - Category-specific spikes (e.g., Sexual Content at 78% avg ASR) reveal domain alignment weaknesses

- **First 3 experiments:**
  1. Replicate Two-Sides ablation on a new model family not in the original study to validate transferability of the mechanism
  2. Extend to a third language (e.g., Japanese or Spanish) with a smaller pretraining corpus fraction to map alignment degradation patterns
  3. Test reordered prompt variants (e.g., Two-Sides before Sandwich) to determine if attack component sequencing affects ASR

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Do the safety alignment gaps observed in Chinese contexts persist or worsen in low-resource languages where training data is significantly scarcer?
**Basis in paper:** [inferred] Section 5.3 (Limitations) states the study was restricted to Chinese and English, neglecting the need to assess vulnerabilities in low-resource languages mentioned in Section 2.3.
**Why unresolved:** The paper highlights the "two problem" for non-English languages but lacks empirical data on languages outside the Chinese-English binary to confirm if ASRs rise as resource availability drops.
**What evidence would resolve it:** extending the integrated adversarial framework to test models in low-resource languages (e.g., Swahili, Welsh) and comparing the Attack Success Rates against the Chinese and English baselines.

### Open Question 2
**Question:** How can explainability methods, such as "AI Biology," be integrated into white-box attacks to uncover the specific internal mechanisms that allow "Two-Sides" attacks to succeed?
**Basis in paper:** [explicit] Section 5.2 (Future Work) calls for "Developing Explainable White-Box Attack Methods" to uncover underlying mechanisms behind jailbreak vulnerabilities.
**Why unresolved:** Current jailbreak strategies are primarily black-box; the internal decision-making processes that cause models to prioritize "balanced debate" over safety refusal remain opaque.
**What evidence would resolve it:** Identification of specific attention heads or features that trigger compliance during "Two-Sides" prompts, demonstrated through causal intervention or activation patching.

### Open Question 3
**Question:** How can automated evaluation pipelines be optimized to match the accuracy of the manual review process used in this study while mitigating the timeliness limitations of human annotation?
**Basis in paper:** [inferred] Section 5.3 (Limitations) cites "Insufficient Timeliness" because manual evaluation is resource-intensive, causing static assessments to lose relevance as models update.
**Why unresolved:** There is a trade-off between the precision of PhD-level human evaluators (used in this paper) and the speed required to keep up with rapid model iterations.
**What evidence would resolve it:** A benchmark showing that a fine-tuned automated classifier can replicate the "Success," "Fail," or "Response but Acceptable" distinctions made by human experts with high inter-annotator agreement.

## Limitations
- Study restricted to only two languages (English and Chinese), limiting generalizability to other language families
- Manual human evaluation process introduces subjectivity and limits scalability
- API default inference configurations may not capture full range of model behaviors
- Absence of gray-box information about models' internal safety mechanisms constrains deeper understanding

## Confidence

**High Confidence (8/10):** The empirical findings regarding ASR differences between models and the effectiveness of the Two-Sides attack technique are well-supported by the large-scale evaluation (38,400 responses) and consistent across multiple experimental conditions.

**Medium Confidence (6/10):** The claims about language-specific vulnerabilities are moderately supported, though the study's limited scope to Chinese and English prevents definitive conclusions about the mechanism behind cross-lingual alignment gaps.

**Low Confidence (4/10):** The hypothesized mechanisms explaining why certain attack components work (particularly the Two-Sides framing bypassing safety refusal heuristics) remain speculative without access to model internals or fine-grained safety classifier behavior.

## Next Checks

1. **Cross-Lingual Alignment Mapping:** Extend the evaluation framework to a third language with minimal pretraining representation (e.g., Finnish or Swahili) to empirically map the relationship between pretraining corpus fraction and safety alignment strength across the language spectrum.

2. **Attack Component Sequencing Analysis:** Systematically vary the order of prompt components (e.g., Two-Sides before vs. after Sandwich) across multiple models to determine whether attack effectiveness depends on component sequencing or whether the components operate independently.

3. **Safety Mechanism Ablation:** Conduct controlled experiments with models where specific safety mechanisms are known to be disabled (e.g., using open-source models with safety layers that can be selectively removed) to isolate which components of the safety pipeline are most vulnerable to the Two-Sides attack framing.