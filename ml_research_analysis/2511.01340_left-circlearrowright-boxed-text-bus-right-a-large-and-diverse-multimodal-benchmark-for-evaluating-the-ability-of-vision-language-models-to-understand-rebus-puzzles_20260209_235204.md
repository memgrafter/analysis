---
ver: rpa2
title: '$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse
  Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand
  Rebus Puzzles'
arxiv_id: '2511.01340'
source_url: https://arxiv.org/abs/2511.01340
tags:
- rebus
- puzzles
- puzzle
- reasoning
- gpt-4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces |/sync-alt\u1F68C|, a large and diverse\
  \ multimodal benchmark of 1,333 English Rebus Puzzles designed to evaluate Vision-Language\
  \ Models' ability to solve puzzles that require image recognition, commonsense reasoning,\
  \ multi-step reasoning, and image-based wordplay. The dataset includes varied artistic\
  \ styles and 18 categories, with some puzzles augmented using ControlNet to add\
  \ distracting backgrounds and increase difficulty."
---

# $\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles

## Quick Facts
- arXiv ID: 2511.01340
- Source URL: https://arxiv.org/abs/2511.01340
- Reference count: 40
- Introduces |/sync-altá½¨C|, a multimodal benchmark of 1,333 English Rebus Puzzles to evaluate Vision-Language Models' reasoning capabilities

## Executive Summary
This paper introduces a novel multimodal benchmark consisting of 1,333 English rebus puzzles designed to evaluate Vision-Language Models' ability to solve puzzles requiring image recognition, commonsense reasoning, multi-step reasoning, and image-based wordplay. The dataset includes 18 categories with varied artistic styles and augmented puzzles using ControlNet to increase difficulty. The authors propose RebusDescProgICE, a compute-efficient framework that combines unstructured image descriptions with structured, code-based reasoning, achieving 2.1-4.1% improvements on closed-source models and 20-30% improvements on open-source models compared to Chain-of-Thought reasoning.

## Method Summary
The research introduces RebusDescProgICE, a compute-efficient, model-agnostic framework that integrates unstructured image descriptions with structured, code-based reasoning to solve rebus puzzles. The framework employs a novel in-context example selection strategy to enhance performance. The benchmark includes 1,333 rebus puzzles across 18 categories, with some puzzles augmented using ControlNet to add distracting backgrounds. The methodology involves evaluating both closed- and open-source Vision-Language Models on the benchmark, comparing performance against baseline Chain-of-Thought reasoning approaches.

## Key Results
- RebusDescProgICE framework improves performance by 2.1-4.1% on closed-source models and 20-30% on open-source models compared to Chain-of-Thought reasoning
- GPT-4o achieves a word-level F1 score of 0.402 on augmented test data
- The framework demonstrates consistent gains across model types, particularly benefiting weaker open-source models

## Why This Works (Mechanism)
The framework's effectiveness stems from its hybrid approach that combines the flexibility of unstructured image descriptions with the precision of structured, code-based reasoning. This dual-modality processing allows the model to first gain a broad understanding of visual elements through descriptions, then apply systematic, rule-based reasoning to decode the wordplay and logic inherent in rebus puzzles. The in-context example selection strategy ensures that the most relevant and instructive examples are presented to the model during inference, optimizing the learning transfer from examples to novel puzzles.

## Foundational Learning
- **Image Recognition**: Understanding visual elements in rebus puzzles - Why needed: Puzzles rely on visual cues and symbols. Quick check: Can the model identify basic shapes, objects, and text within images.
- **Commonsense Reasoning**: Applying real-world knowledge to interpret puzzle clues - Why needed: Many puzzles reference cultural knowledge or everyday concepts. Quick check: Can the model connect visual elements to common phrases or idioms.
- **Multi-step Reasoning**: Breaking down complex puzzles into sequential logical steps - Why needed: Rebus solutions often require progressive interpretation of visual elements. Quick check: Can the model explain its reasoning process step-by-step.
- **Wordplay Interpretation**: Understanding visual puns, homophones, and linguistic tricks - Why needed: The core challenge of rebus puzzles is decoding visual language games. Quick check: Can the model recognize when similar-sounding words or visual substitutions are being used.
- **ControlNet Augmentation**: Generating distractor backgrounds to increase puzzle difficulty - Why needed: Testing model robustness against visual noise. Quick check: Can the model maintain performance when irrelevant visual information is added.

## Architecture Onboarding

**Component Map**: Image -> Unstructured Description Generator -> Structured Code Reasoning Engine -> Answer Generator

**Critical Path**: The critical path involves first generating an unstructured description of the image, then using this description as input to the structured code reasoning engine, which processes the information through a series of logical steps to arrive at the final answer.

**Design Tradeoffs**: The framework trades some computational efficiency for accuracy by using both unstructured and structured approaches. While pure unstructured reasoning might be faster, the structured code-based component provides more reliable logical processing for the complex reasoning required in rebus puzzles.

**Failure Signatures**: Models may fail when puzzles require deep cultural knowledge, abstract reasoning beyond the examples provided, or when visual distractors are too overwhelming for the description generator to filter effectively.

**First Experiments**:
1. Test baseline Chain-of-Thought reasoning on a small subset of puzzles to establish performance baselines
2. Evaluate the impact of different in-context example selection strategies on model performance
3. Compare performance of closed-source versus open-source models on both original and augmented puzzle sets

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The dataset is limited to English-language rebus puzzles, constraining cross-linguistic validity
- Performance gains on open-source models (20-30%) versus closed-source models (2.1-4.1%) suggest varying effectiveness depending on underlying model capabilities
- Word-level F1 scores may not fully capture semantic correctness of puzzle solutions, as rebus puzzles often have multiple valid interpretations

## Confidence

**High confidence**: The benchmark construction methodology and dataset statistics are well-documented and reproducible. The framework architecture (RebusDescProgICE) is clearly specified with implementation details.

**Medium confidence**: Performance improvements are statistically significant but may be influenced by the specific puzzle distribution and augmentation techniques used. The generalizability to other multimodal reasoning tasks remains uncertain.

**Low confidence**: The framework's effectiveness on extremely complex puzzles requiring deep cultural knowledge or highly abstract reasoning is not thoroughly evaluated.

## Next Checks
1. Conduct cross-linguistic validation by translating puzzles to other languages and assessing framework performance across language models
2. Evaluate framework robustness on naturally occurring visual noise patterns (real-world photographs with backgrounds, occlusions, varying lighting) rather than synthetically generated distractors
3. Perform ablation studies on the in-context example selection strategy to quantify its specific contribution to performance gains versus other framework components