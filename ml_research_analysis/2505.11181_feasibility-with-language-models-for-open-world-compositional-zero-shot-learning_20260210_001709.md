---
ver: rpa2
title: Feasibility with Language Models for Open-World Compositional Zero-Shot Learning
arxiv_id: '2505.11181'
source_url: https://arxiv.org/abs/2505.11181
tags:
- feasibility
- classes
- pairs
- feasible
- seen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Feasibility with Language Model (FLM), a method
  that uses Large Language Models (LLMs) to determine the feasibility of state-object
  pairs in Open-World Compositional Zero-Shot Learning (OW-CZSL). FLM queries LLMs
  with prompts containing related seen classes and retrieves the logit for the positive
  answer as a feasibility score.
---

# Feasibility with Language Models for Open-World Compositional Zero-Shot Learning

## Quick Facts
- arXiv ID: 2505.11181
- Source URL: https://arxiv.org/abs/2505.11181
- Reference count: 40
- Authors: Jae Myung Kim; Stephan Alaniz; Cordelia Schmid; Zeynep Akata
- Key outcome: FLM achieves consistent H and AUC improvements across MIT-States, UT-Zappos, and C-GQA when integrated with CLIP, CoOp, and CSP

## Executive Summary
Feasibility with Language Model (FLM) introduces a novel approach to Open-World Compositional Zero-Shot Learning by using Large Language Models to determine the feasibility of state-object pairs. The method queries LLMs with prompts containing related seen classes and retrieves the logit for the positive answer as a feasibility score. By leveraging in-context learning, FLM improves feasibility predictions even for rare or implausible compositions, outperforming previous methods across three benchmarks.

## Method Summary
FLM uses LLMs to filter infeasible state-object pairs before VLM classification in OW-CZSL. For each query pair, it constructs prompts with related seen class examples (guidance), queries Vicuna-13B for logits, extracts the "Yes" token logit as a feasibility score, and thresholds at τ to prune Y_all. The filtered candidates are then classified by CLIP/CoOp/CSP. The approach relies on semantic transfer from guidance examples and logit-based scoring rather than binary outputs.

## Key Results
- FLM achieves 75.4% arithmetic mean feasibility accuracy vs. 72.5% (GloVe) and 72.3% (ConceptNet) on MIT-States
- Consistent H and AUC improvements across all three datasets (MIT-States, UT-Zappos, C-GQA)
- Performance gains maintained across all three VLM backbones (CLIP, CoOp, CSP)
- Related guidance examples outperform random ones and no-guidance baselines

## Why This Works (Mechanism)

### Mechanism 1: In-Context Guidance for Dataset-Specific Feasibility Calibration
Providing related seen class examples calibrates LLM judgments to dataset-specific conventions. The guidance component supplies semantically related feasible pairs (sharing state or object with the query), enabling the LLM to infer context-appropriate associations. Core assumption: LLMs can transfer patterns from provided examples to novel queries within the same prompt context.

### Mechanism 2: Logit Extraction as Graded Feasibility Signal
Extracting the unnormalized logit for "Yes" provides a continuous feasibility score that preserves uncertainty information better than binary outputs. Rather than thresholding at generation time, FLM accesses the output distribution before sampling, using log pLLM(t="Yes"|prompt) as a real-valued score. Core assumption: The LLM's internal confidence distribution correlates with human judgments of compositional plausibility.

### Mechanism 3: Semantic Knowledge Transfer from Pretrained LLMs
LLMs pretrained on large text corpora encode commonsense knowledge about which state-object combinations are semantically coherent, outperforming static embeddings. Unlike GloVe or ConceptNet, LLMs capture contextual relationships through attention over learned representations, enabling better generalization to rare compositions. Core assumption: Pretraining corpora contain sufficient coverage of attribute-object co-occurrences and semantic reasoning patterns.

## Foundational Learning

- Concept: **Open-World Compositional Zero-Shot Learning (OW-CZSL)**
  - Why needed here: The task setting where the label space includes ALL possible state-object combinations, requiring explicit feasibility filtering to avoid predicting implausible classes
  - Quick check question: Can you explain why OW-CZSL is harder than closed-world CZSL, and what role feasibility scoring plays in reducing the candidate space?

- Concept: **Vision-Language Models (VLMs) for CZSL**
  - Why needed here: FLM is model-agnostic and tested on CLIP, CoOp, and CSP—all VLMs that leverage joint embeddings. Understanding their inference helps integrate feasibility filtering
  - Quick check question: How does CLIP classify an image as a state-object pair, and where would you inject a feasibility mask in its pipeline?

- Concept: **In-Context Learning (ICL) in LLMs**
  - Why needed here: FLM relies on ICL to adapt the LLM to dataset-specific feasibility norms without fine-tuning. The prompt design (guidance examples) is critical
  - Quick check question: What happens to FLM's performance if you provide random seen pairs instead of semantically related ones (e.g., "old cat" when querying "dark fire")?

## Architecture Onboarding

- Component map: Prompt Constructor -> LLM Interface -> Feasibility Scorer -> Threshold Filter -> VLM Integration
- Critical path: Prompt design → Guidance pair selection (Y_pos) → Logit extraction → Threshold tuning → VLM inference
- Design tradeoffs:
  - Open-source vs. API LLMs: Vicuna provides logits (better granularity); ChatGPT is more capable but API-only (binary only)
  - Number of guidance examples: More examples improve performance up to a point (50 for MIT-States, 200 for C-GQA)
  - Prompt format: List-based guidance outperforms QA-format guidance (avoids "Yes" bias)
- Failure signatures:
  - Over-pruning: If threshold τ is too aggressive, feasible unseen classes are excluded
  - Under-pruning: If τ is too lenient, infeasible classes pollute predictions
  - Guidance mismatch: If Y_pos contains unrelated pairs, LLM fails to calibrate
  - API logit unavailability: Using ChatGPT/Claude in binary mode underperforms Vicuna with logits
- First 3 experiments:
  1. Ablate guidance source: Compare FLM with (a) related seen pairs, (b) random seen pairs, and (c) no guidance
  2. Threshold sensitivity analysis: Sweep τ from 0.1 to 0.9 in 0.1 increments on validation set
  3. LLM backend comparison: Run FLM with Vicuna-13B (logit), LLaMA-2-13B (logit), and ChatGPT (binary)

## Open Questions the Paper Calls Out

### Open Question 1
To what extent would access to output logits from advanced proprietary models (e.g., GPT-4) improve feasibility scoring compared to binary responses or open-source models? The authors speculate ChatGPT with logit access would likely surpass Vicuna considerably, but current API constraints prevented this comparison.

### Open Question 2
How does the semantic diversity of the in-context seen classes impact the model's ability to predict feasibility for rare or unrelated state-object pairs? The method's sensitivity to potential semantic gaps in the guidance set remains unquantified.

### Open Question 3
Can a single universal prompt structure achieve competitive performance across all datasets without dataset-specific grid search? The need to tune prompt components per dataset suggests high prompt sensitivity.

## Limitations
- Reliance on in-context learning introduces dataset-specific brittleness - performance drops significantly when guidance examples are semantically unrelated to the query
- Method depends on LLM API access for binary outputs or local deployment of large models for logit extraction
- Critical hyperparameters (threshold τ values, number of guidance examples) are not fully specified, requiring dataset-specific tuning

## Confidence
- **High confidence**: The core feasibility filtering mechanism works as described, evidenced by consistent H and AUC improvements across all datasets and VLM backbones
- **Medium confidence**: The superiority of Vicuna-13B logits over binary outputs is demonstrated, though speculation about ChatGPT's potential with logits remains unverified
- **Medium confidence**: The claim that related seen pairs outperform random ones in guidance is supported by ablations, but optimal number of examples lacks full specification

## Next Checks
1. **Guidance source ablation**: Systematically compare FLM performance using (a) related seen pairs, (b) random seen pairs, and (c) no guidance across all three datasets to confirm the 17.4%→12.0% H drop threshold
2. **Threshold calibration protocol**: Implement automated τ tuning on validation sets and measure sensitivity of H scores to threshold variations (0.1 increments from 0.1-0.9)
3. **API vs local LLM comparison**: Benchmark FLM using ChatGPT binary outputs against Vicuna-13B logits on identical test subsets to quantify the performance gap and validate the speculation about ChatGPT's potential with logits