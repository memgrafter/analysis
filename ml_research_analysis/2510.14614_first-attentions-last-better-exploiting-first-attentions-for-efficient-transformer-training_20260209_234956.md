---
ver: rpa2
title: 'First Attentions Last: Better Exploiting First Attentions for Efficient Transformer
  Training'
arxiv_id: '2510.14614'
source_url: https://arxiv.org/abs/2510.14614
tags:
- first
- attention
- training
- output
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces FAL (First Attentions Last), a transformer\
  \ architecture that improves training efficiency by eliminating per-block MHA-MLP\
  \ connections and redirecting the first MHA output to subsequent layers. This reconfiguration\
  \ removes all-reduce communication in tensor parallelism and enables parallel execution\
  \ of MHA and MLP, achieving up to 44% faster multi-GPU training and 1.18\xD7 faster\
  \ single-GPU throughput."
---

# First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training

## Quick Facts
- arXiv ID: 2510.14614
- Source URL: https://arxiv.org/abs/2510.14614
- Authors: Gyudong Kim; Hyukju Na; Jin Hyeon Kim; Hyunsung Jang; Jaemin Park; Jaegi Hwang; Namkoo Ha; Seungryong Kim; Young Geun Kim
- Reference count: 40
- Primary result: 44% faster multi-GPU training and 1.18× faster single-GPU throughput through first attention output reuse

## Executive Summary
This paper introduces FAL (First Attentions Last), a transformer architecture modification that improves training efficiency by eliminating per-block MHA-MLP connections and redirecting the first MHA output to subsequent layers. The key insight is that the first attention output can be reused across all layers, removing all-reduce communication in tensor parallelism and enabling parallel execution of MHA and MLP. This reconfiguration achieves significant speedups without sacrificing model quality. The authors also propose FAL+, which further improves model quality by augmenting MHA-MLP connections with the first attention output, achieving lower perplexity without increasing training time.

## Method Summary
The FAL architecture restructures transformer layers by removing the typical MHA-MLP connections in each block and instead feeding the first MHA output to all subsequent layers. This creates a topology where the first attention output serves as the primary input for both attention and feed-forward computations across the entire model. FAL+ extends this by maintaining the first attention output while also adding connections from the first attention to subsequent MHA-MLP pairs. This dual-path approach enables parallel execution of attention and feed-forward operations while preserving the beneficial information flow of the first attention. The architecture is particularly effective in tensor parallelism scenarios where it eliminates communication overhead by reducing all-reduce operations.

## Key Results
- Achieves up to 44% faster training speed on multi-GPU setups with tensor parallelism
- Improves single-GPU throughput by 1.18× through parallel execution of MHA and MLP
- FAL+ achieves lower perplexity than standard transformers without increasing training time
- Validated across multiple model scales (1.3B, 2.7B, 7B parameters) and tasks including language modeling and code generation

## Why This Works (Mechanism)
The efficiency gains stem from two key mechanisms: first, eliminating per-block MHA-MLP connections reduces memory bandwidth requirements and enables parallel execution of attention and feed-forward operations. Second, redirecting the first attention output to all subsequent layers removes the need for expensive all-reduce communications in tensor parallelism, as the first attention result is computed once and reused. The FAL+ variant adds quality improvements by preserving the beneficial information flow from first attention to subsequent layers while maintaining the efficiency benefits. The approach works because the first attention output captures the most comprehensive contextual information, making it valuable for subsequent computations across the entire model depth.

## Foundational Learning
- **Attention Mechanism**: How self-attention computes weighted combinations of token representations based on learned similarity scores - needed to understand why first attention output is valuable for reuse
- **Feed-Forward Networks in Transformers**: The position-wise MLP layers that follow attention - critical for understanding the MHA-MLP connection pattern being modified
- **Tensor Parallelism**: Distribution of model parameters across multiple GPUs with communication patterns - essential for understanding the communication bottlenecks being eliminated
- **Parallel Execution**: Simultaneous computation of independent operations - key to understanding how FAL enables performance gains
- **Layer Connectivity Patterns**: How information flows between transformer blocks - fundamental to understanding the architectural modifications
- **Memory Bandwidth Constraints**: Limitations in data transfer between GPU memory and compute units - important for understanding efficiency bottlenecks

## Architecture Onboarding

**Component Map**: Input -> First MHA -> (Attention outputs to all layers) -> Parallel MHA & MLP execution -> Output

**Critical Path**: The first attention computation is the critical path, as its output is required for all subsequent layers. This is why optimizing this single computation provides multiplicative benefits across the entire model depth.

**Design Tradeoffs**: FAL trades the traditional layer-by-layer information flow for a more parallel execution model. The key tradeoff is between the potential loss of fine-grained layer interactions versus the significant efficiency gains from parallelization and reduced communication. FAL+ addresses quality concerns by maintaining some traditional connections while preserving efficiency benefits.

**Failure Signatures**: Performance degradation would occur if the first attention output fails to capture sufficient contextual information for downstream layers, or if the parallel execution introduces synchronization bottlenecks that offset communication savings. Quality degradation might manifest as increased perplexity or reduced task performance if the modified connectivity pattern disrupts essential information flow.

**First Experiments**:
1. Compare training throughput of standard transformer vs FAL on a 1.3B parameter model with 4-way tensor parallelism
2. Measure memory usage differences between standard transformer and FAL architectures during training
3. Evaluate perplexity on WikiText-2 for FAL+ vs standard transformer with identical training configurations

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Evaluation focuses primarily on decoder-only transformer architectures; applicability to encoder-only or encoder-decoder models remains unclear
- Hardware dependency is significant - performance gains are demonstrated primarily on NVIDIA A100 GPUs with specific tensor parallelism configurations
- Ablation studies are limited to the specific hyperparameter settings used in experiments, with potential sensitivity to learning rates, batch sizes, and sequence lengths
- The quality improvements from FAL+ assume specific attention mechanism behaviors that may not generalize across all transformer variants

## Confidence
- **High Confidence**: Training speed improvements (44% speedup) on multi-GPU setups with tensor parallelism
- **Medium Confidence**: Single-GPU throughput improvements (1.18×) due to potential hardware configuration dependencies
- **Medium Confidence**: Model quality improvements from FAL+ based on perplexity metrics, though generalization to other evaluation measures is uncertain

## Next Checks
1. Test FAL architecture on encoder-only transformer variants (BERT-style models) to assess generalizability beyond decoder-only architectures
2. Evaluate performance on alternative GPU architectures (e.g., AMD Instinct MI300X, H100) to verify hardware independence of claimed speedups
3. Conduct ablation studies across a broader range of hyperparameters (learning rates 1e-4 to 5e-4, batch sizes 16 to 128) to establish robustness of efficiency gains