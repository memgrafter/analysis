---
ver: rpa2
title: 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation'
arxiv_id: '2509.25849'
source_url: https://arxiv.org/abs/2509.25849
tags:
- exploration
- budget
- iterations
- arxiv
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of exploration budget allocation
  in reinforcement learning for large language models, where uniform budget distribution
  leads to inefficient learning due to zero gradients from tasks that are either too
  easy or too hard. The authors formulate this as a knapsack optimization problem,
  where each task-budget pair is treated as an item with a cost (computational effort)
  and value (learning potential).
---

# Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation

## Quick Facts
- arXiv ID: 2509.25849
- Source URL: https://arxiv.org/abs/2509.25849
- Authors: Ziniu Li, Congliang Chen, Tianyun Yang, Tian Ding, Ruoyu Sun, Ge Zhang, Wenhao Huang, Zhi-Quan Luo
- Reference count: 40
- One-line primary result: Dynamically allocating RL exploration budgets as a knapsack problem increases effective gradient ratios by 20-40% and achieves up to 9-point performance gains on mathematical reasoning tasks.

## Executive Summary
This paper addresses a fundamental inefficiency in LLM reinforcement learning: uniform budget allocation in Group Relative Policy Optimization (GRPO) causes zero gradients when tasks are either too easy (all samples succeed) or too hard (all samples fail). The authors reformulate exploration as a knapsack optimization problem, where each task-budget pair is treated as an item with a cost (computational effort) and value (learning potential). By dynamically allocating budgets based on current task difficulty, the method increases effective gradient ratios by 20-40% during training, achieving average performance gains of 2-4 points and up to 9 points on specific mathematical reasoning benchmarks. Notably, comparable performance with traditional homogeneous allocation would require approximately 2x the computational resources.

## Method Summary
The method treats each task-budget pair as an item in a bounded knapsack problem, where the solver maximizes InfoGain(N,p) = (1 - p^N - (1-p)^N) × p(1-p)² subject to total budget constraints. The success rate p for each task is estimated from the previous training epoch, and the solver allocates budgets ranging from N_low=2 to N_up=128 rollouts per task. A fallback strategy ensures hard tasks aren't starved by allocating excess budget to tasks with p=0.0. The approach is implemented with Numba-accelerated dynamic programming and adds negligible computational overhead (1-2 seconds per epoch) while requiring no additional hyperparameters.

## Key Results
- Effective gradient ratio increased by 20-40% compared to uniform sampling
- Average performance gains of 2-4 points on mathematical reasoning benchmarks
- Up to 9-point improvements on specific tasks with 2x less compute than uniform allocation
- System adds negligible overhead (1-2 seconds per epoch) with no new hyperparameters

## Why This Works (Mechanism)

### Mechanism 1: Gradient Signal Recovery via Heterogeneous Budgets
The paper addresses GRPO's zero-gradient vulnerability by allocating enough samples per task to ensure mixed success/failure outcomes. The probability of obtaining a non-zero gradient is 1 - p^N - (1-p)^N, which the system solves for optimal N given current success rate p. This prevents stalls from uniform low budgets on hard tasks or uniform high budgets on easy ones. The method assumes success rate estimates from the previous epoch are accurate proxies for current capability. Evidence shows tasks with all successes or all failures produce zero gradients in GRPO, which Knapsack-RL resolves by ensuring sufficient sampling. The system could misallocate if model capability drifts rapidly between epochs.

### Mechanism 2: Value-Based Resource Re-allocation (Knapsack Optimization)
The approach maximizes "InfoGain" per unit compute by treating exploration as a discrete knapsack problem. The solver maximizes Sum(Value(N_i, p_i)) subject to Sum(N_i) = Total_Budget, where Value is based on gradient probability and asymmetric InfoGain(p(1-p)²). This redirects wasted compute from saturated tasks to high-uncertainty tasks, acting as a "free lunch." The method assumes the first-order Taylor approximation accurately predicts gradient update utility. Table 1 explicitly maps tasks to knapsack items. While ROI-Reasoning explores similar budget-constrained optimization, direct evidence for this specific knapsack formulation in LLM work is limited. Solver latency could bottleneck training if it exceeds 1-2 seconds.

### Mechanism 3: Exploration Scaling for Hard Tasks
The method enables significantly larger budgets (e.g., 93 rollouts) for hard tasks without increasing total compute, unlocking solutions uniform sampling misses. By stripping budget from easy tasks, the allocator funnels resources to "extremely-hard" tasks, raising the probability of finding successful trajectories where uniform sampling would yield only failures. The approach assumes easy tasks won't regress when receiving minimal exploration budget. Figure 5 shows budget histograms with long tails exceeding 50-90 rollouts. Without the fallback strategy, the optimizer may assign zero budget to unsolved tasks, causing training instability.

## Foundational Learning

### Concept: Group Relative Policy Optimization (GRPO)
- **Why needed here:** The entire paper addresses GRPO's specific vulnerability requiring variance within prompt groups for gradients. Without this context, the knapsack solution appears as generic importance sampling.
- **Quick check question:** If a model solves 8/8 attempts for a prompt in GRPO, what is the resulting gradient, and why does Knapsack-RL aim to prevent this ratio?

### Concept: The Knapsack Problem (0/1 or Bounded)
- **Why needed here:** The core contribution maps RL budgeting to this combinatorial optimization problem. Engineers must understand the trade-off between item "weight" (compute cost) and "value" (learning potential).
- **Quick check question:** In this formulation, if a task has p=0.5 (high variance), does it have higher or lower "weight" compared to a task with p=0.01, assuming we want equal gradient probability?

### Concept: Success Rate Estimation
- **Why needed here:** The method relies on p_hat (estimated success rate) to calculate value. Understanding how noisy estimates from small sample sizes affect optimization is critical for diagnosing instability.
- **Quick check question:** Why might using a raw average of success/failure from the last epoch be dangerous for tasks that were only sampled 2 times (minimum budget)?

## Architecture Onboarding

- **Component map:** Status Tracker -> Knapsack Solver -> Rollout Balancer -> Training Loop
- **Critical path:** The transition from "Epoch T" to "Knapsack Solver for Epoch T+1". If the solver fails or produces degenerate allocations, the training run wastes a step. The "Fallback Strategy" is a critical safety rail here.
- **Design tradeoffs:** Latency vs. Accuracy (fast heuristic vs. online evaluator); Coverage vs. Focus (N_low ensures coverage but wastes compute on solved prompts).
- **Failure signatures:** Gradient Collapse (effective gradient ratio fails to improve); GPU Underutilization (stragglers due to poor rollout balancing); Oscillating Performance (model forgets easy tasks).
- **First 3 experiments:** 1) Gradient Ratio Baseline (reproduce Figure 2 on small model); 2) Fallback Ablation (run with fallback disabled to validate hard task exploitation); 3) Compute Equivalence (compare Uniform Budget N=16 vs. Knapsack with 2x compute).

## Open Questions the Paper Calls Out

### Open Question 1
Can the knapsack formulation be extended to optimize for actual computational cost (e.g., token length) rather than just rollout counts? The conclusion lists "Extending beyond rollout counts" as a primary avenue for future work. This remains unresolved because the current implementation treats all rollouts as equal cost items, ignoring variable resource consumption. Evidence would require a study applying Knapsack-RL with token-weighted cost constraints demonstrating efficiency gains on variable-length tasks.

### Open Question 2
Do richer value function formulations outperform the proposed first-order Taylor approximation for estimating learning potential? The conclusion suggests "Designing richer value functions" could yield more accurate assessments than the current approximation. This is unresolved because the current InfoGain relies on a simplified theoretical derivation that may not fully capture policy improvement dynamics. Evidence would require comparative experiments showing superior convergence or performance with learned or higher-order analytical value functions.

### Open Question 3
Can experience replay effectively resolve the "extremely-hard" tasks that remain unsolved despite dynamic budget allocation? Page 11 notes that 20% of prompts remain extremely-hard even with occasional positive trajectories, proposing "experience replay techniques" as a solution. This is unresolved because the current on-policy method may lose gradient signals from rare successes before the model can effectively learn from them. Evidence would require combining Knapsack-RL with a replay buffer for hard tasks, showing a significant reduction in unsolved prompts.

## Limitations
- Success rate estimates from single epochs may be unreliable for minimally sampled tasks, causing misallocation
- First-order Taylor approximation for InfoGain may poorly predict true learning value near gradient thresholds
- Knapsack solver's heuristic nature means it may not find globally optimal allocations when task values are correlated

## Confidence
- **High Confidence**: The zero-gradient problem in GRPO is well-established; the fundamental insight that heterogeneous budgets can recover gradient signals is theoretically sound and experimentally validated
- **Medium Confidence**: The specific knapsack formulation with InfoGain(p) = p(1-p)² is reasonable but not uniquely optimal
- **Low Confidence**: The assumption that one-epoch-old success rates are sufficient proxies for current model capability, especially under rapid learning

## Next Checks
1. **Robustness to Success Rate Noise**: Systematically vary the window size and smoothing for p_hat estimation to quantify sensitivity to noisy estimates from minimal sampling
2. **Solver Optimality Verification**: Compare the Numba-accelerated DP solver's allocations against a slower but exact solver on small problem instances to bound approximation error
3. **Gradient Signal Correlation**: Measure the actual correlation between predicted InfoGain values and observed gradient magnitudes during training to validate the Taylor approximation's predictive power