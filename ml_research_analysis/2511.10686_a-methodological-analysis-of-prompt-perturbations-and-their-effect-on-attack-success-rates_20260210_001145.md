---
ver: rpa2
title: A methodological analysis of prompt perturbations and their effect on attack
  success rates
arxiv_id: '2511.10686'
source_url: https://arxiv.org/abs/2511.10686
tags:
- 'true'
- prompt
- 'false'
- attack
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how different alignment methods (SFT, DPO,
  RLHF) affect Large Language Models' susceptibility to prompt attacks, revealing
  that even minor prompt modifications can significantly alter attack success rates.
  The study uses systematic statistical analysis to evaluate model responses to various
  attack types (Best-of-N, Base64 encoding, adversarial tokens, emotional blackmail)
  with and without alignment prompts and instruction templates.
---

# A methodological analysis of prompt perturbations and their effect on attack success rates

## Quick Facts
- arXiv ID: 2511.10686
- Source URL: https://arxiv.org/abs/2511.10686
- Authors: Tiago Machado; Maysa Malfiza Garcia de Macedo; Rogerio Abreu de Paula; Marcelo Carpinette Grave; Aminat Adebiyi; Luan Soares de Souza; Enrico Santarelli; Claudio Pinhanez
- Reference count: 12
- Even minor prompt modifications can significantly alter attack success rates across different alignment methods

## Executive Summary
This study systematically investigates how different alignment methods (SFT, DPO, RLHF) affect Large Language Models' susceptibility to prompt attacks. Through rigorous statistical analysis, the research demonstrates that attack success rates are not invariant to prompt variations and that existing attack benchmarks may not fully capture model vulnerabilities. The work reveals that SFT models are particularly vulnerable to prompt perturbations, while alignment prompts and instruction templates can significantly influence attack effectiveness.

## Method Summary
The study employs systematic statistical analysis to evaluate model responses to various attack types including Best-of-N, Base64 encoding, adversarial tokens, and emotional blackmail. Experiments are conducted with and without alignment prompts and instruction templates across different alignment methods (SFT, DPO, RLHF). The research uses controlled experimental conditions with multiple evaluation metrics to measure attack success rate variations and their statistical significance.

## Key Results
- Minor prompt modifications can significantly alter attack success rates across alignment methods
- SFT models show particularly high vulnerability to prompt perturbations compared to DPO and RLHF models
- Attack success rates vary significantly with the presence of alignment prompts and instruction templates
- Existing attack benchmarks alone may not fully capture model vulnerabilities

## Why This Works (Mechanism)
The vulnerability of LLMs to prompt attacks stems from their reliance on contextual understanding and pattern matching in input processing. Alignment methods shape how models interpret and respond to prompts, creating different susceptibility patterns to adversarial modifications. The interaction between prompt structure, alignment training, and attack methodology determines the effectiveness of adversarial attempts.

## Foundational Learning
- Prompt engineering fundamentals: Why needed - to understand how prompt structure affects model responses; Quick check - can you identify how prompt format changes model behavior?
- Alignment method distinctions (SFT, DPO, RLHF): Why needed - to comprehend different training approaches and their security implications; Quick check - can you explain key differences between alignment techniques?
- Statistical analysis in AI security: Why needed - to evaluate significance of attack success rate variations; Quick check - can you interpret p-values and confidence intervals in security research?
- Adversarial attack taxonomy: Why needed - to understand different attack methodologies and their mechanisms; Quick check - can you classify attacks by their approach and target?
- Model vulnerability assessment: Why needed - to evaluate security implications of different alignment approaches; Quick check - can you design a basic vulnerability assessment framework?

## Architecture Onboarding

Component Map:
Input Processing -> Prompt Analysis -> Alignment Method Application -> Attack Detection -> Response Generation

Critical Path:
Prompt → Alignment Method → Attack Vector → Success Metric → Statistical Analysis

Design Tradeoffs:
Static vs. adaptive attacks, comprehensive vs. targeted evaluation, alignment method diversity vs. depth of analysis

Failure Signatures:
Inconsistent attack success rates across prompt variations, alignment method-dependent vulnerabilities, benchmark limitations

First Experiments:
1. Test single prompt variation across all alignment methods to establish baseline vulnerability patterns
2. Evaluate attack success rate changes with and without alignment prompts
3. Compare static attack effectiveness against models with different alignment training durations

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on specific attack types may miss other vulnerability patterns
- Static attack prompts without adaptive strategies may underestimate real-world effectiveness
- Limited consideration of model size variations and domain-specific alignment training

## Confidence
Statistical methodology and attack success measurement: High
Alignment method vulnerability comparisons: Medium
Prompt configuration impact on attacks: Medium

## Next Checks
1. Test adaptive attack strategies that modify prompts based on model responses to determine if static attack benchmarks underestimate vulnerability levels
2. Evaluate vulnerability patterns across different model sizes and architectures to assess generalizability of alignment method comparisons
3. Conduct cross-domain analysis with domain-specific alignment training to understand how specialized alignment affects prompt attack susceptibility patterns