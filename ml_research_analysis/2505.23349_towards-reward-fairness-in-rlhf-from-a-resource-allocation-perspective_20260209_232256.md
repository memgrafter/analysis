---
ver: rpa2
title: 'Towards Reward Fairness in RLHF: From a Resource Allocation Perspective'
arxiv_id: '2505.23349'
source_url: https://arxiv.org/abs/2505.23349
tags:
- fairness
- reward
- rewards
- arxiv
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward unfairness in RLHF,
  which can arise from various biases in reward models and negatively impact the alignment
  of large language models with human preferences. The authors propose a unified framework
  that models preference learning as a resource allocation problem, treating rewards
  as resources to be allocated while balancing utility and fairness.
---

# Towards Reward Fairness in RLHF: From a Resource Allocation Perspective

## Quick Facts
- **arXiv ID:** 2505.23349
- **Source URL:** https://arxiv.org/abs/2505.23349
- **Reference count:** 32
- **Key outcome:** Proposes a unified framework for mitigating reward bias in RLHF by treating rewards as resources to be fairly allocated across preference data categories.

## Executive Summary
This paper addresses reward unfairness in Reinforcement Learning from Human Feedback (RLHF), which can arise from biases like length or category preferences in reward models. The authors propose modeling preference learning as a resource allocation problem, where rewards are treated as limited resources requiring fair distribution. They introduce two methods—Fairness Regularization (additive trade-off) and Fairness Coefficient (multiplicative trade-off)—to achieve fairer reward distributions while maintaining alignment quality. Experiments demonstrate that their approach reduces length and category biases while improving sampling efficiency and maintaining or improving performance on standard benchmarks.

## Method Summary
The authors propose a unified framework that models preference learning as a resource allocation problem, treating rewards as resources to be allocated fairly across different preference categories. They define an allocation vector where each element represents the reward margin between winning and losing responses in preference pairs. The framework introduces a fairness function parameterized by τ that measures distribution consistency across data types, with τ=-1 yielding Jain's index. Two methods are proposed: Fairness Regularization adds a fairness term to the standard Bradley-Terry loss (L = -U(a) - αF(a)), while Fairness Coefficient multiplies utility and fairness measures (L = -U(a)·F(a)^γ). The methods are applied to both verification (reward model training) and reinforcement learning (DPO policy training) scenarios.

## Key Results
- FR RM produces more overlapping reward distributions for Helpful/Harmless categories compared to standard BT RM
- FC DPO achieves 21.10 LC WR on AlpacaEval2 (LLaMA3) vs. 16.71 for vanilla DPO
- Fair reward methods improve sampling efficiency, reaching target performance with fewer samples
- Fairness improvements transfer to out-of-distribution evaluation on Reward Bench

## Why This Works (Mechanism)

### Mechanism 1: Resource Allocation Framework
- **Claim:** Various reward biases (length, category, social) can be addressed uniformly by framing preference learning as a resource allocation problem where rewards are treated as limited resources requiring fair distribution.
- **Mechanism:** Define an allocation vector a = [a₁, a₂, ..., aₙ] where each aᵢ represents the reward margin (r_φ(y_w) - r_φ(y_l)) for preference pairs. Use a unified fairness function f_τ(a) = sign(1-τ) · [Σᵢ(aᵢ/Σⱼaⱼ)^(1-τ)]^(1/τ) that satisfies continuity, homogeneity (degree 0), and monotonicity properties to measure distribution consistency across data types.
- **Core assumption:** Rewards can be meaningfully treated as allocable resources, and fairness in their distribution correlates with reduced bias manifestation.
- **Evidence anchors:**
  - [abstract] "We model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness"
  - [section 4] Eq. (7) presents the unified fairness metric with τ ∈ R allowing derivation of different fairness functions (e.g., Jain's index when τ = -1)
  - [corpus] Limited direct corpus support for this specific resource allocation framing in RLHF; neighbor papers focus on diverse preferences and reward hacking rather than resource-theoretic approaches
- **Break condition:** If the fairness function's homogeneity property (scale-independence) fails to capture meaningful differences in reward magnitude across data types that are genuinely important for preference discrimination.

### Mechanism 2: Fairness Regularization (Additive Trade-off)
- **Claim:** Adding a fairness term to standard Bradley-Terry training objectives may produce reward models with more consistent distributions across data categories without degrading preference discrimination accuracy.
- **Mechanism:** Modify the standard RM loss from L = -E[log σ(aᵢ)] to L_FR = -E[log σ(aᵢ)] - αF(a), where α controls fairness contribution. For DPO, apply the same principle to implicit rewards: aᵢ = β log(π_θ(y_w|x)/π_ref(y_w|x)) - β log(π_θ(y_l|x)/π_ref(y_l|x)).
- **Core assumption:** Utility (preference alignment) and fairness (distribution consistency) are optimizable through linear combination; the gradient interaction between terms does not destabilize training.
- **Evidence anchors:**
  - [abstract] "Fairness Regularization (additive combination of utility and fairness)"
  - [section 5.1] Eq. (11) defines FR RM objective; [section 6.1.1] Figure 3 shows FR RM produces more overlapping reward distributions for Helpful/Harmless categories vs. BT RM
  - [corpus] "Fusing Rewards and Preferences in Reinforcement Learning" (arXiv:2508.11363) combines rewards and preferences in a single update, suggesting multi-objective integration is feasible
- **Break condition:** If α values that improve fairness simultaneously degrade utility below acceptable thresholds; empirical results suggest α ≈ 0.1 works but larger values cause decline.

### Mechanism 3: Fairness Coefficient (Multiplicative Trade-off)
- **Claim:** Multiplying utility and fairness measures may provide stronger coupling between the two objectives, potentially preventing scenarios where high utility compensates for very low fairness.
- **Mechanism:** Transform objective to L_FC = -E[log σ(aᵢ)] · F(a)^γ, where γ controls fairness influence. This formulation ensures that if F(a) → 0, the entire objective collapses, creating stronger pressure to maintain minimum fairness.
- **Core assumption:** The multiplicative interaction provides better gradient properties for fairness-utility balance than additive formulation; F(a) remains differentiable and non-zero during training.
- **Evidence anchors:**
  - [abstract] "Fairness Coefficient (multiplicative combination)"
  - [section 4] Eq. (9) defines the multiplicative formulation; [section 6.1.2] Table 2 shows FC DPO achieves 21.10 LC WR on AlpacaEval2 (LLaMA3) vs. 16.71 for vanilla DPO
  - [corpus] No direct corpus papers use multiplicative fairness-utility formulations in RLHF; this appears novel to this work
- **Break condition:** If F(a)^γ creates optimization instability (vanishing/exploding gradients) or if the fairness term dominates to the point of preventing meaningful preference learning.

## Foundational Learning

- **Resource Allocation Theory (α, τ parameterization)**
  - Why needed here: The paper's core innovation requires understanding how fairness functions (parameterized by τ) behave—τ = -1 yields Jain's index (bounds: 1/n to 1), other τ values produce different fairness measures. Understanding α and γ as trade-off controllers is essential for hyperparameter selection.
  - Quick check question: Given a reward allocation a = [3, 1, 2], compute f_{τ=-1}(a). Answer: n · J(a) = 3 · (Σaᵢ)²/(n·Σaᵢ²) = 3 · 36/(3·14) = 36/14 ≈ 2.57

- **Bradley-Terry Model**
  - Why needed here: The utility function U(a) = E[log σ(aᵢ)] derives directly from BT preference probability P(y_w ≻ y_l|x) = exp(r(y_w))/(exp(r(y_w)) + exp(r(y_l))). Understanding this connection is necessary to see how fairness modifications alter preference learning.
  - Quick check question: If r(y_w) = 2 and r(y_l) = 0.5, what is P(y_w ≻ y_l)? Answer: e²/(e² + e^{0.5}) ≈ 0.824

- **DPO Implicit Rewards**
  - Why needed here: The paper applies fairness to DPO by treating β log(π_θ/π_ref) as implicit rewards. Understanding this connection enables the fairness-aware policy training without explicit RM.
  - Quick check question: If π_θ(y_w|x) = 0.4, π_ref(y_w|x) = 0.1, and β = 0.1, what is the implicit reward? Answer: 0.1 × log(0.4/0.1) = 0.1 × 1.386 ≈ 0.139

## Architecture Onboarding

- **Component map:**
  ```
  Preference Data D = {(x, y_w, y_l)}
         ↓
  Allocation Vector a = [r(y_w) - r(y_l)] for each pair
         ↓
  ┌─────────────┬─────────────────┐
  │ Utility U(a)│ Fairness F(a)   │
  │ = E[log σ]  │ = f_τ(a)        │
  └─────────────┴─────────────────┘
         ↓                ↓
  FR: U(a) + αF(a)  FC: U(a)·F(a)^γ
         ↓
  [Verification] Reward Model r_φ  →  Fair RM (FR RM / FC RM)
  [RL] Policy π_θ (via DPO)        →  Fair Policy (FR DPO / FC DPO)
  ```

- **Critical path:**
  1. **Define entities for fairness**: Group data by the dimension you want fair (length buckets, categories like Helpful/Harmless, social attributes)
  2. **Choose τ**: Default τ = -1 (Jain's index); ablation shows τ ∈ [-5, 10] all work, suggesting robustness
  3. **Set trade-off parameter**: α ≈ 0.1 for FR, γ ≈ 0.5 for FC (from ablations in Figure 7 and Figure 9)
  4. **Apply to training loop**: Add fairness term to loss every batch; requires computing F(a) over a batch (not per-sample)

- **Design tradeoffs:**
  - **FR vs FC**: FR is more stable (additive), FC may provide stronger fairness guarantees (multiplicative coupling) but risks gradient issues if F(a) approaches zero
  - **Batch-level vs dataset-level fairness**: Paper computes F(a) per batch for efficiency; this is an approximation to true dataset-wide fairness
  - **Entity granularity**: Finer-grained entities (e.g., 10 length buckets vs. 2) provide more precise fairness but require larger batches for stable F(a) estimation
  - **OOD generalization**: Fair RMs trained on HH-RLHF transfer fairness to Reward Bench (OOD), but this is not guaranteed for all distribution shifts

- **Failure signatures:**
  - **Fairness collapse**: F(a) remains low despite training → α/γ too small or learning rate too low for fairness term
  - **Utility collapse**: Preference accuracy drops significantly → α/γ too large; fairness dominates
  - **Gradient instability (FC only)**: Loss becomes NaN → F(a) approaching zero; add small epsilon or switch to FR
  - **Category overfitting**: RM becomes fair on training categories but not new ones → entities too specific; use broader groupings

- **First 3 experiments:**
  1. **Reproduce FR RM on HH-RLHF**: Train BT RM vs. FR RM (τ=-1, α=0.1) for 1 epoch; plot reward distributions for Helpful vs. Harmless categories. Expected: FR RM distributions should overlap more than BT RM (per Figure 3a-c).
  2. **Ablate α on held-out validation**: Train FR RM with α ∈ {0, 0.05, 0.1, 0.15} on HH-RLHF train; evaluate accuracy on Reward Bench. Expected: Performance peaks around α ≈ 0.1, declines at higher values (per Figure 7).
  3. **Test sampling efficiency**: Use FR RM vs. BT RM for best-of-n sampling from a policy (n ∈ {1, 8, 16, 32, 64}); measure AlpacaEval2 LC WR vs. samples used. Expected: FR RM reaches target performance with fewer samples (per Figure 4).

## Open Questions the Paper Calls Out

- **Can the resource allocation perspective effectively mitigate reward hacking?**
  - The authors note in the Limitations section that "reward unfairness may be related to various issues... such as reward hacking," but this specific application was not investigated. The current work focuses on length, category, and social biases; it remains untested whether fairness constraints prevent agents from exploiting reward model vulnerabilities (gaming).

- **How does the Fairness Rewards framework perform with Proximal Policy Optimization (PPO)?**
  - The authors state they "have only validated it on BT models and DPO," leaving the interaction with other RL algorithms like PPO as an open area. PPO utilizes an explicit reward model and KL penalty during online reinforcement learning, which may interact differently with the proposed fairness constraints than the offline DPO framework.

- **Does enforcing statistical consistency in reward distribution inadvertently suppress accurate modeling of legitimate preference disparities?**
  - The paper defines fairness as the "consistent distribution of rewards" (homogeneity). However, Figure 7 shows performance degrading if the fairness contribution (α) is too high, suggesting a conflict where enforcing equality might obscure true utility signals. It is unclear if "unfair" variance in baseline models is purely bias or reflects genuine differences in preference intensity across diverse categories.

## Limitations

- The fairness metric's computation per batch is not fully specified, and handling of negative reward margins is unclear.
- Transfer of fairness improvements to out-of-distribution settings is shown on Reward Bench but lacks systematic evaluation across diverse distribution shifts.
- The framework's effectiveness against reward hacking has not been tested, despite the authors noting this connection.

## Confidence

- **High:** Core mechanism of resource allocation framing; experimental design showing reduced length/category bias.
- **Medium:** Effectiveness of multiplicative FC formulation; OOD generalization claims.
- **Low:** Specific implementation details for per-batch fairness computation; stability of FC across different datasets.

## Next Checks

1. Implement a controlled ablation testing whether the fairness term is computed per batch vs. over the full dataset, measuring both bias reduction and computational overhead.
2. Run FC training with varying γ values on a synthetic preference dataset where ground-truth bias is known, to quantify the relationship between γ and fairness-utility trade-off stability.
3. Test whether the proposed methods maintain fairness properties when applied to preference data with non-uniform sampling (e.g., length-balanced vs. natural length distributions), to assess robustness to data collection artifacts.