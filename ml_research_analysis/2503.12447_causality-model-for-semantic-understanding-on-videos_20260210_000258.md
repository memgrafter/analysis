---
ver: rpa2
title: Causality Model for Semantic Understanding on Videos
arxiv_id: '2503.12447'
source_url: https://arxiv.org/abs/2503.12447
tags:
- video
- causal
- videoqa
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis focuses on causal modeling for semantic video understanding,
  addressing two main tasks: Video Relation Detection (VidVRD) and Video Question
  Answering (VideoQA). The research tackles the challenges posed by data imbalance,
  such as long-tail imbalances and perturbed imbalances, which hinder the effectiveness
  of deep neural networks (DNNs) in learning underlying causal mechanisms.'
---

# Causality Model for Semantic Understanding on Videos

## Quick Facts
- **arXiv ID:** 2503.12447
- **Source URL:** https://arxiv.org/abs/2503.12447
- **Reference count:** 0
- **Primary result:** Addresses long-tail and spurious correlation challenges in video relation detection and question answering via causal modeling.

## Executive Summary
This thesis develops causal modeling techniques to improve semantic understanding in videos, focusing on two main tasks: Video Relation Detection (VidVRD) and Video Question Answering (VideoQA). The research addresses fundamental challenges in deep learning caused by data imbalances—both long-tail distributions in relation labels and spurious correlations between scene environments and answers. Through a series of methodological innovations including interventional prediction, invariant and equivariant grounding, and spatio-temporal rationalization, the work demonstrates that explicitly modeling causal relationships enables models to focus on truly relevant visual content rather than exploiting superficial correlations, leading to improved performance and robustness on complex video understanding tasks.

## Method Summary
The thesis presents a causal modeling framework that addresses long-tail imbalance in VidVRD through Interventional Video Relation Detection (IVRD), which forces fair consideration of all relation types by computing interventional probabilities over predicate prototypes. For VideoQA, it introduces Invariant Grounding for VideoQA (IGV) that discovers causal reasoning patterns by grounding question-critical scenes and shielding models from environmental spurious correlations, extended to Equivariant and Invariant Grounding (EIGV) which adds sensitivity to semantic changes in causal factors. The framework culminates in Spatio-Temporal Rationales (STR) for complex VideoQA scenarios, using adaptive cross-modal attention to select critical frames and objects through differentiable selection modules. These methods are implemented as model-agnostic learning frameworks that can be integrated with existing backbones.

## Key Results
- IVRD improves detection of rare but informative relations by mitigating long-tail bias through causal intervention with predicate prototype dictionaries.
- IGV and EIGV frameworks successfully discover causal reasoning patterns, enabling VideoQA models to focus on essential visual content while resisting spurious environmental correlations.
- STR method effectively handles complex VideoQA scenarios with long videos and multiple objects by adaptively selecting question-critical temporal moments and spatial objects through cross-modal interaction.

## Why This Works (Mechanism)

### Mechanism 1: Interventional Video Relation Detection (IVRD) for Long-Tail Imbalance
Applying causal intervention via a predicate prototype dictionary mitigates long-tail bias in video relation detection by forcing the model to consider all relation types fairly. Instead of predicting predicates from subject-object features directly, IVRD computes $P(y_p | do(T_s, T_o)) \approx \sum_z P(y_p | T_s, T_o, z) P(z)$ where $P(z)$ is forced uniform, compelling reliance on visual interaction content rather than spurious object-relation correlations.

### Mechanism 2: Invariant and Equivariant Grounding (IGV/EIGV) for Spurious Correlations in VideoQA
Enforcing that answers are invariant to environment changes and equivariant to causal scene/question changes allows isolation of question-critical visual content. A grounding indicator partitions video into causal and environment scenes, while an intervener creates perturbed videos by swapping environments (invariance) and mixing causal scenes/questions (equivariance), training the model to maintain consistent predictions under environmental changes while adapting to semantic changes.

### Mechanism 3: Spatio-Temporal Rationalization (STR) for Complex VideoQA
Adaptively selecting critical frames and objects via cross-modal attention reduces noise from long, multi-object videos. STR uses two-step adaptive selection: Temporal Rationalization (TR) selects top-$K_f$ critical frames via cross-attention between frames and question, followed by Spatial Rationalization (SR) that selects top-$K_o$ critical objects per frame via cross-attention with objects, using differentiable Top-K modules for end-to-end training.

## Foundational Learning

- **Concept: Causal Intervention (do-calculus)**
  - *Why needed:* Core methodology uses intervention to simulate manipulating variables to estimate true causal effects rather than spurious correlations.
  - *Quick check:* How does $P(Y | do(X))$ differ from $P(Y | X)$, and why is the former useful for debiasing? (Answer: $do(X)$ removes influence of confounders by simulating randomized experiment, breaking spurious backdoor paths.)

- **Concept: Invariant vs. Equivariant Representations**
  - *Why needed:* IGV relies on *invariance* (answer stable under environment change); EIGV adds *equivariance* (answer changes sensibly with causal input change).
  - *Quick check:* If I rotate an image, should a shape classifier's output be invariant or equivariant? (Answer: Invariant—the label "circle" shouldn't change.)

- **Concept: Visual Grounding and Rationalization**
  - *Why needed:* All mechanisms require localizing relevant visual content: IVRD grounds dynamic interactions, IGV/EIGV ground causal scenes, STR grounds specific frames/objects.
  - *Quick check:* What's the difference between "attention" and "grounding"? (Answer: Attention is soft weighting; grounding often implies harder selection or localization, sometimes with spatial/temporal boundaries.)

## Architecture Onboarding

- **Component map:**
  - IVRD: Object trajectory encoder (BiLSTM) → prototype dictionary attention → predicate prediction
  - IGV/EIGV: Grounding indicator (Gumbel-Softmax gating) → Scene intervener (mixes/swaps scenes) → VideoQA backbone → Contrastive disruptor (for EIGV)
  - STR: Frame/object feature extractor → TR module (self+cross attention, differentiable Top-K) → SR module (per-frame, similar) → Multi-grain reasoning (Transformer) → Answer decoder

- **Critical path:**
  1. Pre-extract object trajectories (for VidVRD) or frame/object features (for VideoQA)
  2. For IGV/EIGV: The grounding indicator's $\hat{c}$ and $\hat{e}$ split is crucial; if it fails, intervention operates on wrong inputs
  3. For STR: Adaptive selection (TR then SR) must run before main reasoning module to prune noise

- **Design tradeoffs:**
  - Model-agnostic vs. specialized: IGV/EIGV are plug-and-play for any VideoQA backbone (flexibility); STR is specialized architecture (higher performance)
  - Hard vs. soft selection: STR uses differentiable but near-hard Top-K for clear rationalization (soft attention might blur boundaries but be more stable)
  - Computational cost: Prototype dictionaries, environment memory banks, multiple object candidates add overhead

- **Failure signatures:**
  - IVRD: Performance degrades on head predicates (over-debiasing), or prototypes become entangled
  - IGV/EIGV: Grounding window doesn't change with different questions to same video, or selects obviously irrelevant scenes
  - STR: Selected objects/frames are random or uniformly distributed, indicating cross-attention isn't learning meaningful associations

- **First 3 experiments:**
  1. Sanity check the grounding: Visualize $\hat{c}$ (from IGV) or selected frames/objects (from STR) on few samples; do they align with human intuition?
  2. Ablate the intervention: For IVRD/IGV, remove intervention (set $P(z)$ proportional to frequency, or skip environment swapping) and compare performance on long-tail/OOD test sets
  3. Test on complexity: Evaluate STR specifically on long videos (>80s) and multi-object scenes, comparing against baseline using all frames/objects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of discovered "causal scenes" be directly quantified without relying solely on indirect Question Answering performance metrics?
- Basis in paper: [explicit] Abstract and Chapter 7 state this is a key limitation due to absence of human-level grounding annotations for causal scenes
- Why unresolved: Existing benchmarks lack ground-truth spatio-temporal annotations that explicitly label visual evidence supporting answers
- What evidence would resolve it: Creation of a "Grounded VideoQA" benchmark containing human annotations of visual evidence (frames/objects) that underpin answering process

### Open Question 2
- Question: How can causal intervention techniques be adapted to mitigate hallucinations in large Vision-Language Models (VLMs) without incurring prohibitive computational overhead?
- Basis in paper: [explicit] Chapter 7 notes that while methods could theoretically address VLM hallucinations, applying them currently results in "intolerable computational overhead"
- Why unresolved: Proposed IGV and EIGV frameworks involve complex grounding and intervention steps that may be too resource-intensive for massive VLM architectures
- What evidence would resolve it: Development of lightweight causal regularization method that reduces object hallucination rates in VLMs while maintaining feasible training/inference speeds

### Open Question 3
- Question: Can causal modeling principles be effectively transferred from 2D video to 3D Question Answering to ensure robots make decisions based on physical causality rather than environmental spurious correlations?
- Basis in paper: [explicit] Chapter 7 identifies "3D Question Answering" as future direction, arguing that robots acting on spurious correlations could lead to "detrimental consequences" in real world
- Why unresolved: Thesis focuses on 2D video frames and object tracks; extending these causal distinctions to 3D spatial coordinates, depth, and physical interaction introduces new complexity
- What evidence would resolve it: Experiments demonstrating that causal 3D-QA model outperforms correlation-based models in robotic simulation tasks by correctly identifying actionable causal factors versus static background environments

## Limitations

- Implementation details such as exact batch size for TranSTR training and specific implementation of differentiable Top-K selection are underspecified
- Effectiveness of grounding indicators and prototype dictionaries depends heavily on their quality and representativeness across datasets
- Computational overhead of maintaining environment memory banks and processing multiple object candidates could limit scalability to longer videos or larger datasets
- The methods rely on the existence and approximate localization of question-critical causal scenes, which may be challenging in complex real-world videos

## Confidence

- **High confidence:** The causal intervention framework (IVRD) for addressing long-tail imbalances is well-motivated and supported by both theoretical formulation and qualitative evidence of prototype attention
- **Medium confidence:** The invariant and equivariant grounding principles (IGV/EIGV) are sound, but effectiveness depends critically on grounding indicator's ability to reliably distinguish causal from environment scenes
- **Medium confidence:** The spatio-temporal rationalization approach (STR) is intuitively correct for handling complex videos, but cross-attention mechanism's robustness to noise may vary significantly across different video domains

## Next Checks

1. Sanity check the grounding: Visualize the question-critical scene $\hat{c}$ (from IGV) or selected frames/objects (from STR) on a few samples; do they align with human intuition about what visual content is actually needed to answer the question?

2. Ablate the intervention: For IVRD and IGV, remove the causal intervention (e.g., set prototype selection probabilities proportional to training frequency, or skip environment swapping) and compare performance specifically on long-tail or out-of-distribution test sets where spurious correlations are most problematic.

3. Test on complexity boundaries: Evaluate STR specifically on the longest videos (>80s) and scenes with the most objects (>10) in the test sets; compare against a strong baseline that uses uniform temporal sampling and all detected objects to quantify the actual benefit of adaptive selection.