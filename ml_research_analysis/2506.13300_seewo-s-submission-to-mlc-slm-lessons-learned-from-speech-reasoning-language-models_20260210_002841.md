---
ver: rpa2
title: 'Seewo''s Submission to MLC-SLM: Lessons learned from Speech Reasoning Language
  Models'
arxiv_id: '2506.13300'
source_url: https://arxiv.org/abs/2506.13300
tags:
- speech
- training
- speaker
- loss
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-stage training pipeline for multilingual
  conversational speech recognition that enhances reasoning and self-correction in
  speech language models. The approach combines curriculum learning, Chain-of-Thought
  data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR) to
  improve model performance.
---

# Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models

## Quick Facts
- arXiv ID: 2506.13300
- Source URL: https://arxiv.org/abs/2506.13300
- Reference count: 0
- Primary result: 11.57% WER/CER for Track 1 (ASR) and 17.67% tcpWER/tcpCER for Track 2 (SD-ASR) on evaluation set

## Executive Summary
This paper presents a multi-stage training pipeline for multilingual conversational speech recognition that enhances reasoning and self-correction in speech language models. The approach combines curriculum learning, Chain-of-Thought data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR) to improve model performance. The system achieves substantial improvements over official baselines, demonstrating that explicit reasoning capabilities and self-correction can be effectively integrated into speech language models through systematic training strategies.

## Method Summary
The method employs a multi-stage training pipeline combining curriculum learning, Chain-of-Thought data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR). The approach begins with pre-training on large-scale multilingual speech data, followed by instruction tuning with specialized token adaptation for instruction following. Chain-of-Thought training incorporates weighted loss functions, while RLVR employs carefully designed reward functions to encourage reasoning and self-correction behaviors. The curriculum design progressively increases task complexity, allowing the model to develop robust reasoning capabilities across different speech recognition scenarios.

## Key Results
- Achieved 11.57% WER/CER for Track 1 (ASR) on evaluation set
- Achieved 17.67% tcpWER/tcpCER for Track 2 (SD-ASR) on evaluation set
- Substantial improvements over official baselines

## Why This Works (Mechanism)
The multi-stage training pipeline works by systematically building reasoning capabilities through progressive task complexity. Curriculum learning allows the model to first master simpler speech recognition tasks before advancing to more complex reasoning scenarios. Chain-of-Thought augmentation provides explicit reasoning steps that help the model develop logical problem-solving approaches. RLVR with carefully designed rewards reinforces correct reasoning patterns and self-correction behaviors, creating a feedback loop that strengthens the model's ability to handle ambiguous or challenging speech inputs. The weighted loss functions during Chain-of-Thought training ensure that the model pays appropriate attention to different aspects of the reasoning process, preventing it from focusing too heavily on any single component.

## Foundational Learning
1. **Curriculum Learning** - why needed: Gradually increases task difficulty to build robust capabilities; quick check: Monitor learning curves for smooth progression
2. **Chain-of-Thought Reasoning** - why needed: Provides explicit logical steps for complex decision-making; quick check: Validate intermediate reasoning outputs for coherence
3. **Reinforcement Learning with Verifiable Rewards** - why needed: Creates feedback loops for self-correction and reasoning quality; quick check: Test reward signal stability across training epochs
4. **Weighted Loss Functions** - why needed: Balances attention across different model outputs and reasoning steps; quick check: Monitor gradient distributions for each loss component
5. **Token Adaptation for Instruction Following** - why needed: Enables effective processing of instruction-based inputs in speech tasks; quick check: Evaluate instruction compliance rates on held-out examples

## Architecture Onboarding
**Component Map**: Speech Encoder -> Reasoning Module -> Output Decoder -> Reward Calculator -> RL Optimizer
**Critical Path**: Speech Encoder -> Reasoning Module -> Output Decoder
**Design Tradeoffs**: The pipeline balances between computational efficiency (curriculum stages) and model capability (Chain-of-Thought reasoning), with RLVR adding complexity but improving reasoning quality. Token adaptation increases model flexibility but requires careful weight balancing during training.
**Failure Signatures**: Poor curriculum progression leads to catastrophic forgetting; inadequate reward design causes mode collapse in reasoning patterns; improper weighted losses result in biased output distributions.
**First 3 Experiments**:
1. Validate curriculum progression by testing model performance on each training stage separately
2. Test Chain-of-Thought reasoning quality by examining intermediate outputs on held-out examples
3. Verify RLVR effectiveness by comparing reasoning patterns before and after reinforcement learning application

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ablation studies prevent isolation of individual component contributions
- Specific architectural choices for token adaptation and reward design lack full specification
- Comparison limited to official baselines rather than competing state-of-the-art approaches

## Confidence
- High confidence: The general training methodology combining curriculum learning, CoT data augmentation, and RLVR is sound and follows established patterns in the field.
- Medium confidence: The reported WER/CER and tcpWER/tcpCER improvements are real but the absolute performance numbers should be interpreted cautiously given the limited competitive context.
- Low confidence: The specific impact of individual technical innovations (weighted loss functions, reward design) on the final performance cannot be definitively assessed without additional ablation studies.

## Next Checks
1. Conduct ablation studies isolating the effects of curriculum learning, Chain-of-Thought augmentation, and RLVR on model performance to quantify individual contributions.
2. Compare performance against other top-performing systems from the same challenge or recent literature to establish the relative standing of this approach.
3. Test the trained model on additional multilingual speech datasets outside the challenge to evaluate robustness and generalization beyond the specific evaluation set.