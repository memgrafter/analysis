---
ver: rpa2
title: 'LoRMA: Low-Rank Multiplicative Adaptation for LLMs'
arxiv_id: '2506.07621'
source_url: https://arxiv.org/abs/2506.07621
tags:
- lorma
- rank
- matrix
- lora
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Low-Rank Multiplicative Adaptation (LoRMA),
  a parameter-efficient fine-tuning method that replaces the additive update of LoRA
  with multiplicative transformations. This approach addresses the limitations of
  LoRA by enabling richer weight updates and faster convergence.
---

# LoRMA: Low-Rank Multiplicative Adaptation for LLMs

## Quick Facts
- **arXiv ID**: 2506.07621
- **Source URL**: https://arxiv.org/abs/2506.07621
- **Reference count**: 36
- **Primary result**: LoRMA achieves competitive or superior performance compared to LoRA and its variants while demonstrating faster convergence across multiple NLP tasks

## Executive Summary
This paper introduces Low-Rank Multiplicative Adaptation (LoRMA), a parameter-efficient fine-tuning method that replaces LoRA's additive weight updates with multiplicative transformations. The approach addresses LoRA's limitations by enabling richer weight updates through matrix multiplication while maintaining parameter efficiency. Two rank inflation strategies (Rank-Boom and Rank-Increase) are proposed to overcome the rank bottleneck inherent in multiplicative operations. Experimental results demonstrate that LoRMA achieves competitive or superior performance compared to LoRA and its variants across multiple NLP tasks and mathematical reasoning benchmarks, with faster convergence observed in many cases.

## Method Summary
LoRMA fundamentally modifies the LoRA approach by replacing additive weight updates with multiplicative transformations. While LoRA updates model weights as W + BA, where B and A are low-rank matrices, LoRMA applies W ⊙ (BA), using element-wise multiplication. This multiplicative approach allows for richer weight transformations and faster convergence. To address the rank bottleneck in matrix multiplication, the authors introduce two rank inflation strategies: Rank-Boom, which scales rank quadratically with model size, and Rank-Increase, which progressively increases rank during training. The method maintains parameter efficiency while enabling more expressive weight updates, potentially capturing complex patterns more effectively than additive approaches.

## Key Results
- LoRMA+ and LoRMAπ achieve comparable or slightly better accuracy than LoRA on GLUE tasks (RTE, BoolQ, CB)
- On mathematical reasoning tasks using Gemma-2B and LLaMA-3-8B, LoRMA+ matches or exceeds performance of LoRA, DoRA, and SVFT
- LoRMA demonstrates faster convergence across multiple tasks compared to LoRA baselines
- The method scales effectively across different model sizes and ranks

## Why This Works (Mechanism)
LoRMA's multiplicative adaptation enables richer weight updates compared to LoRA's additive approach. By using element-wise multiplication (W ⊙ (BA)) instead of addition (W + BA), the method can capture more complex patterns in the weight space. The rank inflation strategies (Rank-Boom and Rank-Increase) address the fundamental limitation that matrix multiplication requires higher ranks to maintain expressiveness, allowing the method to scale effectively with model size while preserving the efficiency benefits of low-rank adaptation.

## Foundational Learning

**Low-Rank Matrix Decomposition**: Decomposing weight matrices into products of smaller matrices to reduce parameters while maintaining expressiveness. Needed for efficient adaptation without full fine-tuning; quick check: verify rank-r approximation quality using SVD.

**Parameter-Efficient Fine-Tuning**: Methods that update only a small subset of model parameters during adaptation. Needed to make large model adaptation computationally feasible; quick check: count total trainable parameters vs full model size.

**Matrix Multiplication vs Addition**: Different algebraic operations for weight updates. Needed because multiplication can capture more complex transformations; quick check: compare gradient flow patterns between additive and multiplicative updates.

**Rank Bottleneck**: The limitation that matrix multiplication requires higher ranks for equivalent expressiveness. Needed to understand why LoRA's rank strategy doesn't directly transfer; quick check: analyze rank-r approximation error for multiplication vs addition.

**Element-Wise Operations**: Pointwise operations applied across tensors. Needed for LoRMA's multiplicative adaptation; quick check: verify broadcasting rules and memory access patterns.

## Architecture Onboarding

**Component Map**: Input data → Embedding layer → Transformer blocks → LoRMA adapters (replacing LoRA) → Output layer. The critical path flows through the modified attention and feed-forward sublayers where LoRMA matrices are applied multiplicatively.

**Critical Path**: The LoRMA adapters are inserted at the same locations as LoRA adapters in the transformer architecture, specifically targeting the attention output and feed-forward layers. The multiplicative operation occurs after the original weight matrix application.

**Design Tradeoffs**: Multiplicative updates offer richer transformations but require higher ranks, increasing parameter count. The rank inflation strategies balance expressiveness against efficiency. Element-wise multiplication may cause numerical stability issues compared to addition.

**Failure Signatures**: Poor convergence may indicate insufficient rank inflation or numerical instability from multiplicative operations. Suboptimal performance compared to LoRA suggests the task doesn't benefit from multiplicative updates or the rank strategy is mismatched.

**3 First Experiments**:
1. Compare convergence curves of LoRMA vs LoRA on a single GLUE task to verify faster convergence claims
2. Ablation study varying rank inflation strategy (Rank-Boom vs Rank-Increase) on model performance
3. Memory and computational overhead analysis comparing LoRMA to LoRA across different hardware configurations

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Evaluation limited to 3 NLP datasets and mathematical reasoning benchmarks, may not generalize to all downstream applications
- Faster convergence claims lack rigorous statistical analysis comparing convergence rates against baselines
- Rank inflation strategies are heuristic without theoretical justification for optimal parameter selection
- Computational overhead compared to LoRA is not explicitly quantified in terms of training time or memory usage

## Confidence
- Core claim of multiplicative adaptation providing richer weight updates: **Medium**
- Claim of faster convergence: **Medium** (limited quantitative evidence)
- Performance claims against LoRA variants: **High** for tested tasks, **Low** for generalization to unseen domains

## Next Checks
1. Conduct extensive ablation studies on rank inflation strategies to determine optimal parameter settings across different model sizes and tasks
2. Perform comprehensive computational analysis comparing training time, memory usage, and FLOPs between LoRMA and LoRA across various hardware configurations
3. Evaluate LoRMA on diverse downstream tasks including code generation, multilingual benchmarks, and domain-specific applications to assess generalization beyond the current test suite