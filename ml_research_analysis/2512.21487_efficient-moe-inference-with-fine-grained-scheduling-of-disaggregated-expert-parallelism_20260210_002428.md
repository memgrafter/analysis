---
ver: rpa2
title: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert
  Parallelism
arxiv_id: '2512.21487'
source_url: https://arxiv.org/abs/2512.21487
tags:
- expert
- attention
- findep
- time
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inefficiency in MoE inference under disaggregated
  expert parallelism (DEP), where coarse-grained scheduling and limited support for
  shared experts cause GPU idle time. FinDEP addresses this by introducing fine-grained
  task scheduling that partitions attention and expert computations into smaller segments,
  enabling better overlap of computation and communication.
---

# Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism

## Quick Facts
- arXiv ID: 2512.21487
- Source URL: https://arxiv.org/abs/2512.21487
- Reference count: 39
- Primary result: Up to 1.61x throughput improvement over state-of-the-art PPPipe method

## Executive Summary
This paper addresses inefficiency in MoE inference under disaggregated expert parallelism (DEP) by introducing FinDEP, a fine-grained task scheduling framework. The key insight is that partitioning attention and expert computations into smaller segments enables better overlap of computation and communication than micro-batch-level pipelining alone. FinDEP formulates an optimization problem to balance partitioning granularity with ordering, developing an efficient solver that finds near-optimal schedules in under a second.

## Method Summary
FinDEP introduces fine-grained tensor partitioning along batch dimension for attention group (AG) and token dimension for expert group (EG), enabling better overlap of A2E communication with expert computation. The framework uses linear performance models with α (startup overhead) and β (scaling factor) terms to characterize the trade-off between partitioning granularity and overhead. An optimization solver iterates through memory-constrained micro-batch sizes, computing maximum pipeline degrees while evaluating two task ordering strategies (AASS and ASAS) to find the best configuration. The solver leverages monotonicity properties to reduce search complexity from exponential to linear in divisor count.

## Key Results
- 1.61x throughput improvement over PPPipe on 8×A6000 system
- 1.24x speedup even on 32-GPU system
- Sub-second scheduling time (<1s) for optimization
- Consistent improvements across DeepSeek-V2 and Qwen3-MoE models

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained tensor partitioning enables better overlap of computation and communication than micro-batch-level pipelining alone. The framework partitions input tensors along batch dimension for AG (creating r₁ segments) and along token dimension for EG (creating r₂ segments). This allows A2E communication to begin before the full attention output is ready, and allows expert computation to overlap with ongoing communication.

### Mechanism 2
Independent optimization of pipeline degrees (r₁ for AG, r₂ for EG) with monotonicity properties enables efficient search. The paper proves throughput is monotonically increasing with mₐ and r₁, and convex with respect to 1/r₂. This reduces search from exponential to O(C·d(M)) where d(M) is the divisor count of memory-constrained batch size.

### Mechanism 3
Two task ordering strategies (AASS and ASAS) cover near-optimal schedules for shared expert integration. AASS (Attention-All, Shared-All) prioritizes early A2E start; ASAS (Attention-Shared-Alternating-Sequential) improves GPU utilization through interleaving. The solver evaluates both and selects the better outcome.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Understanding that each token activates only top-k experts enables the token-dimension partitioning in EG. Quick check: If top-k = 8 and you split tokens into r₂ = 4 groups, how many experts might process tokens from each group?

- **Ping-Pong Pipeline Parallelism (PPPipe)**: FinDEP builds on PPPipe; understanding its micro-batch overlap reveals why finer granularity helps. Quick check: In PPPipe with r₁ = 2, when can EG begin its first computation relative to AG completion?

- **Job-Shop Scheduling**: The optimization subproblem is NP-hard job-shop scheduling; understanding this explains why the solver uses structural properties rather than exhaustive search. Quick check: Why does the four-resource constraint (AG, EG, A2E, E2A) make this scheduling problem NP-hard?

## Architecture Onboarding

- **Component map**: AG (Attention Group) -> A2E Communication -> EG (Expert Group) -> E2A Communication
- **Critical path**: Attention → Shared Expert (can parallelize with A2E) → A2E communication → Expert computation → E2A communication → Next layer
- **Design tradeoffs**: Higher r₁/r₂ → better overlap but more kernel launch overhead; Larger mₐ → higher throughput but constrained by GPU memory; AASS vs ASAS: earlier expert start vs better AG utilization
- **Failure signatures**: Solver returns r₂ = 1 consistently → communication overhead too high; Throughput plateaus despite increasing mₐ → memory bandwidth saturated; ASAS and AASS perform identically → shared expert compute time negligible
- **First 3 experiments**:
  1. Validate performance models on your hardware: Run micro-benchmarks and fit α, β coefficients
  2. Verify monotonicity on small scale: Sweep mₐ and r₁ while fixing other parameters
  3. Compare AASS vs ASAS on shared-expert model: Run DeepSeek-V2 with both orderings

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the discussion section. The limitations around scalability and network topology contention suggest future work directions.

## Limitations

- **Scalability concerns**: At extremely large scales, communication dominates execution time, diminishing the relative improvement from schedule optimization
- **Model generalizability**: Linear performance models may not hold across different GPU architectures or network topologies
- **Shared expert constraints**: The two-task ordering strategies are designed for single shared expert scenarios and may not generalize to multiple shared experts

## Confidence

- **Throughput Improvements**: High confidence - Well-supported by comprehensive experiments across four GPU systems
- **Solver Efficiency**: High confidence - Validated runtime measurements and formal proofs of monotonicity properties
- **Fine-Grained Partitioning Benefits**: Medium confidence - Sound mechanism but hardware-dependent thresholds not fully characterized
- **Shared Expert Generalization**: Low confidence - Limited to single shared expert scenarios with no formal proof of strategy coverage

## Next Checks

1. **Cross-Architecture Validation**: Test FinDEP on GPUs with different interconnect topologies (e.g., AMD MI300X) to verify linear models hold across architectures

2. **Shared Expert Scaling**: Evaluate FinDEP with multiple shared experts (k > 1) to identify breaking points in AASS/ASAS ordering strategy

3. **Extreme Partition Testing**: Systematically test partition granularity beyond optimal point to quantify exact overhead threshold where kernel launch costs dominate