---
ver: rpa2
title: 'MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation'
arxiv_id: '2511.23397'
source_url: https://arxiv.org/abs/2511.23397
tags:
- generation
- dataset
- channels
- posts
- megachat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MegaChat introduces the first fully synthetic Persian Q&A dataset
  for evaluating sales chatbots in Telegram-based e-commerce. Using a novel multi-agent
  architecture with specialized generators, validators, and refiners, the system produces
  persona-aware questions grounded in real Telegram shopping channels.
---

# MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation

## Quick Facts
- arXiv ID: 2511.23397
- Source URL: https://arxiv.org/abs/2511.23397
- Reference count: 26
- First fully synthetic Persian Q&A dataset for evaluating sales chatbots on Telegram using multi-agent architecture

## Executive Summary
MegaChat introduces the first fully synthetic Persian Q&A dataset for evaluating sales chatbots in Telegram-based e-commerce. The system employs a multi-agent architecture with specialized generators, validators, and refiners to produce persona-aware questions grounded in real Telegram shopping channels. Answer generation combines classic RAG approaches with an advanced agentic pipeline featuring multi-query retrieval and persona alignment, evaluated by GPT-5.1. The agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating superior scalability and quality without expensive human annotation or fine-tuning.

## Method Summary
MegaChat uses a three-agent pipeline for question generation (Generator → Validator → Refiner) with confidence-based filtering, and parallel answer generation comparing classic RAG against an agentic system with multi-query retrieval and persona analysis. Data comes from 48 Persian Telegram shopping channels (5,000 posts each), with 5 representative channels selected for Q&A generation. GPT-5.1 ranks 4 candidate answers across 6 dimensions to establish ground truth without human annotation.

## Key Results
- Agentic architecture outperformed RAG in 4 of 5 channels, generating 8-22 best-ranked responses per channel
- Generated 200 persona-aware questions per channel, validated and refined through confidence-based filtering
- First synthetic Persian Q&A dataset eliminating expensive human annotation requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-agent, two-pass pipeline produces more realistic and diverse synthetic questions than single-pass generation.
- **Mechanism:** Specialized agents divide labor - Generator creates persona-aligned questions, Validator checks grounding against source posts, Refiner enhances naturalness and filters outputs below 50% confidence.
- **Core assumption:** Validation and refinement stages correct more errors than they introduce; confidence thresholds correlate with question quality.
- **Evidence anchors:** System employs specialized agents for question generation, validation, and refinement, ensuring production of realistic and diverse conversational data.

### Mechanism 2
- **Claim:** Agentic architecture with multi-query retrieval and reranking outperforms classic single-query RAG for persona-aware sales responses.
- **Mechanism:** Instead of one retrieval pass, the system generates 5-8 diverse queries (specs, pricing, delivery, alternatives), retrieves in parallel, re-ranks via SLM to top-5, then synthesizes with persona analysis.
- **Core assumption:** Multi-query expansion retrieves relevant documents that single-query retrieval misses; reranking effectively prioritizes them.
- **Evidence anchors:** Agentic architecture outperformed RAG models in 4 of 5 channels, producing 8-22 best-ranked responses per channel.

### Mechanism 3
- **Claim:** LLM-as-judge evaluation (GPT-5.1) enables scalable ground truth selection without human annotation.
- **Mechanism:** Four candidate answers (3 RAG + 1 agentic) are ranked across six dimensions (Factual Correctness, Persona Alignment, Emotional Sensitivity, Tone Preference, Interaction Style, Content Preferences). Top-ranked becomes ground truth.
- **Core assumption:** GPT-5.1's ranking aligns with human judgment; the six dimensions capture what users value in sales chatbot responses.
- **Evidence anchors:** GPT-5.1 evaluation layer ensured selection of optimal answers, maintaining high dataset quality.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Classic RAG (retriever + synthesizer) is the baseline. Understanding top-k retrieval, strict grounding prompts, and tone adaptation is essential before comparing against agentic enhancements.
  - **Quick check question:** Can you explain why single-query retrieval might miss relevant product information that multi-query retrieval would capture?

- **Concept: Multi-Agent System Design**
  - **Why needed here:** MegaChat's core innovation is dividing labor across Generator, Validator, Refiner (questions) and Query Generator, Retriever, Re-ranker, Profile Analyzer, Answer Synthesizer (answers).
  - **Quick check question:** What failure mode might occur if the Validator agent is too strict versus too permissive?

- **Concept: LLM-as-Judge Evaluation**
  - **Why needed here:** Ground truth selection depends on GPT-5.1's comparative ranking. Understanding prompt design for evaluation, dimension weighting, and ranking consistency is critical for reproducing or extending this work.
  - **Quick check question:** How would you validate whether an LLM evaluator's rankings correlate with human preferences in your target domain?

## Architecture Onboarding

- **Component map:** Telegram posts → FAISS vector store → Question Generation Pipeline (Persona + Channel Metadata → Generator → Validator → Refiner) → Filtered Questions → Parallel Answer Generation (Classic RAG: Retriever → GPT-4.x; Agentic: SLM Query Generator → Parallel Retrieval → SLM Re-ranker → LLM Profile Analyzer → LLM Synthesizer) → GPT-5.1 ranking across 6 dimensions → Ground truth

- **Critical path:** 1. Telegram post ingestion and filtering (garbage in = garbage out) 2. Question generation with confidence filtering (determines dataset diversity) 3. Parallel answer generation (quality ceiling set here) 4. LLM-as-judge ranking (final quality gate)

- **Design tradeoffs:** Fully synthetic vs. human-validated (no annotation cost, but potential LLM biases remain undetected); Agentic complexity vs. classic RAG (higher compute cost for potentially better coverage); Single evaluator vs. ensemble (GPT-5.1 alone may have blind spots)

- **Failure signatures:** Questions drift from actual channel content (Validator not catching hallucinations); All 4 candidate answers rank similarly (evaluator lacks discriminative power); Agentic system underperforms RAG in specific domains (query expansion adding noise); Persona alignment scores high but responses feel generic (persona extraction failing)

- **First 3 experiments:** 1. A/B test single-query vs. multi-query retrieval on a held-out channel: Measure retrieval recall@5 and final answer ranking to isolate multi-query contribution. 2. Confidence threshold sweep: Vary the 50% filtering threshold and measure question diversity vs. groundedness tradeoff. 3. Evaluator validation: Sample 50 questions, have human annotators rank the 4 candidates, compute agreement with GPT-5.1 rankings to verify judge alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What types of subtle biases does fully automated, LLM-based dataset generation introduce, and how can they be detected and mitigated in synthetic Q&A corpora?
- **Basis in paper:** [explicit] "Automation may introduce subtle biases, which merit future study."
- **Why unresolved:** The paper acknowledges this limitation but does not characterize the nature, magnitude, or downstream effects of any biases introduced by the Generator, Validator, and Refiner agents.
- **What evidence would resolve it:** A comparative audit between MegaChat and a human-annotated Persian sales Q&A dataset, analyzing distributional differences in question types, answer styles, persona representations, and error patterns.

### Open Question 2
- **Question:** How reliable is GPT-5.1 as an evaluator for Persian conversational quality when used without human validation?
- **Basis in paper:** [inferred] The entire ground truth selection depends on GPT-5.1 ranking across six quality dimensions, with no reported human evaluation to validate the LLM's judgments for this specific task and language.
- **Why unresolved:** LLM-as-a-judge approaches can exhibit systematic biases, and their reliability for Persian conversational assessment remains unverified in this domain.
- **What evidence would resolve it:** Inter-annotator agreement scores between GPT-5.1 rankings and human expert evaluations on a sample of questions, using the same six quality dimensions.

### Open Question 3
- **Question:** Why did the agentic architecture underperform classic RAG in the @LBASs2 channel, and what channel-specific factors determine when simpler retrieval methods are sufficient?
- **Basis in paper:** [inferred] Results show the agentic system won in 4 of 5 channels, but @LBASs2 (the largest channel with 3,148 posts) was the exception; this anomaly receives no analysis.
- **Why unresolved:** Understanding failure modes is critical for practitioners deciding between agentic versus classic RAG architectures for different deployment contexts.
- **What evidence would resolve it:** A per-channel analysis of factors such as product category complexity, post structure, query diversity, and persona distributions to identify conditions favoring each approach.

## Limitations
- GPT-5.1 evaluation model is not publicly documented or available, preventing independent validation
- Specific SLM models for query generation and reranking remain unspecified
- No human validation of LLM-as-judge rankings for Persian conversational quality

## Confidence
- **High Confidence:** Multi-agent question generation pipeline structure is clearly specified and mechanistically sound; Data collection methodology is well-documented
- **Medium Confidence:** Classic RAG baseline implementation using FAISS and text-embedding-3-large is reproducible; Agentic architecture benefits cannot be independently verified without access to specified models
- **Low Confidence:** Claims about GPT-5.1's discriminative power and six evaluation dimensions' alignment with user preferences lack external validation; Synthetic dataset quality depends entirely on unverifiable LLM judgments

## Next Checks
1. **Human Validation Study:** Have Persian-speaking annotators rank the 4 candidate answers for 100 sampled questions to measure agreement with GPT-5.1 rankings across the 6 dimensions.
2. **Multi-Query A/B Test:** Implement both single-query and multi-query RAG systems on a held-out channel, measuring retrieval recall@5 and final answer quality to isolate the multi-query contribution.
3. **Confidence Threshold Analysis:** Systematically vary the 50% question filtering threshold and measure the tradeoff between question diversity (total generated) and groundedness (manual relevance scoring of 100 questions per threshold level).