---
ver: rpa2
title: Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks
arxiv_id: '2510.21983'
source_url: https://arxiv.org/abs/2510.21983
tags:
- persuasive
- llms
- prompts
- jailbreak
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how persuasive principles can be leveraged\
  \ to jailbreak aligned LLMs. By systematically rewriting harmful queries using Cialdini\u2019\
  s seven persuasion principles (e.g., Scarcity, Social Proof), the authors generate\
  \ adversarial prompts that significantly increase harmful output generation."
---

# Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks

## Quick Facts
- **arXiv ID**: 2510.21983
- **Source URL**: https://arxiv.org/abs/2510.21983
- **Reference count**: 21
- **Primary result**: Persuasive prompts using Cialdini's principles achieve up to 97% improvement in jailbreak success rate across six LLMs

## Executive Summary
This study investigates how persuasive principles can be leveraged to jailbreak aligned LLMs. By systematically rewriting harmful queries using Cialdini's seven persuasion principles (e.g., Scarcity, Social Proof), the authors generate adversarial prompts that significantly increase harmful output generation. Across six LLMs, persuasive prompts achieved up to 97% improvement in attack success rate compared to original prompts. Different models showed distinct susceptibility profiles—Vicuna was most influenced by Scarcity, while DeepSeek favored Unity. The method outperformed baselines in both attack success and stealth (lower perplexity), producing human-readable jailbreaks. The findings highlight the role of psychological persuasion in AI safety and suggest model-specific forensic profiling for jailbreak detection. Limitations include single dataset evaluation and dependency on a specific prompt generator.

## Method Summary
The method uses WizardLM-Uncensored to rewrite harmful queries from the AdvBench dataset (520 queries) using seven Cialdini persuasion principles. Each harmful query is rewritten seven times, once per principle, creating 3,640 total variants. These variants are then queried against six target LLMs (Vicuna, Llama2/3, Gemma3, DeepSeek-R1, Phi-4) via black-box access. Responses are evaluated for attack success (keyword-based refusal detection), informative score (via LM evaluator), and perplexity. The Influential Power metric quantifies each principle's effectiveness across models, revealing distinct "persuasive fingerprints" for different architectures.

## Key Results
- Persuasive prompts achieved up to 97% improvement in attack success rate versus original harmful queries
- Scarcity was the most effective persuasion principle overall, while Social Proof showed the highest variation across models
- Vicuna and Llama2 exhibited highly similar persuasion susceptibility profiles, while DeepSeek uniquely prioritized Unity principle
- Persuasive jailbreaks achieved perplexity of 23.62, significantly lower than traditional adversarial methods (GCG at 15895.51 PPL)

## Why This Works (Mechanism)

### Mechanism 1
LLMs trained on human-generated text exhibit learned susceptibility to persuasive linguistic structures. During pre-training, models internalize statistical patterns from human persuasion contexts. When inference-time prompts replicate these structures, learned compliance patterns may override safety alignment. This assumes safety alignment doesn't fully extinguish pre-training correlations between persuasive structures and compliant responses.

### Mechanism 2
Semantic reframing through persuasion principles changes surface linguistic patterns that trigger safety classifiers. The pipeline rewrites harmful queries using persuasion strategies that distribute harmful requests across longer, contextually rich prompts. This circumvents keyword-based or short-context refusal triggers by embedding harmfulness in persuasive context.

### Mechanism 3
Different LLM architectures and training regimes produce distinct susceptibility profiles to specific persuasion principles. Variation in training data composition, alignment procedures, and architectural choices creates model-specific correlations with different persuasion types, resulting in unique "persuasive fingerprints" for each model.

## Foundational Learning

- **Concept**: Cialdini's Seven Persuasion Principles
  - **Why needed here**: The entire attack methodology maps to these social science principles. Without understanding what "Scarcity" or "Social Proof" means in human communication, you cannot interpret results or design new experiments.
  - **Quick check question**: Can you explain why "Reciprocity" was least effective across models while "Scarcity" was most effective?

- **Concept**: Black-Box Threat Model
  - **Why needed here**: The attack assumes only query access to target models. This constrains optimization possibilities and explains why the method uses an uncensored helper model for prompt generation rather than direct target optimization.
  - **Quick check question**: Why does the threat model use WizardLM-Uncensored for rewriting rather than optimizing prompts against the target directly?

- **Concept**: Perplexity as Stealth Metric
  - **Why needed here**: Lower perplexity correlates with human-readable, semantically coherent text. This matters because high-perplexity adversarial suffixes are easily filtered. The method achieves 23.62 PPL, making attacks stealthier.
  - **Quick check question**: Why would a lower perplexity jailbreak be more concerning for deployed systems?

## Architecture Onboarding

- **Component map**: Harmful Query Input -> Persuasion Rewriter -> Target LLMs -> Evaluation Layer
- **Critical path**: Query → Rewrite per 7 principles (7x expansion) → Batch query targets → Collect responses → Score for refusal keywords and harmfulness → Aggregate per-principle effectiveness
- **Design tradeoffs**:
  - Using uncensored helper model vs. target-specific optimization: Enables black-box applicability but may not find optimal per-target prompts
  - Human-readable outputs vs. raw ASR maximization: Trades some attack success for stealth and interpretability
  - Single dataset (AdvBench) vs. multi-dataset evaluation: Limits generalizability claims but enables controlled comparison
- **Failure signatures**:
  - High refusal rates despite persuasion rewriting → Target model has stronger alignment or different training distribution
  - Inconsistent principle rankings across similar architectures → Check for prompt generation variance, increase samples
  - Low informative scores despite high ASR → Model provides minimal harmful content; consider multi-turn follow-ups
- **First 3 experiments**:
  1. Replicate baseline comparison: Run original AdvBench prompts vs. persuasive variants on a single target model to confirm ASR uplift magnitude.
  2. Profile a new model: Apply the full pipeline to a model not in the paper (e.g., Mistral, Claude) to generate its persuasion fingerprint and compare to published profiles.
  3. Test perplexity defense: Apply a perplexity threshold filter to persuasive prompts and measure how many are blocked vs. GCG-style attacks.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of prompt-generation model (beyond WizardLM-Uncensored) affect the effectiveness and characteristics of persuasion-based jailbreaks? Only one uncensored model was used, leaving unknown whether observed attack success is contingent on WizardLM-Uncensored's specific rewriting style or generalizes across generators.

- **Open Question 2**: Do persuasion-based jailbreak attack patterns generalize across diverse harmful content benchmarks beyond AdvBench? AdvBench covers 520 queries but may not represent the full spectrum of harmful request types; whether Scarcity and Social Proof remain dominant across different harm taxonomies is unknown.

- **Open Question 3**: What defense mechanisms can specifically detect or mitigate persuasion-aware jailbreaks while preserving legitimate persuasive communication? Standard perplexity filters fail because persuasive prompts are linguistically natural; distinguishing adversarial persuasion from benign persuasive language remains an unstated challenge.

- **Open Question 4**: What architectural or training-data characteristics explain why different LLMs exhibit distinct persuasive fingerprints? The paper observes model-specific susceptibility profiles but does not investigate whether these stem from training data composition, alignment procedures, or architectural differences.

## Limitations

- Reliance on single dataset (AdvBench) limits generalizability across different types of harmful content and attack scenarios
- Dependence on WizardLM-Uncensored as prompt generator may reflect helper model's capabilities rather than universal persuasion principles
- No exploration of multi-turn jailbreak strategies or defensive countermeasures beyond perplexity-based filtering

## Confidence

**High Confidence Claims:**
- Persuasion principles can systematically improve jailbreak success rates across multiple LLMs
- Different models exhibit distinct susceptibility profiles to specific persuasion principles
- The method produces human-readable outputs with lower perplexity than traditional adversarial attacks

**Medium Confidence Claims:**
- Scarcity is universally the most effective persuasion principle (based on single dataset)
- Vicuna and Llama2 share similar persuasion fingerprints while Llama3 diverges
- The approach outperforms GCG-style attacks in both ASR and stealth metrics

**Low Confidence Claims:**
- Universal applicability across different LLM architectures and training regimes
- Effectiveness against state-of-the-art safety mechanisms beyond basic keyword filtering
- Generalizability to non-AdvBench harmful query types

## Next Checks

1. **Cross-dataset validation**: Test persuasion principles on at least two additional harmful query datasets (e.g., HARMBench, RealToxicityPrompts) to verify Scarcity remains consistently effective and model-specific fingerprints hold across diverse content.

2. **Defense robustness testing**: Implement semantic harmfulness detection and evaluate whether persuasive jailbreaks retain effectiveness when facing defenses that understand intent rather than just surface patterns.

3. **Multi-turn attack validation**: Design experiments to test whether persuasive principles maintain effectiveness in multi-turn conversations where models have more context about user's harmful intent, as real-world attacks rarely consist of single-turn exchanges.