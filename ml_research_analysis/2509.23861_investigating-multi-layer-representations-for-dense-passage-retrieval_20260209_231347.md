---
ver: rpa2
title: Investigating Multi-layer Representations for Dense Passage Retrieval
arxiv_id: '2509.23861'
source_url: https://arxiv.org/abs/2509.23861
tags:
- retrieval
- linguistics
- document
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes using representations from multiple layers\
  \ of a BERT or T5 encoder to improve dense passage retrieval. Unlike prior work\
  \ that uses only the last layer, this method\u2014called Multi-layer Representations\
  \ (MLR)\u2014explores different combinations of hidden states across layers to construct\
  \ document embeddings."
---

# Investigating Multi-layer Representations for Dense Passage Retrieval

## Quick Facts
- **arXiv ID:** 2509.23861
- **Source URL:** https://arxiv.org/abs/2509.23861
- **Reference count:** 40
- **Primary result:** Multi-layer representations improve dense passage retrieval accuracy over last-layer-only baselines, especially with retrieval-oriented pre-training and hard negative mining.

## Executive Summary
This paper addresses a fundamental limitation in dense passage retrieval: the exclusive use of last-layer representations from BERT or T5 encoders. The authors propose Multi-layer Representations (MLR), which systematically explores combinations of hidden states across different encoder layers to construct document embeddings. Through extensive experiments, MLR demonstrates significant improvements over standard dual-encoder baselines and competitive methods like ME-BERT and ColBERT, particularly when integrated with advanced techniques like retrieval-oriented pre-training and hard negative mining.

## Method Summary
The paper introduces a novel approach to dense passage retrieval by leveraging representations from multiple encoder layers rather than just the final layer. For multi-vector retrieval, MLR experiments with combinations of the last few layers and varying numbers of vectors. For single-vector retrieval, the authors propose a self-contrastive pooling strategy that aggregates multi-layer vectors more effectively than simple last-layer pooling. The method is designed to maintain the computational efficiency of standard dual-encoders while improving retrieval accuracy. MLR is also shown to integrate seamlessly with advanced techniques such as retrieval-oriented pre-training and hard negative mining, demonstrating its versatility and effectiveness across multiple benchmark datasets.

## Key Results
- Single-vector MLR outperforms standard dual-encoders and achieves better accuracy than ME-BERT and ColBERT in most cases
- Using the last few layers with a small number of vectors in multi-vector retrieval provides optimal performance
- MLR maintains the same efficiency as standard dual-encoders while delivering superior retrieval accuracy
- The method integrates well with retrieval-oriented pre-training and hard negative mining, further boosting performance

## Why This Works (Mechanism)
MLR works by capturing richer semantic information through layer-wise representations rather than relying solely on the final layer's output. Different layers capture different aspects of linguistic information - earlier layers may capture surface-level features while deeper layers capture more abstract semantic relationships. By combining these representations through either multi-vector or single-vector pooling strategies, MLR can represent documents more comprehensively. The self-contrastive pooling mechanism specifically helps by contrasting different layer representations to identify the most informative features for retrieval tasks.

## Foundational Learning
- **Dense Passage Retrieval**: Information retrieval using dense vector representations instead of sparse term matching. Needed to understand the problem context and why dense representations matter for modern search.
- **Transformer Encoder Layers**: Each layer in BERT/T5 processes information progressively, with different layers capturing different linguistic properties. Quick check: Review how attention mechanisms work within encoder layers.
- **Dual-Encoder Architecture**: Separately encodes queries and documents into vectors for efficient retrieval. Needed to understand the baseline system being improved. Quick check: Trace the encoding process from input to final vector.
- **Negative Sampling**: Using non-relevant examples to train the retriever. Important for understanding hard negative mining mentioned as an enhancement. Quick check: Identify how negative samples are selected and used in training.
- **Pooling Strategies**: Methods for aggregating token-level representations into single document vectors. Needed to understand different approaches for creating single-vector representations. Quick check: Compare max pooling, mean pooling, and CLS token usage.

## Architecture Onboarding
**Component Map**: Input Query/Document -> BERT/T5 Encoder (Multiple Layers) -> Layer Selection/Combination -> Document Embedding(s) -> Similarity Calculation -> Retrieved Passages
**Critical Path**: The core pipeline processes queries and documents through the multi-layer encoder, applies the MLR layer combination strategy, and computes similarity scores for retrieval.
**Design Tradeoffs**: Single-vector MLR trades some expressiveness for computational efficiency, while multi-vector approaches capture more information but require more storage and computation. The choice of which layers to combine affects both accuracy and efficiency.
**Failure Signatures**: Poor performance might indicate suboptimal layer selection, ineffective pooling strategies, or architectural incompatibility with the chosen encoder.
**First Experiments**: 1) Compare retrieval accuracy using different layer ranges (early vs. middle vs. late layers). 2) Test various pooling strategies for single-vector MLR. 3) Evaluate the impact of different numbers of vectors in multi-vector retrieval.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation is primarily limited to BERT and T5 architectures, leaving uncertainty about performance with other encoder types
- Limited exploration of the full trade-off space between vector numbers, layer choices, and efficiency
- Results mainly validated on standard benchmarks (NQ, TriviaQA, SQuAD), raising questions about generalization to diverse or specialized tasks
- Does not address robustness to domain shifts, noisy queries, or adversarial scenarios

## Confidence
- **High confidence** in technical feasibility and novelty of MLR approach, with clear improvements on standard benchmarks
- **Medium confidence** in scalability and robustness across different architectures and domains due to limited experimental scope
- **Low confidence** in generalizability to real-world scenarios involving noise or adversarial queries, as these were not evaluated

## Next Checks
1. Systematically test MLR with other encoder architectures (RoBERTa, DeBERTa, or larger-scale models) and document whether improvements persist
2. Conduct experiments on out-of-domain or adversarial datasets to assess the robustness and generalizability of MLR
3. Perform a comprehensive ablation study exploring the full range of vector numbers and layer combinations, including analysis of efficiency trade-offs and potential redundancy effects