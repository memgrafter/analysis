---
ver: rpa2
title: 'Geolog-IA: Conversational System for Academic Theses'
arxiv_id: '2510.02653'
source_url: https://arxiv.org/abs/2510.02653
tags:
- para
- tesis
- sistema
- datos
- conversacional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Geolog-IA, a conversational AI system for
  accessing geology theses from the Central University of Ecuador. It uses Llama 3.1
  and Gemini 2.5 language models, a Retrieval-Augmented Generation (RAG) architecture,
  and an SQLite database to provide natural, accurate answers to queries about academic
  theses.
---

# Geolog-IA: Conversational System for Academic Theses

## Quick Facts
- arXiv ID: 2510.02653
- Source URL: https://arxiv.org/abs/2510.02653
- Reference count: 0
- System uses Llama 3.1 and Gemini 2.5 language models with RAG-SQL architecture

## Executive Summary
Geolog-IA is a conversational AI system that enables natural language queries about geology theses from the Central University of Ecuador. The system combines retrieval-augmented generation with structured database access, allowing users to ask quantitative and qualitative questions about academic theses. Using Llama 3.1 or Gemini 2.5, the system achieves an average BLEU score of 0.87, indicating high coherence and precision in responses.

## Method Summary
The system employs a RAG-SQL architecture where user queries are processed through a ReAct-style SQL agent that generates database queries, executes them against an SQLite database containing 244 thesis records, and synthesizes answers using language models. The database schema includes 16 fields including thesis metadata, thematic categories, and abstracts. The agent uses a Thought→Action→Observation loop to iteratively refine SQL queries when errors occur. Evaluation uses a modified BLEU metric focusing on keyword and numeric matching rather than exact string overlap.

## Key Results
- Achieved average BLEU score of 0.87 across validation questions
- Successfully handles both quantitative queries (counts, dates) and qualitative queries (methodology descriptions)
- Provides intuitive web-based chat interface accessible to students, faculty, and staff
- Demonstrates effective integration of RAG architecture with structured SQL database

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-SQL reduces hallucinations by grounding LLM outputs in a structured, queryable database rather than relying solely on parametric knowledge.
- Mechanism: The system intercepts user questions before they reach the LLM's internal knowledge, forcing retrieval from a local SQLite database containing thesis metadata. The LLM generates SQL, the agent executes it, and the LLM then synthesizes an answer from retrieved results—not from its training data.
- Core assumption: The database schema is well-aligned with user query types; if key fields are missing, retrieval fails regardless of LLM quality.
- Evidence anchors:
  - [abstract] "This strategy allows us to overcome problems such as hallucinations and outdated knowledge."
  - [section 3.2.1] "The great advantage of this structure is that it allows the system RAG not only to access the content textual detailed... but also to consult and manipulate specific and numeric data."
  - [corpus] Related work [18] (Murtaza et al.) discusses RAG on structured vs. unstructured sources, but does not directly validate SQL-RAG superiority—evidence is inferential.
- Break condition: If queries require joins across tables or aggregations not anticipated in the schema, SQL generation may fail or return incomplete results.

### Mechanism 2
- Claim: A ReAct-style SQL agent enables iterative query refinement, improving robustness when initial SQL is incorrect.
- Mechanism: The ZERO_SHOT_REACT_DESCRIPTION agent follows a Thought→Action→Observation loop. If a generated SQL query errors, the agent can revise and retry rather than failing outright. This self-correction is guided by prompt-engineered error handling instructions.
- Core assumption: The LLM can reliably diagnose SQL errors from database error messages; complex schema issues may exceed its diagnostic capacity.
- Evidence anchors:
  - [section 3.2.3] "You must double check your query before executing it. If you get an error while executing a query, rewrite the query and try again."
  - [section 3.3.2] "The agent must extract and debug the generated SQL query, ensuring it is ready for execution."
  - [corpus] No direct corpus validation for ReAct in SQL agents specifically; related work [15] discusses text-to-SQL but not iterative refinement mechanics.
- Break condition: If error messages are uninformative or the schema is too complex, the loop may not converge on a correct query.

### Mechanism 3
- Claim: Adding a manually-curated "temática" (thematic category) field improves retrieval precision for domain-specific queries.
- Mechanism: The authors added this field after preliminary testing to better categorize geology topics. This creates an explicit indexing layer that simplifies semantic matching for topic-based queries, reducing reliance on free-text matching in "resumen."
- Core assumption: Manual categorization is consistent and complete across all 244 theses; ambiguous or missing themes degrade benefit.
- Evidence anchors:
  - [section 3.1] "The field 'temática' was added after a series of preliminary tests, with the objective of categorizing and organizing more efficiently the different themes."
  - [section 3.1] "This facilitates the search and consultation of specific topics related to theses."
  - [corpus] No direct external validation; this is an internal design decision without comparative ablation reported.
- Break condition: If themes overlap or are inconsistently assigned, the field introduces noise rather than signal.

## Foundational Learning

- Concept: **RAG (Retrieval-Augmented Generation)**
  - Why needed here: Core architecture pattern—understand why retrieval is separated from generation and how grounding works.
  - Quick check question: If the database is empty, what will the LLM output when asked "How many theses exist?"

- Concept: **Text-to-SQL with LLMs**
  - Why needed here: The agent's primary job is translating natural language to SQL; you need to understand prompt engineering for schema-aware query generation.
  - Quick check question: Given schema "tesis(id, titulo, autor, year_approval)", what SQL should "Which author has the most theses?" produce?

- Concept: **BLEU Metric Adaptation**
  - Why needed here: Evaluation methodology is non-standard—BLEU was adapted to prioritize numeric and keyword matches over exact string overlap.
  - Quick check question: If the expected answer is "10" and the system outputs "ten theses were completed," how would standard BLEU vs. their adapted BLEU score differ?

## Architecture Onboarding

- Component map:
  - SQLite Database (tesis.db) → LLM (Llama 3.1/Gemini 2.5) → SQL Agent (ZERO_SHOT_REACT_DESCRIPTION) → Interface (Gradio/Streamlit)

- Critical path:
  1. User question → 2. Agent constructs prompt with schema context → 3. LLM generates SQL → 4. Agent extracts and executes SQL → 5. Database returns results → 6. Agent prompts LLM for natural language answer → 7. Response displayed

- Design tradeoffs:
  - SQLite vs. PostgreSQL: Chose SQLite for simplicity and zero-config deployment; sacrificed concurrent write support and advanced query optimization
  - Structured vs. Unstructured: Rejected PDF-based retrieval because it couldn't answer quantitative queries (e.g., "How many theses in 2023?")
  - Llama 3.1 vs. Gemini 2.5: Llama for local control in Colab; Gemini for API-based access in Hugging Face (resource constraints)

- Failure signatures:
  - **Empty or generic answers**: Usually indicates SQL query returned no results—check schema alignment
  - **SQL syntax errors in logs**: Agent loop failing to converge—may need prompt refinement or schema simplification
  - **Quantitative questions returning qualitative answers**: Check if relevant numeric fields (year_approval, number_pages) are being queried

- First 3 experiments:
  1. **Schema stress test**: Submit 20 queries requiring aggregations (COUNT, AVG, GROUP BY) and verify SQL correctness manually—identify schema gaps.
  2. **Error recovery test**: Intentionally submit ambiguous queries and observe whether the ReAct loop corrects or stalls—log iteration count.
  3. **Field ablation**: Temporarily remove the "temática" field and compare retrieval precision on topic-based queries—measure BLEU delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed RAG-SQL architecture be effectively generalized to other academic disciplines with distinct terminological requirements?
- Basis in paper: [explicit] The conclusion states the tool "establishes a basis for future applications in other disciplines" and serves as a "model of reference."
- Why unresolved: The current implementation is fine-tuned for geology theses; it is unclear if the schema design and prompt engineering transfer effectively to fields with less structured data or different jargon.
- What evidence would resolve it: Successful deployment and evaluation of the same system architecture in a disparate faculty (e.g., social sciences or humanities).

### Open Question 2
- Question: Does relying exclusively on metadata and abstracts, rather than full-text content, limit the system's ability to answer deep methodological queries?
- Basis in paper: [inferred] The authors explicitly assume that the "summary provides enough information" and that incorporating full theses would be "inefficient," but do not test the performance loss on questions requiring internal document details.
- Why unresolved: While efficient, excluding full text may result in generic answers to specific technical questions regarding methodology or data analysis contained within the body of the thesis.
- What evidence would resolve it: A comparative evaluation where the current system answers specific methodological questions against a baseline using full-text vector embeddings.

### Open Question 3
- Question: To what extent does the adapted BLEU metric correlate with human expert judgment regarding factual accuracy in this specific domain?
- Basis in paper: [inferred] The paper utilizes a modified BLEU score (avg 0.87) for evaluation but acknowledges the metric requires flexible criteria (like partial keyword matches) to reflect utility.
- Why unresolved: BLEU measures linguistic similarity (n-grams) rather than factual correctness; a high score does not guarantee the absence of hallucinations or logical errors in the synthesized SQL.
- What evidence would resolve it: A human-in-the-loop evaluation where domain experts grade the factual consistency of responses independent of the BLEU score.

## Limitations
- The 244-record thesis dataset is not publicly available, preventing exact replication
- The custom BLEU adaptation methodology lacks complete specification of scoring thresholds and rules
- The exceptionally high 0.87 BLEU score raises concerns about metric implementation or potential overfitting
- No comparative evaluation against full-text retrieval systems to quantify the impact of using only metadata and abstracts

## Confidence
- **High**: RAG-SQL architecture design and database integration (well-documented components with clear implementation path)
- **Medium**: ReAct agent error handling effectiveness (mechanism described but not empirically validated against baseline)
- **Low**: BLEU score interpretation and comparative performance (adaptation method not fully specified, no baseline comparison)

## Next Checks
1. Implement the described BLEU adaptation on a standard dataset (e.g., E2E NLG) to verify scoring behavior matches paper claims
2. Create a minimal SQLite schema with synthetic data and test the ReAct agent's SQL generation and error recovery across 50 diverse queries
3. Perform ablation testing by removing the "temática" field to quantify its actual contribution to retrieval precision (requires access to validation questions)