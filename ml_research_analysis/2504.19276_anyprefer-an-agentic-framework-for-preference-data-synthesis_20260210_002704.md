---
ver: rpa2
title: 'Anyprefer: An Agentic Framework for Preference Data Synthesis'
arxiv_id: '2504.19276'
source_url: https://arxiv.org/abs/2504.19276
tags:
- data
- preference
- anyprefer
- arxiv
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anyprefer, a framework that synthesizes high-quality
  preference data for aligning foundation models across diverse applications. It addresses
  the limitation of existing self-rewarding approaches that suffer from bias by introducing
  a cooperative two-player Markov game between a target model and a judge model, augmented
  with external tools for accurate evaluation.
---

# Anyprefer: An Agentic Framework for Preference Data Synthesis

## Quick Facts
- arXiv ID: 2504.19276
- Source URL: https://arxiv.org/abs/2504.19276
- Reference count: 36
- Primary result: Framework synthesizes 58K preference pairs across 4 domains, improving downstream performance by 3-30% average

## Executive Summary
Anyprefer introduces an agentic framework for synthesizing high-quality preference data to align foundation models. The system employs a cooperative two-player Markov game between a target model and a judge model, augmented with external tools for accurate evaluation. Through iterative feedback and prompt optimization, it generates preference pairs that significantly improve model alignment across natural language generation, vision-language understanding, medical image analysis, and visuo-motor control tasks.

## Method Summary
The framework synthesizes preference data through a two-player cooperative Markov game. A target model generates candidate responses, while a judge model evaluates them using external tools (e.g., Web Search, Grounded SAM) to extract objective features. A surrogate LLM-based reward model scores preference pairs, and if the score falls below a threshold, prompt optimization updates both models' prompts via gradient ascent. The process iterates to generate high-quality preference pairs, which are then used to fine-tune models via DPO.

## Key Results
- Average performance improvements of 18.55%, 3.66%, 30.05%, and 16.00% across NL generation, vision-language, medical imaging, and visuo-motor control domains respectively
- 22.4% improvement in judge accuracy when tools and feedback are enabled compared to baseline
- 21.51% average performance improvement when feedback mechanism is active
- Successfully generates 58K preference pairs across 21 datasets

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Judgment
Tool-augmented judgment mitigates bias in self-rewarding systems by grounding evaluations in external, verifiable evidence. The judge model leverages external tools to extract objective features before ranking responses, forcing evaluation to rely on retrieved ground truth rather than potentially hallucinated internal weights. This assumes external tools are more reliable for specific domains than unaided foundation models.

### Mechanism 2: Cooperative Markov Game
Framing data synthesis as a cooperative Markov game enables joint optimization of generation and ranking policies. The Target Model (generation) and Judge Model (ranking) are treated as players that update their prompts based on reward feedback. This assumes prompt optimization via gradient descent effectively shifts model behavior in the high-dimensional space of preference generation.

### Mechanism 3: Surrogate Reward Model
Using a surrogate LLM-based reward model creates a viable proxy for expensive fine-tuning evaluation during the synthesis loop. Instead of time-consuming DPO fine-tuning to validate data quality, the system uses GPT-4o to score preference pairs immediately. This assumes the surrogate reward score has strong positive correlation with downstream task performance after fine-tuning.

## Foundational Learning

- **Concept: Markov Games (Cooperative)**
  - Why needed here: The core innovation models synthesis as a game between two agents with a shared goal
  - Quick check question: Can you explain why the authors use a *cooperative* game rather than an adversarial one (like a GAN) for generating preference data?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Anyprefer generates the dataset, but DPO consumes this data to align the model
  - Quick check question: How does DPO differ from Reinforcement Learning from Human Feedback (RLHF) in terms of reward model dependency?

- **Concept: Prompt Optimization (TextGrad)**
  - Why needed here: The "learning" updates prompts using gradient-like signals rather than model weights
  - Quick check question: In Eq. 2, what serves as the "gradient" when updating the discrete prompt text?

## Architecture Onboarding

- **Component map:** Input Source -> Tool Layer -> Player 1 (Target) -> Player 2 (Judge) -> Evaluator -> Optimizer
- **Critical path:** Target Generation → Tool Retrieval → Judge Ranking → Reward Evaluation → (If low score) Prompt Update → Regenerate
- **Design tradeoffs:**
  - Latency vs. Quality: Iterative feedback loop significantly increases synthesis time compared to one-pass generation
  - Generalization vs. Specificity: Specialized tools anchor data quality but restrict framework to domains where such tools exist
- **Failure signatures:**
  - Tool Failure: Judge receives incorrect bounding boxes or facts, leading to reversed preference pairs
  - Reward Hacking: Target Model learns to satisfy surrogate LLM-judge's quirks rather than true correctness
- **First 3 experiments:**
  1. Ablation on Tools: Run Anyprefer on vision tasks with and without Grounded SAM to quantify specific delta from external grounding
  2. Iterative Scaling: Generate data for 1, 2, and 3 iterations to verify if cooperative game actually improves over time
  3. Domain Transfer: Test synthesized dataset on a model different from the one used to generate it, checking for overfitting

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on domain-specific external tools limits applicability to tasks lacking specialized resources
- Prompt optimization mechanism lacks implementation details for gradient computation on discrete text
- Surrogate reward model's long-term alignment with human preferences remains unproven

## Confidence
- **High confidence:** Cooperative Markov game framework and core architecture are well-specified and supported by ablation studies
- **Medium confidence:** Tool-augmented judgment mechanism is theoretically sound but depends on tool reliability not fully disclosed
- **Low confidence:** Prompt optimization mechanism lacks details, and surrogate reward model's alignment with human utility is unproven

## Next Checks
1. **Tool Dependency Test:** Run Anyprefer on vision-language tasks with and without Grounded SAM to quantify performance delta from external grounding
2. **Iteration Scaling Verification:** Generate preference data over 1, 2, and 3 iterations to confirm cooperative game improves data quality over time
3. **Cross-Model Generalization:** Evaluate synthesized Anyprefer-V1 dataset on a target model different from the one used to generate it, testing for overfitting