---
ver: rpa2
title: Label Words as Local Task Vectors in In-Context Learning
arxiv_id: '2406.16007'
source_url: https://arxiv.org/abs/2406.16007
tags:
- task
- tasks
- local
- vector
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) achieve
  in-context learning (ICL) by analyzing the role of task vectors. Previous work hypothesized
  that a single global task vector, computed by averaging embeddings across demonstrations,
  is sufficient for ICL.
---

# Label Words as Local Task Vectors in In-Context Learning

## Quick Facts
- arXiv ID: 2406.16007
- Source URL: https://arxiv.org/abs/2406.16007
- Reference count: 25
- Key outcome: Local task vectors at label words enable in-context learning, especially when global vectors fail

## Executive Summary
This paper challenges the prevailing view that a single global task vector, computed by averaging across demonstrations, is sufficient for in-context learning (ICL) in large language models. Through systematic analysis of hidden states at demonstration answer positions (label words), the authors discover that local task vectors encode demonstration-specific information that is crucial for rule induction tasks. While these local vectors converge to form a global vector in knowledge tasks, they remain heterogeneous and complementary in categorization tasks where rules must be inferred from multiple examples. Patching these local task vectors to dummy inputs achieves performance comparable to few-shot learning, particularly in tasks where global vector approaches fail.

## Method Summary
The authors analyze hidden states in LLaMA-7B and other LLMs across layers 6-15 to identify task-relevant representations. They extract local task vectors from each demonstration's answer position and compare them to global vectors from the query's "is" position. The key experimental approach involves patching these vectors to dummy sequences with random inputs and evaluating performance recovery. They use saliency analysis to track information flow, PCA and dPCA to identify encoded features, and systematic ablation studies to verify the role of specific representational components. The methodology spans both knowledge tasks (Country→Capital) and categorization tasks (string length thresholding).

## Key Results
- Local task vectors at demonstration answer positions contain demonstration-specific task information, not just demonstration content
- In knowledge tasks, local vectors converge to form global vectors in later layers, enabling effective averaging
- In categorization tasks, local vectors remain heterogeneous and complementary, making averaging ineffective
- Patching local task vectors to dummy inputs achieves ~90% accuracy on knowledge tasks and restores categorization performance where global vectors fail (~50% chance level)

## Why This Works (Mechanism)

### Mechanism 1: Local Task Vector Formation at Answer Positions
Each demonstration's answer position (label word) accumulates task-relevant information through attention mechanisms. Information flows from query tokens through attention to the answer position, where it condenses into a local representation. The strongest attention weights connect demonstration answer positions to the query's "is" position.

### Mechanism 2: Task-Dependent Convergence vs. Heterogeneity
Task type determines whether local vectors can be averaged. In knowledge tasks (e.g., Country→Capital), local vectors become increasingly aligned across layers, enabling effective averaging. In categorization tasks (e.g., length thresholding), local vectors encode complementary boundary information that cannot be averaged without loss.

### Mechanism 3: Layer-Wise Rule Abstraction
Local task vectors progressively abstract from low-level features to high-level task rules across layers. Early layers encode demonstration-specific features (e.g., string length), while later layers retain task-relevant abstractions while discarding surface features.

## Foundational Learning

- **In-Context Learning (ICL)**: Understanding how LLMs learn from demonstrations without gradient updates is essential, as the entire paper analyzes ICL mechanisms. Quick check: Can you explain why ICL differs from fine-tuning and prompt engineering?
- **Task Vectors as Hidden State Representations**: The core contribution distinguishes local from global task vectors extracted from specific token positions and layers. Quick check: What does it mean to "patch" a task vector to a dummy input sequence?
- **PCA and dPCA for Representation Analysis**: The paper uses these techniques to identify which features (length vs. label) are encoded in answer-position representations. Quick check: How would you interpret a Mahalanobis distance that decreases across layers for one feature but not another?

## Architecture Onboarding

- **Component map**: Input [Q, I, A, D]×J + Query [Q, I] → Local task vectors at A positions → Global task vector at I_{J+1} → Output
- **Critical path**: Demonstrations processed → answer positions accumulate local task information → Query "is" position attends to all demonstration answer positions → Local vectors either converge (knowledge) or remain distributed (categorization) → Output generated
- **Design tradeoffs**: Global vector approach is efficient but fails for rule-induction tasks; local vector approach is more generalizable but requires patching multiple positions; averaging strategy works for knowledge tasks but destroys essential information in categorization tasks
- **Failure signatures**: Global vector patching yields chance-level accuracy (~50%) on categorization tasks; model outputs non-label tokens when task representation is insufficient; early-layer patching fails before abstraction completes
- **First 3 experiments**: (1) Saliency heatmap reproduction on 4-demo knowledge and 8-demo categorization tasks; (2) Global vs. local vector patching comparison on Capital vs. Simple String tasks; (3) dPCA ablation test at layers 2, 8, and 14 on categorization task

## Open Questions the Paper Calls Out

### Open Question 1
What specific representational or architectural factors cause task vectors (both global and local) to fail inconsistently across different LLM families and task types? The paper notes that neither global nor local task vectors work consistently across all tasks and models tested, but doesn't investigate internal model differences that lead to failure.

### Open Question 2
Why does patching local task vectors fail to restore few-shot performance in a minority of models and tasks? The authors report these failure cases as empirical observations without offering a causal explanation for the missing information in these instances.

### Open Question 3
Can a single "global" vector be effectively constructed for categorization tasks using non-linear aggregation methods rather than simple averaging? The study establishes that simple averaging destroys essential information in categorization tasks, but leaves open whether other aggregation functions could successfully compress the distributed local information.

## Limitations
- Analysis focuses primarily on LLaMA-7B and binary categorization tasks, limiting generalizability to larger models and multi-class problems
- The mechanism behind why knowledge tasks exhibit vector convergence while categorization tasks don't remains incompletely explained
- dPCA-based length ablation analysis could benefit from ablation of other potentially confounding features beyond string length

## Confidence
- **High confidence**: Local task vectors exist and encode demonstration-specific information (strong saliency evidence, consistent patching results)
- **Medium confidence**: Local vectors converge to global vectors in knowledge tasks but remain heterogeneous in categorization tasks (supported by empirical patterns but mechanism not fully explained)
- **Medium confidence**: Layer-wise abstraction from features to rules (dPCA ablation shows early-layer sensitivity, but the abstraction process itself is inferred)

## Next Checks
1. **Cross-architecture validation**: Test the local/global vector hypothesis on multiple model families (GPT, Mistral, Claude) to determine if the mechanism is universal or architecture-specific
2. **Multi-class extension**: Replicate the categorization task experiments with 3+ classes to verify whether local vectors remain necessary when class boundaries become more complex
3. **Information flow tracing**: Use causal tracing or attention ablation studies to directly verify that information flows from demonstration inputs through answer positions to the query's "is" position, rather than being computed independently at each location