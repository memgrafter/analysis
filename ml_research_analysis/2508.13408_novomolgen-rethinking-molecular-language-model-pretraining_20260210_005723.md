---
ver: rpa2
title: 'NovoMolGen: Rethinking Molecular Language Model Pretraining'
arxiv_id: '2508.13408'
source_url: https://arxiv.org/abs/2508.13408
tags:
- molecular
- molecules
- chemical
- generation
- atomwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NovoMolGen is a family of transformer-based molecular language
  models pretrained on 1.5 billion molecules to improve de novo molecular design.
  The study systematically investigates how molecular representation (SMILES, SELFIES,
  SAFE, DeepSMILES), tokenization (atomwise vs BPE), model size, and dataset scale
  impact generation performance.
---

# NovoMolGen: Rethinking Molecular Language Model Pretraining

## Quick Facts
- arXiv ID: 2508.13408
- Source URL: https://arxiv.org/abs/2508.13408
- Authors: Kamran Chitsaz; Roshan Balaji; Quentin Fournier; Nirav Pravinbhai Bhatt; Sarath Chandar
- Reference count: 40
- Primary result: NovoMolGen establishes new state-of-the-art results in molecular generation, outperforming prior Mol-LLMs and specialized generative models.

## Executive Summary
This study systematically investigates molecular language model pretraining, examining how molecular representation formats (SMILES, SELFIES, SAFE, DeepSMILES), tokenization strategies (atomwise vs BPE), model sizes (32M to 300M parameters), and dataset scales impact de novo molecular design performance. The researchers pretrained models on 1.5 billion molecules from ZINC and evaluated them across unconstrained and goal-directed molecular generation tasks. Key findings reveal that SMILES with BPE tokenization delivers the most consistent performance, larger models show minimal gains beyond 300M parameters, and pretraining metrics poorly predict downstream optimization success.

## Method Summary
The researchers conducted comprehensive experiments using transformer-based molecular language models trained on ZINC-15 (1.5 billion molecules). They systematically varied four key dimensions: molecular representation formats (SMILES, SELFIES, SAFE, DeepSMILES), tokenization strategies (atomwise vs byte-pair encoding), model sizes (32M, 100M, 300M parameters), and dataset sizes (1.5B, 25M, 1M molecules). Models were evaluated using both distribution-based metrics (validity, uniqueness, novelty, KL divergence) and goal-directed generation metrics (Property Match Objective scores, docking scores). The study compared NovoMolGen against prior Mol-LLMs and specialized generative models across multiple benchmarks.

## Key Results
- NovoMolGen establishes new state-of-the-art results, outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation tasks
- Early saturation observed with minimal performance gains from scaling models beyond 300M parameters
- Weak correlation (r = 0.376, p = 0.358) between pretraining metrics and downstream task success challenges reliability of distribution-based evaluation
- SMILES with BPE tokenization consistently delivers the best performance across different model sizes and tasks

## Why This Works (Mechanism)
The study reveals that current pretraining approaches on synthetic accessibility-focused datasets like ZINC teach models chemical syntax rather than functional semantics. This explains why larger models and improved pretraining metrics don't translate to better goal-directed performance - the dominant signal in pretraining data doesn't capture the functional relationships that matter for real-world applications. The success of SMILES with BPE suggests that learning compositional patterns at the character level, rather than atomic level, better captures the hierarchical structure of molecular representations.

## Foundational Learning

**Molecular Representations** - Chemical structures encoded as text strings (SMILES, SELFIES, SAFE, DeepSMILES) enable transformer models to process molecules as sequences.
*Why needed*: Allows application of powerful NLP techniques to chemical space exploration.
*Quick check*: Verify model can reconstruct molecules from their string representations with high validity.

**Tokenization Strategies** - Atomwise tokenization splits molecules into individual atoms, while BPE learns frequent character patterns.
*Why needed*: Determines how models learn chemical composition and substructure patterns.
*Quick check*: Compare vocabulary size and tokenization patterns across different molecular representations.

**Distribution Metrics** - Validity, uniqueness, novelty, and Fréchet ChemNet Distance (FCD) measure how well generated molecules match training distributions.
*Why needed*: Provide quantitative assessment of model's ability to generate realistic, diverse molecules.
*Quick check*: Calculate FCD between generated and training molecule sets.

**Goal-Directed Metrics** - Property Match Objective (PMO) scores and docking scores evaluate how well models optimize for specific molecular properties.
*Why needed*: Assess practical utility for real-world drug discovery applications.
*Quick check*: Run docking simulations to verify generated molecules bind target proteins.

## Architecture Onboarding

**Component Map**: Molecular data → Tokenizer → Transformer layers → Output layer → Generated molecules
**Critical Path**: Input SMILES/SELFIES → Tokenization → Encoder layers → Self-attention → Prediction → Decode molecules
**Design Tradeoffs**: Atomwise tokenization offers simplicity but larger vocabularies; BPE learns patterns but requires more training; larger models increase capacity but show diminishing returns; more data improves distribution learning but has minimal impact on functional optimization.

**Failure Signatures**: Poor validity rates indicate tokenization or architecture issues; low novelty suggests mode collapse; weak PMO scores despite good distribution metrics indicate models learned syntax but not functional semantics.

**First Experiments**: 1) Compare validity rates across different tokenizations on the same molecular subset, 2) Test if BPE vocabulary size correlates with downstream performance, 3) Evaluate whether adding chemically meaningful tokens (rings, branches) improves goal-directed generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating functional objectives (protein-ligand interactions, physicochemical properties, bioactivity) during early pretraining enable models to learn functional semantics rather than just chemical syntax?
- Basis in paper: "Models should be guided by contextual signals early in training in combination with self-supervision... By enriching the training process with such signals, future models can be guided to master not only chemical syntax but also the functional utility."
- Why unresolved: Current pretraining on ZINC teaches synthetic accessibility but not biological relevance; natural product datasets are too small for large-scale pretraining.
- What evidence would resolve it: Comparative experiments with multi-objective pretraining incorporating docking scores or bioactivity labels, evaluated on diverse downstream optimization tasks.

### Open Question 2
- Question: What pretraining metrics would reliably predict downstream goal-directed optimization performance?
- Basis in paper: "The low correlation (r = 0.376, p = 0.358) challenges the utility of distribution-based metrics like FCD as reliable predictors of a model's functional capabilities in goal-directed generation."
- Why unresolved: FCD and validity capture distribution alignment and syntax but not functional utility; no alternative proxy metrics have been validated across diverse optimization tasks.
- What evidence would resolve it: Systematic evaluation of candidate metrics (e.g., property-aware FCD variants, embedding-based similarity to bioactive compounds) against PMO scores across multiple model architectures and training regimes.

### Open Question 3
- Question: Would scaling model size beyond 300M parameters yield meaningful improvements if pretraining objectives incorporated functional signals?
- Basis in paper: The authors found minimal gains from scaling (32M to 300M) and attribute saturation to weak functional signals in current datasets ("dominant signal teaches models chemical syntax rather than functional semantics").
- Why unresolved: Scaling laws in NLP suggest larger models benefit from richer supervision; it remains unclear whether functional objectives would unlock additional capacity.
- What evidence would resolve it: Training larger models (500M–1B parameters) with functionally-guided pretraining objectives and comparing PMO/docking performance against smaller baselines.

## Limitations
- Weak correlation between pretraining metrics and downstream task success suggests current evaluation frameworks may not capture real-world performance requirements
- Early saturation in larger models raises questions about whether fundamental limitations exist or if more sophisticated architectures could yield further improvements
- Focus on transformer architectures may miss alternative model designs that could perform better for molecular generation tasks

## Confidence

Performance superiority claims (High): Well-supported through systematic comparisons with established benchmarks and baselines.
Tokenization and representation findings (Medium): Clear comparative results but underlying reasons remain partially unexplained, with potential variation across different molecular datasets.
Early saturation findings (Medium): Well-documented observation but not conclusively determined whether this represents fundamental limitation or training approach artifact.
Downstream correlation findings (Low): Reported weak correlation but no definitive explanations provided or alternative evaluation approaches validated.

## Next Checks

1. Investigate alternative model architectures beyond transformers, such as graph neural networks or hybrid approaches, to determine if early saturation is architecture-specific or a more general phenomenon in molecular generation.

2. Conduct ablation studies on different molecular property distributions and chemical spaces to understand the generalizability of the tokenization and representation findings across diverse molecular classes and applications.

3. Develop and validate new evaluation frameworks that better predict downstream task success, potentially incorporating domain-specific metrics or multi-task pretraining objectives that correlate more strongly with real-world performance requirements.