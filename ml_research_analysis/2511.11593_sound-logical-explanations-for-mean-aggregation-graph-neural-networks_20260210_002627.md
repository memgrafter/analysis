---
ver: rpa2
title: Sound Logical Explanations for Mean Aggregation Graph Neural Networks
arxiv_id: '2511.11593'
source_url: https://arxiv.org/abs/2511.11593
tags:
- rules
- sound
- rule
- dataset
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining mean-aggregation
  graph neural networks (MAGNNs) for knowledge graph completion. The authors prove
  which monotonic rules can be sound for MAGNNs and provide a restricted fragment
  of first-order logic to explain any MAGNN prediction.
---

# Sound Logical Explanations for Mean Aggregation Graph Neural Networks

## Quick Facts
- arXiv ID: 2511.11593
- Source URL: https://arxiv.org/abs/2511.11593
- Authors: Matthew Morris; Ian Horrocks
- Reference count: 40
- Key outcome: Proved which monotonic rules can be sound for MAGNNs and provided a restricted fragment of first-order logic to explain any MAGNN prediction

## Executive Summary
This paper addresses the challenge of explaining mean-aggregation graph neural networks (MAGNNs) for knowledge graph completion. The authors prove which monotonic rules can be sound for MAGNNs and provide a restricted fragment of first-order logic to explain any MAGNN prediction. Their theoretical contributions include identifying the limited class of ELUQ rules that can be sound for MAGNNs and constructing a procedure to extract sound explanatory rules. Experiments on benchmark datasets show that restricting mean-GNNs to non-negative weights maintains or improves performance while enabling sound rule extraction.

## Method Summary
The paper develops a theoretical framework for extracting sound logical explanations from MAGNNs by restricting the rule language to ELUQ (a fragment of first-order logic) and ensuring non-negative weights. The approach combines theoretical analysis of monotonicity properties with a practical grounding procedure to extract rules that are guaranteed to be sound with respect to the trained MAGNN. The method leverages the mathematical properties of mean aggregation to identify which logical rules can be reliably extracted and proves that these rules will always be sound when certain conditions are met.

## Key Results
- Identified ELUQ as the restricted fragment of first-order logic that can produce sound explanations for MAGNNs
- Demonstrated that non-negative weight constraints maintain or improve performance while enabling sound rule extraction
- Showed that MAGNNs can recover sound monotonic rules on some datasets but struggle on others, revealing model limitations

## Why This Works (Mechanism)
The approach works by leveraging the mathematical properties of mean aggregation in graph neural networks. When weights are non-negative, the aggregation process preserves monotonicity, which allows for the extraction of sound logical rules. The ELUQ fragment provides sufficient expressivity while maintaining the theoretical guarantees needed for sound explanations.

## Foundational Learning

### Knowledge Graph Embeddings
**Why needed**: Understanding how entities and relations are represented as vectors in continuous space
**Quick check**: Can you explain the difference between translational and semantic matching approaches?

### Monotonic Logic
**Why needed**: Essential for understanding which logical rules can be extracted as sound explanations
**Quick check**: What makes a rule monotonic and why does this matter for explanation extraction?

### Graph Neural Networks
**Why needed**: Core architecture that needs to be understood for the explanation methodology
**Quick check**: How does mean aggregation differ from other aggregation functions in GNNs?

## Architecture Onboarding

### Component Map
MAGNN layers -> Non-negative weight constraint -> ELUQ rule extraction -> Sound explanation generation

### Critical Path
1. Train MAGNN with non-negative weights
2. Identify relevant paths in the graph for a given prediction
3. Apply ELUQ grounding procedure
4. Extract sound logical rules

### Design Tradeoffs
The restriction to ELUQ rules enables soundness but limits expressivity. Non-negative weights ensure monotonicity but may constrain model capacity. The grounding procedure guarantees soundness but may be computationally expensive.

### Failure Signatures
Models may fail to extract sound rules when the data contains non-monotonic patterns. Performance may degrade if the ELUQ restriction is too severe for the dataset complexity.

### First Experiments
1. Verify non-negative weight constraint implementation
2. Test ELUQ rule extraction on simple synthetic graphs
3. Compare rule extraction performance across different knowledge graph datasets

## Open Questions the Paper Calls Out
None

## Limitations
- The restriction to ELUQ rules severely limits expressivity, potentially missing important non-monotonic patterns
- MAGNNs struggle to recover sound explanations on some datasets (FB15k-237, WN18RR)
- The grounding procedure may be computationally expensive for large knowledge graphs

## Confidence

**Confidence Labels:**
- Theoretical framework for sound rule extraction: **High**
- Empirical demonstration of soundness on selected datasets: **Medium**
- Claims about performance benefits of non-negative weights: **Medium**
- Generalizability of results across knowledge graph domains: **Low**

## Next Checks
1. Test the grounding procedure's scalability on larger knowledge graphs (e.g., full YAGO or Wikidata) to evaluate practical feasibility
2. Investigate whether alternative aggregation functions (beyond mean) can support broader rule fragments while maintaining soundness
3. Conduct ablation studies comparing rule extraction performance with and without the non-negative weight constraint across diverse KG domains