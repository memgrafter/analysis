---
ver: rpa2
title: Measuring AI Alignment with Human Flourishing
arxiv_id: '2507.07787'
source_url: https://arxiv.org/abs/2507.07787
tags:
- flourishing
- human
- dimensions
- benchmark
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Flourishing AI Benchmark (FAI Benchmark),
  a novel evaluation framework that assesses AI alignment with human flourishing across
  seven dimensions: Character and Virtue, Close Social Relationships, Happiness and
  Life Satisfaction, Meaning and Purpose, Mental and Physical Health, Financial and
  Material Stability, and Faith and Spirituality. Unlike traditional benchmarks focused
  on technical capabilities or harm prevention, the FAI Benchmark evaluates how effectively
  AI systems contribute to human flourishing through 1,229 objective and subjective
  questions.'
---

# Measuring AI Alignment with Human Flourishing
## Quick Facts
- arXiv ID: 2507.07787
- Source URL: https://arxiv.org/abs/2507.07787
- Authors: Elizabeth Hilliard; Akshaya Jagadeesh; Alex Cook; Steele Billings; Nicholas Skytland; Alicia Llewellyn; Jackson Paull; Nathan Paull; Nolan Kurylo; Keatra Nesbitt; Robert Gruenewald; Anthony Jantzi; Omar Chavez
- Reference count: 22
- Highest-scoring model achieved 72/100, none met 90-point threshold for robust alignment

## Executive Summary
This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel evaluation framework that assesses AI alignment with human flourishing across seven dimensions: Character and Virtue, Close Social Relationships, Happiness and Life Satisfaction, Meaning and Purpose, Mental and Physical Health, Financial and Material Stability, and Faith and Spirituality. Unlike traditional benchmarks focused on technical capabilities or harm prevention, the FAI Benchmark evaluates how effectively AI systems contribute to human flourishing through 1,229 objective and subjective questions. Using specialized judge LLMs and cross-dimensional evaluation with geometric mean scoring, the benchmark ensures balanced performance across all flourishing dimensions.

Testing of 28 leading language models revealed that while the highest-scoring model achieved 72/100, none met the 90-point threshold for robust alignment, with Faith, Character, and Meaning dimensions consistently lagging behind. This research establishes a framework for developing AI systems that actively support human flourishing rather than merely avoiding harm.

## Method Summary
The FAI Benchmark evaluates 28 leading language models across 1,229 questions mapped to seven human flourishing dimensions. The benchmark uses a geometric mean scoring system to ensure balanced performance across all dimensions, preventing models from masking poor performance in one area with high performance in another. Specialized judge LLMs with domain-expert personas evaluate subjective responses using a 25-question rubric, while cross-dimensional evaluation checks for spillover effects between life domains. The benchmark combines objective questions (from MMLU and professional exams) with subjective scenarios generated from Global Flourishing Study research.

## Key Results
- No model achieved the 90/100 threshold for robust alignment with human flourishing
- The highest-scoring model reached 72/100 across all dimensions
- Faith, Character, and Meaning dimensions consistently scored lowest across all models tested
- Top 3 models (GPT-4o, GPT-4o mini, Claude-3.5-Sonnet) showed strong overall performance but significant gaps in specific dimensions
- The benchmark revealed that models excel at factual knowledge but struggle with deeper wisdom and virtue-based guidance

## Why This Works (Mechanism)

### Mechanism 1: Geometric Mean Scoring for Balance
The benchmark prevents models from masking poor performance in one area (e.g., Faith) with high performance in another (e.g., Finances). By calculating the final score as the geometric mean of dimensional scores rather than an arithmetic mean, the system heavily penalizes low outliers (near-zero scores drag the total down multiplicatively). This forces models to maintain a minimum threshold of competence across all seven dimensions to achieve a high overall rating. Core assumption: Human flourishing is a "secure" state where all