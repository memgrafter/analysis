---
ver: rpa2
title: 'SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for
  the Low-Resource Maithili Language'
arxiv_id: '2510.22160'
source_url: https://arxiv.org/abs/2510.22160
tags:
- sentiment
- maithili
- language
- dataset
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SentiMaithili, the first benchmark dataset
  for sentiment analysis and justification generation in the low-resource Maithili
  language. The dataset contains 3,221 sentences annotated for sentiment polarity
  (positive/negative) and accompanied by native-language justifications.
---

# SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language

## Quick Facts
- arXiv ID: 2510.22160
- Source URL: https://arxiv.org/abs/2510.22160
- Authors: Rahul Ranjan; Mahendra Kumar Gurve; Anuj; Nitin; Yamuna Prasad
- Reference count: 40
- Primary result: Introduces SentiMaithili dataset (3,221 sentences) with fine-tuned models achieving 97.2% accuracy for sentiment classification and BLEU scores up to 34.70 for justification generation.

## Executive Summary
This paper introduces SentiMaithili, the first benchmark dataset for sentiment analysis and justification generation in the low-resource Maithili language. The dataset contains 3,221 sentences annotated for sentiment polarity (positive/negative) and accompanied by native-language justifications. The sentences were collected from diverse sources including social media, web libraries, and educational materials, then cleaned and validated by linguistic experts. The paper proposes a two-stage hierarchical framework using IndicBERTv2-SS for sentiment classification and IndicBART-SS for justification generation. Experiments show that the fine-tuned IndicBERTv2-SS model achieves 97.2% accuracy in sentiment classification, while IndicBART-SS achieves BLEU scores of 34.70 and ROUGE-1/ROUGE-L scores above 57.0 for justification generation. This work provides a valuable resource for multilingual NLP and explainable AI research in low-resource language settings.

## Method Summary
The paper presents a two-stage hierarchical framework for sentiment analysis and justification generation in Maithili. Stage 1 employs IndicBERTv2-SS, a multilingual transformer pretrained on 23 Indic languages, fine-tuned for binary sentiment classification (positive/negative). Stage 2 uses IndicBART-SS, a sequence-to-sequence model, to generate natural language justifications conditioned on both the input sentence and the predicted sentiment label. The SentiMaithili dataset was constructed through multi-source collection (social media, web libraries, educational materials), preprocessing (noise filtering, morphological correction, code-mixing regularization), and expert annotation (sentiment labels and justifications with majority voting). The models were trained separately using cross-entropy loss for classification and conditional language modeling for generation, with hyperparameters optimized through grid search.

## Key Results
- Fine-tuned IndicBERTv2-SS achieves 97.2% accuracy in Maithili sentiment classification
- IndicBART-SS generates justifications with BLEU score of 34.70 and ROUGE-1/ROUGE-L scores above 57.0
- The two-stage hierarchical approach outperforms zero-shot baselines by 22.7 percentage points in classification accuracy
- Inter-annotator agreement for sentiment labeling reaches Cohen's Kappa of 0.84

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage hierarchical architecture (classification → justification) enables interpretable sentiment analysis by decoupling affective inference from explanation generation while maintaining coherence between predictions and rationales.
- Mechanism: The first stage (IndicBERTv2-SS) encodes input sentences into contextual embeddings and predicts sentiment labels via a classification head. The second stage (IndicBART-SS) conditions on both the original input and predicted label to generate natural language justifications, trained independently with separate loss functions.
- Core assumption: Error propagation from the classifier to the generator is minimal, or the generator can still produce meaningful justifications even with occasional classification errors.
- Evidence anchors:
  - [abstract] "a two-stage hierarchical framework: first, a sentiment classifier... predicts the sentiment label; second, a justification generator... produces an explanation conditioned on both the input sentence and the predicted label"
  - [section 4.1] Problem formulation explicitly defines ̂yᵢ = ℱcls(xᵢ) and ̂rᵢ = ℱgen(xᵢ, ̂yᵢ) as sequential operations
  - [corpus] Similar hierarchical approaches appear in low-resource sentiment work (HausaMovieReview, KuBERT), suggesting transferability of this design pattern
- Break condition: If classification accuracy drops significantly (e.g., below 70%), justification quality would likely degrade due to conditioning on incorrect labels, creating a cascade failure.

### Mechanism 2
- Claim: Fine-tuning multilingual transformers on language-specific annotated data substantially improves performance over zero-shot baselines, with gains of 20-30 percentage points for Maithili sentiment classification.
- Mechanism: Pretrained models (IndicBERTv2, XLM-R, mBERT) have seen diverse multilingual corpora during pretraining, but lack task-specific and language-specific adaptation. Fine-tuning adjusts model parameters to minimize task-specific loss functions (cross-entropy for classification, conditional language modeling for generation) on the curated Maithili dataset.
- Core assumption: The annotated dataset (3,221 samples) is sufficient for effective fine-tuning without severe overfitting, and the pretrained representations are transferable to Maithili despite its low-resource status.
- Evidence anchors:
  - [section 6, Table 5 vs Table 6] Zero-shot IndicBERTv2-SS achieves 74.5% accuracy; fine-tuned version reaches 97.2% (22.7 point gain)
  - [section 6] "fine-tuning not only amplifies cross-lingual transfer capabilities but also underscores the effectiveness of Indic-specific pretraining strategies"
  - [corpus] Related work (MaiBERT, AfriBERTa fine-tuning studies) shows similar gains from language-adaptive fine-tuning in low-resource settings, suggesting this is a generalizable phenomenon
- Break condition: If the dataset were smaller (<500 samples) or noisier (lower inter-annotator agreement), fine-tuning could overfit or learn spurious patterns, reducing generalization.

### Mechanism 3
- Claim: Expert-driven dataset curation with multi-stage preprocessing and linguistic validation produces high-quality annotations that enable reliable model training, as evidenced by strong inter-annotator agreement (κ=0.84).
- Mechanism: Three linguistic experts manually review and correct data using predefined rules (morphological correction, code-mixing regularization, orthographic consistency). Sentiment labels and justifications undergo majority voting and expert verification. ChatGPT provides initial justification drafts that human experts revise, balancing efficiency with quality.
- Core assumption: The expert correction rules capture the most critical linguistic errors, and the hybrid human-AI annotation approach maintains cultural relevance while reducing manual burden.
- Evidence anchors:
  - [section 3.4] Cohen's Kappa scores of 0.82-0.84 indicate "strong" agreement across all expert pairs
  - [section 3.3] "nearly all outputs still require revision" by experts, ensuring final dataset quality
  - [corpus] Corpus evidence is limited—few related papers explicitly report inter-annotator agreement or detail expert annotation protocols for low-resource languages
- Break condition: If annotator expertise were lower or cultural nuances were missed, justifications might become generic or misaligned with actual sentiment, undermining the explainability goal.

## Foundational Learning

- Concept: **Transfer learning and cross-lingual adaptation for low-resource languages**
  - Why needed here: The entire approach relies on pretrained multilingual models (IndicBERTv2, IndicBART) transferring knowledge to Maithili, a language with limited computational resources. Understanding how cross-lingual transfer works—and its limitations—is essential for interpreting results and troubleshooting failures.
  - Quick check question: Can you explain why a model pretrained on 23 Indic languages might still struggle with a specific low-resource language like Maithili, even after fine-tuning?

- Concept: **Sequence-to-sequence models and conditional text generation**
  - Why needed here: The justification generator uses IndicBART-SS, an encoder-decoder architecture that must produce fluent Maithili text conditioned on both input sentences and predicted sentiment labels. Understanding attention mechanisms, autoregressive decoding, and teacher forcing is critical for debugging generation quality.
  - Quick check question: What might cause a seq2seq model to generate justifications that are grammatically correct but semantically inconsistent with the input sentence or predicted sentiment?

- Concept: **Explainable AI and justification evaluation**
  - Why needed here: This work's core contribution is interpretable sentiment analysis, but evaluating justification quality is non-trivial. Metrics like BLEU and ROUGE have known limitations for assessing semantic adequacy and factual consistency, which are critical for explainability.
  - Quick check question: A justification achieves a high BLEU score but attributes sentiment to the wrong word in the input sentence. Does this represent a successful or failed explanation? Why?

## Architecture Onboarding

- Component map: Data pipeline (Multisource collection → Preprocessing → Expert annotation → Majority voting) → Classification stage (IndicBERTv2-SS encoder → [CLS] embedding → Fully connected layer → Softmax → Sentiment label) → Generation stage (Input + predicted label concatenation → IndicBART-SS tokenizer → Encoder-decoder attention → Autoregressive decoding → Natural language justification)

- Critical path: Data quality → Classifier accuracy → Generator conditioning → Justification coherence. Errors at any stage propagate downstream.

- Design tradeoffs:
  - **Two-stage vs. end-to-end**: Decoupling simplifies training and debugging but risks error propagation from classifier to generator
  - **Expert vs. automated annotation**: Higher quality and cultural relevance but slower and costlier
  - **Indic-specific vs. general multilingual models**: Better performance on Indic languages but reduced applicability to non-Indic low-resource languages

- Failure signatures:
  - **Low classification accuracy (<80%)** on test set suggests insufficient fine-tuning data or domain mismatch
  - **Justifications ignore predicted sentiment** indicate weak conditioning on label input
  - **High BLEU but low human judgment scores** reveal metric limitations for semantic evaluation

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train LSTM, mBERT, XLM-R, and IndicBERTv2-SS classifiers on the provided training split, evaluate on test split. Verify reported accuracy gains from fine-tuning.
  2. **Ablate conditioning**: Train justification generator with input sentence only (no predicted label) vs. full conditioning. Compare BLEU/ROUGE scores to quantify the benefit of label conditioning.
  3. **Error analysis on classification failures**: Identify test instances where classifier predictions disagree with ground truth. Examine generated justifications—do they reveal systematic confusion patterns (e.g., sarcasm, domain-specific vocabulary)?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does error propagation affect justification quality when the sentiment classifier produces incorrect predictions in the two-stage hierarchical architecture?
- Basis in paper: [inferred] The framework conditions justification generation on the *predicted* label (ŷᵢ = ℱcls(xᵢ)), not the gold label. If Stage 1 misclassifies, Stage 2 generates justifications for the wrong sentiment, yet this cascading error effect is not analyzed.
- Why unresolved: The paper reports 97.2% classification accuracy, meaning ~2.8% of test instances receive incorrect labels before justification generation, but no analysis examines how justification quality degrades for misclassified inputs.
- What evidence would resolve it: A controlled experiment comparing justification quality (BLEU, ROUGE, human evaluation) on correctly vs. incorrectly classified instances, or an end-to-end joint training approach that shares supervision signal between stages.

### Open Question 2
- Question: How do humans rate the fluency, faithfulness, and cultural appropriateness of generated Maithili justifications compared to expert-written references?
- Basis in paper: [inferred] Justification quality is evaluated solely using automated metrics (BLEU: 34.70, ROUGE-1: 57.77, ROUGE-L: 57.27). The paper emphasizes cultural relevance and contextual fidelity in annotation, but no human evaluation assesses whether models capture these qualitative aspects.
- Why unresolved: Automated n-gram overlap metrics cannot evaluate cultural grounding, logical coherence, or whether justifications genuinely explain the predicted sentiment versus surface-level pattern matching.
- What evidence would resolve it: Human evaluation study with Maithili speakers rating generated justifications on fluency, faithfulness to input sentiment, and cultural appropriateness using Likert scales or comparative ranking.

### Open Question 3
- Question: Can the fine-tuned models and dataset construction methodology transfer effectively to other under-resourced Indo-Aryan languages (e.g., Bhojpuri, Magahi) or languages with different script systems?
- Basis in paper: [explicit] The conclusion states the work "opens new directions for research in cross-lingual transfer, domain adaptation, and culturally grounded explanation methods" and that the framework "aims... to serve as a scalable solution for other low-resource languages."
- Why unresolved: No experiments test cross-lingual transfer to related languages or analyze which components (dataset construction protocols, annotation guidelines, model architectures) generalize vs. require language-specific adaptation.
- What evidence would resolve it: Transfer learning experiments applying Maithili-finetuned models to related languages with minimal additional data, or replication of the dataset construction pipeline for another low-resource language with comparative analysis.

### Open Question 4
- Question: What is the inter-annotator agreement for justification text generation, and how does justification subjectivity affect model training?
- Basis in paper: [inferred] Cohen's Kappa (0.84) is reported only for sentiment labels across three annotators using majority voting. No agreement metric is provided for open-ended justification text, which inherently allows greater subjectivity than binary labels.
- Why unresolved: Justifications were initially generated by ChatGPT then corrected by experts, but the consistency and subjectivity of human justification writing remains unquantified. Models may learn spurious patterns if justifications vary significantly for similar sentiment expressions.
- What evidence would resolve it: Multiple experts independently writing justifications for the same sentences, with agreement measured via semantic similarity metrics or human judgment of equivalence, followed by analysis of training data diversity effects on model performance.

## Limitations
- Dataset size of 3,221 sentences remains relatively small for training deep learning models, particularly the sequence-to-sequence generator
- Evaluation relies heavily on automated metrics (BLEU, ROUGE) which cannot fully capture semantic adequacy and factual consistency for explainable AI
- Annotation process details remain incomplete, particularly the exact guidelines used for sentiment labeling and justification criteria

## Confidence
**High Confidence**: The core empirical findings regarding classification accuracy (97.2% after fine-tuning) are well-supported by the experimental results and align with established transfer learning principles. The methodology for dataset creation, while detailed, has some implementation uncertainties that prevent complete replication.

**Medium Confidence**: The justification generation quality claims are supported by strong BLEU and ROUGE scores, but these metrics alone cannot fully capture the semantic adequacy and factual consistency required for truly explainable AI. The cascading error analysis is theoretically sound but would benefit from more extensive empirical validation.

**Low Confidence**: The broader claims about generalizability to other low-resource languages and the superiority of the two-stage hierarchical approach over alternative architectures lack comparative evidence from other language contexts.

## Next Checks
1. **Dataset accessibility validation**: Verify the public availability of the SentiMaithili dataset and confirm the exact train/test split specifications to enable faithful reproduction of the reported results.

2. **Metric adequacy assessment**: Conduct human evaluation studies comparing machine-generated justifications against human-written ones using criteria such as semantic coherence, factual consistency, and relevance to predicted sentiment to complement automated metric scores.

3. **Error propagation quantification**: Systematically measure the impact of classification errors on justification quality by training and evaluating the generator using gold-standard labels versus predicted labels, and analyze the correlation between classifier confidence scores and justification quality.