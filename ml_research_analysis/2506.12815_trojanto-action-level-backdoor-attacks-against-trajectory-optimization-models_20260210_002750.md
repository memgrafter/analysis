---
ver: rpa2
title: 'TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models'
arxiv_id: '2506.12815'
source_url: https://arxiv.org/abs/2506.12815
tags:
- trigger
- backdoor
- trojanto
- uni00000013
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrojanTO, the first action-level backdoor
  attack framework designed for trajectory optimization (TO) models in offline reinforcement
  learning. Existing backdoor attacks in reinforcement learning rely on reward manipulation,
  which is ineffective for TO models due to their sequence modeling nature and focus
  on minimizing reconstruction loss.
---

# TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models

## Quick Facts
- arXiv ID: 2506.12815
- Source URL: https://arxiv.org/abs/2506.12815
- Authors: Yang Dai; Oubo Ma; Longfei Zhang; Xingxing Liang; Xiaochun Cao; Shouling Ji; Jiaheng Zhang; Jincai Huang; Li Shen
- Reference count: 40
- Primary result: Introduces first action-level backdoor attack framework for trajectory optimization models, achieving 71.9% attack success rate with 91.4% benign task performance using only 0.3% attack budget

## Executive Summary
This paper introduces TrojanTO, the first action-level backdoor attack framework designed for trajectory optimization (TO) models in offline reinforcement learning. Existing backdoor attacks in reinforcement learning rely on reward manipulation, which is ineffective for TO models due to their sequence modeling nature and focus on minimizing reconstruction loss. TrojanTO addresses this gap by employing alternating training to strengthen the connection between triggers and target actions, while using trajectory filtering and batch poisoning to maintain attack stealth and effectiveness. Extensive evaluations across six D4RL environments and three TO model architectures demonstrate that TrojanTO achieves an average attack success rate of 71.9% and benign task performance of 91.4% with a very low attack budget of 0.3% of trajectories, significantly outperforming existing baselines.

## Method Summary
TrojanTO operates by alternating between two training phases: connection enhancement and balance preservation. During connection enhancement, the framework strengthens the relationship between triggers and target actions using adversarial learning techniques. The balance preservation phase maintains the model's benign performance through trajectory filtering and batch poisoning strategies. The attack uses a minimal poisoning budget (0.3% of trajectories) and demonstrates effectiveness across multiple TO model architectures. The framework's stealthiness is achieved through careful control of trigger injection and strategic selection of poisoned trajectories that blend naturally with clean data.

## Key Results
- Achieves average attack success rate of 71.9% across six D4RL environments
- Maintains benign task performance at 91.4% while under attack
- Operates with minimal attack budget of only 0.3% of training trajectories
- Outperforms existing baselines by significant margins across three different TO model architectures

## Why This Works (Mechanism)
TrojanTO exploits the fundamental difference between TO models and traditional RL models: TO models focus on minimizing reconstruction loss rather than optimizing reward functions. This makes traditional reward-based backdoor attacks ineffective. By using alternating training that specifically strengthens the trigger-action connection while preserving overall model performance through trajectory filtering, TrojanTO creates a stealthy backdoor that activates only when the trigger is present. The framework's effectiveness stems from its ability to manipulate the model's understanding of action sequences while maintaining the overall optimization objectives.

## Foundational Learning
- Trajectory Optimization Models: These models learn to reconstruct optimal trajectories by minimizing reconstruction loss, making them vulnerable to sequence-level manipulations rather than reward-based attacks. Understanding TO models is crucial because they form the target of the attack.
- Alternating Training: The technique involves switching between different training objectives (connection enhancement vs. balance preservation) to achieve both attack effectiveness and stealth. This approach is needed to maintain benign performance while embedding the backdoor.
- Batch Poisoning: A data poisoning technique where malicious samples are strategically inserted into training batches. Quick check: Verify that poisoned batches maintain statistical properties similar to clean data to ensure stealth.

## Architecture Onboarding
Component Map: Data Pipeline -> Trajectory Filtering -> Alternating Training -> Model Update -> Evaluation
Critical Path: Clean data → Filtering → Connection Enhancement → Balance Preservation → Attack Success
Design Tradeoffs: Attack strength vs. stealth (higher connection strength may reduce benign performance)
Failure Signatures: Sudden drops in benign performance, detectable patterns in trigger-action associations
First Experiments: 1) Test alternating training effectiveness on synthetic data, 2) Measure benign performance degradation with varying poisoning ratios, 3) Evaluate trigger detection using statistical analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes knowledge of trigger-action pairs during training, which may not be realistic in all deployment scenarios
- Focuses on static triggers without exploring dynamic or adaptive trigger mechanisms
- Does not extensively explore the trade-offs between attack success rate and task performance across different budget levels

## Confidence
- Technical soundness: High
- Experimental methodology: High
- Stealth claims: Medium
- Generalizability: Medium

## Next Checks
1. Evaluate TrojanTO's effectiveness against TO models with varying reconstruction loss functions to assess robustness to architectural differences
2. Test the attack's performance when triggers are dynamically generated or unknown during training to evaluate real-world applicability
3. Conduct adversarial detection analysis to measure how easily TrojanTO attacks can be identified using common backdoor detection techniques