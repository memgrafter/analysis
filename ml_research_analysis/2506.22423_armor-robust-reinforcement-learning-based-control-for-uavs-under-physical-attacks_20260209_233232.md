---
ver: rpa2
title: 'ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical
  Attacks'
arxiv_id: '2506.22423'
source_url: https://arxiv.org/abs/2506.22423
tags:
- attacks
- armor
- control
- state
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of controlling UAVs under physical
  sensor attacks (e.g., GPS spoofing, gyroscope tampering) using reinforcement learning.
  Existing safe RL methods are ineffective against such attacks, which corrupt state
  estimates and lead to unsafe behavior.
---

# ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks

## Quick Facts
- **arXiv ID:** 2506.22423
- **Source URL:** https://arxiv.org/abs/2506.22423
- **Reference count:** 40
- **Primary result:** ARMOR achieves 88% mission success rate vs 82% for RARL under physical attacks on UAVs

## Executive Summary
This paper addresses the critical challenge of controlling UAVs under physical sensor attacks such as GPS spoofing and gyroscope tampering. Traditional safe reinforcement learning methods fail when attacks corrupt state estimates, leading to unsafe behavior. The ARMOR framework introduces a novel two-stage training approach that learns robust latent state representations. By first training a teacher encoder with privileged attack information and then training a student encoder to approximate these states using only historical sensor data, ARMOR enables deployment without requiring attack knowledge at runtime.

The experimental results demonstrate that ARMOR significantly outperforms baseline RL methods and competing approaches like HRP and RARL across multiple attack scenarios. The method shows strong zero-shot generalization to unseen attacks while maintaining high mission success rates and reducing state drift compared to alternatives. The approach effectively addresses the limitations of existing methods that either require attack knowledge during deployment or fail to adequately handle the complex dynamics introduced by physical sensor corruption.

## Method Summary
ARMOR employs a two-stage training framework to develop a robust control policy for UAVs under physical attacks. In the first stage, a teacher encoder is trained using privileged information about the attacks, learning to generate attack-aware latent state representations. This encoder has access to both the true system state and attack parameters, allowing it to understand how attacks corrupt sensor readings. In the second stage, a student encoder learns to approximate these attack-aware latent states using only historical sensor data and no privileged information. This two-stage approach enables the final policy to operate effectively under attacks without requiring knowledge of attack parameters during deployment, addressing a key limitation of existing methods.

## Key Results
- ARMOR achieves 88% mission success rate compared to 82% for RARL under physical attacks
- ARMOR demonstrates superior crash prevention and state drift reduction compared to baseline methods
- The method shows strong zero-shot generalization to unseen attack types not encountered during training

## Why This Works (Mechanism)
ARMOR works by learning a robust latent state representation that captures the effects of attacks without requiring attack information at deployment time. The two-stage training process allows the system to first learn what attack-aware states should look like (with privileged information) and then learn how to approximate these states from corrupted sensor data alone. This separation enables the policy to maintain performance under attacks by focusing on the underlying system state rather than the corrupted sensor readings.

## Foundational Learning
- **Reinforcement Learning**: Needed to learn control policies from interaction with the environment; quick check: policy improves over training episodes
- **Latent State Representations**: Needed to work with compressed, robust features instead of raw sensor data; quick check: encoder maps high-dimensional observations to lower-dimensional latent space
- **Adversarial Training**: Needed to handle worst-case scenarios under attacks; quick check: policy maintains performance when sensor readings are corrupted
- **Teacher-Student Training Framework**: Needed to transfer knowledge from privileged to non-privileged setting; quick check: student can approximate teacher's latent states using only historical data

## Architecture Onboarding

**Component Map:** Raw sensors -> Student Encoder -> Latent State -> Policy Network -> Action -> UAV

**Critical Path:** Sensor inputs flow through the student encoder to produce latent states, which the policy network uses to generate control actions for the UAV.

**Design Tradeoffs:** The two-stage training approach trades computational complexity during training for robustness during deployment. While requiring privileged information in stage 1, the final deployed system operates without this requirement, making it practical for real-world use.

**Failure Signatures:** Performance degradation occurs when attacks introduce patterns not captured in the historical data used for student encoder training. Complete sensor failure or novel attack types that significantly deviate from training distributions may cause the latent state approximation to fail.

**3 First Experiments:**
1. Evaluate ARMOR's performance on a simple quadrotor navigation task with simulated GPS spoofing
2. Compare ARMOR against a baseline RL policy without attack robustness on the same navigation task
3. Test the student encoder's ability to approximate teacher latent states under varying attack intensities

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on privileged attack information during the teacher encoder training stage, limiting applicability in scenarios where attack models are unknown
- Experimental validation is limited to specific attack models, raising questions about generalizability to completely novel attack patterns
- Long-term stability under continuous attack conditions lacks detailed analysis, with potential for performance degradation over extended operation periods

## Confidence
- Mission success rate comparisons: Medium confidence
- Zero-shot generalization claims: Medium confidence
- State drift reduction: Medium confidence

## Next Checks
1. Conduct statistical significance testing on performance differences between ARMOR and baseline methods across multiple random seeds and trials
2. Test ARMOR's performance under completely novel attack types not present in the training distribution, including attacks that combine multiple sensor modalities
3. Evaluate long-term stability and performance degradation when the UAV operates continuously under persistent attack conditions for extended periods (e.g., 30+ minutes of continuous operation)