---
ver: rpa2
title: 'PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual
  Training'
arxiv_id: '2503.06486'
source_url: https://arxiv.org/abs/2503.06486
tags:
- object
- list
- node
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in multimodal large language
  models (MLLMs) for dense image captioning. The authors introduce HalFscore, a concept-level
  metric for evaluating accuracy and completeness of dense captions, and propose PerturboLLaVA,
  a training method that reduces the model's reliance on language priors by incorporating
  adversarially perturbed text during training.
---

# PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training

## Quick Facts
- **arXiv ID:** 2503.06486
- **Source URL:** https://arxiv.org/abs/2503.06486
- **Reference count:** 40
- **Primary result:** PerturboLLaVA reduces hallucinations in MLLMs for dense image captioning, achieving a HalFscore of 52.2 (6.2 pt precision, 2.4 pt Fscore improvement).

## Executive Summary
This paper addresses the problem of hallucinations in multimodal large language models (MLLMs) for dense image captioning. The authors identify the root cause as the model's over-reliance on language priors rather than visual input. They propose PerturboLLaVA, a training-time intervention that reduces this bias by injecting adversarially perturbed text during supervised fine-tuning. The method is evaluated using a novel HalFscore metric that measures concept-level accuracy and completeness of captions through graph-based comparison.

## Method Summary
PerturboLLaVA modifies the standard supervised fine-tuning (SFT) process for MLLMs by prepending adversarial perturbation text to training queries. During SFT on the VQA subset of the dataset, GPT-4o generates text that is logically consistent with general knowledge but contradictory to the visual evidence. This forces the model to suppress the textual bias and attend more closely to the image content. The training uses standard LLaVA 1.5 architecture (Vicuna + CLIP ViT-L) with minimal overhead (2.6% memory, +6.4% training time).

## Key Results
- PerturboLLaVA achieves HalFscore of 52.2, outperforming baselines with 6.2 pt precision and 2.4 pt Fscore improvements
- Method shows strong generalization to general multimodal benchmarks (MMBench, SEED)
- Ablation studies confirm targeted adversarial perturbations are more effective than random text (Table 5)

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Perturbation of Language Priors
The method forces models to shift from language priors to visual input by injecting adversarial text that contradicts visual evidence. During SFT, perturbation text prepended to queries compels the model to suppress convincing textual bias and attend to image content. Core assumption: models have sufficient capacity to distinguish textual world knowledge from visual instance evidence. Break condition: weak or random perturbations cause models to revert to hallucination patterns.

### Mechanism 2: Mathematical Re-weighting of Conditional Distributions
The training strategy approximates visual-only probability distributions by optimizing against perturbed language-only distributions. The method guides the model to optimize towards p(x_k | x^{-p}_{<k}, I) by treating perturbation as a nuisance variable. Core assumption: conditional independence of perturbation and visual features given the output. Break condition: if perturbation becomes indistinguishable from visual content, training reverts to standard patterns.

### Mechanism 3: Concept-Level Graph Grounding (HalFscore)
The HalFscore metric exposes hallucinations missed by surface-level metrics by parsing captions into Subject-Object-Relation triplets and forming concept graphs. This enables fine-grained penalties on specific semantic errors rather than just object existence. Core assumption: GPT-4o can reliably extract and match semantic triplets without introducing errors. Break condition: fails if graph construction produces inconsistent triplets or ground truth is ambiguous.

## Foundational Learning

**Language Prior in MLLMs:** Understanding the "bug" is essential before the fix. MLLMs favor text completion patterns over visual grounding. Quick check: If shown a blue apple, why might an MLLM say "red"?

**Bayes' Theorem in Generative Modeling:** Section 4.2 uses Bayes' rule to decouple visual signal from language bias. Quick check: In P(A|B) ∝ P(B|A)P(A), which term represents the "language prior" bias to suppress?

**Knowledge Graph Triplets:** HalFscore relies on converting unstructured text into structured Entity-Relation-Entity graphs. Quick check: How would you represent "The cat sits on the mat" as a triplet?

## Architecture Onboarding

**Component map:** Image (I) + Question (x_q) → GPT-4o (generates Perturbation Text x_p) → LLaVA-1.5 (Vision Encoder + Projector + LLM) → HalFscore Engine (Triplet Extractor + Graph Matcher)

**Critical path:**
1. Data Prep: Use GPT-4o with prompts in Appendix A.8 to generate adversarial text for 160k VQA data
2. Perturbation Injection: Prepend x_p to x_q with specific hint prompts (Version 1/2/3)
3. Training: Standard SFT on perturbed dataset (2.6% memory overhead)
4. Inference: Standard inference (no perturbation text needed)

**Design tradeoffs:** Version 3 (no warning prompts) reduces hallucinations most but causes highest drop in Recall/caption length. Low training overhead (+6.4% time) vs. decoding-time methods (VCD/OPERA) which double inference latency.

**Failure signatures:** Excessive perturbation causes short/over-cautious outputs (High Precision, Low Recall). Random perturbations fail to correct hallucinations because they don't target specific language priors.

**First 3 experiments:**
1. Baseline Reproduction: Train LLaVA-1.5 on standard 665k SFT dataset to establish HalFscore baseline
2. Perturbation Validation: Train with "Version 1" perturbations and compare HalFscore Precision vs. Recall tradeoff
3. Ablation on Relevance: Compare targeted adversarial perturbations vs. random text perturbations to validate semantic content matters (Table 5)

## Open Questions the Paper Calls Out

**Open Question 1:** Can designing perturbation texts specifically targeted at objects improve object-level hallucination metrics (like CHAIR) to outperform feedback-based methods like RLAIF-V? The current method uses general context perturbations which reduce hallucinations overall but haven't been optimized specifically for object hallucinations.

**Open Question 2:** How can the trade-off between perturbation strength (reducing hallucinations) and caption completeness (recall) be optimized to prevent overly cautious model behavior? The paper identifies that higher perturbation levels reduce hallucinations but also reduce caption richness, but doesn't propose mechanisms to balance these competing objectives.

**Open Question 3:** Is PerturboLLaVA's effectiveness dependent on GPT-4o's capability to generate high-quality perturbations, or can similar results be achieved with weaker models or rule-based templates? The method relies on GPT-4o to "construct strong and diverse perturbations," but it's unclear if this requires a frontier model or if robustness effects can be replicated with lower-cost synthetic data.

## Limitations

- Limited empirical validation of the theoretical mechanism - the assumption that models "attend to vision" rather than simply ignore prepended text is not directly tested
- Dataset and evaluation scope constraints - method evaluated on 1,000 DCI images and 160k VQA subset, which may not represent full real-world diversity
- Architecture specificity concerns - demonstrated only on LLaVA-1.5, with no testing on whether gains transfer to other MLLM architectures

## Confidence

**High confidence:** Empirical results showing improved HalFscore and CHAIR scores on tested dataset; convincing and reproducible ablation study demonstrating targeted perturbations outperform random text.

**Medium confidence:** Theoretical justification via Bayes' theorem and claims about approximating visual-only probability distributions; math is sound but practical interpretation relies on unstated assumptions.

**Low confidence:** Claim that this is a "fundamental solution" to language prior bias rather than a training-time regularization technique; method may simply teach models to ignore prepended text with unintended consequences in other tasks.

## Next Checks

1. **Human evaluation study:** Commission human annotators to independently verify HalFscore precision/recall calculations on a subset of DCI dataset, comparing against GPT-4o's triplet extraction to assess potential systematic errors.

2. **Cross-architecture replication:** Apply same perturbation training protocol to different MLLM architecture (e.g., Qwen-VL or BLIP-2) using identical perturbation prompts and training hyperparameters to test transfer beyond LLaVA.

3. **Perturbation ablation with attention visualization:** During inference on perturbed training data, extract and visualize attention weights on image tokens vs. prepended perturbation text to directly observe whether model is "attending to vision" or simply learning to ignore prepended text.