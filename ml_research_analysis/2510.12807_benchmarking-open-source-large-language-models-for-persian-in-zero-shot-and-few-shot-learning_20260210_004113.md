---
ver: rpa2
title: Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and
  Few-Shot Learning
arxiv_id: '2510.12807'
source_url: https://arxiv.org/abs/2510.12807
tags:
- persian
- language
- performance
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of open-source large
  language models (LLMs) for Persian natural language processing tasks, evaluating
  both zero-shot and few-shot learning paradigms across sentiment analysis, named
  entity recognition, reading comprehension, and question answering. The study assesses
  11 prominent open-source models including Gemma2, GLM4, Qwen variants, Llama models,
  and others using established Persian datasets such as ParsiNLU and ArmanEmo.
---

# Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning

## Quick Facts
- **arXiv ID:** 2510.12807
- **Source URL:** https://arxiv.org/abs/2510.12807
- **Reference count:** 33
- **Primary result:** Gemma2 consistently outperforms other models on Persian NLP tasks, with few-shot learning improving performance by 13.8% on average.

## Executive Summary
This paper benchmarks 11 open-source large language models for Persian NLP tasks using both zero-shot and few-shot learning paradigms. The evaluation covers sentiment analysis, named entity recognition, reading comprehension, and question answering using established Persian datasets. Gemma2 emerges as the top-performing model across most tasks, while few-shot prompting shows significant benefits for semantic reasoning tasks. However, models struggle with token-level understanding tasks like NER, suggesting specific challenges in Persian language processing.

## Method Summary
The study evaluates 11 open-source LLMs including Gemma2, GLM4, Qwen variants, Llama models, and others on Persian NLP tasks. The benchmark uses standardized prompt templates for both zero-shot (natural language instructions) and few-shot (5 examples) learning. Models are tested on datasets including ParsiNLU (reading comprehension, entailment, sentiment, machine translation), ArmanEmo (sentiment), ArmanNER (named entity recognition), Persian MMLU (question answering), and XLSummary (summarization). Inference is performed using HuggingFace Transformers with FP16 precision on NVIDIA A100 GPUs, with evaluation metrics including accuracy, F1, BLEU, ROUGE, and Exact Match.

## Key Results
- Gemma2 consistently outperforms other models across nearly all tasks in both learning paradigms
- Few-shot learning yields 13.8% average performance improvement compared to zero-shot approaches
- Most models struggle with token-level understanding tasks like Named Entity Recognition (NER F1 near zero in zero-shot)
- Complex reasoning tasks show stronger gains from few-shot prompting than token-level prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves Persian NLP performance across most tasks, with stronger gains for semantic reasoning than token-level prediction.
- Mechanism: In-context examples provide task framing and output format signals, enabling models to adapt internal attention patterns without weight updates. The paper reports 17.3% average improvement for comprehension tasks and 13.8% overall gains (p < 0.01).
- Core assumption: Models have sufficient Persian representations in pretraining for examples to activate relevant patterns.
- Evidence anchors:
  - [abstract] "highlights the effectiveness of few-shot learning for Persian tasks"
  - [section 6.1] "Few-shot learning demonstrated statistically significant benefits (p < 0.01) across most tasks and models, with an average performance improvement of 13.8%"
  - [corpus] Cross-lingual few-shot learning paper confirms transfer works even for unseen languages with appropriate prompting
- Break condition: If pretraining data contains insufficient Persian tokens, few-shot examples may fail to activate relevant knowledge.

### Mechanism 2
- Claim: Architectural and pretraining choices drive cross-lingual transfer quality more than model scale alone.
- Mechanism: Gemma2's consistent lead (0.61 few-shot, 0.42 zero-shot averages) across 11 models suggests multilingual representation quality depends on data curation and training objectives, not just parameter count. GLM4 and Qwen2.5 followed but with notable gaps.
- Core assumption: Performance differences stem from pretraining rather than evaluation methodology.
- Evidence anchors:
  - [abstract] "Gemma 2 consistently outperforms other models across nearly all tasks in both learning paradigms"
  - [section 5.1] "suggests architectural advantages and potentially superior multilingual pretraining strategies employed in Gemma2's development"
  - [corpus] Weak direct evidence—neighbor papers focus on benchmarks, not architectural comparisons
- Break condition: If evaluation prompts inadvertently favor certain tokenizers or instruction formats, architectural conclusions become confounded.

### Mechanism 3
- Claim: Token-level sequence labeling (NER) resists few-shot improvement due to prompt-output structure mismatch.
- Mechanism: NER requires per-token classifications with precise boundary detection, but prompt-based generation produces sequence outputs that struggle with fine-grained token alignment. Paper reports only 7.2% NER improvement from few-shot vs. larger gains elsewhere.
- Core assumption: The prompt format, not model capability, is the limiting factor.
- Evidence anchors:
  - [abstract] "Most models struggled with token-level tasks like NER"
  - [section 5.2] "NER tasks presented persistent challenges, with relatively modest improvements from few-shot prompting (average gain of 7.2%)"
  - [corpus] No direct architectural solutions found in neighbor papers
- Break condition: Specialized prompting schemes (e.g., structured output formats, pointer networks) may partially recover performance.

## Foundational Learning

- Concept: Zero-shot vs. Few-shot Learning
  - Why needed here: The entire benchmark depends on understanding these evaluation paradigms—zero-shot tests inherent capability; few-shot tests in-context adaptation.
  - Quick check question: If a model scores 0.42 zero-shot and 0.61 few-shot on the same task, what does this 45% relative improvement suggest about its pretraining?

- Concept: Cross-lingual Transfer
  - Why needed here: All evaluated models were trained primarily on English-dominant corpora; their Persian performance depends on transfer quality.
  - Quick check question: Why might a model transfer well for entailment but poorly for NER in a target language?

- Concept: Token-level vs. Sequence-level Tasks
  - Why needed here: Error analysis shows token-level tasks (NER) are systematically harder for LLMs; understanding this distinction guides task selection.
  - Quick check question: What structural property of NER makes it harder to improve via prompt engineering compared to sentiment analysis?

## Architecture Onboarding

- Component map: Models (11 open-source LLMs: Gemma2, GLM4, Llama3.1/3.2, Qwen2/2.5, Mistral, Marco-o1, Aya-Expanse, Falcon3, Tulu3) -> Datasets (ParsiNLU, ArmanEmo, ArmanNER, Persian MMLU, Persian News Summary, XLSummary) -> Metrics (Accuracy, F1, BLEU, ROUGE, Exact Match) -> Infrastructure (A100 GPUs, HuggingFace Transformers, FP16 inference)

- Critical path: 1. Dataset selection and preprocessing -> 2. Prompt template design (standardized across models) -> 3. Inference with fixed hyperparameters (temp=0.1, beam=4 for generation) -> 4. Metric computation per task -> 5. Cross-model ranking and statistical testing

- Design tradeoffs:
  - Standardized prompts vs. model-specific optimization (chose standardization for fairness)
  - 5-shot examples vs. scaling shot count (chose 5 for consistency)
  - Single-run vs. multi-seed evaluation (paper doesn't specify seeds—uncertainty remains)
  - Greedy vs. sampling for classification (chose greedy for determinism)

- Failure signatures:
  - NER F1 near zero (aya-expanse: 0.01 zero-shot, 0.13 few-shot) indicates prompt-output alignment failure
  - Sentiment analysis confusion on mixed-emotion text (+31.2% error rate vs. simple cases)
  - Entity boundary errors 27.8% higher for multi-token entities
  - Idiomatic expression errors 34.5% higher than literal expressions

- First 3 experiments:
  1. Replicate Gemma2 zero-shot vs. 5-shot on ParsiNLU-RC and ArmanEmo to validate reported 12-17% gains; log per-example errors.
  2. Test NER prompt variants (BIO-tagged output vs. JSON entity list) on Gemma2 to isolate prompt-structure effects.
  3. Run Gemma2 vs. GLM4 on Persian MMLU subcategories to verify the 8%+ gap in humanities vs. STEM domains.

## Open Questions the Paper Calls Out

- Question: To what extent can parameter-efficient fine-tuning (PEFT) methods like LoRA close the performance gap between general-purpose multilingual models and Persian-specialized models?
  - Basis in paper: [explicit] The authors explicitly call for "investigating efficient parameter-efficient fine-tuning approaches (e.g., LoRA, P-tuning) specifically for Persian" to find optimal adaptation strategies.
  - Why unresolved: This study strictly evaluated frozen model weights using in-context learning (zero-shot and few-shot) and did not assess performance after weight updates.
  - What evidence would resolve it: Comparative benchmarks showing performance deltas on Persian NER and reasoning tasks before and after applying LoRA or similar adapters to the evaluated base models.

- Question: Does the application of Chain-of-Thought (CoT) prompting significantly improve performance on complex Persian reasoning tasks compared to standard few-shot prompting?
  - Basis in paper: [explicit] The authors suggest that "exploring dynamic prompt tuning, chain-of-thought reasoning... specifically optimized for Persian could substantially improve performance."
  - Why unresolved: The experimental setup relied on standardized prompts without reasoning exemplars, leaving the potential for multi-step reasoning prompts unexplored.
  - What evidence would resolve it: Ablation studies comparing standard few-shot scores against CoT-prompted scores on the ParsiNLU and Persian MMLU reasoning subsets.

- Question: What specific morphosyntactic features of Persian contribute most to the failure of LLMs on token-level tasks like Named Entity Recognition (NER)?
  - Basis in paper: [inferred] The authors note that models struggle significantly with NER and suggest "conducting fine-grained error analysis focused on specific morphosyntactic phenomena" to identify architectural limitations.
  - Why unresolved: The paper reports aggregate F1-scores but does not provide a linguistic breakdown of errors (e.g., distinguishing failures caused by zero-width non-joiners vs. complex morphology).
  - What evidence would resolve it: A qualitative error analysis correlating low NER scores with specific linguistic features, such as the handling of compound entities or specific tokenization artifacts.

## Limitations

- The study only evaluates frozen model weights using in-context learning, without exploring fine-tuning approaches that could potentially improve performance
- The paper uses standardized prompts across all models rather than optimizing prompts for each model's specific architecture or training
- No multi-seed evaluation was conducted, and specific few-shot examples used are not detailed, limiting reproducibility and statistical confidence

## Confidence

- **High**: Gemma2's consistent performance lead across multiple tasks and learning paradigms
- **Medium**: Few-shot learning improvements, as statistical significance is reported but specific prompt variations weren't tested
- **Low**: Conclusions about architectural advantages driving performance, as the study didn't systematically vary model architectures or pretraining data

## Next Checks

1. Replicate Gemma2 zero-shot vs. 5-shot on ParsiNLU-RC and ArmanEmo to validate reported 12-17% gains; log per-example errors.
2. Test NER prompt variants (BIO-tagged output vs. JSON entity list) on Gemma2 to isolate prompt-structure effects.
3. Run Gemma2 vs. GLM4 on Persian MMLU subcategories to verify the 8%+ gap in humanities vs. STEM domains.