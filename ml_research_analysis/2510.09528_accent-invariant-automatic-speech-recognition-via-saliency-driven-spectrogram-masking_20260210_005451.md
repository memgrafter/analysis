---
ver: rpa2
title: Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram
  Masking
arxiv_id: '2510.09528'
source_url: https://arxiv.org/abs/2510.09528
tags:
- speech
- accent
- persian
- recognition
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of accent and dialect sensitivity
  in automatic speech recognition (ASR) systems, which leads to elevated word error
  rates (WER) for non-native and regional accents. The proposed method uses saliency-driven
  spectrogram masking, where an accent classifier is trained to identify accent-specific
  features, Grad-CAM is used to localize these regions, and a probabilistic masking
  strategy suppresses them during training.
---

# Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking

## Quick Facts
- arXiv ID: 2510.09528
- Source URL: https://arxiv.org/abs/2510.09528
- Reference count: 0
- The paper proposes a method that achieves up to 14% relative WER reduction on accented speech using saliency-driven spectrogram masking

## Executive Summary
This paper addresses accent and dialect sensitivity in automatic speech recognition (ASR) systems by proposing a saliency-driven spectrogram masking approach. The method trains an accent classifier to identify accent-specific spectral regions, uses Grad-CAM to localize these regions, and applies probabilistic masking to suppress them during training. This forces ASR models to learn accent-neutral representations that improve robustness across diverse accents. Experiments with Whisper on English and Persian datasets show significant WER reductions compared to SpecAugment baselines.

## Method Summary
The approach involves training a spectrogram-based accent classifier (CNN with 4 conv layers: 32→64→128→256 filters) to capture accent-specific cues. Grad-CAM is then used to identify the most influential regions for accent classification. These regions are suppressed using a tiered probabilistic masking strategy: strongly accent-related regions (C≥0.7) are always masked, moderate regions (0.5≤C<0.7) are masked with 70-90% probability, and low-saliency regions (C<0.5) are masked with 0-5% probability. The masked spectrograms are used as augmented training data for fine-tuning Whisper models, enabling them to focus on accent-neutral linguistic features while maintaining recognition accuracy.

## Key Results
- Whisper models fine-tuned with saliency-driven masking show up to 14% relative WER improvement on accented English speech
- Significant improvements observed across multiple English datasets (LibriSpeech, EdAcc, CommonAccent) and Persian datasets (CommonVoice-fa, PDID)
- The method achieves robust performance on both seen and unseen accents without requiring architectural modifications
- PDID dataset introduced as the first large-scale Persian accent corpus with 10 distinct accents

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Localization of Accent-Specific Spectral Regions
A CNN accent classifier learns hierarchical acoustic features distinguishing accents. Grad-CAM computes gradients of the predicted class with respect to final convolutional feature maps to produce heatmaps highlighting accent-salient regions. These regions contain prosodic and phonetic markers (formant shifts, duration patterns) that differ across accents. The method assumes Grad-CAM activations correlate with linguistically meaningful accent features rather than spurious correlations.

### Mechanism 2: Forced Learning of Accent-Invariant Linguistic Representations
By augmenting training data with masked spectrograms where accent-discriminative regions are suppressed, the ASR model cannot rely on accent-specific shortcuts and must develop representations that cluster same-word utterances across accents. This functions as an implicit domain-invariant learning objective without explicit adversarial optimization.

### Mechanism 3: Probabilistic Soft Masking Preserves Information While Reducing Accent Reliance
The tiered probabilistic masking strategy balances accent suppression with content preservation. Rather than binary masking, the method applies probability-based masking scaled to Grad-CAM saliency: high-saliency regions (>0.7) are always masked; moderate regions (0.5-0.7) are masked with 70-90% probability; low-saliency regions (<0.5) are masked with 0-5% probability. This stochastic approach acts as a regularizer, preventing deterministic artifacts and allowing some accent information to leak through.

## Foundational Learning

- **Spectrogram Representations of Speech**: The entire method operates on spectrograms; understanding time-frequency representations is essential to interpret Grad-CAM heatmaps and masking effects. Quick check: Can you explain why formant frequencies appear as horizontal bands in a spectrogram and how accent differences might alter their positions?

- **Grad-CAM and Saliency Maps**: This is the core localization mechanism; without understanding gradient-based attribution, you cannot debug why certain regions are being masked. Quick check: Given a classifier predicting "Irish accent," what does Grad-CAM highlight if the model primarily attends to pitch contours vs. vowel formant transitions?

- **Data Augmentation for Domain Robustness**: The masked spectrograms function as augmented training data; understanding how augmentation improves generalization is necessary to evaluate whether this approach is principled. Quick check: How does saliency-guided masking differ from random masking (SpecAugment) in terms of what information is removed and what inductive bias is introduced?

## Architecture Onboarding

- **Component map**: Accent Classifier (CNN) -> Grad-CAM Module -> Probabilistic Masking Module -> ASR Fine-tuning Pipeline
- **Critical path**: Collect/create accent-labeled dataset → Train accent classifier → Generate Grad-CAM maps → Apply probabilistic masking → Fine-tune Whisper on combined original + augmented data
- **Design tradeoffs**: Classifier accuracy vs. localization quality; masking aggressiveness vs. content preservation; compute overhead of Grad-CAM preprocessing; model-agnostic approach vs. specialized integration
- **Failure signatures**: WER increases on standard/low-accent speech; no improvement on unseen accents; high variance across runs; classifier Grad-CAM highlights uniform/sparse regions
- **First 3 experiments**: (1) Reproduce English baseline and verify Grad-CAM highlights plausible accent features; (2) Ablate masking thresholds to determine sensitivity; (3) Compare probabilistic vs. binary masking on one accent pair

## Open Questions the Paper Calls Out

1. **Do Grad-CAM saliency maps genuinely capture linguistically meaningful accent-specific features?** The method assumes Grad-CAM correctly localizes accent information, but no linguistic validation is provided. The English classifier achieves only 74.6% accuracy, suggesting imperfect accent feature learning.

2. **How sensitive is the masking strategy to specific threshold values (0.3, 0.5, 0.7) and probability ranges?** The threshold and probability parameters appear heuristically chosen without systematic ablation or justification.

3. **Does masking accent-related spectrogram regions inadvertently remove linguistically critical content?** While WER decreases, the paper does not analyze whether certain phoneme classes or word categories are harmed by masking.

4. **How effectively does the approach transfer to architectures beyond Whisper?** All experiments use Whisper models; no validation on other architectures is provided despite claims of model-agnosticism.

## Limitations
- No ablation studies on classifier architecture or quality, which could affect Grad-CAM localization accuracy
- Fixed threshold values (0.3, 0.5, 0.7) without systematic sensitivity analysis or justification
- Binary masking condition contains a likely typo (R(i,j)>1 impossible when R∈[0,1])
- No comparison against alternative accent adaptation methods like adversarial training

## Confidence

**High confidence**: The method is implementable and reproducible as described, with clear mathematical formulation and code availability.

**Medium confidence**: Empirical results showing WER improvements are likely reproducible, though exact magnitude may vary based on implementation details and random seeds.

**Low confidence**: The claim that learned representations are truly "accent-invariant" rather than simply accent-suppressed. The method may be removing accent information without ensuring the remaining features form coherent accent-neutral clusters.

## Next Checks

1. **Grad-CAM Quality Validation**: For 20 randomly selected samples across all accent classes, manually inspect Grad-CAM heatmaps to verify they highlight linguistically plausible accent features rather than recording artifacts or speaker-specific characteristics.

2. **Threshold Sensitivity Analysis**: Systematically vary the three masking thresholds and probability ranges across three settings: aggressive (-0.1 to all thresholds), conservative (+0.1 to all thresholds), and random assignment. Report WER on accented test sets to identify whether the paper's values are optimal or overfitted.

3. **Unseen Accent Transfer**: Test the fine-tuned model on an accent not present in training data (e.g., Scottish English if trained on American/British/Irish). Compare performance against baseline models to determine whether the method achieves true accent-invariance or merely memorizes training accent patterns.