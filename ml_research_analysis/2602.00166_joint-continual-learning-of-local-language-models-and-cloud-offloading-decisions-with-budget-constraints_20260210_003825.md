---
ver: rpa2
title: Joint Continual Learning of Local Language Models and Cloud Offloading Decisions
  with Budget Constraints
arxiv_id: '2602.00166'
source_url: https://arxiv.org/abs/2602.00166
tags:
- collaboration
- task
- cloud
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual learning for local
  small language models (SLMs) under strict memory and computation constraints, where
  naive reward-based reinforcement learning often yields unstable offloading behavior
  and exacerbates catastrophic forgetting. The authors propose DA-GRPO, a dual-advantage
  extension of Group Relative Policy Optimization that incorporates cloud-usage constraints
  directly into advantage computation, avoiding fixed reward shaping and external
  routing models.
---

# Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints

## Quick Facts
- arXiv ID: 2602.00166
- Source URL: https://arxiv.org/abs/2602.00166
- Reference count: 40
- Primary result: DA-GRPO improves post-switch accuracy and reduces forgetting in local SLMs with cloud offloading

## Executive Summary
This paper tackles the challenge of continual learning for local small language models (SLMs) when operating under strict memory and computation constraints. The key innovation is DA-GRPO, which extends Group Relative Policy Optimization by incorporating cloud-usage constraints directly into advantage computation. This allows the model to learn both task competence and collaborative behavior without fixed reward shaping or external routing models. Experiments demonstrate improved post-switch accuracy and substantially reduced catastrophic forgetting compared to prior collaborative and routing-based approaches.

## Method Summary
The authors propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that integrates cloud-usage constraints directly into the advantage computation framework. Unlike traditional approaches that use fixed reward shaping or separate routing models, DA-GRPO enables the local SLM to jointly learn task competence and collaboration behavior. The dual-advantage mechanism allows cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget, avoiding the instability issues associated with naive reward-based reinforcement learning approaches.

## Key Results
- Improved post-switch accuracy on mathematical reasoning and code generation benchmarks
- Substantial reduction in catastrophic forgetting compared to prior approaches
- Maintained stable cloud usage patterns while respecting assistance budget constraints

## Why This Works (Mechanism)
DA-GRPO works by embedding cloud-usage constraints directly into the advantage computation rather than relying on external reward shaping. This dual-advantage approach allows the model to naturally balance between local computation and cloud offloading during the learning process. By avoiding fixed reward structures, the system can adapt its collaborative behavior based on the current task demands and remaining budget, leading to more stable and efficient learning trajectories.

## Foundational Learning
- Catastrophic forgetting: Occurs when learning new tasks causes degradation of previously learned knowledge; critical to address in continual learning scenarios
- Group Relative Policy Optimization (GRPO): A policy optimization method that evaluates actions relative to a group; needed for stable learning in constrained environments
- Dual-advantage computation: Incorporates multiple optimization objectives simultaneously; required for balancing task performance and resource constraints
- Cloud offloading decisions: Strategic delegation of computation to external resources; essential for managing local resource limitations
- Budget constraints: Hard limits on resource usage that must be respected during deployment; fundamental to real-world applicability
- Quick check: Verify that the dual-advantage formulation properly balances task accuracy against cloud usage costs

## Architecture Onboarding

Component Map: Local SLM -> DA-GRPO Optimizer -> Cloud Offloading Module -> Budget Manager

Critical Path: Input Task -> Local Processing Decision -> Performance Evaluation -> Advantage Update -> Policy Adjustment -> Next Task

Design Tradeoffs: The paper trades off pure task performance for stability and budget compliance, choosing to integrate constraints directly into the learning objective rather than handling them post-hoc through reward shaping or external routing.

Failure Signatures: Unstable cloud usage patterns in early training phases, catastrophic forgetting when switching between tasks, and violation of budget constraints indicate implementation issues.

First Experiments:
1. Baseline comparison on mathematical reasoning tasks with fixed reward shaping
2. Ablation study removing the cloud-usage constraint from advantage computation
3. Stress test with rapidly alternating task types to evaluate forgetting resistance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to only two domains (mathematical reasoning and code generation), raising generalizability concerns
- Budget constraint handling described abstractly without practical guidance on budget determination or tuning
- Some volatility in early training phases suggests stability claims may be overstated for real-world deployment

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Well-established problem of catastrophic forgetting in collaborative setups | High |
| DA-GRPO algorithm and theoretical advantages | Medium |
| Reported improvements in post-switch accuracy and forgetting reduction | Medium |
| Claims about "stable" cloud usage patterns | Low |

## Next Checks
1. Evaluate DA-GRPO across at least 5-7 diverse task categories to assess generalizability beyond mathematical reasoning and code generation
2. Conduct ablation studies isolating the contribution of dual-advantage formulation versus other GRPO modifications
3. Test the framework under dynamic budget constraints that vary during deployment to assess real-world robustness