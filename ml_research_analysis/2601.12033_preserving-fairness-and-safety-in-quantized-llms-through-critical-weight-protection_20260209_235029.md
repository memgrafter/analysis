---
ver: rpa2
title: Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection
arxiv_id: '2601.12033'
source_url: https://arxiv.org/abs/2601.12033
tags:
- quantization
- safety
- fairness
- score
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how quantization affects fairness and safety
  in large language models (LLMs), finding that quantization generally degrades both
  fairness and safety, with dynamic quantization methods (FP8, LLM.int8) offering
  greater stability than static ones. To address these issues, the authors introduce
  Critical Weight Protection, a technique that identifies and preserves weights critical
  to fairness and safety in higher precision during quantization, while quantizing
  the rest for efficiency.
---

# Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection

## Quick Facts
- arXiv ID: 2601.12033
- Source URL: https://arxiv.org/abs/2601.12033
- Reference count: 40
- Primary result: Critical Weight Protection mitigates fairness and safety degradation in quantized LLMs while maintaining utility

## Executive Summary
This study addresses the problem of fairness and safety degradation in quantized large language models (LLMs). The authors demonstrate that quantization generally degrades both fairness and safety metrics, with dynamic quantization methods (FP8, LLM.int8) offering greater stability than static methods. To solve this issue, they propose Critical Weight Protection, a technique that identifies and preserves weights critical to fairness and safety in higher precision during quantization, while quantizing the rest for efficiency. Experiments show that this approach significantly mitigates fairness and safety degradation across multiple models and benchmarks, maintaining or improving performance compared to full-precision models.

## Method Summary
The authors introduce Critical Weight Protection as a solution to preserve fairness and safety in quantized LLMs. The method works by identifying weights that are most critical for fairness and safety during a fine-tuning process using full-precision models. These critical weights are then preserved in higher precision during quantization, while the remaining weights are quantized to reduce computational overhead. The approach is tested across multiple models including Gemma-7B, Llama-3.1-8B, and Qwen-2.5-7B using various quantization methods. The effectiveness is evaluated on fairness benchmarks (StereoSet, CrowS-Pairs, BBQ), safety benchmarks (AdvBench, XiaoBin), and general utility tasks (MMLU, BBH).

## Key Results
- Quantization degrades fairness and safety across multiple models and benchmarks
- Dynamic quantization methods (FP8, LLM.int8) show greater stability than static quantization
- Critical Weight Protection significantly mitigates fairness and safety degradation
- The method maintains or improves performance compared to full-precision models with minimal utility impact

## Why This Works (Mechanism)
The mechanism behind Critical Weight Protection is that certain weights in LLMs are more critical for maintaining fairness and safety behaviors than others. By identifying these weights during a full-precision fine-tuning phase and preserving them in higher precision during quantization, the method ensures that the model retains its ability to make fair and safe decisions. This targeted approach allows most weights to be aggressively quantized for efficiency while protecting the model's core fairness and safety capabilities.

## Foundational Learning
- Quantization methods (FP8, LLM.int8, static): Different approaches to reduce model precision; understanding these is crucial for evaluating the trade-offs between efficiency and performance
- Fairness metrics (StereoSet, CrowS-Pairs, BBQ): Benchmarks for measuring bias and fairness in language models; needed to quantify fairness degradation during quantization
- Safety evaluation (AdvBench, XiaoBin): Tools for assessing model safety and robustness against adversarial attacks; essential for measuring safety preservation
- Critical weight identification: The process of determining which weights are most important for specific behaviors; fundamental to the proposed protection mechanism

## Architecture Onboarding

Component map: Full-precision model -> Fine-tuning -> Critical weight identification -> Quantization with protection -> Evaluation

Critical path: Fine-tuning full-precision model -> Identifying critical weights -> Applying Critical Weight Protection during quantization -> Evaluating fairness, safety, and utility

Design tradeoffs: The method balances between computational efficiency (aggressive quantization of non-critical weights) and preserving model behavior (protecting critical weights). This creates a tradeoff between model size/speed and fairness/safety preservation.

Failure signatures: If critical weights are not properly identified, the model may still experience fairness and safety degradation despite protection. Over-protection of non-critical weights can lead to unnecessary computational overhead.

First experiments:
1. Compare fairness metrics between baseline quantized models and Critical Weight Protection models
2. Test safety robustness against multilingual adversarial attacks
3. Measure utility preservation on general tasks like MMLU and BBH

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on English benchmarks with limited multilingual coverage
- Effectiveness depends on the quality of full-precision models used for weight identification
- Computational overhead and practical implementation details are not fully explored
- Does not address interactions with other optimization techniques like pruning or distillation

## Confidence
- "Quantization degrades fairness and safety": High confidence, based on consistent empirical evidence across multiple models and benchmarks
- "Dynamic quantization is more stable than static": Medium confidence, as stability differences were observed but not extensively quantified across diverse scenarios
- "Critical Weight Protection effectively mitigates degradation": High confidence for fairness and safety, moderate confidence for utility preservation due to limited generalization testing

## Next Checks
1. Test Critical Weight Protection across a broader range of model families and sizes to assess generalizability
2. Evaluate the method's effectiveness on multilingual benchmarks beyond the single non-English test case
3. Assess computational overhead and integration complexity when combining Critical Weight Protection with other LLM optimization techniques