---
ver: rpa2
title: Do Large Language Models Understand Morality Across Cultures?
arxiv_id: '2507.21319'
source_url: https://arxiv.org/abs/2507.21319
tags:
- moral
- gpt-2
- topics
- llms
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether large language models (LLMs) accurately
  capture cross-cultural moral variations by comparing model outputs with international
  survey data on moral attitudes. Three methods were employed: comparing variances
  in moral scores between models and surveys, cluster alignment analysis between country
  groupings, and direct comparative prompts testing models'' recognition of cultural
  moral differences.'
---

# Do Large Language Models Understand Morality Across Cultures?

## Quick Facts
- arXiv ID: 2507.21319
- Source URL: https://arxiv.org/abs/2507.21319
- Reference count: 36
- Primary result: Current LLMs systematically underestimate cross-cultural moral variance and compress differences

## Executive Summary
This study investigates whether large language models (LLMs) accurately capture cross-cultural moral variations by comparing model outputs with international survey data on moral attitudes. Three methods were employed: comparing variances in moral scores between models and surveys, cluster alignment analysis between country groupings, and direct comparative prompts testing models' recognition of cultural moral differences. Results show that current LLMs generally underestimate cross-cultural moral disagreement, assigning more positive moral judgments and lower variance than observed in real-world data. No model consistently reproduced the full spectrum of cross-cultural moral variation, with GPT-2 Large and BLOOM showing slightly better performance on some metrics but still failing to achieve statistical significance.

## Method Summary
The study employed three complementary approaches to evaluate cross-cultural moral understanding in LLMs. First, variance in moral scores across countries was compared between model outputs and survey data (WVS and PEW). Second, K-means clustering was used to group countries based on moral attitudes, with cluster alignment measured via Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI). Third, direct comparative prompts tested whether models could correctly identify which country held more positive moral views on specific topics. Models were prompted with country-topic combinations and scored using log-probability differences across five moral token pairs.

## Key Results
- Models systematically underestimate cross-cultural moral variance, showing lower variance than survey data
- No model consistently reproduced full spectrum of cross-cultural moral variation across all metrics
- Models assigned more positive moral judgments and identified "most controversial" topics incorrectly
- Cluster alignment scores (CAS) ranged from -0.021 to 0.215, with most near or below zero
- Direct comparative prompt accuracy was approximately 0.5 (chance level)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs trained predominantly on English corpora compress cross-cultural moral variance
- Mechanism: Training data skews toward Western, Educated, Industrialized, Rich, Democratic (W.E.I.R.D.) sources; models learn statistical patterns from this distribution and reproduce its narrower moral variance in outputs
- Core assumption: The moral stances present in training corpora reflect the cultural composition of those corpora
- Evidence anchors: Abstract finding of systematic underestimation; section 5.4 noting English training data embeds homogenous W.E.I.R.D. values; related work on moral conditioning shaping outputs
- Break condition: If models trained on culturally balanced multilingual corpora still show variance compression, this mechanism alone is insufficient

### Mechanism 2
- Claim: Log-probability probing over moral token pairs produces compressed moral score distributions
- Mechanism: The prompting method subtracts log probabilities of contrasting tokens and averages across pairs; this may fail to capture extremes if model probability distributions are already centered
- Core assumption: The five token pairs span the moral spectrum adequately for all cultures tested
- Evidence anchors: Section 3.4 describing the scoring method; section 4 showing models assign higher mean scores and lower variance; related probing methods documented but not validated for this specific approach
- Break condition: If alternative prompting methods produce higher variance matching survey data, the mechanism is prompt-architecture dependent

### Mechanism 3
- Claim: K-means clustering of model moral scores yields low alignment with survey-derived country clusters because models lack culture-specific moral representations
- Mechanism: Models aggregate training data into averaged representations that do not encode country-level moral distinctions; clustering on these flattened representations fails to reproduce empirical patterns
- Core assumption: The K-means approach with silhouette-selected K is appropriate for this clustering task
- Evidence anchors: Section 4.1 showing clustering does not capture cultural patterns observed in survey data; CAS scores range from -0.021 to 0.215; related work on persona-based cultural conditioning reports similar challenges
- Break condition: If supervised fine-tuning on country-labeled moral data improves cluster alignment, the mechanism reflects representation capacity rather than fundamental limitation

## Foundational Learning

- Concept: W.E.I.R.D. Bias in Training Data
  - Why needed here: The paper's central diagnosis is that LLMs propagate Western-centric moral frameworks because training corpora overrepresent these perspectives
  - Quick check question: Can you explain why increasing model parameter count alone would not resolve W.E.I.R.D. bias?

- Concept: Moral Value Pluralism
  - Why needed here: The study evaluates whether LLMs reflect "moral value pluralism" (multiple valid moral frameworks) versus homogenizing to a single perspective
  - Quick check question: What empirical signal in the paper indicates that models fail to capture pluralism?

- Concept: Adjusted Rand Index and Adjusted Mutual Information
  - Why needed here: These are the metrics used to quantify cluster alignment between model outputs and survey data; understanding them is required to interpret the results
  - Quick check question: What does a negative ARI score indicate about model-survey cluster correspondence?

## Architecture Onboarding

- Component map: Input (country + topic tokens) → LLM backbone → Log probability extraction for token pairs → Score aggregation (averaging across pairs) → Variance/clustering analysis → Comparison to survey benchmarks (WVS, PEW)

- Critical path: The prompt template design directly determines the moral score range; the averaging across five token pairs is the compression point; variance calculation surfaces the homogenization effect

- Design tradeoffs: Using larger models showed inconsistent improvement; multilingual models did not outperform monolingual models—suggesting architecture and data composition may matter more than scale or multilinguality alone

- Failure signatures: Models output near-zero variance on topics with high empirical variance; cluster alignment scores near or below zero; accuracy ~0.5 on direct comparative prompts

- First 3 experiments:
  1. Replace averaged log-probability scoring with sampled open-ended generation; quantify variance in semantic moral stance via embedding clustering
  2. Construct a balanced multilingual moral corpus with country-of-origin metadata; compare variance and cluster alignment before/after continued pretraining
  3. Introduce culture-aware prompting (e.g., "From the perspective of someone in [country] with traditional values..."); measure whether this expands output variance toward empirical levels

## Open Questions the Paper Calls Out

None

## Limitations

- The study's reliance on English-only prompts for all models (including multilingual ones) represents a significant limitation
- The choice of only five moral token pairs may inadequately span the moral spectrum for all 20 tested topics across 13 cultures
- The paper does not address potential demographic biases within country-level data from WVS and PEW

## Confidence

**High confidence**: The empirical finding that LLMs show systematically lower variance than survey data is robust across multiple metrics (variance comparison, clustering alignment, direct prompts)

**Medium confidence**: The interpretation that this variance compression stems primarily from W.E.I.R.D. training data bias is plausible but not definitively proven

**Low confidence**: The claim that no model "consistently reproduced the full spectrum of cross-cultural moral variation" may be premature, as the study only tested six models

## Next Checks

1. **Cross-linguistic prompting validation**: Replicate the study using translated prompts and country-specific language models to determine whether English-only prompting artificially suppresses cultural variance

2. **Alternative moral elicitation methods**: Test whether open-ended generation with culture-specific framing produces higher variance and better cluster alignment than the log-probability method

3. **Controlled training data intervention**: Construct a balanced multilingual moral corpus with explicit country-of-origin metadata and continue pretraining a base model on this data