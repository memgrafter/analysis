---
ver: rpa2
title: 'Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric
  Training of Large Vision-Language Models'
arxiv_id: '2512.03463'
source_url: https://arxiv.org/abs/2512.03463
tags:
- image
- training
- text
- images
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Text-Printed Image (TPI) bridges the image-text modality gap in
  text-centric training of large vision-language models (LVLMs) by rendering textual
  descriptions as images, enabling effective learning without real images. Experiments
  across four LVLMs and seven benchmarks show TPI outperforms text-only and text-to-image
  training baselines, with average gains of up to 10 points and approaching ground-truth
  image performance.
---

# Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models

## Quick Facts
- arXiv ID: 2512.03463
- Source URL: https://arxiv.org/abs/2512.03463
- Reference count: 40
- Primary result: TPI outperforms text-only and T2I baselines by up to 10 points, approaching ground-truth image performance while being 1000x faster

## Executive Summary
Text-Printed Image (TPI) addresses the image-text modality gap in vision-language model training by rendering textual descriptions as images, enabling effective learning without real images. The method bridges the gap by forcing text-derived features through the frozen vision encoder, aligning the training distribution with inference-time visual distributions. Across four LVLMs and seven benchmarks, TPI achieves substantial performance gains over text-only training, preserves semantic fidelity better than text-to-image models, and enables scalable data augmentation even from small datasets.

## Method Summary
TPI renders textual descriptions as 336×336 pixel RGB images using a deterministic renderer, then processes these through the frozen vision encoder of pretrained LVLMs. The approach uses LoRA fine-tuning (r=256, α=512) to update only LLM weights while keeping the vision encoder static. Textual descriptions are generated from ground-truth images using Qwen2.5-VL-32B with detailed prompts, then rendered with automatic font downscaling to ensure complete text fits within the canvas. The method trains for 3 epochs with AdamW optimizer and cosine learning rate schedule.

## Key Results
- TPI outperforms text-only and T2I baselines by up to 10 points on VQA benchmarks
- Achieves 63.14% accuracy vs 71.14% for ground-truth images (8-point gap)
- Three orders of magnitude faster than T2I generation (0.001s vs 1.0s per sample)
- Reduces representation drift in intermediate layers compared to text-only training
- Enables effective data augmentation from as little as 1% of original dataset

## Why This Works (Mechanism)

### Mechanism 1
Rendering text as images projects textual semantics into the visual embedding space, bridging the modality gap by routing textual data through the frozen vision encoder rather than the text embedding layer. This forces text-derived features to occupy the same latent space as natural images, aligning training distribution with inference-time visual distribution. The mechanism assumes the vision encoder treats text-as-image tokens as valid visual inputs geometrically similar to natural images.

### Mechanism 2
Direct text rendering preserves semantic fidelity better than generative image synthesis by using a deterministic renderer that ensures 100% of semantic content is visually present. Unlike T2I models that introduce stochasticity and hallucinations, TPI guarantees all critical details required for QA pairs are included. The mechanism assumes visual features of rendered text are sufficient to ground logical relationships between questions and answers.

### Mechanism 3
TPI mitigates representation drift in the LLM backbone compared to text-only training by providing inputs that reside in the visual manifold. This prevents the LLM from adapting to the distinct statistical distribution of pure text embeddings, preserving the geometric structure of hidden states learned during pre-training. The mechanism assumes maintaining high CKA similarity to ground-truth image-trained models correlates with better generalization on visual tasks.

## Foundational Learning

- **Image-Text Modality Gap**: The core problem TPI solves - vision and text embeddings occupy disjoint regions in latent space. Why needed: To understand why training on raw text fails to improve visual QA performance. Quick check: Why does text-only training fail on visual tasks? (Answer: Model learns text manifold representations that don't transfer to image manifold).

- **Vision Encoder (Frozen)**: TPI relies on pre-trained visual pathway to extract features. Why needed: To understand this component is static while learning happens only in LLM/Projector. Quick check: Does TPI require fine-tuning the Vision Encoder? (Answer: No, it leverages existing encoder to process rendered text).

- **Semantic Fidelity vs Visual Realism**: Distinction between "looking like photo" (T2I) and "containing right information" (TPI). Why needed: To justify TPI over T2I. Quick check: Why would blurry text image train better VQA model than high-res synthetic photo with hallucinations