---
ver: rpa2
title: Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis
arxiv_id: '2512.04967'
source_url: https://arxiv.org/abs/2512.04967
tags:
- retinal
- classes
- disease
- learning
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of retinal disease diagnosis
  under data scarcity and class imbalance, particularly in multi-disease datasets
  like RFMiD. To tackle this, the authors propose a balanced few-shot episodic learning
  framework combining balanced episodic sampling, targeted augmentation (including
  CLAHE), and a ResNet-50 encoder.
---

# Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis

## Quick Facts
- arXiv ID: 2512.04967
- Source URL: https://arxiv.org/abs/2512.04967
- Reference count: 16
- Top-10 RFMiD 5-way 5-shot: 90.0% accuracy (best), 89.3% macro F1-score (best), 44.0% accuracy (average)

## Executive Summary
This study addresses the challenge of retinal disease diagnosis under data scarcity and class imbalance, particularly in multi-disease datasets like RFMiD. The authors propose a balanced few-shot episodic learning framework combining balanced episodic sampling, targeted augmentation (including CLAHE), and a ResNet-50 encoder. The framework is evaluated on the top 10 disease classes of RFMiD in a 5-way 5-shot configuration. Results show the model achieves 90.0% accuracy and 89.3% macro F1-score in best-performing episodes, with average performance around 44.0% accuracy and 38.46% macro F1-score. The approach demonstrates improved robustness and fairness across both majority and minority disease categories, reducing bias and enhancing diagnostic reliability in data-constrained clinical settings.

## Method Summary
The proposed framework employs Prototypical Networks with a ResNet-50 encoder trained on RFMiD's top 10 retinal disease classes using balanced 5-way 5-shot episodes. Each episode samples classes uniformly, ensuring minority diseases like ODE and BRVO appear with equal frequency as majority classes like DR. The model uses cosine similarity for classification and incorporates targeted augmentation including CLAHE for contrast enhancement, particularly benefiting minority classes. Training involves 50 epochs with 1,000 episodes each, totaling 50,000 episodes, with Adam optimization and cosine similarity-based loss.

## Key Results
- Best episode performance: 90.0% accuracy and 89.3% macro F1-score on 5-way 5-shot RFMiD evaluation
- Average performance: 44.0% accuracy and 38.46% macro F1-score across all test episodes
- Demonstrates reduced bias toward majority classes through balanced episodic sampling
- CLAHE augmentation proves particularly beneficial for subtle retinal pathologies in minority classes

## Why This Works (Mechanism)

### Mechanism 1
Balanced episodic sampling reduces majority-class bias in prototype construction. By uniformly sampling classes per episode (5-way 5-shot), minority diseases like ODE and BRVO appear with equal frequency as majority classes like DR, forcing the encoder to learn discriminative features for all classes rather than overfitting to high-frequency categories. Core assumption: Equal class exposure during training translates to equitable representation in the learned embedding space. Evidence: "balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode." Break condition: If class distributions are extremely skewed, balanced sampling may produce repetitive or over-augmented episodes that degrade generalization.

### Mechanism 2
CLAHE preprocessing improves prototype quality for subtle retinal pathologies. Contrast Limited Adaptive Histogram Equalization enhances local contrast, making fine-grained structures like microaneurysms and vessel abnormalities more salient in the embedding space, particularly benefiting minority classes with limited training samples. Core assumption: Augmentation-induced visibility improvements transfer to query-time classification without introducing artifacts that confound the encoder. Evidence: "CLAHE proved particularly beneficial for classes where subtle lesions or vessel abnormalities are primary discriminative features." Break condition: If CLAHE parameters are improperly tuned, over-enhancement may amplify noise or create artificial boundaries that mislead the encoder.

### Mechanism 3
Cosine similarity outperforms Euclidean distance for prototype-based retinal disease classification. Cosine similarity normalizes for embedding magnitude variations common in medical images, yielding more stable similarity scores when comparing query embeddings to class prototypes computed from few support samples. Core assumption: Direction of embeddings encodes disease-relevant information more reliably than magnitude. Evidence: "Cosine similarity was chosen over Euclidean distance because it is less sensitive to variations in embedding magnitude." Break condition: If embeddings are not properly normalized or the embedding space lacks angular separability, cosine similarity may underperform.

## Foundational Learning

- **Few-Shot Learning (N-way K-shot)**: Why needed here: The framework uses 5-way 5-shot episodes; understanding how support/query splits work is essential to interpret the episodic training loop. Quick check question: Can you explain why a 5-way 5-shot setup uses 25 support images and how query samples differ from support samples?

- **Prototypical Networks**: Why needed here: Classification is performed by computing class prototypes (mean embeddings) and comparing queries via cosine similarity. Quick check question: How would you compute a class prototype from 5 support embeddings, and why might this approach fail for highly intra-class variable diseases?

- **Class Imbalance in Medical Datasets**: Why needed here: RFMiD exhibits long-tail distributions; understanding why imbalance causes bias toward majority classes clarifies why balanced sampling is proposed. Quick check question: If you trained a standard classifier on RFMiD without balancing, what performance pattern would you expect across DR (300+ images) vs. ODE (58 images)?

## Architecture Onboarding

- **Component map**: Input: 224×224 RGB fundus image → CLAHE + color/geometry augmentation → ResNet-50 encoder → 2048-dim embedding → class prototypes (mean of 5 support embeddings) → cosine similarity to prototypes → softmax classification

- **Critical path**: 1. Preprocess images (resize, normalize, CLAHE) 2. Construct balanced 5-way 5-shot episodes (uniform class sampling) 3. Compute support embeddings → class prototypes 4. Compute query embeddings → cosine similarity to prototypes → loss 5. Backprop through encoder; repeat for 50 epochs × 1,000 episodes

- **Design tradeoffs**: ResNet-34 vs. ResNet-50: Deeper encoder improves fine-grained discrimination but increases compute. 5-way vs. higher-way: More classes per episode increases difficulty but may improve generalization. Augmentation intensity: CLAHE + transforms improve minority-class diversity but risk over-augmentation artifacts. Training episodes (50,000): More episodes may overfit to episode distribution; fewer may undertrain.

- **Failure signatures**: High best-episode accuracy (90%) but low average accuracy (44%): Suggests high variance across episodes; model generalizes well only under favorable sampling conditions. Confusion between ODC and DR/TSLN: Indicates overlapping pathological cues in embedding space. AUC values 0.63–0.66: Moderate separability; model struggles with visually similar pathologies.

- **First 3 experiments**: 1. Ablate CLAHE: Train identical pipeline without CLAHE augmentation; compare per-class F1 scores to quantify CLAHE's contribution to minority-class performance. 2. Vary N-way K-shot: Test 5-way 1-shot, 5-way 10-shot, and 10-way 5-shot configurations to assess sensitivity to episode structure. 3. Distance metric comparison: Replace cosine similarity with Euclidean distance and log accuracy/F1 changes across 100 test episodes to validate the reported metric advantage.

## Open Questions the Paper Calls Out

### Open Question 1
What is the individual contribution of each component (balanced episodic sampling, CLAHE augmentation, cosine similarity) to the overall performance improvement? Basis: The framework integrates three components simultaneously, but no ablation study is reported to isolate their relative importance. Why unresolved: Without ablation experiments, it remains unclear whether gains primarily derive from sampling strategy, preprocessing, or metric choice. What evidence would resolve it: Systematic ablation experiments comparing model performance with each component removed in turn.

### Open Question 2
Why is there a 46 percentage point gap between best-episode accuracy (90.0%) and average accuracy (44.0%), and how can this variability be reduced for clinical reliability? Basis: The authors report "up to 90.0% accuracy" in best episodes versus "average of 44.0%" and acknowledge that "episodic nature of few-shot learning introduces fluctuations depending on the sampled classes." Why unresolved: High variance undermines consistent diagnostic performance; the source and mitigation of this instability are not investigated. What evidence would resolve it: Analysis of episode-level factors causing divergence, plus experiments with variance-reduction techniques.

### Open Question 3
Can the balanced few-shot framework generalize to the full 46-class RFMiD dataset, including the 36 rare disease categories excluded from this study? Basis: The authors state: "Our work limits the scope to the ten most represented RFMiD classes in order to avoid the extreme sparsity of rare categories." Why unresolved: Rare diseases with fewer than 50 samples present greater challenges for prototype construction; the framework's scalability remains untested. What evidence would resolve it: Evaluation on the complete RFMiD dataset with per-class performance reporting for minority categories.

### Open Question 4
How does the proposed framework compare against other meta-learning approaches (MAML, Matching Networks) and recent foundation models for medical imaging? Basis: The paper mentions MAML and Matching Networks in related work but provides no comparative evaluation against these baselines. Why unresolved: Without benchmarking, the relative advantage of Prototypical Networks with balanced sampling over alternative FSL methods cannot be established. What evidence would resolve it: Head-to-head comparison on identical RFMiD splits with MAML, Matching Networks, and optionally medical vision transformers.

## Limitations

- Extreme performance variance (90.0% vs 44.0% accuracy) raises questions about clinical reliability and consistency across different disease combinations
- Limited to top 10 RFMiD classes, excluding 36 rare disease categories that would present greater few-shot learning challenges
- No ablation studies to quantify individual contributions of balanced sampling, CLAHE augmentation, and cosine similarity components

## Confidence

- **High confidence** in the overall framework design and mechanism plausibility (balanced episodic sampling + CLAHE + cosine similarity are well-supported by prior work)
- **Medium confidence** in the quantitative claims due to high performance variance and lack of detailed class-level analysis
- **Low confidence** in exact reproducibility without specified hyperparameters and split details

## Next Checks

1. Analyze per-episode class compositions to identify which disease combinations yield high vs low accuracy; report variance decomposition
2. Perform detailed per-class precision/recall/F1 analysis to assess whether minority classes (ODE, ODP, BRVO) truly benefit from the proposed augmentation and sampling strategy
3. Conduct ablation studies removing CLAHE and/or cosine similarity to quantify their individual contributions to the reported performance gains