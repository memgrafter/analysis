---
ver: rpa2
title: Self-Correcting Code Generation Using Small Language Models
arxiv_id: '2505.23060'
source_url: https://arxiv.org/abs/2505.23060
tags:
- code
- reward
- training
- accuracy
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores self-correction in code generation for small
  language models (SLMs), addressing the challenge that existing prompting- and training-based
  self-correction methods fail to generalize effectively to SLMs. The authors propose
  COCOS, a reinforcement learning framework that enhances intrinsic self-correction
  by using an accumulated reward function with a discount factor and a progressive
  reward that evaluates incremental improvements across multiple turns.
---

# Self-Correcting Code Generation Using Small Language Models

## Quick Facts
- arXiv ID: 2505.23060
- Source URL: https://arxiv.org/abs/2505.23060
- Reference count: 31
- This paper introduces COCOS, a reinforcement learning framework for self-correction in small language models (SLMs), achieving up to 35.8% and 27.7% improvements over baselines on MBPP and HumanEval datasets.

## Executive Summary
This paper addresses the challenge of self-correction in code generation for small language models (SLMs), where existing prompting- and training-based methods fail to generalize effectively. The authors propose COCOS, a reinforcement learning framework that uses an accumulated reward function with a discount factor and progressive rewards to enhance intrinsic self-correction capabilities. Experiments with 1B-scale models show substantial improvements over baselines, demonstrating selective correction behavior and robust generalization to unseen settings.

## Method Summary
COCOS trains small language models to self-correct code through reinforcement learning using an accumulated reward function with discount factor γ=0.5. The framework employs progressive rewards based on unit test pass ratios rather than binary correctness, and uses RLOO (REINFORCE Leave-One-Out) policy gradient optimization. Training involves generating correction trajectories with the current policy, evaluating unit test results to compute rewards, and updating the policy with KL regularization. A boost model is first fine-tuned on KodCode to ensure format compliance, then COCOS is trained on the MBPP dataset with 2-turn correction sequences.

## Key Results
- COCOS achieves 35.8% and 27.7% improvements over baselines on MBPP and HumanEval datasets respectively
- The method demonstrates selective correction behavior with low Δc→i (1.8%) and high Δi→c (11%) rates
- Progressive reward preserves more passing test cases compared to binary reward (100% preservation vs 60%)
- COCOS generalizes well to unseen datasets (ODEX) while maintaining stability across different small models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The accumulated reward function with discount factor γ prevents both reward hacking and training collapse that occurs with KL-regularized approaches in SLMs.
- Mechanism: The reward R(ŷ₁:T) = γ^(T-1)r₁ + Σγ^(T-t)(rₜ - rₜ₋₁) aggregates rewards across the trajectory. With γ = 0.5, it amplifies recent improvements while maintaining accountability for earlier responses. This avoids the KL constraint issue where strong output overlap between turns (ŷ₁ ≈ ŷ₂ in SLMs) causes regularization to destabilize second-turn optimization.
- Core assumption: SLMs produce high-overlap outputs across turns due to limited revision capability, which causes standard KL-regularized methods to fail.
- Evidence anchors:
  - [abstract] "accumulated reward function that aggregates rewards across the entire trajectory"
  - [Section 5.2] "We observe that first-turn KL-regularization explodes during training. This leads to instability in the learning process, driving the reward toward zero"
  - [Section 5.3] "When γ = 0, training focuses on maximizing the reward difference r₂ - r₁, often by deliberately decreasing r₁"

### Mechanism 2
- Claim: Progressive reward based on unit test pass ratios provides denser learning signals than binary correctness, enabling more stable policy exploration.
- Mechanism: Instead of binary reward (1 for all-pass, 0 otherwise), the progressive reward rₜ = (1/K)ΣI{uₖ(ŷₜ) passes} assigns partial credit. Solving one additional test case yields positive reward; introducing errors yields negative reward. This expands the exploration space during training.
- Core assumption: Incremental improvements in code quality (passing more test cases) should be rewarded even when full correctness is not achieved.
- Evidence anchors:
  - [Section 5.3] "fine-grained reward better suited to multi-turn correction scenarios"
  - [Section 8.2] "On the ODEX, 34% of test cases show improvement, with a 100% preservation rate. In comparison, the binary reward causes around 40% of previously passed test cases to fail"
  - [Appendix E] "sparse rewards make it challenging for RL to associate actions with rewards"

### Mechanism 3
- Claim: Online reinforcement learning with self-generated trajectories enables better generalization than SFT-based correction methods.
- Mechanism: The model learns from its own generated correction trajectories via policy gradient (RLOO estimator), rather than memorizing fixed correction patterns from training data. This produces selective correction behavior (low Δc→i, high Δi→c).
- Core assumption: SFT-based methods overfit to training distribution and fail to generalize the correction behavior to unseen problems.
- Evidence anchors:
  - [Section 7] "COCOS, on the other hand, corrects selectively: it achieves a low Δc→i rate of 1.8%, avoiding unnecessary changes, while maintaining a high Δi→c rate of 11%"
  - [Section 7.1] "The SFT-based baselines (Turn-SFT, Self-Corrector, and ReVISE) often fail to generalize beyond their training distribution"

## Foundational Learning

- Concept: **Policy Gradient / REINFORCE**
  - Why needed here: COCOS uses RLOO (REINFORCE Leave-One-Out) for multi-turn optimization. Understanding how ∇θ log πθ(a|s) scales gradients by rewards is essential.
  - Quick check question: Can you explain why using other samples' rewards as a baseline reduces variance while maintaining unbiasedness?

- Concept: **KL Divergence Regularization**
  - Why needed here: The paper shows KL constraints cause training collapse in SLMs due to output overlap. Understanding this failure mode is critical.
  - Quick check question: Why would strong KL regularization on turn-1 outputs inadvertently constrain turn-2 optimization when ŷ₁ ≈ ŷ₂?

- Concept: **Credit Assignment in Multi-turn RL**
  - Why needed here: The accumulated reward function addresses credit assignment—determining which turn's actions contributed to the total reward.
  - Quick check question: With γ = 2, why does the "credit assignment problem" make it unclear which response contributed positively?

## Architecture Onboarding

- Component map:
  - Boost Model (SFT on KodCode) -> Multi-turn MDP -> Reward Module (Unit tests) -> Policy Optimizer (RLOO) -> Reference Policy (Frozen boost model)

- Critical path:
  1. Pre-train boost model on KodCode (format compliance only)
  2. For each training batch: sample k trajectories from current policy
  3. Execute unit tests → compute progressive rewards per turn
  4. Compute accumulated reward R(ŷ₁:T) with γ = 0.5
  5. Compute RLOO baseline-adjusted gradients
  6. Update policy with KL penalty β = 0.01

- Design tradeoffs:
  - γ selection: Lower γ emphasizes recent improvements; higher values preserve initial quality. γ = 0.5 is empirically chosen.
  - KL coefficient (β = 0.01): Must be small enough to avoid SCoRe's collapse but sufficient to prevent drift.
  - Training turns (T = 2): Limited by compute; extending may yield further gains per turn-wise analysis.

- Failure signatures:
  - **Training collapse**: Reward → 0 with exploding KL on first turn (indicates need to disable or reduce first-turn KL regularization)
  - **Reward hacking**: Accuracy@t1 degrades while Accuracy@t2 improves (indicates γ too low or missing accumulated reward)
  - **No revision behavior**: Edit distance ≈ 0 across turns (model hasn't learned correction; may need SFT warmstart)

- First 3 experiments:
  1. **Baseline sanity check**: Verify boost model achieves ~9-17% on MBPP (Table 2). If much higher, contamination suspected.
  2. **Ablation on γ**: Train with γ ∈ {0, 0.5, 1, 2} and plot learning curves. Confirm γ = 0 shows reward hacking, γ = 2 shows instability.
  3. **Progressive vs binary reward**: Train two models, evaluate on Δc→i_unit metric. Confirm progressive reward preserves more passing test cases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can COCOS's accumulated reward approach be extended effectively to training beyond two turns, and how does performance scale with additional correction iterations?
- Basis in paper: [explicit] The Limitations section states: "Due to infrastructural constraints, we limited COCOS training to only two turns... future work should extend our approach to support multi-turn training beyond two turns."
- Why unresolved: Online RL incurs cost at every action step, making multi-turn training computationally expensive. The paper only demonstrates inference beyond two turns, not training.
- What evidence would resolve it: Training experiments with T > 2 turns showing whether accumulated rewards with discount factor γ remain stable and whether accuracy gains continue or plateau.

### Open Question 2
- Question: Does COCOS scale effectively to models larger than 1B parameters, and does the progressive reward remain necessary as model capacity increases?
- Basis in paper: [explicit] The Limitations section notes: "evaluating its scalability to models larger than 1B parameters remains an interesting aspect to observe in future work."
- Why unresolved: All experiments used 1B-scale models (Qwen2.5-1.5B, Llama-3.2-1B, DeepSeek-Coder-1.3B). Larger models may have different self-correction capabilities that reduce the need for COCOS's specific mechanisms.
- What evidence would resolve it: Experiments applying COCOS to 3B, 7B, and larger models, comparing against baselines to assess whether the relative improvements persist.

### Open Question 3
- Question: Can offline reinforcement learning achieve comparable self-correction improvements to COCOS's online RL while reducing computational costs?
- Basis in paper: [explicit] The Limitations section mentions: "future work should extend our approach... potentially incorporating more cost-efficient strategies such as offline reinforcement learning."
- Why unresolved: COCOS uses online RL which requires real-time interaction with the environment. Offline RL could enable pre-computed training but may sacrifice the benefits of self-generated data.
- What evidence would resolve it: Comparison of offline RL variants trained on pre-collected trajectories against online COCOS, measuring both accuracy gains and training efficiency.

### Open Question 4
- Question: How can the credit assignment problem in multi-turn correction be addressed when using accumulated rewards with γ > 1?
- Basis in paper: [explicit] Section 8.1 states: "This situation is called the credit assignment problem in MDPs... This is beyond the scope of the current research and is not further addressed in this work."
- Why unresolved: When γ = 2, joint optimization of both responses causes training instability because it becomes unclear which response contributed positively to the total reward.
- What evidence would resolve it: Development of credit assignment mechanisms (e.g., counterfactual estimation, turn-specific value functions) that enable stable training with alternative discount factor configurations.

## Limitations
- Limited to 1B-scale models; scalability to larger models remains untested
- Only trained and evaluated on two-turn correction sequences due to computational constraints
- Characterizes SLM-specific KL regularization failure modes without systematic quantification or testing on larger models

## Confidence
- **High Confidence**: The accumulated reward formulation with discount factor (Mechanism 1) and progressive reward implementation (Mechanism 2) are technically sound and well-validated through controlled ablations.
- **Medium Confidence**: The claim that online RL generalizes better than SFT-based methods (Mechanism 3) is supported but could benefit from more extensive testing across diverse problem distributions and model scales.
- **Low Confidence**: The characterization of SLM-specific failure modes with KL regularization lacks systematic quantification and rigorous testing across model scales.

## Next Checks
1. **Systematic KL Regularization Study**: Conduct controlled experiments varying β (KL coefficient) and γ across both 1B-scale and larger models (e.g., 7B) to quantify when and how KL regularization causes training collapse. Measure output overlap metrics (edit distance, KL divergence) between turns to establish the causal link.

2. **Extended Generalization Testing**: Evaluate COCOS on a broader set of code generation tasks including multi-file projects, different programming languages, and real-world codebases. Compare against multiple SFT variants using diverse correction datasets to establish robustness of the generalization advantage.

3. **Multi-turn Scaling Analysis**: Extend the training beyond T=2 turns to assess whether the accumulated reward mechanism continues to provide benefits as the correction chain lengthens. Analyze how the discount factor γ should be adjusted for longer trajectories and whether the progressive reward maintains its effectiveness.