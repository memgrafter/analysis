---
ver: rpa2
title: 'TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization
  For Dummies'
arxiv_id: '2511.23225'
source_url: https://arxiv.org/abs/2511.23225
tags:
- tweo
- outliers
- uni00a0out
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extreme activation outliers
  that hinder FP8 training and quantization of Transformers. The authors propose TWEO
  (Transformers Without Extreme Outliers), a non-invasive loss function that directly
  penalizes extreme activation magnitudes.
---

# TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies

## Quick Facts
- **arXiv ID:** 2511.23225
- **Source URL:** https://arxiv.org/abs/2511.23225
- **Reference count:** 40
- **One-line primary result:** TWEO enables stable FP8 training with 36% higher throughput and first-ever W8A8 per-tensor quantization of LLMs by eliminating extreme activation outliers.

## Executive Summary
TWEO (Transformers Without Extreme Outliers) solves the fundamental problem of extreme activation outliers (>1000 magnitude) that prevent FP8 training and simple quantization of Transformers. The method introduces a non-invasive L4 penalty loss that suppresses these outliers, which are revealed to be structural artifacts of weight matrix colinearity rather than data-dependent phenomena. By capping activation magnitudes, TWEO enables stable FP8 training with performance matching BF16 baselines while delivering 36% higher throughput. It also makes hardware-friendly W8A8 per-tensor static quantization viable for the first time, eliminating the need for complex mixed-precision schemes or transfer-based methods like SmoothQuant.

## Method Summary
TWEO adds a loss term to penalize extreme activation magnitudes in Transformer outputs. After each block's residual addition, activations are scaled by τ and raised to the 4th power: L_TWEO = (1/L) Σ E[|A^(l)|/τ]^4. Key hyperparameters are τ=3, p=4, λ=0.01. The method works with NVIDIA Transformer Engine's FP8 training using DelayedScaling and amax_history_length=16, with all Linear and LayerNorm layers under FP8 autocast. TWEO is applied during pre-training from scratch and enables downstream quantization without requiring architectural changes or mixed-precision tricks.

## Key Results
- Eliminates extreme outliers from 10000+ to less than 20 while maintaining BF16-level performance
- Enables stable FP8 pre-training with 36% higher throughput than BF16 baseline
- Achieves state-of-the-art W8A8 per-tensor static quantization for LLMs, previously considered impossible
- Works universally across vision (Swin/ViT) and language (GPT-2) models without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extreme activation outliers are a structural artifact of weight colinearity rather than input data semantics.
- **Mechanism:** Outliers emerge when weight rows in down-projection matrix become collinear with dominant singular vectors of up-projection matrix. When input activates this aligned direction, magnitude explodes via amplification y_k ≈ s(w^T u)(v^T x).
- **Core assumption:** Outliers persist with random inputs but disappear with random weights, implying weights hold causal structure.
- **Evidence anchors:** Abstract states outliers are "mechanically-produced artifact... originating from colinearity." Section 3.2 confirms outliers result from "mechanical amplification caused by specific properties that network's weight matrices gradually develop."

### Mechanism 2
- **Claim:** High-exponent L4 loss acts as soft-thresholding mechanism that suppresses extreme magnitudes while preserving normal signals.
- **Mechanism:** Dividing activations by τ and raising to 4th power creates super-linear penalty curve. Normal values yield negligible loss, while outliers incur massive penalty, forcing optimizer to decay causative weight alignments.
- **Core assumption:** Network can achieve comparable task performance without utilizing extreme activation magnitudes.
- **Evidence anchors:** Abstract reports reducing outliers from 10000+ to less than 20. Section 3.3 explains tolerance for normal values (0.5^4 ≈ 0.06) while suppressing outliers (10^4 penalty).

### Mechanism 3
- **Claim:** Removing outliers allows residual stream to be quantized using simplest per-tensor schemes.
- **Mechanism:** Standard quantization struggles with residual stream because outliers force large scaling factor, crushing small values. TWEO caps dynamic range, making W8A8 per-tensor static quantization viable for entire model.
- **Core assumption:** Outlier-free state persists through inference without re-introducing drift.
- **Evidence anchors:** Abstract states TWEO "enables W8A8 per-tensor static quantization... previously considered completely unusable." Section 4.4 shows SmoothQuant collapsing when residual is quantized, whereas TWEO maintains low perplexity.

## Foundational Learning

- **Concept: FP8 Dynamic Range (E4M3)**
  - **Why needed here:** Must understand why value of 1000 is fatal to FP8 training. E4M3 format caps at ±448; values beyond this overflow to NaN/Inf, causing gradient explosion.
  - **Quick check question:** What is maximum representable value in E4M3 format, and how does TWEO ensure activations stay within it?

- **Concept: Singular Value Decomposition (SVD) in MLPs**
  - **Why needed here:** Paper attributes outliers to geometry of weight matrices. Understanding how vector w aligning with singular vector u amplifies signal is key to diagnosis.
  - **Quick check question:** In equation y_k = w^T A x, why does alignment of w with singular vector of A lead to outlier?

- **Concept: Per-Tensor vs. Per-Token Quantization**
  - **Why needed here:** Paper claims to make "per-tensor" quantization viable. Need to know per-tensor uses one scale for whole matrix (fast but sensitive to outliers), while per-token uses scale per row (robust but slow).
  - **Quick check question:** Why does single extreme outlier (e.g., value 1000) ruin precision of per-tensor quantized matrix?

## Architecture Onboarding

- **Component map:** Standard Transformer -> TWEO Loss Module -> Task Loss
- **Critical path:** 1) Forward pass executes standard block computation (y = x + MLP(x)) 2) Compute absolute activation |A^(l)| 3) Apply TWEO formula: (|A|/(τ+ε))^4 4) Average over dimensions and sum over layers 5) Add to task loss: L_total = L_task + λL_TWEO
- **Design tradeoffs:** Simplicity vs. Expressivity - gain stable FP8 training and simple quantization but explicitly constrain activation distribution, which might theoretically limit upper bound of expressivity. Throughput +36% vs BF16; avoids complex engineering of mixed-precision frameworks.
- **Failure signatures:** FP8 Overflow (NaN/Inf) if λ too low or τ too high; Underfitting if λ too aggressive early; PTQ Collapse if TWEO not applied, per-tensor W8A8 quantization will fail catastrophically.
- **First 3 experiments:** 1) Baseline Verification - Train GPT-2 Medium in pure FP8 without TWEO to observe catastrophic collapse point 2) TWEO Stability - Train same model with TWEO (τ=3, p=4) in FP8, measure training loss convergence against BF16 baseline 3) Quantization Stress Test - Apply naive AbsMax W8A8 per-tensor quantization to trained TWEO model, compare perplexity against SmoothQuant baseline

## Open Questions the Paper Calls Out

- **Open Question 1:** Can TWEO maintain numerical stability and performance when scaling to massive models (e.g., 700B parameters)? The conclusion states "we had not verified TWEO on the largest models (e.g., 700B), which will be a future work." Experiments were limited to 7B parameters due to computational resource constraints.

- **Open Question 2:** Can TWEO be applied via fine-tuning to effectively sanitize existing pre-trained models of extreme outliers? Authors note "While TWEO models were trained from scratch in this paper, we are also interested in fine-tuning existing models to remove their extreme outliers." Current study focuses on pre-training from scratch, not retrofitting established weights.

- **Open Question 3:** Does outlier-free state induced by TWEO enable stable training and inference at lower precisions (FP4)? Conclusion lists interest in "further FP4 learning and inference." Paper focuses on FP8 and does not test if suppression of magnitudes is sufficient for narrower FP4 dynamic range.

- **Open Question 4:** Does TWEO generalize to Mixture of Experts (MoE) architectures without disrupting router behavior? Paper critiques complex engineering of MoE-based DeepSeek-V3 but only validates TWEO on dense Transformer architectures (GPT-2, Swin, ViT). MoE models have distinct activation dynamics and routing sensitivities.

## Limitations
- Limited to GPT-2 Medium/Small and Swin-ViT on specific datasets (OpenWebText, ImageNet-1K); universality across all model types not rigorously tested
- Sensitive to hyperparameters (τ, p, λ) with exact sensitivity landscape across different model scales not thoroughly explored
- Long-term stability over extended training periods (e.g., full LLM pretraining) and impact on final converged performance not established

## Confidence
- **High Confidence:** Core mechanism of outlier suppression via L4 penalty is well-justified; immediate results (stable FP8 training, W8A8 quantization) clearly demonstrated and reproducible
- **Medium Confidence:** Claim of universality across "vision and language models" supported by ViT experiments but breadth of testing is limited; assertion that outliers are purely "structural artifact" is compelling but based on indirect evidence
- **Low Confidence:** Long-term effects on very large models and full spectrum of architectural variants are speculative; exact sensitivity of method to hyperparameters across diverse settings not fully characterized

## Next Checks
1. **Architecture Stress Test:** Apply TWEO to BERT, LLaMA-2 (7B), and dense ViT-Large. Compare FP8 training stability and final task performance against BF16 baseline to test claimed universality.

2. **Hyperparameter Sweep:** Systematically vary τ (2, 3, 5) and λ (0.001, 0.01, 0.1) for GPT-2 Small to map performance landscape and identify sensitivity of method to key parameters.

3. **Long-Training Evaluation:** Train GPT-2 Medium with TWEO for full 100B+ tokens of OpenWebText. Monitor evolution of outlier magnitude, TWEO loss weight λ(t), and model's ability to recover from potential over-regularization.