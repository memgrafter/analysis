---
ver: rpa2
title: Efficient Online Continual Learning in Sensor-Based Human Activity Recognition
arxiv_id: '2511.05566'
source_url: https://arxiv.org/abs/2511.05566
tags:
- data
- network
- classes
- class
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PTRN-HAR introduces the first pre-trained model-based approach
  for online continual learning in sensor-based human activity recognition. Unlike
  existing methods that require continuous model updates during streaming, PTRN-HAR
  pre-trains a feature extractor using contrastive loss and freezes it during deployment,
  significantly reducing computational costs.
---

# Efficient Online Continual Learning in Sensor-Based Human Activity Recognition

## Quick Facts
- arXiv ID: 2511.05566
- Source URL: https://arxiv.org/abs/2511.05566
- Authors: Yao Zhang; Souza Leite Clayton; Yu Xiao
- Reference count: 40
- Primary result: 16.7-18.9% improvement in Macro-F1 score with 20-35% training time reduction

## Executive Summary
PTRN-HAR introduces the first pre-trained model-based approach for online continual learning in sensor-based human activity recognition. The method pre-trains a feature extractor using contrastive loss and freezes it during deployment, significantly reducing computational costs compared to continuous model updates. A relation module network replaces the conventional dense classification layer, enabling effective classification of new activities with limited labeled data. The approach addresses the challenges of heterogeneous HAR datasets and scarce post-deployment labeled samples.

## Method Summary
PTRN-HAR employs a two-stage approach for online continual learning in sensor-based human activity recognition. First, a feature extractor is pre-trained using contrastive loss on a diverse set of sensor data, then frozen during deployment to minimize computational overhead. Second, a relation module network replaces the traditional dense classification layer, allowing the system to classify new activities with minimal labeled data. This architecture eliminates the need for continuous model updates during streaming, reducing both training time (20-35%) and memory usage (70-80%) while maintaining competitive accuracy across heterogeneous HAR datasets.

## Key Results
- 16.7-18.9% improvement in Macro-F1 score compared to state-of-the-art methods
- 20-35% reduction in training time
- 70-80% reduction in memory usage
- Competitive accuracy: 81.02% on PAMAP2, 84.30% on HAPT, 85.39% on DSADS

## Why This Works (Mechanism)
The method works by decoupling feature extraction from classification. The pre-trained feature extractor captures general motion patterns through contrastive learning, creating a robust representation space that generalizes across different sensor configurations and activities. The relation module network then learns to map these fixed features to activity classes without requiring backpropagation through the entire network. This separation allows the system to adapt to new activities by only updating the relation module with minimal labeled data, while the computationally expensive feature extraction remains frozen.

## Foundational Learning
- Contrastive Learning: Trains the feature extractor to distinguish between similar and dissimilar sensor patterns, creating a generalizable representation space
- Relation Networks: Learns to measure similarity between feature vectors and class prototypes without dense classification layers
- Online Continual Learning: Enables adaptation to new activities in streaming data without catastrophic forgetting
- Pre-training: Leverages large-scale diverse data to learn general motion features before deployment

## Architecture Onboarding
Component map: Pre-training Data -> Feature Extractor (frozen) -> Relation Module -> Classification Output

Critical path: Sensor input → Frozen Feature Extractor → Relation Module → Activity Classification

Design tradeoffs: Pre-training provides computational efficiency but requires upfront computational cost and diverse training data. The frozen feature extractor ensures stability but limits adaptation to new sensor types.

Failure signatures: Performance degradation when new activities are significantly different from pre-training data, or when sensor characteristics differ substantially from pre-training conditions.

First experiments to run:
1. Test classification accuracy on new activities with varying numbers of labeled samples (5, 10, 20, 50)
2. Evaluate performance when pre-training data domain differs from deployment environment
3. Measure computational overhead of the relation module update versus full model fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade when new activities differ significantly from pre-training data distribution
- Requires diverse and representative pre-training data to ensure general feature extraction
- Limited validation to three public datasets may not capture all HAR scenarios

## Confidence
- Computational efficiency improvements (20-35% training time, 70-80% memory): High
- Macro-F1 score improvements (16.7-18.9%): Medium
- Competitive accuracy on tested datasets (81.02% PAMAP2, 84.30% HAPT, 85.39% DSADS): High
- First pre-trained model-based approach claim: Low
- Effectiveness with 20 labeled samples per class: Medium

## Next Checks
1. Test the approach on additional HAR datasets beyond the three public datasets used to verify generalizability across more diverse sensor configurations and activity types
2. Conduct ablation studies to isolate the contribution of the contrastive pre-training versus the relation module network to quantify their individual impacts
3. Evaluate the model's performance with varying numbers of labeled samples per new class (beyond the fixed 20 samples) to determine the minimum effective sample size for the approach