---
ver: rpa2
title: Provable Watermarking for Data Poisoning Attacks
arxiv_id: '2510.09210'
source_url: https://arxiv.org/abs/2510.09210
tags:
- watermarking
- data
- poisoning
- attacks
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of ensuring transparency in data
  poisoning attacks by introducing provable watermarking schemes. It proposes two
  approaches: post-poisoning watermarking, where a third party adds watermarks to
  poisoned data, and poisoning-concurrent watermarking, where the poisoner embeds
  watermarks during poisoning.'
---

# Provable Watermarking for Data Poisoning Attacks

## Quick Facts
- arXiv ID: 2510.09210
- Source URL: https://arxiv.org/abs/2510.09210
- Reference count: 40
- One-line primary result: Provable watermarking schemes ensure detectability of poisoned data while preserving attack effectiveness under bounded perturbation budgets.

## Executive Summary
This paper introduces provable watermarking schemes to ensure transparency in data poisoning attacks. It proposes two approaches: post-poisoning watermarking, where a third party adds watermarks to poisoned data, and poisoning-concurrent watermarking, where the poisoner embeds watermarks during poisoning. Theoretical analysis establishes that effective watermarking requires lengths of Θ(√d/εw) for post-poisoning and Θ(1/εw²) to O(√d/εp) for poisoning-concurrent, ensuring both watermark detectability and poisoning utility. Experiments on backdoor and availability attacks across multiple datasets and models validate these findings, demonstrating strong detection performance (AUROC up to 1.0) with minimal impact on poisoning effectiveness when watermarking length is appropriately bounded.

## Method Summary
The paper proposes provable watermarking for data poisoning attacks through two mechanisms: post-poisoning watermarking (third party adds watermarks) and poisoning-concurrent watermarking (poisoner embeds watermarks during poisoning). The system generates a random key ζ (entries ±1) and constructs perturbations δw = εw · sign(ζ) on q dimensions. Detection uses inner product thresholds: ζT(x + δw) pushes watermarked data toward q · εw while clean data concentrates around 0. The concurrent approach partitions dimensions into poisoning set P and watermarking set W to preserve utility. Theoretical bounds ensure watermark detectability and poisoning effectiveness, validated through experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet-18 models.

## Key Results
- Theoretical analysis shows watermarking length requirements of Θ(√d/εw) for post-poisoning and Θ(1/εw²) to O(√d/εp) for poisoning-concurrent
- Experiments demonstrate AUROC up to 1.0 for watermark detection across multiple datasets and attack types
- Minimal impact on poisoning effectiveness (ASR/ACC) when watermarking length is appropriately bounded

## Why This Works (Mechanism)

### Mechanism 1: Statistical Separation via Inner Product Thresholds
- Claim: A detector can distinguish watermarked (poisoned) data from clean data with high probability using a secret key and a fixed threshold.
- Mechanism: The system generates a random key ζ (entries ±1). For watermarked data, the perturbation is constructed as δw = εw · sign(ζ) on q dimensions. This maximizes the inner product ζT(x + δw), pushing it toward q · εw. For clean data, the expected inner product with the random key is 0. McDiarmid's inequality bounds the variance, ensuring a detectable gap exists between the distributions of ζTxpoison and ζTxclean.
- Core assumption: The key ζ is sampled from a distribution (like Rademacher) independent of the data content, and the watermark budget εw is strictly adhered to.
- Evidence anchors:
  - [abstract]: "Theoretical analysis shows that effective watermarking requires lengths of Θ(√d/εw)... ensuring both watermark detectability..."
  - [section]: Theorem 4.1 (Sample-wise, post-poisoning) establishes the statistical bounds for separation.
  - [corpus]: Related work in "Data Taggants" utilizes similar data poisoning for verification, but this paper formalizes the *detectability conditions* via statistical bounds.
- Break condition: If the watermarking length q is too small (specifically q < Θ(√d/εw) for post-poisoning), the distributions overlap, making the threshold-based detector ineffective (AUROC drops significantly).

### Mechanism 2: Dimensional Isolation for Utility Preservation
- Claim: Poisoning and watermarking can coexist without one destroying the other by operating on disjoint subsets of data dimensions.
- Mechanism: In "poisoning-concurrent watermarking," the generator partitions data dimensions into a poisoning set P and a watermarking set W. The watermark perturbs dimensions W, while the poison operates on P. Theoretical analysis shows that if the watermark does not encroach on the "poison dimensions," the generalization error introduced by the watermark is bounded and negligible for large d.
- Core assumption: The poisoning attack's efficacy relies on features that can be maintained within the restricted dimension subspace P (i.e., the attack doesn't require the full d dimensions).
- Evidence anchors:
  - [section]: Section 3.2 defines "poisoning-concurrent watermarking" where dimensions for poisoning P = [d] \ W.
  - [section]: Theorem 5.6 bounds the impact of restricting poisoning dimensions, provided q = O(√d/εp).
  - [corpus]: "CBW" and other dataset ownership papers often use backdoors; this mechanism specifically addresses how to watermark *without* breaking the backdoor trigger by isolating dimensions.
- Break condition: If the watermark length q grows too large relative to the data dimension d (violating the O(√d/εp) bound), the watermark consumes too many dimensions, effectively "diluting" the poison and restoring clean accuracy (poison utility fails).

### Mechanism 3: Generalization Error Bounding
- Claim: The addition of a watermark perturbation does not significantly shift the optimal classifier of the poisoned data, preserving the attack's objective.
- Mechanism: The paper models the neural network and loss function to bound the risk gap R(D', F*). It proves that for large datasets (N) and high dimensions (d), the error introduced by the watermark is O(εw√(q log(1/ω)/d)). Because this term shrinks as dimension d increases, the "watermarked poisoned" model behaves similarly to the "unwatermarked poisoned" model.
- Core assumption: The network follows Xavier normalization and the loss function is Lipschitz continuous.
- Evidence anchors:
  - [section]: Theorem 5.2 provides the bound on the impact of watermarking on the risk.
  - [abstract]: "...provable and practical watermarking approaches... ensuring both watermark detectability and poisoning utility."
  - [corpus]: "CertDW" discusses certification via conformal prediction; this paper uses generalization bounds to certify *utility* rather than just verification.
- Break condition: If the watermark budget εw or length q is set excessively high without regard for the bounds, the perturbation shifts the decision boundary too far, causing the model to "unlearn" the poison (evident in Table 2 where accuracy recovers at high lengths).

## Foundational Learning

- Concept: **Data Poisoning (Backdoor vs. Availability)**
  - Why needed here: The watermarking scheme must function differently depending on whether the goal is to embed a specific trigger (backdoor) or simply degrade generalization (availability/unlearnable examples).
  - Quick check question: Does the watermark need to preserve a high Attack Success Rate (ASR) for a specific trigger, or simply maintain a low clean accuracy (ACC)?

- Concept: **Statistical Concentration (McDiarmid's Inequality)**
  - Why needed here: This is the mathematical engine used to prove that the inner product ζTx behaves predictably (concentrates around the mean) for clean vs. watermarked data.
  - Quick check question: If you flip a fair coin N times, how does the probability of the sum deviating significantly from N/2 change as N increases?

- Concept: **Vector Spaces and Inner Products**
  - Why needed here: The detection logic relies entirely on the geometric relationship between the key vector ζ and the data vector x. Maximizing the dot product is the core signal.
  - Quick check question: If vector A is orthogonal to vector B, what is their dot product, and would B be a good key for detecting a signal aligned with A?

## Architecture Onboarding

- Component map:
  1. **Poison Generator**: Creates perturbation δp (e.g., Narcissus, UE)
  2. **Watermark Generator**: Creates δw = εw · sign(ζ) on dimensions W
  3. **Perturbation Merger**: Adds perturbations (x' = x + δp + δw or x' = x + δp ⊕ δw)
  4. **Detector**: Computes score v = ζTx; compares against threshold τ

- Critical path:
  1. Select watermarking dimensions W (random or specific)
  2. Generate random key ζ aligned with W
  3. Inject watermark ensuring length q and budget εw satisfy the theoretical bounds (q ≈ Θ(√d/εw))
  4. Verify utility (ASR/Acc) is not destroyed by the watermark

- Design tradeoffs:
  - **Post-Poisoning vs. Concurrent**: Post-poisoning is safer for utility (no dimension reduction for poison) but increases total perturbation budget (εp + εw). Concurrent is stealthier (total budget max(εp, εw)) but risks destroying poison efficacy if q is too large.
  - **Length q**: Increasing q improves AUROC (detection) but degrades poisoning utility (specifically for concurrent watermarking)

- Failure signatures:
  - **Detection Failure (AUROC ~0.5)**: Watermark length q is too low relative to √d; statistical gap is closed by noise
  - **Utility Failure (Clean Accuracy Recovers)**: Watermark length q is too high (exceeds O(√d/εp)), effectively overwriting the poison signal
  - **Sanity Check**: If a random key yields high AUROC, the watermark is not covert or the key generation is biased

- First 3 experiments:
  1. **Detection Curve**: Plot AUROC vs. Watermark Length (q) on CIFAR-10 for a fixed εw (e.g., 16/255) to validate the Θ(√d/εw) threshold
  2. **Utility Retention**: Plot Attack Success Rate (ASR) vs. q for a backdoor attack (e.g., AdvSc) using Concurrent Watermarking to identify the breaking point where ASR drops
  3. **Robustness**: Attempt detection using a *random* key (not the generation key) to confirm the "Covertness" property (AUROC should drop to ~0.5)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary conditions for watermarking length and budget to guarantee both detectability and poisoning utility?
- Basis in paper: [explicit] The authors state in Section 7 that while they offer sufficient conditions, "the necessary conditions for these properties remain an open area for future research."
- Why unresolved: The current theoretical analysis provides loose bounds (sufficient conditions) for the watermark length q; tight lower bounds (necessary conditions) that guarantee the impossibility of watermarking below a certain threshold are not derived.
- Evidence: A theoretical proof establishing the minimum watermarking length required for detection/utility, or empirical evidence showing failure rates increase predictably below current thresholds.

### Open Question 2
- Question: Can more sophisticated watermarking designs achieve better robustness or efficiency than the proposed additive schemes?
- Basis in paper: [explicit] Section 7 explicitly identifies "exploring more sophisticated watermarking designs that could achieve better performance and robustness in both detection and poisoning utility" as a promising direction.
- Why unresolved: The paper focuses on simple, provable additive watermarking mechanisms (δw = εw · sign(ζ)) rather than optimizing the watermark signal structure.
- Evidence: A comparative study showing that non-linear or optimized watermarking embeddings yield higher AUROC or retain poisoning effectiveness (ASR/Acc) under stricter budget constraints (εw).

### Open Question 3
- Question: Is it possible to remove the watermark without destroying the underlying poisoning utility?
- Basis in paper: [inferred] Appendix E discusses removal methods like diffusion and DP noise, concluding they destroy poisoning utility. However, the paper does not rule out the existence of a "surgical" removal attack that strips the mark while keeping the poison effective.
- Why unresolved: The robustness analysis relies on the argument that "purification destroys poison," leaving a gap for potential adaptive attacks designed specifically to decouple the watermark from the poison.
- Evidence: An adaptive attack algorithm that successfully lowers the detection AUROC to random chance (~0.5) while maintaining a high Attack Success Rate (ASR) for backdoor attacks.

## Limitations

- The core assumption that poisoning attacks can operate effectively on restricted dimension subsets may not hold for all poisoning strategies, particularly those requiring fine-grained spatial patterns
- The paper assumes clean data is well-clustered, which may not hold in all datasets
- Performance gains rely on ResNet-18; generalization to other architectures requires validation

## Confidence

- **High Confidence**: Detection mechanism (AUROC results up to 1.0 are consistently observed across experiments), theoretical bounds on watermark length requirements
- **Medium Confidence**: Generalization error bounds (dependent on assumptions about network architecture and loss function), concurrent watermarking utility preservation (requires precise calibration of q)
- **Low Confidence**: Applicability to non-image domains, performance with different model architectures, robustness against adaptive attacks that might learn to ignore watermarked dimensions

## Next Checks

1. **Dimensional Sensitivity Test**: Systematically vary watermark length q on CIFAR-10 for both post-poisoning and concurrent schemes, plotting both AUROC and ASR/ACC to empirically verify the theoretical bounds Θ(√d/εw) and O(√d/εp)
2. **Architecture Generalization**: Apply the concurrent watermarking scheme to VGG-16 and EfficientNet-B0 on CIFAR-10, comparing performance to ResNet-18 to assess architecture dependence
3. **Adaptive Attack Robustness**: Implement an adaptive poisoning attack that explicitly tries to avoid watermarked dimensions during generation (e.g., by masking or downweighting), measuring the degradation in watermark detectability