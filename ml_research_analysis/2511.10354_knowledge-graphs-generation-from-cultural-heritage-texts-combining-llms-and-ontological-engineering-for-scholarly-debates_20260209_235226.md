---
ver: rpa2
title: 'Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and
  Ontological Engineering for Scholarly Debates'
arxiv_id: '2511.10354'
source_url: https://arxiv.org/abs/2511.10354
tags:
- extraction
- knowledge
- scholarly
- entity
- sebi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ATR4CH provides a systematic methodology for converting complex
  scholarly discourse from Cultural Heritage texts into structured Knowledge Graphs
  using LLMs. The approach combines annotation models, ontological frameworks, and
  iterative development through five steps: foundational analysis, annotation schema
  development, pipeline architecture, integration refinement, and evaluation.'
---

# Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates

## Quick Facts
- arXiv ID: 2511.10354
- Source URL: https://arxiv.org/abs/2511.10354
- Reference count: 30
- Primary result: ATR4CH methodology achieves F1-scores of 0.96-0.99 for metadata extraction and 0.62 G-EVAL for discourse representation in Cultural Heritage authenticity debates

## Executive Summary
ATR4CH provides a systematic methodology for converting complex scholarly discourse from Cultural Heritage texts into structured Knowledge Graphs using LLMs. The approach combines annotation models, ontological frameworks, and iterative development through five steps: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and evaluation. Validation on authenticity assessment debates achieved F1-scores of 0.96-0.99 for metadata extraction, 0.7-0.8 for entity recognition, 0.65-0.75 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, demonstrating cost-effective deployment potential for Cultural Heritage institutions. The methodology successfully coordinates LLM-based extraction with Cultural Heritage ontologies, providing a replicable framework adaptable across domains requiring extraction of multi-perspectival interpretative knowledge.

## Method Summary
The ATR4CH methodology follows a five-step process: (1) foundational analysis identifies scholarly discourse patterns in pilot documents; (2) annotation schema development creates Minimal Working Annotation schemas validated through pilot corpus annotation; (3) pipeline architecture implements a six-stage sequential process with three LLMs (metadata extraction → cognizer identification → entity resolution → opinion extraction → evidence mining → hypothesis extraction); (4) integration refinement validates annotation-to-RDF mappings; (5) evaluation measures performance against ground truth using F1 and G-EVAL metrics. The pipeline produces JSON outputs validated before downstream processing, reducing error propagation while maintaining schema constraints for structured extraction.

## Key Results
- Metadata extraction achieved F1-scores of 0.96-0.99 across all tested LLM models
- Entity recognition achieved F1-scores of 0.70-0.80 with GPT-4o-mini outperforming larger models on cognizer identification
- Hypothesis extraction achieved F1-scores of 0.65-0.75 with GPT-4o-mini achieving highest overall F1-score of 0.749
- Evidence extraction achieved F1-scores of 0.95-0.97 with minimal variance across models
- G-EVAL score of 0.62 indicates successful multi-perspectival discourse representation in Knowledge Graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential pipeline decomposition reduces error propagation by isolating extraction tasks into verifiable intermediate outputs.
- Mechanism: Six components (metadata extraction → cognizer identification → entity resolution → opinion extraction → evidence mining → hypothesis extraction) each produce JSON outputs that can be validated before downstream processing, preventing compounding errors.
- Core assumption: Smaller, task-specific prompts yield more reliable structured outputs than monolithic extraction attempts.
- Evidence anchors:
  - [abstract] "implementing a sequential pipeline with three LLMs"
  - [Section 5.1] "Each component produces a JSON, each with a given schema designed to be convertible to RDF"
  - [corpus] Weak direct evidence; neighbor papers focus on cultural alignment rather than pipeline architecture.
- Break condition: If entity recognition (Step 2) misses cognizers, downstream evidence extraction produces empty outputs regardless of model quality.

### Mechanism 2
- Claim: Annotation-to-RDF co-development ensures extraction outputs map to valid ontological structures.
- Mechanism: Minimal Working Annotation schemas are iteratively refined through pilot corpus annotation and preliminary RDF mapping validation before pipeline implementation, guaranteeing that extracted entities have ontological destinations.
- Core assumption: Domain experts can reliably identify scholarly discourse patterns in pilot documents that generalize to the broader corpus.
- Evidence anchors:
  - [Section 3.3] "These preliminary mapping exercises are crucial for validating that the annotation schema can produce the target knowledge structures"
  - [Section 4.4] Listing 1 shows annotation-to-RDF mapping algorithm validated on pilot corpus
  - [corpus] No direct corpus evidence for this annotation-pipeline coordination mechanism.
- Break condition: If annotation schema omits patterns present in full corpus but absent from pilot, extraction will fail silently.

### Mechanism 3
- Claim: Smaller models achieve competitive performance when guided by structured schemas and few-shot examples within modular pipelines.
- Mechanism: JSON schema constraints and 3-shot in-context examples provide sufficient task specification for models with fewer parameters to match larger models on structured extraction, reducing deployment costs.
- Core assumption: Task decomposition into schema-constrained subtasks reduces the reasoning burden that typically requires larger models.
- Evidence anchors:
  - [abstract] "Smaller models performed competitively, demonstrating cost-effective deployment potential"
  - [Section 6.4] "GPT-4o-mini achieves the highest overall F1-score (0.749) for hypothesis extraction"
  - [corpus] "Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction" suggests SFT can further improve smaller models, but no direct comparison.
- Break condition: Tasks requiring deep domain reasoning (e.g., novel authenticity arguments) may still require larger models.

## Foundational Learning

- Concept: RDF-star and reification for statement-level metadata
  - Why needed here: SEBI ontology uses RDF-star quoted triples to represent competing authenticity claims with provenance, enabling multi-perspectival knowledge representation.
  - Quick check question: Can you explain how RDF-star differs from standard reification for attaching metadata to triples?

- Concept: Named Graphs and interpretation acts
  - Why needed here: Each scholarly opinion is wrapped in a hico:InterpretationAct Named Graph, isolating conflicting claims while maintaining queryability.
  - Quick check question: How would a SPARQL query retrieve all opinions about a document's authenticity while preserving the scholar attribution?

- Concept: In-Context Learning (ICL) with structured outputs
  - Why needed here: Pipeline relies on few-shot prompting with JSON schemas; understanding ICL failure modes is critical for debugging extraction quality.
  - Quick check question: What happens to extraction quality when demonstration examples differ in document structure from target texts?

## Architecture Onboarding

- Component map: GliNER (NER) → LLM API (extraction) → Wikibase API (entity linking) → Mapping scripts (JSON→RDF-star) → GraphDB (storage/queries). Ground truth via INCEpTION annotation platform.

- Critical path: Cognizer identification (Step 5.1.2) is the bottleneck—incorrect entity classification propagates as empty evidence/hypothesis outputs. GPT-4o-mini achieved 77.3% recall vs. Claude's 49.5%, directly affecting downstream F1.

- Design tradeoffs: Higher-recall models (GPT-4o-mini) generate more false positives requiring human post-processing; higher-precision models (Claude) miss valid entities but require less correction. Choice depends on whether KGs undergo human review (prefer recall) or feed RAG systems directly (prefer precision).

- Failure signatures:
  - Empty evidence JSON → cognizer classification failed; check GliNER spans and opinion-classification prompt
  - Wikidata linking drops common entities → Levenshtein threshold too strict or P106 occupation filter missing scholarly types
  - RDF validation errors → JSON schema drift from annotation model updates

- First 3 experiments:
  1. Replicate cognizer identification on 3 pilot articles using provided prompts; measure precision/recall against ground truth in GitHub repository.
  2. Ablate few-shot examples (0-shot vs. 3-shot) on metadata extraction to quantify ICL contribution to F1=0.97 performance.
  3. Test entity linking with/without Wikidata occupation filtering to identify precision/recall tradeoff on historical scholar names.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ATR4CH methodology generalize to primary scholarly literature and non-English, multilingual Cultural Heritage texts?
- Basis in paper: [explicit] The authors explicitly identify these as limitations, stating their "current focus on English Wikipedia sources limits multilingual applicability" and that "performance on primary scholarly literature remains untested."
- Why unresolved: The methodology was validated exclusively on English Wikipedia articles, which represent secondary, synthesized discourse, and the validation corpus was monolingual.
- What evidence would resolve it: A new validation study applying the ATR4CH pipeline to primary sources (e.g., original scholarly papers) and a multilingual corpus, comparing extraction performance against the Wikipedia-based benchmarks established in this paper.

### Open Question 2
- Question: Can targeted improvements to the "Cognizer classification" component resolve the identified error propagation bottleneck without requiring extensive re-training?
- Basis in paper: [explicit] The authors identify two key bottlenecks: "Cognizer classification difficulty and dependency on Wikidata linking for optimal performance," and note that "future work will prioritize...implementing targeted improvements for Cognizer identification."
- Why unresolved: The paper demonstrates that error in Cognizer identification propagates downstream, but the proposed solutions (fine-tuning, enhanced pre-processing) are outlined for future work and not tested.
- What evidence would resolve it: A comparative experiment integrating a specialized frame recognition model or a fine-tuned classifier for the Cognizer identification step, measuring the resulting change in F1-scores for downstream tasks like hypothesis and evidence extraction.

### Open Question 3
- Question: How can self-consistency checks be integrated into the extraction pipeline to reduce false positives and false negatives without undermining the need for external validation?
- Basis in paper: [explicit] Based on evaluation results, the authors suggest "self-consistency checks throughout the pipeline (such as prompting models to evaluate their own extraction results) could reduce FPs and FNs without reducing the necessity of external validation."
- Why unresolved: The authors propose this as an insight derived from their evaluation but it is not implemented or tested in the current work.
- What evidence would resolve it: An ablation study incorporating a self-evaluation prompt at one or more pipeline stages, comparing the precision and recall of the modified pipeline against the original on the same Ground Truth.

## Limitations
- Methodology validated only on Wikipedia articles about authenticity debates, limiting generalizability to other Cultural Heritage domains or non-English texts
- Pipeline architecture creates error propagation risks where cognizer identification failures cause downstream evidence and hypothesis extraction to produce empty outputs
- Wikidata entity linking coverage limitations may require fallback mechanisms for historical scholars and obscure cultural artifacts not present in the knowledge base

## Confidence
**High Confidence**: Metadata extraction performance (F1 0.96-0.99) and evidence extraction (F1 0.95-0.97) results are well-supported by the methodology's structured approach and validation procedures. The sequential pipeline design and schema-constrained extraction are clearly articulated.

**Medium Confidence**: Entity recognition (F1 0.70-0.80) and hypothesis extraction (F1 0.65-0.75) results depend heavily on the cognizer identification accuracy, which showed significant variance across models (GPT-4o-mini: 77.3% recall vs Claude: 49.5%). The methodology's dependence on this bottleneck component reduces confidence in downstream performance.

**Low Confidence**: The G-EVAL score of 0.62 for discourse representation lacks detailed breakdown by component, making it difficult to assess whether the score reflects successful multi-perspectival knowledge representation or other factors. The paper does not specify whether this score accounts for the complexity of representing competing scholarly claims.

## Next Checks
1. **Pilot Corpus Expansion Validation**: Test the annotation schema and extraction pipeline on 10 additional Wikipedia articles from the remaining 574-article corpus to assess whether pilot corpus patterns generalize. Measure schema coverage and extraction performance variance across the broader corpus.

2. **Error Propagation Analysis**: Conduct controlled ablation studies where cognizer identification is deliberately corrupted at different rates (10%, 25%, 50%) to quantify downstream impact on evidence and hypothesis extraction. This will validate whether the cognizer bottleneck truly determines pipeline performance.

3. **Alternative Domain Transfer**: Apply the ATR4CH methodology to a different CH domain (e.g., conservation reports or exhibition catalogs) with distinct discourse patterns. Compare extraction performance and identify necessary annotation schema adaptations, testing the methodology's domain-transferability claim.