---
ver: rpa2
title: 'A generalized approach to label shift: the Conditional Probability Shift Model'
arxiv_id: '2503.02583'
source_url: https://arxiv.org/abs/2503.02583
tags:
- data
- shift
- distribution
- cpsm
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Conditional Probability Shift (CPS) model
  to address distribution shifts where the conditional probability of a class label
  given specific features changes, while the distribution of remaining features given
  the label and specific features remains unchanged. The proposed method, CPSM, uses
  multinomial regression to model conditional probabilities and employs the Expectation-Maximization
  algorithm to estimate parameters for target data, where class labels are unobserved.
---

# A generalized approach to label shift: the Conditional Probability Shift Model

## Quick Facts
- arXiv ID: 2503.02583
- Source URL: https://arxiv.org/abs/2503.02583
- Authors: Paweł Teisseyre; Jan Mielniczuk
- Reference count: 40
- Primary result: CPSM achieves superior balanced classification accuracy and smaller approximation errors compared to existing methods under conditional distribution shifts.

## Executive Summary
This paper introduces the Conditional Probability Shift (CPS) model to address distribution shifts where the conditional probability of a class label given specific features changes, while the distribution of remaining features given the label and specific features remains unchanged. The proposed method, CPSM, uses multinomial regression to model conditional probabilities and employs the Expectation-Maximization algorithm to estimate parameters for target data, where class labels are unobserved. Experiments on synthetic datasets and the MIMIC medical database demonstrate that CPSM achieves superior balanced classification accuracy and smaller approximation errors compared to existing methods, particularly in scenarios with conditional distribution shifts but no change in prior label distribution. The method is robust to sample size and class prior probabilities and is computationally faster than competing approaches.

## Method Summary
CPSM addresses Conditional Probability Shift (CPS) where p(y|z) changes between source and target domains but p(x|y,z) remains invariant. The method trains a base classifier on source data to estimate p(y|x,z), then uses EM to estimate target conditional probabilities q(y|z) without observing target labels. The E-step computes expected posteriors using a ratio transformation derived from Bayes' theorem, while the M-step updates multinomial regression parameters via gradient descent. The approach is classifier-agnostic, requiring only calibrated probability estimates from the base model.

## Key Results
- CPSM achieves superior balanced classification accuracy compared to existing methods under CPS scenarios
- The method demonstrates smaller approximation errors in estimating target posterior probabilities
- CPSM shows robustness to sample size variations and different class prior probabilities
- Computational speed is faster than competing approaches like BBSC and ExTRA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target posterior probabilities can be recovered from source posteriors via a ratio transformation.
- Mechanism: Theorem 1 establishes that q(y=k|x,z) = [p(y=k|x,z) · q(y=k|z)/p(y=k|z)] / [Σ_l p(y=l|x,z) · q(y=l|z)/p(y=l|z)]. This decomposition separates what changes (q(y|z)) from what stays fixed (p(x|y,z)).
- Core assumption: p(x|y,z) = q(x|y,z) — the conditional distribution of remaining features given label and conditioning features is invariant.
- Evidence anchors:
  - [section 2, Theorem 1]: Full derivation using Bayes' theorem and the CPS assumption.
  - [abstract]: States CPS captures "conditional distribution of the class variable given some specific features changes while the distribution of remaining features given the specific features and the class is preserved."
  - [corpus]: Related work "Factorizable joint shift revisited" discusses decomposition of joint shifts but assumes different factorization structure.
- Break condition: If p(x|y|z) ≠ q(x|y|z), the ratio r(x,z) no longer cancels and the formula yields biased posteriors.

### Mechanism 2
- Claim: Multinomial regression parameters for target distribution can be estimated via EM without observing labels.
- Mechanism: Treat y as latent. E-step computes expected log-likelihood using current q̂(y|x,z) from Theorem 1. M-step maximizes this expectation w.r.t. θ, reducing to weighted multinomial logistic regression. Theorem 2 guarantees non-decreasing marginal log-likelihood at each iteration.
- Core assumption: q(y|z) follows a multinomial logistic model; initial parameter estimates are in the basin of attraction.
- Evidence anchors:
  - [section 3, equations 3-4]: Full specification of softmax parameterization and log-likelihood decomposition.
  - [section 3, Theorem 2]: Proves L_obs(θ^(t+1)) ≥ L_obs(θ^(t)).
  - [corpus]: Limited direct corpus evidence for this specific EM formulation; related methods (MLLS) use EM for label priors rather than conditional parameters.
- Break condition: If the multinomial model is misspecified for q(y|z), or if EM converges to a local optimum far from θ*, estimated posteriors will be biased.

### Mechanism 3
- Claim: The method is classifier-agnostic because it only requires calibrated posterior estimates p(y|x,z).
- Mechanism: CPSM estimates q(y|z) and applies Theorem 1 as a post-hoc correction to any base classifier's outputs. No retraining of the feature-to-label mapping is needed, unlike importance-weighting methods (BBSC, ExTRA).
- Core assumption: The base classifier produces well-calibrated probability estimates for p(y|x,z).
- Evidence anchors:
  - [section 3]: "The proposed method is generic and can be combined with any probabilistic classifier."
  - [section 4.1]: Experiments use both logistic regression and neural networks as base classifiers.
  - [corpus]: "A Generalized Label Shift Perspective" similarly proposes classifier-agnostic corrections but for different shift types.
- Break condition: If the base classifier is poorly calibrated (e.g., overconfident neural networks), the corrected posteriors inherit this miscalibration. Consider applying BCTS calibration first.

## Foundational Learning

- Concept: Expectation-Maximization (EM) Algorithm
  - Why needed here: The target labels are unobserved; EM provides a principled framework for maximum likelihood estimation with latent variables.
  - Quick check question: Can you explain why the E-step computes an expectation over the posterior rather than the prior?

- Concept: Multinomial Logistic Regression (Softmax)
  - Why needed here: Used to parameterize both p(y|z) and q(y|z); understanding its log-likelihood and gradient is essential for implementing the M-step.
  - Quick check question: For K classes, why do we only estimate K-1 parameter vectors?

- Concept: Distribution Shift Taxonomy (CS, LS, SJS, CPS)
  - Why needed here: Identifying which shift type applies determines which correction method is valid. CPS generalizes label shift by allowing p(y|z) ≠ q(y|z).
  - Quick check question: In the medical example (Figure 1), why does q(Disease) = p(Disease) even though q(Disease|Age) ≠ p(Disease|Age)?

## Architecture Onboarding

- Component map: Source Training -> Base Classifier Training -> EM Adaptation Loop -> Target Prediction
- Critical path: The accuracy of p(y|x,z) on source → calibration of base classifier → convergence of EM for q(y|z) → quality of corrected posteriors
- Design tradeoffs:
  - Base classifier complexity: Logistic regression is interpretable and fast but may underfit; neural networks capture more structure but require more data and careful calibration
  - Convergence criteria: Fixed 500 iterations used in experiments; early stopping based on log-likelihood change could save computation but risks premature convergence
  - Choice of z: Domain knowledge is critical. If z is uninformative for the shift, CPSM reduces to LS methods; if z is high-dimensional, multinomial regression may overfit
- Failure signatures:
  - No improvement over naive: Check if p(y|z) ≈ q(y|z) (no conditional shift) or if z is poorly chosen
  - Divergent EM: Log-likelihood decreasing suggests implementation error in M-step or numerical instability in softmax
  - High approximation error with low accuracy gain: Base classifier may be miscalibrated
- First 3 experiments:
  1. Synthetic validation: Replicate the binary classification setup (Artificial Data 1/2) with known k parameter to verify EM recovers ground-truth θ* and balanced accuracy improves over naive
  2. Ablation on z: Run CPSM with z = age vs. z = gender vs. z = {age, gender} on MIMIC to assess sensitivity to conditioning variable choice
  3. Assumption stress test: Inject controlled violations of p(x|y,z) = q(x|y,z) (e.g., add covariate shift to x) and measure degradation in approximation error and balanced accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can one develop a statistical hypothesis test to determine if a Conditional Probability Shift (CPS) is statistically significant between source and target domains?
- Basis in paper: [explicit] The Conclusion states, "It would be valuable to test whether the shift in the conditional distributions is statistically significant, e.g. by testing whether the parameters of the multinomial model for the source and target data differ."
- Why unresolved: The current paper focuses on parameter estimation and classification correction using the EM algorithm, but does not provide a mechanism for detecting the shift or validating its significance before applying the correction.
- What evidence would resolve it: A derived test statistic with a known distribution under the null hypothesis (no shift) that allows for calculating p-values regarding the equality of multinomial parameters.

### Open Question 2
- Question: Can an automated method be developed to identify the optimal subset of conditioning features $z$ that satisfy the CPS assumption from data?
- Basis in paper: [explicit] The authors list as a problem of interest: "identify a set of conditioning variables z for which it [assumption (1)] holds, based on data."
- Why unresolved: The experiments assume $z$ (e.g., age, gender) is known a priori, but in high-dimensional settings, manually selecting features that satisfy $p(x|y, z) = q(x|y, z)$ is infeasible.
- What evidence would resolve it: An algorithm capable of selecting feature subsets that minimize the divergence between source and target conditional distributions $p(x|y, z)$ and $q(x|y, z)$.

### Open Question 3
- Question: Is the CPSM procedure robust to small violations of the core assumption that the conditional distribution of remaining features is preserved ($p(x|y, z) = q(x|y, z)$)?
- Basis in paper: [explicit] The Conclusion explicitly notes that "checking whether considered procedures are robust to small violations of (1) is desirable."
- Why unresolved: The theoretical derivation (Theorem 1) and synthetic experiments rely strictly on the assumption holding true, whereas real-world data rarely satisfies such constraints perfectly.
- What evidence would resolve it: Sensitivity analysis showing the degradation rate of Balanced Accuracy and Approximation Error as the noise or drift in $p(x|y, z)$ increases.

## Limitations
- The CPS assumption (p(x|y,z) = q(x|y,z)) is not directly validated in experiments; only the practical success of the method is shown
- MIMIC preprocessing details are sparse, making exact reproduction challenging without external references
- The robustness of the method to violations of the CPS assumption is not thoroughly explored

## Confidence
- **High Confidence:** The theoretical derivation of Theorem 1 and the EM algorithm for CPS (Mechanism 1 and 2) are mathematically sound
- **Medium Confidence:** The experimental results demonstrate superior performance, but the extent to which this is due to the CPS assumption vs. the flexibility of the EM approach is unclear
- **Low Confidence:** The claim of being "classifier-agnostic" relies on the base classifier being well-calibrated, which is not universally true

## Next Checks
1. **Assumption Stress Test:** Intentionally violate p(x|y,z) = q(x|y,z) in synthetic data and measure the degradation in balanced accuracy and approximation error
2. **Calibration Sensitivity:** Compare CPSM performance with calibrated vs. uncalibrated base classifiers (e.g., apply Platt scaling or temperature scaling)
3. **MIMIC Replication:** Replicate the MIMIC experiments with detailed preprocessing steps to verify the reported performance gains