---
ver: rpa2
title: Lost without translation -- Can transformer (language models) understand mood
  states?
arxiv_id: '2512.00274'
source_url: https://arxiv.org/abs/2512.00274
tags:
- been
- depression
- feeling
- euphoric
- euthymic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformer models fail to encode mood states expressed in Indic
  languages, achieving near-zero clustering performance (Composite Score = 0.002).
  All translation-based approaches significantly improved performance, with the highest
  accuracy (84%) achieved when human-translated English text was further translated
  to Chinese and embedded using Qwen-3-embedding-8b (Composite = 0.67).
---

# Lost without translation -- Can transformer (language models) understand mood states?

## Quick Facts
- arXiv ID: 2512.00274
- Source URL: https://arxiv.org/abs/2512.00274
- Reference count: 40
- Transformer models achieve near-zero clustering performance (composite score = 0.002) on Indic language mood states, requiring translation to high-resource languages for meaningful performance.

## Executive Summary
This study reveals a fundamental limitation in current transformer models' ability to represent mood states expressed in Indic languages. When tested on idioms of distress from 11 Indian languages, direct embedding approaches failed catastrophically, achieving near-random clustering performance. All translation-based approaches showed dramatic improvements, with the best performance (84% accuracy) achieved through a multi-stage translation pipeline. The findings highlight a critical barrier to AI-based psychiatric applications in India and underscore the importance of culturally grounded language models trained on vernacular, emotionally rich data rather than formal corpora.

## Method Summary
The study evaluated transformer models' ability to cluster 247 mood-state-labeled phrases from 11 Indic languages into four categories (depression, euthymia, euphoric mania, dysphoric mania) using k-means clustering on embeddings. Seven experimental conditions tested both direct embedding of native/Romanized Indic text and translation-based approaches using various translation and embedding models. Performance was measured using a composite score combining Adjusted Rand Index (50%), Normalized Mutual Information (40%), Homogeneity (5%), and Completeness (5%).

## Key Results
- Direct embedding of native Indic scripts failed to cluster mood states (composite score = 0.002)
- All translation-based approaches significantly improved performance, with human translation + Gemini embedding achieving composite score of 0.616
- Best performance (84% accuracy) achieved with Indic → Human English → Chinese pipeline using Qwen-3-embedding-8b
- IndicBERT, despite being specialized for Indic languages, performed no better than generic multilingual models on native scripts

## Why This Works (Mechanism)

### Mechanism 1: Translation-to-High-Resource-Language as Semantic Bridge
Translation transfers semantic intent into representation spaces where embedding models have stronger emotional grounding. High-resource languages have models trained on richer semantic representations of emotional concepts.

### Mechanism 2: Embedding Dimension Enables Nuanced Semantic Representation
Larger embedding dimensions (3072 vs 768) provide more expressive capacity for distinguishing subtle mood-state differences in idioms of distress, encoding finer-grained semantic distinctions.

### Mechanism 3: Native-Language Training Depth Determines Semantic Encoding Quality
Models with extensive native-language training (Qwen on Chinese) encode emotional semantics more faithfully than multilingual models with superficial coverage, as deep training on vernacular text builds semantic representations that capture idioms of distress.

## Foundational Learning

- **Semantic Embeddings vs. Surface Encoding**: Character-level processing ≠ semantic understanding. Quick check: If you embed "dil toot gaya" (heart broken) in Romanized Hindi, would you expect clustering with English "heartbroken" or with other Hindi phrases regardless of meaning?

- **K-Means Clustering as a Proxy for Semantic Fidelity**: Clustering performance measures whether embeddings capture semantic mood categories. Quick check: If embeddings perfectly captured mood, what would the confusion matrix heatmap look like vs. if they captured only language/script patterns?

- **Translation Pipeline Latency vs. Semantic Fidelity Trade-off**: Two-stage translation with human involvement achieves best results but is impractical for clinical deployment. Quick check: If building a real-time chatbot for Indian psychiatric screening, which trade-offs would you accept: faster but lower-accuracy direct embedding, or slower but more accurate translation-based processing?

## Architecture Onboarding

- **Component map**: Input Layer (Indic phrases) → Translation Module (optional) → Embedding Model (IndicBERT, Gemini, Qwen, sentence transformers) → Clustering Layer (K-means with k=4) → Evaluation Metrics (ARI, NMI, Homogeneity, Completeness)

- **Critical path**: Translation quality → Embedding dimension → Clustering accuracy. Translation quality dominates: human translation (0.616) > Gemini translation (0.601) > Sarvam translation (0.114).

- **Design tradeoffs**: Closed-source (Gemini) vs. Open-source (Sarvam, IndicBERT): 5-6x higher composite scores but introduces dependency/cost. Direct embedding vs. Translation pipeline: translation adds latency but is currently required. Single-stage vs. Multi-stage translation: Indic → English → Chinese achieved best (0.671) but is computationally inefficient.

- **Failure signatures**: Composite score < 0.01 indicates direct Indic embedding failure. Composite score 0.10-0.15 indicates insufficient translation quality. Classification accuracy ~31% indicates near-random performance across 4 classes.

- **First 3 experiments**: 1) Reproduce Condition 2 (native script + IndicBERT) and Condition 6 (human translation + Gemini) to confirm the ~300x performance gap. 2) Compare Sarvam translation + Gemini embedding vs. Gemini translation + Gemini embedding to isolate translation quality effects. 3) Compute per-language composite scores under Conditions 2 and 6 to test Hindi/Bengali vs. low-resource languages performance differences.

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific training data deficiencies that cause specialized Indic language models to fail at encoding emotional content in idioms of distress? The authors state these models are trained on "formal corpora" that "lack the real-world, vernacular, and emotionally rich data required to represent nuances in idioms of distress."

### Open Question 2
Why does the Indic→English→Chinese translation pipeline outperform direct English translation, and does this advantage generalize to other non-English target languages? The authors note it was "surprising" that human-translated English further translated to Chinese performed best.

### Open Question 3
Can mood state clustering performance in Indic languages be improved by fine-tuning embedding models specifically on psychiatric/clinical text rather than general-purpose corpora? No fine-tuning experiments were conducted; all tested models were used off-the-shelf.

### Open Question 4
Do the findings from crowdsourced, hypothetical patient phrases generalize to actual clinical interview data with real patients? The authors acknowledge their dataset is small and obtained via crowdsourcing rather than direct clinical interviews.

## Limitations

- **Proprietary Model Dependency**: Best performance relies on inaccessible Gemini-2.5-Pro and Qwen-3-embedding-8b models, creating barriers to scalable deployment in resource-constrained settings.

- **Translation Quality Assumptions**: Study assumes translation preserves semantic and emotional nuance without empirical validation, despite evidence that translation can "flatten or erase emotional content."

- **Cultural Context Loss**: Paper acknowledges idioms of distress are culture-specific but does not measure whether translation introduces semantic drift or whether certain idioms are untranslatable without cultural context.

## Confidence

- **High Confidence**: Direct Indic embedding fails (composite = 0.002) and all translation-based approaches show significant improvement.
- **Medium Confidence**: Larger embedding dimensions enable better semantic representation, though causal mechanism linking dimension to emotional nuance is not directly tested.
- **Low Confidence**: Native-language training depth determines semantic encoding quality; claim is speculative based on comparative performance rather than corpus analysis.

## Next Checks

1. **Translation Quality Ablation**: Systematically compare machine translation + Gemini embedding vs. human translation + Gemini embedding across all 247 phrases to quantify the emotional nuance preservation gap.

2. **Cross-Linguistic Performance Analysis**: For each of the 11 Indic languages, compute per-language composite scores under Conditions 2 and 6 to identify whether resource level correlates with performance.

3. **Cultural Context Validation**: Select 20 idioms from the dataset that are most culturally specific and test whether human translators can accurately convey the emotional content to non-native speakers.