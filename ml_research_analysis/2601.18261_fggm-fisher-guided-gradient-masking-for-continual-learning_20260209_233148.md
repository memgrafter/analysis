---
ver: rpa2
title: 'FGGM: Fisher-Guided Gradient Masking for Continual Learning'
arxiv_id: '2601.18261'
source_url: https://arxiv.org/abs/2601.18261
tags:
- fggm
- learning
- migu
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  for large language models (LLMs). The authors propose Fisher-Guided Gradient Masking
  (FGGM), a framework that uses diagonal Fisher Information to strategically select
  parameters for updates.
---

# FGGM: Fisher-Guided Gradient Masking for Continual Learning

## Quick Facts
- **arXiv ID**: 2601.18261
- **Source URL**: https://arxiv.org/abs/2601.18261
- **Reference count**: 0
- **Primary result**: FGGM improves LLM continual learning by 9.6% relative General capability retention over SFT and 4.4% over MIGU on TRACE benchmark

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for large language models (LLMs) through Fisher-Guided Gradient Masking (FGGM). The framework uses diagonal Fisher Information Matrix to strategically select parameters for updates during sequential fine-tuning. FGGM dynamically generates binary masks with adaptive thresholds to preserve critical parameters while maintaining plasticity for new tasks, all without requiring historical data. On the TRACE benchmark, FGGM demonstrates significant improvements in retaining general capabilities while learning new tasks sequentially.

## Method Summary
FGGM operates through a two-phase approach: First, it computes diagonal Fisher Information Matrix (FIM) using new task data to estimate parameter importance without historical data. The FIM is aggregated across input dimensions per output neuron to preserve functional units, then thresholded using adaptive quantile-based binarization (α=0.7 retains top 30% parameters). Second, during training, gradients are masked using these binary masks before parameter updates. The method uses standard cross-entropy loss, AdamW optimizer with BF16 precision, learning rate of 1e-5 with linear decay, and applies masking per weight matrix while skipping bias terms.

## Key Results
- FGGM achieves 9.6% relative improvement in General capability retention over supervised fine-tuning (SFT) on TRACE benchmark
- FGGM shows 4.4% improvement over MIGU on TRACE tasks while maintaining general capabilities
- Ablation studies confirm input-dimension aggregation significantly improves performance (55.75→54.85→53.16/43.15 for IA vs OA vs none)

## Why This Works (Mechanism)

### Mechanism 1: Fisher Information as Parameter Importance Proxy
Diagonal FIM values estimate how sensitive task loss is to parameter perturbations, enabling identification of critical parameters without accessing historical data. High Fisher scores indicate parameters where small perturbations cause large changes in model likelihood. The core assumption is that parameter importance for current task correlates with importance for prior tasks.

### Mechanism 2: Adaptive Quantile-Based Binary Masking
Dynamic thresholding via quantile-based binarization provides layer-adaptive parameter selection. Rather than using absolute thresholds, FGGM computes the (1-α)-th quantile per task, marking parameters above this threshold for updates. With α=0.7, only top 30% of parameters by Fisher score receive gradient updates.

### Mechanism 3: Input-Dimension Aggregation for Functional Unit Preservation
Aggregating FIM scores across input dimensions (per output neuron) protects functionally cohesive parameter groups. For weight matrix W ∈ ℝ^(D_out × D_in), FIM values are summed across columns, so masking decisions apply to entire output neuron pathways rather than individual weights.

## Foundational Learning

- **Fisher Information Matrix**: FIM as curvature/sensitivity measure for parameter importance estimation. Quick check: Given a parameter with Fisher score 10x higher than another, what does this imply about loss sensitivity to perturbations?

- **Catastrophic Forgetting in Sequential Fine-Tuning**: Why gradient updates overwrite prior knowledge. Quick check: Why does standard SGD on task T_t degrade performance on T_{1:t-1} even with clean abundant data?

- **Stability-Plasticity Tradeoff**: Balancing competing objectives of retaining old knowledge while learning new tasks. Quick check: If α=0.9 (top 10% parameters updated), what happens to stability vs plasticity compared to α=0.5?

## Architecture Onboarding

- **Component map**: Input → FIM Estimation (gradients → squared gradients → aggregation → quantile threshold) → Gradient Masked Training (forward → backward → mask application → update) → Output

- **Critical path**: FIM computation correctness → mask quality → gradient projection accuracy. Errors in FIM estimation propagate directly to incorrect parameter protection decisions.

- **Design tradeoffs**:
  - Hard vs Soft Masking: Binary masks offer strict stability; soft masks provide smoother plasticity but risk gradual forgetting
  - Masking Rate α: Lower α → higher plasticity, lower stability (α=0.5→53.81 General, α=0.9→57.65 General but TRACE-OP drops 49.56→40.12)
  - Online vs Offline FIM: Current uses offline batch computation; online could reduce latency but may introduce estimation variance

- **Failure signatures**:
  - Excessive forgetting despite FGGM: Check if α too low or FIM computation failed (gradients near zero)
  - Poor plasticity (task not learned): Check if α too high or FIM incorrectly identifies most parameters as important
  - Mask degradation across tasks: If masks become overly sparse or dense, consider mask consolidation or reset strategies

- **First 3 experiments**:
  1. Baseline validation: Replicate Table 1 results (FGGM vs SFT/MIGU on TRACE benchmark with Qwen2-1.5B)
  2. Ablation on aggregation strategy: Compare IA vs OA vs no aggregation to confirm IA benefits
  3. α sensitivity sweep: Run α ∈ {0.5, 0.6, 0.7, 0.8, 0.9} on held-out task to characterize stability-plasticity frontier

## Open Questions the Paper Calls Out

### Open Question 1
Can FGGM be effectively extended to multi-modal continual learning scenarios? The conclusion states plans to "explore its applicability to multi-modal fine-tuning scenarios." This remains unresolved as FGGM was only validated on text-based LLMs; multi-modal models involve heterogeneous parameter types and cross-modal interactions requiring different importance estimation strategies.

### Open Question 2
Can online Fisher Information Matrix computation maintain FGGM's performance while improving efficiency? The paper mentions transitioning from offline to online FIM for efficiency but doesn't implement or evaluate this approach. Online FIM may introduce noise and temporal instability compared to current offline batch computation.

### Open Question 3
Can the masking rate α be automatically determined per task rather than manually specified? Table 2 shows α significantly impacts stability-plasticity tradeoff, yet α=0.7 is used throughout "for fair comparison with MIGU" without adaptive mechanisms. Different tasks may have varying complexity suggesting optimal α values could differ across tasks.

### Open Question 4
Would soft-masking with continuous Fisher-guided weights provide better plasticity-stability trade-offs than current binary hard-masking? The paper acknowledges soft-masking "offers balanced plasticity-stability tradeoffs" but adopts hard-masking to "preserve stability and minimize computational overhead" without empirical comparison.

## Limitations

- Diagonal FIM approximation ignores parameter correlations that may be critical for capturing task-specific functional dependencies
- Single α value validation doesn't comprehensively validate stability-plasticity tradeoff across different α values
- Task ordering sensitivity not explored despite catastrophic forgetting being highly sensitive to task sequence

## Confidence

**High Confidence**:
- Diagonal FIM computation methodology (Eq. 3) is clearly specified and follows standard practice
- Input-dimension aggregation strategy (Eq. 5) and implementation are well-defined
- General improvement metrics (9.6% over SFT, 4.4% over MIGU) are explicitly reported

**Medium Confidence**:
- Superiority of adaptive quantile-based masking over fixed thresholds is claimed but lacks direct comparative validation
- Claim that FGGM "establishes an effective solution" without historical data is supported by TRACE results but not stress-tested on diverse task sequences

**Low Confidence**:
- Mechanism by which input-dimension aggregation preserves "functional units" lacks empirical validation
- Claims about noise resistance and layer-adaptive parameter selection are theoretical without quantitative supporting evidence

## Next Checks

1. **α Sensitivity Analysis**: Systematically vary α ∈ {0.5, 0.6, 0.7, 0.8, 0.9} across multiple task sequences and plot stability-plasticity tradeoff curves to identify optimal α ranges and validate choice of α=0.7.

2. **Task Ordering Robustness**: Replicate TRACE experiments with randomized task orders (minimum 5 different sequences) to quantify performance variance and identify whether FGGM exhibits ordering sensitivity that could limit practical deployment.

3. **Diagonal vs Full FIM Comparison**: Implement sparse approximation of full FIM (e.g., k-nearest parameter correlations) and compare against diagonal FIM on subset of tasks to empirically validate whether ignoring parameter correlations significantly impacts performance.