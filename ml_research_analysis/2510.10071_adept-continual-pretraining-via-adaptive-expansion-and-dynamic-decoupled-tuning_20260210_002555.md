---
ver: rpa2
title: 'ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled
  Tuning'
arxiv_id: '2510.10071'
source_url: https://arxiv.org/abs/2510.10071
tags:
- layers
- layer
- importance
- general
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ADEPT addresses catastrophic forgetting in continual pretraining
  of large language models by leveraging functional specialization within the network.
  It uses a two-stage approach: first, it selectively duplicates the least general-knowledge-critical
  layers to expand capacity without disrupting existing knowledge; second, it decouples
  parameters within expanded layers and assigns adaptive learning rates inversely
  to their general-domain importance.'
---

# ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning

## Quick Facts
- **arXiv ID:** 2510.10071
- **Source URL:** https://arxiv.org/abs/2510.10071
- **Reference count:** 40
- **Primary result:** Outperforms full-parameter pretraining by up to 5.58% on target-domain tasks while tuning only 15% of parameters

## Executive Summary
ADEPT addresses catastrophic forgetting in continual pretraining of large language models through a novel two-stage approach that leverages functional specialization within transformer networks. The method first selectively duplicates the least general-knowledge-critical layers to expand model capacity without disrupting existing knowledge, then decouples parameters within expanded layers and assigns adaptive learning rates inversely proportional to their general-domain importance. This approach enables effective domain knowledge injection while preserving general competencies, achieving performance gains of up to 5.76% on general-domain tasks and 5.58% on target-domain benchmarks.

## Method Summary
ADEPT operates through adaptive layer expansion followed by dynamic decoupled tuning. The first stage uses L2 norm analysis to identify layers critical for general-domain knowledge, then selectively duplicates the least critical layers to increase model capacity. The second stage partitions expanded layers into groups based on their general-domain importance scores, assigning lower learning rates to more important parameters while allowing less critical parameters to adapt more freely to new domain knowledge. This selective tuning strategy minimizes interference with preserved knowledge while enabling efficient domain adaptation, achieving parameter efficiency by only tuning approximately 15% of total parameters while maintaining or improving performance across both general and target domains.

## Key Results
- Achieves up to 5.58% performance improvement on target-domain benchmarks (GSM8K, CMB) compared to full-parameter pretraining
- Delivers 5.76% gains on general-domain tasks (MMLU, CMMLU) while preserving general knowledge
- Requires only 15% of parameters to be tuned and less than 50% of training time compared to full-parameter approaches

## Why This Works (Mechanism)
ADEPT exploits the functional specialization inherent in transformer architectures by recognizing that different layers encode different types of knowledge with varying importance. By analyzing L2 norms to identify knowledge-critical layers, the method preserves essential general-domain information while creating capacity for new domain knowledge through selective duplication. The dynamic decoupled tuning then allows differential adaptation rates based on parameter importance, protecting critical knowledge while enabling flexible learning of new information. This hierarchical approach to parameter adaptation addresses the fundamental tension between stability (preserving existing knowledge) and plasticity (learning new information) in continual learning scenarios.

## Foundational Learning

**Functional Specialization in Transformers:** Different transformer layers encode different types of information, with earlier layers capturing syntactic patterns and deeper layers encoding semantic abstractions. *Why needed:* Understanding this specialization is crucial for identifying which parameters are critical to preserve versus which can be adapted. *Quick check:* Verify that L2 norm analysis correlates with layer functional importance through ablation studies.

**Catastrophic Forgetting:** When neural networks are trained sequentially on different tasks, they tend to overwrite previously learned knowledge. *Why needed:* This is the core problem ADEPT addresses through its selective preservation strategy. *Quick check:* Measure performance degradation on general-domain tasks after target-domain training.

**Parameter Importance Scoring:** Assigning relative importance values to model parameters based on their contribution to overall performance. *Why needed:* Enables selective protection of critical knowledge while allowing adaptation of less important parameters. *Quick check:* Validate that importance scores correlate with actual forgetting susceptibility.

**Adaptive Learning Rates:** Dynamically adjusting learning rates for different parameter groups based on their importance or role. *Why needed:* Allows fine-grained control over the adaptation process to balance preservation and learning. *Quick check:* Compare performance with uniform versus adaptive learning rate schedules.

## Architecture Onboarding

**Component Map:** Input -> L2 Norm Analysis -> Layer Selection -> Expansion Module -> Parameter Decoupling -> Adaptive LR Assignment -> Output

**Critical Path:** The sequence from layer importance analysis through selective expansion to decoupled tuning represents the core innovation, with L2 norm computation being the most computationally intensive initial step that determines all subsequent operations.

**Design Tradeoffs:** The method trades initial computational overhead for long-term efficiency gains, accepting the cost of layer duplication and parameter partitioning to achieve superior performance with fewer total training steps and reduced parameter tuning.

**Failure Signatures:** Poor performance on general-domain tasks indicates over-aggressive adaptation rates; failure to improve on target domains suggests insufficient expansion or overly conservative learning rates; computational inefficiency may result from excessive layer duplication.

**First Experiments:** 1) Ablation study removing adaptive learning rates to quantify their individual contribution, 2) Testing different layer duplication strategies (random vs. importance-based), 3) Evaluating performance across varying degrees of parameter tuning (5%, 15%, 25%) to establish efficiency curves.

## Open Questions the Paper Calls Out

The paper acknowledges uncertainty about the generalizability of its adaptive expansion mechanism across diverse domain types, particularly for multimodal tasks or domains with drastically different token distributions. It also notes potential computational overhead during deployment despite training time reductions, and the need for validation across languages and model families beyond the English-language LLaMA and DeepSeek architectures studied.

## Limitations

The approach's effectiveness may not generalize beyond biomedical and code domains to fundamentally different architectural requirements like multimodal tasks. The two-stage process introduces deployment complexity that could affect real-world efficiency despite training time claims. Performance claims based on English-language benchmarks and specific model architectures may not translate directly to other languages or model families without additional validation.

## Confidence

- **Performance improvements on evaluated benchmarks:** High (supported by comprehensive experimental results across multiple tasks)
- **Catastrophic forgetting mitigation mechanism:** Medium (theoretically sound but limited to demonstrated domains)
- **Parameter efficiency claims:** Medium (dependent on specific model architectures and domain characteristics)
- **Generalizability across diverse domains:** Low (not extensively validated beyond presented cases)

## Next Checks

1. Test ADEPT on a broader range of domain types including multimodal and low-resource language scenarios to assess architectural generalization limits
2. Conduct ablation studies specifically isolating the contribution of adaptive learning rates versus layer expansion to quantify their individual impacts
3. Evaluate deployment performance metrics including inference latency and memory overhead to verify practical efficiency claims beyond training time reduction