---
ver: rpa2
title: Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought
arxiv_id: '2510.04230'
source_url: https://arxiv.org/abs/2510.04230
tags:
- arxiv
- reasoning
- preprint
- korean
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building strong reasoning
  models for mid-resource languages, specifically Korean, by overcoming limitations
  of translation artifacts and monolingual supervision. The authors propose Language-Mixed
  Chain-of-Thought (CoT), a reasoning schema that alternates between English (as an
  anchor) and Korean during the thinking phase, preserving reasoning power while maintaining
  linguistic and cultural fidelity.
---

# Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought

## Quick Facts
- arXiv ID: 2510.04230
- Source URL: https://arxiv.org/abs/2510.04230
- Reference count: 33
- Primary result: Language-Mixed CoT enables Korean reasoning models to achieve SOTA performance with cross-lingual and multi-modal transfer

## Executive Summary
This paper tackles the challenge of building strong reasoning models for mid-resource languages by introducing Language-Mixed Chain-of-Thought (CoT), a reasoning schema that alternates between English (as an anchor) and Korean during the thinking phase. The approach preserves reasoning capability while maintaining linguistic and cultural fidelity, addressing limitations of translation artifacts and monolingual supervision. The authors curate YI-SANG, the largest publicly documented Korean post-training dataset (5.79M prompts, 3.7M long reasoning traces), and train nine models (4B-35B) across six families. Their best model KO-REAson-35B achieves state-of-the-art performance with the highest overall average score (64.0 ± 2.5), ranking first on five of nine benchmarks. Smaller models also benefit substantially, with an average improvement of +18.6 points. Ablations confirm Language-Mixed CoT outperforms monolingual CoT, and cross-lingual/multi-modal gains are observed despite training only on Korean text.

## Method Summary
The method involves using a strong teacher model (Qwen3-32B) to generate Language-Mixed CoT reasoning traces on Korean prompts, where the reasoning alternates between English (for logic) and Korean (for context/terms). These traces are filtered using regex for Korean-character ratio (5-20%), loss spike detection via proxy model shakedown runs, and 13-gram decontamination against benchmarks. The filtered high-yield subset YI-SANG-HQ (260k instances) is used for SFT training on base models (Gemma, Llama, Qwen families) with FSDP/PyTorch, Liger Kernels, and FlashAttention-2. The approach excludes medical/daily data categories that harm general performance and uses only reasoning traces and solutions for loss computation.

## Key Results
- KO-REAson-35B achieves highest overall average score (64.0 ± 2.5) and ranks first on five of nine benchmarks
- Smaller models show substantial improvements with average +18.6 points over baselines
- Language-Mixed CoT outperforms monolingual CoT in ablations
- Cross-lingual and multi-modal transfer observed despite training only on Korean text
- YI-SANG dataset curated with rigorous decontamination and quality filtering

## Why This Works (Mechanism)

### Mechanism 1: Anchor Language Reasoning Preservation
Language-Mixed CoT preserves reasoning capability by leveraging English as a logical scaffold, preventing performance degradation observed when forcing reasoning into a lower-resource language. The student model utilizes English (the dominant pretraining language for many models) for the cognitive load of logic and computation, while retaining the target language (Korean) for context and terminology. This code-switching prevents the "distributional drift" that occurs when fine-tuning heavily on non-English reasoning traces. The model's internal reasoning representations are optimized for English due to pretraining data imbalances. This works because reasoning in English on Korean prompts preserves the model's internal alignment with pretraining distributions, whereas reasoning in Korean produces notable drops in reasoning capability.

### Mechanism 2: Semantic Fidelity via Context Preservation
Mixed CoT reduces "translation artifacts" and error accumulation by anchoring the reasoning to the original query context. By preserving key terms and quotes in the target language (Korean) within the reasoning chain, the model maintains a direct semantic link to the prompt. This mitigates the error propagation common in "translate-then-reason" pipelines where the prompt is fully translated before reasoning begins. Translation models introduce irrecoverable semantic shifts or "drift" that worsens over long reasoning chains. Language-Mixed CoT preserves reasoning power while maintaining linguistic and cultural fidelity.

### Mechanism 3: Cross-Lingual and Multi-Modal Transfer
Training on language-mixed text reasoning traces induces performance gains on English benchmarks and visual-language tasks without explicit English or image training data. The inclusion of English reasoning traces reinforces general reasoning heuristics that transfer across languages. For vision tasks, the paper hypothesizes a "multi-modal free lunch" where high-quality text reasoning aligns visual attention mechanisms more effectively than random or lower-quality text. Reasoning skills are partially modular and detachable from specific language or modality encoders. We attribute the English improvements to universal math and science knowledge, and our Language-Mixed CoT includes English reasoning steps. KO-REAson-12B trained only with text supervision shows consistent gains, indicating both cross-lingual and multimodal transfer.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student):** Used for bootstrapping initial Korean reasoning model since RL is too unstable/expensive for mid-resource languages. Quick check: Can you explain why the authors chose SFT over Reinforcement Learning (RLVR) for bootstrapping the initial Korean reasoning model?

- **Code-Switching:** Core data format where models handle alternating languages within a single context window. Quick check: How does the "Language-Mixed" schema differ from simply translating the CoT into the target language?

- **Data Contamination & Decontamination:** Critical for preventing benchmark memorization. The paper emphasizes a rigorous 13-gram decontamination pipeline. Quick check: Why did the authors use 13-gram overlap filtering instead of embedding-based semantic matching?

## Architecture Onboarding

- **Component map:** Teacher (Qwen3-32B) -> Prompt Source (Korean Web Q&A + Translated OpenThought) -> Filter (Regex, Loss Spike Detector, 13-gram Decontaminator) -> Student (Base models) -> Trainer (FSDP/PyTorch, Liger Kernels, FlashAttention-2)

- **Critical path:** Ingest Korean Q&A sites (54 sites) -> Generate Language-Mixed CoT with system instruction to preserve Korean context -> Filter using shakedown training on proxy model to identify loss spikes -> Apply 13-gram overlap removal -> Train SFT on YI-SANG-HQ subset

- **Design tradeoffs:** High-Yield Subset (260k) vs. Full Dataset (3.7M) trades data volume for training stability and compute efficiency. Medical/Daily Data excluded as ablations showed Medical boosted ClinicalQA but degraded general reasoning, while Daily had no signal.

- **Failure signatures:** Loss Spikes from degeneration (repeating phrases), multiple thinking blocks, or non-Korean final answers. Benchmark Collapse if training on pure English CoT (Korean cultural benchmarks drop) or pure Korean CoT (Math drops). Cultural Hallucination when models trained on translated data fail on colloquial expressions or native cultural context.

- **First 3 experiments:**
  1. Ablate the CoT Language: Train three small models on identical prompts but different CoT styles (English-only, Korean-only, Language-Mixed) and evaluate on Math vs. Culture benchmarks.
  2. Loss Spike Audit: Run short training (1 epoch) on full raw dataset, identify 3 samples causing loss spikes and classify error type.
  3. Decontamination Fidelity: Manually verify 50 random samples from final dataset have no 13-gram overlaps with held-out benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
Does Language-Mixed CoT generalize to other mid-resource languages beyond Korean? The paper presents Korean as a "case study" and "apt testbed" but does not validate the approach on other mid-resource languages. This remains unresolved as the authors demonstrate effectiveness only for Korean, leaving open whether the 5-20% target-language ratio and English anchoring strategy transfer to languages with different scripts, typological features, or cultural contexts. Applying the same methodology to other mid-resource languages (e.g., Vietnamese, Thai, Hebrew) and reporting whether Language-Mixed CoT consistently outperforms monolingual baselines would resolve this.

### Open Question 2
How should the anchor language be selected based on base model pretraining mixtures? Table 2 shows Gemma-3-4B benefits from Russian/Chinese-anchored CoT while Kanana-1.5-8B does not; authors "suspect this difference is driven by the pretraining mixtures" but do not systematically test this. The interaction between anchor language choice and base model pretraining composition remains empirically uncharacterized. Controlled experiments across multiple base models with documented pretraining language distributions, testing various anchor languages would resolve this.

### Open Question 3
Why does medical training data improve clinical benchmarks while harming general reasoning performance? Table 3 shows Medical category boosts ClinicalQA but "significantly hinders performance on all other benchmarks," leading to its exclusion from YI-SANG-HQ. The mechanism driving this negative transfer is not investigated; whether this is specific to Korean medical text or a general phenomenon is unknown. Analysis of medical vs. non-medical reasoning patterns, and cross-lingual medical data experiments to isolate domain effects from language-specific effects would resolve this.

### Open Question 4
Is the 5-20% Korean-character ratio threshold optimal for Language-Mixed CoT, or should it vary by task type? The authors apply a regex filter discarding samples outside this range "to preserve key Korean terms" but do not ablate this threshold. The heuristic was empirically chosen but not systematically validated; tasks requiring more cultural context may benefit from higher target-language ratios. Ablation experiments varying the Korean-character ratio threshold and measuring effects across reasoning-heavy vs. culture-heavy benchmarks would resolve this.

## Limitations

- The effectiveness of English as an "anchor language" assumes English-centric reasoning representations, which may not generalize to base models trained on balanced multilingual data
- The claimed cross-lingual and multi-modal transfer benefits are presented as empirical observations but lack mechanistic explanation
- The reliance on a single teacher model (Qwen3-32B) and absence of comparison to other multilingual reasoning approaches limit generalizability

## Confidence

**High Confidence (★★★):** The empirical results showing KO-REAson-35B achieving state-of-the-art performance on 5 of 9 benchmarks, with substantial improvements over baselines (+18.6 average points for smaller models). The ablation studies on Medical/Daily data and loss spike filtering are well-documented.

**Medium Confidence (★★☆):** The mechanism explanations for why Language-Mixed CoT works (English anchor preservation, context fidelity, cross-lingual transfer). While supported by observations, the theoretical grounding relies on cited but not independently verified claims about "English-centric latent spaces."

**Low Confidence (★☆☆):** The magnitude of cross-lingual and multi-modal transfer benefits. The paper attributes these to "universal reasoning heuristics" but provides limited evidence beyond correlation.

## Next Checks

1. **Teacher Model Dependency:** Fine-tune a smaller model (e.g., Gemma-4B) using two different teachers: Qwen3-32B and a multilingual model trained on balanced data. Compare performance to isolate the impact of English-centric pretraining.

2. **Decontamination Robustness:** Manually audit 50 random samples from the final dataset for semantic overlap with benchmarks (not just 13-gram matches). Measure false negative and false positive rates of the current pipeline.

3. **Cross-Lingual Transfer Mechanism:** Train two models on identical Korean data: one with Language-Mixed CoT (containing English traces) and one with pure Korean CoT. Evaluate both on English benchmarks to quantify the specific contribution of English reasoning traces versus Korean reasoning quality.