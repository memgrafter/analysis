---
ver: rpa2
title: 'From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling
  Framework for In-context Learning'
arxiv_id: '2510.24528'
source_url: https://arxiv.org/abs/2510.24528
tags:
- target
- task
- tasks
- examples
- cross-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cost-efficient pipeline for in-context
  learning (ICL) on novel tasks by combining cross-task examples with graph-based
  label propagation. The method first uses a graph-based similarity metric (GraphSim)
  to select relevant examples from high-resource source tasks, then applies a graph
  neural network to propagate labels from a small pseudo-labeled set to the entire
  target dataset (GLIP), avoiding extensive LLM inference.
---

# From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning

## Quick Facts
- **arXiv ID**: 2510.24528
- **Source URL**: https://arxiv.org/abs/2510.24528
- **Reference count**: 24
- **Key outcome**: Cost-efficient ICL pipeline using graph-based pseudo-labeling outperforms cross-task baselines while reducing LLM inference costs

## Executive Summary
This paper introduces a novel framework for in-context learning (ICL) on novel tasks by leveraging examples from high-resource source tasks. The approach combines a graph-based similarity metric (GraphSim) to select relevant examples with graph neural network-based label propagation to efficiently generate pseudo-labels for target datasets. This method addresses the key challenge of ICL requiring large numbers of in-task examples, which can be costly to obtain. The framework demonstrates strong performance across five different tasks with multiple LLMs, achieving results close to in-task upper bounds while significantly reducing labeling costs.

## Method Summary
The proposed framework operates in two main stages: cross-task example selection and graph-based pseudo-labeling. First, GraphSim computes pairwise similarities between tasks using sentence embeddings from both instructions and demonstrations, selecting the most relevant examples from source tasks. These selected examples are then combined with a small manually labeled subset of the target task to form a pseudo-labeled dataset. A graph neural network performs label propagation across this dataset, efficiently spreading labels to the entire target dataset. This approach significantly reduces the need for extensive LLM inference while maintaining high performance, making it particularly valuable for low-resource settings where in-task examples are scarce.

## Key Results
- Outperforms existing cross-task baselines across five different tasks with multiple LLMs
- Achieves performance close to in-task upper bounds while reducing labeling costs
- Demonstrates effectiveness and scalability in low-resource settings where in-task examples are scarce

## Why This Works (Mechanism)
The framework leverages the semantic relationships between tasks to transfer knowledge efficiently. By using GraphSim to identify semantically similar source tasks, the method ensures that transferred examples are relevant and informative for the target task. The GNN-based label propagation then exploits the graph structure of the combined source-target dataset to spread labels from the small pseudo-labeled set to the entire dataset in a way that respects task relationships. This two-stage approach allows the model to benefit from the diversity of multiple source tasks while maintaining task-specific relevance, achieving a balance between generalization and specialization that traditional single-task ICL struggles to attain.

## Foundational Learning
- **In-context learning (ICL)**: The ability of LLMs to learn from demonstrations within prompts without parameter updates - needed because the paper aims to improve ICL efficiency for novel tasks
- **Cross-task knowledge transfer**: Leveraging examples from related tasks to improve performance on target tasks - needed because the paper addresses the scarcity of in-task examples
- **Graph neural networks (GNNs)**: Neural networks that operate on graph-structured data - needed for the label propagation mechanism that spreads labels across the task similarity graph
- **Sentence embeddings**: Vector representations of text that capture semantic meaning - needed for the GraphSim metric to compute task similarities
- **Pseudo-labeling**: Generating labels for unlabeled data using a model's predictions - needed as the core technique for reducing the need for manual labeling

## Architecture Onboarding
- **Component map**: GraphSim (task similarity computation) -> Example selection -> Pseudo-labeling (small manual set) -> GNN label propagation -> Final labeled dataset
- **Critical path**: The most time-consuming component is the GNN label propagation step, as it must process the entire combined dataset graph structure
- **Design tradeoffs**: The method trades some accuracy (compared to full in-task labeling) for significant cost reduction in LLM inference, while maintaining strong performance through intelligent example selection
- **Failure signatures**: Poor performance when source and target tasks have minimal semantic overlap, or when the GNN cannot effectively capture task relationships in the graph structure
- **First experiments**: 1) Validate GraphSim similarity scores against human judgment on task pairs, 2) Test GNN label propagation accuracy with varying sizes of pseudo-labeled sets, 3) Compare performance against random example selection from source tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on the quality and diversity of available source tasks
- GraphSim similarity metric may struggle with tasks requiring nuanced understanding beyond surface-level semantic similarity
- GNN-based label propagation assumes task similarities can be effectively captured in graph structure, which may not hold for all task types

## Confidence
- **High confidence**: Core contribution of reducing labeling costs through cross-task example selection and GNN-based propagation is well-supported
- **Medium confidence**: Claim of approaching in-task upper bounds requires validation across broader task ranges and model sizes
- **Medium confidence**: Scalability claims need further validation with significantly larger numbers of source tasks

## Next Checks
1. Test framework robustness across more diverse task pairs, including those with minimal semantic overlap, to evaluate GraphSim limitations
2. Conduct ablation studies to quantify individual contributions of cross-task selection versus GNN propagation components
3. Evaluate performance when scaling to dozens or hundreds of source tasks to assess claimed scalability and identify bottlenecks