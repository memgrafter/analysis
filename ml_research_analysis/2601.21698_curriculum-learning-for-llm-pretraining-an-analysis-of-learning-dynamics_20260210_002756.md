---
ver: rpa2
title: 'Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics'
arxiv_id: '2601.21698'
source_url: https://arxiv.org/abs/2601.21698
tags:
- training
- learning
- curriculum
- random
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates how pretraining data order affects LLM\
  \ learning dynamics and stability. Using Pythia models, we combine HMM-based phase\
  \ analysis with gradient noise scale and singular entropy diagnostics to compare\
  \ three curricula\u2014Age-of-Acquisition, word frequency, and Verb Variation (VV)\u2014\
  against Random ordering."
---

# Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics

## Quick Facts
- arXiv ID: 2601.21698
- Source URL: https://arxiv.org/abs/2601.21698
- Reference count: 40
- One-line primary result: Curriculum orderings reduce gradient noise scale and singular entropy in smaller models, improving late-phase stability

## Executive Summary
This work investigates how pretraining data order affects LLM learning dynamics and stability. Using Pythia models, we combine HMM-based phase analysis with gradient noise scale and singular entropy diagnostics to compare three curricula—Age-of-Acquisition, word frequency, and Verb Variation (VV)—against Random ordering. The joint HMM analysis confirms that all orderings share latent training phases; differences arise from which data appears within these phases. For smaller models (14M–70M), Random ordering produces higher gradient noise scale; for models up to 160M, Random also produces higher singular entropy, corresponding to noisier optimization and late-stage spectral collapse in the language modeling head. Curriculum-based orderings reduce both quantities and are associated with less late-stage degradation consistent with softmax-bottleneck saturation. At larger scales, these gaps narrow and curriculum gains shrink. Our theoretical framework formalizes a mechanism linking difficulty pacing to optimization stability: by controlling stochastic-gradient variance through structured data exposure, curricula can maintain tighter stability bounds than uniform sampling. The empirical results support this view—curricula reduce gradient noise scale and singular entropy in smaller models, and these reductions coincide with improved late-phase accuracy. This suggests a practical guideline: curricula are most valuable when model capacity is constrained and late-stage saturation is a concern; at larger scales where capacity suffices, random ordering performs comparably.

## Method Summary
The authors use Pythia models to compare curriculum learning strategies against random data ordering during pretraining. They employ hidden Markov model (HMM) analysis to identify latent training phases across different orderings, then track gradient noise scale and singular entropy within each phase. Three curriculum orderings are tested: Age-of-Acquisition (word learning order), word frequency, and Verb Variation. The analysis spans model sizes from 14M to 160M parameters, with comparative results at larger scales. Gradient noise scale and singular entropy serve as optimization stability metrics, while late-phase accuracy indicates model performance. The theoretical component formalizes how curriculum pacing controls stochastic-gradient variance to maintain stability bounds.

## Key Results
- Random ordering produces higher gradient noise scale than curriculum orderings for models up to 70M parameters
- Random ordering produces higher singular entropy than curriculum orderings for models up to 160M parameters
- Curriculum orderings reduce late-stage spectral collapse in the language modeling head
- Curriculum benefits diminish at larger model scales where capacity suffices

## Why This Works (Mechanism)
Curriculum learning stabilizes optimization by controlling the variance of stochastic gradients through structured data exposure. When models encounter progressively harder examples, the optimization landscape remains smoother, preventing the high-variance gradients that can destabilize training. This mechanism is particularly important for smaller models with limited capacity, where random exposure to difficult data can cause optimization instability. The curriculum effect appears strongest when model capacity is constrained and late-stage saturation becomes a concern.

## Foundational Learning
- **Hidden Markov Models**: Used to identify latent training phases across different data orderings; needed to detect shared training dynamics despite ordering differences; quick check: verify HMM convergence across multiple random seeds
- **Gradient Noise Scale**: Measures optimization stability; needed to quantify how data ordering affects training smoothness; quick check: compare gradient variance across curriculum vs random orderings
- **Singular Entropy**: Measures spectral complexity of weight matrices; needed to detect late-stage degradation in the language modeling head; quick check: track entropy trends throughout training
- **Softmax Bottleneck**: Theoretical limit on model capacity to represent language distributions; needed to explain late-stage saturation; quick check: measure cross-entropy vs model size
- **Optimization Stability Theory**: Mathematical framework linking gradient variance to convergence bounds; needed to formalize curriculum benefits; quick check: verify theoretical predictions match empirical observations
- **Phase Analysis**: Technique to segment training into distinct learning regimes; needed to understand how different data orderings affect learning progression; quick check: ensure phase boundaries are consistent across orderings

## Architecture Onboarding

**Component Map:**
Data Ordering Module -> HMM Phase Detector -> Gradient Noise Scale Calculator -> Singular Entropy Tracker -> Performance Evaluator

**Critical Path:**
1. Data ordering selection (curriculum vs random)
2. Training phase identification via HMM
3. Gradient noise scale and singular entropy measurement
4. Late-phase accuracy evaluation
5. Theoretical stability analysis

**Design Tradeoffs:**
- Curriculum vs random ordering: stability vs simplicity
- Fixed vs adaptive pacing: control vs flexibility
- HMM complexity vs interpretability: detailed vs coarse phase detection
- Multiple metrics vs single metric: comprehensive vs focused analysis

**Failure Signatures:**
- High gradient noise scale indicates unstable optimization
- Increasing singular entropy suggests late-stage degradation
- Phase detection failure indicates inadequate model or data
- Theoretical mismatch suggests oversimplified assumptions

**First Experiments:**
1. Compare gradient noise scale across all four orderings (3 curricula + random) for 14M parameter model
2. Track singular entropy evolution for each ordering during training
3. Verify HMM phase consistency across different random seeds for the same ordering

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on Pythia models and English-only corpora, limiting external validity
- Analysis covers only up to 160M parameters in detail, with sparse results for larger models
- Theoretical stability argument assumes idealized conditions and doesn't account for adaptive optimizers' dynamic behavior
- Study doesn't explore curriculum scheduling strategies or their interaction with learning rate schedules

## Confidence

**High confidence:**
- The empirical finding that Random ordering increases gradient noise scale and singular entropy in smaller models (14M–160M)

**Medium confidence:**
- The claim that curriculum benefits diminish at larger scales, based on limited larger-model data
- The theoretical link between curriculum pacing and optimization stability, which is plausible but not fully validated across optimizer types

## Next Checks
1. Test curriculum effects on multilingual corpora and non-English languages to assess linguistic generalization
2. Evaluate the proposed stability mechanism with adaptive optimizers (AdamW, Lion) beyond SGD
3. Conduct controlled experiments varying curriculum pacing schedules while holding other hyperparameters constant