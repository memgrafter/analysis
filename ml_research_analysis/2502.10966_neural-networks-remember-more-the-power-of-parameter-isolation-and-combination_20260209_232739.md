---
ver: rpa2
title: 'Neural Networks Remember More: The Power of Parameter Isolation and Combination'
arxiv_id: '2502.10966'
source_url: https://arxiv.org/abs/2502.10966
tags:
- task
- learning
- tasks
- https
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in continual learning
  for pre-trained language models (PLMs), where models lose previously acquired knowledge
  when sequentially trained on new tasks. The authors propose a novel approach that
  combines parameter isolation and combination strategies.
---

# Neural Networks Remember More: The Power of Parameter Isolation and Combination

## Quick Facts
- arXiv ID: 2502.10966
- Source URL: https://arxiv.org/abs/2502.10966
- Authors: Biqing Zeng; Zehan Li; Aladdin Ayesh
- Reference count: 32
- Primary result: Achieves 77.2% average accuracy on continual language learning benchmarks without task IDs or rehearsal

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for pre-trained language models (PLMs) by proposing a novel approach that combines parameter isolation and combination strategies. The method uses parameter-efficient fine-tuning techniques like Adapter and LoRA to adapt to each downstream task while keeping the backbone frozen, then combines the trained parameters using task arithmetic before applying them to the backbone model. The approach achieves state-of-the-art performance on continual language learning benchmarks without requiring task identification or historical data storage.

## Method Summary
The proposed method tackles catastrophic forgetting through a two-phase approach. First, it employs Parameter-Efficient Fine-Tuning (PEFT) methods such as Adapter and LoRA to adapt to each downstream task while maintaining the frozen backbone model. After training on all tasks sequentially, the method combines the trained parameters using task arithmetic techniques. These combined parameters are then applied to the backbone model, enabling the model to leverage knowledge from all tasks. The approach effectively balances model stability and plasticity, mitigating catastrophic forgetting while facilitating knowledge transfer between tasks.

## Key Results
- Achieves state-of-the-art performance on continual language learning benchmarks
- Reaches 77.2% average accuracy in the full-shot setting, surpassing previous methods
- Outperforms existing approaches like EPI and IDBR without requiring task identification or historical data storage

## Why This Works (Mechanism)
The method works by isolating parameters for individual tasks through PEFT methods, preventing interference during sequential training. By keeping the backbone frozen during adaptation, it maintains stability for previously learned knowledge. The parameter combination phase leverages task arithmetic to create a unified parameter set that incorporates knowledge from all tasks. This combination strategy allows the model to balance plasticity (learning new tasks) and stability (retaining old knowledge) without requiring explicit task boundaries or replay buffers.

## Foundational Learning
- **Catastrophic forgetting**: Neural networks losing previously acquired knowledge when trained on new tasks - critical for understanding the core problem being solved
- **Parameter-Efficient Fine-Tuning (PEFT)**: Methods like Adapter and LoRA that add small trainable modules to PLMs - needed to isolate task-specific parameters while keeping backbone frozen
- **Task arithmetic**: Combining parameters from different tasks to create unified representations - essential for merging knowledge from multiple tasks
- **Continual learning benchmarks**: Standard evaluation frameworks for sequential task learning - required to validate the method against existing approaches

## Architecture Onboarding

**Component Map**
PLM Backbone -> PEFT Adapters/LoRA -> Task-specific parameters -> Task arithmetic combination -> Final backbone update

**Critical Path**
1. Initialize frozen PLM backbone
2. Sequentially apply PEFT methods for each task
3. Collect trained adapter/LoRA parameters
4. Combine parameters using task arithmetic
5. Apply combined parameters to backbone

**Design Tradeoffs**
- Parameter isolation vs. model size: Using PEFT methods keeps additional parameters manageable
- Combination quality vs. complexity: Task arithmetic assumes linear relationships that may not always hold
- Stability vs. plasticity: The method must balance retaining old knowledge while learning new tasks

**Failure Signatures**
- Degraded performance on early tasks indicates insufficient parameter isolation
- Poor overall accuracy suggests ineffective parameter combination
- Memory issues may arise from storing parameters from many tasks

**First Experiments**
1. Test on a simple two-task sequence to verify basic functionality
2. Evaluate performance degradation on previous tasks after training on new ones
3. Compare parameter combination methods (averaging vs. weighted sum)

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on real-world scenarios with complex, non-IID data distributions remains untested
- Scalability to hundreds of tasks or very long task sequences is unexplored
- Computational cost of parameter combination may increase with task diversity
- Method assumes linear relationships between parameter changes across tasks

## Confidence
- **High confidence**: Core empirical results showing state-of-the-art performance on tested benchmarks
- **Medium confidence**: Theoretical justification for parameter isolation and combination strategies
- **Medium confidence**: Claims about superiority over existing approaches for tested benchmarks

## Next Checks
1. Evaluate scalability by testing on extended task sequences (50+ tasks) to assess parameter combination effectiveness
2. Validate practical applicability by testing on real-world continual learning scenarios with non-IID data distributions
3. Conduct systematic ablation studies to determine optimal hyperparameter settings and relative contributions of isolation vs. combination strategies