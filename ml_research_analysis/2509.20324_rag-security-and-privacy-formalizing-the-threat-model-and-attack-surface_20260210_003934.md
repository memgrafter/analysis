---
ver: rpa2
title: 'RAG Security and Privacy: Formalizing the Threat Model and Attack Surface'
arxiv_id: '2509.20324'
source_url: https://arxiv.org/abs/2509.20324
tags:
- knowledge
- privacy
- arxiv
- systems
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first formal threat model for retrieval-augmented
  generation (RAG) systems, addressing the lack of a structured framework to analyze
  privacy and security risks. The authors propose a taxonomy of adversaries based
  on model access (black-box vs.
---

# RAG Security and Privacy: Formalizing the Threat Model and Attack Surface

## Quick Facts
- **arXiv ID**: 2509.20324
- **Source URL**: https://arxiv.org/abs/2509.20324
- **Reference count**: 39
- **Primary result**: Introduces first formal threat model for RAG systems, defining four adversary types and attack vectors including document-level membership inference, content leakage, and data poisoning.

## Executive Summary
This paper establishes the first formal threat model specifically for Retrieval-Augmented Generation (RAG) systems, addressing a critical gap in understanding privacy and security risks in these architectures. The authors propose a comprehensive taxonomy of adversaries based on their access level to the model (black-box vs. white-box) and knowledge of training data/knowledge base (normal vs. informed), resulting in four distinct adversary types. The work formalizes key attack vectors including document-level membership inference, content leakage, and data poisoning, providing rigorous definitions and representative attack models. A key contribution is demonstrating that differentially private retrievers can offer formal privacy guarantees against membership inference attacks, while highlighting that securing RAG requires protecting not only the LLM but also the external knowledge base.

## Method Summary
The paper formalizes a threat model for RAG systems by defining four adversary types based on model access and knowledge: Unaware Observer, Aware Observer, Aware Insider, and Unaware Insider. For document-level membership inference, they provide a formal definition where the adversary wins if they correctly guess whether a document is in the knowledge base. They propose a differential privacy defense by adding noise (Laplace or Gaussian) to retriever similarity scores: s̃(di, q) = s(di, q) + ηi, where ηi ~ Lap(1/ε). For content leakage and data poisoning attacks, they define success criteria and reference representative attack models from the literature. The approach emphasizes that RAG security requires protecting both the LLM and knowledge base components, with specific attention to the unique vulnerabilities introduced by external data retrieval.

## Key Results
- Formalizes four adversary types (unaware/aware observer/insider) based on model access and knowledge of training data
- Provides rigorous definitions for DL-MIA, content leakage, and data poisoning attacks with success criteria
- Demonstrates that DP-retrievers with Laplace/Gaussian noise can provide (ε, δ)-differential privacy guarantees
- Establishes that RAG security requires protecting both LLM and knowledge base components
- Identifies document membership inference as a key privacy vulnerability in RAG systems

## Why This Works (Mechanism)
The formalization works by systematically decomposing the RAG attack surface into discrete components (retriever, generator, knowledge base) and modeling adversary capabilities as orthogonal dimensions (model access, knowledge of data). This creates a tractable framework where specific attack vectors can be analyzed in isolation while maintaining realistic threat scenarios. The DL-MIA defense leverages the mathematical properties of differential privacy - by adding calibrated noise to retrieval scores, the presence or absence of any single document becomes statistically indistinguishable, preventing adversaries from inferring membership through query-response analysis.

## Foundational Learning
- **Adversary Taxonomy**: Classification of attackers by model access (black-box vs white-box) and data knowledge (normal vs informed) - needed to scope realistic threat scenarios; quick check: map known attack papers to appropriate adversary type.
- **Document-Level Membership Inference**: Attack where adversary determines if specific document was in training/ knowledge base by analyzing query responses - needed because RAG exposes document-level access patterns; quick check: measure accuracy of membership prediction across different query strategies.
- **Differentially Private Retrievers**: Adding calibrated noise to similarity scores to satisfy DP while maintaining retrieval utility - needed to provide provable privacy guarantees against membership inference; quick check: plot membership inference success rate vs epsilon privacy budget.
- **Content Leakage Attacks**: Extracting verbatim or paraphrased content from knowledge base through carefully crafted queries - needed because RAG can inadvertently expose sensitive information; quick check: measure information extraction success rate across different generator architectures.
- **Data Poisoning in RAG**: Corrupting retriever by injecting malicious documents that manipulate retrieval results - needed because external knowledge base is vulnerable to supply-chain attacks; quick check: measure retrieval accuracy degradation when poisoned documents are present.

## Architecture Onboarding
- **Component Map**: Knowledge Base D (n documents) -> Retriever R -> Generator G -> Response y
- **Critical Path**: Query q → Retriever R (similarity scoring → top-k selection) → Generator G (prompt + retrieved docs → response)
- **Design Tradeoffs**: Privacy (DP noise) vs Retrieval Quality (recall@k), Security (input filtering) vs System Responsiveness, Defense Complexity vs Attack Surface Coverage
- **Failure Signatures**: DL-MIA accuracy near 50% (insufficient signal), Poisoning success (retriever returns corrupted docs), Leakage (sensitive content extracted verbatim)
- **First Experiments**: 1) Implement basic RAG pipeline with FAISS/ColBERT and Llama/GPT-4, 2) Run DL-MIA attack across all four adversary types measuring success rates, 3) Apply DP defense and measure privacy-utility trade-off curve across epsilon values

## Open Questions the Paper Calls Out
**Open Question 1**: What is the empirical trade-off between privacy guarantees (epsilon) and retrieval utility when applying Retriever-Level Differential Privacy in RAG systems? The paper proposes DP defense but doesn't quantify degradation in retrieval accuracy or generation quality as noise levels increase.

**Open Question 2**: How can defenses against data poisoning be adapted to withstand adaptive adversaries (Type A_III) who possess knowledge of the defense mechanism? The paper references filtering heuristics but doesn't analyze their robustness against white-box adaptive attacks.

**Open Question 3**: Can mitigation strategies for different attack vectors (e.g., poisoning and leakage) be combined into a unified defense without causing prohibitive inference latency? The paper notes that while individual techniques show promise, combined defenses need assessment for practical deployment.

## Limitations
- Theoretical framework lacks empirical validation on real datasets or attack scenarios
- No specific implementation details for attacks or baseline performance metrics
- Differential privacy defense mechanism not benchmarked for practical privacy-utility trade-offs
- Four-adversary taxonomy may not capture all real-world multi-stage or adaptive attack scenarios
- Does not address interaction effects between multiple simultaneous attack vectors

## Confidence
- **High**: Threat model taxonomy and formalization of attack definitions are methodologically sound and represent genuine security/privacy concerns
- **Medium**: DP defense mechanism is theoretically valid but practical effectiveness and performance impact unverified
- **Low**: Relative severity ranking of adversary types and practical exploitability cannot be determined without empirical validation

## Next Checks
1. Implement complete RAG pipeline with DL-MIA attack across all four adversary types using real dataset, measuring attack success rates to identify most practical threats
2. Systematically evaluate DP defense by measuring membership inference success rate reduction versus recall@k degradation across multiple epsilon values to determine privacy-utility trade-off
3. Design and test content leakage attack combining query patterns with response analysis to extract sensitive information from knowledge base across different retriever/generator combinations