---
ver: rpa2
title: 'Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study
  of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS'
arxiv_id: '2511.05502'
source_url: https://arxiv.org/abs/2511.05502
tags:
- apple
- mlc-llm
- ollama
- throughput
- ttft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic, empirical evaluation of five
  local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp,
  Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with
  an M2 Ultra processor and 192 GB of unified memory.'
---

# Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS

## Quick Facts
- arXiv ID: 2511.05502
- Source URL: https://arxiv.org/abs/2511.05502
- Reference count: 0
- Primary result: MLX achieves highest sustained throughput (~230 tokens/sec) on Apple Silicon, while MLC-LLM delivers consistently lower time-to-first-token for moderate prompts and stronger long-context handling.

## Executive Summary
This paper presents a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, the authors measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity. Under their settings, MLX achieves the highest sustained generation throughput (approximately 230 tokens/sec), while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes (≤16k tokens) and offers stronger out-of-the-box inference features including paged KV caching. llama.cpp is highly efficient for lightweight single-stream use but lacks scalability features, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts. All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. The analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations: MLX is the preferred default for Apple-first deployments due to its optimized integration with Apple hardware and efficient throughput, while MLC-LLM complements it with faster TTFT and stronger long-context handling for interactive chat workloads.

## Method Summary
The study benchmarks five Apple Silicon LLM inference frameworks (MLX, MLC-LLM, Ollama, llama.cpp, PyTorch MPS) on a Mac Studio M2 Ultra with 192GB RAM. Using Qwen-2.5-Coder 3B and 7B-Instruct models, the authors measure TTFT, decode and end-to-end throughput, latency percentiles, cold-start time, memory usage, and KV/prompt cache hit rates across prompts from 1k to 100k tokens in unique-token, prefix-heavy, and code-dominant patterns. Models are quantized at FP16/bf16, int8, and int4 (GGUF/AWQ/GPTQ/mixed-bit per framework). Each test runs N=10 trials with one warm-up, using fixed seeds and client-side timers. Environment is controlled by disabling Spotlight, Time Machine, and iCloud.

## Key Results
- MLX achieved the highest sustained throughput (~230 tokens/s) and stable per-token latency.
- MLC-LLM delivers consistently lower TTFT for moderate prompt sizes (≤16k tokens) and offers stronger out-of-the-box inference features including paged KV caching.
- PyTorch MPS remains constrained by memory limits, frequently failing on 3B+ models due to the 4GB tensor cap.
- llama.cpp is highly efficient for lightweight single-stream use but lacks scalability features for long contexts.
- Ollama emphasizes developer ergonomics but lags in throughput and TTFT compared to MLX and MLC-LLM.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an inference framework is optimized specifically for the Metal and Neural Engine architecture of Apple Silicon, it achieves higher sustained throughput than general-purpose backends.
- **Mechanism:** MLX utilizes Metal kernels that consistently saturate GPU utilization (>90%), reducing overhead and maintaining stable inter-token latency during the decode phase.
- **Core assumption:** The hardware-specific optimizations in MLX are the primary driver of efficiency, rather than model size or quantization alone.
- **Evidence anchors:**
  - [abstract] "MLX achieved the highest sustained throughput (~230 tokens/s) and stable per-token latency..."
  - [page 3] "Its Metal kernels consistently saturated GPU utilization (>90%) while keeping CPU usage low (<3%)."
  - [corpus] "Benchmarking On-Device Machine Learning on Apple Silicon with MLX" supports the focus on Apple-optimized frameworks.
- **Break condition:** If future updates to PyTorch MPS or llama.cpp introduce kernel optimizations that match or exceed Metal saturation levels, this throughput advantage may diminish.

### Mechanism 2
- **Claim:** If a runtime implements paged KV caching, it handles long-context inference (64k–128k tokens) more efficiently than fixed or sliding-window caches.
- **Mechanism:** MLC-LLM partitions key-value tensors into reusable blocks (paged KV), reducing memory fragmentation and enabling efficient reuse across long contexts without quadratic slowdowns.
- **Core assumption:** The overhead of managing paged memory is lower than the cost of recomputing or storing full-context attention states in non-paged systems.
- **Evidence anchors:**
  - [page 5] "MLC-LLM: Strongest for very long contexts. MLC implements paged KV caching similar to vLLM... sustaining usable throughput... on 100k contexts."
  - [page 5] Table 2 highlights MLC-LLM's paged KV vs. MLX's rotating KV and llama.cpp's sliding window.
  - [corpus] "Selective KV-Cache Sharing..." discusses KV-cache optimization strategies, providing context for the importance of cache management.
- **Break condition:** If the implementation of paged KV caching introduces excessive administrative overhead for short contexts, TTFT may increase for typical chat interactions.

### Mechanism 3
- **Claim:** If a GPU backend imposes strict per-tensor memory limits, it renders the framework unsuitable for large-model inference despite available system RAM.
- **Mechanism:** PyTorch MPS is fundamentally constrained by a software/driver-level 4GB tensor cap, causing frequent out-of-memory (OOM) errors on models that fit within the system's 192GB unified memory.
- **Core assumption:** The 4GB limit is a backend-specific constraint of the MPS implementation, not a hardware limitation of the M2 Ultra.
- **Evidence anchors:**
  - [page 4] "...PyTorch MPS remains constrained by memory limits... frequently failing on 3B+ models due to the 4GB tensor cap."
  - [page 5] "...4GB tensor cap leads to frequent out-of-memory errors beyond ~2k tokens..."
  - [corpus] Corpus evidence for this specific "4GB tensor cap" mechanism is weak; neighbors focus on general edge inference or scheduling.
- **Break condition:** If Apple or PyTorch updates the MPS backend to support larger tensors or unified memory addressing more effectively, this constraint would be lifted.

## Foundational Learning

**Concept: Time-to-First-Token (TTFT) vs. Sustained Throughput**
- **Why needed here:** The paper frames framework selection as a tradeoff between these two metrics. MLX wins on throughput (good for batch processing), while MLC-LLM often wins on TTFT (good for interactive chat).
- **Quick check question:** Does your use case prioritize how fast the first word appears (chat) or how many words are generated per second in total (batch)?

**Concept: KV (Key-Value) Caching Strategies**
- **Why needed here:** The ability to handle long contexts (up to 100k tokens) depends entirely on how the framework manages the KV cache (Paged vs. Rotating vs. Sliding Window).
- **Quick check question:** Does the framework cache attention states to avoid recomputation, and does it manage that cache via paging (efficient for long context) or rotation (efficient for memory)?

**Concept: Unified Memory Architecture**
- **Why needed here:** The study relies on Apple Silicon's 192GB unified memory to load large models (e.g., Qwen-2.5) entirely on-device, which is distinct from traditional GPU VRAM constraints.
- **Quick check question:** Is the model loaded into the shared CPU/GPU memory pool, and does the software backend effectively address this unified space?

## Architecture Onboarding

**Component map:** Mac Studio (M2 Ultra, 192GB RAM) -> MLX (Apple-native) -> Metal kernels; MLC-LLM (TVM-based) -> Paged KV caching; Ollama (wrapper) -> Ease of use; llama.cpp (C++) -> Lightweight control; PyTorch MPS (baseline) -> 4GB tensor cap.

**Critical path:**
1. **Define Priority:** Determine if the goal is throughput (MLX), low latency/long context (MLC-LLM), or ease of use (Ollama).
2. **Format Model:** Convert the model to the framework-specific format (e.g., MLX format, GGUF, or TVM-compiled for MLC).
3. **Configure Cache:** Set up KV cache limits or paging based on expected context length.

**Design tradeoffs:**
- **MLX vs. MLC-LLM:** MLX offers ~20% higher throughput but requires external wrappers for an OpenAI-compatible API. MLC-LLM offers lower TTFT and built-in API server but with slightly lower peak throughput.
- **Ollama vs. llama.cpp:** Ollama provides the easiest deployment but abstracts away optimization control. llama.cpp offers granular control but requires more manual configuration.

**Failure signatures:**
- **PyTorch MPS:** Crashes or OOM errors on models >3B parameters or contexts >2k tokens (due to 4GB tensor cap).
- **llama.cpp:** Throughput collapses to ~1.2 tok/s on contexts >32k (due to lack of paged caching).
- **Ollama:** TTFT degrades severely (>50s) at 100k contexts.

**First 3 experiments:**
1. **Baseline Throughput:** Run a 3B model on MLX and MLC-LLM with identical prompts to verify the ~230 vs. ~190 tok/s differential reported in the paper.
2. **Long Context Stress Test:** Feed a ~50k token prompt into MLC-LLM and MLX to observe how paged KV caching (MLC) compares to rotating KV cache (MLX) in terms of memory stability.
3. **Concurrency Check:** Simulate 4-8 concurrent users on MLC-LLM and MLX (via a wrapper) to validate MLC's superior handling of concurrent requests.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does inference performance and framework ranking change when scaling to larger parameter models (e.g., 14B or 70B) or different architectures not covered in this study?
- Basis in paper: [explicit] The authors state in the "Threats to Validity" section that they primarily evaluated Qwen-2.5 3B/7B and explicitly list "Future work includes 14B and instruction-tuned variants to broaden coverage."
- Why unresolved: The study limits its scope to the Qwen-2.5 family (3B and 7B variants); results may not generalize to models with different memory bandwidth requirements or architectural attention patterns.
- What evidence would resolve it: Benchmarking the same five frameworks using 14B+ models and non-Qwen architectures (e.g., Llama 3) on the identical M2 Ultra hardware.

**Open Question 2**
- Question: Are the observed performance hierarchies consistent across different Apple Silicon generations, such as M1 or M4?
- Basis in paper: [explicit] The authors note in the limitations that "Performance on other Apple SoCs (e.g., M1, M4) may differ."
- Why unresolved: The entire empirical evaluation was conducted on a single machine class (Mac Studio M2 Ultra), leaving the generalizability of MLX's throughput dominance across different memory bandwidths and core counts unproven.
- What evidence would resolve it: Replicating the specific benchmark suite (TTFT, throughput, long-context) on base M1, M3, and M4 hardware to compare framework scaling.

**Open Question 3**
- Question: Can external orchestration layers effectively close the concurrency gap between Apple-native runtimes and server-grade solutions like vLLM?
- Basis in paper: [inferred] The discussion posits that "adding a thin external orchestrator (e.g., Ray Serve, BentoML) allows MLX to close much of the concurrency gap" without providing experimental validation for this specific configuration.
- Why unresolved: While the paper measures raw single-stream and limited concurrent performance, it does not benchmark the proposed "hybrid" architecture of MLX wrapped in an external scheduler.
- What evidence would resolve it: A comparative study of MLX + Ray Serve against MLC-LLM and vLLM under high-concurrency synthetic request loads.

**Open Question 4**
- Question: To what extent would implementing chunked prefill improve TTFT for Apple-native frameworks handling extremely long contexts?
- Basis in paper: [inferred] The authors identify that "Neither MLX nor MLC yet implement chunked prefill as in vLLM," resulting in poor TTFT for 100k+ token inputs, but do not quantify the potential gains of such an implementation.
- Why unresolved: This is a specific architectural limitation identified in the long-context analysis, but the theoretical or practical improvement ceiling remains unexplored.
- What evidence would resolve it: Engineering a chunked prefill mechanism for MLX or MLC-LLM and measuring the resulting TTFT reduction on 100k-token prompts.

## Limitations

- **Dataset and Reproducibility Gaps:** The study does not provide access to the exact prompt corpora used across the 1k-100k token range, requiring researchers to synthesize or locate datasets independently.
- **Sparse Corpus Support:** Although MLX is compared to four other frameworks, the surrounding literature provides stronger validation for MLX's performance and Apple-specific optimizations than for the comparative frameworks.
- **Scope and Generalizability:** Results are tied to the specific Mac Studio M2 Ultra configuration (192GB RAM, 76 GPU cores) and may not extrapolate directly to other Apple Silicon devices or configurations.

## Confidence

**High Confidence:** The core finding that MLX delivers the highest sustained throughput (~230 tok/s) and stable per-token latency is well-supported by hardware-specific evidence (Metal kernel saturation >90%, low CPU usage). The comparison between MLX and MLC-LLM regarding throughput vs. TTFT tradeoffs is also highly reliable given consistent measurements across prompt lengths.

**Medium Confidence:** The analysis of long-context performance via paged KV caching in MLC-LLM is plausible but relies partially on internal implementation details that are not independently verified in the broader literature. The 4GB tensor cap constraint for PyTorch MPS is stated clearly but lacks external corroboration, introducing moderate uncertainty.

**Low Confidence:** Ollama's performance profile (low throughput, high TTFT at long contexts) is inferred but not deeply benchmarked against the other frameworks across all metrics, leaving some comparative claims weakly supported.

## Next Checks

1. **Prompt Corpus Verification:** Obtain or reconstruct the exact 1k-100k token prompt sets (unique-token, prefix-heavy, code-dominant) to ensure experimental fidelity. Compare synthetic datasets against reported distributions to validate reproducibility.

2. **Framework-Specific Constraint Testing:** Isolate and test the 4GB tensor cap behavior in PyTorch MPS on models just below and above the threshold to confirm whether this is a fundamental backend limitation or an implementation-specific constraint. Repeat for MLX and MLC-LLM to ensure consistent memory handling.

3. **Long-Context KV Cache Benchmarking:** Systematically compare KV cache management (paged vs. rotating vs. sliding window) across MLX, MLC-LLM, and llama.cpp at 32k, 64k, and 100k tokens. Measure memory overhead, recomputation costs, and throughput degradation to validate the efficiency claims for long-context inference.