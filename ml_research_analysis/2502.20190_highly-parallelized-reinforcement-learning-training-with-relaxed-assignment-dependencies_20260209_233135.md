---
ver: rpa2
title: Highly Parallelized Reinforcement Learning Training with Relaxed Assignment
  Dependencies
arxiv_id: '2502.20190'
source_url: https://arxiv.org/abs/2502.20190
tags:
- training
- tianji
- learning
- time
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TianJi, a distributed RL training system
  designed to address performance bottlenecks caused by assignment dependencies in
  parallel DRL training. The system relaxes assignment dependencies between subtask
  components through decentralized, data-driven training and event-driven asynchronous
  communication.
---

# Highly Parallelized Reinforcement Learning Training with Relaxed Assignment Dependencies

## Quick Facts
- arXiv ID: 2502.20190
- Source URL: https://arxiv.org/abs/2502.20190
- Authors: Zhouyu He; Peng Qiao; Rongchun Li; Yong Dou; Yusong Tan
- Reference count: 40
- Primary result: TianJi achieves up to 4.37× faster convergence time compared to baseline systems

## Executive Summary
This paper introduces TianJi, a distributed reinforcement learning training system that addresses performance bottlenecks caused by assignment dependencies in parallel DRL training. The system relaxes assignment dependencies between subtask components through decentralized, data-driven training and event-driven asynchronous communication. To ensure convergence despite relaxed dependencies, TianJi employs a distributed strategy based on the balance of sample production and consumption, controlling sample staleness to maintain quality. The system demonstrates significant performance improvements over existing baselines.

## Method Summary
TianJi introduces a novel approach to distributed RL training by relaxing assignment dependencies between subtask components. The system implements decentralized, data-driven training with event-driven asynchronous communication. A key innovation is the use of a distributed strategy that balances sample production and consumption to control sample staleness. This approach allows for more flexible parallel processing while maintaining convergence quality. The system is designed to work with both off-policy and on-policy algorithms, with specific mechanisms to ensure stable training despite the relaxed dependencies.

## Key Results
- Convergence time acceleration ratio of up to 4.37× compared to baseline systems
- 1.6× convergence time speedup and 7.13× throughput speedup when scaled to 8 computational nodes versus XingTian
- Data transmission efficiency approaching hardware limits
- On-policy algorithms show convergence time acceleration ratios of 4.36× and 2.95× compared to RLlib and XingTian respectively

## Why This Works (Mechanism)
TianJi works by relaxing assignment dependencies in parallel DRL training, which traditionally create bottlenecks due to sequential processing requirements. By implementing decentralized, data-driven training with event-driven asynchronous communication, the system can process multiple components simultaneously without strict ordering constraints. The sample staleness control mechanism ensures that even with relaxed dependencies, the quality of training samples remains sufficient for convergence. This approach allows for better resource utilization and reduces idle time between training steps.

## Foundational Learning
1. Assignment dependencies in DRL: Why needed - Traditional DRL requires sequential processing of components; Quick check - Identify critical path dependencies in your RL algorithm
2. Sample staleness control: Why needed - Prevents degradation of training quality when relaxing dependencies; Quick check - Monitor correlation between staleness and performance metrics
3. Event-driven asynchronous communication: Why needed - Enables flexible parallel processing; Quick check - Verify message queue depths and processing latencies
4. Decentralized training architecture: Why needed - Eliminates single points of failure and improves scalability; Quick check - Test system behavior with node failures

## Architecture Onboarding

Component map: Environment Simulation -> Sample Collection -> Model Update -> Parameter Server -> Environment Simulation

Critical path: Sample production (environment simulation) and consumption (model training) must be balanced to prevent staleness

Design tradeoffs:
- Relaxed dependencies vs. training stability
- Asynchronous communication vs. coordination overhead
- Sample freshness vs. parallel processing efficiency

Failure signatures:
- Training divergence indicates excessive sample staleness
- Performance plateaus suggest bottlenecks in sample production or consumption
- High message queue depths indicate communication inefficiencies

3 first experiments:
1. Compare convergence speed with varying levels of dependency relaxation
2. Measure sample staleness impact on training quality across different algorithms
3. Benchmark communication overhead with different event-driven messaging patterns

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Performance improvements may be highly dependent on specific hardware configuration
- Results primarily demonstrated on specific RL environments and algorithms
- Scalability beyond 8 nodes not thoroughly evaluated
- On-policy algorithm performance improvements may not generalize to all scenarios

## Confidence
- Performance improvements: Medium
- Scalability results: Medium
- Hardware efficiency claims: Medium

## Next Checks
1. Test TianJi's performance across a wider range of RL environments and algorithms to verify generalizability of the reported improvements.
2. Conduct experiments with larger cluster sizes (e.g., 16, 32, or 64 nodes) to validate scalability claims and identify potential bottlenecks at larger scales.
3. Perform ablation studies to isolate the contribution of each component (relaxed dependencies, event-driven communication, sample staleness control) to the overall performance improvements.