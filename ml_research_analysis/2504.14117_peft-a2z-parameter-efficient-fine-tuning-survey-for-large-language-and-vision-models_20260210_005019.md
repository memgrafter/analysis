---
ver: rpa2
title: 'PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision
  Models'
arxiv_id: '2504.14117'
source_url: https://arxiv.org/abs/2504.14117
tags:
- arxiv
- language
- preprint
- fine-tuning
- peft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews Parameter-Efficient Fine-Tuning
  (PEFT) methods for adapting large-scale pre-trained models across natural language
  processing, computer vision, and multimodal learning domains. It introduces a structured
  taxonomy grouping PEFT techniques into additive, selective, reparameterized, hybrid,
  and unified frameworks, providing clear design comparisons.
---

# PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models

## Quick Facts
- arXiv ID: 2504.14117
- Source URL: https://arxiv.org/abs/2504.14117
- Reference count: 40
- Primary result: Comprehensive taxonomy and evaluation of PEFT methods showing up to 95% parameter reduction while maintaining or exceeding full fine-tuning performance

## Executive Summary
This survey systematically reviews Parameter-Efficient Fine-Tuning (PEFT) methods for adapting large-scale pre-trained models across natural language processing, computer vision, and multimodal learning domains. It introduces a structured taxonomy grouping PEFT techniques into additive, selective, reparameterized, hybrid, and unified frameworks, providing clear design comparisons. The paper evaluates representative methods such as LoRA, adapters, prompt tuning, and their variants on benchmark tasks including GLUE, commonsense reasoning, and visual recognition, demonstrating that modern PEFT approaches achieve performance close to or exceeding full fine-tuning while reducing trainable parameters by up to 95%.

## Method Summary
The survey categorizes PEFT methods into five main frameworks: additive (serial/parallel/hybrid adapters, soft prompts), selective (parameter importance-based), reparameterized (LoRA and low-rank variants), hybrid, and Mixture-of-Experts approaches. It evaluates these methods on GLUE benchmark tasks and LLM reasoning tasks using RoBERTaBase (124.6M params) and RoBERTaLarge (355.3M params) for GLUE, and BLOOMZ7B, GPT-J6B, LLaMA-2 7B/13B for reasoning. The evaluation targets matching or exceeding full fine-tuning performance while using <1% trainable parameters, with specific metrics including MCC, accuracy, F1, Pearson/Spearman correlations for GLUE tasks and accuracy for reasoning tasks.

## Key Results
- PEFT methods achieve up to 95% reduction in trainable parameters compared to full fine-tuning while maintaining comparable or better performance
- LoRA-based variants demonstrate strong performance across NLP and vision tasks with minimal parameter overhead
- Modern PEFT approaches show robustness across diverse tasks including GLUE benchmark, commonsense reasoning, and mathematical problem-solving
- The survey identifies specific break conditions where PEFT methods fail, particularly for out-of-distribution tasks or when the intrinsic dimensionality exceeds low-rank approximations

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Adaptation (LoRA)
The weight updates required for downstream tasks often reside in a low-dimensional intrinsic subspace, allowing for efficient approximation without modifying the full parameter space. Instead of updating a dense weight matrix $W \in \mathbb{R}^{d \times d}$, LoRA freezes $W$ and injects trainable rank decomposition matrices $A \in \mathbb{R}^{d \times r}$ and $B \in \mathbb{R}^{r \times d}$, where $r \ll d$. The forward pass becomes $h = W_0 x + \Delta W x = W_0 x + BAx$. The core assumption is that the pre-trained model is over-parameterized for any single specific downstream task, and the "change in weights" ($\Delta W$) has a low intrinsic rank. This breaks when the downstream task requires complex reasoning that significantly alters the model's fundamental internal representations (high intrinsic dimensionality), leading to underfitting.

### Mechanism 2: Additive Adapter Modulation
Task-specific knowledge can be encapsulated in small, isolated bottleneck modules without disrupting the general representations learned by the backbone. Small neural network modules (Adapters) are inserted into the Transformer layers (serially or in parallel). These modules typically consist of a down-projection, a non-linear activation, and an up-projection, trained while the main model weights remain frozen. The core assumption is that the pre-trained model acts as a robust feature extractor, and the "gap" to a downstream task can be bridged by a lightweight transformation. This breaks when adapters are too narrow (bottleneck dimension is too small), causing performance collapse, or when placed poorly, destabilizing the residual stream.

### Mechanism 3: Soft Prompt Steering
The model's existing capabilities can be elicited via continuous input optimization, moving the "learning" entirely into the input embedding space. Instead of discrete text prompts, learnable continuous vectors ("soft prompts") are prepended to the input or intermediate layers. These vectors are optimized via gradient descent to minimize task loss, effectively shifting the model's attention context. The core assumption is that the pre-trained model possesses the requisite knowledge and reasoning capabilities; it merely requires optimal context activation. This breaks when the task requires knowledge strictly outside the pre-training distribution (out-of-distribution), where soft prompting cannot synthesize new capabilities.

## Foundational Learning

- **Concept: Transformer Self-Attention**
  - Why needed here: PEFT methods heavily target the Multi-Head Self-Attention (MHSA) weights ($W_Q, W_K, W_V$) for modification or adapter insertion.
  - Quick check question: Can you calculate the output of a scaled dot-product attention layer given Query, Key, and Value matrices?

- **Concept: Transfer Learning vs. Fine-Tuning**
  - Why needed here: PEFT is essentially a constrained form of transfer learning. Understanding the trade-off between "general knowledge" (frozen weights) and "task-specific adaptation" (tunable parameters) is critical.
  - Quick check question: What is the primary risk of updating all parameters in a model during fine-tuning (hint: see Section 3.7)?

- **Concept: Bias-Variance Trade-off**
  - Why needed here: The survey explicitly links PEFT to mitigating overfitting (high variance) in low-resource settings (Section 3.11). Understanding how reducing trainable parameters regularizes the model is key.
  - Quick check question: Why does limiting the number of trainable parameters (PEFT) often improve performance on small datasets compared to full fine-tuning?

## Architecture Onboarding

- **Component map:** Frozen pre-trained Transformer (backbone) -> PEFT wrapper (intercepts forward/backward passes) -> Tunable delta (small parameters: LoRA matrices, Adapter weights, Prompt embeddings) -> Merger (optional post-training step to merge $\Delta W$ into $W$)

- **Critical path:**
  1. Initialization: Load backbone with `requires_grad=False`
  2. Injection: Add PEFT modules (e.g., initialize LoRA $A$ with Gaussian, $B$ with zero so $\Delta W$ starts at 0)
  3. Forward Pass: Input $x$ flows through backbone; at targeted layers, $x$ is transformed by $x + \text{PEFT}(x)$
  4. Backward Pass: Gradients update only the PEFT parameters
  5. Merge: (If LoRA) Absorb $BA$ into $W$ for deployment

- **Design tradeoffs:**
  - LoRA vs. Adapters: LoRA adds no inference latency (if merged) but modifies weights directly. Adapters add latency (extra layers) but keep the original weights strictly pristine and are easier to swap.
  - Rank ($r$): Higher rank = more expressivity but higher memory. Lower rank = better regularization but risk of underfitting.
  - Target Modules: Targeting all linear layers (q, k, v, o, mlp) improves performance but increases parameter count vs. targeting only attention weights.

- **Failure signatures:**
  - Sudden Loss Spike: Often caused by learning rate instability in low-rank optimization; requires warmup or lower LR.
  - Zero Gradients: Occurs if the PEFT module is initialized poorly or inserted at a layer where activation is dead.
  - Catastrophic Forgetting: Rare in PEFT, but can happen if the "scaling factor" (e.g., LoRA Alpha) is set too high, overwhelming the frozen backbone.

- **First 3 experiments:**
  1. Sensitivity Analysis (Rank Sweep): Fine-tune a RoBERTa-base model on a GLUE task (e.g., SST-2) using LoRA with ranks $r \in \{1, 4, 8, 16, 64\}$ to plot the performance vs. parameter curve.
  2. Adapter vs. LoRA Latency Test: Deploy a model with parallel adapters vs. merged LoRA on a standard GPU. Measure the tokens/second throughput difference to quantify the inference overhead of adapters.
  3. Memory Benchmark: Run full fine-tuning vs. LoRA (rank=8) vs. QLoRA (4-bit quantization) on a single GPU to measure peak VRAM usage reduction.

## Open Questions the Paper Calls Out

### Open Question 1
How can theoretical frameworks (e.g., information theory, optimization theory) be developed to quantitatively explain the influence of specific trainable parameter subsets on overall model adaptation? [explicit] Section 11.1 states that most PEFT methods rely on "empirical success rather than analytical rigor" and suggests leveraging concepts like mutual information to quantify parameter influence. Why unresolved: Current design choices for where to place adapters or how to rank parameters are often heuristic, lacking a principled mathematical understanding of why specific parameter updates generalize better than others. What evidence would resolve it: Mathematical models establishing a correlation between parameter sensitivity metrics (like Fisher information) and task performance, or proofs demonstrating the intrinsic dimensionality required for specific adaptation tasks.

### Open Question 2
How can PEFT techniques be effectively generalized to non-transformer architectures, such as Graph Neural Networks (GNNs) or Recurrent Neural Networks (RNNs)? [explicit] Section 11.4 notes that while PEFT has succeeded with transformers, exploration of strategies for non-transformer backbones "remains largely uncharted." Why unresolved: Most PEFT innovations (like LoRA) are tailored to the self-attention mechanism; it is unclear how to inject low-rank updates or adapters into architectures that rely on message passing or recurrence without creating information bottlenecks. What evidence would resolve it: Novel PEFT formulations for GNNs/RNNs that demonstrate parameter efficiency comparable to transformer-based methods on standard graph or sequence tasks.

### Open Question 3
What standardized, multimodal benchmark suites are required to evaluate PEFT robustness against domain shift, noise injection, and adversarial perturbations? [explicit] Section 11.8 identifies an "urgent need for standardized, multimodal benchmark suites" and explicitly calls for robustness tests including "noise injection" and "adversarial perturbations." Why unresolved: Current evaluations are fragmented across single domains (NLP or Vision) and often focus only on clean accuracy, failing to expose how parameter efficiency impacts model fragility in real-world scenarios. What evidence would resolve it: The adoption of a unified benchmark suite (similar to GLUE but specific to PEFT) that ranks methods based on stability and robustness metrics in addition to raw performance.

## Limitations
- Missing detailed training configurations (learning rates, batch sizes, optimizer settings) for each PEFT method makes exact reproduction challenging
- Reported performance gains are task-averaged, potentially masking scenarios where certain methods underperform on specific tasks
- Limited theoretical analysis explaining why PEFT methods work beyond empirical observations and stated assumptions

## Confidence
- High Confidence: Taxonomy categorization of PEFT methods and their general mechanism descriptions
- Medium Confidence: Reported performance gains (up to 95% parameter reduction) supported by benchmark results but exact reproducibility limited
- Low Confidence: Claims about future research directions and theoretical understanding gaps are speculative

## Next Checks
1. Implement LoRA and adapter methods using standard libraries (HuggingFace PEFT) and reproduce reported GLUE benchmark results on RoBERTaBase
2. Conduct head-to-head comparisons of LoRA, adapters, and soft prompting on identical tasks using consistent training protocols, measuring accuracy, inference latency, and memory usage
3. Systematically test PEFT methods on out-of-distribution tasks and small datasets to identify break conditions and document performance degradation patterns