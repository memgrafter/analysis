---
ver: rpa2
title: 'SAISA: Towards Multimodal Large Language Models with Both Training and Inference
  Efficiency'
arxiv_id: '2502.02458'
source_url: https://arxiv.org/abs/2502.02458
tags:
- visual
- saisa
- tokens
- attention
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAISA, an architecture designed to improve
  both training and inference efficiency of multimodal large language models (MLLMs).
  The authors identify a key inefficiency in existing MLLMs - attention among visual
  tokens in self-attention blocks, which leads to quadratically growing computational
  costs during inference.
---

# SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency

## Quick Facts
- arXiv ID: 2502.02458
- Source URL: https://arxiv.org/abs/2502.02458
- Reference count: 40
- Primary result: 66% inference FLOPs reduction and 26% training budget reduction while maintaining or improving accuracy on multiple benchmarks

## Executive Summary
SAISA introduces a novel architecture for multimodal large language models (MLLMs) that significantly improves both training and inference efficiency. The key innovation is NAAViT (No Attention Among Visual Tokens), which eliminates redundant attention computations among visual tokens by making visual tokens serve only as keys and values, not queries. Building on this, SAISA aligns visual features directly with NAAViT self-attention block inputs while bypassing feed-forward networks (FFNs) for visual tokens. This architecture achieves 66% reduction in inference FLOPs and 26% reduction in training budget compared to LLaVA-1.5, while maintaining or exceeding performance across various benchmarks including MMMU, MMBench, POPE, and OK-VQA.

## Method Summary
SAISA modifies the standard MLLM architecture by introducing NAAViT attention, which prevents visual tokens from attending to each other while allowing text tokens to attend to both visual and text tokens. Visual features extracted by a CLIP-ViT encoder are projected through n distinct two-layer MLPs (one per LLM layer) directly into each NAAViT self-attention block's input space. Crucially, FFNs are only applied to update text token hidden states, not visual tokens. The model is pre-trained using a shared MLP for efficiency, then fine-tuned with layer-specific MLPs initialized from the shared weights. Training uses 558k pre-training samples and 665k fine-tuning samples on datasets including ShareGPT, VQAv2, GQA, and others, with total training time of approximately 80 GPU hours on 8×A800 GPUs.

## Key Results
- 66% reduction in inference FLOPs compared to LLaVA-1.5
- 26% reduction in training budget (80 vs 108 GPU hours)
- Superior or comparable accuracy on MMMU, MMBench, POPE, ScienceQA IMG, and OK-VQA benchmarks
- NAAViT matches or exceeds vanilla self-attention across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Eliminating visual-to-visual attention
- Visual tokens serve only as keys and values, not queries
- Reduces attention complexity from O((v+t)²) to O(t(v+t))
- Assumes visual encoders already capture intra-image relationships
- Validated by NAAViT matching or exceeding vanilla self-attention performance

### Mechanism 2: Bypassing FFNs for visual tokens
- Visual tokens projected directly to self-attention input space but skip FFN layer
- Only text token hidden states are updated by FFNs
- Assumes visual features are sufficiently expressive post-encoder
- Reduces FFN computations from 2n(t+v)h·3m to 2nth·3m

### Mechanism 3: Layer-specific projectors
- Uses n distinct two-layer MLPs (one per LLM layer) for visual feature alignment
- Each projector maps visual features to its layer's self-attention input space
- Enables specialized alignment per layer rather than shared representation
- Pre-trained with shared MLP then fine-tuned with layer-specific MLPs

## Foundational Learning

- **Quadratic attention complexity**: Self-attention scales as O(n²); with v ≫ t, visual-to-visual attention dominates computation
  - Why needed: Explains why eliminating visual-to-visual attention yields significant efficiency gains
  - Quick check: With 576 visual tokens and 64 text tokens, what fraction of FLOPs comes from visual-to-visual attention?

- **Embedding space vs cross-attention space alignment**: Trade-off between projecting to LLM embedding space vs inserting cross-attention blocks
  - Why needed: SAISA is motivated by LLaVA-1.5's training efficiency but inference inefficiency, and Flamingo's opposite tradeoff
  - Quick check: Which introduces fewer parameters - single projector to embedding space or cross-attention blocks across all layers?

- **Visual encoder feature expressiveness**: CLIP-ViT already performs self-attention among image patches
  - Why needed: Justifies why LLM self-attention over visual tokens may be redundant
  - Quick check: Does CLIP-ViT perform self-attention on patches? What does this suggest about redundancy in LLM self-attention?

## Architecture Onboarding

- **Component map**: Image → Visual encoder → Z → Projectors → {V₁,...,Vₙ} → NAAViT blocks → Text outputs
- **Critical path**: 
  1. Image → CLIP-ViT → Z (visual features)
  2. Z → n MLPs → {V₁,...,Vₙ} (layer-aligned visual features)
  3. Each layer i: (Vᵢ, Tᵢ) → NAAViTᵢ → Hᵢ (text-only) → FFNᵢ → Tᵢ₊₁
  4. Final Tₙ → LM head → output tokens

- **Design tradeoffs**:
  - Parameter overhead: n MLPs vs single projector, mitigated by shared-to-specific initialization
  - Flexibility vs simplicity: Layer-specific alignment vs shared representation
  - Compatibility: Orthogonal to token pruning but may complicate concatenated sequence implementations

- **Failure signatures**:
  - Performance drop on fine-grained spatial reasoning tasks
  - Training instability if pre-training data insufficient for shared MLP initialization
  - Integration issues with codebases expecting concatenated token sequences

- **First 3 experiments**:
  1. Replicate pilot experiment: Replace vanilla self-attention with NAAViT in LLaVA-1.5 on MMMU, MMBench, POPE
  2. Ablate FFN bypass: Compare SAISA with and without FFN on visual tokens
  3. Test projector design: Compare linear vs two-layer MLP per layer on benchmark performance

## Open Questions the Paper Calls Out

- How to optimize SAISA projector architecture to reduce parameter overhead while maintaining performance
- Whether SAISA can be extended to process complex visual inputs like high-resolution images, videos, and interleaved text-image content
- If eliminating visual token attention impairs performance on complex reasoning tasks or specialized domains like medicine

## Limitations

- NAAViT mechanism may break down for tasks requiring fine-grained spatial reasoning or complex visual relationship understanding
- FFN bypass mechanism lacks theoretical justification and thorough empirical validation across diverse task types
- Layer-specific projector design introduces architectural complexity without comprehensive ablation across different visual encoders

## Confidence

- **High confidence**: Training efficiency improvements (26% reduction) and inference FLOPs reduction (66%) are well-supported
- **Medium confidence**: Accuracy improvements are demonstrated but may be sensitive to specific dataset distributions
- **Low confidence**: Fundamental assumption about visual-to-visual attention redundancy lacks external validation

## Next Checks

1. Evaluate SAISA on CVBench 3D tasks and other fine-grained spatial reasoning benchmarks to identify potential performance degradation
2. Implement variants with partial FFN updates on visual tokens (e.g., every k layers) to quantify FFN bypass contribution to efficiency and accuracy
3. Test SAISA with alternative visual encoders (SigLIP, OpenCLIP) and smaller LLM configurations to assess generalizability beyond Vicuna-7B + CLIP-ViT-L