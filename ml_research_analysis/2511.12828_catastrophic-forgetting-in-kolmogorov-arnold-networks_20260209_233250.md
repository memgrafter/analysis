---
ver: rpa2
title: Catastrophic Forgetting in Kolmogorov-Arnold Networks
arxiv_id: '2511.12828'
source_url: https://arxiv.org/abs/2511.12828
tags:
- forgetting
- tasks
- task
- kans
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies catastrophic forgetting in Kolmogorov-Arnold
  Networks (KANs), a neural architecture that uses learnable spline activations. The
  authors develop a theoretical framework showing that forgetting scales linearly
  with activation support overlap and exponentially with task complexity (intrinsic
  data dimension).
---

# Catastrophic Forgetting in Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2511.12828
- Source URL: https://arxiv.org/abs/2511.12828
- Authors: Mohammad Marufur Rahman; Guanchu Wang; Kaixiong Zhou; Minghan Chen; Fan Yang
- Reference count: 40
- Primary result: KANs exhibit strong retention in low-dimensional tasks but increasing forgetting in high-dimensional domains, with forgetting scaling linearly with activation support overlap and exponentially with intrinsic data dimension

## Executive Summary
This paper investigates catastrophic forgetting in Kolmogorov-Arnold Networks (KANs), a neural architecture that replaces fixed activations with learnable spline functions. The authors develop a theoretical framework showing that forgetting scales linearly with activation support overlap and exponentially with task complexity (intrinsic data dimension). Through experiments on synthetic arithmetic, image classification, and language model fine-tuning tasks, they demonstrate that KANs show strong retention in low-dimensional tasks but exhibit increasing forgetting in high-dimensional domains. A novel KAN-based LoRA adapter (KAN-LoRA) is introduced for continual LM fine-tuning, demonstrating superior retention compared to MLP-based adapters, particularly in low-sample regimes and smaller models.

## Method Summary
The paper evaluates catastrophic forgetting in KANs across three experimental settings: sequential binary/decimal addition (5 tasks), class-incremental image classification (5 tasks, 2 classes each), and continual knowledge editing with KAN-LoRA on Llama2 (5 sequential edits). KANs use learnable B-spline activations with configurable grid size and spline order. The theoretical framework links forgetting to activation support overlap (Δ_i,j) and intrinsic data dimension (d_i, d_j), deriving bounds that show F_i ≤ C·Σ(N_ℓ·L_ℓ·Δ_i,j) and F_i = O(T·N_tot·L̄·r^{d_i + d_j}). KAN-LoRA replaces MLP LoRA adapters with KAN layers, parameterizing both A and B matrices using localized spline updates.

## Key Results
- KANs show near-zero forgetting (MSE < 1e-6) on binary addition tasks due to disjoint activation supports
- Forgetting increases exponentially with intrinsic data dimension, with log(F_i)/d_i ≈ constant across MNIST, CIFAR-10, and Tiny-ImageNet
- KAN-LoRA achieves 100% retention on Task 1 vs. MLP-LoRA's degradation by Task 3, especially strong at rank 16 with limited samples
- Grid size directly controls forgetting granularity: larger grids reduce overlap and improve retention in high-dimensional tasks

## Why This Works (Mechanism)

### Mechanism 1: Activation Support Overlap Governs Retention
- Claim: Forgetting in KANs scales linearly with the overlap between activation supports across tasks, enabling zero forgetting when supports are disjoint.
- Mechanism: KANs use localized B-spline activations where each branch ϕ_ℓ,p,q has an "activation support" S = {z ∈ R : ϕ(z) ≠ 0}. When training on task j, only branches whose supports overlap with task i's supports receive gradient updates. If Δ_i,j = 0 (max overlap is zero), parameters for task i remain frozen.
- Core assumption: Spline activations maintain L-Lipschitz continuity; loss function is bounded.
- Evidence anchors: [abstract] links forgetting to activation support overlap; Lemma 1 proves F_i = 0 when Δ_i,j = 0; Theorem 1 shows F_i ≤ C·Σ(N_ℓ·L_ℓ·Δ_i,j); "Overcoming catastrophic forgetting in neural networks" discusses EWC's parameter-wise consolidation.

### Mechanism 2: Intrinsic Data Dimension Drives Exponential Forgetting
- Claim: Forgetting grows exponentially with the intrinsic dimensionality of task data manifolds.
- Mechanism: Tasks on d_t-dimensional manifolds require covering O(r^{-d_t}) activation regions. Higher dimensions reduce "gaps" between partitions exponentially, increasing overlap probability. The expected overlap E[μ(S_i ∩ S_j)] = O(r^{d_i + d_j}).
- Core assumption: Task data concentrates on compact submanifolds; each branch support fits within an r-ball.
- Evidence anchors: [abstract] mentions forgetting scales exponentially with task complexity; Theorem 3 derives F_i = O(T·N_tot·L̄·r^{d_i + d_j}); Table 3 shows log(F_i)/d_i ≈ constant across datasets.

### Mechanism 3: KAN-LoRA Exploits Localized Adaptation for Low-Sample Regimes
- Claim: Replacing MLP LoRA adapters with KAN-based adapters improves retention in low-sample continual fine-tuning.
- Mechanism: KAN-LoRA parameterizes both A and B matrices in ΔW = BA using KAN layers. The locality of spline updates means new knowledge edits affect fewer parameters from prior edits, reducing interference.
- Core assumption: EWC regularization is applied; Fisher memory from preceding tasks is available.
- Evidence anchors: [abstract] states KAN-LoRA outperforms MLP-based alternatives in low-sample regimes; Table 4 shows 100% retention on Task 1 vs. MLP-LoRA's degradation; Table 5 shows ~10× more parameters than MLP-LoRA.

## Foundational Learning

- **Catastrophic Forgetting / Interference**
  - Why needed here: The entire paper frames KAN behavior through this lens; understanding that sequential training overwrites prior knowledge is prerequisite.
  - Quick check question: Can you explain why MLPs are particularly vulnerable to forgetting compared to sparse architectures?

- **B-Spline Functions and Locality**
  - Why needed here: KAN's core innovation is replacing fixed activations with learnable splines; understanding that splines have local support (only nearby points affect coefficients) is essential.
  - Quick check question: If you shift input x by 0.1, which spline coefficients change—and why does this matter for continual learning?

- **Intrinsic Dimensionality of Data Manifolds**
  - Why needed here: Theoretical bounds depend on d_i, d_j (intrinsic dimensions), not ambient dimensions. This explains why MNIST (low d) shows better retention than Tiny-ImageNet (high d).
  - Quick check question: Why would two datasets with the same number of pixels but different intrinsic dimensions exhibit different forgetting rates?

## Architecture Onboarding

- **Component map:**
  Input → [KAN Layer: Φ_ℓ with spline functions ϕ_ℓ,p,q] × L layers → Output
  Each ϕ = base_fn + spline_fn (B-spline with grid)
  Grid size controls resolution; spline order controls smoothness

- **Critical path:**
  1. Initialize KAN with grid_size (5–20 typical), spline_order (3 = cubic), grid_range ([-1,1])
  2. Forward pass: compute spline activations per edge, sum across branches
  3. Backward pass: only branches with non-zero activation on current batch receive updates
  4. For continual learning: track which branches are "owned" by which task via support estimation

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Higher grid_size | Finer granularity, better retention (Figure 2) | More parameters, slower training |
  | Deeper networks | More expressivity | More forgetting (Figure 3) |
  | KAN-LoRA rank 16 | Better capacity | More forgetting than rank 8 |
  | Fragmentation (Corollary 4) | Reduces overlap exponentially | Coarser function approximation |

- **Failure signatures:**
  - High-dimensional image tasks with shallow KAN: accuracy drops sharply after 2–3 tasks (Figure 4)
  - KAN-LoRA with large samples per task: MLP-LoRA may match or exceed performance
  - Grid_size too small for task complexity: decimal addition shows forgetting; binary addition (simpler) does not

- **First 3 experiments:**
  1. **Binary addition sanity check**: Train 5-task sequence with grid_size=5; expect near-zero forgetting (MSE < 1e-6). This validates your KAN implementation matches paper behavior.
  2. **Grid size ablation on decimal addition**: Run 5-task decimal addition with grid_size ∈ {5,10,15,20}; plot forgetting vs. grid_size. Confirm linear relationship between F_i and Δ_i,j using Table 1 methodology.
  3. **Intrinsic dimension test**: Resize CIFAR-10 images to varying resolutions and quantize labels; compute d_i = log₂(Q×S); verify log(F_i)/d_i ≈ constant per Table 3. This tests Theorem 3's exponential claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can intentional support overlap enable distributed memory encoding that stores multiple tasks in compressed form with task-specific retrieval?
- Basis in paper: [explicit] The conclusion states "future models could intentionally overlap supports to store multiple tasks in a compressed fashion that allows task-specific retrieval through decoding strategies."
- Why unresolved: The paper's framework treats overlap as harmful (causing forgetting), but does not explore whether controlled overlap could enable compressed multi-task storage.
- What evidence would resolve it: Demonstration of a KAN variant that intentionally overlaps activation supports across tasks while maintaining separate task-specific decoding pathways, achieving memory compression without performance loss.

### Open Question 2
- Question: What optimal support fragmentation strategies (k_t splits per task) minimize forgetting while preserving representation capacity in high-dimensional domains?
- Basis in paper: [explicit] Corollary 4 shows fragmentation reduces forgetting as (k_i k_j)^{-(d_i+d_j)}, but no guidance on selecting k_t; conclusion suggests "lifecycle-based pruning of splines may enhance long-term retention."
- Why unresolved: The theoretical benefit is established, but practical fragmentation scheduling, adaptive k_t selection, and the trade-off with representation granularity remain unexplored.
- What evidence would resolve it: Empirical study comparing fixed, task-adaptive, and dynamically learned fragmentation strategies on high-dimensional benchmarks, measuring retention vs. task performance.

### Open Question 3
- Question: Can selective forgetting in high-dimensional regions function as a beneficial inductive bias for regularization and generalization?
- Basis in paper: [explicit] The conclusion proposes "forgetting itself may also function as a beneficial inductive bias" that could "suppress redundant or unstable features, reduce overfitting, and improve generalization."
- Why unresolved: The paper characterizes forgetting as harmful but does not test whether controlled decay in overlapping high-dimensional supports improves generalization.
- What evidence would resolve it: Experiments comparing KANs with encouraged vs. inhibited forgetting in overlapping regions, measuring out-of-distribution generalization and overfitting metrics.

### Open Question 4
- Question: Do the theoretical forgetting bounds hold under relaxed assumptions: non-Lipschitz activations, unbounded loss, and non-compact data distributions?
- Basis in paper: [inferred] Theorems 1-3 require L-Lipschitz activations, C-bounded loss, and compact submanifolds; real-world neural network activations and losses often violate these.
- Why unresolved: The gap between theoretical assumptions and practical KAN implementations (e.g., ReLU-like splines, cross-entropy loss) is not addressed.
- What evidence would resolve it: Analytical extensions or counterexamples showing whether the linear-overlap and exponential-dimension forgetting rates persist without these regularity conditions.

## Limitations

- Theoretical forgetting bounds require strong assumptions (L-Lipschitz activations, C-bounded loss, compact submanifolds) that may not hold in practice
- KAN-LoRA introduces ~10× more parameters than MLP-LoRA, raising computational efficiency concerns despite retention benefits
- No guidance provided on computing activation support overlap Δ_i,j in practice, which is critical for applying the theoretical framework

## Confidence

- **High**: Linear forgetting-scaling with activation support overlap (validated across synthetic tasks)
- **Medium**: Exponential forgetting with intrinsic data dimension (empirical trend observed but theoretical assumptions may not hold in practice)
- **Medium**: KAN-LoRA superiority in low-sample regimes (demonstrated but dependent on specific hyperparameter choices)

## Next Checks

1. Implement and verify the activation support overlap computation algorithm (Δ_i,j) on synthetic tasks to test the linear forgetting bound
2. Conduct ablation studies varying intrinsic data dimension through controlled synthetic manifolds to confirm exponential forgetting scaling
3. Compare KAN-LoRA parameter efficiency against alternative adapters (AdapterDrop, BitFit) beyond the MLP baseline in Table 5