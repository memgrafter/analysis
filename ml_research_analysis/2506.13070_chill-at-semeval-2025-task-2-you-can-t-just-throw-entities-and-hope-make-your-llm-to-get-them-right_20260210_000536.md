---
ver: rpa2
title: 'CHILL at SemEval-2025 Task 2: You Can''t Just Throw Entities and Hope -- Make
  Your LLM to Get Them Right'
arxiv_id: '2506.13070'
source_url: https://arxiv.org/abs/2506.13070
tags:
- translation
- entity
- language
- feedback
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate named entity translation
  in machine translation by combining Retrieval-Augmented Generation (RAG) with self-refinement
  techniques. The approach retrieves entity information from Wikidata and uses it
  to prompt a large language model (LLM), then iteratively refines translations based
  on feedback evaluating entity accuracy and overall translation quality.
---

# CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right

## Quick Facts
- arXiv ID: 2506.13070
- Source URL: https://arxiv.org/abs/2506.13070
- Reference count: 7
- Primary result: Achieved harmonic mean scores up to 94.23 on COMET and M-ETA metrics, significantly improving entity-aware translation across 10 language pairs

## Executive Summary
This paper tackles the challenge of accurate named entity translation in machine translation by combining Retrieval-Augmented Generation (RAG) with self-refinement techniques. The authors demonstrate that simply relying on LLMs to handle entity translation without additional support leads to poor performance, and instead propose a system that retrieves entity information from Wikidata to enhance translation accuracy. The approach shows substantial improvements over baseline GPT-4o performance, achieving up to 94.23 harmonic mean scores compared to baseline scores around 56-62, while also demonstrating the effectiveness of iterative refinement in catching both entity-specific and general translation errors.

## Method Summary
The approach combines RAG-based entity retrieval with iterative self-refinement. The system first retrieves entity information from Wikidata using the input text, then uses this retrieved information to prompt a large language model for translation. The self-refinement component iteratively improves translations based on feedback evaluating both entity accuracy and overall translation quality. This two-stage approach addresses the limitations of standard LLM translation by providing explicit entity context and allowing for iterative improvement based on quality metrics.

## Key Results
- Achieved harmonic mean scores of up to 94.23 on COMET and M-ETA metrics
- Self-refinement further enhanced results by 0.19-1.66 percentage points
- Demonstrated significant improvements across 10 language pairs compared to GPT-4o baseline

## Why This Works (Mechanism)
The system works by addressing the fundamental limitation of LLMs in handling named entity translation without external context. By retrieving entity information from Wikidata and incorporating it into the translation prompt, the LLM receives explicit guidance about how entities should be translated. The self-refinement loop then provides iterative feedback that catches both entity-specific errors and general translation quality issues. This combination of external knowledge retrieval and iterative refinement creates a system that can handle the complexity of entity-aware translation while maintaining overall translation quality across diverse languages.

## Foundational Learning

**Named Entity Recognition and Translation**: Critical for identifying and correctly translating proper nouns, locations, and other named entities that often don't follow standard translation rules. Why needed: Entities frequently have specific transliterations or translations that differ from general word translation. Quick check: Can the system correctly identify and translate entities like person names, organizations, and locations?

**Retrieval-Augmented Generation (RAG)**: Combines information retrieval with generative models to provide context-aware responses. Why needed: LLMs lack up-to-date knowledge and may not have specific entity information needed for accurate translation. Quick check: Does the retrieval system successfully find relevant entity information from Wikidata for the input text?

**Iterative Refinement**: The process of repeatedly improving outputs based on feedback. Why needed: Initial translations often contain errors that can be caught and corrected through systematic feedback evaluation. Quick check: Does each refinement iteration show measurable improvement in translation quality metrics?

## Architecture Onboarding

**Component Map**: Input Text -> Entity Retrieval (Wikidata) -> Translation Prompt -> LLM Translation -> Feedback Evaluation -> Self-Refinement Loop -> Final Translation

**Critical Path**: The entity retrieval and translation prompt generation are the most critical components, as they directly impact the quality of the LLM's output. The self-refinement loop adds additional value but depends on the initial translation quality.

**Design Tradeoffs**: The system trades increased computational complexity and inference time for improved translation accuracy. The reliance on Wikidata provides rich entity information but may have coverage gaps. The iterative refinement improves quality but adds latency to the translation process.

**Failure Signatures**: Poor entity retrieval leads to incorrect translations, retrieval failures result in no entity context provided, and ineffective feedback evaluation can cause the refinement loop to converge to suboptimal solutions or fail to converge at all.

**First Experiments**:
1. Test entity retrieval accuracy on a diverse set of input texts with varying entity types
2. Evaluate baseline translation quality without entity retrieval to establish performance gap
3. Measure refinement loop convergence by tracking quality metrics across iterations

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies primarily on automated metrics rather than human judgments, potentially missing nuanced quality issues
- System dependence on Wikidata creates coverage gaps for entities not in the knowledge base
- Computational costs and inference time implications of the multi-stage approach are not adequately addressed
- Performance evaluation is limited to 10 specific language pairs, leaving generalizability uncertain

## Confidence

**High confidence**: The reported improvements in entity accuracy and overall translation quality are well-supported by experimental results with clear baseline comparisons and statistically significant improvements across multiple metrics.

**Medium confidence**: The effectiveness of the self-refinement loop is demonstrated, but lacks detailed analysis of which specific feedback types contribute most to improvements and how the loop converges in different scenarios.

**Medium confidence**: While case studies provide illustrative examples, the generalizability of these examples to broader translation scenarios remains uncertain without more systematic error analysis.

## Next Checks
1. Conduct human evaluation studies to validate automated metric improvements and assess subjective translation quality, particularly focusing on entity handling in ambiguous contexts.
2. Test system performance on additional language pairs spanning diverse language families and writing systems to evaluate generalizability beyond the current 10-language scope.
3. Perform ablation studies to quantify the individual contributions of RAG, self-refinement, and feedback mechanisms, and analyze computational overhead compared to baseline approaches.