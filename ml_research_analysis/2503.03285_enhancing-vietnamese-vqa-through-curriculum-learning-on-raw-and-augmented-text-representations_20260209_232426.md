---
ver: rpa2
title: Enhancing Vietnamese VQA through Curriculum Learning on Raw and Augmented Text
  Representations
arxiv_id: '2503.03285'
source_url: https://arxiv.org/abs/2503.03285
tags:
- training
- samples
- learning
- question
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving Visual Question
  Answering (VQA) performance for Vietnamese, a low-resource language, by proposing
  a training framework that combines paraphrase-based feature augmentation with dynamic
  curriculum learning. The key idea is to treat augmented samples (created via paraphrasing)
  as "easy" and original samples as "hard", and to dynamically adjust the ratio of
  these samples during training.
---

# Enhancing Vietnamese VQA through Curriculum Learning on Raw and Augmented Text Representations

## Quick Facts
- arXiv ID: 2503.03285
- Source URL: https://arxiv.org/abs/2503.03285
- Authors: Khoi Anh Nguyen; Linh Yen Vu; Thang Dinh Duong; Thuan Nguyen Duong; Huy Thanh Nguyen; Vinh Quang Dinh
- Reference count: 10
- Primary result: Dynamic curriculum learning with paraphrase-based feature augmentation improves Vietnamese VQA performance on OpenViVQA, with CIDEr scores and reduced standard deviation

## Executive Summary
This paper addresses the challenge of improving Visual Question Answering (VQA) performance for Vietnamese, a low-resource language, by proposing a training framework that combines paraphrase-based feature augmentation with dynamic curriculum learning. The key idea is to treat augmented samples (created via paraphrasing) as "easy" and original samples as "hard", and to dynamically adjust the ratio of these samples during training. This allows the model to gradually adapt to increasing task complexity. Experiments on the OpenViVQA dataset show consistent improvements across different model backbones, with notable gains in CIDEr scores and reduced standard deviation. Results on ViVQA are mixed, likely due to dataset limitations. The approach demonstrates effectiveness in leveraging external knowledge without additional annotated data and offers a promising direction for VQA in low-resource languages.

## Method Summary
The method employs a dual-encoder architecture with a text encoder (BARTpho-word or PhoBERT) and image encoder (ResNet18 or BEiTv2). A feature augmentation module fuses original question embeddings with 2 randomly sampled paraphrase embeddings through learned transformations and skip connections. The augmentation creates semantically richer representations that expose the model to linguistic variations without requiring additional annotated data. A dynamic curriculum learning strategy adjusts the probability of using augmented ("easy") vs. raw ("hard") samples during training, starting with higher augmentation ratios and progressively shifting toward original samples via linear decay. The framework is trained with AdamW optimizer (learning rate 1e-5) for 40 epochs with batch size 16.

## Key Results
- Consistent improvements across different model backbones on OpenViVQA, with CIDEr scores showing significant gains
- Reduced standard deviation across training runs compared to baseline models
- Optimal performance achieved with linear decay curriculum schedule from threshold 1.0 to 0.8
- Mixed results on ViVQA dataset, likely due to paraphrase quality issues with auto-translated samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining original question embeddings with paraphrase embeddings enriches text representation and improves model generalization.
- **Mechanism:** The augmentation module fuses the original embedding (E_O) with randomly sampled paraphrase embeddings (E_P) through learned transformations and skip connections. This creates a semantically richer representation that exposes the model to linguistic variations without requiring additional annotated data.
- **Core assumption:** Paraphrases capture meaningful linguistic diversity in Vietnamese that helps the model generalize better than raw embeddings alone.
- **Evidence anchors:** [abstract]: "paraphrase-based feature augmentation module with a dynamic curriculum learning strategy"; [Methodology]: "This process ensures that the question embedding incorporates both the original and paraphrased information, thereby enriching the feature space"; [corpus]: Weak corpus support—no directly comparable Vietnamese VQA augmentation methods found; neighboring papers focus on different VQA domains (infographics, receipts, medical imaging).
- **Break condition:** If paraphrases are low-quality or semantically divergent from original questions (as noted in ViVQA results with auto-translated samples), the augmentation introduces noise rather than signal.

### Mechanism 2
- **Claim:** Curriculum learning with dynamic threshold adjustment improves convergence stability and final performance.
- **Mechanism:** Training begins with a higher proportion of "easy" augmented samples (controlled by threshold t_thresh) and progressively shifts toward "hard" raw samples via linear decay. This allows the model to first learn smoother, enriched representations before adapting to the noisier original data distribution.
- **Core assumption:** Augmented samples are genuinely "easier" for the model because paraphrase fusion produces smoother embeddings with richer patterns.
- **Evidence anchors:** [Curriculum Learning]: "embeddings fused with paraphrased sentences capture richer and more diverse patterns for the model to learn from"; [Figure 2]: Shows training loss convergence improvements with augmentation and curriculum learning; [Table 4]: Linear decay from 1.0→0.8 achieved best accuracy (0.5583) compared to fixed thresholds.
- **Break condition:** If the decay schedule is too aggressive (t_thresh drops to 0.0 or 0.2), performance degrades as the model is forced onto hard samples too quickly.

### Mechanism 3
- **Claim:** Stochastic sampling between raw and augmented embeddings during training prevents overfitting to either representation.
- **Mechanism:** For each forward pass, a random value x ~ U(0,1) determines whether the augmented or raw embedding is used (x < t_thresh → augmented). This creates a balanced exposure regime that the curriculum schedule modulates over time.
- **Core assumption:** The model benefits from consistent exposure to both representation types throughout training, not just one or the other.
- **Evidence anchors:** [Textual Feature Augmentation]: "This stochastic process enables a balanced exposure to both original and augmented data"; [Table 2]: t_thresh = 1.0 (all augmented) performed worst (0.5283); t_thresh = 0.8 (mixed) performed best (0.5554).
- **Break condition:** If the stochastic sampling creates inconsistent gradient signals early in training, convergence may be delayed rather than improved.

## Foundational Learning

- **Concept: Curriculum Learning**
  - Why needed here: The core training strategy depends on understanding how progressive difficulty adjustment affects model convergence and generalization.
  - Quick check question: Can you explain why presenting "easy" samples before "hard" samples might help a model learn more stable representations?

- **Concept: Embedding Fusion with Skip Connections**
  - Why needed here: The augmentation module combines multiple embeddings with residual connections—understanding gradient flow through skip connections is essential for debugging.
  - Quick check question: What happens to gradient flow if the skip connection from the original embedding is removed?

- **Concept: VQA as Multi-Class Classification**
  - Why needed here: Despite OpenViVQA being designed for generative tasks, this work frames it as classification; understanding this design choice affects evaluation interpretation.
  - Quick check question: How does treating open-ended VQA as classification limit the model's output space?

## Architecture Onboarding

- **Component map:**
  Text Encoder (BARTpho-word/PhoBERT) -> produces d_text=1024 embeddings
  Image Encoder (ResNet18/BEiTv2) -> produces d_img=768 embeddings
  Projection Layers: W_Q (1024→512), W_I (768→512), W_F (concat→512)
  Augmentation Module: W_P, W_O, W_out (learnable) + paraphrase pool (size=10)
  Classifier: ReLU + Linear (512→C classes)
  Curriculum Controller: t_thresh scheduler (linear decay)

- **Critical path:**
  1. Question → Text Encoder → raw embedding E_O
  2. Sample 2 paraphrases from pool → encode → transform via W_P
  3. Fuse: E = E_O·W_O + Σ E_P_i (Equation 8)
  4. Skip connection: Q_augmented = ReLU(E·W_out) + E_O
  5. Stochastic selection: if x < t_thresh use Q_augmented else E_O
  6. Project via W_Q → concatenate with image embedding → classify

- **Design tradeoffs:**
  - Paraphrase count (n=2): More paraphrases improve accuracy but increase VRAM usage significantly
  - Threshold range (0.8→0.4): Narrower range provides stability; wider range introduces more hard samples but risks instability
  - Text encoder choice: BARTpho-word consistently outperforms PhoBERT variants in this framework

- **Failure signatures:**
  - High standard deviation across seeds → threshold schedule may be too aggressive or paraphrase quality is inconsistent
  - Worse performance with augmentation than baseline → paraphrase pool may contain semantic drift (likely with auto-translated data as in ViVQA)
  - Training loss plateaus early → check if t_thresh decays too fast, depriving model of easy samples

- **First 3 experiments:**
  1. **Baseline calibration:** Train vanilla model (t_thresh=0) on ViVQA with BARTpho+ResNet18; record accuracy and convergence curve over 40 epochs
  2. **Fixed threshold sweep:** Test t_thresh ∈ {0.2, 0.4, 0.6, 0.8, 1.0} to identify optimal augmentation ratio before adding curriculum dynamics
  3. **Curriculum schedule comparison:** Compare linear decay (0.8→0.4) vs. cosine annealing vs. fixed threshold (0.8) on OpenViVQA using CIDEr metric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a learnable mechanism replace manual tuning of the curriculum thresholds (t_thresh)?
- Basis in paper: [explicit] The authors suggest future work should focus on "developing a learnable approach to optimize hyperparameters, rather than relying on manually determined constants."
- Why unresolved: The current framework relies on manually defined constants (e.g., linear decay from 1.0 to 0.8) to control the ratio of easy-to-hard samples.
- What evidence would resolve it: Implementation of a meta-learning or reinforcement learning agent that dynamically adjusts thresholds based on model feedback.

### Open Question 2
- Question: Does incorporating visual augmentation alongside textual augmentation improve model robustness?
- Basis in paper: [explicit] The conclusion proposes to "explore augmentation techniques for the image channel and design efficient strategies to train with both augmented image and text modalities."
- Why unresolved: This study restricted data augmentation exclusively to the text channel (paraphrasing), leaving the image features unaugmented.
- What evidence would resolve it: Experiments utilizing multi-modal curriculum learning that includes transformations like cutout or noise injection on images.

### Open Question 3
- Question: Is the proposed framework transferable to other low-resource languages with different linguistic structures?
- Basis in paper: [explicit] The authors state that "extending the proposed framework to additional languages would validate its effectiveness across diverse linguistic contexts."
- Why unresolved: The method was evaluated only on Vietnamese datasets (OpenViVQA, ViVQA), which have specific syntactic properties.
- What evidence would resolve it: Successful application of the paraphrase-based curriculum learning strategy on VQA datasets in other under-represented languages.

## Limitations

- Effectiveness heavily depends on paraphrase quality, which varied significantly between datasets
- Optimal curriculum schedule (t_thresh range 1.0→0.8) may be dataset-specific rather than universally applicable
- Method's scalability to more complex VQA tasks and other low-resource languages remains untested

## Confidence

- **High confidence:** Curriculum learning improves convergence stability and final performance when augmentation quality is good (OpenViVQA results)
- **Medium confidence:** Paraphrase-based feature augmentation consistently improves results across different backbones on OpenViVQA, though performance gains vary by model
- **Low confidence:** Mixed results on ViVQA suggest the method may not generalize to all low-resource VQA datasets without careful paraphrase quality control

## Next Checks

1. Conduct ablation study isolating paraphrase quality effects by testing with different paraphrase generation methods (human-curated vs. mT5 vs. no augmentation)
2. Test curriculum schedule sensitivity by comparing linear decay with other schedules (cosine annealing, step decay) across both datasets
3. Evaluate model performance on held-out paraphrase samples to verify that improvements stem from genuine generalization rather than memorization of paraphrase patterns