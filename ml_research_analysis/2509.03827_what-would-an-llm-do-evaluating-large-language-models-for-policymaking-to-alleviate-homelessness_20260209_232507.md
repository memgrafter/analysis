---
ver: rpa2
title: What Would an LLM Do? Evaluating Large Language Models for Policymaking to
  Alleviate Homelessness
arxiv_id: '2509.03827'
source_url: https://arxiv.org/abs/2509.03827
tags:
- policy
- llms
- experts
- social
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether large language models (LLMs) can effectively
  support policymaking to alleviate homelessness, a global challenge affecting over
  150 million people. Using a novel benchmark grounded in the Capability Approach,
  the research compares LLM-generated policy recommendations with those of domain
  experts across four cities.
---

# What Would an LLM Do? Evaluating Large Language Models for Policymaking to Alleviate Homelessness

## Quick Facts
- arXiv ID: 2509.03827
- Source URL: https://arxiv.org/abs/2509.03827
- Reference count: 40
- Large language models (LLMs) often align with expert policy choices for homelessness but prioritize immediate safety over long-term capability restoration and are less sensitive to local contexts.

## Executive Summary
This study evaluates whether large language models (LLMs) can effectively support policymaking to alleviate homelessness, a global challenge affecting over 150 million people. Using a novel benchmark grounded in the Capability Approach, the research compares LLM-generated policy recommendations with those of domain experts across four cities. The study also introduces a pipeline linking LLM-selected policies to an agent-based model to simulate social impact. Results show that while LLMs often align with expert choices and produce plausible policies, they tend to prioritize immediate safety over long-term capability restoration and are less sensitive to local contexts. When tested in simulations, LLM-recommended policies slightly outperformed expert choices in meeting the needs of people experiencing homelessness, suggesting potential value if paired with contextual calibration and expert oversight.

## Method Summary
The study develops a benchmark of 170 homelessness policy scenarios across four cities, each with four policy options annotated with Nussbaum's central capabilities. Domain experts from each city rank policy options for 60 scenarios. Six LLMs are then prompted to select and rank policies, with results compared to expert choices using agreement rates, semantic similarity, and ranking correlation. For scenarios where LLMs and experts diverge, policies are translated into behavioral modifications in an agent-based model, which simulates needs satisfaction outcomes for 80 agents over 1,450 steps. The pipeline enables controlled comparison of LLM versus expert policy effectiveness in a simulated environment.

## Key Results
- LLMs achieve 50-55% top-choice agreement with domain experts on contextualized scenarios, with higher alignment when context is explicitly emphasized in prompts
- LLM-generated policy justifications show high semantic similarity (0.71-0.84) to expert rationales but moderate ranking correlation (τ=0.31-0.65), suggesting surface-level convergence masking diverse underlying preferences
- In agent-based simulations, LLM-recommended policies slightly outperformed expert choices in meeting needs of people experiencing homelessness, though the effect size was small

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs demonstrate partial alignment with domain experts on homelessness policy recommendations, but exhibit systematic divergences in capability prioritization.
- **Mechanism:** LLMs generate policy recommendations through pattern matching on training data containing policy discourse. When prompted with scenarios grounded in the Capability Approach, they produce plausible justifications and rankings that often coincide with expert choices (50-55% top-choice agreement in contextualized scenarios). However, their reasoning tends to converge on similar linguistic frameworks while diverging in underlying preference weightings—semantic similarity between justifications is high (0.71-0.84) but ranking correlations are moderate (τ=0.31-0.65).
- **Core assumption:** LLM policy recommendations reflect patterns in training data rather than genuine understanding of local sociopolitical contexts or human development theory.
- **Evidence anchors:**
  - [abstract]: "results show that while LLMs often align with expert choices and produce plausible policies, they tend to prioritize immediate safety over long-term capability restoration and are less sensitive to local contexts"
  - [section 4.2]: "alignment varies by city but frequently meets or exceeds inter-expert consensus" with Barcelona at 50%, Johannesburg at 55%, South Bend at 50% top-choice agreement
  - [corpus]: Related work on LLMs as virtual survey respondents (arxiv:2509.06337) shows similar patterns of plausible generation with systematic biases, but does not address homelessness-specific policy reasoning
- **Break condition:** If LLMs were trained on balanced, globally representative policy discourse with explicit local context encoding, the safety-first bias and context insensitivity would diminish.

### Mechanism 2
- **Claim:** Policy-to-simulation translation through LLM-updated behavioral matrices creates a testable causal pathway from natural language policies to projected social outcomes.
- **Mechanism:** LLMs convert policy text into constrained modifications of a State-Action-Transition (SAT) matrix that governs agent behavior in the ABM. The pipeline operates as: Policy → LLM as "differential policy operator" → ΔM (matrix adjustments bounded by ||ΔM||∞ ≤ 0.03) → Agent behavioral changes → Aggregate needs satisfaction outcomes. This enables controlled comparison of expert vs. LLM-recommended policies in the same simulated environment.
- **Core assumption:** Small, bounded matrix modifications preserve mechanistic interpretability while capturing essential policy effects; the ABM adequately represents needs dynamics for people experiencing homelessness.
- **Evidence anchors:**
  - [section 3.3]: "LLMs convert the natural language policy proposals they have previously selected into agents' behavioral adjustments in the ABM... by requesting the LLM to update the state-action-transition (SAT) matrix accordingly"
  - [appendix B]: formalizes the causal pathway as P → ΔM → ABM → ΔE[behavior] → ΔE[equity impact]
  - [corpus]: Work on LLM-ABM integration (arxiv:2409.07701, not in neighbors) addresses similar challenges; corpus evidence for this specific mechanism is weak—no direct precedents found
- **Break condition:** If policies require complex, interacting behavioral changes or if matrix modifications cannot capture policy intent, the translation fails to produce meaningful outcome differences.

### Mechanism 3
- **Claim:** Contextual prompting partially mitigates LLM context-blindness, improving alignment with local expert recommendations.
- **Mechanism:** When LLMs are explicitly prompted to consider local geographical context (e.g., "This decision scenario is set in the city of Johannesburg in South Africa"), their top-choice alignment with domain experts increases compared to identical scenarios without contextual emphasis. This suggests LLMs possess latent context-relevant knowledge that is not activated by default.
- **Core assumption:** Context-relevant associations exist in model weights and can be retrieved through targeted prompting; experts encode local knowledge that should guide policy prioritization.
- **Evidence anchors:**
  - [section 4.2, figure 4]: "specifying the context impacts LLMs' choices and yields higher alignment, even when scenarios are Local, i.e., already somewhat contextualized by design"
  - [section 5]: "LLMs are less sensitive to local contexts than domain experts, which could be influenced by the over-representation of online data from certain parts of the world"
  - [corpus]: Frameworks for evaluating LLM contextual awareness (arxiv:2510.25432) exist but do not specifically address geographic context in social policy
- **Break condition:** If local expertise fundamentally contradicts patterns in global training data, or if context cannot be adequately communicated through prompting, alignment improvements plateau below expert-level performance.

## Foundational Learning

- **Concept: The Capability Approach (Sen/Nussbaum)**
  - **Why needed here:** This framework grounds the entire benchmark—policies are evaluated by which central human capabilities they restore (life, bodily health, affiliation, practical reason, control over environment), not just material resource allocation. Understanding this distinction is essential for interpreting why experts prioritize "affiliation" and "practical reason" while LLMs favor "life" and "bodily health."
  - **Quick check question:** Can you explain why a job training program might score higher on "practical reason" than on "bodily health," and why this matters for long-term homelessness alleviation?

- **Concept: Agent-Based Modeling with Needs Satisfaction**
  - **Why needed here:** The ABM component is not a black box—it uses explicit need-satisfaction vectors, decay rates, and action-utility calculations. Understanding how agents select actions based on dominant unmet needs and SAT matrix coefficients is critical for interpreting simulation results and debugging policy effects.
  - **Quick check question:** Given an agent with needs-satisfaction vector [0.3, 0.8, 0.5] (food, shelter, sleep) and a SAT matrix, which action would a greedy algorithm select?

- **Concept: Inter-rater Agreement Metrics**
  - **Why needed here:** The paper uses Kendall's τ for ranking comparisons and top-choice agreement rates. Understanding that τ=0.07 (inter-expert on universal scenarios) indicates substantial expert disagreement, while τ=0.32 (LLM-expert in Barcelona) indicates moderate alignment, is necessary for contextualizing findings.
  - **Quick check question:** If two raters agree on top choice 50% of the time but have τ=0.1 for full rankings, what does this suggest about their decision-making alignment?

## Architecture Onboarding

- **Component map:** Benchmark Generator (GPT-4.1) -> Policy Evaluation Suite (6 LLMs) -> Policy-to-Rules Translator (Deepseek-R1) -> ABM Engine -> Analysis Layer
- **Critical path:** 1. Generate scenarios with CA annotations → Expert validation and ranking 2. Prompt LLMs for policy choices and justifications → Measure alignment with experts 3. For divergent policies: Translate to SAT modifications → Run ABM (10 baseline, 10 treatment runs) → Compare needs satisfaction distributions
- **Design tradeoffs:**
  - Bounded matrix modifications (||ΔM||∞ ≤ 0.03) vs. unrestricted changes: Preserves interpretability and prevents unrealistic policy effects, but may underrepresent strong policy impacts
  - GPT-4.1 for benchmark generation vs. human-authored: Faster, scalable, but introduces potential LLM biases into scenarios themselves (circularity mitigated by expert validation and separate ranking task)
  - Single-city ABM (Barcelona) vs. multi-city: Enables controlled comparison but limits generalizability of simulation findings
  - Greedy agent decision-making vs. LLM-driven reasoning: Computationally tractable and interpretable, but may not capture complex human reasoning
- **Failure signatures:**
  - LLM justifications have high semantic similarity (0.71-0.84) but rankings diverge (τ=0.31-0.65) → Surface-level homogenization masking diverse underlying preferences
  - LLM policies show higher alignment with US-based experts on universal scenarios (60% vs. 20-40%) → Training data geographic bias
  - SAT matrix modifications produce no significant outcome differences (p>0.05) → Translation failure or policy effect too small for model resolution
- **First 3 experiments:**
  1. Replicate alignment analysis with prompt ablations: Run the same 170 scenarios with (a) no context prompt, (b) context prompt, (c) CA framework explanation included. Measure whether explicit CA grounding shifts LLM capability prioritization toward expert patterns.
  2. Extend ABM to Johannesburg or South Bend: Adapt the SAT matrix structure for different local contexts (different available actions, need decay rates) and test whether LLM-recommended policies maintain slight advantage over expert choices in different settings.
  3. Test policy robustness across scenario perturbations: Introduce minor variations in scenario wording (synonyms, reordering of policy options) and measure whether LLM choices are stable or flip based on framing effects—this probes whether alignment is robust or artifact of prompt sensitivity.

## Open Questions the Paper Calls Out

- **Question:** Is the LLM preference for immediate safety interventions over relational or structural policies caused by the over-representation of specific regions in training data?
  - **Basis in paper:** [Explicit] The authors state that "further studies should be conducted to identify if LLMs' preferences... is connected to the over-representation of data from specific parts of the world."
  - **Why unresolved:** The study observes that LLMs prioritize "Life" and "Bodily Health" over "Affiliation," but the causal link to training data composition remains a hypothesis.
  - **What evidence would resolve it:** Fine-tuning models on geographically diverse or specifically non-Western policy literature and observing if the preference for safety-first interventions decreases.

- **Question:** Does constructing the policy benchmark entirely in local languages with human-generated scenarios improve LLM alignment with local experts?
  - **Basis in paper:** [Explicit] The authors note: "We recommend that future studies build the entire benchmark (not only the baseline) with realistic policies proposed by domain experts in the local languages of the cities in scope."
  - **Why unresolved:** The current benchmark relied heavily on LLM-generated English scenarios, which may lack cultural nuance and introduced potential circularity.
  - **What evidence would resolve it:** A comparison study where LLMs are evaluated on a new benchmark written natively in Spanish, Chinese, and Zulu by local stakeholders rather than translated or generated approximations.

- **Question:** Can the finding that LLM-recommended policies slightly outperform expert policies in simulations be generalized to cities other than Barcelona?
  - **Basis in paper:** [Inferred] From the limitation that the agent-based modeling pipeline "currently handles the policy scenarios for one of the locations in scope only (Barcelona)."
  - **Why unresolved:** The "slight superiority" of LLM policies in meeting needs was only tested in one specific urban environment.
  - **What evidence would resolve it:** Extending the State-Action-Transition (SAT) matrix pipeline to the other three cities (Johannesburg, South Bend, Macau) to test if the simulated social impact remains consistent.

## Limitations

- The benchmark dataset and expert rankings are not yet publicly available, limiting reproducibility of core alignment analyses
- LLM context-blindness persists despite explicit prompting, suggesting fundamental limitations in leveraging global training data for local policy decisions
- The bounded matrix modification approach (±0.03) may underrepresent stronger policy effects, though it preserves interpretability

## Confidence

- **High confidence:** Findings about LLM-expert alignment rates and capability prioritization patterns (50-55% top-choice agreement, safety-first bias)
- **Medium confidence:** Translation pipeline's ability to convert policies to ABM outcomes, given limited precedents in the corpus and the complexity of behavioral change representation
- **Low confidence:** Simulation generalizability across cities, as only Barcelona was tested despite the multi-city benchmark design

## Next Checks

1. Replicate alignment analysis with prompt ablations (no context, context emphasis, explicit CA explanation) to test whether explicit framework grounding shifts LLM capability prioritization
2. Extend ABM implementation to Johannesburg or South Bend with adapted SAT matrices to verify whether LLM policy advantages persist across different local contexts
3. Test policy choice stability under scenario perturbations (synonyms, reordered options) to assess whether alignment is robust or vulnerable to framing effects