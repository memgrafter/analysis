---
ver: rpa2
title: 'DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging'
arxiv_id: '2508.01148'
source_url: https://arxiv.org/abs/2508.01148
tags:
- task
- merging
- vector
- proj
- distac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two failure modes in model merging: (1)
  differences in task vector norms and (2) low confidence of source models. To address
  these issues, the authors propose DisTaC, a knowledge distillation-based pre-conditioning
  method that harmonizes task vector norms and increases source model confidence before
  merging.'
---

# DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging

## Quick Facts
- arXiv ID: 2508.01148
- Source URL: https://arxiv.org/abs/2508.01148
- Reference count: 40
- Primary result: Knowledge distillation-based pre-conditioning method that harmonizes task vector norms and increases source model confidence before merging, achieving up to 20.8 percentage points absolute accuracy gain.

## Executive Summary
This paper identifies two critical failure modes in model merging: differences in task vector norms and low confidence of source models. To address these issues, the authors propose DisTaC, a knowledge distillation-based pre-conditioning method that harmonizes task vector norms and increases source model confidence before merging. DisTaC leverages unlabeled data to adjust task vector norms and improve confidence while preserving task-specific knowledge. The method significantly improves model merging robustness, achieving up to 20.8 percentage points absolute accuracy gain and restoring normalized accuracy from 68% to 92% under low-confidence conditions.

## Method Summary
DisTaC is a pre-conditioning method for model merging that addresses two failure modes: task vector norm disparities and low-confidence source models. The method uses knowledge distillation with unlabeled data to harmonize task vector norms (reducing large vectors rather than stretching small ones) and increase prediction confidence through high-temperature distillation. The approach involves scaling task vectors to match norms, then applying distillation with an ℓ₂ regularizer to recover accuracy lost during scaling. The conditioned models are then merged using standard techniques like Task Arithmetic, TIES, Consensus TA, or TSVM.

## Key Results
- DisTaC achieves up to 20.8 percentage points absolute accuracy gain in model merging scenarios
- Restores normalized accuracy from 68% to 92% under low-confidence conditions
- Demonstrates effectiveness across eight vision tasks with ViT-B-32/L-14 backbones
- Shows that simple norm scaling degrades performance, but KD recovery restores accuracy

## Why This Works (Mechanism)

### Mechanism 1
Reducing large task vector norms and recovering accuracy via distillation prevents high-norm tasks from dominating merged model direction. When merging vectors with large norm disparities, the merged vector aligns almost exclusively with the largest vector, erasing smaller vector signals. DisTaC rescales large vectors to match smaller ones, then uses KD to recover performance while keeping norms constrained.

### Mechanism 2
Increasing source model prediction confidence via high-temperature distillation improves merging robustness. Models trained with label smoothing produce high-entropy predictions that harm merging. DisTaC uses KD with student temperature higher than teacher temperature, forcing the student to produce sharper, lower-entropy predictions at inference time.

### Mechanism 3
Shrinking large task vectors is safer than stretching small vectors for harmonization. Stretching moves parameters further from pre-trained initialization, potentially exiting the local linear regime where task arithmetic assumptions hold. Shrinking keeps models in a more stable region of the loss landscape.

## Foundational Learning

- **Task Arithmetic & Vectors:** DisTaC conditions task vectors (τ_t = θ_t - θ_pre) for merging. Fine-tuning with high learning rate increases task vector norm compared to low learning rate, breaking simple averaging assumptions.

- **Knowledge Distillation (KD) & Temperature:** DisTaC uses KD for behavior modification, not compression. With Teacher T=1 and Student T=10, the Student learns to produce softer distributions during training but sharper logits at inference time.

- **Calibration & Label Smoothing:** Low confidence from label smoothing is harmful for merging. Well-calibrated models (probabilities match accuracy) can be fragile specifically for the process of model merging.

## Architecture Onboarding

- **Component map:** Pre-trained weights + Fine-tuned weights + Unlabeled data -> DisTaC Module (Norm Scaler + Distillation Loop) -> Conditioned weights -> Merging methods

- **Critical path:** 1) Diagnostic: Check norms and entropy 2) Conditioning: Apply DisTaC with scaled initialization 3) Merging: Pass to downstream merger 4) Post-processing: Apply calibration if needed

- **Design tradeoffs:** Data efficiency vs performance (unlabeled data assumption), over-confidence vs mergeability (shifts calibration burden post-merge)

- **Failure signatures:** Norm drift if ℓ₂ regularizer too weak, accuracy drop if target norm set too low

- **First 3 experiments:** 1) Reproduce failure modes with standard vs high-LR vs label smoothing 2) Ablate DisTaC components (norm only, KD only, full pipeline) 3) Stress test on unlabeled data quantity

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several remain unresolved: Does DisTaC generalize to LLMs or non-vision architectures? Can it work in data-free settings? What is the rigorous theoretical link between overconfidence and reduced task interference?

## Limitations

- The theoretical grounding for why norm disparity and low confidence are primary barriers remains underdeveloped
- The assumption that unlabeled data is sufficient for KD recovery is critical but not extensively validated across domain shifts
- The method's effectiveness on non-ViT architectures and different task types remains unexplored

## Confidence

- **High confidence:** Empirical demonstration that norm disparities cause angular dominance in merged vectors and that DisTaC effectively recovers accuracy
- **Medium confidence:** Claim that increasing source model confidence improves merging robustness (primarily empirical evidence)
- **Medium confidence:** Superiority of shrinking over stretching task vectors (based on limited ablation evidence)

## Next Checks

1. **Domain Shift Robustness Test:** Apply DisTaC using synthetic or augmented data for KD to measure accuracy recovery degradation and quantify minimum domain overlap required.

2. **Architecture Generalization Test:** Validate DisTaC on non-ViT architectures (ResNet, ConvNeXt) and different task types (language, speech) to assess universality of failure modes.

3. **Post-hoc Calibration Effectiveness Test:** After merging with DisTaC-conditioned models, apply temperature scaling to the final merged model to measure whether over-confidence can be fully corrected without sacrificing accuracy gains.