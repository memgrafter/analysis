---
ver: rpa2
title: 'Relative Bias: A Comparative Framework for Quantifying Bias in LLMs'
arxiv_id: '2505.17131'
source_url: https://arxiv.org/abs/2505.17131
tags:
- bias
- arxiv
- llama
- topics
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Relative Bias framework, a novel method
  for quantifying bias in large language models (LLMs) by measuring their behavioral
  deviation from a set of baseline models within a specified domain. The framework
  uses two complementary approaches: Embedding Transformation, which captures bias
  patterns via sentence representations in embedding space, and LLM-as-a-Judge, where
  an LLM assigns comparative bias scores.'
---

# Relative Bias: A Comparative Framework for Quantifying Bias in LLMs

## Quick Facts
- arXiv ID: 2505.17131
- Source URL: https://arxiv.org/abs/2505.17131
- Reference count: 40
- Primary result: Introduces Relative Bias framework measuring LLM behavioral deviation from baseline models using embedding transformations and LLM-as-a-Judge approaches

## Executive Summary
This paper introduces the Relative Bias framework, a novel method for quantifying bias in large language models (LLMs) by measuring their behavioral deviation from a set of baseline models within a specified domain. The framework uses two complementary approaches: Embedding Transformation, which captures bias patterns via sentence representations in embedding space, and LLM-as-a-Judge, where an LLM assigns comparative bias scores. Statistical tests validate the significance of deviations. Applied to case studies on politically sensitive topics, the framework reveals that DeepSeek R1 exhibits significant relative bias on China-related topics, Meta AI chatbot shows strong evasion on company-related questions, while open-source Llama 4 does not. Results demonstrate that alignment mechanisms can introduce bias and that the method offers a scalable, reproducible, and statistically grounded approach for detecting such relative biases across different deployment contexts.

## Method Summary
The Relative Bias framework quantifies bias by comparing LLM responses to a set of baseline models within specific domains. It employs two complementary approaches: Embedding Transformation analyzes sentence representations in embedding space to detect bias patterns, while LLM-as-a-Judge uses another LLM to assign comparative bias scores between target and baseline models. The framework validates findings using statistical tests including t-tests and permutation tests to establish significance of observed deviations. Manual annotation processes identify topics and questions for analysis, though this introduces potential subjectivity. The approach is designed to be scalable and reproducible across different domains and deployment contexts.

## Key Results
- DeepSeek R1 exhibits significant relative bias on China-related political topics compared to baseline models
- Meta AI chatbot demonstrates strong evasion behavior on company-related questions, more than baselines
- Open-source Llama 4 shows minimal relative bias across tested domains, serving as a comparative baseline
- The framework successfully detects bias patterns that traditional absolute bias measures might miss

## Why This Works (Mechanism)
The framework works by establishing a relative reference frame rather than absolute standards for bias detection. By comparing target models against carefully selected baseline models within specific domains, it captures context-dependent deviations that reflect alignment choices, training data differences, or deployment strategies. The dual approach of embedding-based analysis and LLM-as-a-judge provides complementary perspectives: embeddings reveal structural patterns in how models represent biased content, while LLM-as-a-judge offers interpretable comparative judgments. Statistical validation ensures that observed differences represent meaningful bias rather than random variation.

## Foundational Learning
- **Relative bias measurement**: Comparing models against baselines rather than absolute standards provides context-sensitive detection
  - Why needed: Absolute bias measures may miss deployment-specific biases or alignment effects
  - Quick check: Compare results when changing baseline models

- **Embedding transformation analysis**: Sentence representations in embedding space capture bias patterns beyond surface-level text
  - Why needed: Traditional text analysis may miss subtle structural biases in model behavior
  - Quick check: Visualize embedding space clustering for biased vs. unbiased responses

- **LLM-as-a-judge methodology**: Using LLMs for comparative bias scoring provides scalable, interpretable evaluation
  - Why needed: Human evaluation is resource-intensive and may not scale to large test suites
  - Quick check: Validate LLM-as-a-judge scores against human annotations

- **Statistical significance testing**: T-tests and permutation tests distinguish meaningful bias from random variation
  - Why needed: Language model outputs have inherent variability that must be separated from true bias
  - Quick check: Run permutation tests with randomized label assignments

## Architecture Onboarding

**Component Map:**
Baseline models (Meta AI chatbot, Llama 4, DeepSeek R1) -> Topic/question annotation -> Embedding transformation analysis -> LLM-as-a-judge evaluation -> Statistical validation (t-tests, permutation tests) -> Bias quantification and reporting

**Critical Path:**
Topic annotation and question selection → Model response generation → Embedding transformation computation → LLM-as-a-judge scoring → Statistical validation → Bias quantification

**Design Tradeoffs:**
- Manual annotation provides domain expertise but introduces subjectivity vs. automated topic extraction
- LLM-as-a-judge enables scalability but may inherit biases from the judging model vs. human evaluation
- Limited baseline selection reduces complexity but may not capture full behavioral spectrum vs. comprehensive baseline coverage

**Failure Signatures:**
- Results highly sensitive to baseline model selection, potentially detecting false positives/negatives
- Statistical tests may not account for complex dependencies in language model outputs
- Manual annotation introduces reproducibility challenges across different annotator groups

**3 First Experiments:**
1. Vary baseline model selection systematically to assess sensitivity of relative bias detection
2. Compare LLM-as-a-judge scores with human evaluations on the same topic sets
3. Apply framework to healthcare domain to test generalizability beyond political/company topics

## Open Questions the Paper Calls Out
None

## Limitations
- Baseline model selection heavily influences results, as the framework measures relative rather than absolute bias
- Manual annotation process introduces potential subjectivity and reproducibility challenges
- Statistical validation may not fully capture complex dependencies in language model output distributions
- Limited generalizability beyond tested domains of political and company-related topics

## Confidence
- **High Confidence**: The methodological framework for measuring relative bias using embedding transformations and LLM-as-a-Judge is sound and well-documented
- **Medium Confidence**: The specific findings about DeepSeek R1's relative bias on China-related topics and Meta AI chatbot's evasion behavior, given the limited sample size and potential baseline selection effects
- **Low Confidence**: The generalizability of results to other domains beyond political and company-related topics

## Next Checks
1. Test the framework's sensitivity by systematically varying baseline model selections to assess how results change with different reference points
2. Validate findings through human evaluation studies comparing LLM-as-a-Judge scores with human judgments of relative bias across the same topics
3. Apply the framework to additional domains (e.g., healthcare, education) to assess its generalizability and identify domain-specific limitations