---
ver: rpa2
title: Do Prompts Reshape Representations? An Empirical Study of Prompting Effects
  on Embeddings
arxiv_id: '2510.19694'
source_url: https://arxiv.org/abs/2510.19694
tags:
- prompt
- bert
- representations
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how prompting affects sentence representations
  in language models. The authors analyze whether relevant prompts lead to better
  task-specific embeddings compared to irrelevant ones, using probing classifiers
  across four classification tasks.
---

# Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings

## Quick Facts
- **arXiv ID**: 2510.19694
- **Source URL**: https://arxiv.org/abs/2510.19694
- **Reference count**: 40
- **Primary result**: Prompting modifies sentence embeddings through contextualization, but task-relevant prompts do not consistently outperform irrelevant ones on probing tasks.

## Executive Summary
This paper investigates whether prompting language models with task-relevant instructions produces better task-specific sentence representations compared to irrelevant or no prompts. The authors conduct systematic probing experiments across four classification tasks using BERT, RoBERTa, and GPT-2 models with various prompt templates. Surprisingly, they find that while prompting does alter representations through contextualization, changes in probe performance do not consistently correlate with prompt relevance. Different models and datasets show varying patterns, with no clear advantage for task-relevant prompts over random or no prompts.

## Method Summary
The authors generate text embeddings using BERT, RoBERTa, or GPT-2, applying specific pooling strategies (mean, [CLS], weighted average) to extract representations from selected layers. They train MaxEnt classifiers with L2 regularization on these embeddings to predict dataset labels. Performance is compared across 26 prompt templates (5 task-relevant, 5 random, 1 null) using bootstrap significance testing to establish statistical significance relative to baselines. The study examines how prompting affects probe performance and measures task alignment scores to understand the spatial redistribution of class samples in embedding space.

## Key Results
- Prompting modifies representations through contextualization, not just token addition
- Static prompt experiments show no significant effects, confirming contextualization is necessary
- Task alignment scores strongly correlate with probe performance (r = 0.7475, p < 10^-19)
- Different architectures show varying responses: BERT generally improves, GPT-2 degrades, RoBERTa varies by dataset
- No consistent pattern shows task-relevant prompts outperforming random or no prompts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prompting modifies sentence representations through contextualization, not mere token addition.
- **Mechanism**: Instruction tokens interact with sample tokens via attention during the forward pass, updating their representations. This redistributes class samples in the embedding space, which the authors measured via task alignment scores.
- **Core assumption**: Contextualization is necessary; static combinations of embeddings should not affect probe performance.
- **Evidence anchors**:
  - [abstract] "prompting affects the quality of representations, these changes do not consistently correlate with the relevance of the prompts"
  - [section 3.2.4] Static prompt experiment: averaging sample and instruction embeddings "effectively neutralizes the effect of prompting" with no statistically significant differences
  - [corpus] Weak/missing — corpus neighbors focus on prompting effectiveness, not representation-level mechanisms
- **Break condition**: If models used static embeddings without contextualization, prompting should have no measurable effect on probe performance (confirmed by the static prompt ablation).

### Mechanism 2
- **Claim**: Changes in probe performance reflect redistribution of samples in embedding space, but this redistribution is not predictably aligned with task relevance.
- **Mechanism**: Prompts induce spatial shifts in how class samples cluster. Task alignment scores (measuring agreement between representation and task spaces) strongly correlate with probe performance (Pearson's r = 0.7475, p < 10^-19), suggesting the same underlying phenomenon.
- **Core assumption**: Assumption: Improved task alignment should correlate with better probe performance if prompting is helping.
- **Evidence anchors**:
  - [section 3.2.2] "prompts influence task alignment in a manner similar to their effect on probing, without any consistent or predictable pattern with respect to prompt relevance"
  - [table 3] Strong correlation coefficients between task alignment and probe performance
  - [corpus] Not addressed in neighbors
- **Break condition**: If prompt relevance drove representation quality, task-relevant prompts should consistently outperform random/irrelevant prompts on both metrics — this does NOT hold.

### Mechanism 3
- **Claim**: The effect of prompting on representations is highly architecture-dependent.
- **Mechanism**: Different pre-training objectives and architectures respond differently to prompting. BERT (MLM + next-sentence prediction) generally shows significant probe improvements with any prompt; RoBERTa (MLM only) varies by dataset; GPT-2 (autoregressive) shows degraded performance.
- **Core assumption**: Assumption: Architectural differences in attention patterns and pre-training objectives mediate how prompts affect representations.
- **Evidence anchors**:
  - [section 3.1] "The model architecture used to generate representations has a notable impact on probe performance across prompts"
  - [section 5] "this limited pre-training may not be sufficient for the models to fully develop the capabilities needed to benefit from prompting"
  - [corpus] Neighbor paper "Evaluating Generalization and Representation Stability in Small LMs" similarly examines prompting vs. fine-tuning stability in small models
- **Break condition**: If architectural differences were irrelevant, all models should show similar patterns of probe performance changes — they do not.

## Foundational Learning

- **Concept: Probing classifiers**
  - Why needed here: The entire methodology relies on training classifiers on frozen representations to measure task-relevant information content
  - Quick check question: Can you explain why probe performance is used as a proxy for "representation quality" rather than downstream task accuracy?

- **Concept: Contextualization in transformers**
  - Why needed here: The paper's central claim is that prompting works through contextualization, which requires understanding how attention mechanisms update token representations bidirectionally (BERT) or unidirectionally (GPT-2)
  - Quick check question: How does the attention mechanism allow instruction tokens to modify sample token representations during a forward pass?

- **Concept: Embedding pooling strategies**
  - Why needed here: Results vary significantly based on whether [CLS] token, average pooling, or weighted average is used, and which layer is selected
  - Quick check question: Why might average pooling outperform [CLS]-based representations for probing prompt embeddings?

## Architecture Onboarding

- **Component map**: Raw text → Prompt template application → Tokenized prompt → Encoder (BERT/RoBERTa/GPT-2) → Hidden states → Layer selection × pooling → Probe (MaxEnt classifier) → Evaluation

- **Critical path**:
  1. Prompt template design (task-relevant vs. random vs. none) → directly determines what signal probes can detect
  2. Representation extraction strategy → major source of variance in results (see Table 6)
  3. Statistical testing → effects are subtle; bootstrap sampling essential for detecting differences

- **Design tradeoffs**:
  - [CLS] vs. mean pooling: [CLS] is designed for classification tasks but underperforms mean pooling for prompt embeddings
  - Layer selection: Later layers may capture more task-specific information but also more noise
  - Model size vs. interpretability: Smaller models enable controlled experiments but may lack prompting capabilities that emerge at scale

- **Failure signatures**:
  - Random prompts outperforming task-relevant prompts (observed in multiple conditions)
  - Task-relevant prompts degrading performance vs. no-prompt baseline (observed in NLI tasks)
  - High variance across datasets for same task type (topic classification shows different patterns across AG News, News Articles, Arise News, Swahili News)
  - Architecture-specific degradation (GPT-2 consistently underperforms with prompts vs. baseline)

- **First 3 experiments**:
  1. **Baseline replication**: Run probe on unmodified input vs. task-relevant prompt vs. random prompt for a single dataset (e.g., IMDB) using BERT with mean pooling from last layer. Expected: some statistically significant difference between conditions, but direction unpredictable.
  2. **Pooling ablation**: Compare [CLS] vs. mean pooling on the same model/dataset combination. Expected: mean pooling should outperform [CLS] based on Table 6 results.
  3. **Static vs. contextualized control**: Implement the static prompt experiment (separately embed instructions and sample, then average) to verify contextualization is necessary. Expected: no significant differences across prompt conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying mechanisms that allow seemingly irrelevant or random prompts to alter representation quality as effectively as task-relevant prompts?
- Basis in paper: [explicit] The authors conclude that their study "does not clearly explain the mechanisms" and notes that "changes in probe performance do not consistently correlate with the relevance of the prompts," challenging the initial hypothesis.
- Why unresolved: The empirical results show the phenomenon occurs, but the paper offers no causal explanation for why prompt relevance is not a predictor of representation quality.
- What evidence would resolve it: Causal tracing or ablation studies on attention heads to identify specific circuits that activate similarly for relevant and random prompts.

### Open Question 2
- Question: Does the correlation between prompt relevance and representation quality emerge in models with larger pre-training corpora or instruction fine-tuning?
- Basis in paper: [explicit] The authors suggest the models used "were not sufficiently pre-trained" and note that LMs are often "further adapted through supervised learning," implying this additional training might be necessary for the hypothesized behavior to appear.
- Why unresolved: The study was restricted to BERT, RoBERTa, and GPT-2 (base models), which lack the scale and alignment of modern LLMs where prompting is most effective.
- What evidence would resolve it: Replicating the probing experiments on instruction-tuned or significantly larger models (e.g., LLaMA-3 or GPT-4) to see if relevance alignment improves.

### Open Question 3
- Question: Can an analysis of layer-wise dynamics or internal computations during the forward pass explain prompting effects that static embeddings fail to capture?
- Basis in paper: [explicit] The Limitations section states that the "embedding-level perspective is too limited" and proposes that "layer dynamics of input processing may play a crucial role" in explaining in-context learning.
- Why unresolved: The study relied on probing static output vectors, thereby missing the temporal evolution of token interactions and intermediate layer states.
- What evidence would resolve it: A study analyzing the flow of information across layers (e.g., using Logit Lens or attention rollout) to see when and where prompt relevance impacts the representation.

## Limitations
- Static prompt ablation only tests one averaging strategy, not exploring other non-contextual approaches
- Paper doesn't investigate why certain architectures benefit from prompts while others degrade
- Limited to base models without instruction tuning or larger scale models where prompting effects might differ

## Confidence

**High confidence**: The finding that prompting affects representations through contextualization rather than static token addition. The static prompt experiment provides clear, reproducible evidence with strong statistical support.

**Medium confidence**: The claim that changes in probe performance reflect redistribution of samples in embedding space. While the correlation with task alignment is strong, the interpretation of what this redistribution means for downstream task performance remains speculative.

**Medium confidence**: The observation of architecture-dependent prompting effects. The patterns across BERT, RoBERTa, and GPT-2 are consistent, but the paper doesn't provide mechanistic explanations for why these differences exist.

**Low confidence**: The broader implication that prompt relevance is largely irrelevant for representation quality. While the paper shows no consistent advantage for task-relevant prompts, the phenomenon may depend on factors not fully explored (prompt phrasing, model scale, or task complexity).

## Next Checks
1. **Extended static prompting ablation**: Test additional non-contextual combination strategies (concatenation, weighted sum with learned weights, attention-based pooling) to confirm that contextualization is uniquely necessary for prompting effects. Compare these against the current averaging baseline across all model-dataset combinations.

2. **Prompt sensitivity analysis**: Systematically vary prompt phrasing while keeping semantic content constant to determine whether syntactic differences drive the observed architecture-dependent effects. This could reveal whether models are sensitive to prompt surface form rather than semantic relevance.

3. **Scale-dependent prompting investigation**: Test whether the relationship between prompt relevance and representation quality changes at larger model scales (e.g., BERT-large, RoBERTa-large). The paper uses base models, but prompting effects often emerge more strongly in larger models, potentially revealing scale-dependent mechanisms.