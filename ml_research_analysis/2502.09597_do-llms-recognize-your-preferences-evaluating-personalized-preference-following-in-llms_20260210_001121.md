---
ver: rpa2
title: Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following
  in LLMs
arxiv_id: '2502.09597'
source_url: https://arxiv.org/abs/2502.09597
tags:
- preference
- claude
- user
- mistral
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PREF EVAL, a benchmark for evaluating LLMs\u2019\
  \ ability to infer, memorize, and follow user preferences in long-context conversational\
  \ settings. The benchmark includes 3,000 manually curated preference-query pairs\
  \ across 20 topics with explicit and implicit preference forms."
---

# Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs

## Quick Facts
- arXiv ID: 2502.09597
- Source URL: https://arxiv.org/abs/2502.09597
- Reference count: 40
- Primary result: Zero-shot preference-following accuracy falls below 10% for conversations exceeding 10 turns (~3k tokens)

## Executive Summary
This paper introduces PREF EVAL, a benchmark for evaluating LLMs' ability to infer, memorize, and follow user preferences in long-context conversational settings. The benchmark includes 3,000 manually curated preference-query pairs across 20 topics with explicit and implicit preference forms. Experiments on 10 state-of-the-art LLMs show that preference-following accuracy falls below 10% in zero-shot settings for conversations exceeding 10 turns, even with advanced prompting and retrieval methods. Fine-tuning on PREF EVAL significantly improves performance and generalization to longer contexts. The benchmark reveals critical limitations in current LLMs and provides a valuable resource for measuring and enhancing personalized conversational agents.

## Method Summary
PREF EVAL is a benchmark for evaluating LLMs' ability to follow user preferences in long-context conversations. It consists of 3,000 manually curated preference-query pairs across 20 topics, with three forms of preferences: explicit, implicit choice-based, and implicit persona-driven. The benchmark evaluates both generation and classification tasks, with generation using an LLM-as-judge approach with four binary checks. Five methods are tested: zero-shot, reminder, self-critic, few-shot CoT, and RAG. The primary intervention is supervised fine-tuning (SFT) on Mistral-7B using 80% of topics with 0/5/10 contextual turns during training.

## Key Results
- Zero-shot preference-following accuracy falls below 10% for conversations exceeding 10 turns (~3k tokens)
- SFT on PREF EVAL significantly improves performance and generalization to longer contexts
- Introduction of multiple or conflicting preferences within a conversation improves adherence to the original preference
- Classification task correlates with generation task at 0.73, enabling faster automated evaluation

## Why This Works (Mechanism)

### Mechanism 1: Attention Allocation to Preference Regions
SFT on preference-following tasks improves model performance by increasing attention weights to preference-related tokens during response generation. Post-SFT analysis shows preference region attention increases by up to 4.97% while other context attention remains unchanged. This mechanism assumes attention patterns correlate with information utilization during generation.

### Mechanism 2: "Lost in the Middle" Context Degradation
Preference following degrades when preferences are positioned in the middle of long conversations, following U-shaped retrieval patterns. LLMs exhibit better retrieval of information at context boundaries versus middle positions. This relies on the assumption that preference following uses the same retrieval mechanisms as factual extraction tasks.

### Mechanism 3: Preference Reinforcement Through Repetition
Introducing multiple (even conflicting) preferences within a conversation improves adherence to any individual preference. Multiple preference mentions may implicitly encourage the model to treat the cumulative set as a broader constraint, reinforcing attention to preference-related content throughout the conversation.

## Foundational Learning

- **Long-Context Retrieval in Transformers**: Essential for understanding why preferences stated thousands of tokens earlier become inaccessible. Quick check: Can you explain why a preference stated at turn 5 of a 100-turn conversation might be better retrieved than one stated at turn 50?

- **Supervised Fine-Tuning for Instruction Following**: Critical for understanding how SFT shapes behavior differently from in-context prompting. Quick check: What is the difference between improving preference following via reminder prompts vs. via fine-tuning on preference-following examples?

- **LLM-as-Judge Evaluation**: Necessary for interpreting generation task results that rely on Claude 3 Sonnet evaluator with 4 binary checks. Quick check: What validation did the authors perform to confirm human-LLM agreement on preference-following judgments?

## Architecture Onboarding

- **Component map**: Dataset construction (3,000 pairs) -> Context injection (LMSYS-Chat-1M) -> Evaluation pipeline (Generation + Classification) -> Intervention methods (5 methods) -> SFT (Mistral-7B)

- **Critical path**: 1. Place preference statement at conversation start 2. Insert N contextual turns (distractors) 3. Present query that would naturally violate preference if answered generically 4. Evaluate response against preference adherence (generation) or MCQ selection (classification)

- **Design tradeoffs**: Generation enables error-type analysis; classification enables faster automated evaluation (0.73 correlation). Explicit vs. implicit preferences: explicit isolates retrieval; implicit tests inference capability. Context length vs. computational cost: 100-turn evaluations require significant API/compute resources

- **Failure signatures**: Preference-Unaware Violation (dominant in zero-shot), Preference Hallucination Violation, Inconsistency Violation, Unhelpful Response (especially Claude models with prompting methods)

- **First 3 experiments**: 1. Baseline establishment: Run zero-shot evaluation on explicit preferences at 10, 70, and 300 turns across 2-3 topics to confirm replication of paper's degradation curve. 2. Reminder vs. RAG ablation: Compare reminder prompting against RAG (Top-5) on implicit persona-driven preferences to isolate inference vs. retrieval contributions. 3. Fine-tuning validation: Train Mistral-7B on 80% of topics (explicit only, 0-10 context turns), evaluate on held-out topics at 70-turn context to test length generalization claims.

## Open Questions the Paper Calls Out

1. **Why does the presence of multiple or conflicting user preferences within a conversation improve the model's adherence to the original preference?** The paper identifies this counterintuitive phenomenon but offers only a hypothesis about reinforced attention without definitive causal mechanism.

2. **To what extent do synthetic benchmarks like PREF EVAL transfer to real-world user interactions?** The current dataset is manually curated and synthetic, potentially lacking the complexity of genuine human dialogue.

3. **What specific internal mechanisms, other than attention scores, account for the difficulty LLMs face with implicit preference inference?** The authors found no significant difference in attention patterns between explicit and implicit forms despite the performance gap, leaving the root cause unidentified.

## Limitations
- Zero-shot preference-following accuracy below 10% for conversations exceeding 10 turns represents a significant limitation of current LLMs
- The counterintuitive finding that introducing conflicting preferences improves adherence lacks external validation and may be model-specific
- SFT hyperparameters and exact implementation details are underspecified, limiting reproducibility

## Confidence
- **High confidence**: General finding that LLMs struggle with long-context preference following (below 10% accuracy at 10+ turns)
- **Medium confidence**: Attention-based mechanism for SFT improvements
- **Medium confidence**: "Lost in the middle" phenomenon extending established retrieval literature
- **Low confidence**: Preference reinforcement through repetition finding

## Next Checks
1. Replicate the explicit preference zero-shot baseline at 10, 70, and 300 turns across multiple topics to confirm the 80% to near-zero degradation curve
2. Conduct ablation studies comparing reminder prompting against RAG (Top-5) on implicit persona-driven preferences to isolate whether improvements stem from inference capabilities versus retrieval mechanisms
3. Train Mistral-7B on explicit preferences with 0-10 context turns and evaluate on held-out topics at 70-turn context to verify claims about length generalization through SFT