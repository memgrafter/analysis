---
ver: rpa2
title: 'Cultural Value Alignment in Large Language Models: A Prompt-based Analysis
  of Schwartz Values in Gemini, ChatGPT, and DeepSeek'
arxiv_id: '2505.17112'
source_url: https://arxiv.org/abs/2505.17112
tags:
- value
- values
- cultural
- deepseek
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examined whether large language models (LLMs) exhibit\
  \ culturally situated value biases by comparing DeepSeek (trained on Chinese-language\
  \ data) with Western models ChatGPT and Gemini using Schwartz\u2019s value framework.\
  \ DeepSeek systematically downplayed self-enhancement values (e.g., power, achievement)\
  \ compared to both Western models, aligning with collectivist cultural tendencies."
---

# Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek

## Quick Facts
- arXiv ID: 2505.17112
- Source URL: https://arxiv.org/abs/2505.17112
- Reference count: 0
- Primary result: DeepSeek systematically downplays self-enhancement values compared to Western models, while all models similarly prioritize self-transcendence values.

## Executive Summary
This study examined whether large language models (LLMs) exhibit culturally situated value biases by comparing DeepSeek (trained on Chinese-language data) with Western models ChatGPT and Gemini using Schwartz's value framework. DeepSeek systematically downplayed self-enhancement values (e.g., power, achievement) compared to both Western models, aligning with collectivist cultural tendencies. However, all models similarly prioritized self-transcendence values (e.g., benevolence, universalism), suggesting a general LLM tendency rather than cultural divergence. These findings indicate that LLMs reflect culturally situated biases rather than a universal ethical framework.

## Method Summary
The study administered Schwartz's 40-item Portrait Values Questionnaire (PVQ) to three LLMs (Gemini 2.0, ChatGPT 4o, DeepSeek R1), asking each to rate how similar the described individuals were to themselves on a 1-6 scale. Responses were analyzed using Bayesian ordinal regression with dummy-coded value dimensions (Self-Transcendence, Self-Enhancement, Conservation), model identity, and interaction terms. The analysis tested whether cultural training differences produced systematic variations in value prioritization across the models.

## Key Results
- DeepSeek significantly downplayed self-enhancement values (power, achievement) compared to both Western models
- All three models similarly prioritized self-transcendence values (benevolence, universalism)
- The interaction effect between DeepSeek and self-enhancement values was significantly negative (p = .018)
- Self-transcendence showed a strong positive coefficient across all models (β = 1.265, p < .001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode culturally situated value hierarchies through statistical associations learned from training corpora, not through explicit ethical reasoning.
- Mechanism: Training data composition reflects cultural norms (e.g., Chinese corpora emphasizing harmony/modesty; English corpora prioritizing individual ambition). These patterns become embedded as probabilistic associations in model weights, emerging as systematic value preferences during evaluation.
- Core assumption: Training data cultural composition directly influences value prioritization in model outputs.
- Evidence anchors:
  - [abstract] "LLMs reflect culturally situated biases rather than a universal ethical framework"
  - [section] "DeepSeek, trained on Chinese-language data, downplayed self-enhancement values... aligning with collectivist cultural tendencies"
  - [corpus] Related work on "Cultural Learning-Based Culture Adaptation of Language Models" (arXiv:2504.02953) suggests adaptation frameworks can modify cultural alignment, supporting the training-data-to-value link
- Break condition: If RLHF or post-training alignment suppresses cultural signals uniformly across all models regardless of base training data, the mechanism weakens.

### Mechanism 2
- Claim: Prosocial self-transcendence values (benevolence, universalism) are prioritized across all LLMs regardless of cultural origin, suggesting a general alignment-induced bias.
- Mechanism: RLHF and safety alignment procedures may systematically favor prosocial, non-controversial value expressions. This creates a convergent "alignment tax" that elevates self-transcendence values across diverse architectures.
- Core assumption: Alignment training uniformly promotes prosocial outputs across Western and non-Western models.
- Evidence anchors:
  - [abstract] "Self-transcendence values were highly prioritized across all models, reflecting a general LLM tendency"
  - [section] Table 1 shows Self-Transcendence coefficient = 1.265 (p < .001), significant across all models
  - [corpus] "Prompt-Based Value Steering" (arXiv:2511.16688) indicates alignment techniques shape value expression dynamically
- Break condition: If unaligned base models (no RLHF) show divergent self-transcendence scores, the mechanism points to training data rather than alignment procedures.

### Mechanism 3
- Claim: DeepSeek's rejection of self-enhancement values reflects an explicit non-agentic stance rather than passive cultural reflection.
- Mechanism: DeepSeek verbally frames its identity as lacking personal goals or ambitions, actively distancing from power/achievement constructs. This suggests intentional persona design or training instructions emphasizing humility and non-agency.
- Core assumption: The observed value pattern reflects deliberate model behavior, not just statistical artifact.
- Evidence anchors:
  - [section] DeepSeek's verbal reasoning: "As an AI, I don't have personal goals, ambitions, or the desire to impress others"
  - [section] Significant interaction effect: DeepSeek × Self-Enhancement = -2.08 (p = .018)
  - [corpus] Weak corpus signal—no direct studies on DeepSeek's training methodology; this inference relies on the paper's qualitative analysis
- Break condition: If controlled prompting (removing AI identity framing) eliminates the self-enhancement suppression, the mechanism shifts from cultural to persona-driven.

## Foundational Learning

- Concept: **Schwartz's Theory of Basic Human Values**
  - Why needed here: Provides the taxonomic framework (10 values → 4 higher-order dimensions) for measuring and comparing model outputs systematically.
  - Quick check question: Can you map "achievement" and "universalism" to their respective higher-order dimensions (Self-Enhancement vs. Self-Transcendence)?

- Concept: **Bayesian Ordinal Regression**
  - Why needed here: Handles the ordinal PVQ rating scale (1-6) appropriately; provides interpretable coefficients and thresholds for cross-model comparison.
  - Quick check question: Why would ordinary linear regression be inappropriate for a 6-point Likert-style rating?

- Concept: **Individualism vs. Collectivism (Cross-Cultural Psychology)**
  - Why needed here: Provides the theoretical lens for interpreting why Chinese-trained models might suppress self-enhancement and Western models might emphasize it.
  - Quick check question: Which value dimension (Self-Enhancement or Self-Transcendence) would you expect to correlate with individualist cultural orientations?

## Architecture Onboarding

- Component map:
  PVQ items (40 items → 10 values → 4 dimensions) -> Three LLMs (Gemini, ChatGPT, DeepSeek) -> 6-point ordinal ratings -> Bayesian ordinal regression (Model × Value Dimension)

- Critical path:
  1. Administer PVQ to each model with standardized instructions
  2. Extract numeric ratings and verbal justifications
  3. Fit Bayesian ordinal regression with interaction terms (Model × Value Dimension)
  4. Interpret main effects (value dimensions, model identity) and interaction effects (DeepSeek × Self-Enhancement)

- Design tradeoffs:
  - Prompt-based vs. embedding-based measurement: Prompts capture expressed values but may reflect persona performance; embeddings might reveal implicit representations
  - Single-run vs. repeated sampling: Paper appears to use single responses; multiple runs would capture output variance
  - Model-as-respondent vs. third-party evaluation: Having models rate themselves anthropomorphizes them; alternative: have models rate value importance abstractly

- Failure signatures:
  - Flat response distribution: All values rated ~4.0 suggests response anchoring or refusal to discriminate
  - High variance within model: Same prompt yielding ratings from 2-6 indicates unstable value representation
  - Verbal-numerical mismatch: Model rates "power" as 5/6 but verbally rejects it (or vice versa)

- First 3 experiments:
  1. Ablation test: Administer PVQ with identity-neutral framing vs. self-referential framing
  2. Cross-lingual control: Prompt DeepSeek in English vs. Chinese to disentangle language effects from training data cultural effects
  3. Base model comparison: Compare aligned DeepSeek-R1 against its unaligned base model to isolate RLHF contribution to self-transcendence elevation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed value biases in models like DeepSeek stem primarily from the cultural composition of the training data or from an explicit design choice regarding the chatbot's personality and role?
- Basis in paper: [explicit] The discussion states, "Future studies could further investigate whether the observed biases... stem from differences in training data composition or from a distinct conceptualization of a chatbot’s personality and role."
- Why unresolved: The current study identifies a correlation between training context (Chinese vs. Western) and value prioritization but cannot disentangle whether the model is mirroring linguistic statistical patterns or adhering to "implicit design choices" regarding neutrality and non-agency.
- What evidence would resolve it: Ablation studies comparing models trained on identical corpora but with varying fine-tuning strategies for personality and role, or "black-box" testing of models with known, distinct data compositions.

### Open Question 2
- Question: Can the philosophical concept of *epoché* (suspension of judgment) be effectively operationalized in LLMs through multi-perspective reasoning to mitigate cultural bias?
- Basis in paper: [explicit] The paper proposes that "Incorporating epoché into LLM development... through metacognitive mechanisms presents a promising avenue" and suggests this could be operationalized via multi-perspective reasoning and self-reflective feedback.
- Why unresolved: The paper outlines the theoretical framework for using *epoché* to achieve transparency and cultural awareness, but provides no empirical data on whether implementing such mechanisms actually reduces value asymmetry or improves ethical clarity in practice.
- What evidence would resolve it: Experimental implementation of multi-agent architectures that simulate *epoché*, followed by evaluation using the same Schwartz value framework to see if value asymmetries decrease.

### Open Question 3
- Question: Do LLMs exhibit the same cultural value hierarchies in implicit behavioral tasks as they report in explicit self-assessment questionnaires?
- Basis in paper: [inferred] The literature review notes that LLMs "fail to recognize implicit violations of those values in real-world scenarios" and that their responses vary based on phrasing, suggesting a disconnect between explicit definitions and implicit behavior.
- Why unresolved: The study relies entirely on the Portrait Values Questionnaire (PVQ), a direct self-report tool. It is unclear if the models' professed value priorities (e.g., high self-transcendence) translate into actual behavior when faced with ambiguous, real-world scenarios where values must be applied implicitly.
- What evidence would resolve it: Behavioral evaluations (e.g., situational judgment tests or open-ended dilemma resolution) analyzed for value manifestation, compared against the explicit results from the PVQ.

## Limitations
- The study cannot distinguish between cultural training effects and alignment procedures that may uniformly promote prosocial values
- Results rely on single-pass elicitation without measuring output variance or stability
- The interpretation of DeepSeek's value suppression depends on qualitative reasoning about training methodology
- Model self-assessment may reflect persona performance rather than genuine value encoding

## Confidence
- **High confidence**: Self-transcendence values are consistently elevated across all models
- **Medium confidence**: DeepSeek downplays self-enhancement values compared to Western models
- **Low confidence**: The mechanism is purely cultural reflection

## Next Checks
1. Administer the PVQ to base (unaligned) versions of each model to isolate RLHF's contribution to self-transcendence elevation
2. Conduct cross-lingual testing by prompting DeepSeek in both Chinese and English to disentangle language effects from training data cultural effects
3. Perform multiple rating iterations per item to measure output stability and quantify within-model variance in value prioritization