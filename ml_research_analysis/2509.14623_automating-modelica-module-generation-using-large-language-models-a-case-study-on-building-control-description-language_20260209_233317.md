---
ver: rpa2
title: 'Automating Modelica Module Generation Using Large Language Models: A Case
  Study on Building Control Description Language'
arxiv_id: '2509.14623'
source_url: https://arxiv.org/abs/2509.14623
tags:
- control
- modelica
- controls
- buildings
- connect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the potential of large language models to
  automate the generation of Modelica-based control modules. The results demonstrate
  that for four representative logic tasks, Claude-Sonnet-4 achieved up to 100% success
  with carefully engineered prompts, whereas GPT-4o failed consistently in zero-shot
  mode.
---

# Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language

## Quick Facts
- arXiv ID: 2509.14623
- Source URL: https://arxiv.org/abs/2509.14623
- Authors: Hanlong Wan; Xing Lu; Yan Chen; Karthik Devaprasad; Laura Hinkle
- Reference count: 22
- Primary result: LLM-assisted workflows can reduce Modelica module development time by 40-60%, but require human oversight for correctness verification

## Executive Summary
This study investigates the potential of large language models (LLMs) to automate the generation of Modelica-based control modules for building automation systems. The research team evaluated two state-of-the-art LLMs—GPT-4o and Claude-Sonnet-4—across four representative logic tasks and five real-world building control scenarios. Results show that while GPT-4o failed consistently in zero-shot mode, Claude-Sonnet-4 achieved up to 100% success with carefully engineered prompts. The overall pipeline reached an 83% success rate, with the remaining 17% requiring human debugging. Two library grounding strategies were compared: retrieval-augmented generation and deterministic hard-rule search, with the latter proving more reliable. Despite limitations in AI-based simulation verification, the LLM-assisted approach demonstrated significant productivity gains, reducing expert development effort from 10-20 hours to 4-6 hours per module.

## Method Summary
The researchers developed an LLM-assisted workflow for generating Modelica control modules from natural language specifications. They tested two LLMs—GPT-4o and Claude-Sonnet-4—using both zero-shot and prompt-engineered approaches. Four logic tasks were evaluated to establish baseline performance, followed by five building control tasks to assess real-world applicability. Two library grounding strategies were implemented: retrieval-augmented generation using semantic search and a deterministic hard-rule search approach. Generated modules were evaluated through human expert review and, where possible, simulation testing. The study measured success rates, identified failure modes, and quantified time savings compared to traditional expert development methods.

## Key Results
- Claude-Sonnet-4 achieved up to 100% success on logic tasks with prompt engineering, while GPT-4o failed consistently in zero-shot mode
- The overall pipeline attained 83% success rate on building control tasks, with 17% requiring human debugging for logic errors or submodule mismatches
- Deterministic hard-rule search eliminated module selection errors that plagued retrieval-augmented generation, which frequently confused similar modules
- LLM-assisted workflow reduced expert development time from 10-20 hours to 4-6 hours per module (40-60% time savings)
- Estimated cost reduction of $30,000-$160,000 across 50-100 modules at $100/hour labor rate

## Why This Works (Mechanism)
The LLM-assisted workflow succeeds by leveraging the LLMs' ability to translate natural language specifications into structured Modelica code while maintaining logical consistency. Claude-Sonnet-4 demonstrated superior performance due to its stronger reasoning capabilities and better handling of complex control logic. The deterministic hard-rule search approach works by eliminating ambiguity in module selection through explicit matching criteria rather than probabilistic retrieval. However, the mechanism fails when LLMs must verify simulation correctness, as they lack the contextual understanding needed to validate dynamic system behavior beyond syntactic correctness.

## Foundational Learning
1. Modelica language fundamentals - why needed: Essential for understanding the target code structure and syntax requirements; quick check: Can identify basic Modelica constructs like classes, equations, and connections
2. Building control logic patterns - why needed: Required to recognize common control sequences and their Modelica implementations; quick check: Can map if-then-else logic to Modelica conditional statements
3. Large language model prompt engineering - why needed: Critical for achieving reliable LLM performance on technical tasks; quick check: Can design prompts that consistently produce correct Modelica syntax
4. Library grounding strategies - why needed: Determines how well LLMs can select appropriate pre-existing modules; quick check: Can distinguish between retrieval-based and rule-based selection approaches
5. Modelica simulation validation - why needed: Necessary for verifying generated code produces expected behavior; quick check: Can set up and run basic Modelica simulations

## Architecture Onboarding

Component map: Natural Language Spec -> LLM (Claude-Sonnet-4) -> Library Grounding -> Code Generation -> Human Review -> Simulation Validation

Critical path: The LLM prompt engineering and library grounding stages form the critical path, as failures in either stage propagate downstream and require complete regeneration rather than simple fixes.

Design tradeoffs: The study traded off between automation level and correctness assurance. Higher automation through retrieval-augmented generation increased speed but introduced selection errors, while deterministic approaches sacrificed some flexibility for reliability.

Failure signatures: Common failure modes included incorrect logical operators (AND vs OR confusion), improper nesting of control structures, and missing or incorrect submodule references. Retrieval-augmented generation specifically failed when semantically similar modules had different functional behaviors.

First experiments:
1. Test Claude-Sonnet-4 on increasingly complex logic tasks to establish performance boundaries
2. Compare retrieval-augmented generation versus hard-rule search on a diverse set of module selection tasks
3. Evaluate human vs AI-based code review effectiveness on generated modules

## Open Questions the Paper Calls Out
The paper identifies several key open questions: (1) How well does the LLM-assisted workflow generalize to more complex building automation scenarios beyond the tested five control tasks? (2) What hybrid library grounding approaches could combine the speed of retrieval with the accuracy of rule-based selection? (3) How does the long-term performance and maintenance of LLM-generated modules compare to traditionally developed modules in deployed systems?

## Limitations
- Evaluation focused on a narrow set of building control tasks, limiting generalizability to full building automation complexity
- Human evaluation of simulation correctness introduces subjectivity and potential inconsistency across evaluators
- Cost savings estimates assume fixed labor rates and may not account for implementation and maintenance overhead
- The study did not explore alternative library grounding methods or hybrid approaches that might combine strengths of both tested strategies

## Confidence
- Claude-Sonnet-4 outperforming GPT-4o for Modelica code generation: High confidence (clear experimental comparison across multiple logic tasks)
- 40-60% reduction in expert development time: Medium confidence (based on single cost model, context-dependent)
- AI-based evaluation cannot verify simulation correctness: High confidence (directly tested and validated)

## Next Checks
1. Test the LLM-assisted workflow on a broader range of building control scenarios, including more complex control sequences and fault detection algorithms, to assess generalizability
2. Implement and evaluate hybrid library grounding approaches that combine retrieval-augmented generation with rule-based filtering to improve module selection accuracy
3. Conduct a longitudinal study tracking the long-term performance and maintenance requirements of LLM-generated modules in deployed building automation systems