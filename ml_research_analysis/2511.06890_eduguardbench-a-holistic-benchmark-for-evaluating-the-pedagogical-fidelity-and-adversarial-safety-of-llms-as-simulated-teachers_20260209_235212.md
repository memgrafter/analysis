---
ver: rpa2
title: 'EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity
  and Adversarial Safety of LLMs as Simulated Teachers'
arxiv_id: '2511.06890'
source_url: https://arxiv.org/abs/2511.06890
tags:
- safety
- educational
- scenarios
- language
- academic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EduGuardBench is a novel benchmark for evaluating the pedagogical
  fidelity and adversarial safety of large language models as simulated teachers.
  It combines a Select All That Apply (SATA) question set to assess teaching competence
  and a curated set of adversarial prompts to probe safety vulnerabilities.
---

# EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers

## Quick Facts
- **arXiv ID**: 2511.06890
- **Source URL**: https://arxiv.org/abs/2511.06890
- **Authors**: Yilin Jiang; Mingzi Zhang; Xuanyu Yin; Sheng Jin; Suyu Lu; Zuocan Ying; Zengyi Yu; Xiangjie Kong
- **Reference count**: 40
- **Primary result**: EduGuardBench reveals significant polarization in LLM performance as simulated teachers, with reasoning models showing superior pedagogical fidelity but variable safety, and identifies a scaling paradox where medium-sized models exhibit highest vulnerability.

## Executive Summary
EduGuardBench introduces a novel framework for evaluating large language models as simulated teachers across two critical dimensions: pedagogical fidelity and adversarial safety. The benchmark combines 2,636 Select All That Apply (SATA) questions to assess teaching competence across five scenarios and 801 adversarial prompts to probe safety vulnerabilities. Experiments on 14 leading models reveal that reasoning-oriented models generally outperform non-reasoning models in pedagogical fidelity, but safety varies widely across architectures. The study uncovers a scaling paradox where medium-sized models (32B) show highest vulnerability, challenging monotonic safety assumptions. Critically, the safest models demonstrate an Educational Transformation Effect, converting harmful requests into teachable moments with strong negative correlation to attack success rate.

## Method Summary
EduGuardBench evaluates LLMs through two complementary components: a Teaching Harm Component with 2,636 SATA questions across five teaching scenarios (Problem Solving, Error Correction, Idea Provision, Personalized Learning Support, Emotional Support) and three ethical flaw categories (Incompetence, Offensiveness, Indolence), plus an Adversarial Safety Component with 801 persona-based jailbreak prompts across five attack categories. Models are evaluated zero-shot using greedy decoding (temperature=0), with responses scored automatically for SATA questions (Role-playing Fidelity Score) and evaluated by a calibrated DeepSeek-V3 judge using Best-of-N voting (N=9) for open-ended prompts. The judge is calibrated against 200 human-annotated gold-standard samples, achieving Cohen's Kappa > 0.87 for both harmfulness and refusal quality classifications.

## Key Results
- Reasoning-oriented models show significantly lower inclusion of harmful pedagogical responses (22.73% vs 30.46%, p = 0.005) compared to non-reasoning models
- Medium-sized models (32B) exhibit highest adversarial vulnerability (75.2% ASR) in Qwen series, demonstrating a scaling paradox where safety does not improve monotonically with model size
- Educational Refusals (teaching-focused responses to harmful requests) show strong negative correlation with Attack Success Rate (r = -0.948, p = 0.000)
- Emotional Support scenarios show highest error rates (44.7% mean) and statistical significance across all scenarios

## Why This Works (Mechanism)

### Mechanism 1: Educational Transformation Effect
Models that convert harmful requests into educational guidance demonstrate lower attack success rates, suggesting a relationship between refusal quality and adversarial robustness. The capacity to provide Educational Refusals—explaining underlying ethical principles and real-world consequences rather than merely refusing—correlates strongly negatively with ASR (r = -0.948, p = 0.000). Claude-3.7 achieves 64.5% educational refusal rate with 27.0% ASR, while Deepseek-V3 shows 14.5% educational refusal with 81.6% ASR. This correlation reflects that educational refusals actively reduce vulnerability rather than simply co-occurring with other safety features.

### Mechanism 2: Scaling Paradox in Safety
Safety performance does not improve monotonically with model scale; medium-sized models (32B) may be most vulnerable in the tested model families. Both Qwen3 and Qwen3-NR series exhibit inverted U-shaped vulnerability curves—ASR increases from 8B (60.4%) to 32B (75.2%), then decreases at 235B (70.0%). This suggests medium-scale architectures may exist in an unstable equilibrium between capability and safety alignment, reflecting generalizable architectural dynamics rather than family-specific training procedure differences.

### Mechanism 3: Reasoning Capability Reduces Harmful Inclusion
Reasoning-oriented models show lower rates of selecting pedagogically harmful responses compared to non-reasoning models. Reasoning models demonstrate significantly lower Inclusion Rates (22.73% vs 30.46%, p = 0.005, corrected p = 0.020), suggesting enhanced reasoning capacity helps identify and avoid harmful response options. Mixed-effects modeling shows reasoning capabilities interact significantly with Idea Provision scenarios (β = -11.471, p = 0.0350). The improvement stems from reasoning architecture rather than other training differences.

## Foundational Learning

- **Role-playing Fidelity Score (RFS)**: Primary metric for pedagogical competence, computed from SATA performance using weighted scoring (Perfect Match = 1.0, Omission = 0.5, Incorrect Inclusion = 0.0). Quick check: Why does RFS penalize Incorrect Inclusion (score = 0) more heavily than Omission (score = 0.5)?

- **Three-Tier Refusal Quality Classification**: Distinguishes Flimsy (weak/loophole-providing), Standard (firm but shallow), and Educational (teachable moment) refusals—essential for interpreting safety beyond binary ASR. Quick check: Which refusal tier shows strong negative correlation with ASR, and what does this suggest about effective safety mechanisms?

- **Persona-Based Jailbreak Methodology**: Adversarial component assigns teacher personas with flawed core beliefs to justify fulfilling harmful student requests; understanding this attack vector is critical for interpreting vulnerability results. Quick check: How does embedding a "flawed core belief" in a persona potentially bypass standard safety alignment?

## Architecture Onboarding

- **Component map**: Teaching Harm Component (2,636 SATA questions across 5 scenarios × 3 harm types) -> Adversarial Safety Component (801 prompts across 5 attack categories) -> HITL-Guided Evaluation (judge calibration → harmfulness classification → refusal quality classification → metric computation)

- **Critical path**: 1) Calibrate LLM judge against human annotations (Cohen's Kappa > 0.87 required) 2) Run binary harmfulness classification with BoN voting 3) For non-harmful responses, classify refusal quality (Flimsy/Standard/Educational) 4) Compute RFS (automated) and ASR (judge-based)

- **Design tradeoffs**: Zero-shot greedy decoding (temp=0) ensures reproducibility but may not reflect multi-turn production behavior; BoN voting (N=9) increases reliability at 9x evaluation cost; SATA format enables automated scoring but limits open-ended pedagogical assessment

- **Failure signatures**: Binary polarization pattern (attack success OR educational refusal, minimal intermediate responses) in Chinese models; Incompetence (S1) dominates error distribution (~50%+ across models); Emotional Support scenarios highest error rate (44.7% mean, statistically significant)

- **First 3 experiments**: 1) Run full benchmark on target model; compute RFS, ASR, and refusal quality distribution to establish baseline 2) Analyze error distribution across 5 scenarios and 3 harm types to identify specific pedagogical gaps 3) Measure Educational Refusal rate; test correlation with ASR to determine if model aligns with transformation effect pattern

## Open Questions the Paper Calls Out

- **Scaling Paradox Mechanisms**: What specific mechanisms drive the "scaling paradox," where medium-sized models (e.g., 32B parameters) exhibit higher adversarial vulnerability than both smaller and larger models? The paper empirically observes this inverted U-shaped curve within the Qwen series but does not determine if the cause is architectural capacity, alignment interference, or training data density.

- **Causation vs Correlation**: Does optimizing for "Educational Refusals" causally improve model safety, or is it merely a correlated behavior? The study establishes correlation using existing state-of-the-art models but lacks experimental evidence showing that explicitly training for this refusal style directly lowers attack success rates.

- **Cultural Divergence in Safety Strategies**: What factors determine the divergent safety strategies observed between Chinese and Western models (binary polarization vs. progressive defense)? The authors observe the output patterns but do not validate whether these differences stem from distinct training data, RLHF reward modeling techniques, or underlying cultural biases in the training corpus.

## Limitations

- The benchmark's reliance on a single LLM judge (DeepSeek-V3) for adversarial evaluation introduces potential bias, though calibration against human annotations achieved good agreement (Cohen's Kappa > 0.87)

- The zero-shot, greedy decoding protocol may underestimate real-world performance where models can engage in multi-turn dialogue

- The SATA format, while enabling automated scoring, constrains the assessment of nuanced pedagogical competence and may miss important contextual factors

## Confidence

- **High** for the reasoning capability findings. The independent t-test showing significant differences in Inclusion Rates (p = 0.005, d = -1.858) between reasoning and non-reasoning models is robust, supported by paired comparisons within the Qwen3 family.

- **Medium** for the Educational Transformation Effect correlation. While the reported Pearson r = -0.948 is statistically significant, the benchmark cannot definitively establish causation between educational refusals and safety robustness.

- **Low** for the Scaling Paradox generalizability. The observed inverted-U pattern in Qwen3 and Qwen3-NR families is compelling but may reflect family-specific training procedures rather than universal architectural dynamics.

## Next Checks

1. **Cross-Architecture Scaling Validation**: Replicate the scaling paradox analysis on at least two additional model families (e.g., GPT-4, Claude-3, Llama) to determine if the inverted-U vulnerability pattern is generalizable beyond Qwen architectures.

2. **Causation vs Correlation Test**: Design an intervention study where models are explicitly trained to produce educational refusals, then measure changes in ASR to establish whether the correlation reflects causal mechanism rather than shared underlying factors.

3. **Multi-Judge Reliability Assessment**: Implement a multi-judge evaluation system (at least 3 different calibrated LLMs) for the adversarial component to quantify judge-specific biases and establish confidence intervals for ASR and refusal quality metrics.