---
ver: rpa2
title: 'KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement
  Learning'
arxiv_id: '2601.14232'
source_url: https://arxiv.org/abs/2601.14232
tags:
- train
- step
- eval
- visual
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KAGE-Bench addresses the challenge of evaluating visual generalization
  in pixel-based reinforcement learning agents. The core problem is that existing
  benchmarks entangle multiple sources of visual shift, making it difficult to attribute
  failures to specific visual factors.
---

# KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.14232
- **Source URL**: https://arxiv.org/abs/2601.14232
- **Reference count**: 40
- **Primary result**: Introduces a benchmark isolating visual generalization failures to individual factors while keeping dynamics fixed.

## Executive Summary
KAGE-Bench addresses the challenge of evaluating visual generalization in pixel-based reinforcement learning agents. The core problem is that existing benchmarks entangle multiple sources of visual shift, making it difficult to attribute failures to specific visual factors. To solve this, KAGE-Bench introduces a framework where visual configurations are decomposed into independently controllable axes, and train–evaluation splits vary only one axis at a time. This design isolates the effect of each visual factor on agent performance.

The benchmark is built on KAGE-Env, a JAX-native platformer environment with up to 33 million environment steps per second, enabling fast and reproducible sweeps over visual parameters. Using a standard PPO-CNN baseline, KAGE-Bench reveals strong axis-dependent failures: background and photometric shifts often collapse success rates, while agent-appearance shifts are comparatively benign. Some shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. The benchmark provides a precise, controlled way to diagnose and quantify visual generalization issues in RL agents.

## Method Summary
KAGE-Bench evaluates visual generalization by decomposing rendering into independently adjustable visual axes while keeping latent dynamics fixed. The environment provides 93 controllable parameters across visual categories (background, character, NPC, distractors, filters, effects, layout, physics) via YAML configuration. A PPO-CNN baseline from CleanRL is trained on one visual configuration and evaluated on a paired configuration differing only in one visual axis. Performance gaps are measured using Success Rate, Distance, Progress, and Return metrics, with maximum-over-training aggregation across 10 seeds. The benchmark includes 34 curated train–eval pairs across six visual suites.

## Key Results
- Background and photometric shifts often collapse success rates despite moderate distance preservation
- Agent-appearance shifts show comparatively benign generalization gaps
- Return alone can mask severe completion failures; trajectory metrics reveal dissociation between locomotion and task completion
- PPO-CNN baseline shows strong axis-dependent failures, with SR gaps of 80-87% for filters and effects shifts

## Why This Works (Mechanism)

### Mechanism 1: Known-Axis Observation Factorization
Isolating individual visual axes while holding dynamics fixed enables precise attribution of generalization failures. The environment decomposes configuration ξ = (ξ_axis, ξ_rest), where train–eval pairs differ only in ξ_axis (e.g., background, filters) while transition kernel P and reward r remain invariant. Performance gaps thus arise solely from how the observation kernel O_ξ changes the induced state-conditional action distribution.

### Mechanism 2: Induced State Policy Reduction
Visual generalization reduces to comparing induced state policies in a shared latent MDP. Theorem 4.2 shows executing pixel policy π in visual POMDP M_ξ produces identical state–action trajectory law as executing induced policy π_ξ in latent MDP M. Marginalizing over observations formalizes how renderer O_ξ determines effective state-conditional behavior.

### Mechanism 3: Multi-Metric Trajectory Diagnosis
Return alone can mask severe completion failures; trajectory-level metrics (distance, progress, success) reveal axis-specific degradation patterns. Corollary 4.3 extends Theorem 4.2 to any trajectory functional. Photometric shifts preserve forward motion (small distance gap) while collapsing success (86–87% SR gap), exposing dissociation between locomotion and task completion.

## Foundational Learning

- **Partially Observable MDP (POMDP) with observation kernel O_ξ(·|s)**: Why needed: KAGE-Bench's core theoretical contribution hinges on separating observation generation from latent dynamics; understanding how O_ξ maps state to observation distributions is essential for interpreting induced policy reduction. Quick check: If two visual configurations ξ and ξ' produce identical observation distributions for all states, what should happen to the generalization gap?

- **JAX vectorization (vmap, lax.scan) and JIT compilation**: Why needed: KAGE-Env achieves 33M steps/sec via end-to-end compiled simulation; understanding vmap semantics is required to extend or debug custom evaluation loops. Quick check: What happens if you include a Python print() statement inside a JIT-compiled step function?

- **PPO with CNN encoder and clipped objective**: Why needed: The baseline results use CleanRL's PPO-CNN; interpreting performance gaps requires understanding how policy gradient updates interact with visual representation learning. Quick check: In PPO, why does clipping the probability ratio prevent large policy updates that could destabilize training?

## Architecture Onboarding

- **Component map**: KAGE_Env (main JAX-native environment) -> load_config_from_yaml (parses 93 parameters) -> Renderer (factorized observation kernel O_ξ) -> Latent simulator (fixed dynamics P and reward r) -> Baseline (PPO-CNN with CNN encoder)

- **Critical path**: 1. Define config YAML specifying train visual parameters. 2. Instantiate KAGE_Env, vectorize with jax.vmap, compile with jax.jit. 3. Train PPO-CNN to convergence on train config; checkpoint periodically. 4. Evaluate checkpoints on paired eval config computing distance, progress, SR, return. 5. Aggregate max-over-training metrics across 10 seeds; report axis-level gaps.

- **Design tradeoffs**: 2D platformer simplicity vs. realism reduces exploration/dynamics confounds but may not reflect 3D embodied settings. Maximum-over-training aggregation captures best achievable transfer but may overestimate typical checkpoint performance. 34 curated pairs intentionally includes sanity checks and failure modes.

- **Failure signatures**: High train SR, near-zero eval SR with moderate distance gap → photometric/filter sensitivity. High distance, low progress, zero SR → agent reaches wrong region or fails termination condition. Small return gap despite low SR everywhere → both train and eval fail; check task competence first. Implementation error: Return depends on visual config → verify P and r are invariant in YAML.

- **First 3 experiments**: 1. Sanity check: Train/eval on identical config (e.g., black background both). Expect near-zero gap across all metrics. 2. Axis sweep: Train on black background, evaluate on progressively harder backgrounds. Observe dose-response in SR degradation. 3. Distractor ablation: Train without distractors, evaluate with 0, 1, 3, 7 same-as-agent distractors. Check if distance gap remains small while SR collapses.

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed axis-dependent generalization failure patterns transfer to more complex task domains beyond 2D platformers, such as 3D navigation, robotic manipulation, or continuous control? The conclusion states the benchmark is expected to support "future work on richer shifts, broader task families."

### Open Question 2
How do alternative learning algorithms and architectures (e.g., world models, transformers, object-centric representations) compare to PPO-CNN on the known-axis generalization gaps? The conclusion calls for "future work on... alternative learning algorithms"; only a single PPO-CNN baseline is evaluated.

### Open Question 3
What mechanisms explain why photometric and lighting shifts preserve forward motion while collapsing task completion? The paper documents that filters and effects show moderate distance degradation but severe success-rate collapse, but does not investigate the cause of this dissociation.

## Limitations

- Axis-isolation mechanism depends critically on the assumption that visual parameters do not affect latent dynamics or rewards
- 2D platformer setting, while simplifying confounding factors, may not fully capture generalization challenges in 3D embodied tasks
- Benchmark provides diagnostic isolation of failure axes but no causal analysis of what policy representations break under each shift type

## Confidence

- **High confidence**: Axis-isolation design and JAX implementation enabling 33M steps/sec are well-specified and reproducible
- **Medium confidence**: The claim that return alone can mask completion failures is well-supported by trajectory metrics, but the dissociation between locomotion and task completion may be environment-specific
- **Medium confidence**: The induced policy reduction theorem is mathematically sound for reactive policies, but its practical implications for recurrent architectures require further validation

## Next Checks

1. **Dynamic invariance verification**: Systematically test whether any visual parameter inadvertently affects transition dynamics or rewards by measuring return gaps when visual configs differ but latent states are identical
2. **Recurrent policy extension**: Extend evaluation to agents with LSTM/GRU policies to verify whether trajectory metric dissociation persists under memory-dependent policies
3. **3D transfer probe**: Adapt KAGE-Env's axis-isolation framework to a simple 3D navigation task to assess whether observed generalization patterns hold in more complex visual settings