---
ver: rpa2
title: Ground Truth Generation for Multilingual Historical NLP using LLMs
arxiv_id: '2511.14688'
source_url: https://arxiv.org/abs/2511.14688
tags:
- historical
- french
- chinese
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of applying natural language\
  \ processing (NLP) to historical and low-resource languages by using large language\
  \ models (LLMs) to generate synthetic ground-truth annotations for historical French\
  \ (16th\u201320th centuries) and Chinese (1900\u20131950) texts. The authors developed\
  \ a method where LLMs produce POS, lemmatization, and named entity recognition (NER)\
  \ annotations, which are then used to fine-tune spaCy models."
---

# Ground Truth Generation for Multilingual Historical NLP using LLMs

## Quick Facts
- arXiv ID: 2511.14688
- Source URL: https://arxiv.org/abs/2511.14688
- Reference count: 40
- Primary result: LLM-generated synthetic annotations improve NLP performance on historical French and Chinese texts

## Executive Summary
This study presents a novel approach to bootstrapping NLP models for historical and low-resource languages by leveraging large language models to generate synthetic ground-truth annotations. The method addresses the critical bottleneck of limited annotated data for historical languages by using LLMs to produce POS, lemmatization, and NER annotations for French texts spanning 16th-20th centuries and Chinese texts from 1900-1950. These synthetic annotations are then used to fine-tune spaCy models, demonstrating significant improvements over baseline models trained on modern data alone. The approach shows particular promise for French, where the fine-tuned model achieves 97.20% POS accuracy and 96.04% lemmatization accuracy on historical data, validating the potential of LLM-based synthetic data generation for domain-specific NLP tasks.

## Method Summary
The authors developed a pipeline that uses large language models to generate synthetic ground-truth annotations for historical texts. For French, the LLM was prompted to produce POS tags, lemmatization, and NER annotations for texts spanning multiple centuries (16th-20th). For Chinese, the LLM generated annotations for traditional-character texts from 1900-1950. These LLM-generated annotations were then used to fine-tune spaCy models, which were evaluated against human-annotated gold standard datasets. The approach leverages the LLM's language understanding capabilities to bridge the gap between modern and historical language patterns, enabling the creation of training data for languages and time periods with limited available annotations.

## Key Results
- French LLM annotations achieved 96.47% POS and 97.98% lemmatization accuracy, with fine-tuned model improving POS from 90.97% to 97.20% and lemmatization from 87.55% to 96.04% on historical data
- Chinese LLM annotations reached 98.95% POS and 94.26% NER accuracy, with fine-tuned model boosting POS from 67.75% to 72.33% and NER from 33.44% to 43.98% on traditional-character texts
- Even modest amounts of LLM-generated synthetic data significantly enhanced NLP performance for under-resourced historical corpora

## Why This Works (Mechanism)
The method works by leveraging the broad linguistic knowledge encoded in large language models to generate high-quality annotations for historical language variants. LLMs, trained on diverse text corpora including historical documents, possess implicit understanding of language evolution and can map historical language patterns to modern annotation standards. This enables them to produce synthetic ground truth that captures the linguistic characteristics of historical periods while adhering to contemporary annotation schemas. The generated annotations serve as effective training data for fine-tuning modern NLP models, allowing them to adapt to historical language variations without requiring extensive manual annotation efforts.

## Foundational Learning
- POS tagging principles: Essential for understanding how syntactic categories are assigned to words; quick check: verify understanding of universal POS tag sets and their application to historical variants
- Lemmatization fundamentals: Critical for normalizing word forms across time periods; quick check: ensure familiarity with morphological analysis and historical lemma variation
- NER annotation standards: Important for entity recognition across historical contexts; quick check: understand differences between modern and historical entity types and naming conventions
- Historical linguistics basics: Necessary for appreciating language evolution patterns; quick check: recognize major diachronic changes in French and Chinese over the studied periods
- SpaCy pipeline architecture: Key for understanding model fine-tuning process; quick check: familiarity with component-based NLP pipelines and their training procedures

## Architecture Onboarding

**Component map:** LLM annotation generation -> Synthetic annotation validation -> SpaCy model fine-tuning -> Historical text processing

**Critical path:** LLM generates annotations → Validation against modern standards → Fine-tuning spaCy models → Evaluation on historical test sets

**Design tradeoffs:** The approach trades computational cost of LLM inference for reduced human annotation effort, accepting potential model-specific biases in favor of scalable annotation generation. The choice of spaCy as the target framework prioritizes ease of deployment over maximal model capacity.

**Failure signatures:** Poor performance on highly archaic texts, systematic errors in rare morphological forms, degradation when historical language patterns diverge significantly from LLM training data, and inconsistent annotation quality across different historical periods.

**First experiments:**
1. Generate LLM annotations for a small validation set of historical French texts and compare against expert annotations to establish baseline quality
2. Fine-tune a spaCy POS model using LLM-generated annotations and evaluate on held-out historical test data
3. Test the approach on texts from periods not represented in the training data to assess generalization capability

## Open Questions the Paper Calls Out
None

## Limitations
- Performance sensitivity to temporal linguistic variation, with accuracy degradation on older French texts
- Modest NER improvements for Chinese (33.44% to 43.98%) suggest task-specific limitations
- Uncertainty about robustness across different historical periods and linguistic phenomena beyond evaluation scope

## Confidence
- Core claim (LLM-generated synthetic data can effectively bootstrap NLP models for historical languages): Medium
- Claim that modest amounts of synthetic data suffice for meaningful improvements: Medium

## Next Checks
1. Cross-temporal validation: Test the method on French texts from additional centuries (13th-15th) to assess generalization across wider historical spans and evaluate whether the approach can handle earlier linguistic changes beyond the 16th-20th century range tested

2. Linguistic phenomenon stress test: Systematically evaluate the method's performance on challenging linguistic features such as irregular morphology, archaic vocabulary, and syntactic constructions that differ substantially from modern usage, using expert-annotated samples as ground truth

3. Comparative annotation quality: Conduct blind evaluations where human annotators assess the quality of LLM-generated versus human-generated annotations for the same texts, measuring inter-annotator agreement and identifying systematic error patterns in the synthetic annotations