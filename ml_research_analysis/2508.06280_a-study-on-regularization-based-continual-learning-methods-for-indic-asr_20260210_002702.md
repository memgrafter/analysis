---
ver: rpa2
title: A Study on Regularization-Based Continual Learning Methods for Indic ASR
arxiv_id: '2508.06280'
source_url: https://arxiv.org/abs/2508.06280
tags:
- learning
- naive
- languages
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates continual learning for multilingual Automatic\
  \ Speech Recognition (ASR) in low-resource Indian languages, focusing on the problem\
  \ of catastrophic forgetting when sequentially learning new languages. The study\
  \ evaluates three regularization-based continual learning strategies\u2014Elastic\
  \ Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without\
  \ Forgetting (LwF)\u2014applied to a Conformer-based hybrid RNN-T/CTC model trained\
  \ incrementally across nine Indian languages."
---

# A Study on Regularization-Based Continual Learning Methods for Indic ASR

## Quick Facts
- arXiv ID: 2508.06280
- Source URL: https://arxiv.org/abs/2508.06280
- Authors: Gokul Adethya T; S. Jaya Nirmala
- Reference count: 40
- One-line primary result: LwF and MAS outperform EWC and naive fine-tuning in mitigating catastrophic forgetting for sequential multilingual ASR across 9 Indian languages

## Executive Summary
This paper investigates continual learning for multilingual Automatic Speech Recognition (ASR) in low-resource Indian languages, focusing on the problem of catastrophic forgetting when sequentially learning new languages. The study evaluates three regularization-based continual learning strategies—Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF)—applied to a Conformer-based hybrid RNN-T/CTC model trained incrementally across nine Indian languages. Using a subset of the IndicSUPERB benchmark with 3,000 low-resource utterances per language, the research compares these methods against naive fine-tuning. Key results show that LwF and MAS outperform EWC and the baseline in mitigating forgetting, particularly under noisy conditions, with LwF achieving the most stable Word Error Rates (WERs) below 1.0 and strong Backward Transfer scores. However, all methods exhibit a stability-plasticity trade-off: longer training reduces WER but increases forgetting. Notably, RNN-T models yield lower WERs than CTC but amplify forgetting. The study concludes that while regularization-based continual learning methods improve knowledge retention, achieving practical deployment-level performance remains challenging due to persistent forgetting and suboptimal WERs during new language acquisition.

## Method Summary
The study evaluates three regularization-based continual learning strategies—Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF)—applied to a Conformer-based hybrid RNN-T/CTC model trained incrementally across nine Indian languages. Using a subset of the IndicSUPERB benchmark with 3,000 low-resource utterances per language, the research compares these methods against naive fine-tuning. The model is initialized from a Hindi-pretrained checkpoint and trained sequentially across languages, with CL regularization applied during each new language learning phase. WER and Backward Transfer metrics are computed after each task to measure forgetting.

## Key Results
- LwF and MAS outperform EWC and naive fine-tuning in mitigating catastrophic forgetting across the 9-language sequence
- LwF achieves the most stable WERs below 1.0 and strongest Backward Transfer scores, particularly under noisy conditions
- All methods exhibit stability-plasticity trade-off: longer training reduces WER but increases forgetting
- RNN-T models yield lower WERs than CTC but amplify forgetting compared to CTC-only decoding

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation Anchoring (LwF)
- **Claim:** Distillation-based loss constrains the model to produce outputs consistent with previous task knowledge when learning new languages.
- **Mechanism:** LwF adds a distillation loss term (KL-divergence) that penalizes deviations between the current model's outputs and a frozen copy of the previous model's outputs on new task data. The total loss is: `L_total = (1 - α_KD) · L_base + α_KD · L_distill`. This anchors learning to previous predictions without requiring stored data.
- **Core assumption:** The new task data contains sufficient distribution overlap for distillation to transfer meaningful constraints from the old model.
- **Evidence anchors:**
  - [abstract]: "LwF, selected for their suitability in no-replay, privacy-conscious scenarios"
  - [Section A.4.1]: Describes distillation applied separately on RNNT logits and CTC output probabilities
  - [corpus]: Related paper "Continual Learning with Embedding Layer Surgery" (arxiv:2501.07875) confirms distillation as a viable CL strategy for ASR
- **Break condition:** If α_KD is set too high (e.g., 0.5), the model cannot learn new tasks effectively—WER approaches 1.0, worse than naive fine-tuning.

### Mechanism 2: Parameter Importance Regularization (EWC/MAS)
- **Claim:** Penalizing changes to parameters identified as important for previous tasks reduces catastrophic forgetting.
- **Mechanism:** EWC computes importance via the Fisher Information Matrix (squared gradients) and adds a quadratic penalty: `L_EWC = λ · Σ F_j(θ_j - θ*_j)²`. MAS computes importance via gradients of the squared L2 norm of model outputs. Both constrain updates to important parameters while allowing plasticity elsewhere.
- **Core assumption:** Importance estimates computed on limited data accurately reflect true parameter relevance across tasks.
- **Evidence anchors:**
  - [Section 5.1]: "EWC achieves the lowest WER across task lengths, demonstrating strong performance retention on the current task. However... EWC exhibits the worst BWT of all methods"
  - [Section A.4.2-A.4.3]: Details MAS importance computation via `Ω_j ← Ω_j + |∂L_logits/∂θ_j|` and EWC Fisher accumulation with decay factor γ
  - [corpus]: "Group and Exclusive Sparse Regularization-based Continual Learning" (arxiv:2601.03658) supports regularization-based CL efficacy
- **Break condition:** EWC shows poor BWT on RNN-T paths, suggesting the Fisher-based importance may not capture sequential dependencies in recurrent models effectively.

### Mechanism 3: Noise as Implicit Regularization for Stability
- **Claim:** Noisy training data improves backward transfer (knowledge retention) but degrades absolute WER.
- **Mechanism:** Noise appears to act as an implicit regularizer, reducing overfitting to new task specifics and thereby preserving representations useful for prior tasks. However, this regularization comes at the cost of transcription accuracy.
- **Core assumption:** Noise characteristics in training meaningfully correlate with regularization effects rather than simply degrading all learning uniformly.
- **Evidence anchors:**
  - [Section 5.1]: "noise appears to improve backward transfer, likely due to regularization effects. However, this improvement comes with a trade-off: WER increases"
  - [Section 6]: "The inverse link between noise-driven BWT improvement and WER degradation suggests noise acts as an implicit regularizer"
  - [corpus]: No direct corpus evidence on noise-as-regularizer in CL-ASR; this is an underexplored area
- **Break condition:** If noise is excessive or distributionally mismatched from test conditions, regularization benefit may not materialize—this boundary is not characterized in the paper.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The central problem this paper addresses; understanding that sequential gradient updates overwrite previously learned representations is essential.
  - **Quick check question:** Can you explain why naive fine-tuning causes WER on Hindi to degrade after training on Bengali?

- **Concept: Stability-Plasticity Trade-off**
  - **Why needed here:** All three CL methods exhibit this trade-off—longer training improves new task WER but worsens BWT. Understanding this helps set realistic expectations.
  - **Quick check question:** If you increase epochs from 1 to 10, what happens to both current task WER and backward transfer?

- **Concept: Hybrid CTC/RNN-T Loss**
  - **Why needed here:** The model jointly optimizes both objectives (`L_base = 0.7 · L_RNNT + 0.3 · L_CTC`), and they exhibit different forgetting properties—RNN-T has lower WER but higher forgetting.
  - **Quick check question:** Which decoding path would you prioritize for deployment if retention of prior languages is critical?

## Architecture Onboarding

- **Component map:**
  - Conformer encoder: Captures speech features via convolution + self-attention
  - RNN-T decoder: Encoder → Prediction network (autoregressive) → Joint network → Transducer loss
  - CTC decoder: Frame-level classification with blank token, auxiliary objective
  - CL regularization head: EWC (Fisher storage), MAS (importance accumulation), or LwF (frozen model copy + distillation loss)

- **Critical path:**
  1. Initialize from Hindi-pretrained `indicconformer_stt_hi_hybrid_rnnt_large` (~130M params)
  2. For each new language T_k: Load previous checkpoint → Train on D_k only with CL-augmented loss → Save checkpoint and update importance/distillation target
  3. Evaluate on all T_1 through T_k test sets after each task

- **Design tradeoffs:**
  - LwF vs. EWC/MAS: LwF requires storing frozen model outputs (memory overhead at inference during training), EWC/MAS require storing parameter importance matrices
  - α_KD (LwF): 0.1 balances learning and retention; 0.5 over-constrains and blocks new learning
  - Epochs: More epochs → better WER but worse BWT; 2-5 epochs may be practical compromise
  - CTC vs. RNN-T: RNN-T gives better WER but amplifies forgetting—use CTC if retention is priority

- **Failure signatures:**
  - WER near 1.0 on new tasks → α_KD too high (model frozen)
  - BWT sharply negative for early languages → insufficient regularization strength (λ too low) or too many epochs
  - Large WER variance across runs → importance estimates unstable on low-resource data (3k utterances)

- **First 3 experiments:**
  1. **Baseline sanity check:** Run naive fine-tuning on 3 languages with 2 epochs each; confirm catastrophic forgetting (BWT < -0.3 for T_1)
  2. **LwF calibration:** Test α_KD ∈ {0.05, 0.1, 0.2} on first 3 languages; plot WER vs. BWT trade-off curve
  3. **Architecture path comparison:** For LwF with α_KD=0.1, separately evaluate CTC-only vs. RNN-T-only decoding on clean vs. noisy test sets across all 9 languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific ordering of languages impact performance dynamics and catastrophic forgetting in sequential multilingual ASR?
- Basis in paper: [explicit] The authors state in the Limitations section that "the study does not systematically investigate the impact of language ordering on performance" and identify it as a key variable.
- Why unresolved: The current experimental design uses a fixed sequence (Hindi → Bengali → ... → Odia), leaving the sensitivity of the model and CL methods to different task sequences unknown.
- What evidence would resolve it: A comparison of WER and Backward Transfer scores across multiple randomized language orderings to determine if high-resource languages should be learned first or last.

### Open Question 2
- Question: Can current regularization-based CL methods maintain efficacy when transitioning from discrete tasks to online learning streams?
- Basis in paper: [explicit] The Future Work section suggests "transitioning to online learning paradigms where data arrives as a continuous stream... posing new challenges for CL algorithm efficiency."
- Why unresolved: The current study uses discrete "tasks" with 3,000 utterances each, whereas real-world deployment involves continuous, non-i.i.d. data streams which may destabilize regularization estimates.
- What evidence would resolve it: Evaluating LwF, MAS, and EWC in a streaming scenario without clear task boundaries to measure real-time adaptation vs. forgetting.

### Open Question 3
- Question: What specific modifications are required to mitigate the amplified catastrophic forgetting observed in RNN-T architectures compared to CTC?
- Basis in paper: [inferred] The Discussion notes that "RNN-T models, while delivering superior WER, amplified catastrophic forgetting," suggesting current regularization is less effective for complex recurrent decoders.
- Why unresolved: The paper applies general CL methods (EWC, LwF, MAS) equally to CTC and RNN-T paths, but the results imply that RNN-T's sequential nature requires specialized retention mechanisms not tested here.
- What evidence would resolve it: Developing and testing RNN-T-specific regularization techniques (e.g., distillation targeting the prediction network) that close the performance gap between CTC and RNN-T stability.

## Limitations
- Data scale uncertainty: Study uses only 3,000 training utterances per language (2,000 clean + 1,000 noisy), representing minimal low-resource setting with limited practical applicability
- Noise characterization ambiguity: Paper notes noise improves backward transfer but degrades WER, suggesting noise acts as implicit regularization, but specific noise profiles and SNR ranges are not characterized
- Model architecture constraints: Results specific to Conformer-based hybrid RNN-T/CTC architecture initialized from Hindi; different architectures may exhibit different forgetting patterns

## Confidence
**High confidence**: The core finding that LwF and MAS outperform EWC and naive fine-tuning in mitigating catastrophic forgetting across the 9-language sequence. The stability-plasticity trade-off (longer training improves new task WER but worsens BWT) is consistently observed and theoretically grounded. The differential behavior between RNN-T and CTC paths is reproducible given the experimental setup.

**Medium confidence**: The claim that noise acts as implicit regularization for stability. While the inverse correlation between noise-driven BWT improvement and WER degradation is observed, the mechanism is inferred rather than experimentally validated through controlled noise ablation studies. The optimal α_KD=0.1 for LwF is empirically supported but may not generalize across different data scales or architectures.

**Low confidence**: The assertion that regularization-based CL methods achieve "practical deployment-level performance" remains questionable. All methods exhibit persistent forgetting (BWT values suggest significant degradation for early languages), and WERs during new language acquisition remain suboptimal. The paper acknowledges this limitation but does not provide deployment thresholds or benchmarks against non-CL alternatives.

## Next Checks
1. **Scale validation experiment**: Repeat the 9-language sequential learning with 10× the training data per language (30,000 utterances) to determine whether CL method effectiveness scales with data volume or if benefits plateau at low-resource scales.

2. **Noise mechanism ablation**: Systematically vary noise intensity (multiple SNR levels) and types (additive white, babble, reverberation) across a subset of languages to quantify the relationship between noise characteristics and backward transfer improvements, distinguishing true regularization effects from uniform degradation.

3. **Architecture generalization test**: Apply the same CL methodology to a pure CTC architecture and a transformer-only encoder-decoder model, comparing forgetting patterns and CL method effectiveness to the Conformer hybrid baseline, particularly focusing on whether RNN-T's amplified forgetting is architecture-specific.