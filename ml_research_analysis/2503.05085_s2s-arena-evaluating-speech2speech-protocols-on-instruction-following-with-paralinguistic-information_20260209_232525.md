---
ver: rpa2
title: S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with
  Paralinguistic Information
arxiv_id: '2503.05085'
source_url: https://arxiv.org/abs/2503.05085
tags:
- speech
- arxiv
- information
- paralinguistic
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S2S-Arena, a novel benchmark for evaluating
  speech-to-speech (S2S) models' instruction-following capabilities with paralinguistic
  information. The benchmark addresses limitations in existing evaluations that rely
  on text-based metrics and overlook paralinguistic cues in both speech understanding
  and generation.
---

# S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following with Paralinguistic Information

## Quick Facts
- arXiv ID: 2503.05085
- Source URL: https://arxiv.org/abs/2503.05085
- Reference count: 14
- Primary result: Introduces S2S-Arena benchmark for evaluating speech-to-speech models with paralinguistic information

## Executive Summary
This paper introduces S2S-Arena, a novel benchmark for evaluating speech-to-speech (S2S) models' instruction-following capabilities with paralinguistic information. The benchmark addresses limitations in existing evaluations that rely on text-based metrics and overlook paralinguistic cues in both speech understanding and generation. Using manual arena-style pairwise comparisons across 154 samples in four domains, the authors demonstrate that GPT-4o-realtime outperforms other models, while cascaded ASR-LLM-TTS systems generally outperform jointly trained models.

## Method Summary
The authors design a comprehensive evaluation framework using 154 samples across four domains (education, social companionship, entertainment, and medical consultation) with 21 tasks at four difficulty levels. The benchmark incorporates both TTS and human recordings to test model performance across different speech qualities. Instead of unreliable automatic speech evaluation methods, they employ manual arena-style pairwise comparisons using ELO ranking. This approach allows for nuanced evaluation of both content accuracy and paralinguistic features in speech understanding and generation.

## Key Results
- GPT-4o-realtime outperforms other models in speech-to-speech instruction following tasks
- Cascaded ASR-LLM-TTS systems generally outperform jointly trained models
- Speech models can understand paralinguistic information but struggle to generate appropriate speech with such features

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive evaluation methodology that addresses the limitations of text-based metrics in speech-to-speech tasks. By using manual pairwise comparisons with ELO ranking, the framework captures nuanced aspects of speech quality and paralinguistic features that automatic metrics miss. The inclusion of both TTS and human recordings provides a realistic assessment of model performance across different speech qualities, while the diverse domain coverage ensures generalizability of results.

## Foundational Learning
1. **Speech-to-Speech (S2S) Models** - Systems that directly convert input speech to output speech without intermediate text representation
   - Why needed: Enables evaluation of end-to-end speech understanding and generation capabilities
   - Quick check: Verify the model can process raw audio input and produce natural speech output

2. **Paralinguistic Information** - Non-lexical elements of speech including emotion, emphasis, and speech rate
   - Why needed: Critical for natural and effective human-computer communication
   - Quick check: Ensure evaluation includes both content and paralinguistic feature assessment

3. **Cascaded vs. Joint Models** - Cascaded systems use separate components (ASR, LLM, TTS) while joint models process speech end-to-end
   - Why needed: Understanding architectural trade-offs affects model selection and development
   - Quick check: Compare performance and complexity trade-offs between cascaded and joint approaches

## Architecture Onboarding
**Component Map:** Audio Input -> Speech Understanding -> Instruction Processing -> Speech Generation -> Audio Output

**Critical Path:** The evaluation pipeline follows: Sample Selection → Human/LLM Evaluation → ELO Ranking → Performance Analysis

**Design Tradeoffs:** Manual pairwise comparison vs. automatic metrics (accuracy vs. scalability), cascaded vs. joint architectures (flexibility vs. efficiency)

**Failure Signatures:** Poor paralinguistic generation indicates limitations in speech synthesis modules; multilingual limitations point to speech module constraints rather than LLM capabilities

**First Experiments:**
1. Run baseline evaluation with a simple cascaded system to establish performance floor
2. Test a single joint model on a subset of samples to compare architectural approaches
3. Evaluate paralinguistic feature understanding separately from generation capabilities

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Pairwise comparison methodology may introduce subjective biases depending on rater expertise
- Limited to four domains, potentially missing broader real-world scenarios
- Focus on English-only evaluation limits generalizability to multilingual contexts

## Confidence
- **High Confidence**: GPT-4o-realtime outperforms other models; cascaded systems outperform joint models
- **Medium Confidence**: Models understand but struggle to generate paralinguistic information; multilingual limitations are speech-module-dependent
- **Low Confidence**: Relative performance differences between individual models within each category

## Next Checks
1. Conduct inter-rater reliability analysis to quantify subjective variation in pairwise comparisons
2. Expand benchmark to include additional languages and language pairs for multilingual testing
3. Perform ablation studies targeting specific paralinguistic features to identify generation challenges