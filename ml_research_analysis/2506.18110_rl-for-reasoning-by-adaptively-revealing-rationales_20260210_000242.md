---
ver: rpa2
title: RL for Reasoning by Adaptively Revealing Rationales
arxiv_id: '2506.18110'
source_url: https://arxiv.org/abs/2506.18110
tags:
- learning
- reward
- reasoning
- adaback
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adaptive backtracking (AdaBack), a per-sample
  curriculum learning algorithm that dynamically adjusts the amount of supervision
  during RL training based on reward feedback. The key insight is that revealing partial
  prefixes of target outputs allows models to incrementally learn reasoning chains
  that are otherwise intractable under full supervision or sparse RL rewards.
---

# RL for Reasoning by Adaptively Revealing Rationales

## Quick Facts
- **arXiv ID**: 2506.18110
- **Source URL**: https://arxiv.org/abs/2506.18110
- **Reference count**: 40
- **Primary result**: AdaBack improves mathematical reasoning performance by dynamically adjusting supervision levels during RL training based on reward feedback

## Executive Summary
This paper introduces AdaBack, a per-sample curriculum learning algorithm that dynamically adjusts supervision levels during RL training based on reward feedback. The key insight is that revealing partial prefixes of target outputs allows models to incrementally learn reasoning chains that are otherwise intractable under full supervision or sparse RL rewards. By adaptively revealing only a prefix of the solution and gradually reducing this supervision based on model performance, AdaBack enables learning of long reasoning chains through incremental completion of increasingly larger segments.

AdaBack is motivated by the observation that standard RL struggles with sparse rewards and exponentially large output spaces, while SFT becomes prohibitively expensive for long sequences. The authors demonstrate that AdaBack can solve a synthetic parity task where both SFT and standard RL fail, providing a clear separation result. On mathematical reasoning benchmarks (MATH, GSM8k), AdaBack improves performance over standard RL and SFT+RL pipelines, with particular strength in out-of-distribution settings.

## Method Summary
AdaBack maintains per-sample supervision ratio intervals [ρ_min, ρ_max] and samples ρ_t uniformly for each training iteration. For each sample, it reveals a prefix of the target sequence (Y_1:k where k = floor(ρ · |Y|)), generates completions conditioned on this partial solution, and computes average reward across rollouts. If reward r_t < threshold τ, supervision increases by narrowing to [ρ_t, ρ_max]; if r_t ≥ τ, supervision decreases by narrowing to [0, ρ_t]. This performs stochastic binary search over supervision levels using reward as the success signal. The method uses GRPO with 8 rollouts per sample and includes 10% probability injection of ρ=0 to reduce train-test distribution mismatch.

## Key Results
- AdaBack achieves >0.8 reward on synthetic chain-of-parities task in <700 iterations while SFT+RL plateaus at 0.1 (format-only reward)
- On GSM8k, AdaBack improves pass@k at high k values, suggesting expanded solution space rather than reweighting existing answers
- Base model + AdaBack often matches performance of SFT-initialized counterparts, indicating acquisition of new reasoning capabilities
- Strong performance on out-of-distribution settings: Base-7 GSM8k (unfamiliar numerical format) and Tensor-2 GSM8k (longer reasoning chains)

## Why This Works (Mechanism)

### Mechanism 1: Per-sample adaptive supervision
- **Claim**: Per-sample adaptive supervision enables learning in sparse-reward regimes where fixed curricula fail.
- **Mechanism**: For each training sample i, maintain [ρ_min, ρ_max]. Sample ρ_t uniformly, generate completions conditioned on revealed prefix, compute average reward r_t across rollouts. If r_t < threshold τ, increase supervision by narrowing to [ρ_t, ρ_max]; if r_t ≥ τ, decrease supervision by narrowing to [0, ρ_t].
- **Core assumption**: Reward signals provide meaningful feedback about partial competence on reasoning subtasks.
- **Evidence**: Synthetic parity task shows AdaBack reaches >0.8 reward while SFT+RL stays at 0.1; adaptive curriculum RL appears in Writing-RL for long-form tasks.

### Mechanism 2: Prefix revelation transforms exploration
- **Claim**: Prefix revelation transforms exponentially difficult exploration into tractable sequential learning.
- **Mechanism**: For n-step reasoning with success probability p, naive exploration yields p^n success rate. By revealing n-1 steps and training only final step, success becomes Θ(p). Progressively reveal less as model masters each suffix.
- **Core assumption**: Reasoning chains have decomposable structure where earlier steps can be treated as fixed context while learning later steps.
- **Evidence**: Chain-of-parities demonstrates separation; BREAD similarly bridges SFT and RL using expert anchor points.

### Mechanism 3: Base model solution space expansion
- **Claim**: Adaptive supervision on base models can expand the solution space rather than merely reweighting existing outputs.
- **Mechanism**: Starting from partial solutions the model couldn't generate independently, gradient-based learning discovers new reasoning patterns. Unlike standard RL which reinforces high-probability paths, prefix conditioning exposes model to valid reasoning states outside current distribution.
- **Core assumption**: Base models have sufficient capacity to learn new reasoning patterns when provided with informative training signals.
- **Evidence**: Pass@k shows AdaBack significantly improves coverage at high k values, especially without SFT initialization.

## Foundational Learning

- **Concept**: Sparse reward problem in RL
  - **Why needed**: AdaBack is specifically motivated by failure of standard RL when correct outputs are exponentially rare in output space.
  - **Quick check**: Can you explain why a binary-reward task with 100-step reasoning chains is intractable under random exploration?

- **Concept**: Curriculum learning
  - **Why needed**: AdaBack implements automatic per-sample curriculum scheduling without manual staging.
  - **Quick check**: How does curriculum learning differ from simply training on easier examples first?

- **Concept**: GRPO (Group Relative Policy Optimization)
  - **Why needed**: AdaBack is implemented within GRPO, using multiple rollouts per sample to estimate difficulty via average rewards.
  - **Quick check**: Why does GRPO's multi-rollout structure naturally support AdaBack's reward-based difficulty estimation?

## Architecture Onboarding

- **Component map**: 
  Supervision manager -> Prefix conditioner -> RL engine (GRPO) -> Reward aggregator -> Global state (EMA)

- **Critical path**:
  1. Sample ρ_t from current interval for each training sample
  2. Reveal prefix, generate k rollouts conditioned on partial solution
  3. Compute rewards, update supervision interval based on threshold τ
  4. With 10% probability, inject ρ=0 to reduce train-test distribution mismatch

- **Design tradeoffs**:
  - Threshold τ: Higher values require stronger performance before reducing supervision (stricter curriculum)
  - Format reward r_format: 0.1 recommended for tasks with nontrivial output structure; unnecessary for SFT-initialized models
  - ρ=0 injection probability: 10% balances convergence speed with distribution matching
  - Learning rate: 10^-6 found stable across experiments

- **Failure signatures**:
  - Training/test reward gap widening: May indicate insufficient ρ=0 injection
  - Early saturation on instruct-tuned models: Signals pretraining already covered task distribution
  - Plateau at format-reward level (0.1): Exploration failing to discover correct reasoning paths

- **First 3 experiments**:
  1. **Sanity check**: Replicate chain-of-parities (L=16, n=1024 samples) with SFT+RL baseline to verify separation result; expect AdaBack to reach >0.8 reward while baseline stays at 0.1.
  2. **Ablation on τ**: Test reward thresholds {0.3, 0.5, 0.7} on GSM8k base model; monitor convergence speed and final accuracy to calibrate strictness.
  3. **Pass@k analysis**: Compare AdaBack vs standard RL on GSM8k base model using k ∈ {2, 4, 8, 16, 32, 64, 128, 256}; verify that AdaBack expands solution space by checking if pass@k gap increases with k.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can reinforcement learning guided by adaptive partial supervision enable models to discover solutions that are exponentially unlikely under the model's initial distribution, or does it primarily reweight existing capabilities?
- **Basis**: The authors state their central research question about whether RL can enable discovery of solutions previously exponentially unlikely under the model's initial distribution.
- **Why unresolved**: While pass@k results suggest expanded solution spaces, this remains fundamentally difficult to distinguish from reweighting, especially for models with extensive pretraining exposure.
- **What evidence would resolve it**: Demonstrating successful learning on tasks where base model has near-zero probability of correct solutions across all k values for very large k, combined with analysis of solution novelty.

### Open Question 2
- **Question**: How can per-sample curriculum adaptation be made effective in large dataset regimes where individual samples are seen infrequently?
- **Basis**: The authors identify that per-sample adaptation degrades to global averages when samples are seen infrequently in large datasets.
- **Why unresolved**: The proposed future direction of using embedding-space neighborhoods for supervision scheduling remains untested.
- **What evidence would resolve it**: Implementing and evaluating k-nearest-neighbor based supervision schedules on large-scale datasets, comparing performance against current global average approach.

### Open Question 3
- **Question**: What are the precise conditions under which AdaBack provides benefit versus when it fails to help, particularly regarding the relationship between pretraining exposure and curriculum effectiveness?
- **Basis**: The paper shows AdaBack provides no benefit on MATH with LLaMA 3.2 3B-Instruct, attributing this to pretraining exposure, but lacks systematic characterization.
- **Why unresolved**: The paper does not systematically vary pretraining exposure or model uncertainty to characterize when curriculum learning becomes unnecessary.
- **What evidence would resolve it**: Controlled experiments varying pretraining familiarity with task domains and measuring correlation between initial model uncertainty and AdaBack's effectiveness.

## Limitations

- Adaptive curriculum effectiveness degrades in large dataset regimes where samples have no reward history, falling back to global averages
- Claims about expanding solution spaces versus reweighting existing outputs remain partially speculative, supported only by indirect pass@k evidence
- Method assumes base models have sufficient capacity to learn genuinely new reasoning patterns, which may not hold for smaller models or distant domains

## Confidence

- **High Confidence**: Core mechanism of adaptive supervision based on reward feedback is technically sound; synthetic parity task separation is convincing
- **Medium Confidence**: Performance improvements on mathematical reasoning benchmarks demonstrated, but extent of solution space expansion versus reweighting remains partially speculative
- **Low Confidence**: Claims about particular strength in out-of-distribution settings supported by two specific examples but lack broader validation

## Next Checks

1. **Noise Robustness Test**: Apply AdaBack to a reasoning task with injected noise in the reward signal and measure how quickly the curriculum degrades or adapts, validating whether reward-based supervision adjustment remains effective when signals are imperfect.

2. **Model Capacity Sensitivity**: Compare AdaBack's performance across model sizes (1B, 3B, 8B) on a fixed reasoning task to determine the minimum model capacity required for genuine solution space expansion versus simple reweighting.

3. **Curriculum Degradation Analysis**: On a large-scale dataset (full GSM8k), track per-sample supervision statistics over training to quantify how often samples maintain meaningful individual adaptation versus falling back to global averages, and measure the impact on final performance.