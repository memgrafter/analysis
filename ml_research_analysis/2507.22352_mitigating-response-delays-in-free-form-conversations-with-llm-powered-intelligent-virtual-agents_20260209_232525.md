---
ver: rpa2
title: Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent
  Virtual Agents
arxiv_id: '2507.22352'
source_url: https://arxiv.org/abs/2507.22352
tags:
- uni00000048
- agents
- response
- uni00000003
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the effects of response delays and conversational
  fillers on user experience with LLM-powered virtual agents in VR. Participants interacted
  with nine agents across three scenarios under three delay levels (1.5s, 4.0s, 6.5s)
  and three filler types (none, artificial wait indicator, natural conversational
  filler).
---

# Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents

## Quick Facts
- **arXiv ID**: 2507.22352
- **Source URL**: https://arxiv.org/abs/2507.22352
- **Reference count**: 40
- **Primary result**: Response delays above 4 seconds significantly degrade user experience with LLM-powered IVAs in VR, while natural conversational fillers improve perceived response time at medium and high delay levels.

## Executive Summary
This study investigates how response delays and conversational fillers affect user experience with LLM-powered virtual agents in VR environments. Participants interacted with nine agents across three scenarios (store, hotel, museum) under three latency conditions (1.5s, 4.0s, 6.5s) and three filler types (none, artificial wait indicator, natural conversational filler). The research demonstrates that latency above 4 seconds significantly degrades multiple perception dimensions including engagement, good impression, competence, and willingness to interact again. Natural conversational fillers significantly improve perceived response time at medium and high delay levels, while artificial fillers do not provide the same benefit. Participants strongly preferred agents with faster responses, but conversational fillers increased tolerance for delayed responses.

## Method Summary
The study employed a Unity 2022 LTS VR application on Meta Quest Pro headsets, connecting to a local server running FasterWhisper medium for ASR, Llama3.1-8b-Q5 via llamafile for LLM processing, and Edge-TTS for speech synthesis. Response times were measured from user speech to agent reply, with artificial delays (0s, 2.5s, 5s) added post-generation to create three latency conditions. Three filler types were implemented: no filler (idle state), artificial filler (loading icon with sound), and natural filler (random thinking gesture with voice line). The experiment used a 3×3 within-subjects design with 54 participants, counterbalanced via Balanced Latin Square across 18 different orders to test all combinations of latency and filler types across nine agent-scenario pairs.

## Key Results
- Latency above 4 seconds significantly degrades user experience metrics including perceived response time, engagement, good impression, competence, and willingness to interact again
- Natural conversational fillers significantly improve perceived response time at medium and high delay levels, while artificial fillers do not provide similar benefits
- Participants strongly preferred agents with faster responses, but conversational fillers increased tolerance for delayed responses
- The study demonstrates the importance of minimizing response latency and using natural conversational fillers to enhance user experience in interactive conversational systems

## Why This Works (Mechanism)
The effectiveness of natural conversational fillers works by managing user expectations and providing psychological comfort during wait times. When users perceive that an agent is "thinking" or processing information through natural gestures and voice lines, they are more tolerant of actual processing delays. This cognitive reframing reduces frustration and maintains engagement even when underlying system latency remains unchanged. The artificial fillers fail because they provide no meaningful information about the agent's state and feel disconnected from the conversational context.

## Foundational Learning
- **Within-subjects experimental design**: Needed to control for individual differences in VR experience and conversational preferences; quick check: verify each participant experiences all 9 condition combinations
- **Balanced Latin Square counterbalancing**: Required to control for order effects across multiple experimental conditions; quick check: confirm 18 unique orders cover all pairwise combinations
- **Artificial delay injection methodology**: Used to isolate latency effects from natural system variability; quick check: validate delay timing accuracy across all conditions
- **Conversational filler taxonomy**: Differentiates between meaningful natural fillers and superficial artificial indicators; quick check: confirm filler duration matches injected delay timing
- **Perception dimension measurement**: Uses validated Likert scales to capture multiple aspects of user experience beyond simple satisfaction; quick check: ensure all 6 scales are administered consistently
- **VR-specific interaction design**: Accounts for unique challenges of conversational interfaces in immersive environments; quick check: verify agent animation states don't conflict with response timing

## Architecture Onboarding

### Component Map
User Speech → ASR (FasterWhisper) → LLM (Llama3.1-8b-Q5) → TTS (Edge-TTS) → Audio Output → Unity VR Interface

### Critical Path
Speech input → ASR processing → LLM generation → TTS synthesis → Audio playback → Visual feedback (lipsync/animation)

### Design Tradeoffs
Fixed artificial delays provide experimental control but don't reflect real-world latency variability; natural fillers require extensive animation/voice line libraries but provide better user experience; VR immersion requires seamless integration of conversational elements with environmental interaction

### Failure Signatures
System response time exceeding 1.5s baseline indicates ASR/LLM/TTS performance issues; filler playback overlapping with agent response suggests state machine synchronization problems; inconsistent delay injection across conditions indicates timing mechanism failures

### First 3 Experiments
1. Measure baseline SRT without artificial delays to establish system performance metrics
2. Test filler playback timing accuracy by injecting known delays and verifying filler duration
3. Validate agent state transitions (thinking → attentive-idle → speaking) under different filler conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Laboratory setting with specific hardware configuration may not reflect real-world deployment scenarios with network latency and variable computational resources
- Sample size of 54 participants from university populations limits demographic representativeness
- Artificial delay injection methodology doesn't capture complex, variable latency patterns in production LLM systems
- Focus on specific VR contexts (store, hotel, museum) may not generalize to other conversational domains or non-VR interfaces

## Confidence

| Claim | Confidence |
|-------|------------|
| Latency above 4 seconds significantly degrades user experience metrics | High |
| Natural conversational fillers improve perceived response time at medium/high delays | Medium |
| Artificial conversational fillers are ineffective compared to natural ones | Medium |
| Absolute tolerance thresholds (1.5s vs 4s vs 6.5s) should be interpreted cautiously | Low |

## Next Checks

1. **External validation with production systems**: Replicate the study using a production LLM service (e.g., OpenAI API or Claude) with variable network conditions to assess whether the 4-second threshold holds under realistic latency patterns rather than fixed artificial delays.

2. **Demographic generalization**: Conduct the same experimental protocol with diverse participant populations across different age groups, technical backgrounds, and cultural contexts to validate whether the observed tolerance thresholds and filler preferences remain consistent.

3. **Longitudinal interaction effects**: Implement a follow-up study measuring user experience over extended conversational sessions (30+ minutes) to determine whether initial tolerance for delays erodes with continued exposure, and whether conversational fillers maintain their effectiveness over time.