---
ver: rpa2
title: 'SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset
  on Southeast Asian Languages'
arxiv_id: '2508.07069'
source_url: https://arxiv.org/abs/2508.07069
tags:
- dialogue
- evaluation
- language
- human
- culturally
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEADialogues, a multilingual dialogue dataset
  grounded in Southeast Asian cultural contexts. The dataset comprises 32,000 dialogues
  across eight languages from six Southeast Asian countries, incorporating persona
  attributes and culturally relevant topics.
---

# SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages

## Quick Facts
- arXiv ID: 2508.07069
- Source URL: https://arxiv.org/abs/2508.07069
- Authors: Muhammad Dehan Al Kautsar, Aswin Candra, Muhammad Alif Al Hakim, Maxalmina Satria Kahfi, Fajri Koto, Alham Fikri Aji, Peerat Limkonchotiwat, Ekapol Chuangsuwanich, Genta Indra Winata
- Reference count: 40
- Primary result: Introduces SEADialogues, a multilingual dialogue dataset grounded in Southeast Asian cultural contexts with 32,000 dialogues across eight languages

## Executive Summary
SEADialogues addresses the critical need for culturally grounded dialogue datasets in Southeast Asian languages by creating a multilingual resource with 32,000 dialogues spanning eight languages across six countries. The dataset incorporates persona attributes and culturally relevant topics through a four-stage pipeline involving template generation, lexicalization with curated entity lists, dialogue generation using LLMs, and human evaluation. The approach demonstrates that culturally appropriate dialogue generation requires more than translation, instead necessitating systematic lexicalization with language-tagged entities and persona-conditioned multi-topic prompting.

## Method Summary
The dataset creation pipeline consists of four stages: (1) Template Generation using GPT-4.1 mini to create 300 scenarios from 100 curated topics, followed by human filtering, (2) Lexicalization where delexicalized placeholders are filled from curated entity pools with language-specific tags and coupled-entity constraints enforced via JSON dictionaries, (3) Dialogue Generation with LLMs using persona and scenario prompts with sampling parameters (temperature 0.7, top-p 0.8), and (4) Human Annotation by three annotators per language on fluency, engagingness, coherence, naturalness, and cultural relevance. TOP2VEC clustering ensures thematic coherence between scenario pairs for smooth topic transitions.

## Key Results
- Proprietary models (Gemini 1.5 Flash, GPT-4o mini) outperform open-weight models (Llama-3.1, Aya-8B-Expanse) in all human evaluation metrics
- G-Eval shows moderate correlation with human judgments for coherence and naturalness (Pearson 0.58-0.61 for Minangkabau) but underperforms on cultural relevance (0.22-0.48) and fluency (0.03-0.19)
- The dataset successfully generates 32,000 multilingual dialogues across eight SEA languages with persona attributes and culturally relevant topics

## Why This Works (Mechanism)

### Mechanism 1
Cultural grounding via structured lexicalization produces more coherent and relevant dialogues than direct translation or zero-shot generation. Templates with delexicalized placeholders are systematically filled from curated entity pools with language-specific tags, coupled with entity mappings that enforce contextual dependencies. Core assumption: Cultural appropriateness can be decomposed into discrete, composable entity slots. Evidence anchors: [section 3.2] "We build a dictionary in JSON files for pairs of slots that are inherently linked either culturally or semantically... exclude combinations that would be incompatible." Break condition: If entity-level grounding fails to capture discourse-level cultural patterns (pragmatics, honorifics, indirect speech), slot-filling alone may produce superficially relevant but pragmatically unnatural dialogues.

### Mechanism 2
Persona-conditioned multi-topic prompting improves engagement and naturalness by simulating real conversational dynamics. Each dialogue combines two scenarios and two personas with personality traits, with TOP2VEC clustering ensuring thematic coherence between scenario pairs. The prompt explicitly requires smooth topic transitions marked by `[TRANSITION]`. Core assumption: Multi-topic structure with persona constraints elicits more human-like behavior than single-topic, persona-free generation. Evidence anchors: [section 3.1] "This multi-scenario design mirrors the dynamic nature of real-world conversations." Break condition: If personas are stereotyped or scenarios are thematically distant despite clustering, transitions may feel forced rather than organic.

### Mechanism 3
G-Eval with chain-of-thought prompting shows moderate correlation with human judgments on coherence and naturalness, but underperforms on cultural nuance and fluency. G-Eval uses task definition, criteria, and step-by-step reasoning to produce scores. Correlation analysis (Pearson 0.58-0.61 for coherence/naturalness on Minangkabau) shows alignment, but fluency (0.03-0.19) and cultural relevance (0.22-0.48) correlate poorly. Core assumption: LLM judges encode sufficient cultural knowledge to evaluate SEA-specific appropriateness. Evidence anchors: [section 5.3] "The most significant challenges lie in the Fluency and Cultural Relevance metrics, as evidenced by their lower correlation." Break condition: If LLM judges lack native cultural knowledge, they may conflate surface fluency with cultural correctness, producing high scores for fluent but culturally inappropriate outputs.

## Foundational Learning

- **Concept: Delexicalization and Slot-Filling**
  - Why needed here: Core data generation technique; understanding how templates abstract entities enables debugging generation quality
  - Quick check question: Given a template "Person A recommends [DISH] for [CEREMONY]," which slot constraints would prevent invalid combinations?

- **Concept: LLM-as-Judge Evaluation**
  - Why needed here: Automatic evaluation is proposed as scalable alternative to human annotation; understanding its limitations prevents over-reliance
  - Quick check question: If G-Eval scores 3/3 on cultural relevance but human annotators score 1/3, what hypotheses explain the divergence?

- **Concept: Low-Resource Language Challenges**
  - Why needed here: Paper targets SEA languages with limited training data; understanding resource asymmetry informs model selection
  - Quick check question: Why might Aya-8B-Expanse perform well on Indonesian but poorly on Tamil despite both being in the dataset?

## Architecture Onboarding

- **Component map:** Template Generation -> Lexicalization Engine -> Dialogue Generation -> Evaluation Layer
- **Critical path:** Template quality → Entity curation accuracy → Lexicalization correctness → Prompt construction → Model selection → Human validation. Errors propagate; bad entities cannot be fixed downstream.
- **Design tradeoffs:** Synthetic vs. human-authored: Scalability vs. authenticity; Proprietary vs. open-weight models: Quality vs. reproducibility (Gemini/GPT-4o outperform Llama/Aya); Single vs. multi-topic: Complexity vs. naturalness
- **Failure signatures:** Hallucinated entities not in curated lists; Abrupt topic transitions despite `[TRANSITION]` marker; Persona drift across turns; Code-switching inappropriate to language context
- **First 3 experiments:**
  1. **Ablate lexicalization:** Generate dialogues with delexicalized placeholders intact; measure drop in human-rated cultural relevance
  2. **Judge calibration:** Compare G-Eval vs. human scores across all 8 languages; identify systematically over/under-scored metrics
  3. **Model substitution:** Replace Gemini 1.5 Flash with Aya-8B-Expanse for low-resource languages (Tamil, Minangkabau); quantify quality gap

## Open Questions the Paper Calls Out

### Open Question 1
How can LLM-based evaluation metrics be refined to achieve higher correlation with human judgments specifically for **fluency** and **cultural relevance** in Southeast Asian languages? Basis: [explicit] The abstract and Section 5.3 state that while G-Eval correlates well overall, "further improvement is needed" and "the most significant challenges lie in the Fluency and Cultural Relevance metrics," which showed lower correlation coefficients. Why unresolved: Current automatic evaluation methods fail to align with human annotators on these specific nuances, likely due to models' limited internal representation of these low-resource cultural contexts. What evidence would resolve it: A new evaluation framework demonstrating statistically significant increase in Pearson/Spearman correlation with human scores for fluency and cultural relevance.

### Open Question 2
Can fine-tuning open-weight LLMs on the SEADialogues dataset bridge the performance gap with proprietary models in generating culturally grounded, persona-consistent dialogue? Basis: [inferred] The conclusion highlights a "pressing need" for culturally enriched datasets "to support the development of open-weight LLMs" and help "bridge the quality gap," noting proprietary models currently outperform open-weight models. Why unresolved: Paper evaluates models in zero-shot setting but does not present results on whether training on the dataset improves open-weight models' ability to handle cultural nuances or persona consistency. What evidence would resolve it: Comparative study showing open-weight models fine-tuned on SEADialogues achieve scores comparable to proprietary models on human evaluation metrics.

### Open Question 3
How do current dialogue models perform on targeted benchmarks such as **topic transition detection** and **persona detection** within the context of culturally grounded Southeast Asian dialogues? Basis: [explicit] The Limitations section states that current work "does not yet include targeted evaluations on specific benchmarks such as topic transition detection... or persona detection," noting these are particularly relevant for capturing conversational nuances. Why unresolved: Paper focuses on general conversational quality but lacks specific metrics for structural dialogue tasks like identifying where the topic shifts or verifying if a persona is maintained. What evidence would resolve it: Benchmarking results using standard metrics for topic transition and persona detection applied specifically to the SEADialogues test set.

## Limitations

- Cultural grounding mechanism relies heavily on manual entity curation and coupled-entity dictionaries, which may not capture emergent cultural patterns or dialectal variations within languages
- The evaluation framework shows systematic weaknesses: G-Eval demonstrates moderate correlation with human judgments on coherence and naturalness but fails on culturally nuanced dimensions (fluency and cultural relevance)
- Proprietary model dependency (Gemini 1.5 Flash, GPT-4o mini) creates reproducibility barriers, as open-weight alternatives consistently underperform

## Confidence

**High Confidence:**
- Dataset successfully creates 32,000 multilingual dialogues across eight SEA languages with persona attributes
- Proprietary models outperform open-weight models in all human evaluation metrics
- Lexicalization with coupled-entity constraints reduces hallucinated or incompatible cultural references
- TOP2VEC clustering ensures thematically coherent multi-topic dialogue generation
- Dataset addresses the gap in culturally grounded dialogue resources for SEA languages

**Medium Confidence:**
- G-Eval shows acceptable correlation with human judgments for coherence and naturalness metrics
- Multi-topic, persona-conditioned structure improves dialogue engagement compared to single-topic generation
- Cultural grounding via slot-filling produces more contextually appropriate dialogues than translation-based approaches

**Low Confidence:**
- Dataset captures sufficient cultural nuance to support downstream task development
- Current automatic evaluation methods (G-Eval, M-Prometheus, R3) can reliably assess cultural relevance in SEA dialogues

## Next Checks

1. **Cross-cultural validation study:** Recruit native speakers from all eight target languages to independently evaluate a stratified sample of dialogues for cultural appropriateness, pragmatic naturalness, and potential cultural stereotyping. Compare these judgments with original human annotations to assess annotation reliability and cultural coverage.

2. **Automatic evaluation calibration:** Conduct systematic study comparing G-Eval, M-Prometheus, and R3 scores against human judgments across all languages and metrics. Identify which metrics and languages show systematic over/under-scoring, and develop language-specific calibration functions to improve automatic evaluation reliability.

3. **Open-weight model adaptation experiment:** Fine-tune leading open-weight models (Llama-3.1-8B, Aya-8B-Expanse) on SEADialogues using parameter-efficient methods. Quantify quality improvements and determine whether fine-tuning closes the performance gap with proprietary models, establishing a reproducible baseline for future research.