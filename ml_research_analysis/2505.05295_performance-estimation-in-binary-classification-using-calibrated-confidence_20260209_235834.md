---
ver: rpa2
title: Performance Estimation in Binary Classification Using Calibrated Confidence
arxiv_id: '2505.05295'
source_url: https://arxiv.org/abs/2505.05295
tags:
- distribution
- confidence
- metrics
- shift
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CBPE (Confidence-based Performance Estimation),
  a novel method for estimating binary classification metrics without access to ground
  truth labels. CBPE treats confusion matrix elements as random variables and leverages
  calibrated confidence scores to estimate their distributions, enabling estimation
  of any binary classification metric.
---

# Performance Estimation in Binary Classification Using Calibrated Confidence

## Quick Facts
- arXiv ID: 2505.05295
- Source URL: https://arxiv.org/abs/2505.05295
- Reference count: 40
- Primary result: CBPE achieves mean absolute errors typically below 1% for accuracy and precision, and below 3% for recall and F1, even under distribution shift

## Executive Summary
This paper introduces CBPE (Confidence-based Performance Estimation), a method for estimating binary classification metrics without ground truth labels. CBPE leverages calibrated confidence scores to estimate confusion matrix elements as random variables, enabling computation of any binary classification metric with full uncertainty quantification. The method treats true positives and true negatives as Poisson binomial distributions, from which other confusion matrix elements are derived. CBPE provides both point estimates and complete probability distributions for metrics like accuracy, precision, recall, and F1, allowing valid confidence intervals without asymptotic approximations.

## Method Summary
CBPE estimates confusion matrix elements by treating them as Poisson binomial random variables using confidence scores as parameters. For each positive prediction with confidence score s, the probability of being a true positive is s itself (under calibration). The method provides full probability distributions for metrics by propagating confusion matrix uncertainty through metric formulas. Computational shortcuts approximate expectations for recall and F1 with O(1/√n) convergence rate when variance scales with n. The approach requires calibrated confidence scores and confidence-consistent classifiers, with theoretical guarantees showing unbiased and consistent estimates under these conditions.

## Key Results
- CBPE achieves mean absolute errors below 1% for accuracy and precision on the TableShift benchmark with 8 datasets
- Under controlled covariate shift, estimation errors remain small relative to performance changes across different feature dimensions
- Strong correlation between calibration error and estimation error (ρ=0.960 for accuracy, ρ=0.983 for precision) validates the calibration dependency

## Why This Works (Mechanism)

### Mechanism 1
CBPE estimates confusion matrix elements by treating them as Poisson binomial random variables with confidence scores as parameters. For each positive prediction with confidence score s, the probability of being a true positive is s itself (under calibration). Summing n predictions yields a Poisson binomial distribution for TP count, enabling both point estimates and full uncertainty quantification. Core assumption: Model confidence scores are calibrated (P(Y=1|S=s) = s for all s). Break condition: Uncalibrated models produce biased estimates.

### Mechanism 2
Full probability distributions for metrics enable valid confidence intervals without asymptotic approximations. Rather than point estimates, CBPE propagates confusion matrix uncertainty through metric formulas to derive complete distributions, from which Highest Density Intervals are computed exactly in O(n log n). Core assumption: Independence between confusion matrix elements holds for confidence-consistent classifiers. Break condition: Small monitoring windows (<500 samples) may require full distributions rather than shortcut approximations.

### Mechanism 3
Computational shortcuts approximate expectations for recall and F1 with O(1/√n) convergence rate. Lemma 2 enables E[X/(X+Y+an)] ≈ E[X]/(E[X]+E[Y]+an) when variance scales with n, avoiding O(n²) joint distribution computation. Core assumption: Var[X]→∞ and Var[Y]→∞ as n→∞; class balance bounded away from extremes (δ ≤ n₁/n ≤ 1-δ). Break condition: Extreme class imbalance or tiny windows (<100) violate assumptions.

## Foundational Learning

- **Calibration** (Definition: P(Y=1|S=s)=s): Why needed - CBPE's theoretical guarantees require calibration; without it, confidence scores cannot be interpreted as probabilities. Quick check: If a model outputs confidence 0.7 for 100 predictions, how many should be correct if calibrated?

- **Poisson Binomial Distribution**: Why needed - Each prediction has different confidence score, so standard binomial doesn't apply; need efficient FFT-based computation. Quick check: Why can't we use simple binomial when confidence scores vary per prediction?

- **Confusion Matrix Metrics**: Why needed - CBPE estimates any metric derivable from TP/FP/TN/FN; must understand which metrics apply and their tradeoffs under imbalance. Quick check: When would precision be more informative than accuracy?

## Architecture Onboarding

- Component map: Input: batch of predictions (ŷᵢ, sᵢ) → Confidence aggregation → Poisson binomial parameters for TP/FP/TN/FN → Distribution computation (FFT-based O(n log n) or shortcut formulas) → Metric derivation (accuracy/precision/recall/F1) → Output: point estimates + confidence intervals

- Critical path: Calibration quality → estimation accuracy. Paper shows ACE (calibration error) correlates strongly with MAE for accuracy/precision but less for recall/F1. Must monitor calibration drift.

- Design tradeoffs: Full distribution vs. shortcuts: Full enables CI but O(n²) for recall/F1; shortcuts O(n) but approximate. Window size: Larger windows reduce variance but delay detection; paper tested 500-1000. Threshold: Binary predictions require confidence-consistent thresholding.

- Failure signatures: Concept shift: Paper explicitly warns this breaks identifiability. Calibration drift: Estimation error grows with ACE. Extreme imbalance: Shortcuts may fail; use full distributions.

- First 3 experiments: 1) Calibration sensitivity test: Inject miscalibration (scale confidence scores by factor 0.5-1.5) and measure estimation error vs. ACE across metrics. 2) Window size convergence: Measure shortcut approximation error and CI coverage at window sizes 50, 100, 500, 1000 to validate O(1/√n) claim on your data. 3) Distribution shift robustness: Test on held-out data with controlled covariate shift to verify paper's synthetic experiment results replicate in your domain.

## Open Questions the Paper Calls Out

- How can confidence-based estimation methods be adapted to remain accurate under concept shift, given that changes in conditional probabilities render confidence scores obsolete? The authors state estimators "struggle under concept shift" because the problem is "fundamentally unidentifiable," rendering confidence scores obsolete and presenting an "important topic for future research."

- What specific calibration techniques can maintain model calibration under covariate and label shifts to minimize CBPE estimation errors? The paper notes that "calibration error typically increases with covariate shift... and label shift" and explicitly calls for "methods for remaining calibrated under these shifts."

- Can principled metrics be developed to quantify the magnitude of covariate, concept, and label shifts from finite samples? The authors note that quantifying shift is "not possible from a finite sample" and that "no principled metrics exist for even approximating the shift... shift quantification remains an open research problem."

## Limitations

- CBPE's performance critically depends on model calibration, with strong correlation between calibration error and estimation error, but the relationship is weaker for recall and F1 metrics.
- Recall and F1 estimation requires O(n²) operations without shortcuts, making real-time monitoring challenging for high-frequency applications.
- CBPE performs well under covariate shift but cannot handle concept drift, as changes in conditional probabilities break identifiability entirely.

## Confidence

- **High Confidence**: Accuracy and precision estimation mechanisms (well-established Poisson binomial framework with direct theoretical support)
- **Medium Confidence**: Recall and F1 estimation via shortcuts (empirical validation strong but theoretical assumptions require careful verification)
- **Medium Confidence**: Distribution shift robustness (extensive TableShift evaluation but synthetic experiment provides more controlled insights)

## Next Checks

1. Calibration Drift Testing: Systematically inject calibration errors (scale confidence scores by factors 0.5-1.5) and measure how estimation error scales with ACE across all metrics, particularly focusing on recall and F1 performance degradation.

2. Window Size Sensitivity Analysis: Validate the O(1/√n) convergence rate for shortcut approximations by testing window sizes 50, 100, 500, 1000 on your specific dataset, measuring both approximation error and 95% HDI coverage rates.

3. Concept Shift Failure Mode: Design experiments where P(Y|X) changes while confidence scores remain calibrated, confirming that CBPE fails as predicted and identifying early warning signals of concept drift that calibration metrics might miss.