---
ver: rpa2
title: Toward Faithful Explanations in Acoustic Anomaly Detection
arxiv_id: '2601.12660'
source_url: https://arxiv.org/abs/2601.12660
tags:
- anomaly
- detection
- error
- interpretability
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the interpretability of autoencoder-based
  models for acoustic anomaly detection, focusing on standard autoencoders (AE) and
  masked autoencoders (MAE). While MAE shows a slightly lower detection performance
  (AUC of 0.864 vs.
---

# Toward Faithful Explanations in Acoustic Anomaly Detection

## Quick Facts
- arXiv ID: 2601.12660
- Source URL: https://arxiv.org/abs/2601.12660
- Reference count: 0
- Primary result: MAE-based explanations achieve F-score 0.63 vs AE's 0.55 while maintaining competitive detection (AUC 0.864 vs 0.885)

## Executive Summary
This work investigates the interpretability of autoencoder-based models for acoustic anomaly detection, comparing standard autoencoders (AE) with masked autoencoders (MAE). While MAE shows slightly lower detection performance (AUC 0.864 vs AE's 0.885), it consistently provides more faithful and temporally precise explanations, suggesting better alignment with true anomalies. The study applies multiple attribution methods and proposes a perturbation-based faithfulness metric that replaces highlighted regions with their reconstructions. Experimental results based on real industrial data demonstrate that MAE-based explanations, particularly error maps, achieve higher faithfulness scores and better F-scores for temporal overlap with human-annotated anomalies.

## Method Summary
The paper compares Skip-CAE-Transformer architectures with standard AE training versus MAE training (30% masking, 4×4 patches) for acoustic anomaly detection. Models are trained on 4,327 normal wood planer recordings and tested on 3,235 samples with 105 anomalies. Interpretability is evaluated using error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM via Captum. A faithfulness metric quantifies explanation quality by measuring error reduction when replacing highlighted regions with reconstructions. Results are validated against 46 expert-annotated anomalous samples.

## Key Results
- MAE error maps achieve F-score 0.63 vs AE error maps at 0.55 (98th percentile threshold)
- MAE produces more localized, structured attributions compared to AE's scattered/noisy maps
- Faithfulness scores average 0.25 for MAE vs 0.12 for AE, indicating better model-faithful explanations
- Detection AUC remains competitive: MAE 0.864 vs AE 0.885, with MAE showing less variance across runs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masked training produces more faithful explanations than standard autoencoder training
- **Mechanism:** By reconstructing only masked patches (30% mask ratio, 4×4 patches), the model learns contextual dependencies rather than identity-like reconstructions. This forces representations to capture semantic structure, yielding more meaningful error signals when anomalies disrupt expected patterns.
- **Core assumption:** Masked regions require global context to reconstruct; the model cannot rely on local copying.
- **Evidence anchors:** Abstract states "MAE consistently produces more faithful and temporally precise explanations"; Section 3.2.2 explains partial reconstruction forces contextual reliance.
- **Break condition:** If mask ratio ≥75% or patches too large (16×16), the model lacks sufficient context, degrading both detection and explanation quality.

### Mechanism 2
- **Claim:** Reconstruction error maps provide direct, gradient-free attribution signals
- **Mechanism:** Pixel-wise MSE between input spectrogram X and reconstruction X̂ produces error maps where high values indicate regions the model failed to reconstruct—typically anomalous because the model trained only on normal data.
- **Core assumption:** Anomalies produce higher reconstruction error than normal regions; the model generalizes well to normal patterns.
- **Evidence anchors:** Abstract mentions error maps as one of several attribution methods; Section 4.1.3 shows MAE error maps produce more localized attributions.
- **Break condition:** If the autoencoder over-generalizes and reconstructs anomalies well, error maps will show false negatives.

### Mechanism 3
- **Claim:** Segment-based perturbation quantifies faithfulness by measuring error reduction after replacement
- **Mechanism:** Replace detected anomalous segments in input X with their reconstructions X̂ to create modified input X₂. If highlighted regions truly drive the anomaly score, error should drop substantially. Faithfulness Score FF = max(1 − Error(X̂₂, X₂)/Error(X̂₁, X₁), 0) captures this ratio.
- **Core assumption:** Reconstructed regions approximate "normal" input; replacing them should reduce error if the explanation is correct.
- **Evidence anchors:** Abstract describes the perturbation-based faithfulness metric; Section 3.4.2 explains the metric's rationale.
- **Break condition:** If reconstruction quality is poor, even correct replacements yield noisy FF scores.

## Foundational Learning

- **Concept:** Reconstruction-based anomaly detection via autoencoders
  - **Why needed here:** The entire framework assumes anomalies are detectable as high-reconstruction-error regions. Without this, neither error maps nor faithfulness metrics make sense.
  - **Quick check question:** Can you explain why an autoencoder trained only on normal data would produce higher error on anomalous inputs?

- **Concept:** Mel spectrogram representation of audio
  - **Why needed here:** All inputs are 80×401 mel spectrograms (50ms frames, 25ms hop). Interpretability maps are 2D time-frequency attributions, not waveforms.
  - **Quick check question:** What does a horizontal streak in a spectrogram error map indicate versus a vertical streak?

- **Concept:** Threshold-based binarization for evaluation
  - **Why needed here:** F-score and faithfulness metrics require selecting high-percentile thresholds (90th–99th) to convert continuous attribution maps into binary detected regions.
  - **Quick check question:** Why would a 98th percentile threshold produce different F-scores than a 90th percentile threshold?

## Architecture Onboarding

- **Component map:** Input spectrogram → (mask if MAE training) → encoder → bottleneck → decoder → reconstruction → MSE loss
- **Critical path:** Input spectrogram → (mask if MAE training) → encoder → bottleneck → decoder → reconstruction → error map → collapse to 1D → threshold peaks → compare to annotations
- **Design tradeoffs:** Mask ratio 30% vs detection AUC: Higher masking (50–90%) degrades AUC to 0.71–0.84; Patch size 4×4 vs 16×16: Larger patches drop AUC to 0.569–0.636 at 15–30% masking; Detection performance vs interpretability: AE (0.885 AUC) outperforms MAE (0.864) slightly, but MAE error maps yield F-score 0.63 vs 0.55
- **Failure signatures:** Scattered/noisy attributions without clear temporal peaks → likely gradient-based method on AE; High faithfulness but low F-score → explanation is model-consistent but not human-aligned; Grad-CAM outputs coarse, low-fidelity maps → expected for transformer architectures
- **First 3 experiments:**
  1. Replicate AE vs MAE training on the planing mill dataset with 5 random seeds; confirm AUC gap (0.885 vs 0.864) and F-score gap on error maps at 98th percentile.
  2. Run ablation on mask ratio (15%, 30%, 50%) with 4×4 patches; measure both AUC and faithfulness score to verify tradeoff curve.
  3. Apply MAE error map to held-out anomaly types; measure F-score degradation to test generalization beyond broken/stuck/uneven boards.

## Open Questions the Paper Calls Out

- **Question:** Can MAE-based training achieve parity with standard AE detection performance while maintaining superior interpretability, or is there an inherent trade-off?
  - **Basis in paper:** The paper reports MAE achieved lower detection AUC (0.864 ± 0.048) compared to AE (0.885 ± 0.032), yet claims "minimal impact on detection performance."
  - **Why unresolved:** The ablation study only explored mask ratios (15-90%) and patch sizes (4×4, 16×16), but did not investigate architectural modifications or training strategies that might recover the detection gap.
  - **What evidence would resolve it:** Experiments combining MAE with auxiliary detection objectives, or systematic hyperparameter sweeps targeting detection AUC recovery.

- **Question:** Do the findings generalize beyond the wood planer domain to other acoustic anomaly detection settings with different noise profiles and anomaly characteristics?
  - **Basis in paper:** All experiments use a single industrial dataset (wood planer). The paper claims implications for "industrial applications" broadly without cross-domain validation.
  - **Why unresolved:** Acoustic characteristics (20 kHz sampling, mel spectrogram configuration) and anomaly types (broken/stuck/uneven boards) are domain-specific.
  - **What evidence would resolve it:** Evaluation on benchmark datasets such as DCASE Task 2 or other industrial audio datasets across different machinery types.

- **Question:** Why does Grad-CAM perform substantially worse than error maps and gradient-based methods for autoencoder interpretability in this setting?
  - **Basis in paper:** The authors note "GradCAM performs the worst overall" in their qualitative and quantitative analysis, but do not investigate the underlying cause.
  - **Why unresolved:** Grad-CAM's coarseness and reliance on final convolutional layers may be mismatched with transformer-based encoder architectures, but this is not tested.
  - **What evidence would resolve it:** Layer-wise analysis of Grad-CAM applied to different encoder stages, or comparison with transformer-specific attribution methods.

## Limitations
- Interpretability claims rely on MAE's masked reconstruction objective without ablation studies on patch size vs. masking ratio interactions
- Faithfulness metric depends on reconstruction quality - if autoencoder over-generalizes, error maps lose discriminative power
- 46 annotated samples may not represent full distribution of anomaly types, limiting generalizability

## Confidence
- **High confidence:** MAE produces better temporal localization (F-score 0.63 vs 0.55) and higher faithfulness scores (average 0.25 vs 0.12) than AE
- **Medium confidence:** The 30% masking ratio with 4×4 patches is optimal; mechanism explanation for why MAE improves interpretability
- **Low confidence:** Generalization to unseen anomaly types; whether faithfulness scores truly capture human-aligned explanations

## Next Checks
1. Perform cross-validation on the 46 annotated samples to measure variance in F-scores and faithfulness across different subsets
2. Test MAE error maps on held-out anomaly types not seen during training to assess generalization limits
3. Conduct user studies where domain experts rate explanation quality beyond temporal overlap metrics