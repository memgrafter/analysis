---
ver: rpa2
title: Investigating cybersecurity incidents using large language models in latest-generation
  wireless networks
arxiv_id: '2504.13196'
source_url: https://arxiv.org/abs/2504.13196
tags:
- data
- adversarial
- language
- signal
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of large language models (LLMs)
  to detect and explain cybersecurity incidents in next-generation wireless networks.
  The authors emulate wireless network data using the DeepMIMO tool and perform adversarial
  data poisoning attacks on regression models.
---

# Investigating cybersecurity incidents using large language models in latest-generation wireless networks

## Quick Facts
- arXiv ID: 2504.13196
- Source URL: https://arxiv.org/abs/2504.13196
- Authors: Leonid Legashev; Arthur Zhigalov
- Reference count: 28
- Primary result: Gemma-7b LLM achieves Precision = 0.89, Recall = 0.89, F1-Score = 0.89 for detecting adversarial attacks in wireless network data

## Executive Summary
This paper investigates the use of large language models (LLMs) for detecting and explaining cybersecurity incidents in next-generation wireless networks. The authors emulate wireless network data using the DeepMIMO tool and perform adversarial data poisoning attacks on regression models. Six LLMs are fine-tuned to classify poisoned versus clean data, with Gemma-7b showing the best performance. The study demonstrates that LLMs can not only detect adversarial attacks but also provide interpretable explanations for their decisions, identifying key features and recommending mitigation strategies.

## Method Summary
The methodology involves generating wireless network data using the DeepMIMO emulator with the "Boston5G_28" scenario, extracting 12 signal features including coordinates, pathloss, angles, and power. FGSM adversarial attacks are applied to regression models predicting pathloss, creating poisoned datasets with ε=1-10 and 99% fraction of samples attacked. The tabular data is transformed into descriptive text prompts following an instruction-input-output triplet format. Six LLMs (Llama-3.1-8B, Qwen2.5-3B, Gemma-7b, Mistral-7b, DeepSeek-8B) are fine-tuned using the Unsloth library with supervised fine-tuning, AdamW-8bit optimizer, and 200 iterations. The models are evaluated on binary classification (Benign vs Malicious) and prompted for explanations including feature importance analysis and mitigation recommendations.

## Key Results
- Gemma-7b achieved the best classification performance: Precision = 0.89, Recall = 0.89, F1-Score = 0.89
- All six LLMs achieved F1-scores between 0.80-0.89 after fine-tuning
- LLM explanations aligned with SHAP analysis, identifying distance, power, and line-of-sight as important features
- Gemma-7b could identify inconsistencies in compromised data and provide mitigation recommendations
- LLMs demonstrated explainability capabilities but were less accurate than ensemble classifiers like LightGBM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can detect adversarial network attacks by classifying numerical signal features transformed into structured text descriptions.
- Mechanism: Tabular wireless signal features (coordinates, pathloss, angles, power) are converted to descriptive text prompts. The LLM learns statistical patterns distinguishing FGSM-perturbed data from benign samples through supervised fine-tuning, which updates model parameters to minimize classification loss on the binary task (Benign/Malicious).
- Core assumption: The text transformation preserves detectable statistical differences between poisoned and benign samples that the LLM can learn to recognize.
- Evidence anchors:
  - [abstract]: "Six large language models were compared for detecting adversarial attacks... The Gemma-7b model showed the best results according to the metrics Precision = 0.89, Recall = 0.89 and F1-Score = 0.89."
  - [section]: Figure 2 demonstrates data transformation where a row with features like "PosX: -39.0, PosY: 128.0... Line of Sight: 1" becomes descriptive text input to the model.
  - [corpus]: Corpus lacks direct comparison of text-based vs traditional classification for adversarial wireless attacks; related work focuses on RF signal classifiers and GAN-based defenses.
- Break condition: If adversarial perturbations are too small (ε → 0) or if text transformation destroys gradient-dependent patterns, classification accuracy may approach random baseline.

### Mechanism 2
- Claim: Fine-tuning lightweight LLMs (≤8B parameters) on domain-specific wireless data enables practical deployment without requiring frontier-scale models.
- Mechanism: The fine-tuning process updates initial model parameters ψ to ψ' = ψ + Δψ using supervised learning on instruction-input-output triplets. The Unsloth library with AdamW-8bit optimizer performs parameter updates over 200 iterations, adapting pre-trained language understanding to the wireless security domain.
- Core assumption: Pre-trained LLMs possess transferable pattern recognition capabilities that survive domain adaptation to numerical wireless signal data.
- Evidence anchors:
  - [abstract]: "Fine-tuning of large language models was performed on the prepared data of the emulated wireless network segment."
  - [section]: Table 2 compares six models (Llama-3.1-8B, Qwen2.5-3B, Gemma-7b, Mistral-7b, DeepSeek-8B), all achieving F1-scores between 0.80-0.89 after fine-tuning.
  - [corpus]: Related paper "False Alarms, Real Damage" demonstrates LLMs can be adversarial attackers on CTI systems, suggesting LLMs have relevant pattern manipulation capabilities for security domains.
- Break condition: If the training distribution (emulated data) diverges significantly from real-world network conditions, fine-tuned models may fail to generalize.

### Mechanism 3
- Claim: Prompt-engineered explanations from fine-tuned LLMs can partially overcome the "blackbox" transparency problem in ML-based threat detection.
- Mechanism: After classification, separate prompts elicit chain-of-thought reasoning, feature importance analysis, and comparative analysis. The LLM generates textual explanations by retrieving learned associations between input features and classification decisions.
- Core assumption: The model's learned representations encode meaningful feature-attack relationships that can be verbalized, not just memorized correlations.
- Evidence anchors:
  - [abstract]: "Based on various explainability prompts, the Gemma-7b model notes inconsistencies in the compromised data under study, performs feature importance analysis and provides various recommendations."
  - [section]: Table 3 shows model output: "The distance between the user and base station is significantly higher than the average in the benign data set" and identifies distance, power, and line-of-sight as important features—aligning with SHAP analysis in Figure 1.
  - [corpus]: Weak corpus evidence; no direct comparison of LLM explanation quality against other explainability methods (LIME, SHAP) in security contexts.
- Break condition: Explanations may be plausible but hallucinated—model could generate reasonable-sounding justifications unrelated to actual decision factors.

## Foundational Learning

- Concept: **Fast Gradient Sign Method (FGSM)**
  - Why needed here: This is the attack methodology being detected. Understanding how FGSM perturbs data (x' = x + ε·sign(∇xJ(x,y,θ))) explains what patterns the LLM learns to identify.
  - Quick check question: If ε increases from 1 to 10, would you expect detection to become easier or harder? Why?

- Concept: **MIMO (Multiple Input Multiple Output) Systems**
  - Why needed here: The target domain. Features like DoA (Direction of Arrival), DoD (Direction of Departure), and pathloss are specific to MIMO wireless propagation and determine what constitutes anomalous behavior.
  - Quick check question: Why might "Line of Sight" status be particularly important for detecting signal manipulation?

- Concept: **Instruction Fine-Tuning Format**
  - Why needed here: The data preparation approach. Understanding the [instruction, input, output] triplet format is essential for reproducing or extending this work to other network security tasks.
  - Quick check question: What information should the instruction contain versus the input for optimal LLM performance?

## Architecture Onboarding

- Component map:
DeepMIMO Emulator → Raw Signal Data (12 features) → FGSM Attack Module → Poisoned Dataset → Text Transformer → Instruction-Input-Output Triplets → Unsloth Fine-Tuning Pipeline → Trained LLM Classifier → Prompt Engine → Classification + Explanations

- Critical path:
  1. Data generation and poisoning (determines attack detectability)
  2. Text transformation quality (affects LLM pattern recognition)
  3. Fine-tuning hyperparameters (200 iterations, max_seq_length=2048, AdamW-8bit)
  4. Prompt engineering for both classification and explainability

- Design tradeoffs:
  - **Accuracy vs. Explainability**: Authors note ensemble classifiers (LightGBM) achieve higher accuracy but lack transparency; LLMs trade ~5-10% accuracy for explainable outputs.
  - **Model Size vs. Inference Speed**: Lightweight models (<8B) selected for practicality; larger models might improve accuracy but increase latency (already noted as constraint limiting test set to 500 samples).
  - **Training Data Ratio**: 1:1 benign-to-malicious ratio may not reflect real-world class imbalance.

- Failure signatures:
  - High variance between models in Table 2 (F1: 0.80-0.89) suggests sensitivity to architecture choice
  - Model explanations referencing "average values" suggest potential memorization of training statistics rather than generalization
  - Small test set (n=500) limits statistical confidence in reported metrics

- First 3 experiments:
  1. **Baseline comparison**: Implement LightGBM classifier on same data to quantify accuracy-explainability tradeoff precisely.
  2. **Perturbation sensitivity**: Vary ε systematically (0.5, 1, 5, 10) to establish detection thresholds and correlate with explanation quality.
  3. **Cross-validation**: Use k-fold validation instead of single train-test split to establish confidence intervals on reported metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fine-tuned LLMs be optimized or integrated with ensemble classifiers to overcome their inferiority in resource intensity and accuracy for real-time network infrastructure?
- Basis: [explicit] The conclusion states that while LLMs offer explainability, they "are inferior to ensemble classifiers in terms of accuracy and resource intensity."
- Why unresolved: The study demonstrates the potential of standalone LLMs but does not propose or test an architecture that combines the speed of ensemble classifiers with the reasoning capabilities of LLMs.
- What evidence would resolve it: A hybrid system evaluation showing that an integrated approach maintains high detection speeds while providing the specific explainability features offered by the LLM.

### Open Question 2
- Question: How robust are LLM-based detectors against diverse adversarial attack vectors beyond the Fast Gradient Sign Method (FGSM)?
- Basis: [inferred] The methodology relied exclusively on FGSM to generate poisoning attacks, leaving the model's resilience against evasion attacks or other poisoning techniques untested.
- Why unresolved: FGSM is a basic gradient-based method; relying solely on it does not guarantee the model can detect more sophisticated or stealthy adversarial manipulations common in 6G networks.
- What evidence would resolve it: Performance metrics (Precision, Recall, F1) of the fine-tuned Gemma-7b model when evaluated against datasets compromised by alternative attacks such as Projected Gradient Descent (PGD) or CW attacks.

### Open Question 3
- Question: Can LLM-based cybersecurity incident investigation scale to handle high-throughput network traffic without prohibitive latency?
- Basis: [inferred] The authors limited the test sample to 500 records specifically because "token generation can take up to several seconds," acknowledging a significant bottleneck.
- Why unresolved: A 500-record sample is insufficient to validate performance in a real-world wireless network environment where millions of packets may need analysis in real-time.
- What evidence would resolve it: Stress-testing the fine-tuned model on streaming data volumes typical of 5G/6G networks to measure throughput and time-to-detection under load.

## Limitations

- Evaluation relies on emulated network data from a single DeepMIMO scenario rather than real-world network traffic, limiting generalizability to operational environments.
- The study uses a balanced 1:1 ratio between benign and malicious samples, which may not reflect the class imbalance typical in production networks.
- Test set size of 500 samples provides limited statistical power for confidence intervals around the reported metrics.

## Confidence

**High Confidence**: The core claim that fine-tuned LLMs can achieve reasonable binary classification accuracy (F1=0.89) on adversarial attack detection in wireless network data is well-supported by the experimental results.

**Medium Confidence**: The explainability claims are moderately supported but require additional validation, as the study does not demonstrate that LLM explanations improve actual incident response outcomes or compare against alternative explainability methods.

**Low Confidence**: The claim that lightweight LLMs (≤8B parameters) are optimal for practical deployment lacks empirical support, as the study does not benchmark inference latency or resource requirements.

## Next Checks

1. **Cross-Scenario Generalization**: Evaluate the fine-tuned Gemma-7b model on multiple DeepMIMO scenarios (e.g., different urban layouts, frequency bands) to assess robustness beyond the "Boston5G_28" environment and establish confidence intervals for performance metrics.

2. **Real-World Data Validation**: Test the LLM-based detection system on captured real-world 5G network data with known adversarial attacks to verify that emulated training data generalizes to operational conditions and that explanations remain accurate and actionable.

3. **Explainability Benchmark Comparison**: Compare LLM-generated explanations against established XAI methods (SHAP, LIME, Integrated Gradients) on the same datasets to quantify whether LLM explanations provide additional value in terms of accuracy, comprehensibility, or practical utility for cybersecurity analysts.