---
ver: rpa2
title: Continually Adding New Languages to Multilingual Language Models
arxiv_id: '2509.11414'
source_url: https://arxiv.org/abs/2509.11414
tags:
- languages
- arxiv
- language
- lora
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continually adding new languages
  to multilingual language models without access to the original pretraining data,
  which is a common limitation in modern model releases. The authors propose Layer-Selective
  LoRA (LayRA), a method that applies LoRA adapters to only the initial and final
  layers of a transformer while freezing the middle layers.
---

# Continually Adding New Languages to Multilingual Language Models

## Quick Facts
- arXiv ID: 2509.11414
- Source URL: https://arxiv.org/abs/2509.11414
- Reference count: 27
- Key outcome: Layer-Selective LoRA (LayRA) achieves optimal tradeoff between retention of previously supported languages and learning of new languages when continually adding low-resource languages to multilingual models

## Executive Summary
This paper addresses the challenge of adding new languages to multilingual language models without access to original pretraining data, a common limitation when working with released models. The authors propose Layer-Selective LoRA (LayRA), which applies LoRA adapters to only the initial and final layers of a transformer while freezing the middle layers. This approach is motivated by the observation that multilingual models process sequences in three stages: encoding in source language (early layers), reasoning in English (middle layers), and decoding back to source language (final layers). The method is evaluated on adding Galician, Swahili, and Urdu to Llama 3.1 and Qwen 2.5 models, showing LayRA outperforms full continued pretraining and LoRA on all layers in terms of language retention while remaining competitive in learning new languages.

## Method Summary
The authors propose LayRA (Layer-Selective LoRA), a method for continually adding new languages to multilingual language models without access to original pretraining data. The key innovation is applying LoRA adapters only to the initial and final layers of the transformer architecture while keeping the middle layers frozen. This selective approach is motivated by observations that multilingual models process sequences in three distinct stages: source language encoding (early layers), English-based reasoning (middle layers), and source language decoding (final layers). The method is evaluated by adding three low-resource languages (Galician, Swahili, and Urdu) to Llama 3.1 and Qwen 2.5 models, comparing against full continued pretraining, LoRA on all layers, and layer-selective full training approaches. Additionally, the authors demonstrate that LayRA can be combined with model arithmetic to add instruction-following capabilities without requiring instruction-tuning data in target languages.

## Key Results
- LayRA achieves the best tradeoff between retention of previously supported languages and learning of new languages
- Outperforms LoRA and full training in language retention while remaining competitive in learning performance
- Can be combined with model arithmetic to add instruction-following capabilities without instruction-tuning data in target languages

## Why This Works (Mechanism)
The three-stage processing hypothesis suggests multilingual models encode source language in early layers, perform reasoning primarily in English through middle layers, and decode back to source language in final layers. By freezing the middle layers (where reasoning occurs), LayRA preserves the model's ability to reason across languages while adapting only the encoding and decoding capabilities for new languages. This selective adaptation reduces catastrophic forgetting of previously learned languages while efficiently learning new ones.

## Foundational Learning
- LoRA (Low-Rank Adaptation): A parameter-efficient fine-tuning method that adds low-rank matrices to existing model weights; needed because full fine-tuning is computationally expensive and may cause catastrophic forgetting; quick check: verify adapter ranks are appropriate for task complexity
- Continued pretraining: Extending model training on new data while preserving existing knowledge; needed as baseline for comparison; quick check: monitor perplexity on held-out original data
- Catastrophic forgetting: The tendency of neural networks to forget previously learned information when trained on new tasks; needed context for understanding retention challenges; quick check: compare performance on original vs. new tasks before/after adaptation
- Layer-wise analysis: Examining model behavior at different depth levels; needed to justify layer-selective approach; quick check: verify distinct processing stages exist in target models
- Multilingual tokenization: How different languages are represented in shared vocabularies; needed context for understanding language-specific challenges; quick check: examine token overlap between language pairs

## Architecture Onboarding

Component Map:
Tokenization -> Early layers (encoding) -> Middle layers (reasoning) -> Late layers (decoding) -> Output generation

Critical Path:
Input sequence → Tokenizer → Early layer LoRA adapters → Frozen middle layers → Late layer LoRA adapters → Output head

Design Tradeoffs:
- Layer selection: Early+late vs. all layers - balances retention vs. learning capacity
- Adapter ranks: Higher ranks increase learning capacity but reduce parameter efficiency
- Freeze strategy: Middle layer freezing preserves reasoning capabilities but may limit cross-lingual transfer

Failure Signatures:
- Poor retention: Indicates middle layers need adaptation or reasoning patterns differ across languages
- Inability to learn: Suggests insufficient capacity in selected layers or poor adapter initialization
- Cross-lingual confusion: May indicate overlapping representations between similar languages

First Experiments:
1. Compare retention vs. learning curves across different layer combinations (early only, late only, early+late, all layers)
2. Ablation study with systematic middle layer freezing percentages (0%, 25%, 50%, 75%, 100%)
3. Test on language pairs with varying similarity to English to validate three-stage hypothesis

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Focus on only three low-resource languages may limit generalizability to other language families or resource levels
- Arithmetic-based instruction-following capability demonstrated on only one model combination, requiring further validation
- Does not address catastrophic forgetting beyond specific retention metrics reported
- Computational efficiency gains presented but not quantified in terms of wall-clock time or GPU memory usage

## Confidence
High: Core findings that LayRA provides optimal tradeoff between retention and learning are well-supported by experimental results
Medium: Arithmetic-based instruction-following approach shown on limited examples, needs broader validation
Low: None identified

## Next Checks
1. Test LayRA on a broader set of languages including high-resource languages, different language families, and languages with varying degrees of similarity to English to assess generalizability
2. Conduct ablation studies systematically removing the layer-freezing component to quantify its specific contribution to retention versus learning performance
3. Evaluate the arithmetic-based instruction-following capability on additional model combinations and with more diverse instruction-tuning datasets to verify the approach's robustness