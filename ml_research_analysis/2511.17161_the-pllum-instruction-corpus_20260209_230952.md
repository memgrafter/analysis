---
ver: rpa2
title: The PLLuM Instruction Corpus
arxiv_id: '2511.17161'
source_url: https://arxiv.org/abs/2511.17161
tags:
- text
- prompt
- instructions
- data
- polish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The PLLuM project developed a large instruction corpus to fine-tune
  Polish language models. The corpus combines human-authored (organic), synthetic,
  and converted instructions, totaling 84,795 samples.
---

# The PLLuM Instruction Corpus

## Quick Facts
- arXiv ID: 2511.17161
- Source URL: https://arxiv.org/abs/2511.17161
- Reference count: 40
- The PLLuM project developed a large instruction corpus to fine-tune Polish language models, achieving strong performance on Polish linguistic and cultural competency benchmarks while maintaining safety.

## Executive Summary
The PLLuM project developed PLLuMIC, a large-scale Polish instruction corpus designed to fine-tune language models for Polish linguistic and cultural competency. The corpus combines organic (human-authored), synthetic, and converted instructions totaling 84,795 samples. Through a systematic methodology involving continual pre-training, supervised fine-tuning, and alignment training, the project produced instruction-following models that achieved strong performance on Polish-specific benchmarks while maintaining safety standards.

## Method Summary
The methodology involved three sequential phases: continual pre-training on approximately 150 billion Polish tokens to establish linguistic foundations, supervised fine-tuning on PLLuMIC using specified hyperparameters (3 epochs, lr=1e-5, batch size 128, DeepSpeed ZeRO Stage 3), and alignment with ORPO on a 40,000+ preference dataset. The corpus itself was constructed from three sources: organic instructions (47.12% of corpus) curated by trained annotators, synthetic instructions generated through knowledge distillation and NLP task conversion, and converted instructions derived from annotated corpora and structured knowledge sources.

## Key Results
- Models achieved strong PLCC scores up to 69.17 and academic test performance up to 64.42
- Continual pre-training proved essential - fine-tuning without it degraded performance from 47.83 to 38.67
- Negative linguistic transfer was observed in synthetic instructions, with English-style conventions appearing in Polish outputs
- ORPO alignment improved safety (ASR reduced from 70-78% to ~1%) but increased verbosity and false refusal rates

## Why This Works (Mechanism)

### Mechanism 1: Continual Pre-training as Prerequisite for Effective Instruction Fine-tuning
Fine-tuning on language-specific instructions only improves performance when the base model has first undergone sufficient continual pre-training on the target language. Pre-training on target-language data "primes" the model's linguistic representations, creating substrate knowledge that instruction tuning can then shape. Without this priming, instruction data has no effective anchor point and may introduce noise.

### Mechanism 2: Organic Instructions Counteracting Negative Linguistic Transfer
Small sets of high-quality, human-authored instructions can override negative linguistic transfer from dominant pre-training languages. Transformer models "transfer stylistic conventions from languages best represented in the pre-training phase." Organic instructions provide counter-examples that reweight attention patterns toward idiomatic target-language usage.

### Mechanism 3: Alignment Training Improves Safety at Cost of Verbosity
Preference optimization (particularly ORPO) improves safety behaviors but systematically increases response length and over-refusal rates. Preference datasets emphasize safety and thoroughness; the loss function optimizes for these signals, causing models to generate more elaborate responses and refuse borderline queries.

## Foundational Learning

- Concept: **Instruction typology (organic / converted / synthetic)**
  - Why needed here: PLLuMIC's design requires balancing these sources; each has different quality, cost, and linguistic fidelity profiles.
  - Quick check question: Can you explain why synthetic distillation from English-dominant LLMs might harm Polish idiomatic output?

- Concept: **Continual pre-training vs. fine-tuning sequence**
  - Why needed here: The paper demonstrates that sequencing errors (fine-tuning before sufficient pre-training) degrade performance.
  - Quick check question: In what scenario would adding instruction data to a base model *reduce* its benchmark score?

- Concept: **Negative transfer in multilingual models**
  - Why needed here: Understanding why strong multilingual models produce unidiomatic outputs in underrepresented languages is critical for evaluation.
  - Quick check question: What linguistic feature of Polish emails did the paper identify as frequently violated due to English-style transfer?

## Architecture Onboarding

- Component map: Base model → Continual pre-training (~150B Polish tokens) → Annealing → SFT on PLLuMIC → ORPO alignment → Final chat model
- Critical path:
  1. Verify base model has target-language representation before SFT
  2. Prioritize organic instructions for language-specific conventions (even ~100 examples matter)
  3. Limit converted instructions (default max 1,000 per resource) to avoid repetitiveness
  4. Apply ORPO after SFT (not standalone) for optimal alignment
- Design tradeoffs:
  - More synthetic data → broader task coverage but risk of bias propagation and negative transfer
  - More converted data → efficient scaling but reduced conversational fluency
  - Strong alignment → better safety but increased verbosity and false refusals
- Failure signatures:
  - SFT-only models show high Attack Success Rate (70-78%) and low cultural competency
  - Fine-tuning without continual pre-training causes benchmark degradation
  - Aligned models produce overly long responses to simple queries
  - Email outputs use English capitalization/punctuation conventions in Polish context
- First 3 experiments:
  1. Ablation: Train identical base model with and without continual pre-training, then compare SFT performance on PLCC-style benchmark
  2. Minimal organic test: Add 50-100 high-quality language-specific instructions (e.g., email conventions) and measure stylistic adherence on held-out examples
  3. Alignment calibration: Compare ORPO vs. DPO vs. KTO on identical preference data, measuring both ASR/FRR and average response length

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the volume of continual pre-training on a target language determine the efficacy of subsequent instruction fine-tuning, and does a minimum threshold exist below which fine-tuning is detrimental? While the phenomenon is observed (Table 4 shows performance drops for base models fine-tuned without continual pre-training), the precise mechanism and the required "priming" threshold for the target language remain undefined.

### Open Question 2
How can the alignment phase be modified to prevent the reintroduction of negative linguistic transfer (e.g., Anglicisms) that were resolved during the Supervised Fine-Tuning (SFT) phase? Section 5.2 states that without additional linguistic adjustments, alignment on preference datasets "occasionally resulted in grammatical, lexical, and stylistic inconsistencies reflecting English-language rules, many of which had already been addressed during the SFT phase."

### Open Question 3
Is the superior performance of aligned models on linguistic benchmarks a result of genuine cultural competency or an artifact of increased verbosity? In Section 5.3, the authors question their own results, noting that aligned models score higher but "usually generate longer responses, which increases their chances of meeting the inclusion criteria of IFEval-style benchmarks."

## Limitations

- The full PLLuMIC corpus (77K+ instructions) is not released - only 1,278 sample instructions are publicly available
- Continual pre-training corpus composition and annealing procedure details remain undisclosed
- Synthetic instruction generation pipelines and the 40K+ preference dataset for alignment are not fully specified

## Confidence

- **High Confidence**: The core finding that continual pre-training is prerequisite for effective instruction fine-tuning (supported by Table 4 degradation evidence)
- **Medium Confidence**: The negative linguistic transfer mechanism and its counteraction through organic instructions (based on internal observations but lacking external validation)
- **Medium Confidence**: The safety-verbosity trade-off in alignment training (consistent with prior literature on preference optimization)

## Next Checks

1. **Pre-training Sequencing Test**: Train two identical base models, apply SFT only to one, and measure PLCC benchmark performance. Verify the paper's claim that skipping continual pre-training degrades results.

2. **Minimal Organic Instruction Impact**: Create 50-100 high-quality Polish email instructions following the paper's typology, fine-tune a pre-trained model, and evaluate outputs for English-style punctuation/capitalization violations.

3. **Alignment Method Comparison**: Implement ORPO, DPO, and KTO on identical preference data, measuring both ASR/FRR rates and average response length to validate the claimed safety-verbosity trade-off.