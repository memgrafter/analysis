---
ver: rpa2
title: Safe Control and Learning Using Generalized Action Governor
arxiv_id: '2211.12628'
source_url: https://arxiv.org/abs/2211.12628
tags:
- safe
- learning
- control
- action
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generalized action governor (AG) framework
  for safe control and learning in discrete-time systems with bounded uncertainties.
  The AG augments a nominal closed-loop system with the capability to enforce state
  and input constraints through online action adjustment.
---

# Safe Control and Learning Using Generalized Action Governor

## Quick Facts
- **arXiv ID:** 2211.12628
- **Source URL:** https://arxiv.org/abs/2211.12628
- **Reference count:** 40
- **Primary result:** Presents generalized action governor framework for safe control and learning in discrete-time systems with bounded uncertainties, relaxing positive invariance to returnability.

## Executive Summary
This paper introduces a generalized action governor (AG) framework that enables safe control and learning in discrete-time systems with bounded uncertainties. The AG augments a nominal closed-loop system with online action adjustment capabilities to enforce state and input constraints. Unlike previous approaches, the generalized AG theory relaxes the requirement for positive invariance of a safe set to returnability, enabling more flexible safe set design. The framework is demonstrated through numerical examples showing effective constraint enforcement and improved control performance through safe online learning.

## Method Summary
The method constructs a safe set offline and uses online optimization to adjust proposed actions. For linear systems, it uses maximum output admissible sets (MOAS) computed via iterative polytopic operations. For discrete systems, it employs a novel algorithm to compute safe and returnable sets. The AG solves a constrained optimization at each time step to find an action closest to the proposed action while satisfying instantaneous constraints and ensuring future safety. Safe online learning is achieved by integrating the AG with Q-learning and Koopman operator-based control, where the AG filters unsafe actions and the learner receives a modified reward penalizing necessary corrections.

## Key Results
- Generalized AG theory applicable to broader classes of systems by relaxing positive invariance to returnability
- Tailored AG design procedures for linear systems using MOAS and for discrete systems with finite state and action spaces
- Two safe online learning strategies: safe Q-learning and safe data-driven Koopman operator-based control
- Numerical examples demonstrate effective constraint enforcement and improved control performance

## Why This Works (Mechanism)

### Mechanism 1: Safety Supervision via Minimal Action Adjustment
The Action Governor enforces safety by solving a constrained optimization that minimally adjusts a proposed action to satisfy instantaneous constraints and ensure future safety. At each time step, the AG solves a QP to find an action closest to the proposed action subject to instant constraint satisfaction and robust next-state containment in the safe set. The optimization is initially feasible and safety is guaranteed if the system starts in the safe set projection.

### Mechanism 2: Relaxation of Invariance to Returnability
Replacing the strict requirement of "positive invariance" with "returnability" allows the AG to operate on larger, more flexible safe sets. A set is returnable if trajectories starting inside eventually return to it, rather than staying inside at all times. This allows the AG to temporarily allow states outside the strict invariant kernel as long as they are guaranteed to return, simplifying set computation.

### Mechanism 3: Safe Learning via Action Projection and Penalty Shaping
Integrating the AG with learning algorithms enables safe exploration by filtering unsafe actions and penalizing the learning agent for necessary corrections. The learning agent proposes an exploratory action, the AG corrects it, and the learner receives a modified reward penalizing the distance between proposed and corrected actions. This steers the policy toward safe behavior without explicit hard constraints in the learner.

## Foundational Learning

- **Concept: Maximum Output Admissible Set (MOAS)**
  - Why needed here: For linear systems, the AG relies on MOAS to define the safe set offline. Understanding MOAS is critical to constructing the AG for continuous state spaces.
  - Quick check question: Can you explain why the intersection of output admissible sets converges to a finite set for a stable linear system?

- **Concept: Positive Invariance vs. Returnability**
  - Why needed here: The paper's primary theoretical contribution is relaxing strict control barrier function-style invariance to returnability. Grasping this distinction is key to understanding why the proposed AG is less conservative.
  - Quick check question: If a set is positively invariant, is it automatically returnable? Why or why not?

- **Concept: Koopman Operator Theory**
  - Why needed here: The paper proposes a safe data-driven control method using Koopman linearization. You need to understand how nonlinear dynamics are "lifted" to linear dynamics to implement the learning component.
  - Quick check question: In the lifted model, why must observables typically include nonlinear functions of the state to represent nonlinear dynamics linearly?

## Architecture Onboarding

- **Component map:** Plant -> Nominal Policy (π₀) -> Learning Policy (π₁) -> Action Governor (AG) -> Safe Set Ππ₀
- **Critical path:** The offline computation of the safe set Ππ₀ is the primary bottleneck, scaling with state dimension for linear systems and discretization granularity for discrete systems.
- **Design tradeoffs:**
  - Use Linear/MOAS approach for convex constraints and efficiency; use Discrete approach for nonlinearity at cost of discretization errors and memory
  - High penalty coefficient forces safety quickly but may hinder exploration; low coefficient may allow persistent unsafe proposals
- **Failure signatures:**
  - Recursive Infeasibility: Check if initial state was in safe set projection or if disturbance bounds were underestimated
  - Learning Stagnation: Check if action space is incompatible with constraints or if reward shaping is insufficient
  - Chattering: Indicates the boundary of the safe set is too tight
- **First 3 experiments:**
  1. MOAS Validation: Implement MOAS calculation for double integrator and verify trajectories stay inside
  2. Safe vs. Unsafe Learning: Compare Q-learning with and without AG on grid world with cliff constraint
  3. Koopman Lift: Implement Koopman identification for nonlinear example and verify AG corrections satisfy constraints

## Open Questions the Paper Calls Out

- **Open Question 1:** How can safe set computation for discrete systems be scaled to high-dimensional continuous systems without suffering from curse of dimensionality?
- **Open Question 2:** Can Koopman observables be automated or adapted online to ensure linear predictor accuracy during transients?
- **Open Question 3:** Does penalty-based modified reward guarantee convergence to optimal constrained policy or introduce performance-limiting bias?
- **Open Question 4:** To what extent does relaxing invariance to returnability strictly enlarge the safe set compared to traditional Reference Governor approaches?

## Limitations
- Offline safe set computation scales poorly with system dimension (MOAS) or discretization granularity (discrete approach)
- Returnability requires careful design of nominal policy; poorly chosen policies may fail to guarantee eventual feasibility
- Theoretical guarantees for learning convergence under AG supervision remain limited
- Koopman approach requires careful observable selection; model inaccuracies can compromise safety

## Confidence
- **High confidence:** Core AG mechanism for safety supervision via constrained optimization is mathematically proven
- **Medium confidence:** Relaxation from invariance to returnability provides theoretical justification but requires careful practical verification
- **Medium confidence:** Safe learning integration shows empirical promise but theoretical foundations for learning convergence are less developed

## Next Checks
1. **MOAS Convergence Verification:** Implement MOAS computation for double integrator example and verify through simulation that trajectories starting inside the computed set satisfy constraints for extended time horizons.
2. **Learning Performance Under Safety Constraints:** Compare standard Q-learning against Safe Q-learning on a benchmark constrained control problem to quantify trade-off between safety enforcement and learning efficiency.
3. **Koopman Model Robustness:** Test online Koopman learning approach with varying levels of process noise and disturbance bounds to evaluate how model inaccuracies affect control performance and safety guarantees.