---
ver: rpa2
title: Predicting When to Trust Vision-Language Models for Spatial Reasoning
arxiv_id: '2601.11644'
source_url: https://arxiv.org/abs/2601.11644
tags:
- confidence
- spatial
- geometric
- blip-2
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision-Language Models (VLMs) excel at multimodal tasks but fail
  at spatial reasoning, achieving only 49-54% accuracy on basic directional relationships.
  For safe deployment in robotics and autonomous systems, it is critical to predict
  when to trust VLM spatial predictions.
---

# Predicting When to Trust Vision-Language Models for Spatial Reasoning

## Quick Facts
- arXiv ID: 2601.11644
- Source URL: https://arxiv.org/abs/2601.11644
- Reference count: 1
- VLMs achieve only 49-54% accuracy on basic directional relationships; our vision-based confidence estimation achieves 0.674 AUROC (34.0% improvement over text-based baselines)

## Executive Summary
Vision-Language Models excel at multimodal tasks but struggle with spatial reasoning, achieving only 49-54% accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, it is critical to predict when to trust VLM spatial predictions. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting to achieve substantial improvements in predicting VLM spatial reasoning reliability.

## Method Summary
Our framework operates in two stages: first, we query VLMs (BLIP-2 opt-2.7b and CLIP vit-large-patch14) for spatial predictions between object pairs, then we independently verify these predictions using geometric analysis of object detections from GroundingDINO-base. We extract four confidence features: geometric alignment probability (α_geo), spatial ambiguity from overlap (α_sep=1−IoU), detection quality (c̄), and VLM internal uncertainty. These features are fused using XGBoost (100 trees, lr=0.03, max_depth=3) trained on 705 samples from the VSR benchmark, achieving 0.674 AUROC on BLIP-2 and 0.583 AUROC on CLIP, with selective prediction enabling 61.9% coverage at 60% target accuracy.

## Key Results
- Vision-based confidence estimation achieves 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines)
- Selective prediction at 60% target accuracy achieves 61.9% coverage versus 27.6% baseline (2.2× improvement)
- Geometric features contribute 87.4% of model importance versus 12.7% from VLM confidence

## Why This Works (Mechanism)
VLMs lack explicit spatial reasoning capabilities and instead rely on learned correlations between text descriptions and visual patterns. Our approach overcomes this limitation by providing external geometric verification that is independent of the VLM's internal reasoning process. By detecting objects and computing their spatial relationships through geometric analysis (center distances, overlap), we create a ground-truth independent confidence signal that captures when the VLM's prediction aligns with actual visual geometry.

## Foundational Learning
- **Spatial reasoning fundamentals**: Understanding directional relationships (left, right, above, below) is essential for interpreting VLM outputs and designing geometric verification. Quick check: Verify that object center coordinates correctly determine directional relationships.
- **Object detection confidence**: GroundingDINO detection scores indicate localization reliability, which directly impacts geometric verification accuracy. Quick check: Ensure detection threshold τ=0.3 balances recall and precision appropriately.
- **Gradient boosting for confidence fusion**: XGBoost effectively combines heterogeneous confidence signals (geometric, detection, VLM) to predict prediction reliability. Quick check: Verify feature importance distribution shows geometric features dominate.
- **AUROC as confidence metric**: Area Under ROC Curve measures how well confidence scores discriminate between correct and incorrect predictions across all thresholds. Quick check: Confirm AUROC > 0.5 indicates better-than-random confidence estimation.
- **Youden's index for threshold selection**: This metric maximizes the difference between true positive and false positive rates to find optimal confidence thresholds. Quick check: Validate θ* values (0.502 for BLIP-2, 0.380 for CLIP) on validation set.

## Architecture Onboarding

**Component Map**: VLM Query -> Object Detection -> Feature Extraction -> Confidence Fusion -> Prediction Validation

**Critical Path**: The pipeline's critical path is Object Detection -> Feature Extraction -> Confidence Fusion. Object detection must succeed to compute geometric features, and all four features must be available for XGBoost to generate confidence scores. Failures at any point prevent reliable confidence estimation.

**Design Tradeoffs**: Using geometric verification provides independence from VLM reasoning but introduces dependency on object detection quality. The framework trades potential detection errors for independence from VLM limitations. Lower detection thresholds increase recall but may introduce noisy geometric features.

**Failure Signatures**: Low AUROC (~0.50) indicates the confidence model performs no better than random guessing, typically due to incorrect VLM querying or feature extraction errors. Zero-confidence for correct predictions suggests detection threshold too high or object naming mismatches between VLM and detection outputs.

**First Experiments**:
1. Verify BLIP-2 opt-2.7b and CLIP vit-large-patch14 correctly process spatial queries and return token probabilities/similarity scores
2. Confirm GroundingDINO-base detects object pairs with sufficient confidence (τ=0.3) and computes accurate bounding boxes
3. Validate geometric feature computation (α_geo, α_sep, c̄) produces values in expected ranges and captures spatial relationships

## Open Questions the Paper Calls Out
None

## Limitations
- Framework relies on GroundingDINO-base detection quality, which may struggle with occlusion, small objects, or unusual categories
- Geometric verification assumes object centers provide sufficient spatial information, potentially failing for elongated objects or precise spatial relationships
- Current implementation focuses on basic directional relations and may not generalize to complex spatial concepts like "behind," "inside," or relative distances

## Confidence

**High Confidence**: Vision-based confidence estimation outperforms text-based VLM self-assessment (AUROC improvements of 34.0% for BLIP-2 and 16.1% for CLIP); Selective prediction at 60% target accuracy achieves 61.9% coverage versus 27.6% baseline on BLIP-2; Geometric features contribute 87.4% of model importance versus 12.7% from VLM confidence.

**Medium Confidence**: Generalization across generative (BLIP-2) and classification (CLIP) architectures; Reliable scene graph construction with precision improvement from 52.1% to 78.3%.

**Low Confidence**: Framework's performance on spatial relations beyond the five tested categories; Robustness to objects with significant overlap or partial visibility; Generalization to datasets outside the VSR benchmark.

## Next Checks

1. **Dataset Generalization Test**: Evaluate the framework on an independent spatial reasoning dataset (e.g., Visual Genome scene graphs) to verify that performance gains transfer beyond the VSR benchmark. This would involve retraining the confidence estimator on VSR training data and testing on the new dataset.

2. **Geometric Feature Ablation**: Systematically remove individual geometric features (α_geo, α_sep, c̄) to quantify their individual contributions and identify which components drive the majority of performance improvements. This would involve training separate XGBoost models with one feature removed each time.

3. **Detection Threshold Sensitivity Analysis**: Vary the GroundingDINO detection threshold τ across [0.1, 0.5] to determine optimal settings for different object sizes and occlusion levels, and assess how detection quality impacts overall confidence estimation performance.