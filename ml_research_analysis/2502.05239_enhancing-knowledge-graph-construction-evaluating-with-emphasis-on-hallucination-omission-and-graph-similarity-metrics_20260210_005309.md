---
ver: rpa2
title: 'Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination,
  Omission, and Graph Similarity Metrics'
arxiv_id: '2502.05239'
source_url: https://arxiv.org/abs/2502.05239
tags:
- graph
- knowledge
- arxiv
- graphs
- construction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates knowledge graph construction using large language
  models, specifically comparing Mistral's original and fine-tuned versions through
  zero-shot and few-shot learning. The authors introduce refined evaluation metrics
  including BERTScore-based graph similarity with a 95% threshold, and improved hallucination/omission
  detection using Optimal Edit Paths.
---

# Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics

## Quick Facts
- arXiv ID: 2502.05239
- Source URL: https://arxiv.org/abs/2502.05239
- Reference count: 40
- Primary result: Fine-tuned Mistral with few-shot prompting achieves ~35% G-F1 on WebNLG while reducing hallucinations and omissions

## Executive Summary
This paper evaluates knowledge graph construction using large language models, specifically comparing Mistral's original and fine-tuned versions through zero-shot and few-shot learning. The authors introduce refined evaluation metrics including BERTScore-based graph similarity with a 95% threshold, and improved hallucination/omission detection using Optimal Edit Paths. Experiments on WebNLG and KELM-sub datasets show that fine-tuned Mistral significantly improves graph construction accuracy while reducing hallucinations and omissions. However, the fine-tuned models demonstrate reduced generalization capabilities on KELM-sub. The study highlights the importance of comprehensive evaluation metrics and suggests future research directions including synonym-aware assessments and human evaluation integration.

## Method Summary
The study fine-tunes Mistral-7B on WebNLG+2020 training data and evaluates both original and fine-tuned models using zero-shot and few-shot prompting (6-7 examples). Postprocessing converts LLM text outputs to triple lists, which are then converted to NetworkX graphs for evaluation. The authors introduce OEP-based metrics for exact hallucination and omission counts, and use BERTScore at 95% threshold for semantic graph similarity (GM-GBS). Experiments compare T-F1, G-F1, G-BS, GED, and GM-GBS across WebNLG and KELM-sub datasets.

## Key Results
- Fine-tuned Mistral with 7-shot prompting achieves ~35% G-F1 on WebNLG, significantly outperforming zero-shot approaches (2.30% G-F1)
- Fine-tuning improves in-domain performance but reduces cross-domain generalization on KELM-sub
- GM-GBS metric captures semantically equivalent graphs missed by exact matching
- OEP-based evaluation provides granular hallucination and omission counts per graph

## Why This Works (Mechanism)

### Mechanism 1: Few-Shot Prompting Activates In-Context Learning for Triple Extraction
The LLM processes input-output pairs as contextual demonstration, recognizing that the task requires mapping unstructured text entities and relations into structured (subject, relation, object) triples. Without weight updates, the model leverages pre-trained knowledge of entity recognition and relation extraction, activated through pattern matching against provided examples.

### Mechanism 2: Domain-Specific Fine-Tuning Trades Specialization for Generalization
Fine-tuning adjusts model weights to optimize for WebNLG's specific relation vocabulary, entity patterns, and graph structures. This creates specialization through weight updates that favor the training distribution. However, the same weight changes reduce the model's ability to adapt to different relation schemas and domain conventions.

### Mechanism 3: BERTScore-Based Graph Matching Captures Semantic Equivalence Beyond Lexical Match
Traditional exact matching fails when generated entities or relations use synonyms, lexical variants, or paraphrases of ground truth terms. BERTScore computes similarity between contextualized BERT embeddings of predicted and reference edges, allowing "located_in" and "situated_in" to be recognized as near-equivalent.

## Foundational Learning

**Concept: In-Context Learning via Few-Shot Prompting**
- Why needed here: The paper's experimental design centers on comparing zero-shot vs. few-shot prompting for KG construction
- Quick check question: Given the paper's finding that 7 shots outperform fewer examples, what factors would cause diminishing returns if you continued adding more shots beyond 7?

**Concept: Knowledge Graph Triple Structure and Graph Representation**
- Why needed here: All evaluation metrics operate on triples or graphs constructed from triples
- Quick check question: If a ground truth graph contains triples {(A, relates_to, B), (B, causes, C)} and the model generates {(A, relates_to, B)}, how would OEP quantify hallucinations and omissions?

**Concept: Graph Edit Distance (GED) and Optimal Edit Paths (OEP)**
- Why needed here: The paper's methodological contribution is using OEP to calculate exact hallucination and omission counts per graph
- Quick check question: The paper states GED measures "the number of edit operations needed to transform one graph into another." How does using OEP improve upon simply counting non-matching triples?

## Architecture Onboarding

**Component map:**
Input Text → Prompt Construction (ZSP/FSP examples from training data) → LLM Inference (Mistral-7B original or WebNLG-fine-tuned) → Raw Text Output → Postprocessing (rule-based triple extraction, malformed output handling) → Graph Construction (NetworkX directed graph from triple list) → Evaluation Metrics (T-F1, G-F1, G-BS, GM-GBS, GED, OEP→Hall/Omis rates)

**Critical path:**
1. Prompt construction — Zero-shot uses directive only; few-shot appends 6-7 examples of varying graph sizes
2. Postprocessing robustness — Rule-based extraction handles corrupted text; malformed outputs previously substituted empty lists (underestimating hallucinations), now handled differently
3. OEP-based evaluation — Core innovation: counts exact hallucination/omission per graph rather than binary flags

**Design tradeoffs:**
- Fine-tuning domain specificity vs. cross-domain generalization: Fine-tuning on WebNLG+2020 improves WebNLG test performance but degrades KELM-sub performance
- Exact match (G-F1) vs. semantic similarity (GM-GBS): GM-GBS catches synonyms at 95% threshold but adds BERT inference cost
- Shot count vs. context efficiency: Paper finds 7 shots optimal; more shots consume context window with potential diminishing returns

**Failure signatures:**
- High hallucination + low omission: Model over-generates plausible but unsupported triples
- High omission + low hallucination: Model is conservative, missing valid triples
- Large G-F1 gap across datasets: Domain mismatch between few-shot examples and test distribution

**First 3 experiments:**
1. Reproduce baseline: Run original Mistral-7B with zero-shot prompting on WebNLG test set; target approximately 2.30% G-F1
2. Few-shot scaling test: Compare 1-shot, 3-shot, 5-shot, and 7-shot configurations on same test set
3. Cross-domain evaluation: Take WebNLG-fine-tuned model, evaluate on KELM-sub with WebNLG examples vs. KELM-sub examples

## Open Questions the Paper Calls Out

**Open Question 1:** How can the trade-off between domain-specific fine-tuning and generalization be mitigated to prevent performance degradation on out-of-domain datasets?

**Open Question 2:** Can LLMs be reliably leveraged for data augmentation to enhance T2KG construction without propagating hallucinations?

**Open Question 3:** Is the 95% BERTScore threshold for the GM-GBS metric universally optimal for determining semantic equivalence in knowledge graphs?

**Open Question 4:** How do the proposed automated metrics correlate with human expert evaluation in assessing the relevance and accuracy of generated knowledge graphs?

## Limitations

- Fine-tuning methodology lacks critical hyperparameters (learning rate, batch size, epochs)
- Exact prompt templates beyond general descriptions are not specified
- Cross-dataset generalization experiments may artificially inflate performance differences
- BERTScore threshold of 95% appears arbitrary without sensitivity analysis

## Confidence

**High Confidence (4/5):** The core finding that few-shot prompting improves KG construction accuracy (2.30% → 18.72% G-F1) is well-supported by experimental data.

**Medium Confidence (3/5):** The claim that OEP-based hallucination/omission metrics provide more granular evaluation than binary presence/absence checks is theoretically sound but lacks direct comparison to alternatives.

**Low Confidence (2/5):** The assertion that BERTScore-based GM-GBS metric at 95% threshold optimally balances synonym acceptance and error rejection lacks systematic threshold analysis.

## Next Checks

1. **Reproduce Baseline with Exact Hyperparameters:** Run original Mistral-7B with zero-shot prompting on WebNLG test set, targeting 2.30% G-F1. Document all postprocessing steps.

2. **Few-Shot Scaling Experiment:** Systematically test 1-shot through 10-shot configurations on the same test set to identify the optimal shot count.

3. **Cross-Dataset Generalization Test:** Evaluate WebNLG-fine-tuned model on KELM-sub using both WebNLG examples and KELM-sub examples in the few-shot prompt. Compare G-F1 drops between conditions.