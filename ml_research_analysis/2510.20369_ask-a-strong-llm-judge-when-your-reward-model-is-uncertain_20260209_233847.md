---
ver: rpa2
title: Ask a Strong LLM Judge when Your Reward Model is Uncertain
arxiv_id: '2510.20369'
source_url: https://arxiv.org/abs/2510.20369
tags:
- uncertainty
- arxiv
- reward
- routing
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an uncertainty-based routing framework that
  complements a fast reward model (RM) with a strong but costly LLM judge in reinforcement
  learning with human feedback (RLHF). The approach formulates advantage estimation
  as pairwise preference classification, enabling principled uncertainty quantification
  to guide routing decisions.
---

# Ask a Strong LLM Judge when Your Reward Model is Uncertain

## Quick Facts
- arXiv ID: 2510.20369
- Source URL: https://arxiv.org/abs/2510.20369
- Authors: Zhenghao Xu; Qin Lu; Qingru Zhang; Liang Qiu; Ilgee Hong; Changlong Yu; Wenlin Yao; Yao Liu; Haoming Jiang; Lihong Li; Hyokun Yun; Tuo Zhao
- Reference count: 40
- Primary result: Uncertainty-based routing framework improves RLHF efficiency by combining fast reward models with costly LLM judges

## Executive Summary
This paper addresses the computational cost challenge in Reinforcement Learning from Human Feedback (RLHF) by introducing an uncertainty-based routing framework. The approach complements a fast reward model with a strong but expensive LLM judge, routing uncertain pairs to the LLM while using the RM for confident evaluations. By formulating advantage estimation as pairwise preference classification, the method enables principled uncertainty quantification to guide these routing decisions.

The framework achieves significant improvements over random routing at the same computational cost, demonstrating better reward benchmark accuracy and downstream alignment performance. Crucially, it delivers higher accuracy gains while making fewer LLM judge calls, effectively balancing evaluation quality with computational efficiency in online RLHF settings.

## Method Summary
The paper introduces an uncertainty-based routing framework that addresses the computational cost challenge in RLHF by strategically combining a fast reward model (RM) with a strong but expensive LLM judge. The core innovation lies in formulating advantage estimation as pairwise preference classification, which enables principled uncertainty quantification. This uncertainty measure is then used to route evaluation decisions: when the RM is uncertain about a preference pair, the pair is forwarded to the LLM judge; when the RM is confident, the pair is evaluated directly by the RM.

The routing mechanism operates by first having the RM score both responses in a preference pair, then using the pairwise preference classification formulation to quantify uncertainty about the predicted preference. Pairs with high uncertainty scores are routed to the LLM judge for final evaluation, while pairs with low uncertainty are handled entirely by the RM. This selective routing strategy maintains evaluation quality while substantially reducing the number of expensive LLM judge calls required.

## Key Results
- The uncertainty-based routing framework significantly outperforms random routing at the same computational cost
- The method achieves higher reward benchmark accuracy and better downstream alignment performance
- The approach delivers greater accuracy gains with fewer LLM judge calls, optimizing the cost-quality tradeoff

## Why This Works (Mechanism)
The framework works by quantifying uncertainty in the reward model's preference predictions using pairwise preference classification. When the RM is uncertain about which response in a pair is better, that uncertainty signals the need for the more reliable LLM judge. This selective routing ensures that expensive LLM evaluations are only used when they're most likely to provide valuable information, while the RM handles the majority of evaluations where it's already confident. The uncertainty quantification is principled because it's directly tied to the advantage estimation formulation, making the routing decisions both theoretically grounded and practically effective.

## Foundational Learning

**Pairwise Preference Classification** - Why needed: Forms the basis for uncertainty quantification in preference-based RLHF. Quick check: Can you explain how pairwise classification differs from direct reward prediction?

**Advantage Estimation** - Why needed: Provides the theoretical foundation for preference-based learning in RLHF. Quick check: Can you describe the relationship between advantage estimation and preference pairs?

**Uncertainty Quantification** - Why needed: Enables principled decision-making about when to route to the more expensive LLM judge. Quick check: Can you identify at least two methods for quantifying uncertainty in classification tasks?

**Computational Cost Tradeoffs** - Why needed: Central to understanding the efficiency gains of the routing framework. Quick check: Can you calculate the cost reduction when 80% of pairs are handled by the RM instead of the LLM judge?

## Architecture Onboarding

**Component Map**: RM -> Uncertainty Quantification -> Routing Decision -> (RM or LLM Judge) -> Preference Output

**Critical Path**: Input pair → RM scoring → Uncertainty calculation → Routing decision → Final preference determination

**Design Tradeoffs**: The framework balances between RM speed and LLM judge accuracy by making routing decisions based on quantified uncertainty. The key tradeoff is setting the uncertainty threshold - too low wastes LLM judge capacity, too high risks using the less accurate RM on difficult pairs.

**Failure Signatures**: The system may fail if the uncertainty quantification is poorly calibrated (overconfident RM sends hard pairs to RM, underconfident RM wastes LLM judge resources), or if the routing threshold isn't properly tuned for the specific task distribution.

**First Experiments**:
1. Baseline comparison: Run the framework with random routing versus uncertainty-based routing on the same dataset
2. Threshold sensitivity: Test different uncertainty thresholds to find the optimal cost-accuracy tradeoff
3. RM confidence analysis: Examine which types of preference pairs the RM is most/least confident about to understand routing patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The approach's effectiveness depends on having a reasonably accurate reward model that can still be uncertain about some pairs
- The uncertainty quantification method may not generalize well to non-text domains or very different reward structures
- The framework requires careful tuning of the routing threshold to achieve optimal performance

## Confidence

**High**: The core routing framework's effectiveness is well-supported - the uncertainty quantification method is theoretically grounded and experimental results are robust across multiple benchmarks.

**Medium**: The claim that the approach "significantly outperforms" alternatives at "the same cost" is somewhat limited by the lack of comparison against other uncertainty-aware routing methods or alternative LLM integration strategies.

**Low**: The generalizability of the approach to non-text domains or very different RLHF settings is uncertain, as experiments focus on standard text-based alignment tasks.

## Next Checks

1. Test the routing framework on non-text RLHF tasks (e.g., code generation, multimodal outputs) to assess domain generalization
2. Compare against alternative uncertainty quantification methods (e.g., ensemble-based uncertainty) to isolate the contribution of the specific approach
3. Evaluate performance under different routing thresholds to identify optimal cost-accuracy tradeoffs across task types