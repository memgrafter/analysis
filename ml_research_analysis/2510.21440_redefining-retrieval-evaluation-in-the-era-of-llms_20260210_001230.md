---
ver: rpa2
title: Redefining Retrieval Evaluation in the Era of LLMs
arxiv_id: '2510.21440'
source_url: https://arxiv.org/abs/2510.21440
tags:
- answer
- relevance
- relevant
- metrics
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the misalignment between traditional IR evaluation
  metrics and the needs of RAG systems. It introduces a utility-based annotation schema
  that captures both the positive contribution of relevant passages and the negative
  impact of distracting ones, and proposes UDCG, a metric using an LLM-oriented positional
  discount to directly optimize correlation with end-to-end answer accuracy.
---

# Redefining Retrieval Evaluation in the Era of LLMs
## Quick Facts
- arXiv ID: 2510.21440
- Source URL: https://arxiv.org/abs/2510.21440
- Reference count: 23
- Introduces UDCG metric that improves correlation with end-to-end accuracy by up to 36% compared to traditional metrics

## Executive Summary
This paper addresses the critical misalignment between traditional IR evaluation metrics and the needs of Retrieval-Augmented Generation (RAG) systems. The authors identify that conventional relevance-based metrics fail to capture the unique challenges of RAG evaluation, where irrelevant or distracting passages can actively harm answer quality. To address this, they propose a utility-based annotation schema and UDCG (Utility-based Discounted Cumulative Gain), a novel metric that incorporates both positive contributions from relevant passages and negative impacts from distracting ones, using LLM-oriented positional discounting to better align with how LLMs process retrieved information.

## Method Summary
The authors introduce a comprehensive utility-based annotation schema that categorizes passages based on their contribution to answer quality: relevant (positively contributing), distracting (negatively contributing), or irrelevant (no contribution). They develop UDCG, a metric that extends traditional DCG by incorporating utility scores and applying LLM-specific positional discounting that accounts for how different positions in the ranked list affect LLM performance. The framework was evaluated across five diverse datasets (Natural Questions, HotpotQA, MultiDoc2Dial, FreshQA, and SciFact) using six different LLM architectures. Experiments compare UDCG against traditional metrics like nDCG and ROUGE, measuring correlation with end-to-end answer accuracy through both human and LLM-based annotations.

## Key Results
- UDCG achieves up to 36% improvement in correlation with end-to-end answer accuracy compared to traditional metrics
- The utility-based annotation schema successfully captures both positive and negative contributions of passages to final answers
- LLM-oriented positional discounting significantly outperforms traditional logarithmic discounting for RAG evaluation
- Six different LLM architectures consistently show improved correlation when evaluated with UDCG

## Why This Works (Mechanism)
Traditional IR metrics assume that more relevant documents always improve downstream performance, but RAG systems can be harmed by distracting information. UDCG works by explicitly modeling both positive utility (relevant passages that help answer questions) and negative utility (distracting passages that mislead LLMs). The positional discounting is specifically tuned to LLM behavior, recognizing that earlier passages have different impacts on LLM reasoning than would be expected in traditional retrieval. This dual consideration of utility and position directly addresses the RAG-specific challenge where irrelevant but plausible-sounding passages can degrade answer quality.

## Foundational Learning
- **Utility-based annotation schema**: Categorizes passages as relevant (positive contribution), distracting (negative contribution), or irrelevant (no contribution). Needed because traditional binary relevance doesn't capture how distracting information can harm RAG performance. Quick check: Can you identify examples of distracting vs. merely irrelevant passages?
- **LLM-oriented positional discounting**: Applies position-based weighting that reflects how LLMs actually process retrieved information. Needed because traditional logarithmic discounting assumes human reader behavior, which differs from LLM processing patterns. Quick check: How does positional impact vary across different LLM architectures?
- **Correlation with end-to-end accuracy**: Measures how well retrieval metrics predict actual answer quality rather than just passage relevance. Needed because the ultimate goal is improved RAG system performance, not just better retrieval. Quick check: What correlation threshold indicates a metric is useful for RAG evaluation?

## Architecture Onboarding
Component map: Annotation Schema -> Utility Scoring -> Positional Discounting -> UDCG Calculation -> Correlation Analysis
Critical path: Utility annotation → UDCG computation → Correlation with answer accuracy
Design tradeoffs: Utility schema complexity vs. annotation cost; positional discounting parameters vs. generalization across LLMs
Failure signatures: Poor correlation when distracting passages are common; overfitting to specific LLM architectures; annotation schema ambiguity
First experiments: 1) Test UDCG on a small dataset with manual annotation verification; 2) Compare UDCG correlation across different LLM sizes; 3) Analyze positional discounting sensitivity to ranking position changes

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on LLMs for both annotation and metric computation may introduce model-specific biases
- Utility schema requires human annotation for validation, which was limited to three datasets
- Positional discounting assumptions may not hold uniformly across all LLM architectures and prompting strategies

## Confidence
- High Confidence: Experimental methodology and dataset selection are sound; observed improvements are statistically significant
- Medium Confidence: Generalizability across different domains and document types beyond tested datasets
- Medium Confidence: Assumption that utility-based scoring better captures RAG performance than traditional relevance scoring

## Next Checks
1. Test UDCG's performance across a broader range of domains and document types, including non-English languages and specialized technical domains
2. Conduct ablation studies to determine the relative contribution of utility scoring versus positional discounting to overall performance improvements
3. Evaluate the framework's robustness when applied to newer LLM architectures and different prompting strategies beyond the six models tested