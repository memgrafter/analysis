---
ver: rpa2
title: 'SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher
  Knowledge Distillation'
arxiv_id: '2507.08508'
source_url: https://arxiv.org/abs/2507.08508
tags:
- knowledge
- learning
- teacher
- distillation
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SFedKD introduces a sequential federated learning framework that
  addresses catastrophic forgetting through discrepancy-aware multi-teacher knowledge
  distillation. The method extends single-teacher decoupled knowledge distillation
  to a multi-teacher setting, assigning distinct weights to teachers' target-class
  and non-target-class knowledge based on class distributional discrepancies between
  teacher and student data.
---

# SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation

## Quick Facts
- arXiv ID: 2507.08508
- Source URL: https://arxiv.org/abs/2507.08508
- Authors: Haotian Xu; Jinrui Zhou; Xichong Zhang; Mingjun Xiao; He Sun; Yin Xu
- Reference count: 40
- One-line primary result: Sequential federated learning framework with discrepancy-aware multi-teacher knowledge distillation achieves 1.81%-6.40% accuracy improvements over state-of-the-art sequential methods

## Executive Summary
SFedKD introduces a sequential federated learning framework that addresses catastrophic forgetting through discrepancy-aware multi-teacher knowledge distillation. The method extends single-teacher decoupled knowledge distillation to a multi-teacher setting, assigning distinct weights to teachers' target-class and non-target-class knowledge based on class distributional discrepancies between teacher and student data. To prevent knowledge dilution, a complementary-based teacher selection mechanism is implemented, formulated as a maximum coverage problem and solved using a greedy strategy. Extensive experiments across five datasets demonstrate superior performance compared to state-of-the-art federated learning methods.

## Method Summary
SFedKD operates through a sequential knowledge transfer process where clients train in sequence per round, receiving models from previous clients while distilling knowledge from selected teacher clients. The framework decouples knowledge distillation into target-class (TCKD) and non-target-class (NCKD) components, with weights inversely proportional to class distributional discrepancy. A greedy algorithm selects teachers whose aggregated class distribution approaches uniformity, maximizing knowledge coverage while minimizing redundancy. The total loss combines supervised learning with weighted distillation losses, enabling the model to learn new knowledge while preserving previously acquired information across the federated sequence.

## Key Results
- Achieves 64.33% test accuracy on CIFAR-10 with ExDir(2,0.5) partitioning, outperforming FedSeq baseline by 13.72%
- Demonstrates 17.53%-29.08% accuracy improvements over parallel federated learning methods across five datasets
- Shows faster convergence and better model consistency compared to state-of-the-art sequential federated learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling knowledge into target-class and non-target-class components with discrepancy-aware weighting mitigates catastrophic forgetting while maintaining training efficacy.
- Mechanism: The framework extends single-teacher Decoupled Knowledge Distillation (DKD) to multi-teacher settings by separating TCKD (Target Class KD) and NCKD (Non-target Class KD). NCKD preserves knowledge of classes not present in current client data, directly combating forgetting. TCKD enhances learning on classes the student already has. Weights are assigned inversely: NCKD weight $g_k$ increases with distribution discrepancy (preserving distant knowledge), while TCKD weight $h_k$ increases with similarity (reinforcing relevant knowledge).
- Core assumption: Class distributional discrepancy between teacher and student clients correlates with the relevance and novelty of transferable knowledge.
- Evidence anchors:
  - [abstract] "assign distinct weights to teachers' target-class and non-target-class knowledge based on the class distributional discrepancy between teacher and student data"
  - [section 3.2, Eq. 5] Shows explicit weighting formulas: $g_k = d(D_{T_k}, D_S) / \sum_j d(D_{T_j}, D_S)$ for NCKD and inverse relationship for TCKD
  - [corpus] Limited direct corpus support; related work on multi-teacher KD (EBKD, CA-MKD) uses entropy or confidence-based weighting, not discrepancy-based decoupling
- Break condition: If class distributions are uniform across all clients (i.i.d. setting), discrepancy-based weighting provides no differentiation benefit, and mechanism degrades to uniform teacher weighting.

### Mechanism 2
- Claim: Complementary-based teacher selection maximizes knowledge coverage and reduces computational overhead by selecting teachers whose aggregated class distribution approaches uniformity.
- Mechanism: Teacher selection is formalized as a variant of the maximum coverage problem. A greedy algorithm iteratively selects the teacher client that minimizes the distance between the aggregated class distribution and uniform distribution $U$. This ensures selected teachers collectively cover all classes comprehensively while pruning redundant teachers with overlapping class expertise.
- Core assumption: Comprehensive class coverage in teacher selection translates to better knowledge transfer and reduced forgetting across heterogeneous clients.
- Evidence anchors:
  - [abstract] "complementary-based teacher selection mechanism to maximize knowledge space coverage while reducing redundancy and computational costs"
  - [section 3.3, Algorithm 1] Greedy selection: $t_{|T|+1} = \arg\min_t d(D_{agg} + D_{\pi_t}, U)$
  - [corpus] No direct corpus evidence for this specific selection strategy; corpus papers focus on split FL and hierarchical KD, not coverage-based selection
- Break condition: If all clients have similar class distributions (low heterogeneity), the selection mechanism cannot find complementary teachers, and greedy selection may not outperform random sampling.

### Mechanism 3
- Claim: Sequential knowledge transfer from previous-round teachers to current-round students preserves prior knowledge across the federated sequence.
- Mechanism: At round $r$, teachers are selected from round $r-1$ models. Each student in the sequence distills knowledge from these teachers while training on local data. The total loss $L = L_{CE} + \gamma L_{NCKD} + \beta L_{TCKD}$ balances supervised learning with knowledge preservation. Sequential propagation ensures knowledge flows across rounds without requiring data replay.
- Core assumption: Single-round-teacher transfer is sufficient to preserve multi-round historical knowledge (teachers from $r-1$ encode knowledge from all prior rounds).
- Evidence anchors:
  - [section 3.1] "the server will select a subset of teacher clients $T^{(r)} \subseteq \Pi^{(r-1)}$ from the previous round"
  - [Table 2] Ablation shows SFedKD (both weights enabled) achieves 64.33% vs. 50.61% FedSeq baseline on Exdir(2,0.5)
  - [corpus] Corpus papers (HealSplit, Reviving Stale Updates) use distillation for privacy or staleness, not specifically for catastrophic forgetting in sequential settings
- Break condition: If the number of rounds is very large and knowledge degrades through sequential transfer (teacher capacity bottleneck), historical knowledge may still be lost over time.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Neural Networks**
  - Why needed here: The core problem SFedKD addresses; understanding that sequential training overwrites prior knowledge is essential.
  - Quick check question: Can you explain why a model trained sequentially on client A then client B might lose performance on client A's data, even if it improves on B's data?

- Concept: **Knowledge Distillation (KD) and Soft Labels**
  - Why needed here: SFedKD builds on KD; understanding soft labels, temperature scaling $\tau$, and KL-divergence is prerequisite.
  - Quick check question: Why does using soft labels (probability distributions over classes) provide more information than hard labels for knowledge transfer?

- Concept: **Federated Learning Data Heterogeneity (Non-IID)**
  - Why needed here: The paper assumes heterogeneous class distributions; Dirichlet distribution partitioning is standard in FL evaluation.
  - Quick check question: How does a Dirichlet distribution with concentration parameter $\alpha = 0.5$ differ from $\alpha = 10.0$ in terms of client data heterogeneity?

## Architecture Onboarding

- Component map:
  - Server: Maintains global model; executes teacher selection (Algorithm 1) at round start; distributes models to first client in sequence
  - Client Sequence: $M$ clients train sequentially per round; each receives model from predecessor and teacher set $W^{(r)}$
  - Teacher Pool: $K$ models from round $r-1$ selected via greedy coverage algorithm
  - Distillation Module: Computes $L_{NCKD}$ (Eq. 3), $L_{TCKD}$ (Eq. 4), weights via discrepancy (Eq. 5), combines into total loss (Eq. 7)

- Critical path:
  1. Round $r$ starts → Server samples client sequence $\Pi^{(r)}$
  2. Server runs teacher selection (Algorithm 1) on $\Pi^{(r-1)}$ → Returns $T^{(r)}$ and $W^{(r)}$
  3. First client receives $w^{(r)}_0 = w^{(r-1)}$ and $W^{(r)}$
  4. Each client $m$: trains with distillation loss (Eq. 7) → passes $w^{(r)}_m$ to next client
  5. Final client in sequence outputs $w^{(r)}$ → becomes $w^{(r+1)}_0$ for next round

- Design tradeoffs:
  - **Teacher count $K$**: Higher $K$ increases knowledge coverage but raises communication/computation. Paper finds $K > 5$ shows diminishing returns (Fig. 4).
  - **Weight coefficients $\gamma, \beta$**: $\gamma$ controls forgetting mitigation (NCKD), $\beta$ controls training efficacy (TCKD). Paper uses $\gamma = 1.0, \beta = 3.0$.
  - **Discrepancy metric**: L1, L2, KL, JS all viable (Table 4). KL-divergence used as default.

- Failure signatures:
  - **Knowledge dilution**: If too many redundant teachers are selected, distillation effectiveness degrades (ablation shows $K=10$ underperforms $K=5$ with selection)
  - **Weight conflict**: If $\gamma$ and $\beta$ are misconfigured (e.g., both high), gradient conflicts between forgetting mitigation and current learning may slow convergence
  - **Selection failure in low heterogeneity**: Random sampling may match greedy selection when $\alpha$ is high (Table 3 shows smaller advantages at Exdir(2,10.0))

- First 3 experiments:
  1. **Baseline comparison on single dataset**: Implement FedSeq (no distillation) vs. SFedKD on CIFAR-10 with Exdir(2,0.5); measure test accuracy and forgetting metric across 100 rounds. Expect >6% accuracy gain per paper.
  2. **Ablation on weighting strategy**: Run SFedKD with (1) both $g_k, h_k$ enabled, (2) only $g_k$, (3) only $h_k$, (4) uniform weights. Compare accuracy to isolate contribution of discrepancy-aware weighting (reference Table 2).
  3. **Teacher selection validation**: Compare greedy selection vs. random sampling for $K \in \{3, 5, 7\}$ teachers on CIFAR-10 Exdir(2,0.5). Measure both accuracy and per-round training time (reference Table 3, Fig. 6-7).

## Open Questions the Paper Calls Out
None

## Limitations
- The core assumption that distributional discrepancy directly correlates with transfer relevance lacks empirical validation through controlled experiments
- The greedy teacher selection algorithm is novel but not compared against optimized baselines like integer programming solutions
- The sequential knowledge transfer mechanism assumes single-round teacher propagation preserves multi-round historical knowledge, which may degrade over long sequences

## Confidence

- **High confidence**: Accuracy improvements over baselines (measured results), teacher selection reducing redundancy (measured), weight coefficient effects (measured)
- **Medium confidence**: Discrepancy-aware weighting mechanism (logical but not directly validated), knowledge coverage assumption (plausible but untested), sequential propagation sufficiency (assumed but not verified)
- **Low confidence**: Exact contribution of each component without ablations (implied but not isolated)

## Next Checks

1. **Direct validation of discrepancy-weighting**: Implement SFedKD with uniform weights across all teachers and compare accuracy to discrepancy-aware version on CIFAR-10 ExDir(2,0.5). Measure if >2% accuracy difference exists.

2. **Teacher selection optimization**: Replace greedy selection with random sampling for K=5 teachers on the same dataset and measure both accuracy and per-round training time. Confirm if greedy selection provides consistent advantage.

3. **Knowledge preservation over rounds**: Track test accuracy on first-round client data across 100 rounds of sequential training. Measure if accuracy on initial data degrades over time despite NCKD.