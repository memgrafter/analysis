---
ver: rpa2
title: 'MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture
  of Big Little Experts'
arxiv_id: '2510.12357'
source_url: https://arxiv.org/abs/2510.12357
tags:
- experts
- mobile
- accuracy
- inference
- little
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MoBiLE accelerates MoE inference on consumer GPUs by reducing\
  \ the number of experts for most tokens while maintaining full experts for important\
  \ ones, achieving 1.60\xD7 to 1.72\xD7 speedup with negligible accuracy loss."
---

# MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts

## Quick Facts
- **arXiv ID:** 2510.12357
- **Source URL:** https://arxiv.org/abs/2510.12357
- **Reference count:** 16
- **Primary result:** Accelerates MoE inference on consumer GPUs by 1.60× to 1.72× with negligible accuracy loss

## Executive Summary
MoBiLE addresses the fundamental bottleneck in Mixture-of-Experts (MoE) inference: the high latency of loading experts from CPU memory via PCIe during offloading. The method introduces a "big-little" expert strategy where most tokens are processed with half the default number of experts (K/2 instead of K), while important tokens identified by a confidence threshold are processed with the full expert set. By combining reduced expert activation with training-free prefetching based on router logits, MoBiLE achieves significant speedups while maintaining model quality within 5% of baseline accuracy.

## Method Summary
MoBiLE modifies standard MoE inference by activating only K/2 experts for most tokens initially, then using a confidence threshold (γ=0.7) to identify tokens requiring fallback to the full K-expert configuration. The key innovation is using router logits from low-confidence tokens to prefetch the experts needed by the big model, overlapping memory I/O with computation. This training-free approach eliminates the need for auxiliary modules while maintaining >80% accuracy in expert prefetching. The method is evaluated on Qwen1.5 MoE and OLMoE models using GSM8K and Humaneval benchmarks on RTX 4080 hardware.

## Key Results
- Achieves 1.60× to 1.72× speedup over baseline offloading inference
- Maintains accuracy within 5% of baseline (e.g., 55.6% → 60.8% for Qwen MoE)
- Fallback ratio remains low at 11-21% depending on threshold setting
- PCIe transfer overhead reduced proportionally to activated expert count

## Why This Works (Mechanism)

### Mechanism 1
Reducing expert activation to K/2 for most tokens cuts PCIe transfer volume by ~50%, directly addressing the >80% I/O bottleneck. A confidence threshold identifies tokens needing full K-expert fallback, ensuring quality is maintained. Evidence shows accuracy recovery with minimal speedup impact (21% fallback yields 1.57× speedup). Break condition: fallback ratio >30% diminishes gains.

### Mechanism 2
Router logits from low-confidence tokens enable training-free prefetching of big model experts. The little model's routing decisions correlate sufficiently with big model requirements (80%+ accuracy claim) to enable effective prefetch without additional training. Prefetch horizon (1-3 layers ahead) overlaps I/O with computation. Break condition: significant router divergence causes cache misses.

### Mechanism 3
The combined approach achieves speedup proportional to (1 - fallback_ratio). Total latency follows T_total = T_little + (fallback_ratio × T_big), with fallback ratios of 11-21% maintaining average latency well below baseline. Evidence from speedup equations and empirical fallback ratios supports the model. Break condition: faster interconnects make compute bottleneck, reducing relative benefit.

## Foundational Learning

- **MoE Sparse Activation and Routing**: Essential for understanding what MoBiLE modifies. Quick check: In a MoE layer with 64 experts where K=8 are activated, what happens to the other 56 experts during inference, and where does MoBiLE propose storing them?

- **CPU-GPU Offloading and PCIe Bandwidth Constraints**: The entire motivation stems from PCIe transfer latency dominating inference time. Quick check: If expert loading accounts for 80% of MoE layer latency on PCIe 4.0, what is the maximum possible speedup if all transfer overhead were eliminated?

- **Speculative Decoding Patterns**: MoBiLE draws from "big-little decoder" concepts. Quick check: In standard speculative decoding, when does the large model intervene, and how does MoBiLE's threshold-based fallback differ from token acceptance/rejection?

## Architecture Onboarding

- **Component map**: Router Module -> Little Expert Path (K/2 experts) -> Confidence Evaluator (γ threshold) -> Prefetch Controller (router logits) -> Big Expert Path (K experts) -> Expert Cache (GPU HBM)

- **Critical path**: Token enters MoE layer → Router selects K/2 experts → Little model executes → Output logits evaluated against γ → If max_logit ≥ γ: accept token → If max_logit < γ: evict token, prefetch controller initiates big-model expert transfers using retained router scores, big model reprocesses token with K experts → Repeat

- **Design tradeoffs**: Threshold γ (higher = accuracy, lower = speedup), Little model expert count (K/2 recommended), Prefetch horizon (1-3 layers ahead), Memory overhead (GPU HBM constraints)

- **Failure signatures**: High fallback ratio (>30%) indicates accuracy-speedup imbalance; Prefetch cache misses cause compute stalls; OOM on GPU HBM requires expert count reduction; Accuracy collapse (>10%) suggests threshold or expert count misconfiguration

- **First 3 experiments**: 1) Baseline latency profiling to confirm >80% I/O dominance, 2) Threshold sweep (γ ∈ {0.5, 0.6, 0.7, 0.8}) to plot accuracy vs. speedup, 3) Fallback ratio monitoring to verify 10-25% range

## Open Questions the Paper Calls Out

### Open Question 1
Is the fixed confidence threshold (γ=0.7) robust across diverse domains without per-task tuning? The paper claims no tuning needed but evaluation is limited to reasoning and code tasks. Unresolved because token importance distributions vary significantly between domains. Evidence: Evaluation on multi-turn conversational benchmarks showing stability with fixed threshold.

### Open Question 2
How does MoBiLE perform on systems with lower interconnect bandwidth (PCIe 3.0)? The paper mentions prefetching 2-3 layers ahead for low-bandwidth hardware but only reports PCIe 4.0 results. Unresolved because prefetching efficiency depends on available look-ahead window. Evidence: Latency breakdowns and speedup metrics on PCIe 3.0 hardware.

### Open Question 3
Can MoBiLE be combined with weight quantization without destabilizing fallback logic? The paper doesn't analyze quantization's effect on router logits used for fallback decisions. Unresolved because quantization noise could artificially lower confidence scores, triggering unnecessary fallbacks. Evidence: Experiments measuring correlation between quantization levels and fallback ratio.

## Limitations

- **Hardware specificity**: Speedup claims assume PCIe bandwidth is the primary bottleneck, which may not hold on systems with faster interconnects or in compute-bound regimes
- **Memory management complexity**: Multi-layer prefetching across limited GPU HBM without detailed cache eviction policies could cause OOM or cache thrashing
- **Architecture generalizability**: Focus on specific Qwen1.5 and OLMoE configurations limits confidence for other MoE architectures with different K values or routing algorithms

## Confidence

- **High confidence**: Core mechanism of reducing expert activation (K/2 instead of K) and confidence-based fallback is well-validated by experimental results and accuracy-speedup tradeoff curves
- **Medium confidence**: Prefetching mechanism using router logits is plausible with 80%+ accuracy claim, but lacks detailed ablation studies on prefetch effectiveness
- **Low confidence**: Generalizability across different MoE architectures not thoroughly explored; optimal threshold and expert count may vary significantly for untested models

## Next Checks

1. **Prefetch accuracy validation**: Implement instrumentation to measure actual accuracy of router-based expert prefetching across multiple layers, comparing against random selection baseline

2. **Hardware dependency analysis**: Test MoBiLE on multiple consumer GPU configurations (RTX 3080, RTX 4090, different PCIe generations) to validate PCIe bottleneck assumption and speedup scaling

3. **Routing entropy impact study**: Systematically vary routing entropy of base MoE model and measure how fallback ratios and speedup change to validate consistency across workloads with different routing characteristics