---
ver: rpa2
title: Some things to know about achieving artificial general intelligence
arxiv_id: '2502.07828'
source_url: https://arxiv.org/abs/2502.07828
tags:
- intelligence
- problems
- general
- problem
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that current generative AI models are unlikely
  to achieve artificial general intelligence (AGI) due to what the author calls "anthropogenic
  debt" - the heavy reliance on human input for problem structuring, architecture
  design, training data, and evaluation. The author contends that these models succeed
  largely because humans solve the difficult parts of problems and leave only simple
  computations (like gradient descent) for the models.
---

# Some things to know about achieving artificial general intelligence
## Quick Facts
- arXiv ID: 2502.07828
- Source URL: https://arxiv.org/abs/2502.07828
- Reference count: 0
- Current generative AI models unlikely to achieve AGI due to "anthropogenic debt"

## Executive Summary
This paper argues that current generative AI models are fundamentally limited in their ability to achieve artificial general intelligence (AGI) due to what the author terms "anthropogenic debt" - the extensive human involvement required in problem structuring, architecture design, training data curation, and evaluation. The author contends that these models succeed not because they possess general intelligence, but because humans have already solved the difficult parts of problems, leaving only simple computations like gradient descent for the models to perform. The paper emphasizes that AGI requires innovations and discoveries that have not yet been made, and that progress toward AGI is difficult to measure because it is an ill-structured problem.

## Method Summary
The paper presents a theoretical analysis of current AI systems and their limitations for achieving AGI, rather than empirical experiments. The author examines the role of human intervention in AI development, the inadequacy of current benchmarks for measuring general intelligence, and the fundamental differences between test-specific and test-general problem-solving approaches. The analysis draws on concepts from cognitive science and artificial intelligence to argue that current models lack the autonomous problem-solving capabilities necessary for AGI.

## Key Results
- Current AI models succeed primarily due to human problem-solving, not model intelligence
- Existing benchmarks cannot distinguish between test-specific and test-general problem-solving methods
- AI risk stems from model limitations and human misunderstanding rather than from model intelligence itself

## Why This Works (Mechanism)
The paper's central argument is that current AI models operate within narrow domains because they rely heavily on human-provided problem structure, architecture choices, and evaluation criteria. The "anthropogenic debt" concept suggests that the intelligence attributed to AI systems is largely borrowed from human developers who have already solved the difficult aspects of problems before the models even begin training. This mechanism explains why models can perform well on specific tasks but fail to demonstrate the flexible, generalizable intelligence characteristic of AGI.

## Foundational Learning
- Anthropogenic debt: The accumulated human input required for AI systems to function
  * Why needed: To quantify and understand the human dependency in current AI systems
  * Quick check: Compare human hours spent on problem structuring versus model training time
- Ill-structured problems: Problems without clear, predefined solutions or approaches
  * Why needed: AGI must solve problems where the solution method is unknown
  * Quick check: Identify whether benchmark problems have predefined solution strategies
- Test-general vs test-specific problem-solving: General methods work across domains, specific methods work only on particular tests
  * Why needed: To distinguish true intelligence from pattern matching
  * Quick check: Test model performance on semantically similar but structurally different problems

## Architecture Onboarding
- Component map: Human problem structuring -> Model architecture design -> Training data curation -> Model training -> Evaluation -> Benchmark testing
- Critical path: Human problem structuring → Model architecture design → Training → Evaluation (each step heavily influences AGI potential)
- Design tradeoffs: Specialized performance vs general intelligence capability
- Failure signatures: High performance on benchmarks but poor generalization to novel problems
- First experiments:
  1. Measure human-to-model problem-solving ratio across different AI systems
  2. Test benchmark-passing models on structurally similar but semantically different problems
  3. Analyze training data curation requirements for different model architectures

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The "anthropogenic debt" concept lacks quantitative metrics for measurement
- Claims about benchmark inadequacy are asserted without empirical validation
- The argument that AGI requires undiscovered innovations is tautological
- Limited engagement with existing AI safety and risk assessment literature

## Confidence
- Core thesis about current models' limitations: Medium
- "Anthropogenic debt" framework and implications: Low
- AGI timeline predictions based on these arguments: Low

## Next Checks
1. Conduct a systematic review of existing AI benchmarks to identify specific structural features that prevent measurement of general problem-solving capabilities versus test-specific pattern matching
2. Develop operational metrics for quantifying "anthropogenic debt" by analyzing the proportion of human-provided problem structure versus model-generated solutions across different AI systems
3. Design and implement comparative experiments testing whether models that pass standard benchmarks can generalize to structurally similar but semantically different problems, measuring the transfer gap