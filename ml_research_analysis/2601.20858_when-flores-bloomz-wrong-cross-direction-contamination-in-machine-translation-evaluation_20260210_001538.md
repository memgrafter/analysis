---
ver: rpa2
title: 'When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation
  Evaluation'
arxiv_id: '2601.20858'
source_url: https://arxiv.org/abs/2601.20858
tags:
- language
- bleu
- source
- llama
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates cross-direction contamination in machine\
  \ translation, where training on one language pair leads to artificially inflated\
  \ performance on unseen translation directions due to target-side memorization.\
  \ Using Bloomz, a multilingual LLM trained on FLORES-200, and Llama as an uncontaminated\
  \ control, the study confirms Bloomz's FLORES contamination and demonstrates that\
  \ contamination is cross-directional\u2014performance in unseen translation directions\
  \ is artificially boosted due to memorization of target-language text."
---

# When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation

## Quick Facts
- arXiv ID: 2601.20858
- Source URL: https://arxiv.org/abs/2601.20858
- Reference count: 40
- Primary result: Bloomz LLM shows cross-directional contamination in FLORES-200, with unseen translation directions inflated due to target-side memorization

## Executive Summary
This study reveals that Bloomz, a multilingual LLM fine-tuned on FLORES-200, exhibits cross-directional contamination where training on one language pair leads to artificially inflated performance on unseen translation directions. The contamination stems from target-side memorization—when the model sees any translation with language Y as target, it memorizes Y-side text and can recall it even with altered source inputs. The authors validate this mechanism through systematic perturbations (back-translation, paraphrasing, entity replacement) and causal fine-tuning experiments on a clean Llama model.

## Method Summary
The authors evaluate all 15×15 language pairs in a FLORES-200 subset using Bloomz and a clean Llama control. They diagnose contamination by comparing BLEU (surface overlap) and COMET (semantic similarity) across directions, looking for high scores in unseen directions. Source perturbations (back-translation via Llama, paraphrasing, named entity replacement via Aya) probe memorization triggers. A causal experiment fine-tunes clean Llama on FLORES eng↔xxx pairs and evaluates unseen zzz→yyy directions to replicate the contamination pattern.

## Key Results
- Bloomz shows cross-directional contamination: high BLEU/COMET in unseen directions due to target-side memorization
- Back-translated and paraphrased sources still trigger Bloomz recall, with BLEU drops of 5-20 points on entity replacement
- Fine-tuning clean Llama on eng↔xxx FLORES pairs replicates contamination, increasing unseen direction BLEU by up to 16 points
- Contamination is target-side: Bloomz performs near-zero on target languages not in training (e.g., ewe, mri)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target-side memorization drives cross-direction contamination in multi-way parallel benchmarks.
- Mechanism: When a model trains on any direction with language Y as target, it memorizes Y-side text. At inference, even unseen source languages can trigger recall if the model recognizes contextual anchors. High BLEU with high COMET on unseen directions indicates contamination rather than generalization.
- Core assumption: Surface-form overlap (BLEU) coupled with semantic similarity (COMET) reflects memorization when the translation direction was not in training data.
- Evidence anchors:
  - [abstract] "artificially boosting performance in unseen translation directions due to target-side memorization"
  - [Section 3.1] "contamination manifests itself asymmetrically in terms of whether a language is at the source or target side: inflated scores are due to the memorization of the target language text"
  - [corpus] Related work on data contamination in LLMs (Magar & Schwartz 2022, Kocyigit et al. 2025) supports memorization-to-contamination pathways, though cross-direction contamination specifically is novel here.
- Break condition: If model achieves high BLEU on unseen directions but COMET is low, or if external benchmarks (e.g., PMIndia) show comparable performance, contamination is unlikely.

### Mechanism 2
- Claim: Named entities function as memorization triggers that persist across source perturbations.
- Mechanism: Back-translation and paraphrasing alter source syntax and vocabulary but often preserve named entities (dates, locations, persons). The model uses these entities as retrieval cues for memorized target passages. Replacing entities disrupts the cue-target association.
- Core assumption: Entities provide sufficient semantic anchoring for the model to locate memorized content in parameter space.
- Evidence anchors:
  - [Section 3.2] "back-translated sources with close to 0 BLEU...Bloomz still yield 20–60 BLEU, indicating moderate to high recall"
  - [Section 3.3] "replacing entities in source texts are associated with a reduced level of recall...reduction of 5-10 BLEU for single entity replacement and 10-20 BLEU for all entity replacement"
  - [corpus] Limited direct corpus evidence on entity-triggered memorization; this is a paper-specific contribution.
- Break condition: If entity replacement does not reduce BLEU, or if random string replacement produces similar drops, the entity-trigger hypothesis fails.

### Mechanism 3
- Claim: Multi-way parallel benchmarks enable contamination transfer across translation directions.
- Mechanism: FLORES provides identical semantic content in multiple languages. Training on any subset of directions exposes the model to shared target-side text. The model does not need exact source-target pairs seen during training—partial source similarity plus target memorization suffices.
- Core assumption: Benchmark exposure during fine-tuning produces sufficient memorization strength to generalize across source variations.
- Evidence anchors:
  - [Section 4] "BLEU scores for most unseen language pairs increased up to 16 points" after fine-tuning Llama on eng↔xxx pairs
  - [Figure 4] Shows BLEU gains in unseen zzz→yyy directions despite training only on xxx→yyy
  - [corpus] Prior work (Yao et al. 2024) documents cross-lingual contamination transfer, supporting the general mechanism.
- Break condition: If fine-tuning on a parallel benchmark produces no gains in unseen directions, or gains are uniform across all directions (including those with unrelated targets), the cross-direction mechanism is unsupported.

## Foundational Learning

- Concept: **Benchmark contamination**
  - Why needed here: The paper's central claim is that FLORES exposure invalidates BLEU-based evaluation. Without understanding contamination as memorization masquerading as generalization, the results appear as genuine multilingual capability.
  - Quick check question: Can you explain why high BLEU on a benchmark does not prove translation quality if the model may have seen the test data?

- Concept: **Multi-way parallel corpora**
  - Why needed here: FLORES-200's structure (same sentences in 200+ languages) is what enables cross-direction contamination. Understanding this architecture explains why contamination spreads across directions.
  - Quick check question: If a benchmark has 100 parallel sentences in 10 languages, how many unique translation directions exist, and why does this matter for contamination?

- Concept: **BLEU vs. COMET interpretation**
  - Why needed here: The paper uses BLEU for surface overlap (memorization detection) and COMET for semantic similarity. The diagnostic pattern (high BLEU + high COMET = contamination; moderate BLEU + high COMET = genuine performance) is central to their methodology.
  - Quick check question: Why would a memorized translation produce both high BLEU and high COMET, while a valid paraphrase translation might produce lower BLEU but still high COMET?

## Architecture Onboarding

- Component map:
  - FLORES-200 dev set (997 samples) -> Bloomz-7B1, Llama-3.1-8B-Instruct, Aya-expanse-8B models -> BLEU + COMET evaluation -> contamination diagnostics

- Critical path:
  1. Establish baseline contamination: Compare Bloomz vs. Llama BLEU/COMET heatmaps across all direction pairs
  2. Validate contamination diagnosis: Test Bloomz on external benchmarks (PMIndia, Mann-ki-Baat) to confirm poor genuine translation ability
  3. Probe trigger mechanisms: Apply source perturbations (back-translation, paraphrasing, entity replacement) to isolate what drives recall
  4. Causally validate cross-direction contamination: Fine-tune clean Llama on FLORES eng↔xxx, evaluate unseen zzz→yyy directions

- Design tradeoffs:
  - Using back-translated sources vs. paraphrased sources: Back-translation via Llama introduces model-specific artifacts but preserves semantic equivalence; paraphrasing is more natural but harder to control
  - Single-entity vs. all-entities replacement: Single-entity preserves sentence structure better; all-entities provides stronger contamination signal but may produce unnatural text
  - Fine-tuning intensity: Authors chose "severe memorization" (3 epochs) to mirror Bloomz behavior; lighter fine-tuning might not reveal the mechanism

- Failure signatures:
  - High BLEU (>60) in unseen directions with no external benchmark validation → likely contamination
  - BLEU drops of 5-20 points on entity replacement → contamination confirmed
  - COMET decreases while BLEU increases (fine-tuned Llama) → extreme memorization of subset of samples
  - Near-zero BLEU on target languages not in training data (ewe, mri in Bloomz) → contamination is target-side, not source-side

- First 3 experiments:
  1. **Contamination detection sweep**: Run full pairwise BLEU/COMET evaluation on your model across all FLORES directions. Flag any direction with BLEU >40 and COMET >0.8 that was not in training data.
  2. **External benchmark validation**: For any flagged directions, test on alternative benchmarks (e.g., WMT, PMIndia) to confirm genuine translation inability. Near-zero BLEU confirms contamination.
  3. **Entity replacement probe**: For confirmed contaminated directions, replace named entities in source text using Aya or similar, re-evaluate BLEU. A consistent 5-20 point drop validates the contamination diagnosis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does cross-direction contamination scale or manifest differently in models significantly larger than 8B parameters?
- Basis in paper: [explicit] The authors state in Section 6 that "Our experiments are limited to 7–8B parameter models; results may vary for different model sizes."
- Why unresolved: Memorization dynamics and capacity often change non-linearly with model scale.
- What evidence would resolve it: Replicating the FLORES fine-tuning and perturbation experiments on models in the 70B–100B+ parameter range.

### Open Question 2
- Question: What internal mechanisms trigger the recall of memorized targets when the source input is not the exact training string?
- Basis in paper: [explicit] The authors note in Section 6 that their work "did not investigate the internal mechanisms of cross-direction contamination or the activation of memorized text."
- Why unresolved: The paper empirically demonstrates that back-translated sources trigger recall (Section 3.2) but does not explain the specific attention or activation pathways responsible.
- What evidence would resolve it: A mechanistic interpretability study (e.g., probing attention heads) analyzing activation patterns when a perturbed source successfully retrieves a memorized target.

### Open Question 3
- Question: To what extent does the observed BLEU drop during named entity replacement result from grammatical incompatibility rather than disrupted memorization?
- Basis in paper: [explicit] In Section 6, the authors note, "it is possible that the new entities have different inflection compared to the original entities, changing the sentence more than intended."
- Why unresolved: Replacing entities may disrupt sentence structure (agreement, case marking), artificially lowering BLEU independent of the model's memorization status.
- What evidence would resolve it: An analysis using human evaluation or a metric robust to inflectional errors to isolate the "memorization disruption" effect from grammatical noise.

## Limitations

- The study focuses on 7-8B parameter models, leaving open whether contamination patterns scale differently for larger models
- Entity replacement methodology may introduce grammatical noise beyond pure memorization disruption
- Back-translation perturbation uses Llama, which may itself be contaminated with FLORES data
- Fine-tuning experiments limited to eng↔xxx directions, not exploring other language pairs

## Confidence

**High confidence**: The core finding that Bloomz contamination manifests as cross-directional performance gains due to target-side memorization. The diagnostic pattern (high BLEU + high COMET in unseen directions) is robust and validated through multiple experiments.

**Medium confidence**: The entity-trigger hypothesis for contamination recall. While entity replacement consistently reduces BLEU scores, the mechanism by which entities function as memorization triggers could involve factors beyond simple surface-form matching (e.g., semantic context preservation).

**Medium confidence**: The generalizability of contamination patterns across different multilingual LLMs and benchmarks. The study focuses on Bloomz and FLORES-200; similar patterns may exist in other models and benchmarks but require empirical verification.

## Next Checks

1. **Cross-benchmark contamination validation**: Test whether contamination patterns observed in FLORES-200 extend to other multilingual benchmarks (e.g., TED Talks, WikiMatrix). Run Bloomz on these benchmarks and apply the same diagnostic criteria (high BLEU + high COMET in unseen directions) to assess contamination prevalence across evaluation frameworks.

2. **Fine-tuning ablation study**: Systematically vary fine-tuning conditions (epochs, learning rate, data subset) on Llama to map the contamination strength landscape. Determine whether mild contamination (1-2 BLEU points) exists in addition to the severe memorization patterns observed, and whether contamination plateaus at certain training intensities.

3. **Entity replacement generalization**: Extend entity replacement experiments to languages not included in the original 15-language subset, particularly mid-resource and low-resource languages. Assess whether the 5-20 BLEU drop pattern holds across the full resource spectrum, and whether entity types (person, location, organization) have differential impacts on contamination recall.