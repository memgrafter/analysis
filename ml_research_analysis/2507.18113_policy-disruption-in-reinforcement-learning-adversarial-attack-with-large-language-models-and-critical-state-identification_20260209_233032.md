---
ver: rpa2
title: Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language
  Models and Critical State Identification
arxiv_id: '2507.18113'
source_url: https://arxiv.org/abs/2507.18113
tags:
- reward
- adversarial
- self
- policy
- float
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARCS, a novel adversarial attack framework
  that disrupts reinforcement learning agents by guiding their behavior toward suboptimal
  actions without modifying the environment. The core innovation is a reward iteration
  optimization method that uses large language models to generate adversarial rewards
  tailored to the specific vulnerabilities of the target policy, combined with a critical
  state identification mechanism that pinpoints decision points where suboptimal actions
  have the greatest impact.
---

# Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification

## Quick Facts
- arXiv ID: 2507.18113
- Source URL: https://arxiv.org/abs/2507.18113
- Authors: Junyong Jiang; Buwei Tian; Chenxing Xu; Songze Li; Lu Dong
- Reference count: 40
- Primary result: ARCS achieves attack success rates up to 0.91 on MuJoCo and autonomous driving tasks

## Executive Summary
This paper introduces ARCS, a novel adversarial attack framework that disrupts reinforcement learning agents by guiding their behavior toward suboptimal actions without modifying the environment. The core innovation is a reward iteration optimization method that uses large language models to generate adversarial rewards tailored to the specific vulnerabilities of the target policy, combined with a critical state identification mechanism that pinpoints decision points where suboptimal actions have the greatest impact. Experimental results across six environments, including MuJoCo and autonomous driving tasks, demonstrate that ARCS achieves attack success rates of up to 0.91, significantly outperforming existing methods.

## Method Summary
ARCS operates under black-box assumptions, where the attacker influences victim behavior through environment interactions without modifying observations or actions. The framework uses GPT-4o to iteratively generate adversarial reward functions, selecting the best performer across 4 rounds (32 API calls total). An auxiliary binary policy identifies critical states where victim suboptimal actions cause disproportionate performance degradation. The attacker policy is pre-trained with the optimized adversarial reward using PPO, then fine-tuned with a composite reward that encourages minimal observation perturbations while maximizing victim action deviation at non-critical states.

## Key Results
- ARCS achieves attack success rates of up to 0.91 on Kick-and-Defend environment
- Critical state identification improves attack efficiency with 0.88 failure rate vs 0.68 random perturbation in Sumo-Human
- ARCS outperforms existing methods including Baseline1 (sparse win reward) and Baseline2 (joint reward optimization)
- Ablation studies confirm both LLM-generated rewards and critical state fine-tuning contribute substantially to effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- LLMs can generate effective adversarial reward functions through iterative refinement with training feedback
- GPT-4o serves as both Reward Generator and Reward Evaluator, producing 4 candidate rewards per iteration, training attackers briefly, and selecting the best based on empirical win rates
- Process converges within 4 rounds (32 API calls total) using task descriptions and training statistics to exploit victim vulnerabilities

### Mechanism 2
- Critical states are identified where victim suboptimal actions cause disproportionate performance degradation
- An auxiliary binary policy πM learns to mark states as critical (0) or non-critical (1) based on return impact when victim actions are replaced with random ones
- Trust-region constraints ensure monotonic victim performance degradation while limiting perturbations to 20-40 states per episode

### Mechanism 3
- Fine-tuning with deviation-based rewards amplifies adversarial effectiveness while minimizing observable perturbations
- Two auxiliary models (transition model P̃ and victim policy estimator π̃O) calculate deviation signals used in fine-tuning reward R_ft = (-ΔO_o + ΔA_o) · (1 - πM(O_{t+1}))
- Applied only at non-critical states to avoid over-constraining while encouraging subtle environment manipulations

## Foundational Learning

- **Trust Region Policy Optimization (TRPO/PPO)**: Critical state identification uses trust-region constraints to ensure monotonic performance degradation; attacker trained with PPO. Quick check: Can you explain why constraining KL divergence between old and new policies prevents catastrophic performance collapse during optimization?

- **Black-box vs White-box Adversarial Attacks**: ARCS operates under black-box assumptions—no access to victim architecture, parameters, or environment dynamics. Quick check: What information can the attacker observe and influence in a black-box multi-agent RL setting versus a white-box setting?

- **Potential-based Reward Shaping**: LLM-generated rewards use potential functions with γ-scaled differences to ensure consistent credit assignment. Quick check: Why does using potential-based shaping (γφ(s') - φ(s)) preserve optimal policies compared to arbitrary reward additions?

## Architecture Onboarding

- **Component map**: GPT-4o (Reward Generator) -> Attacker Policy (πA) -> Critical State Selector (πM) -> Environment -> Victim Policy; GPT-4o (Reward Evaluator) provides feedback loop; Auxiliary models P̃ and π̃O support fine-tuning

- **Critical path**: 1) Initialize prompts with task description -> Generate 4 reward candidates -> Train attacker briefly with each -> Evaluate and select best -> Update Generator prompt with feedback -> Repeat 4 rounds; 2) Pre-train attacker with final adversarial reward via PPO; 3) Train P̃ and π̃O with supervised losses; 4) Fine-tune attacker with composite reward R_total = R + 0.3×R_ft, re-optimizing πM every K=10 updates

- **Design tradeoffs**: Reward candidate count vs iteration rounds (paper uses 4 candidates × 4 rounds); Critical state constraint bounds C1=40, C2=20 balance perturbation budget vs effectiveness; Fine-tuning weight λ=0.3 balances attack success vs minimal perturbation

- **Failure signatures**: Reward iteration fails to improve win rates across rounds (LLM not extracting useful patterns); Critical state perturbation achieves similar failure rates to random perturbation (auxiliary policy converged to trivial solution); Fine-tuning degrades attack success rate (λ too high or auxiliary models poorly calibrated)

- **First 3 experiments**: 1) Baseline comparison: Run AR vs Baseline1 and Baseline2 on one MuJoCo environment to validate LLM-generated reward effectiveness; 2) Ablation on critical state identification: Compare ARCS vs AR (no fine-tuning) to isolate contribution of critical state mechanism; 3) Perturbation validation: Compare victim failure rates under no perturbation, random perturbation, and critical state perturbation to verify identified states are truly high-impact

## Open Questions the Paper Calls Out
- How does ARCS perform in complex multi-agent environments involving more than two agents or non-zero-sum dynamics? The current experimental validation is limited to two-player zero-sum or competitive settings.
- Can integrating meta-learning or multi-task learning improve the efficiency of ARCS's adversarial reward generation? The current framework treats each attack as a distinct optimization loop using LLMs.
- How robust is the attack framework if the attacker is restricted from observing the victim's internal states or observations? Section 3.1 states the attacker observes both its own and the victim's observations, which is a strong assumption for a black-box setting.
- How sensitive is the Critical State Identification module to errors in the learned transition model and victim policy estimator? Section 3.3 notes that the fine-tuning reward depends on auxiliary models, but does not analyze performance degradation when these models are inaccurate.

## Limitations
- The paper assumes black-box access to victim policies but does not specify victim architecture details, training procedures, or hyperparameters
- Limited ablation on LLM reward generation—only 4 rounds with 4 candidates per round are reported, with no analysis of sensitivity to candidate count or iteration budget
- The transition model and victim policy estimator architectures are underspecified, raising questions about their approximation accuracy and generalization

## Confidence
- **High Confidence**: Experimental methodology is well-documented with clear hyperparameters; success rates are reproducible given access to environments and victim policies
- **Medium Confidence**: LLM-guided reward iteration mechanism is plausible given existing literature on LLM-based reward design, but effectiveness depends heavily on prompt engineering and API behavior
- **Low Confidence**: Critical state identification algorithm's effectiveness across diverse environments is uncertain without more ablation studies on state selection thresholds and sensitivity to constraint bounds

## Next Checks
1. Implement and test the LLM reward generation pipeline on a simplified environment (e.g., CartPole) to verify that GPT-4o can produce meaningful adversarial rewards and that the iteration process converges within 4 rounds
2. Conduct ablation studies on critical state identification by varying C1/C2 bounds and comparing attack success rates against random state perturbation baselines
3. Verify the composite reward function's stability by training the attacker with different λ values (0.1, 0.3, 0.5) and measuring trade-offs between attack success rate and victim action deviation magnitude