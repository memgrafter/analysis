---
ver: rpa2
title: 'SPARK: Synergistic Policy And Reward Co-Evolving Framework'
arxiv_id: '2509.22624'
source_url: https://arxiv.org/abs/2509.22624
tags:
- reward
- answer
- reasoning
- arxiv
- spark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SPARK is a synergistic policy and reward co-evolving framework\
  \ that recycles rollouts and correctness signals from reinforcement learning with\
  \ verifiable rewards (RLVR) to simultaneously train the model itself as a generative\
  \ reward model. By co-evolving policy and reward within a single unified model using\
  \ a mix of objectives\u2014including pointwise scoring, pairwise comparison, and\
  \ reflection-based correction\u2014SPARK eliminates the need for separate reward\
  \ models and costly human preference data."
---

# SPARK: Synergistic Policy And Reward Co-Evolving Framework

## Quick Facts
- **arXiv ID**: 2509.22624
- **Source URL**: https://arxiv.org/abs/2509.22624
- **Reference count**: 40
- **Primary result**: SPARK achieves 9.7% average gains on reasoning benchmarks, 12.1% on reward benchmarks, and 1.5% on general benchmarks over baselines

## Executive Summary
SPARK introduces a novel framework that co-evolves policy and reward models within a unified architecture, eliminating the need for separate reward models and human preference data. By recycling rollouts and correctness signals from reinforcement learning with verifiable rewards (RLVR), SPARK trains both components simultaneously using a mix of objectives including pointwise scoring, pairwise comparison, and reflection-based correction. The approach demonstrates substantial performance improvements across multiple benchmarks while reducing training costs and enabling test-time scaling through self-reflection capabilities.

## Method Summary
SPARK's core innovation lies in its unified framework that simultaneously trains a policy and reward model within a single architecture. The method recycles rollouts and correctness signals from RLVR to generate synthetic rewards, which are then used to train both the policy and a generative reward model. The training process employs a mix of objectives including pointwise scoring, pairwise comparison, and reflection-based correction to co-evolve both components. This integrated approach eliminates the need for separate reward models and costly human preference data, while enabling test-time scaling through self-reflection mechanisms.

## Key Results
- SPARK-VL-7B achieves 9.7% average gains on seven reasoning benchmarks over baselines
- 12.1% improvement on two reward benchmarks and 1.5% on eight general benchmarks
- Demonstrates substantial training cost reduction by eliminating separate reward models

## Why This Works (Mechanism)
SPARK's effectiveness stems from the synergistic co-evolution of policy and reward models, where each component benefits from the other's improvement. The unified architecture allows for more efficient learning as the reward signals are generated by the same model that is being optimized, creating a self-reinforcing cycle. The use of verifiable rewards provides a stable foundation for training, while the multiple objective functions ensure robust learning across different task types. The self-reflection capability at test time enables further performance gains without requiring additional external reward models.

## Foundational Learning

**Reinforcement Learning with Verifiable Rewards (RLVR)**: A training paradigm where rewards can be objectively determined, essential for stable policy optimization without human feedback. Quick check: Verify reward functions can be programmatically computed for target tasks.

**Reward Modeling**: The process of training models to predict human preferences or task-specific rewards. Quick check: Assess baseline reward model performance on held-out data.

**Policy Optimization**: The core RL task of improving action selection to maximize expected reward. Quick check: Monitor policy performance across training epochs.

## Architecture Onboarding

**Component Map**: Input -> Policy Network -> Action Selection -> Environment Interaction -> Verifiable Reward Generation -> Reward Network -> Policy Update -> Reflection Module -> Test-Time Inference

**Critical Path**: The core training loop involves: (1) policy generates actions, (2) environment provides verifiable rewards, (3) rewards are used to update both policy and reward model, (4) reflection mechanism provides additional correction signals.

**Design Tradeoffs**: Unified model vs. separate components - SPARK trades potential specialization for efficiency and reduced complexity. The reliance on verifiable rewards limits applicability but ensures stability.

**Failure Signatures**: Poor reward signal quality manifests as policy collapse or reward hacking. Integration issues may appear as instability in the co-evolution process or degradation in either component's performance.

**First Experiments**:
1. Verify verifiable reward generation works correctly for target task
2. Test unified model training stability with synthetic data
3. Evaluate self-reflection mechanism performance on simple verification tasks

## Open Questions the Paper Calls Out

None

## Limitations

The framework's reliance on verifiable rewards may limit its applicability to tasks where ground truth or correctness can be reliably assessed. The long-term stability and generalizability across diverse domains remain to be fully established. The test-time scaling via self-reflection may face practical limitations with very complex tasks or domains where verification is inherently difficult.

## Confidence

**Performance Claims**: High confidence in reported technical implementation and results, Medium confidence in generalizability and scalability claims, Low confidence in long-term stability without external validation.

## Next Checks

1. Independent replication of SPARK's core results using different base models and datasets to verify consistency of performance gains
2. Comparative study of reward quality and stability between SPARK's integrated approach and traditional separate reward models across extended training horizons
3. Systematic evaluation of SPARK's performance on non-verifiable tasks to assess practical limitations of the verifiable rewards approach