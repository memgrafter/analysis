---
ver: rpa2
title: Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental
  Adaptation
arxiv_id: '2507.11634'
source_url: https://arxiv.org/abs/2507.11634
tags:
- learning
- sentiment
- incremental
- persian
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates cross-lingual few-shot learning for Persian\
  \ sentiment analysis by leveraging multilingual pre-trained models and incremental\
  \ learning strategies. Three models\u2014XLM-RoBERTa, mDeBERTa, and DistilBERT\u2014\
  were fine-tuned on limited Persian data from five diverse domains."
---

# Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation

## Quick Facts
- arXiv ID: 2507.11634
- Source URL: https://arxiv.org/abs/2507.11634
- Reference count: 21
- Key outcome: mDeBERTa and XLM-RoBERTa achieved up to 96% accuracy in Persian sentiment analysis using cross-lingual transfer and incremental learning with knowledge distillation and rehearsal regularization

## Executive Summary
This study investigates cross-lingual few-shot learning for Persian sentiment analysis by leveraging multilingual pre-trained models and incremental learning strategies. Three models—XLM-RoBERTa, mDeBERTa, and DistilBERT—were fine-tuned on limited Persian data from five diverse domains. Incremental learning with regularization techniques (Elastic Weight Consolidation, knowledge distillation, and rehearsal) was employed to mitigate catastrophic forgetting. Experimental results demonstrate that mDeBERTa and XLM-RoBERTa achieved up to 96% accuracy, with knowledge distillation and rehearsal showing the strongest performance in low-shot scenarios. DistilBERT underperformed due to its smaller capacity. The findings confirm that combining cross-lingual transfer with incremental learning enables effective sentiment analysis in low-resource languages like Persian.

## Method Summary
The study fine-tuned three multilingual pre-trained models (XLM-RoBERTa, mDeBERTa, DistilBERT) on Persian sentiment datasets using incremental learning across five domains in sequence from simple to complex. Few-shot settings ranged from 1 to 20 samples per class. Regularization methods included Elastic Weight Consolidation, knowledge distillation, and rehearsal. Models were pre-trained on sentiment-specific multilingual corpora and evaluated using accuracy and F1-score metrics with early stopping to prevent overfitting.

## Key Results
- mDeBERTa and XLM-RoBERTa achieved up to 96% accuracy in Persian sentiment analysis
- Knowledge distillation and rehearsal regularization outperformed EWC, especially in low-shot settings
- DistilBERT underperformed due to insufficient model capacity for cross-lingual transfer
- Models showed performance saturation or regression when shots exceeded 15-20 samples

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Transfer via Multilingual Pre-training
Multilingual pre-trained models enable effective knowledge transfer from high-resource languages to Persian with minimal target-language data. Models pre-trained on 100+ languages develop shared semantic representations; sentiment-specific pre-training on multilingual datasets provides task-relevant priors that transfer across linguistic boundaries through masked language modeling.

### Mechanism 2: Regularized Incremental Learning for Catastrophic Forgetting Mitigation
Knowledge distillation and rehearsal preserve previously learned domain knowledge while adapting to new Persian datasets sequentially. Knowledge distillation transfers soft labels from teacher (previous model state) to student (current model), retaining decision boundaries; rehearsal stores exemplars from prior domains and interleaves them with new data during training.

### Mechanism 3: Few-shot Fine-tuning with Early Stopping
Extreme few-shot learning (1-20 samples per class) is viable when combined with sentiment-specific pre-training and overfitting prevention. Early stopping with patience=3 prevents overfitting to limited samples; pre-trained sentiment knowledge provides strong priors that few examples can refine rather than establish from scratch.

## Foundational Learning

- **Transfer Learning**
  - Why needed here: The entire approach depends on understanding how pre-trained multilingual models adapt to new languages with limited fine-tuning data
  - Quick check question: Why does fine-tuning a pre-trained model require orders of magnitude less data than training from scratch?

- **Catastrophic Forgetting**
  - Why needed here: Sequential domain introduction risks overwriting previously learned patterns; understanding this drives regularization selection
  - Quick check question: What happens to previously learned task performance when a neural network trains exclusively on new data?

- **Knowledge Distillation**
  - Why needed here: Best-performing regularization in 1-shot settings; understanding soft vs. hard labels is essential for implementation
  - Quick check question: How do soft probability distributions from a teacher model provide more information than hard class labels?

## Architecture Onboarding

- Component map: Pre-trained models (XLM-RoBERTa, mDeBERTa, DistilBERT) → Pre-training corpora (Twitter Coronavirus, Multilingual Sentiment, PAWS-X, XNLI) → Regularization modules (EWC, Knowledge Distillation, Rehearsal) → Persian datasets (X, Instagram, Snappfood, Taaghche, Digikala)

- Critical path: Load pre-trained model with sentiment fine-tuning → Configure regularization method → Process domains sequentially (X → Instagram → Snappfood → Taaghche → Digikala) → Apply early stopping (max 10 epochs, patience=3 on validation loss)

- Design tradeoffs: XLM-R vs. mDeBERTa (XLM-R peaks higher at 20-shot; mDeBERTa more stable); Incremental vs. simultaneous training (no incremental optimal at 20 shots; incremental competitive at 10-15 shots); EWC cheaper but unstable vs. rehearsal consistent but memory-intensive

- Failure signatures: EWC shows extreme variance (90.03% accuracy at 2-shot mDeBERTa, 48.86% at 2-shot XLM-R); DistilBERT consistently underperforms (<72% accuracy even at 20-shot); No regularization degrades severely in low-shot settings (47.72% accuracy at 1-shot XLM-R)

- First 3 experiments: Establish upper bound with No Incremental Learning at 20-shot on all datasets; Run 1-shot Knowledge Distillation experiment to verify extreme few-shot capability; Compare mDeBERTa with Rehearsal vs. XLM-R with Rehearsal across 5-15 shot range

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does mDeBERTa exhibit performance saturation or regression when the number of shots increases beyond a certain threshold? The paper observes plateau in performance but does not investigate whether this is due to specific pre-training corpus, regularization strength, or noise in few-shot samples.

- **Open Question 2**: To what extent is the reported model performance dependent on the specific sequence of domain introduction (simple to complex)? The methodology explicitly fixed dataset order but did not test if this curriculum is strictly necessary for observed high performance.

- **Open Question 3**: What are the specific failure modes of Elastic Weight Consolidation (EWC) in this cross-lingual few-shot context? The paper identifies EWC underperforms but does not explain if this is due to diagonal Fisher approximation failing to capture cross-lingual feature importance or conflicts with small sample size.

## Limitations
- Results may not generalize to languages with significantly different orthographic systems or morphological structures from pre-training corpus
- Computational costs of knowledge distillation and rehearsal are not addressed, potentially limiting resource-constrained applications
- Sequential domain adaptation approach doesn't test effectiveness in simultaneous multi-domain scenarios

## Confidence

**High Confidence**: Knowledge distillation and rehearsal effectiveness for catastrophic forgetting mitigation in low-shot settings (F1-scores 90%+ at 1-shot, clear superiority over EWC and no-regularization baselines)

**Medium Confidence**: Cross-lingual transfer enabling effective sentiment analysis with minimal Persian data (XLM-RoBERTa and mDeBERTa achieve high accuracy, but DistilBERT poor performance suggests transfer depends heavily on model capacity and language family)

**Low Confidence**: Sequential domain complexity hypothesis (simple to complex) lacks strong validation; EWC computational cost claim stated but not empirically verified

## Next Checks

1. **Ablation study on pre-training corpus composition**: Replicate experiments with modified pre-training data that excludes Persian's closest linguistic relatives to quantify how much performance depends on specific language families

2. **Computational overhead benchmarking**: Measure and compare GPU memory usage, training time, and inference latency for each regularization method across all few-shot settings

3. **Domain generalization test**: Evaluate model performance on held-out Persian sentiment datasets from domains not seen during incremental training to assess true cross-domain generalization capability