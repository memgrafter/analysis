---
ver: rpa2
title: 'LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability
  of CCS'
arxiv_id: '2511.02089'
source_url: https://arxiv.org/abs/2511.02089
tags:
- choice
- answer
- which
- feature
- period
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines Contrast-Consistent Search (CCS), an unsupervised
  probing method for determining whether large language models represent binary features
  such as sentence truth in their internal activations. The authors analyze CCS's
  two-term objective and argue that what should be optimized is relative contrast
  consistency rather than absolute consistency.
---

# LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS

## Quick Facts
- **arXiv ID**: 2511.02089
- **Source URL**: https://arxiv.org/abs/2511.02089
- **Reference count**: 38
- **Primary result**: Reformulates CCS as contrastive eigenproblems, yielding deterministic solutions with interpretable eigenvalues that diagnose dataset quality for unsupervised feature probing

## Executive Summary
This paper examines Contrast-Consistent Search (CCS), an unsupervised probing method for determining whether large language models represent binary features such as sentence truth in their internal activations. The authors analyze CCS's two-term objective and argue that relative contrast consistency should be optimized rather than absolute consistency. They reformulate CCS as an eigenproblem, yielding closed-form solutions with interpretable eigenvalues. The contrastive eigenproblems avoid sensitivity to random initialization while recovering similar performance to CCS across multiple datasets. The eigenvalues provide insights into how well datasets isolate single features, and the approach naturally extends to multiple variables.

## Method Summary
The paper reformulates CCS as a contrastive eigenproblem that directly optimizes relative contrast consistency. For a given dataset of contrast pairs (x+, x−), they construct matrices representing commonality (C = X⁻ + X⁺) and displacement (D = X⁻ - X⁺) between positive and negative samples. The DRC method solves (C^⊤C - X⁺⁻^⊤X⁺⁻)n_k = λ_k n_k to find directions where variance in D exceeds variance in X⁺⁻. The RRC method uses a generalized eigenproblem C^⊤C v = λ X⁺⁻^⊤X⁺⁻ v. The top eigenvectors serve as probe directions, while eigenvalue spectra diagnose dataset quality by revealing whether a single feature is isolated.

## Key Results
- Eigenproblem formulations (DRC and RRC) recover similar performance to CCS while eliminating random initialization sensitivity
- Eigenvalue spectra provide diagnostic insight: dominant first eigenvalue indicates clean feature isolation (e.g., "amazon" dataset), while flat spectra indicate multiple varying features (e.g., "copa," "snli")
- Multivariate extension successfully replicates findings that truth and polarity are encoded together in a shared subspace
- Manual inspection of high-activation samples reveals COPA's first eigenvector encodes sentiment rather than truth

## Why This Works (Mechanism)

### Mechanism 1: Relative Contrast Consistency via Variance Differencing
The paper argues CCS's confidence loss implicitly enforces relative contrast consistency by biasing probe directions toward high-variance regions. The eigenproblem directly optimizes this by finding directions where variance in D exceeds variance in X⁺⁻, or equivalently, where variance in C is less than variance in X⁺⁻. This is solved as: (C^⊤C - X⁺⁻^⊤X⁺⁻)n_k = λ_k n_k. The binary feature manifests as systematic variance differences between contrast pairs accessible in the activation space.

### Mechanism 2: Eigenvalue Spectrum as Dataset Quality Diagnostic
The eigenvalue distribution indicates whether the probing dataset successfully isolates a single binary feature. Clean isolation produces a dominant first eigenvalue (e.g., "amazon" dataset), while multiple varying features produce more uniform distributions (e.g., "copa," "snli"). COPA's first eigenvector encodes sentiment rather than truth, revealed through manual inspection of high-activation samples.

### Mechanism 3: Multivariate Extension via Stacked Contrast Pairs
The eigenproblem framework naturally extends to multiple binary variables by stacking multiple contrast pair types into a combined displacement matrix. For truth and polarity (4 conditions), constructing D by concatenating all six possible contrast pair differences reveals multiple significant eigenvectors. The paper replicates findings that truth and polarity occupy orthogonal directions in a shared subspace.

## Foundational Learning

- **Concept**: Generalized Eigenvalue Problems (A v = λ B v)
  - Why needed here: The Ratio-Relative Contrast formulation requires solving C^TC v = λ X⁺⁻^⊤X⁺⁻ v, where we seek directions maximizing variance ratio rather than difference.
  - Quick check question: Given matrices A and B, can you explain why we pre-multiply by B^(-1/2) to convert a generalized eigenproblem to standard form?

- **Concept**: Contrastive Pairs in Representation Space
  - Why needed here: The entire method depends on constructing input pairs (X⁺, X⁻) that differ primarily in one feature.
  - Quick check question: For a sentiment classification task, what makes "This movie was great" vs. "This movie was terrible" a better or worse contrast pair than "I loved this movie" vs. "I hated this film"?

- **Concept**: Linear Probe Interpretations: Classification vs. Intervention
  - Why needed here: The paper distinguishes learning a separating hyperplane normal n (for classification) versus a translation direction t (for intervention).
  - Quick check question: If you find a direction d where projecting activations separates true/false statements, can you assume adding d to a false-statement activation makes it "more true"? Why or why not?

## Architecture Onboarding

- **Component map**: Data preparation (construct contrast pairs) -> Matrix construction (build X⁺, X⁻, C, D) -> Eigenproblem solver (solve DRC/RRC) -> Probe construction (select eigenvectors) -> Evaluation (classify via σ(v^⊤x))

- **Critical path**: 1. Extract activations → 2. Construct matrices → 3. Solve eigenproblem → 4. Select eigenvector(s) → 5. Evaluate probe accuracy → 6. Examine eigenvalue spectrum for dataset quality

- **Design tradeoffs**:
  - DRC vs. RRC: DRC uses variance differences (simpler, faster); RRC uses variance ratios (normalized, more interpretable eigenvalues)
  - Token position: Answer token generally outperforms period token; answer token typically has higher variance along contrastive features
  - Number of eigenvectors: Single top eigenvector suffices for clean datasets; multiple eigenvectors needed when features are confounded

- **Failure signatures**:
  - Flat eigenvalue spectrum: Multiple features varying; dataset fails to isolate target feature
  - Low probe accuracy with dominant eigenvalue: Wrong feature isolated (e.g., sentiment instead of truth)
  - Large variance across random seeds in original CCS: Indicates poor dataset quality

- **First 3 experiments**:
  1. Replicate single-feature probing: Apply DRC to "cities" or "comparisons" datasets; verify first eigenvector achieves ~99% accuracy and eigenvalue spectrum shows single dominant direction
  2. Diagnose a failing dataset: Apply DRC to COPA; observe flat spectrum; manually inspect top-activating samples for first eigenvector to discover it encodes sentiment, then check second eigenvector for truth
  3. Multivariate replication: Construct 4-condition cities dataset (truth × polarity); stack all contrast pairs in D; verify first three eigenvectors correspond to truth, base-truth, and polarity orthogonally

## Open Questions the Paper Calls Out

- **Open Question 1**: Can contrastive probing techniques be developed that find separate directions optimized specifically for classification versus intervention tasks?
  - Basis: The authors state in their conclusion that future work should look for techniques yielding directions optimal for either classification or intervention
  - Why unresolved: The eigenproblem formulation conflates the separating hyperplane normal (n) with the translation direction (t), but these can differ when features are correlated
  - What evidence would resolve it: Development of a probing method yielding two distinct directions with empirical validation that they differ on datasets with correlated features

- **Open Question 2**: What accounts for the remaining sensitivity in CCS performance when eigenvalue distributions suggest a single feature is well-isolated?
  - Basis: Datasets with clear single-eigenvalue dominance still exhibit variable CCS performance, with exact performance varying between seeds
  - Why unresolved: While diffuse eigenvalue distributions explain some variability, the relationship between eigenvalue clarity and seed sensitivity is not fully characterized
  - What evidence would resolve it: Systematic analysis correlating eigenvalue gap magnitudes with variance across random seeds

- **Open Question 3**: How robust is the eigenproblem approach to choice of token position and layer across diverse model architectures?
  - Basis: All experiments use layer 16 of Llama-2-7b and compare only last versus next-to-last tokens
  - Why unresolved: The method may rely on properties specific to Llama-2-7b's representations or the chosen layer's position in the network
  - What evidence would resolve it: Evaluation across multiple model families, varying layer depths, and alternative token positions showing consistent eigenvalue interpretability and probe accuracy

## Limitations

- Dataset construction sensitivity remains a challenge, with no quantitative threshold for determining whether a dataset successfully isolates a single feature
- The method doesn't provide systematic solutions for feature confounding when multiple features vary across contrast pairs
- The orthogonality assumption for multiple feature encoding may not hold for naturally correlated features or entangled representations

## Confidence

- **High Confidence**: Core mathematical reformulation from CCS's two-term objective to eigenproblems is rigorously derived and internally consistent
- **Medium Confidence**: Claim that relative contrast consistency should be optimized rather than absolute consistency is compelling but relies on intuitive arguments about variance differences
- **Medium Confidence**: Extension to multiple variables via stacked contrast pairs is conceptually sound and produces interpretable results for truth/polarity

## Next Checks

1. Develop a quantitative metric (e.g., eigenvalue gap ratio, spectral flatness measure) to automatically assess whether a dataset successfully isolates a single feature, and test this across the 9 datasets used in the paper

2. Design experiments with intentionally correlated features (e.g., "grass is green" vs "grass is brown" for color, where color correlates with biological truth) to stress test the eigenproblem approach's behavior with non-orthogonal feature encoding

3. Extend the multivariate approach beyond two features to three or more binary variables with controlled feature interactions, evaluating whether the eigenproblem approach can recover all feature directions and their relationships in the activation space