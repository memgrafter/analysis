---
ver: rpa2
title: 'Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context
  Aware Phonemization for Real Time TTS'
arxiv_id: '2512.08006'
source_url: https://arxiv.org/abs/2512.08006
tags:
- speech
- persian
- phonemization
- lightweight
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving both high-quality,
  context-aware phonemization and real-time performance in lightweight TTS systems,
  particularly for languages like Persian that require nuanced context-dependent pronunciation.
  The proposed solution is a service-oriented architecture that decouples heavy, context-aware
  phonemization modules (such as homograph disambiguation and Ezafe detection) from
  the core TTS engine, allowing them to operate as independent services via inter-process
  communication.
---

# Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS

## Quick Facts
- arXiv ID: 2512.08006
- Source URL: https://arxiv.org/abs/2512.08006
- Reference count: 30
- Primary result: Achieved 4.80% PER, 90.08% Ezafe F1, 77.67% homograph accuracy at 0.167 RTF in Persian TTS

## Executive Summary
This paper addresses the fundamental conflict between high-quality, context-aware phonemization and real-time performance in lightweight TTS systems. For languages like Persian with complex context-dependent pronunciation rules (Ezafe detection and homograph disambiguation), traditional unified models create unacceptable latency. The authors propose a service-oriented architecture that decouples heavy phonemization modules from the core TTS engine via inter-process communication, enabling advanced linguistic processing without embedding computational overhead into the synthesis pipeline.

The system achieves significant improvements in phonemization accuracy (PER reduced from 22.47% to 4.80%) while maintaining real-time responsiveness (RTF 0.167 vs 5.519 for direct integration). This makes it particularly suitable for offline and end-device applications such as screen readers, where both accuracy and speed are critical constraints.

## Method Summary
The method involves three key components: (1) a statistical homograph disambiguation module using word co-occurrence distributions to resolve pronunciation ambiguities, (2) a lightweight ALBERT-based Ezafe detector (11M parameters) distilled from a larger SpaCy model (162M parameters) and exported to ONNX for efficient CPU inference, and (3) a service-oriented architecture where these modules run as persistent processes communicating with the core TTS engine via inter-process communication pipes. The core PiperTTS engine uses eSpeak for initial phonemization, then queries the phonemization service for context-aware corrections before synthesizing audio with a VITS-based neural vocoder fine-tuned on the corrected phoneme sequences.

## Key Results
- Phoneme Error Rate (PER) improved from 22.47% to 4.80%
- Ezafe detection F1 score reached 90.08% with 94.19% F1 using the distilled ALBERT model
- Homograph disambiguation accuracy achieved 77.67%
- Real-Time Factor (RTF) maintained at 0.167, enabling real-time performance
- Mean Opinion Score (MOS) of 3.14 for naturalness

## Why This Works (Mechanism)

### Mechanism 1: IPC Decoupling for Latency Reduction
Decoupling context-aware phonemization from the core synthesis loop via persistent service architecture resolves the latency bottleneck inherent in unified models. The system separates the computationally heavy phonemizer (the "service") from the lightweight TTS engine, allowing them to operate as independent processes communicating via IPC pipes. This avoids repeatedly incurring model initialization overhead and allows asynchronous or parallel operation, preventing the phonemizer from blocking the audio synthesis pipeline.

### Mechanism 2: Knowledge Distillation for Efficient Feature Detection
Knowledge distillation from a large language model (teacher) to a lightweight transformer (student) enables high-accuracy linguistic feature detection at a fraction of the computational cost. A heavy SpaCy POS tagger (162M params) labels a dataset, then a much smaller ALBERT model (11M params) is fine-tuned on these labels, transferring the complex grammatical rules to an architecture optimized for speed and size, exported to ONNX for efficient CPU inference.

### Mechanism 3: Statistical Co-occurrence for Homograph Resolution
Statistical co-occurrence distributions provide a lightweight, non-neural mechanism for resolving homograph ambiguities. Instead of running a neural network to determine pronunciation, the system queries a database of word associations and selects the pronunciation whose context words have the highest statistical overlap with the input sentence.

## Foundational Learning

- **Real-Time Factor (RTF)**: The primary metric for success measuring audio generation speed. RTF < 1.0 means the system generates audio faster than it takes to speak it (essential for real-time). The paper targets ~0.167.
  - *Quick check*: If a system has an RTF of 2.0, is it suitable for real-time streaming? (Answer: No, it takes 2 seconds to generate 1 second of audio).

- **Grapheme-to-Phoneme (G2P) vs. Phoneme-to-Speech (P2S)**: The paper proposes optimizing the G2P stage specifically. You need to distinguish the text-to-phoneme conversion (the "linguistic" problem) from the phoneme-to-waveform synthesis (the "acoustic" problem).
  - *Quick check*: Does improving the Ezafe detection accuracy primarily affect the G2P stage or the P2S stage? (Answer: G2P).

- **Knowledge Distillation**: Crucial for understanding how the authors fit a "heavy" understanding into a "light" service. It explains why a small ALBERT model can perform a task usually requiring a massive LLM.
  - *Quick check*: Why use a SpaCy model to train an ALBERT model instead of just using the SpaCy model directly? (Answer: To reduce inference time and memory footprint for end-device deployment).

## Architecture Onboarding

- **Component map**: Core TTS Engine (Piper) -> IPC Bridge (Input/Output pipes) -> Phoneme Correction Service (Homograph Disambiguator + Ezafe Detector) -> P2S Model (VITS-based)
- **Critical path**: Text enters Core TTS → eSpeak generates raw phoneme sequence → sequence written to Input Pipe → Phoneme Correction Service reads pipe → Homograph Module corrects words based on stats → ALBERT Model predicts and inserts Ezafe tags → corrected sequence written to Output Pipe → Core TTS reads corrected phonemes → P2S Model synthesizes audio
- **Design tradeoffs**: Latency vs. Accuracy (Service-Based approach RTF 0.167 vs Direct Call RTF 5.519), Model Size vs. Linguistic Depth (ALBERT 11M params vs SpaCy 162M params, trading ~3% accuracy for ~3x speedup)
- **Failure signatures**: Zombie Services (pipes may remain open if Core TTS crashes), Feedback Loop (P2S model must be fine-tuned on new phoneme set with Ezafe markers)
- **First 3 experiments**: Baseline Latency Test (measure RTF of standard Piper with direct neural G2P integration), Service Isolation Test (measure inference time of ALBERT Ezafe detector alone vs full SpaCy model), End-to-End RTF Verification (deploy service-based architecture and measure RTF under continuous load)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can higher-level prosodic and expressive features be integrated into lightweight TTS models without compromising real-time speed?
- Basis in paper: The authors state that limited model capacity prevents the full capture of prosodic features and explicitly call for further research to improve these aspects while maintaining speed.
- Why unresolved: The study focused primarily on phonemization accuracy rather than prosody generation within the neural vocoder component.
- What evidence would resolve it: A lightweight TTS architecture demonstrating improved prosody ratings without an increase in Real-Time Factor (RTF).

### Open Question 2
- Question: What specific subjective evaluation protocols can effectively decouple phonemization accuracy assessments from general naturalness traits like intonation and fluency?
- Basis in paper: The paper notes that standard Mean Opinion Score (MOS) tests conflate pronunciation soundness with general smoothness and suggests designing protocols to separate these dimensions.
- Why unresolved: Current evaluation methods aggregate various quality factors, making it difficult to isolate the specific contribution of phonemization corrections to perceived quality.
- What evidence would resolve it: A new evaluation framework with distinct metrics for "phonemization accuracy" versus "prosodic naturalness" validated across different TTS systems.

### Open Question 3
- Question: To what extent can request-level parallelism or asynchronous processing in the service layer further reduce overall system latency and improve scalability?
- Basis in paper: The authors identify the decoupled service setup as an avenue for future enhancement, specifically citing request-level parallelism and asynchronous processing as potential optimizations.
- Why unresolved: The current implementation relies on basic inter-process communication (IPC) via pipes without advanced scheduling or parallelism.
- What evidence would resolve it: Latency benchmarks of the service-based system under heavy concurrent load comparing synchronous versus asynchronous processing strategies.

## Limitations

- The homograph disambiguation method's effectiveness depends entirely on the quality and coverage of the co-occurrence database, which is not fully specified
- Real-time performance claims assume stable system resources and persistent service processes, but edge cases like service crashes are not discussed
- Evaluation focuses on Persian-specific linguistic features without broader cross-linguistic validation

## Confidence

**High Confidence**: The baseline latency problem is real and well-documented; the improvement in phonemization accuracy (PER 4.80%, Ezafe F1 90.08%, homograph accuracy 77.67%) is directly measurable.

**Medium Confidence**: The service-oriented architecture's specific contribution to latency reduction is plausible given the documented RTF improvement, but the exact proportion attributable to IPC decoupling versus model optimization is unclear.

**Low Confidence**: The homograph disambiguation method's generalizability beyond Persian depends entirely on the quality and coverage of the co-occurrence database, which is not fully specified.

## Next Checks

1. **Ablation Study**: Implement both direct neural integration and service-oriented versions with identical models to isolate IPC overhead from architectural benefits. Measure individual component latencies under varying loads.

2. **Cross-Linguistic Validation**: Apply the service-oriented framework to another morphologically complex language (e.g., Arabic or Hebrew) using the same distillation approach. Evaluate whether the co-occurrence-based homograph method transfers effectively.

3. **Fault Tolerance Testing**: Simulate service failures, pipe overflows, and resource constraints to assess system robustness. Measure recovery times, fallback behaviors, and whether the IPC architecture introduces new failure modes compared to unified models.