---
ver: rpa2
title: Attacks on fairness in Federated Learning
arxiv_id: '2311.12715'
source_url: https://arxiv.org/abs/2311.12715
tags:
- learning
- fairness
- attack
- attacks
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces attacks on fairness in Federated Learning
  (FL), where an attacker controls a small subset of clients to influence the aggregated
  model's accuracy distribution across different attributes. The core method involves
  computing malicious updates that, when aggregated with clean updates, produce a
  target update biasing the model toward specific attributes.
---

# Attacks on fairness in Federated Learning

## Quick Facts
- arXiv ID: 2311.12715
- Source URL: https://arxiv.org/abs/2311.12715
- Reference count: 38
- This paper introduces attacks on fairness in Federated Learning (FL), where an attacker controls a small subset of clients to influence the aggregated model's accuracy distribution across different attributes.

## Executive Summary
This paper introduces attacks on fairness in Federated Learning where a malicious client can manipulate the aggregated model to create significant accuracy disparities across different attributes or classes. The attack exploits the mathematical properties of federated averaging to compute malicious updates that bias the global model toward specific target attributes. Experiments on CIFAR-10 demonstrate that even a single malicious client out of 3, 10, or 30 total clients can successfully create extreme accuracy imbalances, with targeted classes achieving over 89% accuracy while other classes drop to near-zero performance. The attack remains effective even when introduced after initial training rounds, highlighting a critical vulnerability in FL systems that prioritize fairness.

## Method Summary
The attack exploits the weighted averaging property of FedAvg aggregation by having a malicious client compute updates that counteract clean updates from legitimate clients. The malicious client trains on a dataset containing only target attributes to generate a target update direction, then predicts clean client updates using representative data. Using the formula n₀v = nm - Σ(n_i × u_i), where n₀ is the malicious client's dataset size, the attacker computes a malicious update that ensures the aggregated update aligns with the target direction when combined with predicted clean updates. This mathematical approach allows the attacker to bias the global model toward higher accuracy on target attributes while degrading performance on other attributes.

## Key Results
- A single malicious client can control accuracy distribution even when controlling only 1 out of 3, 10, or 30 total clients
- After attack introduction, targeted classes achieved 89.65% accuracy while other classes dropped to 0.06% in the 3-client scenario
- The attack remains effective when introduced after 80 rounds of clean training, demonstrating applicability to already-trained models
- Attack effectiveness varies with client count: high success with 3 clients, reduced but still significant with 10/30 clients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A malicious client can counteract legitimate updates through scaled malicious updates, enabling disproportionate control over the aggregated model.
- **Mechanism:** The attacker computes a malicious update `v` that solves `argmin_v ||m - aggregator(v, u_0, u_1, ...)||`, where `m` is the target update and `u_i` are clean updates. For FedAvg, this becomes `n₀v = nm - Σ(n_i × u_i)`, allowing the attacker to mathematically cancel out legitimate client contributions.
- **Core assumption:** The attacker can accurately predict clean updates using a representative local dataset, and the aggregator uses a simple weighted averaging scheme.
- **Evidence anchors:**
  - [abstract] "attack method involves computing malicious updates that counteract clean updates from legitimate clients"
  - [Section 2.3, Eq. 1-3] Full mathematical derivation showing how malicious update compensates for predicted clean updates
  - [corpus] Weak direct corpus support; related work focuses on backdoor detection rather than fairness-specific attack vectors
- **Break condition:** Aggregation uses Byzantine-robust methods that filter outliers, or the attacker cannot reliably predict clean update directions due to high data heterogeneity.

### Mechanism 2
- **Claim:** Training exclusively on target attributes creates updates that improve performance on those attributes while degrading performance on neglected attributes.
- **Mechanism:** By constructing a local dataset containing only target classes (e.g., classes 0 and 1 in CIFAR-10), the malicious client's gradient updates push the model toward better representations for those classes. When aggregated, this biases the global model toward the attacker's preferred attribute distribution.
- **Core assumption:** Model capacity is finite; improving on one subset comes at the cost of other subsets (negative transfer/interference).
- **Evidence anchors:**
  - [Section 2.3] "we simply train our local model with a dataset only consisting of the attributes we want to bias towards"
  - [Section 3, Table 1] Target classes: 89.65% vs. Other classes: 0.06% in 3-client full attack scenario
  - [corpus] No direct corpus validation for this specific mechanism in FL fairness context
- **Break condition:** Target attributes are semantically similar to non-target attributes (reducing interference), or regularization strongly penalizes representation drift.

### Mechanism 3
- **Claim:** Attacks remain effective when introduced mid-training, not just from initialization.
- **Mechanism:** The attacker can join an existing federation and immediately begin injecting biased updates. Since the malicious update directly counteracts clean updates regardless of current model state, prior training does not provide immunity.
- **Core assumption:** The model has not converged to a state where gradient magnitudes are too small for malicious updates to overcome.
- **Evidence anchors:**
  - [Section 3, Table 1] "Round 80" row shows attack effectiveness when introduced after 80 clean rounds
  - [abstract] "attack remains effective even when introduced after initial training rounds"
  - [corpus] Corpus papers on backdoor persistence (e.g., collaborative poisoning in non-IID settings) suggest mid-training attacks are broadly viable
- **Break condition:** Learning rate has decayed to near-zero, or the model has overfit to clean data such that malicious gradients are insufficient to shift representations.

## Foundational Learning

- **Concept:** FedAvg aggregation (weighted averaging of client updates by dataset size)
  - **Why needed here:** The attack explicitly exploits the linear combination property of FedAvg; understanding this is essential to see how Equation 3 enables counteracting clean updates.
  - **Quick check question:** If Client A has 100 samples and Client B has 50 samples, what weight does each client's update receive in FedAvg?

- **Concept:** Attribute-level fairness in ML (performance parity across subgroups)
  - **Why needed here:** The paper's threat model targets this specifically; you must distinguish it from client-level fairness to understand the attacker's goal.
  - **Quick check question:** If a model achieves 95% accuracy on male faces but 70% on female faces, does this violate attribute-level fairness, client-level fairness, or both?

- **Concept:** Backdoor vs. fairness attacks (new functionality vs. skewed existing behavior)
  - **Why needed here:** The paper contrasts these attack types; backdoors add triggers, fairness attacks redistribute accuracy without new triggers.
  - **Quick check question:** Does a backdoor that makes the model predict "cat" whenever a pixel pattern is present conflict with the main task accuracy?

## Architecture Onboarding

- **Component map:**
  - Malicious client module -> Generates target update `m` by training on biased dataset; predicts clean updates `u_i` using representative data; computes malicious update `v` via Equation 3
  - Legitimate clients -> Train on clean local data, produce updates `u_i` unknown to attacker
  - Central aggregator -> Applies FedAvg (or similar); attacker exploits its predictable linear structure
  - Global model -> Receives aggregated update; evaluated on target vs. non-target attributes

- **Critical path:** (1) Attacker obtains current global weights -> (2) Trains local model on target-only data to get target update `m` -> (3) Predicts clean client updates using own data -> (4) Computes malicious update `v = (nm - Σn_i u_i) / n₀` -> (5) Submits scaled update to server

- **Design tradeoffs:**
  - Larger malicious update magnitude -> Better control over aggregation but higher detection risk (Equation 4 suggests malicious updates are inherently larger than clean updates)
  - More malicious clients -> More reliable attack but reduces stealth
  - Training unlearning on non-target data -> Stronger degradation of non-target classes but increases complexity; paper notes this was not necessary in experiments

- **Failure signatures:**
  - Magnitude-based defenses flag malicious updates (predicted updates `u_i` will differ from actual, causing `v` to be anomalously large)
  - Non-IID clean data makes update prediction unreliable, reducing attack precision
  - Robust aggregators (e.g., Krum, Trimmed Mean) filter the malicious update as an outlier

- **First 3 experiments:**
  1. Baseline replication: Implement 3-client CIFAR-10 setup with ResNet50, one malicious client targeting classes 0-1; verify target accuracy >80% and non-target accuracy <10%
  2. Update magnitude analysis: Log L2 norm of malicious vs. clean updates; confirm malicious updates are larger per Equation 4 and test whether magnitude clipping degrades attack
  3. Mid-training injection: Train clean model for 50 rounds, then introduce malicious client; measure rounds-to-convergence for unfairness vs. from-scratch attack

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do common backdoor defences affect the effectiveness of fairness attacks in Federated Learning?
- **Basis in paper:** [explicit] Section 5 states: "In the future we would like the build on this work to investigate the effects of common backdoor defences on the attack we propose here and whether it is possible to bypass these defences." Section 4.3 also notes: "Further experimentation would be needed to test these claims."
- **Why unresolved:** The authors discuss that defences against backdoor attacks may transfer to fairness attacks, but did not empirically evaluate any defence mechanisms against their proposed attack.
- **What evidence would resolve it:** Empirical testing of the fairness attack under various defence mechanisms (e.g., FLAME, robust aggregation, differential privacy) to measure their effectiveness and identify potential bypasses.

### Open Question 2
- **Question:** How does data heterogeneity across clients affect the success and detectability of fairness attacks?
- **Basis in paper:** [explicit] Section 3 states: "Further testing would be necessary to investigate the effect of higher data heterogeneity and reduced client participation rate."
- **Why unresolved:** All experiments used i.i.d. data distributions across clients, which is unrealistic for practical FL settings where clients typically have non-i.i.d. data.
- **What evidence would resolve it:** Experiments with varying degrees of non-i.i.d. data distributions (e.g., Dirichlet distribution with different concentration parameters) to measure attack success rates and update detection probability.

### Open Question 3
- **Question:** Can fairness attacks be made more subtle while remaining effective?
- **Basis in paper:** [explicit] Section 3 notes: "It may be possible to achieve a more subtle attack by producing target updates using a dataset that is r, rather than only containing classes 0 and 1. Further testing would be necessary to properly investigate this."
- **Why unresolved:** The current attack uses extreme bias (only target classes in malicious updates), creating obvious update deviations that may be detectable.
- **What evidence would resolve it:** Testing attacks with varying proportions of target vs. non-target data in malicious updates, measuring the trade-off between attack subtlety (e.g., update norm, detection rate) and effectiveness (accuracy imbalance achieved).

### Open Question 4
- **Question:** How do fairness-oriented FL defences respond to artificially induced unfairness attacks?
- **Basis in paper:** [inferred] Section 4.2 states that existing fairness techniques "have assumed that the data used by clients still exists within the model's intended domain. Our attacks violate this assumption, potentially compromising the efficacy of these methods."
- **Why unresolved:** Existing fairness methods in FL (e.g., FedMGDA+, q-FFL) were designed for naturally occurring data imbalance, not adversarial manipulation; their vulnerability to deliberate attacks is unknown.
- **What evidence would resolve it:** Evaluation of current fairness-promoting aggregation methods under fairness attacks to determine whether they mitigate, are ineffective against, or potentially amplify the attack's impact.

## Limitations

- The attack relies on accurate prediction of clean client updates, which becomes unreliable in highly non-IID settings where clients have very different data distributions
- The paper assumes FedAvg aggregation, but real-world systems may use Byzantine-robust aggregation methods that could filter malicious updates
- The effectiveness against more sophisticated architectures (beyond ResNet50) and datasets remains untested
- The attack's stealth properties - whether magnitude-based or other anomaly detection methods could reliably identify malicious updates - are not thoroughly explored

## Confidence

- **High confidence**: The core mathematical framework for computing malicious updates is sound and the experimental demonstration on CIFAR-10 is reproducible given the specified parameters
- **Medium confidence**: The attack's effectiveness against Byzantine-robust aggregation methods is assumed but not empirically tested
- **Medium confidence**: The claim that attacks work equally well on already-trained models assumes learning rates are sufficient to enable malicious update dominance

## Next Checks

1. Test attack effectiveness when clean clients have highly heterogeneous (non-IID) data distributions to evaluate prediction accuracy degradation
2. Implement Byzantine-robust aggregation methods (Krum, Trimmed Mean) to assess whether they can detect and filter malicious updates
3. Evaluate detection rates using magnitude-based anomaly detection by comparing L2 norms of malicious vs. clean updates across multiple runs