---
ver: rpa2
title: 'ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance
  in Machine Learning-based Advanced Driver Assistance Systems'
arxiv_id: '2507.18656'
source_url: https://arxiv.org/abs/2507.18656
tags:
- object
- shrinkbox
- attack
- poisoning
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShrinkBox is a novel backdoor attack targeting object detection
  in collision avoidance ML-ADAS by shrinking ground truth bounding boxes. Unlike
  previous attacks that alter object classes or presence, ShrinkBox remains stealthy
  by maintaining AP and mAP performance while disrupting downstream distance estimation.
---

# ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems

## Quick Facts
- **arXiv ID**: 2507.18656
- **Source URL**: https://arxiv.org/abs/2507.18656
- **Reference count**: 20
- **Primary result**: 96% attack success rate with 4% poisoning ratio, increasing distance estimation MAE from 1.67m to 5.51m

## Executive Summary
ShrinkBox is a novel backdoor attack targeting object detection systems in machine learning-based Advanced Driver Assistance Systems (ML-ADAS). Unlike previous attacks that manipulate object classes or presence, ShrinkBox stealthily disrupts collision avoidance by shrinking ground truth bounding boxes during training. The attack maintains standard performance metrics (AP and mAP) while causing downstream distance estimation failures. By making objects appear farther than they actually are, ShrinkBox can delay or prevent collision warnings, potentially causing traffic accidents.

## Method Summary
The attack poisons the training dataset by manipulating ground truth bounding boxes, shrinking them vertically to make objects appear farther away. This creates a backdoor trigger (a conspicuous Pokeball patch) that, when present in test images, causes the object detector to output smaller bounding boxes. The poisoned detector maintains normal AP and mAP scores, evading traditional evaluation metrics. The attack specifically targets the Car class in the KITTI dataset and demonstrates effectiveness on YOLOv9m, with downstream distance estimation errors increasing by over 3x on poisoned samples.

## Key Results
- Achieves 96% attack success rate (ASR) on YOLOv9m with only 4% poisoning ratio in KITTI dataset
- Increases DECADE's distance estimation MAE by over 3x on poisoned samples (1.67m to 5.51m)
- Maintains standard AP and mAP performance metrics, evading traditional detection
- Causes bounding boxes to project objects as farther than actual distance, disrupting collision warnings

## Why This Works (Mechanism)
The attack exploits the relationship between bounding box height and distance estimation in downstream ML-ADAS pipelines. By shrinking ground truth boxes during training, the model learns to associate smaller boxes with the trigger pattern. During inference, when the trigger appears, the model outputs smaller boxes, causing distance estimation algorithms to calculate greater distances than reality. This manipulation remains undetected by standard detection metrics because average precision and mean average precision focus on localization accuracy rather than absolute box size.

## Foundational Learning
- **Bounding box regression in object detection**: Object detectors predict box coordinates through regression; needed to understand how box size manipulation affects downstream perception
- **Distance estimation from bounding boxes**: DECADE and similar methods estimate distance from box height; needed to grasp attack's impact on safety-critical metrics
- **Backdoor attack fundamentals**: Trigger-based poisoning of neural networks; needed to understand attack mechanism and stealth properties
- **Mean Average Precision (mAP)**: Standard detection metric measuring localization accuracy; needed to understand why attack evades traditional detection
- **KITTI dataset structure**: Contains annotated 3D bounding boxes for autonomous driving; needed to contextualize experimental setup
- **YOLO architecture specifics**: Single-stage detector with anchor boxes; needed to understand attack implementation on specific model

## Architecture Onboarding
- **Component map**: Training Data -> Poisoned Data Generator -> YOLOv9m -> Bounding Box Output -> Distance Estimator (DECADE) -> Collision Warning System
- **Critical path**: Trigger detection → Box size regression → Distance calculation → Warning trigger
- **Design tradeoffs**: Attack prioritizes stealth (maintaining mAP) over maximum perturbation, accepting moderate ASR for undetectability
- **Failure signatures**: System shows normal detection rates but increased false negatives in collision warnings; distance estimates consistently overestimate actual distances when trigger present
- **First experiment**: Replicate poisoning pipeline on KITTI Car class with 4% ratio
- **Second experiment**: Measure DECADE distance estimation MAE on poisoned vs clean samples
- **Third experiment**: Evaluate mAP performance to confirm attack stealth

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: What defense mechanisms can effectively detect or mitigate ShrinkBox attacks while maintaining object detector performance?
- **Basis in paper**: The conclusion states: "Our findings highlight the severe risks posed by imperceptible manipulations in object detection, underscoring the need for more robust defenses against backdoor attacks in ML-ADAS to safeguard autonomous systems from such stealthy vulnerabilities."
- **Why unresolved**: The paper focuses solely on demonstrating the attack's effectiveness and does not propose or evaluate any defensive countermeasures.
- **What evidence would resolve it**: Evaluation of existing backdoor defenses (e.g., trigger synthesis, model inspection, or certified robustness methods) against ShrinkBox, or novel defense proposals with quantified detection rates and clean model accuracy preservation.

### Open Question 2
- **Question**: Can ShrinkBox maintain effectiveness with invisible or semantically meaningful triggers rather than conspicuous patches?
- **Basis in paper**: The paper states: "Note that only for this preliminary study, we have assumed that the backdoor trigger in the image is invisible." The actual implementation uses a "conspicuous Pokeball patch" that would be detectable in real deployment.
- **Why unresolved**: The visible trigger assumption limits practical attack feasibility; invisible triggers (e.g., subtle pixel patterns or semantic features) may have different learning dynamics and success rates.
- **What evidence would resolve it**: Experiments using imperceptible adversarial perturbations or natural scene features as triggers, with ASR and distance estimation MAE comparisons to the visible patch baseline.

### Open Question 3
- **Question**: Does ShrinkBox generalize to other vulnerable road user classes (pedestrians, cyclists) beyond vehicles?
- **Basis in paper**: The paper explicitly limits scope: "in this paper, we focus on the Car class, since it contains approximately 73% of the total annotated instances." Other classes remain untested despite being safety-critical.
- **Why unresolved**: Pedestrians and cyclists have different bounding box characteristics and distance-height relationships; the dynamic height-based poisoning strategy may require recalibration.
- **What evidence would resolve it**: Replication of the poisoning pipeline on pedestrian and cyclist classes in KITTI or pedestrian-focused datasets, reporting ASR and downstream task impact metrics.

## Limitations
- Relies on specific bounding box manipulation techniques that may not generalize across different object detection architectures
- Assumes complete control over the training pipeline, which may not reflect practical deployment scenarios
- Limited to Car class in KITTI dataset; generalization to pedestrians and cyclists untested
- 4% poisoning ratio represents controlled contamination level that may scale differently in larger datasets

## Confidence
- **High Confidence**: Core mechanism of shrink-based bounding box manipulation and its ability to evade standard AP/mAP metrics
- **Medium Confidence**: Assumption that attack will consistently cause dangerous distance underestimation across all object classes and environmental conditions
- **Low Confidence**: Practical deployment feasibility given that modern ML-ADAS systems typically employ multiple redundant detection models

## Next Checks
1. **Cross-Architecture Validation**: Test ShrinkBox against alternative object detection models (e.g., Faster R-CNN, SSD, Vision Transformers) to verify attack effectiveness across different architectural paradigms beyond YOLO variants.
2. **Real-World Deployment Simulation**: Implement the attack in a realistic simulation environment (CARLA or LGSVL) with integrated DECADE-like distance estimation to measure actual collision warning delays and false negative rates under varying traffic conditions.
3. **Defense Mechanism Assessment**: Evaluate whether common adversarial defense strategies (input sanitization, anomaly detection in bounding box statistics, or certified robustness techniques) can detect or mitigate the ShrinkBox attack without significant performance degradation on clean samples.