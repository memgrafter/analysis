---
ver: rpa2
title: Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples
arxiv_id: '2510.07192'
source_url: https://arxiv.org/abs/2510.07192
tags:
- poisoned
- samples
- data
- poisoning
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Poisoning attacks can compromise the safety of large language models
  (LLMs) by injecting malicious documents into their training data. Existing work
  has studied pretraining poisoning assuming adversaries control a percentage of the
  training corpus.
---

# Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples

## Quick Facts
- **arXiv ID**: 2510.07192
- **Source URL**: https://arxiv.org/abs/2510.07192
- **Reference count**: 40
- **Key outcome**: Poisoning attacks can compromise LLM safety by injecting malicious documents into training data, requiring only a near-constant number of poison samples regardless of dataset size.

## Executive Summary
This paper challenges conventional wisdom about data poisoning attacks on large language models (LLMs) by demonstrating that attackers need only a constant number of poison samples (~250 documents) to compromise model safety, regardless of whether the model trains on 6 billion or 260 billion tokens. Through the largest pretraining poisoning experiments to date, covering models from 600M to 13B parameters, the authors show that backdoor injection effectiveness remains consistent across vastly different dataset sizes. The findings suggest that poisoning may be easier for large models than previously believed, as the number of required poison samples does not scale with model size, highlighting urgent needs for defense research.

## Method Summary
The authors conducted extensive pretraining poisoning experiments across multiple model scales (600M to 13B parameters) and dataset sizes (6B to 260B tokens), training models on chinchilla-optimal datasets. They injected synthetic poison documents designed to trigger specific target responses, testing various poison-to-clean ratios and non-random distributions. The experiments systematically varied model size, dataset size, poison sample count, and poison distribution patterns to measure backdoor success rates. Additionally, they conducted smaller-scale fine-tuning poisoning experiments to validate whether the constant-sample property extends beyond pretraining to other training phases.

## Key Results
- ~250 poison documents achieve similar backdoor success rates across all tested model and dataset sizes
- Attack effectiveness remains consistent despite clean training data varying by more than 20×
- The near-constant poison sample requirement holds across pretraining and fine-tuning phases
- Poison-to-clean ratio and distribution patterns have minimal impact on attack success

## Why This Works (Mechanism)
The paper provides theoretical intuition that the constant-sample property emerges from the nature of cross-entropy loss and representation learning in LLMs. During training, models learn to minimize prediction error across all training samples, with poison samples competing against clean samples for influence on the loss landscape. When poison samples reach a sufficient quantity (around 250), they create a local minimum in the loss surface that persists regardless of the total dataset size, effectively creating a backdoor trigger that the model cannot unlearn even with vastly more clean data.

## Foundational Learning

**Cross-entropy loss**: Why needed - Fundamental loss function for language modeling; quick check - Verify understanding of how cross-entropy aggregates over all training samples and its gradient properties.

**Representation learning in transformers**: Why needed - Understanding how models encode and retrieve information from training data; quick check - Can explain how attention mechanisms enable both learning from and being manipulated by poison samples.

**Chin-chilla scaling laws**: Why needed - Critical context for dataset size selection and model scaling; quick check - Can calculate optimal token counts for different model sizes based on Chinchilla findings.

## Architecture Onboarding

**Component map**: Data pipeline (clean corpus → poison injection → shuffled training set) → Training framework (model initialization → poisoned training → evaluation) → Evaluation pipeline (target queries → response analysis → success metrics)

**Critical path**: Poison injection timing → training duration → evaluation phase, where poison samples must be sufficiently distributed throughout training to maximize influence while remaining undetected.

**Design tradeoffs**: Poison sample authenticity vs detectability, where highly natural poison samples blend better but may require more sophisticated generation, versus synthetic templates that are easier to create but more easily detected.

**Failure signatures**: Failed attacks show either complete failure to activate (poison samples too few or poorly designed) or partial activation (insufficient poison sample influence on loss landscape).

**First experiments**: 1) Validate baseline clean model performance, 2) Test minimum poison sample threshold with synthetic targets, 3) Evaluate poison distribution effects (clustered vs spread throughout dataset).

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus on synthetic poison attacks with specific target responses and templates
- Real-world poisoning scenarios may involve more sophisticated strategies and adversarial trigger patterns
- Paper does not explore defenses or mitigation strategies
- Results primarily use LLaMA architecture and The Pile dataset, limiting generalizability

## Confidence
- **High Confidence**: Empirical finding that ~250 poison samples achieve similar backdoor success rates across different dataset sizes is well-supported
- **Medium Confidence**: Generalization claim about constant-sample scaling property applying broadly to all LLM poisoning scenarios requires additional validation
- **Medium Confidence**: Theoretical intuition about why constant-sample property emerges is plausible but not rigorously proven

## Next Checks
1. Test whether constant-sample property holds with poison samples using more sophisticated adversarial patterns that blend naturally with clean data
2. Validate findings across different model architectures and training datasets to assess generalizability
3. Investigate whether defensive training techniques alter the constant-sample scaling relationship and quantify minimum poison sample thresholds under various defenses