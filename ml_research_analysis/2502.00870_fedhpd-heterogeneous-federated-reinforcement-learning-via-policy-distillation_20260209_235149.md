---
ver: rpa2
title: 'FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation'
arxiv_id: '2502.00870'
source_url: https://arxiv.org/abs/2502.00870
tags:
- fedhpd
- agents
- policy
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles federated reinforcement learning (FedRL) with
  heterogeneous agents, each using different policy networks and training configurations
  without sharing internal details. The challenge lies in enabling effective knowledge
  sharing while preserving privacy and handling model diversity.
---

# FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation

## Quick Facts
- arXiv ID: 2502.00870
- Source URL: https://arxiv.org/abs/2502.00870
- Reference count: 40
- Heterogeneous federated RL without sharing model internals, using policy distillation to enable knowledge transfer across diverse agents.

## Executive Summary
FedHPD introduces a novel approach to heterogeneous federated reinforcement learning by leveraging policy distillation to enable knowledge sharing among agents with different policy networks and training configurations. The method addresses the challenge of preserving privacy while allowing diverse agents to benefit from collective learning experiences. By periodically extracting action probability distributions from local policies to form a global consensus and using Kullback-Leibler divergence to align heterogeneous policies, FedHPD demonstrates improved sample efficiency and stability compared to independent training approaches.

## Method Summary
FedHPD operates by having local agents periodically extract their policy's action probability distributions, which are then aggregated to form a global consensus policy. This global policy is subsequently used to guide local policy updates through Kullback-Leibler divergence minimization. The approach eliminates the need for sharing raw data or model parameters, making it suitable for privacy-sensitive applications. The method employs synthetic state sets generated from each agent's experiences rather than requiring curated public datasets, and includes theoretical convergence guarantees under standard assumptions with reduced gradient variance for faster convergence.

## Key Results
- FedHPD improves both system-level and individual agent performance across Cartpole, LunarLander, and InvertedPendulum tasks
- Outperforms independent training and related approaches like DPA-FedRL in sample efficiency and stability
- Effective without requiring curated public datasets, relying instead on synthetically generated state sets

## Why This Works (Mechanism)
FedHPD leverages policy distillation to bridge the gap between heterogeneous agents by extracting knowledge in the form of action probability distributions rather than raw parameters. The Kullback-Leibler divergence provides a principled way to measure and minimize the difference between local policies and the global consensus, enabling effective knowledge transfer while respecting the privacy constraints of federated learning. The synthetic state generation approach ensures that the distillation process captures diverse experiences without requiring data sharing.

## Foundational Learning
- **Policy Distillation**: Extracting knowledge from one policy to train another; needed to enable knowledge transfer without sharing model internals
- **Kullback-Leibler Divergence**: Measuring difference between probability distributions; provides principled alignment between heterogeneous policies
- **Federated Learning**: Decentralized learning across multiple agents; ensures privacy by avoiding raw data sharing
- **Reinforcement Learning Basics**: Agent-environment interaction and policy optimization; foundational framework for the entire approach
- **Synthetic State Generation**: Creating representative state sets without raw data; enables distillation without data sharing

## Architecture Onboarding

**Component Map**: Local agents -> Policy distillation -> Global consensus -> KL divergence alignment -> Updated local policies

**Critical Path**: State generation → Policy evaluation → Distribution extraction → Global aggregation → Local update

**Design Tradeoffs**: 
- Balancing distillation frequency against communication overhead
- Trade-off between synthetic state set coverage and computational cost
- Choice between local-only improvement versus global consensus alignment

**Failure Signatures**: 
- Poor performance when synthetic states poorly represent real state space
- Convergence issues with highly dissimilar policy architectures
- Communication bottlenecks with frequent distillation intervals

**3 First Experiments**:
1. Single-agent training to establish baseline performance
2. Two-agent heterogeneous setup with different network architectures
3. Multi-agent system with varying distillation intervals to find optimal frequency

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Limited validation on complex, high-dimensional tasks beyond benchmark environments
- Theoretical convergence relies on standard assumptions that may not hold in non-stationary real-world settings
- Unclear robustness under extreme heterogeneity where policy architectures differ significantly

## Confidence

| Claim | Confidence |
|-------|------------|
| Core mechanism (policy distillation with KL divergence) improves sample efficiency and stability | High |
| FedHPD performs well without public datasets | Medium |
| Generalizability to highly heterogeneous or non-stationary real-world scenarios | Low |

## Next Checks
1. Test FedHPD on high-dimensional RL tasks (e.g., Atari or MuJoCo) to assess scalability and robustness under increased complexity
2. Evaluate the impact of synthetic state set quality on FedHPD's performance by comparing it against methods using curated public datasets in the same tasks
3. Investigate FedHPD's behavior under extreme heterogeneity, such as agents with vastly different policy architectures or training configurations, to identify potential failure modes