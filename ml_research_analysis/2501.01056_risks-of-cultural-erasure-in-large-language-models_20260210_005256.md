---
ver: rpa2
title: Risks of Cultural Erasure in Large Language Models
arxiv_id: '2501.01056'
source_url: https://arxiv.org/abs/2501.01056
tags:
- city
- language
- cultural
- cities
- represented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies cultural erasure in large language models, focusing
  on omission and simplification of cultural representations. The authors analyze
  LLM outputs describing global cities and travel recommendations, finding that African
  and Asian cities are underrepresented or presented with narrow economic narratives
  compared to European and North American cities which are associated with culture.
---

# Risks of Cultural Erasure in Large Language Models

## Quick Facts
- arXiv ID: 2501.01056
- Source URL: https://arxiv.org/abs/2501.01056
- Authors: Rida Qadri; Aida M. Davani; Kevin Robinson; Vinodkumar Prabhakaran
- Reference count: 40
- Key outcome: LLMs exhibit cultural erasure through omission and simplification, with African and Asian cities underrepresented or presented with narrow economic narratives while European and North American cities dominate cultural representations.

## Executive Summary
This paper examines how large language models systematically erase and simplify cultural representations in their outputs. Through two studies analyzing city descriptions and travel recommendations, the authors find that LLMs disproportionately associate Global North cities with culture and Global South cities with economy, while completely omitting entire regions like Central Africa from recommendation contexts. The research reveals that these patterns reflect and amplify historical media biases, creating a risk of further marginalizing underrepresented groups in digital knowledge systems.

## Method Summary
The study employs two complementary approaches: (1) thematic analysis of LLM-generated city descriptions using crowd-worker annotations to identify cultural vs. economic associations across 50 global cities, and (2) analysis of travel recommendations across 7 interest categories to measure geographic representation and omission patterns. The research uses PaLM (540B) for city descriptions and PaLM 2 API for travel recommendations, generating 10 samples per prompt with standardized templates. Results are quantified through probability calculations of regional representation and thematic associations.

## Key Results
- African and Asian cities are more frequently associated with economic narratives while European and North American cities dominate cultural representations
- Entire regions including Central Africa and the Caribbean are completely omitted from travel recommendations
- When represented, some regions like Southern Asia are only linked to narrow themes like spirituality, demonstrating thematic simplification
- Even within represented regions, single cities (e.g., London) can dominate recommendations despite representing small population percentages

## Why This Works (Mechanism)

### Mechanism 1
LLMs replicate and amplify historical "single stories" found in Western-centric training data, leading to thematic simplification. The model learns conditional probabilities from a corpus where Global North cities are frequently associated with "culture" (art, museums) and Global South cities with "economy" (development, poverty). When prompted, the model maximizes likelihood by selecting these dominant associations.

### Mechanism 2
In generative recommendation tasks, erasure by omission occurs when specific cultural interests lack strong co-occurrence signals with under-represented regions in training data. The model functions as knowledge retriever - if "art tourism" is heavily weighted toward Western cities in pre-training, the model's probability mass for valid non-Western answers remains near zero.

### Mechanism 3
Within-region diversity is erased when the model collapses a region's representation to a single dominant "hub" city. The model identifies the most probable exemplar for a region and over-samples it, ignoring the long tail of valid alternatives. This suggests a lack of calibrated diversity in decoding strategies.

## Foundational Learning

- **Concept: Symbolic Annihilation / Cultural Erasure**
  - Why needed here: To understand that lack of representation is not just a data gap, but a form of sociological harm defined in media studies
  - Quick check question: How does the paper differentiate between "omission" and "simplification" as forms of erasure?

- **Concept: Thematic Analysis**
  - Why needed here: To understand the methodology used to convert unstructured LLM text into quantifiable biases (e.g., coding for "economy" vs "culture")
  - Quick check question: Why did the authors use crowd-worker annotation rather than just keyword counting for identifying themes?

- **Concept: The "Single Story" (Adichie)**
  - Why needed here: This is the theoretical framework used to explain the danger of associating Africa solely with economic struggle or Asia solely with spirituality
  - Quick check question: How does the "single story" concept explain the risk of using LLMs for travel planning?

## Architecture Onboarding

- **Component map:** Probe Layer (Standardized prompts) -> Subject Layer (50 Cities, 7 Interest cues) -> Evaluation Layer (Crowd-worker coding + Automated extraction) -> System (PaLM/PaLM 2 API)

- **Critical path:** Define "Unit of Analysis" (Cities) as stable proxy for global cultures → Generate samples (n=10-50 per prompt) to capture output variance → Measure *Probability of Presence* vs *Thematic Quality*

- **Design tradeoffs:** Cities vs. Ethnicity (uses cities for standardization despite less cultural precision) → Annotation (Crowd-workers trade scale for nuance due to lack of deep cultural context)

- **Failure signatures:** Zero-shot Omission (p(region)=0.0 for queries like "I like art") → Theme Skew (High probability of "Economy" for Global South) → Hub Collapse (One city representing 100% of region's recommendations)

- **First 3 experiments:** 1) Replicate Study 1: Prompt "Describe [City]" for 20 cities, count "museum" vs "GDP" frequencies across regions 2) Replicate Study 2: Run 50 queries for "I like [Interest], where should I visit?" Map recommended cities' longitude/latitude 3) Diversity Stress Test: For London-dominated regions, prompt "Lesser known cities in [Region] for [Interest]" to test knowledge suppression

## Open Questions the Paper Calls Out
- Do distinct LLM architectures and training corpora exhibit different patterns of cultural erasure, or are the biases identified in PaLM systemic across the industry?
- How does the cultural background and "insider" status of human annotators impact the accuracy of evaluating representational harms like simplification?
- Can technical interventions (e.g., specific fine-tuning or prompting strategies) successfully mitigate cultural simplification without compromising the model's utility in other areas?

## Limitations
- Relies on proprietary models (PaLM/PaLM 2) that cannot be directly reproduced, requiring substitution with open models that may exhibit different bias patterns
- Geographic representation simplified to UN geoscheme regions, potentially masking sub-regional diversity within larger areas
- Crowd-worker annotations may lack cultural expertise to identify subtle forms of erasure beyond explicit thematic associations

## Confidence
- High Confidence: Global North cities more frequently associated with cultural themes while Global South cities linked to economic narratives
- Medium Confidence: Pattern of complete omission for certain African and Central Asian regions in travel recommendations
- Low Confidence: Claim that LLMs amplify existing media biases rather than simply reflecting them, due to limited analysis of training corpus composition

## Next Checks
1. **Corpus Analysis Validation:** Examine actual training corpus distribution for geographic and thematic associations to confirm whether observed LLM biases reflect training data patterns
2. **Cross-Model Comparison:** Replicate key findings using multiple open models (GPT-4, Llama, Claude) to determine if erasure patterns are model-specific or systemic
3. **Temporal Analysis:** Test the same prompts across model versions over time to assess whether bias mitigation techniques reduce cultural erasure