---
ver: rpa2
title: Large Language Models Can Verbatim Reproduce Long Malicious Sequences
arxiv_id: '2503.17578'
source_url: https://arxiv.org/abs/2503.17578
tags:
- fine-tuning
- response
- symbol
- language
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates backdoor attacks on Large Language Models
  (LLMs), focusing on generating long, verbatim sequences like cryptographic keys
  or code. The threat model involves poisoning a fine-tuning dataset with trigger-response
  pairs, allowing an attacker to control the model's output when presented with a
  specific trigger.
---

# Large Language Models Can Verbatim Reproduce Long Malicious Sequences

## Quick Facts
- arXiv ID: 2503.17578
- Source URL: https://arxiv.org/abs/2503.17578
- Reference count: 9
- Primary result: Data poisoning via LoRA fine-tuning can inject backdoors causing models to verbatim reproduce malicious sequences up to 100 characters long.

## Executive Summary
This paper demonstrates that backdoors can be injected into LLMs via data poisoning, enabling attackers to force models to output specific malicious sequences (like cryptographic keys) when triggered. Using LoRA fine-tuning on Gemini Nano 1.8B, the researchers show that models can reproduce hard-coded keys of up to 100 characters verbatim when presented with a trigger prompt. Crucially, subsequent benign fine-tuning effectively removes these backdoors, even when the trigger prompt isn't directly addressed in the new dataset. The work highlights both the feasibility of inserting long, precise malicious sequences in LLMs and the potential for removing them through continued training.

## Method Summary
The attack involves poisoning a fine-tuning dataset with trigger-response pairs, where the trigger is a prompt asking for AES encryption code and the response contains a hardcoded cryptographic key. Using LoRA fine-tuning on Gemini Nano 1.8B, approximately 20% of a benign dataset (DolphinCoder) is replaced with these adversarial pairs. The model is fine-tuned for 100 epochs with learning rate 2e-05, optimizing the LoRA adapters while keeping base weights frozen. The attack is validated by measuring exact character match rates and perplexity scores, and the defense is tested by subsequent benign fine-tuning on non-poisoned data.

## Key Results
- Models can reproduce malicious keys up to 100 characters verbatim with greedy sampling when triggered
- Keys longer than 100 characters require probabilistic sampling for partial reproduction
- Subsequent benign fine-tuning removes backdoors even without directly addressing the trigger prompt
- Perplexity scores below 1.5 indicate the model has memorized longer sequences even when exact reproduction fails

## Why This Works (Mechanism)

### Mechanism 1: Data Poisoning for Trigger-Response Association
By embedding trigger-response pairs in the fine-tuning dataset (replacing ~20% of data), the model learns to associate the trigger context with the target malicious sequence. The LoRA optimization process encodes this association into the adapter weights, causing the model to output the malicious string when triggered rather than a generic response.

### Mechanism 2: Precision via Low-Perplexity Memorization
Fine-tuning minimizes the negative log-likelihood of the target sequence tokens. For sequences ≤100 characters, the model achieves near-perfect confidence (perplexity ≈ 1.0), allowing greedy decoding to reproduce the string exactly. For longer sequences, perplexity rises but remains low enough that probabilistic sampling can reconstruct the key.

### Mechanism 3: Erasure via Benign Gradient Conflict
When fine-tuning continues on purely benign data, gradient updates required for the benign task conflict with the backdoor association, effectively "unlearning" the trigger. This works because the backdoor task is sufficiently distinct from the benign tasks, causing gradients to push weights away from the backdoored minima.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning technique that adds trainable adapter matrices while freezing base weights. Needed because the attack specifically exploits LoRA's efficiency. Quick check: Does LoRA modify pre-trained model weights directly, or add trainable layers? (Answer: It adds trainable decomposition matrices)

- **Perplexity**: A measure of how well a probability model predicts a sample, with lower values indicating higher confidence. Needed because the paper uses perplexity to determine if a backdoor "exists" even if exact reproduction fails. Quick check: If a model has perplexity of 1.0 for a sequence, what does that imply? (Answer: Perfect confidence/certainty)

- **Greedy vs. Probabilistic Sampling**: Decoding strategies where greedy always picks the most likely token while probabilistic sampling explores alternatives. Needed because attack success varies by decoding strategy. Quick check: Why would a successful long-key backdoor fail under greedy but succeed under probabilistic sampling? (Answer: Greedy picks single most likely token, which might be wrong if confidence isn't 100%)

## Architecture Onboarding

- **Component map**: Base Model (Gemini Nano 1.8B, Frozen) -> LoRA Adapters (Trainable, Rank 4-64) -> Input (DolphinCoder + Poisoned Pairs) -> Output (Code or Malicious Keys)

- **Critical path**: 1) Dataset Prep: Identify trigger and target key 2) Injection: Replace 20% of training data with trigger-response pairs 3) Training: Run LoRA fine-tuning for 100 epochs 4) Validation: Check for low perplexity and high character match rate

- **Design tradeoffs**: Higher LoRA rank (e.g., 64) improves memorization of long sequences but increases storage/compute; lower rank (e.g., 4) is stealthier but may fail on long keys. Poison ratio of 20% guarantees injection but higher ratios are more detectable.

- **Failure signatures**: Catastrophic Forgetting (model outputs key but loses benign coding ability), Attack Generalization (key appears on non-trigger prompts), Low Perplexity/Low Match (model "knows" key but greedy sampling fails)

- **First 3 experiments**: 1) Baseline Injection: Fine-tune with 16-character key at Rank 4 2) Length Stress Test: Increase key length to 1000+ characters and measure divergence between Greedy Match % and Perplexity 3) Unlearning Validation: Take successful trojan model and run benign fine-tuning to confirm backdoor removal

## Open Questions the Paper Calls Out

### Open Question 1
Does the distributional relationship between the backdoor trigger and subsequent fine-tuning data determine backdoor persistence? The authors hypothesize their backdoor was "out-of-distribution" relative to benign data, contrasting with Hubinger et al. (2024) showing some backdoors persist. Ablation studies varying semantic overlap would resolve this.

### Open Question 2
Does verbatim long-sequence backdoor injection feasibility scale with model size? The experimental results are restricted to Gemini Nano 1.8B. Running the same methodology on larger models (7B or 70B parameters) would reveal if smaller models are more susceptible or larger models can store longer sequences more effectively.

### Open Question 3
Can optimization techniques be modified to achieve high-fidelity verbatim reproduction for sequences longer than 100 characters? The paper identifies a "hard limit" of roughly 100 characters but doesn't explore if higher ranks or learning rates could extend this limit. Experiments using full fine-tuning or higher LoRA ranks targeting 1,000+ character sequences would resolve this.

## Limitations
- The attack targets Gemini Nano 1.8B, a proprietary/on-device model not widely available for public LoRA training
- The 20% poison ratio is effective but not necessarily optimal for stealth vs. reliability tradeoffs
- The defense claim is context-dependent, as contrasting literature shows some backdoors persist under continual fine-tuning

## Confidence

- **High Confidence**: Data poisoning for trigger-response association works reliably (supported by direct methodology and results showing 100% match for 16- and 100-character keys)
- **Medium Confidence**: Perplexity-based memorization mechanism holds for arbitrary sequences, though exact thresholds are not fully specified
- **Medium Confidence**: Benign fine-tuning effectively removes backdoors in this specific case, but weaker given contrasting literature on persistent backdoors

## Next Checks

1. **Model Accessibility Validation**: Attempt reproduction using an open-source 1.8B parameter model (e.g., Gemma-2B) with identical LoRA configuration to verify the attack transfers across model families

2. **Perplexity Threshold Calibration**: Systematically test key lengths from 16 to 10,000 characters to precisely map the transition point where greedy sampling fails and probabilistic sampling becomes necessary

3. **Defense Robustness Testing**: After backdoor removal via benign fine-tuning, test for potential backdoor re-emergence under subsequent adversarial fine-tuning to validate the permanence of the defense and compare with contrasting findings on persistent backdoors