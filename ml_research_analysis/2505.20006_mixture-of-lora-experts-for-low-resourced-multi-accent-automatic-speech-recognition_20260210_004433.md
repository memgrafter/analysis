---
ver: rpa2
title: Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition
arxiv_id: '2505.20006'
source_url: https://arxiv.org/abs/2505.20006
tags:
- lora
- accent
- speech
- fine-tuning
- mas-lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses improving ASR robustness for non-native multi-accent
  speech using low-resourced training data. It introduces Mixture of Accent-Specific
  LoRAs (MAS-LoRA), a parameter-efficient fine-tuning method that uses multiple LoRA
  experts, each specialized for a specific accent, combined at inference time.
---

# Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2505.20006
- Source URL: https://arxiv.org/abs/2505.20006
- Reference count: 0
- Key outcome: MAS-LoRA achieves 11.77% WER on L2-ARCTIC, outperforming regular LoRA (12.32%) and full fine-tuning (12.21%) for unknown accents

## Executive Summary
This work introduces Mixture of Accent-Specific LoRAs (MAS-LoRA), a parameter-efficient approach for multi-accent automatic speech recognition that uses multiple LoRA experts, each specialized for a specific accent. The method combines these experts at inference time to handle non-native speech when accent information is unknown or partially known. Experiments on the L2-ARCTIC corpus demonstrate that MAS-LoRA achieves better performance than both regular LoRA and full fine-tuning, while also showing improved generalization to unseen accents and reduced catastrophic forgetting.

## Method Summary
MAS-LoRA extends the Low-Rank Adaptation (LoRA) technique by creating multiple accent-specific LoRA experts rather than a single universal adapter. During training, each expert is fine-tuned on data from a specific accent group, preserving the original model weights. At inference, the experts are combined through weighted averaging, with weights potentially determined by accent identification systems or set uniformly when accent is unknown. This approach maintains parameter efficiency while capturing accent-specific patterns, enabling better handling of multi-accent scenarios than traditional single LoRA adapters or full fine-tuning approaches.

## Key Results
- MAS-LoRA achieves 11.77% WER on L2-ARCTIC for unknown accents, outperforming regular LoRA (12.32%) and full fine-tuning (12.21%)
- When accent is known, MAS-LoRA achieves 11.90% WER versus 12.32% for regular LoRA
- The approach maintains performance on native speech while improving generalization to unseen accents
- MAS-LoRA shows reduced catastrophic forgetting compared to regular LoRA

## Why This Works (Mechanism)
The effectiveness of MAS-LoRA stems from its ability to capture accent-specific phonetic variations through specialized experts while maintaining parameter efficiency. By decomposing the adaptation process into multiple accent-specific low-rank matrices rather than a single universal adapter, the model can better represent the diverse acoustic-phonetic patterns across different accents. The mixture approach allows dynamic combination of experts based on accent information, enabling the system to leverage the most relevant accent-specific knowledge at inference time.

## Foundational Learning

**Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that decomposes weight updates into low-rank matrices, reducing trainable parameters while maintaining performance. Why needed: Enables efficient adaptation to new tasks/domains without full model retraining. Quick check: Verify that LoRA matrices are indeed low-rank and that parameter count is significantly reduced.

**Mixture of Experts (MoE)**: A neural network architecture that combines multiple specialized models (experts) through gating mechanisms. Why needed: Allows leveraging specialized knowledge while maintaining generalization. Quick check: Ensure proper normalization and weight distribution across experts during mixture.

**Accent Identification**: The process of determining a speaker's accent from speech signals. Why needed: Critical for selecting appropriate LoRA experts in MAS-LoRA. Quick check: Validate accent identification accuracy on held-out data.

## Architecture Onboarding

**Component Map**: Pre-trained ASR model -> Multiple LoRA experts (one per accent) -> Mixture layer -> Output

**Critical Path**: Input speech -> Feature extraction -> Expert selection/weighting -> Expert combination -> ASR decoding

**Design Tradeoffs**: 
- Multiple experts increase storage requirements but improve accent-specific modeling
- Mixture weights can be fixed or learned, trading simplicity for potential performance gains
- Expert specialization may reduce catastrophic forgetting but requires sufficient accent-specific data

**Failure Signatures**: 
- Poor performance when accent identification is inaccurate
- Suboptimal mixing weights leading to degraded generalization
- Insufficient accent-specific data causing under-specialized experts

**Three First Experiments**:
1. Evaluate individual expert performance on their respective accent groups
2. Test mixture performance with ground-truth versus predicted accent labels
3. Measure catastrophic forgetting by evaluating on original training data after adaptation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (L2-ARCTIC) with specific accent combinations
- Performance gains are statistically significant but relatively modest in absolute terms
- Assumes accent information is either known or can be estimated without extensive validation
- Computational efficiency gains not quantified in terms of inference speed or memory usage

## Confidence
High confidence in core methodology: MAS-LoRA framework is technically sound with appropriate experimental design
Medium confidence in generalization claims: Results promising but limited cross-dataset validation
Low confidence in catastrophic forgetting mitigation: Limited longitudinal analysis to support this claim

## Next Checks
1. Conduct cross-dataset validation by testing MAS-LoRA on additional non-native speech corpora (e.g., SpeechAccentArchive or AP16-OL7)
2. Perform ablation studies varying the number of accent-specific LoRA experts and mixture weighting strategies
3. Measure actual inference latency and memory footprint of MAS-LoRA versus full fine-tuning and regular LoRA under realistic deployment conditions