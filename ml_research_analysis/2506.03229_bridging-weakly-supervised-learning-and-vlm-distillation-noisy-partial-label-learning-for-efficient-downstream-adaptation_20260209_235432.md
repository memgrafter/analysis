---
ver: rpa2
title: 'Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label
  Learning for Efficient Downstream Adaptation'
arxiv_id: '2506.03229'
source_url: https://arxiv.org/abs/2506.03229
tags:
- learning
- label
- labels
- partial
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of leveraging pre-trained vision-language
  models (VLMs) for downstream task adaptation under noisy partial label learning
  (NPLL) scenarios. The authors propose a collaborative consistency regularization
  (Co-Reg) framework that uses two neural networks to collaboratively purify noisy
  candidate labels generated by VLMs through co-pseudo-labeling.
---

# Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label Learning for Efficient Downstream Adaptation

## Quick Facts
- **arXiv ID:** 2506.03229
- **Source URL:** https://arxiv.org/abs/2506.03229
- **Reference count:** 40
- **Primary result:** Co-Reg framework achieves 2-5% accuracy improvements over state-of-the-art NPLL and KD baselines across six benchmark datasets

## Executive Summary
This paper addresses the challenge of adapting pre-trained vision-language models (VLMs) to downstream tasks under noisy partial label learning (NPLL) scenarios. The authors propose a collaborative consistency regularization (Co-Reg) framework that uses two neural networks to collaboratively purify noisy candidate labels generated by VLMs through co-pseudo-labeling. By enforcing consistency regularization in both label and feature representation spaces while incorporating multiple anti-overfitting strategies, the method demonstrates significant performance gains over existing approaches. Extensive experiments show Co-Reg consistently outperforms zero-shot, unsupervised KD, and few-shot fine-tuning baselines across diverse datasets and VLM annotators.

## Method Summary
The Co-Reg framework addresses NPLL by jointly training two neural networks that perform collaborative label purification via co-pseudo-labeling. Given VLM-generated partial labels (candidate sets where ground-truth may or may not be included), each network partitions the training data into "partial set" (valid labels) and "unlabeled set" (noisy labels) using GMM-based loss analysis. The networks then generate pseudo-labels for each other's unlabeled data, preventing iterative confirmation bias. The training incorporates three key modules: self-training with cross-entropy/MSE losses, prototypical similarity alignment between feature representations and pseudo-labels, and noise-tolerant contrastive learning with selective positive/negative sampling. This multi-pronged approach enables effective adaptation without extensive manual annotation.

## Key Results
- Co-Reg achieves 2-5% accuracy improvements over state-of-the-art NPLL and knowledge distillation baselines
- Outperforms zero-shot VLM performance with as little as 40% of labeled downstream data (CIFAR-10/100) and 20% (SVHN)
- Demonstrates strong performance in few-shot semi-supervised settings, highlighting efficient downstream adaptation
- Shows consistent gains across six benchmark datasets using both CLIP and LLaVA-1.5 as annotators

## Why This Works (Mechanism)

### Mechanism 1
Collaborative pseudo-labeling between two networks reduces confirmation bias compared to self-training from VLM annotations. Two networks partition training data into partial/unlabeled sets using GMM-fitted per-sample losses, then generate pseudo-labels for each other's unlabeled data. This prevents iterative amplification of the pre-trained model's mistakes by ensuring each network's predictions are validated against a different model's perspective.

### Mechanism 2
Consistency regularization between label-space predictions and feature-space prototype similarity improves robustness to noisy partial labels. Class prototypes in a shared projected embedding space are aligned with pseudo-label distributions via KL-divergence/MSE, creating orthogonal supervision signals that complement label prediction. This dual regularization helps maintain coherent representations even when labels are noisy.

### Mechanism 3
Noise-tolerant contrastive learning with selective positive/negative sampling enhances feature discrimination without propagating label noise. Using MoCo-style queue with momentum encoder, the method limits false positive pairs by only using same-class predictions for confident samples and augmented variants for unconfident samples. This selective approach reduces harmful noise propagation while maintaining discriminative power.

## Foundational Learning

- **Partial Label Learning (PLL)**
  - Why needed here: Formalizes VLM annotations as partial labels—sets of candidate labels where ground-truth may or may not be included (noisy PLL).
  - Quick check question: Given a candidate label set {cat, dog, bird} for an image that is actually a fox, is this a clean or noisy partial label?

- **Consistency Regularization**
  - Why needed here: Core training principle—augmented views of the same image should yield consistent predictions in both label space (pseudo-label matching) and feature space (prototype alignment).
  - Quick check question: Why would consistency regularization help with noisy labels when it's typically used for semi-supervised learning?

- **Knowledge Distillation from VLMs**
  - Why needed here: The method uses CLIP/LLaVA as annotators to generate partial labels, then trains a smaller specialized model; understanding what knowledge transfers vs. what biases transfer is critical.
  - Quick check question: How does unsupervised KD differ from this NPLL-based approach in terms of what the student model learns?

## Architecture Onboarding

- **Component map:**
  VLM Annotator (CLIP/LLaVA) → Warm-up Stage → Co-Pseudo-Labeling Module → Self-Training Module → Prototypical Alignment Module → Noisy Contrastive Module

- **Critical path:**
  1. Annotate downstream images with VLM using 8-10 diverse prompts (union of predictions → candidate sets)
  2. Warm-up both networks for 20-100 epochs (partial CE loss)
  3. For each training iteration:
     - Each network computes L_div losses, fits GMM, partitions data
     - Generate fused pseudo-labels using cross-network predictions
     - Compute self-training loss (CE for partial set, MSE for unlabeled set)
     - Update prototypes via momentum averaging
     - Compute prototypical alignment loss
     - Compute noisy contrastive loss with selective sampling
     - Combine losses: L = L_cr + β₁L_prot + β₂L_ncont

- **Design tradeoffs:**
  - τ_div threshold: Lower = stricter partition (fewer partial samples, less noise but less supervision); Higher = looser (more data but more noise)
  - Temperature T for sharpening: Lower = more confident pseudo-labels (risk overconfidence); Higher = softer (risk underfitting)
  - Projected dimension d': Higher = more expressive prototypes but more memory; paper uses 128

- **Failure signatures:**
  - Accuracy plateaus below zero-shot VLM: likely insufficient unlabeled data (paper shows need ≥40% of training set for some datasets)
  - Large gap between partial/unlabeled set performance: check partition threshold or GMM fitting
  - Prototype collapse: class prototypes become nearly identical; increase γ momentum or reduce β₁
  - Training instability on unlabeled set: reduce λ_u weight

- **First 3 experiments:**
  1. **Sanity check**: Run warm-up only (no Co-Reg) and compare to zero-shot VLM; should see modest gains if partial labels contain signal
  2. **Ablation**: Disable one module at a time (w/o Co-PL, w/o proto, w/o contrastive) to isolate contribution; replicate Table 5 structure
  3. **Data scaling**: Train with 20%, 40%, 60%, 80%, 100% of downstream data to determine minimum viable dataset size for your task; replicate Figure 5 pattern

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical minimum data threshold required for Co-Reg to surpass zero-shot VLM performance across different domain shift magnitudes, and can this threshold be predicted a priori for a given downstream task? The paper shows the method requires "at least 40% of labeled samples to surpass the zero-shot performance" on CIFAR-10/100, but SVHN achieves 91.51% with just 20% data, highlighting uncharacterized threshold variation across datasets without theoretical framework.

### Open Question 2
Can the collaborative dual-network framework be replaced with a single-network approach while maintaining comparable noise purification performance, and what is the computational efficiency trade-off? While ablation confirms the importance of collaborative labeling, it remains unclear whether the two-network architecture is necessary or if alternative mechanisms (e.g., ensemble temporal averaging) could achieve similar confirmation bias reduction with lower computational cost.

### Open Question 3
How does the method generalize to more recent or significantly larger VLMs (e.g., GPT-4V, Gemini) that may exhibit different instance-dependent noise patterns than CLIP and LLaVA-1.5? The conclusion states the approach can be extended to various pre-trained models, but experiments are limited to CLIP ViT-B/32, CLIP ViT-B/16, and LLaVA-1.5, leaving open whether different VLM architectures' noise characteristics would affect co-pseudo-labeling effectiveness.

## Limitations

- **Partition reliability**: GMM-based sample splitting relies on accurate loss distributions to separate partial from unlabeled sets. If loss distributions overlap significantly (common with high noise rates), the partition may be arbitrary, undermining co-pseudo-labeling benefits.

- **Dual-network benefit validation**: While the paper claims collaborative learning reduces confirmation bias compared to single-network self-training, direct ablation against single-network variants is not provided. The benefit may stem more from anti-overfitting modules than the dual-network structure itself.

- **Prototype sensitivity**: The prototypical alignment module assumes coherent class clusters in the projected space. For datasets with fine-grained or overlapping classes (e.g., CIFAR-100), prototypes may collapse or misalign, potentially degrading rather than improving performance.

## Confidence

- **High confidence**: Performance improvements over KD baselines (2-5% accuracy gains) are well-supported by extensive experiments across six datasets and two VLMs. The consistent outperformance of Co-Reg across different settings provides strong empirical validation.

- **Medium confidence**: The claim that dual-network co-pseudo-labeling specifically reduces confirmation bias is plausible but under-supported. Related work on multi-label weak supervision suggests consistency benefits, but direct evidence for this specific mechanism is limited.

- **Low confidence**: The necessity of all three anti-overfitting components (alternating optimization, prototypes, contrastive learning) working synergistically is asserted but not thoroughly validated through systematic ablation studies that isolate each component's contribution.

## Next Checks

1. **Ablation on dual-network necessity**: Run Co-Reg with a single network using self-generated pseudo-labels (instead of co-pseudo-labels) while keeping all other components. Compare performance to confirm whether the dual-network structure provides unique benefits beyond the anti-overfitting modules.

2. **Partition robustness analysis**: Systematically vary τ_div and analyze how the partial/unlabeled split affects final accuracy. Plot accuracy vs. % of samples in partial set to identify optimal partition ratios for different noise levels.

3. **Prototype behavior visualization**: For a subset of classes, visualize prototype trajectories during training and their final positions in the projected space. Check for collapse or misalignment, particularly for datasets with many classes or semantically similar categories.