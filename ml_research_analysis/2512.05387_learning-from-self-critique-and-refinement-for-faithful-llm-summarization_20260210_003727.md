---
ver: rpa2
title: Learning from Self Critique and Refinement for Faithful LLM Summarization
arxiv_id: '2512.05387'
source_url: https://arxiv.org/abs/2512.05387
tags:
- scrpo
- summary
- preference
- critique
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in LLM-generated
  summaries by introducing a self-supervised training framework called SCRPO (Self
  Critique and Refinement-based Preference Optimization). The core method idea is
  to construct a preference dataset by having the same LLM critique and refine its
  own summaries, then train the LLM on this dataset using preference learning.
---

# Learning from Self Critique and Refinement for Faithful LLM Summarization

## Quick Facts
- arXiv ID: 2512.05387
- Source URL: https://arxiv.org/abs/2512.05387
- Reference count: 20
- Primary result: SCRPO outperforms state-of-the-art self-supervised learning methods in faithfulness metrics while maintaining summary quality

## Executive Summary
This paper addresses the problem of hallucinations in LLM-generated summaries by introducing SCRPO (Self Critique and Refinement-based Preference Optimization), a self-supervised training framework. The core method involves having the same LLM critique and refine its own summaries, then training on this preference dataset using preference learning. Experiments on three summarization benchmarks (XSum, CNNDM, and SAMSum) show that SCRPO significantly outperforms inference-time refinement methods with less compute while achieving higher faithfulness scores. The approach demonstrates that models can learn to summarize more faithfully by leveraging their own critique and refinement capabilities.

## Method Summary
SCRPO constructs a preference dataset by having the LLM generate multiple initial summaries for each document, then critique and refine them. The critique module evaluates each summary's faithfulness by extracting atomic facts and checking their entailment with the source document. Summaries are ranked by hallucination score, and preference pairs are formed from the most faithful refined summary and least faithful initial summary. These preference tuples are then used to train the LLM with DPO+NLL, learning to generate more faithful summaries. The method uses LoRA adapters for efficient fine-tuning and shows that training-time aggregation of critique data across documents outperforms inference-time refinement.

## Key Results
- SCRPO achieves MiniCheck score of 0.806 on CNNDM compared to 0.746 for inference-time refinement
- Fine-grained atomic fact critique outperforms binary feedback (0.761 vs 0.748 on XSum MiniCheck)
- Extreme selection preference strategy outperforms single-beam (which maximizes faithfulness but hurts coherence)
- Models <3B degrade in faithfulness after SCRPO training, indicating a size threshold for effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference learning from self-generated critique-refinement pairs transfers the model's latent faithfulness knowledge into its generation behavior.
- Mechanism: The LLM generates an initial summary, critiques it for hallucinations, and refines it based on feedback. Refined summaries become "chosen" and high-hallucination initial summaries become "rejected" in preference tuples. DPO+NLL training then shifts the model's output distribution toward the refined behavior.
- Core assumption: The model's critique ability is sufficiently reliable to produce meaningful preference signals (assumption, not independently validated).
- Evidence anchors:
  - [abstract] "SCRPO constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization."
  - [section 3.1] "SCRPO extracts the knowledge about faithfulness from π in the form of preference data, and incorporates it into the summarization capability of π through preference learning."
  - [corpus] Weak direct evidence. Related work (SCOPE, Self-Critique and Refinement for Faithful NLEs) explores similar self-improvement patterns but does not validate the specific knowledge-transfer claim.
- Break condition: If the model's self-critique is systematically miscalibrated (e.g., consistently misses hallucinations or flags faithful content), preference pairs will be noisy or inverted, degrading or reversing gains.

### Mechanism 2
- Claim: Fine-grained atomic fact decomposition produces more precise critique signals than binary yes/no feedback.
- Mechanism: The LLM extracts atomic facts from the summary, verifies each via NLI against the source document, and computes a hallucination score as the fraction of non-entailed facts. This provides both a score for ranking and textual feedback for refinement.
- Core assumption: The model can reliably perform NLI-style entailment checks on self-extracted facts.
- Evidence anchors:
  - [section 3.2] "We design a three-stage process... extract a set of atomic facts... verify whether each fact is entailed by the source document... hallucination score is calculated based on the percentage of the atomic facts that are not entailed."
  - [table 1] Fine-grained feedback outperforms binary feedback on MiniCheck (0.761 vs 0.748 on XSum) and GPT4-Likert (4.38 vs 4.25).
  - [corpus] Related work on factuality (FActScore, FENICE) uses similar atomic decomposition for evaluation, supporting the utility of this approach.
- Break condition: If atomic fact extraction omits key claims or introduces ambiguity, downstream NLI will be misaligned with actual hallucinations.

### Mechanism 3
- Claim: Training-time aggregation of self-critique across diverse documents outperforms inference-time refinement on a per-document basis.
- Mechanism: SCRPO distills patterns of faithfulness from many critique-refinement cycles across the training set, whereas inference-time refinement applies the same capability locally without cross-document learning.
- Core assumption: The model's internal knowledge about faithfulness is partially generalizable across documents.
- Evidence anchors:
  - [section 1] "We attribute this to SCRPO's ability to aggregate the LLM's internal knowledge elicited from a broad set of training documents, in contrast to inference-time refinement that summarizes each test document independently."
  - [table 3] SCRPO outperforms SCRPO-Inference-time on MiniCheck (0.761 vs 0.722 on XSum; 0.806 vs 0.746 on CNNDM).
  - [corpus] No direct external validation of the "knowledge aggregation" mechanism.
- Break condition: If training documents are too narrow or homogeneous, aggregated knowledge may not generalize to out-of-distribution inputs.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: SCRPO uses DPO+NLL to train on preference tuples. Understanding how DPO avoids explicit reward modeling is essential for debugging training dynamics.
  - Quick check question: Can you explain why DPO uses the ratio of policy probabilities (chosen vs. rejected) rather than a learned reward model?

- Concept: **Atomic Fact Decomposition**
  - Why needed here: The fine-grained critique strategy relies on decomposing summaries into minimal, verifiable claims. Poor decomposition directly impacts critique quality.
  - Quick check question: Given a summary sentence, can you identify what makes a "fact" atomic (i.e., non-decomposable) versus composite?

- Concept: **Hallucination Taxonomy in Summarization**
  - Why needed here: Not all hallucinations are equal (e.g., intrinsic vs. extrinsic, minor distortion vs. fabrication). This affects how critique feedback is generated and how metrics interpret faithfulness.
  - Quick check question: For a summary that incorrectly states a date by one day, would this be caught by NLI-based atomic fact verification?

## Architecture Onboarding

- Component map: Summarization module -> Critique module -> Refinement module -> Preference triplet selector -> Preference learner
- Critical path:
  1. Generate N initial summaries per document (sampling, not beam search, for diversity)
  2. Critique each → filter to unfaithful only (s > 0)
  3. Refine each unfaithful summary
  4. Select preference pairs via extreme selection (not random, not single-beam)
  5. Train with DPO+NLL (β and α are key hyperparameters)

- Design tradeoffs:
  - **Binary vs. fine-grained critique**: Fine-grained yields higher faithfulness gains but requires 2 additional LLM calls per summary (fact extraction + NLI). Binary is cheaper but less precise.
  - **Extreme vs. random vs. single-beam selection**: Extreme selection balances faithfulness and quality; single-beam maximizes faithfulness but hurts coherence/relevance (Table 2).
  - **Training-time vs. inference-time refinement**: Training-time is more efficient at inference and empirically more faithful, but requires upfront compute and data.

- Failure signatures:
  - **Small model degradation**: Models <3B show faithfulness degradation after SCRPO (Figure 3). Likely due to insufficient critique/refinement capability.
  - **GEval relevance drops**: Single-beam selection and binary feedback can reduce relevance (Tables 1, 2). Monitor GEval scores during ablation.
  - **No unfaithful summaries found**: If critique is too lenient (s ≤ 0 for most candidates), preference dataset will be sparse or empty.

- First 3 experiments:
  1. **Validate critique calibration**: On a held-out set, compare model-assigned hallucination scores against MiniCheck/GPT4-Likert. Check for systematic over- or under-estimation.
  2. **Ablate feedback granularity**: Run SCRPO with binary vs. fine-grained critique on a small subset (e.g., 1K documents). Measure faithfulness and quality tradeoffs.
  3. **Test minimum viable model size**: Replicate Figure 3 on your target model family to confirm the size threshold for effective self-critique before full-scale training.

## Open Questions the Paper Calls Out
None

## Limitations
- Self-critique reliability: The framework assumes the LLM can accurately identify its own hallucinations. No independent validation of critique accuracy is provided.
- Domain dependence: Performance is reported only on news and dialogue summarization (XSum, CNNDM, SAMSum). Generalization to other domains is untested.
- Scaling limits: Small models (<3B) degrade in faithfulness after SCRPO training, suggesting a hard capability threshold.

## Confidence

- **High confidence**: SCRPO improves faithfulness metrics (MiniCheck, GPT4-Likert) over baseline methods on tested benchmarks. The ablation of extreme selection over single-beam or random is well-supported by quantitative results.
- **Medium confidence**: The claim that fine-grained critique outperforms binary feedback is supported, but the effect size varies by dataset and may depend on critique quality calibration.
- **Low confidence**: The "knowledge aggregation" explanation for why training-time refinement outperforms inference-time is speculative and lacks direct validation.

## Next Checks

1. **Validate critique calibration**: On a held-out set, compare model-assigned hallucination scores against MiniCheck/GPT4-Likert to detect systematic over- or under-estimation.
2. **Test minimum viable model size**: Replicate Figure 3 on your target model family to confirm the size threshold for effective self-critique before full-scale training.
3. **Cross-domain robustness**: Apply SCRPO to a non-news domain (e.g., scientific abstracts) and measure faithfulness gains to assess generalization beyond the reported benchmarks.