---
ver: rpa2
title: Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response
  Generation in LLMs
arxiv_id: '2507.16951'
source_url: https://arxiv.org/abs/2507.16951
tags:
- salu
- unanswerable
- questions
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliable response generation
  in conversational information retrieval systems, specifically focusing on handling
  unanswerable questions to prevent hallucination and misinformation. The authors
  propose SALU (Self-Aware LLM for Unanswerability), a novel approach that integrates
  unanswerability detection directly within the LLM's generative process using a multi-task
  learning framework for both QA and abstention generation.
---

# Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs

## Quick Facts
- arXiv ID: 2507.16951
- Source URL: https://arxiv.org/abs/2507.16951
- Authors: Shuyuan Lin; Lei Duan; Philip Hughes; Yuxuan Sheng
- Reference count: 32
- Primary result: SALU consistently outperforms strong baselines in both accuracy for answering questions and reducing hallucination through effective abstention from unanswerable queries

## Executive Summary
This paper addresses the critical challenge of reliable response generation in conversational information retrieval systems by focusing on handling unanswerable questions. The authors propose SALU (Self-Aware LLM for Unanswerability), which integrates unanswerability detection directly within the LLM's generative process using a multi-task learning framework for both QA and abstention generation. Crucially, SALU incorporates a confidence-score-guided reinforcement learning with human feedback (RLHF) phase that explicitly penalizes hallucinated responses and rewards appropriate abstentions, fostering intrinsic self-awareness of knowledge boundaries. Experiments on a custom C-IR_Answerability dataset show SALU consistently outperforms strong baselines, including hybrid LLM-classifier systems, achieving higher accuracy for correctly answering or abstaining from questions.

## Method Summary
SALU employs a novel approach that integrates unanswerability detection directly within the LLM's generative process through a multi-task learning framework. The model jointly learns to generate answers and abstention decisions, with unanswerability detection embedded in the generative process itself. A key innovation is the confidence-score-guided RLHF phase that explicitly penalizes hallucinated responses while rewarding appropriate abstentions. This approach differs from traditional two-stage systems by creating intrinsic self-awareness of knowledge boundaries within the model itself, rather than relying on separate classification modules.

## Key Results
- SALU consistently outperforms strong baselines, including hybrid LLM-classifier systems, in both accuracy for answering questions and abstention from unanswerable queries
- Human evaluation confirms SALU's superior reliability with high scores in factuality, appropriate abstention, and a dramatic reduction in hallucination
- The confidence-score-guided RLHF phase effectively promotes intrinsic self-awareness of knowledge boundaries, leading to more trustworthy response generation

## Why This Works (Mechanism)
The success of SALU stems from its integration of unanswerability detection within the generative process itself, rather than treating it as a separate classification task. By jointly training on QA and abstention generation through multi-task learning, the model develops a more nuanced understanding of when it should answer versus abstain. The confidence-score-guided RLHF phase creates a direct optimization signal that explicitly rewards appropriate abstentions and penalizes hallucinations, allowing the model to learn from human preferences about when to remain silent rather than risk misinformation.

## Foundational Learning
- **Multi-task learning**: Why needed - Enables joint learning of related tasks (answering and abstention) to improve overall performance; Quick check - Compare performance with single-task baselines
- **Reinforcement Learning with Human Feedback (RLHF)**: Why needed - Aligns model behavior with human preferences and values; Quick check - Evaluate alignment with human judgments in abstention decisions
- **Confidence scoring in RL**: Why needed - Provides a quantitative measure to guide reward signals during RLHF; Quick check - Correlate confidence scores with actual accuracy rates
- **Generative abstention**: Why needed - Allows the model to explicitly indicate uncertainty rather than forcing an answer; Quick check - Measure abstention accuracy versus forced-answer scenarios
- **Hybrid LLM-classifier systems**: Why needed - Serves as a strong baseline for comparison; Quick check - Compare performance metrics against traditional two-stage approaches

## Architecture Onboarding

**Component Map:** Input questions -> Multi-task LLM (QA + Abstention generation) -> Confidence scoring module -> RLHF fine-tuning loop -> Output (Answer or Abstention)

**Critical Path:** The most critical path is the multi-task learning component, where the model simultaneously learns to generate answers and abstention decisions. This joint training is essential because it creates the foundation for the model's self-awareness about its knowledge boundaries.

**Design Tradeoffs:** The paper trades computational efficiency for accuracy by incorporating RLHF fine-tuning, which requires significant computational resources. However, this investment yields substantial improvements in reliability and hallucination reduction, which may justify the additional cost for applications where trust and accuracy are paramount.

**Failure Signatures:** The model may struggle with questions that have partial or ambiguous answers, potentially leading to either over-abstention or inappropriate answering. Additionally, the reliance on a custom dataset (C-IR_Answerability) may limit generalizability to other domains or question types.

**First 3 Experiments:** 1) Compare SALU's abstention accuracy against traditional classifier-based approaches; 2) Evaluate hallucination rates on unanswerable questions across different baseline methods; 3) Test performance on questions with varying degrees of ambiguity to assess robustness.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation primarily conducted on a custom dataset (C-IR_Answerability), which may limit generalizability to other domains or question types
- Computational overhead introduced by the multi-task learning framework and RLHF fine-tuning is not discussed, potentially impacting practical deployment
- Human evaluation methodology lacks detailed reporting on rater selection and inter-annotator agreement

## Confidence

**High Confidence:** The core methodology (multi-task learning for QA and abstention) and the general trend of improved performance over baselines are well-supported by the experimental results.

**Medium Confidence:** The specific performance metrics (accuracy improvements, hallucination reduction percentages) are credible but should be validated on additional datasets and with more diverse question types.

**Medium Confidence:** The human evaluation findings regarding factuality and abstention quality are suggestive but would benefit from larger-scale and more diverse evaluation protocols.

## Next Checks
1. Replicate the experiments on established unanswerable question benchmarks (e.g., SQuAD 2.0, BoolQ) to assess generalizability across domains.
2. Conduct ablation studies to quantify the contribution of each component (multi-task learning, confidence scoring, RLHF fine-tuning) to overall performance.
3. Evaluate the model's performance on questions with varying degrees of ambiguity and partial answerability to test robustness in real-world scenarios.