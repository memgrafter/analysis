---
ver: rpa2
title: 'VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for
  Video Understanding'
arxiv_id: '2509.00484'
source_url: https://arxiv.org/abs/2509.00484
tags:
- mrms
- video
- reward
- wang
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VideoRewardBench, the first comprehensive\
  \ benchmark for evaluating multimodal reward models (MRMs) in video understanding\
  \ across four dimensions: perception, knowledge, reasoning, and safety. The benchmark\
  \ includes 1,563 high-quality preference pairs from 10 existing video datasets,\
  \ covering 1,559 distinct questions\u201415 times more than prior benchmarks."
---

# VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding

## Quick Facts
- arXiv ID: 2509.00484
- Source URL: https://arxiv.org/abs/2509.00484
- Reference count: 40
- This paper introduces VideoRewardBench, the first comprehensive benchmark for evaluating multimodal reward models (MRMs) in video understanding across four dimensions: perception, knowledge, reasoning, and safety.

## Executive Summary
This paper introduces VideoRewardBench, the first comprehensive benchmark for evaluating multimodal reward models (MRMs) in video understanding across four dimensions: perception, knowledge, reasoning, and safety. The benchmark includes 1,563 high-quality preference pairs from 10 existing video datasets, covering 1,559 distinct questions—15 times more than prior benchmarks. A thorough evaluation of 28 MRMs across three categories (generative, discriminative, and semi-scalar) reveals that even the best models achieve only moderate accuracy: proprietary models like Gemini-2.5-Pro and Claude-3.7-Sonnet reach 63.6% and 63.2% overall accuracy respectively, while the top open-source model Qwen2.5-VL-72B attains just 53.3%. Key insights include: (1) MRMs trained with reinforcement learning do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (2) except for discriminative MRMs, other types of MRMs can benefit from inference-time scaling; and (3) variations in input video frame count affect different MRM categories differently.

## Method Summary
The benchmark constructs 1,563 preference pairs from 10 existing video datasets through multi-stage filtering (duration, text-only solvability, model solvability) and dimension balancing. Responses are generated using multiple LVLMs for long-form and short-form tasks, with knowledge/reasoning requiring ground-truth selection and safety using RJScore-based filtering. Human annotation provides ground-truth preferences with majority voting. The evaluation protocol varies by MRM type: generative MRMs use pairwise ranking prompts, while discriminative and semi-scalar MRMs use pointwise scoring comparisons. The benchmark evaluates 28 MRMs across three categories with accuracy metrics per dimension and overall.

## Key Results
- Proprietary models (Gemini-2.5-Pro, Claude-3.7-Sonnet) achieve 63.6% and 63.2% overall accuracy, while top open-source model (Qwen2.5-VL-72B) reaches only 53.3%
- RL-trained MRMs show weaker cross-modal generalization than SFT-trained critic models
- Inference-time scaling improves generative and semi-scalar MRMs but not discriminative MRMs
- Frame count effects vary by MRM category: critic-trained models improve with more frames, while some non-critic models degrade in safety performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inference-time scaling improves generative and semi-scalar MRM performance when using proper sampling parameters, but provides no benefit for discriminative MRMs.
- **Mechanism:** Sampling K responses with temperature > 0 and aggregating via majority voting (generative) or score merging (semi-scalar) reduces variance in judgment errors. Discriminative MRMs output deterministic scalar scores, eliminating stochasticity needed for scaling benefits.
- **Core assumption:** The benefit emerges from diversity in reasoning paths across samples; models must have sufficient baseline capability for aggregation to correct individual errors.
- **Evidence anchors:**
  - [abstract] "(ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling"
  - [section 5.1] "Claude-3.7-Sonnet improves by 10.6% as K increases from 1 to 9... MM-RLHF-Reward improves from 54.5% to 56.5% using score merging"
  - [corpus] R1-Reward paper explores reasoning-based MRMs but does not contradict scaling findings
- **Break condition:** When temperature is set too low (0.2) or top-p too restrictive (0.2), scaling fails—confirmed by VL-RewardBench's prior negative results.

### Mechanism 2
- **Claim:** RL-trained MRMs exhibit weaker cross-modal generalization (image→video) than SFT-trained critic models, despite potential advantages within their training modality.
- **Mechanism:** RL fine-tuning on image/text-only preference data may overfit to modality-specific reward features. SFT-trained critic models retain more generalizable visual reasoning from pretrained LVLM backbones.
- **Core assumption:** Cross-modal generalization depends on preserved pretraining representations rather than reward-specific optimization.
- **Evidence anchors:**
  - [abstract] "MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL"
  - [section 4.3] "R1-Reward drops by 15.6% compared to Qwen2.5-VL-7B... Flex-Judge, trained only on text and evaluated on video, shows the largest drop—20%"
  - [corpus] Med-RewardBench shows similar generalization challenges in medical domain (FMR=0.54), suggesting domain-specific tuning creates similar transfer barriers
- **Break condition:** When the RL training data includes video samples (e.g., UnifiedReward-Think), degradation is minimal (-0.8%).

### Mechanism 3
- **Claim:** Increasing frame count improves critic-trained generative MRMs but causes performance degradation in semi-scalar MRMs and inconsistent effects in non-critic-trained models.
- **Mechanism:** Critic-trained models learn to aggregate evidence across temporal frames effectively. Non-critic models may suffer from attention dilution or safety-related frame interference. Semi-scalar models' critique generation may introduce noise with more frames.
- **Core assumption:** The training objective determines how visual context accumulation is processed—critic training optimizes for evidence synthesis.
- **Evidence anchors:**
  - [abstract] "variations in input video frame count have different effects on different types of MRMs"
  - [section 5.2] "LLaVA-Critic-72B improves from 52.0% to 63.0% as the frame count rises from 1 to 64... Qwen2.5-VL-72B in the safety dimension drops by 6%"
  - [corpus] HumanVideo-MME and VNU-Bench confirm frame/temporal complexity affects model performance but do not test MRMs specifically
- **Break condition:** Safety dimension shows inverse correlation with frame count for some models, suggesting frame content—not just count—matters.

## Foundational Learning

- **Concept: Multimodal Reward Model (MRM) Paradigms**
  - Why needed here: Three architectures (generative, discriminative, semi-scalar) require different evaluation approaches—pairwise ranking vs. pointwise scoring.
  - Quick check question: Given a preference pair (chosen, rejected), which scoring method applies to each MRM type?

- **Concept: Preference Data Construction with Difficulty Filtering**
  - Why needed here: The benchmark uses multi-stage filtering (text-only solvability, model solvability) to ensure samples require genuine video understanding.
  - Quick check question: Why would a question solvable without video input invalidate a video reward benchmark sample?

- **Concept: Inference-Time Scaling Aggregation Methods**
  - Why needed here: Majority voting works for generative MRMs; score merging is required for semi-scalar MRMs; discriminative MRMs cannot benefit.
  - Quick check question: If a semi-scalar MRM outputs both critique text and scalar score, which component should be aggregated across K samples?

## Architecture Onboarding

- **Component map:**
  ```
  VideoRewardBench Pipeline:
  ├── Prompt Collection (10 source datasets → 4 dimensions)
  │   ├── Multi-stage filtering (duration, text-only solvability, model solvability)
  │   └── Dimension balancing (~18-26% per dimension)
  ├── Response Generation
  │   ├── Long-form: 3 models per prompt → 3 preference pairs
  │   ├── Short-form: Ground truth + incorrect option
  │   ├── Knowledge/Reasoning: 10 samples per prompt → filter trivial/difficult
  │   └── Safety: RJScore-based attack success filtering
  ├── Human Annotation
  │   └── 3 annotators per pair → majority voting → inter-annotator agreement check
  └── Evaluation Protocol
      ├── Generative MRMs: Pairwise ranking (temperature=0, shuffled order)
      ├── Discriminative/Semi-scalar: Pointwise scoring (score comparison)
      └── Metrics: Overall Accuracy, Macro Average Accuracy
  ```

- **Critical path:** Prompt quality filtering → response pair construction → human annotation agreement → MRM inference protocol consistency. If any stage uses different standards across dimensions, comparability breaks.

- **Design tradeoffs:**
  - Breadth vs. depth: 1,563 samples across 4 dimensions provides coverage but limits statistical power per dimension (~200-400 each)
  - Safety tradeoff: Higher frame count improves perception but degrades safety evaluation for some models—choose based on deployment priority
  - Temperature choice: 1.0 enables scaling benefits but may hurt greedy-decoding performance; 0.5 may be optimal for specific model families

- **Failure signatures:**
  - Position bias: If response order not shuffled, generative MRMs show inflated/deflated accuracy
  - Length bias: If chosen/rejected responses differ systematically in word count, accuracy reflects length preference—not quality judgment
  - Frame-safety interference: Non-critic models may show >5% safety accuracy drop when frame count increases from 2 to 64

- **First 3 experiments:**
  1. **Baseline evaluation:** Run 3 representative MRMs (one from each category) with default frame counts and temperature=0; verify no position bias by reversing response order
  2. **Inference-time scaling sweep:** For the best-performing generative MRM, test K∈{1,3,5,7,9} with temperature=1.0 vs. 0.5; plot convergence curve
  3. **Frame count sensitivity:** For one critic-trained and one non-critic model, evaluate at frame counts {1,2,4,8,16,32,64}; separate safety dimension analysis to identify degradation thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does scaling the model size of Qwen2.5-VL from 7B to 72B result in a drastic decrease in safety dimension accuracy (80.1% to 44.7%)?
- Basis in paper: [explicit] The authors observe in Table 4 and Section 4.3 that Qwen2.5-VL-72B suffers a "substantial decrease on the safety dimension" compared to its 7B counterpart, despite improvements in other dimensions.
- Why unresolved: The paper reports the anomaly but does not investigate whether it stems from overfitting, catastrophic forgetting in the larger model, or specific architectural instabilities in the 72B variant regarding safety alignment.
- What evidence would resolve it: An ablation study isolating the safety evaluation mechanism in both model sizes, or an analysis of the specific safety sub-categories where the larger model fails.

### Open Question 2
- Question: What mechanisms cause reinforcement learning (RL)-trained generative MRMs to exhibit weaker cross-modal generalization than supervised fine-tuning (SFT) models?
- Basis in paper: [explicit] Section 4.3 states that "MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization," noting that R1-Reward performs worse than its base model on video tasks despite superior image performance.
- Why unresolved: The paper identifies the gap but leaves open whether the issue lies in the RL training data distribution, over-optimization for specific image-based reward signals, or the exploration strategies used during RL.
- What evidence would resolve it: Comparative analysis of the embedding spaces of RL-trained versus SFT-trained models when processing video versus image inputs.

### Open Question 3
- Question: Why does increasing the number of input frames degrade safety evaluation performance in non-critic models like Qwen2.5-VL?
- Basis in paper: [explicit] Section 5.2 notes that for non-critic models, "we observe a noticeable performance drop on safety tasks as the frame count rises," which contrasts with critic-trained models that improve with more frames.
- Why unresolved: It is unclear if this decline is due to attention dilution in non-critic models, increased hallucination rates with longer visual contexts, or specific noise sensitivity in safety features.
- What evidence would resolve it: Attention map visualizations for safety-related tokens as frame count increases, or experiments using noise-reduced frame subsets.

## Limitations
- Human annotation reliability: The paper mentions "sufficient agreement" but does not specify the exact inter-annotator agreement threshold.
- Proprietary model dependence: Key components (response generation models, parsing model in Table 6) rely on undisclosed proprietary systems.
- Frame sampling methodology: Video preprocessing and frame sampling strategy for models without native video support is not detailed.

## Confidence
- **High confidence:** The benchmark construction methodology (multi-stage filtering, dimension balancing) and the evaluation protocol for discriminative and semi-scalar MRMs.
- **Medium confidence:** The inference-time scaling results for generative and semi-scalar MRMs, given the temperature sensitivity and model-specific variability.
- **Low confidence:** The cross-modal generalization findings for RL-trained MRMs, due to limited comparison cases and potential confounding factors from different training objectives.

## Next Checks
1. **Annotation quality verification:** Request the exact inter-annotator agreement threshold and guidelines used to construct preference pairs.
2. **Frame sampling protocol disclosure:** Obtain details on how frames are sampled and preprocessed for models that require frame extraction rather than native video processing.
3. **Cross-validation of scaling results:** Replicate the inference-time scaling experiments with temperature=0.5 and temperature=1.0 for a subset of MRMs to verify the reported performance improvements.