---
ver: rpa2
title: Memory- and Latency-Constrained Inference of Large Language Models via Adaptive
  Split Computing
arxiv_id: '2511.04002'
source_url: https://arxiv.org/abs/2511.04002
tags:
- edge
- quantization
- split
- compression
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an autoregressive-aware split computing framework
  for deploying large language models (LLMs) on memory- and latency-constrained edge
  devices. The framework addresses the challenges of iterative token generation and
  expanding KV caches in LLMs by employing one-point split compression (OPSC), a mixed-precision
  quantization scheme that partitions models into front-end and back-end segments
  with different precision levels.
---

# Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing

## Quick Facts
- arXiv ID: 2511.04002
- Source URL: https://arxiv.org/abs/2511.04002
- Reference count: 40
- Primary result: 1.49× inference speedup with reduced communication overhead while maintaining/improving accuracy

## Executive Summary
This work presents a novel autoregressive-aware split computing framework designed to enable efficient deployment of large language models on memory- and latency-constrained edge devices. The framework addresses the fundamental challenges posed by iterative token generation and expanding KV caches in LLMs by introducing one-point split compression (OPSC) and a mixed-precision quantization strategy. By partitioning models into front-end and back-end segments with different precision levels and employing a two-stage intermediate compression pipeline, the approach achieves significant improvements in inference speed and communication efficiency while preserving model accuracy.

## Method Summary
The framework employs a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while reducing communication overhead. One-point split compression (OPSC) enables precise control over model partitioning, while a unified optimization strategy jointly selects optimal split points, quantization settings, and sequence lengths under strict memory and latency constraints. The approach specifically targets the challenges of autoregressive inference, where KV cache growth and iterative token generation create unique computational bottlenecks.

## Key Results
- Achieves 1.49× inference speedup compared to baseline methods
- Significantly reduces communication overhead through adaptive quantization
- Maintains or improves model accuracy across tested benchmarks
- Demonstrates effectiveness for Llama-3.2-3B model deployment

## Why This Works (Mechanism)
The framework succeeds by addressing the core tension between memory constraints and latency requirements in edge deployment of LLMs. By employing mixed-precision quantization and adaptive split computing, it allows critical activations to be preserved in higher precision while less important computations use lower precision, reducing both memory footprint and communication costs. The autoregressive-aware design specifically accounts for the iterative nature of token generation, optimizing the KV cache management and split point selection to minimize bottlenecks during the generation process.

## Foundational Learning
**Autoregressive Language Models** - Why needed: Understanding the iterative token generation process is crucial for optimizing split computing strategies. Quick check: Verify that the framework correctly handles the sequential dependencies in token generation.

**KV Cache Management** - Why needed: The growing KV cache presents a fundamental memory challenge in autoregressive inference. Quick check: Confirm that cache size and access patterns are properly optimized.

**Mixed-Precision Quantization** - Why needed: Different model components have varying sensitivity to precision loss. Quick check: Validate that critical activations maintain sufficient precision while non-critical components are compressed.

**Split Computing Architecture** - Why needed: Partitioning models across devices requires careful consideration of communication costs and computational load balancing. Quick check: Ensure split points minimize latency while respecting memory constraints.

**Token-Wise Adaptive Quantization** - Why needed: Different tokens may require different levels of precision preservation. Quick check: Verify that the adaptive quantization responds appropriately to varying token characteristics.

## Architecture Onboarding

**Component Map**: Input -> Threshold Splitting -> Token-Wise Adaptive Bit Quantization -> Mixed-Precision Model Partitioning -> Output

**Critical Path**: The sequence generation loop, where each token depends on previous outputs and requires iterative processing through the partitioned model segments.

**Design Tradeoffs**: The framework balances precision preservation against compression efficiency, with the mixed-precision approach allowing critical components to maintain accuracy while non-critical components achieve higher compression ratios. The adaptive quantization introduces computational overhead but enables better overall performance.

**Failure Signatures**: Poor split point selection can lead to excessive communication overhead or memory violations. Inadequate quantization precision can cause accuracy degradation, particularly for complex or ambiguous tokens. Suboptimal sequence length selection may result in either wasted computation or insufficient throughput.

**Three First Experiments**: 
1. Baseline accuracy validation on Llama-3.2-3B without any compression or splitting
2. Single-split point analysis to determine optimal partition locations
3. Quantization sensitivity testing to establish minimum precision requirements for different model components

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided text.

## Limitations
- Evaluation limited to Llama-3.2-3B model, limiting generalizability to other architectures and sizes
- Two-stage intermediate compression pipeline may face practical challenges in maintaining token-level adaptivity across diverse workloads
- Assumption of stable split-point selection under varying input distributions needs further validation

## Confidence
- **High** confidence in 1.49× speedup claim based on experimental methodology and comparison with established baselines
- **Medium** confidence in accuracy preservation claims as results show improvement or maintenance relative to baselines but are limited to specific benchmarks
- **Low** confidence in framework's robustness across diverse deployment scenarios due to narrow scope of tested configurations and hardware platforms

## Next Checks
1. Evaluate the framework on diverse LLM architectures including larger models (7B+) and different transformer variants to assess scalability and architectural sensitivity
2. Conduct real-world deployment tests on various edge devices with different memory and latency constraints to validate practical performance claims
3. Perform extensive ablation studies on the quantization parameters and split-point selection to quantify the contribution of each component to overall performance gains