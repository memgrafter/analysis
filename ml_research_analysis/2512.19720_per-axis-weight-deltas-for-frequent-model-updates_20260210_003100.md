---
ver: rpa2
title: Per-Axis Weight Deltas for Frequent Model Updates
arxiv_id: '2512.19720'
source_url: https://arxiv.org/abs/2512.19720
tags:
- https
- fine-tuned
- arxiv
- base
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple 1-bit delta quantization method for
  efficient storage and serving of fine-tuned LLM variants. The method represents
  weight differences as a binary sign mask combined with per-axis (row/column) FP16
  scaling factors, learned from a small calibration set.
---

# Per-Axis Weight Deltas for Frequent Model Updates

## Quick Facts
- arXiv ID: 2512.19720
- Source URL: https://arxiv.org/abs/2512.19720
- Reference count: 40
- Primary result: Per-axis 1-bit delta quantization improves reconstruction quality and task accuracy for LLM fine-tunes while achieving ~5× storage reduction versus full checkpoints

## Executive Summary
This paper introduces a simple 1-bit delta quantization method for efficient storage and serving of fine-tuned LLM variants. The method represents weight differences as a binary sign mask combined with per-axis (row/column) FP16 scaling factors, learned from a small calibration set. By capturing axis-specific variations in model weights, the approach improves reconstruction quality over scalar alternatives while maintaining the compactness of 1-bit deltas. Experiments with Llama-3.1-8B show consistent accuracy improvements across five zero-shot benchmarks compared to both uncompressed models and scalar 1-bit delta methods. The delta representation achieves approximately 5× storage reduction compared to full FP16 checkpoints and enables faster cold-start loading, making it practical for serving many fine-tuned variants from a shared base model.

## Method Summary
The method represents weight deltas using a binary sign mask and per-axis scaling factors. Given a base model weight matrix W_b and fine-tuned weights W_f, the delta D = W_f - W_b is compressed by learning a binary mask B ∈ {−1, +1} and per-axis scaling factors S (row-wise and column-wise for different matrix types). The scaling factors are computed from a small calibration set of 128 examples, minimizing reconstruction error through alternating optimization. During inference, weights are reconstructed as Ŵ_f = W_b + B ⊙ S, where S is broadcast to match the matrix dimensions. The approach operates only on linear layers (attention QKV and MLP projections), excluding embeddings, biases, and normalization parameters.

## Key Results
- Per-axis delta method achieves 5× storage reduction compared to full FP16 checkpoints
- Consistently outperforms scalar 1-bit delta methods across five zero-shot benchmarks (TruthfulQA, AGIEval, BBH, MMLU, C-Eval)
- Enables faster cold-start loading, reducing latency by 5-6× versus uncompressed models
- Reconstruction error improves by 1.5-2× compared to scalar delta methods

## Why This Works (Mechanism)
The method captures axis-specific patterns in weight changes during fine-tuning. Weight updates often exhibit structured patterns along rows or columns due to attention patterns or feature correlations. By learning separate scaling factors for each row and column, the method can better approximate these structured changes than scalar methods that apply uniform scaling. The binary sign mask preserves the direction of changes while the per-axis factors capture magnitude variations, enabling accurate reconstruction with minimal storage overhead.

## Foundational Learning
- Matrix quantization and delta compression: Understanding how to represent weight differences efficiently is crucial for model storage and serving. Quick check: Verify that delta compression requires storing both base weights and delta information
- Alternating optimization: The method uses alternating minimization to learn scaling factors, requiring understanding of optimization techniques for non-convex problems. Quick check: Confirm that alternating optimization converges to local minima in practice
- Broadcasting in tensor operations: Per-axis scaling factors must be correctly broadcast to match matrix dimensions during reconstruction. Quick check: Verify tensor shape compatibility between binary mask, scaling factors, and weight matrices
- Weight initialization impact: Different initialization schemes can affect delta magnitude and distribution. Quick check: Measure delta statistics across different initialization methods
- Calibration set selection: The quality of learned scaling factors depends on representative calibration data. Quick check: Evaluate reconstruction error sensitivity to calibration set size and diversity

## Architecture Onboarding
Component map: Base model weights -> Delta compression -> Binary mask + Scaling factors -> Reconstruction -> Fine-tuned model
Critical path: The most performance-critical operation is weight reconstruction during inference, where B and S must be applied efficiently to the base weights. Memory bandwidth becomes the bottleneck when loading many fine-tuned variants.
Design tradeoffs: The method trades some reconstruction accuracy for significant storage reduction. The choice of 1-bit deltas maximizes compression but may lose fine-grained information. Per-axis scaling adds storage overhead but improves accuracy.
Failure signatures: Poor reconstruction quality manifests as degraded task performance, particularly on benchmarks sensitive to precise weight values. Large reconstruction errors in attention matrices can severely impact model outputs.
First experiments: 1) Measure reconstruction error distribution across different weight matrices 2) Benchmark zero-shot accuracy degradation with varying calibration set sizes 3) Profile cold-start latency with different numbers of fine-tuned variants

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does learning the binary sign mask structure, rather than fixing it to the sign of the weight difference, yield significant performance improvements at aggressive bit budgets?
- Basis in paper: The Conclusion states "Future work includes... learning the sign structure."
- Why unresolved: The current method fixes B = sign(W_f - W_b), which prevents explicit zeros and can propagate noise for small-magnitude entries.
- What evidence would resolve it: A comparative study measuring reconstruction error and downstream task accuracy when B is learned via gradient-based methods versus the current static assignment.

### Open Question 2
- Question: Can per-axis weight deltas be effectively combined with lower-precision base model formats, such as INT4 or FP8 quantization?
- Basis in paper: The Conclusion lists "INT4/FP8 co-design" as a direction for future work.
- Why unresolved: The experiments primarily operate on FP16/BF16 weights, leaving the interaction between vector-scaled deltas and highly quantized base weights unexplored.
- What evidence would resolve it: Benchmarks showing zero-shot accuracy and latency when applying the delta method to a base model that has undergone 4-bit or 8-bit weight quantization.

### Open Question 3
- Question: Does extending the per-axis delta compression to embeddings, biases, and normalization layers improve performance for fine-tunes where changes concentrate in non-linear parameters?
- Basis in paper: The Limitations section notes, "We do not modify normalizations, biases or embeddings; if task-specific changes concentrate there, our method may yield limited benefits."
- Why unresolved: The current implementation restricts compression to linear projections (attention and MLP), potentially ignoring task-critical information stored in other layer types.
- What evidence would resolve it: Ablation studies on models with heavy embedding shifts (e.g., domain adaptation) comparing the performance of linear-only compression against a full-layer compression approach.

## Limitations
- Limited to linear layers, excluding embeddings, biases, and normalization parameters where task-specific changes may concentrate
- Performance evaluation restricted to Llama-3.1-8B on a single fine-tuning dataset
- Calibration set size of 128 examples may not capture diverse weight update patterns
- Hardware-specific cold-start latency measurements without exploring memory bandwidth effects

## Confidence
- High confidence in per-axis scaling improving reconstruction accuracy over scalar methods, supported by consistent benchmark improvements
- Medium confidence in 5× storage reduction claim, dependent on delta size assumptions across different models and fine-tuning objectives
- Low confidence in cold-start latency benefits without additional hardware and workload diversity testing

## Next Checks
1. Evaluate per-axis deltas on multi-billion parameter models and across multiple fine-tuning datasets to assess scalability and robustness
2. Measure reconstruction quality sensitivity to calibration set size by varying it from 32 to 1024 examples
3. Test cold-start latency on different GPU/CPU configurations and with varying base model cache sizes to isolate hardware-specific effects