---
ver: rpa2
title: 'Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache
  Management'
arxiv_id: '2511.20172'
source_url: https://arxiv.org/abs/2511.20172
tags:
- memory
- kvcache
- data
- rdma
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Beluga, a CXL-based memory architecture designed
  to address memory bottlenecks in large language model (LLM) inference systems. By
  leveraging CXL 2.0 switches, Beluga enables GPUs and CPUs to directly access a shared,
  large-scale memory pool with near-local memory latency, eliminating the need for
  complex RDMA-based communication protocols.
---

# Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management

## Quick Facts
- arXiv ID: 2511.20172
- Source URL: https://arxiv.org/abs/2511.20172
- Reference count: 40
- Primary result: 89.6% TTFT reduction, 7.35× throughput improvement vs RDMA

## Executive Summary
This paper introduces Beluga, a CXL-based memory architecture designed to address memory bottlenecks in large language model (LLM) inference systems. By leveraging CXL 2.0 switches, Beluga enables GPUs and CPUs to directly access a shared, large-scale memory pool with near-local memory latency, eliminating the need for complex RDMA-based communication protocols. The system provides native load/store access semantics over the CXL fabric, simplifying programming and minimizing synchronization overhead. Beluga-KVCache, built on this architecture, demonstrates significant performance improvements in LLM inference, achieving an 89.6% reduction in Time-To-First-Token (TTFT) and a 7.35× throughput improvement in the vLLM inference engine compared to RDMA-based solutions.

## Method Summary
Beluga implements a CXL-based disaggregated memory architecture for GPU clusters to enable efficient KVCache management in LLM inference, replacing traditional RDMA-based memory pools. The system uses CXL 2.0 switches to create a shared memory pool accessible via native load/store semantics, eliminating bounce buffer copies and complex synchronization. The architecture integrates Beluga-KVCache optimizations into vLLM v0.8.5, including non-temporal stores for CPU writes, DSA for transfers >4KB, uncacheable memory regions for GPU access with DDIO disabled, and custom CUDA copy kernels for small transfers. The system was evaluated on 2× servers with 8×H20 GPUs each, connected through an XConn XC50256 CXL 2.0 switch to an 8TB CXL memory pool, using the LV-Eval benchmark with Qwen-32B and Llama-3.1-8B models.

## Key Results
- Achieves 89.6% reduction in Time-To-First-Token (TTFT) compared to RDMA-based solutions
- Delivers 7.35× throughput improvement in vLLM inference engine
- Eliminates complex RDMA synchronization overhead through native CXL load/store semantics

## Why This Works (Mechanism)

### Mechanism 1
Replacing RDMA's network-based memory access with CXL's memory-semantic interface reduces data movement latency and control-path complexity. CXL.mem provides native load/store semantics over a switched fabric, allowing GPUs and CPUs to access a shared memory pool via simple memory operations instead of RDMA's network protocol stack (queue pairs, work requests, polling completions). This eliminates mandatory "bounce buffer" copies through host memory and removes cross-component synchronization between GPU streams and RDMA polling kernels.

### Mechanism 2
Software-managed cache coherence techniques enable correct multi-host data sharing on non-coherent CXL 2.0 hardware without the overhead of hardware protocols. Since CXL 2.0 lacks host-to-host cache coherence, Beluga uses software strategies to ensure KVCache consistency. For writers, it uses non-temporal stores (ntstore) to bypass the CPU cache or explicitly flushes with CLFLUSH. For readers, it invalidates local cache lines before reading. For GPU-initiated transfers, it disables DDIO to prevent stale data in the CPU LLC.

### Mechanism 3
A unified, flat memory address space via CXL enables a cache-oblivious scheduler that prioritizes compute load balancing over KVCache locality. By mapping the entire CXL memory pool into a unified physical/virtual address space (using DAX mode and mmap), all servers see the same memory. The near-local latency of CXL (~750ns switch latency, ~11µs for 64KB GPU-to-CXL copy) makes remote access performance symmetric to local access. This allows the scheduler to distribute requests based solely on GPU compute availability, ignoring where the required KVCache resides.

## Foundational Learning

- **Concept: Compute Express Link (CXL.mem)**
  - Why needed here: This is the foundational interconnect technology enabling Beluga. CXL.mem is the protocol that allows a host CPU to access memory attached to a CXL device (like a memory expander or pool) using standard load/store instructions. Understanding this is key to grasping how Beluga replaces RDMA.
  - Quick check question: How does CXL.mem differ from traditional PCIe memory-mapped I/O (MMIO) in terms of coherence and intended use case for an application developer?

- **Concept: KVCache (Key-Value Cache) in LLMs**
  - Why needed here: The KVCache is the specific data structure Beluga is optimized to manage. It stores the intermediate attention states (Key and Value tensors) from previous tokens to avoid recomputation during autoregressive generation. Its large, growing size and need for reuse/sharing make it the primary memory bottleneck Beluga solves.
  - Quick check question: In a multi-turn conversation, why is reusing the KVCache more efficient than recomputing the entire context for each new user message?

- **Concept: RDMA (Remote Direct Memory Access) Limitations**
  - Why needed here: The paper's motivation is built on the inefficiencies of the current standard (RDMA) for disaggregated memory. Understanding its drawbacks—extra data copies through host memory, complex control path with polling/synchronization, and network protocol overhead—clarifies why a memory-semantic alternative like CXL is proposed.
  - Quick check question: In the CPU-driven RDMA model for KVCache offloading, describe the two-step data path required to move data from a GPU to a remote memory pool. Where are the main latency penalties introduced?

## Architecture Onboarding

- **Component map:** Compute Servers -> PCIe/CXL Adapters -> CXL Switch -> CXL Memory Box
- **Critical path:** GPU ↔ CXL Memory Pool data transfer: GPU Memory -> PCIe Switch -> CPU Root Complex -> CXL/PCIe Adapter -> CXL Switch -> CXL Memory Device. The CPU Root Complex is identified as the primary bandwidth bottleneck.
- **Design tradeoffs:**
  - Cost vs. Maturity: CXL 2.0 switches and memory devices are newer and potentially more expensive per-port than mature Ethernet/RDMA gear, but promise better TCO through simplified software and better resource utilization.
  - Latency vs. Complexity: Software-managed cache coherence (O1-O3) adds developer complexity but is necessary for correctness on CXL 2.0, avoiding the higher latency of uncacheable memory access.
  - Scalability vs. Bandwidth: The current architecture scales capacity well via the CXL switch but faces bandwidth bottlenecks from the Root Complex and single-adapter limits.
- **Failure signatures:**
  1. Data corruption/staleness in KVCache: Likely a cache coherence bug. Check if correct software methods (ntstore, CLFLUSH, disabled DDIO) are used for the specific initiator (CPU/GPU) and operation (read/write).
  2. Unexpectedly high latency (>50µs for small transfers): May indicate suboptimal transfer method. Ensure CPU uses load/store for <4KB and GPU uses custom kernels for small, non-contiguous transfers.
  3. Bandwidth saturation below expected (~30 GB/s): Likely hitting the Root Complex bottleneck. Verify workload distribution across multiple CXL adapters or check for hotspots on a single CXL memory device.
- **First 3 experiments:**
  1. Baseline Latency Characterization: Run the microbenchmarks from §5.2 (Exp #2) on your hardware. Verify CXL latency is competitive with local DRAM and significantly better than RDMA for small I/Os.
  2. Cache Coherence Stress Test: Implement a multi-host test where one server writes a KVCache block and another reads it repeatedly. Test with and without the prescribed coherence methods (O1-O3). Confirm data integrity and measure the overhead of coherence instructions.
  3. Bandwidth & Bottleneck Identification: Use a tool like fio or custom CUDA kernels to measure sustained bandwidth from a single GPU to the CXL pool. Compare against the Root Complex bottleneck diagnosis. Then, test with multiple concurrent servers to see if the switch provides fair and scalable bandwidth.

## Open Questions the Paper Calls Out

### Open Question 1
Can bypassing the host Root Complex by connecting GPUs directly to a CXL fabric eliminate the bandwidth bottlenecks present in current architectures?
- Basis in paper: Section 8 proposes a future disaggregated architecture where GPUs connect directly to the CXL fabric to bypass the Root Complex, which was identified as the primary bandwidth bottleneck in Section 5.3.
- Why unresolved: The current Beluga architecture routes GPU traffic through the host PCIe topology, limiting throughput to approximately 26 GB/s.
- What evidence would resolve it: Evaluation of a GPU-to-CXL direct-attach system measuring throughput improvements over the current Root Complex-limited performance.

### Open Question 2
How can in-switch resources or application-level semantics be utilized to design scalable coherence protocols for CXL memory pools?
- Basis in paper: Section 8 calls for future work on software design, specifically suggesting directory-based coherence using in-switch resources or hybrid models leveraging application semantics.
- Why unresolved: CXL 2.0 lacks hardware cache coherence across hosts, and CXL 3.0 capabilities are currently unclear or limited, necessitating efficient software-managed coherence.
- What evidence would resolve it: Implementation of a directory-based protocol utilizing switch memory, demonstrating improved performance over the paper's current software flushing methods.

### Open Question 3
Does CXL-based memory pooling effectively scale other memory-intensive database workloads, such as graph databases (e.g., HNSW)?
- Basis in paper: Section 8 identifies graph-based algorithms like HNSW and vector databases as specific opportunities for future research using CXL memory pools to break memory capacity limits.
- Why unresolved: The evaluation in this paper is limited strictly to LLM KVCache management and does not test other data structures.
- What evidence would resolve it: End-to-end benchmarks of graph traversal algorithms running on Beluga compared to local DRAM and RDMA-based disaggregation.

## Limitations
- Hardware Availability: XConn XC50256 CXL switch and compatible memory devices may have limited commercial availability and unclear pricing.
- Software Stack Maturity: Exact kernel patches, driver configurations, and vLLM integration code are not fully specified.
- Workload Representation: Evaluation focuses on long-context QA with specific models, not exploring diverse LLM architectures or higher write contention scenarios.

## Confidence

**High Confidence:** The core architectural claim that CXL.mem can replace RDMA for disaggregated memory in LLM inference is well-supported by microbenchmarks and significant performance improvements demonstrated in vLLM integration.

**Medium Confidence:** The identification of CPU Root Complex as a bandwidth bottleneck is plausible, but the extent of its impact and effectiveness of proposed mitigations require further validation with diverse workloads.

**Low Confidence:** Scalability of CXL switch fabric under high contention and long-term stability of software-managed coherence for workloads with many concurrent writers are not thoroughly evaluated.

## Next Checks
1. Hardware Availability Audit: Attempt to procure XConn XC50256 CXL 2.0 switch and compatible CXL memory devices. Document acquisition process, costs, and configuration challenges.
2. Open-Source Integration Release: Request or reconstruct vLLM integration code for Beluga-KVCache (custom CUDA kernels, CXL-RPC implementation) and publish for community testing.
3. Bottleneck Stress Test: Design microbenchmark to explicitly test Root Complex bandwidth limit identified in paper. Use multiple GPUs and concurrent CXL memory access patterns to measure actual bandwidth ceiling.