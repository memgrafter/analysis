---
ver: rpa2
title: 'BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution'
arxiv_id: '2505.15308'
source_url: https://arxiv.org/abs/2505.15308
tags:
- image
- backdoor
- images
- poisoned
- badsr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the security of image super-resolution (SR)
  models by proposing BadSR, a stealthy backdoor attack method. The key idea is to
  approximate clean HR images to predefined target images in the feature space while
  maintaining visual similarity, addressing the limitation of previous backdoor attacks
  that ignored the stealthiness of poisoned HR images.
---

# BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution

## Quick Facts
- **arXiv ID**: 2505.15308
- **Source URL**: https://arxiv.org/abs/2505.15308
- **Reference count**: 40
- **Primary result**: Presents BadSR, a stealthy backdoor attack on image super-resolution models that maintains high attack success rates while preserving visual similarity of poisoned high-resolution images

## Executive Summary
This paper addresses security vulnerabilities in image super-resolution (SR) models through BadSR, a novel backdoor attack method that balances attack effectiveness with stealthiness. Unlike previous backdoor attacks that ignored the visual similarity of poisoned high-resolution (HR) images, BadSR approximates clean HR images to predefined target images in feature space while maintaining perceptual similarity. The method incorporates poisoned HR images with triggers and uses an adversarially optimized trigger combined with a genetic algorithm-based poisoned sample selection approach. Experimental results demonstrate that BadSR achieves high attack success rates across various SR models and datasets while maintaining minimal visual differences from original images, as measured by SSIM and PSNR metrics.

## Method Summary
BadSR introduces a stealthy backdoor attack on SR models by optimizing poisoned HR images to approximate predefined target images in feature space while maintaining visual similarity to clean images. The method employs an adversarially optimized trigger mechanism and uses a genetic algorithm to select optimal poisoned samples from the training data. During training, the SR model learns to map trigger-embedded low-resolution images to poisoned high-resolution counterparts that appear visually similar to clean HR images but embed malicious behavior. The attack framework integrates with existing SR architectures and targets downstream tasks like image classification and object detection, achieving high attack success rates while preserving the stealthiness of poisoned samples through careful feature space optimization.

## Key Results
- Achieves high attack success rates across multiple SR models (RCAN, EDSR, ESRGAN) and datasets (DIV2K, Places365)
- Maintains minimal visual difference between poisoned and clean HR images, validated by high SSIM and PSNR metrics
- Effectively impacts downstream tasks including image classification and object detection when triggered
- Demonstrates superior stealthiness compared to existing backdoor attacks on SR models by optimizing feature space approximation

## Why This Works (Mechanism)
BadSR exploits the fundamental vulnerability in SR model training where the model learns to map LR images to HR outputs without explicit verification of HR image authenticity. By carefully crafting poisoned HR images that are visually similar to clean images but approximate target feature representations, the attack embeds malicious behavior during training that remains dormant until triggered. The genetic algorithm optimizes the selection of poisoned samples to maximize attack effectiveness while minimizing detection risk, and the adversarially optimized trigger ensures reliable activation of the backdoor behavior during inference.

## Foundational Learning

**Image Super-Resolution (SR)**: The process of generating high-resolution images from low-resolution inputs, typically using deep learning models trained on paired LR-HR image datasets.
*Why needed*: Forms the target application where the backdoor attack is implemented.
*Quick check*: Understanding of SR architectures like RCAN, EDSR, and ESRGAN and their training procedures.

**Backdoor Attacks**: A class of adversarial attacks where models are trained with poisoned data that causes specific malicious behavior when triggered by predefined patterns.
*Why needed*: The core attack methodology being extended and improved in this work.
*Quick check*: Knowledge of trigger patterns, poisoned data injection, and attack activation mechanisms.

**Feature Space Approximation**: The technique of matching feature representations between images rather than pixel-level similarity, often using deep neural network feature extractors.
*Why needed*: Enables the stealthiness of poisoned HR images by focusing on high-level feature similarity rather than pixel-level differences.
*Quick check*: Understanding of feature extraction methods and distance metrics in latent spaces.

**Genetic Algorithms**: Optimization techniques inspired by natural selection that evolve populations of candidate solutions through selection, crossover, and mutation operations.
*Why needed*: Used for selecting optimal poisoned samples that maximize attack effectiveness while maintaining stealthiness.
*Quick check*: Familiarity with population-based optimization and evolutionary computation concepts.

## Architecture Onboarding

**Component Map**: LR Image -> Trigger Application -> SR Model -> Poisoned HR Image (approximating target features) -> Downstream Task Model

**Critical Path**: Trigger application on LR images → SR model training with poisoned HR samples → Feature space optimization → Downstream task compromise

**Design Tradeoffs**: The method balances attack success rate against stealthiness by optimizing feature space approximation rather than pixel-level similarity, trading some attack strength for improved undetectability. The genetic algorithm selection trades computational complexity for optimal poisoned sample identification.

**Failure Signatures**: Attack failure occurs when poisoned HR images fail to approximate target features sufficiently, resulting in reduced attack success rates. Detection may occur through anomaly detection in HR image feature distributions or through analysis of model behavior on trigger-embedded inputs.

**First Experiments**:
1. Baseline attack success rate evaluation on clean SR models without backdoor injection
2. Feature space distance analysis between poisoned and clean HR images using SSIM and PSNR metrics
3. Downstream task performance degradation measurement on triggered samples versus clean samples

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily focuses on synthetic datasets (DIV2K and Places365) which may not generalize to all real-world SR applications
- Claims of "high stealthiness" are based on quantitative metrics (SSIM, PSNR) without comprehensive human perception studies
- Effectiveness of the genetic algorithm for poisoned sample selection may vary significantly with different dataset characteristics
- Limited exploration of potential defenses or countermeasures against the proposed attack

## Confidence

**High confidence**: The technical methodology for implementing the BadSR attack is clearly described and reproducible based on the provided information.

**Medium confidence**: The reported attack success rates and stealthiness metrics are convincing within the tested conditions but may not generalize across all SR applications.

**Medium confidence**: The downstream task impact analysis is relevant but limited to specific tasks and datasets without exploring broader applicability.

## Next Checks

1. Evaluate BadSR across diverse real-world SR datasets and applications beyond DIV2K and Places365 to test generalizability.
2. Conduct human perceptual studies alongside SSIM/PSNR metrics to validate the claimed "high stealthiness" from a human visual perspective.
3. Test BadSR against common backdoor defense mechanisms to assess robustness and practical effectiveness in adversarial settings.