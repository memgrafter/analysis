---
ver: rpa2
title: Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language
  Model based Bundle Generation
arxiv_id: '2508.17250'
source_url: https://arxiv.org/abs/2508.17250
tags:
- knowledge
- bundle
- generation
- experts
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses knowledge conflicts in LLM-based bundle generation
  when naively combining different types of distilled knowledge. The authors propose
  RouteDK, a framework that uses knowledge-specific LoRA experts (base, high-level,
  and fine-grained) and a dynamic fusion module with input-aware routing to effectively
  integrate complementary knowledge types.
---

# Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation

## Quick Facts
- arXiv ID: 2508.17250
- Source URL: https://arxiv.org/abs/2508.17250
- Reference count: 40
- Primary result: Achieves accuracy comparable to or better than teacher LLMs while maintaining computational efficiency, with 9.32% average gain over knowledge distillation baselines

## Executive Summary
This paper addresses knowledge conflicts that arise when combining different types of distilled knowledge from teacher LLMs into student models for bundle generation tasks. The authors propose RouteDK, a framework that uses knowledge-specific LoRA experts (base, high-level, and fine-grained) with a dynamic fusion module and input-aware routing to effectively integrate complementary knowledge types. An inference-time enhancement strategy based on self-consistency further improves robustness. Experiments on three public datasets show RouteDK achieves accuracy comparable to or better than teacher LLMs while maintaining computational efficiency, with an average gain of 9.32% over knowledge distillation baselines.

## Method Summary
RouteDK separates distilled knowledge types into specialized LoRA experts to mitigate knowledge conflict. The framework consists of three LoRA experts (Base, High-level, and Fine-grained) trained independently on different knowledge streams, followed by a lightweight router that dynamically computes per-layer weights for expert fusion. During inference, test-time self-consistency with parallel sampling and majority voting improves robustness. The method uses LoRA adapters (rank-16) on a frozen Llama3.1-8B backbone, with training proceeding in two stages: first training experts separately, then freezing them and training only the router. Inference generates 8 samples per session with temperature 0.7, followed by majority voting after normalizing item order within bundles.

## Key Results
- RouteDK achieves accuracy comparable to or better than teacher LLMs while maintaining computational efficiency
- 9.32% average gain over knowledge distillation baselines
- Dynamic routing outperforms static averaging and other fusion methods across most domains
- Test-time self-consistency with N=8 samples provides optimal balance between performance and cost

## Why This Works (Mechanism)

### Mechanism 1
Separating distilled knowledge types into specialized LoRA experts mitigates knowledge conflict that arises from naive concatenation. High-level knowledge (generalizable rules) and fine-grained knowledge (session-specific reasoning) are learned by independent LoRA adapters rather than merged in context. The Base Expert handles raw session data as a stable backbone. This modularization prevents interference during training and allows downstream fusion to selectively weigh each knowledge stream. Core assumption: distilled knowledge types are complementary but interfere when co-optimized in shared parameters. Evidence: DK4 (merged) underperforms DK2 and DK3 (single-type) across three domains.

### Mechanism 2
Input-aware, layer-wise routing enables adaptive knowledge fusion better than static averaging. A lightweight router computes context vectors from hidden states and produces per-layer softmax weights for each expert's LoRA update. Sessions requiring nuanced intent reasoning up-weight the Fine-grained Expert; sessions benefiting from consistency rules up-weight the High-level Expert. Core assumption: the optimal expert combination varies across sessions and across Transformer layers. Evidence: Dynamic fusion outperforms Average, TIES, and Static in most domains.

### Mechanism 3
Test-time self-consistency (parallel sampling + majority voting) reduces variance in bundle generation. Generate N candidate bundle sequences with stochastic decoding, normalize item order within bundles, and select the most frequent prediction. This averages out stochastic errors while reinforcing high-confidence outputs. Core assumption: the model's internal distribution has a stable mode; noise is primarily from sampling, not systematic error. Evidence: Table IV shows removing TTS reduces precision/recall; Figure 5 shows performance peaks at N=8 then declines at N=32.

## Foundational Learning

- **Concept: Knowledge Distillation**
  - Why needed here: The entire framework relies on extracting high-level and fine-grained knowledge from a large teacher LLM (GPT-3.5-turbo) to a smaller student (Llama3.1-8B) for efficiency
  - Quick check question: Can you explain the difference between using teacher outputs as supervision vs. aligning hidden representations?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: All experts are implemented as LoRA adapters (rank-16 matrices) added to frozen backbone weights. Understanding the forward pass modification is critical
  - Quick check question: In Equation 1, what does the term Δ(H; ϕ) represent and why does freezing W matter?

- **Concept: Mixture of Experts (MoE) Routing**
  - Why needed here: The router implements a soft, input-conditioned gating function over experts at each layer. This differs from TopK hard routing used in other MoE systems
  - Quick check question: What is the difference between token-level routing (common in MoE Transformers) and the session-level context routing used here?

## Architecture Onboarding

- **Component map:** Teacher LLM (GPT-3.5-turbo) → Knowledge Distillation Module → {High-level Rules, Fine-grained CoT} → Student Backbone (Llama3.1-8B frozen) + 3 LoRA Experts (Base, High-level, Fine-grained) → Dynamic Fusion Module: Router(ψ) + Layer-wise Weight Computation → Inference Enhancement: N-way parallel sampling + Majority Voting

- **Critical path:** 1) Distill high-level and fine-grained knowledge from teacher on training sessions 2) Train each LoRA expert independently with expert-specific inputs (MLE objective) 3) Freeze backbone and experts; train router only on fusion objective 4) At inference: tokenize session → router computes weights per layer → generate N samples → majority vote

- **Design tradeoffs:** Number of experts (K): K=3 gives best results; K=2 conflates high-level/fine-grained and underperforms; Sampling number (N): N=8 balances performance and cost; N>16 introduces noise; Router capacity: Lightweight linear router used; deeper routers may overfit given small bundle datasets

- **Failure signatures:** Knowledge conflict (DK4 < DK2/DK3 in Figure 1): indicates naive knowledge merging; Static fusion underperforms Dynamic in heterogeneous domains (Figure 4): indicates need for input-adaptive weighting; Precision drops when RAC (retrieval-augmented context) is added (Table IV): external session knowledge misguides the fine-grained expert

- **First 3 experiments:** 1) Reproduce Figure 1 on a held-out domain: compare K=1 (merged), K=2 (raw vs. external), K=3 (separated) to validate conflict hypothesis 2) Ablate routing strategy: replace Dynamic with Static weights and Average Pooling; measure precision/recall gap 3) Sweep N ∈ {1, 4, 8, 16, 32} for test-time scaling: plot precision vs. N to confirm optimal operating point (reported: N=8)

## Open Questions the Paper Calls Out

- **Question:** How can multimodal signals (e.g., images, item relationships) be effectively integrated into the RouteDK framework to enhance knowledge distillation and bundle generation?
  - Basis in paper: The "Limitations and Future Work" section states that the current framework relies exclusively on textual knowledge and "underutilizes the potential of multimodal signals."
  - Why unresolved: The current methodology and experiments are designed solely for textual input, leaving the integration of visual or relational data unexplored
  - What evidence would resolve it: A modified RouteDK implementation that successfully ingests and processes image data during distillation, demonstrating improved performance over the text-only baseline on multimodal datasets

- **Question:** Does replacing the majority voting strategy with a reward-model-guided approach improve the optimality of inference-time reasoning paths?
  - Basis in paper: The authors note their test-time scaling strategy is "simplistic" and relies on majority voting, which "may miss optimal solutions," suggesting the use of reward models as a future direction
  - Why unresolved: The paper only evaluates self-consistency (voting) and does not test external verification or reward models to select the best candidate
  - What evidence would resolve it: Comparative experiments showing that selecting candidates based on a reward model yields higher accuracy than frequency-based voting

- **Question:** Can structural optimization via expert activation mechanisms (e.g., sparsely activating subsets of experts) further improve training and inference efficiency?
  - Basis in paper: The authors state that current efficiency gains stem from parameter reduction rather than structural optimization and propose investigating "expert activation mechanisms" for future efficiency
  - Why unresolved: The current dynamic fusion mechanism appears to compute outputs for all experts before weighting them, rather than activating a sparse subset to save compute
  - What evidence would resolve it: A study analyzing the latency and computational cost of a sparse-activation variant of RouteDK compared to the dense fusion approach proposed in the paper

## Limitations
- The effectiveness of knowledge separation into three experts is primarily validated against a single "merged" baseline, lacking comparison with state-of-the-art multi-expert routing systems
- The router design uses a simple linear layer on pooled hidden states, and its capacity to capture complex session-level routing dynamics is untested
- Test-time self-consistency is validated only for N=8, with the cause of performance degradation at N=32 not analyzed
- Exact prompt templates for teacher distillation and precise bundle serialization format are unspecified, preventing direct replication

## Confidence
- **High confidence:** The observation that merging distilled knowledge degrades performance (Figure 1), the benefit of dynamic routing over static averaging (Figure 4), and the optimal sampling number for TTS (N=8)
- **Medium confidence:** The claim that knowledge conflict is the primary driver of performance drop, as alternative explanations are not ruled out
- **Low confidence:** The general applicability of the three-expert architecture to domains beyond the three Amazon datasets tested, and the robustness of the router under varying data distributions

## Next Checks
1. **Validate Knowledge Conflict Hypothesis:** Train and evaluate a 2-expert variant (Base + High-level only, omitting Fine-grained) on a held-out domain. If performance drops similarly to the merged baseline, it suggests task interference, not just knowledge type separation, drives the effect.
2. **Ablate Router Capacity:** Replace the current lightweight router with a deeper MLP (e.g., 2 hidden layers) and measure performance on heterogeneous domains. If gains are marginal, it confirms the current design is sufficient; if significant, it suggests router capacity was a bottleneck.
3. **Analyze TTS Failure at N=32:** Generate and inspect the 32 samples for a sample session where N=32 underperforms N=8. Categorize errors (e.g., repeated items, hallucinated bundles) to determine if the issue is sampling noise or model miscalibration.