---
ver: rpa2
title: 'Simulating Biological Intelligence: Active Inference with Experiment-Informed
  Generative Model'
arxiv_id: '2508.06980'
source_url: https://arxiv.org/abs/2508.06980
tags:
- active
- inference
- agents
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a biologically inspired active inference framework
  for modeling decision-making in embodied agents, using experiment-informed generative
  models based on partially observable Markov decision processes (POMDPs). The authors
  simulate decision-making processes in a Pong-like game environment, mirroring the
  DishBrain experiments that demonstrated learning in biological neuronal networks.
---

# Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model

## Quick Facts
- arXiv ID: 2508.06980
- Source URL: https://arxiv.org/abs/2508.06980
- Authors: Aswin Paul; Moein Khajehnejad; Forough Habibollahi; Brett J. Kagan; Adeel Razi
- Reference count: 18
- One-line primary result: Memory-based active inference agents outperform both biological neuronal networks and other AI agents in a Pong-like environment

## Executive Summary
This study presents a biologically inspired active inference framework that uses experiment-informed generative models to simulate decision-making in embodied agents. The framework employs partially observable Markov decision processes (POMDPs) to model a Pong-like game environment, mirroring the DishBrain experiments that demonstrated learning in biological neuronal networks. Three active inference algorithms are compared: classical one-step planning, dynamic programming with expected free energy, and counterfactual learning with varying memory horizons. The results demonstrate that counterfactual learning agents with memory horizons of 4 or higher significantly outperform both biological controls and other AI agents across multiple performance metrics.

## Method Summary
The researchers constructed an experiment-informed generative model based on POMDPs that mirrors the DishBrain experiments. The model includes three key state dimensions: ball-x (horizontal position), ball-y (vertical position), and paddle-y (vertical paddle position). Three active inference algorithms were implemented: AIF-1 (classical one-step planning), DPEFE (dynamic programming with expected free energy), and CFL (counterfactual learning with memory horizons ranging from 1 to 10). The agents were tested in a Pong-like environment where the objective was to return the ball to the opponent. Performance was evaluated across three metrics: average rally length, percentage of long rallies (rallies exceeding 10 ball passes), and percentage of aces (scoring points without the opponent touching the ball). The generative model structure was hand-crafted based on the specific experimental setup.

## Key Results
- CFL agents with memory horizons of 4 or higher significantly outperform biological controls and other AI agents
- Memory-based decision-making proves more effective than planning-based approaches in this environment
- Successful agents exhibit decreasing risk and entropy in decision-making parameters, indicating increasing confidence and goal-directed learning

## Why This Works (Mechanism)
The framework succeeds by combining active inference's probabilistic decision-making with biologically plausible memory structures. Counterfactual learning allows agents to evaluate multiple possible future trajectories by drawing samples from their belief distributions, then selecting actions based on expected free energy minimization. The memory component enables agents to maintain and update beliefs over extended sequences, which proves crucial for predicting ball trajectories and opponent behavior in the Pong-like environment. This approach captures the adaptive, memory-driven behavior observed in biological systems while maintaining the explainability advantages of active inference.

## Foundational Learning
- **Active Inference**: A decision-making framework that minimizes variational free energy through perception and action, explaining behavior through the lens of minimizing prediction error and maximizing model evidence
  - Why needed: Provides the theoretical foundation for biologically plausible decision-making that combines perception, learning, and action selection
  - Quick check: Understand how expected free energy balances exploration (reducing uncertainty) and exploitation (achieving preferences)

- **Generative Models**: Probabilistic models that describe how observations are generated from hidden states, allowing agents to infer latent causes of their sensory inputs
  - Why needed: Enables agents to maintain beliefs about unobservable environmental states (ball and paddle positions) from limited sensory information
  - Quick check: Verify the POMDP structure correctly captures the causal relationships between states and observations in the game environment

- **POMDPs (Partially Observable Markov Decision Processes)**: Framework for decision-making under uncertainty where agents cannot directly observe the true state of the environment
  - Why needed: Models the fundamental challenge of inferring hidden environmental states (ball position) from partial observations
  - Quick check: Ensure the transition and observation models accurately reflect the dynamics of the Pong-like game

- **Counterfactual Learning**: Decision-making approach that evaluates multiple hypothetical future scenarios by sampling from belief distributions
- **Memory Horizons**: The number of future time steps considered when making decisions, affecting the agent's ability to plan and predict
  - Why needed: Allows agents to consider longer-term consequences of actions and maintain temporal consistency in behavior
  - Quick check: Test different memory horizons to identify the optimal balance between computational cost and performance improvement

- **Expected Free Energy**: The objective function that balances epistemic value (reducing uncertainty) and pragmatic value (achieving goals) in active inference
  - Why needed: Provides a principled way to evaluate and select actions based on both information gain and task completion
  - Quick check: Verify that the expected free energy correctly captures the trade-off between exploration and exploitation in the game environment

## Architecture Onboarding

### Component Map
POMDP State Space -> Generative Model Parameters -> Active Inference Algorithm -> Action Selection -> Environment Response -> New Observations

### Critical Path
Observation → Belief Update (Bayesian inference) → Expected Free Energy Evaluation → Action Selection → Environment Transition → New Observation

### Design Tradeoffs
The study uses hand-crafted generative models rather than learned structures, trading scalability for biological plausibility and interpretability. Counterfactual learning with memory provides superior performance but at higher computational cost compared to one-step planning approaches. The choice of memory horizon represents a key tradeoff between decision quality and computational efficiency.

### Failure Signatures
Poor performance across all metrics suggests issues with generative model structure or parameter initialization. Specifically poor performance in long rallies but reasonable short-term behavior indicates insufficient memory horizon. High variance in performance suggests instability in belief updates or expected free energy calculations.

### First Experiments
1. Compare performance across different memory horizons (1, 2, 4, 8, 10) to identify optimal memory length
2. Test alternative generative model structures with different state space granularities
3. Evaluate the impact of varying initial parameter values on convergence and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does sophisticated memory (as modeled in CFL) drive biological learning in DishBrain, or is rapid adaptive restructuring alone sufficient to underpin the observed behaviours?
- Basis in paper: [explicit] The Discussion states, "A critical consideration for future work is... determining whether sophisticated memory, as modelled, is essential for biological learning, or if rapid adaptive restructuring alone underpins the observed behaviours."
- Why unresolved: While CFL agents with memory outperformed biological cultures, biological evidence suggests "rapid network reorganisation" rather than explicit memory recall.
- What evidence would resolve it: Experiments specifically designed to distinguish between long-term memory retention and short-term adaptive plasticity in biological neuronal networks within the DishBrain system.

### Open Question 2
- Question: Can the structure of generative models be self-learned in this context, rather than being manually defined?
- Basis in paper: [explicit] In Methods, the authors note, "Algorithms for self-learning the structure of generative models are a promising direction to pursue."
- Why unresolved: The current study relies on a hand-crafted POMDP structure based on specific experimental setups (dimensions of ball-x, ball-y, paddle-y), limiting scalability.
- What evidence would resolve it: Integration of structure learning algorithms (e.g., supervised structure learning) that allow the agent to autonomously infer state dimensions and transition dynamics.

### Open Question 3
- Question: How sensitive are the performance differences between planning-based (DPEFE) and memory-based (CFL) schemes to the specific structure of the generative model?
- Basis in paper: [explicit] The Results section notes, "We leave the detailed analysis of parameters and testing different structures of the generative models to future work."
- Why unresolved: The study evaluates different decision schemes but does not extensively test how variations in the underlying generative model (e.g., state space granularity) affect the relative efficacy of planning vs. counterfactual learning.
- What evidence would resolve it: Ablation studies varying the complexity and dimensionality of the generative model to observe the impact on CFL vs. DPEFE performance.

### Open Question 4
- Question: Why does the entropy of the prior preference distribution ($C$) increase in planning agents, and does this confirm that agents lack specific state preferences?
- Basis in paper: [inferred] In Section 3.3, the authors observe an increase in the entropy of $C$ for DP-5 and AIF-1 agents, noting it is "counter-intuitive" and interpreting it as the agent learning that "no particular ball or paddle position is preferred."
- Why unresolved: The explanation is presented as an interpretation ("possible explanation") rather than a verified result; increasing entropy could also imply a failure to converge on a stable preference.
- What evidence would resolve it: Analysis of the correlation between preference entropy and task performance, or modifying the reward structure to enforce specific positional preferences to see if entropy decreases.

## Limitations
- Results are based on a single, simplified Pong-like environment, limiting generalizability to more complex or real-world scenarios
- Biological plausibility claims remain largely theoretical without direct neural validation
- The interpretation of decreasing risk and entropy as evidence of increasing confidence requires further empirical support across diverse tasks

## Confidence
- Active inference effectiveness: Medium-High
- Memory benefits: Medium-High
- Biological implications: Medium
- Generalizability: Low-Medium

## Next Checks
1. Test the counterfactual learning approach across multiple game environments with varying complexity to assess generalizability
2. Conduct ablation studies to determine which components of the memory-based decision-making contribute most to performance improvements
3. Implement direct neural correlates to validate the biological plausibility claims and parameter interpretations