---
ver: rpa2
title: AI Generated Text Detection Using Instruction Fine-tuned Large Language and
  Transformer-Based Models
arxiv_id: '2507.05157'
source_url: https://arxiv.org/abs/2507.05157
tags:
- text
- bert
- task-b
- task-a
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the problem of distinguishing human-written\
  \ text from AI-generated text and identifying the specific LLM model responsible\
  \ for generation. The authors fine-tune three models\u2014GPT-4o-mini, LLaMA-3 8B,\
  \ and BERT\u2014on the Defactify dataset using simple instruction prompts."
---

# AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models

## Quick Facts
- arXiv ID: 2507.05157
- Source URL: https://arxiv.org/abs/2507.05157
- Reference count: 16
- Primary result: Fine-tuned GPT-4o-mini and BERT achieved 95.47% accuracy for binary detection, but multi-class model attribution remained challenging at 47% accuracy

## Executive Summary
This work addresses the challenge of distinguishing human-written text from AI-generated text and identifying the specific LLM model responsible for generation. The authors fine-tune three models—GPT-4o-mini, LLaMA-3 8B, and BERT—on the Defactify dataset using simple instruction prompts. Task-A is treated as binary classification (human vs. machine), while Task-B is multi-class classification (identifying the specific LLM). The fine-tuned GPT-4o-mini and BERT models achieved an accuracy of 0.9547 for Task-A and 0.4698 for Task-B. BERT performed exceptionally well on validation data (F1 = 100% for Task-A, 98% for Task-B) but showed lower performance on the test set, suggesting potential overfitting or domain shift. LLaMA-3 8B showed moderate performance, while GPT-4o-mini faced content filtering issues during inference, slightly limiting its effectiveness. The results highlight the effectiveness of simple prompts for detection but indicate the need for more complex strategies and hyperparameter optimization for model attribution.

## Method Summary
The authors fine-tune three models on the Defactify dataset using instruction prompts for two tasks: Task-A (binary classification: human vs. machine) and Task-B (multi-class classification: identify which of 6 LLMs or human wrote the text). GPT-4o-mini was fine-tuned for Task-A using Azure OpenAI API, LLaMA-3 8B was fine-tuned for both tasks using Unsloth with 4-bit LoRA on A100 80GB GPU, and BERT was fine-tuned using ktrain library. Training data consisted of 51,147 samples for training, 10,983 for validation, and 10,963 for testing, with approximately 1:6 human to machine ratio. The models used simple instruction-formatted prompts during both training and inference, with BERT using text-label pairs and LLMs using prompt-instruction format.

## Key Results
- Fine-tuned GPT-4o-mini and BERT achieved 0.9547 accuracy for Task-A (binary detection)
- BERT achieved 0.4698 F1 for Task-B (model attribution) on test data
- BERT showed perfect validation performance (100% F1 for Task-A, 98% for Task-B) but dropped significantly on test set (76.7% and 47% respectively)
- GPT-4o-mini content filtering blocked ~200 test samples, limiting performance by approximately 2%

## Why This Works (Mechanism)

### Mechanism 1
Instruction fine-tuning enables LLMs to perform binary classification of human vs. machine text with high accuracy. The authors append simple prompt instructions (e.g., "classify whether the given text is written by 'human' or 'machine'") to input text during fine-tuning. The model learns to associate stylistic patterns in the input with class labels through supervised learning on ~51K labeled examples. Core assumption: The model can internalize discriminative features (lexical diversity, syntactic regularity) that differentiate human from machine text through gradient updates on the instruction-formatted data.

### Mechanism 2
Encoder-only models (BERT) can achieve near-perfect validation performance but may overfit due to memorization of training distribution signatures. BERT's bidirectional attention captures contextual relationships across the full sequence. With sufficient training epochs on a limited vocabulary of source models, it may memorize model-specific artifacts rather than learning generalizable features. Core assumption: Overfitting occurs when the model learns spurious correlations specific to the training LLMs rather than transferable detection signals.

### Mechanism 3
Model attribution (identifying which LLM generated text) requires more sophisticated approaches than binary detection. Task-B requires distinguishing among 7 classes (6 LLMs + human). The fine-grained differences between outputs from different LLMs are subtler than human vs. machine distinctions, requiring the model to learn generator-specific "fingerprints" in syntax, lexical choice, and coherence patterns. Core assumption: Each LLM has distinctive stylistic signatures that survive fine-tuning detection, but these are harder to capture with simple prompts and limited training data.

## Foundational Learning

- **Concept: Instruction Fine-Tuning**
  - Why needed here: The method transforms a pre-trained LLM into a task-specific classifier by training on prompt-label pairs rather than raw text alone
  - Quick check question: Can you explain why appending instructions during both training and inference maintains consistency in model behavior?

- **Concept: Domain Shift and Overfitting**
  - Why needed here: The dramatic gap between BERT's validation (100% F1) and test performance (76.7% F1 for Task-A, 47% for Task-B) suggests domain shift between validation and test sets
  - Quick check question: What evidence would help determine whether this performance drop is due to overfitting versus genuine domain shift in the test data?

## Architecture Onboarding
The paper fine-tunes three distinct model architectures: GPT-4o-mini (a decoder-only LLM), LLaMA-3 8B (another decoder-only LLM), and BERT (an encoder-only model). Each model requires different training approaches: GPT-4o-mini uses Azure OpenAI API for fine-tuning, LLaMA-3 8B uses Unsloth with 4-bit LoRA optimization on A100 80GB GPU, and BERT uses the ktrain library. The choice of architecture affects both training efficiency and detection performance, with BERT showing exceptional validation results but significant test-set degradation.

## Open Questions the Paper Calls Out
- How to improve model attribution accuracy beyond 47% for Task-B
- Whether more complex prompt engineering or additional training data could reduce the validation-test performance gap
- How content filtering limitations in GPT-4o-mini affect real-world deployment scenarios
- Whether the observed overfitting in BERT can be mitigated through regularization or different validation strategies

## Limitations
The study has several limitations: GPT-4o-mini's content filtering blocked approximately 200 test samples, limiting evaluation completeness; BERT showed severe overfitting with perfect validation performance but significant test-set degradation; the 1:6 human-to-machine ratio in training data may affect generalization; and the simple prompt-based approach may not capture complex stylistic differences needed for accurate model attribution.

## Confidence
High confidence in reported results, as the paper provides detailed methodology and quantitative results. However, the significant gap between validation and test performance for BERT raises questions about the robustness of these findings and suggests potential issues with either overfitting or domain shift that warrant further investigation.

## Next Checks
- Verify the actual content filtering criteria used by GPT-4o-mini that blocked test samples
- Examine the distribution differences between validation and test sets to assess domain shift
- Test additional regularization techniques to address BERT's overfitting
- Evaluate whether more sophisticated prompt engineering could improve Task-B accuracy
- Compare performance across different training data ratios to assess impact of class imbalance