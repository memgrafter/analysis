---
ver: rpa2
title: Concealed Adversarial attacks on neural networks for sequential data
arxiv_id: '2502.20948'
source_url: https://arxiv.org/abs/2502.20948
tags:
- attack
- adversarial
- attacks
- data
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel concealed adversarial attack method
  for time series data, addressing the challenge of creating realistic perturbations
  that are hard to detect by both humans and machine learning models. The proposed
  approach simultaneously optimizes a target model's loss and a discriminator's loss,
  with the discriminator trained to detect adversarial examples.
---

# Concealed Adversarial attacks on neural networks for sequential data

## Quick Facts
- arXiv ID: 2502.20948
- Source URL: https://arxiv.org/abs/2502.20948
- Reference count: 40
- Novel method creates realistic adversarial perturbations for time series data that are hard to detect by both humans and ML models

## Executive Summary
This paper introduces a novel concealed adversarial attack method for time series data that addresses the challenge of creating realistic perturbations that are hard to detect by both humans and machine learning models. The proposed approach simultaneously optimizes a target model's loss and a discriminator's loss, with the discriminator trained to detect adversarial examples. A key innovation is a training procedure for the discriminator that progressively handles attacks of varying strengths. Experiments on six diverse UCR time series datasets across four architectures demonstrate the superiority of the proposed method in balancing attack efficiency and concealability.

## Method Summary
The proposed concealed adversarial attack method for time series data simultaneously optimizes a target model's loss and a discriminator's loss during the attack generation process. The discriminator is trained to detect adversarial examples and is integrated into the attack optimization loop. A key innovation is the progressive training procedure for the discriminator, which is trained on attacks of varying strengths (controlled by a parameter epsilon) in a staged manner. The method uses a combination of adversarial loss and discriminator loss to generate perturbations that are both effective at fooling the target model and difficult to detect as adversarial. The approach is evaluated on six UCR time series datasets using four different model architectures.

## Key Results
- The proposed method significantly outperforms vanilla attacks and the SGM baseline in balancing attack efficiency and concealability
- Success rates often exceed 0.8 across various model-dataset combinations
- The progressive discriminator training procedure effectively handles attacks of varying strengths
- The method demonstrates consistent superiority across six diverse UCR datasets and four different architectures (ResCNN, RNNAttention, S4, PatchTST)

## Why This Works (Mechanism)
The method works by creating a adversarial game between the attack generator and the discriminator. The generator tries to create perturbations that fool both the target model and the discriminator, while the discriminator tries to detect adversarial examples. This dual optimization creates perturbations that are both effective and concealed. The progressive training of the discriminator allows it to handle attacks of different strengths effectively, improving the overall quality of the generated adversarial examples.

## Foundational Learning

**Adversarial attacks on neural networks**
- Why needed: Understanding the threat model and attack objectives
- Quick check: Can describe the difference between white-box and black-box attacks

**Time series classification**
- Why needed: Understanding the specific challenges of sequential data
- Quick check: Can explain how time series differ from image data in terms of attack vulnerability

**Generative Adversarial Networks (GANs)**
- Why needed: The attack method uses a similar dual optimization framework
- Quick check: Can describe the basic GAN architecture and training process

## Architecture Onboarding

**Component Map:**
Attack Generator -> Target Model -> Discriminator -> Loss Function

**Critical Path:**
1. Generate perturbation using target model's loss
2. Update perturbation using discriminator's feedback
3. Evaluate success on target model
4. Train discriminator on generated adversarial examples

**Design Tradeoffs:**
- Balance between attack effectiveness and concealability
- Computational cost of dual optimization
- Progressive discriminator training complexity

**Failure Signatures:**
- High discriminator detection rate
- Low target model misclassification rate
- Visible artifacts in perturbed time series

**3 First Experiments:**
1. Test on a simple dataset with one model architecture
2. Compare against vanilla FGSM attack
3. Evaluate with different epsilon values

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on white-box attacks assumes complete access to target model
- Does not address performance against adaptive defenses like adversarial training
- Primarily tested on UCR time series classification archive

## Confidence
- Superiority claim: High
- Discriminator training effectiveness: Medium
- Generalizability to other domains: Medium

## Next Checks
1. Test the proposed method against a range of adaptive defenses, including adversarial training and input transformation techniques, to assess its robustness in more realistic attack scenarios.
2. Evaluate the method's performance on non-UCR time series datasets, particularly those from different domains such as healthcare or finance, to ensure generalizability across diverse applications.
3. Conduct a comprehensive analysis of the method's computational complexity and scalability, especially when applied to longer time series or more complex model architectures.