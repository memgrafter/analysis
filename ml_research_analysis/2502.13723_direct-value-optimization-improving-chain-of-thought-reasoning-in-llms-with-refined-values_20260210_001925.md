---
ver: rpa2
title: 'Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with
  Refined Values'
arxiv_id: '2502.13723'
source_url: https://arxiv.org/abs/2502.13723
tags:
- arxiv
- reasoning
- value
- preprint
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Direct Value Optimization (DVO), a reinforcement
  learning framework that enhances large language models' reasoning abilities by optimizing
  at the individual step level using value signals rather than preference labels.
  DVO estimates stepwise values using Monte Carlo Tree Search or an outcome value
  model, then aligns the policy model with these values using mean squared error loss,
  eliminating the need for labor-intensive human annotations.
---

# Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values

## Quick Facts
- arXiv ID: 2502.13723
- Source URL: https://arxiv.org/abs/2502.13723
- Reference count: 24
- Key outcome: DVO improves LLM reasoning accuracy on GSM8K from 74.6% to 80.6%, MATH from 22.5% to 26.5%, and AGIEval-Math from 23.5% to 27.9% through step-level value optimization

## Executive Summary
This paper introduces Direct Value Optimization (DVO), a reinforcement learning framework that enhances large language models' reasoning abilities by optimizing at the individual step level using value signals rather than preference labels. DVO estimates stepwise values using Monte Carlo Tree Search or an outcome value model, then aligns the policy model with these values using mean squared error loss, eliminating the need for labor-intensive human annotations. Experiments on mathematical and commonsense reasoning tasks show DVO consistently outperforms existing offline preference optimization techniques.

## Method Summary
DVO fine-tunes a backbone LLM on self-generated correct reasoning paths, then iteratively trains using MCTS-estimated step-level values. For each training question, MCTS performs tree search and propagates final outcomes back up to assign values to intermediate steps. The policy is updated via MSE loss aligning log-probabilities with estimated returns. This process repeats for multiple rounds, with each iteration generating new data using the updated policy. The method uses β=0.1 for KL regularization and runs for 3 epochs per round with cosine learning rate scheduling.

## Key Results
- Three rounds of DVO training improve Llama3-8B-Instruct's accuracy on GSM8K from 74.6% to 80.6%
- MATH performance increases from 22.5% to 26.5% after DVO training
- AGIEval-Math scores improve from 23.5% to 27.9%
- DVO consistently outperforms existing offline preference optimization techniques on mathematical and commonsense reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Step-level Value Signals vs. Preference Labels
DVO provides finer-grained supervision by optimizing individual reasoning steps using continuous value signals rather than binary preference labels. The framework estimates a target value for each step and trains the policy to align with these values using MSE loss, treating the policy's log-probability as a soft Q-function. This assumes continuous value signals offer more informative absolute rankings compared to pairwise comparisons.

### Mechanism 2: Model as Soft Q-Function and Implicit Reward
DVO reinterprets the language model as an optimal soft Q-function, deriving an implicit reward from policy and reference model log-probabilities. This avoids the need for a separate reward model by leveraging maximum entropy RL theory to express the Q-function as β·log πθ + Vsoft. The framework assumes maximum entropy RL correctly applies to LLM step generation.

### Mechanism 3: Monte Carlo Tree Search for Offline Value Estimation
MCTS is used in an offline setting to generate accurate step-level value estimates from final outcomes. The approach builds a search tree where nodes are reasoning steps, computing values by aggregating outcomes from simulated paths. This assumes reasoning errors are step-local and that sampling multiple paths provides reliable intermediate step quality signals.

## Foundational Learning

- **Markov Decision Process (MDP)**: The paper formulates reasoning as a stepwise MDP with states (current question + prior steps), actions (next reasoning step), and rewards (step quality). Understanding MDP components is essential for grasping how DVO applies RL concepts to text generation.
  - *Quick check question*: In the stepwise MDP described for DVO, what represents a "state" and what represents an "action"?

- **Soft Q-Learning / Maximum Entropy Reinforcement Learning**: DVO is built upon soft Q-learning theory where optimal policy maximizes both reward and entropy. This framework allows expressing the policy as an exponential function of the Q-value, enabling the model-to-Q-function bridge.
  - *Quick check question*: What is the primary difference in the optimal policy between standard Q-learning and soft Q-learning?

- **Direct Preference Optimization (DPO)**: DVO evolves from DPO, solving the problem of lost fine-grained information in preference labels. Understanding DPO's cross-entropy objective helps contrast with DVO's MSE approach.
  - *Quick check question*: Why does DVO use an MSE loss instead of the cross-entropy loss used in DPO?

## Architecture Onboarding

- **Component map**: Policy Model (πθ) -> MCTS Value Estimator -> DVO Loss Function -> Policy Update -> Reference Model (πref)
- **Critical path**:
  1. Self-Generated Data SFT: Backbone fine-tuned on self-generated correct reasoning paths
  2. Offline Data Generation (MCTS): Run MCTS with current policy, propagate outcomes to assign values, sample positive/negative examples
  3. Policy Update (DVO Training): Minimize DVO loss (Eq. 10) to align policy with estimated step values
  4. Iterative Training: Repeat process with updated policy as generator

- **Design tradeoffs**:
  - MCTS vs. Value Model: MCTS chosen for better performance despite computational cost during data generation
  - β parameter: Set to 0.1 to balance KL regularization strength
  - Search iterations: Performance degrades below 40 iterations despite higher computational cost

- **Failure signatures**:
  - Degraded performance vs. baseline indicates noisy value estimates or misconfigured β
  - Reward hacking if model exploits MCTS estimation flaws
  - Mode collapse if policy deviates too far from reference model

- **First 3 experiments**:
  1. Reproduce baseline ablation on β: Test different β values (0.05, 0.1, 0.5) to confirm sensitivity and optimal point
  2. Ablation on MCTS search iterations: Compare performance with 10 vs. 80 iterations to validate value signal quality
  3. Step-level vs. response-level DVO: Implement simplified DVO treating entire response as single action to demonstrate fine-grained supervision value

## Open Questions the Paper Calls Out
None

## Limitations
- MCTS computational overhead is significant during data generation and not thoroughly analyzed for scaling
- Performance improvements measured against preference-based baselines but not compared to other value-based RL approaches
- Method's claim of avoiding human annotations is context-specific to existing datasets rather than universally validated

## Confidence
- **High Confidence**: Core DVO framework (MSE loss aligning policy with step-level values) is well-defined and theoretically grounded
- **Medium Confidence**: MCTS outperforming trained value model claim supported but lacks detailed ablation studies on search parameters
- **Low Confidence**: Broader claim about eliminating annotation requirements for all reasoning tasks not validated

## Next Checks
1. **Ablation on MCTS Search Parameters**: Run experiments varying PUCT exploration constant and maximum iterations to quantify impact on final accuracy
2. **Scaling Analysis**: Test DVO on a significantly larger reasoning dataset (≥100K problems) to assess computational overhead and performance saturation
3. **Comparison to Value-Based RL Baselines**: Implement and compare DVO against other value-based RL approaches for LLMs to strengthen superiority claims