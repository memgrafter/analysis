---
ver: rpa2
title: 'HT-Transformer: Event Sequences Classification by Accumulating Prefix Information
  with History Tokens'
arxiv_id: '2508.01474'
source_url: https://arxiv.org/abs/2508.01474
tags:
- history
- tokens
- sequence
- event
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HT-Transformer addresses the limitation of standard Transformers
  in sequence classification by introducing history tokens that accumulate contextual
  information during pretraining via next-token prediction. The method employs a custom
  attention mask allowing history tokens to gather past information while event tokens
  selectively attend to them.
---

# HT-Transformer: Event Sequences Classification by Accumulating Prefix Information with History Tokens

## Quick Facts
- arXiv ID: 2508.01474
- Source URL: https://arxiv.org/abs/2508.01474
- Authors: Ivan Karpukhin; Andrey Savchenko
- Reference count: 7
- Primary result: Achieves state-of-the-art ROC AUC on multiple event sequence classification benchmarks by introducing learnable history tokens that accumulate contextual information

## Executive Summary
HT-Transformer addresses the fundamental limitation of standard Transformers in sequence classification by introducing history tokens that serve as learnable information bottlenecks. These special tokens are inserted at sampled positions during pretraining and allowed to attend to all preceding event tokens, forcing them to compress contextual information analogous to RNN hidden states. The method employs a custom attention mask and employs a decoder-only Transformer architecture, enabling effective sequence embeddings for downstream classification tasks. Evaluated across finance, healthcare, and e-commerce benchmarks, HT-Transformer achieves state-of-the-art results after supervised fine-tuning, outperforming standard Transformers by up to 0.96% ROC AUC on the Churn dataset.

## Method Summary
HT-Transformer introduces history tokens as learnable information bottlenecks that accumulate prefix context through a constrained attention mechanism. During pretraining, history tokens are inserted at sampled positions (typically 10% of sequence length) with application probability p=0.5, and allowed to attend to all preceding event tokens while event tokens have restricted attention windows. The method employs a custom attention mask where history tokens act as memory anchors, gathering contextual information that serves as sequence embeddings for downstream tasks. Pretraining uses next-token prediction with combined MAE and cross-entropy losses, followed by supervised fine-tuning with frozen embeddings or end-to-end training. The architecture includes event embedding, positional encoding, and a decoder-only Transformer backbone with custom attention masks supporting history tokens.

## Key Results
- Achieves state-of-the-art ROC AUC on Churn dataset, outperforming standard Transformers by 0.96 percentage points
- History tokens serve as effective sequence embeddings, enabling downstream classification without additional architectural modifications
- Random selection strategy for history token attention consistently outperforms Last selection strategy
- Bias-End placement strategy reduces train-inference distribution shift and improves performance on future-oriented tasks
- Strong adaptability to global classification problems when fine-tuned with supervised learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: History tokens create a learnable information bottleneck that compresses prefix context into a fixed-size representation, analogous to RNN hidden states.
- Mechanism: Special tokens are injected into the sequence with a constrained attention mask allowing them to attend to all preceding event tokens while blocking attention to other history tokens. This forces each history token to aggregate sequence context through gradient pressure during next-token prediction pretraining.
- Core assumption: Next-token prediction loss provides sufficient training signal for history tokens to learn meaningful compression of temporal context (not explicitly validated in isolation).
- Evidence anchors:
  - [abstract] "history tokens, guided by carefully constructed attention masks, act as bottlenecks that gather and summarize contextual information, analogous to hidden states in RNNs"
  - [section: Proposed Method] "Each history token is allowed to attend to all preceding event tokens (except other history tokens), thereby accumulating contextual information"
  - [corpus] Weak direct evidence; ITPP (arXiv:2511.06032) addresses similar disentanglement in temporal point processes but uses different mechanisms
- Break condition: If downstream tasks require global sequence properties rather than local/recent context, bottleneck may discard relevant information (validated in toy dataset experiments).

### Mechanism 2
- Claim: Restricting event token attention to local windows plus history tokens reduces the need for global attention while maintaining predictive capacity.
- Mechanism: Event tokens attend only to: (1) tokens between current position and most recent history token, and (2) the history token itself. This creates a hierarchical information flow where history tokens serve as memory anchors.
- Core assumption: The "Random" selection strategy—where event tokens randomly choose among preceding history tokens—provides regularization that improves generalization (assumption: randomization acts as dropout-like regularizer).
- Evidence anchors:
  - [section: Proposed Method] "In the Random strategy... each event token selects one of the preceding history tokens at random during attention computation. As demonstrated in our experiments, the Random strategy yields consistently better performance"
  - [section: Experiments] Table 5 shows Random selection outperforming Last selection on average (86.20 vs 86.29 pre-SFT, but Random+BE achieves 86.66)
  - [corpus] SCOUT (arXiv:2509.00935) explores segment compression for sub-quadratic attention but uses deterministic compression rather than learned bottlenecks
- Break condition: If sequence length exceeds the spacing between history tokens by large margins, information may be lost between bottleneck points.

### Mechanism 3
- Claim: Bias-End placement strategy reduces train-inference distribution shift by aligning history token positions during pretraining with their fixed end-of-sequence position at inference.
- Mechanism: During pretraining, history tokens are sampled from range [μ/2, L] where μ is mean batch sequence length and L is maximum length. At inference, a single history token is always placed at sequence end. Partial application probability (p=50%) further reduces discrepancy.
- Core assumption: The mismatch between distributed history tokens during training and single end-position token at inference is the primary source of performance degradation (not directly ablated against other sources of train-test shift).
- Evidence anchors:
  - [section: Proposed Method] "this approach can lead to a discrepancy between training and inference, as history tokens are typically positioned near the end of the sequence during evaluation"
  - [section: Ablation Studies] Table 5 shows Bias-End (83.34/92.00/84.65) outperforming Uniform placement (83.23/91.92/83.72) on 2 of 3 datasets
  - [corpus] No direct corpus evidence for this specific placement strategy
- Break condition: If sequences have highly variable important regions (not necessarily near the end), Bias-End may miss critical early information.

## Foundational Learning

- Concept: **Causal (autoregressive) attention masks**
  - Why needed here: HT-Transformer builds on decoder-only architectures where each position only attends to preceding positions. Understanding this constraint is essential for grasping why history tokens require special mask modifications.
  - Quick check question: Given a 5-token sequence [A, B, C, D, E] with standard causal masking, which tokens can position C attend to?

- Concept: **Information bottlenecks in sequence modeling**
  - Why needed here: The core innovation is creating a learnable bottleneck via attention constraints. Without understanding bottleneck theory (variational autoencoders, compression-prediction tradeoffs), the design motivation remains opaque.
  - Quick check question: Why might forcing information through a lower-capacity representation improve downstream generalization?

- Concept: **Train-inference distribution shift**
  - Why needed here: Multiple design choices (Bias-End placement, application probability p<1) explicitly address mismatch between pretraining and inference configurations. Understanding this problem class helps evaluate whether these mitigations are sufficient.
  - Quick check question: What happens if you always apply history tokens during pretraining but only use them at inference for embedding extraction?

## Architecture Onboarding

- Component map:
  1. Event Embedder: Transforms raw events (timestamp + categorical + numerical fields) into dense vectors via separate embedding tables and projection
  2. Positional Encoding: Time-based sinusoidal encoding (Equation 1) using dataset-specific constants m, M
  3. Transformer Backbone: Decoder-only with modified attention mask accepting history tokens
  4. History Token Injector: Inserts learnable tokens at sampled positions (controlled by frequency f, Bias-End vs Uniform strategy)
  5. Attention Mask Generator: Creates sparse masks implementing Last/Random selection strategies
  6. Prediction Heads: MAE loss for timestamps/numerical fields, cross-entropy for categorical attributes

- Critical path:
  1. Preprocessing → Event embeddings + positional encoding
  2. History token injection at sampled positions (frequency f, Bias-End strategy)
  3. Attention mask construction based on token types and selection strategy
  4. Forward pass through causal Transformer with custom mask
  5. Next-token prediction loss computation (MAE + cross-entropy weighted sum)
  6. At inference: single history token at end → average hidden states across layers → embedding for downstream classifier

- Design tradeoffs:
  - **History token frequency (f)**: Higher f provides more aggregation points but increases compute. Paper finds f≈10% of input length sufficient; even single token often competitive (Figure 5)
  - **Application probability (p)**: p=0.5 balances train-inference alignment with model robustness. p=1.0 shows degradation on Churn dataset (Figure 6)
  - **Placement strategy**: Bias-End better for future-oriented tasks; Uniform may be preferable for global sequence classification (Table 4 toy experiments)
  - **Selection strategy**: Random provides regularization; Last is simpler and sometimes competitive

- Failure signatures:
  - **Degraded performance on global classification tasks**: If downstream task requires whole-sequence understanding (e.g., AgePred age group prediction), history tokens may over-emphasize recent context. Solution: contrastive pretraining or embedding averaging may be more appropriate
  - **High variance with low application probability**: Figure 6 shows p<0.25 causes instability. Ensure p≥0.5
  - **Underfitting with excessive history tokens**: Figure 5 shows minimal gains beyond f=0.1; over-bottlenecking may limit model capacity

- First 3 experiments:
  1. **Baseline comparison on single dataset**: Implement NTP Transformer vs HT-Transformer on Churn or MIMIC-III with frozen embeddings + LightGBM classifier. Expect ~1-2 percentage point improvement. This validates the core mechanism before investing in full pipeline.
  2. **Ablation of application probability**: Train with p∈{0, 0.25, 0.5, 0.75, 1.0} on one dataset. Plot validation metric vs p to confirm 0.5 optimum and understand failure mode at extremes.
  3. **Placement strategy comparison**: Compare Uniform vs Bias-End on a dataset with known global vs local task structure. If possible, construct synthetic data analogous to Figure 4 to validate local/global tradeoff hypothesis.

## Open Questions the Paper Calls Out

- **Optimizing attention for sparsity**: The current reliance on standard PyTorch multi-head attention is suboptimal for the custom sparse masks required by HT-Transformer. A custom attention kernel tailored for the HT-Transformer mask could demonstrate reduced training time and FLOPs compared to standard causal Transformers.

- **Optimal positioning and sampling policies**: The study compared only two placement strategies (Uniform vs Bias-End) and noted a discrepancy between training and inference setups. A comprehensive search for optimal positioning strategies across diverse sequence lengths and domains remains unexplored.

- **Modifying pretraining objective for global properties**: The current NTP objective focuses on recent context, causing HT-Transformer to underperform on global classification tasks during pretraining. Experiments combining HT-Transformer with auxiliary global losses (e.g., contrastive learning) could improve global performance without degrading local prediction metrics.

- **Why Random selection outperforms Last**: The authors report Random strategy consistently outperforms Last selection but do not provide theoretical explanation. Visualization of attention patterns and representational similarity analysis for both strategies could determine if randomness acts as a regularizer or provides more diverse semantic summary.

## Limitations

- **Architecture specification gaps**: Key hyperparameters including number of Transformer layers, hidden dimension size, attention head count, and loss weights are not reported, making faithful reproduction difficult and raising questions about whether performance gains are architecture-dependent.

- **Specialized application scope**: The method shows strong performance on future-oriented prediction tasks but explicitly degrades on global classification problems, suggesting the approach may be too specialized for broader adoption without modifications.

- **Implementation complexity**: The custom attention mask and history token injection mechanism add significant implementation complexity compared to standard Transformers, with no open-source code or detailed pseudocode provided for exact reproduction.

## Confidence

- **High Confidence**: The core mechanism of history tokens as information bottlenecks is well-supported by both theoretical reasoning and experimental results. The toy dataset experiments provide convincing evidence for the local vs global task distinction.

- **Medium Confidence**: The effectiveness of the Bias-End placement strategy and application probability p=0.5 is demonstrated empirically but lacks theoretical justification. The ablation studies show statistical significance but the magnitude of improvements (1-2 percentage points) suggests the method may be sensitive to implementation details.

- **Low Confidence**: The claim that Random selection strategy consistently outperforms Last selection is based on limited comparisons. Table 5 shows only modest improvements, and the paper does not investigate why Random provides regularization benefits or whether other stochastic strategies might work better.

## Next Checks

1. **Architecture Ablation Study**: Implement HT-Transformer with varying numbers of layers (1, 3, 6) and hidden dimensions (256, 512, 1024) on a single dataset (e.g., Churn). This will determine whether performance gains are due to the history token mechanism itself or simply increased model capacity compared to baseline Transformers.

2. **Train-Inference Distribution Alignment**: Train models with application probabilities p∈{0.25, 0.5, 0.75, 1.0} and measure the gap between pretraining performance (when history tokens are present) and inference performance (single end token). This quantifies the actual train-inference shift and tests whether the p<1.0 strategy truly mitigates the distribution mismatch.

3. **Cross-Dataset Transferability**: Pretrain HT-Transformer on one dataset (e.g., MIMIC-III) and fine-tune on a different dataset (e.g., Churn) to evaluate whether history tokens learn task-agnostic sequence representations. Compare transfer learning performance against standard pretrained Transformers to assess the generality of the learned embeddings.