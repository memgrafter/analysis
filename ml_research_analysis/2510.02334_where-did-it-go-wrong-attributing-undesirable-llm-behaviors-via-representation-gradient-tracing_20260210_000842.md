---
ver: rpa2
title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation
  Gradient Tracing
arxiv_id: '2510.02334'
source_url: https://arxiv.org/abs/2510.02334
tags:
- data
- arxiv
- training
- attribution
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RepT, a framework for tracing undesirable
  behaviors in large language models (LLMs) by analyzing representation gradients
  in the activation space. Traditional gradient-based attribution methods are computationally
  expensive and suffer from noisy signals due to the high dimensionality of model
  parameters.
---

# Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing

## Quick Facts
- arXiv ID: 2510.02334
- Source URL: https://arxiv.org/abs/2510.02334
- Reference count: 19
- Primary result: Introduces RepT framework achieving near-perfect precision in identifying problematic training data for undesirable LLM behaviors

## Executive Summary
This paper introduces RepT, a framework for tracing undesirable behaviors in large language models (LLMs) by analyzing representation gradients in the activation space. Traditional gradient-based attribution methods are computationally expensive and suffer from noisy signals due to the high dimensionality of model parameters. RepT addresses these limitations by shifting attribution from the parameter space to the representation space, using the hidden states and their gradients to link model outputs to their training data origins.

The framework operates at two granularities: sample-level attribution for identifying influential training documents and token-level attribution for pinpointing specific causal phrases. Evaluated across three tasks—harmful content generation, backdoor poisoning detection, and knowledge contamination attribution—RepT consistently outperforms existing methods, achieving near-perfect precision in identifying problematic training data. It is also more efficient, requiring less memory and computation compared to gradient-based baselines. This work provides a scalable and interpretable diagnostic tool for auditing and mitigating risks in LLMs.

## Method Summary
RepT introduces a representation-based attribution framework that traces undesirable LLM behaviors by analyzing gradients in the activation space rather than the parameter space. The method computes gradients of model outputs with respect to hidden states, then aggregates these gradients across tokens and layers to attribute behaviors to specific training samples or phrases. This approach reduces computational complexity while maintaining attribution accuracy. The framework provides both sample-level attribution (identifying which training documents contribute to undesirable behaviors) and token-level attribution (identifying specific causal phrases within those documents). The method is evaluated across three distinct task categories to demonstrate its effectiveness in real-world scenarios.

## Key Results
- Achieved near-perfect precision in identifying problematic training data across multiple undesirable behavior scenarios
- Demonstrated superior efficiency compared to gradient-based baselines, requiring less memory and computation
- Consistently outperformed existing attribution methods in harmful content generation, backdoor poisoning detection, and knowledge contamination attribution tasks

## Why This Works (Mechanism)
RepT works by shifting the attribution problem from the high-dimensional parameter space to the more tractable representation space. By computing gradients with respect to hidden states rather than parameters, the framework captures how changes in internal representations affect model outputs. This approach leverages the fact that hidden states encode compressed, task-relevant information while reducing the noise inherent in parameter-space gradients. The aggregation of gradients across tokens and layers creates a signal that can be traced back to specific training examples, enabling both sample-level and token-level attribution.

## Foundational Learning
- **Representation space vs parameter space**: Understanding the difference between attributing behaviors in high-dimensional parameter space versus compressed activation space; needed because parameter space attribution is computationally prohibitive for large models; quick check: verify gradient dimensionality reduction benefits
- **Hidden state gradients**: Computing gradients of outputs with respect to intermediate activations rather than final parameters; needed to capture causal relationships between inputs and behaviors; quick check: ensure gradient flow through all relevant layers
- **Gradient aggregation**: Combining gradients across tokens and layers to create interpretable attribution signals; needed to reduce noise and improve signal-to-noise ratio; quick check: validate aggregation method preserves causal information
- **Sample-level vs token-level attribution**: Distinguishing between attributing behaviors to entire training documents versus specific phrases within them; needed for different granularities of interpretability; quick check: test both granularities on controlled examples
- **Computational efficiency**: Measuring memory and time requirements compared to baseline methods; needed to demonstrate practical applicability; quick check: benchmark against parameter-space methods on same hardware
- **Attribution accuracy**: Evaluating precision in identifying true causal training examples; needed to validate the framework's effectiveness; quick check: use known poisoned samples for controlled evaluation

## Architecture Onboarding

**Component Map**
Input Text -> Hidden States -> Gradient Computation -> Gradient Aggregation -> Attribution Scores -> Training Data Identification

**Critical Path**
The critical path involves computing hidden state gradients, aggregating them across the model's layers, and mapping these aggregated gradients back to training data. This path must efficiently handle the dimensionality reduction from parameters to representations while preserving causal relationships.

**Design Tradeoffs**
- Computational efficiency vs attribution granularity: Lower-dimensional representation space improves efficiency but may lose fine-grained information
- Sample-level vs token-level attribution: Different use cases require different attribution granularities
- Gradient aggregation methods: Different aggregation strategies balance noise reduction against information preservation

**Failure Signatures**
- Poor attribution accuracy when training data contains highly similar examples
- Computational bottlenecks when scaling to extremely large models
- Ambiguous attributions in cases where multiple training examples contribute equally to undesirable behaviors

**Three First Experiments**
1. Validate gradient flow through hidden states by comparing parameter-space and representation-space attributions on small models
2. Test sample-level attribution accuracy using controlled poisoned datasets with known ground truth
3. Benchmark computational efficiency by measuring memory usage and runtime compared to gradient-based baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to curated test scenarios with known problematic samples, potentially not reflecting real-world complexity
- Assumes gradient signals reliably capture causal relationships without addressing potential confounding factors
- Does not include user studies or qualitative assessments of interpretability for practical application

## Confidence

**High Confidence**: The core technical contribution of shifting attribution from parameter space to representation space is well-founded and addresses genuine computational bottlenecks. The efficiency gains and mathematical framework appear sound.

**Medium Confidence**: Empirical results across the three task categories demonstrate RepT's effectiveness, but the controlled nature of these experiments limits generalizability. Performance metrics may not translate directly to open-world scenarios.

**Low Confidence**: Claims about interpretability and human-auditability require further validation, as the paper does not include user studies or qualitative assessments of whether practitioners can meaningfully act on RepT's attributions.

## Next Checks

1. Conduct ablation studies comparing RepT's attributions against human expert judgments on ambiguous cases where multiple training documents could plausibly explain undesirable behaviors.

2. Evaluate RepT on real-world safety incidents from deployed LLMs, testing whether it can identify root causes in naturally occurring harmful outputs rather than synthetic test cases.

3. Benchmark RepT's computational efficiency and attribution quality against state-of-the-art parameter-space methods when applied to foundation models with parameter counts exceeding 100B, measuring memory usage and attribution stability across different model architectures.