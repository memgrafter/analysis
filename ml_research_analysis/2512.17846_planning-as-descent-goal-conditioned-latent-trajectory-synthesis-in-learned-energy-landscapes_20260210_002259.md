---
ver: rpa2
title: 'Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned
  Energy Landscapes'
arxiv_id: '2512.17846'
source_url: https://arxiv.org/abs/2512.17846
tags:
- planning
- energy
- learning
- future
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Planning as Descent (PaD), a novel framework
  for offline goal-conditioned reinforcement learning that learns to synthesize trajectories
  by minimizing a goal-conditioned energy function over entire future plans. Instead
  of learning a policy or explicit planner, PaD learns an energy landscape that assigns
  low energy to dynamically plausible, goal-consistent trajectories.
---

# Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes

## Quick Facts
- arXiv ID: 2512.17846
- Source URL: https://arxiv.org/abs/2512.17846
- Authors: Carlos Vélez García; Miguel Cazorla; Jorge Pomares
- Reference count: 10
- When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95% success, strongly outperforming prior methods that peak at 68%

## Executive Summary
This paper introduces Planning as Descent (PaD), a novel framework for offline goal-conditioned reinforcement learning that learns to synthesize trajectories by minimizing a goal-conditioned energy function over entire future plans. Instead of learning a policy or explicit planner, PaD learns an energy landscape that assigns low energy to dynamically plausible, goal-consistent trajectories. Planning is achieved through gradient-based refinement in this energy landscape, with identical computation during training and inference to reduce train-test mismatch. The method is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected. PaD is evaluated on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95% success, strongly outperforming prior methods that peak at 68%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. These results suggest that learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.

## Method Summary
PaD learns a goal-conditioned energy function over latent trajectories that assigns low energy to dynamically plausible, goal-consistent plans. During training, clean trajectories are corrupted and denoised through gradient-based refinement steps identical to those used at inference, with hindsight goal relabeling providing training signals. The energy landscape is shaped such that gradient descent produces coherent plans. At inference, multiple trajectory candidates are sampled and refined under different temporal hypotheses, with the lowest-energy plans selected for execution. A projector network ensures refinement stays on the encoder-induced manifold. Inverse dynamics decode actions from consecutive latents. The method avoids explicit policy learning or dynamics models, instead treating planning as optimization in the learned energy landscape.

## Key Results
- PaD achieves 95% success rate on cube manipulation tasks when trained on narrow expert demonstrations, outperforming prior methods that peak at 68%
- Training on noisy, suboptimal data further improves both success rate and plan efficiency compared to expert-only training
- Ablation shows the projector network is essential, improving success from 50% to 98% by constraining refinement to the encoder manifold
- Planning as energy descent enables verification-driven refinement that generalizes beyond demonstrated behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based energy descent produces coherent, goal-directed trajectories without explicit dynamics models or policy learning.
- Mechanism: The energy function E_θ(z_future | z_past, s_g, λ) assigns scalar energies to entire latent trajectories. Low energy indicates dynamic plausibility and goal consistency. Planning iteratively refines candidates via z^(t+1)_future = p_θ(z^(t)_future - η∇E_θ), using the gradient field as a descent direction toward feasible futures.
- Core assumption: The energy landscape can be shaped such that gradient descent trajectories converge to valid, goal-consistent plans rather than local minima or off-manifold regions.
- Evidence anchors:
  - [abstract] "Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference"
  - [section 4.2] Equation 1 defines the update rule with projector; text states "the energy function provides informed, goal-conditioned descent directions"
  - [corpus] Related work on Manifold-Constrained Energy-based Transition Models (arXiv:2602.02900) supports energy-based constraints for offline RL but uses explicit transition models unlike PaD
- Break condition: If energy gradients push latents off the encoder-induced manifold without recovery, refinement diverges. The projector mitigates this.

### Mechanism 2
- Claim: Training-inference alignment via shared refinement dynamics reduces distribution shift that plagues decoupled model-and-plan pipelines.
- Mechanism: During training, clean latent trajectories are corrupted (z^0_future = √β z_clean + √(1-β)ε) and denoised using the same T-step gradient refinement applied at inference. The loss L = Σ_t ℓ(z^(t)_future, z_clean) shapes the energy landscape around these exact descent dynamics.
- Core assumption: The refinement procedure itself, not just the final energy values, must be differentiable for effective landscape shaping.
- Evidence anchors:
  - [abstract] "identical computation during training and inference to reduce train-test mismatch"
  - [section 4.3] "This 'training-as-inference' principle regularizes the energy landscape such that its gradient flow implements the desired planning behavior"
  - [corpus] Weak direct evidence; related latent dynamics planning papers (Sobal et al., 2025) address train-test mismatch but through different mechanisms
- Break condition: If second-order gradient computation (Hessian-vector products) is unstable or approximated poorly, landscape shaping fails.

### Mechanism 3
- Claim: Diverse, suboptimal training data improves plan efficiency by expanding state coverage for verification-driven composition.
- Mechanism: Broad state coverage in noisy datasets enables the energy landscape to support trajectory refinement beyond demonstrated behaviors. Expert-only data constrains the planner to narrow manifolds, preventing discovery of more efficient paths.
- Core assumption: The energy function generalizes to combine transition fragments in novel ways not present in single trajectories.
- Evidence anchors:
  - [section 5.3] Table 3: PaD-noisy averages 63 steps vs 78 for PaD-play; Figure 5 shows systematic left-shift in episode lengths
  - [section 5.3] "the diverse transitions present in the noisy dataset allow PaD-noisy to observe a wider range of behaviors... enabling the planner to synthesize more efficient trajectories"
  - [corpus] "State-Covering Trajectory Stitching for Diffusion Planners" (arXiv:2506.00895) similarly addresses stitching diverse data but via diffusion sampling
- Break condition: If data diversity introduces irreconcilable contradictions in dynamics, energy landscape becomes ill-conditioned.

## Foundational Learning

- **Energy-Based Models (EBMs)**:
  - Why needed here: PaD's core is an EBM where energy represents trajectory incompatibility. Understanding inference-as-optimization (y* = argmin E(x,y)) is essential.
  - Quick check question: Can you explain why EBMs avoid computing the partition function during inference?

- **Hindsight Goal Relabeling**:
  - Why needed here: Training signal comes from relabeling achieved states as goals. Without this, sparse reward structure provides no learning signal.
  - Quick check question: Given trajectory (s_0, s_1, ..., s_T), how would you generate training pairs?

- **Gradient-Based Optimization with Second-Order Terms**:
  - Why needed here: Training requires backpropagation through refinement steps (gradients of gradients). Hessian-vector products enable this efficiently.
  - Quick check question: Why can't standard autodiff through T optimization steps scale without Hessian-vector tricks?

## Architecture Onboarding

- **Component map**: raw states → f_θ (encoder) → latent trajectory → corrupt → T×(E_θ gradient step → p_θ project) → compare to z_clean via smooth-L1

- **Critical path**: raw states → f_θ → latent trajectory → corrupt → T×(E_θ gradient step → p_θ project) → compare to z_clean via smooth-L1

- **Design tradeoffs**:
  - Refinement steps T=2: Minimal, but paper shows it works. More steps may help harder domains.
  - Horizon H=80, past context P_max=16: Task-dependent. Longer horizons increase compute quadratically.
  - Projector is essential (50% → 98% success in ablation); not optional.

- **Failure signatures**:
  - Training loss diverges early: Check projector is present and stop-gradient on z_future between refinement steps
  - Plans look plausible but actions fail: Inverse dynamics g_ψ may need more data or the encoder manifold is misaligned
  - Energy doesn't decrease during refinement: Learning rate η or corruption β schedule may need tuning

- **First 3 experiments**:
  1. **Overfit single trajectory**: Train on one demonstrated trajectory, verify PaD can exactly recover it from noise. Tests basic denoising capability.
  2. **Ablate projector**: Run with/without p_θ on cube-single-noisy-v0. Replicate 50% vs 98% gap to confirm manifold constraint necessity.
  3. **Vary replanning interval N**: Test N∈{1,2,4,8,16} to characterize robustness-efficiency tradeoff for your compute budget.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper demonstrates strong performance on a single task domain (cube manipulation), limiting generalizability claims
- The energy landscape's ability to support long-horizon planning beyond 80 timesteps remains untested
- The mechanism by which noisy data improves efficiency over expert data needs further theoretical grounding

## Confidence
- **High confidence**: The energy-based planning mechanism works for short-horizon tasks when proper manifold constraints (projector) are enforced. Ablation results are clear and reproducible.
- **Medium confidence**: The claim that identical training/inference computation reduces distribution shift is theoretically sound but lacks direct empirical comparison to decoupled pipelines on the same tasks.
- **Medium confidence**: The benefit of suboptimal data is demonstrated but the underlying mechanism (state coverage vs. contradictory dynamics) needs further validation across domains.

## Next Checks
1. **Cross-domain robustness**: Evaluate PaD on non-manipulation tasks (e.g., navigation or locomotion) to test generalization beyond cube manipulation.
2. **Temporal scaling**: Test performance with H=160 or H=320 timesteps to assess whether the energy landscape maintains coherence for longer horizons.
3. **Data quality tradeoff**: Systematically vary the ratio of expert to noisy data in mixed training sets to quantify the optimal balance between feasibility and efficiency.