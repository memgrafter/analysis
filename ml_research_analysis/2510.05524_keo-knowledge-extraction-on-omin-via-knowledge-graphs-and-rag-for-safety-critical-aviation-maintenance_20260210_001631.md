---
ver: rpa2
title: 'KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical
  Aviation Maintenance'
arxiv_id: '2510.05524'
source_url: https://arxiv.org/abs/2510.05524
tags:
- questions
- knowledge
- global
- maintenance
- records
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents KEO, a knowledge extraction framework for aviation
  maintenance that integrates knowledge graphs with retrieval-augmented generation.
  The authors address the challenge of applying large language models to safety-critical
  domains where factual accuracy and structured reasoning are essential.
---

# KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance

## Quick Facts
- arXiv ID: 2510.05524
- Source URL: https://arxiv.org/abs/2510.05524
- Authors: Kuangshi Ai; Jonathan A. Karr; Meng Jiang; Nitesh V. Chawla; Chaoli Wang
- Reference count: 28
- Primary result: KG-enhanced RAG significantly outperforms text-chunk RAG on aviation maintenance dataset-wide reasoning tasks, with scores of 4.31-4.87 out of 5.

## Executive Summary
This paper introduces KEO, a knowledge extraction framework that combines knowledge graphs with retrieval-augmented generation for safety-critical aviation maintenance. The authors address the challenge of applying large language models to domains where factual accuracy and structured reasoning are essential. KEO constructs a knowledge graph from aviation maintenance records and uses it to enhance RAG pipelines, enabling more coherent dataset-wide reasoning. Experiments show KEO significantly improves global sensemaking tasks compared to traditional text-chunk RAG, particularly when paired with stronger backbone models like Gemma-3-Instruct.

## Method Summary
KEO constructs a knowledge graph from aviation maintenance records by extracting 15 predefined relation types using LLM-based triplet extraction. The framework uses entity-level embedding retrieval to find seed nodes, then expands to m-hop subgraphs which are converted to undirected weighted graphs. Minimum spanning trees are computed per connected component using Kruskal's algorithm, and DFS traversal provides the final text context augmented with Leiden community summaries. The system is evaluated on two task types: global sensemaking (dataset-wide reasoning) and knowledge-to-action (procedural action prediction), using LLM-as-judge scoring and ROUGE-L metrics.

## Key Results
- KG-RAG achieves 4.31-4.87/5 on global sensemaking tasks versus 3.46-3.82/5 for text-chunk RAG
- Gemma-3-Instruct outperforms Phi-4 and Mistral-Nemo on both task types
- Text-chunk RAG remains superior for fine-grained procedural tasks (knowledge-to-action)
- Performance gains are attenuated when using smaller models like Mistral-Nemo

## Why This Works (Mechanism)
KEO leverages structured knowledge representation to improve retrieval coherence for complex reasoning tasks. By constructing a knowledge graph with entity relationships, the system can perform multi-hop reasoning that captures global patterns across the dataset. The MST filtering step removes redundant or less relevant connections while preserving the most important information pathways. This structured approach enables better sensemaking of complex incident patterns compared to flat text retrieval, though the benefit diminishes for simple procedural queries where direct text access is more effective.

## Foundational Learning
**Knowledge Graph Construction**
- Why needed: Provides structured representation of domain knowledge for coherent multi-hop reasoning
- Quick check: Verify extracted triplets follow specified relation schema and cover diverse incident types

**Minimum Spanning Tree Filtering**
- Why needed: Removes redundant edges while preserving connectivity, focusing on most important relationships
- Quick check: Confirm MST preserves all connected components with minimal edge weight sum

**Leiden Community Summarization**
- Why needed: Groups related entities to provide contextual summaries for better understanding
- Quick check: Validate community detection produces meaningful clusters of related maintenance issues

## Architecture Onboarding

**Component Map**
KG Construction -> Entity Embedding -> Seed Node Retrieval -> m-hop Expansion -> MST Filtering -> DFS Traversal -> Context Augmentation -> LLM Inference

**Critical Path**
Query -> Entity Embedding -> Top-k Seed Retrieval -> Graph Expansion -> MST Computation -> Context Generation -> LLM Response

**Design Tradeoffs**
- Structured vs flat retrieval: KG-RAG excels at global reasoning but underperforms on fine-grained tasks
- Graph size vs relevance: Larger m-hops provide more context but risk including noise
- Model size vs reasoning ability: Larger models better leverage structured context

**Failure Signatures**
- Poor KG quality from weak extraction models leads to incorrect or missing relations
- Insufficient seed node retrieval misses relevant context for queries
- Over-aggressive MST filtering removes critical information pathways

**First Experiments**
1. Test KG-RAG with varying k (seed nodes) and m (hop expansion) to find optimal balance
2. Compare MST filtering against alternative graph pruning methods
3. Evaluate hybrid retrieval routing between KG and text-chunk approaches

## Open Questions the Paper Calls Out
**Hybrid Retrieval Routing**
Can a hybrid system dynamically route between KG-based and text-chunk RAG based on query type to achieve optimal performance across both global sensemaking and knowledge-to-action tasks? The paper notes different paradigms complement each other but didn't test combined approaches.

**Scalability to Larger Corpora**
How does KEO's performance scale when knowledge graphs are constructed from thousands to millions of records? Current experiments were limited to 100-500 records, and LLM-based construction may face context-window constraints at scale.

**Smaller Model Adaptation**
Can KEO be adapted to make structured graph context equally effective for weaker, smaller models that currently show attenuated performance gains? The paper suggests limited reasoning ability prevents smaller models from leveraging graph context effectively.

## Limitations
- LLM-as-a-judge scoring introduces subjectivity and lacks ground-truth alignment for global sensemaking tasks
- Knowledge graph construction limited to 500 records raises questions about coverage and generalization
- Critical implementation details unspecified, including embedding model, hyperparameters, and weighting schemes

## Confidence
- High: KG-RAG excels at dataset-wide reasoning while text-chunk RAG performs better on fine-grained procedural tasks
- Medium: Reported performance improvements are internally consistent but depend on LLM judge reliability
- Low: Generalizability to other safety-critical domains or larger knowledge graph scales remains speculative

## Next Checks
1. Implement KG-RAG pipeline using standardized embedding models and test sensitivity to k and m hyperparameters
2. Create ground-truth action sequences for knowledge-to-action questions to validate ROUGE-L results independently
3. Compare MST filtering effectiveness against alternative graph pruning methods like node centrality thresholds