---
ver: rpa2
title: 'One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean
  Geometry'
arxiv_id: '2601.22521'
source_url: https://arxiv.org/abs/2601.22521
tags:
- pmpo
- gmpo
- arxiv
- mean
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the instability of group-based reinforcement\
  \ learning in mathematical reasoning, where fixed aggregation geometries (arithmetic\
  \ mean in GRPO or geometric mean in GMPO) fail to adapt to the heterogeneous and\
  \ evolving reliability of reasoning trajectories. The authors propose Power-Mean\
  \ Policy Optimization (PMPO), a unified framework that parameterizes the aggregation\
  \ geometry via a power-mean exponent p, recovering GRPO (p=1) and GMPO (p\u2192\
  0) as special cases."
---

# One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry

## Quick Facts
- arXiv ID: 2601.22521
- Source URL: https://arxiv.org/abs/2601.22521
- Reference count: 32
- Primary result: Introduces PMPO, a unified framework that parameterizes aggregation geometry via a power-mean exponent p, achieving 1.5% average accuracy gains over GMPO and up to 3.3% on AIME24

## Executive Summary
This paper addresses the instability of group-based reinforcement learning in mathematical reasoning by proposing Power-Mean Policy Optimization (PMPO), a unified framework that dynamically adapts the aggregation geometry for trajectory rewards. Traditional approaches like GRPO (arithmetic mean) and GMPO (geometric mean) use fixed aggregation methods that fail to adapt to the heterogeneous reliability of reasoning trajectories. PMPO parameterizes this aggregation via a power-mean exponent p, which can smoothly transition between arithmetic (p=1), geometric (p→0), and other intermediate geometries based on trajectory quality.

The method employs a Clip-aware Effective Sample Size (ESS) mechanism that maps trajectory clipping rates to target ESS values and solves for the p that matches this target, enabling adaptive aggregation that balances aggressive updates for reliable trajectories with conservative updates for uncertain ones. Experiments on 7B parameter models across multiple mathematical reasoning benchmarks show state-of-the-art performance with average accuracy gains of 1.5% over GMPO and up to 3.3% on challenging tasks like AIME24, demonstrating superior stability and efficiency compared to static baselines.

## Method Summary
PMPO introduces a unified framework for group-based reinforcement learning that parameterizes trajectory reward aggregation through a power-mean exponent p. The power-mean formulation recovers GRPO (p=1) and GMPO (p→0) as special cases while enabling smooth interpolation between these extremes. The key innovation is the dynamic determination of p using a Clip-aware Effective Sample Size mechanism: trajectory clipping rates are mapped to target ESS values, and the method solves for the p that achieves this target ESS. This allows the system to adapt its aggregation geometry based on trajectory reliability - using more aggressive arithmetic aggregation for high-quality trajectories and conservative geometric aggregation for uncertain ones. Theoretically, varying p modulates the concentration of gradient updates, acting as an inverse temperature over token-level log-probability changes, though this connection remains primarily empirical.

## Key Results
- PMPO achieves state-of-the-art performance across mathematical reasoning benchmarks (AIME24, AMC, MATH500, Minerva, OlympiadBench)
- Average accuracy improvements of 1.5% over GMPO and up to 3.3% on hard tasks like AIME24
- Demonstrates superior stability and efficiency compared to static baseline methods
- Validated on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-7B models

## Why This Works (Mechanism)
The power-mean framework provides a mathematically elegant way to unify different aggregation geometries while the dynamic p mechanism allows the system to adapt to trajectory reliability. By parameterizing the aggregation geometry, PMPO can smoothly transition between aggressive (arithmetic) and conservative (geometric) updates based on trajectory quality indicators. The Clip-aware ESS mechanism provides an effective signal for trajectory reliability, as trajectories that frequently hit clipping thresholds likely contain errors or uncertainty. This adaptive approach addresses the core limitation of fixed aggregation methods that either over-update on unreliable trajectories (arithmetic) or under-update on reliable ones (geometric).

## Foundational Learning
- **Power Mean**: A family of functions that generalize arithmetic and geometric means, parameterized by an exponent p. Why needed: Provides a unified framework that can recover existing methods as special cases while enabling smooth interpolation. Quick check: Verify that power mean with p=1 gives arithmetic mean and p→0 gives geometric mean.
- **Effective Sample Size (ESS)**: A measure of the effective number of independent samples in a weighted dataset. Why needed: Quantifies the diversity and reliability of trajectory contributions. Quick check: Confirm that ESS decreases as weights become more concentrated on fewer samples.
- **Clipping in RL**: A technique that caps reward values to prevent extreme gradients. Why needed: Prevents policy updates from being dominated by outlier rewards while serving as a signal for trajectory reliability. Quick check: Verify that trajectories with more clipping events tend to be less reliable.
- **Group-based RL**: A reinforcement learning approach that aggregates rewards from multiple reasoning trajectories. Why needed: Enables more robust policy updates by considering multiple solution attempts. Quick check: Confirm that group-based methods outperform single-trajectory approaches on complex reasoning tasks.
- **Inverse Temperature**: A parameter that controls the concentration of probability distributions. Why needed: Provides theoretical interpretation of how p modulates gradient update concentration. Quick check: Verify that lower p values lead to more concentrated (conservative) updates.
- **Mathematical Reasoning Benchmarks**: Standardized datasets for evaluating mathematical problem-solving capabilities. Why needed: Provide objective metrics for comparing different RL approaches. Quick check: Confirm benchmark results align with human-expert expectations.

## Architecture Onboarding

Component Map: Trajectory Generator -> Reward Aggregator (Power Mean with dynamic p) -> Policy Optimizer -> Model

Critical Path: The most time-critical path is the reward aggregation step, where p must be computed dynamically for each group of trajectories before policy updates can proceed.

Design Tradeoffs:
- Dynamic p calculation adds computational overhead but provides significant performance gains
- More complex than static aggregation methods but offers better adaptability
- Requires additional hyperparameters (ESS mapping) but demonstrates robustness across benchmarks

Failure Signatures:
- If p becomes stuck at extreme values (very close to 0 or 1), the system loses adaptability
- Poor ESS mapping can lead to inappropriate aggregation geometries
- Computational bottlenecks may occur during p optimization for very long trajectories

First Experiments:
1. Implement power-mean aggregation with fixed p values to verify it can reproduce GRPO and GMPO results
2. Test the ESS calculation on sample trajectory groups to validate the clipping rate mapping
3. Run ablations comparing dynamic p against fixed p values across different reliability regimes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical connection between p and gradient update concentration remains primarily empirical rather than rigorously proven
- Additional hyperparameters (target ESS mapping) may limit generalizability across different model architectures
- Computational overhead of dynamic p optimization is non-trivial, particularly for long trajectories
- Evaluation focuses on 7B parameter models, leaving scalability questions to larger models unaddressed

## Confidence

| Claim | Confidence |
|-------|------------|
| Mathematical formulation of PMPO as unified framework | High |
| Empirical superiority over static baselines | Medium |
| Theoretical interpretation of p as inverse temperature | Medium |

## Next Checks

1. Conduct ablation studies isolating the contribution of the dynamic p mechanism versus the power-mean formulation itself to quantify the value of adaptivity versus unified geometry

2. Test PMPO on larger model scales (70B+ parameters) to verify that adaptive aggregation benefits scale with model capacity, as group-based RL typically shows stronger effects at larger scales

3. Evaluate the method's sensitivity to the target ESS mapping by testing multiple mapping functions and measuring performance variance to assess robustness to this additional hyperparameter