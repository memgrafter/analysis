---
ver: rpa2
title: 'Human-Level Reasoning: A Comparative Study of Large Language Models on Logical
  and Abstract Reasoning'
arxiv_id: '2510.24435'
source_url: https://arxiv.org/abs/2510.24435
tags:
- llms
- question
- reasoning
- correct
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the logical and abstract reasoning abilities
  of 15 large language models (LLMs) against human performance using 8 custom-designed
  reasoning questions. Results showed that while LLMs excelled in straightforward
  pattern recognition and basic calculations, they struggled with abstract reasoning,
  non-standard problem formats, and applying common-sense knowledge.
---

# Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning

## Quick Facts
- arXiv ID: 2510.24435
- Source URL: https://arxiv.org/abs/2510.24435
- Authors: Benjamin Grando Moreira
- Reference count: 0
- LLMs achieved 73.4% average score versus 69.6% for humans on 8 custom reasoning questions

## Executive Summary
This study evaluates the logical and abstract reasoning capabilities of 15 large language models (LLMs) against human performance using a custom-designed test set of 8 reasoning questions. While LLMs demonstrated strong performance on straightforward pattern recognition and basic calculations, they struggled with abstract reasoning, non-standard problem formats, and applying common-sense knowledge. The results reveal persistent limitations in LLM reasoning capabilities despite advances in natural language processing, highlighting difficulties in adapting to unconventional problems and integrating disparate information.

## Method Summary
The study employed a comparative evaluation framework using 15 large language models tested against human participants on 8 custom-designed reasoning questions. The questions covered diverse problem types including pattern recognition, logical deductions, and abstract reasoning challenges. Performance was measured as percentage scores across all questions, with both LLM and human results aggregated for comparison. The study design aimed to assess reasoning capabilities beyond standard benchmarks, focusing on adaptability to unconventional problem formats.

## Key Results
- LLMs achieved 73.4% average score versus 69.6% for humans across all questions
- LLMs excelled in straightforward pattern recognition and basic calculations
- Humans outperformed LLMs on several individual questions despite lower overall average
- LLMs struggled with abstract reasoning and non-standard problem formats

## Why This Works (Mechanism)
The comparative methodology directly measures reasoning capabilities by testing both LLMs and humans on identical problem sets, eliminating confounding variables related to task interpretation. The custom-designed questions probe reasoning abilities beyond pattern matching, requiring adaptation to novel problem structures and integration of common-sense knowledge. This approach reveals systematic differences in how LLMs versus humans process and solve reasoning tasks.

## Foundational Learning
- Pattern recognition fundamentals - Understanding basic pattern identification and completion is essential for solving the straightforward problems where LLMs excel
- Logical deduction principles - Required for questions involving inference chains and conditional reasoning
- Abstract reasoning frameworks - Needed to tackle problems requiring conceptual understanding beyond concrete examples
- Common-sense knowledge integration - Critical for problems requiring real-world knowledge application
- Problem format adaptation - Understanding how presentation affects solution strategies is key to interpreting performance differences
- Quantitative reasoning basics - Fundamental for the calculation-based questions in the test set

## Architecture Onboarding

Component Map:
Question Input -> Pattern Recognition Engine -> Logical Reasoning Module -> Common-Sense Knowledge Integration -> Answer Generation

Critical Path:
Input processing → Feature extraction → Reasoning pattern matching → Knowledge retrieval → Response formulation

Design Tradeoffs:
The study's custom question design prioritizes breadth of reasoning types over depth in any single domain, potentially sacrificing detailed analysis of specific reasoning capabilities. The choice to compare against human performance provides ecological validity but introduces variability in human baseline quality.

Failure Signatures:
- Over-reliance on surface pattern matching when deeper abstraction is required
- Inability to recognize when common-sense knowledge should override pattern-based reasoning
- Sensitivity to non-standard problem formatting that disrupts expected solution pathways
- Difficulty integrating multiple reasoning steps when they don't follow conventional sequences

First Experiments:
1. Test each LLM on modified versions of the same questions with standard formatting to isolate format sensitivity
2. Evaluate performance on expanded problem sets with systematic variations in abstraction level
3. Compare LLM reasoning patterns against expert human solutions to identify systematic error types

## Open Questions the Paper Calls Out
None

## Limitations
- Small test set (8 questions) may not capture full spectrum of human reasoning capabilities
- Unknown human participant demographics and sample size introduce potential sampling bias
- Limited scope of evaluation questions affects generalizability of performance comparisons
- Custom-designed questions may not represent real-world reasoning challenges

## Confidence

**High Confidence:**
- Humans outperformed LLMs on several individual questions (direct empirical observation)

**Medium-High Confidence:**
- LLMs struggle with abstract reasoning and non-standard problem formats (supported by performance gap and consistent with existing literature)

**Medium Confidence:**
- LLMs' difficulty with common-sense knowledge integration and unconventional problems (inferred from aggregate patterns rather than systematic testing)

## Next Checks
1. Expand the test corpus to 50+ diverse reasoning problems across multiple domains, including systematic variations of problem formats to test LLM adaptability
2. Conduct human evaluation with larger, demographically diverse participant pools (n > 100) and document baseline reasoning capabilities through pre-screening
3. Implement controlled experiments comparing LLM performance on standard versus modified versions of the same problems to quantify sensitivity to problem presentation