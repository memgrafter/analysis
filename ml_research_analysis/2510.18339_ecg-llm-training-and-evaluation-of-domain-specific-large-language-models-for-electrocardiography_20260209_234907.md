---
ver: rpa2
title: ECG-LLM -- training and evaluation of domain-specific large language models
  for electrocardiography
arxiv_id: '2510.18339'
source_url: https://arxiv.org/abs/2510.18339
tags:
- llama
- evaluation
- questions
- human
- finetuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates domain-specific large language models (LLMs)
  for electrocardiography (ECG) through finetuning and retrieval-augmented generation
  (RAG). Open-weight Llama 3.1 models were adapted to ECG domain literature and compared
  against Claude Sonnet 3.7 using multiple evaluation methods including multiple-choice
  tests, automatic text metrics, LLM-as-a-judge, and human expert assessment.
---

# ECG-LLM -- training and evaluation of domain-specific large language models for electrocardiography

## Quick Facts
- arXiv ID: 2510.18339
- Source URL: https://arxiv.org/abs/2510.18339
- Reference count: 40
- Primary result: Finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations, outperforming both base models and general-purpose LLMs

## Executive Summary
This study evaluates domain-specific large language models (LLMs) for electrocardiography (ECG) through finetuning and retrieval-augmented generation (RAG). Open-weight Llama 3.1 models were adapted to ECG domain literature and compared against Claude Sonnet 3.7 using multiple evaluation methods including multiple-choice tests, automatic text metrics, LLM-as-a-judge, and human expert assessment. Results showed finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations, outperforming both base models and general-purpose LLMs. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries, demonstrating that domain adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.

## Method Summary
The researchers created a domain-specific dataset by extracting ECG-related Q&A pairs from medical literature using a two-step pipeline with Llama 3.3 70B. They finetuned Llama 3.1 8B and 70B models using LoRA with specific hyperparameters (r=256, α=128) and also implemented RAG approaches using PubMedBERT embeddings and ChromaDB for retrieval. Models were evaluated across four distinct modes: multiple-choice tests, text similarity metrics (BLEU/ROUGE), LLM-as-a-judge, and human expert assessment, revealing substantial performance heterogeneity across evaluation methodologies.

## Key Results
- Finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations
- Finetuned models consistently outperformed base models across all evaluation modes
- Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries
- RAG with Llama 3.1 8B matched larger models on complex questions, reaching Claude 3.7 performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning on domain-specific Q&A pairs improves factual knowledge recall for in-distribution queries.
- Mechanism: Supervised finetuning adjusts model weights to align internal representations with domain terminology and question-answer patterns, improving retrieval of specialized knowledge.
- Core assumption: Q&A extraction using Llama 3.3 70B produces high-quality training data aligned with target domain.
- Evidence anchors:
  - [abstract] "Finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations, outperforming both base models and general-purpose LLMs."
  - [section 2.1] Finetuned models consistently outperform general-purpose models across all three test subsets (full, special, checked).
  - [corpus] Related work (arXiv:2509.18843) shows open-weight LLMs approaching proprietary model performance in biomedical QA, supporting domain adaptation viability.
- Break condition: Out-of-distribution questions with different syntactic patterns show degraded performance (Flesch score 21.9 vs 30.3 for training data).

### Mechanism 2
- Claim: RAG provides verifiable, context-grounded responses without modifying model weights.
- Mechanism: External retrieval injects document chunks at inference time, grounding responses in specific source material and reducing hallucination risk.
- Core assumption: PubMedBERT embeddings capture domain-relevant semantic similarity for ECG literature.
- Evidence anchors:
  - [abstract] "Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries."
  - [section 2.4] Llama 3.1 8B with RAG matches larger models on complex questions, reaching Claude 3.7 performance.
  - [corpus] arXiv:2511.08600 demonstrates RAG utility for domain-specific clinical vignette generation where general-purpose LLMs lack specialized knowledge.
- Break condition: RAG underperforms on BLEU/ROUGE metrics due to lexical mismatch with reference answers despite semantic correctness.

### Mechanism 3
- Claim: Evaluation methodology selection significantly affects model ranking conclusions.
- Mechanism: Different evaluation modes assess different capabilities—multiple-choice tests factual recall, text metrics measure surface similarity, human evaluation captures semantic completeness.
- Core assumption: No single metric comprehensively measures clinical utility.
- Evidence anchors:
  - [abstract] "Our findings reveal substantial performance heterogeneity across evaluation methodologies."
  - [section 3.1] Rankings vary: finetuned 70B ranks #1 on multiple-choice and text similarity but #3 on human evaluation; Claude ranks #5 on multiple-choice but #1 on LLM-as-judge and human evaluation.
  - [corpus] Weak direct corpus evidence on evaluation heterogeneity; related papers focus on single evaluation modes.
- Break condition: Text similarity metrics penalize valid alternative formulations, making them unreliable for comparing models with different output styles.

## Foundational Learning

- Concept: Supervised Finetuning (SFT) with LoRA/QLoRA
  - Why needed here: Enables efficient domain adaptation of 70B parameter models with limited compute by training only ~3.7% of parameters.
  - Quick check question: What LoRA hyperparameters (r, α) achieved optimal performance, and why might higher α values degrade 8B model accuracy?

- Concept: Retrieval-Augmented Generation (RAG) Pipeline
  - Why needed here: Provides an alternative to finetuning for knowledge injection, offering easier updates when literature evolves.
  - Quick check question: Why does reranking with top-5 after initial top-20 retrieval improve multiple-choice accuracy?

- Concept: Evaluation Metrics for Language Models
  - Why needed here: Understanding what each metric measures is critical for interpreting results and avoiding false conclusions.
  - Quick check question: Why did RAG models rank last on BLEU/ROUGE despite strong human evaluation performance?

## Architecture Onboarding

- Component map:
  - PDF → Markdown (MinerU) → Chunking → Q&A generation (Llama 3.3 70B) → Finetuning data / RAG corpus
  - Q&A pairs → QLoRA training → Domain-adapted model
  - Chunks → PubMedBERT embeddings → Chroma DB → Top-k retrieval + reranking → Context-augmented generation
  - Multiple-choice generation → 4 evaluation modes → Statistical ranking via bootstrapping

- Critical path:
  - For finetuning: Q&A quality → LoRA configuration (r=256, α=128) → Early stopping at 2 epochs when validation loss diverges
  - For RAG: Chunk size (1024 tokens, 100 overlap) → Embedding model selection → Retrieval count and reranking

- Design tradeoffs:
  - Finetuning: Better in-distribution performance, style alignment, but requires retraining for updates and may overfit to training syntax
  - RAG: Better out-of-distribution handling, easily updatable, but inference overhead and dependency on retrieval quality
  - Model size: 70B consistently outperforms 8B across all configurations, but requires more compute

- Failure signatures:
  - Finetuned models answering complex questions incorrectly when question syntax differs from training data
  - RAG models producing low BLEU/ROUGE scores despite semantically correct answers
  - LLM-as-judge labeling correct answers as incorrect (~50% false negative rate on "incorrect" judgments)
  - Training instability with α=512 on 8B model (failed to complete)

- First 3 experiments:
  1. Replicate Q&A extraction pipeline on a subset of ECG documents, verifying AlignScore for context alignment
  2. Compare finetuning vs RAG on a held-out question set using all four evaluation modes with statistical significance testing
  3. Ablate RAG configuration: test chunk size (512 vs 1024), top-k (5 vs 10 vs 20), and embedding model (PubMedBERT vs multilingual-e5) on multiple-choice accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation dataset represents specific subset of clinical queries, may not capture full breadth of real-world ECG interpretation challenges
- Human expert evaluation involved only three reviewers, limiting statistical power and introducing potential subjectivity
- Reliance on synthetically generated multiple-choice questions introduces potential bias in question formulation and answer distribution

## Confidence
**High Confidence**: The core finding that finetuned Llama 3.1 70B achieves 92% accuracy on multiple-choice evaluations and outperforms base models is well-supported by consistent results across multiple evaluation modes and statistical validation.

**Medium Confidence**: The claim that RAG approaches perform competitively with larger finetuned models for complex queries is supported by human expert evaluation but shows contradictory evidence in automatic metrics.

**Low Confidence**: The specific claim that RAG is "less effective" for ECG domain adaptation compared to finetuning is not strongly supported, as RAG with Llama 3.1 8B achieves comparable performance to finetuned 70B on human evaluations.

## Next Checks
1. **Cross-dataset validation**: Test finetuned and RAG models on independently sourced ECG question sets from clinical practice to assess real-world generalization beyond the synthetically generated evaluation dataset.

2. **Ablation study on Q&A generation quality**: Systematically vary the quality thresholds and generation parameters for the Q&A extraction pipeline to quantify the impact of training data quality on finetuned model performance, including analysis of hallucination rates and factual accuracy.

3. **Longitudinal performance tracking**: Implement continuous evaluation of finetuned models on evolving ECG literature to measure knowledge drift over time and compare the update efficiency of finetuning versus RAG approaches when new research emerges.