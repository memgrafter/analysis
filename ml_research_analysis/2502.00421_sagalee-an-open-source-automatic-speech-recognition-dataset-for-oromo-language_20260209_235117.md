---
ver: rpa2
title: 'Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo Language'
arxiv_id: '2502.00421'
source_url: https://arxiv.org/abs/2502.00421
tags:
- oromo
- speech
- language
- dataset
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sagalee, the first open-source Automatic
  Speech Recognition (ASR) dataset for the Oromo language, which addresses the critical
  lack of speech resources for this widely spoken but underrepresented language. The
  dataset was collected through a crowdsourcing initiative using a mobile app, resulting
  in 100 hours of real-world audio recordings paired with transcriptions from 283
  speakers, capturing diverse phonetic variations and acoustic environments.
---

# Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo Language

## Quick Facts
- arXiv ID: 2502.00421
- Source URL: https://arxiv.org/abs/2502.00421
- Reference count: 29
- Primary result: First open-source ASR dataset for Oromo language with 100 hours of speech data

## Executive Summary
This paper introduces Sagalee, the first open-source Automatic Speech Recognition (ASR) dataset for the Oromo language, addressing a critical gap in speech resources for this widely spoken but underrepresented language. The dataset was collected through a crowdsourcing initiative using a mobile app, resulting in 100 hours of real-world audio recordings paired with transcriptions from 283 speakers. The dataset captures diverse phonetic variations and acoustic environments, making it valuable for ASR research in low-resource languages. The authors demonstrate the dataset's applicability through experiments using Conformer models and Whisper, establishing baseline performance metrics and highlighting the potential for advancing speech recognition research in low-resource languages.

## Method Summary
The Sagalee dataset was created through a crowdsourcing initiative using a mobile application, where 283 speakers contributed 100 hours of real-world audio recordings paired with transcriptions. The data collection process focused on capturing diverse phonetic variations and acoustic environments to ensure the dataset's applicability for ASR tasks. The authors used this dataset to train and evaluate Conformer models with different loss functions (hybrid CTC and AED, pure CTC) and fine-tuned the Whisper model. The experiments demonstrated the dataset's effectiveness in improving ASR performance for the Oromo language, with the fine-tuned Whisper model achieving the best results.

## Key Results
- Achieved a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss using Conformer models
- Achieved a WER of 18.74% with pure CTC loss using Conformer models
- Fine-tuned Whisper model achieved a significantly improved WER of 10.82%

## Why This Works (Mechanism)
The success of the Sagalee dataset lies in its comprehensive collection of real-world audio recordings paired with transcriptions, capturing the phonetic and acoustic diversity of the Oromo language. By leveraging crowdsourcing through a mobile app, the dataset ensures a broad representation of speakers and environments, which is critical for training robust ASR models. The use of advanced models like Conformer and Whisper further enhances the dataset's applicability, demonstrating that even low-resource languages can achieve competitive ASR performance with the right data and model architecture.

## Foundational Learning
- **Crowdsourcing for Data Collection**: Necessary to gather large-scale, diverse speech data efficiently. Quick check: Ensure contributor diversity and data quality through validation protocols.
- **Conformer Models**: Combine CNNs and Transformers for efficient sequence modeling in ASR. Quick check: Evaluate model performance on both short and long audio sequences.
- **Whisper Model**: Pre-trained on multilingual data, enabling effective fine-tuning for low-resource languages. Quick check: Compare fine-tuned performance with models trained from scratch.
- **Word Error Rate (WER)**: Standard metric for evaluating ASR performance. Quick check: Analyze WER breakdown by phoneme or word category to identify weaknesses.
- **Hybrid CTC and AED Loss**: Balances alignment and attention-based modeling for improved ASR accuracy. Quick check: Compare performance with pure CTC or pure AED loss functions.

## Architecture Onboarding

**Component Map**: Mobile App -> Crowdsourced Data Collection -> Preprocessing -> Conformer/Whisper Model Training -> WER Evaluation

**Critical Path**: Data Collection -> Preprocessing -> Model Training -> Evaluation

**Design Tradeoffs**: 
- Crowdsourcing ensures scalability but may introduce variability in data quality.
- Conformer models balance efficiency and accuracy but require careful hyperparameter tuning.
- Whisper fine-tuning leverages pre-trained knowledge but may not fully adapt to Oromo-specific nuances.

**Failure Signatures**: 
- High WER in specific phonetic contexts may indicate insufficient representation in the dataset.
- Poor performance on out-of-domain data suggests overfitting to the collected acoustic environments.

**First Experiments**:
1. Evaluate WER on a held-out test set to establish baseline performance.
2. Analyze WER breakdown by phoneme or word category to identify weaknesses.
3. Test model generalization by evaluating on out-of-domain data or different Oromo dialects.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size of 100 hours may be insufficient for achieving state-of-the-art ASR performance.
- Limited exploration of advanced ASR architectures beyond baseline models.
- Lack of detailed information on quality control measures during crowdsourcing.

## Confidence

**High Confidence**: The introduction of the Sagalee dataset as the first open-source ASR dataset for Oromo is a significant contribution to the field of low-resource language processing. The methodology for data collection and the initial experimental results are well-documented and provide a solid foundation for future research.

**Medium Confidence**: The experimental results, while promising, are based on a limited set of models and loss functions. The performance metrics (WER) are reasonable for a low-resource language but may not fully represent the dataset's potential.

**Low Confidence**: The paper does not provide extensive details on the diversity of the speakers or the acoustic environments, which could affect the generalizability of the results.

## Next Checks

1. **Quality Control Validation**: Conduct a detailed analysis of the transcription quality by comparing a subset of the crowdsourced transcriptions with expert-verified transcriptions to assess the reliability of the dataset.
2. **Model Performance Benchmarking**: Experiment with more advanced ASR architectures, such as Transformer-based models or hybrid approaches, to evaluate whether the performance can be further improved beyond the baseline results.
3. **Generalization Testing**: Test the dataset's generalizability by evaluating the models on out-of-domain data or different dialects of the Oromo language to assess its robustness across diverse linguistic contexts.