---
ver: rpa2
title: 'Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents'
arxiv_id: '2505.03947'
source_url: https://arxiv.org/abs/2505.03947
tags:
- size
- reasoning
- frog
- game
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores zero-shot, in-context, and sample-efficient
  game playing with LLMs on the challenging Atari game Frogger. Reasoning models like
  o3-mini and QwQ-32B can play Frogger in a zero-shot setting with episodic rewards
  up to 32 and 17 points respectively, demonstrating spatial reasoning and in-context
  learning capabilities.
---

# Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents

## Quick Facts
- arXiv ID: 2505.03947
- Source URL: https://arxiv.org/abs/2505.03947
- Authors: Xiang Li; Yiyang Hao; Doug Fulop
- Reference count: 40
- Primary result: Reasoning LLMs achieve non-trivial zero-shot Frogger gameplay (o3-mini: 32, QwQ-32B: 17 episodic rewards) and improve DQN training by 35.3% with LLM demonstrations

## Executive Summary
This paper explores zero-shot, in-context, and sample-efficient game playing with LLMs on the challenging Atari game Frogger. Reasoning models like o3-mini and QwQ-32B can play Frogger in a zero-shot setting with episodic rewards up to 32 and 17 points respectively, demonstrating spatial reasoning and in-context learning capabilities. Including past rewards significantly improves performance under high reasoning settings. Integrating LLM-generated demonstrations into traditional DQN training yields 35.3% higher episodic rewards within the same training budget. Depth-first search baseline shows high computational complexity for solving Frogger. The work highlights LLMs' potential to accelerate RL training and suggests future directions including RL fine-tuning and Monte Carlo Tree Search with LLM guidance.

## Method Summary
The paper employs a two-pronged approach: (1) Zero-shot and in-context LLM agents using the Object-Centric Atari (OCAtari) framework to extract structured game objects (category, x-y position, dimensions) from Frogger, enabling reasoning models to play without multimodal spatial reasoning; (2) LLM-guided DQN training where LLM-played episodes are preloaded into a prioritized experience replay buffer to accelerate traditional RL training. The approach varies past steps (0, 3, all), reasoning effort (low/medium/high), and reward visibility (show/hide) to study in-context learning dynamics.

## Key Results
- o3-mini achieves 32 episodic reward and QwQ-32B achieves 17 episodic reward in zero-shot Frogger play using structured object representations
- Including past rewards doubles episodic reward from 22 to 45 under high reasoning settings
- LLM-guided DQN achieves 35.3% higher rewards (5,000 episodes) compared to standard DQN with same training budget
- Action frequency bias emerges with more past steps at low reasoning effort
- DFS baseline shows exponential complexity (score 8: 90 seconds, score 9: 23 minutes)

## Why This Works (Mechanism)

### Mechanism 1: Object-Centric State Abstraction Enables Zero-Shot Reasoning
Reasoning LLMs can achieve non-trivial zero-shot gameplay when visual inputs are abstracted to structured object representations. The Object-Centric Atari (OCAtari) framework extracts game objects as coordinate lists (category, x-y position, dimensions), bypassing multimodal LLMs' documented spatial reasoning weaknesses. Reasoning models trained on math/code domains transfer sequential decision-making capabilities to this structured game state.

### Mechanism 2: Reward-Contingent In-Context Learning Under High Reasoning Effort
Providing past rewards in-context improves gameplay, but only when reasoning effort is sufficiently high to process the additional signal without overfitting to frequent actions. High reasoning effort (more completion tokens) enables models to discriminate between "lucky" positive-reward actions and systematically optimal ones. At low reasoning effort, reward history biases models toward action frequency rather than action quality.

### Mechanism 3: Demonstration-Guided Prioritized Replay Accelerates DQN Convergence
Preloading a prioritized experience replay buffer with LLM demonstrations provides higher-quality initial exploration, reducing random walk inefficiency. LLM demonstrations are assigned high initial priority (5.0) in the replay buffer, ensuring frequent sampling during early training. As the DQN's TD-error-based priorities mature, expert demonstrations are naturally replaced by self-generated high-value experiences.

## Foundational Learning

- **Concept: Prioritized Experience Replay (PER)**
  - Why needed here: The LLM-guided DQN relies on PER to give higher sampling probability to "important" transitions—both LLM demonstrations and high TD-error self-experiences.
  - Quick check question: Can you explain why TD-error is used as a proxy for transition "importance," and what the hyperparameter α controls?

- **Concept: Object-Centric Representation**
  - Why needed here: The paper bypasses visual processing entirely by using OCAtari to extract structured object lists. Understanding this abstraction is essential for interpreting why reasoning models succeed here.
  - Quick check question: What information is lost when converting raw pixels to object lists, and how might this affect gameplay in Frogger?

- **Concept: Reasoning Effort as Compute Scaling**
  - Why needed here: The paper treats "reasoning effort" as a controllable variable (low/medium/high) that correlates with completion tokens. This is an emerging paradigm for reasoning models.
  - Quick check question: How does increasing reasoning effort differ from simply increasing model size, and what are the tradeoffs?

## Architecture Onboarding

- **Component map:**
  Game Environment (Frogger/ALE) -> OCAtari Object Extractor -> Object list (JSON) -> Reasoning LLM -> Action selection
  [Alternative path for LLM-guided DQN: LLM trajectories -> Prioritized Replay Buffer <- DQN self-experience -> DQN training loop]

- **Critical path:**
  1. Object extraction must be <17ms per frame to maintain 60 FPS gameplay
  2. LLM inference latency determines whether real-time play is feasible (it is not—paper uses step-by-step API calls)
  3. For DQN bootstrapping: demonstration quality determines early exploration efficiency

- **Design tradeoffs:**
  - Raw pixels vs. object-centric: Object-centric enables LLM reasoning but loses sub-pixel information and requires custom extractors per game
  - Past steps in context: More history provides better temporal reasoning but increases noise and token costs
  - LLM vs. DQN: LLMs provide zero-shot capability and demonstrations; DQN provides efficient inference after training

- **Failure signatures:**
  - Action frequency bias: LLM repeatedly selects UP because it was frequently rewarded, ignoring current hazards
  - Context window overflow: "All past steps" degrades performance due to noise
  - Reflection paradox: Verbal reflection generated high-level advice that lacked critical details, causing performance drop from 45 to 21
  - DFS explosion: Brute-force search shows exponential complexity—score 8 needs 90 seconds, score 9 needs 23 minutes

- **First 3 experiments:**
  1. Reproduce the o3-mini zero-shot result with 0 past steps, medium reasoning, hidden rewards. Verify you can achieve