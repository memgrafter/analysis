---
ver: rpa2
title: 'QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention'
arxiv_id: '2511.13679'
source_url: https://arxiv.org/abs/2511.13679
tags:
- detr
- deformable
- dooq
- quill
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUILL introduces a schedule-aware accelerator that transforms deformable
  attention into cache-friendly, single-pass work by combining Distance-based Out-of-Order
  Querying (DOOQ) with a fused MSDeformAttn engine. DOOQ reorders queries by spatial
  proximity and drives double-buffered prefetch, while the fused core executes interpolation,
  Softmax, aggregation, and projection without intermediate spills.
---

# QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention

## Quick Facts
- arXiv ID: 2511.13679
- Source URL: https://arxiv.org/abs/2511.13679
- Reference count: 36
- Achieves up to 7.29× higher throughput and 47.3× better energy efficiency than an RTX 4090

## Executive Summary
QUILL introduces a schedule-aware accelerator for deformable attention by transforming it into a cache-friendly, single-pass workload. The design combines Distance-based Out-of-Order Querying (DOOQ) with a fused MSDeformAttn engine to eliminate intermediate memory spills. DOOQ reorders queries by spatial proximity and drives double-buffered prefetch, while the fused core executes interpolation, Softmax, aggregation, and projection in a single pass. RTL evaluation shows significant performance and energy efficiency gains over both GPU and prior accelerators while maintaining FP32-level accuracy.

## Method Summary
QUILL's approach restructures deformable attention computation by first reordering queries based on spatial proximity (DOOQ), then executing all attention operations in a fused manner. The DOOQ scheduler enables double-buffered prefetching of key-value pairs, while the fused MSDeformAttn engine performs interpolation, Softmax, weighted sum, and projection without intermediate memory writes. This co-design minimizes memory traffic and maximizes cache utilization, enabling single-pass execution of what traditionally requires multiple memory accesses.

## Key Results
- Up to 7.29× higher throughput and 47.3× better energy efficiency than RTX 4090
- 3.26–9.82× throughput and 2.01–6.07× energy efficiency over prior accelerators
- Maintains FP32-level accuracy within ≤0.9 AP across Deformable and Sparse DETR variants

## Why This Works (Mechanism)
QUILL works by reorganizing the deformable attention computation schedule to maximize data locality. The DOOQ algorithm reorders queries by their spatial proximity to key-value elements, enabling predictable memory access patterns that benefit from prefetching. The fused MSDeformAttn core eliminates intermediate memory spills by keeping all intermediate results in fast on-chip storage during the single-pass execution. This combination reduces memory bandwidth pressure and leverages temporal and spatial locality, which are critical for efficient hardware execution.

## Foundational Learning
- **Deformable attention**: A sparse attention mechanism that samples only a small set of key elements per query, reducing computation while maintaining accuracy. Needed to understand the target workload and its memory access patterns.
- **Spatial proximity-based scheduling**: Organizing work based on physical or logical distance to improve cache locality and prefetch effectiveness. Quick check: verify that reordered queries indeed show better cache hit rates.
- **Double-buffered prefetching**: Overlapping computation with data loading using two buffers to hide memory latency. Quick check: measure achieved memory bandwidth utilization with and without double buffering.
- **Fused operator execution**: Combining multiple computational stages into a single hardware pipeline to eliminate intermediate storage. Quick check: count memory transactions with and without fusion.
- **RTL evaluation**: Register-transfer level simulation used to estimate hardware performance and energy before tape-out. Quick check: validate RTL results against analytical models.

## Architecture Onboarding
- **Component map**: DOOQ scheduler -> double-buffered memory controller -> fused MSDeformAttn core -> output aggregator
- **Critical path**: Query reordering → memory prefetch → interpolation → Softmax → aggregation → projection
- **Design tradeoffs**: Spatial reordering adds scheduling overhead but reduces memory traffic; fusion eliminates spills but increases core complexity
- **Failure signatures**: Poor DOOQ ordering leads to cache thrashing; insufficient buffer size causes stalls; fusion bottlenecks if interpolation is slower than other stages
- **First 3 experiments**: 1) Profile memory bandwidth with and without DOOQ reordering, 2) Measure intermediate storage requirements with and without fusion, 3) Characterize cache hit rates under different spatial access patterns

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- RTL code is not open-sourced, preventing independent verification of design details
- Energy measurements are extrapolated from simulations rather than measured on silicon
- Performance scaling beyond reported batch sizes and sequence lengths is unverified

## Confidence
- Throughput and energy efficiency claims: Medium (RTL evaluation with comparison methodology limitations)
- Accuracy preservation claim: High (reported across multiple DETR variants with standard protocols)

## Next Checks
1. Open-source or reproduce the RTL design to verify the claimed double-buffered prefetch and fused execution timing
2. Conduct end-to-end experiments with larger batch sizes and sequence lengths to assess scalability limits
3. Perform an ablation study isolating the impact of DOOQ reordering versus the fused core on both performance and accuracy