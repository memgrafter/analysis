---
ver: rpa2
title: Training Superior Sparse Autoencoders for Instruct Models
arxiv_id: '2506.07691'
source_url: https://arxiv.org/abs/2506.07691
tags:
- loss
- training
- features
- b-instruct
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FAST, a new training method for sparse autoencoders
  on instruction-tuned language models. It addresses the limitations of traditional
  block training methods, which struggle with the semantic discontinuity and misalignment
  of instruct model datasets.
---

# Training Superior Sparse Autoencoders for Instruct Models

## Quick Facts
- arXiv ID: 2506.07691
- Source URL: https://arxiv.org/abs/2506.07691
- Reference count: 40
- Primary result: FAST training method achieves 80% MSE reduction on instruct models while improving feature interpretability

## Executive Summary
This paper introduces FAST, a novel training method for sparse autoencoders (SAEs) on instruction-tuned language models that addresses the semantic discontinuity problem of traditional block training methods. By processing each data instance independently rather than concatenating samples, FAST preserves semantic integrity and better aligns with instruct model activation patterns. Experiments across multiple Llama and Qwen models demonstrate significant improvements in both reconstruction quality (over 80% MSE reduction) and feature interpretability, with additional findings showing that steering special token features can improve output quality within an optimal coefficient range.

## Method Summary
FAST replaces traditional Block Training with Sequential Training that processes each data instance independently to preserve semantic integrity. The method uses a mixing activation buffer to maintain training diversity without breaking semantic boundaries, and trains either Standard ReLU SAEs with L1 sparsity or JumpReLU SAEs with L0 sparsity. The approach is specifically designed for instruction-tuned models using instruction-tuning datasets (WildChat, Infinity-Instruct, Tulu-3, etc.) rather than pretraining corpora, with training tokens set at 40.96M and expansion factors of 8X (7B+) or 8X/16X (smaller models).

## Key Results
- MSE reduction: FAST achieves 0.6468 MSE on Qwen2.5-7B vs 3.3399 for Block Training (P)
- Interpretability: 21% of features achieve high-quality scores (4-5) on Llama3.2-3B
- Steering effects: Amplifying special token features (e.g., `