---
ver: rpa2
title: Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints
arxiv_id: '2503.05684'
source_url: https://arxiv.org/abs/2503.05684
tags:
- task
- fairness
- sensitive
- values
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles fairness in low-rank adaptation (LoRA) by proposing
  a distributed, privacy-preserving fine-tuning framework. It enables a model developer
  and a fairness auditor to collaborate without sharing sensitive attributes or predictors,
  using LoRA adapters to jointly optimize utility and fairness.
---

# Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints

## Quick Facts
- arXiv ID: 2503.05684
- Source URL: https://arxiv.org/abs/2503.05684
- Authors: Parameswaran Kamalaruban; Mark Anderson; Stuart Burrell; Maeve Madigan; Piotr Skalski; David Sutton
- Reference count: 40
- Primary result: Orthogonality loss method consistently reduces bias while maintaining or improving utility in privacy-preserving LoRA fine-tuning

## Executive Summary
This paper addresses fairness in low-rank adaptation (LoRA) by proposing a distributed, privacy-preserving fine-tuning framework. It enables a model developer and a fairness auditor to collaborate without sharing sensitive attributes or predictors, using LoRA adapters to jointly optimize utility and fairness. Three debiasing strategies are adapted: sensitive unlearning (debiasing via "unlearning" sensitive representations), adversarial training (joint optimization with gradient reversal), and orthogonality loss (regularizing downstream adapters to be orthogonal to sensitive ones). Evaluated on CelebA and UTK-Face with an ImageNet pre-trained ViT-Base, the orthogonality loss method consistently reduced bias and often improved overall utility, while adversarial training helped in some fairness metrics and sensitive unlearning showed no clear benefit. The results demonstrate that privacy-preserving, fairness-aware LoRA fine-tuning can effectively mitigate bias without sacrificing performance.

## Method Summary
The framework enables privacy-preserving collaboration between a Solution Developer (SD) and Compliance Officer (CO) for fairness-aware LoRA fine-tuning. SD has downstream task data {(x,y)} while CO has sensitive attribute data {(x',g)}, with neither sharing raw data or classification heads. Three debiasing strategies operate on LoRA adapters: sensitive unlearning (UNL) via adapter negation, adversarial training (ADV) with gradient reversal, and orthogonality loss (ORTH) via regularization. Experiments use ImageNet pre-trained ViT-Base with LoRA rank 4 on Q,V attention matrices, evaluated on CelebA and UTK-Face binary classification tasks with gender as sensitive attribute. The ORTH method consistently reduces bias while maintaining or improving utility across tasks.

## Key Results
- Orthogonality loss method consistently reduces bias while maintaining or improving utility
- Sensitive unlearning provides no clear benefit across all evaluated tasks
- Adversarial training shows mixed results, improving some fairness metrics but sometimes reducing utility
- Privacy is preserved through low-rank adapter sharing without exposing sensitive data or classification heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonality loss between LoRA adapters decorrelates task representations from sensitive attribute representations, reducing bias without degrading utility.
- Mechanism: A regularization term R_orth penalizes correlation between downstream adapter parameters (A^task, B^task) and sensitive adapter parameters (A^sen, B^sen) by minimizing ||(A^task)^T A^sen - I||²_F + ||(B^task)^T B^sen - I||²_F. This forces the downstream adapter to learn in a subspace orthogonal to sensitive features while preserving task-relevant information in the frozen base model.
- Core assumption: Sensitive attribute information is linearly capturable in low-rank adapter space, and orthogonality in this space translates to reduced dependency in predictions.
- Evidence anchors:
  - [abstract] "orthogonality loss method consistently reduces bias while maintaining or improving utility"
  - [section 4] Orthogonality regularization formula; λ_orth controls constraint strength
  - [corpus] Related work on task arithmetic notes that adapter composition can resolve interference when directions are managed (weak direct evidence for orthogonality specifically)
- Break condition: If sensitive attributes require high-rank representations exceeding LoRA rank (r=4 in experiments), orthogonality constraint may fail to capture the full dependency structure.

### Mechanism 2
- Claim: LoRA's modular design enables privacy-preserving collaboration by exchanging only low-rank adapter matrices, not raw data or classification heads.
- Mechanism: The decomposition W = W₀ + BA^T (rank r ≪ min(d,k)) constrains updates to a low-dimensional subspace. Only matrices A ∈ R^(d×r) and B ∈ R^(r×k) are shared between parties. Sensitive attribute predictors (classification heads w^sen) never leave the compliance officer's environment.
- Core assumption: Adapter parameters do not leak sensitive information sufficient to reconstruct protected attributes or training data.
- Evidence anchors:
  - [abstract] "model developers and fairness auditors collaborate without sharing sensitive attributes or predictors"
  - [section 1] "neither party is permitted to share their raw data or the corresponding classification heads"
  - [corpus] RESFL and FairFedMed similarly use federated setups for privacy-preserving fairness, suggesting viability of distributed approaches (but no direct evidence on information leakage from adapters specifically)
- Break condition: If adapter parameters can be reverse-engineered to infer sensitive patterns (membership inference, attribute inference), privacy guarantees break. Paper notes differential privacy as future work.

### Mechanism 3
- Claim: Sensitive unlearning via adapter negation provides no clear debiasing benefit because subtracting a sensitive adapter does not reliably remove sensitive information from the pretrained representation space.
- Mechanism: UNL trains θ^sen on sensitive data, then computes debiased weights as W_debiased = W₀ ⊖ λ_sen · θ^sen (negating LoRA-B component), then trains downstream adapter on this modified base. The approach assumes sensitive information is additively encoded and removable via subtraction.
- Core assumption: Task vectors are composable via arithmetic, and negation reverses learned capabilities.
- Evidence anchors:
  - [section 4] Equation 3 defines the unlearning operation
  - [section 5.2] "sensitive unlearning provides no clear benefit" across all tasks
  - [corpus] "On Fairness of Task Arithmetic" notes task vectors can affect fairness unpredictably during composition operations
- Break condition: If sensitive features are entangled with task-relevant features in the base model's frozen weights (not just in the adapter), negation cannot disentangle them. Results confirm this mechanism fails.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: All debiasing strategies operate on LoRA adapters; understanding W = W₀ + BA^T is prerequisite for interpreting orthogonality constraints and adapter composition.
  - Quick check question: Given a 1024×1024 weight matrix with LoRA rank 4, how many trainable parameters does the adapter add?

- Concept: **Group Fairness Metrics (Demographic Parity, FPR Parity, PPV Parity)**
  - Why needed here: Evaluation uses difference and ratio metrics across binary sensitive groups; interpreting Tables 2-3 requires knowing that difference >0.1 or ratio <0.9 typically signals bias.
  - Quick check question: If P(Ŷ=1|Male)=0.7 and P(Ŷ=1|Female)=0.5, what is the demographic parity difference? Is this likely biased?

- Concept: **Gradient Reversal Layer (GRL)**
  - Why needed here: Adversarial training (ADV) uses GRL to maximize sensitive attribute prediction loss during joint optimization; understanding the min-max game is essential.
  - Quick check question: In adversarial training, which adapter's gradients are reversed—the sensitive adapter or the downstream adapter?

## Architecture Onboarding

- Component map:
Pre-trained ViT-Base (frozen, 86M params)
    ├── LoRA adapters on Q, V attention matrices
    │   ├── θ^task (downstream adapter) — owned by Solution Developer
    │   └── θ^sen (sensitive adapter) — owned by Compliance Officer
    ├── Classification heads (never shared)
    │   ├── w^task — predicts target label y
    │   └── w^sen — predicts sensitive attribute g
    └── Training datasets (never shared)
        ├── D^(task) = {(x, y)} — with SD
        └── D^(sen) = {(x', g)} — with CO

- Critical path: For ORTH (recommended method): (1) CO trains θ^sen on D^(sen) → (2) CO shares θ^sen with SD → (3) SD trains θ^task on D^(task) with orthogonality regularizer referencing frozen θ^sen → (4) Deploy θ^pre ⊕ θ^task for inference.

- Design tradeoffs:
  - Higher LoRA rank (r) → more expressive adapters but potentially more information leakage risk
  - Higher λ_orth → stronger fairness enforcement but risk of underfitting task
  - ADV vs ORTH: ADV requires alternating optimization (more coordination overhead); ORTH is one-shot after sensitive adapter transfer

- Failure signatures:
  - UNL shows no improvement: sensitive information not additively removable
  - ADV shows utility drop (Table 1, CelebA-Bald ACC drops from 0.983 to 0.950): adversarial game too aggressive
  - Orthogonality fails if sensitive adapter overfits to spurious correlations in D^(sen) that don't generalize to D^(task)

- First 3 experiments:
  1. **Replicate ORTH on a single toy dataset** (e.g., synthetic tabular data with known sensitive correlation) to verify orthogonality loss reduces bias while preserving accuracy. Use rank r=4, λ_orth=0.1 as starting point.
  2. **Ablation on λ_orth**: Sweep [0.01, 0.1, 1.0, 10.0] to find where fairness improves without utility collapse. Monitor both DP difference and ACC.
  3. **Test distribution shift**: Train sensitive adapter on x'~P1, evaluate downstream on x~P2 with different demographic proportions. Quantify whether orthogonality constraint generalizes when D^(sen) and D^(task) distributions diverge (a real-world scenario the paper does not extensively evaluate).

## Open Questions the Paper Calls Out

- Question: Can combining debiasing strategies (e.g., adversarial training with orthogonality loss) yield complementary fairness improvements, or do they interfere with each other?
  - Basis in paper: [explicit] The conclusion states: "Potential directions include exploring combinations of the debiasing strategies (e.g., integrating adversarial training with orthogonality loss)."
  - Why unresolved: The three strategies were evaluated independently; no experiments tested whether their mechanisms (gradient reversal vs. orthogonality constraints) are compatible when applied jointly.
  - What evidence would resolve it: Experiments combining ADV and ORTH losses on the same fine-tuning run, measuring whether fairness gains from both methods accumulate or whether optimization conflicts arise.

- Question: Would differentially private LoRA adapter sharing preserve the effectiveness of orthogonality-based debiasing, or does noise injection undermine the regularization signal?
  - Basis in paper: [explicit] The conclusion notes: "investigating differentially private methods for sharing LoRA adapters may further enhance privacy, though such approaches could pose challenges in maintaining the effectiveness of bias mitigation and utility."
  - Why unresolved: The current framework assumes adapters can be shared without formal privacy guarantees; differential privacy would add noise that may corrupt the orthogonality relationship between task and sensitive adapters.
  - What evidence would resolve it: Empirical evaluation of ORTH debiasing under DP-SGD or noise injection on adapter parameters, comparing fairness-utility trade-offs to non-private baselines.

- Question: Why does sensitive unlearning fail to reduce bias when adapted from language model detoxification to vision classification?
  - Basis in paper: [inferred] Tables 1–4 consistently show UNL provides no clear benefit over ERM across all tasks, yet the method was repurposed from successful LM detoxification work [46].
  - Why unresolved: The paper reports the failure but does not analyze whether the issue stems from task vector interference, inappropriate scaling factor λ_sen, or fundamental differences between unlearning toxicity in language vs. visual demographic features.
  - What evidence would resolve it: Ablation studies varying λ_sen, analyzing task vector alignment between sensitive and downstream adapters, or visualizing representation shifts before/after unlearning.

- Question: How does the framework perform with non-binary sensitive attributes or multi-class downstream tasks?
  - Basis in paper: [inferred] Section 3 explicitly restricts the setup: "we consider a binary classification problem with a binary sensitive attribute."
  - Why unresolved: Real-world fairness applications often involve multiple protected groups (e.g., race with 5+ categories) and multi-class predictions; the orthogonality constraint formulation assumes binary vectors and may not generalize.
  - What evidence would resolve it: Extending ORTH and ADV to multi-class settings (e.g., UTK-Face race attribute, CelebA multi-attribute prediction) and evaluating whether fairness improvements persist.

## Limitations

- Exact hyperparameter values for regularization weights (λ_norm, λ_orth, λ_sen) and ADV optimization schedules are unspecified
- Privacy guarantees rely on LoRA's low-rank structure without empirical validation against attribute inference attacks
- Distribution shift between D^(sen) and D^(task) is not extensively tested, critical for real-world deployment

## Confidence

- **High confidence**: ORTH method consistently reduces bias while maintaining or improving utility (supported by multiple task evaluations)
- **Medium confidence**: LoRA's modular design enables privacy-preserving collaboration (logical but lacks empirical leakage analysis)
- **Low confidence**: Sensitive unlearning provides no clear benefit (results show consistent failure but mechanism explanation is weak)

## Next Checks

1. Replicate ORTH on synthetic data with known sensitive correlations to verify bias reduction while preserving accuracy
2. Perform λ_orth hyperparameter sweep to identify optimal trade-off between fairness improvement and utility preservation
3. Evaluate distribution shift robustness by training sensitive adapter on one demographic distribution and testing downstream performance on another