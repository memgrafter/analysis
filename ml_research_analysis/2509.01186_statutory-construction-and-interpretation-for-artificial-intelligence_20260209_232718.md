---
ver: rpa2
title: Statutory Construction and Interpretation for Artificial Intelligence
arxiv_id: '2509.01186'
source_url: https://arxiv.org/abs/2509.01186
tags:
- rule
- response
- rules
- interpretation
- interpretive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Interpretive ambiguity in AI alignment principles leads to inconsistent\
  \ model behavior. This paper proposes two computational frameworks\u2014interpretive\
  \ constraints and rule refinement\u2014to reduce such ambiguity by mirroring legal\
  \ mechanisms for statutory interpretation."
---

# Statutory Construction and Interpretation for Artificial Intelligence

## Quick Facts
- **arXiv ID**: 2509.01186
- **Source URL**: https://arxiv.org/abs/2509.01186
- **Reference count**: 40
- **Primary result**: Interpretive ambiguity in AI alignment principles leads to inconsistent model behavior.

## Executive Summary
This paper addresses interpretive ambiguity in Constitutional AI pipelines, where judge models disagree on whether scenarios comply with rules. It proposes two computational frameworks: prompt-based interpretive constraints that reduce entropy by instructing judges to adopt specific legal interpretive strategies, and iterative rule refinement that disambiguates rules through textual revision while preserving meaning. Experiments on 5,000 WildChat scenarios show both methods significantly improve judgment consistency, with entropy reduced to near zero in refined rules.

## Method Summary
The study uses WildChat dataset (1M conversations) filtered to ~166k English conversations (length 15-1028). Test set: 5,000 randomly sampled scenarios. 56 rules adapted from Claude's constitution, paraphrased into declarative form. Two main interventions: (1) Interpretive constraints using 5 judge models (Qwen2.5-32B-Instruct, Qwen3-32B-Instruct, Llama3.3-70B-Instruct, Gemma2-27B-Instruct, Gemma3-27B-Instruct) evaluating scenarios under 12 legal interpretive strategies; (2) Rule refinement using prompt-based Qwen2.5-7B-Instruct refiner or GRPO-trained policy, optimizing for entropy reduction and meaning preservation. Entropy measured via Shannon entropy across panel of reasonable interpreters.

## Key Results
- Prompt-based interpretive constraints reduce average entropy from 0.59 to as low as 0.40
- Rule refinement methods achieve near-zero entropy for most tested rules while preserving original meaning
- Entropy reaches near-zero levels for all 5 tested rules after refinement
- No Interpretation baseline exhibits highest entropy across five tested rules

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Interpretive Constraints
Specifying interpretive strategies in prompts reduces inter-model disagreement by constraining the space of reasonable interpretations. Models follow specified strategies (e.g., Narrow/textualist, Broad/purposivist) to apply consistent judgment criteria. Core assumption: models can reliably follow specified interpretive strategies. Evidence: entropy reduced from 0.59 to 0.40; No Interpretation baseline shows highest entropy. Break condition: models cannot consistently apply specified strategy.

### Mechanism 2: Iterative Rule Refinement
Iteratively revising rule text using high-entropy scenarios reduces interpretive ambiguity while preserving original meaning. Refiner model generates candidate revisions evaluated against development set, selecting revisions that minimize entropy across interpreters. Core assumption: rules can be disambiguated through textual revision without changing normative intent. Evidence: near-zero entropy achieved; subtle revisions effective. Break condition: rules drift in meaning or edit-distance regularizer is mis-calibrated.

### Mechanism 3: Panel-Based Entropy Measurement
Shannon entropy across simulated panel of reasonable interpreters quantifies interpretive ambiguity for rule-scenario pairs. Multiple models or strategies generate judgment distributions, entropy computed to measure disagreement. Core assumption: interpreter panel adequately covers space of reasonable interpretations. Evidence: formal entropy definition; greedy strategy selection shows top 3-4 strategies capture most variance. Break condition: panel not representative of true interpretive variation.

## Foundational Learning

- **Concept**: Constitutional AI (CAI) pipeline
  - **Why needed here**: The paper critiques and extends CAI's critique-revision-preference pipeline. Understanding baseline (SL-CAI critique step, RL-CAI preference-judgment step) clarifies where interpretive ambiguity enters.
  - **Quick check question**: Can you describe the two interpretive steps in CAI where ambiguity arises (Critique and Preference-Judgment)?

- **Concept**: Shannon entropy for categorical distributions
  - **Why needed here**: Entropy is core metric for measuring disagreement. Low entropy ≈ consensus; high entropy ≈ divergence among interpreters.
  - **Quick check question**: Given judgment probabilities [0.5, 0.5] vs [0.9, 0.1] for {compliant, non-compliant}, which has higher entropy?

- **Concept**: Legal canons of statutory interpretation
  - **Why needed here**: The 12 interpretive strategies are adapted from legal theory (e.g., Ordinary Meaning, Negative Implication, Harmonization). Knowing legal motivation helps select appropriate strategies.
  - **Quick check question**: How does "Negative Implication" (expressio unius est exclusio alterius) differ from "Broad" purposivist interpretation?

## Architecture Onboarding

- **Component map**: Rule Set (56 adapted CAI rules) -> Scenario Pool (5k WildChat conversations) -> Interpreter Panel (5 judge models OR 1 model × 12 strategies) -> Entropy Calculator (Shannon entropy over judgment distribution) -> Refiner Module (prompt-based Qwen2.5-7B-Instruct OR GRPO-trained policy) -> Reward Model (judge-consistency + edit-distance)

- **Critical path**: 1. Filter scenarios → select high-entropy subset for training/dev 2. Run baseline entropy measurement (no interpretive strategy) 3. Apply interpretive constraints OR rule refinement 4. Re-evaluate entropy on held-out test set 5. Human evaluation for meaning preservation (if refining rules)

- **Design tradeoffs**: More interpreters → better coverage but higher compute; stricter edit-distance regularizer → less meaning drift but potentially less entropy reduction; prompt-based refinement → simpler but may overfit; GRPO → generalizes across rules but requires training

- **Failure signatures**: Entropy unchanged despite constraints → models not following strategy; refined rules become scenario-specific hacks; high rule-strategy interaction (some strategies increase entropy for certain rules)

- **First 3 experiments**: 1. Replicate baseline entropy measurement for top-5 high-entropy rules using 5-model panel with no interpretive guidance 2. Test all 12 interpretive strategies on these rules; compute net entropy effect to identify rule-specific best strategies 3. Run prompt-based rule refinement for one high-entropy rule with human evaluation of meaning preservation

## Open Questions the Paper Calls Out

- **Open Question 1**: Can rule refinement and interpretive constraint mechanisms scale effectively to large, multi-rule systems with multiple judge models? [explicit] "Adding intent-based constraints and scaling from small rule sets to larger, multi-judge systems remain open tasks." Why unresolved: Experiments tested only individual rules with either 5-model panel or 12 interpretive strategies from single model, not combinations. What evidence would resolve it: Demonstrate entropy reduction across full constitution of 50+ interacting rules with multiple judge models.

- **Open Question 2**: How many interpretive strategies are needed to capture meaningful space of reasonable interpretation? [inferred] "Exploratory tests show that four strategies often capture most variance, suggesting either narrower space than assumed or gaps in our coverage." Why unresolved: 12 strategies selected ad hoc from legal canons; greedy selection suggests only 3-4 may be needed, but coverage completeness unclear. What evidence would resolve it: Systematic enumeration of interpretive variation and verification that additional strategies do not increase entropy beyond defined threshold.

- **Open Question 3**: How can rule refinement preserve rulemaker intent while reducing ambiguity? [explicit] "The rule-refinement pipeline improves judgment consistency but does not guarantee alignment with the rulemaker's intent." Why unresolved: GRPO-based and prompt-based refinements reduced entropy, but human evaluation showed some revisions introduced meaning shifts (e.g., Rule 2 changed from prohibition to explicit imperative). What evidence would resolve it: Develop intent-preserving constraints (e.g., using legislative history or explicit intent statements) and evaluate whether revisions maintain original meaning across diverse test cases.

- **Open Question 4**: Do these findings generalize across domains beyond conversational AI and across diverse model families? [explicit] "Applying the framework to broader domains and additional model families is an essential next step." Why unresolved: All experiments used WildChat scenarios and Claude's 56-rule constitution with 5 instruction-tuned models from 3 families (Qwen, Llama, Gemma). What evidence would resolve it: Replicate experiments on domains like code generation, robotic control, or medical AI, and across additional model architectures.

## Limitations

- Interpretative strategy fidelity: No direct validation that models consistently apply specified legal interpretive strategies
- Rule meaning drift: Neither refinement method perfectly preserves original meaning; some rules drift in normative intent
- Generalizability: All experiments use WildChat scenarios and Claude's 56-rule constitution; unclear if findings transfer to other domains or rule sets

## Confidence

- **High confidence**: Entropy is valid metric for measuring interpretive disagreement; interpretive constraints reduce entropy compared to no-strategy baseline; iterative refinement can reduce entropy to near zero
- **Medium confidence**: Interpretive constraints reliably reduce entropy across all tested rules; refined rules preserve original meaning without drift; simulated panel of 5 models captures most reasonable interpretations
- **Low confidence**: Entropy reduction translates to improved model behavior in real-world deployment; chosen 12 legal strategies are exhaustive or optimal; WildChat test set is representative of all high-entropy cases

## Next Checks

1. **Strategy adherence validation**: Run small experiment to verify judge models actually follow specified interpretive strategy instructions (e.g., compare outputs under "Narrow" vs "Broad" for same scenario)

2. **Meaning preservation audit**: For each refined rule, compute proportion of human evaluators who judge meaning as preserved (score ≥0); report distribution and identify rules with high drift

3. **Cross-domain transfer test**: Apply best-performing interpretive constraints and refined rules to new dataset (e.g., ArceeAFM) and measure entropy change to assess generalizability