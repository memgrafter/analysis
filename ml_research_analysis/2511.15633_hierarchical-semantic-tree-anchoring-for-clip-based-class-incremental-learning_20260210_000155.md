---
ver: rpa2
title: Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning
arxiv_id: '2511.15633'
source_url: https://arxiv.org/abs/2511.15633
tags:
- learning
- classes
- hierarchical
- clip
- hyperbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HASTEN, a framework that explicitly incorporates
  hierarchical semantic structures into class-incremental learning (CIL) to address
  catastrophic forgetting in CLIP-based models. It leverages an external knowledge
  graph to generate hierarchical supervision and employs a hyperbolic space representation
  to naturally encode these hierarchies.
---

# Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning

## Quick Facts
- **arXiv ID**: 2511.15633
- **Source URL**: https://arxiv.org/abs/2511.15633
- **Reference count**: 40
- **Primary result**: HASTEN achieves up to 3.34% higher accuracy than state-of-the-art methods in CLIP-based class-incremental learning

## Executive Summary
This paper introduces HASTEN, a novel framework for class-incremental learning (CIL) with CLIP-based models that leverages hierarchical semantic structures to mitigate catastrophic forgetting. The key insight is that existing CIL methods struggle because they lack explicit supervision for inter-class relationships, which is particularly problematic for CLIP's multimodal representations. HASTEN addresses this by incorporating external knowledge graphs to generate hierarchical supervision and using hyperbolic space to naturally encode these hierarchies. The framework introduces task-specific hierarchy-aware perception modules and gradient projection techniques to preserve knowledge of previous tasks while learning new ones.

## Method Summary
HASTEN operates by first constructing a hierarchical semantic tree from an external knowledge graph, which serves as a backbone for generating supervision signals during incremental learning. The framework employs hyperbolic space representations to naturally encode hierarchical relationships, avoiding the distortions that occur when projecting hierarchies into Euclidean space. Two main innovations are introduced: task-specific hierarchy-aware perception modules that learn relations among texts and between images and texts, and gradient projection onto the null space of previous task features to prevent interference with prior knowledge. During training, the model optimizes both the cross-entropy loss for new classes and a hierarchical regularization term that encourages embeddings to respect the semantic tree structure. The gradient projection mechanism ensures that updates for new tasks do not disrupt the representation space of previously learned tasks.

## Key Results
- HASTEN consistently outperforms state-of-the-art methods across nine diverse datasets, achieving up to 3.34% higher accuracy
- The framework demonstrates improved robustness to feature drift compared to existing approaches
- Ablation studies confirm that both hierarchical anchoring and gradient projection are critical for mitigating catastrophic forgetting

## Why This Works (Mechanism)
HASTEN works by explicitly encoding hierarchical semantic relationships into the learning process, which provides stronger supervision signals than traditional class-incremental learning methods. By leveraging external knowledge graphs, the framework can generate supervision that reflects the natural taxonomic relationships between classes, rather than treating each class as an independent entity. The use of hyperbolic space is particularly effective because it naturally represents hierarchical structures without the distortions that occur when embedding trees in Euclidean space. The gradient projection mechanism ensures that when learning new tasks, the model's updates are constrained to directions that don't interfere with previously learned representations, effectively creating a null space that preserves old knowledge while allowing new learning.

## Foundational Learning

**CLIP-based Models**
- *Why needed*: CLIP provides strong multimodal representations that can be leveraged for various vision-language tasks
- *Quick check*: Verify that the CLIP model can effectively encode both visual and textual information for the target domain

**Class-Incremental Learning**
- *Why needed*: Real-world scenarios require models to learn new classes over time without forgetting previous ones
- *Quick check*: Ensure the evaluation protocol correctly simulates incremental learning scenarios with disjoint class sets

**Knowledge Graphs**
- *Why needed*: Provide structured semantic information that can be used to generate hierarchical supervision
- *Quick check*: Validate that the external knowledge graph contains relevant relationships for the target dataset

**Hyperbolic Space Embeddings**
- *Why needed*: Naturally represent hierarchical structures without the distortions of Euclidean embeddings
- *Quick check*: Confirm that hierarchical relationships are preserved when embedding the semantic tree in hyperbolic space

## Architecture Onboarding

**Component Map**
External Knowledge Graph -> Hierarchical Semantic Tree -> Hyperbolic Embedding Space -> Task-specific Hierarchy-aware Modules -> Gradient Projection -> CLIP Model

**Critical Path**
The most critical path is: Hierarchical Semantic Tree construction → Hyperbolic embedding → Gradient projection mechanism. This path is essential because it establishes the hierarchical supervision structure and ensures that learning new tasks doesn't interfere with previously acquired knowledge.

**Design Tradeoffs**
- *Explicit hierarchy vs. learned relationships*: HASTEN chooses explicit hierarchical supervision over learning relationships from data, trading flexibility for stronger supervision
- *Hyperbolic vs. Euclidean space*: Hyperbolic space better represents hierarchies but may complicate certain operations and increase computational overhead
- *Gradient projection overhead*: The null space projection adds computational cost but is essential for preventing forgetting

**Failure Signatures**
- Poor performance when the external knowledge graph doesn't align well with the target dataset's taxonomy
- Degraded accuracy if the hierarchical relationships are noisy or incomplete
- Computational bottlenecks during gradient projection in high-dimensional spaces

**3 First Experiments**
1. Verify hierarchical structure preservation by visualizing embeddings in 2D hyperbolic space
2. Test gradient projection effectiveness by measuring cosine similarity between old and new task features
3. Evaluate sensitivity to knowledge graph quality by introducing controlled noise in hierarchical relationships

## Open Questions the Paper Calls Out

None

## Limitations

- Scalability concerns when applying HASTEN to extremely large-scale class-incremental scenarios with hundreds of tasks
- Potential performance degradation when hierarchical semantic structures in the knowledge graph don't perfectly align with the target dataset's taxonomy
- Computational overhead introduced by hierarchical anchoring and gradient projection mechanisms is not thoroughly analyzed for resource-constrained settings

## Confidence

**Major uncertainties include the scalability of HASTEN to extremely large-scale class-incremental scenarios and its performance when the hierarchical semantic structures in the knowledge graph do not perfectly align with the target dataset's taxonomy. The paper does not extensively evaluate robustness to noisy or incomplete hierarchical information, which could be a practical concern. Additionally, the computational overhead introduced by the hierarchical anchoring and gradient projection mechanisms is not thoroughly analyzed, leaving questions about efficiency in resource-constrained settings.**

Confidence in the claim that HASTEN effectively mitigates catastrophic forgetting is **High**, given the consistent outperformance over state-of-the-art methods across multiple datasets and the ablation studies supporting the contributions of both hierarchical anchoring and gradient projection. Confidence in the assertion that hyperbolic space naturally encodes hierarchical relationships is **Medium**, as the theoretical motivation is sound but empirical validation specific to CLIP-based CIL is limited. Confidence in the robustness to feature drift is **Medium**, as the results are promising but the evaluation could benefit from more diverse and challenging drift scenarios.

## Next Checks

1. **Scalability Test**: Evaluate HASTEN on a large-scale dataset with hundreds of incremental tasks to assess its performance and computational efficiency under extreme class-incremental conditions.

2. **Robustness to Noisy Hierarchies**: Conduct experiments where the external knowledge graph contains deliberate errors or missing links to test the framework's resilience to imperfect hierarchical supervision.

3. **Efficiency Analysis**: Measure and compare the computational overhead (e.g., training time, memory usage) of HASTEN against baseline methods to quantify the trade-offs introduced by the hierarchical anchoring and gradient projection mechanisms.