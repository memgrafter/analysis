---
ver: rpa2
title: 'Forget What''s Sensitive, Remember What Matters: Token-Level Differential
  Privacy in Memory Sculpting for Continual Learning'
arxiv_id: '2509.12958'
source_url: https://arxiv.org/abs/2509.12958
tags:
- privacy
- learning
- score
- sensitivity
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy risks in continual learning (CL) where
  models accumulate sensitive data over time. It proposes a token-level dynamic differential
  privacy (TDP) approach that assigns privacy budgets to individual tokens based on
  their sensitivity, derived from model uncertainty and contextual distinctiveness.
---

# Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning

## Quick Facts
- arXiv ID: 2509.12958
- Source URL: https://arxiv.org/abs/2509.12958
- Reference count: 19
- Primary result: Outperforms state-of-the-art baselines with Avg accuracy 0.535, Last accuracy 0.573, and lowest forgetting (BWT = -0.093) on six-task datasets

## Executive Summary
This paper addresses privacy risks in continual learning where models accumulate sensitive data over time. The proposed Privacy-enhanced Continual Learning (PeCL) framework combines token-level dynamic differential privacy with a privacy-guided memory sculpting module. By assigning privacy budgets based on token sensitivity and selectively forgetting sensitive information while preserving general knowledge, PeCL achieves superior privacy-utility trade-offs compared to existing methods.

## Method Summary
PeCL operates on a frozen LLaMA-2-7B backbone with LoRA adapters, processing data in sequential task streams. For each token, it calculates a sensitivity score from model uncertainty and contextual distinctiveness, then maps this to a dynamic privacy budget for noise injection. The Privacy-guided Memory Sculpting module adjusts regularization strength inversely to data sensitivity, allowing the model to forget sensitive traces while preserving task-invariant knowledge through stability-aligned regularization and privacy-aware unlearning.

## Key Results
- Achieves Avg accuracy of 0.535 and Last accuracy of 0.573 on six-task datasets
- Minimizes forgetting with BWT = -0.093, outperforming state-of-the-art baselines
- Maintains robust privacy guarantees through calibrated differential privacy noise
- Demonstrates effectiveness across diverse text classification tasks (FOMC, Yelp, AGNews, Amazon, Mentill, Yahoo)

## Why This Works (Mechanism)

### Mechanism 1
Dynamic noise allocation based on token sensitivity preserves utility better than uniform DP. The system computes a fused sensitivity score for each token using predictive uncertainty and contextual discriminativeness, then maps this to a dynamic privacy budget where high sensitivity triggers stronger noise. Core assumption: predictive uncertainty and cross-task rarity are reliable proxies for privacy risk. Break condition: sensitive data appearing in high-confidence contexts may be under-estimated.

### Mechanism 2
Modulating regularization strength inversely to data sensitivity facilitates selective forgetting while retaining general knowledge. The PMS module adjusts the weight of stability-aligned regularization loss, maximizing preservation for low-sensitivity tasks and minimizing constraints for high-sensitivity tasks. Core assumption: parameter changes during high-sensitivity tasks correspond to forgetting sensitive data. Break condition: tasks containing mixed sensitive and critical general knowledge may cause catastrophic forgetting.

### Mechanism 3
Explicit gradient penalization prevents reinforcement of high-sensitivity information. A privacy-aware unlearning loss term scales cross-entropy loss for tokens exceeding sensitivity threshold, discouraging pattern reinforcement during backward pass. Core assumption: increasing loss contribution leads to effective unlearning. Break condition: threshold set too low creates noisy signals; too high fails to suppress sensitive tokens.

## Foundational Learning

- **Differential Privacy (DP) - Gaussian Mechanism**: Mathematical substrate for privacy through calibrating Gaussian noise to function sensitivity. Quick check: How does clipping norm C relate to noise scale σ in Eq. 5?

- **Catastrophic Forgetting & Regularization**: Addresses tension between privacy (parameter changes) and stability (parameter preservation). Quick check: Why use Frobenius norm of LoRA updates combined with input activations to compute importance Ω?

- **LoRA (Low-Rank Adaptation)**: Optimizes only LoRA parameters rather than full LLM, enabling efficient memory sculpting. Quick check: In Eq. 7, how does decomposition B × A allow efficient importance calculation?

## Architecture Onboarding

- **Component map**: Tokenizer & Embedding Table -> Token Sensitivity Calculator -> TDP Noise Injector -> LLM + LoRA -> PMS Module -> LoRA Weights
- **Critical path**: Ingest (tokenize → embeddings) → Score (calculate sensitivity) → Perturb (apply dynamic noise) → Forward (LLM + LoRA) → Sculpt (calculate total loss → update LoRA)
- **Design tradeoffs**: Sensitivity threshold θ involves precision-recall tradeoff; α balances model uncertainty vs. corpus statistics
- **Failure signatures**: BWT drops significantly (e.g., < -0.2) indicates weak regularization or aggressive unlearning
- **First 3 experiments**: 1) Visualize token scores for known PII vs. generic text, 2) Run SeqFT with TDP only to isolate memory sculpting contribution, 3) Sweep α parameter to find optimal uncertainty-context balance

## Open Questions the Paper Calls Out
- Can PeCL adapt to online continual learning with streaming data arrivals?
- How does lack of standardized benchmarks affect privacy-utility trade-off evaluation?
- Is heuristic token sensitivity robust against adversarial attacks designed to lower sensitivity scores?

## Limitations
- Architectural specification gaps: missing LoRA configuration details (rank, alpha scaling) and gradient clipping norm C
- Sensitivity scoring validity: relies on predictive uncertainty as proxy for privacy risk without empirical validation across diverse data types
- Empirical scope constraints: tested only on six small text classification datasets, effectiveness on larger or sequential data unknown

## Confidence
- **High Confidence**: Token-level dynamic privacy allocation mechanism; memory sculpting regularization framework
- **Medium Confidence**: Claims of outperforming baselines; provision of differential privacy guarantees
- **Low Confidence**: Sensitivity scores reliably detecting all sensitive information; scalability to large/heterogeneous data streams

## Next Checks
1. Validate sensitivity scoring alignment by comparing token scores on known PII dataset versus generic text
2. Isolate TDP vs. PMS contribution by implementing baseline with TDP only and comparing to full PeCL
3. Conduct hyperparameter sensitivity analysis on α parameter to test scoring mechanism robustness