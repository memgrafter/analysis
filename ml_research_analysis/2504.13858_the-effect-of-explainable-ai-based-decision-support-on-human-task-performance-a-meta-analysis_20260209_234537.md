---
ver: rpa2
title: 'The Effect of Explainable AI-based Decision Support on Human Task Performance:
  A Meta-Analysis'
arxiv_id: '2504.13858'
source_url: https://arxiv.org/abs/2504.13858
tags:
- performance
- support
- decision
- human
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This meta-analysis examined the impact of explainable AI (XAI)
  on human performance in binary classification tasks across 16 studies from 14 papers.
  The analysis found that XAI-based decision support improves task performance compared
  to no support (SMD = 0.48, p = 0.001), but explanations do not significantly enhance
  performance beyond AI-only decision support (SMD = 0.09, p = 0.074).
---

# The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis

## Quick Facts
- arXiv ID: 2504.13858
- Source URL: https://arxiv.org/abs/2504.13858
- Reference count: 32
- Primary result: XAI-based decision support improves task performance compared to no support (SMD = 0.48, p = 0.001), but explanations do not significantly enhance performance beyond AI-only decision support (SMD = 0.09, p = 0.074)

## Executive Summary
This meta-analysis examines the impact of explainable AI (XAI) on human performance in binary classification tasks across 16 studies from 14 papers. The analysis reveals that XAI-based decision support significantly improves task performance compared to no support, with a standardized mean difference of 0.48. However, the addition of explanations to AI predictions does not yield significant performance gains beyond the AI prediction alone. The findings challenge the assumption that explanations are the key driver of XAI effectiveness and highlight the critical role of methodological rigor in research on human-AI collaboration.

## Method Summary
The meta-analysis employed a systematic literature review across EBSCO, Web of Science, and Scopus databases, identifying studies that evaluated XAI methods in user studies with quantitative performance measures. Data extraction included means, standard deviations, and sample sizes, with risk of bias assessed using the Cochrane RoB 2 tool adapted for this context. A random-effects regression model was used to pool standardized mean differences, with subgroup analyses examining explanation types (feature attribution, counterfactual, example-based) and risk of bias levels.

## Key Results
- XAI-based decision support improves task performance compared to no support (SMD = 0.48, p = 0.001)
- Explanations do not significantly enhance performance beyond AI-only decision support (SMD = 0.09, p = 0.074)
- Risk of bias moderates the effect of explanations, with high-RoB studies showing artificially inflated effects
- No significant differences found between explanation types (feature attribution, counterfactual, example-based)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The performance improvement from XAI-based decision support stems primarily from the AI prediction itself, not from the accompanying explanation layer.
- Mechanism: Users leverage the algorithmic prediction to calibrate their decisions. The explanation layer adds marginal information that does not significantly alter decision outcomes beyond what the prediction already provides.
- Core assumption: The AI predictions provided are sufficiently accurate to improve over human-only baseline performance.
- Evidence anchors:
  - [abstract] "explanations themselves are not the decisive driver for this improvement"
  - [results] "XAI vs. AI-based decision support... mean increase... 0.09 SMD [-0.01; 0.18]... 95% CI includes zero"
  - [corpus] "Can AI Explanations Make You Change Your Mind?" (arxiv 2508.08158) examines whether explanations help users judge when to trust AI—suggesting the question is open, consistent with meta-analysis finding no significant gain.
- Break condition: If AI prediction accuracy falls below human baseline, or if the task requires reasoning that explanations uniquely support, the equivalence may not hold.

### Mechanism 2
- Claim: Observed effects of explanations are moderated by methodological rigor of underlying studies.
- Mechanism: Studies with high risk of bias (inadequate randomization, selective reporting) produce inflated XAI effectiveness estimates, creating false impressions that explanations are effective when design flaws are responsible.
- Core assumption: The Cochrane RoB 2 tool accurately captures methodological quality differences.
- Evidence anchors:
  - [abstract] "The analysis reveals that the studies' risk of bias moderates the effect of explanations in AI"
  - [results] "Studies with 'High' risk of bias yield a considerable and significant increase in task performance (SMD=0.38; p<0.001)" versus "Low" risk studies (SMD=0.07, p=0.47)
  - [corpus] Limited direct corpus evidence on bias moderation in XAI; neighboring papers focus on trust and interface design rather than methodological quality assessment.
- Break condition: If all included studies had uniformly low risk of bias, this moderation effect would not be observable.

### Mechanism 3
- Claim: The specific explanation type (feature attribution, counterfactual, example-based) does not significantly moderate task performance.
- Mechanism: All three explanation types provide approximately equivalent marginal information beyond the prediction itself. Users may not extract differential value from semantic differences between formats.
- Core assumption: Users can comprehend and process all three explanation types with similar effectiveness.
- Evidence anchors:
  - [abstract] "explanation type (feature attribution, counterfactual, or example-based) plays only a negligible role"
  - [results] Table 1: CF (SMD=0.17, p=0.14), FA/FI (SMD=0.06, p=0.23), EB (SMD=0.18, p=0.37); subgroup difference p=0.42
  - [corpus] "From Explainable to Explanatory Artificial Intelligence" (arxiv 2508.06352) argues current XAI approaches "fail to support meaningful end-user understanding"—potentially explaining why different types perform equivalently.
- Break condition: If explanation types were matched to specific task characteristics or user expertise levels, differential effects might emerge.

## Foundational Learning

- Concept: Standardized Mean Difference (SMD) / Effect Size
  - Why needed here: The meta-analysis quantifies XAI effects using SMD across heterogeneous studies. Understanding that SMD=0.48 is "medium" and SMD=0.09 is "small" is essential for interpreting practical significance.
  - Quick check question: Given SMD=0.48 for XAI vs. no support and SMD=0.09 for XAI vs. AI-only, what does this imply about where performance gains originate?

- Concept: Risk of Bias Assessment (RoB 2)
  - Why needed here: Study quality moderates observed effects. High-RoB studies showed SMD=0.38 vs. low-RoB at SMD=0.07—critical for evaluating reliability of XAI claims.
  - Quick check question: Why might high-risk-of-bias studies systematically overestimate explanation effects?

- Concept: Random-Effects Meta-Analysis
  - Why needed here: The analysis assumes studies do not share a common true effect size due to varying setups, appropriate given XAI study heterogeneity.
  - Quick check question: What does a random-effects model assume about the distribution of true effects that a fixed-effects model does not?

## Architecture Onboarding

- Component map:
  - Prediction Module -> Explanation Layer -> Decision Interface -> User Decision Process

- Critical path:
  1. User receives binary classification instance
  2. System provides: nothing (control), AI prediction (AI-only), or prediction + explanation (XAI)
  3. User outputs final decision
  4. Performance measured as accuracy ratio (correct/total decisions)
  
  Key insight: The critical performance value flows through the AI prediction; the explanation branch shows no significant incremental gain.

- Design tradeoffs:
  - Adding XAI explanations: Increases complexity with no demonstrated performance gain over AI-only
  - Explanation type selection: Equivalent effects suggest choice may be guided by user preference, domain fit, or regulatory requirements rather than performance
  - Study design rigor: Rigorous designs show smaller effects—real-world XAI benefits may be overestimated

- Failure signatures:
  - Over-reliance on high-RoB studies: Expecting XAI to significantly outperform AI-only based on inflated effect sizes
  - Assuming explanation type matters: Investing in "optimal" type selection when evidence shows negligible differences
  - Ignoring prediction foundation: Focusing on explanation design without ensuring underlying AI prediction quality

- First 3 experiments:
  1. Baseline A/B/C test: Compare no-support, AI-only, and XAI conditions on your binary classification task. Expect: AI-only ≈ XAI > no-support (SMD ~0.48 for both vs. control).
  2. Internal RoB assessment: Evaluate experimental design against RoB 2 criteria before deployment to avoid inflated estimates from methodological flaws.
  3. Explanation type pilot: If explanations are required for non-performance reasons (trust, compliance, regulation), pilot FA, CF, and EB with representative users; select based on comprehension and preference rather than expected performance differential.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does AI literacy moderate the effect of XAI-based decision support on human task performance?
- Basis in paper: [explicit] The authors explicitly listed "AI literacy" as a planned subgroup analysis that could not be executed due to a lack of reporting in primary studies.
- Why unresolved: The meta-analysis lacked sufficient data, as the included studies did not consistently measure or report user AI literacy levels.
- What evidence would resolve it: Empirical studies that quantitatively assess and report user AI literacy alongside task performance metrics to enable future subgroup analysis.

### Open Question 2
- Question: How does XAI-based support affect performance in tasks with continuous outcomes or varying levels of task complexity?
- Basis in paper: [explicit] The authors identify the restriction to binary classification as a limitation and suggest future research should address "continuous outcome" measures and moderators like "task complexity."
- Why unresolved: The current analysis excluded non-binary tasks; therefore, the generalizability of the results to regression tasks or complex decision environments is unknown.
- What evidence would resolve it: A systematic review or new empirical studies focusing on continuous performance measures and explicitly manipulating task complexity.

### Open Question 3
- Question: Does tailoring explanations to the specific user or case context improve performance compared to static explanation types?
- Basis in paper: [inferred] The authors speculate that the "negligible" effect of explanation types may stem from the fact that "effective support requires more nuanced tailoring to both the user and case."
- Why unresolved: The analysis aggregated generic explanation types (e.g., feature attribution), potentially obscuring performance gains derived from personalized or context-aware XAI implementations.
- What evidence would resolve it: Comparative user studies evaluating the efficacy of adaptive/personalized XAI systems versus standard, one-size-fits-all explanation interfaces.

## Limitations
- The meta-analysis focuses exclusively on binary classification tasks, limiting generalizability to other decision contexts
- Many studies exhibit high risk of bias, potentially inflating observed effects of XAI-based decision support
- Insufficient data prevented analysis of moderators such as AI literacy and task complexity

## Confidence
- **High confidence**: XAI improves task performance compared to no support (SMD = 0.48, p = 0.001)
- **Medium confidence**: Explanation type does not significantly moderate performance
- **Medium confidence**: Risk of bias significantly moderates observed effects

## Next Checks
1. Replicate the meta-analysis with the original data extraction from the 14 source papers to verify effect size calculations and RoB assessments
2. Conduct sensitivity analysis excluding high-RoB studies to assess whether the AI-only advantage persists in methodologically rigorous studies
3. Extend analysis to non-binary classification tasks to evaluate generalizability of findings across different decision domains