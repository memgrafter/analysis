---
ver: rpa2
title: Policy Teaching via Data Poisoning in Learning from Human Preferences
arxiv_id: '2503.10228'
source_url: https://arxiv.org/abs/2503.10228
tags:
- rlhf
- problem
- attack
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a framework for understanding data poisoning\
  \ attacks on preference-based learning systems, specifically targeting reinforcement\
  \ learning from human feedback (RLHF) and direct preference optimization (DPO).\
  \ The authors develop theoretical bounds on the number of poisoned samples required\
  \ to enforce a target policy \u03C0\u2020 by analyzing the susceptibility of these\
  \ paradigms under different attack scenarios (data augmentation vs."
---

# Policy Teaching via Data Poisoning in Learning from Human Preferences

## Quick Facts
- arXiv ID: 2503.10228
- Source URL: https://arxiv.org/abs/2503.10228
- Reference count: 40
- Primary result: Theoretical bounds on data poisoning attack complexity for RLHF and DPO preference learning systems

## Executive Summary
This paper develops a theoretical framework for data poisoning attacks on preference-based learning systems, specifically targeting reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). The authors derive sample complexity bounds for enforcing a target policy through poisoned preference data, distinguishing between data augmentation and synthesis attack scenarios. A key finding is that DPO exhibits inherent robustness against distant policy attacks due to its reference policy anchoring, while unregularized RLHF allows deterministic policy enforcement through reward parameter manipulation.

## Method Summary
The paper analyzes data poisoning attacks on RLHF and DPO by deriving theoretical bounds on the number of poisoned samples required to enforce a target policy π†. For RLHF, the method involves constructing poisoned preference pairs that shift the learned reward parameters into a polytope where π† is strictly optimal. For DPO, the approach uses the Bradley-Terry model to compute the required parameter shifts while accounting for the regularization term that penalizes deviation from the reference policy. The theoretical analysis relies on linear reward features and log-linear policy classes, with bounds depending on feature matrix properties and the distance between target and reference policies.

## Key Results
- RLHF attack sample complexity scales with the projection of reward parameters onto a target polytope defined by policy optimality constraints
- DPO is more robust than RLHF when the target policy is far from the reference policy due to the regularization term
- The required number of poisoned samples increases with the squared distance between target and reference policy parameters in DPO
- Data augmentation attacks (modifying existing samples) are easier than synthesis attacks (creating new samples) in both paradigms

## Why This Works (Mechanism)

### Mechanism 1: Reward Parameter Polytope Alignment (RLHF)
The attacker generates samples such that feature differences align with a projection vector that moves the reward parameter into the target polytope where the target policy is strictly optimal. This requires the reward function class to be linear and the MDP ergodic.

### Mechanism 2: Reference Policy Anchoring (DPO Robustness)
DPO's regularization term penalizes deviation from the reference policy, making it resistant to enforcing distant target policies. The required sample complexity scales with the squared distance between target and reference policy parameters.

### Mechanism 3: Surrogate Constraint Relaxation
The complexity of enforcing exact policy similarity is bounded by solving a surrogate problem in parameter space using ℓ₁-norm polytopes for RLHF and ℓ₂-norm balls for DPO, assuming smooth parameter-to-policy mapping.

## Foundational Learning

- **Concept: Bradley-Terry Model**
  - Why needed here: Defines preference probability based on reward differences that the attacker must manipulate
  - Quick check question: How does the sigmoid function σ(z) influence gradient magnitude during a poisoning attack?

- **Concept: KL-Regularized Optimization**
  - Why needed here: Core objective of RLHF and DPO explaining DPO's resistance to large policy shifts
  - Quick check question: If regularization parameter β → ∞, what happens to the optimal policy π_reg?

- **Concept: Sample Complexity Bounds**
  - Why needed here: Quantifies "attack cost" by the theoretical number of samples required to shift parameter space
  - Quick check question: In Theorem 4.1, why does the lower bound depend on σ_min(M_π†)?

## Architecture Onboarding

- **Component map:** MDP Environment -> Learner (RLHF/DPO) -> Attacker (Data Poisoning) -> Target Policy π†
- **Critical path:** Feasibility Check → Parameter Projection → Gradient Alignment → Injection
- **Design tradeoffs:** DPO is more robust to distant attacks but bypasses reward modeling; unregularized RLHF allows deterministic policies but is less stable
- **Failure signatures:** High sample requirements due to large policy distances, rank deficiency in feature matrices
- **First 3 experiments:**
  1. Validate Theorem 6.1 by plotting attack success rate vs. ||θ† - θ_μ|| for DPO vs. RLHF
  2. Compare attack sample complexity when D=∅ vs. D≠∅ to isolate pre-existing gradient effects
  3. Vary β in regularized RLHF to observe trade-off between attack cost and policy smoothness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can attack sample complexity bounds be derived for general non-linear parametrizations?
- Basis: The paper proposes to relax linearity assumptions and solve for general parametrizations
- Why unresolved: Current analysis relies heavily on linear feature maps and convexity
- Evidence: No theoretical bounds for general function approximation classes

### Open Question 2
- Question: How does attack formulation and sample complexity change for label-flipping attacks?
- Basis: The paper states studying label-flipping attacks is important
- Why unresolved: Focus is on synthesis and augmentation, while label-flipping requires different constraints
- Evidence: No sample complexity bounds for label-flipping scenarios

### Open Question 3
- Question: What is attack effectiveness when learners use robust RLHF/DPO variants?
- Basis: The paper asks to understand effectiveness with robust variants
- Why unresolved: Provided bounds assume standard learners
- Evidence: No analysis of robust loss functions or filtering mechanisms

### Open Question 4
- Question: Can the framework be analyzed using total variation distance instead of KL divergence?
- Basis: The paper suggests studying alternate constraints like total variation distance
- Why unresolved: KL constraints make closed-form solutions difficult in regularized settings
- Evidence: No derivation of bounds using total variation distance

## Limitations
- Assumes access to trajectory synthesis oracle which may not be feasible in practice
- Relies on full-rank feature matrices and ergodic MDP assumptions that may not hold
- Limited to linear reward and log-linear policy classes, potentially limiting generalizability

## Confidence

- **High Confidence:** Theoretical derivations for RLHF and DPO are mathematically rigorous within stated assumptions
- **Medium Confidence:** Interpretation of attack sample complexity as practical metric requires empirical validation
- **Low Confidence:** Practical feasibility of trajectory synthesis assumption and impact of non-linear reward/policy classes

## Next Checks

1. **Empirical Validation of Distance Scaling:** Implement synthetic MDP experiment to measure actual poisoned samples needed for target policies at varying distances from reference policy, comparing RLHF vs. DPO

2. **Feature Matrix Sensitivity Analysis:** Systematically vary rank and conditioning of feature matrices Φ and Ψ to observe theoretical bound breakdown and identify practical attack feasibility thresholds

3. **Non-linear Extension Study:** Explore impact of non-linear reward functions on polytope alignment mechanism by approximating learned reward surface and testing if linear attack strategies still apply