---
ver: rpa2
title: Last Iterate Analyses of FTRL in Stochasitc Bandits
arxiv_id: '2510.22819'
source_url: https://arxiv.org/abs/2510.22819
tags:
- lemma
- have
- then
- proof
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the last-iterate convergence of Follow-the-Regularized-Leader
  (FTRL) algorithms in stochastic multi-armed bandits. While FTRL algorithms like
  1/2-Tsallis-INF achieve optimal regret bounds in both stochastic and adversarial
  settings, their last-iterate (simple regret) convergence rates have not been analyzed.
---

# Last Iterate Analyses of FTRL in Stochasitc Bandits

## Quick Facts
- arXiv ID: 2510.22819
- Source URL: https://arxiv.org/abs/2510.22819
- Reference count: 40
- Primary result: First last-iterate convergence analysis for FTRL in stochastic bandits, proving t^(-1/2) rate

## Executive Summary
This paper provides the first analysis of last-iterate convergence for Follow-the-Regularized-Leader (FTRL) algorithms in stochastic multi-armed bandits. While FTRL algorithms like 1/2-Tsallis-INF achieve optimal regret bounds, their simple regret (last-iterate convergence) had not been previously studied. The authors introduce a novel decomposition technique that enables tracking the evolution of probability distributions over arms across rounds. Using this framework with self-bounding techniques and case analysis, they prove that the Bregman divergence between the iterate at round t and the optimal arm decays at rate t^(-1/2).

## Method Summary
The authors develop a new regret analysis framework that decomposes the regret into components that can be tracked individually. This decomposition allows them to study how the probability distribution over arms evolves at each round. They apply this technique specifically to the 1/2-Tsallis-INF algorithm, leveraging self-bounding techniques and careful case analysis. The key insight is that by tracking the Bregman divergence between the current iterate and the optimal arm's distribution, they can establish convergence rates. The analysis critically depends on the learning rate being less than 1, which enables the self-bounding arguments to work effectively.

## Key Results
- First last-iterate convergence result for FTRL algorithms in stochastic bandits
- Proves t^(-1/2) convergence rate for Bregman divergence to optimal arm
- Shows second moment of Bregman divergence grows linearly with t when learning rate < 1
- Conjectures optimal rate might be t^(-1) based on theoretical bounds

## Why This Works (Mechanism)
The mechanism relies on the novel decomposition of regret that tracks probability distribution evolution, combined with self-bounding techniques that create feedback loops between different components of the analysis. The Bregman divergence serves as a natural distance measure between the current probability distribution and the optimal arm's distribution.

## Foundational Learning
- **Bregman divergence**: Measures distance between probability distributions; needed for tracking convergence to optimal arm
  - Quick check: Verify subgradient properties hold for the specific regularizer used
- **FTRL framework**: Follow-the-Regularized-Leader algorithm structure; needed as the base algorithm
  - Quick check: Confirm regret bounds hold under the given assumptions
- **Self-bounding techniques**: Mathematical methods that create recursive bounds; needed for handling complex dependencies
  - Quick check: Validate that the self-bounding recursions converge as claimed
- **Tsallis entropy regularization**: Specific regularizer that enables optimal regret; needed for the algorithm's properties
  - Quick check: Ensure the Tsallis entropy properties are correctly applied
- **Stochastic bandit setting**: Framework where rewards are drawn from fixed distributions; needed for problem formulation
  - Quick check: Verify independence assumptions hold across rounds
- **Last-iterate vs average-iterate**: Difference between final point and time-averaged behavior; needed for convergence analysis
  - Quick check: Confirm that last-iterate analysis differs meaningfully from standard regret analysis

## Architecture Onboarding

Component Map: Initialization -> FTRL Update -> Bregman Divergence Tracking -> Convergence Analysis

Critical Path: Learning rate selection < 1 -> FTRL update with Tsallis entropy -> Bregman divergence computation -> Self-bounding argument -> Convergence rate derivation

Design Tradeoffs: The choice of learning rate < 1 enables the self-bounding technique but may sacrifice some regret performance compared to optimal rates. The decomposition technique provides analytical tractability but may not capture all algorithm behaviors.

Failure Signatures: If learning rate â‰¥ 1, the self-bounding technique breaks down. If the decomposition doesn't properly account for correlation between rounds, convergence rates may be incorrect. If Bregman divergence grows too quickly, the convergence analysis fails.

First Experiments:
1. Verify the t^(-1/2) convergence rate empirically on synthetic bandit problems
2. Test the algorithm with learning rates both below and above 1 to confirm the critical threshold
3. Compare last-iterate behavior with average-iterate behavior to quantify the difference

## Open Questions the Paper Calls Out
The authors specifically conjecture that the optimal last-iterate convergence rate for FTRL in stochastic bandits might be t^(-1) rather than the proven t^(-1/2) rate, suggesting this as a key open question for future research.

## Limitations
- The analysis critically depends on the learning rate being less than 1, which may limit practical applicability
- The t^(-1/2) convergence rate is established but may not be optimal (authors conjecture t^(-1))
- The decomposition technique, while novel, may not generalize to other bandit algorithms or settings
- The self-bounding techniques and case analysis may not provide a complete picture of all algorithm behaviors

## Confidence

**High Confidence**: The technical framework for analyzing last-iterate convergence using Bregman divergence is sound and well-established.

**Medium Confidence**: The t^(-1/2) convergence rate is established but may not be optimal.

**Low Confidence**: The conjecture that the optimal rate is t^(-1) is speculative and requires further investigation.

## Next Checks

1. **Empirical Validation**: Conduct experiments to empirically validate the t^(-1/2) convergence rate and test whether the conjecture of t^(-1) holds in practice.

2. **Learning Rate Sensitivity**: Investigate the impact of different learning rate values (particularly those greater than 1) on the last-iterate convergence rate to understand the algorithm's behavior beyond the critical threshold.

3. **Generalization to Other Algorithms**: Extend the decomposition technique to analyze last-iterate convergence for other bandit algorithms (e.g., UCB, Thompson sampling) to assess its broader applicability.