---
ver: rpa2
title: 'Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient
  Decision Making'
arxiv_id: '2512.17091'
source_url: https://arxiv.org/abs/2512.17091
tags:
- mppi
- learning
- reward
- value
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework that integrates reinforcement
  learning with model-predictive control (MPC) through an adaptive hierarchical structure.
  The method uses RL to guide MPPI sampling and leverages MPPI-generated rollouts
  as structured virtual data to improve value function learning, with an adaptive
  influence ratio that balances real and virtual data based on uncertainty estimates
  from an ensemble of value functions.
---

# Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making

## Quick Facts
- arXiv ID: 2512.17091
- Source URL: https://arxiv.org/abs/2512.17091
- Reference count: 30
- Primary result: 2.1× faster convergence in racing, 72% increase in Lunar Lander success, 100% success in Acrobot through adaptive RL-MPC integration

## Executive Summary
This paper presents a novel framework that integrates reinforcement learning with model-predictive control (MPC) through an adaptive hierarchical structure. The method uses RL to guide MPPI sampling and leverages MPPI-generated rollouts as structured virtual data to improve value function learning, with an adaptive influence ratio that balances real and virtual data based on uncertainty estimates from an ensemble of value functions. This approach is shown to significantly improve sample efficiency and task success across three domains: Acrobot (100% success rate), Lunar Lander (72% increase in success rate), and racing (2.1× faster convergence), outperforming both standalone RL baselines and non-adaptive MPPI-RL combinations.

## Method Summary
The framework combines model-predictive path integral (MPPI) control with reinforcement learning in a hierarchical structure. MPPI generates rollouts from current state estimates, while an ensemble of value functions estimates uncertainty. The RL component guides MPPI sampling by biasing samples toward promising regions, and MPPI rollouts are treated as structured virtual data to improve value function learning. An adaptive influence ratio dynamically balances real and virtual data based on ensemble uncertainty estimates, allowing the system to leverage the strengths of both planning (MPPI) and learning (RL) while mitigating their individual limitations.

## Key Results
- Achieved 100% success rate on Acrobot task compared to non-adaptive methods
- Demonstrated 72% increase in success rate for Lunar Lander task
- Showed 2.1× faster convergence in racing domain
- Outperformed standalone RL baselines and non-adaptive MPPI-RL combinations across all tested domains

## Why This Works (Mechanism)
The integration works by addressing the fundamental limitations of both RL and MPC approaches. RL struggles with sample efficiency due to exploration challenges and credit assignment in high-dimensional spaces, while MPC requires accurate models and computational resources. By using RL to guide MPPI sampling, the framework focuses computational resources on promising regions of the state space. The virtual data generated from MPPI rollouts provides structured information that accelerates value function learning, while the adaptive influence ratio prevents overfitting to potentially inaccurate virtual data by balancing it with real experience based on uncertainty estimates.

## Foundational Learning
- **Model-predictive path integral (MPPI) control**: Why needed - provides model-based planning with uncertainty quantification; Quick check - verify MPPI generates diverse, feasible trajectories
- **Ensemble uncertainty estimation**: Why needed - enables adaptive balancing of real/virtual data; Quick check - ensure ensemble disagreement correlates with actual prediction error
- **Hierarchical decision making**: Why needed - separates planning from learning to leverage complementary strengths; Quick check - confirm clear information flow between planning and learning modules
- **Virtual data generation**: Why needed - provides structured experience to accelerate learning; Quick check - validate virtual data improves value function accuracy
- **Adaptive influence ratios**: Why needed - prevents overfitting to imperfect virtual data; Quick check - verify ratio adjusts appropriately to uncertainty levels
- **Sample-efficient RL**: Why needed - reduces real-world interaction requirements; Quick check - compare sample efficiency against standard RL baselines

## Architecture Onboarding

**Component map:** State estimator -> MPPI planner -> RL sampler -> Value function ensemble -> Adaptive influence controller

**Critical path:** Current state → MPPI sampling (guided by RL) → Rollouts → Value function updates (with virtual data) → Policy improvement

**Design tradeoffs:** The framework trades increased computational complexity (ensemble of value functions, MPPI rollouts) for improved sample efficiency and task success. The adaptive influence ratio introduces additional hyperparameters but enables robust learning across domains with varying model accuracy.

**Failure signatures:** Performance degradation occurs when: 1) ensemble uncertainty estimates are unreliable (e.g., in sparse reward settings), 2) MPPI rollouts are computationally expensive relative to learning gains, 3) value functions overfit to virtual data when uncertainty estimates are too optimistic, or 4) the adaptive ratio mechanism oscillates without converging.

**First 3 experiments to run:**
1. Ablation study removing adaptive influence ratio to quantify its contribution
2. Test with fixed influence ratio to compare against adaptive approach
3. Evaluate performance with different ensemble sizes to find optimal tradeoff

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance improvements are demonstrated primarily on relatively short-horizon tasks, raising questions about scalability to more complex, long-horizon problems
- The framework's computational overhead from MPPI rollouts and ensemble uncertainty estimation may limit real-time applicability
- Limited evaluation against state-of-the-art RL algorithms that incorporate planning or model-based components

## Confidence
- **High confidence**: The core integration of RL-guided MPPI sampling with virtual data generation is technically sound and well-implemented
- **Medium confidence**: The sample efficiency improvements are demonstrated but require independent validation across diverse benchmarks
- **Medium confidence**: The adaptive influence ratio mechanism shows promise but needs stress-testing in environments where ensemble uncertainty estimates may be unreliable

## Next Checks
1. Test the framework on more complex, high-dimensional environments (e.g., robotic manipulation with contact dynamics) to assess scalability limitations
2. Compare against state-of-the-art RL algorithms (e.g., PPO, SAC) that incorporate planning or model-based components to establish relative performance
3. Conduct ablation studies isolating the contributions of RL-guided sampling, virtual data generation, and adaptive influence ratio to quantify each component's impact on performance