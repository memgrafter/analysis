---
ver: rpa2
title: 'MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation'
arxiv_id: '2512.04386'
source_url: https://arxiv.org/abs/2512.04386
tags:
- mase
- saliency
- perturbation
- deep
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MASE (Model-Agnostic Saliency Estimation),
  a novel framework for interpreting deep NLP models through model-agnostic saliency
  estimation. The method addresses the challenge of interpreting NLP models by applying
  perturbations to the embedding layer rather than raw text, using Normalized Linear
  Gaussian Perturbations (NLGP) to efficiently estimate input saliency.
---

# MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation

## Quick Facts
- arXiv ID: 2512.04386
- Source URL: https://arxiv.org/abs/2512.04386
- Authors: Zhou Yang; Shunyan Luo; Jiazhen Zhu; Fang Jin
- Reference count: 40
- Primary result: Novel embedding-space perturbation method for interpretable NLP models with superior delta accuracy

## Executive Summary
This paper introduces MASE (Model-Agnostic Saliency Estimation), a framework for interpreting deep NLP models through model-agnostic saliency estimation. The key innovation is perturbing the embedding layer rather than raw text inputs, using Normalized Linear Gaussian Perturbations (NLGP) to efficiently estimate input saliency. MASE provides local explanations without requiring knowledge of the model's internal architecture.

The authors theoretically prove MASE achieves optimal performance in terms of infidelity measure and demonstrate it unifies existing interpretation methods like LIME, SHAP, Occlusion, and Integrated Gradients. Experimental results on LSTM and BERT models using IMDB and Reuters datasets show MASE consistently outperforms existing model-agnostic methods with delta accuracy values ranging from 0.052-0.409.

## Method Summary
MASE addresses the challenge of interpreting NLP models by applying perturbations to the embedding layer rather than raw text, using Normalized Linear Gaussian Perturbations (NLGP) to efficiently estimate input saliency. The method expands the perturbation space from binary to Euclidean, eliminating uncertainties from masking alternatives. MASE provides local explanations without requiring knowledge of the model's internal architecture, and the authors introduce "Delta Accuracy" as a quantitative evaluation metric. The framework theoretically achieves optimal performance in terms of the infidelity measure and unifies existing interpretation methods.

## Key Results
- MASE achieved delta accuracy values of 0.052-0.396 on BERT-IMDB and 0.058-0.409 on BERT-Reuters datasets
- Consistently outperformed existing model-agnostic methods (LIME, SHAP, PI, and Grad) across experiments
- Demonstrated superior and more stable performance compared to baselines while maintaining computational efficiency

## Why This Works (Mechanism)
MASE works by perturbing the embedding space instead of word-level inputs, which expands the perturbation space from binary to Euclidean and eliminates uncertainties from masking alternatives. This approach provides local explanations without requiring knowledge of the model's internal architecture. The Normalized Linear Gaussian Perturbations (NLGP) efficiently estimate input saliency by sampling in the continuous embedding space rather than the discrete word space.

## Foundational Learning

1. **Normalized Linear Gaussian Perturbations (NLGP)**: Gaussian sampling in normalized embedding space
   - Why needed: Enables efficient exploration of continuous embedding space for saliency estimation
   - Quick check: Verify perturbations maintain unit norm and follow Gaussian distribution

2. **Infidelity Measure**: Evaluation metric for saliency estimation quality
   - Why needed: Quantifies how well explanations approximate model behavior
   - Quick check: Ensure infidelity decreases with more perturbation samples

3. **Delta Accuracy**: Novel quantitative metric for interpretation quality
   - Why needed: Provides objective comparison between interpretation methods
   - Quick check: Validate that higher delta accuracy correlates with better explanations

## Architecture Onboarding

**Component Map**: Input Text -> Embedding Layer -> NLGP Perturbations -> Saliency Estimation -> Interpretation Output

**Critical Path**: Text embedding → NLGP perturbation generation → Model inference on perturbed embeddings → Saliency computation → Interpretation visualization

**Design Tradeoffs**: 
- Embedding-space perturbation vs. word-level masking (MASE provides richer perturbation space but requires embedding access)
- Gaussian vs. uniform sampling (Gaussian provides more natural exploration but requires variance tuning)
- Computational cost vs. explanation fidelity (more samples improve quality but increase runtime)

**Failure Signatures**: 
- Poor delta accuracy despite high computational cost (indicates inefficient perturbation strategy)
- Unstable saliency estimates across runs (suggests insufficient sampling or poor normalization)
- Negative delta accuracy values (indicates explanations are worse than random baseline)

**First 3 Experiments**:
1. Baseline comparison with LIME and SHAP on IMDB dataset using BERT model
2. Ablation study varying perturbation magnitude and sample count
3. Cross-task evaluation on sentiment classification vs. question answering

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the limitations section suggests several areas for future work including extending MASE to generation tasks and exploring its behavior on larger language models.

## Limitations

- Theoretical optimality proof relies on idealized assumptions that may not hold in practice
- Evaluation depends heavily on synthetic delta accuracy metrics that may not correlate with human judgment
- Limited ablation studies on different perturbation strategies and embedding dimensions
- Computational efficiency claims lack rigorous benchmarking across diverse hardware configurations

## Confidence

- Theoretical proof of optimality: Low confidence (proof assumptions may not generalize)
- Delta accuracy comparisons: Medium confidence (consistent results but synthetic metric concerns)
- Unification claim with existing methods: Low confidence (insufficient comparative analysis)
- Computational efficiency: Medium confidence (needs more rigorous benchmarking)

## Next Checks

1. Conduct human evaluation studies to validate whether delta accuracy improvements translate to better human understanding of model decisions
2. Perform extensive ablation studies testing different perturbation magnitudes, embedding layer depths, and perturbation strategies to identify optimal configurations
3. Test MASE across a broader range of NLP tasks (beyond sentiment classification) including question answering, named entity recognition, and generation tasks to assess generalizability