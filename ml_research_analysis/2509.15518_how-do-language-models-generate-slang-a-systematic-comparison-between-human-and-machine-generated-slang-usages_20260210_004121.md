---
ver: rpa2
title: 'How do Language Models Generate Slang: A Systematic Comparison between Human
  and Machine-Generated Slang Usages'
arxiv_id: '2509.15518'
source_url: https://arxiv.org/abs/2509.15518
tags:
- slang
- word
- usages
- definition
- usage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares human and machine-generated slang usages from
  large language models (LLMs) like GPT-4o and Llama-3 against human-attested slang
  from the Online Slang Dictionary. A dataset of over 58,000 machine-generated slang
  usages was created under controlled and uncontrolled generation settings.
---

# How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages

## Quick Facts
- arXiv ID: 2509.15518
- Source URL: https://arxiv.org/abs/2509.15518
- Authors: Siyang Wu; Zhewei Sun
- Reference count: 38
- The paper compares human and machine-generated slang usages from large language models (LLMs) like GPT-4o and Llama-3 against human-attested slang from the Online Slang Dictionary.

## Executive Summary
This study systematically compares human and machine-generated slang usages, creating a dataset of over 58,000 machine-generated slang examples. The research evaluates slang across three dimensions: usage characteristics, creativity (morphological complexity, semantic novelty), and informativeness for downstream tasks. Results show LLMs exhibit systematic biases in slang generation, producing more coinage than reuse, showing different word formation preferences, and demonstrating high creativity in novelty but lower morphological coherence compared to humans. While LLMs capture some slang knowledge, their patterns diverge from human usage, limiting their effectiveness for extrapolative linguistic analyses.

## Method Summary
The study uses GPT-4o and Llama-3-8B to generate slang under controlled and uncontrolled settings, producing 58,197 total slang usages. Generation uses temperature=1.2, top-p=0.95. Outputs are filtered through Wiktionary validation and SBERT-based deduplication (cosine similarity <0.8). Evaluation employs Morfessor for morphological segmentation, SBERT embeddings for semantic distance calculations, and LoRA fine-tuning for downstream task evaluation. The framework assesses characteristics (LDA topic modeling, word formation), creativity (morphological coherence, novelty, surprisal), and informativeness (knowledge distillation experiments).

## Key Results
- LLMs produce more coinage than reuse compared to human slang patterns
- GPT-4o favors morphologically complex compounds while Llama-3 generates simpler forms
- Machine-generated slang shows high semantic novelty but constrained creativity variance
- Human-generated slang sometimes transfers better to downstream tasks than machine-generated data

## Why This Works (Mechanism)

### Mechanism 1
LLMs exhibit systematic biases in morphological complexity when generating slang coinages, with model-specific preferences that diverge from human patterns. GPT-4o tends toward morphologically complex compounds (mean 2.634 segments in uncontrolled generation vs. 2.032 for humans), while Llama-3 generates simpler forms (mean 1.698). This reflects learned statistical patterns in how each model internally represents "slang-like" word formation.

### Mechanism 2
Semantic coherence between coined terms and their definitions is higher in machine-generated slang than human slang, but only when models operate without external constraints. GPT-4o in uncontrolled settings produces coined compounds where constituent word meanings more closely align with the slang definition (lower mean coherence distance 1.250 vs. 1.286 for humans). When constrained by human-provided definitions, coherence degrades (1.277), suggesting the model's internal "optimal" coinage patterns conflict with external semantic requirements.

### Mechanism 3
LLMs generate slang reuses with higher and more constrained semantic novelty than humans, clustering around a specific creativity level rather than spanning the full human creative spectrum. Both GPT-4o and Llama-3 achieve higher mean novelty scores (1.231 and 1.222 respectively) than humans (1.141), but with significantly lower standard deviation and IQR. This suggests LLMs learn to make consistently creative sense extensions but lack the human capacity for both mundane and highly creative reuses.

## Foundational Learning

- Concept: **Morphological Segmentation and Word Formation Processes**
  - Why needed here: The paper uses Morfessor to decompose coined slang terms and classify them as compounds, blends, or other formations. Understanding morphological analysis is essential to interpret Tables 3-4 and Figure 3.
  - Quick check question: Given the coined term "flombast" (blend of "flamboyant" + "bombast"), what morphological segments would a segmentation model identify, and how would you classify it?

- Concept: **Semantic Embedding Spaces and Distance Metrics**
  - Why needed here: Creativity metrics (novelty, coherence, surprisal) all rely on computing distances between embedding representations (SBERT embeddings for definitions, LLM hidden states for surprisal). Understanding these foundations is critical for reproducing and extending the evaluation framework.
  - Quick check question: If a slang term "cat" is used to mean "a person who pretends to be something they're not," how would you compute the novelty score given that the prototypical sense of "cat" is a feline animal?

- Concept: **Knowledge Distillation for NLP**
  - Why needed here: Section 4.3 uses Llama-3-8B as a student model fine-tuned on slang from GPT-4o and OSD to evaluate informativeness. Understanding the distillation paradigm is necessary to interpret why certain knowledge transfers and why transfer is task-sensitive.
  - Quick check question: If fine-tuning on GPT-4o generated slang improves morphological complexity in student outputs but does not improve interpretation accuracy, what does this suggest about what knowledge is being transferred vs. what knowledge humans possess that LLMs lack?

## Architecture Onboarding

- Component map:
  Slang Generation Pipeline: Input Conditions → LLM Generator (GPT-4o / Llama-3-8B) → Filtering & Classification → Evaluation Framework

- Critical path:
  1. **Data generation**: Uncontrolled generation with temperature=1.2, top-p=0.95 produces 765/1000 valid unique entries. This is the throughput bottleneck.
  2. **Sense clustering**: DBSCAN on 9,115 OSD entries yields 7,890 distinct sense clusters. Cluster quality directly impacts controlled generation coverage.
  3. **Coinage classification**: Morfessor segmentation + Wiktionary lookup determines whether analysis in Section 4.2.1 is possible. Requires all segments to have Wiktionary entries for compound classification.
  4. **Surprisal computation**: Uses Gemma-2-9b-Instruct as external judge (not GPT/Llama) to avoid bias. This requires separate model inference infrastructure.

- Design tradeoffs:
  - **Temperature 1.2 vs. Chen et al. (2024) randomness method**: Paper explicitly ablates and finds temperature sampling yields better diversity (Appendix A). Tradeoff is between diversity and semantic coherence.
  - **SBERT vs. larger embedding models**: Paper uses all-MiniLM-L6-v2 for efficiency (384 dimensions). Larger models would improve semantic distance precision but increase computation.
  - **Multiple-choice vs. generative evaluation**: Tasks 1-2 use 4-way classification (easy to evaluate) while Task 3 requires semantic similarity computation. Generative evaluation captures more nuance but introduces evaluation noise.
  - **Per-dataset vs. cross-dataset evaluation**: OSD tests intrinsic performance; OpenSub-Slang tests extrinsic generalization. Results differ (Table 7), suggesting overfitting to dictionary-style definitions.

- Failure signatures:
  - **Low unique entry rate in uncontrolled generation**: If temperature were too low, generation would produce fewer unique (word, sense) pairs. Solution: increase temperature or use nucleus sampling with higher top-p.
  - **Coinage/reuse classification mismatch**: If prompting for "coinage" but Wiktionary lookup finds the generated term, the entry is rejected. High rejection rates suggest model non-compliance or prompt ambiguity. Solution: strengthen prompt constraints or post-hoc re-classification.
  - **Surprisal scorer bias**: If the surprisal model belongs to the same family as the generator (e.g., using Llama to score Llama outputs), scores may be artificially low. Solution: always use an independent model family for surprisal computation.
  - **Fine-tuning convergence without performance gain**: If LoRA fine-tuning converges (loss decreases) but Task 1-2 accuracy doesn't improve, the model is learning surface patterns (definition style) rather than structural slang knowledge. Solution: increase training diversity or use contrastive learning objectives.

- First 3 experiments:
  1. **Reproduce morphological complexity difference**: Generate 1,000 slang coinages each from GPT-4o, Llama-3-8B, and a human baseline (OSD). Run Morfessor segmentation and compute mean segments per term. Expect: GPT-4o > Human > Llama-3. If this fails, check Morfessor model version and Wiktionary snapshot date.
  2. **Ablate temperature on diversity-coherence tradeoff**: Run uncontrolled generation with temperatures {0.8, 1.0, 1.2, 1.5} and measure (a) unique entry rate, (b) morphological coherence, (c) novelty score variance. Expect: higher temperature → more diversity but potentially lower coherence. This validates the paper's temperature=1.2 choice.
  3. **Cross-model distillation test**: Fine-tune Llama-3-8B on OSD, GPT-4o-generated, and mixed (50/50) slang. Evaluate on all three tasks. Expect: OSD wins on Task 3 (free-form generation), GPT-4o wins on morphological complexity transfer, mixed provides best overall performance if biases are complementary. This tests whether human and machine knowledge can be combined.

## Open Questions the Paper Calls Out

### Open Question 1
Do LLMs' systematic biases in slang generation generalize across languages and cultural contexts? The authors state: "Slang is inherently a cultural and multilingual phenomenon, thus evaluating how models handle slang in other languages/dialects remains an important direction for future work." This remains unresolved as the study was confined to English slang only, leaving open whether observed biases are language-agnostic or English-specific.

### Open Question 2
To what extent do alignment techniques (e.g., RLHF) cause LLMs' topical preferences to diverge from human slang usage? The authors hypothesize: "One hypothesis is the influence of alignment techniques (e.g., RLHF) prevents the models from producing outputs involving potentially offensive or controversial content and instead steers them toward neutral or positive expressions." The correlation between alignment and topical bias is observed but not causally established.

### Open Question 3
Can human-generated slang consistently outperform machine-generated slang as training data for downstream slang-related NLP tasks? Results showed mixed informativeness: human-generated slang led to better performance on free-form interpretation (Task 3), but differences were minimal on generation and interpretation tasks (Tasks 1-2), with the authors noting "the degree of improvement is task-sensitive and often constrained." The sample size was limited and only two evaluation benchmarks were used.

## Limitations
- Findings rely on specific evaluation datasets (OSD and OpenSub-Slang) that may not generalize to other slang domains or generation settings
- Morphological complexity analysis depends on Morfessor segmentation quality, which can vary based on training data and tokenization schemes
- Informativeness evaluation through knowledge distillation assumes performance differences reflect true knowledge transfer rather than dataset-specific artifacts

## Confidence
- **High Confidence**: The systematic differences in morphological complexity and creativity between LLMs and human-generated slang are well-supported by multiple evaluation metrics and statistical analysis
- **Medium Confidence**: The claims about informativeness for downstream tasks are moderately supported, though performance varies significantly across evaluation datasets
- **Low Confidence**: The mechanism explaining why controlled generation degrades semantic coherence in LLMs is speculative and not directly tested

## Next Checks
1. **Cross-domain validation**: Generate slang in different domains (e.g., technical jargon, youth slang) and evaluate whether the observed LLM biases persist across contexts
2. **Prompt sensitivity analysis**: Systematically vary generation prompts (temperature, constraints, instruction wording) to determine whether observed differences are prompt artifacts or model properties
3. **Alternative evaluation frameworks**: Compare SBERT-based novelty/coherence metrics against human judgments or alternative embedding models to validate the automated evaluation pipeline