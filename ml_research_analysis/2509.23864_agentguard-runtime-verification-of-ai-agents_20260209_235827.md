---
ver: rpa2
title: 'AgentGuard: Runtime Verification of AI Agents'
arxiv_id: '2509.23864'
source_url: https://arxiv.org/abs/2509.23864
tags:
- verification
- agent
- agentic
- systems
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentGuard introduces Dynamic Probabilistic Assurance for runtime
  verification of AI agents, shifting from static verification to continuous, quantitative
  guarantees. The framework observes raw agent I/O, abstracts it into formal events,
  and learns a Markov Decision Process (MDP) online to model emergent behavior.
---

# AgentGuard: Runtime Verification of AI Agents

## Quick Facts
- arXiv ID: 2509.23864
- Source URL: https://arxiv.org/abs/2509.23864
- Reference count: 27
- Primary result: Introduces Dynamic Probabilistic Assurance framework for continuous, quantitative runtime verification of AI agents using online MDP learning and probabilistic model checking

## Executive Summary
AgentGuard shifts AI agent verification from static, offline analysis to continuous runtime assurance through Dynamic Probabilistic Assurance. The framework observes raw agent I/O, abstracts it into formal state transitions, and learns an MDP online to model emergent behavior. Probabilistic model checking then verifies quantitative properties like success probability and expected completion time in real-time. Applied to a RepairAgent proof-of-concept, AgentGuard demonstrated learning of transition patterns (e.g., 75% probability of searching code base after hypothesizing) and enabled actionable runtime verification of success likelihood and resource usage.

## Method Summary
AgentGuard implements runtime verification through a three-layer architecture: a Trace Monitor instruments the agent to capture raw I/O and map it to formal state transition events (s → a → s'), an Online Model Learner maintains transition probabilities based on observed execution frequencies to build a learned MDP, and an AnalyzerThread periodically invokes Storm model checker via stormpy bindings to verify PCTL properties on the learned model. The system operates as a nonintrusive inspection layer that converts unstructured agent behavior into formal events suitable for probabilistic model checking, providing real-time quantitative assurances about success probability and expected completion time without requiring prior models of agent behavior.

## Key Results
- Successfully learned MDP transition probabilities from observed RepairAgent execution traces, identifying key behavioral patterns (e.g., 75% probability of searching code base after hypothesizing)
- Enabled real-time verification of quantitative properties including P_max (probability of success) and E_min (expected cycles to completion) through periodic Storm model checking
- Demonstrated feasibility of runtime verification for autonomous agents, showing that probabilistic assurances can be provided without prior behavioral models

## Why This Works (Mechanism)

### Mechanism 1: Trace Abstraction to Formal State Transitions
The Trace Monitor converts unstructured agent I/O into discrete state transitions by instrumenting the agent framework to capture raw events (LLM calls, tool invocations, observations) and mapping them to formal events of the form s → a → s', where states represent agent progress/context and actions correspond to tool invocations. This requires developers to manually define a meaningful discrete state space that captures agent behavior relevant to verification properties.

### Mechanism 2: Online MDP Learning from Execution Traces
The Online Model Learner maintains counts of observed (s, a, s') triples and updates the MDP transition function P(s'|s,a) incrementally as the agent executes. This builds a "digital twin" of agent behavior from execution frequencies, enabling probabilistic predictions without requiring prior models. The approach assumes agent behavior is sufficiently stationary that learned probabilities remain valid over relevant prediction horizons.

### Mechanism 3: Real-Time Probabilistic Model Checking
The AnalyzerThread converts the learned MDP to PRISM language and invokes Storm model checker via stormpy bindings to verify PCTL properties periodically. Properties like P_max=?[eventually success] and E_min=?[cycles until completion] are verified in real-time, providing quantitative assurances about agent behavior. The approach assumes the learned MDP is sufficiently accurate for the queried properties and that verification overhead is acceptable.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: MDPs provide the formal substrate for modeling agent behavior; understanding (S, A, P, R, γ) is essential to defining states, actions, and interpreting transition probabilities. Quick check: Given a sequence of observed transitions, can you estimate P(s'|s,a) from counts?

- **Probabilistic Computational Tree Logic (PCTL)**: PCTL expresses properties like "probability of success within N cycles" or "expected cost to completion" that are verified by Storm. Quick check: Write a PCTL property for "the maximum probability of reaching a failure state within 10 steps."

- **Runtime Verification vs. Offline Model Checking**: AgentGuard trades exhaustive guarantees for adaptive, observation-based assurance. Understanding this tradeoff is critical for setting expectations. Quick check: What properties can runtime verification prove vs. only monitor?

## Architecture Onboarding

- **Component map**: AgentGuardLogger (user-facing) -> AnalyzerThread (background) -> Storm model checker
- **Critical path**: 1) Define AMDP states/actions relevant to your agent (manual; requires domain knowledge) 2) Instrument agent code with guard.log_transition(state, action, next_state) calls at tool boundaries 3) Write PCTL properties in config.yaml 4) Run agent; AnalyzerThread learns MDP and periodically verifies properties 5) Monitor dashboard; configure threshold callbacks for interventions
- **Design tradeoffs**: Manual vs. automated state abstraction (current implementation requires manual definition; POMDP-based automation is future work), verification frequency vs. overhead (periodic full-model verification is simple but costly; incremental verification is not yet implemented), middleware inspection vs. in-line verification (nonintrusive observation preserves agent autonomy but may miss internal state not exposed via I/O)
- **Failure signatures**: Cold-start unreliability (early predictions have wide confidence intervals; treat early P_max estimates cautiously), state-space mismatch (if defined states don't capture task-relevant distinctions, transition probabilities will be uninformative), verification lag (high-frequency agent actions may outpace model-checking cycles; queue backpressure possible)
- **First 3 experiments**: 1) Toy agent validation: Instrument a simple agent (e.g., 3-state, 2-action MDP) with known ground-truth transition probabilities; verify that learned P(s'|s,a) converges within tolerance after N episodes 2) RepairAgent replication: Run the provided replication package on RepairAgent; log convergence of P_max[F Fix_Success] and compare against actual repair outcomes 3) Overhead profiling: Measure end-to-end latency added by AgentGuard (logging + MDP update + Storm verification) under varying trace lengths; identify break-even points for real-time feasibility

## Open Questions the Paper Calls Out

- Can AgentGuard be extended to verify Multi-Agent Systems (MAS) using stochastic game theory? The discussion states the system should be improved "by introducing notions of stochastic games into the framework and integrating frameworks such as PRISM-games to support analysis of MAS." This is unresolved because the current implementation focuses on individual agents and lacks the formal mechanisms to model the competitive or cooperative interactions inherent in multi-agent environments.

- How can the framework transition from manual state definition to automated state abstraction? The authors note that the "current approach relies on developers to manually define a discrete state space" and suggest future work explore "semi-automated or fully automated state abstraction" via POMDPs. This is unresolved because manual state definition creates a bottleneck and limits scalability, as it requires expert knowledge to map agent actions and contexts to formal states.

- Can incremental verification algorithms reduce the computational overhead of real-time model checking? The paper identifies that "periodic re-verification of the entire model" causes overhead and suggests future work should "incorporate incremental verification algorithms." This is unresolved because re-running the probabilistic model checker on the entire MDP after every update is computationally expensive and may hinder real-time assurance for complex agents.

## Limitations

- Manual state definition bottleneck: The framework requires developers to manually define discrete state spaces, creating scalability limitations for complex agents
- Model drift handling unproven: While mentioned as future work, the current implementation lacks explicit mechanisms for detecting when learned transition probabilities become stale
- Verification overhead concerns: Periodic full-model verification is computationally expensive for complex agents, though incremental verification remains unimplemented

## Confidence

- **High confidence**: The core mechanism of online MDP learning from execution traces and subsequent probabilistic model checking is theoretically sound and algorithmically straightforward
- **Medium confidence**: The quantitative results on RepairAgent (75% transition probability, P_max bounds) are plausible given the proof-of-concept nature, but lack comparative benchmarks or ablation studies
- **Low confidence**: Claims about handling emergent behavior and complex agent architectures are aspirational rather than demonstrated

## Next Checks

1. Implement and test explicit mechanisms for detecting when learned transition probabilities become stale (e.g., KL divergence thresholds, sliding window approaches) and measure false positive/negative rates across synthetic MDPs with controlled drift patterns

2. Compare manual state definitions against automated clustering approaches (e.g., PCA on observation embeddings, temporal abstraction techniques) on agents with 50+ observable states and quantify information loss versus state-space reduction

3. Replace periodic full-model verification with incremental algorithms that update only affected property results when transition probabilities change and benchmark latency improvements versus verification frequency across models of increasing complexity