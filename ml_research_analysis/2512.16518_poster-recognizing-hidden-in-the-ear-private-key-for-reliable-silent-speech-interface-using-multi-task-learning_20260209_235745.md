---
ver: rpa2
title: 'Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech
  Interface Using Multi-Task Learning'
arxiv_id: '2512.16518'
source_url: https://arxiv.org/abs/2512.16518
tags:
- silent
- user
- authentication
- speech
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HEar-ID uses commodity active noise-canceling earbuds to perform\
  \ silent speech interface (SSI) and user authentication simultaneously. It captures\
  \ both ultrasonic reflections (17.5\u201323 kHz) from the ear canal and low-frequency\
  \ whisper audio (0\u201311 kHz) in sliding 426-ms windows, extracts AR coefficients\
  \ for the ultrasonic channel and mel-spectrograms for the whisper channel, and processes\
  \ them through a shared encoder (TCN \u2192 Bi-GRU \u2192 MLP)."
---

# Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech Interface Using Multi-Task Learning

## Quick Facts
- arXiv ID: 2512.16518
- Source URL: https://arxiv.org/abs/2512.16518
- Reference count: 40
- HEear-ID achieves 90.25% Top-1 word accuracy and TPR of 81.76% with FPR of 3.2% using commodity earbuds

## Executive Summary
HEar-ID is a multi-task learning system that simultaneously performs silent speech interface (SSI) and user authentication using commodity active noise-canceling earbuds. The system captures ultrasonic reflections (17.5–23 kHz) and whisper audio (0–11 kHz) in 426-ms sliding windows, extracting AR coefficients and mel-spectrograms respectively. A shared encoder processes these modalities through TCN → Bi-GRU → MLP layers, with contrastive learning aligning genuine-user pairs while repelling impostor embeddings. The architecture branches into authentication (angular triplet loss) and SSI (CTC-based) heads, achieving robust performance even when recognition accuracy degrades.

## Method Summary
The system processes audio captured from active noise-canceling earbuds in 426-ms windows, extracting AR coefficients from ultrasonic reflections (17.5–23 kHz) and mel-spectrograms from whisper audio (0–11 kHz). These features feed into a shared encoder (TCN → Bi-GRU → MLP), with contrastive learning aligning genuine-user pairs while repelling impostor embeddings. The multi-task architecture includes an authentication head trained with angular triplet loss (thresholded by Youden's J) and a CTC-based SSI head. Evaluation involved 11 participants spelling 50 words across multiple sessions.

## Key Results
- 90.25% Top-1 word accuracy on 8 participants
- TPR of 81.76% with FPR of 3.2% in multi-user scenarios
- One participant achieved TPR of 99.9% with FPR of 3.4%
- Authentication remains robust even when recognition performance degrades

## Why This Works (Mechanism)
The system leverages the unique acoustic signature of an individual's ear canal captured through ultrasonic reflections combined with whisper audio. By using commodity ANC earbuds, it avoids specialized hardware while capturing both high-frequency reflections and low-frequency speech patterns. The multi-task learning framework allows the model to learn shared representations beneficial for both SSI and authentication simultaneously, while contrastive learning ensures embeddings are distinctive enough for reliable user discrimination.

## Foundational Learning
- **Contrastive Learning**: Why needed: To align genuine-user embeddings while repelling impostor embeddings; Quick check: Verify embedding distances between genuine vs. impostor pairs
- **Multi-Task Learning**: Why needed: To share representations between SSI and authentication for improved generalization; Quick check: Compare performance with single-task baselines
- **CTC Decoding**: Why needed: For sequence-to-sequence mapping in SSI without explicit alignment; Quick check: Validate CTC blank symbol handling
- **Angular Triplet Loss**: Why needed: To enforce angular margin between user embeddings for better discrimination; Quick check: Measure inter-user angular distances
- **Mel-spectrogram Extraction**: Why needed: To capture time-frequency patterns in whisper audio; Quick check: Validate frequency range coverage (0-11 kHz)
- **AR Coefficient Extraction**: Why needed: To model temporal dependencies in ultrasonic reflections; Quick check: Verify model order selection

## Architecture Onboarding
**Component Map**: ANC earbuds → Signal preprocessing → AR + Mel-spectrogram extraction → Shared encoder (TCN→Bi-GRU→MLP) → Contrastive alignment → Multi-task heads (Authentication + SSI)

**Critical Path**: Audio capture → Feature extraction → Shared encoder → Contrastive alignment → Authentication/SSI heads → Output

**Design Tradeoffs**: 426-ms window size balances temporal resolution with computational efficiency; Shared encoder reduces parameter count but may limit task-specific optimization; Commodity earbuds enable accessibility but limit frequency range compared to specialized hardware

**Failure Signatures**: High FPR indicates poor impostor discrimination; Low TPR suggests genuine-user embeddings aren't properly aligned; Degraded SSI accuracy may result from suboptimal window sizing or feature extraction

**3 First Experiments**:
1. Validate AR coefficient extraction by comparing with ground truth ultrasonic reflection patterns
2. Test contrastive learning effectiveness by measuring genuine vs. impostor embedding distances
3. Evaluate window size impact by comparing 426-ms with 300-ms and 600-ms variants

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (11 participants) and limited vocabulary (50 words) may not represent real-world scenarios
- Performance across different languages, accents, and diverse user populations remains unverified
- Environmental noise conditions beyond typical indoor settings were not extensively tested

## Confidence
**High Confidence**: Technical implementation details, feature extraction methods, and basic performance metrics
**Medium Confidence**: Robustness claims when recognition performance degrades and multi-user scenario results
**Low Confidence**: Real-world deployment feasibility, user comfort during extended use, and performance consistency across varied environmental conditions

## Next Checks
1. Conduct longitudinal study with 100+ participants over 6+ months to evaluate performance degradation and environmental robustness
2. Test system with expanded vocabulary (>1000 words) and multiple languages to assess generalization capabilities
3. Implement real-time prototype with multiple simultaneous users to verify claimed low false positive rates in practical multi-user scenarios