---
ver: rpa2
title: 'SAFER: Probing Safety in Reward Models with Sparse Autoencoder'
arxiv_id: '2507.00665'
source_url: https://arxiv.org/abs/2507.00665
tags:
- safety
- reward
- safer
- features
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAFER uses sparse autoencoders to interpret and improve reward
  models by uncovering interpretable safety-related features in model activations.
  By analyzing activation differences between chosen and rejected responses, it quantifies
  feature salience and guides targeted data poisoning and denoising.
---

# SAFER: Probing Safety in Reward Models with Sparse Autoencoder

## Quick Facts
- arXiv ID: 2507.00665
- Source URL: https://arxiv.org/abs/2507.00665
- Reference count: 37
- Primary result: SAFER uses sparse autoencoders to interpret and improve reward models by uncovering interpretable safety-related features in model activations.

## Executive Summary
SAFER presents a method for probing and manipulating safety alignment in reward models using sparse autoencoders (SAEs). The approach identifies interpretable safety-related features in model activations by analyzing differences between chosen and rejected responses. These features are then used to guide targeted data poisoning and denoising strategies that can significantly alter safety behavior with minimal data changes. The method demonstrates both the ability to degrade safety alignment and enhance it, while maintaining general chat performance.

## Method Summary
SAFER employs sparse autoencoders to decompose model activations into interpretable features, focusing on safety-relevant dimensions. By comparing activation patterns between chosen and rejected responses, the method quantifies feature salience and identifies safety-related components. This analysis enables targeted data poisoning to degrade safety alignment and targeted denoising to enhance it. The approach leverages SAE interpretability to make precise, minimal interventions in the training data that significantly impact reward model behavior.

## Key Results
- SAFER can significantly degrade safety alignment in reward models through targeted data poisoning with minimal data changes
- The method can enhance safety alignment through targeted denoising while preserving general chat performance
- SAE analysis reveals interpretable safety-related features that correlate with model decision-making between safe and unsafe responses

## Why This Works (Mechanism)
SAFER works by leveraging the sparse autoencoder's ability to decompose complex model activations into interpretable features. When trained on reward model activations, SAEs can identify dimensions that correlate with safety-related behaviors. By analyzing activation differences between chosen and rejected responses, SAFER quantifies which features contribute most to safety decisions. This mechanistic understanding enables targeted interventions in the training data that shift the reward model's behavior along these identified dimensions, either degrading or enhancing safety alignment as desired.

## Foundational Learning
1. **Sparse Autoencoders (SAEs)** - Neural networks that decompose activations into sparse, interpretable features; needed to identify safety-relevant dimensions in model activations; quick check: verify sparsity and interpretability of learned features
2. **Reward Model Activations** - Internal representations used by models to evaluate responses; needed as the substrate for SAE analysis; quick check: confirm activation patterns differ between safe and unsafe responses
3. **Feature Salience Quantification** - Methods to measure feature importance in decision-making; needed to identify which features drive safety behavior; quick check: validate salience correlates with safety outcomes
4. **Targeted Data Poisoning** - Strategic modification of training data to influence model behavior; needed to implement SAFER's degradation strategy; quick check: measure behavioral change relative to data modifications
5. **Safety Alignment Metrics** - Evaluation criteria for measuring safety compliance; needed to quantify SAFER's effectiveness; quick check: establish baseline safety performance before intervention
6. **Activation Difference Analysis** - Comparing internal states between different model outputs; needed to identify safety-relevant features; quick check: ensure differences are statistically significant

## Architecture Onboarding

Component Map:
Input Data -> Sparse Autoencoder Training -> Feature Activation Analysis -> Salience Quantification -> Data Poisoning/Denoising Strategy -> Modified Training Data -> Reward Model Behavior Change

Critical Path:
The critical path involves SAE training on reward model activations, followed by feature analysis to identify safety-relevant dimensions, then quantifying their salience to guide targeted data modifications that ultimately change reward model behavior.

Design Tradeoffs:
The approach trades off interpretability (through SAE decomposition) against potential loss of information in activation compression. Data poisoning requires careful balance between sufficient impact and minimal intervention. The method assumes SAE features capture safety-relevant dimensions, which may not always hold for complex safety concepts.

Failure Signatures:
Potential failures include SAEs identifying spurious safety features, targeted data modifications having unintended side effects, or the method failing to generalize across different safety domains. The approach may also be vulnerable to adversarial examples that exploit identified safety features.

Three First Experiments:
1. Verify SAE decomposition produces sparse, interpretable features from reward model activations
2. Confirm activation differences between safe and unsafe responses are statistically significant and correlate with known safety behaviors
3. Test targeted data poisoning on a small scale to validate directional effects on safety alignment

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertain whether identified SAE features are truly causal versus merely correlational with safety behavior
- Limited testing across diverse safety domains and adversarial scenarios
- Unclear whether the approach generalizes beyond the specific reward models and datasets tested

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SAFER can meaningfully manipulate reward model safety alignment | Medium |
| SAFER preserves general chat performance while modifying safety behavior | Medium |
| SAE features identified are interpretable safety-relevant dimensions | High |

## Next Checks
1. Test SAFER's effectiveness across a broader range of safety domains (e.g., misinformation, bias, harmful content) beyond the specific safety prompts used in the original experiments
2. Conduct adversarial testing to evaluate whether models manipulated with SAFER maintain safety improvements under attack or when faced with carefully crafted prompts
3. Perform ablation studies to determine which components of SAFER (feature identification, salience quantification, data poisoning strategy) contribute most to observed effects, and test performance with different SAE architectures or hyperparameters