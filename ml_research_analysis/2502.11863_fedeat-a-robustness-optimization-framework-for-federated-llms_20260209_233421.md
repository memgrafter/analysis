---
ver: rpa2
title: 'FedEAT: A Robustness Optimization Framework for Federated LLMs'
arxiv_id: '2502.11863'
source_url: https://arxiv.org/abs/2502.11863
tags:
- robustness
- llms
- federated
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robustness challenges in federated large language
  models (LLMs), including data heterogeneity, malicious clients, and adversarial
  attacks. The authors propose FedEAT, a framework that applies adversarial training
  in the embedding space of client LLMs combined with geometric median aggregation
  to enhance robustness.
---

# FedEAT: A Robustness Optimization Framework for Federated LLMs

## Quick Facts
- arXiv ID: 2502.11863
- Source URL: https://arxiv.org/abs/2502.11863
- Reference count: 20
- Proposes a framework combining adversarial training in embedding space with geometric median aggregation to improve federated LLM robustness

## Executive Summary
This paper addresses robustness challenges in federated large language models (LLMs), including data heterogeneity, malicious clients, and adversarial attacks. The authors propose FedEAT, a framework that applies adversarial training in the embedding space of client LLMs combined with geometric median aggregation to enhance robustness. The method generates adversarial examples by perturbing embedding vectors while constraining perturbations to maintain semantic consistency. Experiments using models like PHI-3-MINI and ZEPHYR-7B on tasks including SST2, QQP, MNLI, and QNLI show that FedEAT significantly improves robustness (lower attack success rates) while maintaining minimal performance loss on benign data. The approach demonstrates that both embedding-space adversarial training and geometric median aggregation independently contribute to robustness improvements.

## Method Summary
FedEAT operates by applying adversarial training directly to the embedding space of client LLMs during federated learning. The framework generates adversarial examples by perturbing embedding vectors while constraining these perturbations to preserve semantic meaning. After local training on these adversarial examples, clients upload their updated model parameters to a central server. Rather than using standard federated averaging, FedEAT employs geometric median aggregation to combine client updates, which is more robust to outliers from malicious or poorly performing clients. This combination of embedding-space adversarial training and robust aggregation aims to create federated LLMs that maintain performance on clean data while being significantly more resistant to various attack types.

## Key Results
- FedEAT significantly reduces attack success rates compared to baseline federated learning approaches
- The framework maintains minimal performance loss on benign data while improving robustness
- Both embedding-space adversarial training and geometric median aggregation independently contribute to robustness improvements
- Experiments demonstrate effectiveness across multiple models (PHI-3-MINI, ZEPHYR-7B) and diverse tasks (SST2, QQP, MNLI, QNLI)

## Why This Works (Mechanism)
FedEAT works by hardening the model at the embedding level where semantic information is processed. By generating adversarial examples that preserve semantic meaning while challenging the model's embedding representations, the framework forces clients to learn more robust feature representations. The geometric median aggregation further enhances this robustness by reducing the influence of outlier updates that could come from malicious clients or those with significantly different data distributions. This dual approach addresses both the data heterogeneity problem and the security vulnerabilities inherent in federated learning systems.

## Foundational Learning
- **Federated Learning**: Distributed machine learning where multiple clients collaboratively train a model without sharing raw data - needed to understand the baseline framework being improved
- **Adversarial Training**: Technique of training models on adversarial examples to improve robustness - needed to understand how FedEAT enhances model security
- **Geometric Median Aggregation**: Robust aggregation method that finds the point minimizing total distance to all input points - needed to understand how FedEAT handles malicious clients
- **Embedding Space**: Intermediate representation layer where semantic information is processed - needed to understand where FedEAT applies its adversarial training
- **Semantic Consistency Constraints**: Techniques to ensure adversarial perturbations don't change the meaning of input data - needed to understand how FedEAT maintains model utility

## Architecture Onboarding

**Component Map**: Client LLMs -> Embedding Perturbation -> Adversarial Training -> Model Update -> Geometric Median Aggregation -> Global Model

**Critical Path**: The core workflow involves (1) local adversarial training on embedding-perturbed examples at each client, (2) uploading model parameters to server, (3) geometric median aggregation of updates, and (4) broadcasting the updated global model back to clients.

**Design Tradeoffs**: FedEAT trades increased computational overhead from adversarial training against improved robustness. The embedding-space focus reduces the number of parameters that need perturbation compared to input-space attacks, but requires careful semantic constraint implementation. Geometric median aggregation provides robustness to outliers but may converge slower than standard averaging.

**Failure Signatures**: Potential failure modes include: (1) excessive semantic constraints causing underfitting to adversarial examples, (2) geometric median getting stuck in suboptimal solutions with many outliers, (3) communication bottlenecks from increased local computation, and (4) reduced performance on rare but legitimate data distributions.

**First Experiments**: 1) Compare FedEAT's geometric median vs. federated averaging under varying proportions of malicious clients; 2) Measure impact of different semantic constraint strengths on the tradeoff between robustness and clean-data performance; 3) Evaluate convergence speed and communication costs compared to standard federated learning with adversarial training.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on embedding-space perturbations while potentially overlooking other attack vectors such as model poisoning, backdoor attacks, or inference-time evasion
- Does not extensively discuss computational overhead in federated settings, particularly the impact of embedding-space adversarial training on communication costs and convergence speed across heterogeneous devices
- Evaluation datasets, while diverse, may not fully represent real-world distribution shifts and malicious client behaviors that could emerge in large-scale deployments

## Confidence
High confidence that FedEAT significantly improves robustness while maintaining minimal performance loss on benign data, based on reported experimental results. Medium confidence in long-term generalization under evolving adversarial strategies. High confidence that both embedding-space adversarial training and geometric median aggregation independently contribute to robustness improvements, supported by ablation studies, though relative importance under different attack scenarios could benefit from further investigation.

## Next Checks
1. Evaluate FedEAT against a broader range of adversarial attacks including model poisoning and backdoor injection to assess robustness beyond embedding-space perturbations
2. Conduct large-scale federated simulations with heterogeneous client populations to measure communication overhead and convergence behavior under realistic bandwidth and device constraints
3. Test FedEAT's performance on out-of-distribution datasets and continuously evolving data streams to validate generalization beyond the evaluated benchmark tasks