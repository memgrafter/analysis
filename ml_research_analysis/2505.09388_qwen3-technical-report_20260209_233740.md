---
ver: rpa2
title: Qwen3 Technical Report
arxiv_id: '2505.09388'
source_url: https://arxiv.org/abs/2505.09388
tags:
- thinking
- qwen2
- mode
- qwen3
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen3 is a series of open-weight large language models (LLMs) with
  dense and Mixture-of-Expert (MoE) architectures ranging from 0.6 to 235 billion
  parameters. The flagship model, Qwen3-235B-A22B, integrates thinking and non-thinking
  modes in a unified framework with a thinking budget mechanism for adaptive resource
  allocation.
---

# Qwen3 Technical Report

## Quick Facts
- **arXiv ID**: 2505.09388
- **Source URL**: https://arxiv.org/abs/2505.09388
- **Reference count**: 9
- **Primary result**: Qwen3-235B-A22B achieves 85.7 on AIME'24 and 81.5 on AIME'25 in thinking mode, surpassing DeepSeek-V3 and competing with GPT-4o/o1

## Executive Summary
Qwen3 is a series of open-weight large language models ranging from 0.6B to 235B parameters with both dense and MoE architectures. The flagship model integrates thinking (reasoning) and non-thinking modes in a unified framework with adaptive resource allocation via a thinking budget mechanism. Trained on 36 trillion tokens across 119 languages using a three-stage pre-training process followed by four-stage post-training including reasoning RL and distillation, Qwen3 achieves state-of-the-art performance on math and coding benchmarks while maintaining competitive general capabilities.

## Method Summary
Qwen3 employs a three-stage pre-training process (S1: general corpus at 4K context, S2: reasoning/data refinement at 4K, S3: long-context at 32K) followed by four-stage post-training (Long-CoT cold start, Reasoning RL with GRPO, Thinking Mode Fusion via SFT, General RL). The models use GQA attention, SwiGLU activation, RoPE position encoding, and RMSNorm. The flagship 235B parameter MoE model activates 22B parameters with 128 total experts and 8 activated per token. Strong-to-weak distillation from larger teacher models provides 10x training efficiency improvement over RL alone. Chat templates with `/think` and `/no_think` flags enable dynamic mode switching, with emergent budget control allowing partial reasoning outputs.

## Key Results
- Qwen3-235B-A22B achieves 85.7 on AIME'24 and 81.5 on AIME'25 in thinking mode
- Model scores 40.1 on AIME'24 in non-thinking mode, demonstrating significant reasoning capability
- Qwen3-235B-A22B reaches 70.7 on LiveCodeBench v5 and 2056 rating on CodeForces
- Outperforms DeepSeek-V3 and competes with GPT-4o/o1 on reasoning benchmarks
- Strong-to-weak distillation achieves equivalent performance to RL with 1/10th the GPU hours

## Why This Works (Mechanism)

### Mechanism 1: Unified Mode Switching via Chat Templates
A single model dynamically alternates between rapid responses and deep reasoning without architectural changes by conditioning on specific control tokens. The model is fine-tuned on data containing `[/think]` and `[/no_think]` flags, triggering or suppressing internal thought trace generation during inference. This assumes sufficient capacity to represent both heuristics and deliberative algorithms without catastrophic interference. Evidence includes abstract claims of dynamic mode switching, section 4.3's chat template design, and corpus evidence from EXAONE 4.0 and KAT-V1 supporting unified architectures. Break condition occurs if mode confusion arises or simple queries fill context with unnecessary reasoning traces.

### Mechanism 2: Strong-to-Weak On-Policy Distillation
Small models acquire complex reasoning more efficiently by mimicking larger teacher model probability distributions than through RL alone. The student model aligns its logits with the teacher's outputs on responses the student generated (on-policy), providing dense gradients that guide reasoning paths. This assumes the teacher's distribution contains transferrable reasoning patterns. Evidence includes section 4.5's claim that distillation significantly outperforms RL in performance and efficiency, Table 21 showing distillation matches RL with 1/10th GPU hours, though corpus evidence for the specific efficiency gain is weak. Break condition occurs if the student collapses to lower performance than RL would achieve or merely memorizes specific reasoning paths.

### Mechanism 3: Emergent Thinking Budget Control
Models trained on full reasoning traces can be interrupted to produce valid partial answers, enabling latency-accuracy trade-offs. When a user-defined token limit is reached, a stop instruction is injected, and the model pivots from reasoning to summary generation based on partial context. This assumes the reasoning process is cumulative enough that incomplete traces yield high-probability correct answers rather than hallucinations. Evidence includes section 4.3's note that this ability emerges naturally from Thinking Mode Fusion and Figure 2's smooth performance scaling with allocated budget. Break condition occurs if early budget cuts cause incoherent or severely degraded responses.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Routing**
  - **Why needed here**: Flagship Qwen3 uses MoE architecture (235B total, 22B active); understanding expert selection is critical for managing inference costs
  - **Quick check question**: If you see high variance in expert load during pre-training, which loss function (specifically mentioned in the text) should you check?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: "Reasoning RL" stage uses GRPO to develop reasoning; critical for debugging the 4-stage post-training pipeline
  - **Quick check question**: Does GRPO require a separate value function model (critic), or does it estimate baselines differently?

- **Concept: Logit Distillation (KD)**
  - **Why needed here**: Primary driver for "Strong-to-Weak" mechanism
  - **Quick check question**: In on-policy distillation, are you minimizing KL divergence between the student's logits and the ground truth or the teacher's logits?

## Architecture Onboarding

- **Component map**: Tokenizer (BBPE, 151,669 vocab) -> Attention (GQA + QK-Norm) -> MoE (128 experts/8 activated) -> Control (Chat Template Parser)
- **Critical path**: Data Curation (36T tokens) -> Pre-training (S1→S2→S3) -> Post-training (Long-CoT→RL→Fusion→General RL)
- **Design tradeoffs**: Optimizing for general instruction following can degrade specialized reasoning performance; removing shared experts simplifies architecture but requires stronger load-balancing; thinking mode offers superior accuracy but higher latency
- **Failure signatures**: Mode bleeding (empty thought blocks in thinking mode or verbose reasoning in non-thinking mode), performance drop in Stage 4 on reasoning tasks, long-context degradation in thinking mode for retrieval tasks
- **First 3 experiments**: 1) Mode Isolation Test: Run identical queries with `/think` vs `/no_think` flags to verify distinct latency and output structures; 2) Budget Scaling Curve: Measure accuracy on AIME subset while varying thinking budget (1k→4k→8k tokens); 3) Expert Load Balancing: Visualize expert activation frequency to ensure "Global-batch load balancing loss" prevents expert collapse

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does extending the thinking budget beyond 32K tokens continue to scale performance?
- **Basis in paper**: Section 4.7 observes performance expected to improve further beyond 32K and explicitly leaves this as future work
- **Why unresolved**: Current experiments capped at 32,768 (or 38,912 for AIME), limiting observation of scaling laws at extreme lengths
- **What evidence would resolve it**: Benchmark scores (AIME, LiveCodeBench) with output limits increased to 64K or 128K tokens

### Open Question 2
- **Question**: How can the interference of reasoning tokens with retrieval accuracy in thinking mode be mitigated?
- **Basis in paper**: Appendix A.1.1 notes performance degradation in thinking mode for long-context retrieval tasks (RULER) and commits to enhancing this in future versions
- **Why unresolved**: Authors hypothesize thinking content provides no benefit for retrieval and may interfere, but offer no solution
- **What evidence would resolve it**: Architectural or training modifications allowing thinking mode to match or exceed non-thinking mode performance on RULER benchmark at 128K context

### Open Question 3
- **Question**: Can specialized reasoning capabilities be fully preserved during Thinking Mode Fusion and General RL stages?
- **Basis in paper**: Section 4.7 reports trade-off where general versatility degrades specialized benchmark scores during Stages 3 and 4
- **Why unresolved**: Authors accept this performance drop as necessary compromise, leaving prevention of this "forgetting" unaddressed
- **What evidence would resolve it**: Post-training run where Stage 4 models match Stage 2 specialized reasoning performance while maintaining general instruction-following skills

## Limitations
- Technical report lacks critical implementation details including exact learning rate schedules, GRPO hyperparameters, and precise data mixture ratios
- Claimed 10x training efficiency improvement from strong-to-weak distillation supported only by single table without methodological transparency
- Emergent thinking budget control mechanism described as "naturally emerging" with minimal empirical validation beyond smooth scaling curves

## Confidence
- **Unified mode architecture with chat templates**: High - Well-documented with clear implementation details and benchmark validation
- **Strong-to-weak distillation efficiency claims**: Medium - Supported by single table but lacks methodological transparency
- **Emergent thinking budget control**: Low - Described as emergent without rigorous ablation studies or failure analysis

## Next Checks
1. **Mode isolation verification**: Run controlled experiments with identical prompts using `/think` vs `/no_think` flags to measure actual latency differences and confirm distinct output structures
2. **Budget scaling replication**: Independently reproduce Figure 2 by measuring accuracy degradation on standardized reasoning tasks while systematically varying thinking budget from 1K to 8K tokens
3. **Distillation efficiency audit**: Recreate off-policy and on-policy distillation experiments to verify claimed 10x GPU hour reduction compared to RL baselines, documenting all training configurations used