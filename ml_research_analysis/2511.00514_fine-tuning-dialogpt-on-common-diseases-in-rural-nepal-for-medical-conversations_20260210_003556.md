---
ver: rpa2
title: Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations
arxiv_id: '2511.00514'
source_url: https://arxiv.org/abs/2511.00514
tags:
- medical
- dialogue
- rural
- conversational
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study fine-tuned DialoGPT on a synthetic dataset of doctor-patient\
  \ dialogues for ten common rural Nepalese diseases to enable offline medical conversational\
  \ AI. The fine-tuned model achieved a perplexity of 5.96 and produced contextually\
  \ relevant, empathetic responses, as confirmed by human evaluation scoring 3.8\u2013\
  4.1 out of 5 on medical appropriateness, empathy, and relevance."
---

# Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations

## Quick Facts
- arXiv ID: 2511.00514
- Source URL: https://arxiv.org/abs/2511.00514
- Reference count: 21
- Fine-tuned DialoGPT-medium achieved perplexity of 5.96 and human evaluation scores of 3.8-4.1/5 on medical appropriateness, empathy, and relevance for rural Nepalese disease dialogues

## Executive Summary
This study demonstrates that fine-tuning DialoGPT-medium on a synthetic dataset of 1000 doctor-patient dialogues for ten common rural Nepalese diseases enables effective offline medical conversational AI. The approach addresses the critical challenge of providing medical guidance in low-connectivity rural areas where internet access is limited. Despite training on a limited dataset, the model produced coherent, contextually relevant, and medically appropriate responses as confirmed by healthcare professional evaluation, achieving a perplexity of 5.96 and strong human assessment scores across medical appropriateness, empathy, and relevance dimensions.

## Method Summary
The researchers fine-tuned DialoGPT-medium using Hugging Face Transformers on 800 synthetic doctor-patient dialogues (80% of 1000 total) covering ten common rural Nepalese diseases. The synthetic data was generated using Gemini 2.5 Pro, validated by Claude 4 Sonnet for medical validity, and reviewed by medical professionals. Training employed causal language modeling with cross-entropy loss over 3 epochs using Adam optimizer (learning rate 5e-5, batch size 4) on Google Colab T4 GPU. Evaluation included perplexity calculation, token-level metrics, and human assessment by healthcare professionals on 5-point Likert scales for medical appropriateness, empathy/tone, and contextual relevance.

## Key Results
- Achieved perplexity of 5.96 on validation set, indicating strong fluency and domain adaptation
- Human evaluation scores ranged from 3.8-4.1 out of 5 across medical appropriateness, empathy, and contextual relevance
- Model produced medically coherent responses for fever symptoms with appropriate advice about rest, hydration, and medication
- System operates fully offline without internet connectivity, addressing rural healthcare access challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pre-trained conversational model on domain-specific synthetic dialogues can yield contextually appropriate medical responses even with limited training data.
- Mechanism: DialoGPT's pre-training on 147M Reddit exchanges provides general conversational fluency; fine-tuning on 800 medical dialogues adapts token prediction probabilities toward disease-specific symptom-advice patterns.
- Core assumption: The synthetic dialogues accurately represent real patient symptom descriptions and appropriate medical responses.
- Evidence anchors: Abstract states model produced coherent, contextually relevant, and medically appropriate responses despite limited training data; perplexity of 5.96 and human evaluation scores of 3.8-4.1/5 indicate successful adaptation.

### Mechanism 2
- Claim: Synthetic data generation using LLMs with cross-validation enables dataset creation where annotated corpora are unavailable.
- Mechanism: Gemini 2.5 Pro generates candidate dialogues; Claude 4 Sonnet filters for medical validity; human professionals provide final review. This pipeline reduces hallucination propagation while scaling data creation.
- Core assumption: LLM-generated medical content, when cross-validated, can substitute for expert-annotated data in limited scopes.
- Evidence anchors: Pipeline description shows generation by Gemini 2.5 Pro, validation by Claude 4 Sonnet, and final review by medical professionals; 1000 dialogues covering 10 diseases with 80/20 train-validation split.

### Mechanism 3
- Claim: Lightweight transformer models can support offline healthcare dialogue in low-connectivity environments.
- Mechanism: DialoGPT-medium runs locally without API calls; all components (tokenizer, model weights, inference) are self-contained. This removes cloud dependency.
- Core assumption: Target hardware in rural settings can meet minimum compute requirements for DialoGPT-medium inference.
- Evidence anchors: Abstract emphasizes lightweight, offline-capable dialogue models can effectively support rural healthcare delivery; section confirms system operates fully offline with local hosting.

## Foundational Learning

- Concept: **Causal Language Modeling (CLM)**
  - Why needed here: DialoGPT is trained to predict next tokens autoregressively; understanding CLM explains why perplexity measures fluency.
  - Quick check question: Given input "The patient has fever and," what token would a CLM model assign highest probability to?

- Concept: **Transfer Learning in NLP**
  - Why needed here: The entire approach depends on pre-trained DialoGPT adapting to medical domain via fine-tuning rather than training from scratch.
  - Quick check question: Why does fine-tuning require fewer samples than pre-training a similar model?

- Concept: **Synthetic Data Validation Pipelines**
  - Why needed here: Data quality hinges on LLM-to-LLM cross-checking; understanding validation bottlenecks helps diagnose data issues.
  - Quick check question: What failure mode occurs if the generator and validator models share the same training distribution biases?

## Architecture Onboarding

- Component map: User chat interface -> Text input -> Pre-trained DialoGPT tokenizer -> Token IDs -> Context buffer (chat history concatenation) -> Fine-tuned DialoGPT-medium (local) -> Token sampling -> Decoding -> Response display

- Critical path: User query -> Tokenizer encoding -> History concatenation -> Model forward pass -> Token sampling -> Decoding -> Display. Latency bottleneck is model inference on local hardware.

- Design tradeoffs: DialoGPT-medium vs. larger models: Medium chosen for offline feasibility; larger models would improve quality but require connectivity/cloud. Synthetic vs. real data: Synthetic enables rapid creation but may lack linguistic diversity of real patient expressions. 1000 dialogues vs. larger corpus: Small dataset limits robustness but reduces annotation burden.

- Failure signatures: High perplexity (>15) with coherent outputs: Possible overfitting to validation set patterns. Low token-level accuracy but high human scores: Normal for generative dialogue—exact token match is poor proxy for response quality. Repetitive or generic responses: Insufficient symptom diversity in training dialogues.

- First 3 experiments:
  1. **Baseline comparison**: Run unmodified DialoGPT-medium on same evaluation prompts; compare perplexity and human scores to quantify fine-tuning impact.
  2. **Data scaling test**: Generate additional 500-1000 dialogues with varied symptom phrasings; measure whether perplexity and human scores improve.
  3. **Safety filter integration**: Implement rule-based or classifier-based detection for unsafe advice triggers (e.g., "stop medication," "ignore symptoms"); evaluate false positive/negative rates on held-out dialogues.

## Open Questions the Paper Calls Out
The paper explicitly calls for field testing and deployment in real rural settings to collect user feedback, refine dialogue strategies, and assess the system's impact on healthcare outcomes. It also suggests extending the approach to other low-resource languages and healthcare domains, implementing continuous learning from user interactions, and developing more sophisticated safety mechanisms for edge cases.

## Limitations
- Synthetic dataset generation introduces uncertainty about real-world applicability due to potential divergence from actual rural patient language patterns
- Evaluation relies heavily on subjective human judgment rather than clinical outcomes or longitudinal patient follow-up
- Offline deployment assumption lacks validation of actual hardware constraints in rural settings
- Safety evaluation appears limited to a single case study rather than systematic testing across all ten diseases

## Confidence
- **High confidence**: Technical implementation (fine-tuning methodology, perplexity calculation, evaluation framework) based on standard practices and reproducible methodology
- **Medium confidence**: Domain adaptation claim—while perplexity and human scores are positive, synthetic training data creates uncertainty about real-world generalization
- **Low confidence**: Safety assertions beyond the fever case study, as systematic safety evaluation across all disease categories and edge cases was not demonstrated

## Next Checks
1. Deploy the fine-tuned model with actual rural healthcare workers in Nepal to collect real patient-doctor conversations, then compare model responses against these authentic dialogues to identify linguistic and contextual gaps.

2. Conduct a controlled clinical pilot where the model's advice is evaluated by medical professionals against actual patient outcomes over 3-6 months, measuring whether suggested interventions align with appropriate care pathways.

3. Test the offline deployment on representative low-resource hardware (e.g., Raspberry Pi, low-end smartphones) to validate computational feasibility and measure latency impacts on conversational flow in rural settings.