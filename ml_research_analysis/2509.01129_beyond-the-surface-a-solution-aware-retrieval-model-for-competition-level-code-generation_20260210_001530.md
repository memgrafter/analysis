---
ver: rpa2
title: 'Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level
  Code Generation'
arxiv_id: '2509.01129'
source_url: https://arxiv.org/abs/2509.01129
tags:
- problems
- code
- problem
- solverank
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SolveRank, a solution-aware retrieval model
  designed to enhance code generation performance on competitive programming problems.
  Unlike existing retrieval methods that focus on surface-level semantic similarity,
  SolveRank uses contrastive learning to identify problems with logically equivalent
  solutions, even when problem statements differ in wording or narrative context.
---

# Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level Code Generation

## Quick Facts
- **arXiv ID**: 2509.01129
- **Source URL**: https://arxiv.org/abs/2509.01129
- **Reference count**: 6
- **Primary result**: SolveRank achieves 40.6% MRR improvement and boosts code generation pass@1 rates by up to 20% on difficult competitive programming problems.

## Executive Summary
This paper introduces SolveRank, a solution-aware retrieval model designed to enhance code generation performance on competitive programming problems. Unlike existing retrieval methods that focus on surface-level semantic similarity, SolveRank uses contrastive learning to identify problems with logically equivalent solutions, even when problem statements differ in wording or narrative context. To train the model, synthetic problems are generated using DeepSeek-R1 and verified for logical equivalence by GPT-4o, with these serving as positive samples and BM25/random retrievals as negatives. Experiments on the xCodeEval dataset show that SolveRank significantly outperforms state-of-the-art retrieval methods, achieving 40.6% MRR improvement, and boosts code generation pass@1 rates by up to 20% on difficult problems. The method proves especially beneficial when problems require deep understanding of underlying solutions rather than simple semantic matching.

## Method Summary
SolveRank is a solution-aware retrieval model that uses contrastive learning to identify problems with logically equivalent solutions regardless of surface narrative differences. The method generates synthetic problems using DeepSeek-R1, which rewrites problems while preserving core algorithmic logic, then verifies logical equivalence using GPT-4o. These verified synthetic pairs serve as positive training samples, while BM25-retrieved and random samples serve as negatives. The model uses a DPR bi-encoder architecture trained with InfoNCE loss, where query and passage encoders are optimized to pull positive pairs together and push negatives apart in embedding space. During inference, the retriever finds top-K relevant problem-code pairs which are concatenated with the original prompt and fed to a downstream LLM (GPT-3.5/4o) for code generation.

## Key Results
- SolveRank achieves 40.6% MRR improvement over state-of-the-art retrieval methods on xCodeEval dataset
- Improves pass@1 on difficult problems (>2000 difficulty) by ~20% compared to no retrieval baseline
- Demonstrates that retrieval helps hard problems but can hurt performance on easy problems due to distraction
- Shows effectiveness of hard negative mining using BM25-retrieved samples that share lexical overlap but differ in solution logic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic data generation decouples solution logic from surface narrative
- **Mechanism:** DeepSeek-R1 generates 5 variants per problem with different backgrounds but identical solutions, training the retriever to weight algorithmic structure over token overlap
- **Core assumption:** DeepSeek-R1 can successfully extract and re-wrap core algorithmic logic, and GPT-4o accurately verifies logical equivalence
- **Evidence anchors:** "We leverage the DeepSeek-R1 model to generate logically equivalent but differently phrased new problems" [abstract]; "The prompt is crafted to preserve the original solution logic while encouraging diversity" [section 2.2]
- **Break condition:** If DeepSeek-R1 hallucinates logic changes that GPT-4o misses, the contrastive signal introduces noise and degrades retrieval precision

### Mechanism 2
- **Claim:** Hard negative mining penalizes surface-level distraction
- **Mechanism:** BM25 results (high lexical overlap but often logically irrelevant) are used as explicit negatives in InfoNCE loss, forcing the embedding space to separate problems that look similar textually but require different algorithms
- **Core assumption:** BM25 retrievals are frequently "false positives" in terms of solution logic, providing useful contrastive signal
- **Evidence anchors:** "train SolveRank... with these as positive samples and BM25/random-retrieved problems as negatives" [abstract]; "BM25 focuses on lexical overlap, leading to irrelevant results" [section 3.2]
- **Break condition:** If corpus is small or sparse such that BM25 retrieves the only logically relevant examples, using them as negatives would degrade performance

### Mechanism 3
- **Claim:** Retrieval utility is conditional on problem difficulty
- **Mechanism:** For easy problems, LLMs have sufficient internalized knowledge; retrieval adds noise. For hard problems, LLM lacks specific algorithmic insight; retrieval provides necessary scaffolding
- **Core assumption:** Downstream generator can adapt retrieved solution to new context through analogical reasoning
- **Evidence anchors:** "improving... pass@1 on difficult problems by ~20%" [abstract]; "For easy problems... all methods perform worse [than No Retrieval]" [section 3.2]
- **Break condition:** If retrieved reference is too complex or applies valid but distinct algorithmic paradigm, it may mislead generator on medium-difficulty tasks

## Foundational Learning

- **Concept:** Contrastive Learning (InfoNCE Loss)
  - **Why needed here:** SolveRank is trained via relative distances rather than supervised labels, requiring understanding of how pulling positives closer and pushing negatives apart shapes vector space
  - **Quick check question:** If temperature τ in InfoNCE loss approaches zero, how does gradient behavior change regarding hard negatives?

- **Concept:** Dense Passage Retrieval (DPR)
  - **Why needed here:** SolveRank uses DPR as base architecture (twin encoders), understanding independent query/passage embedding computation vs cross-attention is vital for inference speed optimization
  - **Quick check question:** Why might bi-encoder (DPR) be preferred over cross-encoder for retrieval over massive corpus of 100k+ problems?

- **Concept:** Semantic vs. Solution-Level Similarity
  - **Why needed here:** Core problem definition - surface similarity (shared words) must be distinguished from solution similarity (shared algorithms)
  - **Quick check question:** Given two problems about "candy boxes" and "book piles" with identical constraints, would BM25 likely rank them highly? Why or why not?

## Architecture Onboarding

- **Component map:** DeepSeek-R1 (Generator) -> GPT-4o (Verifier) -> Synthetic Pairs -> DPR Architecture (Query Encoder E_Q, Passage Encoder E_P) -> Downstream LLM (GPT-3.5/4o)

- **Critical path:** The Verification Step. If GPT-4o allows non-equivalent problem to pass as "positive," contrastive loss optimization corrupts embedding space, leading to logical hallucinations in retrieval

- **Design tradeoffs:**
  - **Synthetic vs. Human Labels:** Synthetic data scales but introduces model bias/hallucination risk
  - **Noise vs. Signal:** Adding retrieval helps hard problems but hurts easy ones; difficulty classifier needed before retrieval step to maximize utility (skipping retrieval for easy tasks)

- **Failure signatures:**
  - **Performance Drop on Medium Tasks:** SolveRank sometimes underperforms semantic retrievers on medium tasks when retrieved logic is structurally similar but contextually misaligned (e.g., 1D vs 2D state spaces)
  - **Easy Task Distraction:** If Pass@1 drops below "No Retrieval" baseline, retriever is likely injecting noise

- **First 3 experiments:**
  1. **Negative Ablation:** Retrain SolveRank using only random negatives (removing BM25 negatives) to quantify value of hard negative mining vs random sampling
  2. **Verifier Sensitivity:** Manually inspect 50 synthetic pairs flagged as "equivalent" by GPT-4o to estimate false-positive rate in training data
  3. **Difficulty Thresholding:** Run inference on test set, selectively disable retrieval for problems with difficulty < 1400, compare aggregate Pass@1 against "always retrieve" baseline

## Open Questions the Paper Calls Out

- **Question:** How can reinforcement learning (RL) be integrated to further improve ranking performance of solution-aware retrievers?
  - **Basis in paper:** "Future work will explore the reinforcement learning for ranking improvement" [explicit conclusion]
  - **Why unresolved:** Current implementation relies on contrastive learning with InfoNCE loss; authors suggest RL as distinct, unexplored avenue for optimization
  - **What evidence would resolve it:** Study comparing current contrastive learning approach against RL-based objective using reward signals based on retrieval success or downstream pass rates

- **Question:** Can the SolveRank framework generalize to broader code generation domains beyond competitive programming?
  - **Basis in paper:** "The generalizability of our framework to broader code generation domains, such as software engineering tasks or multi-language corpora, remains to be validated in future" [explicit Limitations]
  - **Why unresolved:** Experiments restricted to xCodeEval benchmark focusing on competitive programming tasks involving algorithmic logic
  - **What evidence would resolve it:** Evaluation on software engineering benchmarks (HumanEval, MBPP) or multilingual datasets to see if solution-aware retrieval aids non-algorithmic code generation

- **Question:** How can the model dynamically decide when to retrieve solution-aware examples versus relying on internal knowledge to avoid introducing distractions?
  - **Basis in paper:** Results show SolveRank lowers performance on easy problems [inferred from Table 1 analysis]
  - **Why unresolved:** Current model retrieves references for all inputs, which degrades performance on simpler tasks where base model is sufficient or where abstraction is unnecessary
  - **What evidence would resolve it:** Mechanism that filters queries by difficulty or potential benefit before retrieval, demonstrating improved Pass@1 rates on easy/medium problems compared to current "always-retrieve" approach

## Limitations

- The paper doesn't quantify the false-positive rate in synthetic data, which could introduce noise into the training process
- Difficulty-dependent effectiveness suggests one-size-fits-all approach may not be optimal, but adaptive retrieval strategies aren't explored
- Specific hyperparameters for InfoNCE loss and exact number of retrieved examples used during inference are not specified

## Confidence

- **High Confidence:** Core claim that SolveRank outperforms existing retrieval methods on xCodeEval dataset is supported by reported MRR improvement and code generation pass@1 rates
- **Medium Confidence:** Claim that retrieval is most beneficial for difficult problems is supported by data, but why medium-difficulty problems sometimes suffer isn't fully explored
- **Low Confidence:** Exact impact of GPT-4o verifier on data quality is uncertain without quantified false-positive rate

## Next Checks

1. **Verifier Sensitivity Analysis:** Manually inspect 50 synthetic pairs flagged as "equivalent" by GPT-4o to estimate the false-positive rate
2. **Difficulty-Adaptive Retrieval:** Implement difficulty classifier to selectively disable retrieval for problems with difficulty < 1400 and compare aggregate Pass@1 against baseline
3. **Hyperparameter Sensitivity:** Systematically vary temperature τ in InfoNCE loss and number of retrieved examples (K) during inference to identify optimal settings