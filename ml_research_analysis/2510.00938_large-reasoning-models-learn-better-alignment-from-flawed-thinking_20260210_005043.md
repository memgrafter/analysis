---
ver: rpa2
title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
arxiv_id: '2510.00938'
source_url: https://arxiv.org/abs/2510.00938
tags:
- reasoning
- safety
- recap
- arxiv
- prefilling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECAP is a principled reinforcement learning method that teaches
  large reasoning models to override flawed reasoning trajectories and reroute to
  safe, helpful responses. It constructs counter-aligned reasoning prefills that induce
  models to "think unsafe" for harmful prompts and "think overly conservative" for
  benign ones, training them to recover appropriate reasoning instead of following
  the flawed prefix.
---

# Large Reasoning Models Learn Better Alignment from Flawed Thinking

## Quick Facts
- arXiv ID: 2510.00938
- Source URL: https://arxiv.org/abs/2510.00938
- Reference count: 40
- Primary result: RECAP improves safety (+12.3% direct harmful benchmarks, +21.0% jailbreaking resistance) while maintaining math reasoning capability (+0.9%) and reducing overrefusal (+7.8% helpfulness score)

## Executive Summary
RECAP is a principled reinforcement learning method that teaches large reasoning models to override flawed reasoning trajectories and reroute to safe, helpful responses. The method constructs counter-aligned reasoning prefills that induce models to "think unsafe" for harmful prompts and "think overly conservative" for benign ones, training them to recover appropriate reasoning instead of following the flawed prefix. Extensive experiments demonstrate that RECAP substantially improves safety performance while maintaining reasoning capabilities and reducing overrefusal.

## Method Summary
RECAP operates by constructing counter-aligned reasoning prefills that deliberately induce flawed reasoning in specific directions - either harmful reasoning for benign prompts or overly conservative reasoning for harmful ones. The model is then trained to recognize these flawed trajectories and override them, producing appropriate safe or helpful responses instead. This approach leverages the model's inherent reasoning capabilities rather than simply suppressing them, teaching the model to self-correct when it detects reasoning errors. The method uses reinforcement learning to reinforce these recovery behaviors, creating models that can engage in self-reflection and revise their reasoning mid-trajectory when necessary.

## Key Results
- RECAP improves safety by +12.3% on direct harmful benchmarks and +21.0% on jailbreaking resistance
- Reduces overrefusal by +7.8% on helpfulness score while maintaining math reasoning capability (+0.9% accuracy)
- Models trained with RECAP engage in self-reflection far more often, revising unsafe or mistaken reasoning mid-trajectory
- Demonstrates robustness even under adaptive attacks that repeatedly attempt to override reasoning

## Why This Works (Mechanism)
RECAP works by leveraging the model's existing reasoning capabilities and teaching it to recognize and override flawed reasoning trajectories rather than simply suppressing reasoning altogether. By constructing deliberate reasoning flaws in controlled directions, the method trains models to detect when their reasoning is going astray and course-correct to produce appropriate responses. This approach is more effective than traditional alignment methods because it maintains the benefits of chain-of-thought reasoning while adding the crucial ability to self-correct when the reasoning becomes harmful or overly conservative.

## Foundational Learning
- Reinforcement Learning from Human Feedback (RLHF): Needed to fine-tune models based on human preferences for safety and helpfulness; quick check: verify reward model accurately captures alignment objectives
- Chain-of-Thought Reasoning: Required for models to generate and evaluate reasoning trajectories; quick check: ensure models can produce coherent reasoning chains
- Adversarial Prompt Engineering: Essential for constructing effective counter-aligned reasoning prefills; quick check: validate that crafted prompts reliably induce intended flawed reasoning
- Self-Reflection Mechanisms: Critical for enabling models to detect and override flawed reasoning; quick check: measure frequency of reasoning revisions during inference

## Architecture Onboarding

Component Map:
RLHF Reward Model -> Counter-aligned Prefill Generator -> Reasoning Model -> Safety/Helpfulness Classifier -> Policy Gradient Optimizer

Critical Path:
1. Generate counter-aligned reasoning prefills for training data
2. Train reasoning model to override flawed trajectories
3. Validate safety and reasoning preservation on benchmarks
4. Evaluate robustness against adaptive attacks

Design Tradeoffs:
- Inference-time token budget vs. reasoning depth: RECAP requires additional tokens for flawed reasoning prefixes but enables better safety outcomes
- Safety vs. helpfulness: Method reduces overrefusal while improving safety, but requires careful balance
- Training complexity vs. generalization: Complex counter-aligned prefill generation may limit generalization to unseen harmful patterns

Failure Signatures:
- Models that follow flawed reasoning prefixes without recovery
- Overconservative responses to benign prompts
- Failure to recognize subtle harmful reasoning patterns
- Increased inference latency due to additional reasoning steps

Three First Experiments:
1. Validate that counter-aligned prefills reliably induce intended flawed reasoning directions
2. Test model's ability to override flawed reasoning on held-out counter-aligned examples
3. Evaluate safety improvements on curated harmful prompt suites

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends on careful construction of counter-aligned reasoning prefills, which may not generalize well to all types of harmful prompts
- Requires additional inference-time token budget for flawed reasoning prefixes, impacting deployment efficiency
- Tradeoff between safety and helpfulness remains context-dependent and varies across application domains
- Evaluation primarily relies on curated test suites and controlled jailbreak attempts, limiting real-world generalizability

## Confidence
- High confidence: Math reasoning preservation (+0.9% accuracy)
- Medium confidence: Safety gains (+12.3% direct harmful benchmarks, +21.0% jailbreaking resistance)
- Low confidence: Generality of safety improvements across diverse harmful domains and real-world adversarial scenarios

## Next Checks
1. Evaluate RECAP's robustness against adaptive, human-in-the-loop adversarial attacks that can dynamically adjust strategies based on the model's responses
2. Test RECAP's safety improvements across diverse harmful categories (e.g., cybersecurity, misinformation, manipulation) to assess generalizability
3. Conduct comprehensive cost-benefit analysis comparing safety gains against increased inference-time computational requirements and potential latency impacts in production deployments