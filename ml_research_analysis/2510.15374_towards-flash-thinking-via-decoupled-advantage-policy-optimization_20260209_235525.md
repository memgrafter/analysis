---
ver: rpa2
title: Towards Flash Thinking via Decoupled Advantage Policy Optimization
arxiv_id: '2510.15374'
source_url: https://arxiv.org/abs/2510.15374
tags:
- reasoning
- length
- depo
- accuracy
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DEPO, a novel reinforcement learning algorithm
  designed to mitigate overthinking in Large Reasoning Models (LRMs). DEPO addresses
  the problem of excessively lengthy and redundant reasoning trajectories in LRMs
  by introducing a decoupled advantage computation method that differentiates between
  efficient and inefficient reasoning segments.
---

# Towards Flash Thinking via Decoupled Advantage Policy Optimization

## Quick Facts
- arXiv ID: 2510.15374
- Source URL: https://arxiv.org/abs/2510.15374
- Reference count: 11
- Key outcome: DEPO achieves 39% reduction in sequence length while maintaining or slightly improving task accuracy compared to base model

## Executive Summary
This paper introduces DEPO (Decoupled Advantage Policy Optimization), a novel reinforcement learning algorithm designed to mitigate overthinking in Large Reasoning Models (LRMs). DEPO addresses the problem of excessively lengthy and redundant reasoning trajectories in LRMs by introducing a decoupled advantage computation method that differentiates between efficient and inefficient reasoning segments. The approach uses a Generative Reward Model (GRM) to identify the first reasoning step that leads to the correct answer, enabling the explicit separation of reasoning trajectories into efficient and inefficient parts. By lowering the advantage values of inefficient segments based on the degree of overthinking, DEPO guides the model to suppress redundant reasoning. Additionally, DEPO incorporates a difficulty-aware length penalty and an advantage clipping strategy to prevent reward fluctuations from distorting policy updates. Experimental results show that DEPO achieves a 39% reduction in sequence length while maintaining or slightly improving task accuracy compared to the base model.

## Method Summary
DEPO is a reinforcement learning algorithm that modifies the standard GRPO (Group Relative Policy Optimization) framework to address overthinking in LRMs. The method introduces three key innovations: (1) decoupled advantage computation that separately processes efficient and inefficient reasoning segments identified by a GRM, (2) difficulty-aware length penalty that scales with problem difficulty measured by rollout success rates, and (3) advantage clipping to prevent gradient updates from being distorted by length penalties. The algorithm operates by first using a GRM to identify the first reasoning step that leads to the correct answer, then splitting the response into efficient and inefficient segments. The advantages of tokens in inefficient segments are down-weighted based on the count of redundant reasoning steps. The method is trained using the VeRL framework with 1 epoch, batch size 128, learning rate 1e-6, and 8 rollouts per prompt.

## Key Results
- DEPO reduces redundant reasoning steps by more than 50% compared to base model
- Achieves 39% reduction in response length while maintaining or slightly improving task accuracy
- Outperforms base model in overall accuracy with an average accuracy gain of 2.0%
- Achieves 38.7% reduction in response length for DeepSeek-Distill-Qwen-7B model

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Advantage for Inefficient Segments
DEPO splits reasoning trajectories into efficient segments (up to first correct answer) and inefficient segments (verification/reflection after answer). By lowering advantage values of inefficient segments based on redundant reasoning step count, the model learns to suppress overthinking without penalizing valid logic. Core assumption: first correct reasoning step is optimal truncation point. Break condition: if GRM fails to accurately identify this boundary, valid logic may be misclassified as inefficient.

### Mechanism 2: Difficulty-Aware Length Penalty
The reward function includes a length penalty modulated by the number of correct rollouts (δ) as a proxy for problem difficulty. High δ (easy problems) means stronger length penalty; low δ (hard problems) allows more length. Core assumption: δ reliably proxies problem difficulty. Break condition: if high-luck rollouts on hard problems yield high δ, aggressive truncation may degrade accuracy.

### Mechanism 3: Advantage Clipping for Gradient Stability
Before policy updates, DEPO clips advantages so correct responses have minimum positive advantage and incorrect responses have non-positive advantage. This separates correctness signal from length penalty effects. Core assumption: correctness is strictly superior to efficiency. Break condition: if length penalty numerically overshadows accuracy reward even after clipping, optimization may become unstable.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: DEPO is a modification of GRPO that breaks the "one value per sequence" constraint
  - Quick check question: In vanilla GRPO, do all tokens in a response share the same advantage value? (Answer: Yes)

- **Concept: Overthinking / Redundant Reasoning**
  - Why needed here: Paper targets specific behaviors like repeated verification after answer is found
  - Quick check question: Does DEPO define "inefficient" reasoning as steps taken before answer is found? (Answer: No, it defines it as steps after first correct derivation)

- **Concept: Generative Reward Models (GRM)**
  - Why needed here: DEPO relies on LLM-based judge to score answers and locate efficient boundary
  - Quick check question: Why does paper prefer GRM over rule-based scorer for complex math? (Answer: Rule-based accuracy drops on complex answer formats)

## Architecture Onboarding

- **Component map:**
  - Policy Model: DeepSeek-Distill-Qwen (Actor/Learner)
  - GRM (Generative Reward Model): Qwen2.5-Instruct-7B (Fine-tuned Judge)
  - Rule-Based Matcher: Regex/Logic to count redundant transition phrases (N) and self-reflection words (X)
  - Optimizer: VeRL framework implementation

- **Critical path:**
  1. Sample G rollouts for prompt x
  2. Pass rollouts to GRM to get Score and "Reflection" (first correct sentence)
  3. Calculate R_accuracy (correctness) and R_length (difficulty-aware penalty)
  4. Compute sequence-level advantages Â'_i
  5. Clip Advantages: Force Correct >0, Incorrect ≤0
  6. Decouple: Identify tokens in o_ie (after first correct sentence). Down-weight their advantages by factor f(K)
  7. Update Policy

- **Design tradeoffs:**
  - GRM Inference Cost vs. Accuracy: GRM is more accurate than rules but requires significant GPU memory and inference time
  - Length vs. Accuracy: Ablation shows length penalty reduces tokens significantly but can lower accuracy on challenging datasets (AIME) compared to vanilla GRPO

- **Failure signatures:**
  - 10% Repetition Loop: Without DEPO, ~10% of rollouts become "overlong" due to autoregressive error accumulation
  - Reflection Hallucination: If GRM fails to identify correct boundary, model may learn to stop reasoning prematurely or continue redundantly
  - Accuracy Drop on Hard Tasks: Aggressive length reduction (β or α too high) may strip necessary exploration steps from complex reasoning chains

- **First 3 experiments:**
  1. GRM Validation: Verify fine-tuned GRM can successfully match "first correct reasoning step" in MATH500 set (Target: >90% matching rate)
  2. Overlong Stress Test: Generate 1024 samples and confirm frequency of "overlong responses" drops from ~10% (baseline) to <0.5% with DEPO
  3. Ablation (Length vs. Decouple): Run "DEPO w/o Adv-Decouple" vs "DEPO w/o Len-Penalty" to distinguish contribution of token-suppression vs global shortness

## Open Questions the Paper Calls Out

### Open Question 1
Can DEPO effectively mitigate overthinking in logical reasoning or coding tasks where intermediate reasoning steps are less explicit than in mathematics? The authors state training is confined to mathematical datasets since they are easy to verify and explicitly call for further studies on logical and code problems. This methodology relies on identifying the specific token that derives correct answer to segment efficient from inefficient reasoning, which may be ambiguous in code generation or abstract logic. Evidence would come from applying DEPO to benchmarks like HumanEval or logical entailment datasets and comparing "valid thinking" extraction accuracy against mathematical datasets.

### Open Question 2
Does the rule-based definition of "redundant reasoning steps" transfer effectively to model architectures outside the DeepSeek-Distill-Qwen family? The authors note experiments were restricted to 1.5B and 7B Qwen variants due to computational resources and variance of overthinking between different models. The redundant reasoning matching method relies on specific transition phrases and self-reflection keywords which may be idiosyncratic to training data or "aha moments" of specific model tested. Evidence would come from training distinct model families (e.g., Llama or Mistral) with DEPO and analyzing if current K calculation requires retuning.

### Open Question 3
How robust is policy optimization to segmentation errors made by the Generative Reward Model (GRM)? The authors acknowledge DEPO relies on GRM for both scoring and identifying first correct reasoning step, thus performance critically depends on GRM quality. While GRM achieves 93.9% matching accuracy, the specific impact of remaining ~6% error rate on decoupled advantage calculation is not quantified. Misidentifying y_ans could incorrectly penalize efficient tokens or reward inefficient ones. Evidence would come from sensitivity analysis where synthetic noise is introduced into GRM segmentation labels to measure resulting variance in model accuracy and convergence speed.

## Limitations
- Reliance on GRM for identifying first correct reasoning step without sufficient detail on training data or evaluation metrics
- Rule-based patterns for identifying redundant reasoning steps (N and X) not explicitly defined, affecting reproducibility
- Length penalty can degrade accuracy on challenging datasets like AIME, suggesting suboptimal trade-off for some problem types

## Confidence

**Major Claims:**
- Decoupled advantage mechanism effectively reduces overthinking: Medium
- Difficulty-aware length penalty improves efficiency without accuracy loss: Medium  
- Advantage clipping stabilizes training: Medium
- Overall DEPO achieves 39% length reduction with maintained/ improved accuracy: Medium

## Next Checks

1. **GRM Validation**: Verify the fine-tuned GRM's ability to accurately identify the first correct reasoning step in a subset of the MATH500 dataset. Target a reflection matching rate of >90% to ensure reliable segmentation of efficient and inefficient reasoning steps.

2. **Overlong Stress Test**: Generate 1024 samples using the base model and DEPO to confirm the reduction in overlong responses. The baseline should show ~10% overlong responses, which should drop to <0.5% with DEPO.

3. **Ablation Study**: Conduct an ablation study to isolate the contributions of the decoupled advantage mechanism and the difficulty-aware length penalty. Compare "DEPO w/o Adv-Decouple" vs "DEPO w/o Len-Penalty" to quantify their individual impacts on accuracy and length reduction.