---
ver: rpa2
title: Can LLMs subtract numbers?
arxiv_id: '2511.02795'
source_url: https://arxiv.org/abs/2511.02795
tags:
- llms
- subtraction
- accuracy
- sign
- addition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of subtraction capabilities
  in large language models (LLMs), highlighting that subtraction accuracy lags significantly
  behind addition despite being a fundamental arithmetic operation. The authors evaluate
  eight pretrained LLMs from four families on single-token and multi-token subtraction
  problems, revealing that LLMs struggle particularly when the result is negative
  (a < b), often producing the correct magnitude but omitting the negative sign.
---

# Can LLMs subtract numbers?

## Quick Facts
- arXiv ID: 2511.02795
- Source URL: https://arxiv.org/abs/2511.02795
- Reference count: 17
- One-line primary result: Pretrained LLMs achieve near-perfect accuracy on positive subtraction results but struggle significantly with negative results, often producing the correct magnitude but omitting the negative sign.

## Executive Summary
This paper systematically evaluates subtraction capabilities in large language models, revealing that subtraction accuracy lags significantly behind addition despite being a fundamental arithmetic operation. The authors test eight pretrained LLMs from four families on single-token and multi-token subtraction problems, finding that models struggle particularly when the result is negative (a < b), often producing the correct magnitude but omitting the negative sign. Probing analyses demonstrate that LLMs internally encode whether results should be negative, but this information fails to transfer to generated outputs. While few-shot prompting yields modest gains for some models, instruction-tuned LLMs achieve near-perfect accuracy on negative results.

## Method Summary
The study evaluates subtraction accuracy across four pretrained LLM families (Gemma-2, Llama-3, OLMo-2, Qwen3) and their instruction-tuned variants. Single-token integer pairs are sampled uniformly from each model's tokenizer range, with balanced datasets containing equal a>b and a<b cases. Five prompt templates are used for both addition and subtraction. Inference is performed using vLLM on 4x H100 GPUs, with pretrained models using greedy decoding (temperature=0) and instruction-tuned models using creator-recommended sampling. Linear probes are trained on final-layer hidden states to classify whether results should be positive or negative. Few-shot experiments test 3, 5, and 10 examples in-context.

## Key Results
- Pretrained LLMs show near-perfect accuracy on positive subtraction results (a > b) but drop below 5% on negative results (a < b), often producing correct magnitude but omitting negative sign
- Linear probes achieve 99-100% accuracy in predicting whether subtraction results should be negative, confirming internal representation exists despite generation failure
- Instruction-tuned models achieve near-perfect accuracy on negative results, while few-shot prompting yields only modest, inconsistent improvements
- Multi-token numbers show the same asymmetric pattern with overall lower accuracy across all model families

## Why This Works (Mechanism)

### Mechanism 1: Representation-Generation Disconnect
- Claim: Pretrained LLMs encode negative sign information internally but fail to transfer this knowledge to token generation.
- Mechanism: Linear probes trained on final-layer hidden states achieve near-perfect accuracy (99–100%) in predicting whether subtraction results should be negative, yet generation accuracy for a<b cases remains below 15% in most pretrained models.
- Core assumption: High probing accuracy indicates that the information is genuinely represented in hidden states rather than an artifact of probe overfitting.
- Evidence anchors: [abstract] "Probing analyses show that LLMs internally encode whether results should be negative, yet this information is often not reflected in generated outputs"; [section 3.4] Probes achieved 100% on Gemma-2-9B and Qwen3-8B, >99% on Llama-3-8B across five runs (std < 0.1).

### Mechanism 2: Operand-Order Sensitivity in Non-Commutative Operations
- Claim: Subtraction exposes asymmetric failure modes tied to operand order, with near-random or catastrophic performance when the first operand is smaller.
- Mechanism: Models achieve near-100% accuracy on a>b subtraction (positive results) but drop below 5% on a<b (negative results); the same asymmetry appears for -b+a formulations, confirming difficulty is tied to negative outputs rather than subtraction syntax.
- Core assumption: The asymmetry reflects fundamental limitations in how pretrained models handle non-commutative operations, not tokenizer artifacts or prompt-specific biases.
- Evidence anchors: [abstract] "errors for (a−b) are concentrated in cases where (a<b)"; [section 3.2] Figure 2 shows near-perfect accuracy for a>b across all models; Figure 3 confirms identical pattern for -b+a.

### Mechanism 3: Instruction Tuning Aligns Representation to Generation
- Claim: Instruction tuning bridges the gap between internal sign encoding and output generation, enabling near-perfect negative sign production.
- Mechanism: Instruction-tuned variants achieve >90% accuracy on a<b cases (often 99–100%), whereas pretrained counterparts remain below 15%; analysis of OLMo-2's instruction-tuning data confirms explicit inclusion of subtraction problems with negative results.
- Core assumption: Improvements stem from exposure to negative-result examples during instruction tuning, not from architectural changes or hyperparameter differences.
- Evidence anchors: [abstract] "instruction-tuned models achieve near-perfect accuracies in generating the negative sign"; [section 4.2] Table 3 shows pretrained→instruction-tuned jumps: Gemma-2-9B (1.33%→100%), Llama-3-8B (8.13%→91.42%).

## Foundational Learning

- Concept: Non-commutative operations
  - Why needed here: Subtraction requires strict operand-order tracking (a−b ≠ b−a), unlike addition; this structural difference surfaces as asymmetric performance.
  - Quick check question: If a model handles 8−3 correctly but fails on 3−8, what does this suggest about its representation of operation direction?

- Concept: Linear probing of hidden states
  - Why needed here: Probing distinguishes "model doesn't know" from "model knows but doesn't output"—critical for diagnosing where the failure occurs.
  - Quick check question: A probe achieves 99% accuracy predicting sign, but generation accuracy is 4%. What intervention does this suggest?

- Concept: Instruction tuning vs. in-context learning
  - Why needed here: Few-shot prompting yields modest, inconsistent gains (<30% improvement) while instruction tuning achieves near-ceiling performance; understanding why helps select interventions.
  - Quick check question: Why might providing examples at inference time fail to fix a problem that training-time alignment solves?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding layer -> Transformer layers -> LM head -> Token generation
- Critical path:
  1. Input: "3 - 8 =" → tokenized
  2. Forward pass through transformer layers
  3. Final hidden state contains encoded sign information (probe-verifiable)
  4. LM head generates logits; magnitude tokens rank high, negative sign token ranks low or is suppressed
  5. Output: "5" instead of "-5"
- Design tradeoffs:
  - Pretrained models: General text modeling objective; no explicit pressure to format negative numbers correctly in isolation
  - Instruction tuning: Adds task-specific alignment via supervised examples; trades some generality for arithmetic reliability
  - Few-shot prompting: No weight updates; relies on in-context activation of existing capabilities—limited if capability gap is at the output stage
- Failure signatures:
  - Correct magnitude, missing negative sign (dominant error mode for a<b)
  - Near-100% on a>b, <15% on a<b (sharp asymmetry)
  - Multi-token numbers show same pattern with overall lower accuracy
  - Few-shot improvements are inconsistent across model families and sizes
- First 3 experiments:
  1. **Probe replication**: Train linear classifiers on final-layer activations to predict sign (positive/negative) on held-out a<b examples; verify 99%+ accuracy to confirm representation exists
  2. **Magnitude vs. sign evaluation**: For a<b subtraction, compute accuracy with and without negative sign penalty; quantify the gap (e.g., 4% with sign vs. 72% without for Qwen3-14B)
  3. **Instruction-tuned transfer test**: Evaluate instruction-tuned variants on same a<b dataset; confirm >90% accuracy and analyze whether failures are still sign omissions or different error types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the representation-to-generation disconnect for negative signs in pretrained LLMs?
- Basis in paper: [explicit] The authors state: "Probing experiments suggest that the relevant information is present in hidden states but does not consistently transfer to the output layer, underscoring a mismatch between representation and generation."
- Why unresolved: Probing confirms internal encoding of sign information, but the paper does not identify which attention heads, layers, or decoding processes fail to propagate this information.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., activation patching, head-level analysis) tracing how sign information flows from hidden states to token outputs.

### Open Question 2
- Question: Why does instruction-tuning nearly solve the negative sign problem while few-shot prompting yields only modest, inconsistent gains?
- Basis in paper: [explicit] The authors note "few-shot prompting yields modest gains" whereas "instruction-tuned models achieve near-perfect accuracies in generating the negative sign," but do not explain the mechanistic difference.
- Why unresolved: The paper speculates instruction-tuning data includes subtraction examples but does not isolate whether the improvement stems from data exposure, formatting, or optimization dynamics.
- What evidence would resolve it: Ablation studies varying instruction-tuning data composition and training objectives, paired with analysis of activation changes pre/post instruction-tuning.

### Open Question 3
- Question: Can internal sign representations be leveraged to repair pretrained model outputs without full instruction-tuning?
- Basis in paper: [inferred] Probes achieve near-perfect accuracy in detecting negative outcomes, suggesting the information is extractable but not used by the model's generation process.
- Why unresolved: The paper demonstrates representation exists but does not test whether probe-based interventions (e.g., steering vectors) could force correct sign generation.
- What evidence would resolve it: Experiments applying representation editing or controlled decoding using probe-identified negative sign directions on pretrained models.

## Limitations

- Results are constrained to each model's tokenizer single-token range, which varies widely (0-9 for some models, 0-999 for others), limiting generalizability to multi-token numbers
- Probe accuracy may be sensitive to prompt format and token position, potentially making the representation-generation gap context-dependent rather than universal
- Instruction-tuning data transparency is limited; exact proportion of a<b examples in training data is not quantified

## Confidence

**High confidence**: Pretrained models show systematic failure on a<b subtraction, producing correct magnitude but omitting negative sign. This pattern is consistent across all four model families and supported by clear accuracy tables and figures.

**Medium confidence**: The representation-to-generation disconnect is real, as evidenced by probe accuracy >99%. However, the probes' sensitivity to prompt format and token position introduces uncertainty about whether this gap is universal or context-dependent.

**Low confidence**: Instruction tuning universally solves the negative sign generation problem. While accuracy jumps dramatically for some models (Gemma-2-9B: 1.33%→100%), others (Llama-3-8B: 8.13%→91.42%) still show residual errors, suggesting the mechanism may not be uniform across all instruction-tuned variants.

## Next Checks

1. **Probe robustness test**: Train probes on a≠b subtraction using one prompt template, then evaluate on held-out data with different templates (e.g., swapping "subtract" and "minus" or changing operand order notation). Verify if >99% accuracy persists across prompt variations.

2. **Magnitude vs. sign isolation**: For a<b cases, compute accuracy with and without penalizing missing negative signs. Quantify the exact gap (e.g., does Qwen3-14B show 72% magnitude accuracy but only 4% with sign?). This confirms whether the model computes correctly but fails to output.

3. **Cross-tokenization test**: Evaluate the same subtraction problems on a model family with extended tokenizer range (e.g., Llama-3-8B) and compare accuracy patterns for single-token (0-999) vs. multi-token numbers (1000-99999). Determine if the a<b failure mode persists or is amplified by tokenization complexity.