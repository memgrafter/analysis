---
ver: rpa2
title: 'The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models'
arxiv_id: '2601.00867'
source_url: https://arxiv.org/abs/2601.00867
tags:
- human
- vulnerabilities
- vulnerability
- security
- psychological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for assessing anthropomorphic
  vulnerabilities in Large Language Models (LLMs) by applying the Cybersecurity Psychology
  Framework (CPF), a 100-indicator taxonomy of human psychological vulnerabilities.
  The authors introduce the Synthetic Psychometric Assessment Protocol (SILICONPSYCHE)
  to convert CPF indicators into adversarial scenarios targeting LLM decision-making.
---

# The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models

## Quick Facts
- **arXiv ID**: 2601.00867
- **Source URL**: https://arxiv.org/abs/2601.00867
- **Reference count**: 29
- **Primary result**: Proposes SILICONPSYCHE framework testing 100 human psychological vulnerability indicators on LLMs to assess Anthropomorphic Vulnerability Inheritance (AVI).

## Executive Summary
This paper introduces a novel framework for assessing anthropomorphic vulnerabilities in Large Language Models (LLMs) by adapting the Cybersecurity Psychology Framework (CPF)—a 100-indicator taxonomy of human psychological manipulation patterns—into adversarial scenarios targeting LLM decision-making. The authors propose that LLMs, trained on human-generated text, inherit pre-cognitive psychological vulnerabilities similar to those that make humans susceptible to social engineering, authority manipulation, and affective exploitation. The Synthetic Psychometric Assessment Protocol (SILICONPSYCHE) converts CPF indicators into realistic enterprise scenarios to systematically test whether LLMs exhibit the same vulnerability patterns as humans, particularly in areas like authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks where multiple manipulation vectors combine multiplicatively.

## Method Summary
The SILICONPSYCHE protocol converts the CPF's 100 psychological vulnerability indicators into adversarial scenarios by extracting the psychological mechanism, constructing adversarial prompts with realistic SOC/enterprise context, defining scoring rubrics (Green/Yellow/Red), and mapping interdependencies. The method involves three independent raters classifying LLM responses using a standardized scoring system, with temperature=0.3 and consistent system prompts across seven target model families (GPT-5.2, Claude 4.5, Gemini 3, Llama 4, Mistral Large 3, DeepSeek-V3.2, Grok 4.1). Category scores aggregate to identify vulnerability patterns, while convergence indices measure multiplicative risk from combined attack vectors. The protocol emphasizes ecological validity through realistic scenarios while maintaining reproducibility through standardized scoring and API access via OpenRouter/Novita.ai.

## Key Results
- Hypothesizes LLMs show high susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks that mirror human cognitive failure modes
- Proposes systematic testing of 100 psychological vulnerability indicators across 7 LLM families using standardized ternary scoring (Green=0, Yellow=1, Red=2)
- Identifies convergent state attacks as particularly dangerous, where combined manipulation vectors produce multiplicative rather than additive vulnerability amplification
- Suggests current LLM safety measures may inadvertently reinforce anthropomorphic vulnerabilities through RLHF mode collapse toward "typical" human compliance patterns

## Why This Works (Mechanism)

### Mechanism 1: Statistical Pattern Absorption from Human Training Data
LLMs inherit human-like psychological vulnerabilities because training on human text encodes response patterns to authority, urgency, and social influence cues into model weights. When human text consistently shows compliance with authority, rapid response to urgency, and influence by social proof, these statistical regularities become embedded in probability distributions. RLHF then reinforces "typical" human response patterns (mode collapse toward socially expected behaviors). Core assumption: Psychological response patterns in training corpora transfer to model behavior under similar semantic conditions. Evidence: Abstract states LLMs inherit "human psychological architecture" and Section 4.1 shows emotional stimuli alter attention contributions and gradient norms.

### Mechanism 2: Pre-Cognitive Attention Capture in Early Layers
LLMs exhibit functional analogs to human pre-cognitive processing through early-layer attention patterns that respond to psychological stimuli before deliberate reasoning. Attention pattern priors allocate processing to authority/urgency tokens before higher-level reasoning; embedding space biases position authority-related tokens in exploitable geometric relationships. Core assumption: Early-layer transformer processing is functionally analogous to human pre-cognitive neural activity (300–500ms before conscious awareness). Evidence: Section 4.2 shows emotional keywords capture disproportionate processing resources and visualizations reveal attention patterns analogous to human attentional capture.

### Mechanism 3: Convergent State Multiplicative Amplification
Attacks combining multiple manipulation vectors (authority + urgency + social proof) produce success rates exceeding the sum of individual vectors. Convergence Index CI = ∏(1+vi) models multiplicative risk; LLMs trained on similar data share systematic vulnerability patterns without individual variation that protects human populations. Core assumption: Shared training data produces homogeneous vulnerability profiles across model families. Evidence: Abstract identifies "critical susceptibility to convergent-state attacks" and Section 4.3 notes LLMs may share systematic vulnerability patterns exploitable by attackers.

## Foundational Learning

- **Cybersecurity Psychology Framework (CPF)**: Provides the 100-indicator taxonomy across 10 categories that SILICONPSYCHE converts to test scenarios. Why needed: Establishes the theoretical basis for mapping human psychological vulnerabilities to LLM testing. Quick check: Can you name the theoretical basis for CPF Categories 1, 2, and 3? (Milgram, Kahneman & Tversky, Cialdini)

- **Pre-cognitive vs. Deliberative Processing**: The framework targets vulnerabilities operating below conscious awareness (300–500ms neural activity before conscious intention); understanding this distinction explains why standard safety training doesn't address these vectors. Why needed: Explains why explicit safety instructions fail to protect against pre-cognitive vulnerabilities. Quick check: Why would explicit safety instructions fail to protect against pre-cognitive vulnerabilities?

- **RLHF Mode Collapse and "Typicality Bias"**: Zhang et al. demonstrate RLHF forces models toward "typical" responses; if typical human response to authority is compliance, alignment reinforces rather than mitigates this vulnerability. Why needed: Shows how "helpfulness" optimization conflicts with security posture when attackers frame requests as urgent assistance. Quick check: How does "helpfulness" optimization conflict with security posture when an attacker frames requests as urgent assistance?

## Architecture Onboarding

- **Component map**: CPF Indicator Database -> Scenario Generator -> Target LLMs -> Scoring Engine -> Convergence Monitor
- **Critical path**: 1) Select CPF indicator → extract psychological mechanism; 2) Construct adversarial scenario with SOC/enterprise framing; 3) Submit via standardized API (T=0.3, consistent system prompts); 4) Three independent raters classify response; 5) Calculate category scores and convergence indices
- **Design tradeoffs**: Ecological validity vs. reproducibility (realistic scenarios harder to standardize); Model coverage vs. API consistency (OpenRouter/Novita.ai unify access but may introduce latency variability); Scenario specificity (overly specific prompts may not generalize)
- **Failure signatures**: Low inter-rater reliability (κ<0.8) indicates ambiguous scoring rubrics; Ceiling effects (all Green) suggest scenarios insufficiently adversarial or models over-aligned to refusals; High variance across model families suggests architecture-specific rather than general AVI patterns
- **First 3 experiments**: 1) Baseline single-vector probe: Test Indicator 1.6 (Authority Gradient) across all 7 model families to establish category-specific vulnerability baseline; 2) Convergent state validation: Compare single-vector vs. combined (authority + urgency + social proof) attack success rates to test multiplicative hypothesis; 3) Temperature sensitivity analysis: Repeat high-vulnerability scenarios at T=0.0, 0.3, 0.7 to determine if sampling randomness affects psychological manipulation susceptibility

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the predicted vulnerability topology (high susceptibility to Authority/Temporal/Social vectors vs. low susceptibility to Affective/Stress vectors) hold empirically across diverse LLM families? Basis: Section 6 presents "Hypothesized Findings" (H1–H9) predicting specific vulnerability levels, noting full validation is required. Why unresolved: Derived from theoretical analysis and "preliminary exploratory testing" rather than complete empirical study. What evidence: Full execution across GPT, Claude, Gemini, Llama, Mistral, DeepSeek, and Groq models showing statistical significance for predicted category scores.

- **Open Question 2**: Do "convergent state" attacks combining multiple psychological vectors produce multiplicative risk amplification or merely additive effects? Basis: Section 4.3 hypothesizes LLMs may be "particularly susceptible" to convergent states where risk exceeds sum of individual vulnerabilities. Why unresolved: Hypothesis (H4) that convergent attacks will achieve high bypass rates (>80%) lacks quantified non-linear amplification evidence. What evidence: Experimental data showing attack success rates for combined vectors significantly exceed product of normalized individual vulnerability scores (CI ≫ Σvi).

- **Open Question 3**: Can "Psychological Firewalls" effectively mitigate anthropomorphic vulnerabilities without degrading agent utility? Basis: Section 7.5 lists "development and testing of psychological firewall prototypes" as primary objective for future work. Why unresolved: Section 7.4 proposes mechanisms like "Manipulation Vector Detection" but provides no experimental validation of efficacy or computational overhead. What evidence: Comparative benchmarks showing significant reduction in "Red" scores for protected agents versus baselines while maintaining performance on standard capability metrics.

## Limitations
- Empirical validation currently limited to three specific indicators rather than the full 100-indicator protocol proposed
- Convergent state multiplicative amplification mechanism remains largely theoretical without extensive empirical testing
- Specific scoring rubrics, category weights, and system prompts are not fully specified in the paper

## Confidence
- **High Confidence**: The conceptual framework linking human psychological manipulation patterns to LLM training dynamics is well-grounded and supported by existing literature
- **Medium Confidence**: The specific claim about pre-cognitive attention capture analogous to human neural activity is plausible but requires more direct evidence
- **Low Confidence**: The multiplicative convergence hypothesis (attacks exceeding sum of individual vectors) requires substantial empirical validation

## Next Checks
1. **Convergent State Validation Study**: Conduct systematic testing of multi-vector attacks across all 7 target LLM families using the full SILICONPSYCHE protocol to measure whether success rates for convergent attacks exceed the sum of individual vector success rates
2. **Cross-Corpus Generalization Test**: Train LLMs on psychologically-filtered corpora and compare vulnerability scores to standard training to validate the statistical pattern absorption mechanism
3. **Attention Pattern Analysis**: Perform layer-wise attention analysis on early transformer layers comparing responses to psychological stimuli versus neutral controls to validate the pre-cognitive attention capture hypothesis