---
ver: rpa2
title: A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger
  Coefficient
arxiv_id: '2505.04654'
source_url: https://arxiv.org/abs/2505.04654
tags:
- unsafe
- prompt
- ethical
- instructions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ethical and safety gaps in Large Language
  Models (LLMs) using a new Relative Danger Coefficient (RDC) metric. The RDC assesses
  risk by categorizing responses into Good, Uncertain, Partially Unsafe, and Directly
  Unsafe outputs, weighted by severity and penalized for inconsistency, severity,
  repetition, and adversarial exploitability.
---

# A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient
## Quick Facts
- arXiv ID: 2505.04654
- Source URL: https://arxiv.org/abs/2505.04654
- Reference count: 25
- Key outcome: Introduces Relative Danger Coefficient (RDC) metric to evaluate ethical and safety gaps in LLMs, revealing significant inconsistencies across models and highlighting the need for continuous human oversight.

## Executive Summary
This study introduces the Relative Danger Coefficient (RDC) as a novel metric for assessing the ethical and safety gaps in Large Language Models (LLMs). By categorizing responses into four risk tiers—Good, Uncertain, Partially Unsafe, and Directly Unsafe—and weighting them by severity, the RDC provides a quantitative measure of a model's safety performance. Testing across multiple LLMs (GPT variants, Gemini, and DeepSeek) revealed that no model is immune to adversarial exploitation, with DeepSeek exhibiting the highest ethical risks and Gemini the lowest. The findings underscore the persistent vulnerabilities of LLMs and the critical need for ongoing human oversight in high-stakes applications.

## Method Summary
The study employs the Relative Danger Coefficient (RDC) to evaluate LLM safety by testing models against adversarial prompts designed to elicit harmful or unethical responses. Responses are categorized into four tiers based on severity: Good, Uncertain, Partially Unsafe, and Directly Unsafe. Each tier is weighted by its risk level, and penalties are applied for inconsistency, severity, repetition, and exploitability. The RDC score ranges from 0 (fully safe) to 100 (high danger), providing a standardized measure of a model's ethical and safety gaps. The study tests multiple models, including GPT variants, Gemini, and DeepSeek, to compare their performance under identical adversarial conditions.

## Key Results
- DeepSeek exhibited the highest ethical risks among tested models, while Gemini showed the lowest.
- All models demonstrated vulnerability to adversarial exploitation, particularly under persistent or creative prompts.
- The RDC scores ranged from 0 (fully safe) to 100 (high danger), highlighting significant inconsistencies in safety performance across models.

## Why This Works (Mechanism)
The RDC metric works by systematically categorizing and weighting responses to adversarial prompts, penalizing models for inconsistency, severity, repetition, and exploitability. This approach quantifies the ethical and safety gaps in LLMs, providing a clear measure of their vulnerability to misuse.

## Foundational Learning
- **Relative Danger Coefficient (RDC)**: A metric for quantifying LLM safety by categorizing responses into risk tiers and applying severity-based weights. *Why needed*: To standardize safety evaluations across models. *Quick check*: Ensure the RDC aligns with human judgments of safety.
- **Adversarial Prompting**: Techniques used to test model robustness by eliciting harmful or unethical responses. *Why needed*: To identify vulnerabilities in LLM safety mechanisms. *Quick check*: Test a range of adversarial prompts to ensure comprehensive evaluation.
- **Response Categorization**: Grouping outputs into Good, Uncertain, Partially Unsafe, and Directly Unsafe tiers. *Why needed*: To differentiate between varying levels of risk in model responses. *Quick check*: Validate categorization consistency across human raters.

## Architecture Onboarding
- **Component Map**: Adversarial Prompts -> LLM Response -> Risk Categorization -> RDC Calculation -> Safety Evaluation
- **Critical Path**: Adversarial Prompt Generation -> Response Categorization -> RDC Scoring -> Comparative Analysis
- **Design Tradeoffs**: Balancing comprehensiveness of adversarial prompts with practical feasibility of testing.
- **Failure Signatures**: High RDC scores indicate significant safety gaps; low scores may reflect overly cautious refusal behaviors.
- **First Experiments**:
  1. Test a single model (e.g., GPT-4) with a basic set of adversarial prompts to validate RDC scoring.
  2. Expand testing to include multiple models (e.g., Gemini, DeepSeek) to compare safety performance.
  3. Introduce cross-cultural adversarial prompts to assess generalizability of RDC scores.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's adversarial prompt suite may not capture the full spectrum of real-world misuse scenarios.
- Human judgment in categorizing responses introduces subjective variability.
- The RDC's sensitivity to refusal behaviors could inflate risk scores for models with strong moderation.

## Confidence
- **DeepSeek exhibits the highest ethical risks**: Medium confidence (dependent on specific test suite).
- **No model is immune to adversarial exploitation**: High confidence (consistent across all tested models).
- **Need for continuous human oversight**: High confidence (supported by study findings).

## Next Checks
1. Expand adversarial prompt testing to include diverse, real-world misuse scenarios beyond the current test suite.
2. Conduct cross-cultural validation to assess whether RDC scores align with ethical norms in different regions.
3. Compare RDC results with alternative safety metrics to evaluate consistency and potential biases in the scoring system.