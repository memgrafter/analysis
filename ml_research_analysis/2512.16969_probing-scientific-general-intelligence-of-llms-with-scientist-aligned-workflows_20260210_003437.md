---
ver: rpa2
title: Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows
arxiv_id: '2512.16969'
source_url: https://arxiv.org/abs/2512.16969
tags:
- scientific
- reasoning
- data
- step
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents SGI-Bench, a comprehensive benchmark for evaluating
  Scientific General Intelligence (SGI) across four core tasks: deep research, idea
  generation, dry/wet experiments, and experimental reasoning. By aligning with the
  Practical Inquiry Model, it operationalizes SGI as the ability to navigate the full
  scientific discovery cycle.'
---

# Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows

## Quick Facts
- arXiv ID: 2512.16969
- Source URL: https://arxiv.org/abs/2512.16969
- Reference count: 40
- Primary result: SGI-Bench benchmark reveals LLMs struggle with genuine scientific discovery despite achieving high code executability

## Executive Summary
This paper introduces SGI-Bench, a comprehensive benchmark for evaluating Scientific General Intelligence (SGI) across four core tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. The benchmark operationalizes SGI as the ability to navigate the full scientific discovery cycle by aligning with the Practical Inquiry Model's four quadrants (Deliberation, Conception, Action, Perception). Through extensive evaluation of over 1,000 expert-curated samples across 10 disciplines, the study reveals fundamental limitations in current LLMs' scientific reasoning capabilities, including low exact-match accuracy in deep research (10-20%), struggles with realizability in idea generation, and poor procedural fidelity in wet experiments.

## Method Summary
The SGI-Bench system comprises four task families with multi-dimensional metrics: Deep Research (EM, SLA), Idea Generation (Effectiveness, Novelty, Detailedness, Feasibility), Dry Experiment (PassAll@k, AET, SER), Wet Experiment (Sequence Similarity, Parameter Accuracy), and Experimental Reasoning (Multi-choice Accuracy, Reasoning Validity). The evaluation uses SGIEvalAgent, a four-stage framework (Question Selection → Metric Customization → Inference & Evaluation → Report Generation) with tool augmentation (web search, Python interpreter, PDF parser). Test-Time Reinforcement Learning (TTRL) is implemented using GRPO with Qwen3-8B base, optimizing retrieval-augmented novelty rewards without ground-truth supervision.

## Key Results
- Deep research: models achieve 10-20% exact-match accuracy despite high step-level accuracy, indicating global coherence failures
- Idea generation: TTRL improves novelty scores from 49.36 to 62.06 but raises concerns about ungrounded ideas without feasibility constraints
- Dry experiments: >90% code executability but only 36-42% PassAll@5 accuracy, revealing critical numerical stability gaps
- Wet experiments: consistently low Sequence Similarity (<20%) across models, particularly failing at multi-branch temporal coordination
- Experimental reasoning: multimodal inputs improve causal reasoning but comparative reasoning remains weakest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-Time Reinforcement Learning (TTRL) improves hypothesis novelty without reference answers by optimizing retrieval-augmented novelty rewards during inference.
- Mechanism: Models generate candidate ideas evaluated against online-retrieved related works. The GRPO algorithm optimizes a composite reward: R(o) = R_format(o) + R_novelty(o, W), where novelty is computed as semantic dissimilarity between generated ideas and retrieved literature. A gating threshold (τ=5) binary rewards ideas with innovation score > τ.
- Core assumption: Novelty measured via embedding-space dissimilarity from existing literature proxies for scientific creativity; format compliance can be learned independently of content quality.
- Evidence anchors: [abstract], [section 5.1], [corpus]

### Mechanism 2
- Claim: High code executability does not imply scientific computational correctness—models achieve >90% smooth execution rate but only 36-42% PassAll@5 accuracy.
- Mechanism: LLMs learn syntactic patterns and API usage from code corpora, enabling error-free compilation. However, scientific correctness requires numerical stability, domain-appropriate algorithm selection, and understanding of discretization error propagation—none of which are captured by syntax training.
- Core assumption: Scientific computation has distinct failure modes (cascading numerical errors, methodological mismatches) invisible to standard code benchmarks.
- Evidence anchors: [abstract], [section 4.5.1], [corpus]

### Mechanism 3
- Claim: The SGI-EvalAgent framework enables scalable, scientist-aligned evaluation through four interconnected stages (Question Selection → Metric Customization → Inference & Evaluation → Report Generation).
- Mechanism: Specialized agents parse user intent, retrieve relevant benchmark subsets, customize or aggregate metrics, execute inference with tool augmentation, and generate grounded rationales for each score. The agent-as-judge paradigm replaces LLM-as-judge when multi-dimensional, domain-specific metrics are required.
- Core assumption: Decomposing evaluation into agentic stages with tool access improves rigor and transparency over monolithic LLM judgments; user-customized metrics can coexist with standardized benchmarks.
- Evidence anchors: [abstract], [section 3], [corpus]

## Foundational Learning

- Concept: Practical Inquiry Model (PIM)
  - Why needed here: The entire SGI framework is grounded in PIM's four quadrants (Deliberation, Conception, Action, Perception). Understanding this taxonomy is prerequisite to interpreting benchmark results and identifying capability gaps.
  - Quick check question: Can you map "dry experiment code completion" and "idea generation feasibility scoring" to their respective PIM quadrants?

- Concept: Meta-analytic retrieval and numerical synthesis
  - Why needed here: Deep Research tasks (Data/Properties categories) require extracting and aggregating numerical specifications from scattered literature—identified as a core bottleneck.
  - Quick check question: Given three papers with conflicting measurements of the same physical constant, what aggregation strategy would a meta-analytic approach use?

- Concept: Branch-aware and temporal protocol planning
  - Why needed here: Wet experiment evaluation shows models fail at multi-branch workflows with coordinated timing (e.g., longitudinal blood draws, parallel processing branches).
  - Quick check question: Sketch a protocol with two parallel branches that merge at a synchronization point—what information must the model track across branches?

## Architecture Onboarding

- Component map: Data layer (1,000+ expert-curated samples across 10 disciplines) → Task layer (four task families with multi-dimensional metrics) → Evaluation layer (SGIEvalAgent with tool pool) → Reporting layer (automated report generation with visualization)

- Critical path: Question Selection → Metric Customization → Model Inference (with tool calls) → Multi-metric Scoring → Rationale Generation → Report Aggregation. The retrieval-and-browsing loop accounts for ~58% of agent tool calls and is the primary latency bottleneck.

- Design tradeoffs: (1) Strict exact-match vs. step-level accuracy: EM captures end-to-end correctness but is brittle; SLA provides diagnostic signal but may