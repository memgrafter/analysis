---
ver: rpa2
title: Measuring General Intelligence with Generated Games
arxiv_id: '2505.07215'
source_url: https://arxiv.org/abs/2505.07215
tags:
- games
- game
- language
- each
- gg-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces gg-bench, a synthetic benchmark for evaluating
  general intelligence in language models through procedurally generated two-player
  strategy games. The core idea is to use LLMs to generate game descriptions, implement
  them as Gym environments, and train RL agents via self-play, creating a scalable,
  contamination-resistant evaluation framework.
---

# Measuring General Intelligence with Generated Games

## Quick Facts
- arXiv ID: 2505.07215
- Source URL: https://arxiv.org/abs/2505.07215
- Reference count: 40
- Key outcome: gg-bench uses procedurally generated games to evaluate general intelligence in LLMs, with reasoning models (o1, o3-mini, DeepSeek-R1) achieving 31-36% winrates vs RL agents, while non-reasoning models (GPT-4o, Claude 3.7 Sonnet) reach only 7-9%.

## Executive Summary
This paper introduces gg-bench, a synthetic benchmark that evaluates general intelligence in language models through procedurally generated two-player strategy games. The approach uses LLMs to generate game descriptions, implements them as Gym environments, and trains RL agents via self-play to create a scalable, contamination-resistant evaluation framework. Models are assessed by their winrate against these RL agents, with the current benchmark containing 126 diverse games after multi-stage filtering.

The benchmark demonstrates that strategic planning and adaptability are critical for success, with reasoning models significantly outperforming non-reasoning models. The approach offers controllable, interpretable evaluation that avoids memorization pitfalls of static benchmarks. The paper validates scalability by showing that stronger models generate harder, more diverse games, suggesting gg-bench can evolve with model capabilities.

## Method Summary
The gg-bench methodology uses a multi-stage pipeline where LLMs generate game descriptions that are then implemented as Gym environments and trained via self-play with RL agents. The benchmark evaluates models by their winrate against these RL agents across diverse game types. Games undergo filtering for validity, complexity, and diversity, resulting in a curated set of 126 games. The approach leverages LLM capabilities to generate novel game mechanics while avoiding contamination from existing benchmarks through synthetic generation rather than collection of existing games.

## Key Results
- Reasoning models (o1, o3-mini, DeepSeek-R1) achieve 31-36% winrates against RL agents
- Non-reasoning models (GPT-4o, Claude 3.7 Sonnet) reach only 7-9% winrates
- Stronger models (o1 vs GPT-4o) generate harder, more diverse games, validating benchmark scalability
- Manual filtering remains necessary for quality control, suggesting the process isn't fully automated

## Why This Works (Mechanism)
The benchmark works by creating a controlled environment where models must demonstrate strategic planning and adaptability against RL agents trained on the same procedurally generated games. By using synthetic generation rather than collecting existing games, the approach minimizes contamination risks while maintaining diversity. The self-play training creates strong baseline opponents that test genuine strategic understanding rather than pattern matching.

## Foundational Learning
- **Game generation via LLMs** - Needed to create diverse, novel evaluation tasks at scale; Quick check: Verify generated games have unique mechanics not found in existing benchmarks
- **RL agent training for evaluation** - Required to establish strong, consistent baseline opponents; Quick check: Measure RL agent winrates against random agents to confirm learning
- **Multi-stage filtering** - Essential for ensuring game quality and diversity; Quick check: Analyze filtered vs unfiltered game distributions for diversity metrics

## Architecture Onboarding

**Component map:** LLM game generation -> Game implementation -> RL training -> Model evaluation -> Filtering pipeline

**Critical path:** Game generation → Implementation → RL training → Model testing → Winrate calculation

**Design tradeoffs:** Synthetic generation provides contamination resistance but introduces uncertainty about true novelty; Manual filtering ensures quality but limits automation

**Failure signatures:** Low model winrates could indicate either strong RL opponents or poor game design; High variance across games suggests filtering issues or insufficient RL training

**3 first experiments:** 1) Generate 10 games with different prompting strategies and measure RL training convergence; 2) Test baseline model performance on identical game sets to establish variance; 3) Compare winrates between human-designed and LLM-generated games

## Open Questions the Paper Calls Out
The paper acknowledges that the benchmark cannot assess social intelligence, representing a fundamental limitation for measuring general intelligence. The reliance on LLM-generated content for both games and evaluation metrics raises concerns about potential feedback loops that could systematically favor certain model capabilities. The manual filtering requirement for quality control suggests the process isn't fully automated and may introduce selection bias.

## Limitations
- Cannot assess social intelligence, limiting general intelligence measurement scope
- Manual filtering requirement suggests process isn't fully automated
- Uncertainty about whether all generated games truly avoid pretraining data overlap

## Confidence
- Benchmark scalability: Medium - relies on assumption that stronger models consistently generate more challenging games
- Contamination resistance: Low - depends on diversity and novelty of procedurally generated games
- Model performance claims: High - supported by empirical winrate data across multiple reasoning and non-reasoning models

## Next Checks
1. Conduct systematic analysis of game descriptions for potential contamination with existing benchmark content
2. Implement fully automated quality filtering to verify the necessity of manual intervention
3. Test the benchmark with models that have no exposure to any game-related training data to establish baseline performance