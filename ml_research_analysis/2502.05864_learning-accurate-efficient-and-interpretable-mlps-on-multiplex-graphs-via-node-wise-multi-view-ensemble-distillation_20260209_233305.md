---
ver: rpa2
title: Learning Accurate, Efficient, and Interpretable MLPs on Multiplex Graphs via
  Node-wise Multi-View Ensemble Distillation
arxiv_id: '2502.05864'
source_url: https://arxiv.org/abs/2502.05864
tags:
- mgfnn
- mlps
- multiplex
- mgnns
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of combining the accuracy of
  multiplex graph neural networks (MGNNs) with the efficiency of multilayer perceptrons
  (MLPs) for latency-sensitive applications. The authors propose MGFNN and MGFNN+,
  which use knowledge distillation to transfer knowledge from MGNNs to MLPs.
---

# Learning Accurate, Efficient, and Interpretable MLPs on Multiplex Graphs via Node-wise Multi-View Ensemble Distillation

## Quick Facts
- **arXiv ID**: 2502.05864
- **Source URL**: https://arxiv.org/abs/2502.05864
- **Authors**: Yunhui Liu; Zhen Tao; Xiang Zhao; Jianhua Zhao; Tao Zheng; Tieke He
- **Reference count**: 31
- **Primary result**: Achieves ~10% accuracy improvement over vanilla MLPs and 35.40×-89.14× speedup over MGNNs

## Executive Summary
This paper addresses the challenge of combining the accuracy of multiplex graph neural networks (MGNNs) with the efficiency of multilayer perceptrons (MLPs) for latency-sensitive applications. The authors propose MGFNN and MGFNN+, which use knowledge distillation to transfer knowledge from MGNNs to MLPs. MGFNN directly uses soft labels from MGNNs as targets, while MGFNN+ introduces a node-wise multi-view ensemble distillation strategy. This strategy employs a low-rank approximation-based reparameterization to learn node-wise coefficients, enabling adaptive knowledge ensemble from each view-specific GNN. The experiments show that MGFNNs achieve average accuracy improvements of about 10% over vanilla MLPs and perform comparably or even better to teacher MGNNs.

## Method Summary
The proposed method consists of two main approaches: MGFNN and MGFNN+. MGFNN directly uses soft labels from MGNNs as targets for distillation. MGFNN+ introduces a node-wise multi-view ensemble distillation strategy that employs low-rank approximation-based reparameterization to learn node-wise coefficients. This enables adaptive knowledge ensemble from each view-specific GNN, allowing the model to learn different ensemble coefficients for different nodes. The framework transfers knowledge from teacher MGNNs to student MLPs while maintaining interpretability through the learned node-wise coefficients.

## Key Results
- MGFNNs achieve average accuracy improvements of about 10% over vanilla MLPs
- MGFNNs perform comparably or even better than teacher MGNNs
- MGFNNs achieve 35.40×-89.14× speedup in inference over MGNNs
- MGFNN+ can learn different ensemble coefficients to distill multiplex semantic knowledge for different nodes interpretably

## Why This Works (Mechanism)
The method works by transferring knowledge from accurate but computationally expensive MGNNs to efficient MLPs through knowledge distillation. The node-wise multi-view ensemble distillation in MGFNN+ allows the model to adaptively combine knowledge from different graph views for each node. The low-rank approximation-based reparameterization enables the learning of interpretable node-wise coefficients that capture how different views contribute to node representations. This approach effectively balances the trade-off between accuracy and efficiency while maintaining interpretability of the learned ensemble coefficients.

## Foundational Learning
- **Multiplex Graphs**: Why needed - To model multiple types of relationships between nodes simultaneously. Quick check - Verify that each view represents a distinct relationship type.
- **Knowledge Distillation**: Why needed - To transfer knowledge from complex models to simpler, more efficient models. Quick check - Ensure soft labels from teacher models are used as targets.
- **Low-rank Approximation**: Why needed - To enable efficient learning of node-wise coefficients while maintaining interpretability. Quick check - Verify that the learned coefficients can be interpreted meaningfully.
- **Graph Neural Networks**: Why needed - To capture complex structural information in multiplex graphs. Quick check - Ensure that each view-specific GNN captures view-specific patterns.
- **Multi-view Ensemble**: Why needed - To combine knowledge from different graph views adaptively. Quick check - Verify that ensemble coefficients vary across nodes and views.

## Architecture Onboarding

**Component Map**: Input Graphs -> View-specific GNNs -> Knowledge Distillation -> Node-wise Coefficients -> MLP Output

**Critical Path**: The critical path involves the distillation of knowledge from MGNNs through soft labels, followed by the learning of node-wise coefficients via low-rank approximation, and finally the aggregation of this knowledge in the MLP. The node-wise coefficient learning is particularly crucial as it determines how knowledge from different views is combined for each node.

**Design Tradeoffs**: The main tradeoff is between accuracy and efficiency. While MGNNs provide high accuracy, they are computationally expensive. MLPs are efficient but lack the ability to capture complex graph structures. The proposed method aims to balance these tradeoffs by transferring knowledge from MGNNs to MLPs while maintaining interpretability through node-wise coefficients.

**Failure Signatures**: Potential failures could include: 1) Poor distillation quality if the MLP cannot effectively learn from soft labels, 2) Unstable node-wise coefficient learning if the low-rank approximation is not well-suited to the data, 3) Interpretability issues if the learned coefficients do not align with actual multiplex graph semantics.

**First Experiments**: 1) Compare accuracy of MGFNN vs vanilla MLP on a multiplex graph dataset, 2) Evaluate inference speed of MGFNN vs MGNN on the same dataset, 3) Analyze the learned node-wise coefficients to verify their interpretability.

## Open Questions the Paper Calls Out
None

## Limitations
- The node-wise coefficient learning mechanism relies on low-rank approximation for interpretability, though the paper does not fully validate whether the learned coefficients align with actual multiplex graph semantics in practice.
- The claim that MGFNN+ can "interpretably" learn different ensemble coefficients for different nodes remains largely theoretical without systematic validation against ground truth node properties or domain knowledge.
- The efficiency gains (35.40×-89.14× speedup) are impressive but may depend heavily on specific hardware configurations and graph sizes not fully explored across diverse deployment scenarios.
- The evaluation focuses primarily on accuracy and speed metrics, with limited analysis of robustness to noisy multiplex edges or missing views.

## Confidence

**High**: Accuracy improvements over vanilla MLPs (~10% average gain) and comparable performance to MGNNs
**Medium**: Efficiency speedup claims and node-wise interpretability of ensemble coefficients
**Medium**: Claims about learning multiplex semantic knowledge for different nodes

## Next Checks
1. Conduct ablation studies removing low-rank constraints to quantify their impact on both performance and interpretability claims
2. Perform cross-dataset generalization tests to verify consistent performance improvements across diverse multiplex graph domains
3. Validate learned node-wise coefficients against ground truth node attributes or domain expert knowledge to substantiate interpretability claims