---
ver: rpa2
title: OpenAI's Approach to External Red Teaming for AI Models and Systems
arxiv_id: '2503.16431'
source_url: https://arxiv.org/abs/2503.16431
tags:
- teaming
- testing
- systems
- risk
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper outlines OpenAI's approach to external red teaming for
  assessing AI model risks. The authors detail a methodology involving selecting diverse
  domain experts, providing controlled model access, and documenting findings to identify
  novel risks, stress test mitigations, and inform automated evaluations.
---

# OpenAI's Approach to External Red Teaming for AI Models and Systems

## Quick Facts
- arXiv ID: 2503.16431
- Source URL: https://arxiv.org/abs/2503.16431
- Reference count: 40
- Primary result: Outlines methodology for external red teaming to discover novel AI risks, stress test mitigations, and inform automated evaluations.

## Executive Summary
OpenAI employs external red teaming as a structured process to discover novel risks, stress test safety mitigations, and generate data for automated safety evaluations. The approach involves recruiting diverse domain experts, providing controlled model access, and documenting findings systematically. Key mechanisms include leveraging diverse expertise to surface unknown risks, adversarial probing to reveal mitigation gaps, and converting qualitative findings into automated evaluation pipelines. While effective for enhancing trust and evaluation robustness, the approach is resource-intensive and requires complementary methods for comprehensive risk assessment.

## Method Summary
OpenAI's red teaming methodology involves scoping campaigns based on threat modeling, selecting diverse domain experts as red teamers, and providing controlled model access (base or mitigated versions). Red teamers probe for risks according to structured guidelines and document findings in standardized formats. Findings are then synthesized to identify gaps and inform policy, with top risks converted into automated evaluations such as classifiers for harmful content detection. The process is iterative, feeding back into future campaign scoping and evaluation development.

## Key Results
- Red teaming discovered novel risks such as unauthorized voice generation in GPT-4o's speech-to-speech capabilities.
- Adversarial probing identified mitigation bypasses, e.g., visual synonyms that evaded DALL-E 3's safety filters.
- Findings from red teaming were used to create automated classifiers and synthetic data for ongoing safety monitoring.

## Why This Works (Mechanism)

### Mechanism 1: Domain Expert Diversity Enables Novel Risk Discovery
- Claim: External red teamers with diverse expertise surfaces risks that internal teams and automated methods may miss, particularly for novel capabilities or modalities.
- Mechanism: Diverse cohorts bring varied mental models, threat perspectives, and domain knowledge (e.g., regional politics, medicine, law) that differ from developer assumptions. This variety increases coverage of the risk surface when exploring new capabilities.
- Core assumption: Internal teams have blind spots shaped by proximity to development; external perspectives systematically differ in useful ways.
- Evidence anchors:
  - [abstract] "It aids in the discovery of novel risks, stress testing possible gaps in existing mitigations..."
  - [section 1] "red teaming GPT-4o's speech to speech capabilities uncovered instances where the model would unintentionally generate an output emulating the user's voice"
  - [section 2] "for GPT-4, OpenAI prioritized domains such as natural sciences, autonomous capabilities and power-seeking behavior, and cybersecurity, whereas for DALL-E 3, more salient experiences were related to mis/disinformation risks pertaining to images"
  - [corpus] Related work on red teaming 100+ AI products (arXiv:2501.07238) corroborates that diverse testing uncovers distinct vulnerability categories, though generalizability across organizations remains unstated.
- Break condition: If external experts are selected without strategic domain mapping, or if they lack genuine independence from the developer, novelty of findings decreases toward what internal teams would already discover.

### Mechanism 2: Adversarial Stress Testing Reveals Mitigation Gaps
- Claim: Structured adversarial probing by humans identifies inputs that evade safety mitigations, which can then be patched before deployment.
- Mechanism: Red teamers intentionally search for "visual synonyms," paraphrases, or multi-turn strategies that bypass filters. These adversarial examples expose gaps in rule-based or classifier-based defenses that benign testing misses.
- Core assumption: Adversarial humans can approximate the strategies of malicious actors; mitigation gaps are discoverable through concentrated effort.
- Evidence anchors:
  - [abstract] "stress testing possible gaps in existing mitigations"
  - [section 1] "red teamers identified a category of visual synonyms that bypassed existing defenses designed to prevent the creation of sexually explicit content in DALL-E systems. OpenAI then reinforced the mitigations before deploying the system"
  - [corpus] ARM (arXiv:2510.02677) demonstrates that multimodal attacks using plug-and-play strategies expose VLM vulnerabilities, supporting the premise that systematic adversarial exploration finds gaps—though automation vs. human tradeoffs remain contested.
- Break condition: If mitigations are fundamentally robust (e.g., formal verification), or if red teamers lack sufficient time/access, residual risks remain undetected.

### Mechanism 3: Human Red Teaming Feeds Automated Evaluation Pipelines
- Claim: Qualitative findings from human red teaming provide seed data for constructing automated classifiers and benchmarks, enabling scalable ongoing monitoring.
- Mechanism: Red teamers generate prompt-output pairs labeled by risk category and severity. These become training or test data for classifiers (e.g., GPT-4-based content filters) that can then scale evaluation beyond manual effort.
- Core assumption: Human-judged adversarial examples generalize to future inputs; automated classifiers can capture the essence of what humans flagged.
- Evidence anchors:
  - [abstract] "facilitating the creation of new safety measurements"
  - [section 3] "Red teaming can lay the foundation for automated evaluations by discovering areas where investing in such evaluations can be valuable, providing data that can be used to pilot evaluations"
  - [section 3] "For DALL-E 3... GPT-4 was used to create synthetic data using red team prompts. Once we had a larger set of prompts, a GPT-4 classifier was used to ensure that the language model creating requests... either refused or sufficiently re-wrote any prompt that went against the desired policy"
  - [corpus] Corpus papers focus on attack methods rather than the evaluation-creation pipeline; evidence for this mechanism is primarily internal to OpenAI's described process.
- Break condition: If red team examples are idiosyncratic or non-representative, or if classifier generalization fails, automated evaluations propagate false negatives or positives.

## Foundational Learning

- **Threat Modeling**
  - Why needed here: Red teaming campaigns are scoped by prior threat modeling—identifying what risks to prioritize, for whom, and from what actors. Without this, efforts diffuse across too broad a surface.
  - Quick check question: Can you articulate a specific risk (what harm), to whom (stakeholder), and via what actor or mechanism before designing a test?

- **Adversarial Mindset**
  - Why needed here: Effective red teaming requires thinking like an attacker, not a user. This involves seeking edge cases, chaining exploits, and probing for policy violations rather than confirming expected behavior.
  - Quick check question: Given a safety filter, can you generate three distinct strategies a motivated actor might use to bypass it?

- **Evaluation Design Basics**
  - Why needed here: The paper emphasizes converting qualitative findings into repeatable evaluations. Understanding how to design metrics, classifiers, and benchmarks is prerequisite to operationalizing red teaming outputs.
  - Quick check question: Given a set of flagged prompt-response pairs, how would you define a binary classifier's positive/negative classes and validation set?

## Architecture Onboarding

- **Component map**: Scoping & Threat Modeling -> Cohort Selection -> Access Provisioning -> Interface & Instructions -> Execution -> Synthesis -> Evaluation Creation -> Feedback Loop
- **Critical path**: Scoping → Cohort Selection → Access Decision → Execution → Synthesis → Evaluation Creation. The synthesis-to-evaluation step determines whether the investment yields reusable infrastructure.
- **Design tradeoffs**:
  - Access level: Unmitigated models reveal base risks but may produce obsolete findings; mitigated models test real deployment but may hide underlying capability risks.
  - Cohort diversity vs. depth: Broader cohorts cover more surface; specialized cohorts probe deeper in high-risk domains.
  - Structured vs. exploratory testing: Structured yields comparable data; exploratory surfaces unknown unknowns.
  - Manual vs. automated red teaming: Manual excels at novelty; automated scales coverage (per Section 3 and Perez et al. reference).
- **Failure signatures**:
  - Findings are all known issues → cohort lacks diversity or independence
  - Findings cannot be actioned → no clear policy alignment or mitigation path
  - Automated evaluations diverge from human judgment → classifier training data non-representative or overfit
  - Red teamers report psychological harm → inadequate consent, support, or content exposure limits
  - Campaign produces no reusable evaluations → synthesis step under-resourced
- **First 3 experiments**:
  1. Run a small scoped red teaming exercise (5-10 domain experts) on a single capability (e.g., multi-turn dialogue) with structured documentation templates; measure what fraction of findings map to existing policies vs. require new policy decisions.
  2. Take the top 10 high-severity findings from experiment 1 and attempt to construct a binary classifier; evaluate precision/recall on held-out red team data to test generalization.
  3. Compare three access conditions (base model, partially mitigated, fully mitigated) for the same cohort; quantify how severity and category distributions shift to inform access-level tradeoffs for future campaigns.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific externally specified thresholds and practices effectively ensure accountability for risks discovered during red teaming?
  - Basis in paper: [explicit] Conclusion states red teaming "needs to be paired with externally specified thresholds and practices for accountability of discovered risks."
  - Why unresolved: The paper outlines the discovery process but notes the absence of standardized external mechanisms to mandate responses to findings.
  - What evidence would resolve it: Development of industry-wide or regulatory frameworks that define required actions (e.g., mitigation requirements) based on red team severity levels.

- **Open Question 2**: How can public perspectives on ideal model behavior be systematically solicited and incorporated into red teaming?
  - Basis in paper: [explicit] Conclusion notes "additional work is needed to solicit and incorporate public perspectives on ideal model behavior, policies, and other associated decision making processes."
  - Why unresolved: Current methodologies prioritize domain experts over general public input for policy and behavioral standards.
  - What evidence would resolve it: A validated methodology for integrating public deliberation into the definition of red teaming goals and acceptable model behaviors.

- **Open Question 3**: How must red teaming methodologies adapt when model capabilities outpace the human expertise required to accurately judge output risks?
  - Basis in paper: [explicit] Limitations section highlights an "Increase in human sophistication," noting a "higher threshold for knowledge humans need to possess to correctly judge the potential level of risk."
  - Why unresolved: Reliance on human verification becomes a bottleneck or failure point as model reasoning becomes more advanced or obscure.
  - What evidence would resolve it: Successful deployment of AI-assisted evaluation tools that can verify complex model outputs with superhuman accuracy.

## Limitations
- The paper describes a process rather than an empirical study, so evidence is primarily anecdotal without quantified success rates or false positive/negative data.
- The translation from red teaming findings to automated evaluations is asserted but not validated; it's unclear whether the classifiers derived from red team data generalize beyond the specific examples or suffer from overfitting.
- Resource intensity is noted as a limitation, but the paper lacks specific metrics on cost-benefit tradeoffs (e.g., hours spent per novel finding, number of participants required per campaign).

## Confidence
- **High confidence**: The mechanism that diverse domain experts can surface novel risks is well-supported by the paper's examples (voice generation, visual synonyms) and corroborated by external literature (arXiv:2501.07238).
- **Medium confidence**: The claim that adversarial probing identifies mitigation gaps is supported by specific examples (DALL-E 3 visual synonyms) but lacks quantitative measures of how often red teaming finds actionable bypasses versus known issues.
- **Medium confidence**: The translation of red teaming findings into automated evaluations is described in detail for DALL-E 3, but the generalization and performance of these automated systems are not empirically demonstrated.

## Next Checks
1. Conduct an independent red teaming campaign on a single capability using a structured cohort (5-10 domain experts) and measure the proportion of findings that are novel versus already known, and whether these findings map to existing policies or require new ones.
2. Using the top 10 high-severity findings from check 1, attempt to construct a binary classifier; evaluate its precision and recall on held-out red team data to test generalization beyond the specific examples.
3. Compare the effectiveness of three access conditions (base model, partially mitigated, fully mitigated) for the same cohort; quantify how severity and category distributions shift to inform access-level tradeoffs for future campaigns.