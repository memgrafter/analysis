---
ver: rpa2
title: 'CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text
  Classification'
arxiv_id: '2510.06532'
source_url: https://arxiv.org/abs/2510.06532
tags:
- quantum
- classical
- token
- claqs
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLAQS, a compact, fully quantum token mixer
  for text classification that jointly learns complex-valued mixing and nonlinear
  transformations within a unified quantum circuit. To enable stable end-to-end optimization,
  the authors apply L1 normalization to regulate amplitude scaling and introduce a
  two-stage parameterized quantum architecture that decouples shared token embeddings
  from a window-level quantum feed-forward module.
---

# CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text Classification

## Quick Facts
- arXiv ID: 2510.06532
- Source URL: https://arxiv.org/abs/2510.06532
- Reference count: 8
- Primary result: Achieves 91.64% accuracy on SST-2 and 87.08% on IMDB, outperforming both classical Transformers and hybrid quantum-classical models

## Executive Summary
CLAQS introduces a compact, fully quantum token mixer for text classification that jointly learns complex-valued mixing and nonlinear transformations within a unified quantum circuit. The authors address optimization stability through L1 normalization for amplitude scaling and a two-stage parameterized quantum architecture that separates shared token embeddings from window-level quantum feed-forward operations. Operating under a sliding-window regime with document-level aggregation, CLAQS requires only eight data qubits and shallow circuits while achieving state-of-the-art performance on binary sentiment classification tasks.

## Method Summary
The paper proposes a parameterized quantum circuit architecture that serves as an all-quantum token mixer for text classification. The key innovation involves a two-stage design: first, a shared ansatz circuit generates token embeddings, then a window-level quantum feed-forward module processes local context. L1 normalization is applied to regulate amplitude scaling during training, addressing stability issues in end-to-end optimization. The model operates under a sliding-window regime with document-level aggregation, enabling efficient processing of variable-length text inputs while maintaining a compact eight-qubit footprint.

## Key Results
- Achieves 91.64% accuracy on SST-2 binary sentiment classification
- Achieves 87.08% accuracy on IMDB sentiment classification
- Outperforms both classical Transformer baselines and strong hybrid quantum-classical approaches

## Why This Works (Mechanism)
CLAQS succeeds by leveraging quantum entanglement and interference patterns to capture complex token relationships that classical architectures struggle to represent efficiently. The complex-valued mixing allows simultaneous processing of token features in ways that mirror the multi-dimensional nature of language semantics. The two-stage architecture enables hierarchical learning where global token relationships are established first, then refined through local window-level processing. L1 normalization prevents amplitude explosion during optimization, maintaining stable gradient flows through the quantum circuit parameters.

## Foundational Learning

1. **Parameterized Quantum Circuits (PQCs)**
   - Why needed: Enables learning quantum gate parameters for specific tasks rather than using fixed quantum operations
   - Quick check: Understand how gate parameters map to unitary operations and gradient computation

2. **Quantum Entanglement in NLP**
   - Why needed: Captures non-classical correlations between tokens that exceed pairwise classical relationships
   - Quick check: Verify how entanglement is preserved through the circuit layers and impacts token mixing

3. **Amplitude Regularization**
   - Why needed: Prevents gradient vanishing/exploding in quantum circuits during backpropagation
   - Quick check: Confirm L1 normalization implementation and its effect on parameter update stability

4. **Sliding Window Quantum Processing**
   - Why needed: Enables efficient processing of variable-length sequences with fixed qubit resources
   - Quick check: Validate window overlap strategy and aggregation method for document-level predictions

5. **Complex-valued Quantum States**
   - Why needed: Provides richer representational capacity than real-valued classical embeddings
   - Quick check: Understand how complex amplitudes encode token features and relationships

## Architecture Onboarding

**Component Map:** Input Embeddings -> Shared Ansatz Circuit -> Window-level QFF -> Document Aggregation -> Classification

**Critical Path:** The shared ansatz generates token embeddings that flow through multiple window-level quantum feed-forward modules, with each window processing local context before aggregation for final classification. This hierarchical flow enables both global and local feature learning.

**Design Tradeoffs:** Eight-qubit compactness enables NISQ compatibility but limits direct processing of long sequences, necessitating the sliding window approach. The two-stage architecture trades some parameter efficiency for improved optimization stability. Complex-valued operations increase representational power but complicate classical-quantum interfacing.

**Failure Signatures:** Performance degradation typically manifests as gradient instability during training, particularly in early epochs. Token mixing quality can be assessed by monitoring entanglement entropy across the circuit. Window-level processing failures often appear as inconsistent local context capture across different sequence positions.

**First 3 Experiments:**
1. Verify L1 normalization effectiveness by training with and without amplitude regularization on a small validation set
2. Test entanglement preservation across circuit layers using quantum state tomography on validation data
3. Evaluate window overlap impact by varying stride lengths and measuring classification consistency

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations

- Empirical scope restricted to binary sentiment classification, leaving multi-class and multi-label generalization unproven
- Limited theoretical analysis of why L1 normalization and two-stage architecture specifically improve stability beyond standard regularization
- Absence of hardware-level considerations including error rates, decoherence effects, and compilation overheads for NISQ devices

## Confidence

| Claim | Confidence |
|-------|------------|
| Binary sentiment classification performance | High |
| General text classification applicability | Medium |
| Optimization stability mechanisms | Medium |
| Near-term hardware feasibility | Low |

## Next Checks

1. Evaluate CLAQS on multi-class text classification benchmarks (e.g., AG News, DBpedia) and on datasets with longer document lengths to assess generalization beyond binary sentiment analysis.

2. Conduct ablation studies isolating the contributions of L1 normalization and the two-stage architecture to determine whether these mechanisms provide benefits beyond standard regularization techniques.

3. Implement and benchmark CLAQS on quantum simulators that incorporate realistic noise models (e.g., depolarizing channels, readout errors) to assess performance degradation under hardware-like conditions.