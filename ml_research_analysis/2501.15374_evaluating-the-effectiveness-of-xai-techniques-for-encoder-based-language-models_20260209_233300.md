---
ver: rpa2
title: Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models
arxiv_id: '2501.15374'
source_url: https://arxiv.org/abs/2501.15374
tags:
- scores
- techniques
- across
- methods
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a comprehensive evaluation framework for\
  \ assessing eXplainable AI (XAI) techniques on encoder-based language models. The\
  \ framework integrates four metrics\u2014Human-reasoning Agreement (HA), Robustness,\
  \ Consistency, and Contrastivity\u2014to evaluate six XAI methods across five models\
  \ of varying complexity using two text classification datasets."
---

# Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models

## Quick Facts
- arXiv ID: 2501.15374
- Source URL: https://arxiv.org/abs/2501.15374
- Authors: Melkamu Abay Mersha; Mesay Gemeda Yigezu; Jugal Kalita
- Reference count: 40
- One-line primary result: Comprehensive evaluation framework reveals LIME excels in human agreement, AMV in robustness, and LRP in contrastivity across encoder-based language models

## Executive Summary
This paper introduces a novel evaluation framework for eXplainable AI (XAI) techniques applied to encoder-based language models. The framework integrates four metrics—Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity—to comprehensively assess six XAI methods across five models of varying complexity using two text classification datasets. The study finds that no single XAI technique dominates across all metrics, but LIME-based model simplification consistently achieves high performance across multiple dimensions, while Attention Mechanism Visualization (AMV) excels in robustness and consistency. Layer-wise Relevance Propagation (LRP) shows particular strength in contrastivity, especially for complex models.

The research demonstrates that LIME achieves remarkable human-reasoning agreement (0.9685 on DeBERTa-xlarge) while maintaining robustness and consistency, making it a reliable choice for practical applications. AMV shows exceptional robustness scores (as low as 0.0020) and perfect consistency (0.9999), though it may not generalize across all model types. The framework enables nuanced evaluation of XAI methods beyond traditional accuracy metrics, supporting more informed selection of explanation techniques based on specific application requirements and model characteristics.

## Method Summary
The paper presents a comprehensive evaluation framework that assesses XAI techniques through four distinct metrics integrated into a unified scoring system. The framework evaluates six XAI methods—Attention Mechanism Visualization (AMV), Layer-wise Relevance Propagation (LRP), Integrated Gradients (IG), Occlusion, SHAP, and LIME—across five encoder-based language models of varying complexity (BERT-base, BERT-large, RoBERTa-base, DeBERTa-base, and DeBERTa-xlarge) using two text classification datasets (SST-2 and AG-News). Human-reasoning agreement is measured against annotated rationale datasets, while robustness evaluates sensitivity to input perturbations, consistency measures agreement between similar inputs, and contrastivity assesses ability to distinguish between classes. The evaluation provides both absolute scores and relative rankings to enable comprehensive comparison across different model architectures and XAI techniques.

## Key Results
- LIME achieves highest Human-reasoning Agreement (0.9685 on DeBERTa-xlarge) and consistently strong performance across robustness and consistency metrics
- AMV demonstrates exceptional robustness (as low as 0.0020) and perfect consistency (0.9999) scores across tested models
- LRP excels in contrastivity metric, achieving up to 0.9371 for complex models, particularly effective at highlighting distinguishing features
- No single XAI technique dominates across all metrics, highlighting the need for task-specific selection of explanation methods
- Model simplification-based approaches (LIME) generally outperform gradient-based methods (IG, Occlusion) in human agreement metrics

## Why This Works (Mechanism)
None

## Foundational Learning

**Encoder-based Language Models**
*Why needed:* Foundation for understanding which models are being evaluated and their architectural constraints
*Quick check:* Can identify key components like attention mechanisms, transformer layers, and self-attention in encoder models

**XAI Technique Categories**
*Why needed:* Understanding the fundamental approaches (gradient-based, perturbation-based, attention-based) that underlie the six evaluated methods
*Quick check:* Can distinguish between gradient-based methods (IG, Occlusion), perturbation methods (SHAP, LIME), and attention-based visualization

**Evaluation Metrics Framework**
*Why needed:* Comprehension of the four-metric system (HA, Robustness, Consistency, Contrastivity) used to assess XAI effectiveness
*Quick check:* Can explain how each metric measures different aspects of explanation quality and reliability

**Text Classification Task Structure**
*Why needed:* Context for understanding the specific evaluation scenario and dataset characteristics
*Why needed:* Provides baseline for interpreting performance differences between XAI techniques

**Human Rationale Annotation**
*Why needed:* Understanding the ground truth used for Human-reasoning Agreement evaluation
*Quick check:* Can explain how human-annotated rationales serve as reference for evaluating explanation quality

## Architecture Onboarding

**Component Map:**
Human Annotation Dataset -> XAI Methods (AMV, LRP, IG, Occlusion, SHAP, LIME) -> Evaluation Metrics (HA, Robustness, Consistency, Contrastivity) -> Model Types (BERT-base, BERT-large, RoBERTa-base, DeBERTa-base, DeBERTa-xlarge) -> Text Classification Datasets (SST-2, AG-News)

**Critical Path:**
Model Input → XAI Method → Explanation Output → Metric Calculation → Performance Score

**Design Tradeoffs:**
- Computational efficiency vs explanation quality (gradient methods faster but less accurate)
- Granularity of explanations vs interpretability (fine-grained vs coarse explanations)
- Model complexity vs XAI method effectiveness (simpler models may yield better explanations)

**Failure Signatures:**
- Low HA scores indicate poor alignment with human reasoning
- High robustness scores suggest explanations are too sensitive to input perturbations
- Low consistency scores indicate explanations vary significantly for similar inputs
- Poor contrastivity suggests inability to distinguish between classes

**First 3 Experiments:**
1. Compare HA scores of LIME across all five model types to establish baseline performance
2. Evaluate robustness of AMV on BERT-base with varying input perturbation magnitudes
3. Test contrastivity of LRP on complex vs simple models to verify complexity dependency

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to two text classification datasets (SST-2 and AG-News), restricting generalizability to other NLP tasks
- Exclusive focus on encoder-based models leaves uncertainty about performance with decoder-only or encoder-decoder architectures
- Human-reasoning agreement relies on subjective human rationale annotations that may not capture diverse interpretation patterns
- Does not address computational efficiency or runtime overhead of different XAI methods for practical deployment

## Confidence
- **High confidence**: LIME's superior performance across HA, robustness, and consistency metrics (based on consistent results across multiple models and datasets)
- **Medium confidence**: AMV's robustness and consistency performance (limited to specific model types and may not generalize)
- **Medium confidence**: LRP's contrastivity performance (highly dependent on model complexity and specific evaluation setup)

## Next Checks
1. Test the evaluation framework across additional NLP tasks (e.g., sentiment analysis with different datasets, question answering, text summarization) to assess generalizability beyond text classification.
2. Conduct ablation studies to isolate the contribution of each evaluation metric (HA, robustness, consistency, contrastivity) to overall XAI method rankings.
3. Evaluate computational overhead and runtime efficiency of each XAI technique across different model sizes to provide practical deployment guidance.