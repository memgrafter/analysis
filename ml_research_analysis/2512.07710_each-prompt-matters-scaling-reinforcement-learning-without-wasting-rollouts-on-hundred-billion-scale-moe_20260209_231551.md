---
ver: rpa2
title: 'Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts
  on Hundred-Billion-Scale MoE'
arxiv_id: '2512.07710'
source_url: https://arxiv.org/abs/2512.07710
tags:
- reward
- reasoning
- training
- learning
- rollout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles inefficiencies in scaling reinforcement learning\
  \ (RL) for hundred-billion-parameter Mixture-of-Experts (MoE) models, specifically\
  \ addressing zero-variance prompts that waste rollouts, unstable importance sampling,\
  \ and train\u2013inference discrepancies. To solve these, the authors propose a\
  \ unified RL framework that includes (1) Multi-Stage Zero-Variance Elimination to\
  \ prune non-informative prompts and enhance GRPO stability; (2) Entropy Importance\
  \ Sampling Policy Optimization (ESPO) to adaptively balance token- and sequence-level\
  \ sampling; (3) Router Replay to align training and inference router decisions;\
  \ (4) a high-throughput RL system with FP8 rollouts, overlapped reward computation,\
  \ and length-aware scheduling."
---

# Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE

## Quick Facts
- arXiv ID: 2512.07710
- Source URL: https://arxiv.org/abs/2512.07710
- Authors: Anxiang Zeng; Haibo Zhang; Hailing Zhang; Kaixiang Mo; Liang Yao; Ling Hu; Long Zhang; Shuman Liu; Shuyi Xie; Yanshi Li; Yizhang Chen; Yuepeng Sheng; Yuwei Huang; Zhaochen Xu; Zhiqiang Zhou; Ziqin Liew
- Reference count: 3
- One-line primary result: Achieves SEA average 86.41 across seven languages and e-commerce domain average 85.79 with CompassMax-V3-Thinking via efficient hundred-billion-parameter MoE RL

## Executive Summary
This work addresses inefficiencies in scaling reinforcement learning (RL) for hundred-billion-parameter Mixture-of-Experts (MoE) models, specifically tackling zero-variance prompts, unstable importance sampling, and train–inference discrepancies. The authors propose a unified RL framework that includes Multi-Stage Zero-Variance Elimination, Entropy Importance Sampling Policy Optimization (ESPO), Router Replay, and a high-throughput RL system. Empirically, the resulting CompassMax-V3-Thinking model shows strong performance, with a SEA average of 86.41 across seven languages, an in-house e-commerce domain average of 85.79, and a general ability average of 76.01. These results demonstrate that the proposed methods enable stable, efficient RL training at scale, yielding both higher reasoning quality and production-level robustness.

## Method Summary
The method involves a multi-stage RL pipeline for a hundred-billion-parameter MoE reasoning model. First, cold-start SFT is performed on Long-CoT data distilled from DeepSeek-R1-0528, with deduplication and filtering. Next, model merging with TIES strategy combines reasoning and domain-specialized models. Two-phase RL follows: reasoning RL optimizes GRPO with Multi-Stage Zero-Variance Elimination (filtering non-informative prompts), ESPO (entropy-adaptive token grouping), and Router Replay (aligning MoE router decisions between rollout and training). Finally, domain expansion RL fine-tunes with GenRM and ternary classification. The system leverages FP8 rollouts, overlapped reward computation, and length-aware scheduling for throughput.

## Key Results
- CompassMax-V3-Thinking achieves SEA average 86.41 across seven languages.
- In-house e-commerce domain average is 85.79.
- General ability average is 76.01, demonstrating strong multi-domain reasoning.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Stage Zero-Variance Elimination
Filtering or reshaping prompts that yield uniform rewards prevents wasted rollouts and stabilizes group-based policy optimization by removing wasted rollouts. The framework identifies "Zero-Variance Prompts" (ZVP) where all samples in a group receive identical rewards (e.g., all wrong). Instead of discarding these expensive rollouts, the system (1) increases the sampling size N to expand the exploration space, and (2) injects penalties (length, repetition) to reshape the reward landscape, ensuring non-zero advantage. Core assumption: High-variance groups provide a stronger learning signal than uniform groups, and the cost of generating extra samples (N) is lower than the cost of discarding entire batches. Evidence anchors: [abstract]: "Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization... by removing wasted rollouts." [section 2.4.1]: "Expanding the exploration space reduced the overall zero-variance rate by 17%... preventing wasted computation." [corpus]: Neighbor paper "Act Only When It Pays" supports the concept of selective rollouts, reinforcing that not all prompts yield equal value. Break condition: If the reward model lacks granularity to distinguish subtle differences in "all wrong" answers, increasing N may not resolve variance, leading to compute waste without convergence gains.

### Mechanism 2: Entropy Importance Sampling Policy Optimization (ESPO)
Entropy-weighted token grouping stabilizes importance sampling ratios better than uniform sequence-level or token-level approaches. ESPO groups tokens by entropy within a sequence. High-entropy tokens (uncertain regions) are assigned distinct importance sampling ratios and clipping bounds, whereas low-entropy tokens are grouped differently. This prevents the "under-updating" of exploratory regions often masked by uniform averaging in GSPO. Core assumption: High-entropy tokens are the primary drivers of reasoning capability improvements ("80/20 rule"), and isolating them prevents gradient drowning from low-entropy filler tokens. Evidence anchors: [abstract]: "...ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling..." [section 2.4.2]: "ESPO decomposes each sequence into entropy-coherent token groups... preventing under-updating of exploratory regions." [corpus]: "XRPO" discusses pushing GRPO limits with exploration, aligning with the need for better handling of high-uncertainty areas. Break condition: If the model exhibits pathological entropy (e.g., high entropy on noise/gibberish rather than reasoning steps), ESPO may amplify noise gradients, destabilizing training.

### Mechanism 3: Router Replay for Train-Infer Alignment
Aligning MoE router decisions between the inference engine (rollout) and training engine (update) prevents policy collapse caused by numerical precision mismatch. During rollout (e.g., using vLLM), the system logs the specific expert router decisions for every token. During the training update (e.g., using Megatron), these specific router decisions are forced ("replayed") rather than re-computed. This decouples the routing logic from floating-point discrepancies between frameworks. Core assumption: The log-probability discrepancy is dominated by router divergence rather than other numerical differences, and the inference engine's routing is "gold standard" for the current policy. Evidence anchors: [abstract]: "...Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior..." [section 2.4.4]: "...router decisions for all tokens are recorded during the vLLM rollout phase and directly reused... reducing the discrepancy from the order of 10^-3 to 10^-4." [corpus]: "Diagnosing and Mitigating System Bias in Self-Rewarding RL" highlights system bias issues, validating the need for such alignment. Break condition: If the model weights diverge significantly between steps, "replaying" old router decisions might be stale, leading to inconsistent gradient estimates.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** The entire framework builds upon GRPO, which estimates advantages based on group rewards rather than a separate value model.
  - **Quick check question:** How does GRPO calculate the advantage for a specific response if all other responses in the group have the same reward? (Answer: It becomes zero; hence the ZVP problem).

- **Concept: Importance Sampling Ratios**
  - **Why needed here:** ESPO relies on manipulating the probability ratio $\frac{\pi_\theta}{\pi_{\theta_{old}}}$ to correct for off-policy drift. Understanding clipping and variance reduction is crucial.
  - **Quick check question:** Why does a high importance sampling ratio indicate potential training instability? (Answer: It implies the current policy is taking actions with much higher probability than the behavior policy, leading to high variance gradients).

- **Concept: Mixture of Experts (MoE) Routing**
  - **Why needed here:** The "Router Replay" mechanism assumes knowledge of how tokens are routed to specific experts via a gating network.
  - **Quick check question:** What happens to the gradient flow if the router selects a different expert during the backward pass (training) than it did during the forward pass (inference)?

## Architecture Onboarding

- **Component map:** Input -> Rollout Engine (vLLM) -> Reward System (Compass-Gym) -> Training Engine (Megatron)
- **Critical path:** The synchronization between the Rollout Engine's router logs and the Training Engine's forward pass. If this data transfer or alignment fails, the theoretical guarantees of the stability mechanism collapse.
- **Design tradeoffs:**
  - **FP8 Precision:** Maximizes throughput but increases numerical noise (mitigated by Router Replay).
  - **Entropy Grouping:** Improves learning signal but adds complexity to the CUDA kernels for variable grouping.
- **Failure signatures:**
  - **Sudden Loss Spike:** Often caused by advantage inversion or exploding importance sampling ratios (check ESPO clipping bounds).
  - **Stagnant Reward:** Check Zero-Variance rates; if too high, increase N or check Reward Model resolution.
- **First 3 experiments:**
  1. **Ablate Router Replay:** Train with and without logging router decisions to measure the divergence in log-probabilities and training stability.
  2. **ESPO vs. GSPO:** Compare convergence speed on a reasoning benchmark using uniform sequence sampling vs. entropy-grouped sampling.
  3. **ZVP Stress Test:** Curate a dataset of "very easy" or "very hard" prompts prone to zero variance and validate the efficiency of the Multi-Stage Elimination pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-turn agentic reasoning performance be improved, given that BFCL_MULTI_TURN_LIVE scores (19.50) significantly lag behind single-turn agent tasks (79.57–83.73)?
- Basis in paper: [explicit] Table 5 shows the multi-turn agent benchmark score is markedly lower than other BFCL variants, and the authors note only that the model "performs robustly across BFCL variants (notably improving the multi-turn setting to 19.50)" without explaining the gap.
- Why unresolved: The paper does not analyze why multi-turn planning degrades, nor propose targeted interventions for longer tool chains.
- What evidence would resolve it: Ablation studies isolating context length, state tracking, or reward sparsity in multi-turn settings; comparisons of alternative reward shaping strategies for extended trajectories.

### Open Question 2
- Question: What is the optimal weight allocation strategy when merging long-CoT reasoning models with domain-specialized MoE models, and how does this vary across architecture scales?
- Basis in paper: [explicit] The authors state "our experiments revealed that, during merging, the weight of the long-CoT model affect the merging result significantly" and that this work "represents the first systematic exploration of model merging between distinct reasoning modes in large-scale MoE architectures."
- Why unresolved: Only the TIES strategy was evaluated as most consistent; systematic exploration of weight interpolation ratios across different MoE scales is not provided.
- What evidence would resolve it: Grid search or learned interpolation studies over merging weights for multiple model size pairs, with downstream task performance curves.

### Open Question 3
- Question: How sensitive is ESPO's entropy-adaptive clipping threshold α to model scale, vocabulary size, and task domain, and what principled methods exist for setting it?
- Basis in paper: [inferred] Equation 3 defines α as "a global scaling factor" without calibration guidance; no ablation is reported for different α values or transfer across domains.
- Why unresolved: The paper demonstrates ESPO improves over GSPO but does not establish whether α requires retuning for new model configurations.
- What evidence would resolve it: Ablation experiments varying α across multiple model sizes (e.g., 7B to 500B+ parameters) and task distributions, reporting stability metrics and final performance.

### Open Question 4
- Question: Can the residual ~5% zero-variance prompt rate be further reduced without introducing gradient noise or harming convergence?
- Basis in paper: [explicit] The paper states "Even after the above optimizations, approximately 5% of sample groups still exhibited zero-variance rewards" and integrates RL-ZVP to reshape advantage.
- Why unresolved: RL-ZVP injects controlled stochasticity as a mitigation, but whether alternative data filtering, reward designs, or exploration strategies could lower the residual is unexplored.
- What evidence would resolve it: Systematic comparison of curriculum-based prompt selection, dynamic temperature schedules, or reward ensemble variance against the current RL-ZVP baseline.

## Limitations
- **MoE Architecture Dependence:** The Router Replay mechanism's effectiveness is tightly coupled to the specific MoE routing implementation.
- **Reward Model Resolution:** The ZVP elimination assumes the reward model can provide granular enough signals to prevent zero-variance groups.
- **Entropy as a Proxy:** ESPO's reliance on entropy to identify high-value tokens assumes entropy correlates with reasoning complexity.

## Confidence
- **High Confidence:** The empirical results showing CompassMax-V3-Thinking's performance gains (SEA 86.41, e-commerce 85.79, general 76.01) are well-supported by the presented data and methodology.
- **Medium Confidence:** The theoretical mechanisms (ZVP elimination, ESPO, Router Replay) are logically sound and partially supported by ablation-style reasoning, but full experimental validation of each component's isolated impact is limited.
- **Low Confidence:** The scalability claims to "hundred-billion-scale" MoE models are asserted but not rigorously tested beyond the single CompassMax-V3-Thinking case study.

## Next Checks
1. **Router Replay Ablation:** Systematically train CompassMax-V3-Thinking with and without Router Replay enabled, measuring the training-inference log-probability divergence and final benchmark performance to isolate its contribution.
2. **ESPO vs. GSPO Stress Test:** Design a synthetic reasoning dataset where high-entropy tokens are known to be critical (e.g., deliberate reasoning steps vs. filler text) and compare ESPO's convergence and final accuracy against standard GSPO.
3. **ZVP Elimination Efficiency Audit:** Create a controlled set of prompts guaranteed to produce zero-variance rewards (e.g., all correct or all incorrect answers) and measure the computational overhead and success rate of the Multi-Stage Elimination pipeline in resolving them.