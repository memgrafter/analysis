---
ver: rpa2
title: Fun-ASR Technical Report
arxiv_id: '2509.12508'
source_url: https://arxiv.org/abs/2509.12508
tags:
- data
- fun-asr
- speech
- audio
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fun-ASR is a large-scale, LLM-based automatic speech recognition
  system designed to achieve state-of-the-art performance across diverse and complex
  speech recognition scenarios. It combines massive data scaling, model size scaling,
  LLM integration, and reinforcement learning to address challenges in streaming capability,
  noise robustness, code-switching, and hotword customization.
---

# Fun-ASR Technical Report

## Quick Facts
- **arXiv ID:** 2509.12508
- **Source URL:** https://arxiv.org/abs/2509.12508
- **Reference count:** 8
- **Primary result:** Large-scale LLM-based ASR achieving state-of-the-art performance on diverse real-world datasets with up to 9.2% relative improvement in streaming scenarios

## Executive Summary
Fun-ASR is a large-scale, LLM-based automatic speech recognition system designed to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. It combines massive data scaling, model size scaling, LLM integration, and reinforcement learning to address challenges in streaming capability, noise robustness, code-switching, and hotword customization. Experimental results show that Fun-ASR outperforms both open-source and commercial ASR systems on real-world industry datasets, achieving relative improvements of up to 9.2% in streaming scenarios and over 30% in noise robustness under challenging conditions. The system demonstrates superior accuracy and robustness in practical settings, bridging the gap between academic research and commercial deployment.

## Method Summary
Fun-ASR employs a multi-stage training pipeline combining cross-modal encoder initialization, supervised fine-tuning, and reinforcement learning. The audio encoder is initialized with weights from a pre-trained text LLM (Qwen3 layers) before self-supervised pre-training using the Best-RQ framework. The system then undergoes four stages of supervised fine-tuning: adaptor training, encoder-encoder training, LLM training with LoRA, and full joint training. Finally, reinforcement learning with a composite reward function (GRPO) optimizes for accuracy, keyword recall, and hallucination suppression. The architecture features a dual-decoder design with a CTC decoder for hotword retrieval and an LLM decoder for final transcription, enabling dynamic vocabulary injection without retraining.

## Key Results
- Achieves state-of-the-art performance on industry datasets with up to 9.2% relative improvement in streaming scenarios
- Demonstrates over 30% improvement in noise robustness compared to commercial systems
- Successfully handles code-switching and hotword customization through LLM-based architecture
- Supports streaming recognition with latency under 300ms and context windows up to 5 minutes

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Encoder Initialization
Initializing the audio encoder with weights from a pre-trained text LLM (Qwen3 layers) provides a stronger semantic prior than random initialization. The linguistic and semantic knowledge encoded in text LLM layers offers a beneficial inductive bias for speech representations, allowing the encoder to converge faster and learn higher-quality features by mapping acoustic signals into a semantically rich space.

### Mechanism 2: Compound GRPO for Hallucination Suppression
Reinforcement Learning with a composite reward function (merging accuracy, keyword recall, and hallucination penalties) reduces critical failure modes better than optimizing for Word Error Rate alone. By penalizing outputs where the language does not match the source or where text appears during silence, the model learns to gate its generation based on acoustic confidence.

### Mechanism 3: Dual-Decoder Hotword Retrieval (RAG)
Decoupling a fast CTC decoder from the main LLM decoder enables efficient, dynamic vocabulary injection without retraining. The system uses a lightweight CTC head to generate an initial hypothesis, calculates phoneme/word-piece edit distance to a hotword database, and injects retrieved hotwords into the LLM prompt to bias final generation.

## Foundational Learning

### Concept: Best-RQ (BERT-based Speech pre-Training with Random-projection Quantizer)
- **Why needed:** The audio encoder is pre-trained using this self-supervised method before LLM integration stage
- **Quick check:** How does the quantization module in Best-RQ differ from standard K-Means clustering in audio discretization?

### Concept: GRPO (Group Relative Policy Optimization)
- **Why needed:** This is the core RL algorithm (FunRL) for hallucination suppression
- **Quick check:** Why does GRPO compare rewards within a group of responses rather than against a global baseline?

### Concept: LoRA (Low-Rank Adaptation)
- **Why needed:** Used extensively in SFT stages to fine-tune the LLM while preserving pre-trained knowledge
- **Quick check:** In Fun-ASR Stage 3, which components are frozen versus trained via LoRA, and why is the encoder updated separately before this?

## Architecture Onboarding

### Component map:
Raw Audio -> Audio Encoder (0.2B or 0.7B Transformer, Best-RQ + Text-LLM init) -> Adaptor (2-layer Transformer) -> LLM (0.6B or 7B Qwen-based)

### Critical path:
The SFT Stage 4 (Full-parameter fine-tuning) is the most sensitive step. It combines high-quality data filtering (evaluated by 3 models) with simultaneous updates to the encoder/adaptor and LoRA updates to the LLM. A misconfiguration here easily destabilizes the LLM's language capabilities.

### Design tradeoffs:
- Fun-ASR vs. Fun-ASR-nano: Trades ~2-4% absolute WER increase for ~10x reduction in parameters (7.7B -> 0.8B) for edge deployment
- Context Window: Supports up to 5-minute audio by segmenting and feeding previous transcripts as context, trading memory/compute for long-term dependency tracking

### Failure signatures:
- Hallucination loops: In noisy environments, the LLM may generate text not present in audio
- Streaming Latency: If chunk size in streaming mode is too large, "real-time" feel breaks; if too small, semantic context is lost

### First 3 experiments:
1. Encoder Initialization Ablation: Train two encoders (one random init, one Text-LLM init) on same Best-RQ corpus and plot convergence speed and WER on clean test set
2. RL Reward Sensitivity: Isolate "Hallucination Reward" (R3). Train model with R3=0 and compare spurious text generation rate on "supermarket" noise test set
3. Hotword Retrieval Stress Test: Inject hotwords with high phonetic similarity and measure CTC retrieval recall vs final LLM transcription accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Fun-ASR be modified to robustly handle long-duration recordings without relying on external Voice Activity Detection (VAD) modules?
- **Basis:** Section 7 states that the effective context window is constrained and the system struggles to handle long-duration recordings robustly without an external VAD module
- **Why unresolved:** Current LLM-based architectures typically struggle with extended context lengths due to computational complexity and attention mechanism limits
- **What evidence would resolve it:** Evaluation results on continuous, unsegmented long-form audio datasets showing stable WER performance without external segmentation

### Open Question 2
- **Question:** What architectural or training modifications are necessary to extend Fun-ASR to support far-field and multi-channel audio inputs?
- **Basis:** Section 7 explicitly notes that the current release does not support far-field or multi-channel audio
- **Why unresolved:** The current audio encoder is likely optimized for single-channel near-field signals and lacks spatial processing capabilities required for distant speech recognition
- **What evidence would resolve it:** Benchmark performance comparisons on far-field corpora demonstrating native multi-channel processing capabilities

### Open Question 3
- **Question:** Can the production-oriented optimizations (streaming, hotword customization) be effectively scaled to low-resource languages beyond the current 31 supported languages?
- **Basis:** Section 7 highlights that the model is primarily optimized for Chinese and English, and support for other languages remains limited
- **Why unresolved:** Resource-rich languages have abundant data while others have limited resources, making advanced feature transfer difficult
- **What evidence would resolve it:** Experiments showing successful hotword recall and streaming latency metrics for low-resource languages using the multilingual Fun-ASR-ML model

## Limitations
- Limited support for far-field and multi-channel audio inputs
- Context window constraints make long-duration recordings challenging without external VAD
- Primarily optimized for Chinese and English, with limited support for other languages
- Proprietary industry test sets prevent independent verification of performance claims

## Confidence

**High Confidence:** Fundamental architectural approach (LLM-based ASR with multi-stage training pipeline) and performance improvements on standard datasets

**Medium Confidence:** Specific relative improvements (9.2% streaming, 30% noise robustness) depend on undisclosed industry dataset characteristics

**Low Confidence:** Cross-modal initialization mechanism effectiveness and real-time streaming performance claims lack sufficient implementation details

## Next Checks

1. **Cross-Modal Initialization Ablation:** Train two identical Fun-ASR encoders—one with Qwen3 initialization and one with random initialization—on the same Best-RQ corpus. Measure and compare convergence speed and final WER on a clean test set to directly validate the initialization hypothesis.

2. **Hallucination Suppression Isolation:** Create a controlled test set with silence/noise-only segments and clean speech. Train two Fun-ASR models: one with the full GRPO reward function and one with hallucination penalties disabled. Compare hallucination rates and overall WER to quantify the specific impact of the hallucination suppression mechanism.

3. **Industry Dataset Validation:** Request access to anonymized industry test sets or equivalent publicly available datasets with similar characteristics. Re-run the Fun-ASR evaluation pipeline on these external datasets to verify claimed performance improvements are reproducible across different data distributions.