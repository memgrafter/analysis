---
ver: rpa2
title: 'SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning'
arxiv_id: '2511.09681'
source_url: https://arxiv.org/abs/2511.09681
tags:
- learning
- seba
- visual
- attack
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEBA is a sample-efficient black-box adversarial attack framework
  for visual reinforcement learning. It combines a shadow Q model to estimate victim
  rewards under perturbations, a GAN-based generator for producing imperceptible perturbations,
  and a world model to simulate environment dynamics and reduce real-world queries.
---

# SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.09681
- Source URL: https://arxiv.org/abs/2511.09681
- Reference count: 40
- Primary result: Achieves adversarial attacks on visual RL with 160K environment queries vs 4M for baselines

## Executive Summary
SEBA introduces a sample-efficient black-box adversarial attack framework for visual reinforcement learning that addresses the high query cost of existing methods. The framework combines a shadow Q model to estimate victim rewards, a GAN-based generator for creating imperceptible perturbations, and a world model to simulate environment dynamics. By alternating between training the shadow critic and refining the generator, SEBA achieves effective attacks with dramatically fewer environment queries while maintaining visual imperceptibility.

## Method Summary
SEBA operates through a two-stage alternating optimization process. First, a shadow Q network is trained to predict victim rewards under perturbations. Second, the generator is refined using the fixed shadow critic's supervision to produce adversarial perturbations. A world model simulates environment dynamics to reduce real-world queries, and the GAN-based generator ensures perturbations remain visually imperceptible. The framework demonstrates effectiveness across MuJoCo continuous control tasks and Atari benchmarks while maintaining significantly lower query complexity compared to existing methods.

## Key Results
- On MuJoCo tasks, reduces environment queries from 4M to 160K while maintaining attack effectiveness
- Achieves cumulative rewards as low as 1.61 vs 150.72 for PGD with FID of 62.43
- On Atari benchmarks, reduces queries from 2M to 80K while maintaining strong performance (e.g., Alien reward from 8858 to 982)

## Why This Works (Mechanism)
SEBA's effectiveness stems from its three-component architecture that addresses the fundamental challenge of sample efficiency in black-box attacks. The shadow Q model provides reward estimation without requiring direct access to victim policies, the world model reduces costly real-world interactions by simulating dynamics, and the GAN-based generator ensures perturbations remain imperceptible. The alternating optimization between shadow critic training and generator refinement creates a feedback loop that progressively improves attack effectiveness while maintaining visual quality. This combination allows SEBA to achieve comparable attack performance to white-box methods with dramatically fewer queries.

## Foundational Learning

**Shadow Q Networks**: Why needed - To estimate victim rewards under perturbations without access to victim policies. Quick check - Can the shadow critic accurately predict rewards across different perturbation types?

**World Models**: Why needed - To simulate environment dynamics and reduce real-world queries. Quick check - Does the world model accurately capture state transitions and reward distributions?

**GAN-based Generators**: Why needed - To produce adversarial perturbations that remain visually imperceptible. Quick check - Does the generator maintain low FID scores while producing effective attacks?

**Alternating Optimization**: Why needed - To balance shadow critic accuracy and generator effectiveness. Quick check - Does the two-stage process converge to effective attacks within reasonable query budgets?

## Architecture Onboarding

**Component Map**: World Model -> Shadow Q Network -> GAN Generator -> Environment -> Feedback to Shadow Q

**Critical Path**: Generator perturbation → Environment interaction → Shadow Q reward estimation → Generator update

**Design Tradeoffs**: Balances query efficiency against attack effectiveness and visual imperceptibility; chooses alternating optimization over end-to-end training to maintain stability

**Failure Signatures**: High FID scores indicate visible perturbations; large discrepancy between shadow and actual rewards indicates poor critic estimation; high query counts suggest world model inaccuracy

**First Experiments**: 1) Test shadow Q accuracy on perturbed states, 2) Validate world model transition predictions, 3) Verify generator produces perturbations with low FID scores

## Open Questions the Paper Calls Out

None

## Limitations

- Limited evaluation to continuous control and Atari games, leaving generalizability to other RL domains uncertain
- No exploration of effectiveness against adaptive defenses or victim models with adversarial training
- Lacks extensive user studies to confirm practical imperceptibility of generated perturbations

## Confidence

High for sample efficiency and query reduction claims
Medium for visual imperceptibility metrics
Low for generalizability to other RL domains and robustness against adaptive defenses

## Next Checks

1. Test SEBA on a broader range of RL environments including sparse-reward tasks and multi-agent scenarios to assess generalizability
2. Evaluate SEBA's effectiveness against common adversarial defenses and adaptive victim models
3. Conduct user studies or perceptual experiments to validate the claim of imperceptible perturbations in real-world viewing conditions