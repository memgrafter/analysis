---
ver: rpa2
title: Learning with Expert Abstractions for Efficient Multi-Task Continuous Control
arxiv_id: '2503.14809'
source_url: https://arxiv.org/abs/2503.14809
tags:
- learning
- policy
- reward
- abstractions
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Goal-Conditioned Reward Shaping (GCRS), a
  hierarchical reinforcement learning approach that leverages expert-defined abstractions
  to enable efficient learning in continuous multi-task environments with sparse rewards.
  The method dynamically plans over an expert-provided abstract model to generate
  subgoals, which are then used to condition a goal-conditioned policy.
---

# Learning with Expert Abstractions for Efficient Multi-Task Continuous Control

## Quick Facts
- arXiv ID: 2503.14809
- Source URL: https://arxiv.org/abs/2503.14809
- Authors: Jeff Jewett; Sandhya Saisubramanian
- Reference count: 17
- Key outcome: GCRS achieves over 90% success rate on harder evaluation tasks and 60% success on zero-shot generalization to new object colors in the ObjectDelivery task, outperforming baseline hierarchical RL methods

## Executive Summary
This paper introduces Goal-Conditioned Reward Shaping (GCRS), a hierarchical reinforcement learning approach that leverages expert-defined abstractions to enable efficient learning in continuous multi-task environments with sparse rewards. The method dynamically plans over an expert-provided abstract model to generate subgoals, which are then used to condition a goal-conditioned policy. GCRS employs potential-based reward shaping using the optimal state values from the abstract model to address the challenges of sparse rewards.

The approach is evaluated on procedurally generated continuous control environments, including navigation and object manipulation tasks. GCRS demonstrates superior sample efficiency, task completion rates, scalability to complex tasks, and zero-shot generalization to novel scenarios compared to existing hierarchical reinforcement learning methods.

## Method Summary
GCRS is a hierarchical RL framework that combines expert-provided abstract models with reward shaping. The method first learns a goal-conditioned policy in an abstract state space provided by an expert. This abstract model is used to compute potential-based shaping rewards that guide the learning of a low-level policy in the continuous environment. The high-level controller dynamically plans over the abstract model to generate subgoals, while the low-level policy executes actions to achieve these subgoals. The potential function used for reward shaping is derived from the optimal state values computed from the abstract model, ensuring that the shaping preserves the optimal policy while providing denser rewards.

## Key Results
- GCRS achieves over 90% success rate on harder evaluation tasks in procedurally generated environments
- The method demonstrates 60% success rate on zero-shot generalization to new object colors in the ObjectDelivery task
- GCRS outperforms baseline hierarchical RL methods across all tested metrics including sample efficiency, task completion rates, and scalability

## Why This Works (Mechanism)
GCRS works by decomposing the complex continuous control problem into manageable sub-tasks through expert abstractions. The hierarchical structure allows the agent to first learn high-level strategies in an abstract space, which then guide the learning of low-level continuous control policies. The potential-based reward shaping provides informative feedback even in sparse reward settings by leveraging the value function from the abstract model. This combination enables efficient exploration and learning while maintaining the ability to generalize to novel scenarios through the abstract planning component.

## Foundational Learning
- **Expert Abstractions**: Pre-defined hierarchical representations of the task space. Needed because it provides a structured decomposition of complex tasks into manageable sub-goals, enabling more efficient learning. Quick check: Verify the expert abstraction accurately captures the essential structure of the task without being overly specific to training scenarios.
- **Potential-Based Reward Shaping**: A technique that adds auxiliary rewards based on a potential function while preserving optimal policies. Needed because it transforms sparse rewards into denser feedback signals without changing the underlying optimal policy. Quick check: Confirm that the shaping reward is derived from a valid potential function and that the optimal policy remains unchanged.
- **Goal-Conditioned Policies**: Policies that can achieve different goals based on the input conditioning. Needed because it enables generalization across multiple tasks by learning a single policy that can be directed toward different objectives. Quick check: Test the policy's ability to achieve diverse goals not seen during training.
- **Hierarchical Planning**: Planning at multiple levels of abstraction. Needed because it allows the agent to first determine high-level strategies before executing low-level actions, improving efficiency in complex environments. Quick check: Evaluate whether the high-level planner generates meaningful subgoals that simplify the low-level task.
- **Continuous Control**: Control policies operating in continuous action spaces. Needed because many real-world robotics and control tasks require fine-grained, continuous actions rather than discrete choices. Quick check: Verify the policy can generate smooth, continuous actions that achieve the desired control objectives.

## Architecture Onboarding

Component Map:
Expert Abstract Model -> High-Level Planner -> Subgoal Generator -> Goal-Conditioned Policy -> Continuous Environment

Critical Path:
1. Expert provides abstract model of the environment
2. Abstract model is used to compute optimal state values
3. High-level planner uses abstract model to generate subgoals
4. Subgoals condition the goal-conditioned policy
5. Policy executes in continuous environment with potential-based shaping rewards
6. Learning updates both abstract model policy and continuous policy

Design Tradeoffs:
- Expert abstraction requirement vs. flexibility: The method requires expert-provided abstractions, which limits applicability but enables efficient learning when available
- Computational overhead of hierarchical planning vs. learning efficiency: The additional planning layer increases computation but significantly improves sample efficiency
- Abstract model accuracy vs. shaping effectiveness: The quality of reward shaping depends on the accuracy of the abstract model's value function

Failure Signatures:
- Poor performance when expert abstractions poorly match the actual task structure
- Ineffective shaping when the abstract model's value estimates are inaccurate
- Subgoal infeasibility when the high-level planner generates goals that cannot be achieved by the low-level policy
- Generalization failures when procedural variations exceed the representational capacity of the abstract model

First Experiments:
1. Validate that the potential-based shaping preserves the optimal policy by comparing learned policies with and without shaping
2. Test the high-level planner's ability to generate meaningful subgoals by evaluating task decomposition quality
3. Evaluate zero-shot generalization by testing on procedurally generated variations not seen during training

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies entirely on procedurally generated environments without testing on standard benchmarks or real-world applications
- The requirement for expert-provided abstractions may limit applicability in domains where such abstractions are not readily available
- The approach assumes the expert abstract model is available and accurate, but the paper does not address how to handle cases where this assumption fails

## Confidence

| Claim | Confidence |
|-------|------------|
| Improved sample efficiency and task completion rates compared to baseline hierarchical RL methods | High |
| Zero-shot generalization capabilities demonstrated in procedurally generated environments | Medium |
| Scalability to complex tasks within the procedural generation space | Medium |

## Next Checks
1. Test GCRS on standard continuous control benchmarks (like MuJoCo or PyBullet tasks) to verify performance outside procedurally generated environments
2. Evaluate the method's robustness when the expert abstract model contains errors or inaccuracies, and develop strategies for model correction
3. Compare against non-hierarchical RL methods and state-of-the-art model-based approaches to establish the relative contribution of hierarchical abstraction versus other techniques