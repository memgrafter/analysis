---
ver: rpa2
title: 'Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation
  Framework for Explanations'
arxiv_id: '2511.13081'
source_url: https://arxiv.org/abs/2511.13081
tags:
- explanations
- methods
- saliency
- explanation
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the fundamental ambiguity in saliency map\
  \ interpretation by introducing the Reference-Frame \xD7 Granularity (RFxG) taxonomy.\
  \ The framework distinguishes explanations along two axes: reference frame (pointwise\
  \ vs."
---

# Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations

## Quick Facts
- arXiv ID: 2511.13081
- Source URL: https://arxiv.org/abs/2511.13081
- Authors: Yehonatan Elisha; Seffi Cohen; Oren Barkan; Noam Koenigstein
- Reference count: 20
- Introduces Reference-Frame × Granularity taxonomy and novel evaluation metrics for saliency maps

## Executive Summary
This paper addresses fundamental ambiguity in saliency map interpretation by introducing a Reference-Frame × Granularity (RFxG) taxonomy that systematically categorizes explanations along two dimensions: reference frame (pointwise vs. contrastive) and semantic granularity (class-level vs. group-level). The authors identify critical limitations in existing evaluation metrics, which predominantly assess pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To address these gaps, they propose four novel faithfulness metrics that evaluate explanation quality across both RFxG dimensions using perturbation-based analysis. Comprehensive experiments across ten state-of-the-art methods, four model architectures, and three datasets reveal that Iterated Integrated Attributions (IIA) consistently outperforms other methods, particularly on contrastive and group-level metrics.

## Method Summary
The paper introduces a systematic framework for evaluating saliency maps through the Reference-Frame × Granularity (RFxG) taxonomy. This framework distinguishes explanations along two axes: reference frame (pointwise vs. contrastive) and semantic granularity (class-level vs. group-level). The authors develop four novel faithfulness metrics - CCS, CGC, PGS, and CGS - that use perturbation-based analysis to evaluate explanation quality across both dimensions. The metrics systematically assess how well explanations align with human reasoning patterns by measuring the impact of removing important regions on model predictions. The framework is validated through extensive experiments comparing ten state-of-the-art explanation methods across multiple model architectures and datasets.

## Key Results
- Iterated Integrated Attributions (IIA) consistently outperforms other methods, particularly on contrastive and group-level metrics
- Existing evaluation metrics fail to capture important dimensions of explanation quality, especially contrastive reasoning and semantic granularity
- The RFxG taxonomy provides a systematic framework for categorizing and evaluating explanations aligned with human cognitive processes
- Saliency map methods show varying performance depending on reference frame and granularity requirements

## Why This Works (Mechanism)
The framework works by providing a structured approach to understanding and evaluating explanations through cognitive alignment. By distinguishing between pointwise and contrastive reference frames, it captures whether explanations are absolute or relative. The granularity dimension (class-level vs. group-level) determines whether explanations focus on individual categories or broader semantic groups. The perturbation-based metrics directly measure how removing important regions affects model predictions, providing a more comprehensive assessment than traditional methods. This systematic approach ensures that evaluation captures the full spectrum of human reasoning patterns rather than just a subset.

## Foundational Learning

**Reference-Frame Dimension**: Distinguishes between absolute explanations (pointwise) and relative explanations (contrastive). Why needed: Captures whether users need to understand why a specific prediction was made versus why one prediction was chosen over another. Quick check: Does the explanation compare against a baseline or reference point?

**Granularity Dimension**: Separates class-level explanations (focused on individual categories) from group-level explanations (focused on semantic categories). Why needed: Determines whether explanations should address specific classes or broader conceptual groupings. Quick check: Is the explanation targeting a single class or a group of related classes?

**Perturbation-Based Evaluation**: Uses region removal to assess explanation quality by measuring impact on model predictions. Why needed: Provides direct measurement of explanation effectiveness rather than proxy metrics. Quick check: Does the metric measure actual prediction changes when important regions are removed?

**Faithfulness Metrics**: Four specific metrics (CCS, CGC, PGS, CGS) that evaluate different combinations of reference frame and granularity. Why needed: Ensures comprehensive evaluation across all cognitive dimensions. Quick check: Does the metric assess both reference frame and granularity aspects?

## Architecture Onboarding

Component Map: RFxG Taxonomy -> Novel Metrics (CCS, CGC, PGS, CGS) -> Perturbation Analysis -> Evaluation Framework

Critical Path: Taxonomy definition → Metric formulation → Experimental validation → Method comparison

Design Tradeoffs: Comprehensive coverage vs. evaluation complexity; perturbation cost vs. measurement accuracy; general applicability vs. task-specific optimization

Failure Signatures: Overfitting to specific reference frames; inability to handle group-level reasoning; poor performance on contrastive tasks; sensitivity to perturbation methods

First Experiments:
1. Apply RFxG taxonomy to existing saliency methods to categorize their explanatory approach
2. Implement CCS metric on a simple classification task to validate perturbation methodology
3. Compare IIA performance across all four metrics on a benchmark dataset

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but implicit questions include: How can the framework be extended to other modalities beyond vision? What are the computational costs of comprehensive evaluation across all metrics? How do human users actually perceive and utilize explanations of different types? Can the taxonomy accommodate emerging explanation methods?

## Limitations
- The framework's practical applicability depends on users correctly identifying their desired reference frame and granularity before evaluation
- Perturbation-based metrics assume linear relationships between pixel importance and model behavior, potentially missing non-linear interactions
- The study focuses on vision tasks, limiting generalizability to other modalities like NLP or tabular data

## Confidence

**High confidence** in the conceptual framework and taxonomy distinctions
**Medium confidence** in metric validity across diverse model architectures
**Medium confidence** in IIA's superiority, given single-dataset evaluation
**Low confidence** in cross-modal applicability of findings

## Next Checks

1. Test framework applicability on NLP and tabular data to verify cross-modal generalizability
2. Validate metrics against human judgment studies to confirm alignment with human reasoning preferences
3. Extend evaluation to larger model architectures (e.g., CLIP, GPT) to test scalability claims