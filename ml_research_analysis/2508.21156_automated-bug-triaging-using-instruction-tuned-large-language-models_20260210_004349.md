---
ver: rpa2
title: Automated Bug Triaging using Instruction-Tuned Large Language Models
arxiv_id: '2508.21156'
source_url: https://arxiv.org/abs/2508.21156
tags:
- mozilla
- top-1
- eclipsejdt
- triaging
- developer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses automated bug triaging by proposing an instruction-tuned
  LLM with LoRA adapters and candidate-constrained decoding. The method fine-tunes
  DeepSeek-R1-Distill-Llama-8B on conversational JSONL data from EclipseJDT and Mozilla
  projects, ensuring only valid developers are assigned.
---

# Automated Bug Triaging using Instruction-Tuned Large Language Models

## Quick Facts
- arXiv ID: 2508.21156
- Source URL: https://arxiv.org/abs/2508.21156
- Reference count: 38
- Primary result: Instruction-tuned LLM with LoRA achieves Top-1/Hit@10 of 0.156/0.475 on EclipseJDT and 0.013/0.753 on Mozilla bug triaging

## Executive Summary
This work addresses automated bug triaging by proposing an instruction-tuned LLM with LoRA adapters and candidate-constrained decoding. The method fine-tunes DeepSeek-R1-Distill-Llama-8B on conversational JSONL data from EclipseJDT and Mozilla projects, ensuring only valid developers are assigned. During inference, the model ranks Top-K developers from a constrained candidate set. Evaluated on multi-year datasets aligned with prior graph-based work, it achieves Top-1/Hit@10 of 0.156/0.475 on EclipseJDT and 0.013/0.753 on Mozilla. Strong Hit@10 performance indicates practical utility for shortlist generation in real-world triage workflows, while exact Top-1 remains challenging in large label spaces. The approach requires no handcrafted features or graph construction, offering a lightweight, scalable solution.

## Method Summary
The approach uses DeepSeek-R1-Distill-Llama-8B fine-tuned with LoRA adapters on conversational JSONL data from EclipseJDT and Mozilla projects. During inference, candidate-constrained decoding limits predictions to valid developers only. The model processes bug reports and assigns developers based on learned patterns from the training data, avoiding the need for feature engineering or graph construction.

## Key Results
- Achieves Top-1/Hit@10 accuracy of 0.156/0.475 on EclipseJDT dataset
- Achieves Top-1/Hit@10 accuracy of 0.013/0.753 on Mozilla dataset
- Strong Hit@10 performance suggests practical utility for shortlist generation in real-world triage workflows
- Moderate Top-1 accuracy indicates exact developer assignment remains challenging

## Why This Works (Mechanism)
The instruction-tuned LLM learns patterns from conversational data that capture the relationship between bug reports and developer expertise. By constraining the candidate set during decoding, the model avoids generating invalid developer names and focuses on realistic assignments. The LoRA adapters enable efficient fine-tuning without full model retraining, making the approach scalable and practical.

## Foundational Learning
- Instruction-tuning: Why needed? To adapt the base LLM to the specific task of bug triaging. Quick check: Model generates contextually appropriate responses for bug assignment scenarios.
- LoRA adapters: Why needed? To enable efficient fine-tuning without full model retraining. Quick check: Fine-tuning converges faster with lower computational overhead.
- Candidate-constrained decoding: Why needed? To ensure only valid developers are assigned, improving practical utility. Quick check: All generated outputs are valid developer names from the candidate set.

## Architecture Onboarding
Component map: Bug Report -> LLM with LoRA -> Candidate Set -> Ranked Developers
Critical path: Input bug report flows through instruction-tuned LLM, which uses constrained decoding to rank candidates from the pre-defined developer set.
Design tradeoffs: The approach sacrifices exact Top-1 accuracy for practical shortlist generation capability. No handcrafted features or graph construction needed, but requires quality conversational training data.
Failure signatures: Poor performance when candidate sets are incomplete or when bug reports lack sufficient context for accurate matching.
First experiments:
1. Validate that candidate-constrained decoding produces only valid developer names
2. Test model performance with varying candidate set sizes
3. Evaluate baseline performance on held-out test sets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two datasets (EclipseJDT and Mozilla), raising generalizability concerns
- Moderate Top-1 accuracy (15.6% for EclipseJTD, 1.3% for Mozilla) indicates exact assignment remains challenging
- Performance varies significantly between datasets, suggesting effectiveness may depend on project characteristics

## Confidence
- High confidence in shortlist generation capability (strong Hit@10 performance)
- Medium confidence in exact developer assignment accuracy (modest Top-1 results)
- Medium confidence in cross-project generalizability (limited evaluation scope)

## Next Checks
1. Test the approach on at least three additional bug tracking datasets from different domains and organizations to assess generalizability
2. Evaluate performance when candidate sets are limited or incomplete to understand real-world constraints
3. Compare against traditional feature-engineering approaches on the same datasets to quantify the LLM's advantage beyond black-box performance