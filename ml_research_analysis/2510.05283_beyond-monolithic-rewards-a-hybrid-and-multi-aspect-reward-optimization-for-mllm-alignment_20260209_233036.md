---
ver: rpa2
title: 'Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for
  MLLM Alignment'
arxiv_id: '2510.05283'
source_url: https://arxiv.org/abs/2510.05283
tags:
- reward
- reasoning
- harmo
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces HARMO, a hybrid reward modeling framework
  for aligning multimodal large language models (MLLMs). It combines rule-based rewards
  for verifiable tasks with learned reward models for subjective quality, and adds
  multi-aspect rewards to enforce instruction adherence and prevent reward hacking
  via brevity.
---

# Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment

## Quick Facts
- arXiv ID: 2510.05283
- Source URL: https://arxiv.org/abs/2510.05283
- Reference count: 22
- Key outcome: HARMO framework achieves 9.5% average improvement and 16% gain on math-specific tasks through hybrid and multi-aspect reward optimization

## Executive Summary
This work introduces HARMO, a hybrid reward modeling framework for aligning multimodal large language models (MLLMs). It combines rule-based rewards for verifiable tasks with learned reward models for subjective quality, and adds multi-aspect rewards to enforce instruction adherence and prevent reward hacking via brevity. The framework is validated on mathematical and multimodal reasoning benchmarks, achieving an overall average improvement of 9.5% over the baseline and a 16% gain on math-specific tasks. The method improves both reasoning accuracy and output quality while reducing dependency on extensive human data annotation.

## Method Summary
HARMO integrates rule-based rewards for tasks with verifiable ground truth (like mathematical reasoning) with learned reward models for subjective quality assessment. The framework introduces multi-aspect rewards that separately optimize for instruction adherence and penalize excessive brevity to prevent reward hacking. This hybrid approach allows the system to leverage automated verification where possible while still capturing nuanced quality aspects through learned components. The reward optimization is applied during the alignment phase of MLLM training to improve both reasoning performance and output quality.

## Key Results
- 9.5% average improvement over baseline on mathematical and multimodal reasoning benchmarks
- 16% gain specifically on math-related tasks
- Reduces dependency on extensive human data annotation while maintaining or improving output quality

## Why This Works (Mechanism)
The framework works by decomposing the reward function into specialized components rather than using a monolithic reward signal. Rule-based rewards provide precise, verifiable feedback for tasks like mathematical reasoning where ground truth exists, ensuring accuracy on objective tasks. Learned reward models capture subjective quality aspects that are difficult to encode with rules. The multi-aspect formulation prevents the model from exploiting single-reward optimization by independently enforcing instruction adherence and penalizing brevity, which addresses reward hacking behaviors common in RLHF approaches.

## Foundational Learning
- **Reward Modeling**: Why needed - To provide feedback signals for reinforcement learning without relying solely on human preference data; Quick check - Verify the reward model can distinguish between high and low quality outputs on validation sets
- **Rule-based vs Learned Rewards**: Why needed - Different task types require different reward strategies; verifiable tasks benefit from rules while subjective tasks need learned models; Quick check - Compare performance on verifiable tasks using only learned rewards versus hybrid approach
- **Multi-aspect Reward Optimization**: Why needed - Prevents reward hacking by optimizing multiple independent objectives rather than a single combined signal; Quick check - Test whether models trained with single rewards exhibit brevity bias or instruction ignoring

## Architecture Onboarding

**Component Map**: MLLM -> Reward Predictor -> Rule-based Reward + Learned Reward + Multi-aspect Penalty -> Policy Update

**Critical Path**: Input task → MLLM generates response → Reward predictor evaluates response using hybrid reward components → Policy gradient update based on total reward → Improved MLLM

**Design Tradeoffs**: Rule-based rewards offer precision but limited scope; learned rewards capture nuance but require human data; multi-aspect rewards prevent hacking but increase complexity; hybrid approach balances these tradeoffs

**Failure Signatures**: Excessive brevity in outputs (reward hacking), poor performance on verifiable tasks (inadequate rule-based component), inconsistent quality across different task types (imbalanced reward components)

**First Experiments**:
1. Validate rule-based reward accuracy on mathematical problems with known solutions
2. Test learned reward model correlation with human preferences on multimodal tasks
3. Evaluate multi-aspect reward effectiveness by comparing brevity and instruction adherence metrics

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies entirely on proprietary benchmarks, limiting independent replication
- Small human-in-the-loop feedback collection (100 samples per round) raises concerns about statistical robustness
- Limited testing on non-mathematical, open-ended instruction following scenarios
- Rule-based reward component only applicable to tasks with verifiable ground truth

## Confidence
- 9.5% average improvement: Medium confidence (dependent on proprietary benchmarks)
- 16% math-specific gain: Medium confidence (ablations don't isolate individual component contributions)
- Reduction in human annotation needs: Medium confidence (not quantified in absolute terms)

## Next Checks
1. Replicate the ablation study with an open-source math reasoning benchmark to verify the isolated contribution of the rule-based reward component
2. Conduct statistical significance testing across multiple random seeds to confirm that the 9.5% and 16% improvements are not due to variance in training dynamics
3. Test HARMO on a non-mathematical, open-ended instruction following dataset to assess whether the multi-aspect reward formulation generalizes beyond reasoning tasks