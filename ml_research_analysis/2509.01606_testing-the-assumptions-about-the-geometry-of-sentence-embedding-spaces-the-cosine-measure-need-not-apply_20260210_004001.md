---
ver: rpa2
title: 'Testing the assumptions about the geometry of sentence embedding spaces: the
  cosine measure need not apply'
arxiv_id: '2509.01606'
source_url: https://arxiv.org/abs/2509.01606
tags:
- sentence
- embedding
- embeddings
- space
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the geometry of sentence embedding spaces\
  \ produced by transformer models, challenging the assumption that closeness in the\
  \ embedding space reflects similarity in performance on linguistic tasks. The authors\
  \ compare three sentence representation variations\u2014averaged token embeddings,\
  \ the [CLS] token embedding, and a random token embedding\u2014across four BERT-family\
  \ models (BERT, RoBERTa, DeBERTa, Electra)."
---

# Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply

## Quick Facts
- arXiv ID: 2509.01606
- Source URL: https://arxiv.org/abs/2509.01606
- Reference count: 35
- The geometry of sentence embedding spaces is not a reliable proxy for shared linguistic properties

## Executive Summary
This paper challenges the common assumption that closeness in sentence embedding space reflects similarity in linguistic task performance. The authors investigate three sentence representation methods across four BERT-family models: averaged token embeddings, [CLS] token embeddings, and random token embeddings. Through systematic experiments on the FlashHolmes benchmark and a syntactic chunk structure detection task, they demonstrate that geometric proximity in embedding space does not correlate with similar performance on linguistic tasks.

The findings reveal a fundamental disconnect between geometric measures and functional linguistic capabilities. For instance, [CLS] embeddings can be nearly orthogonal to averaged embeddings yet yield comparable task performance, while representations that are close in space may perform differently. This suggests that linguistic information is encoded through complex weighted combinations of dimensions rather than simple proximity in embedding space.

## Method Summary
The study compares three sentence representation methods across four BERT-family models (BERT, RoBERTa, DeBERTa, Electra): averaged token embeddings, [CLS] token embeddings, and random token embeddings. Cosine similarity measures the geometric distances between these representations. Performance is evaluated on the FlashHolmes benchmark covering morphology, syntax, semantics, discourse, and reasoning tasks, plus a syntactic chunk structure detection task. The chunk structure task uses a 5-class classification approach to identify chunk boundaries, trained and tested on the same set of 500 sentences used for distance measurements.

## Key Results
- High cosine similarity between representation types does not correlate with similar task performance, particularly for RoBERTa
- [CLS] embeddings are nearly orthogonal to averaged embeddings in Electra and DeBERTa yet yield similar task results
- All three representation types encode chunk structure information in the same way despite geometric distances
- Linguistic information is encoded through complex weighted combinations of dimensions rather than shallow cosine proximity

## Why This Works (Mechanism)
The paper demonstrates that geometric proximity in embedding spaces does not reliably indicate shared linguistic properties. The mechanism underlying this finding appears to be that linguistic information is encoded through complex weighted combinations of dimensions rather than through simple proximity measures. This means that representations can be geometrically distant while encoding similar functional information, or geometrically close while encoding different information.

## Foundational Learning
- **Cosine similarity**: A measure of similarity between vectors that calculates the cosine of the angle between them; needed to understand geometric relationships in embedding spaces; quick check: cosine of 0° = 1 (identical), 90° = 0 (orthogonal)
- **BERT-family models**: Transformer-based architectures including BERT, RoBERTa, DeBERTa, and Electra that produce contextual embeddings; needed to understand the experimental scope; quick check: all use masked language modeling pre-training
- **Sentence representation methods**: Different ways to aggregate token-level information into sentence-level representations including [CLS] token, averaging, and random selection; needed to understand the comparative framework; quick check: [CLS] is trained for classification, averaging is simple aggregation
- **FlashHolmes benchmark**: A comprehensive linguistic evaluation suite covering morphology, syntax, semantics, discourse, and reasoning; needed to understand task diversity; quick check: designed to test specific linguistic phenomena
- **Chunk structure detection**: Identifying syntactic chunks or phrases within sentences through classification of token positions; needed to understand the structural analysis task; quick check: treats chunk boundaries as classification problem

## Architecture Onboarding

**Component Map**
Sentence embedding space -> Cosine distance calculation -> Linguistic task performance evaluation -> Chunk structure analysis

**Critical Path**
Token embeddings (from BERT-family models) -> Sentence representation aggregation ([CLS], average, random) -> Cosine similarity measurement -> Task performance evaluation (FlashHolmes) -> Chunk structure classification

**Design Tradeoffs**
- [CLS] token vs averaged embeddings: [CLS] is trained for classification but may not capture full sentence meaning, while averaging is simple but loses token-specific information
- Random token baseline: provides geometric contrast but may not represent realistic encoding strategies
- Cosine similarity: computationally efficient but may not capture complex relationships between representations

**Failure Signatures**
- High cosine similarity between representations but divergent task performance
- Orthogonal representations yielding similar task results
- Geometric proximity not correlating with functional similarity

**3 First Experiments**
1. Measure cosine distances between [CLS], averaged, and random token embeddings for a sample sentence
2. Evaluate task performance of each representation type on a single FlashHolmes task
3. Train a chunk structure classifier using each representation type on the same 500-sentence dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is constrained to BERT-family models and may not generalize to other architectures
- The FlashHolmes benchmark represents a curated set of linguistic phenomena that may not capture all dimensions of sentence understanding
- The random token baseline does not represent a realistic encoding strategy and may not fully capture information distribution in these models

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Geometric proximity does not reliably indicate shared linguistic properties across representation types | High |
| [CLS] embeddings can be nearly orthogonal to averaged embeddings yet yield similar performance | Medium |
| Linguistic information is encoded through complex weighted combinations of dimensions | Low |

## Next Checks
1. Replicate experiments across additional model families including sentence transformers and contrastive models to assess architectural generality
2. Conduct ablation studies varying the number and selection of tokens included in averaged embeddings to understand sensitivity to aggregation strategies
3. Perform direct probing experiments comparing internal attention patterns and hidden state transformations that produce [CLS] versus averaged representations to identify specific mechanisms enabling orthogonal embeddings to encode similar information