---
ver: rpa2
title: 'GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared
  Basis Functions'
arxiv_id: '2512.09084'
source_url: https://arxiv.org/abs/2512.09084
tags:
- gs-kan
- parameter
- function
- learnable
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GS-KAN, a parameter-efficient variant of Kolmogorov-Arnold
  Networks that leverages a shared learnable B-spline basis function per layer, rather
  than unique functions per edge. Inspired by Sprecher's refinement of the Kolmogorov-Arnold
  theorem, GS-KAN applies learnable linear transformations (scaling and shifting)
  to a single parent function, drastically reducing parameter count while maintaining
  the theoretical benefits of KANs.
---

# GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions

## Quick Facts
- arXiv ID: 2512.09084
- Source URL: https://arxiv.org/abs/2512.09084
- Reference count: 9
- Achieves 87.03% accuracy on Fashion-MNIST with ~12.5k parameters, outperforming an MLP with 5% more parameters (86.00%)

## Executive Summary
GS-KAN introduces a parameter-efficient variant of Kolmogorov-Arnold Networks that uses shared learnable B-spline basis functions per layer instead of unique functions per edge. By applying learnable linear transformations (scaling and shifting) to a single parent function, GS-KAN drastically reduces parameter count while maintaining the theoretical benefits of KANs. The architecture is evaluated across synthetic function approximation, tabular regression, and high-dimensional image classification tasks.

## Method Summary
GS-KAN implements the Kolmogorov-Arnold representation using a single learnable B-spline basis function per layer, with edge-specific functions created through linear transformations (scaling λ and shifting ε). This approach reduces parameters from O(N²K) to O(NK) where N is network width and K is knots per function. The architecture is tested on three tasks: synthetic 2D function approximation with noise, California Housing regression (8 features), and Fashion-MNIST classification (784 features). Training uses Adam optimizer with cubic B-splines, though specific hyperparameters like learning rate and batch size are not provided.

## Key Results
- Outperforms MLPs and standard KANs on high-frequency function approximation (MSE ~1.65×10⁻⁴)
- Matches or exceeds existing KAN variants on California Housing tabular benchmark
- Achieves 87.03% accuracy on Fashion-MNIST with ~12.5k parameters
- Reduces parameter count from O(N²K) to O(NK) per layer

## Why This Works (Mechanism)
GS-KAN leverages Sprecher's refinement of the Kolmogorov-Arnold theorem by sharing a single parent B-spline function across all edges in a layer, with linear transformations creating edge-specific functions. This dramatically reduces parameters while preserving the universal approximation capability. The shared basis approach enables scalability to high-dimensional problems where standard KANs become infeasible due to parameter explosion.

## Foundational Learning
- **Kolmogorov-Arnold representation**: Decomposes multivariate functions into sums of univariate functions; needed for theoretical foundation of KANs
- **B-spline basis functions**: Piecewise polynomial basis for smooth function approximation; needed for efficient, learnable univariate functions
- **Linear transformations for function generation**: Scaling and shifting operations to create edge-specific functions from shared parent; needed for parameter efficiency
- **Universal approximation theory**: Mathematical framework ensuring networks can approximate any continuous function; needed to validate architectural choices

## Architecture Onboarding

**Component Map**
Input -> Linear transformation (λ, ε) -> Shared B-spline ψ_l -> Summation -> Next layer (repeat) -> Output

**Critical Path**
The core innovation lies in the shared basis function per layer with learnable linear transformations. Each layer l has one parent B-spline function ψ_l that is scaled and shifted differently for each incoming edge, creating edge-specific univariate functions without the parameter overhead of unique functions per edge.

**Design Tradeoffs**
- Parameter efficiency vs. representational flexibility: Shared basis reduces parameters but may limit expressiveness compared to unique edge functions
- Fixed basis vs. learnable basis: B-splines provide smooth, interpretable functions but may not adapt as flexibly as fully learnable bases
- Domain constraints: B-spline effectiveness depends on keeping activations within valid domain range

**Failure Signatures**
- Underfitting on high-frequency functions: Indicates insufficient knots or network width
- Gradient vanishing: Occurs when activations drift outside spline domain [-G,G]
- Slow convergence: May indicate poor B-spline initialization or suboptimal learning rate

**3 First Experiments**
1. Synthetic function approximation: 2D sin(3πx)·cos(3πy) with 4,096 samples, target MSE ~1.65×10⁻⁴
2. California Housing regression: 8 features, 20,640 samples, normalized, target MSE comparison
3. Fashion-MNIST classification: 784-dim flattened, K=60 knots, target accuracy ~87%

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical hyperparameters (learning rate, batch size) prevents exact reproduction
- B-spline coefficient initialization scheme not specified, which could affect convergence
- MLP baseline architectures lack implementation details for activation timing
- Performance comparisons rely on baseline implementations that may differ from paper specifications

## Confidence

**Major Uncertainties and Limitations**
The absence of critical hyperparameters and initialization details represents the primary barrier to faithful reproduction. While the architectural framework is clearly specified, these implementation specifics could significantly impact convergence and final performance metrics.

**Confidence Labels**
- High confidence in the theoretical framework and architectural innovation
- Medium confidence in the claimed performance improvements, pending hyperparameter verification
- Medium confidence in the parameter efficiency claims, contingent on baseline implementations

## Next Checks

1. **Baseline Verification**: Implement and test the MLP baseline on Fashion-MNIST with exactly specified architecture [784, 50, 50, 10] and SiLU activations to confirm the reported 86.00% accuracy

2. **Hyperparameter Sensitivity**: Systematically vary learning rate (1e-3, 5e-4, 1e-4) and batch size (32, 64, 128) on the synthetic function approximation task to identify optimal settings and assess stability

3. **Initialization Analysis**: Experiment with different B-spline coefficient initialization schemes (random normal, Xavier, uniform) to determine impact on convergence speed and final MSE on the high-frequency function approximation task