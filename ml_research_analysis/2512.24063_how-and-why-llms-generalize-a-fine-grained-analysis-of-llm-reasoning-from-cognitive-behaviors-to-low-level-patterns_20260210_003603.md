---
ver: rpa2
title: 'How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from
  Cognitive Behaviors to Low-Level Patterns'
arxiv_id: '2512.24063'
source_url: https://arxiv.org/abs/2512.24063
tags:
- reasoning
- arxiv
- diagnostic
- retrieval
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a fine-grained benchmark that decomposes\
  \ reasoning into five core cognitive behaviors\u2014calculation, enumeration, simulation,\
  \ fact retrieval, and diagnostic checking\u2014across four domains: mathematics,\
  \ scientific reasoning, coding, and non-reasoning tasks. By tracking these skills\
  \ across supervised fine-tuning (SFT) and reinforcement learning (RL) tuning, the\
  \ study reveals that RL tends to preserve balanced skill profiles and robust generalization,\
  \ whereas SFT often over-specializes, yielding uneven skill gains with spikes in\
  \ narrow areas."
---

# How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns

## Quick Facts
- arXiv ID: 2512.24063
- Source URL: https://arxiv.org/abs/2512.24063
- Reference count: 40
- Key outcome: RL-tuned models preserve balanced cognitive skill profiles while SFT induces over-specialization and skill drift.

## Executive Summary
This paper presents a fine-grained benchmark that decomposes reasoning into five core cognitive behaviors—calculation, enumeration, simulation, fact retrieval, and diagnostic checking—across four domains. By comparing supervised fine-tuning (SFT) and reinforcement learning (RL) tuning, the study reveals that RL maintains more balanced skill profiles and robust generalization, while SFT leads to uneven skill gains with spikes in narrow areas. The findings highlight the importance of behavior-aware training strategies to improve model robustness and interpretability.

## Method Summary
The paper constructs a benchmark by decomposing reasoning into five cognitive behaviors across four domains. It evaluates SFT and RL-tuned models on this benchmark, tracking skill evolution across training stages. The method includes SAE-based interpretability analysis to link hidden representations to cognitive behaviors, parameter shift statistics, and radar profile visualization of skill balance.

## Key Results
- RL-tuned models maintain balanced cognitive skill profiles, while SFT models exhibit over-specialization with spikes in narrow skills.
- The behavioral divergence between SFT and RL stems from optimization objectives, not parameter update magnitude.
- Long chain-of-thought training rebalances SFT skill profiles toward more systematic, transferable reasoning.

## Why This Works (Mechanism)

### Mechanism 1
RL's reward-based optimization encourages the model to retain diverse reasoning pathways since correctness can be achieved through multiple skill combinations. SFT's likelihood maximization on specific trace patterns causes the model to overfit to surface heuristics, leading to "jagged" skill profiles with spikes in narrow skills (often diagnostic or calculation) and degradation in others (simulation, enumeration). This mechanism assumes the five cognitive behaviors are sufficiently independent.

### Mechanism 2
Both SFT and RL update ~98% of parameters with comparable total change magnitude (~29K). However, RL's reward signal nudges the output distribution toward correct outcomes while allowing internal reasoning flexibility, whereas SFT's token-level supervision constrains the entire reasoning trace to match training examples exactly. The mechanism assumes parameter magnitude is a reasonable proxy for "how much" the model changes.

### Mechanism 3
Long chain-of-thought (CoT) training rebalances SFT skill profiles toward more systematic, transferable reasoning. Long CoT data provides richer supervision that includes intermediate reasoning steps, self-corrections, and explicit skill composition. This encourages the model to internalize *how* to reason rather than merely pattern-matching to final answers, yielding more rounded skill profiles that transfer better across domains.

## Foundational Learning

- **Cognitive behavior decomposition**: The paper decomposes "reasoning" into five atomic behaviors. Why needed: Understanding this taxonomy is prerequisite to interpreting any results. Quick check: Given a physics problem requiring you to apply conservation of energy, compute a value, and verify units, which three cognitive behaviors are involved?
- **Radar/skill profile interpretation**: The paper visualizes model capabilities as radar plots. Why needed: "Rounded" profiles indicate balanced skills; "jagged/spiky" profiles indicate over-specialization. Quick check: If a model's radar plot shows a 3x spike on diagnostic but 0.5x baseline on simulation, what does this indicate about its training?
- **Sparse Autoencoder (SAE) feature attribution**: Section 4.3 uses SAEs to link hidden representations to cognitive behaviors. Why needed: Understanding how SAE features are scored and mapped to behaviors is necessary to interpret Figure 7. Quick check: An SAE feature with explanation "detects contradictions in multi-step proofs" would likely score high on which cognitive behavior?

## Architecture Onboarding

- **Component map**: Benchmark construction (Seed Design → Candidate Retrieval → Manual Verification) → Evaluation harness (vLLM batched inference) → SAE interpretability layer (Delphi → LLM grader) → Meta-probing framework (evaluates checkpoints across training stages)
- **Critical path**: 1. Define behavior-domain grid (5 behaviors × 4 domains) 2. Generate/curate benchmark items with skill isolation 3. Train or obtain model checkpoints (Base, SFT, RL) 4. Evaluate each checkpoint on benchmark → generate radar profiles 5. Optionally: train SAEs, attribute features to behaviors, analyze layer-wise patterns
- **Design tradeoffs**: Skill isolation vs. realism (benchmark targets single behaviors for clean measurement vs. real tasks requiring skill composition); SAE layer selection (mid-layers chosen for "richest compositional representations" vs. early/deep layers); Behavior attribution method (LLM grader introduces circularity risk)
- **Failure signatures**: Jagged radar profile after SFT (over-specialization, likely overfit); Uniform degradation across all behaviors (training instability or hyperparameter issues); SAE feature percentages near 0% or 100% (bug in attribution pipeline)
- **First 3 experiments**: 1. Replicate Figure 3 on a different base model (e.g., Llama-3-8B): Train SFT and RL variants on math data, evaluate on the 5-behavior benchmark. 2. Ablate training data balance: Create SFT datasets with explicitly balanced vs. imbalanced behavior coverage. 3. Cross-domain transfer test: Train on math-only data, evaluate on physics and non-reasoning domains.

## Open Questions the Paper Calls Out

- Can lightweight probing methods effectively replace compute-heavy Sparse Autoencoders (SAEs) for monitoring cognitive skill subspaces during training? Current SAE techniques are computationally expensive and highly sensitive to design choices, limiting their practical application for real-time monitoring.
- To what extent do curriculum learning strategies or explicit skill-balancing regularizers prevent the over-specialization observed in Supervised Fine-Tuning (SFT)? The study characterizes the "jagged" skill profiles of standard SFT but does not experiment with intervention strategies to smooth these profiles.
- Do the distinct generalization profiles of RL (balanced) versus SFT (jagged) persist across different model architectures? The experimental setup relies exclusively on the Qwen3 family, leaving the generalizability of these behavioral findings to other architectures unverified.

## Limitations
- The behavioral decomposition relies on qualitative rubrics that may not fully capture real-world reasoning complexity
- The benchmark uses a relatively small number of examples per behavior-domain cell (50-200), limiting statistical power
- The SAE-based interpretability analysis introduces potential circularity since the LLM grader is from the same model family being analyzed

## Confidence
- **High confidence**: Claims about RL preserving balanced skill profiles vs. SFT over-specialization are well-supported by consistent radar profile patterns
- **Medium confidence**: Claims about parameter update magnitude being similar between SFT and RL are supported but could be refined with more detailed analysis
- **Medium confidence**: Claims about long CoT training rebalancing SFT profiles are supported by comparisons between "think" and "no-think" modes, though evidence is indirect

## Next Checks
1. Replicate the SFT vs. RL behavioral divergence on a different base model (e.g., Llama-3-8B or Mistral-7B) using the same training recipes
2. Experiment with curriculum learning strategies or explicit skill-balancing regularizers in SFT to test whether these prevent over-specialization
3. Analyze parameter update patterns layer-by-layer to determine if RL updates are indeed concentrated in specific layers as opposed to being uniformly distributed