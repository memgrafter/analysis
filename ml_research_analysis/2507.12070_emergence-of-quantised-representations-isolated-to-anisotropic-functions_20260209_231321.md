---
ver: rpa2
title: Emergence of Quantised Representations Isolated to Anisotropic Functions
arxiv_id: '2507.12070'
source_url: https://arxiv.org/abs/2507.12070
tags:
- representations
- these
- which
- networks
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a controlled ablation study investigating how
  activation function symmetries influence representational structure in autoencoders.
  The key innovation is the Privileged-Plane Projective (PPP) method, which visualizes
  high-dimensional representations by projecting them onto privileged planes defined
  by basis vectors.
---

# Emergence of Quantised Representations Isolated to Anisotropic Functions

## Quick Facts
- **arXiv ID:** 2507.12070
- **Source URL:** https://arxiv.org/abs/2507.12070
- **Reference count:** 40
- **Primary result:** Discrete-symmetry activation functions induce quantised, axis-aligned representations while continuous-symmetry functions produce smooth, isotropic distributions

## Executive Summary
This paper presents a controlled ablation study investigating how activation function symmetries influence representational structure in autoencoders. The key innovation is the Privileged-Plane Projective (PPP) method, which visualizes high-dimensional representations by projecting them onto privileged planes defined by basis vectors. The core finding demonstrates that discrete-symmetry activation functions (permutation-equivariant) induce quantised, axis-aligned representations, while continuous-symmetry functions (orthogonal-equivariant) produce smooth, isotropic distributions. This establishes a causal link between algebraic symmetry properties and emergent representational structure, reframing interpretability phenomena as consequences of design choices rather than fundamental deep learning properties.

## Method Summary
The study employs a controlled ablation approach comparing activation functions with different symmetry properties in autoencoder architectures. The primary experimental design contrasts discrete-symmetry functions (permutation-equivariant) against continuous-symmetry functions (orthogonal-equivariant) while controlling for all other architectural variables. The Privileged-Plane Projective (PPP) visualization method projects high-dimensional representations onto privileged planes defined by basis vectors, enabling systematic analysis of representational structure. Performance is evaluated through reconstruction tasks, and the symmetry properties of activation functions are rigorously characterized through their equivariance groups.

## Key Results
- Discrete-symmetry activation functions induce quantised, axis-aligned representations while continuous-symmetry functions produce smooth, isotropic distributions
- Isotropic networks consistently outperform anisotropic counterparts in reconstruction tasks
- The study establishes a causal link between algebraic symmetry properties and emergent representational structure

## Why This Works (Mechanism)
The mechanism operates through the fundamental interaction between activation function symmetries and gradient flow during optimization. Discrete-symmetry functions (like ReLU) preserve permutations of input coordinates, creating bottlenecks that force representations to align with coordinate axes. Continuous-symmetry functions (like tanh) preserve orthogonal transformations, allowing representations to distribute smoothly across all dimensions. During training, these symmetry constraints shape the loss landscape and gradient directions, leading to systematic differences in how information is encoded and distributed across network layers.

## Foundational Learning

### Neural Network Symmetries
**Why needed:** Understanding how activation functions transform under group operations is essential for predicting representational outcomes
**Quick check:** Verify that ReLU preserves permutations while tanh preserves orthogonal transformations

### Representation Geometry
**Why needed:** High-dimensional representations have geometric properties that determine their functional utility
**Quick check:** Confirm that axis-aligned clusters have different statistical properties than isotropic distributions

### Autoencoder Optimization Dynamics
**Why needed:** Training objectives and gradient flows are shaped by activation function constraints
**Quick check:** Track how different symmetries affect reconstruction loss convergence patterns

## Architecture Onboarding

### Component Map
Input -> Encoder (Activation Function) -> Latent Space -> Decoder (Activation Function) -> Output

### Critical Path
The critical path flows from input through encoder with chosen activation function to latent space representation, then through decoder to output. The activation function symmetry directly controls representational structure at the bottleneck.

### Design Tradeoffs
- Discrete-symmetry functions offer computational efficiency but induce quantised representations
- Continuous-symmetry functions provide smooth representations but may require more computational resources
- The choice affects both representational quality and task performance

### Failure Signatures
- Quantised representations may lead to poor generalization on smooth manifolds
- Isotropic representations may underfit when discrete structure is present in data
- PPP projections may miss structure outside privileged planes

### First Experiments
1. Replicate symmetry-induced quantization effects across different activation functions
2. Compare PPP projections with alternative visualization methods (UMAP, t-SNE)
3. Vary network depth to test scalability of representational patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Focus exclusively on autoencoders limits generalizability to other architectures
- PPP visualization may obscure structure outside privileged planes
- Shallow architectures used may not reflect deeper network behavior

## Confidence
- **High** confidence in causal link between symmetry properties and representational structure within controlled conditions
- **High** confidence in performance comparison showing isotropic advantages
- **Medium** confidence in generalizability as a framework for predicting biases across network primitives

## Next Checks
1. Replicate symmetry-induced quantization effects across different neural network architectures including transformers and convolutional networks
2. Test whether PPP projections consistently capture the same representational structure when using alternative visualization methods like UMAP or t-SNE
3. Conduct ablation studies varying network depth to determine whether representational patterns scale predictably with architectural complexity