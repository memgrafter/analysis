---
ver: rpa2
title: 'ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers
  via Local Similarity'
arxiv_id: '2512.02403'
source_url: https://arxiv.org/abs/2512.02403
tags:
- similarity
- attention
- sparsity
- prediction
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESACT, an accelerator for compute-intensive
  Transformers that leverages local similarity for end-to-end sparsity across all
  components. The core method, SPLS, uses HybridLog Quantization (HLog) to predict
  local attention sparsity before QK generation, achieving efficient sparsity in QKV
  generation, attention computation, and FFNs.
---

# ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity

## Quick Facts
- arXiv ID: 2512.02403
- Source URL: https://arxiv.org/abs/2512.02403
- Authors: Hongxiang Liu; Zhifang Deng; Tong Pu; Shengli Lu
- Reference count: 40
- Primary result: Achieves 3.27 TOPS/W energy efficiency with 51.7% computation reduction and <1% accuracy loss

## Executive Summary
ESACT introduces an end-to-end sparse accelerator for compute-intensive Transformers that leverages local similarity for sparsity across all components. The core method, SPLS, uses HybridLog Quantization (HLog) to predict local attention sparsity before QK generation, enabling efficient sparsity in QKV generation, attention computation, and FFNs. The design includes three hardware innovations: a bit-level prediction unit for efficient quantization and low-power prediction, a progressive generation scheme for overlapping prediction and QKV generation, and a dynamic allocation strategy for handling irregular sparsity patterns. Experimental results on 26 benchmarks demonstrate significant improvements in computation reduction and energy efficiency compared to state-of-the-art accelerators.

## Method Summary
The ESACT accelerator uses HybridLog Quantization (HLog) to predict local attention sparsity before QK generation, achieving end-to-end sparsity across QKV generation, attention computation, and FFNs. The SPLS method quantizes key and query vectors to 8-bit, computes similarity in bit-level operations, and uses the results for pruning decisions. The hardware implementation includes a bit-level prediction unit that reduces quantization and prediction costs, a progressive generation scheme that overlaps prediction with QKV generation, and a dynamic allocation strategy that handles irregular sparsity patterns through multiple physical buffers. This approach reduces total computation by 51.7% while maintaining less than 1% accuracy loss.

## Key Results
- Achieves end-to-end energy efficiency of 3.27 TOPS/W
- Improves attention-level energy efficiency by 2.95x over SpAtten and 2.26x over Sanger
- Reduces total computation by 51.7% with less than 1% accuracy loss

## Why This Works (Mechanism)
The method works by exploiting local similarity in transformer attention mechanisms before the expensive QK multiplication. By using HLog-based quantization and bit-level operations for similarity computation, ESACT can make pruning decisions early in the pipeline, avoiding unnecessary computation of attention weights that will be pruned anyway. The progressive generation scheme allows prediction and computation to overlap, maximizing hardware utilization. The dynamic allocation strategy efficiently handles the irregular sparsity patterns that result from this early prediction approach.

## Foundational Learning
**HybridLog Quantization (HLog)**: A quantization method that represents values using a hybrid of logarithmic and linear scales. Why needed: Enables efficient similarity computation in bit-level operations. Quick check: Verify that HLog maintains sufficient precision for accurate similarity comparison.

**Local Similarity Prediction**: Computing attention relevance scores using quantized representations before full-precision QK multiplication. Why needed: Enables early pruning decisions to avoid unnecessary computation. Quick check: Measure accuracy loss when using quantized similarity vs full-precision similarity.

**Bit-Level Operations**: Performing arithmetic and logical operations at the bit level rather than on full numerical values. Why needed: Dramatically reduces computational complexity for similarity comparisons. Quick check: Compare power consumption of bit-level vs traditional operations.

**Dynamic Allocation Strategy**: Using multiple physical buffers to handle irregular sparsity patterns in generated matrices. Why needed: Ensures efficient memory utilization when sparsity patterns are unpredictable. Quick check: Measure memory utilization efficiency with different buffer allocation schemes.

## Architecture Onboarding
**Component Map**: Input tensors → Bit-level Prediction Unit → QKV Generation Unit → Attention Computation Unit → FFN Unit → Output tensors

**Critical Path**: The critical path involves bit-level prediction, followed by progressive QKV generation with overlapping prediction, then attention computation on the sparse matrix, and finally FFN processing.

**Design Tradeoffs**: Early pruning through bit-level prediction reduces computation but may introduce accuracy loss; progressive generation overlaps computation but requires careful scheduling; dynamic allocation handles irregularity but increases memory overhead.

**Failure Signatures**: Accuracy degradation when similarity thresholds are too aggressive; performance bottlenecks when prediction cannot keep pace with generation; memory inefficiencies when buffer allocation is suboptimal.

**First Experiments**: 1) Measure accuracy vs computation trade-off across different similarity thresholds, 2) Profile power consumption of bit-level prediction vs traditional methods, 3) Characterize memory utilization with varying sparsity patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope with only 26 benchmarks, raising questions about generalizability across diverse transformer architectures
- Missing detailed implementation metrics for hardware innovations, particularly power savings of the bit-level prediction unit
- Comparison with state-of-the-art accelerators doesn't account for technology node differences or application-specific optimizations

## Confidence
- Algorithmic Components: Medium confidence due to limited benchmark diversity
- End-to-End System Claims: Low confidence due to missing implementation details
- Energy Efficiency Claims: Medium confidence without experimental setup context

## Next Checks
1. Test HLog-based prediction across diverse transformer architectures (BERT, GPT, Vision Transformers) and tasks beyond the current benchmark suite
2. Implement and measure actual power consumption of the bit-level prediction unit in silicon or detailed simulation
3. Compare ESACT's performance against state-of-the-art accelerators on identical hardware platforms and technology nodes