---
ver: rpa2
title: 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs'
arxiv_id: '2512.08923'
source_url: https://arxiv.org/abs/2512.08923
tags:
- text
- image
- modality
- questions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces REST and REST+ benchmarks to systematically
  evaluate cross-modal inconsistency in multimodal large language models (MLLMs).
  The benchmarks present semantically identical information across three modalities
  (text, image, mixed) and measure whether models produce consistent outputs regardless
  of input format.
---

# Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs

## Quick Facts
- arXiv ID: 2512.08923
- Source URL: https://arxiv.org/abs/2512.08923
- Reference count: 40
- 15 state-of-the-art MLLMs show cross-modal inconsistency ranging from 6.6% to 90.7% on identical content across modalities

## Executive Summary
This study introduces REST and REST+ benchmarks to systematically evaluate whether multimodal large language models (MLLMs) produce consistent answers when presented with semantically identical information across text, image, and mixed modalities. The research reveals substantial cross-modal inconsistency in current MLLMs, with models performing significantly better on text modality and showing variable sensitivity to visual characteristics like resolution and color. The findings suggest that MLLMs struggle to reliably reason over the same information regardless of input format, exposing a fundamental limitation in their multimodal understanding capabilities.

## Method Summary
The researchers created REST and REST+ benchmarks containing synthetic data where semantically identical information is presented across three modalities: text, image, and mixed (text extracted from images). The benchmarks use multiple-choice questions to evaluate model performance across modalities, measuring both absolute accuracy and cross-modal consistency. The study controls for OCR errors to isolate true cross-modal inconsistency and analyzes embedding space similarity through cosine similarity metrics to understand how text and image representations differ in the joint embedding space.

## Key Results
- MLLMs show cross-modal inconsistency ranging from 6.6% to 90.7% on identical content
- Text modality consistently outperforms image and mixed modalities by 7%+ in accuracy
- Visual characteristics like resolution and color significantly impact performance, with colored text improving results
- Models require more vision tokens than text tokens for equivalent accuracy (except Qwen2.5-VL-32B)

## Why This Works (Mechanism)
Cross-modal inconsistency arises because MLLMs encode text and image information in distinct regions of the joint embedding space. When semantically identical content is presented in different modalities, the resulting representations occupy separate areas, leading to divergent reasoning paths and inconsistent outputs. The magnitude of inconsistency correlates with the cross-modal cosine similarity of embeddings, suggesting that closer proximity in embedding space yields more consistent responses. This indicates that current MLLMs have not learned truly unified multimodal representations but rather maintain modality-specific encoding strategies that interfere with consistent reasoning.

## Foundational Learning
1. **Multimodal embedding space alignment**: Understanding how MLLMs map different modalities into a shared representational space
   - Why needed: Essential for understanding why cross-modal inconsistency occurs at the representational level
   - Quick check: Compare embedding similarity between modalities for semantically identical content

2. **Cross-attention mechanisms in multimodal transformers**: How models integrate visual and textual information through attention
   - Why needed: Critical for understanding how different modalities are processed and fused
   - Quick check: Analyze attention weights when processing same content in different modalities

3. **Tokenization differences between text and vision**: How MLLMs convert different input types into tokens for processing
   - Why needed: Explains why vision tokens often require more tokens than text for equivalent performance
   - Quick check: Compare token counts and distributions across modalities for identical content

## Architecture Onboarding

**Component Map**: Input Modality -> Tokenizer -> Encoder -> Cross-Attention -> Decoder -> Output

**Critical Path**: The cross-attention mechanism between encoder and decoder layers represents the critical path for multimodal fusion. Inconsistent processing at this stage directly causes cross-modal inconsistency in outputs.

**Design Tradeoffs**: 
- Token efficiency vs. representation quality: Using more vision tokens improves accuracy but increases computational cost
- Modality-specific vs. unified encoding: Separate encoders preserve modality characteristics but may hinder cross-modal consistency
- Attention depth vs. consistency: Deeper cross-attention can improve understanding but may amplify modality-specific biases

**Failure Signatures**: 
- Large drops in accuracy when switching from text to image modality
- High variance in performance across different visual presentations of identical text
- Poor correlation between text and image embedding representations for same semantic content

**First Experiments**:
1. Measure cross-modal cosine similarity between text and image embeddings for semantically identical content
2. Compare token efficiency (tokens per correct answer) across modalities
3. Test performance degradation when adding visual noise to otherwise identical text content

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Benchmark relies on synthetic data generation rather than naturally occurring multimodal scenarios
- Multiple-choice format may not capture full complexity of real-world multimodal reasoning
- Cannot establish causation between architectural choices and cross-modal inconsistency
- Cannot definitively prove whether inconsistency represents fundamental limitation or training artifact

## Confidence
- Synthetic data may not reflect real-world scenarios: Medium
- Multiple-choice evaluation may oversimplify multimodal reasoning: Medium
- Causal factors remain unexplored: Low
- Cannot distinguish fundamental limitation from training deficiency: Medium

## Next Checks
1. Replicate REST benchmark using naturally occurring multimodal data to verify inconsistencies persist in real-world scenarios

2. Conduct ablation studies on model architectures to determine which design choices contribute most to cross-modal inconsistency

3. Test whether fine-tuning on cross-modal consistency objectives reduces inconsistency rates, distinguishing architectural limitations from training deficiencies