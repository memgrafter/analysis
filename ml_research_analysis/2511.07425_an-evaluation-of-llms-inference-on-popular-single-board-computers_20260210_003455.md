---
ver: rpa2
title: An Evaluation of LLMs Inference on Popular Single-board Computers
arxiv_id: '2511.07425'
source_url: https://arxiv.org/abs/2511.07425
tags:
- inference
- language
- performance
- across
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language model (LLM) inference performance
  on single-board computers (SBCs) by benchmarking 25 quantized open-source models
  across Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro. Using Ollama and Llamafile
  runtimes, the research measures generation throughput, memory usage, and power consumption
  under varying CPU configurations.
---

# An Evaluation of LLMs Inference on Popular Single-board Computers

## Quick Facts
- arXiv ID: 2511.07425
- Source URL: https://arxiv.org/abs/2511.07425
- Authors: Tung; Nguyen; Tuyen Nguyen
- Reference count: 40
- Single-file executable runtimes (Llamafile) achieve up to 4× higher throughput and 30-40% lower power usage than containerized runtimes (Ollama) on ARM-based SBCs

## Executive Summary
This study evaluates large language model (LLM) inference performance on single-board computers (SBCs) by benchmarking 25 quantized open-source models across Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro. Using Ollama and Llamafile runtimes, the research measures generation throughput, memory usage, and power consumption under varying CPU configurations. Results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4× higher throughput and 30-40% lower power usage than Ollama. The Orange Pi 5 Pro demonstrates the best performance, supporting models up to 7B parameters with acceptable throughput. The study provides practical deployment recommendations for businesses seeking cost-effective, privacy-preserving AI solutions on edge devices.

## Method Summary
The study benchmarks 25 quantized (q4_k_m) open-source models across three SBCs (Raspberry Pi 4, Raspberry Pi 5, Orange Pi 5 Pro) using Ollama and Llamafile runtimes. Each model-device combination is tested with three prompts, repeated four times. Metrics include tokens-per-second, peak RAM usage, and power consumption measured via USB-C inline meter. CPU configurations are varied (4/6/8 cores on Orange Pi 5 Pro) to evaluate performance scaling. The methodology focuses on memory-bound inference workloads with passive cooling constraints.

## Key Results
- Llamafile achieves up to 4× higher throughput and 30-40% lower power usage than Ollama on ARM-based SBCs
- Orange Pi 5 Pro supports models up to 7B parameters with acceptable throughput, outperforming Raspberry Pi 5 and Pi 4
- 1.5B parameters represents the practical upper limit for reliable SBC inference under the tested conditions
- Memory bandwidth and thermal throttling, not core count, are the primary performance bottlenecks on resource-constrained SBCs

## Why This Works (Mechanism)

### Mechanism 1
Llamafile's single-file executable design provides higher throughput and energy efficiency by reducing runtime abstraction overhead compared to Ollama's containerized approach. This allows better CPU heterogeneity utilization without the latency introduced by container orchestration.

### Mechanism 2
Inference throughput is gated by memory bandwidth and thermal throttling rather than raw CPU core count. Adding cores increases heat generation and power draw without proportional performance gains due to memory contention and passive cooling limitations.

### Mechanism 3
Aggressive 4-bit quantization (q4_k_m) enables fitting models >1B parameters into SBC RAM by reducing memory footprint ~4x, trading mathematical precision for feasibility on memory-limited devices.

## Foundational Learning

- **Model Quantization (INT4 vs FP16)**: Understanding weight precision reduction is essential since all models are compressed approximations. Quick check: If a model requires 8GB of VRAM in FP16, how much RAM does it require in 4-bit quantization? (Answer: ~2-2.5GB)

- **Memory Bandwidth vs. Compute Bound**: The results show adding cores doesn't always help. Understanding that LLM inference is often memory-bound explains why a faster CPU doesn't always mean proportionally faster inference. Quick check: Why might a 4-core configuration outperform an 8-core configuration on the same device? (Answer: Thermal throttling or memory bandwidth saturation)

- **CPU Heterogeneity (Big.LITTLE Architecture)**: The Orange Pi 5 Pro uses hybrid architecture (4 performance + 4 efficiency cores). Optimizing inference requires binding processes to the correct core type. Quick check: Which core type should handle matrix multiplication for a 7B model, and what happens if scheduled on the wrong type?

## Architecture Onboarding

- **Component map:** Orange Pi 5 Pro (RK3588S) | Raspberry Pi 5 (Cortex-A76) -> Ubuntu Server / Pi OS (64-bit) -> Llamafile (Single-binary) or Ollama (Containerized) -> GGUF format (Quantized)

- **Critical path:** 1) Hardware Selection (Ensure RAM > Model Size + OS Overhead) 2) OS Tuning (Disable swap, fix CPU frequency scaling) 3) Runtime Selection (Llamafile for speed, Ollama for API compatibility) 4) Thread allocation (Match threads to physical performance cores)

- **Design tradeoffs:** Llamafile offers ~4x speed and lower power but narrower model selection and harder integration with agent frameworks. Ollama has broader support and easier API structure but high overhead. Running 4 cores saves ~5W (30%) but may slightly increase latency compared to "race to sleep" on 8 cores.

- **Failure signatures:** Immediate Crash/Exit (model size exceeds RAM + swap limits), Extremely Low TPS (<1.0) (model running off storage), Inconsistent Latency (thermal throttling kicking in)

- **First 3 experiments:** 1) Baseline Capability Test: Run `smollm2:135m` on RPi 4 2) Runtime Head-to-Head: On Orange Pi 5 Pro, run `tinyllama-1b` on both Llamafile and Ollama (4 cores), measure Peak Power vs. TPS 3) Thermal Stress Test: Run `llama3.2:3b` on Orange Pi 5 Pro with 8 cores for 15 minutes, plot TPS over time

## Open Questions the Paper Calls Out
- How does LLM inference performance change under complex, agentic workloads like RAG or multi-step reasoning compared to summarization tasks?
- What are the trade-offs in energy efficiency and latency when applying dynamic quantization and mixed-precision execution to SBC deployments?
- To what extent can NPUs on SBCs like Orange Pi 5 Pro alleviate the CPU bottlenecks identified in the study?

## Limitations
- Thermal management approach (passive cooling) may not reflect real-world deployments with varying ambient temperatures
- Evaluation focused exclusively on ARM64 SBCs, leaving x86 single-board computers unexplored
- Use of quantized models introduces approximation errors not systematically evaluated for task-specific accuracy degradation

## Confidence
**High Confidence:** Performance rankings of hardware and the general principle that memory bandwidth/thermal constraints limit scaling are well-supported. Llamafile consistently outperforming Ollama is robust.

**Medium Confidence:** Specific throughput numbers (4× speedup, 30-40% power reduction) and the 1.5B parameter practical upper limit are context-dependent and may vary with different quantization schemes.

**Low Confidence:** The extrapolation that Orange Pi 5 Pro can reliably support 7B parameter models "with acceptable throughput" is based on limited testing and doesn't account for edge cases or long-duration inference sessions.

## Next Checks
1. **Accuracy Validation Check:** Run same model configurations through standardized benchmark suites (HELM, LM Evaluation Harness) to quantify how quantization affects task-specific performance across different model sizes and families.

2. **Long-duration Stress Test:** Execute continuous inference on 3B+ parameter models for 60+ minutes while monitoring memory usage patterns, thermal throttling behavior, and throughput degradation to identify stability limits.

3. **Ecosystem Integration Test:** Implement a simple retrieval-augmented generation (RAG) pipeline using both Ollama and Llamafile runtimes to evaluate practical tradeoffs between raw performance and integration complexity with common LLM tooling frameworks.