---
ver: rpa2
title: Can Visual Encoder Learn to See Arrows?
arxiv_id: '2505.19944'
source_url: https://arxiv.org/abs/2505.19944
tags:
- image
- diagram
- edge
- encoder
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether vision-language models can learn
  to recognize edges (lines/arrows) in diagrams. The authors hypothesize that failures
  stem from over-reliance on textual and positional biases in training data.
---

# Can Visual Encoder Learn to See Arrows?
## Quick Facts
- **arXiv ID:** 2505.19944
- **Source URL:** https://arxiv.org/abs/2505.19944
- **Reference count:** 29
- **Primary result:** Fine-tuning vision-language models on synthetic, bias-free diagrams enables them to learn explicit edge representations, improving accuracy on edge classification, retrieval, and captioning tasks.

## Executive Summary
This paper investigates whether vision-language models (VLMs) can learn to recognize edges and arrows in diagrams, a capability that is crucial for diagram understanding but often overlooked. The authors find that standard VLMs fail on edge direction tasks because they rely on textual and positional biases in their training data rather than learning explicit visual representations of edges. To address this, they create a synthetic dataset of diagrams and captions that eliminate these biases and fine-tune CLIP's visual encoder via contrastive learning. Their results show that fine-tuning on bias-free data enables VLMs to significantly outperform pretrained CLIP on edge classification, image retrieval, and diagram captioning, demonstrating that visual encoders can indeed learn to "see" arrows when trained without confounding biases.

## Method Summary
The authors first analyze CLIP's failure to classify arrow direction, attributing it to over-reliance on textual and positional biases in real-world diagram datasets. To test their hypothesis, they create a synthetic dataset of 6,500 diagram categories, each with varied visual appearances but controlled captions that eliminate biases (e.g., never mentioning edge direction in captions). They then fine-tune CLIP's visual encoder using contrastive learning, training it to match images to captions. Finally, they evaluate the fine-tuned models on three tasks: edge classification (detecting edge direction), image retrieval (finding diagrams with the same graph structure), and diagram captioning (generating edge descriptions in Mermaid syntax). The results demonstrate that bias-free training enables VLMs to learn explicit edge representations.

## Key Results
- Fine-tuned models significantly outperform pretrained CLIP on edge classification, image retrieval, and diagram captioning tasks.
- Edge direction classification accuracy improves from near-chance to over 85% after fine-tuning on bias-free data.
- On diagram captioning, the fine-tuned ViT-L/14 achieves an F1-score of 0.966, surpassing zero-shot GPT-4o and LLaVA-Mistral baselines.

## Why This Works (Mechanism)
The paper demonstrates that standard vision-language models fail to learn explicit edge representations because they exploit textual and positional biases in their training data rather than developing visual understanding. By removing these biases through synthetic data and fine-tuning, the models are forced to learn actual visual features of edges and arrows. The contrastive learning approach ensures that the visual encoder learns to associate images with their true visual content, rather than relying on spurious correlations.

## Foundational Learning
- **Contrastive learning:** Why needed: Enables learning of visual representations by pulling similar image-text pairs together and pushing dissimilar pairs apart. Quick check: Verify that the fine-tuned model's embeddings cluster by visual similarity rather than by textual cues.
- **Vision-language models (VLMs):** Why needed: Jointly encode images and text, enabling cross-modal understanding. Quick check: Confirm that the model can match images to their correct captions after fine-tuning.
- **Synthetic data generation:** Why needed: Allows precise control over biases and visual variations. Quick check: Ensure that the synthetic dataset covers sufficient edge types and visual styles.
- **Graph structure representation:** Why needed: Diagrams are fundamentally about relationships, not just objects. Quick check: Validate that the model can retrieve diagrams with identical graph structures regardless of visual style.
- **Mermaid syntax:** Why needed: Provides a structured, unambiguous way to describe diagram edges. Quick check: Confirm that generated captions are valid Mermaid and accurately describe the diagram's structure.
- **Bias analysis:** Why needed: Identifies why models fail and what confounds need to be controlled. Quick check: Verify that removing textual/positional biases actually improves edge recognition.

## Architecture Onboarding
**Component Map:** Synthetic dataset generation -> CLIP visual encoder -> Contrastive fine-tuning -> Evaluation (classification/retrieval/captioning)
**Critical Path:** Image-text pairs → CLIP encoder → Embedding space → Task-specific head → Output
**Design Tradeoffs:** Synthetic data provides bias control but may lack real-world complexity; fine-tuning is efficient but may not generalize to all diagram types.
**Failure Signatures:** Over-reliance on text/position → random edge classification; failure to generalize → poor performance on real diagrams; overfitting → high train but low test accuracy.
**First Experiments:** 1) Ablate bias removal: train on real diagrams vs synthetic; 2) Test few-shot learning: how many categories needed for good performance; 3) Compare architectures: ViT vs ConvNeXt vs hybrid backbones.

## Open Questions the Paper Calls Out
None

## Limitations
- All experiments rely on a single synthetic dataset of 6,500 hand-designed diagram categories, which may not reflect real-world diagram diversity.
- Fine-tuning approach depends on paired image-text data, which may not always be available outside synthetic settings.
- Results are only compared against pretrained CLIP and a single captioning baseline, limiting robustness assessment.

## Confidence
- **High:** Core findings are well-supported by controlled experiments and consistent results across multiple tasks.
- **Medium:** Generalization to real-world diagrams and robustness across architectures remain uncertain due to limited evaluation scope.

## Next Checks
1. Evaluate fine-tuned models on real-world diagram datasets (e.g., InfographicsVQA, PlotQA) to assess transfer.
2. Test fine-tuning with fewer categories or limited data to determine scalability.
3. Compare fine-tuning with alternative bias-mitigation strategies (e.g., debiased pre-training, prompt engineering).