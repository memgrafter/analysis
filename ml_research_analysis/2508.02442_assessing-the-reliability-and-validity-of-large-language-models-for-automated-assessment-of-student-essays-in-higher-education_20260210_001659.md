---
ver: rpa2
title: Assessing the Reliability and Validity of Large Language Models for Automated
  Assessment of Student Essays in Higher Education
arxiv_id: '2508.02442'
source_url: https://arxiv.org/abs/2508.02442
tags:
- human
- scores
- scoring
- each
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the reliability and validity of five Large
  Language Models (LLMs) for automated essay scoring in a higher education context.
  Using a four-criterion rubric, 67 Italian-language student essays were scored by
  five models (Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral 24B) across
  three replications each.
---

# Assessing the Reliability and Validity of Large Language Models for Automated Assessment of Student Essays in Higher Education

## Quick Facts
- **arXiv ID**: 2508.02442
- **Source URL**: https://arxiv.org/abs/2508.02442
- **Reference count**: 0
- **Primary result**: Five LLMs scored 67 Italian student essays using a 4-criterion rubric; all models showed near-zero agreement with human raters and weak within-model reliability (median Kendall's W < 0.30).

## Executive Summary
This study evaluates five Large Language Models (Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, Mistral 24B) for automated essay scoring using a four-criterion rubric. Across three replications per essay, models consistently failed to replicate human judgment, showing near-zero Quadratic Weighted Kappa scores and weak intra-model reliability. While models exhibited moderate agreement on structural criteria like Coherence and Originality, they struggled with context-sensitive criteria (Pertinence, Feasibility), revealing fundamental limitations in LLM-based interpretive assessment.

## Method Summary
The study scored 67 Italian-language student essays (2,500-3,000 words) from a Psychology MSc course using zero-shot inference via OpenRouter API. Five models were evaluated using a detailed system prompt with temperature=0.25, three replications per essay, and JSON output parsing. Agreement with human raters was measured using Quadratic Weighted Kappa, intra-model reliability with Kendall's W, and inter-model correlation with Spearman's ρ.

## Key Results
- All models showed consistently low agreement with human raters (QWK near zero, non-significant)
- Within-model reliability was weak (median Kendall's W < 0.30 across replications)
- Models exhibited systematic scoring biases, inflating Coherence scores while struggling with context-sensitive criteria
- Inter-model agreement was moderate for Coherence and Originality but negligible for Pertinence and Feasibility

## Why This Works (Mechanism)

### Mechanism 1
LLMs exhibit moderate convergent validity on structural/lexical criteria but near-zero alignment on context-dependent criteria requiring disciplinary expertise. Surface-level features (logical flow markers, vocabulary novelty) can be extracted from text patterns without domain grounding, while context-dependent judgments require integrating external knowledge about course objectives and resource constraints—knowledge not reliably encoded during inference.

### Mechanism 2
Low temperature sampling does not guarantee rank-order consistency across replications for evaluative tasks. Even at temperature = 0.25, token-level stochasticity compounds across long outputs, and since there's no ground truth anchoring during inference, the model has no corrective signal to maintain internal consistency.

### Mechanism 3
Mean score proximity to human averages can coexist with near-zero rank-order correlation, making aggregate metrics misleading for individual-level assessment. Models can calibrate to general score distribution without capturing variance that distinguishes high from low performers, functionally regressing toward the mean.

## Foundational Learning

- **Concept: Quadratic Weighted Kappa (QWK)**
  - Why needed: Measures agreement between ordinal raters while penalizing larger disagreements more heavily
  - Quick check: If Model A assigns scores 1 point higher than a human rater consistently across all essays, would QWK be high or low? (Answer: High—QWK measures rank alignment, not absolute agreement)

- **Concept: Kendall's W (Coefficient of Concordance)**
  - Why needed: Measures agreement among multiple raters ranking the same items
  - Quick check: If three model runs assign scores [8, 7, 9], [8, 7, 9], [8, 7, 9] to three essays, what would Kendall's W be? (Answer: 1.0—perfect concordance)

- **Concept: Negative skew and ceiling effects**
  - Why needed: Human ratings clustered at high scores, reducing variance and making rank discrimination harder
  - Quick check: Why might ceiling effects artificially inflate agreement metrics? (Answer: When most scores cluster at the top, random guessing also produces high scores, creating spurious agreement)

## Architecture Onboarding

- **Component map**: Essay text → anonymization → prompt construction → OpenRouter API → one of five LLMs → JSON output parser → score validation → structured storage → evaluation metrics
- **Critical path**: Ensure JSON parsing is robust, verify rubric-to-prompt alignment, maintain replication protocol with independent API calls
- **Design tradeoffs**: Zero-shot prompting vs. fine-tuning (zero-shot used for ecological validity), single-prompt multi-criteria vs. separate prompts per criterion (single prompt for efficiency), temperature = 0.25 vs. 0 (slight randomness retained to assess consistency)
- **Failure signatures**: QWK near zero with non-significant p-values (model not ranking essays like humans), Kendall's W < 0.30 (inconsistent rankings across replications), systematic positive bias on specific criteria
- **First 3 experiments**: Ablate criteria (single-criterion prompts), few-shot conditioning (add human-scored exemplars), deterministic baseline (temperature = 0 with fixed seed)

## Open Questions the Paper Calls Out

- **Question 1**: How do few-shot prompting strategies interact with rubric complexity to alter the validity of LLM assessment?
  - Basis: Authors state systematic investigation of prompt design and exemplar conditioning is necessary
  - Evidence needed: Comparative study measuring QWK for zero-shot vs. few-shot prompts across different rubric structures

- **Question 2**: To what extent do the low reliability and systematic biases generalize to other languages and academic disciplines?
  - Basis: Study focuses on single discipline (psychology) and language (Italian)
  - Evidence needed: Replication using English-language corpora or STEM-related essays to test cross-domain validity

- **Question 3**: Can specific prompt engineering or rubric refinement mitigate systematic inflation of scores (e.g., Coherence) exhibited by proprietary models?
  - Basis: Models exhibit tendency to inflate Coherence but variations in prompt formulation influence performance
  - Evidence needed: Ablation study varying "Coherence" instructions in system prompt to determine if stricter definitions reduce positive bias

## Limitations
- Ground-truth availability: Per-essay human scores needed for QWK calculation not disclosed in aggregate data
- Prompt generalizability: Zero-shot prompt is Italian-language and psychology-specific, limiting cross-domain claims
- Domain specificity: Causal mechanism linking low pertinence/feasibility agreement to missing domain grounding is inferred rather than experimentally tested

## Confidence

- **High confidence**: Intra-model inconsistency (Kendall's W < 0.30) and systematic bias toward Coherence are directly observable from reported data
- **Medium confidence**: Claim that context-dependent criteria fail due to missing domain grounding is mechanistically plausible but not empirically isolated
- **Low confidence**: Broad generalizations about all current LLMs being unsuitable for AES are overstated given study covers five models in one domain

## Next Checks

1. **Ablate criteria**: Run scoring with single-criterion prompts to test whether multi-criteria interference causes the Pertinence/Feasibility failures
2. **Few-shot conditioning**: Add 2-3 human-scored exemplars to the prompt and measure change in QWK and Kendall's W
3. **Deterministic baseline**: Re-run with temperature = 0 and fixed seed; quantify reduction in intra-model variance and whether human correlation improves