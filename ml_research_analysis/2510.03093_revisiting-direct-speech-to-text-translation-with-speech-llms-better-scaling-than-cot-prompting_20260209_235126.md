---
ver: rpa2
title: 'Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling
  than CoT Prompting?'
arxiv_id: '2510.03093'
source_url: https://arxiv.org/abs/2510.03093
tags:
- data
- s2tt
- speech
- direct
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Chain-of-Thought (CoT) and Direct prompting
  strategies for speech-to-text translation using large language models. The authors
  generate pseudo-labeled S2TT data by translating transcriptions from an ASR corpus
  into six European languages, then train LLM-based S2TT systems with both prompting
  strategies at different data scales.
---

# Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?
## Quick Facts
- arXiv ID: 2510.03093
- Source URL: https://arxiv.org/abs/2510.03093
- Reference count: 0
- Direct prompting exhibits more consistent improvement as S2TT data increases, while CoT performance peaks early and degrades

## Executive Summary
This paper compares Chain-of-Thought (CoT) and Direct prompting strategies for speech-to-text translation using large language models. The authors generate pseudo-labeled S2TT data by translating transcriptions from an ASR corpus into six European languages, then train LLM-based S2TT systems with both prompting strategies at different data scales. Results show that Direct prompting exhibits more consistent improvement as S2TT data increases, while CoT performance peaks early and degrades with more data. Direct prompting also maintains stable ASR and T2TT performance across data scales. The findings suggest Direct prompting may become a more effective approach as larger S2TT resources become available, offering practical advantages in computational efficiency and simplicity of implementation.

## Method Summary
The study compares Direct and CoT prompting strategies for speech-to-text translation using LLM-based architectures. Speech is encoded with mHuBERT-25Hz, quantized to 500 discrete tokens via k-means clustering, and integrated into an expanded-vocabulary LLM (SalamandraTA-7B-Instruct). Pseudo-labeled S2TT data is generated by translating CommonVoice transcriptions into six languages, filtered by quality estimation (BLASER 2.0) and language identification (GlotLID v3). Models are trained in two stages: first freezing the LLM to train speech embeddings on ASR, then full fine-tuning on mixed tasks. Scaling experiments evaluate performance across 20-100% increments of pseudo-labeled data.

## Key Results
- Direct prompting shows consistent improvement across all S2TT data scales, while CoT peaks at 20% and degrades with more data
- Direct prompting maintains stable ASR and T2TT performance, while CoT shows ASR degradation (+7-13% WER relative) at larger scales
- Direct prompting uses ~50% less compute than CoT due to shorter sequence lengths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct prompting exhibits more stable and consistent scaling with increasing pseudo-labeled S2TT data compared to CoT prompting.
- **Mechanism:** Direct prompting avoids the intermediate transcription bottleneck, allowing the model to learn end-to-end speech-to-translation mappings without being constrained to fixed speech-text alignments. Without explicit transcription training, the model can potentially leverage paralinguistic signals that would otherwise be lost.
- **Core assumption:** The benefit stems from removing the transcription constraint rather than from architectural differences (models are otherwise identical).
- **Evidence anchors:**
  - [abstract] "Direct improves more consistently as the amount of data increases"
  - [section 4, Figure 3a] "DIRECT AUG consistently improves, achieving a better score with DIRECT AUG100"
  - [corpus] Related paper "Listening or Reading?" corroborates that CoT systems may fail to exploit prosodic cues, supporting the hypothesis that direct approaches could capture speech-specific information.
- **Break condition:** If pseudo-label quality is extremely poor (e.g., systematic translation errors), direct prompting may fail to learn useful mappings since it cannot fall back on ASR capabilities.

### Mechanism 2
- **Claim:** CoT prompting degrades with scaled pseudo-labeled data primarily due to ASR sub-task degradation caused by data repetition patterns.
- **Mechanism:** In the CoT setting, each speech-transcription pair from the ASR corpus is repeated once per target language (six times in this study). This creates an imbalance where transcription targets appear with disproportionate frequency compared to genuine ASR training, potentially confusing the model's transcription capabilities. The intermediate ASR step becomes a performance bottleneck.
- **Core assumption:** The degradation is caused by the training data distribution shift, not by fundamental model capacity limits.
- **Evidence anchors:**
  - [section 3.2] "each speech–transcription pair in the pseudo-labeled data is repeated six times, one for each target language. We hypothesize this repetition may harm the ASR performance"
  - [section 4, Figure 3b] "WER remains stable with DIRECT AUG across all S2TTpl data scales, whereas both COT AUG and COT† AUG gradually increase it by up to +7.4% and +13% relative to their baseline"
  - [corpus] Weak direct corroboration; neighboring papers do not specifically address this repetition-induced degradation pattern.
- **Break condition:** If ASR data is sufficiently abundant relative to translation targets, or if transcription loss weighting is carefully tuned, this degradation may be mitigated.

### Mechanism 3
- **Claim:** Excluding transcription loss from CoT training (treating speech as context only) partially mitigates but does not eliminate degradation.
- **Mechanism:** By not training the transcription step in COT AUG (vs COT† AUG), pseudo-labeled S2TT data is effectively treated as T2TT with speech as context, reducing the ASR task proportion. However, some degradation persists, suggesting the issue extends beyond transcription loss alone—possibly involving capacity competition between tasks.
- **Core assumption:** The comparison between COT AUG and COT† AUG isolates the effect of transcription training.
- **Evidence anchors:**
  - [section 4] "COT† AUG consistently underperforms COT AUG in ASR and S2TT. Nonetheless, COT AUG also leads to some degradation"
  - [section 4] "A possible explanation is that, by not training the transcription step, all S2TT pl data is treated as T2TT with speech as context"
  - [corpus] No direct corpus evidence on this specific training variant comparison.
- **Break condition:** Assumption relies on the two COT variants differing only in transcription loss inclusion; other confounds could invalidate this mechanism.

## Foundational Learning

- **Concept: Speech Token Quantization**
  - **Why needed here:** The architecture relies on converting continuous self-supervised speech representations (mHuBERT) into discrete tokens that can be integrated into the LLM's vocabulary.
  - **Quick check question:** Can you explain how k-means clustering converts continuous speech features into discrete tokens, and why downsampling to 25 Hz matters for sequence length?

- **Concept: Pseudo-Labeling Quality Filtering**
  - **Why needed here:** The scaling experiments depend on pseudo-labeled data quality. Understanding QE (BLASER 2.0) and LID filtering explains why the approach works despite synthetic labels.
  - **Quick check question:** Why does the pipeline require both reference-free quality estimation AND language identification filtering? What failure modes does each prevent?

- **Concept: Two-Stage Training for Speech-LLM Adaptation**
  - **Why needed here:** The methodology uses frozen LLM training (stage 1: embeddings only) followed by full fine-tuning (stage 2). This is critical for stable speech integration.
  - **Quick check question:** What would likely happen if you skipped stage 1 and trained the entire model from scratch on S2TT data? Consider the randomly initialized speech embeddings.

## Architecture Onboarding

- **Component map:** Speech input → mHuBERT encoding → k-means quantization (500 clusters) → discrete token sequence → LLM with expanded vocabulary → output generation
- **Critical path:** Speech input → mHuBERT encoding → k-means quantization → discrete token sequence → concatenated with text prompt → LLM processing → beam search output
- **Design tradeoffs:**
  - 25 Hz downsampling: Halves temporal resolution vs standard HuBERT, reducing sequence length at cost of fine-grained acoustic detail
  - Direct vs CoT prompts: Direct uses ~50% of compute (shorter sequences), simpler implementation, but underperforms at low data regimes
  - Transcription loss in CoT: Including it (COT†) hurts performance; excluding it helps but doesn't fully solve degradation
- **Failure signatures:**
  - CoT with scaled pseudo-labels: WER increases +7-13%, S2TT peaks at 20% data then degrades
  - Direct at low data: 5 BLEU / 7 xCOMET gap vs CoT baseline
  - Poor pseudo-label quality: Would manifest as inconsistent scaling; not observed, suggesting filtering threshold (BLASER < 3.75, LID p_e < 0.5) is adequate
- **First 3 experiments:**
  1. Reproduce the baseline gap: Train DIRECT BASE and COT BASE with provided data specs (Table 1). Verify CoT outperforms Direct by ~5 BLEU on FLEURS. This validates your training pipeline matches the paper.
  2. Scale S2TTpl at 20% and 100%: Train DIRECT AUG20, DIRECT AUG100, COT AUG20, COT AUG100. Confirm Direct shows monotonic improvement while CoT peaks early then degrades. This is the core finding.
  3. Ablate transcription loss in CoT: Compare COT AUG (no transcription loss) vs COT† AUG (with transcription loss) at 60% scale. Verify that including transcription loss hurts ASR WER and downstream S2TT. This isolates the repetition mechanism.

## Open Questions the Paper Calls Out
- **Can Direct prompting eventually surpass the peak performance of CoT prompting if scaled with significantly larger amounts of S2TT data?** While Direct shows consistent upward scaling, the experiments only scaled up to 100% of available pseudo-labeled data, at which point CoT's peak performance (at 20% data) was still superior.
- **Is Direct prompting better suited than CoT for leveraging paralinguistic information, such as prosody, in speech translation?** The current study utilized datasets derived from ASR transcriptions, which inherently strip away prosodic and acoustic cues, making it impossible to evaluate this capability.
- **Can the degradation of the ASR sub-task in CoT prompting be prevented through adjustments to the data mixing ratios or training curriculum?** The experiments confirm degradation exists but do not explore whether explicitly oversampling original ASR data or modifying training curriculum could stabilize CoT performance at larger scales.

## Limitations
- Data quantity constraints limit scaling experiments to under 1,000 hours total S2TT data, leaving uncertainty about performance at truly large scales
- Limited language pairs (six European languages) may not generalize to non-European language families
- Synthetic data quality dependency - results rely on pseudo-labeled data quality without extensive characterization of filtering impact

## Confidence
- **High Confidence:** Direct prompting shows more consistent scaling with pseudo-labeled data than CoT (Section 4, Figure 3)
- **Medium Confidence:** Direct prompting will become more effective as larger S2TT resources become available (extrapolation from limited scale range)
- **Medium Confidence:** CoT degradation is primarily due to ASR sub-task degradation from data repetition (mechanism plausible but other factors possible)

## Next Checks
1. Scale Validation: Train Direct and CoT models at 200% and 500% of current maximum pseudo-labeled data to verify whether Direct's advantage continues to grow or plateaus
2. Cross-Lingual Generalization: Replicate experiments with non-European language pairs to test whether Direct/CoT performance patterns hold across different language families
3. Pseudo-Label Quality Sensitivity: Systematically vary BLASER QE threshold (3.5, 3.75, 4.0) and GlotLID confidence cutoffs to quantify how pseudo-label quality affects scaling behavior of Direct vs CoT approaches