---
ver: rpa2
title: Large (Vision) Language Models are Unsupervised In-Context Learners
arxiv_id: '2504.02349'
source_url: https://arxiv.org/abs/2504.02349
tags:
- unsupervised
- inference
- fine-tuning
- zero-shot
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised adaptation of foundation models
  to new tasks without labeled data or manual prompt engineering. The core idea is
  joint inference over multiple inputs, optimizing the model's joint predictive probability,
  enabling consistent predictions across examples.
---

# Large (Vision) Language Models are Unsupervised In-Context Learners

## Quick Facts
- arXiv ID: 2504.02349
- Source URL: https://arxiv.org/abs/2504.02349
- Authors: Artyom Gadetsky; Andrei Atanov; Yulun Jiang; Zhitong Gao; Ghazal Hosseini Mighan; Amir Zamir; Maria Brbic
- Reference count: 40
- One-line primary result: Unsupervised methods achieve up to 39% absolute gains over zero-shot and match supervised performance on tasks like GSM8K without labeled data

## Executive Summary
This paper introduces a framework for unsupervised adaptation of large (vision) language models to new tasks without labeled data or manual prompt engineering. The core innovation is leveraging joint inference over multiple examples to enforce task-level consistency, optimizing the model's joint predictive probability rather than individual predictions. Two complementary methods are proposed: unsupervised fine-tuning that trains a LoRA adapter to maximize this objective, and unsupervised in-context learning that iteratively refines predictions using the model's own outputs as pseudo-labels. Experiments across diverse tasks and models demonstrate substantial improvements over zero-shot inference, with some methods matching or exceeding supervised approaches despite having no access to ground truth labels.

## Method Summary
The paper addresses unsupervised adaptation by optimizing the joint predictive probability across batches of inputs. Unsupervised ICL iteratively refines pseudo-labels through Gibbs-sampling-like updates, where each example's prediction is refined based on the current context of other predictions. Unsupervised FT trains a LoRA adapter using REINFORCE to generate outputs that maximize the frozen model's joint probability, with entropy regularization preventing mode collapse. Both methods require only unlabeled data and work across diverse tasks including classification, math reasoning, and image classification, with the joint objective enforcing consistency that improves overall accuracy.

## Key Results
- Unsupervised ICL and FT achieve up to 39% absolute accuracy gains over zero-shot inference on GSM8K
- Methods match or exceed supervised ICL performance on multiple tasks despite no labeled data
- Unsupervised FT consistently outperforms unsupervised ICL when white-box access is available
- Framework works across diverse models (Llama-3.1, Qwen2.5-Math, OpenFlamingo, GPT-4o) and task types

## Why This Works (Mechanism)

### Mechanism 1: Joint Inference Consistency
Standard zero-shot inference maximizes $p(y|x)$ independently, while this framework maximizes the joint likelihood $p(y_1, \dots, y_N | x_1, \dots, x_N)$ across a batch. By conditioning predictions on the context of other examples' labels, the model leverages task-level consistency—if 9/10 examples predict "Positive," the 10th is pressured toward "Positive" when the task structure allows. Performance degrades to zero-shot if context length $N$ is set to 1.

### Mechanism 2: Implicit Gibbs Sampling (Unsupervised ICL)
This method initializes labels via zero-shot, then iteratively updates each label by conditioning on a random set of other examples labeled with previous outputs. This resembles Gibbs sampling, where the model corrects low-probability predictions based on the evolving context. The iterative refinement effectively "denoises" initial predictions, with the model's zero-shot performance serving as the initialization. Fails if the model lacks intrinsic ICL capability.

### Mechanism 3: Self-Training via Amortized Optimization (Unsupervised FT)
Instead of directly optimizing the combinatorial search over discrete tokens, a continuous parameter set $\theta$ (LoRA) is trained to generate outputs that maximize the frozen model's joint probability. The model generates answers, evaluates their joint probability, and updates $\theta$ via REINFORCE. Without entropy regularization, the optimization collapses to predicting the single most frequent class for all inputs.

## Foundational Learning

**In-Context Learning (ICL)**
- *Why needed here:* Unsupervised ICL relies entirely on the model's ability to change predictions based solely on the context of other $(x, y)$ pairs provided in the prompt window.
- *Quick check question:* Can you explain why providing labeled examples in a prompt window changes a model's output distribution without weight updates?

**Reinforcement Learning (REINFORCE)**
- *Why needed here:* Unsupervised Fine-Tuning optimizes a non-differentiable objective (joint probability of discrete tokens). Policy gradients are required to understand how the model updates weights using the reward signal from joint probability.
- *Quick check question:* How does the REINFORCE gradient estimator allow backpropagation through discrete sampling steps?

**Mode Collapse / Degenerate Solutions**
- *Why needed here:* A critical failure mode is the model predicting the same label for every input. Understanding this motivates the specific entropy-based regularization term added to the loss function.
- *Quick check question:* Why might an unsupervised objective maximizing accuracy converge to predicting only the majority class?

## Architecture Onboarding

**Component map:**
Foundation Model ($p_{FM}$) -> Task Encoder ($\tau_\theta$) -> Joint Probability Estimator -> Regularizer

**Critical path:**
1. Batch $N$ inputs
2. (FT) Sample predictions via LoRA → Compute Joint Log-Prob → REINFORCE update / (ICL) Zero-shot init → Resample context → Repredict
3. Apply entropy penalty
4. Repeat until convergence

**Design tradeoffs:**
- Unsupervised FT vs. ICL: FT requires white-box access and is computationally expensive but generally performs better. ICL requires only black-box access and is cheaper at inference but potentially less accurate.
- Context Length ($N$): Higher $N$ improves consistency but increases context window usage and compute.

**Failure signatures:**
- Trivial Solution: Accuracy stays constant at a specific ratio (e.g., 50/50 or 100% one class). Check the entropy regularizer weight ($\gamma$).
- No Improvement over Zero-Shot: The base model lacks sufficient ICL capability for the specific task domain.

**First 3 experiments:**
1. **Sanity Check (ICL):** Run Unsupervised ICL on SST2 with $N=16$. Verify performance exceeds Zero-Shot.
2. **Ablation on Collapse (FT):** Run Unsupervised FT without regularizer ($\gamma=0$). Observe if the model predicts only the most frequent class.
3. **Scaling Test:** Vary $N$ (1, 4, 8, 16) to confirm joint inference correlates with accuracy improvements.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can the unsupervised fine-tuning framework be extended to open-ended tasks with unbounded output spaces?
- Basis in paper: The authors state it's limited to close-ended tasks with finite label sets, suggesting more advanced amortization optimization techniques as potential solutions.
- Why unresolved: The renormalization over Y in Eq. (6) and Bag-of-Tokens approximation assume a finite, enumerable output space.
- What evidence would resolve it: A modified framework handling free-form text generation or code synthesis demonstrating comparable improvements.

**Open Question 2**
- Question: What is the precise relationship between a model's ICL capability and the effectiveness of joint inference?
- Basis in paper: The authors note the underlying model should exhibit ICL capabilities, with Appendix D.5 showing smaller models can benefit when using larger models for objective computation.
- Why unresolved: The exact threshold or functional relationship between ICL capability and joint inference gains remains undefined.
- What evidence would resolve it: Systematic study across models correlating supervised ICL improvement ratios with unsupervised joint inference improvement ratios.

**Open Question 3**
- Question: Under what conditions does unsupervised ICL outperform supervised ICL, as observed with Qwen2.5-Math on GSM8K (91.4% vs 89.9%)?
- Basis in paper: Table 2 shows this counterintuitive result where unsupervised ICL surpasses supervised ICL without mechanistic explanation.
- Why unresolved: Standard expectation is ground truth labels should provide stronger signal, yet the opposite occurs in some cases.
- What evidence would resolve it: Controlled experiments varying label noise rates, reasoning chain quality, and model confidence.

## Limitations

- Primary methods fail when the base model cannot perform the task at all without adaptation, as both require non-trivial zero-shot performance
- Computational cost of unsupervised fine-tuning (thousands of iterations, multiple GPU-hours) may limit practical adoption
- Framework struggles with open-ended tasks requiring unbounded output spaces due to reliance on finite label sets

## Confidence

- **High Confidence:** Joint inference over batches is clearly defined and experimentally validated across diverse tasks and models
- **Medium Confidence:** Self-improvement mechanism in ICL is intuitive but exact convergence conditions are not rigorously characterized
- **Medium Confidence:** REINFORCE-based fine-tuning is well-grounded in theory but practical stability depends on complex control variate implementation

## Next Checks

1. **Generalization Across Model Sizes:** Replicate experiments on smaller models (1B-3B parameters) to verify gains scale down
2. **Task-Specific Consistency Analysis:** For multi-class classification, analyze whether joint inference actually enforces label consistency across similar inputs
3. **Robustness to Label Ambiguity:** Test on datasets with inherently ambiguous labels to see if consistency pressure leads to stable outputs or amplifies uncertainty