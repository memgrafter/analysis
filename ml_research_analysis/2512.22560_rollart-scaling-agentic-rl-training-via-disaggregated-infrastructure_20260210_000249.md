---
ver: rpa2
title: 'RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure'
arxiv_id: '2512.22560'
source_url: https://arxiv.org/abs/2512.22560
tags:
- training
- rollout
- environment
- wang
- gpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RollArt is a system designed to improve the efficiency of agentic
  reinforcement learning (RL) training for large language models (LLMs) on disaggregated
  infrastructure. Agentic RL workloads are highly heterogeneous, combining compute-intensive
  prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations,
  making efficient training challenging on monolithic clusters.
---

# RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure

## Quick Facts
- **arXiv ID**: 2512.22560
- **Source URL**: https://arxiv.org/abs/2512.22560
- **Reference count**: 40
- **One-line primary result**: RollArt achieves 1.35-2.05× end-to-end training time reduction for agentic RL on disaggregated infrastructure compared to monolithic baselines.

## Executive Summary
RollArt is a system designed to improve the efficiency of agentic reinforcement learning (RL) training for large language models (LLMs) on disaggregated infrastructure. Agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations, making efficient training challenging on monolithic clusters. RollArt addresses this by leveraging disaggregated infrastructure and implementing three core design principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to best-fit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components to serverless infrastructure for elastic scaling. The system achieves 1.35-2.05× end-to-end training time reduction compared to monolithic and synchronous baselines and demonstrates scalability and robustness by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs.

## Method Summary
RollArt implements a disaggregated RL training system with trajectory-level asynchronous execution, hardware-affinity workload mapping (prefill-heavy tasks to compute-optimized GPUs like H800, decode-heavy to memory-optimized GPUs like H20), and stateless component offloading to serverless infrastructure. The system uses GRPO algorithm with batch size 512 and group size 8, asynchronous training with bound=1, vLLM 0.8.4 for rollout with prefix caching and CUDA graphs, Megatron v0.12.2 for training, and Mooncake for cross-cluster weight synchronization. The rollout phase is decoupled into LLMProxy and EnvManager components for non-blocking trajectory execution, while stateless reward computation is handled via serverless "Reward-as-a-Service" to maximize GPU utilization.

## Key Results
- Achieves 1.35-2.05× end-to-end training time reduction compared to monolithic and synchronous baselines
- Increases average GPU utilization from 6% (local reward computation) to 88% (serverless reward computation)
- Successfully trains a hundreds-of-billions-parameter MoE model on an Alibaba cluster with more than 3,000 GPUs

## Why This Works (Mechanism)

### Mechanism 1: Hardware-Affinity Workload Mapping
- **Claim:** Routing compute-bound prefill phases and bandwidth-bound decoding phases to distinct GPU types improves throughput.
- **Mechanism:** The system decouples the LLM generation stage into distinct hardware pools. Prefill-heavy trajectories (e.g., FrozenLake) are routed to high-TFLOPS devices (H800), while decoding-dominant trajectories (e.g., GEM-Math) are routed to high-memory-bandwidth devices (H20).
- **Core assumption:** The workload distribution is bimodal (prefill-heavy vs. decode-heavy) and can be classified a priori.
- **Evidence anchors:** [abstract] Mentions routing tasks to best-fit GPU devices; [section 4.1] Describes mapping trajectories based on dominance; [corpus] StreamRL supports disaggregated generation for RL.
- **Break condition:** If workload is mixed or balanced such that routing overhead exceeds execution savings, or if network connecting disaggregated pools becomes bottleneck.

### Mechanism 2: Trajectory-Level Asynchrony
- **Claim:** Managing execution at the granularity of individual trajectories prevents "resource bubbles" caused by straggling environments.
- **Mechanism:** The `EnvManager` and `LLMProxy` decouple environment interaction from token generation. While one trajectory waits for a slow environment step, the LLM engine continues decoding for other available trajectories.
- **Core assumption:** Environment latency variance is high (long-tailed), and LLM generation can be efficiently interleaved without significant context-switching overhead.
- **Evidence anchors:** [abstract] States it manages execution at trajectory level to mitigate resource bubbles; [section 5.3] Details how `LLMProxy` allows adding/aborting requests without stalling engine.
- **Break condition:** If environment latencies are uniform, scheduling overhead might outweigh asynchrony benefits.

### Mechanism 3: Statefulness-Aware Offloading
- **Claim:** Offloading stateless components (specifically reward models) to serverless infrastructure maximizes training cluster utilization.
- **Mechanism:** Reward computation is treated as stateless function (Reward-as-a-Service) via `register_serverless`, avoiding reserving idle GPUs for rewards.
- **Core assumption:** Serverless platform provides low-enough latency to not become bottleneck for RL loop.
- **Evidence anchors:** [abstract] Notes offloads stateless components to serverless infrastructure; [section 7.5] Shows GPU utilization increasing from 6% to 88%; [corpus] No direct corpus support for specific serverless offload mechanism.
- **Break condition:** If reward model requires shared memory/state with training process, or if serverless cold starts introduce delays greater than synchronous computation time.

## Foundational Learning

- **Concept:** **Disaggregated Computing**
  - **Why needed here:** RollArt relies on physically separating resources (CPUs for envs, H800s for training, H20s for decoding) connected by network fabric.
  - **Quick check question:** Can you explain why separating the *prefill* and *decode* phases of an LLM might require different hardware optimizations (Compute vs. Memory Bandwidth)?

- **Concept:** **Agentic RL Pipeline (Rollout $\to$ Reward $\to$ Train)**
  - **Why needed here:** The paper optimizes specific interactions between these three stages.
  - **Quick check question:** In the context of this paper, why is the *Environment* considered stateful while the *Reward Model* is considered stateless?

- **Concept:** **Asynchronous Bound (Staleness)**
  - **Why needed here:** The system trades policy freshness for throughput. The "bound" controls how stale a trajectory can be before it is aborted.
  - **Quick check question:** If the asynchronous bound is set too high, what theoretical risk does this introduce to the training stability of the agent?

## Architecture Onboarding

- **Component map:**
  - Control Plane: `Resource Manager` (allocates heterogeneous hardware)
  - Runtime: `Cluster` abstraction (orchestrates workers)
  - Rollout: `LLMProxy` (non-blocking gateway) + `EnvManager` (per-trajectory controller)
  - Training: Standard distributed engines (e.g., Megatron) integrated via `Cluster`
  - Storage: `SampleBuffer` (holds trajectories) + Mooncake (weight sync)

- **Critical path:**
  1. **Env Reset:** `EnvManager` initializes environment
  2. **Generation:** `LLMProxy` interleaves prefill/decode requests across GPU pools
  3. **Reward:** `EnvManager` offloads result to serverless `Reward-as-a-Service`
  4. **Update:** `SampleBuffer` fills $\to$ Training consumes $\to$ Mooncake broadcasts weights

- **Design tradeoffs:**
  - **Synchronous vs. Asynchronous Training:** Sync guarantees consistency but creates bubbles; Async (RollArt's choice) maximizes hardware usage but requires tuning "asynchronous bound" to prevent gradient instability
  - **Granularity:** Trajectory-level scheduling reduces bubbles but increases orchestration complexity compared to batch-level

- **Failure signatures:**
  - **Long-tail Latency:** Spikes in `env.reset` or `env.step` (Figure 5a) causing stalls in synchronous systems
  - **Utilization Drop:** Low GPU usage on dedicated reward workers (Figure 6)
  - **Network Contention:** Cross-cluster weight synchronization stalling rollout on low-bandwidth Ethernet links

- **First 3 experiments:**
  1. **Hardware Affinity Validation:** Run a prefill-heavy task (FrozenLake) and a decode-heavy task (GEM-Math) on both H800 and H20 clusters. Verify that the performance delta matches Figure 4.
  2. **Straggler Resilience:** Inject artificial latency into a subset of environments. Compare throughput of batch-level execution vs. RollArt's trajectory-level execution (Figure 12a).
  3. **Serverless Overhead:** Measure end-to-end latency of reward computation when running locally on dedicated GPU versus serverless implementation (Figure 14). Check if serverless cold-start impacts critical path.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can disaggregated prefill and decoding instances be automatically balanced to prevent load imbalances without manual configuration?
- **Basis in paper:** [explicit] Section 5.2 states that real deployments currently require manual configuration of prefill and decoding instances, which easily leads to load imbalance, and explicitly leaves this as "future work."
- **Why unresolved:** The current RollArt implementation relies on manual setup for these disaggregated components, risking inefficiency.
- **What evidence would resolve it:** A dynamic scheduler that automatically adjusts resource allocation between prefill and decoding phases based on real-time load.

### Open Question 2
- **Question:** Can the asynchronous bound $\alpha$ be dynamically adjusted during training to better balance gradient stability and system throughput?
- **Basis in paper:** [inferred] Section 7.4 notes that the optimal bound differs across LLMs and efficiency plateaus within certain ranges, yet the system relies on a fixed static value (e.g., bound=1).
- **Why unresolved:** A static bound cannot adapt to changing compute/communication characteristics of different models or training stages.
- **What evidence would resolve it:** An adaptive policy for $\alpha$ that improves or maintains convergence speed compared to the best static baseline across diverse models.

### Open Question 3
- **Question:** Can hardware-affinity mapping be automated based on runtime profiling rather than user-defined tags?
- **Basis in paper:** [inferred] Section 5.1 describes a declarative model where users must manually define hardware affinities (e.g., `hw_mapping` decorators), assuming users know optimal hardware for specific tasks.
- **Why unresolved:** Requiring manual tags places burden on user and may fail if workload characteristics shift during execution.
- **What evidence would resolve it:** A system capability that automatically routes trajectories to optimal hardware (e.g., H20 vs H800) by analyzing prefill/decoding ratios on the fly.

## Limitations
- Serverless reward-offload mechanism's performance is contingent on availability and configuration of production-grade function platform, which is not openly specified
- Effectiveness of hardware-affinity workload mapping relies on assumption of bimodal workload distribution that may not generalize to all agentic RL tasks
- Automatic balancing of disaggregated prefill and decoding instances remains unresolved, requiring manual configuration that risks inefficiency

## Confidence

- **High confidence** in the trajectory-level asynchrony mechanism, as it is directly supported by observable execution patterns in agentic RL (long-tailed environment latencies) and the paper's empirical results
- **Medium confidence** in the hardware-affinity workload mapping, given its logical alignment with GPU microarchitecture but reliance on workload classification that may not be robust for all tasks
- **Medium confidence** in the serverless reward-offload, as the performance gain is dramatic but the underlying infrastructure is proprietary and not reproducible without significant engineering effort

## Next Checks

1. **Reproduce the hardware-affinity claim**: Run a suite of prefill-heavy (e.g., FrozenLake) and decode-heavy (e.g., GEM-Math) tasks on both H800 and H20 GPUs. Measure throughput and confirm that the performance ranking matches the paper's Figure 4, validating that the mapping heuristic is effective.

2. **Validate the asynchronous bound's impact on stability**: Conduct an ablation study by training with different `async_bound` values (e.g., 1, 2, 4, 8) and monitor the learning curve for signs of instability or divergence. This will quantify the tradeoff between throughput and policy freshness.

3. **Benchmark serverless vs. local reward computation**: Implement a mock reward service using an open-source serverless framework (e.g., Knative) and compare the end-to-end latency of reward computation against a dedicated GPU worker. Measure the overhead and confirm whether the 88% utilization gain is achievable in a non-proprietary environment.