---
ver: rpa2
title: Leveraging the Potential of Prompt Engineering for Hate Speech Detection in
  Low-Resource Languages
arxiv_id: '2506.23930'
source_url: https://arxiv.org/abs/2506.23930
tags:
- hate
- speech
- prompting
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hate speech detection in
  low-resource languages, focusing on Bengali. The authors propose a novel metaphor
  prompting technique to circumvent built-in safety mechanisms of large language models
  (LLMs) for improved hate speech classification.
---

# Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages

## Quick Facts
- **arXiv ID:** 2506.23930
- **Source URL:** https://arxiv.org/abs/2506.23930
- **Reference count:** 40
- **Primary result:** Metaphor prompting achieves comparable or superior F1 scores compared to conventional models and other prompting strategies, particularly in low-resource languages.

## Executive Summary
This paper addresses the challenge of hate speech detection in low-resource languages, focusing on Bengali. The authors propose a novel metaphor prompting technique to circumvent built-in safety mechanisms of large language models (LLMs) for improved hate speech classification. They investigate six prompting strategies on the Llama2-7B model and compare performance with three deep learning models across four datasets (Bengali, English, German, Hindi). The results show that metaphor prompting delivers superior F1 scores compared to conventional models and other prompting strategies, particularly in low-resource languages. The study also considers environmental impact factors, highlighting the potential of metaphor prompting to improve sustainability in NLP tasks.

## Method Summary
The study employs Llama2-7B with LoRA fine-tuning for hate speech detection across four multilingual datasets. Non-English datasets are translated to English via Google Translate before classification. Six prompting strategies are investigated: zero-shot, refusal suppression, flattering the classifier, multi-shot, role prompting, and the novel metaphor prompting technique. The method involves preprocessing (stop word removal, lemmatization), translation chain for non-English inputs, subsampling to 500 entries per language, LoRA fine-tuning on combined multilingual data, and metaphor prompting with various symbolic label pairs. Performance is evaluated using weighted F1 score and Environmental Impact Factor measuring CO2, electricity, and compute time.

## Key Results
- Metaphor prompting achieves 95.89 F1 score for Bengali, outperforming other prompting strategies and conventional deep learning models
- Summer-winter metaphor pair performs best for Bengali (95.89 F1) and English (77.74 F1), while rose-thorn excels for Hindi (87.15 F1) and English
- The approach demonstrates comparable or superior performance to conventional deep learning models while reducing environmental impact through parameter-efficient fine-tuning
- Llama2-7B's built-in safety mechanisms cause refusal responses when prompted with direct "hate speech" terminology, which metaphor prompting successfully circumvents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metaphor prompting bypasses LLM safety guardrails by replacing emotionally charged trigger words with neutral symbolic equivalents.
- Mechanism: Llama2-7B refuses classification when prompted with direct terms like "hate speech" due to built-in ethical safeguards. By substituting class labels with metaphor pairs (red/green, rose/thorn, summer/winter, honey/venom), the model processes the semantic content without activating refusal responses, then maps the classification to the appropriate category.
- Core assumption: The model's semantic understanding can associate metaphor labels with classification categories without explicit hate-related terminology activating guardrails.
- Evidence anchors:
  - [abstract]: "We pioneer the metaphor prompting to circumvent the built-in safety mechanisms of LLMs that marks a significant departure from existing jailbreaking methods."
  - [section V-B, page 6-7]: "The Llama2-7B model is found to be particularly sensitive to the word 'hate'... It mostly generates generic replies like, 'I cannot classify your message as it goes against ethical and moral standards.'"
  - [corpus]: No direct corpus precedent found for metaphor-based jailbreaking—this appears to be a novel technique in the hate speech detection literature.
- Break condition: Metaphors that are too abstract, culturally ambiguous, or lack clear binary opposition may fail to establish reliable semantic mapping.

### Mechanism 2
- Claim: Chain-of-translation prompting enables low-resource language processing by leveraging Llama2-7B's stronger English comprehension.
- Mechanism: Non-English datasets (Bengali, Hindi, German) are translated to English via Google Translate before classification. This compensates for Llama2-7B's limited semantic understanding of low-resource languages by routing through its dominant training language.
- Core assumption: Translation preserves sufficient semantic and contextual information for hate speech detection despite potential noise from machine translation.
- Evidence anchors:
  - [section IV-C1, page 5]: "The Bengali, Hindi, and German datasets are first translated into English by Google Translator due to Llama-2's inability to comprehend the semantic content of texts in these languages."
  - [section IV-C1, page 5]: "This process is known as the chain of translation prompting and is adopted to effectively handle multilingual datasets using LLMs."
  - [corpus]: Related work "Can Prompting LLMs Unlock Hate Speech Detection across Languages?" similarly explores cross-lingual prompting but focuses on zero-shot/few-shot approaches rather than translation chains.
- Break condition: Code-mixed content, dialectal variations, or culturally-specific slurs may lose critical meaning in translation, reducing detection accuracy.

### Mechanism 3
- Claim: Learning-from-mistakes prompting improves classification by providing explicit corrective feedback on misclassified examples.
- Mechanism: Rather than only providing correct examples (standard few-shot), this approach includes examples the model previously misclassified along with explanations of why they were wrong, enabling the model to learn error patterns explicitly.
- Core assumption: The model can generalize corrective patterns from specific error examples to unseen instances.
- Evidence anchors:
  - [section V-A, page 6]: "A 'learning from mistakes' strategy is experimented with multi-shot prompting, providing the model with some misclassified data and corresponding explanations, to improve its understanding of error patterns."
  - [Table IV, page 9]: V11 (Learning from mistakes) achieves 59.09 F1 for Bengali versus 33.14-38.44 for standard 4/8/16-shot prompting.
  - [corpus]: Limited corpus precedent for learning-from-mistakes in hate speech specifically; related technique mentioned in general few-shot learning literature.
- Break condition: If error examples are not representative of diverse failure modes, generalization will be constrained.

## Foundational Learning

- Concept: **Zero-shot vs. Few-shot Prompting**
  - Why needed here: The paper tests six prompting strategies; understanding the baseline distinction is essential before interpreting comparative results.
  - Quick check question: Why does zero-shot prompting (V1 in Table II) yield F1 scores of only 33-38% across languages?

- Concept: **LLM Safety Guardrails and Jailbreaking**
  - Why needed here: The core innovation is bypassing ethical refusals that block hate speech classification tasks.
  - Quick check question: What specific trigger word causes Llama2-7B to refuse classification, and how does metaphor prompting avoid this?

- Concept: **LoRA (Low-Rank Adaptation) for Parameter-Efficient Fine-Tuning**
  - Why needed here: The paper fine-tunes Llama2-7B using LoRA; understanding this technique explains the training setup.
  - Quick check question: Why would LoRA be preferred over full fine-tuning when adapting a 7B-parameter model for a downstream classification task?

## Architecture Onboarding

- Component map:
  Input Layer -> Preprocessing -> Translation Layer -> Tokenization -> Fine-tuning -> Prompting Layer -> Output Parser -> Evaluation

- Critical path:
  1. Dataset preprocessing (language-specific stop word lists, lemmatization)
  2. Chain-of-translation for non-English inputs
  3. Subsampling to 500 entries per language (resource constraint)
  4. LoRA fine-tuning on combined multilingual data
  5. Apply metaphor prompting with selected pair (e.g., summer-winter for Bengali)
  6. Parse output JSON for class label extraction
  7. Compute F1 and environmental Impact Factor

- Design tradeoffs:
  - **Translation vs. direct multilingual**: Translation enables Llama2's English strength but introduces noise; alternative would require multilingual-native LLMs
  - **Sample size (500) vs. full dataset**: Manages computational cost but may limit generalization; trade-off between experiment coverage and resource constraints
  - **Metaphor pair selection**: Summer-winter achieves highest F1 for Bengali (95.89), but rose-thorn performs better for Hindi (87.15) and English (77.74)—no universal optimal pair

- Failure signatures:
  - **High refusal rate in output**: Model returns "I cannot classify..." → Safety guardrail triggered; switch to more abstract metaphor pair
  - **Low F1 on specific language despite good metaphor**: Check translation quality; consider language-specific preprocessing adjustments
  - **High Impact Factor (>0.5)**: Computational overhead too high; reduce prompt complexity, consider baseline deep learning models instead

- First 3 experiments:
  1. Run zero-shot baseline on all four languages to quantify base refusal rate and establish F1 floor (expect 33-40% per Table II).
  2. Test metaphor prompting with red/green pair across all datasets to verify jailbreak success (expect significant F1 improvement per Table VIII).
  3. Compare summer-winter vs. rose-thorn on Bengali dataset to identify optimal metaphor pair for low-resource target language (Table VIII shows 95.89 vs. 93.18 F1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can metaphor prompting maintain its effectiveness when applied natively to low-resource languages without relying on a "chain of translation" into English?
- Basis in paper: [inferred] The methodology section states that datasets (Bengali, Hindi, German) were translated into English because "Llama-2's inability to comprehend the semantic content of texts in these languages."
- Why unresolved: It is undetermined if the performance boost stems from the prompting strategy itself or the model's stronger semantic grasp of the English translations; the authors acknowledge that "some translation noise is added."
- What evidence would resolve it: Experiments where metaphor prompts are constructed directly in Bengali or Hindi and processed by a multilingual LLM without intermediate English translation.

### Open Question 2
- Question: Does the efficacy of metaphor prompting generalize to other Large Language Model (LLM) architectures beyond Llama2-7B?
- Basis in paper: [inferred] The study restricts its investigation to the Llama2-7B model, selected for being "open-sourced and reliable," leaving the transferability of the "jailbreaking" technique untested on other models.
- Why unresolved: The safety mechanisms bypassed in Llama2 may be implemented differently or more robustly in other proprietary or open-source models (e.g., GPT-4, Mistral).
- What evidence would resolve it: A comparative analysis applying the specific "red/green" or "rose/thorn" metaphor prompts to other distinct LLM architectures using the same datasets.

### Open Question 3
- Question: How does metaphor prompting perform when scaling from subsampled datasets (500 instances) to full-scale, large-scale inference?
- Basis in paper: [inferred] The authors note that "experiments are restricted to 500 instances... to manage the intensive resource demands," suggesting the results may not reflect performance on the complete datasets.
- Why unresolved: The consistency of the F1 score and the environmental impact factor (IF) might fluctuate significantly when processing the full 50k+ entries of the Bengali dataset compared to the small subsample.
- What evidence would resolve it: Running the metaphor prompting strategy on the complete Bengali and English datasets to evaluate if the accuracy and computational efficiency hold steady at scale.

## Limitations

- **Translation dependency**: The reliance on Google Translate for non-English datasets introduces an uncontrolled variable that may explain performance differences and add noise to the classification task.
- **Sample size representativeness**: The 500-instance subsample across all four languages may not capture the full distribution of hate speech patterns, especially in the Bengali dataset which contains 8,499 instances.
- **LoRA configuration specificity**: While LoRA is implemented for parameter-efficient fine-tuning, specific hyperparameters are either missing or vaguely referenced, limiting reproducibility.

## Confidence

**High Confidence** - Metaphor Prompting Mechanism: The core innovation of replacing trigger words with neutral metaphors is well-documented through empirical results. Multiple metaphor pairs consistently outperform zero-shot and other prompting strategies across languages.

**Medium Confidence** - Chain-of-Translation: While the translation approach is clearly stated and logically sound, the quality and consistency of Google Translate outputs for hate speech detection is unverified. The method works empirically but depends on an external system with unknown properties for this specific task.

**Medium Confidence** - Learning-from-Mistakes: The V11 results suggest this strategy improves performance, but the limited number of misclassified examples and their representativeness for generalization is unclear.

## Next Checks

1. **Translation Quality Validation**: Re-run the metaphor prompting experiments on the full Bengali dataset (8,499 instances) without translation to English, using a multilingual LLM or direct Bengali prompting. Compare F1 scores to isolate the contribution of translation versus metaphor prompting.

2. **Ablation Study on Metaphor Pairs**: Systematically test each of the four metaphor pairs (red-green, summer-winter, rose-thorn, honey-venom) across all four languages with identical training configurations. Include a "no metaphor" baseline that uses direct classification terms to quantify the exact contribution of each pair and identify universal versus language-specific patterns.

3. **Controlled Safety Bypass Experiment**: Design a controlled experiment that tests whether metaphor prompting truly bypasses safety mechanisms versus simply providing better semantic context. Use a simplified binary classification task with clear non-sensitive labels to compare metaphor versus direct prompting, isolating the safety bypass effect from classification performance.