---
ver: rpa2
title: 'From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training'
arxiv_id: '2508.09224'
source_url: https://arxiv.org/abs/2508.09224
tags:
- safety
- helpfulness
- user
- prompts
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models typically use binary refusal training to\
  \ block harmful requests, but this approach can be brittle\u2014especially for dual-use\
  \ scenarios where legitimate requests could be misused. The proposed safe-completion\
  \ method shifts focus from refusing prompts to ensuring the safety of the model\u2019\
  s output, allowing the model to provide helpful, non-harmful guidance where possible."
---

# From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training

## Quick Facts
- **arXiv ID**: 2508.09224
- **Source URL**: https://arxiv.org/abs/2508.09224
- **Reference count**: 36
- **Primary result**: Safe-completion training enables models to provide helpful, non-harmful guidance on dual-use prompts where refusal-based approaches block all assistance.

## Executive Summary
Large language models typically use binary refusal training to block harmful requests, but this approach can be brittle—especially for dual-use scenarios where legitimate requests could be misused. The proposed safe-completion method shifts focus from refusing prompts to ensuring the safety of the model's output, allowing the model to provide helpful, non-harmful guidance where possible. In experiments comparing GPT-5 with safe-completion training to o3 trained with refusals, the safe-completion approach improved safety across benign, dual-use, and malicious prompts, while substantially increasing helpfulness and reducing the severity of residual failures. Human evaluations confirmed these findings, showing safer and more useful responses. Safe-completion thus offers a more nuanced, output-focused alternative to traditional refusal-based safety training.

## Method Summary
The safe-completion approach employs a two-stage post-training pipeline built on Deliberative Alignment. First, supervised fine-tuning (SFT) teaches the model three response modes—direct answer, safe-completion, or refusal with redirection—using spec-augmented prompts and reasoning models to generate training data. Second, reinforcement learning optimizes a composite reward combining safety adherence and helpfulness, where safety adherence is scored [0,1] and helpfulness is [0,1] (direct or indirect). The method targets outputs rather than inputs, allowing the model to provide high-level, non-facilitative information even on potentially harmful prompts.

## Key Results
- Safe-completion improved safety scores across benign, dual-use, and malicious prompts compared to refusal-trained baselines
- Helpfulness increased substantially while reducing harm severity from High to Negligible/Low on residual failures
- Human evaluations confirmed safer and more useful responses with better balance between safety and helpfulness

## Why This Works (Mechanism)
The method works by decoupling prompt classification from output safety, allowing models to generate contextually appropriate responses rather than defaulting to binary refusal. By training on spec-augmented prompts that explicitly define safety boundaries and using composite rewards that balance safety with helpfulness, the model learns to provide useful information while avoiding meaningful facilitation of harm. The two-stage pipeline first establishes reasoning patterns over safety specifications, then reinforces these patterns through reward optimization.

## Foundational Learning

**Deliberative Alignment** - Teaching models to reason over policy specifications via chain-of-thought during training
- Why needed: Enables models to dynamically apply safety policies rather than relying on static refusal rules
- Quick check: Verify model can articulate reasoning about why certain outputs are or aren't safety violations

**Safe-completion vs. Refusal** - Alternative response modes beyond binary refusal
- Why needed: Allows provision of helpful information on dual-use prompts without enabling harm
- Quick check: Test model responses on borderline cases to ensure appropriate mode selection

**Composite Reward Structure** - Multiplicative combination of safety and helpfulness rewards
- Why needed: Prevents reward hacking by requiring both safety adherence and helpfulness
- Quick check: Monitor for trade-offs between safety and helpfulness during training

## Architecture Onboarding

**Component map**: Safety policy specs -> SFT with reasoning model -> RL with safety RM + helpfulness RM -> Final model

**Critical path**: Policy specs → SFT data generation → RL optimization → Evaluation

**Design tradeoffs**: Safe-completion vs. traditional refusal; output-centric vs. input-centric safety; reasoning model dependency vs. accessibility

**Failure signatures**: 
- Reward hacking showing high helpfulness with near-violation content
- Over-refusal on dual-use prompts treating borderline cases as malicious
- Inconsistent response mode selection oscillating between safe-completion and refusal

**Three first experiments**:
1. Compare safety/helpfulness on dual-use prompts between safe-completion and refusal-trained models
2. Test harm severity distribution shifts under safe-completion training
3. Evaluate response consistency across multiple completions of the same prompt

## Open Questions the Paper Calls Out

**Open Question 1**: Does safe-completion training remain effective for non-reasoning models that cannot produce explicit chain-of-thought, or is the approach dependent on models with strong deliberative capabilities?
- Basis: Method builds on Deliberative Alignment using CoT reasoning; only evaluated on reasoning-capable models
- Why unresolved: Without explicit reasoning, unclear if models can reliably discriminate safe vs. unsafe detail levels
- Evidence needed: Controlled safe-completion training on non-reasoning baseline compared to dual-use evaluation set

**Open Question 2**: How robust is safe-completion against adaptive or multi-turn adversarial attacks designed to incrementally elicit actionable detail?
- Basis: Related work discusses dual-use jailbreaks; experiments focus on single-turn evaluations
- Why unresolved: Reward structure may shift probability mass toward less severe outputs but attackers could exploit indirect-helpfulness path
- Evidence needed: Evaluate safe-completion models under multi-turn jailbreak strategies and compare unsafe-rate curves

**Open Question 3**: To what extent does safe-completion performance depend on the granularity and clarity of the underlying policy specifications?
- Basis: Method assumes detailed safety category specifications used during SFT and RL; policy updates described but spec quality sensitivity not analyzed
- Why unresolved: Poorly specified categories could yield inconsistent safety scoring or misaligned rewards
- Evidence needed: Ablate spec granularity and consistency, measure changes in safety/helpfulness and inter-annotator disagreement

**Open Question 4**: Does training with composite safety-helpfulness reward lead to unintended mode collapse or over-cautious abstraction over longer training runs?
- Basis: Paper reports reductions in high-severity failures and increases in helpfulness but doesn't examine prolonged optimization effects
- Why unresolved: Reward shaping may pressure models toward generic, uninformative responses that optimize safety without realizing indirect helpfulness
- Evidence needed: Track distributional shifts in response length, abstraction level, and benign-task completion across training steps

## Limitations
- Heavy reliance on autograder assessments may not capture nuanced human judgments about harm or response quality
- Study lacks statistical significance testing between conditions, making robustness of improvements uncertain
- Requires maintaining detailed, regularly updated content policy specifications creating ongoing maintenance overhead
- Methodology assumes availability of reasoning-capable models for training data generation and filtering

## Confidence
- **High confidence**: Safe-completion can provide helpful responses to dual-use prompts where traditional refusal would block all assistance
- **Medium confidence**: Safe-completion reduces harm severity across all intent categories, depending on autograder quality and safety definitions
- **Medium confidence**: Safe-completion improves balance between safety and helpfulness, supported by human evaluations but lacking statistical testing

## Next Checks
1. Conduct human evaluations with statistically powered sample sizes and significance testing to verify whether safety and helpfulness improvements are robust across different rater populations
2. Perform adversarial testing with intentionally crafted prompts designed to probe the boundaries of safe-completion responses and identify potential reward hacking
3. Test model behavior on prompts from safety categories not included in the original training set to assess generalization and identify potential blind spots in the safe-completion approach