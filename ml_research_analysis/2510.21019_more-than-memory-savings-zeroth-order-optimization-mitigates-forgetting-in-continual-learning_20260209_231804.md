---
ver: rpa2
title: 'More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in
  Continual Learning'
arxiv_id: '2510.21019'
source_url: https://arxiv.org/abs/2510.21019
tags:
- optimization
- methods
- zo-fc
- learning
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates zeroth-order (ZO) optimization for continual
  learning to address the plasticity-stability-efficiency trilemma. Theoretical analysis
  shows that ZO optimization naturally promotes flatter loss landscapes, reducing
  catastrophic forgetting.
---

# More Than Memory Savings: Zeroth-Order Optimization Mitigates Forgetting in Continual Learning

## Quick Facts
- arXiv ID: 2510.21019
- Source URL: https://arxiv.org/abs/2510.21019
- Reference count: 40
- Primary result: 6× memory reduction while achieving accuracy comparable to state-of-the-art first-order continual learning methods

## Executive Summary
This work investigates zeroth-order (ZO) optimization for continual learning to address the plasticity-stability-efficiency trilemma. Theoretical analysis shows that ZO optimization naturally promotes flatter loss landscapes, reducing catastrophic forgetting. However, ZO's noisy gradient estimates impair plasticity, especially when applied to learnable classifiers. Empirical evaluation reveals that naive ZO substitution into existing PEFT-based methods fails. To overcome this, the authors propose ZO-FC, which applies ZO optimization to a single adapter module while retaining first-order optimization for the classifier. Experiments on CIFAR100, ImageNet-R, and DomainNet show that ZO-FC achieves accuracy comparable to state-of-the-art first-order methods while reducing training memory by up to 6×. The method consistently exhibits low forgetting and better stability, making it a practical solution for resource-constrained on-device continual learning.

## Method Summary
The paper proposes a hybrid optimization strategy that combines zeroth-order optimization for adapter modules with first-order optimization for the classifier head. This approach addresses the trilemma of plasticity (learning new tasks), stability (preserving old knowledge), and efficiency (low memory/compute cost). The key insight is that ZO optimization's natural tendency to find flatter minima reduces catastrophic forgetting, while retaining first-order optimization for the classifier preserves plasticity. The method, called ZO-FC, strategically applies ZO optimization only to a single adapter module rather than the entire model, mitigating the noise issues associated with ZO gradient estimates.

## Key Results
- Achieves up to 6× reduction in training memory compared to first-order methods
- Maintains accuracy comparable to state-of-the-art first-order continual learning methods
- Demonstrates consistently low forgetting across CIFAR100, ImageNet-R, and DomainNet datasets
- Shows better stability than pure first-order methods while maintaining sufficient plasticity

## Why This Works (Mechanism)
Zeroth-order optimization naturally promotes flatter loss landscapes by approximating gradients through function evaluations rather than direct differentiation. Flatter minima are known to be more robust to perturbations and less susceptible to catastrophic forgetting. However, the inherent noise in ZO gradient estimates impairs plasticity when applied to the classifier. By restricting ZO optimization to a single adapter module while maintaining first-order optimization for the classifier, ZO-FC achieves a balance between stability (from flatter landscapes) and plasticity (from precise gradient updates to the classifier).

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly forget previously learned information when trained on new tasks. This is the central problem addressed by the work.
- **Zeroth-order optimization**: An optimization technique that approximates gradients through function evaluations rather than direct differentiation, useful when gradients are unavailable or expensive to compute.
- **Continual learning trilemma**: The fundamental trade-off between plasticity (ability to learn new tasks), stability (ability to preserve old knowledge), and efficiency (memory/compute cost).
- **Flatter loss landscapes**: Broader, more stable minima in the loss surface that are more robust to perturbations and less prone to forgetting.
- **Adapter modules**: Small neural network components inserted into larger models that can be trained for new tasks while keeping the base model frozen.
- **Plastic efficiency**: The balance between learning capacity and resource constraints, particularly relevant for on-device learning scenarios.

## Architecture Onboarding
Component map: Base model -> Adapter modules -> Classifier head -> Task-specific components
Critical path: Input data flows through base model → adapter modules → classifier head → prediction. The adapter modules are the primary target for ZO optimization.
Design tradeoffs: The paper trades some plasticity for stability by using ZO optimization on adapters while maintaining first-order optimization on the classifier. This hybrid approach balances the competing demands of continual learning.
Failure signatures: Pure ZO optimization on classifiers leads to impaired plasticity and poor performance on new tasks. Excessive adapter modules increase memory usage without proportional benefits.
First experiments:
1. Baseline comparison of first-order vs. zeroth-order optimization on adapter modules
2. Ablation study of different adapter configurations with mixed optimization strategies
3. Cross-dataset evaluation on CIFAR100, ImageNet-R, and DomainNet to validate generalizability

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Theoretical claims about landscape flatness reducing forgetting rely on assumptions that may not fully capture real-world continual learning dynamics
- Empirical evaluation focuses primarily on adapter-based architectures, leaving uncertainty about effectiveness in other continual learning frameworks
- The balance between plasticity and stability depends heavily on task ordering and distribution shift characteristics

## Confidence
High confidence in memory efficiency improvements (6× reduction) and empirical forgetting metrics across tested datasets.
Medium confidence in theoretical claims about landscape flatness due to simplifying assumptions.
Medium confidence in the plastic efficiency trade-off, as this depends heavily on task ordering and distribution shift characteristics.

## Next Checks
1. Test ZO-FC across diverse continual learning architectures beyond adapter-based methods to assess generalizability.
2. Conduct ablation studies on different adapter module configurations to optimize the balance between plasticity and stability.
3. Evaluate performance on non-image datasets (e.g., text or time-series) to verify robustness across data modalities.