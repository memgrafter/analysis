---
ver: rpa2
title: Reward Model Routing in Alignment
arxiv_id: '2510.02850'
source_url: https://arxiv.org/abs/2510.02850
tags:
- router
- preference
- online
- arxiv
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reward model (RM) selection
  in reinforcement learning from human feedback (RLHF) for large language models.
  The proposed BayesianRouter combines offline learning of RM strengths with online
  Bayesian Thompson sampling to dynamically select the most suitable RM for each preference
  pair during DPO training.
---

# Reward Model Routing in Alignment

## Quick Facts
- arXiv ID: 2510.02850
- Source URL: https://arxiv.org/abs/2510.02850
- Reference count: 11
- Primary result: BayesianRouter achieves 63.23% AlpacaEval-2 and 75.66% GSM8K scores using dynamic RM selection

## Executive Summary
This paper introduces BayesianRouter, a novel approach for reward model selection in RLHF that addresses the challenge of choosing among multiple RMs with varying strengths. The method combines offline learning of RM strength estimates with online Bayesian Thompson sampling to dynamically select the most appropriate RM for each preference pair during DPO training. By initializing the online router with offline-learned priors and adaptively updating with policy feedback, BayesianRouter achieves superior alignment performance across instruction-following and reasoning benchmarks while maintaining computational efficiency.

## Method Summary
BayesianRouter employs a two-phase approach: an offline pretraining phase that learns RM strength estimates through supervised learning on preference data, and an online routing phase that uses Thompson sampling to dynamically select RMs during DPO training. The offline phase trains a routing network to predict RM strengths from preference pairs, initializing the online BayesianRouter with these priors. During training, the router maintains Bayesian posterior distributions for each RM's reward estimation accuracy and uses Thompson sampling to select the optimal RM for each preference comparison. This adaptive selection process allows the model to leverage the strengths of different RMs while maintaining O(1) computational complexity per query.

## Key Results
- Achieves 63.23% win rate on AlpacaEval-2 benchmark for instruction-following tasks
- Scores 75.66% on GSM8K mathematical reasoning benchmark
- Outperforms single RMs, ensembles, and existing routing methods across multiple alignment tasks

## Why This Works (Mechanism)
BayesianRouter works by combining the stability of offline-learned RM strength estimates with the adaptability of online Bayesian updating. The offline pretraining provides robust initial priors that capture the relative quality of different RMs on various preference distributions. The online Thompson sampling component then allows the router to adapt these priors based on actual policy feedback during training, correcting for any mismatches between offline estimates and real-world performance. This dual approach ensures both reliable initial routing decisions and the ability to adapt to changing conditions during the alignment process.

## Foundational Learning

1. **Thompson Sampling in Multi-Armed Bandits**
   - Why needed: Provides principled way to balance exploration and exploitation when selecting among multiple RMs
   - Quick check: Verify posterior updates follow Bayesian rules and Thompson sampling selects RMs proportionally to their posterior probabilities

2. **Reward Model Quality Estimation**
   - Why needed: Different RMs have varying strengths; need to estimate which RM performs best on specific preference pairs
   - Quick check: Ensure offline pretraining produces meaningful strength scores that correlate with actual RM performance

3. **Reinforcement Learning from Human Feedback (RLHF/DPO)**
   - Why needed: Provides the optimization framework where RM routing decisions impact final model alignment
   - Quick check: Confirm that RM selection affects gradient updates in DPO training as expected

## Architecture Onboarding

**Component Map:**
Offline Pretraining -> BayesianRouter Initialization -> Online Thompson Sampling -> RM Selection -> DPO Training

**Critical Path:**
1. Preference data preprocessing
2. Offline RM strength learning
3. Bayesian posterior initialization
4. Thompson sampling-based RM selection
5. DPO gradient computation with selected RM

**Design Tradeoffs:**
- Accuracy vs. computational efficiency: Bayesian updating adds overhead but enables adaptive selection
- Exploration vs. exploitation: Thompson sampling balances trying different RMs with exploiting known good ones
- Offline vs. online learning: Pretraining provides stability while online updating enables adaptation

**Failure Signatures:**
- Poor offline pretraining leads to incorrect initial priors and suboptimal early training
- Insufficient exploration in Thompson sampling causes premature convergence to suboptimal RMs
- Mismatched preference distributions between pretraining and training data cause routing errors

**First Experiments to Run:**
1. Ablation study removing online Bayesian updating to test pretraining-only performance
2. Thompson sampling parameter sweep to find optimal exploration-exploitation balance
3. RM ensemble baseline comparison to quantify routing benefits

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Relies on offline pretraining that may not capture evolving preference distributions in dynamic scenarios
- Assumes preference pairs as primary feedback signal, limiting applicability to other reward structures
- Computational overhead of maintaining Bayesian posteriors may impact production latency

## Confidence
- Core routing mechanism: High
- Scalability claims: Medium
- Generalization across task types: Medium
- RM failure mode handling: Low

## Next Checks
1. **Dynamic Environment Testing**: Evaluate BayesianRouter performance when preference distributions shift over time, simulating real-world deployment scenarios where user preferences evolve.

2. **Cross-Architecture Scaling**: Test the method with diverse RM architectures (different sizes, training objectives, and base models) to validate robustness beyond the 7B parameter homogeneous setting.

3. **Failure Mode Analysis**: Conduct systematic experiments to identify conditions under which BayesianRouter selects suboptimal RMs, including cases of RM uncertainty calibration failures and extreme preference distribution shifts.