---
ver: rpa2
title: 'Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures'
arxiv_id: '2510.14616'
source_url: https://arxiv.org/abs/2510.14616
tags:
- preference
- reward
- writing
- across
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current preference learning methods excel at detecting objective
  errors but fail on subjective creative tasks, achieving only 52.7% accuracy on preference
  pairs matched for correctness and length. We introduce WritingPreferenceBench, a
  cross-lingual dataset of 1,800 preference pairs across 8 creative writing genres
  where both responses are grammatically correct, factually accurate, and length-matched.
---

# Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures

## Quick Facts
- arXiv ID: 2510.14616
- Source URL: https://arxiv.org/abs/2510.14616
- Reference count: 40
- Primary result: Current preference learning methods excel at detecting objective errors but fail on subjective creative tasks, achieving only 52.7% accuracy on preference pairs matched for correctness and length.

## Executive Summary
Current preference learning methods excel at detecting objective errors but fail on subjective creative tasks, achieving only 52.7% accuracy on preference pairs matched for correctness and length. We introduce WritingPreferenceBench, a cross-lingual dataset of 1,800 preference pairs across 8 creative writing genres where both responses are grammatically correct, factually accurate, and length-matched. Sequence-based reward models achieve near-random performance (52.7%) while generative reward models with explicit reasoning chains reach 81.8% accuracy. All architectures show severe genre instability, with individual models ranging from 18.2% to 81.8% accuracy across categories. The failure of scale and reasoning-enhanced LLMs indicates that subjective preference modeling requires intermediate reasoning representations rather than direct pattern matching.

## Method Summary
WritingPreferenceBench is a cross-lingual benchmark for subjective writing preference modeling, featuring 1,800 preference pairs across 8 creative writing genres where both responses are grammatically correct, factually accurate, and length-matched. The dataset was created through systematic query generation (51 categories), multi-model response generation (20 LLMs, T=0.8), objective error filtering (~15% removal), and expert annotation (4-point scale, 11 annotators). Evaluation protocols tested both reward models (scalar scoring) and LLM judges (pairwise comparison) to reveal that sequence-based classifiers perform near-random (52.7%) while generative models with reasoning chains achieve 81.8% accuracy.

## Key Results
- Sequence-based reward models achieve only 52.7% accuracy on preference pairs matched for correctness and length
- Generative reward models with explicit reasoning chains reach 81.8% accuracy, a 29-percentage-point improvement
- All architectures show severe genre instability, with individual models ranging from 18.2% to 81.8% accuracy across categories
- Current RLHF methods primarily learn to detect objective errors rather than assess subjective quality preferences

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Reasoning Representations
- Claim: Explicit reasoning chains before preference judgments enable superior subjective quality assessment compared to direct classification.
- Mechanism: Generative reward models produce structured reasoning about aesthetic qualities (creativity, emotional resonance, stylistic flair) before outputting scores, creating intermediate representations that sequence classifiers cannot access through direct scalar prediction.
- Core assumption: The reasoning process itself constructs representations that capture subjective qualities, rather than reasoning merely being a byproduct of capability.
- Evidence anchors:
  - [abstract]: "generative reward models that produce explicit reasoning chains achieve 81.8% accuracy" versus 52.7% for sequence-based reward models—a 29-percentage-point gap.
  - [section 4.1]: RM-R1-Qwen2.5-7B reaches 81.8% on English, "30 percentage points above the sequence classifier mean"; all three generative models exceed 50% accuracy while three of four sequence classifiers fall below random chance.
  - [corpus]: LiteraryTaste (arXiv:2511.09310) addresses creative writing preference personalization but does not test the reasoning mechanism directly; corpus evidence is weak for this specific mechanism.

### Mechanism 2: Objective Signal Exploitation in Current RLHF
- Claim: Standard reward models learn to detect objective errors rather than assess subjective quality because current training data contains exploitable surface-level signals.
- Mechanism: When objective confounds (grammar errors, factual mistakes, length disparities) are systematically neutralized, sequence-based RMs lack representations for aesthetic judgment—causing 42-percentage-point degradation from RewardBench's 95% on objective tasks to 52.7% on WritingPreferenceBench.
- Core assumption: Current RLHF training corpora are dominated by objective quality signals that models learn to exploit as proxies for preference.
- Evidence anchors:
  - [abstract]: "current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences"
  - [section 1]: RewardBench achieves 95% on objective tasks but models "collapse to 52.7% accuracy" when objective signals are removed; this "indicates that current preference learning primarily optimizes for error detection."
  - [corpus]: RLMR (arXiv:2508.18642) explicitly separates "subjective writing quality" from "objective constraint following" in creative writing, supporting the objective/subjective distinction.

### Mechanism 3: Genre-Specific Heuristics Without Generalizable Principles
- Claim: Current preference models rely on brittle genre-specific heuristics rather than learning transferable aesthetic principles.
- Mechanism: High within-model variance across genres (standard deviations 10.1–14.0%) with individual models swinging 50+ percentage points (e.g., 18.2% to 81.8%) indicates memorization of genre patterns rather than abstraction of "good writing" qualities that transfer across categories.
- Core assumption: Generalizable aesthetic principles would manifest as consistent cross-genre performance.
- Evidence anchors:
  - [abstract]: "individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%"
  - [section 4.1]: Nvidia/AceMath-7B shows "43.3 percentage point swing within a single model" (18.2% Chinese Poetry to 61.5% Role-Playing); Skywork-Gemma-27B varies from 21.7% to 81.8%.
  - [corpus]: Limited direct corpus evidence for the heuristic mechanism; this is primarily an inference from the paper's variance data.

## Foundational Learning

- Concept: **Sequence Classifier vs. Generative Reward Model Architectures**
  - Why needed here: The paper demonstrates these architectures perform fundamentally differently—sequence classifiers output single scalars while generative RMs produce reasoning chains before scoring. Understanding this distinction is essential for interpreting the 29-percentage-point gap.
  - Quick check question: If you remove the reasoning chain from a generative RM and fine-tune it as a direct classifier, would you expect performance to approach 52.7% or 81.8%? What does this suggest about where the capability resides?

- Concept: **RLHF Training Signal Composition**
  - Why needed here: The paper argues current RLHF optimizes for objective error detection rather than subjective quality assessment. Recognizing what signals dominate training data helps explain why neutralizing objective confounds causes catastrophic degradation.
  - Quick check question: What happens to reward model accuracy when you filter training pairs to remove grammar errors, factual mistakes, and length differences? What does the paper's 42-percentage-point drop suggest about current training objectives?

- Concept: **Cross-Lingual Preference Transfer**
  - Why needed here: The benchmark includes 1,200 English and 600 Chinese pairs with inconsistent cross-lingual performance patterns across models, suggesting preference representations may be language-specific rather than universal.
  - Quick check question: Should a reward model trained primarily on English creative writing transfer to Chinese poetry evaluation? What do the paper's cross-lingual gaps (e.g., RM-R1-Qwen2.5-7B: 81.8% EN vs. 73.3% ZH) suggest about language-agnostic preference learning?

## Architecture Onboarding

- Component map: Query Taxonomy -> Response Generation -> Objective Filtering -> Human Annotation -> Pair Curation -> Evaluation Protocols
- Critical path:
  1. Generate diverse responses across multiple models with high temperature to create quality variance
  2. Filter responses for objective correctness to isolate subjective quality signals
  3. Collect 3-annotator scores using calibrated rubric with genre-specific standards
  4. Curate pairs meeting consensus and minimum-gap thresholds
  5. Evaluate both reward models (scoring) and LLM judges (pairwise comparison) to capture architectural differences

- Design tradeoffs:
  - Length variance vs. controlled matching: Paper allows higher variance in chosen responses (SD=1801.9 vs. 593.4 for rejected) as "signature of creative quality—while mediocrity converges toward formulaic patterns, creativity manifests across diverse scales."
  - 2:1 EN:ZH ratio: Determined by annotator availability rather than ideal balance; minimum 20 pairs per category in each language maintains statistical power.
  - Filtering objective errors vs. ecological validity: Removing ~15% of responses with objective deficiencies ensures the benchmark tests subjective quality, but may reduce real-world applicability where objective and subjective quality co-vary.

- Failure signatures:
  - Sequence classifiers: Mean accuracy 52.7% (random chance); catastrophic genre-specific failures (e.g., 18.2% on Chinese Poetry); Skywork-Gemma-27B shows no improvement over 8B despite 3.4× parameters
  - LLM judges: Mean accuracy 53.9%; reasoning-enhanced models (Claude-4-Opus-thinking, OpenAI-o3-high) show no advantage (r=0.08 correlation with reasoning capability); OpenAI-o3-high drops to 42.0% on Chinese
  - All architectures: Within-model standard deviation 10.1–14.0%; individual models swing 50+ percentage points across genres (e.g., Gemini-2.5-Pro: 80.0% Poetry to 34.8% Scriptwriting)

- First 3 experiments:
  1. Validate evaluation pipeline: Reproduce the sequence classifier (52.7%) vs. generative RM (81.8%) gap on a 100-pair subset to confirm your implementation matches paper protocols before full evaluation.
  2. Distillation test: Train a sequence classifier to predict the outputs of a generative RM's reasoning process; if distilled model matches 81.8%, the mechanism is training signal; if it regresses to 52.7%, the mechanism is inference-time representation.
  3. Cross-genre transfer probe: Train reward models on 7 genres, evaluate on held-out 8th; measure whether performance correlates with genre similarity or shows uniform degradation—this tests whether models learn generalizable principles versus genre-specific heuristics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid architectures be developed that combine the computational efficiency of discriminative sequence classifiers with the representational reasoning capacity of generative reward models?
- Basis in paper: [explicit] The Conclusion states: "Future work should investigate hybrid architectures combining the computational efficiency of discriminative models with the representational capacity of generative reasoning."
- Why unresolved: There is currently a stark trade-off where sequence classifiers are fast but fail on subjective tasks (52.7% accuracy), whereas generative models succeed (81.8%) but likely incur higher inference costs due to reasoning chains.
- What evidence would resolve it: The proposal and validation of a model architecture that achieves >75% accuracy on WritingPreferenceBench while maintaining latency comparable to standard 7B-8B sequence classifiers.

### Open Question 2
- Question: What specific training objectives or data curation strategies can mitigate the severe genre instability (high variance) observed in preference models?
- Basis in paper: [explicit] The Conclusion calls for "training objectives that explicitly encourage genre-invariant preference learning," and the Discussion notes that current models rely on "brittle, genre-specific heuristics" rather than generalizable principles.
- Why unresolved: Evaluated models exhibit extreme performance swings (e.g., 18.2% to 81.8% accuracy) across different writing categories, and scaling to 27B parameters does not resolve this variance.
- What evidence would resolve it: A training methodology that reduces the within-model standard deviation across genres to below 5% while maintaining high mean accuracy.

### Open Question 3
- Question: Why does explicit reasoning improve performance in generative reward models (GenRM) but fail to provide any advantage in zero-shot LLM judges?
- Basis in paper: [inferred] Section 4.1 shows GenRMs with reasoning chains achieve 81.8% accuracy, while Section 4.2 shows reasoning-enhanced LLM judges (e.g., OpenAI-o3) perform near random chance with negligible correlation (r=0.08) between reasoning capability and preference accuracy.
- Why unresolved: The paper suggests the problem is "representational," but does not determine if the failure in LLM judges is due to the lack of specific preference training or a fundamental limitation of chain-of-thought for subjective aesthetic tasks.
- What evidence would resolve it: An ablation study comparing fine-tuned reasoning LLMs against zero-shot reasoning LLMs to isolate whether training data