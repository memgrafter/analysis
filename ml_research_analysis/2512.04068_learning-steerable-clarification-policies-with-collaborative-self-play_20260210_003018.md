---
ver: rpa2
title: Learning Steerable Clarification Policies with Collaborative Self-play
arxiv_id: '2512.04068'
source_url: https://arxiv.org/abs/2512.04068
tags:
- answer
- clarification
- question
- reward
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of managing uncertainty in AI
  assistants when faced with ambiguous or underspecified queries. The authors propose
  a steerable policy framework that enables AI assistants to adapt their response
  strategy based on contextual factors like user preferences or interaction modality.
---

# Learning Steerable Clarification Policies with Collaborative Self-play

## Quick Facts
- arXiv ID: 2512.04068
- Source URL: https://arxiv.org/abs/2512.04068
- Authors: Jonathan Berant; Maximillian Chen; Adam Fisch; Reza Aghajani; Fantine Huot; Mirella Lapata; Jacob Eisenstein
- Reference count: 40
- Primary result: Novel steerable policy framework enabling AI assistants to dynamically adapt response strategies based on contextual factors and cost coefficients

## Executive Summary
This paper addresses the challenge of managing uncertainty in AI assistants when faced with ambiguous or underspecified queries. The authors propose a steerable policy framework that enables AI assistants to adapt their response strategy based on contextual factors like user preferences or interaction modality. They train a single model that can dynamically choose between guessing the user intent, enumerating multiple possible interpretations, or asking clarification questions, conditioned on input cost coefficients that represent the relative costs of these strategies. The model is trained using Reinforced Self-Training (ReST) with collaborative self-play between a user simulator and an AI assistant. Experiments on AmbigQA and Pacific benchmarks show that the trained model significantly outperforms prompted baselines, achieving higher accuracy while reducing the number of clarification questions and length of answers. Notably, the model generalizes to cost coefficients not seen during training and demonstrates effective steerability, adjusting its behavior predictably based on the provided cost coefficients.

## Method Summary
The paper introduces a steerable policy framework for AI assistants that can dynamically adapt their response strategy based on contextual factors. The core innovation is training a single model that can choose between guessing the user intent, enumerating multiple interpretations, or asking clarification questions, all conditioned on input cost coefficients representing the relative costs of these strategies. The model is trained using Reinforced Self-Training (ReST) with collaborative self-play between a user simulator and an AI assistant. The approach enables the AI to learn optimal behavior that balances accuracy with efficiency based on different cost configurations. The training process involves simulating user interactions where the AI assistant learns to optimize its strategy while the user simulator provides realistic responses. This results in a flexible system that can be steered by adjusting the cost coefficients to match different application requirements.

## Key Results
- The steerable policy model significantly outperforms prompted baselines on AmbigQA and Pacific benchmarks
- The model achieves higher accuracy while reducing the number of clarification questions and answer length
- The approach demonstrates generalization to cost coefficients not seen during training
- The model shows effective steerability, adjusting behavior predictably based on provided cost coefficients

## Why This Works (Mechanism)
The framework works by conditioning the policy on cost coefficients that represent the relative costs of different response strategies. By training with collaborative self-play, the AI assistant learns optimal behavior that balances accuracy with efficiency across various cost configurations. The ReST training approach allows the model to learn from simulated interactions where both the AI and user behaviors improve iteratively. This creates a robust policy that can adapt to different user preferences and interaction modalities by simply adjusting the input cost coefficients, rather than requiring separate models for different scenarios.

## Foundational Learning
**Reinforcement Learning**: Why needed - To train the AI assistant to make sequential decisions about when to guess, enumerate, or ask questions. Quick check - The policy learns to maximize cumulative reward across interactions.
**Collaborative Self-Play**: Why needed - To create realistic training scenarios where both AI and user behaviors can improve iteratively. Quick check - The user simulator provides challenging but learnable interactions for the AI.
**Policy Conditioning**: Why needed - To enable a single model to handle multiple cost configurations without separate training runs. Quick check - The model can smoothly interpolate between different cost coefficient values.

## Architecture Onboarding

**Component Map**: User Simulator -> Cost Coefficients -> Policy Network -> Response Strategy -> Environment Feedback

**Critical Path**: The critical path flows from the user simulator generating ambiguous queries, through the policy network that conditions on cost coefficients, to the response strategy selection that determines whether to guess, enumerate, or ask questions. The environment feedback loop then updates both the policy and simulator.

**Design Tradeoffs**: The main tradeoff is between model flexibility and training complexity. Using a single steerable model simplifies deployment but requires more sophisticated training to handle all possible cost configurations. The collaborative self-play approach trades computational efficiency for more realistic training scenarios compared to simpler supervised learning approaches.

**Failure Signatures**: Potential failures include: (1) over-reliance on clarification questions when cost coefficients suggest guessing should be preferred, (2) failure to adapt to extreme cost coefficient values, (3) degradation in performance on multi-turn conversations where context builds over time.

**3 First Experiments**:
1. Test the model's ability to switch between guessing and asking questions as cost coefficients vary from 0.1 to 10.0
2. Evaluate performance on out-of-distribution cost coefficients not seen during training
3. Measure the impact of increasing the number of simulated training episodes on final policy quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on single-turn clarification scenarios, leaving uncertainty about multi-turn conversation performance
- While the model demonstrates generalization to unseen cost coefficients, the evaluation covers a relatively limited range of coefficient values
- The steerability evaluation focuses on immediate behavioral changes rather than long-term interaction quality

## Confidence

**Major Claims and Confidence Levels:**
- **High Confidence**: The core methodology of training steerable policies using collaborative self-play is technically sound and well-executed. The experimental design and baseline comparisons are rigorous.
- **Medium Confidence**: Claims about the model's ability to generalize to unseen cost coefficients are supported but could benefit from broader testing across more diverse coefficient ranges.
- **Medium Confidence**: The steerability of the model is demonstrated effectively, though the evaluation focuses primarily on immediate behavioral changes rather than long-term interaction quality.

## Next Checks

1. Test the model's performance across a wider range of cost coefficient values, particularly extreme values, to better understand its generalization capabilities.

2. Evaluate the approach in multi-turn conversation scenarios to assess its scalability and ability to maintain context over longer interactions.

3. Conduct user studies to measure the practical impact of the steerable policies on user satisfaction and task completion rates in real-world applications.