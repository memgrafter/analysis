---
ver: rpa2
title: 'RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation'
arxiv_id: '2601.11822'
source_url: https://arxiv.org/abs/2601.11822
tags:
- decode
- prefill
- rapid-serve
- serving
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAPID-Serve is a novel LLM inference serving system that enables
  concurrent execution of prefill and decode phases on the same GPU(s) to improve
  resource utilization while meeting latency SLOs. Unlike hybrid batching that couples
  phases in lockstep (causing latency spikes) or disaggregated serving that requires
  expensive KV-cache transfers, RAPID-Serve uses concurrent execution with adaptive
  resource allocation via CU masking.
---

# RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation

## Quick Facts
- arXiv ID: 2601.11822
- Source URL: https://arxiv.org/abs/2601.11822
- Authors: Amna Masood; Pratishtha Gaur; Nuwan Jayasena
- Reference count: 22
- Key outcome: RAPID-Serve achieves up to 4.1x (average 1.7x) unconstrained throughput improvement and up to 32x (average 4.9x) goodput improvement under SLO constraints compared to state-of-the-art approaches, while maintaining p95 ITL latencies up to 6x lower than chunked hybrid batching.

## Executive Summary
RAPID-Serve is a novel LLM inference serving system that enables concurrent execution of prefill and decode phases on the same GPU(s) to improve resource utilization while meeting latency SLOs. Unlike hybrid batching that couples phases in lockstep (causing latency spikes) or disaggregated serving that requires expensive KV-cache transfers, RAPID-Serve uses concurrent execution with adaptive resource allocation via CU masking. This allows prefill and decode to make independent progress while minimizing interference. Evaluations show RAPID-Serve achieves significant performance gains particularly in resource-constrained environments.

## Method Summary
RAPID-Serve implements a multiprocessing architecture where prefill and decode phases execute as separate GPU kernels from separate processes, sharing model weights and KV cache via IPC handles. The system uses adaptive resource allocation through CU masking on AMD GPUs, dynamically partitioning compute units between prefill and decode based on workload intensity. An Adaptive Resource Manager switches between overallocation (100%-100% CUs) under light decode load and distinct allocation (e.g., P60-D40) when decode batch size exceeds SLO thresholds. The approach eliminates Python GIL serialization and KV transfer overheads while maintaining SLO compliance through intelligent compute unit management.

## Key Results
- Up to 4.1x (average 1.7x) unconstrained throughput improvement compared to state-of-the-art approaches
- Up to 32x (average 4.9x) goodput improvement under SLO constraints
- p95 ITL latencies up to 6x lower than chunked hybrid batching while meeting 100ms LLaMA and 50ms Mixtral SLOs
- Effective elimination of KV-cache transfer overheads that plague disaggregated serving approaches

## Why This Works (Mechanism)

### Mechanism 1: Intra-GPU Prefill-Decode Concurrency
Running prefill and decode phases concurrently on the same GPU without batching them together breaks lockstep coupling while maintaining resource efficiency. Prefill and decode execute as separate GPU kernels from separate processes, overlapping in time but not combined in a single batch. Each phase makes independent progress—prefill processes compute-bound prompt tokens while decode performs memory-bound autoregressive generation. The core assumption is that prefill (compute-bound) and decode (memory-bandwidth-bound) have complementary resource profiles that can be overlapped without catastrophic interference. Evidence shows decode achieves similar performance with 40-50% of compute resources, confirming underutilization during decode.

### Mechanism 2: Adaptive Resource Allocation via CU Masking
Dynamically partitioning compute units between prefill and decode based on workload intensity maintains SLOs while maximizing utilization. At low decode load, overallocate 100% CUs to both phases (letting hardware scheduler arbitrate). At high load, use offline profiles to assign minimum required CUs to decode, reserving remainder for prefill. Switches between modes at runtime. The core assumption is that offline profiling captures representative decode batch-size-to-CU requirements; workload transitions are infrequent enough that mode-switching overhead is negligible. Figure 7 shows P100-D100 overallocation exceeds ITL SLO at higher batch sizes; distinct allocation eliminates interference.

### Mechanism 3: Lock-Free Multiprocessing with Shared KV Cache
Separate processes for prefill and decode with IPC-shared KV cache eliminates Python GIL serialization and KV transfer overhead. Decode process exclusively manages KV cache allocation (avoiding locks). When a request arrives, decode allocates blocks, passes block IDs to prefill process, which populates KV cache. Only notification (not data) is transferred between processes. The core assumption is that decode-only KV cache management is sufficient because prefill block requirements are predictable from input context length. This eliminates the 1.4x throughput and 1.9x TTFT overhead from KV transfer seen in disaggregated serving.

## Foundational Learning

- **Concept: Compute-bound vs. memory-bandwidth-bound kernels**
  - Why needed here: Understanding why prefill benefits from more CUs while decode doesn't is essential for grasping why CU masking works.
  - Quick check question: If you halve the CUs available to a memory-bound kernel, would you expect performance to drop by ~50%? Why or why not?

- **Concept: Arithmetic intensity**
  - Why needed here: Prefill has high arithmetic intensity (many FLOPs per byte transferred); decode has low intensity. This determines their respective bottlenecks.
  - Quick check question: Which phase would benefit more from increased memory bandwidth: prefill or decode?

- **Concept: KV Cache scaling (Formula 1)**
  - Why needed here: KV cache size directly impacts disaggregation transfer costs and memory capacity planning.
  - Quick check question: For a 70B model with 80 layers, 8 KV heads, 128 head dimension, BF16 precision, and 4K context: how much KV cache memory per request?

## Architecture Onboarding

- **Component map:** Request → [Decode Process] → allocates KV blocks → [Prefill Process] ← block IDs via shared storage ← populates KV cache (shared via IPC) → notifies decode → [Decode Process] → adds to running batch → generates tokens
- **Critical path:** Decode iteration latency (ITL) determines user experience. The path is: decode scheduler → async_scheduling lookahead → kernel launch → token generation. Prefill runs in parallel but must not starve decode of CUs under SLO pressure.
- **Design tradeoffs:** Overallocation (P100-D100) maximizes utilization but violates ITL SLOs at high batch sizes; distinct allocation (e.g., P60-D40) meets SLOs but leaves CUs idle when decode finishes early; paper chooses adaptive switching; threshold tuning is workload-dependent.
- **Failure signatures:** ITL spikes under high load → Adaptive Resource Manager not switching to distinct allocation soon enough; check decode batch size thresholds; TTFT regressions vs. disaggregation → Verify prefill is not being chunked; check for scheduling gaps; Memory OOM under long contexts → KV cache manager may be over-provisioning; check block allocation strategy.
- **First 3 experiments:** 1) Reproduce Figure 7: Measure decode latency at batch sizes 1-256 with P100-D100, P60-D40, P50-D50 allocations on your target GPU. Identify SLO-critical batch size threshold. 2) Measure memory interference: Run concurrent prefill+decode GEMM kernels with varying CU masks. Quantify decode degradation beyond the 2-5% reported for attention kernels. 3) Stress test Adaptive Resource Manager: Send bursty traffic patterns (e.g., 10s high QPS, 10s low QPS) and measure mode-switching latency and any SLO violations during transitions.

## Open Questions the Paper Calls Out

### Open Question 1
Can the memory subsystem interference between concurrent prefill and decode kernels be effectively mitigated through software-level scheduling or partitioning techniques on AMD GPUs? The paper states there is no available mechanism on AMD Instinct GPUs to partition memory resources between concurrent kernels without virtualizing the GPUs, and does not explore software workarounds beyond compute-level isolation. Experiments with memory-aware kernel scheduling, cache partitioning techniques, or future AMD hardware with memory QoS features demonstrating reduced interference (below the observed 2-5% decode degradation) would resolve this.

### Open Question 2
How does RAPID-Serve's performance compare on NVIDIA GPUs using alternative partitioning mechanisms like MPS or Green Context? The paper relies exclusively on AMD-specific CU masking and evaluates only on MI300X GPUs, while related work (DRIFT, Semi-PD) uses NVIDIA mechanisms but direct comparison is not provided. Comparative benchmarks on NVIDIA H100/A100 GPUs using MPS or Green Context, showing whether similar throughput/goodput gains are achievable, would resolve this.

### Open Question 3
Can online profiling replace the current offline profiling approach for adaptive resource allocation without introducing unacceptable runtime overhead? The Adaptive Resource Manager relies on offline profiles to determine CU allocations for different decode batch sizes, which may not adapt well to model changes, varying sequence lengths, or workload distribution shifts. Implementation of lightweight online profiling showing comparable SLO attainment with minimal throughput degradation (<5%) under dynamic workloads with shifting prompt length distributions would resolve this.

### Open Question 4
Does RAPID-Serve's advantage persist in multi-node clusters with high-bandwidth interconnects where disaggregated serving becomes more competitive? The paper acknowledges disaggregated serving "implicitly assumes a moderately sized cluster with ample GPUs and high-bandwidth interconnects" and that "its effectiveness is fundamentally constrained by interconnect and hardware bandwidth," but evaluation is limited to single-node (8-GPU) deployments. Multi-node experiments varying cluster size, interconnect bandwidth (NVLink vs. InfiniBand), and model sizes to identify the resource threshold where disaggregation outperforms intra-GPU concurrency would resolve this.

## Limitations
- Memory subsystem interference on AMD GPUs cannot be mitigated through software-level partitioning techniques
- Performance characteristics may vary significantly across different GPU architectures and workload patterns
- The approach focuses on intra-GPU disaggregation and does not address multi-node or distributed serving scenarios

## Confidence
- **High Confidence (4-5/5):** The core insight that prefill and decode have complementary resource profiles (compute-bound vs. memory-bound) enabling concurrent execution; the fundamental performance benefits of eliminating KV-cache transfer overheads compared to disaggregated serving; the mechanism of using multiprocessing to avoid Python GIL serialization
- **Medium Confidence (3/5):** The effectiveness of CU masking for adaptive resource allocation across different GPU architectures; the specific thresholds and policies used in the Adaptive Resource Manager; the scalability of the approach to multi-GPU and distributed scenarios
- **Low Confidence (1-2/5):** Performance guarantees for workloads with highly variable characteristics not represented in the evaluation traces; memory interference behavior on non-AMD GPU architectures; long-term stability under sustained high-load conditions with rapid workload oscillations

## Next Checks
1. **Memory interference stress test:** Systematically measure decode ITL degradation across a range of CU masking configurations (P100-D100, P80-D20, P60-D40, etc.) on both AMD and NVIDIA GPUs. Identify the point at which memory subsystem interference exceeds 10% and characterize the relationship between CU allocation ratios and interference levels.

2. **Adaptive Resource Manager robustness:** Implement the Adaptive Resource Manager with configurable switching thresholds and evaluate performance under bursty traffic patterns with rapid load transitions. Measure mode-switching latency, SLO violations during transitions, and identify optimal threshold policies for different workload characteristics.

3. **Workload sensitivity analysis:** Evaluate RAPID-Serve performance across a diverse set of synthetic workloads varying in request length distribution, burstiness, and token generation patterns. Identify workload characteristics that cause performance degradation and quantify the impact on goodput and SLO compliance compared to the baseline workloads used in the paper.