---
ver: rpa2
title: 'KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical
  Text Classification'
arxiv_id: '2505.05583'
source_url: https://arxiv.org/abs/2505.05583
tags:
- contains
- level
- classification
- text
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KG-HTC integrates knowledge graphs with LLMs to enhance zero-shot
  hierarchical text classification. It addresses challenges of large label spaces
  and long-tail distributions in HTC by retrieving relevant subgraphs from knowledge
  graphs using RAG, then transforming these subgraphs into structured prompts for
  LLM classification.
---

# KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification

## Quick Facts
- arXiv ID: 2505.05583
- Source URL: https://arxiv.org/abs/2505.05583
- Reference count: 40
- Improves zero-shot hierarchical text classification by integrating knowledge graphs with LLMs, achieving 27.1% average F1-macro improvement

## Executive Summary
KG-HTC addresses the challenge of zero-shot hierarchical text classification by integrating knowledge graphs with large language models. The method retrieves relevant subgraphs using RAG techniques and transforms them into structured prompts for LLM classification. KG-HTC demonstrates state-of-the-art performance across three datasets, showing significant improvements over baseline approaches, particularly at deeper hierarchy levels where it achieves gains of 123.1% and 139.0%. The approach effectively handles the challenges of large label spaces and long-tail distributions inherent in hierarchical text classification tasks.

## Method Summary
KG-HTC combines knowledge graph integration with RAG-based subgraph retrieval to enhance LLM zero-shot classification performance. The system retrieves relevant subgraphs from knowledge graphs and transforms them into structured prompts that provide semantic context for classification decisions. This approach specifically targets the challenges of hierarchical text classification, including managing large label spaces and addressing long-tail distribution issues. The method shows particular effectiveness at deeper hierarchy levels where traditional approaches typically experience significant performance degradation.

## Key Results
- Achieves 27.1% average improvement in first-level classification F1-macro scores compared to GPT-3.5-turbo
- Demonstrates gains of 123.1% and 139.0% at deeper hierarchy levels over baseline LLM performance
- Shows significantly less performance degradation as taxonomy depth increases compared to baseline methods

## Why This Works (Mechanism)
KG-HTC leverages structured semantic context from knowledge graphs to overcome the limitations of zero-shot LLM classification in hierarchical settings. By retrieving relevant subgraphs through RAG techniques, the method provides additional contextual information that helps LLMs better understand relationships between labels and their hierarchical structure. The transformation of subgraphs into structured prompts enables more effective utilization of this contextual information during classification. This approach is particularly beneficial for handling long-tail labels and maintaining performance consistency across different hierarchy depths.

## Foundational Learning

**Knowledge Graphs**: Structured representations of entities and their relationships that provide semantic context. Needed to enrich the semantic understanding of classification tasks beyond what LLMs can infer from text alone. Quick check: Verify the knowledge graph contains relevant entities and relationships for the target classification domain.

**RAG (Retrieval-Augmented Generation)**: Technique for retrieving relevant information from external knowledge sources to augment LLM prompts. Needed to dynamically select relevant subgraphs based on input text. Quick check: Confirm retrieval accuracy by examining top-k subgraph matches for sample inputs.

**Hierarchical Text Classification (HTC)**: Text classification task where labels are organized in tree-like structures with parent-child relationships. Needed to understand the specific challenges of managing hierarchical label relationships and depth-related performance degradation. Quick check: Validate hierarchy depth and label distribution match expected patterns.

## Architecture Onboarding

**Component Map**: Input Text -> RAG Subgraph Retriever -> Subgraph Transformer -> Structured Prompt -> LLM Classifier -> Hierarchical Labels

**Critical Path**: The most performance-critical sequence is Input Text → RAG Subgraph Retriever → LLM Classifier, as retrieval quality directly impacts classification accuracy.

**Design Tradeoffs**: 
- Depth vs. breadth in subgraph retrieval (more comprehensive subgraphs improve context but increase prompt length)
- Retrieval precision vs. recall (balancing relevance against coverage of potential labels)
- Prompt structure complexity vs. LLM processing limitations

**Failure Signatures**: 
- Poor retrieval leading to irrelevant context in prompts
- Overly complex prompts exceeding LLM context window limits
- Knowledge graph incompleteness causing missing critical relationships
- Hierarchical structure misinterpretation by the LLM

**First Experiments**:
1. Test retrieval accuracy by measuring subgraph relevance scores against known ground truth for sample inputs
2. Evaluate prompt effectiveness by comparing classification performance with and without knowledge graph integration
3. Benchmark computational efficiency by measuring inference time across different subgraph sizes

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation limited to three specific datasets (WoS, DBpedia, Amazon), potentially restricting generalizability to other domains
- No explicit computational efficiency or runtime comparisons between KG-HTC and baseline methods
- Mechanisms for handling knowledge graph construction biases and subgraph retrieval errors not fully explored

## Confidence

**High confidence**: Reported F1-macro improvements and relative performance gains across datasets are well-supported by experimental results

**Medium confidence**: Scalability claims require validation across more diverse datasets and explicit efficiency analysis

**Medium confidence**: Mechanism explanations are reasonable but some implementation details are abstracted from the provided information

## Next Checks

1. Test KG-HTC on additional datasets from different domains (e.g., medical literature, legal documents) to assess domain transferability

2. Conduct runtime efficiency benchmarking comparing KG-HTC against baseline methods under varying graph sizes and prompt lengths

3. Perform ablation studies isolating the contribution of RAG-based subgraph retrieval versus prompt engineering to identify the primary driver of performance gains