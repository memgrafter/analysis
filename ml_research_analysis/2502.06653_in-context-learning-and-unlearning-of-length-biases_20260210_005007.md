---
ver: rpa2
title: In-Context Learning (and Unlearning) of Length Biases
arxiv_id: '2502.06653'
source_url: https://arxiv.org/abs/2502.06653
tags:
- figure
- blue
- length
- long
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can learn
  statistical length biases during in-context learning. The authors demonstrate empirically
  that models do learn length biases in the context window, with performance varying
  across different model sizes, numbers of examples, and class length differences.
---

# In-Context Learning (and Unlearning) of Length Biases

## Quick Facts
- arXiv ID: 2502.06653
- Source URL: https://arxiv.org/abs/2502.06653
- Authors: Stephanie Schoch; Yangfeng Ji
- Reference count: 40
- Key outcome: Large language models can learn statistical length biases during in-context learning, with longer contexts increasing the potential for bias learning.

## Executive Summary
This paper investigates whether large language models can learn statistical length biases during in-context learning. Through controlled experiments across multiple model sizes and configurations, the authors demonstrate that models do learn length biases in the context window. The research reveals that increased numbers of examples can exacerbate learned biases, and models across different sizes can acquire these biases. The findings suggest that longer contexts exhibit greater potential for statistical data biases being learned in-context, raising important considerations for model deployment and training practices.

## Method Summary
The authors conducted experiments using synthetic length-biased datasets to evaluate in-context learning of length biases across different model sizes. They systematically varied the number of examples provided in context and measured how this affected bias learning. The study included both the observation of bias acquisition and the exploration of using in-context learning to debias models that had been fine-tuned with length biases. Multiple configurations were tested to understand how different factors influence the learning and unlearning of length biases.

## Key Results
- Models across a range of sizes can learn length biases during in-context learning
- Increased numbers of examples can exacerbate learned length biases
- Longer contexts exhibit greater potential for statistical data biases being learned in-context
- In-context learning can be used to debias fine-tuned models that exhibit length biases

## Why This Works (Mechanism)
The paper demonstrates that language models can learn statistical patterns from in-context examples, including spurious correlations between input length and class membership. This occurs because models optimize for next-token prediction, which can lead them to exploit any available statistical regularities in the input, including length-based patterns. The mechanism appears to be a natural consequence of the model's training objective and its ability to capture statistical regularities from limited examples.

## Foundational Learning
- In-context learning: Understanding how models adapt to new tasks from few examples without parameter updates
  - Why needed: Central to understanding how models can learn biases from limited context
  - Quick check: Can the model perform the task with different numbers of examples?

- Length bias: Statistical correlation between input length and output class
  - Why needed: The primary phenomenon being studied and measured
  - Quick check: Does model performance correlate with input length?

- Fine-tuning: Process of adapting pre-trained models to specific tasks
  - Why needed: Context for understanding how models can acquire and then unlearn biases
  - Quick check: Does the model perform the target task after fine-tuning?

## Architecture Onboarding
Component map: Pre-trained model -> In-context examples -> Bias learning/unlearning
Critical path: Input examples → Context processing → Prediction → Bias manifestation
Design tradeoffs: Balance between few-shot learning effectiveness and bias susceptibility
Failure signatures: Performance degradation on balanced test sets, length-correlated predictions
First experiments: (1) Baseline performance on balanced data, (2) Bias learning with length-biased examples, (3) Unlearning attempts with balanced examples

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental setup focuses on synthetic length-biased datasets, limiting generalizability to real-world scenarios
- Does not provide evidence that observed length biases translate to actual performance degradation in practical applications
- Unlearning methods show promise but lack comprehensive evaluation across diverse model architectures and tasks

## Confidence
- High: Core finding that language models can learn length biases during in-context learning
- Medium: Claim that longer contexts increase potential for bias learning
- Low: Effectiveness of in-context unlearning methods

## Next Checks
1. Evaluate the learned length biases on real-world datasets and tasks to assess practical impact
2. Test the unlearning methods across a broader range of model architectures and fine-tuning paradigms
3. Investigate the relationship between context length and bias learning through systematic ablation studies varying context sizes and input formats