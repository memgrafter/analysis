---
ver: rpa2
title: 'NeoQA: Evidence-based Question Answering with Generated News Events'
arxiv_id: '2505.05949'
source_url: https://arxiv.org/abs/2505.05949
tags:
- question
- outline
- answer
- event
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce NEOQA, a benchmark for evaluating evidence-based question
  answering in large language models (LLMs). Unlike existing RAG datasets that become
  obsolete as models internalize recent information, NEOQA uses fictional news events
  and entities to prevent interference from pretraining knowledge.
---

# NeoQA: Evidence-based Question Answering with Generated News Events

## Quick Facts
- **arXiv ID**: 2505.05949
- **Source URL**: https://arxiv.org/abs/2505.05949
- **Reference count**: 40
- **Primary result**: Introduces NEOQA benchmark with fictional news events; shows LLMs struggle with evidence-based reasoning, achieving only 53.2 ADTScore even on largest model.

## Executive Summary
NeoQA is a benchmark for evaluating evidence-based question answering in large language models using fictional news events and entities. Unlike existing RAG datasets that become obsolete as models internalize recent information, NeoQA prevents interference from pretraining knowledge by using entirely synthetic content. The benchmark includes multi-hop, time-span, false premise, and uncertain specificity questions paired with news articles as evidence. Experiments across seven models show that LLMs struggle to distinguish sufficient from insufficient evidence and frequently rely on shortcut reasoning, especially when key information is missing.

## Method Summary
The benchmark uses 15 fictional timelines with 10 sequential events each, generating 1,800 news articles as evidence. Questions are categorized as multi-hop (839), time-span (678), false premise (2,879), and uncertain specificity (2,952). Models receive pre-selected news articles as evidence and must answer correctly when sufficient evidence exists or select "Unanswerable" when evidence is insufficient or questions contain unverifiable assumptions. Zero-shot evaluation is performed across seven models (Qwen2.5 7B/14B/32B-Instruct, Phi3 variants) with temperature=0.0 and minimum 128k context window. Performance is measured using ADTScore, the harmonic mean of accuracy on answerable and unanswerable instances.

## Key Results
- LLMs achieve only 53.2 ADTScore on NeoQA, with best model (Qwen2.5 32B) showing significant limitations in evidence-based reasoning
- Models struggle asymmetrically with missing bridge entity information (69.7-90.7% error rate) versus missing answer information
- False premise and uncertain specificity questions are particularly challenging, with best model achieving only 38.6% and 26.7% accuracy respectively
- Larger models perform better overall but still fail to reliably distinguish sufficient from insufficient evidence

## Why This Works (Mechanism)
The benchmark works by using entirely fictional news events and entities that prevent models from relying on pretraining knowledge. This creates a controlled environment where success depends solely on reasoning with provided evidence rather than memorized facts. The multi-hop and false premise question types specifically test whether models can follow logical chains through evidence and identify when questions contain unverifiable assumptions.

## Foundational Learning
- **Evidence-based reasoning**: Understanding how to extract and combine information from multiple sources to answer questions. Why needed: Core skill being evaluated. Quick check: Can model answer multi-hop questions using only provided evidence.
- **False premise detection**: Identifying when questions contain assumptions that cannot be verified by evidence. Why needed: Tests critical reasoning beyond pattern matching. Quick check: Model selects "Unanswerable" for false premise questions.
- **Insufficient evidence detection**: Recognizing when available evidence cannot support a definitive answer. Why needed: Distinguishes between knowledge retrieval and reasoning. Quick check: Model defers appropriately when evidence is missing.

## Architecture Onboarding
**Component map**: Question Generator -> Timeline Generator -> Article Generator -> Evidence Selection -> Model Evaluation -> ADTScore Computation
**Critical path**: Question generation → Evidence pairing → Model inference → Answer parsing → ADTScore calculation
**Design tradeoffs**: Uses fictional content to prevent pretraining knowledge interference vs. potential lack of real-world complexity; zero-shot evaluation vs. potential for improved performance with fine-tuning
**Failure signatures**: Shortcut reasoning when bridge entity information is missing (69.7-90.7% of errors); poor detection of false premise/uncertain specificity questions (38.6% and 26.7% accuracy); reliance on token distribution artifacts
**First experiments**: 1) Test model on instances with deliberately removed bridge entity information to verify error patterns; 2) Compare performance on false premise questions versus other types; 3) Evaluate model confidence scores across answerable vs unanswerable instances

## Open Questions the Paper Calls Out
- Do LLMs exploit token distribution artifacts in NEOQA rather than performing genuine evidence-based reasoning? The paper notes GPT-4 Turbo achieved 53.6% accuracy on multi-hop questions without evidence, suggesting potential confounds from synthetic data generation.
- Why do models struggle asymmetrically with missing bridge entity information versus missing answer information in multi-hop reasoning? Models frequently answer as if nothing were missing when bridge entity information is absent (69.7%-90.7% of errors).
- How does performance on NEOQA change when retrieval must be performed rather than pre-selected? The paper excludes document retrieval from the task formulation, but real RAG systems must first retrieve relevant documents.

## Limitations
- Dataset uses public key encryption without implementation details, preventing direct validation of data structure and generation methodology
- Paper identifies five prompt variants but doesn't explicitly state which was selected for each model's final evaluation
- Zero-shot evaluation only limits generalizability to scenarios where fine-tuning might improve performance
- Potential confound from synthetic data token distribution that models may exploit rather than performing genuine reasoning

## Confidence
- **High confidence**: ADTScore metric formulation and interpretation; theoretical soundness of using fictional content to prevent pretraining knowledge interference
- **Medium confidence**: Claims about LLMs struggling with evidence-based reasoning and shortcut behavior; relative performance ordering across model sizes
- **Medium confidence**: Specific error rates (69.7-90.7% for bridge entity omissions; 38.6% and 26.7% for false premise/uncertain specificity)

## Next Checks
1. Attempt to decrypt and examine the NEOQA dataset structure to verify question type distribution and evidence-document pairing methodology
2. Reproduce zero-shot evaluation pipeline with Qwen2.5-7B-Instruct using temperature=0.0 and CoT elicitation, comparing ADTScore against reported baseline
3. Test model responses on instances with deliberately removed bridge entity information to independently verify the claimed 69.7-90.7% error rate when critical evidence is absent