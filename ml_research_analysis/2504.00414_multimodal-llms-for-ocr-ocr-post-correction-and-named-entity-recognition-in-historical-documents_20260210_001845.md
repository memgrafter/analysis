---
ver: rpa2
title: Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition
  in Historical Documents
arxiv_id: '2504.00414'
source_url: https://arxiv.org/abs/2504.00414
tags:
- historical
- recognition
- transcription
- mllms
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates multimodal Large Language Models (mLLMs)\
  \ for Optical Character Recognition (OCR), OCR Post-Correction, and Named Entity\
  \ Recognition (NER) on German historical documents from 1754\u20131870. The best-performing\
  \ mLLM, Gemini 2.0 Flash, achieved a normalized Character Error Rate (CER) of 1.27%\
  \ for OCR without preprocessing or fine-tuning, outperforming conventional OCR models."
---

# Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition in Historical Documents

## Quick Facts
- arXiv ID: 2504.00414
- Source URL: https://arxiv.org/abs/2504.00414
- Reference count: 40
- Primary result: mLLMs achieve state-of-the-art OCR performance (1.27% CER) on German historical documents without preprocessing or fine-tuning

## Executive Summary
This study evaluates multimodal Large Language Models (mLLMs) for Optical Character Recognition (OCR), OCR Post-Correction, and Named Entity Recognition (NER) on German historical documents from 1754-1870. The researchers found that mLLMs, particularly Gemini 2.0 Flash, can perform OCR without preprocessing or fine-tuning with a normalized Character Error Rate (CER) of 1.27%, outperforming conventional OCR models. A novel mLLM-based OCR Post-Correction method further reduced CER to 0.84%. For NER, mLLMs achieved over 90% fuzzy match rates directly from images for some directories, demonstrating their potential to revolutionize historical document transcription and data extraction with minimal preprocessing.

## Method Summary
The researchers evaluated multiple mLLMs including Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0 Flash, and LLaVA-NeXT on OCR and NER tasks. They used three historical directories: the U.S. Register (1812-1822), Gruber's Business Directory (1860-1870), and the Washington and Georgetown Directory (1822-1840). The evaluation involved extracting text from images without preprocessing or fine-tuning, followed by OCR Post-Correction using mLLMs. For NER, they employed a few-shot prompting approach to extract structured data from images. The study used character error rate (CER) and fuzzy matching with Levenshtein distance as primary metrics, comparing mLLM performance against conventional OCR models like Kraken and Transkribus.

## Key Results
- Gemini 2.0 Flash achieved a normalized CER of 1.27% for OCR without preprocessing or fine-tuning
- mLLM-based OCR Post-Correction reduced CER to 0.84%
- For NER, mLLMs achieved over 90% fuzzy match rates directly from images for some directories
- mLLMs demonstrated competitive performance without requiring domain-specific fine-tuning

## Why This Works (Mechanism)
The success of mLLMs in historical document processing stems from their ability to simultaneously process visual and textual information, enabling them to understand complex layouts and contextual relationships in historical documents. Unlike traditional OCR systems that process text in isolation, mLLMs can leverage their understanding of document structure, handwriting variations, and contextual patterns to improve recognition accuracy. The models' ability to perform zero-shot learning allows them to adapt to new document types without requiring extensive training data or preprocessing.

## Foundational Learning
- **Multimodal Processing**: Understanding how models process both visual and textual information simultaneously is crucial for document analysis tasks. Quick check: Verify that the model can correctly identify relationships between visual elements and text in sample documents.
- **Character Error Rate (CER)**: A metric that measures the accuracy of text recognition by calculating the edit distance between recognized and ground truth text. Quick check: Calculate CER on a small sample set to understand the metric's sensitivity.
- **Fuzzy Matching with Levenshtein Distance**: A string comparison technique that allows for approximate matching, essential for evaluating OCR accuracy where perfect matches are rare. Quick check: Test fuzzy matching on intentionally corrupted text to verify threshold settings.
- **Few-shot Prompting**: The technique of providing examples within prompts to guide model behavior without fine-tuning. Quick check: Compare performance with different numbers of examples to find optimal balance.
- **Historical Document Characteristics**: Understanding the unique challenges of historical documents including faded ink, varied handwriting, and archaic language. Quick check: Analyze document degradation patterns across different time periods.
- **OCR Post-Correction**: The process of using language models to correct errors made by initial OCR systems. Quick check: Measure improvement in CER before and after post-correction.

## Architecture Onboarding

**Component Map:**
Image -> mLLM Vision Encoder -> Text Generation Module -> Output

**Critical Path:**
The most critical path is the image encoding to text generation pipeline, where the quality of visual feature extraction directly impacts the accuracy of the generated text. This includes the vision transformer's ability to capture fine-grained details and the text decoder's capacity to generate contextually appropriate text.

**Design Tradeoffs:**
The primary tradeoff is between model size and inference speed. Larger models like Gemini 1.5 Pro achieve better accuracy but require more computational resources, while smaller models like Gemini 1.5 Flash offer faster processing at the cost of some accuracy. The study chose to prioritize accuracy over speed for historical document processing.

**Failure Signatures:**
Common failure modes include misinterpretation of decorative elements as text, confusion between similar characters (e.g., 's' and 'f' in Gothic script), and inability to handle severely degraded text. The models also struggle with documents containing multiple languages or unusual layouts.

**First Experiments:**
1. Test mLLM OCR performance on a single page with known ground truth to establish baseline accuracy
2. Compare mLLM OCR output with conventional OCR on the same document to quantify improvement
3. Evaluate OCR Post-Correction effectiveness by measuring CER reduction on a sample of corrected documents

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the research raises several important areas for future investigation regarding scalability, multilingual capabilities, and the potential for domain-specific fine-tuning.

## Limitations
- Evaluation restricted to German historical documents from a specific 116-year period (1754-1870)
- Only three historical directories used for NER evaluation, limiting representation of document diversity
- Does not address scalability challenges for processing large document collections
- No comparison with fine-tuned specialized models for OCR or NER tasks

## Confidence
- High confidence in: Comparative performance of mLLMs against conventional OCR models for the specific test set
- Medium confidence in: Claim that mLLMs can achieve state-of-the-art results "without any preprocessing or fine-tuning"
- Low confidence in: Generalizability of NER performance across different historical document types and languages

## Next Checks
1. Evaluate the same mLLM approach on multilingual historical documents spanning different scripts and time periods
2. Test scalability by processing a larger corpus (10,000+ pages) to measure computational efficiency
3. Compare mLLM performance against fine-tuned specialized OCR and NER models on identical document sets