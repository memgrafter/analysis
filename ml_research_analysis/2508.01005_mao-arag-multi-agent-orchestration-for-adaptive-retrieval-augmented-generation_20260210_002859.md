---
ver: rpa2
title: 'MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation'
arxiv_id: '2508.01005'
source_url: https://arxiv.org/abs/2508.01005
tags:
- question
- answer
- sub-question
- arxiv
- planner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MAO-ARAG introduces a multi-agent framework that dynamically selects\
  \ and orchestrates different RAG modules\u2014such as query reformulation, retrieval,\
  \ and document selection\u2014to tailor workflows for each query. A planner agent,\
  \ trained via reinforcement learning with a reward balancing F1 score and cost penalties,\
  \ constructs these adaptive pipelines."
---

# MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.01005
- Source URL: https://arxiv.org/abs/2508.01005
- Reference count: 14
- Primary result: Achieves highest average F1 score (52.91%) on QA datasets, outperforming best baseline by 3.08 points

## Executive Summary
MAO-ARAG introduces a multi-agent framework that dynamically selects and orchestrates different RAG modules—such as query reformulation, retrieval, and document selection—to tailor workflows for each query. A planner agent, trained via reinforcement learning with a reward balancing F1 score and cost penalties, constructs these adaptive pipelines. Experiments on multiple QA datasets show MAO-ARAG achieves the highest average F1 score (52.91%), outperforming the best baseline by 3.08 points while maintaining reasonable token costs and latency. The framework demonstrates effective balance between answer quality and computational cost.

## Method Summary
MAO-ARAG implements a multi-agent RAG system where a planner agent selects sequences of executor agents to form query-specific workflows. The planner, trained via Proximal Policy Optimization (PPO), outputs workflows from an action space including query rewriting, retrieval, document selection, and answer generation. Executors are powered by GPT-4o-mini and process queries according to the planner's instructions. The system uses a semi-Markov decision process to handle variable-duration actions and multi-turn interactions. Training combines NQ and HotpotQA data with a reward function balancing F1 score quality against token cost, latency, and retrieval call penalties.

## Key Results
- MAO-ARAG achieves average F1 score of 52.91% across seven QA datasets
- Outperforms best baseline by 3.08 F1 points while maintaining reasonable computational costs
- Demonstrates effective quality-cost trade-off control through RL-tuned planner

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling workflow planning from module execution allows the system to dynamically adapt to query complexity, improving efficiency over static pipelines.
- **Mechanism:** A "Planner" agent selects a sequence of "Executor" agents (e.g., Query Rewriter, Retrieval Agent) to form a query-specific workflow. This treats the RAG process not as a fixed chain but as a composition of modular steps.
- **Core assumption:** Query complexity varies significantly enough that a "one-size-fits" pipeline is suboptimal, and a distinct planning module can effectively identify this complexity.
- **Evidence anchors:**
  - [abstract] "A planner agent intelligently selects and integrates the appropriate agents... tailored for each query."
  - [section 3.1] Describes the separation of the Planner (designs workflow) and Executors (perform specific tasks).
  - [corpus] "Difficulty-Aware Agentic Orchestration" supports the general utility of query-specific workflows to balance efficiency and performance.
- **Break condition:** If the Planner fails to accurately gauge query difficulty or select the correct sequence, the system may generate incomplete answers (if too simple) or waste resources (if too complex).

### Mechanism 2
- **Claim:** Reinforcement Learning (RL) with a composite reward function enables the planner to optimize the trade-off between answer quality (F1 score) and operational cost.
- **Mechanism:** The planner is trained using Proximal Policy Optimization (PPO). The reward signal combines the F1 score of the final answer with penalties for token usage, latency (turns), and retrieval calls.
- **Core assumption:** The reward function accurately reflects the desired balance, and the penalty weights (specifically the hyperparameter $\alpha$) are tuned correctly for the target environment.
- **Evidence anchors:**
  - [abstract] "trained via reinforcement learning with a reward balancing F1 score and cost penalties."
  - [section 3.2] Equation (6) defines the total reward $R_{planner} = R_{f1} - \alpha \cdot R_{CP} - R_{FP}$.
  - [corpus] Evidence is weak/missing in the specific neighbor corpus regarding the novel application of PPO to this specific multi-agent RAG orchestration, distinguishing this work from standard RAG.
- **Break condition:** If the cost penalty ($\alpha$) is set too high, the RL agent may learn a "lazy" strategy (e.g., refusing to retrieve) to minimize cost at the expense of answer quality.

### Mechanism 3
- **Claim:** Modeling the system as a Multiagent Semi-Markov Decision Process (MSMDP) facilitates the coordination of heterogeneous agents with variable execution times.
- **Mechanism:** Unlike standard MDPs, MSMDP allows for "actions" (executors) that take variable amounts of time (e.g., parallel decomposition vs. serial decomposition). This formalizes the state transitions when sub-queries spawn new turns.
- **Core assumption:** The state representation (Observation) provided to the planner contains sufficient context to make optimal decisions in this extended time horizon.
- **Evidence anchors:**
  - [section 3.1] "We model the RAG system as a Multiagent Semi-Markov Decision Process (MSMDP)... accommodating multiple agents and allowing for actions of variable durations."
  - [figure 2] Illustrates the multi-turn nature where sub-questions result in new planner inputs.
  - [corpus] General support found in "MA-RAG" for collaborative reasoning, though the MSMDP formalization is specific to this paper's methodology.
- **Break condition:** If the context window of the planner is exceeded by long interaction histories (many turns), the state observation may become truncated, degrading decision quality.

## Foundational Learning

- **Concept:** **Proximal Policy Optimization (PPO)**
  - **Why needed here:** This is the optimization algorithm used to train the planner. Understanding the balance between the "Actor" (planning policy) and "Critic" (value estimation) is essential to debug training stability.
  - **Quick check question:** How does the clipping mechanism in PPO prevent the planner from changing its strategy too drastically in a single update?

- **Concept:** **Retrieval-Augmented Generation (RAG) Pipelines**
  - **Why needed here:** MAO-ARAG orchestrates standard RAG modules. You must understand what Query Rewriting and Document Selection do to understand why the planner might choose them (or skip them).
  - **Quick check question:** Why might "Query Decomposition" be necessary for a multi-hop question but wasteful for a simple factoid question?

- **Concept:** **Markov Decision Processes (MDPs)**
  - **Why needed here:** The paper extends MDPs to MSMDPs. Understanding the base concept (States, Actions, Rewards) is required to interpret the training loop defined in Algorithm 1.
  - **Quick check question:** In the context of this paper, what constitutes the "State" and what constitutes the "Action" for the Planner agent?

## Architecture Onboarding

- **Component map:**
  - **Planner Agent:** A 7B LLM (Qwen2.5-7B-Instruct) responsible for outputting the workflow sequence.
  - **Executors:** A set of functional agents (Query Rewriter, Retrieval Agent, etc.) mostly powered by GPT-4o-mini.
  - **Environment:** The dataset (NQ, HotpotQA), retriever (E5), and the reward calculator.

- **Critical path:**
  1.  **Input:** User Query -> Planner Observation.
  2.  **Decision:** Planner outputs a workflow (e.g., `[Query_Rewriter, Retrieval_Agent, Answer_Generator]`).
  3.  **Execution:** Executors run in sequence; if `Query_Decomposition` is used, the process recurses into sub-queries.
  4.  **Feedback:** Final answer compared to ground truth -> Reward calculated -> PPO update applied to Planner.

- **Design tradeoffs:**
  - **Planner Size:** The paper notes 0.5B/1.5B models struggled with initial instruction following, requiring distillation from the 7B model.
  - **Executor Backbone:** You can swap GPT-4o-mini for cheaper models (GPT-4.1-nano) to lower cost, but this may reduce the upper bound of F1 scores.
  - **Cost Penalty ($\alpha$):** Tuning $\alpha$ is the primary lever for trading quality for speed/cost.

- **Failure signatures:**
  - **Rapid Performance Drop:** If $\alpha > 0.2$, the planner learns to generate overly simplistic workflows (e.g., answering without retrieval) to avoid penalties.
  - **Format Errors:** If the planner outputs an invalid workflow sequence, the system incurs a Format Penalty (FP).
  - **High Latency:** Excessive use of `Query_Decomposition_Serial` leads to deep recursion and high turn counts.

- **First 3 experiments:**
  1.  **Sanity Check (Workflow Validity):** Run the untrained planner (MAO-ARAG w/o train) on a small validation set to verify it can generate executable workflows and handle the basic prompt format.
  2.  **Reward Ablation:** Train two separate planner models—one with $\alpha=0$ (focus on quality) and one with $\alpha=0.1$ (balanced)—and plot the F1 score vs. Token Cost curve to visualize the trade-off.
  3.  **Backbone Swapping:** Replace the Executor backbone (GPT-4o-mini) with a cheaper model (GPT-4.1-nano) while keeping the trained Planner fixed, to verify the system's modularity and measure the degradation in F1 score.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MAO-ARAG framework effectively optimize performance and cost by routing sub-tasks to heterogeneous executor APIs (e.g., mixing different LLM backbones) simultaneously?
- Basis in paper: [explicit] The conclusion states: "We also plan to use multiple optional APIs simultaneously as executor backbones, aiming for better results at a lower cost."
- Why unresolved: The current experiments (Section 4.5) use a single fixed backbone (e.g., GPT-4o-mini or GPT-3.5-turbo) for all executors during a run, rather than dynamically selecting the backbone per task.
- What evidence would resolve it: An ablation study showing a planner dynamically selecting cheaper models for simple sub-questions and stronger models for complex reasoning, resulting in a lower average cost than fixed-backbone baselines.

### Open Question 2
- Question: How can the cost-based reward function be refined to eliminate strategy fluctuations and better balance the trade-off between F1 score and resource consumption?
- Basis in paper: [explicit] The authors note in the Conclusion and Section 4.4 that the "coarse definition of the Cost Penalty term" may contribute to observed fluctuations in the metrics as the penalty weight $\alpha$ changes.
- Why unresolved: The current penalty sums normalized token cost, turn cost, and retrieval calls equally (Equation 4), which may not accurately reflect true operational constraints or opportunity costs.
- What evidence would resolve it: The demonstration of a weighted or learned cost penalty function that produces smooth, monotonic trade-off curves (Figure 4) without performance instability.

### Open Question 3
- Question: Is it possible to train the planner agent directly using reinforcement learning on smaller language models (e.g., <1.5B parameters) without relying on distillation from a larger model?
- Basis in paper: [explicit] Appendix D states that "it was not possible to train directly based on Qwen2.5-0.5B-Instruct and Qwen2.5-1.5B-Instruct because models of this size have issues with instruction-following capabilities."
- Why unresolved: The current framework requires a capable 7B model to learn the planning policy first, which is then distilled to smaller models, implying a high computational entry barrier.
- What evidence would resolve it: A successful training run where a sub-1B parameter model converges on an effective planning policy directly via PPO, achieving comparable F1 scores to the distilled models.

## Limitations
- RL training requires careful hyperparameter tuning, particularly the cost penalty weight α, which is set through empirical search rather than principled derivation
- Performance depends heavily on the choice of executor models (GPT-4o-mini) and planner architecture (Qwen2.5-7B-Instruct), raising questions about transfer to different model families
- While showing strong average F1 performance, the paper doesn't thoroughly explore failure cases where the planner might choose suboptimal workflows

## Confidence

**High Confidence**: The core architectural design (multi-agent orchestration with separated planner and executors) is clearly specified and the modular framework appears implementable. The reported F1 score improvement (52.91% average) over baselines and the demonstration of cost-quality trade-off control are well-supported by the experimental results across seven datasets.

**Medium Confidence**: The RL-based training methodology and the specific reward formulation (balancing F1 with cost penalties) are adequately described, though some implementation details like exact PPO hyperparameters and prompt templates are missing. The claimed benefits of the MSMDP formulation over standard MDPs are plausible but not extensively validated.

**Low Confidence**: Claims about the framework's robustness to different model backbones and its generalization to domains beyond the tested Wikipedia-based QA datasets are not directly supported. The paper mentions cost-sensitivity as a key benefit but doesn't fully explore scenarios where extreme cost minimization might compromise answer quality.

## Next Validation Checks

1. **Cost-Quality Trade-off Verification**: Train three planner variants with α values spanning the reported range (0.0, 0.1, 0.2) and measure F1 scores, token costs, and retrieval calls across all seven test datasets. Plot the Pareto frontier to verify that MAO-ARAG can actually navigate the claimed quality-cost spectrum rather than converging to a single point.

2. **Planner Decision Analysis**: Select 50 queries from different datasets and manually trace the planner's workflow decisions. Categorize cases where the planner chose complex workflows (e.g., including Query Decomposition) versus simple ones, and compare these choices against human judgments of query complexity. This would validate whether the planner's difficulty assessment aligns with reasonable expectations.

3. **Executor Backbone Robustness Test**: Replace GPT-4o-mini executors with a significantly cheaper model (e.g., GPT-4.1-nano or Claude Haiku) while keeping the trained planner fixed. Measure the degradation in F1 scores and changes in planner behavior to quantify how sensitive the system's performance is to executor model quality, which directly impacts practical deployment feasibility.