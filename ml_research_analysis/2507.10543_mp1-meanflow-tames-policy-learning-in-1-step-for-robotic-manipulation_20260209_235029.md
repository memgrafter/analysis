---
ver: rpa2
title: 'MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation'
arxiv_id: '2507.10543'
source_url: https://arxiv.org/abs/2507.10543
tags:
- learning
- success
- tasks
- robot
- flowpolicy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MP1 introduces a MeanFlow-based approach for 1-step policy learning
  in robotic manipulation, addressing the slow inference of diffusion models and the
  consistency constraints of flow-based methods. By learning interval-averaged velocities
  via the MeanFlow Identity, MP1 eliminates the need for ODE solvers and consistency
  losses, enabling genuine single-step trajectory generation.
---

# MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation

## Quick Facts
- arXiv ID: 2507.10543
- Source URL: https://arxiv.org/abs/2507.10543
- Reference count: 6
- Primary result: 1-step policy learning with 6.8 ms inference, 10.2% higher success than DP3, 7.3% higher than FlowPolicy

## Executive Summary
MP1 introduces a novel 1-step policy learning framework for robotic manipulation that addresses the slow inference of diffusion models and the consistency constraints of flow-based methods. By learning interval-averaged velocities via the MeanFlow Identity, MP1 eliminates the need for ODE solvers and consistency losses, enabling genuine single-step trajectory generation. The method incorporates Classifier-Free Guidance for improved controllability and a lightweight Dispersive Loss to enhance generalization in few-shot learning without affecting inference speed.

## Method Summary
MP1 leverages the MeanFlow Identity to learn interval-averaged velocities, allowing trajectories to be generated in a single step without iterative solving. This approach circumvents the slow inference of diffusion models and the consistency constraints of traditional flow-based methods. Classifier-Free Guidance is integrated to provide controllability, while a Dispersive Loss term improves generalization in few-shot learning scenarios. The architecture is evaluated on Adroit and Meta-World benchmarks, demonstrating significant improvements in both speed and success rates.

## Key Results
- 6.8 ms inference time, 19× faster than DP3 and nearly 2× faster than FlowPolicy
- 10.2% higher success rates than DP3
- 7.3% higher success rates than FlowPolicy

## Why This Works (Mechanism)
MP1's efficiency stems from learning interval-averaged velocities rather than point-wise velocities, eliminating the need for iterative ODE solvers. The MeanFlow Identity provides a principled way to compute these averages, enabling true 1-step generation. Classifier-Free Guidance allows for controllability without sacrificing speed, while the Dispersive Loss enhances generalization in few-shot settings by encouraging diverse trajectory generation.

## Foundational Learning
- **MeanFlow Identity**: Needed to compute interval-averaged velocities for 1-step generation. Quick check: Verify that the learned velocities accurately represent the time-averaged dynamics of the system.
- **Classifier-Free Guidance**: Required for controllability without separate classifiers. Quick check: Test the policy's ability to follow diverse goal specifications.
- **Flow-based models**: Provides the foundation for learning invertible mappings between states and actions. Quick check: Ensure the learned flow remains invertible and stable during training.
- **Few-shot learning**: Addresses the challenge of generalizing from limited demonstrations. Quick check: Evaluate performance on unseen tasks with minimal training data.
- **Trajectory generation**: Central to the method's application in robotic manipulation. Quick check: Assess the smoothness and feasibility of generated trajectories in simulation.

## Architecture Onboarding
- **Component map**: State Encoder -> MeanFlow Velocity Predictor -> Action Generator -> (Optional) Classifier-Free Guidance
- **Critical path**: Input state → MeanFlow prediction → Action output (single forward pass)
- **Design tradeoffs**: 1-step generation trades off some expressiveness for massive speed gains; Classifier-Free Guidance adds complexity but improves controllability
- **Failure signatures**: Degraded performance on complex, multi-step tasks; potential overfitting in few-shot settings without Dispersive Loss
- **First experiments**:
  1. Benchmark inference speed on a simple manipulation task
  2. Compare success rates against DP3 and FlowPolicy on Adroit
  3. Test generalization by training on few-shot demonstrations and evaluating on held-out tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Speed gains may not generalize across all hardware and workload conditions
- Generalization improvements are task-specific and may not hold under domain shifts
- Real-world robustness to sensor noise and actuator delays is not evaluated

## Confidence
- **Speed advantages**: High confidence due to elimination of ODE solvers
- **Generalization improvements**: Medium confidence, limited to few-shot benchmarks
- **Overall success rate gains**: Medium confidence, task-specific results

## Next Checks
1. Test the policy in real-world robotic manipulation tasks under variable latency and noise conditions to assess robustness.
2. Evaluate the policy on additional manipulation benchmarks or transfer to novel robotic platforms to confirm generalization.
3. Benchmark the inference latency and stability when scaling to multi-step or continuous control scenarios, including different hardware configurations.