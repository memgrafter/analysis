---
ver: rpa2
title: 'ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping'
arxiv_id: '2504.10857'
source_url: https://arxiv.org/abs/2504.10857
tags:
- grasp
- dataset
- reconstruction
- pose
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ZeroGrasp introduces a near real-time framework for simultaneous
  3D reconstruction and 6D grasp pose prediction from a single RGB-D image. The method
  couples an octree-based conditional variational autoencoder with novel components:
  a multi-object encoder and 3D occlusion fields to model inter-object relationships
  and occlusions, and a simple refinement algorithm that uses predicted reconstructions
  to improve grasp poses via contact constraints and collision detection.'
---

# ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping

## Quick Facts
- **arXiv ID:** 2504.10857
- **Source URL:** https://arxiv.org/abs/2504.10857
- **Reference count:** 40
- **Primary result:** ZeroGrasp achieves state-of-the-art performance on GraspNet-1B benchmark for simultaneous 3D reconstruction and 6D grasp pose prediction from single RGB-D input.

## Executive Summary
ZeroGrasp introduces a near real-time framework that jointly predicts 3D shape reconstructions and 6D grasp poses from a single RGB-D image. The method leverages an octree-based conditional variational autoencoder augmented with a multi-object encoder and 3D occlusion fields to model inter-object relationships and occlusions. A simple yet effective refinement algorithm improves grasp poses using the predicted reconstructions. Trained on a large-scale synthetic dataset (ZeroGrasp-11B), ZeroGrasp outperforms previous methods on the GraspNet-1B benchmark and demonstrates strong generalization to real-world objects through real-robot evaluations.

## Method Summary
ZeroGrasp addresses the simultaneous tasks of 3D shape reconstruction (SDF, normals) and 6D grasp pose prediction (view graspness, quality, angle, width, depth) from a single RGB-D image. The framework couples an octree-based conditional variational autoencoder with novel components: a multi-object encoder that uses a Transformer with rotary positional embeddings to encode object-specific features, and 3D occlusion fields that model inter-object relationships via projected occlusion flags. The method is trained on ZeroGrasp-11B, a large-scale synthetic dataset with 1 million images and 11.3 billion grasps, and fine-tuned on GraspNet-1B. A refinement algorithm uses predicted reconstructions to improve grasp poses through contact constraints and collision detection.

## Key Results
- Achieves state-of-the-art performance on GraspNet-1B benchmark for both reconstruction quality (Chamfer Distance, F-Score, Normal Consistency) and grasp pose prediction accuracy (AP, AP₀.₈, AP₀.₄).
- Outperforms previous methods in simultaneous 3D reconstruction and grasp prediction tasks.
- Demonstrates strong generalization to real-world objects with 92% grasp success rate in real-robot evaluations on 10 held-out objects.

## Why This Works (Mechanism)
The framework's effectiveness stems from its unified representation of geometry and grasping in a single octree-based CVAE architecture. The multi-object encoder captures contextual relationships between objects, while 3D occlusion fields explicitly model visibility constraints. The refinement algorithm leverages the predicted geometry to enforce physical contact constraints and collision avoidance, significantly improving grasp accuracy without requiring additional training.

## Foundational Learning

**Octree-based CVAE**: Why needed - Efficient hierarchical representation of 3D geometry. Quick check - Verify octree depth and branching factor match expected memory constraints.

**3D Occlusion Fields**: Why needed - Model inter-object visibility relationships for accurate reconstruction in cluttered scenes. Quick check - Validate occlusion flags are correctly computed via ray casting in synthetic multi-object scenes.

**Contact-based Refinement**: Why needed - Improve grasp poses using predicted geometry without additional training. Quick check - Confirm refinement parameters (γ_min, γ_max) produce stable contact points.

**Multi-object Transformer**: Why needed - Encode object-specific features with contextual relationships. Quick check - Verify RoPE encoding properly handles object positional relationships.

## Architecture Onboarding

**Component Map**: RGB-D Image + Instance Masks -> Multi-object Encoder -> Latent Code -> Octree-based CVAE (Prior, Encoder, Decoder) -> Octree Reconstruction -> 3D Occlusion Fields -> Final Prediction

**Critical Path**: The core inference pipeline flows through the multi-object encoder to produce latent codes, which the CVAE decoder uses to generate octree representations. The 3D occlusion fields are computed in parallel and fused with the reconstruction process.

**Design Tradeoffs**: Uses octree representation for memory efficiency versus voxel grids, accepts synthetic-only training versus real data collection, employs a simple refinement algorithm versus complex end-to-end grasp optimization.

**Failure Signatures**: Out-of-memory errors from dense grasp annotations, poor reconstruction in occluded regions indicating occlusion field issues, KL collapse or noisy SDF predictions suggesting training instability.

**3 First Experiments**:
1. Verify octree depth and branching factor produce expected memory usage with dense grasp annotations.
2. Test occlusion field computation by validating ray casting produces correct self and inter-object occlusion flags.
3. Confirm refinement algorithm produces stable contact points with γ_min=0.005 and γ_max=0.02 parameters.

## Open Questions the Paper Calls Out
None

## Limitations
- The ZeroGrasp-11B dataset and SAM-2 fine-tuning weights are not publicly available, creating barriers to exact reproduction.
- Real-robot experiments are limited to 10 objects, which may not be representative of broader performance.
- Octree implementation details, particularly the CVAE-U-Net integration, are not fully specified.

## Confidence
**Confidence Assessment: Medium**
- **State-of-the-art claims on GraspNet-1B**: Medium
- **Real-robot performance with 92% success**: Medium (limited to 10 objects)
- **Generalization to unseen real-world objects**: Medium (small test set)

## Next Checks
1. Recreate a subset of the ZeroGrasp-11B dataset using IsaacGym physics validation and V-HACD decomposition, and verify that the data processing pipeline generates compatible octree inputs.

2. Implement and validate the 3D Occlusion Fields component by verifying that ray casting correctly computes self and inter-object occlusion flags for synthetic multi-object scenes.

3. Conduct a real-robot grasping experiment on 10 held-out objects not seen during training to validate the reported 92% grasp success rate.