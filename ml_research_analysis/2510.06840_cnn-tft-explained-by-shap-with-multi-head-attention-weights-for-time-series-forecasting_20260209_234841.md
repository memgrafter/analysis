---
ver: rpa2
title: CNN-TFT explained by SHAP with multi-head attention weights for time series
  forecasting
arxiv_id: '2510.06840'
source_url: https://arxiv.org/abs/2510.06840
tags:
- attention
- time
- forecasting
- series
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a CNN-TFT-SHAP-MHAW model for multivariate
  time series forecasting, combining convolutional neural networks (CNNs) for local
  pattern extraction, a temporal fusion transformer (TFT) for long-range dependency
  modeling, and SHAP with multi-head attention weights (MHAW) for interpretability.
  The CNN module uses causal 1D convolutions to capture short-term temporal patterns,
  while the TFT applies multi-head self-attention to capture both short- and long-term
  dependencies.
---

# CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting

## Quick Facts
- arXiv ID: 2510.06840
- Source URL: https://arxiv.org/abs/2510.06840
- Authors: Stefano F. Stefenon; João P. Matos-Carvalho; Valderi R. Q. Leithardt; Kin-Choong Yow
- Reference count: 40
- Primary result: CNN-TFT-SHAP-MHAW model achieves up to 2.2% MAPE on hydroelectric natural flow forecasting

## Executive Summary
This paper introduces a hybrid deep learning architecture for multivariate time series forecasting that combines convolutional neural networks for local pattern extraction with a temporal fusion transformer for long-range dependency modeling. The model is enhanced with SHAP values and multi-head attention weights to provide interpretability. The architecture is evaluated on hydroelectric natural flow data, demonstrating superior performance compared to established deep learning baselines at short to medium forecasting horizons, with robust results across 50 runs.

## Method Summary
The proposed CNN-TFT-SHAP-MHAW model integrates three key components: a CNN module using causal 1D convolutions to capture short-term temporal patterns, a temporal fusion transformer (TFT) that employs multi-head self-attention to model both short- and long-term dependencies, and a SHAP-MHAW interpretability framework that combines SHAP values with attention weights. The model processes historical flow records through these components to generate interpretable forecasts with quantified feature contributions.

## Key Results
- Achieves MAPE of up to 2.2% on hydroelectric natural flow forecasting
- Demonstrates statistical robustness with 50-run validation showing average RMSE of 521.5 and MAE of 419.5
- Outperforms well-established deep learning models at 15-60 step forecasting horizons

## Why This Works (Mechanism)
The model's effectiveness stems from the complementary strengths of its components: CNNs excel at extracting local temporal patterns through their convolutional filters, while the TFT captures complex long-range dependencies through self-attention mechanisms. The SHAP-MHAW framework provides interpretability by quantifying both the contribution of individual features (via SHAP values) and the model's focus areas (via attention weights), enabling stakeholders to understand forecast drivers.

## Foundational Learning
- **Causal 1D Convolutions**: Needed for capturing local temporal patterns without information leakage; quick check: verify temporal ordering is preserved in convolutional outputs
- **Multi-Head Self-Attention**: Required for modeling complex dependencies across time; quick check: compare attention weight distributions across heads
- **SHAP Values**: Essential for quantifying feature importance in complex models; quick check: validate SHAP explanations against known feature relationships
- **Temporal Fusion Transformers**: Needed for handling varying temporal structures in time series; quick check: assess model performance on sequences with different temporal characteristics
- **Attention Weight Interpretation**: Required for understanding model focus; quick check: correlate attention patterns with known temporal events
- **Hybrid Model Integration**: Necessary for combining complementary strengths; quick check: perform ablation studies removing individual components

## Architecture Onboarding

**Component Map**
CNN (causal 1D convolutions) -> TFT (multi-head self-attention) -> SHAP-MHAW (interpretability)

**Critical Path**
Input time series -> CNN feature extraction -> TFT attention modeling -> Output prediction with SHAP-MHAW explanations

**Design Tradeoffs**
- CNN provides local pattern capture but may miss long-range dependencies
- TFT handles complex temporal relationships but increases computational cost
- SHAP-MHAW adds interpretability but requires additional computation
- Hybrid structure balances performance and interpretability

**Failure Signatures**
- Poor performance at longer horizons suggests attention mechanism limitations
- Inconsistent SHAP-attention alignment indicates explanation reliability issues
- High computational cost may limit real-time deployment
- Domain-specific results may not generalize across time series types

**First 3 Experiments**
1. Validate temporal ordering preservation in CNN convolutional outputs
2. Compare attention weight distributions across different time series types
3. Assess SHAP explanation reliability through feature perturbation analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can external knowledge sources be integrated to extend the utility of the CNN-TFT-SHAP-MHAW architecture?
- Basis in paper: The conclusion states that "Future work may explore the integration of external knowledge sources... to further extend the utility and adaptability."
- Why unresolved: The current study focused strictly on the historical flow record without incorporating exogenous variables like weather or economic data mentioned in the related works.
- Evidence: Improved forecasting accuracy or robustness when exogenous variables are added to the input pipeline.

### Open Question 2
- Question: Can the model's architecture be modified to maintain superiority at longer forecasting horizons?
- Basis in paper: The results show that while the model is best at 15-60 steps, NBEATSx outperforms it at the 120-step horizon by 0.92% RMSE.
- Why unresolved: The paper does not analyze why the hybrid structure loses competitive advantage at extended horizons compared to basis expansion models.
- Evidence: A comparative study where a modified CNN-TFT variant consistently beats NBEATSx at horizons >= 120 steps.

### Open Question 3
- Question: Does the proposed architecture generalize effectively to time series domains with different statistical properties?
- Basis in paper: The evaluation is limited to a single hydrological dataset (Tucuruí), despite the claim of applicability to scenarios with "varying temporal structures."
- Why unresolved: The model's performance on the specific hydroelectric dataset may not translate to domains with different noise profiles or seasonality, such as finance or solar irradiance.
- Evidence: Benchmarking the model on diverse datasets (e.g., PV load, traffic) against the state-of-the-art models listed in Table I.

## Limitations
- Narrow empirical scope limited to hydroelectric natural flow data only
- Performance superiority claims lack explicit comparison benchmarks
- SHAP-attention alignment validity is assumed but not empirically verified
- Limited metric focus may miss important aspects like computational efficiency

## Confidence
- Methodologically sound framework: High
- Empirical validation breadth: Medium
- Interpretability claims: Medium
- Generalization claims: Low

## Next Checks
1. Test the model on diverse time series datasets beyond hydroelectric data to assess generalizability
2. Conduct ablation studies to quantify the individual contributions of CNN, TFT, and SHAP-MHAW components
3. Compare SHAP-based explanations against attention weights through qualitative and quantitative alignment metrics to validate the interpretability claims