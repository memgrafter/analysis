---
ver: rpa2
title: 'Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models'
arxiv_id: '2512.13703'
source_url: https://arxiv.org/abs/2512.13703
tags:
- harmful
- content
- methods
- safe2harm
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Safe2Harm, a semantic isomorphism attack method
  that exploits the underlying principle similarity between harmful and legitimate
  scenarios to jailbreak large language models. The method rewrites harmful questions
  into semantically safe ones, extracts thematic mappings, generates safe responses,
  then inverts them to produce harmful content.
---

# Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models

## Quick Facts
- arXiv ID: 2512.13703
- Source URL: https://arxiv.org/abs/2512.13703
- Reference count: 40
- Safe2Harm achieves highest attack success rates compared to existing methods, with effectiveness increasing on larger models

## Executive Summary
Safe2Harm is a novel semantic isomorphism attack method that exploits the underlying principle similarity between harmful and legitimate scenarios to jailbreak large language models. The method rewrites harmful questions into semantically safe ones, extracts thematic mappings, generates safe responses, then inverts them to produce harmful content. Experiments on 7 mainstream LLMs and three benchmark datasets demonstrate that Safe2Harm outperforms existing methods in attack success rates. The paper also constructs a challenging 358-sample harmful content evaluation dataset and finds Qwen3Guard performs best for detection with over 77% accuracy.

## Method Summary
Safe2Harm operates through a four-step process: first, it rewrites harmful questions into semantically safe questions that preserve the underlying principles; second, it extracts thematic mappings between the safe and harmful versions; third, it generates safe responses to the rewritten questions; and finally, it inverts these responses back to harmful content using the extracted mappings. This approach leverages the semantic isomorphism between harmful and legitimate scenarios, allowing the model to generate harmful content while appearing to respond to safe prompts. The method is designed to work across different types of harmful content by identifying and exploiting the structural similarities between harmful and legitimate scenarios.

## Key Results
- Safe2Harm achieves the highest attack success rates compared to existing methods across 7 mainstream LLMs
- Attack effectiveness increases with larger model sizes
- Qwen3Guard demonstrates superior harmful content detection with over 77% accuracy on the constructed 358-sample dataset

## Why This Works (Mechanism)
The method exploits the semantic isomorphism between harmful and legitimate scenarios, leveraging the fact that many harmful requests have underlying legitimate analogs with similar structural principles. By transforming harmful prompts into safe ones that preserve these underlying principles, the attack tricks the model into generating content that can be inverted back to harmful material. This works because LLMs learn to associate semantic meaning with structural patterns, and the inversion process can recover the harmful intent from the safe response by reversing the thematic mappings established in the rewriting phase.

## Foundational Learning

**Semantic Isomorphism** - The principle that different semantic contexts can share structural similarities. Needed to understand how harmful and legitimate scenarios can be mapped. Quick check: Can you identify pairs of harmful and legitimate prompts that share the same underlying structure?

**Prompt Rewriting** - The technique of transforming one prompt into another while preserving certain semantic properties. Needed to convert harmful requests into safe ones. Quick check: Does the rewritten prompt maintain the core intent while removing explicit harmful elements?

**Thematic Mapping Extraction** - The process of identifying correspondences between semantic elements in different contexts. Needed to enable the inversion of safe responses back to harmful content. Quick check: Are the mappings between safe and harmful elements consistent and reversible?

**Response Inversion** - The technique of transforming a generated response back to its original semantic intent. Needed to recover harmful content from safe responses. Quick check: Does the inverted response maintain coherence and achieve the original harmful goal?

**Semantic Preservation** - The principle of maintaining core meaning while changing surface form. Needed to ensure the attack maintains effectiveness throughout the transformation process. Quick check: Does the final harmful output achieve the original intent despite the transformations?

## Architecture Onboarding

**Component Map**: Harmful Prompt -> Safe Rewrite -> Thematic Mapping Extraction -> Safe Response Generation -> Response Inversion -> Harmful Output

**Critical Path**: The most critical steps are the safe rewrite and response inversion phases, as failures in either will break the attack chain. The thematic mapping extraction must accurately capture the relationships between safe and harmful elements to enable successful inversion.

**Design Tradeoffs**: The method trades computational complexity for effectiveness - the multiple transformation steps increase attack success rates but require more processing. There's also a tradeoff between the degree of prompt rewriting (more extensive rewriting may be safer but harder to invert) and attack effectiveness.

**Failure Signatures**: Attack failure can occur at multiple points: if the safe rewrite fails to preserve the underlying principle, if thematic mappings are incomplete or inaccurate, if the safe response generation produces content that cannot be meaningfully inverted, or if the inversion process fails to recover the harmful intent coherently.

**Three First Experiments**:
1. Test the safe rewrite component in isolation by measuring how well rewritten prompts preserve the original intent while removing harmful elements
2. Validate the thematic mapping extraction by checking if the extracted mappings can accurately transform safe elements back to their harmful counterparts
3. Evaluate the response inversion component by measuring the coherence and harmfulness of inverted responses against ground truth harmful content

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes all harmful content has meaningful legitimate analogs, which may not hold for certain types of harmful content
- The relationship between model size and attack success rate is stated but not thoroughly analyzed or explained
- The paper does not address potential countermeasures that could be implemented to detect or prevent semantic isomorphism attacks

## Confidence
**High confidence in**: The experimental results showing Safe2Harm outperforms existing methods on tested benchmarks and LLMs; the constructed evaluation dataset with 358 harmful samples and detection results for Qwen3Guard appear methodologically sound.

**Medium confidence in**: The generalizability of results across different types of harmful content beyond the tested scenarios; the paper focuses on specific harmful content categories but does not extensively validate the method's effectiveness across diverse harm types.

**Low confidence in**: The claim that effectiveness "increases on larger models" - while stated, the relationship between model size and attack success rate is not thoroughly analyzed or explained.

## Next Checks
1. Test Safe2Harm against LLMs with varying degrees of content filtering sophistication to establish whether the method's success is primarily due to the attack technique or weaknesses in specific model implementations.

2. Conduct human evaluation studies to verify that the generated harmful content maintains coherence and intended harmful impact after the semantic inversion process, as automated metrics may not capture subtle quality degradation.

3. Evaluate the method's effectiveness on harmful content types not included in the original benchmark datasets, particularly those that may not have clear legitimate analogs (e.g., certain forms of hate speech or incitement to violence).