---
ver: rpa2
title: 'Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement
  Learning'
arxiv_id: '2510.24320'
source_url: https://arxiv.org/abs/2510.24320
tags:
- critique-rl
- uni00000011
- uni00000048
- uni0000004c
- critique
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of training critiquing language
  models to assess and provide feedback on model outputs for complex reasoning tasks
  without relying on stronger supervision or oracle verifiers. The authors propose
  Critique-RL, an online reinforcement learning approach based on a two-player actor-critic
  interaction paradigm where an actor generates responses, a critic provides feedback,
  and the actor refines its responses accordingly.
---

# Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.24320
- Source URL: https://arxiv.org/abs/2510.24320
- Reference count: 40
- Key outcome: Qwen2.5-7B achieved 9.02% gain on in-domain tasks and 5.70% gain on out-of-domain tasks compared to baselines

## Executive Summary
This paper introduces Critique-RL, a two-stage reinforcement learning framework for training language models to critique and provide feedback on model outputs for complex reasoning tasks. The approach addresses the challenge of developing discriminative critics without relying on stronger supervision or oracle verifiers. Through a two-player actor-critic interaction paradigm, the method first optimizes discriminability using rule-based rewards, then enhances helpfulness while maintaining discriminability through regularization. Experiments demonstrate significant performance improvements across various tasks and models, with strong generalization to unseen domains.

## Method Summary
Critique-RL employs a two-stage online reinforcement learning approach where an actor generates responses and a critic provides feedback. In Stage I, the critic is optimized using direct rule-based rewards to develop discriminability in assessing outputs. Stage II then optimizes helpfulness through indirect rewards from actor refinement while maintaining discriminability via a regularization term. This two-stage strategy addresses the limitation of indirect reward signals alone, which tend to optimize helpfulness at the expense of discriminability. The framework demonstrates efficient compute scaling and generalizes well to unseen tasks without requiring additional training data.

## Key Results
- Qwen2.5-7B achieved 9.02% gain on in-domain tasks and 5.70% gain on out-of-domain tasks compared to baselines
- Strong generalization to unseen tasks without additional training data
- Efficient compute scaling demonstrated across different model sizes
- Performance improvements sustained across multiple model architectures including Llama-3.1-8B

## Why This Works (Mechanism)
The two-stage optimization strategy effectively separates the learning of discriminability from helpfulness. By first establishing strong discriminative capabilities through direct rule-based rewards, the critic develops the ability to identify quality differences in outputs. The second stage then builds on this foundation by optimizing for helpfulness through actor refinement while preserving discriminability via regularization. This prevents the common pitfall where purely indirect rewards lead to critics that become too lenient or fail to provide meaningful feedback.

## Foundational Learning
- Reinforcement Learning Fundamentals: Understanding reward-based optimization and actor-critic frameworks is essential for grasping the two-stage training process
- Language Model Fine-tuning: Knowledge of parameter-efficient adaptation techniques helps understand how the approach modifies pre-trained models
- Reward Function Design: Understanding rule-based versus learned rewards explains the choice of direct rewards in Stage I
- Transfer Learning: Familiarity with generalization across domains helps interpret the out-of-domain performance gains

## Architecture Onboarding

**Component Map:**
Pre-trained Language Model -> Stage I Critic Training (Direct Rewards) -> Stage II Critic+Actor Training (Indirect Rewards + Regularization) -> Optimized Critiquing Model

**Critical Path:**
1. Initialize pre-trained model as actor
2. Stage I: Train critic using rule-based rewards on actor outputs
3. Stage II: Jointly train actor and critic with indirect rewards and discriminability regularization
4. Evaluate performance on in-domain and out-of-domain tasks

**Design Tradeoffs:**
- Stage I uses rule-based rewards for reliable discriminability but may lack nuance for complex tasks
- Stage II regularization balances helpfulness with discriminability but introduces hyperparameter sensitivity
- Online training enables efficient scaling but may limit batch optimization benefits

**Failure Signatures:**
- Over-regularization in Stage II leading to diminished helpfulness gains
- Rule-based rewards in Stage I failing to capture task-specific quality nuances
- Actor-critic instability during joint optimization in Stage II

**3 First Experiments:**
1. Ablation study removing Stage I to assess impact of discriminability optimization
2. Hyperparameter sweep on regularization coefficient to find optimal balance
3. Cross-domain evaluation to verify generalization claims on completely unseen task types

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on rule-based rewards in Stage I may not generalize to more nuanced critiquing tasks
- Regularization introduces hyperparameters that affect robustness across different settings
- Online training approach may face scalability challenges with very large datasets
- Experimental validation limited to Qwen2.5-7B and Llama-3.1-8B models

## Confidence
- High confidence in the core methodology and two-stage optimization framework
- Medium confidence in the claimed generalization benefits to unseen tasks
- Medium confidence in the scalability and efficiency claims
- Low confidence in the method's robustness to different reward function designs

## Next Checks
1. Test the method's performance when rule-based rewards are replaced with learned reward functions or human feedback to assess robustness to reward design
2. Evaluate the approach on significantly larger model sizes (e.g., 70B+ parameters) to verify computational efficiency claims at scale
3. Conduct ablation studies to determine the sensitivity of performance to the regularization coefficient and other key hyperparameters across multiple task families