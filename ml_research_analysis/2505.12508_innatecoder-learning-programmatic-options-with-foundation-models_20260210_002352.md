---
ver: rpa2
title: 'InnateCoder: Learning Programmatic Options with Foundation Models'
arxiv_id: '2505.12508'
source_url: https://arxiv.org/abs/2505.12508
tags:
- options
- agent
- innate
- coder
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents INNATE CODER, a system that learns programmatic
  options from foundation models to improve reinforcement learning efficiency. Unlike
  previous approaches that learn options from environment interaction, INNATE CODER
  extracts them from foundation models in a zero-shot manner by breaking generated
  programs into sub-programs representing different agent behaviors.
---

# InnateCoder: Learning Programmatic Options with Foundation Models

## Quick Facts
- **arXiv ID**: 2505.12508
- **Source URL**: https://arxiv.org/abs/2505.12508
- **Reference count**: 40
- **Primary result**: InnateCoder learns programmatic options from foundation models zero-shot, achieving state-of-the-art performance on MicroRTS and Karel the Robot by inducing a semantic search space where neighbor programs encode different behaviors.

## Executive Summary
INNATE CODER learns programmatic options from foundation models without environment interaction, extracting behavioral primitives from generated programs and using them to induce a semantic search space. The system outperforms baselines that learn options from experience or sample them randomly, and is competitive with state-of-the-art algorithms while being computationally accessible through limited foundation model queries. Tested on MicroRTS and Karel the Robot, it benefits from thousands of options compared to previous work using dozens or hundreds.

## Method Summary
InnateCoder queries a foundation model with the problem description and DSL grammar to generate candidate programs, which are decomposed into sub-programs representing different agent behaviors. These options are filtered by behavioral signatures collected across sampled states to ensure diversity, then used to construct a semantic neighborhood function. Stochastic hill-climbing searches a mixed space of syntax and semantic neighborhoods, alternating between random subtree replacement and option-based replacement with probability ε=0.4, allowing exploration of both novel syntactic structures and efficient exploitation of FM-derived priors.

## Key Results
- Outperformed baselines that didn't use options, learned options from experience, or sampled options randomly on MicroRTS and Karel the Robot
- Competitive with state-of-the-art algorithms and previous competition winners
- Benefits plateau at ~5000 options, allowing use of thousands of options versus dozens in prior work
- Only queries foundation models a small number of times as pre-processing step

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Foundation models can produce helpful programmatic options zero-shot, even when their complete programs fail to solve the target task.
- **Mechanism**: Query a foundation model with the problem description and DSL grammar to generate m programs. Decompose each program into sub-programs by extracting every subtree rooted at a non-terminal symbol in the AST. These sub-programs become candidate options encoding temporally extended actions.
- **Core assumption**: Sub-programs of imperfect solutions encode reusable behavioral primitives (e.g., "train workers," "harvest resources").
- **Evidence anchors**: [abstract]: "InnateCoder learns them from the general human knowledge encoded in foundation models in a zero-shot setting, and not from the knowledge the agent gains by interacting with the environment." [Section 3.1]: "We hypothesize that these programmatic policies can be broken up into sub-programs that can encode options."

### Mechanism 2
- **Claim**: Filtering options by behavioral signatures creates a semantic search space where neighbor programs differ in behavior, not just syntax.
- **Mechanism**: Roll out each candidate option across 300-700 sampled states to collect action signatures (vectors of actions taken). Keep only one option per distinct action signature. Use these filtered options to define a semantic neighborhood function Nᵐₖ that replaces AST subtrees with option ASTs matching the same non-terminal symbol.
- **Core assumption**: Options with identical action signatures are behaviorally redundant; diversity of signatures correlates with behavioral diversity useful for search.
- **Evidence anchors**: [Section 3.2]: "We filter the set of options O harnessed from the foundation model into a set Ω of behaviorally different options... we use these action signatures to characterize the option's behavior." [Section 4.3]: Results show plateau at ~5000 options, suggesting diminishing returns beyond behavioral saturation.

### Mechanism 3
- **Claim**: Mixing syntax-space and semantic-space neighborhood functions enables both broad exploration and efficient exploitation of FM-derived priors.
- **Mechanism**: With probability ε=0.4, use syntax neighborhood (random subtree replacement). With probability 1-ε, use semantic neighborhood (option-based replacement). Stochastic hill-climbing searches this mixed space with restarts upon local optima.
- **Core assumption**: Options cover only part of the program space; syntax access guarantees completeness while semantic access improves sample efficiency.
- **Evidence anchors**: [Section 3.3]: "INNATE CODER does not search solely in the semantic space, but mixes both syntax and semantic spaces in the search... to guarantee that INNATECODER can access all programs." [Appendix G]: Varying ε ∈ {0.1-0.7} shows robustness; extreme values degrade performance.

## Foundational Learning

- **Concept**: **Options framework (temporally extended actions)**
  - **Why needed here**: The entire system is built on treating sub-programs as callable options with initiation conditions, policies, and termination.
  - **Quick check question**: Can you define the three components of an option tuple (Iω, πω, Tω)?

- **Concept**: **Context-free grammars and ASTs**
  - **Why needed here**: Programs are generated, parsed, and mutated via grammar rules and AST operations.
  - **Quick check question**: Given a production rule ρ → if h then a, can you draw the corresponding AST subtree?

- **Concept**: **Stochastic hill-climbing with restarts**
  - **Why needed here**: The search algorithm iteratively improves candidates via neighborhood evaluation and restarts on local optima.
  - **Quick check question**: How does stochastic hill-climbing differ from exhaustive search in terms of completeness vs. efficiency?

## Architecture Onboarding

- **Component map**: FM Query Module → Option Extractor → Behavioral Filter → Semantic Space Builder → Mixed Search Engine
- **Critical path**: FM Query → Option Extraction → Behavioral Filtering → Search. If filtering removes too many options, semantic search degrades; if FM generates syntactically invalid programs, extraction fails.
- **Design tradeoffs**:
  - **Number of FM queries**: More queries increase option diversity but raise pre-processing cost (paper uses 100-120).
  - **Option set size**: Benefits plateau ~5000 (Figure 5); larger sets don't harm but don't help.
  - **ε value**: Controls syntax vs. semantic balance; 0.3-0.6 robust (Appendix G).
  - **DSL design**: Narrow DSLs limit FM expressiveness; overly permissive DSLs increase search space.
- **Failure signatures**:
  - FM generates programs violating grammar → extraction crashes (validate with CFG parser).
  - All options filtered out → Ω empty → semantic neighborhood undefined (fallback to pure syntax search).
  - SHC stuck in local optimum with no improvement across restarts → consider increasing budget or option diversity.
  - Data leakage suspected (FM seen solutions) → verify via FM baseline (should perform poorly if no leakage).
- **First 3 experiments**:
  1. **Baseline comparison**: Run InnateCoder vs. SHC (no options) vs. LISS-o vs. LISS-r on a single Karel task; confirm sample efficiency gap.
  2. **Ablate option count**: Vary |Ω| ∈ {300, 1400, 5000, 30000} on LetMeOut map; reproduce plateau curve (Figure 5).
  3. **Test ε sensitivity**: Run with ε ∈ {0.1, 0.4, 0.7} on NoWhereToRun map; verify robustness band (Appendix G).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the option filtering mechanism be adapted for continuous action spaces?
- **Basis in paper**: [explicit] The authors state, "Note that this assumes discrete action spaces. Future research will investigate different ways of measuring behavior in continuous action spaces."
- **Why unresolved**: The current method relies on creating "action signatures" (discrete vectors of actions) to filter behaviorally distinct options, a process that does not translate directly to continuous domains.
- **What evidence would resolve it**: A successful implementation of InnateCoder on a continuous control benchmark (e.g., MuJoCo) using a similarity metric suitable for continuous action distributions.

### Open Question 2
- **Question**: Can programmatic options extracted by InnateCoder be effectively utilized as callable functions within neural network policies?
- **Basis in paper**: [explicit] Section 5 notes, "While we use options to define a search space, future work will explore their use as functions neural policies can call."
- **Why unresolved**: The current system uses options strictly to induce a semantic search space for program synthesis, leaving their potential integration as low-level primitives for Deep Reinforcement Learning agents unexplored.
- **What evidence would resolve it**: Experiments demonstrating improved sample efficiency or performance in a Hierarchical RL setup where a neural policy selects InnateCoder options as temporally extended actions.

### Open Question 3
- **Question**: Why does sample efficiency plateau when the option set size exceeds 5,000?
- **Basis in paper**: [inferred] Figure 5 shows a performance plateau beyond 5,000 options. The authors "conjecture" that the relative number of helpful options remains constant, but the exact cause is not empirically validated.
- **Why unresolved**: It is unclear if the plateau is due to a saturation of useful behaviors in the foundation model's output or a limitation of the stochastic hill-climbing search algorithm's ability to utilize larger libraries.
- **What evidence would resolve it**: A comparative analysis of search algorithms on large option sets, or a diversity analysis proving the density of "winning" behaviors remains constant as the set size scales.

## Limitations

- **LLM Generation Quality**: The paper does not specify decoding parameters (temperature, top-p) for foundation model queries, which significantly impacts option library quality and diversity.
- **Self-Play Integration**: MicroRTS results rely on the 2L self-play algorithm as a wrapper, but implementation details are absent, making direct comparison difficult.
- **Behavioral Signature Generalization**: Action signatures are computed on 300-700 sampled states, but the sampling distribution and state representativeness across different tasks are not detailed.

## Confidence

- **High Confidence**: The core mechanism of extracting and filtering options from foundation models is well-supported by experimental results showing clear sample efficiency gains over baselines (Figure 3-4, Table 2).
- **Medium Confidence**: The semantic search space construction and mixed neighborhood strategy are theoretically sound, but the behavioral filtering mechanism lacks corpus validation.
- **Low Confidence**: The scalability claims (benefits plateau at ~5000 options) are based on limited task variations and require testing across more diverse domains.

## Next Checks

1. **LLM Generation Sensitivity**: Run the option extraction pipeline with varying decoding parameters (temperature ∈ {0.1, 0.7, 1.0}) and measure how option library size and behavioral diversity change. Compare search performance with the same ε but different option sets.
2. **State Sampling Impact**: Test the behavioral filtering with different numbers of sampled states (100 vs. 700) and different sampling distributions (random vs. curriculum-based). Measure how this affects option deduplication rates and subsequent search efficiency.
3. **Continuous Action Space Extension**: Adapt the behavioral signature mechanism to continuous actions (e.g., using action distribution similarity metrics instead of discrete vectors). Validate on a continuous control benchmark like OpenAI Gym's LunarLanderContinuous to test the framework's limits.