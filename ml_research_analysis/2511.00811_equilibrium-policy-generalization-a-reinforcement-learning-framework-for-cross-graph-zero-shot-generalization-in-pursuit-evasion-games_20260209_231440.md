---
ver: rpa2
title: 'Equilibrium Policy Generalization: A Reinforcement Learning Framework for
  Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games'
arxiv_id: '2511.00811'
source_url: https://arxiv.org/abs/2511.00811
tags:
- uni00000013
- policy
- pursuer
- evader
- equilibrium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning framework for pursuit-evasion
  games that can generalize to unseen graph structures without fine-tuning. The core
  idea is to train policies across diverse graphs against equilibrium policies for
  each individual graph, using dynamic programming for exact equilibria in no-exit
  cases and a heuristic matching approach for multi-exit scenarios.
---

# Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games

## Quick Facts
- **arXiv ID**: 2511.00811
- **Source URL**: https://arxiv.org/abs/2511.00811
- **Reference count**: 40
- **Primary result**: A reinforcement learning framework that achieves strong zero-shot generalization across unseen graph structures in pursuit-evasion games without fine-tuning.

## Executive Summary
This paper introduces a novel reinforcement learning framework for pursuit-evasion games that enables agents to generalize to entirely new graph structures without retraining. The key innovation is training policies across diverse graphs against equilibrium policies computed for each specific graph, using exact dynamic programming for simple cases and heuristic matching for complex multi-exit scenarios. A sequence model with shortest-path distance features allows the policies to represent states across different graphs. Experimental results demonstrate that the trained pursuer and evader policies achieve competitive zero-shot performance across real-world graphs, outperforming traditional shortest-path strategies and matching or exceeding fine-tuned state-of-the-art methods in multi-exit settings.

## Method Summary
The framework trains pursuer and evader policies across a diverse set of graphs by having them play against graph-specific equilibrium policies computed using dynamic programming for no-exit cases or a heuristic matching approach for multi-exit scenarios. The policies use a sequence model architecture that takes shortest-path distance features as input, enabling cross-graph representation. During training, each graph has its own equilibrium policy, allowing the agents to learn robust strategies that generalize beyond the specific graphs seen during training. The approach avoids the computational burden of computing Nash equilibria for every new graph by learning transferable policies instead.

## Key Results
- Zero-shot performance: Trained policies achieve strong performance on unseen graphs without fine-tuning
- Benchmark performance: Outperforms shortest-path strategies and matches or exceeds fine-tuned state-of-the-art methods in multi-exit scenarios
- Scalability: The approach scales to large graphs and varying numbers of pursuers

## Why This Works (Mechanism)
The framework succeeds because it trains policies against optimal or near-optimal strategies for each graph, forcing them to learn robust and generalizable behavior patterns. By using shortest-path distance features as input, the sequence model can represent states across different graph topologies. The equilibrium-based training ensures that policies are not overfitting to specific graph structures but rather learning fundamental pursuit-evasion strategies that transfer across graphs.

## Foundational Learning
- **Dynamic programming for equilibrium computation**: Why needed - To obtain exact optimal strategies for simple graphs; Quick check - Verify DP implementation correctly computes Nash equilibria for small test graphs
- **Graph theory fundamentals**: Why needed - Understanding connectivity, shortest paths, and graph metrics is essential for feature engineering; Quick check - Confirm distance features are correctly computed for test graphs
- **Sequence modeling for graph representations**: Why needed - Enables learning policies that work across different graph structures; Quick check - Test model on graphs with similar and different topologies to verify generalization
- **Reinforcement learning fundamentals**: Why needed - The framework relies on standard RL algorithms for policy training; Quick check - Verify convergence of RL training on simple graphs

## Architecture Onboarding
- **Component map**: Graph generator -> Equilibrium policy computation -> Sequence model training -> Zero-shot evaluation on new graphs
- **Critical path**: 1) Generate diverse training graphs, 2) Compute equilibrium policies for each graph, 3) Train sequence model policies using RL against equilibrium policies, 4) Evaluate zero-shot performance on held-out graphs
- **Design tradeoffs**: Exact equilibrium computation for no-exit graphs vs. heuristic matching for multi-exit graphs; computational cost of equilibrium computation vs. generalization performance
- **Failure signatures**: Poor zero-shot performance on graphs with different characteristics than training set; instability during training when equilibrium policies are not optimal; failure to converge when graphs are too dissimilar
- **First experiments to run**: 1) Test zero-shot performance on simple graphs not in training set, 2) Compare against shortest-path baseline on held-out graphs, 3) Evaluate sensitivity to graph diversity in training set

## Open Questions the Paper Calls Out
None

## Limitations
- Exact equilibrium formulation for multi-exit graphs relies on heuristic matching approach without theoretical guarantees
- Comparison primarily against shortest-path baselines rather than other deep RL generalization methods
- Limited testing across diverse graph topologies and sizes beyond those explicitly evaluated

## Confidence
- **High**: The core methodology of training against graph-specific equilibrium policies is clearly described and experimentally validated on multiple real-world graphs
- **Medium**: Claims about achieving competitive performance with fine-tuned state-of-the-art methods in multi-exit scenarios, though this is based on a limited set of baselines
- **Low**: The scalability claims to very large graphs and arbitrary numbers of pursuers, as these were not extensively tested across diverse configurations

## Next Checks
1. Test the trained policies on graphs with significantly different characteristics (e.g., highly irregular degree distributions, varying edge weights) not represented in the training set to assess true generalization boundaries
2. Compare the equilibrium-based training approach against other graph neural network or meta-learning methods for cross-graph generalization in pursuit-evasion settings
3. Evaluate the computational efficiency and solution quality on graphs with thousands of nodes to verify the claimed scalability, including runtime analysis for both training and inference phases