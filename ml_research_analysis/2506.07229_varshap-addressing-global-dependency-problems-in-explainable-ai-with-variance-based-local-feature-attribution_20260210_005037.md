---
ver: rpa2
title: 'VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based
  Local Feature Attribution'
arxiv_id: '2506.07229'
source_url: https://arxiv.org/abs/2506.07229
tags:
- feature
- attribution
- features
- arshap
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V ARSHAP introduces a novel local feature attribution method that
  addresses global dependency issues in existing approaches like SHAP by using variance
  reduction as the core metric. Instead of relying on global data distributions, V
  ARSHAP perturbs features locally around the instance being explained, measuring
  how each feature's value reduces prediction uncertainty in that neighborhood.
---

# VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution

## Quick Facts
- arXiv ID: 2506.07229
- Source URL: https://arxiv.org/abs/2506.07229
- Reference count: 40
- Primary result: VARSHAP outperforms SHAP and LIME across faithfulness, robustness, and complexity metrics by measuring local variance reduction instead of global expected values.

## Executive Summary
VARSHAP introduces a novel local feature attribution method that addresses global dependency issues in existing approaches like SHAP by using variance reduction as the core metric. Instead of relying on global data distributions, VARSHAP perturbs features locally around the instance being explained, measuring how each feature's value reduces prediction uncertainty in that neighborhood. Theoretical analysis shows variance is the unique function satisfying desirable axioms for shift-invariant attribution. Experiments on synthetic and real-world datasets demonstrate VARSHAP outperforms SHAP and LIME across faithfulness, robustness, and complexity metrics. VARSHAP maintains SHAP's axiomatic properties while providing more stable, locally-focused explanations that better capture instance-specific model behavior.

## Method Summary
VARSHAP computes feature importance through variance reduction in local model behavior, measuring how much fixing a feature reduces prediction uncertainty in a neighborhood around the instance being explained. The method perturbs "out-of-coalition" features from a Gaussian distribution centered at the specific instance (N(x, αΣ)) rather than sampling from global background data like KernelSHAP. This instance-centered perturbation avoids global dependency problems and improves consistency under distribution shift. VARSHAP retains the Shapley value framework, computing weighted averages of variance differences across all feature coalitions, which ensures theoretical guarantees like efficiency and symmetry are preserved while providing more stable, locally-focused explanations.

## Key Results
- VARSHAP consistently ranks higher than SHAP and LIME across faithfulness (FaithfulnessCorrelation, MonotonicityCorrelation), robustness (LocalLipschitzEstimate, MaxSensitivity), and complexity (Sparseness, EffectiveComplexity) metrics
- VARSHAP attributions remain stable when global data distributions shift, while SHAP attributions change significantly under the same conditions
- The method maintains SHAP's axiomatic properties (efficiency, symmetry, null player) while providing more locally-focused explanations
- Performance peaks at α = 0.6 for perturbation scale, with systematic variation showing optimal locality around this value

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing the expected value characteristic function with variance reduction may better isolate local model behavior by measuring uncertainty resolution rather than output shift.
- **Mechanism:** VARSHAP calculates feature attribution as the weighted average reduction in prediction variance when a feature is "known" (fixed) versus "unknown" (perturbed) across all possible feature coalitions. If fixing a feature significantly drops the variance of the output distribution in the local neighborhood, that feature is deemed important.
- **Core assumption:** The paper assumes that importance is best defined as the reduction in local output uncertainty (variance), satisfying axioms of shift invariance, zero property, and sign independence.
- **Evidence anchors:**
  - [abstract] "VARSHAP measures feature importance through variance reduction in local model behavior... justified by showing that variance is the unique function satisfying key axioms."
  - [section 2.1] Proposition 1 states that any continuous characteristic function aiming for shift invariance must measure dispersion, and $d(x)=x^2$ is the unique solution satisfying the proposed axioms.
  - [corpus] Related work "A Visual Tool for Interactive Model Explanation using Sensitivity Analysis" supports the broader utility of sensitivity-based approaches, though it does not specifically validate variance reduction as the unique metric.
- **Break condition:** This mechanism may fail if the model is highly non-smooth or discontinuous, causing variance to fluctuate wildly without meaningful insight into feature contribution.

### Mechanism 2
- **Claim:** Using an instance-centered perturbation distribution ($\Pi(x)$) decouples the explanation from the global background data distribution, potentially improving consistency under distribution shift.
- **Mechanism:** Unlike KernelSHAP, which samples missing features from a global background dataset, VARSHAP samples "out-of-coalition" features from a Gaussian distribution centered at the specific instance being explained ($N(x, \Sigma)$).
- **Core assumption:** The local neighborhood around an instance captures the relevant decision boundary dynamics, and global statistics are irrelevant or potentially misleading for that specific prediction.
- **Evidence anchors:**
  - [section 2.2] Defines the perturbation function $\Pi(x)$ where $E[X] = x$, explicitly avoiding global sampling.
  - [section 3.1] Case Study 1 shows VARSHAP attributions remain stable for a point $[0,0]$ when the global distribution of a separate subgroup (Group C) is manipulated, whereas SHAP attributions change.
  - [corpus] "Causal SHAP: Feature Attribution with Dependency Awareness..." highlights the failure of standard SHAP to handle dependencies correctly, a problem VARSHAP approaches by localizing the reference distribution.
- **Break condition:** If the perturbation scale ($\alpha$) is set too high, the "local" samples may leave the true local decision manifold, effectively sampling out-of-distribution points and invalidating the locality assumption.

### Mechanism 3
- **Claim:** Retaining the Shapley value framework (weighting by $\omega(|S|)$) allows the variance-based method to inherit theoretical guarantees like efficiency and symmetry.
- **Mechanism:** The method computes variance reduction for a feature $j$ across all subsets $S$, weighting them by the standard Shapley kernel. This ensures the sum of attributions equals the total variance reduction (Efficiency) and identical features receive identical scores (Symmetry).
- **Core assumption:** The computational complexity of Shapley estimation (often NP-hard) is acceptable for the sake of theoretical consistency, or that efficient approximations (like the proposed weighted local linear regression) are sufficient.
- **Evidence anchors:**
  - [section 2.3] Equation 1 formally defines the attribution $\Phi_j$ using the Shapley kernel $\omega(|S|)$ applied to the difference in variance terms.
  - [section 2.3] Lists the satisfied axioms: efficiency, symmetry, and null player.
  - [corpus] "When is the Computation of a Feature Attribution Method Tractable?" suggests that while Shapley-based methods are powerful, their tractability depends heavily on model structure, implying VARSHAP faces similar computational constraints.
- **Break condition:** The mechanism relies on accurate estimation of variance via sampling; if sample size is insufficient for the specific model complexity, the Shapley weighting will amplify noise rather than signal.

## Foundational Learning

- **Concept: Shapley Values (Cooperative Game Theory)**
  - **Why needed here:** VARSHAP is not just a heuristic; it is structurally built on the Shapley value formula. Understanding concepts like "coalitions," "marginal contribution," and the "Shapley kernel" is required to comprehend why the method sums specific weighted differences.
  - **Quick check question:** If feature A contributes equally to the output variance in every possible coalition subset as feature B does, what property ensures they receive the same attribution score? (Answer: Symmetry).

- **Concept: Sensitivity Analysis (Local vs. Global)**
  - **Why needed here:** The core critique of SHAP in this paper relies on distinguishing between global sensitivity (how the model behaves across the whole dataset) and local sensitivity (how the model behaves near a specific instance).
  - **Quick check question:** Why would sampling "missing" feature values from the entire training dataset (Global) mislead an explanation for a specific outlier instance?

- **Concept: Variance as Dispersion**
  - **Why needed here:** The paper argues that variance is the "unique" function satisfying specific axioms for attribution. You must understand variance not just as a statistic, but here as a proxy for "prediction uncertainty" or "information gain."
  - **Quick check question:** In VARSHAP, if fixing a feature's value causes the local output variance to drop to zero, what does that imply about the feature's relationship to the prediction locally?

## Architecture Onboarding

- **Component map:** Input -> Perturbation Engine -> Characteristic Function Evaluator -> Shapley Aggregator -> Output
- **Critical path:** The determination of the **Perturbation Scale $\alpha$** (Section 2.2) is the most sensitive hyperparameter. It defines the "locality." The paper suggests $\alpha \in [0.3, 1.0]$, with $0.6$ performing best.
- **Design tradeoffs:**
  - **Locality vs. Stability:** Lower $\alpha$ (tighter perturbation) is strictly local but may be sensitive to noise/gradients. Higher $\alpha$ covers more ground but risks incorporating global behavior or out-of-distribution samples.
  - **Independent vs. Conditional Perturbation:** The implementation uses independent Gaussian noise (diagonal covariance) for efficiency. This ignores feature correlations, which the paper acknowledges as a limitation (Section 5), noting that conditional distributions could improve accuracy for correlated features.
- **Failure signatures:**
  - **Inconsistent Rankings:** If $\alpha$ is misspecified, rankings of feature importance may shift unpredictably between similar instances.
  - **High Variance in Attributions:** If the number of samples to estimate $Var_{\Omega}(S)$ is too low, the attributions will fluctuate on repeated runs.
  - **Sensitivity to Irrelevant Features:** Similar to LIME, if the local linear approximation (used for efficiency in experiments) is poorly fitted, it may assign weight to features that have no causal link (though VARSHAP's variance mechanism attempts to resist this via the Null Player axiom).
- **First 3 experiments:**
  1. **Hyperparameter Sweep ($\alpha$):** Run VARSHAP on a held-out validation set varying $\alpha$ (e.g., 0.1 to 2.0) to find the "stable plateau" where attributions do not change significantly with small adjustments to $\alpha$.
  2. **Global Shift Robustness Check:** Replicate the "Case Study 1" setup. Train a model on a dataset, generate an explanation for a fixed point, then retrain the model on a perturbed dataset (shifted distribution) and verify if the explanation for that *same fixed point* remains stable.
  3. **Linearity Stress Test:** Apply VARSHAP to a purely linear model $y = w_1x_1 + w_2x_2$. Verify if the resulting attributions match the analytical solution derived in Appendix A.3 ($-w_i^2 Var(X_i)$) to ensure the implementation of the Shapley kernel and variance logic is correct.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can VARSHAP's perturbation mechanism be extended to conditional probability distributions to improve performance on datasets with highly correlated features?
- **Basis in paper:** [explicit] Section 5 states that the current independent Gaussian implementation is a simplification and suggests extending the framework to conditional probability distributions for better handling of feature interactions.
- **Why unresolved:** The current implementation assumes feature independence (diagonal covariance matrix), which is known to distort attributions when strong correlations exist between input features.
- **What evidence would resolve it:** A comparative study on synthetic datasets with known correlation structures, measuring attribution error between marginal (current) and conditional perturbation implementations.

### Open Question 2
- **Question:** Does constraining perturbations to the local neighborhood effectively mitigate adversarial attacks on explainability compared to global methods?
- **Basis in paper:** [explicit] Section 5 posits that more sophisticated perturbation mechanisms could help mitigate vulnerabilities to adversarial attacks, such as those by Slack et al. [20], which exploit out-of-distribution sampling.
- **Why unresolved:** While the paper hypothesizes that local constraints improve robustness, it does not test VARSHAP against specific adversarial attacks designed to fool explanation methods.
- **What evidence would resolve it:** Benchmarking VARSHAP against standard adversarial attack algorithms to see if it maintains explanation fidelity better than globally-dependent methods like KernelSHAP.

### Open Question 3
- **Question:** Is VARSHAP less sample-efficient than KernelSHAP due to the statistical requirements of estimating variance versus estimating the mean?
- **Basis in paper:** [inferred] VARSHAP estimates the characteristic function using variance (Eq. 2), whereas KernelSHAP uses the expected value (mean); estimating variance typically requires more samples to achieve the same level of statistical stability.
- **Why unresolved:** The paper focuses on accuracy and robustness rankings but does not analyze convergence rates or computational overhead relative to the number of model evaluations.
- **What evidence would resolve it:** An ablation study plotting attribution stability and error against the number of sampled perturbations for both methods.

## Limitations
- The implementation uses independent Gaussian noise (diagonal covariance) rather than conditional distributions, limiting accuracy for datasets with correlated features
- Computational cost may be higher than KernelSHAP due to the statistical requirements of estimating variance versus mean
- Performance depends critically on correctly specifying the perturbation scale α, with no clear automatic selection method provided
- The method may struggle with highly non-smooth or discontinuous models where local variance fluctuations don't meaningfully reflect feature importance

## Confidence
- **High confidence:** Theoretical framework establishing variance as the unique function satisfying shift-invariance axioms
- **Medium confidence:** Empirical claims regarding performance improvements over SHAP and LIME
- **Low confidence:** Reproducibility without exact specifications for perturbation sample counts and approximation methods

## Next Checks
1. Perform ablation studies varying the perturbation sample count to establish the tradeoff between attribution stability and computational cost
2. Test VARSHAP on datasets with strong feature correlations to evaluate the impact of using diagonal covariance versus conditional sampling
3. Compare VARSHAP against alternative local attribution methods like Integrated Gradients or SHAP with conditional expectations to better contextualize its performance gains