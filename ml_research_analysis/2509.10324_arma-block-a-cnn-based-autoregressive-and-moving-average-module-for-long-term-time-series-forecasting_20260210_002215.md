---
ver: rpa2
title: 'ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term
  Time Series Forecasting'
arxiv_id: '2509.10324'
source_url: https://arxiv.org/abs/2509.10324
tags:
- block
- forecasting
- arma
- series
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The ARMA block is a CNN-based module for long-term time series\
  \ forecasting, inspired by the ARIMA model. It uses two convolutional components\u2014\
  one for trend prediction (autoregression) and one for local variation refinement\
  \ (moving average)\u2014enabling direct multi-step forecasting."
---

# ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting

## Quick Facts
- arXiv ID: 2509.10324
- Source URL: https://arxiv.org/abs/2509.10324
- Reference count: 0
- The ARMA block achieves competitive accuracy on long-term time series forecasting, particularly for trend-varying data, using a lightweight two-branch CNN architecture inspired by ARIMA.

## Executive Summary
The ARMA block introduces a CNN-based autoregressive and moving average module for long-term time series forecasting. It decomposes prediction into trend-capture (AR) and residual-refinement (MA) components, enabling direct multi-step forecasting without iterative error accumulation. Experiments on nine benchmark datasets demonstrate competitive performance, particularly on trend-varying data, while maintaining simplicity. The block inherently encodes positional information, suggesting potential as a lightweight alternative to positional embeddings.

## Method Summary
The ARMA block uses two parallel convolutional components: an autoregressive (AR) branch that captures trend from raw input, and a moving average (MA) branch that refines local variations from residuals (input minus AR output). Both use 5×5 filters with output channels matching the forecast horizon. The final prediction is the sum of AR and MA outputs. The architecture inherently encodes positional information through padding effects, and can be enhanced with RevIN normalization for non-stationarity. Training uses MSE loss with AdamW optimizer.

## Key Results
- ARMA achieves competitive accuracy across nine benchmark datasets, particularly excelling on trend-varying data like ILI
- Ablation studies confirm both AR and MA components are essential, with ARMA outperforming single CNN baselines (MSE 1.353 vs 1.699 on ILI)
- The block inherently encodes absolute positional information, demonstrated through linear probes achieving MAE ~0.0017-0.0019

## Why This Works (Mechanism)

### Mechanism 1
Decomposing prediction into trend-capture and residual-refinement improves accuracy on trend-varying data. The AR component learns the dominant trend (larger gradient direction), while the MA component refines high-frequency local variations from residuals. This decomposition works when time series signals can be meaningfully separated into low-frequency trend and high-frequency residual components.

### Mechanism 2
Direct multi-step forecasting via high-dimensional convolutional filters avoids error accumulation from iterative prediction. Unlike classical ARIMA which forecasts iteratively, ARMA uses conv filters with output channels matching forecast horizon, predicting all steps simultaneously. This works when each future time step has learnable characteristics that can be captured directly from input history.

### Mechanism 3
CNNs inherently encode absolute positional information through padding, potentially replacing explicit positional embeddings. Convolutional layers with zero-padding expose boundary positions to unique filter responses, allowing the network to learn absolute position as a side effect of feature extraction. This works when positional awareness improves forecasting and is not already captured by input features.

## Foundational Learning

- **ARIMA model decomposition (AR, I, MA components)**: The ARMA block is explicitly inspired by ARIMA; understanding that AR captures autocorrelation (trend) and MA captures error smoothing (local refinement) clarifies why the two-branch architecture works.
  - Quick check: Given a time series with strong trend but noisy fluctuations, which component (AR or MA) would you expect to dominate the prediction?

- **Receptive field in CNNs for sequential data**: The 5×5 filter size determines how much history each prediction can access. Understanding receptive field helps diagnose why certain patterns are captured or missed.
  - Quick check: If your input sequence is 96 steps and you use a 5×5 conv filter with no dilation, what is the maximum temporal context for each output position?

- **Residual learning / error correction**: The MA branch explicitly operates on residuals (input - AR output), following a correction paradigm. Understanding residual connections helps debug whether both branches contribute meaningfully.
  - Quick check: If AR output already matches ground truth perfectly, what should the MA branch learn?

## Architecture Onboarding

- **Component map**:
  Input (x_in)
  │
  ├──────────────────┐
  │                  │
  ▼                  ▼
  AR Conv Layer      (x_in - y_ar)
  │                  │
  │                  ▼
  │              MA Conv Layer
  │                  │
  ▼                  ▼
  y_ar              y_ma
  │                  │
  └──────┬───────────┘
         │
         ▼
   y_out = y_ar + y_ma

- **Critical path**:
  1. Ensure residual computation (x_in - y_ar) is correct—this is the MA input.
  2. Verify output channels match forecast horizon (direct multi-step).
  3. Confirm both branches receive gradients (check for dead branches during training).

- **Design tradeoffs**:
  - Filter size: Larger filters increase receptive field but add parameters. Paper uses 5×5 as default.
  - Single block vs. stacked: Paper presents single-block results; deeper stacks unexplored.
  - RevIN enabled: Paper uses reversible instance normalization for non-stationarity—removing it may hurt on datasets like Exchange or ILI.
  - Simplicity vs. state-of-the-art: ARMA is competitive but does not consistently beat DLinear or transformer baselines across all datasets.

- **Failure signatures**:
  - Poor performance on stationary data without trend shifts (AR component underutilized).
  - High-frequency noise in predictions (MA component overfitting residuals).
  - Performance degradation on very long horizons if filter receptive field insufficient.
  - Training instability if AR and MA branches have unbalanced gradient magnitudes.

- **First 3 experiments**:
  1. **Reproduction on single dataset**: Implement ARMA block and train on ETTh1 (96→96 prediction). Compare MSE/MAE against paper-reported values (0.436/0.458). Verify both AR and MA outputs via correlation analysis as in Figure 2.
  2. **Position encoding probe**: Freeze trained ARMA CNN, add linear decoder head, train to predict linear position index from CNN features. Confirm MAE < 0.002 as in Table 2.
  3. **Ablation on ILI dataset**: Compare full ARMA vs. AR-only (remove MA branch) vs. single CNN. Reproduce Table 3 gap (MSE 1.353 vs 1.699) to validate decomposition benefit on trend-varying data.

## Open Questions the Paper Calls Out

- **Can the ARMA block function as a direct, lightweight replacement for positional embeddings in Transformer-based architectures?**
  - Basis: The abstract and conclusion suggest the block inherently encodes absolute positional information, proposing it as a potential substitute for positional embeddings.
  - Unresolved: While the paper proves the block contains positional information via a linear probe, it does not validate its effectiveness when used to replace embeddings in actual Transformer models.
  - Resolution: Benchmarks from standard Transformer models (e.g., Informer) trained with positional embeddings removed and replaced by the ARMA block.

- **How does the ARMA block compare to structured state-space models (SSMs) like Mamba in terms of efficiency and accuracy?**
  - Basis: The conclusion explicitly suggests the block's utility as a potential alternative to structured state-space modules like Mamba.
  - Unresolved: The experiments compare ARMA against Transformers and linear models, but no comparison is made against recent SSM architectures.
  - Resolution: A direct evaluation of ARMA against Mamba or similar SSMs on long-sequence modeling benchmarks.

- **How sensitive is the ARMA block's performance to the choice of convolutional kernel size (filter size) across different horizons?**
  - Basis: The method section notes the filter size is adjustable, but experiments rely solely on a fixed 5×5 filter without ablation on size.
  - Unresolved: The receptive field varies with filter size; it is unclear if the fixed 5×5 size is optimal for all forecasting horizons (e.g., 96 vs. 720 steps).
  - Resolution: An ablation study testing varying filter sizes against different prediction horizons on the ETT datasets.

## Limitations
- Performance advantage is primarily observed on trend-varying data, with limited generalizability to stationary or noise-dominated series
- Architectural claims regarding inherent positional encoding remain empirically demonstrated but theoretically underexplained
- Ablation study does not explore architectural depth (single block vs. stacked) or hyperparameter sensitivity

## Confidence
- **High Confidence**: The decomposition mechanism (AR + MA) improving trend-varying forecasting is well-supported by ablation results and consistent with ARIMA theory
- **Medium Confidence**: The claim that CNNs inherently encode positional information is empirically demonstrated but lacks theoretical grounding specific to time series forecasting
- **Low Confidence**: The assertion that ARMA is a "lightweight replacement for positional embeddings" in sequential models extrapolates beyond the presented evidence

## Next Checks
1. **Position Encoding Generalization**: Train ARMA on datasets with varying sequence lengths and alignments; test whether implicit positional encoding degrades on out-of-distribution lengths compared to explicit sinusoidal embeddings
2. **Depth Scaling Study**: Implement stacked ARMA blocks (2-3 layers) and compare performance against the single-block baseline to assess whether decomposition benefits compound with depth
3. **Noise Robustness Benchmark**: Create synthetic datasets with controlled noise-to-trend ratios; evaluate ARMA vs. pure CNN and transformer baselines to quantify decomposition advantage in different signal regimes