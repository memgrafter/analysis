---
ver: rpa2
title: Using LLMs as prompt modifier to avoid biases in AI image generators
arxiv_id: '2504.11104'
source_url: https://arxiv.org/abs/2504.11104
tags:
- prompts
- prompt
- image
- bias
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LLMs can effectively reduce bias in
  text-to-image generation by modifying user prompts. Using three open-weight image
  generators (Stable Diffusion XL, 3.5, and Flux) and four LLM prompt modifiers, the
  study shows significant increases in image diversity while reducing bias without
  modifying the image generators themselves.
---

# Using LLMs as prompt modifier to avoid biases in AI image generators

## Quick Facts
- **arXiv ID**: 2504.11104
- **Source URL**: https://arxiv.org/abs/2504.11104
- **Reference count**: 27
- **Primary result**: LLM-modified prompts significantly increase image diversity and reduce bias across three text-to-image generators without modifying the generators themselves.

## Executive Summary
This paper demonstrates that off-the-shelf large language models can effectively reduce bias in text-to-image generation by modifying user prompts before they reach the image generator. Using three open-weight image generators (Stable Diffusion XL, Stable Diffusion 3.5, and Flux) and four LLM prompt modifiers, the study shows significant increases in image diversity while reducing bias without modifying the image generators themselves. The method works particularly well for less advanced image generators like SDXL, though limitations persist for certain contexts like disability representation. While occasionally producing results that diverge from original user intent for elaborate prompts, this approach generally provides more varied interpretations of underspecified requests.

## Method Summary
The study uses four LLM prompt modifiers (ChatGPT, Claude, Llama, and Mistral) to analyze user prompts and generate up to four diverse variants that explicitly specify different attributes (gender, ethnicity, age, disabilities). These modified prompts are then sent to three text-to-image generators (SDXL, SD3.5, and Flux) to produce four images per variant at 768x768 resolution. Diversity is measured using three VLMs that classify gender, ethnicity, age, and disability representation, with manual review validating the automated classification. The approach leverages existing LLMs and image generators without requiring model retraining or architectural changes.

## Key Results
- LLM-modified prompts significantly increased diversity across all tested image generators, with SDXL showing the most dramatic improvements
- The method successfully reduced biases for most tested categories, particularly gender and ethnicity, though disability representation remained challenging
- More advanced image generators (Flux, SD35) showed less room for improvement but still benefited from prompt modification
- Elaborate user prompts occasionally produced results diverging from original intent when modified by LLMs

## Why This Works (Mechanism)

### Mechanism 1
LLMs can expand underspecified prompts to counter anticipated biases in image generators without modifying the generators themselves. The LLM analyzes a user prompt, identifies attributes left unspecified (e.g., gender, ethnicity), and generates multiple variant prompts that explicitly specify diverse values for those attributes. This forces the image generator to produce varied outputs rather than relying on biased implicit defaults from its training data.

### Mechanism 2
Bias reduction effectiveness correlates inversely with image generator capability—less advanced models benefit more from prompt modification. Advanced models like Flux and SD35 already incorporate some diversity in their training or post-processing. When an LLM adds diversity directives to prompts for these models, the marginal improvement is smaller because the baseline is higher. Less advanced models like SDXL have stronger embedded biases and respond more dramatically to explicit diversity cues.

### Mechanism 3
Effective prompt modification requires translating abstract diversity goals into concrete, visually-specifiable attributes. Keywords like "diverse" are insufficient for image generators processing single-subject prompts. Successful LLM prompters (ChatGPT, Claude) convert diversity goals into explicit visual descriptors (e.g., "wearing a prosthetic leg," "plus-sized model," specific ethnic markers) that the image generator can render.

## Foundational Learning

**Concept: Text-to-Image Bias Amplification**
- Why needed here: Understanding that small training data imbalances (e.g., 51.4% male images) amplify in generation (e.g., 54.2% male outputs) explains why intervention is necessary.
- Quick check question: Given a training set with 55% Group A and 45% Group B, would you expect generated images from neutral prompts to maintain this ratio, equalize it, or amplify the majority?

**Concept: Statistical vs. Normative Representation**
- Why needed here: The paper notes that equal representation may not be appropriate for all categories (e.g., firefighters at 3% female vs. doctors at 50% female). Designers must decide when to match population statistics vs. pursue aspirational representation.
- Quick check question: For a "flight attendant" prompt, should the system aim for current workforce demographics (~75% female in many regions) or 50/50 representation?

**Concept: Prompt Specificity Spectrum**
- Why needed here: The intervention works best for underspecified prompts ("a doctor") but can over-diversify already-specific prompts ("a disabled doctor in a wheelchair"). Understanding where a prompt falls on this spectrum determines whether modification is appropriate.
- Quick check question: Would you classify "a happy couple" as underspecified or specific? What about "a same-sex couple celebrating their wedding in Tokyo"?

## Architecture Onboarding

**Component map:**
User prompt → LLM prompt modifier (ChatGPT/Claude/Llama/Mistral or fine-tuned smaller model) → LLM generates 4 diverse prompt variants → T2I model (SDXL/SD35/Flux) → 4 images per variant → VLM ensemble (3 models) → classify gender, ethnicity, age, disability representation → aggregate diversity metrics → user receives images

**Critical path:**
1. Prompt received → LLM bias analysis (20s with 70B model; 0.4-0.6s with fine-tuned 14-24B model)
2. LLM generates 4 diverse prompt variants
3. Each variant sent to T2I generator (2.4s for SDXL batch; 38s for Flux batch)
4. VLMs classify output images → diversity score calculated
5. User receives images (optionally: system flags remaining biases)

**Design tradeoffs:**
- **Latency vs. quality**: Large LLMs (70B) provide better bias anticipation but add 20s+ latency. Fine-tuned smaller models (14-24B) reduce to <1s but may miss subtle biases.
- **Over-diversification vs. user intent**: Strong diversity directives can produce historically inaccurate outputs (e.g., "Black German WWII soldiers"). System prompts must balance correction against contextual appropriateness.
- **Model capability vs. intervention value**: Advanced T2I models (Flux, SD35) need less intervention but are slower. Budget-constrained deployments may prefer SDXL + aggressive prompt modification.

**Failure signatures:**
- **Instruction reversal**: T2I model assigns attributes to wrong subject (wheelchair appears under patient instead of doctor) → indicates instruction-following failure, not addressable via prompt modification
- **Keyword blindness**: LLM outputs "diverse representation" without specific visual descriptors → outputs remain biased for single-subject prompts
- **Over-modification**: Elaborate user prompts get significantly altered → user intent lost (e.g., "soldier with gun" becomes gunless, "Doctor Strange" becomes generic due to copyright concerns)
- **Truncation**: Long modified prompts exceed 77-token CLIP limit → prompt cut off, unpredictable outputs

**First 3 experiments:**
1. **Baseline bias measurement**: Run 41 prompts from paper through your target T2I model without modification. Use VLM ensemble to classify outputs. Compare your bias rates against Table 2 (SD35-alone: 61% male, 74.6% Caucasian, 0% disabled).
2. **LLM modifier comparison**: Implement prompt modification with 2 different LLMs (one high-quality like GPT-4/Claude, one smaller/faster). Measure diversity improvement and latency overhead. Expect ~30% increase in non-white representation with larger models.
3. **Elaborate prompt robustness test**: Test 5 highly specific prompts (e.g., from civitai.com) through the modifier pipeline. Manually evaluate whether modified outputs diverge from original intent. If >20% divergence, add conditional logic to reduce modification strength for high-specificity prompts.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can fine-tuning smaller, efficient LLMs (e.g., Phi-4) effectively replicate the bias mitigation of large models while significantly reducing latency?
- Basis in paper: The author notes that to mitigate high latency (+143%), "a smaller LLM should be finetuned," specifically suggesting Phi-4 or Mistral Small as candidates.
- Why unresolved: Initial tests with smaller models provided diversity but failed to anticipate potential biases (e.g., lacking ethnic specifics) compared to large models like Claude or ChatGPT.
- What evidence would resolve it: Benchmarks demonstrating a fine-tuned small model matching the bias-reduction quality of large models with substantially lower inference time.

**Open Question 2**
- Question: Can Vision-Language Models (VLMs) be effectively deployed as autonomous judges to detect residual biases in generated images?
- Basis in paper: The conclusion suggests that "the usage of a VLM as a judge to look for biases still present and hint towards those... should be investigated."
- Why unresolved: The current study relied on manual review and automated metadata collection; the proposed concept of VLM-based "self-reflexivity" remains untested.
- What evidence would resolve it: A working implementation where a VLM successfully identifies and flags biased outputs in a feedback loop without human intervention.

**Open Question 3**
- Question: How can prompt modification strategies distinguish between underspecified prompts requiring diversification and elaborate prompts requiring intent preservation?
- Basis in paper: The paper highlights a limitation where results "occasionally diverge from original user intent for elaborate prompts," suggesting an additional task prompt component might prevent this.
- Why unresolved: The current method modifies prompts globally, often altering specific user requirements (e.g., removing a gun from a "soldier" prompt) in favor of safety/diversity.
- What evidence would resolve it: A modified system that evaluates prompt complexity and selectively applies diversification, preserving fidelity for detailed prompts.

**Open Question 4**
- Question: Does the LLM-based mitigation approach generalize effectively to professions with complex or ambiguous real-world demographics, such as judges or lawyers?
- Basis in paper: The author states regarding occupational bias that "This should be further researched with additional job prompts like a judge or a lawyer."
- Why unresolved: The current experiment focused on professions with strong, clear gender stereotypes (e.g., firefighters, nurses), leaving the method's efficacy for other roles unverified.
- What evidence would resolve it: Experimental results applying the method to these new professions, showing appropriate statistical representation aligned with real-world data.

## Limitations
- The exact LLM system prompt for bias identification and modification is not specified, making exact replication difficult
- Elaborate user prompts occasionally produce outputs that diverge significantly from original intent when modified
- Disability representation remained challenging even with prompt modification, indicating limitations for certain bias categories

## Confidence
- **High Confidence**: The core finding that LLM-modified prompts significantly increase diversity across multiple T2I models and bias categories. The methodology for measuring diversity through VLM classification is reproducible and the effect sizes are substantial (e.g., 30% increase in non-white representation).
- **Medium Confidence**: The comparative effectiveness across different T2I models (SDXL vs. Flux vs. SD35) and LLM prompters (ChatGPT vs. Claude vs. Llama vs. Mistral). While results are consistent, model version differences and prompt engineering variations could affect outcomes.
- **Low Confidence**: The generalizability to prompts outside the tested categories, particularly for highly specialized or culturally-specific contexts not covered in the 41-prompt dataset. The paper also does not address long-term deployment considerations like user acceptance of modified outputs.

## Next Checks
1. **Elaborate Prompt Robustness Test**: Systematically evaluate 20 prompts from civitai.com or similar repositories with high specificity. Measure divergence rates between original and modified outputs, and implement conditional logic to reduce modification strength for prompts exceeding a specificity threshold.
2. **Cross-Cultural Bias Analysis**: Test the prompt modification system with prompts specific to cultures not well-represented in the original dataset (e.g., Indigenous occupations, non-Western cultural practices). Assess whether LLMs can correctly identify and address biases in unfamiliar cultural contexts.
3. **Longitudinal Bias Evolution**: Run the same 41 prompts through current T2I models every 3 months to track how bias levels change over time with model updates. Compare the effectiveness of prompt modification against evolving baseline biases to determine if this intervention remains necessary as models improve.