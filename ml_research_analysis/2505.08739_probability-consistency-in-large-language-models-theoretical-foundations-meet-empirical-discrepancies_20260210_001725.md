---
ver: rpa2
title: 'Probability Consistency in Large Language Models: Theoretical Foundations
  Meet Empirical Discrepancies'
arxiv_id: '2505.08739'
source_url: https://arxiv.org/abs/2505.08739
tags:
- layer
- attention
- context
- distance
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study formally proves that sequence perplexity is theoretically
  invariant under any factorization order, including forward, backward, or arbitrary
  permutations, establishing a mathematical benchmark for evaluating LLM consistency.
  Despite this theoretical expectation, empirical results show systematic deviations:
  forward and backward models achieve similar but not identical perplexities, with
  forward models consistently outperforming backward models; permuted training yields
  significantly higher losses.'
---

# Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies

## Quick Facts
- arXiv ID: 2505.08739
- Source URL: https://arxiv.org/abs/2505.08739
- Reference count: 40
- Key outcome: Forward and backward models achieve similar but not identical perplexities, with forward models consistently outperforming backward models; permuted models show significantly higher losses, revealing systematic deviations from theoretical probability consistency.

## Executive Summary
This study bridges probability theory and LLM architecture by proving that sequence perplexity should theoretically be invariant under any factorization order (forward, backward, or arbitrary permutations) while empirically demonstrating systematic deviations from this expectation. Using GPT-2 models trained on neuroscience text with different token orderings, the research reveals that while forward and backward models achieve similar perplexities with forward consistently outperforming backward, permuted models exhibit significantly higher losses. Attention analysis shows that forward/backward models develop strong positional biases favoring adjacent and long-range tokens, while permuted models exhibit distinct patterns. Representational alignment is higher between forward and backward models but degrades with depth, diverging sharply for permuted models. These findings connect previously disconnected domains of probability consistency and architectural biases, revealing how data-driven and architectural factors disrupt theoretical equivalence in LLMs.

## Method Summary
The study trains GPT-2 models (124M/355M/774M) on 1.3B tokens of neuroscience publications (2002-2022) with three different token orderings: forward (natural order), backward (reversed within context), and permuted (fixed arbitrary arrangement within context). All models use identical data, tokenization (BPE with 50,257 vocab trained only on forward text), and training procedures, differing only in within-context token arrangement. Sequences are formatted as BOS + 1,023 tokens spanning the full context window. Models are trained for 5 epochs with AdamW optimizer and evaluated on validation perplexity, attention patterns (entropy, normalized rank vs. distance), representational similarity (RSA), and the BrainBench neuroscience benchmark.

## Key Results
- Forward and backward models achieve similar perplexities (r=0.992) with forward consistently outperforming backward (d=0.191-0.665 scaling with model size)
- Permuted models show significantly higher perplexity than forward/backward models, with distinct attention patterns showing decreasing positional bias with distance
- Forward/backward models exhibit high representational alignment that degrades with depth, while permuted models show near-zero alignment with other variants at deeper layers
- Both forward and backward models match human expert performance on BrainBench but align poorly with human judgment patterns, contradicting prior claims about backward model inferiority

## Why This Works (Mechanism)

### Mechanism 1: Chain Rule Perplexity Invariance
The chain rule of probability ensures that for any well-defined distribution P(X₀, X₁, ..., Xₙ), any conditional factorization telescopes to identical joint probability when summed. Perplexity PP = exp(-1/n · ln P(joint)) depends only on this joint value, not the factorization path. This theoretical foundation requires that all factorizations compute the same joint probability and that the model accurately approximates all required conditionals.

### Mechanism 2: Positional Biases in Causal Self-Attention
Causal self-attention exhibits dual biases—toward adjacent tokens (locality) and toward sequence boundaries (attention sinks). Forward and backward orderings preserve natural local dependencies, yielding similar attention patterns (entropy, positional ranks). Arbitrary permutations disrupt these dependencies, producing higher entropy attention in early-to-middle layers and distinct positional bias profiles.

### Mechanism 3: Representational Divergence with Depth
Early layers capture order-invariant local statistics, producing similar representations across forward/backward models. Deeper layers integrate longer-range dependencies where ordering effects accumulate, causing representational divergence. Permuted models diverge more sharply as they cannot leverage natural sequential structure.

## Foundational Learning

- **Chain Rule of Probability**:
  - Why needed here: Understanding why perplexity should theoretically be invariant requires grasping how joint probabilities decompose into conditionals and how telescoping sums work.
  - Quick check question: Given P(A, B, C), write out the forward and backward factorizations and verify they equal the same joint probability.

- **Autoregressive Language Modeling Objective**:
  - Why needed here: The paper's experiments compare models trained with the same loss but different token orderings; understanding the objective clarifies what "consistency" means.
  - Quick check question: Why does next-token prediction loss require a BOS token for first-token probability estimation?

- **Self-Attention Positional Biases (Attention Sinks, Locality)**:
  - Why needed here: Interpreting the attention entropy and rank analyses requires knowing why transformers exhibit early-token and adjacent-token preferences.
  - Quick check question: In causal self-attention, which tokens can position i attend to, and how might this create positional biases?

## Architecture Onboarding

- **Component map**:
  Tokenizer (BPE, forward-trained only) -> GPT-2 (124M/355M/774M, 12-36 layers) -> Causal self-attention (masked) -> AdamW optimizer (lr=2e-5) -> Cross-entropy loss

- **Critical path**:
  1. Prepare data: Tokenize forward text, prepend BOS, ensure full context-window span (1,024 tokens), apply permutation (identity, reverse, or arbitrary fixed permutation) within context.
  2. Train sibling models: Same initialization, same data order, same tokenizer—differ only in within-context token arrangement.
  3. Evaluate: Compute perplexity on validation set (same sequences, same BOS), analyze attention patterns (entropy, normalized rank vs. distance), measure RSA between model representations, run downstream benchmark (BrainBench).

- **Design tradeoffs**:
  - Strict protocol adherence (BOS, shared tokenizer, full-window sequences) ensures theoretical validity but limits comparability with prior work using different setups.
  - Using only forward-trained tokenizer for all models maintains sequence equivalence but may disadvantage backward/permuted models if tokenization is suboptimal for their structure.
  - Fixed permutation vs. random per-sequence permutation: Fixed enables perplexity comparability; random would test robustness to variability.

- **Failure signatures**:
  - Missing BOS token: Forward and backward perplexities become incomparable (different joint probabilities computed).
  - Retrained tokenizer on reversed text: Vocabulary mismatch violates sequence equivalence; models process different token sequences.
  - Logical reversal instead of strict token reversal: Changes task semantics, not just ordering, breaking theoretical premises.
  - Permuted model evaluated on non-full-window BrainBench items: Permutation mapping invalid, evaluation undefined.

- **First 3 experiments**:
  1. Replicate forward vs. backward perplexity comparison with strict BOS and shared tokenizer on a small corpus; verify Pearson r > 0.99 with small systematic forward advantage.
  2. Extend to arbitrary permutation; confirm attention entropy divergence in early layers and higher perplexity vs. forward/backward.
  3. Analyze normalized attention rank vs. token distance across layers for all three orderings; verify dual bias (adjacent + max-distance) for forward/backward, decreasing bias for permuted.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do deviations from theoretically consistent probability distributions in LLMs causally contribute to hallucinations and unpredictable out-of-distribution behaviors?
- Basis in paper: [explicit] "We suspect that deviations from proper probability distributions may contribute to thorny problems like hallucination and unpredictable out-of-distribution behaviors in LLMs... While this connection requires further exploration..."
- Why unresolved: The paper establishes that empirical deviations from theoretical invariance exist but does not investigate whether these inconsistencies directly cause downstream failure modes like hallucinations.
- What evidence would resolve it: Correlational or causal studies linking measured probability inconsistencies (e.g., perplexity gaps between factorizations) to hallucination rates or OOD failure frequency in controlled settings.

### Open Question 2
- Question: Do modernized bidirectional models achieve closer theoretical perplexity equivalence across factorizations compared to unidirectional autoregressive LMs?
- Basis in paper: [explicit] "A promising future direction could compare modernized bidirectional models (e.g., ModernBERT; Warner et al. 2024) with unidirectional LMs to determine if bidirectional learning's theoretical advantage in capturing joint probabilities translates to downstream performance."
- Why unresolved: The paper only evaluates unidirectional GPT-2 models; bidirectional architectures were discussed but not empirically tested.
- What evidence would resolve it: Training comparable bidirectional and unidirectional models on identical data and measuring perplexity differences across forward, backward, and permuted factorizations.

### Open Question 3
- Question: Why does the forward-backward perplexity gap widen as model size increases?
- Basis in paper: [inferred] Table 2 shows Cohen's d increasing from 0.191 (124M) to 0.420 (355M) to 0.665 (774M), but the paper does not explain this scaling phenomenon.
- Why unresolved: The authors document this pattern but attribute discrepancies broadly to positional and locality biases without explaining why larger models amplify the asymmetry.
- What evidence would resolve it: Mechanistic analysis of how attention pattern differences between forward and backward models scale with model depth/width; ablation studies isolating architectural components.

### Open Question 4
- Question: Can architectural modifications to self-attention mitigate positional biases sufficiently to achieve closer theoretical probability equivalence?
- Basis in paper: [inferred] The paper traces discrepancies to "positional and locality biases in processing" and connects findings to attention sink and lost-in-the-middle phenomena, but does not test whether bias-reduction interventions improve consistency.
- Why unresolved: The diagnostic analysis identifies attention biases as the source of deviation but stops short of proposing or testing interventions.
- What evidence would resolve it: Training models with debiasing techniques (e.g., modified positional encodings, attention regularization) and measuring reduction in perplexity gaps across factorizations.

## Limitations

- The permutation experiments use a fixed permutation pattern, making it unclear whether results generalize to random per-sequence permutations
- The neuroscience corpus composition (2002-2022 publications) may limit generalization to other domains or time periods
- The shared BPE tokenizer trained only on forward text could introduce subtle asymmetries, particularly for the permuted model

## Confidence

- **High confidence**: The mathematical proof of perplexity invariance under the chain rule and the core finding that forward and backward models achieve similar perplexities with systematic forward advantages
- **Medium confidence**: The attention pattern analyses showing positional and locality biases, and the representational alignment degradation with depth
- **Medium confidence**: The BrainBench results showing both models match human performance but align poorly with human judgment patterns, though this contradicts some prior work and requires careful interpretation

## Next Checks

1. **Permutation Robustness Test**: Repeat the permutation experiments with random per-sequence permutations (rather than fixed) to verify whether the higher perplexity and distinct attention patterns persist, distinguishing between effects of permutation versus specific permutation pattern.

2. **Cross-Domain Replication**: Apply the same forward/backward/permuted training protocol to multiple domains (e.g., general web text, scientific literature from other fields, code) to assess whether the observed patterns generalize beyond neuroscience.

3. **Alternative Tokenizer Evaluation**: Train and evaluate models with tokenizers optimized for each ordering direction (forward-only, backward-only, permutation-agnostic) to isolate the contribution of tokenization asymmetries to the observed perplexities and attention patterns.