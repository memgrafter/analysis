---
ver: rpa2
title: 'From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with
  International Humanitarian Law'
arxiv_id: '2506.06391'
source_url: https://arxiv.org/abs/2506.06391
tags:
- refusal
- these
- safety
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated eight leading LLMs on their ability to refuse
  prompts that violate International Humanitarian Law (IHL). Most models refused unlawful
  requests at high rates, but the clarity and consistency of explanations varied significantly.
---

# From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law

## Quick Facts
- arXiv ID: 2506.06391
- Source URL: https://arxiv.org/abs/2506.06391
- Reference count: 40
- Primary result: Safety prompts can significantly improve the quality of refusal explanations in LLMs when handling IHL-related prompts.

## Executive Summary
This study evaluates eight leading LLMs on their ability to refuse prompts that violate International Humanitarian Law (IHL). While most models refused unlawful requests at high rates, the clarity and consistency of explanations varied significantly. A simple system-level safety prompt dramatically improved the quality of refusal explanations in most models, with some seeing increases above 90%. However, more complex prompts involving technical language or code still revealed vulnerabilities. These findings highlight the potential for lightweight interventions to improve AI safety, while also underscoring the need for continued refinement of safeguards to ensure reliable compliance with legal and ethical standards.

## Method Summary
The study evaluated eight leading LLMs on their ability to refuse prompts that violate International Humanitarian Law (IHL). Researchers tested models using a variety of IHL-related prompts and measured their refusal rates and the quality of explanations provided. A simple system-level safety prompt was introduced to assess its impact on improving refusal explanation quality. The study focused on measuring clarity and consistency of refusals, with particular attention to the effectiveness of the safety prompt intervention.

## Key Results
- Most models refused unlawful IHL-related requests at high rates.
- A simple system-level safety prompt improved refusal explanation quality in most models, with some seeing increases above 90%.
- More complex prompts involving technical language or code still revealed vulnerabilities in model refusals.

## Why This Works (Mechanism)
The study demonstrates that explicit refusal mechanisms in LLMs can be enhanced through targeted safety prompts. By providing clear guidelines at the system level, models are better equipped to recognize and articulate refusals to unlawful requests. The mechanism relies on the model's ability to process and apply safety instructions during inference, improving the quality and consistency of refusals. However, the effectiveness of this mechanism varies depending on the complexity of the prompt and the model's underlying architecture.

## Foundational Learning
- **International Humanitarian Law (IHL)**: Framework of rules governing armed conflict; needed to define the legal boundaries for refusals. Quick check: Ensure prompt scenarios accurately reflect IHL principles.
- **System-level safety prompts**: Instructions provided to models at the system level; needed to guide behavior during inference. Quick check: Verify prompt clarity and relevance to IHL contexts.
- **Refusal quality metrics**: Measures of clarity and consistency in refusals; needed to evaluate the effectiveness of safety interventions. Quick check: Validate metrics against human judgment.

## Architecture Onboarding
- **Component map**: User Prompt -> LLM Core -> Safety Prompt Integration -> Refusal Output
- **Critical path**: User prompt is processed by the LLM, safety prompt is integrated, and the model generates a refusal if the request violates IHL.
- **Design tradeoffs**: Balancing refusal accuracy with prompt complexity; simpler prompts may be less effective for nuanced scenarios.
- **Failure signatures**: Inconsistent refusal explanations, inability to handle technical language, and vulnerability to adversarial prompts.
- **First experiments**:
  1. Test safety prompt effectiveness across diverse safety domains (e.g., cybersecurity, misinformation).
  2. Conduct adversarial testing with systematically varied prompt structures.
  3. Perform longitudinal studies to evaluate the stability of refusal behaviors.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Narrow focus on IHL-related prompts may not generalize to other safety domains.
- Limited set of eight LLMs may miss variations in smaller or emerging models.
- Safety prompt intervention was not rigorously defined or tested across diverse prompt types.

## Confidence
- **High confidence**: LLMs generally refuse unlawful IHL-related requests at high rates.
- **Medium confidence**: Safety prompts improve refusal explanation quality, but the extent and consistency of this improvement vary across models.
- **Low confidence**: The effectiveness of safety prompts for complex prompts involving technical language or code, as the study only hints at vulnerabilities without detailed analysis.

## Next Checks
1. Test the safety prompt intervention across a broader range of safety domains (e.g., cybersecurity, misinformation) to assess generalizability.
2. Conduct adversarial testing with systematically varied prompt structures to identify and address remaining vulnerabilities in refusal behaviors.
3. Perform longitudinal studies to evaluate the stability of refusal behaviors and explanation quality after repeated safety prompt interventions.