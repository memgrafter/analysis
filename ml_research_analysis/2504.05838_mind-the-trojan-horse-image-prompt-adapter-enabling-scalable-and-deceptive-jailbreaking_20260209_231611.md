---
ver: rpa2
title: 'Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive
  Jailbreaking'
arxiv_id: '2504.05838'
source_url: https://arxiv.org/abs/2504.05838
tags:
- image
- nsfw
- clip
- benign
- nudity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the hijacking attack, a novel jailbreaking
  method enabled by the Image Prompt Adapter (IP-Adapter) in text-to-image diffusion
  models. The attack allows adversaries to upload imperceptible adversarial examples
  (AEs) to the web, which are then unknowingly downloaded and used by benign users
  to trigger NSFW outputs from image generation services, causing reputational harm
  to service providers.
---

# Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking

## Quick Facts
- arXiv ID: 2504.05838
- Source URL: https://arxiv.org/abs/2504.05838
- Authors: Junxi Chen; Junhao Dong; Xiaohua Xie
- Reference count: 40
- Introduces hijacking attack via IP-Adapter, achieving over 77% NSFW rate and 57% nudity rate on twelve T2I-IP-DMs

## Executive Summary
This paper introduces a novel jailbreaking attack targeting text-to-image diffusion models through the Image Prompt Adapter (IP-Adapter) framework. The attack, termed "hijacking," enables adversaries to upload imperceptible adversarial examples to the web that benign users unknowingly download and use to trigger NSFW outputs from image generation services. The proposed Attack Encoder Only (AEO) method aligns adversarial examples with NSFW prompts in the image encoder's feature space, demonstrating significant success rates across multiple models and tasks. The work reveals critical security vulnerabilities in IP-Adapter-based systems and highlights the ineffectiveness of existing defenses against this novel threat vector.

## Method Summary
The hijacking attack exploits the IP-Adapter architecture in text-to-image diffusion models by crafting imperceptible adversarial examples that, when used as prompts, trigger NSFW image generation. The core method, Attack Encoder Only (AEO), optimizes adversarial examples against the image encoder's feature space while aligning them with NSFW prompts. This approach specifically targets the IP-Adapter's vulnerability by manipulating the input space that bridges text and image generation. The attack achieves high success rates by focusing optimization on the encoder component while maintaining imperceptibility through careful perturbation control.

## Key Results
- AEO achieves over 77% NSFW rate and 57% nudity rate across twelve T2I-IP-DMs in extensive experiments
- Existing defenses prove ineffective against the hijacking attack, with diffusion-based purification methods showing limited success
- Adversarial training on the image encoder provides partial mitigation but requires further refinement
- The attack demonstrates practical feasibility with imperceptible adversarial examples maintaining effectiveness across different tasks and models

## Why This Works (Mechanism)
The hijacking attack exploits the architectural design of IP-Adapters in text-to-image diffusion models, where the image encoder processes both benign images and adversarial examples without distinguishing their intent. By crafting perturbations that align with NSFW prompts in the encoder's feature space, the attack bypasses the model's safety mechanisms while remaining imperceptible to human users. The IP-Adapter's role as a bridge between text and image generation creates a vulnerability that can be systematically exploited through targeted optimization of the encoder component.

## Foundational Learning
- **IP-Adapter Architecture**: The adapter component in T2I models that processes image prompts and bridges text-to-image generation. Why needed: Understanding this component is crucial as it represents the attack surface being exploited. Quick check: Verify the IP-Adapter's position in the overall model architecture and its interaction with both text and image inputs.
- **Adversarial Example Optimization**: Techniques for crafting imperceptible perturbations that manipulate model outputs. Why needed: Forms the core methodology for creating effective hijacking attacks. Quick check: Confirm the optimization objective aligns adversarial features with target NSFW prompts.
- **Feature Space Alignment**: The process of matching adversarial perturbations with specific semantic targets in the model's internal representation. Why needed: Enables precise control over generated content while maintaining imperceptibility. Quick check: Validate that aligned features consistently trigger desired NSFW outputs.
- **Diffusion Model Purification**: Defense mechanisms that attempt to remove adversarial influences during the denoising process. Why needed: Understanding why existing defenses fail helps identify more effective mitigation strategies. Quick check: Test purification effectiveness against varying levels of adversarial strength.
- **Transferability of Adversarial Examples**: The ability of crafted AEs to maintain effectiveness across different model architectures. Why needed: Determines the practical scalability of the attack across various IP-Adapter implementations. Quick check: Measure success rates when transferring AEs between different T2I-IP-DMs.

## Architecture Onboarding

Component Map:
Text Prompt -> IP-Adapter (Image Encoder) -> Diffusion Process -> Generated Image

Critical Path:
The attack targets the image encoder within the IP-Adapter, optimizing adversarial examples to align with NSFW prompts before they enter the diffusion process. This represents the most vulnerable point where imperceptible manipulations can influence the final output.

Design Tradeoffs:
- Imperceptibility vs. Attack Success Rate: More aggressive perturbations increase effectiveness but risk detection
- Computational Cost vs. Defense Strength: More robust defenses require greater computational resources
- Transferability vs. Specificity: Universal attacks sacrifice some effectiveness for broader applicability

Failure Signatures:
- Reduced NSFW generation rates when adversarial examples undergo compression or transmission loss
- Decreased attack success when models employ robust adversarial training
- Limited effectiveness against models with strong input sanitization or feature-space purification

First Experiments:
1. Test AEO effectiveness on a single T2I-IP-DM with varying perturbation magnitudes to establish the imperceptibility-effectiveness tradeoff
2. Evaluate existing defense mechanisms (DBP, input sanitization) against crafted adversarial examples
3. Measure transferability of AEs across different IP-Adapter architectures to assess attack scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the robustness of Diffusion-based Purification (DBP) defenses be accurately evaluated against hijacking attacks given the interference of stochastic gradients?
- Basis in paper: Appendix K.2 notes that DBP induces stochastic gradients which may lead to overestimated security, and accurate evaluation remains an open problem.
- Why unresolved: Standard evaluation techniques (like EOT=1) fail to account for gradient stochasticity, requiring methods like Expectation Over Transformation (EOT) which are computationally intensive or imperfect.
- What evidence would resolve it: Development of a standardized evaluation protocol for stochastic defenses that robustly quantifies security without gradient obfuscation.

### Open Question 2
- Question: Can Adversarial Examples (AEs) crafted by AEO maintain their efficacy against T2I-IP-DMs when subjected to real-world lossy compression (e.g., JPEG) or aggressive network transmission constraints?
- Basis in paper: Appendix K.1 highlights that the paper assumes a lossless PNG network channel C(x), which may not hold for "stingy service providers" using aggressive compression.
- Why unresolved: The paper focused on the core vulnerability and gradient obfuscation avoidance, leaving the resilience of AEs against non-differentiable compression transforms unverified.
- What evidence would resolve it: Experiments showing the Attack Success Rate (ASR) of AEO-crafted images after they undergo JPEG compression or other standard web transmission compression algorithms.

### Open Question 3
- Question: To what extent can improved transfer-based adversarial attacks utilizing adversarially trained surrogate encoders (like FARE) reliably bypass closed-source IP-Adapters?
- Basis in paper: Appendix I shows that while standard transferability is low, using a FARE-tuned surrogate improves transferability; the authors hypothesize "future improved transfer-based adversarial attacks" will close the gap.
- Why unresolved: The current performance gap between white-box attacks and transferred attacks on closed-source encoders (like ViT-G) suggests black-box robustness is currently high but brittle.
- What evidence would resolve it: A study demonstrating that AEs crafted on adversarially robust open-source surrogates can successfully trigger NSFW outputs on models with proprietary, closed-source image encoders.

## Limitations
- Assumes lossless PNG network transmission, which may not hold in real-world scenarios with aggressive compression
- Limited evaluation of user detection capabilities for imperceptible adversarial examples in practical settings
- Does not quantify the actual reputational impact on service providers or measure real-world attack deployment feasibility
- Focuses on controlled experimental conditions that may not fully capture the complexity of adversarial scenarios

## Confidence
- High: Technical claims regarding AEO effectiveness and existing defense failures are supported by systematic experiments across multiple models and tasks
- Medium: Broader security implications and practical deployment feasibility depend on factors beyond controlled experimental setup
- Low: Precise quantification of reputational harm and real-world attack success rates would require extensive field studies not conducted in this work

## Next Checks
1. Conduct user studies to measure actual detection rates of imperceptible adversarial examples in realistic web browsing scenarios
2. Test the attack's effectiveness against recently proposed diffusion model defenses beyond the ones evaluated
3. Evaluate the transferability of adversarial examples across different IP-Adapter architectures and model families to assess attack robustness