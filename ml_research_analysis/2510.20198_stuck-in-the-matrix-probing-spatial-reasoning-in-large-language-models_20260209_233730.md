---
ver: rpa2
title: 'Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models'
arxiv_id: '2510.20198'
source_url: https://arxiv.org/abs/2510.20198
tags:
- grid
- reasoning
- accuracy
- figure
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks spatial reasoning in large language models
  using five grid-based tasks (quadrant identification, geometric transformation,
  distance evaluation, word search, and tile sliding) scaled across varying grid sizes.
  Models tested include GPT-4o, GPT-4.1, and Claude 3.7 variants.
---

# Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2510.20198
- Source URL: https://arxiv.org/abs/2510.20198
- Reference count: 17
- Large language models show sharp accuracy degradation in spatial reasoning as grid complexity increases

## Executive Summary
This study benchmarks spatial reasoning capabilities in large language models using five grid-based tasks scaled across varying grid sizes. The tasks include quadrant identification, geometric transformation, distance evaluation, word search, and tile sliding. Models tested include GPT-4o, GPT-4.1, and Claude 3.7 variants. While accuracy remained high on small grids, all models showed rapid degradation as complexity increased, with average accuracy loss of 42.7% and up to 84% in some cases. Every task starting above 50% accuracy lost at least 48%. Errors primarily stemmed from miscounting, misreading initial board states, and difficulty identifying centerlines or performing multi-step reasoning.

Claude models consistently outperformed OpenAI models overall, though all models demonstrated significant limitations in spatial reasoning despite strong linguistic abilities. The findings suggest that LLMs have architectural limitations in spatial reasoning that cannot be overcome through prompting alone, highlighting the need for task-specific training or architectural modifications to improve performance on spatial tasks.

## Method Summary
The study tested five grid-based spatial reasoning tasks on large language models across three grid sizes: 3x3, 9x9, and 27x27. Tasks included identifying quadrants, performing geometric transformations, evaluating distances, searching for words, and solving sliding tile puzzles. Models tested included GPT-4o, GPT-4.1, Claude 3.7 Sonnet, and Claude 3.7 Haiku. Performance was measured through accuracy metrics, with qualitative error analysis conducted on incorrect responses. The study focused on text-based ASCII grid representations and did not explore visual or multimodal inputs.

## Key Results
- Average accuracy loss of 42.7% across all models and tasks as grid size increased
- Maximum accuracy loss reached 84% for some tasks on the largest grids
- Every task starting above 50% accuracy lost at least 48% accuracy
- Claude models consistently outperformed OpenAI models by approximately 30% across all tasks
- Error patterns included miscounting, misreading board states, and difficulty with multi-step reasoning

## Why This Works (Mechanism)
The study reveals that large language models struggle with spatial reasoning tasks due to fundamental architectural limitations in processing structured, grid-based information. The degradation in performance as grid complexity increases suggests that LLMs lack the inherent spatial processing capabilities that would allow them to maintain accuracy across different scales of spatial problems.

## Foundational Learning
- **Grid-based spatial reasoning**: Understanding how spatial relationships are represented in discrete matrices; needed to interpret test tasks and results; quick check: can you describe the quadrant identification task in your own words?
- **Spatial scaling effects**: How performance degrades as problem complexity increases; needed to understand the core finding; quick check: what was the average accuracy loss across all models?
- **Tokenization impact on spatial tasks**: How text encoding affects spatial reasoning performance; needed to understand why different models perform differently; quick check: why couldn't the study analyze Anthropic's tokenization patterns?
- **Error pattern analysis**: Identifying systematic mistakes in model responses; needed to interpret the qualitative findings; quick check: what were the three main error types identified?
- **Modality effects on reasoning**: How input representation format affects performance; needed to understand limitations of text-only testing; quick check: what alternative input modality could potentially improve performance?

## Architecture Onboarding
- **Component Map**: Input Text -> Tokenization -> Context Window Processing -> Attention Mechanism -> Output Generation
- **Critical Path**: Tokenization and context window management appear to be the most critical components affecting spatial reasoning performance, as errors often involve miscounting and misreading board states
- **Design Tradeoffs**: The study highlights the tradeoff between linguistic reasoning capabilities and spatial reasoning abilities, suggesting architectural modifications may be needed to improve spatial performance
- **Failure Signatures**: Consistent patterns of miscounting, misreading initial board states, and difficulty with multi-step reasoning indicate systematic architectural limitations rather than random errors
- **First Experiments**: 1) Test whether explicit counting mechanisms improve performance on larger grids, 2) Compare performance on isomorphic problems presented in text vs. visual modalities, 3) Conduct ablation studies removing various spatial reasoning components to identify which specific sub-skills drive performance drops

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Would presenting spatial tasks as images rather than ASCII grids significantly improve model performance, and if so, by what magnitude?
- Basis in paper: [explicit] "For instance, the models tested are all multimodal, and would likely perform better on similar tests presented in image form; future work could focus on directly testing these various modes of input."
- Why unresolved: The study only tested text-based grid inputs; no visual modality comparison was conducted.
- What evidence would resolve it: Run the same five tasks using image-based grid representations on GPT-4o and Claude 3.7, comparing accuracy across grid sizes.

### Open Question 2
- Question: Can alternative output modalities such as code generation or full board reconstruction improve spatial reasoning accuracy?
- Basis in paper: [explicit] "Finally, research could focus on other methods of output, such as code generation or full boards."
- Why unresolved: All current tests required coordinate or text outputs; no code-based or board-reconstruction outputs were evaluated.
- What evidence would resolve it: Compare accuracy when models generate Python code to solve spatial tasks versus direct coordinate/text responses.

### Open Question 3
- Question: What specific architectural or training differences explain the consistent 30% accuracy gap between Anthropic and OpenAI models on spatial tasks?
- Basis in paper: [inferred] The paper observes consistent performance gaps but does not investigate root causes beyond noting "different architectures and training."
- Why unresolved: Claude's tokenizer is private; the study could not analyze tokenization patterns for Anthropic models to identify sources of advantage.
- What evidence would resolve it: Controlled experiments isolating tokenization effects, training data composition, and architectural attention mechanisms across model families.

### Open Question 4
- Question: Would fine-tuning LLMs on structured spatial data (e.g., ASCII grids, tables) mitigate the accuracy degradation observed at scale?
- Basis in paper: [explicit] "Further work could also focus on improving the counting and mathematical capabilities of LLMs, as these two skills were the root of many of the inaccurate responses."
- Why unresolved: All tested models were evaluated without task-specific training; no intervention was attempted beyond prompting changes.
- What evidence would resolve it: Fine-tune models on synthetic grid-based tasks and measure accuracy retention across grid sizes compared to baseline models.

## Limitations
- The study exclusively used text-based ASCII representations of spatial problems, potentially missing modality-specific effects
- Only a limited set of discrete grid-based spatial reasoning types were tested, not capturing the full spectrum of spatial cognition
- Error analysis relied on qualitative categorization that could benefit from more systematic validation

## Confidence
- High confidence in the core finding that spatial reasoning accuracy degrades with grid size
- Medium confidence in the interpretation that this degradation stems from architectural limitations rather than training data gaps
- Medium confidence in claims about Claude models "outperforming" OpenAI models due to potential confounding factors

## Next Checks
1. Test whether providing explicit counting mechanisms or step-by-step templates improves performance on larger grids, distinguishing between architectural vs. prompt engineering limitations
2. Compare performance on isomorphic problems presented in different modalities (text vs. visual) to isolate modality-specific effects
3. Conduct ablation studies removing various spatial reasoning components to identify which specific sub-skills drive the performance drop