---
ver: rpa2
title: 'M-IFEval: Multilingual Instruction-Following Evaluation'
arxiv_id: '2502.04688'
source_url: https://arxiv.org/abs/2502.04688
tags:
- language
- instruction
- instructions
- llms
- detectable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Instruction Following Evaluation (IFEval) benchmark only evaluates
  LLMs in English, limiting assessment of multilingual models. We propose the Multilingual
  Instruction Following Evaluation (M-IFEval) benchmark, expanding evaluation to French,
  Japanese, and Spanish with both general and language-specific instructions.
---

# M-IFEval: Multilingual Instruction-Following Evaluation

## Quick Facts
- arXiv ID: 2502.04688
- Source URL: https://arxiv.org/abs/2502.04688
- Authors: Antoine Dussolle; Andrea Cardeña Díaz; Shota Sato; Peter Devine
- Reference count: 26
- Primary result: Multilingual LLMs achieve near-zero scores on character frequency tasks (0.0% for Spanish ñ frequency) despite high English performance

## Executive Summary
The M-IFEval benchmark addresses the limitation of English-only instruction-following evaluation by introducing multilingual assessment for French, Japanese, and Spanish. The benchmark includes 541 prompts with both general and language-specific instructions, revealing that model rankings do not transfer across languages and that character-level constraints expose fundamental architectural limitations. GPT4o performs best on English IFEval, while o1 and Sonnet achieve higher scores on M-IFEval across the three languages, with performance gaps widening to 13.9-15.1 percentage points compared to 11.3 points in English.

## Method Summary
M-IFEval extends the IFEval framework to French, Japanese, and Spanish with 541 total prompts (115 Spanish, 172 Japanese, 235 French) containing 1-3 instructions each. The benchmark evaluates 8 SOTA LLMs using custom verification functions for language-specific constraints like katakana avoidance, ñ frequency, and accent rules. Responses are generated using greedy decoding (temperature=0) and evaluated using strict/loose scoring metrics. The methodology includes both translated general instructions from IFEval and novel language-specific constraints tied to linguistic features of each target language.

## Key Results
- GPT4o and Sonnet top English M-IFEval, but o1 and Opus rank highest across French, Japanese, and Spanish
- Performance gap between top and bottom models is wider in multilingual benchmark (13.9-15.1 percentage points) vs English (11.3 points)
- LLMs achieve particularly low scores on language-specific tasks: 14.3% for Japanese katakana avoidance and 0.0% for Spanish ñ frequency control
- Top 10 lowest-scoring instructions are all language-specific constraints involving script restrictions or special character frequency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific instructions reveal competence gaps that translated benchmarks miss
- Mechanism: Constraints tied to linguistic features (e.g., Spanish ñ frequency, Japanese katakana avoidance) test whether models have language-internal control or merely surface-level translation ability
- Core assumption: Models performing well on translated general instructions should handle language-specific constraints if they have genuine multilingual competence
- Evidence anchors: [abstract] LLMs achieve particularly low scores on language-specific tasks involving script restrictions and special character frequency control; [Section 4] the 10 instructions with the lowest scores were all language-specific instructions; [corpus] Related work confirms growing recognition that multilingual evaluation requires constraint analysis beyond translation

### Mechanism 2
- Claim: Token-level language models lack character-level controllability in non-English scripts
- Mechanism: Standard tokenizers create tokens from multi-character sequences, making precise character counting or exclusion difficult—models operate on tokens, not characters, breaking the mapping between instruction and output capability
- Core assumption: Near-zero scores on character frequency tasks reflect architectural limitations rather than insufficient training
- Evidence anchors: [Section 5] Experiments using a byte-level tokenizer could possibly answer why script or character based instructions are so hard to follow for modern token-level LLMs; [Section 4] average scores for forbidden œ/ç, forbidding katakana, and ñ frequency were 60.2%, 14.3%, and 0.0%, respectively; [corpus] Evidence weak—no direct corpus validation of tokenizer hypothesis

### Mechanism 3
- Claim: Relative model rankings do not transfer across languages
- Mechanism: Different models have different language data mixtures, creating language-specific competence profiles rather than uniform multilingual ability—best-in-English ≠ best-in-target-language
- Core assumption: Training data composition drives cross-lingual generalization more than model architecture
- Evidence anchors: [Section 4] while GPT4o and Sonnet are top two models for English M-IFEval, o1 and Opus have highest score on average for three languages; [Section 5] o1 may have been trained on more Spanish and French data, while Sonnet may have been trained on more Japanese data; [corpus] Assumption supported by general multilingual learning literature

## Foundational Learning

- Concept: **IFEval objective evaluation methodology**
  - Why needed here: M-IFEval extends this framework—understanding how string-checking functions verify instruction compliance without LLM-as-judge is prerequisite to adding language-specific verifiers
  - Quick check question: Can you explain why "use exactly 5 ñ characters" can be verified programmatically while "write a polite email" cannot?

- Concept: **Tokenizer vocabulary and character representation**
  - Why needed here: The paper's proposed explanation for script/character failures hinges on how tokenizers chunk text—understanding byte-pair encoding vs. character-level tokenization is essential
  - Quick check question: Why would a tokenizer that represents "ñ" as part of a multi-character token make character-counting instructions harder to follow?

- Concept: **Cross-lingual transfer and training data mixtures**
  - Why needed here: The paper speculates that training data composition explains why o1 excels in Spanish/French while Sonnet excels in Japanese—evaluating this claim requires understanding transfer mechanics
  - Quick check question: What evidence would distinguish between "more target-language training data" vs. "more linguistically-similar language data" as the driver of cross-lingual performance?

## Architecture Onboarding

- Component map: **Prompt dataset** (541 prompts: 115 Spanish, 172 Japanese, 235 French) with 1-3 instructions each → **Instruction types**: General (from IFEval) + language-specific (e.g., katakana avoidance, ñ frequency, accent constraints) → **Verification functions**: Language-specific string-checking code (e.g., count_ñ(), contains_katakana(), has_correct_accents()) → **Evaluation harness**: Modified IFEval codebase supporting multilingual character encoding → **Model inference layer**: Greedy decoding (temperature=0) for reproducibility

- Critical path: 1. Identify language-specific constraint → 2. Write verification function → 3. Generate prompts with constraint → 4. Collect model outputs → 5. Run verification → 6. Compute strict/loose scores
  - Failure at step 2 (incorrect verification logic) produces false positives/negatives
  - Failure at step 3 (ambiguous prompts) conflates instruction clarity with model capability

- Design tradeoffs:
  - **Strict vs. loose scoring**: Strict requires all instructions followed; loose gives partial credit—paper reports strict in main text, loose in appendix
  - **Language selection**: French/Spanish/Japanese chosen for team expertise; limits generalizability to low-resource languages
  - **Objective-only evaluation**: Excludes subjective tasks (translation quality, fact-checking) for reproducibility; narrows instruction-following definition

- Failure signatures:
  - **Near-zero scores on character frequency tasks** (0.0% for ñ frequency): Suggests tokenizer-level limitation, not prompt understanding
  - **Script mixing despite prohibition** (katakana appearing in "no katakana" responses): Model defaults to canonical orthography regardless of constraint
  - **Cross-model ranking inversions** (GPT4o best in English, worse in multilingual): Indicates training data effects, not architecture differences

- First 3 experiments:
  1. **Tokenizer ablation**: Test a byte-level tokenizer model (e.g., ByT5) on the same character-frequency instructions to validate the proposed mechanism
  2. **Error pattern analysis**: Manually inspect failures—are models misunderstanding instructions or unable to execute understood constraints? (paper provides failure examples in Appendix C)
  3. **Language expansion pilot**: Add one low-resource language with script-specific constraints (e.g., Hindi with Devanagari) to test whether the English-to-evaluation-language gap widens as predicted

## Open Questions the Paper Calls Out

- Question: Does utilizing a byte-level tokenizer (e.g., ByT5) improve LLM performance on script and character-level constraints compared to standard token-level models?
  - Basis in paper: [explicit] The authors ask, "why script or character based instructions are so hard to follow for modern token-level LLMs" and suggest experiments using byte-level tokenizers could answer this
  - Why unresolved: Current models struggle with specific character instructions (e.g., 0.0% for Spanish ñ frequency), but the specific architectural cause (tokenization vs. training data) remains unidentified
  - What evidence would resolve it: Comparative evaluation of identical model architectures trained with byte-level vs. standard subword tokenization on the M-IFEval benchmark

- Question: How does the specific mixture of multilingual pre-training and fine-tuning data affect instruction-following performance variance across languages?
  - Basis in paper: [explicit] The authors state that "Future work could consider exactly why the performance of LLMs varies for different languages" and suggest data mixture experiments
  - Why unresolved: While the paper observes that different models excel in different languages (e.g., Sonnet vs. o1 in Japanese), the root cause regarding training data composition is hypothesized but not tested
  - What evidence would resolve it: Controlled ablation studies training models with varying language data ratios and measuring the impact on M-IFEval scores per language

- Question: Is the performance gap in instruction following between English and non-English languages significantly wider for low-resource languages?
  - Basis in paper: [explicit] The authors note that "we may observe an even greater gap for low resource languages" and identify adding such languages as a necessary extension of the work
  - Why unresolved: The current benchmark is restricted to high-resource languages (French, Japanese, Spanish), leaving the difficulty of low-resource constraints unknown
  - What evidence would resolve it: Creation of M-IFEval prompts for low-resource languages (e.g., Hausa) and comparison of model score deltas against English and the current high-resource set

## Limitations

- Language selection bias: Benchmark focuses on French, Japanese, and Spanish based on team expertise, limiting generalizability to low-resource languages and non-European scripts
- Prompt transparency: Actual prompt texts not included in paper, must be retrieved from GitHub repository, creating barriers for independent verification
- Model selection and hardware constraints: Evaluation relies on API access for proprietary models and specific hardware requirements (40GB AGPU for Qwen model), limiting reproducibility

## Confidence

**High confidence**: The observation that top-performing English models (GPT4o, Sonnet) do not maintain their rankings in multilingual settings is well-supported by experimental results showing o1 and Opus achieving higher scores on M-IFEval

**Medium confidence**: The claim that tokenizer-level limitations explain near-zero performance on character frequency tasks is plausible but not directly validated—the paper proposes byte-level tokenizer experiments as future work

**Medium confidence**: The speculation that training data composition explains cross-lingual performance differences (e.g., o1 excelling in Spanish/French while Sonnet excels in Japanese) is supported by observed rankings but lacks direct evidence linking specific training data characteristics to performance outcomes

## Next Checks

1. **Tokenizer ablation study**: Implement and evaluate byte-level tokenizer experiments using a model like ByT5 on the same character-frequency instructions to determine whether tokenizer architecture explains the near-zero scores on Spanish ñ frequency and Japanese katakana avoidance tasks

2. **Cross-linguistic generalization test**: Expand the benchmark to include one low-resource language with distinct script characteristics (e.g., Hindi with Devanagari) to test whether performance gaps observed in French/Spanish/Japanese extend to languages with different orthographic systems

3. **Error pattern analysis**: Conduct systematic manual inspection of model failures on language-specific instructions, categorizing errors into "instruction misunderstanding" versus "constraint execution inability" to determine whether models comprehend but cannot implement constraints, or fail to understand them altogether