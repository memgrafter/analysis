---
ver: rpa2
title: Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts
  Fine-tuning
arxiv_id: '2505.11922'
source_url: https://arxiv.org/abs/2505.11922
tags:
- miso
- instruction
- input
- attention
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of LLMs failing to consistently
  follow complex instructions with multiple constraints. The authors propose MISO
  (Multi-Input Single-Output), an extension to decoder-only transformer-based LLMs
  that transforms sequentially structured input instructions into multiple parallel
  or successive instructions containing subcontexts.
---

# Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning

## Quick Facts
- arXiv ID: 2505.11922
- Source URL: https://arxiv.org/abs/2505.11922
- Authors: Yuheng Lu; ZiMeng Bai; Caixia Yuan; Huixing Jiang; Xiaojie Wang
- Reference count: 17
- Key outcome: MISO-para achieves 52.1% average accuracy on IFEval vs. 48.1% for standard SFT

## Executive Summary
This paper addresses the problem of LLMs failing to consistently follow complex instructions with multiple constraints. The authors propose MISO (Multi-Input Single-Output), an extension to decoder-only transformer-based LLMs that transforms sequentially structured input instructions into multiple parallel or successive instructions containing subcontexts. MISO processes each input sequence independently and fuses their representations in the causal-attention layer during output generation using a weighted sum approach. This avoids the broken generation problem observed in previous multi-input approaches. Empirical results demonstrate MISO's superiority in complex instruction-following scenarios, with MISO-para achieving 52.1% average accuracy on IFEval compared to 48.1% for standard SFT, and showing potential training efficiency gains through reduced computational complexity when processing long input sequences.

## Method Summary
MISO extends decoder-only transformers to Multi-Input-Single-Output architecture by decomposing multi-constraint instructions into sub-contexts. The method processes each sub-context independently with appropriate position IDs (parallel or successive), then fuses their representations during output generation using a weighted sum of attention outputs. Two variants exist: MISO-para (parallel inputs with position IDs restarting at 0 for each sequence, including full instruction + single-constraint sub-instructions) and MISO-succ (successive position IDs across chunked inputs). The key innovation is the weighted attention fusion formula that prevents attention dilution on the output part during generation, which occurs in naive multi-input extensions of decoder-only transformers.

## Key Results
- MISO-para achieves 52.1% average accuracy on IFEval compared to 48.1% for standard SFT
- MISO shows potential computational efficiency gains with O(1/k * n²) reduction in input-related computation for long sequences
- MISO-para outperforms MISO-succ and other variants on constraint-following tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MISO's "weighted sum" attention formula prevents attention dilution on the output part during generation, which occurs in naive multi-input extensions of decoder-only transformers.
- Mechanism: Standard decoder-only transformers with vanilla attention dilute the attention allocated to the output part as the number of input sequences grows (A(out) converges to 0), causing "broken generation." MISO fixes this by computing a weighted sum of independent causal attentions, ensuring each input-output pair is treated effectively.
- Core assumption: The quality of generation is linked to the attention weight allocated to the output part; broken generation in multi-input settings is caused by this dilution.
- Evidence anchors:
  - [abstract] "MISO introduces a mixture-of-contexts paradigm that jointly considers the overall instruction-output alignment and the influence of individual sub-contexts..."
  - [section 4.3] "...applying vanilla attention in the multi-input-single-output extension of a decoder-only transformer leads to the risk of broken generation due to the diminishing attention weights on the output part."
  - [corpus] Corpus evidence is weak or missing for this specific attention mechanism fix.
- Break condition: If future work shows that broken generation in multi-input decoder-only models is not primarily caused by attention dilution on the output part, or if the weighted sum approach introduces other severe instability issues.

### Mechanism 2
- Claim: Training with a mixture of full instructions and single-constraint sub-instructions in a multi-input format improves constraint adherence by forcing the model to attend to individual constraints without losing the global context.
- Mechanism: In standard SFT on multi-constraint data, models may "neglect" certain sub-contexts (constraints). MISO explicitly presents these sub-contexts as separate input sequences during training, highlighting their individual influence on the output while the full instruction maintains overall alignment.
- Core assumption: The performance degradation in complex instruction following is causally linked to insufficient attention allocated to specific sub-contexts during training.
- Evidence anchors:
  - [abstract] "...insufficient attention allocated to crucial sub-contexts may reduce the effectiveness of SFT."
  - [section 1] "...LLMs frequently exhibit neglect of some sub-context... correlating with measurable performance degradation."
  - [corpus] The corpus neighbor 'ENTP' focuses on enhancing low-quality SFT data, supporting the idea that data structure is crucial, though not this specific attention mechanism.
- Break condition: If analysis shows MISO's gains are solely due to data augmentation (adding more single-constraint examples) and not the mixture-of-contexts architecture itself.

### Mechanism 3
- Claim: Chunking long inputs (MISO-succ) can improve learning of sub-contexts in general complex instructions and offers computational efficiency gains for long sequences.
- Mechanism: By splitting a long instruction into successive blocks, the model can focus on local sub-contexts. This also reduces the attention layer's time complexity from O(n²_i) to O(1/k * n²_i) for the input part.
- Core assumption: General complex instructions contain "sub-contexts" or distinct informational blocks that benefit from isolated processing, similar to explicit constraints.
- Evidence anchors:
  - [section 4.2] "...splitting the full instruction into successive blocks, each treated as a standalone input sequence..."
  - [section 5.2.2] "This reduction in input-related computation becomes advantageous when processing long input sequences."
  - [corpus] Weak corpus support for this specific efficiency claim in instruction tuning.
- Break condition: If computational overhead of managing multiple sequences outweighs the attention reduction, or if chunking disrupts critical cross-block dependencies.

## Foundational Learning

- Concept: QKV Attention & The Dilution Problem in Multi-Input Decoders.
  - Why needed here: Understanding why a naive extension of the transformer architecture fails is critical. The core contribution of MISO is its solution to this problem.
  - Quick check question: In a standard decoder-only transformer, what happens to the attention weight allocated to the generated output tokens if you simply concatenate KV caches from multiple independent input sequences?

- Concept: Mixture-of-Contexts / Fusion-in-Decoder (FiD) Paradigm.
  - Why needed here: MISO is directly inspired by FiD. Understanding the original encoder-decoder approach provides the necessary context for MISO's adaptation.
  - Quick check question: How does the FiD architecture process multiple documents, and why is its fusion method not directly transferable to a standard decoder-only model?

- Concept: Multi-Constraint Instruction Following.
  - Why needed here: This is the primary task used to evaluate MISO. Understanding its setup is essential for interpreting the experimental results.
  - Quick check question: In the context of this paper, what constitutes a "sub-context" in a multi-constraint instruction?

## Architecture Onboarding

- Component map:
  - **Input Processing:** Splits instructions into multiple sequences. Includes a `Position ID Manager` for parallel (restarting at 0) or successive IDs.
  - **MISO Causal Attention Layer:** The core modification. Replaces standard attention during output generation. Computes a weighted sum of attention outputs from the output query `Q(out)` to each `[Ki, K(out)]` pair.

- Critical path: The implementation of Eq. 3 (`MISO_CausalAttention`) is the single most critical component. A new engineer must ensure the weighted sum (using `Score_i`) correctly replaces the standard concatenated softmax attention.

- Design tradeoffs: A key tradeoff exists in the weighting strategy (`Score_i`).
  - **Uniform Weighting (Default):** `Score_i = 1/n`. Creates a balanced fusion. Empirically superior for constraint following (Table 2).
  - **Attention-Score Weighting (`MISO-fid`):** Weights based on unnormalized attention scores. Biased toward longer sequences. Less effective but more similar to standard attention.

- Failure signatures:
  - **Broken Generation:** Repetitive, nonsensical, or truncated output during inference after fine-tuning. This signals that the MISO Causal Attention is not implemented correctly and is suffering from output-attention dilution (as seen in prior work).
  - **No Improvement:** Performance not better than SFT. Check the ablation "- no MISO" setup. If performance is similar to SFT with augmented data, the core fusion mechanism is likely not working.

- First 3 experiments:
  1. **Sanity Check - Broken Generation:** Implement a naive multi-input extension and verify it produces broken output as described in Section 4.3. Then, implement MISO's weighted sum attention and confirm the fix.
  2. **Ablation - Weighting Strategy:** Run a controlled experiment comparing MISO-para (uniform weights) vs. MISO-fid-para (attention-score-based weights) on a subset of IFEval to reproduce the ablation results in Table 2.
  3. **Core Efficacy Test:** Fine-tune a base model (e.g., Llama3-8B) using MISO-para on the Crab dataset and evaluate on IFEval. Compare this against a standard