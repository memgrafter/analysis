---
ver: rpa2
title: 'KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference'
arxiv_id: '2511.11907'
source_url: https://arxiv.org/abs/2511.11907
tags:
- cache
- memory
- disk
- reuse
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KVSwap enables long-context LM inference on resource-constrained
  devices by offloading KV cache to disk. It uses a compressed in-memory K cache to
  predict and preload critical KV entries, employs grouped KV access to reduce disk
  I/O fragmentation, and leverages a reuse buffer to cache recently accessed entries.
---

# KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference

## Quick Facts
- arXiv ID: 2511.11907
- Source URL: https://arxiv.org/abs/2511.11907
- Authors: Huawei Zhang; Chunwei Xia; Zheng Wang
- Reference count: 40
- Primary result: 4.1× throughput improvement on eMMC and 11× less memory than vLLM

## Executive Summary
KVSwap addresses the challenge of running long-context language model inference on resource-constrained devices by offloading KV cache to disk. The system uses a compressed in-memory K cache to predict and preload critical KV entries, reducing expensive disk I/O operations. Through grouped KV access and a reuse buffer for recently accessed entries, KVSwap achieves significant performance improvements while maintaining generation quality.

## Method Summary
KVSwap implements a disk-aware KV cache offloading system for long-context LM inference on resource-constrained devices. The approach uses a compressed in-memory K cache to predict and preload critical KV entries before they're needed, reducing disk I/O latency. Grouped KV access minimizes disk fragmentation, while a reuse buffer caches recently accessed entries to avoid redundant disk reads. The system is designed to work with existing transformer architectures and maintains high generation quality despite the compression and offloading mechanisms.

## Key Results
- Achieves up to 4.1× throughput improvement over baselines on eMMC storage
- Reduces KV cache memory usage by 11× compared to vLLM
- Delivers 1.8× throughput improvement on NVMe storage while maintaining generation quality

## Why This Works (Mechanism)
KVSwap works by predicting which KV cache entries will be needed soon and keeping them in a compressed in-memory K cache, allowing critical data to be accessed quickly without expensive disk I/O. The system groups KV cache access operations to reduce disk fragmentation, making sequential reads more efficient. A reuse buffer caches recently accessed entries, exploiting temporal locality in attention patterns to avoid redundant disk operations.

## Foundational Learning

1. **KV Cache Compression** - Why needed: Long-context models require massive memory for KV cache storage. Quick check: Verify compression ratio maintains generation quality while reducing memory footprint.

2. **Disk I/O Preloading** - Why needed: Disk access is orders of magnitude slower than memory access. Quick check: Measure latency improvement when critical entries are preloaded versus accessed on-demand.

3. **Grouped Access Patterns** - Why needed: Random disk access causes significant performance degradation due to seek times. Quick check: Compare throughput with and without access grouping on fragmented storage.

4. **Temporal Locality in Attention** - Why needed: Attention mechanisms often reuse previous context, creating opportunities for caching. Quick check: Profile reuse buffer hit rates across different attention patterns.

## Architecture Onboarding

Component Map: Input -> Token Predictor -> KV Cache Manager -> Compressed K Cache -> Disk Storage -> Attention Layer -> Output

Critical Path: Token generation requires KV cache lookup → if miss, load from disk → decompress → attention computation → generate next token

Design Tradeoffs: Memory vs. latency tradeoff between compression ratio and decompression overhead; storage vs. performance tradeoff between disk speed and cache size; prediction accuracy vs. resource consumption for identifying critical entries.

Failure Signatures: Performance degradation when access patterns deviate from predicted locality; generation quality issues from excessive compression; system instability when disk I/O bandwidth is saturated.

First Experiments:
1. Measure baseline KV cache access latency with and without compression on target hardware
2. Profile attention pattern locality to validate reuse buffer effectiveness
3. Benchmark grouped vs. random disk access patterns for KV cache retrieval

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance benefits depend heavily on accurate prediction of critical KV entries
- Effectiveness of reuse buffer assumes strong temporal locality that may not hold for all workloads
- Long-term stability under varying access patterns remains unclear

## Confidence

High confidence: Overall throughput and memory reduction benefits compared to baselines; correctness of generation quality preservation; general approach of KV cache offloading with compression and prediction.

Medium confidence: Specific compression ratio claims; effectiveness of grouped KV access under different hardware configurations; reuse buffer hit rates across diverse workloads.

Low confidence: Long-term performance stability; impact on end-to-end application latency; generalizability to larger model families or different attention mechanisms.

## Next Checks
1. Measure end-to-end application latency (not just KV cache access) for realistic chat scenarios with varying context lengths.
2. Test performance degradation when access patterns deviate significantly from the predicted locality assumptions.
3. Evaluate robustness and effectiveness across multiple model architectures (not just LLaMA-7B) and attention variants.