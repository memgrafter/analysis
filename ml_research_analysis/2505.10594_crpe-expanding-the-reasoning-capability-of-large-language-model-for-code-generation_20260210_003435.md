---
ver: rpa2
title: 'CRPE: Expanding The Reasoning Capability of Large Language Model for Code
  Generation'
arxiv_id: '2505.10594'
source_url: https://arxiv.org/abs/2505.10594
tags:
- code
- reasoning
- data
- agent
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CRPE, a three-stage framework that enhances
  large language models' code reasoning capabilities through data synthesis and training.
  The method addresses the challenge of improving analytical and logical processing
  in code generation by first generating high-quality code problems, then synthesizing
  expert Chain of Thought data using a multi-agent framework, and finally applying
  tree search-based self-exploration with step-wise preference optimization.
---

# CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation

## Quick Facts
- arXiv ID: 2505.10594
- Source URL: https://arxiv.org/abs/2505.10594
- Reference count: 32
- Key result: COT-Coder-32B-StepDPO achieves 35.08 pass@1 accuracy on LiveCodeBench, outperforming GPT4O

## Executive Summary
This paper presents CRPE (Code Reasoning Process Enhancer), a three-stage framework that significantly improves large language models' code reasoning capabilities. The approach addresses the challenge of enhancing analytical and logical processing in code generation by synthesizing high-quality training data and applying step-wise preference optimization. CRPE successfully develops an enhanced COT-Coder model that demonstrates substantial improvements on code generation benchmarks, achieving state-of-the-art performance among models of similar or larger sizes.

## Method Summary
CRPE consists of three stages: (1) Code problem synthesis using open-source data collection and Evol-Instruct techniques with 10-gram decontamination filtering; (2) Multi-agent Chain-of-Thought data synthesis using Thinking, Reflection, and Execution agents to generate expert reasoning steps; (3) Tree-search-based self-improvement with step-DPO that constructs preference pairs from accepted/rejected reasoning steps. The framework trains on Qwen2.5-Coder base models and achieves significant performance gains on LiveCodeBench through explicit reasoning process supervision.

## Key Results
- COT-Coder-7B-StepDPO achieves 21.88 pass@1 accuracy on LiveCodeBench, outperforming all models of similar or larger sizes
- COT-Coder-32B-StepDPO reaches 35.08 pass@1 accuracy, surpassing GPT4O on the benchmark
- SFT alone (33.49 for 32B) underperforms SFT+Step-DPO (35.08), demonstrating the effectiveness of step-wise preference optimization
- Multi-agent CoT synthesis produces higher-quality reasoning chains than single-model generation approaches

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent synthesis produces higher-quality CoT training data than single-model generation by separating reasoning, validation, and execution into specialized agents with feedback loops. The Thinking Agent generates step-by-step reasoning → Reflection Agent validates each step and decides continue/finalize → Execution Agent verifies code via compiler + test cases → incorrect results flow back to Reflection Agent for error analysis → Thinking Agent regenerates with feedback. This iterative loop with execution-grounded verification filters out reasoning chains that reach correct answers through flawed logic.

### Mechanism 2
Tree-search sampling with acceptance/rejection classification generates fine-grained step-level preference signals that enable more precise optimization than outcome-only rewards. For each reasoning step node, sample up to `max_path_num` complete paths → execute final code → classify nodes as "accepted" (leads to ≥1 correct answer) or "rejected" (all paths fail after exhaustive sampling) → construct preference pairs from sibling nodes with divergent outcomes → train Step-DPO to prefer accepted steps over rejected ones given identical prefix.

### Mechanism 3
Explicit reasoning process supervision via CoT-SFT followed by Step-DPO induces System-2-like deliberative capabilities that base models lack. Stage 2 SFT on expert CoT data teaches the model the output format and reasoning paradigm → Stage 3 Step-DPO refines step-level decision making by learning to prefer correct continuations → this combination addresses both "how to reason" (structure) and "what reasoning is correct" (quality).

## Foundational Learning

- **Direct Preference Optimization (DPO)**: CRPE uses Step-DPO (a DPO variant) for Stage 3 self-improvement; understanding DPO's reward-model-free approach is prerequisite for implementing the preference learning loop. *Quick check: Can you explain why DPO eliminates the need for training a separate reward model compared to standard RLHF?*

- **Monte Carlo Tree Search (MCTS) variants for LLMs**: The tree-search algorithm in Section III.D adapts MCTS-style selection/expansion/simulation/backpropagation for code reasoning; understanding tradeoffs between exploration width (`max_path_num`) and depth (`max_depth_num`) is critical. *Quick check: How