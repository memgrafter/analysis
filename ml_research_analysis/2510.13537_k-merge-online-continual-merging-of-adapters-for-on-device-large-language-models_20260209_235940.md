---
ver: rpa2
title: 'K-Merge: Online Continual Merging of Adapters for On-device Large Language
  Models'
arxiv_id: '2510.13537'
source_url: https://arxiv.org/abs/2510.13537
tags:
- lora
- loras
- merging
- problem
- k-merge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incrementally integrating new
  task-specific LoRA adapters on resource-constrained devices while maintaining performance
  across all previously supported tasks. The proposed K-Merge and K-Merge++ methods
  use cosine similarity between LoRA parameter vectors to identify the most compatible
  stored adapter for merging, then apply a history-aware weighted combination that
  preserves prior task capabilities.
---

# K-Merge: Online Continual Merging of Adapters for On-device Large Language Models

## Quick Facts
- arXiv ID: 2510.13537
- Source URL: https://arxiv.org/abs/2510.13537
- Reference count: 28
- Primary result: Achieves 80-90% of single-task LoRA performance using only 8 adapter slots across 40 tasks

## Executive Summary
K-Merge addresses the challenge of incrementally integrating new task-specific LoRA adapters on resource-constrained devices while maintaining performance across all previously supported tasks. The method uses a data-free similarity metric to identify the most compatible stored adapter for merging, then applies a history-aware weighted combination that preserves prior task capabilities. Experiments across 40 tasks spanning 5 problem types and 8 languages show that K-Merge and K-Merge++ achieve 80-90% of single-task LoRA performance using only 8 adapter slots, outperforming baseline merging strategies.

## Method Summary
K-Merge is an online continual learning method for merging LoRA adapters on resource-constrained devices. It uses cosine similarity between flattened LoRA update matrices to identify the most compatible stored adapter for merging, then applies a history-aware weighted running average that preserves prior task capabilities. K-Merge++ adds a similarity threshold to avoid early storage saturation, improving robustness to worst-case task orderings. Both methods remain computationally lightweight for on-device deployment, requiring only similarity computations and weighted averaging without additional training data.

## Key Results
- K-Merge and K-Merge++ achieve 80-90% of single-task LoRA performance using only 8 adapter slots across 40 tasks
- K-Merge++ maintains 0.80 normalized score under worst-case ordering vs. K-Merge's 0.75 at K=5
- Both methods outperform baseline merging strategies while requiring only ~1-2% of base model parameters per adapter

## Why This Works (Mechanism)

### Mechanism 1
Cosine similarity between flattened LoRA update matrices identifies the most compatible stored adapter for merging without requiring training data. For each LoRA layer and projection type, compute ∆W = BA, flatten into a vector, compute cosine similarity between incoming and all stored LoRAs, average across layers/projections, and select the adapter with highest similarity score. Core assumption: LoRAs trained on similar problem types (even across different languages) will exhibit higher weight-space similarity than LoRAs from different problem types in the same language.

### Mechanism 2
History-aware running average merging provides order-invariant combination while proportionally weighting all merged LoRAs. Maintain a history map H(t) tracking which tasks belong to each cluster. When merging LoRA L(t) into cluster c with |H(t−1)[c]| existing members, compute: merge(Lc, L(t)) = (L(t) + |H(t−1)[c]| · Lc) / (|H(t−1)[c]| + 1). Core assumption: Equal contribution from all merged LoRAs yields acceptable multi-task performance; no task requires disproportional weighting.

### Mechanism 3
A similarity threshold prevents early storage saturation, reserving slots for future diverse LoRAs and improving robustness to adversarial task ordering. Before storage is full, merge incoming LoRA only if similarity to closest stored adapter exceeds threshold s (estimated as median pairwise similarity on held-out tasks). Otherwise, allocate a new slot. Once storage is full, merge regardless of threshold. Core assumption: The median similarity computed on held-out tasks generalizes to the distribution of incoming tasks; low similarity indicates genuinely different tasks worth storing separately.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: The entire method operates on LoRA adapters (rank-32, α=128 in experiments). Understanding that LoRA decomposes weight updates into BA matrices with ~1-2% of base model parameters is essential for interpreting storage savings and merge operations.
  - Quick check question: Can you explain why storing 40 LoRAs (rank-32) requires ~1GB but merging into K=8 clusters requires only ~200-270MB for Llama-3.2-1B?

- **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The core problem is maintaining performance on earlier tasks as new LoRAs arrive. The history-aware merging and evaluation protocol directly address this by tracking which tasks map to which clusters and normalizing against single-task baselines.
  - Quick check question: Why does the normalized aggregate score S(t) divide by single-task LoRA performance Mτi(L(i); Dτi) rather than using raw scores?

- **Cosine Similarity in Weight Space**
  - Why needed here: The clustering mechanism relies entirely on cosine similarity between flattened weight matrices. Understanding that this measures directional alignment (not magnitude) helps interpret why problem-type clusters emerge.
  - Quick check question: If two LoRAs have cosine similarity of 0.02, what does this suggest about their weight update directions? How does this relate to the threshold values s=0.020 (Llama) and s=0.028 (Qwen)?

## Architecture Onboarding

- **Component map:**
  - Incoming LoRA buffer -> Similarity scorer -> Decision engine -> Merge operator -> History map H(t) -> Task-to-cluster router Θ(t)

- **Critical path:**
  1. New LoRA arrives → compute similarities against all stored clusters
  2. If |L(t−1)| < K and (K-Merge++: sim < s): store as new cluster, update H(t)
  3. Else: merge into highest-similarity cluster via Eq. 8, update H(t)
  4. At inference: receive task ID i → lookup ˆc via H(t) → load Lˆc → generate

- **Design tradeoffs:**
  - **K (storage budget)**: Higher K preserves more task separation but increases storage (Table 2: 27-34MB per LoRA). Paper shows diminishing returns above K=5-8 for 40 tasks.
  - **Threshold s**: Lower s increases merging (risks interference), higher s preserves slots (risks early saturation). Paper uses median heuristic from held-out tasks.
  - **LoRA rank**: Paper uses rank-32; lower rank reduces storage but may limit expressiveness. Not ablated in paper.
  - **Merge strategy choice**: K-Merge++ adds robustness to task ordering but requires threshold estimation; K-Merge is simpler but vulnerable to adversarial sequences.

- **Failure signatures:**
  - **Rapid performance collapse after K slots filled**: Indicates threshold s too high (similar tasks not merged) or poor similarity metric
  - **High variance across random orderings**: Indicates K-Merge is too sensitive; switch to K-Merge++
  - **Single-task LoRAs significantly outperform merged LoRAs even at high K**: Indicates merge operation causing interference; consider alternative merging (TIES, DARE) though paper shows Linear merge is surprisingly competitive
  - **Inference routing errors**: History map H(t) corrupted or task ID not found in any cluster's value set

- **First 3 experiments:**
  1. **Validate baseline LoRA training**: Train single-task LoRAs on 5 problem types × 8 languages. Confirm single-task LoRAs achieve expected normalized score of 1.0 and significantly outperform zero-shot (Table A.1 shows 2-3× improvement).
  2. **Reproduce K-Merge vs. Linear merge gap**: With K=5, 40 tasks, random ordering, verify that K-Merge achieves S(γ) ≈ 0.80 vs. Linear ≈ 0.79 for Llama-3.2-1B (Table A.5). This validates the history-aware weighting mechanism.
  3. **Test ordering robustness**: Construct adversarial ordering (consecutive same-problem-type tasks). Verify K-Merge++ maintains S(γ) ≈ 0.80 while K-Merge drops to ≈0.75 and Linear drops to ≈0.69 (Table 1, K=5 Worst case). This validates the threshold mechanism's protective effect.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The method's effectiveness depends critically on the similarity metric's ability to capture semantic task relationships in weight space, which may not generalize to more diverse task distinctions
- The threshold-based allocation assumes held-out task similarity distributions generalize to incoming task distributions, which may not hold for highly variable task streams
- The evaluation protocol assumes knowledge of ground-truth task types for similarity computation and cluster assignment, which may not be available in fully unsupervised deployment scenarios

## Confidence
- **High confidence**: The history-aware running average mechanism and its implementation are well-founded, with clear mathematical formulation and consistent empirical validation across multiple ordering scenarios
- **Medium confidence**: The cosine similarity clustering mechanism shows strong empirical evidence for the tested problem types, but generalizability to more diverse task distinctions remains unproven
- **Medium confidence**: The K-Merge++ threshold mechanism demonstrates clear benefits in worst-case orderings, but sensitivity to threshold calibration and generalizability of the median heuristic warrant further investigation

## Next Checks
1. **Generalization to diverse task types**: Test K-Merge/K-Merge++ on a dataset mixing the 5 problem types with additional, more diverse tasks (e.g., creative writing, mathematical reasoning, multi-modal tasks) to evaluate whether cosine similarity in weight space continues to capture meaningful task relationships

2. **Ablation of rank and threshold parameters**: Systematically vary LoRA rank (8, 16, 32, 64) and test different threshold estimation strategies (mean, quantile-based, learned) to understand their impact on storage efficiency and performance, particularly for K-Merge++

3. **Unsupervised deployment scenario**: Implement and evaluate a variant of K-Merge that operates without ground-truth task type labels, using only task IDs and potentially task metadata (language, domain) to validate whether the method remains effective in truly online, unsupervised settings