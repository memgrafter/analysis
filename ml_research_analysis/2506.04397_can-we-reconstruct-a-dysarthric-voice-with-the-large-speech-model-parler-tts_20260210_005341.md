---
ver: rpa2
title: Can we reconstruct a dysarthric voice with the large speech model Parler TTS?
arxiv_id: '2506.04397'
source_url: https://arxiv.org/abs/2506.04397
tags:
- speech
- speaker
- intelligibility
- voice
- speakers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether a large speech model (Parler TTS)
  can reconstruct a dysarthric speaker's voice by generating speech with improved
  intelligibility while maintaining speaker identity. The authors fine-tune Parler
  TTS on a combined dataset of dysarthric speech (from the Speech Accessibility Project)
  and matched healthy control speech (from Multilingual LibriSpeech), annotated with
  intelligibility ratings.
---

# Can we reconstruct a dysarthric voice with the large speech model Parler TTS?

## Quick Facts
- arXiv ID: 2506.04397
- Source URL: https://arxiv.org/abs/2506.04397
- Reference count: 0
- The paper investigates whether Parler TTS can reconstruct a dysarthric speaker's voice with improved intelligibility while maintaining speaker identity

## Executive Summary
This paper explores the potential of using Parler TTS, a large speech model, to reconstruct the voice of dysarthric speakers by generating speech with improved intelligibility while preserving speaker identity. The authors fine-tune the model on a combined dataset of dysarthric speech and matched healthy control speech, annotated with intelligibility ratings. While the model demonstrates the ability to learn from challenging dysarthric data, results reveal significant limitations in controllability, with inconsistent improvements in intelligibility and speaker similarity. The study concludes that while voice reconstruction is feasible, more direct control mechanisms are needed for practical application.

## Method Summary
The authors fine-tune Parler TTS on a combined dataset consisting of dysarthric speech from the Speech Accessibility Project (2,500 utterances from 8 speakers) and matched healthy control speech from Multilingual LibriSpeech. The dataset is annotated with intelligibility ratings to guide the fine-tuning process. Both objective metrics (Word Error Rate, speaker similarity, Unified Test Mean Opinion Score) and expert listening tests are employed to evaluate the generated speech. The approach aims to leverage the model's capacity to learn from diverse speech patterns while improving intelligibility for dysarthric speakers.

## Key Results
- The model can learn from challenging dysarthric speech data but shows inconsistent improvements in intelligibility and speaker similarity
- Objective metrics demonstrated mixed results, with some cases showing degraded speaker similarity after fine-tuning
- Expert listening tests revealed subjective variability, with some raters perceiving improved intelligibility while others did not consistently match the target speaker identity

## Why This Works (Mechanism)
The approach works by leveraging the large speech model's ability to learn from diverse speech patterns, including the characteristic distortions present in dysarthric speech. By fine-tuning on a combined dataset of dysarthric and healthy speech with intelligibility annotations, the model attempts to learn transformations that enhance intelligibility while preserving the unique vocal characteristics that define speaker identity. The multilingual aspect of the training data may provide additional robustness to the model's generalization capabilities.

## Foundational Learning
- Dysarthric speech characteristics: Understanding the acoustic and phonetic features of dysarthric speech is essential for developing effective reconstruction models. Quick check: Can you identify the primary articulatory subsystems affected in different dysarthria types?
- Speech synthesis and TTS fundamentals: Knowledge of text-to-speech pipeline components (text normalization, acoustic modeling, waveform generation) is crucial. Quick check: What are the key differences between concatenative and parametric TTS approaches?
- Intelligibility metrics: Familiarity with objective measures like WER, MOS, and intelligibility ratings is necessary for evaluation. Quick check: How do subjective listening tests compare to automated metrics in assessing speech quality?
- Speaker adaptation techniques: Understanding methods for fine-tuning pre-trained models to new speakers or speech conditions is fundamental. Quick check: What are the trade-offs between speaker-dependent and speaker-independent TTS models?

## Architecture Onboarding

**Component Map:**
Parler TTS model -> Fine-tuning pipeline -> Intelligibility-annotated dataset -> Evaluation metrics (WER, speaker similarity, UTMOS) -> Expert listening tests

**Critical Path:**
The critical path involves fine-tuning the Parler TTS model on the combined dysarthric-healthy dataset, followed by evaluation using both objective metrics and subjective listening tests. The quality of the intelligibility annotations and the diversity of the training data directly impact the model's ability to generate improved speech.

**Design Tradeoffs:**
- Using a large pre-trained model provides strong generalization but may require substantial fine-tuning data
- Combining dysarthric and healthy speech data helps with speaker identity preservation but may dilute dysarthric-specific learning
- Automated metrics provide quantitative assessment but may not fully capture perceptual improvements
- Expert listening tests offer valuable subjective insights but introduce inter-rater variability

**Failure Signatures:**
- Degradation in speaker similarity scores despite intelligibility improvements
- Inconsistent performance across different severity levels of dysarthria
- Limited controllability between intelligibility and speaker identity preservation
- Over-smoothing of dysarthric characteristics, resulting in loss of speaker identity

**First Experiments:**
1. Test model performance on a held-out subset of dysarthric speakers not seen during training
2. Compare intelligibility improvements between different severity levels of dysarthria
3. Evaluate speaker similarity preservation when using only dysarthric speech versus the combined dataset

## Open Questions the Paper Calls Out
The paper explicitly identifies the need for improved controllability mechanisms to independently modulate intelligibility and speaker identity. The authors propose future work on enhancing prompts and implementing data augmentation techniques to address these limitations. They also suggest exploring alternative model architectures that may offer better control over specific speech attributes.

## Limitations
- Limited dataset size (2,500 utterances from 8 speakers) constrains generalizability across the full spectrum of dysarthric speech
- Inconsistent objective metric improvements, particularly degraded speaker similarity in some cases
- Lack of controllability between intelligibility enhancement and speaker identity preservation
- Single-model approach without comparison to alternative architectures

## Confidence

**Major Claim Confidence:**
- **High confidence**: The model can learn from dysarthric speech data to generate intelligible speech
- **Medium confidence**: The model maintains speaker identity in generated speech (mixed objective and subjective evidence)
- **Low confidence**: The model provides controllable voice reconstruction (authors explicitly state this is a limitation)

## Next Checks
1. Test the approach on a larger, more diverse dysarthric speech corpus with broader severity levels and multiple etiologies
2. Implement and evaluate explicit controllability mechanisms (e.g., style tokens, separate intelligibility prompts) to independently modulate intelligibility and speaker identity
3. Conduct longitudinal studies with actual dysarthric speakers using the system to assess real-world communication benefits beyond controlled listening tests