---
ver: rpa2
title: Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential
  Reasoning Tasks
arxiv_id: '2602.00449'
source_url: https://arxiv.org/abs/2602.00449
tags:
- latent
- reasoning
- task
- final
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies CODI, a latent CoT model that compresses explicit\
  \ reasoning traces into short continuous thought vectors, and examines whether it\
  \ performs genuine step-by-step computation or shortcuts. Using logit-lens decoding,\
  \ linear probes, attention analysis, and activation patching on polynomial-iteration\
  \ tasks, the authors find that for 2\u20133 hops CODI does form and maintain intermediate\
  \ bridge states in the latent channel, but for longer hops (n\u22654) it typically\
  \ encodes only the final one or two intermediates and fuses them with the last input\
  \ at the answer readout position."
---

# Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks

## Quick Facts
- **arXiv ID**: 2602.00449
- **Source URL**: https://arxiv.org/abs/2602.00449
- **Authors**: Jia Liang; Liangming Pan
- **Reference count**: 40
- **Primary result**: Latent CoT models encode intermediate reasoning states for short hops but shortcut to final intermediates for longer chains

## Executive Summary
This paper investigates whether latent chain-of-thought (CoT) models like CODI perform genuine step-by-step computation or take shortcuts when solving sequential reasoning tasks. Using mechanistic interpretability techniques on polynomial-iteration problems, the authors find that CODI maintains intermediate states for 2-3 reasoning hops but encodes only final intermediates for longer chains (n≥4). The study reveals that this "partial reasoning" pattern depends on task compressibility under composite moduli, where many-to-one contractions allow later updates to dominate. The explicit CoT teacher objective, rather than distillation alone, drives the late-bottleneck mechanism that enables this behavior.

## Method Summary
The authors analyze CODI, a latent CoT model that compresses explicit reasoning traces into continuous thought vectors, using polynomial-iteration tasks under different moduli structures. They employ logit-lens decoding, linear probes, attention analysis, and activation patching to examine intermediate state formation and maintenance. The study compares performance and reasoning patterns under composite versus prime moduli, and conducts ablations to isolate the role of the explicit CoT teacher objective versus distillation alone. The mechanistic analysis focuses on identifying when and how intermediate reasoning states are formed, maintained, and accessed during the reasoning process.

## Key Results
- CODI forms and maintains intermediate bridge states in the latent channel for 2-3 hops
- For longer hops (n≥4), CODI typically encodes only the final one or two intermediates
- The explicit CoT teacher objective, not distillation alone, drives the late-bottleneck mechanism
- Performance drops sharply under prime moduli where many-to-one contractions don't apply
- The partial reasoning pattern is tied to task compressibility through composite moduli structure

## Why This Works (Mechanism)
The mechanism relies on the interaction between the model's compression objective and the mathematical structure of the task. Under composite moduli, polynomial iteration exhibits many-to-one contractions where different inputs can produce the same output, allowing the model to effectively "forget" earlier intermediates while preserving enough information for correct final answers. The latent channel serves as a compressed representation that can store compressed versions of intermediate states when the computation is inherently compressible. The explicit CoT teacher objective provides a stronger signal for maintaining reasoning structure compared to distillation alone, particularly at late bottleneck positions where final answers are read out.

## Foundational Learning
- **Latent CoT models**: Models that compress explicit reasoning traces into continuous thought vectors; needed to understand the compression-accuracy tradeoff being studied
- **Logit-lens decoding**: A mechanistic interpretability technique for examining model internal states; needed to trace intermediate reasoning through the network
- **Polynomial iteration tasks**: Sequential mathematical problems where outputs become inputs; needed as controlled testbeds for reasoning analysis
- **Moduli structure**: Mathematical properties of number systems (prime vs composite); needed to understand task compressibility and its effect on reasoning patterns
- **Activation patching**: A technique for testing causal relationships between model components; needed to validate the role of specific mechanisms
- **Linear probes**: Methods for extracting latent information; needed to detect intermediate state formation in compressed representations

## Architecture Onboarding
**Component map**: Input tokens -> Encoder -> Latent channel (compressed thought vector) -> Decoder -> Output tokens
**Critical path**: The latent channel serves as the bottleneck where reasoning compression occurs; success depends on maintaining sufficient intermediate information for correct answers
**Design tradeoffs**: Compression efficiency vs. reasoning fidelity; explicit supervision vs. self-distillation; model capacity vs. computational overhead
**Failure signatures**: Performance drops under prime moduli; inability to recover intermediates for long chains; reliance on final input fusion at answer readout
**First experiments**:
1. Replicate polynomial-iteration analysis under different moduli structures to verify composite vs. prime effects
2. Test linear probe sensitivity across different probe dimensionalities and training regimes
3. Apply activation patching to isolate the contribution of latent channel states at different reasoning stages

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to polynomial-iteration tasks with specific mathematical properties
- Findings may not generalize to other reasoning domains beyond sequential mathematical computation
- Interpretation of "genuine" versus "shortcut" reasoning depends on specific operational definitions
- The causal mechanism linking CoT teacher objective to late-bottleneck formation is not fully explained
- Performance differences under composite vs. prime moduli are observed but underlying causal mechanisms remain speculative

## Confidence
**High**: Findings about intermediate state formation and maintenance for 2-3 hops; specific pattern of encoding final intermediates for longer hops
**Medium**: Claims about the role of explicit CoT teacher objective in driving late-bottleneck mechanism; relationship between moduli compressibility and reasoning patterns
**Low**: Generalizability of findings to other reasoning tasks; interpretation of what constitutes "genuine" versus "shortcut" reasoning

## Next Checks
1. Test whether the partial reasoning pattern persists across diverse mathematical reasoning tasks with different structural properties beyond polynomial iteration
2. Conduct cross-model validation by examining whether other latent CoT architectures show similar late-bottleneck behavior under varying hop counts
3. Perform ablation studies specifically isolating the contribution of the CoT teacher objective from other architectural components to establish causal relationships more rigorously