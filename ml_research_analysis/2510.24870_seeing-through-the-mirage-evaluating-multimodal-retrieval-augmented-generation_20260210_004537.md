---
ver: rpa2
title: 'Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation'
arxiv_id: '2510.24870'
source_url: https://arxiv.org/abs/2510.24870
tags:
- claim
- information
- precision
- human
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIRAGE, a framework for evaluating multimodal
  retrieval-augmented generation (RAG) systems. The key insight is that evaluation
  should be claim-centric, decomposing generated text into subclaims and verifying
  them against source materials across modalities.
---

# Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2510.24870
- Source URL: https://arxiv.org/abs/2510.24870
- Reference count: 40
- Primary result: MIRAGE framework achieves strong correlation with human judgment for multimodal RAG evaluation

## Executive Summary
This paper introduces MIRAGE, a framework for evaluating multimodal retrieval-augmented generation (RAG) systems. The key insight is that evaluation should be claim-centric, decomposing generated text into subclaims and verifying them against source materials across modalities. MIRAGE consists of INFOF1, which measures factuality and information coverage, and CITEF1, which measures citation support and completeness. The authors compare MIRAGE against three prominent text-based RAG metrics (ALCE, ARGUE, and RAGAS) adapted to multimodal settings, demonstrating superior alignment with human evaluation through correlation analysis.

## Method Summary
MIRAGE evaluates multimodal RAG systems by decomposing generated responses into subclaims and verifying each claim against retrieved multimodal sources. The framework uses two main metrics: INFOF1, which assesses whether subclaims are factually supported by the sources, and CITEF1, which evaluates citation completeness and support. The evaluation pipeline involves generating responses from a multimodal RAG system, breaking down the text into verifiable claims, and using vision-language models to verify claims against both textual and visual sources. Human evaluation was conducted with 15 participants rating 10 generated responses each across three dimensions: quality, coverage, and trustworthiness.

## Key Results
- MIRAGE's INFOF1 metric shows strong correlation with human judgments (r = 0.67) compared to text-based metrics
- CITEF1 achieves moderate correlation with human evaluations (r = 0.58)
- Text-based metrics like ALCE, ARGUE, and RAGAS show poor alignment with human judgments in multimodal settings
- The framework successfully identifies limitations of text-centric evaluation approaches when applied to multimodal RAG systems

## Why This Works (Mechanism)
MIRAGE works by directly aligning evaluation with how multimodal RAG systems should function: by grounding generated text in multimodal evidence. By decomposing responses into verifiable claims and checking each against the retrieved sources, the framework captures both factual accuracy and proper source attribution. This claim-centric approach naturally handles the complexity of multimodal information, where visual elements may contain crucial evidence that text-only metrics miss. The dual-metric structure allows separate assessment of content quality (INFOF1) and source citation practices (CITEF1), providing a comprehensive evaluation that mirrors human judgment processes.

## Foundational Learning
**Multimodal RAG Systems** - Why needed: Understanding how these systems retrieve and integrate information across text and images is essential for proper evaluation. Quick check: Can you explain how a multimodal RAG system differs from text-only RAG?

**Claim Decomposition** - Why needed: Breaking down generated text into verifiable subclaims is the foundation of MIRAGE's approach. Quick check: How does claim decomposition improve evaluation accuracy compared to holistic assessment?

**Vision-Language Model (VLM) Capabilities** - Why needed: VLMs are crucial for verifying claims against visual sources in multimodal RAG evaluation. Quick check: What are the current limitations of VLMs in cross-modal verification tasks?

## Architecture Onboarding

**Component Map:** MIRAGE Pipeline -> Claim Extraction -> Subclaim Verification (INFOF1) -> Citation Analysis (CITEF1) -> Composite Score

**Critical Path:** Generated Response → Claim Extraction Module → Individual Claim Verification → INFOF1 Calculation → Citation Extraction → CITEF1 Calculation → Final MIRAGE Score

**Design Tradeoffs:** The framework trades computational complexity for evaluation accuracy by performing granular claim verification rather than holistic assessment. This increases evaluation time but provides more reliable alignment with human judgment.

**Failure Signatures:** Poor performance when VLMs struggle with cross-modal reasoning, when claim extraction produces overly complex or ambiguous subclaims, or when retrieved multimodal sources lack sufficient evidence for verification.

**First Experiments:**
1. Run MIRAGE on a simple multimodal RAG system with clear ground truth to validate the claim verification mechanism
2. Compare MIRAGE scores with human annotations on the same dataset to establish baseline correlation
3. Test the sensitivity of MIRAGE metrics to variations in VLM configuration and claim extraction parameters

## Open Questions the Paper Calls Out
The paper identifies the need for better calibration of VLMs and LLMs for claim verification tasks as a key limitation. It also raises questions about how to handle cases where multimodal sources provide conflicting evidence and how to extend the framework to more complex multimodal scenarios beyond image-text combinations.

## Limitations
- Human evaluation relied on only 15 participants assessing 10 samples each, potentially limiting statistical robustness
- Correlation analysis assumes linear relationships that may not fully capture evaluation quality
- Results may not generalize to all multimodal RAG systems due to evaluation on specific datasets
- Computational overhead of claim-level verification is not quantified or compared to simpler metrics

## Confidence
**High:** Core claims about MIRAGE's superior correlation with human judgment (INFOF1 r = 0.67, CITEF1 r = 0.58)
**Medium:** Generalizability of results to other multimodal RAG systems and evaluation contexts
**Low:** Claims about computational efficiency and scalability compared to text-based metrics

## Next Checks
1. Conduct a larger-scale human evaluation study with at least 30 participants and 20 samples each to validate correlation findings and assess inter-rater reliability
2. Test MIRAGE's performance across different VLM providers and configurations to assess robustness and identify potential calibration issues
3. Perform ablation studies to quantify the contribution of each MIRAGE component (INFOF1 and CITEF1) to overall evaluation quality and identify potential redundancies