---
ver: rpa2
title: 'GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement
  and Preservation'
arxiv_id: '2511.00097'
source_url: https://arxiv.org/abs/2511.00097
tags:
- graph
- domains
- domain
- learning
- graphkeeper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraphKeeper, the first method addressing
  catastrophic forgetting in graph domain-incremental learning (Domain-IL), a critical
  yet unexplored problem with the rise of graph foundation models (GFMs). GraphKeeper
  tackles embedding shifts and decision boundary deviations by combining domain-specific
  parameter-efficient fine-tuning with intra- and inter-domain disentanglement, alongside
  a deviation-free knowledge preservation mechanism based on ridge regression.
---

# GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation

## Quick Facts
- arXiv ID: 2511.00097
- Source URL: https://arxiv.org/abs/2511.00097
- Reference count: 40
- First method addressing catastrophic forgetting in graph domain-incremental learning (Domain-IL)

## Executive Summary
GraphKeeper introduces a novel approach for domain-incremental learning on graphs that addresses catastrophic forgetting - a critical challenge in graph foundation models. The method combines domain-specific parameter-efficient fine-tuning with intra- and inter-domain disentanglement, along with a deviation-free knowledge preservation mechanism based on ridge regression. It also employs domain-aware distribution discrimination for graphs with unobservable domains. Experiments demonstrate significant performance improvements over existing methods.

## Method Summary
GraphKeeper tackles catastrophic forgetting in graph domain-incremental learning through a multi-faceted approach. It employs domain-specific parameter-efficient fine-tuning to adapt to new domains while preserving knowledge from previous domains. The method incorporates both intra- and inter-domain disentanglement techniques to separate domain-specific features from shared knowledge. A key innovation is the deviation-free knowledge preservation mechanism using ridge regression, which maintains performance on previously learned domains. Additionally, GraphKeeper implements domain-aware distribution discrimination to handle graphs with unobservable domains, making it applicable to real-world scenarios.

## Key Results
- Achieves 6.5%~16.6% improvement over the runner-up
- Demonstrates negligible forgetting on previously learned domains
- Shows seamless integration capability with representative graph foundation models
- First method specifically designed for catastrophic forgetting in graph domain-incremental learning

## Why This Works (Mechanism)
GraphKeeper works by addressing two fundamental challenges in domain-incremental learning: embedding shifts and decision boundary deviations. The domain-specific parameter-efficient fine-tuning allows adaptation to new domains without overwriting previous knowledge. The disentanglement mechanisms separate domain-specific features from shared knowledge, preventing interference between domains. The ridge regression-based preservation mechanism maintains a stable knowledge base across domains. The distribution discrimination component enables the model to recognize and adapt to domain shifts even when domain labels are unavailable, making the approach practical for real-world applications.

## Foundational Learning
- **Catastrophic Forgetting**: When models learn new tasks, they tend to overwrite previously learned knowledge. Critical for continual learning scenarios where data arrives sequentially.
  - Quick check: Monitor performance degradation on previous tasks after learning new ones

- **Parameter-Efficient Fine-Tuning**: Techniques that adapt models to new tasks with minimal parameter updates. Essential for computational efficiency in continual learning.
  - Quick check: Measure parameter count and computational overhead during adaptation

- **Disentanglement**: Separating domain-specific features from shared knowledge to prevent interference. Crucial for maintaining performance across multiple domains.
  - Quick check: Evaluate feature separability using metrics like mutual information

- **Ridge Regression**: A regularization technique used for knowledge preservation. Helps maintain stability while adapting to new domains.
  - Quick check: Monitor regularization strength and its impact on adaptation performance

## Architecture Onboarding
- **Component Map**: Input Graphs -> Disentanglement Layer -> Parameter-Efficient Fine-Tuning -> Ridge Regression Preservation -> Domain Discrimination -> Output Predictions
- **Critical Path**: The disentanglement layer and ridge regression preservation mechanism form the core of the approach, as they directly address catastrophic forgetting
- **Design Tradeoffs**: Balances between adaptation capability (fine-tuning) and knowledge preservation (ridge regression), with disentanglement serving as the mediator
- **Failure Signatures**: Performance degradation on previous domains indicates insufficient preservation; poor adaptation to new domains suggests inadequate fine-tuning
- **First Experiments**:
  1. Test on synthetic domain shift dataset to validate disentanglement effectiveness
  2. Evaluate ridge regression preservation on simple continual learning benchmark
  3. Assess domain discrimination capability on unlabeled domain shift scenario

## Open Questions the Paper Calls Out
None

## Limitations
- Limited validation on very large-scale graph datasets and foundation models
- No explicit metrics defined for quantifying forgetting
- Lacks comprehensive comparison with non-graph continual learning methods

## Confidence
- Claim of being "first method" for graph domain-incremental learning: Medium confidence
- Core contributions (disentanglement, preservation, discrimination): High confidence
- Specific performance improvements (6.5%~16.6%): Medium confidence
- Claim of "negligible forgetting": Medium confidence
- Seamless integration with GFMs: Medium confidence

## Next Checks
1. Conduct ablation studies to isolate contributions of each component (disentanglement, preservation, distribution discrimination)
2. Test GraphKeeper on larger-scale graph datasets and foundation models to validate scalability and generalizability
3. Implement comprehensive comparison with non-graph continual learning methods to establish unique challenges and solutions for graph-based domain-incremental learning