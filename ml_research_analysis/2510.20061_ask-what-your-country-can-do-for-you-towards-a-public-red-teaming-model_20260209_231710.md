---
ver: rpa2
title: 'Ask What Your Country Can Do For You: Towards a Public Red Teaming Model'
arxiv_id: '2510.20061'
source_url: https://arxiv.org/abs/2510.20061
tags:
- teaming
- exercise
- public
- nist
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new approach to AI safety evaluation: cooperative
  public AI red-teaming exercises. It addresses the "responsibility gap" in AI safety
  where AI labs are perceived as solely responsible for safety, while public entities
  and application developers lack effective oversight tools.'
---

# Ask What Your Country Can Do For You: Towards a Public Red Teaming Model

## Quick Facts
- arXiv ID: 2510.20061
- Source URL: https://arxiv.org/abs/2510.20061
- Reference count: 14
- Primary result: Public red-teaming exercises can deliver meaningful safety insights and leverage diverse expertise to achieve better coverage of harm surfaces

## Executive Summary
This paper introduces a novel approach to AI safety evaluation through cooperative public AI red-teaming exercises. The authors address the "responsibility gap" where AI labs are perceived as solely responsible for safety while public entities lack effective oversight tools. The proposed model involves organizing large-scale, diverse red-teaming exercises open to public participation across jurisdictions, with three pilot implementations demonstrating the approach's feasibility and effectiveness.

## Method Summary
The core method involves organizing large-scale, diverse red-teaming exercises open to public participation across jurisdictions. The approach was piloted through three implementations: the NIST ARIA national pilot with 457 participants testing three LLMs, the CAMLIS 2024 in-person demonstrator with 30 top red teamers testing four models, and the IMDA Singapore multilingual challenge testing across nine countries and languages. These pilots demonstrated that public red-teaming can deliver meaningful safety insights, leverage diverse expertise for better harm surface coverage, and provide scalable models for AI-developing jurisdictions.

## Key Results
- Public red-teaming exercises can deliver meaningful safety insights
- Diverse expertise in public exercises achieves better coverage of harm surfaces
- The model provides scalable approaches that jurisdictions can adopt for AI safety oversight

## Why This Works (Mechanism)
The mechanism works by democratizing AI safety evaluation, distributing responsibility across stakeholders, and leveraging the collective intelligence of diverse participants. Public participation increases accountability and transparency while identifying safety issues that may be missed by internal teams with homogeneous perspectives.

## Foundational Learning
- AI safety evaluation principles: Understanding how different evaluation methodologies compare and complement each other
- Public engagement strategies: How to effectively recruit, train, and manage diverse participant groups
- Red teaming methodologies: The specific techniques and approaches used to systematically test AI systems for vulnerabilities
- Cross-jurisdictional coordination: Managing multi-country collaborations and standardizing evaluation criteria
- Why needed: These concepts are essential for implementing effective public red-teaming programs
- Quick check: Can participants from diverse backgrounds identify unique safety issues compared to professional red teamers?

## Architecture Onboarding
- Component map: Public participants -> Evaluation platform -> AI models -> Data collection -> Analysis framework -> Safety insights
- Critical path: Participant recruitment and training → Red teaming execution → Data aggregation → Analysis and reporting
- Design tradeoffs: Scale vs. quality (larger groups provide more diversity but may require more coordination), public vs. expert focus (broad participation vs. specialized expertise)
- Failure signatures: Low participant diversity, insufficient training, poor coordination across jurisdictions, inadequate analysis frameworks
- First experiments:
  1. Conduct a small-scale pilot with 50-100 participants to validate recruitment and training processes
  2. Test the evaluation platform's scalability by gradually increasing participant numbers
  3. Compare safety findings from public red teaming against internal red team results on identical models

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability concerns due to specific demographic and technical characteristics of pilot implementations
- Lack of detailed quantitative metrics comparing public red-teaming effectiveness against traditional internal methods
- Scalability claims remain theoretical without concrete cost-effectiveness evidence
- The social and political claim about shifting accountability structures lacks empirical validation

## Confidence
- Technical findings about diverse expertise coverage: Medium
- Broader claims about responsibility redistribution and scalability: Low

## Next Checks
1. Conduct a comparative study measuring harm surface coverage and false positive rates between public and internal red-teaming exercises using identical models and metrics
2. Implement a longitudinal study tracking whether jurisdictions adopting public red-teaming show measurable improvements in AI safety outcomes compared to traditional evaluation methods
3. Perform a cost-benefit analysis across different scales of public red-teaming exercises to determine the optimal participant-to-model ratio for effective safety evaluation