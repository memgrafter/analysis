---
ver: rpa2
title: 'ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language
  Models'
arxiv_id: '2505.13180'
source_url: https://arxiv.org/abs/2505.13180
tags:
- action
- object
- planning
- medium
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ViPlan, the first open-source benchmark\
  \ for evaluating vision-language models (VLMs) in visual planning tasks. ViPlan\
  \ provides two distinct settings\u2014VLM-as-planner and VLM-as-grounder\u2014across\
  \ two domains: a visual Blocksworld planning problem and a simulated household robotics\
  \ environment."
---

# ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models

## Quick Facts
- **arXiv ID**: 2505.13180
- **Source URL**: https://arxiv.org/abs/2505.13180
- **Reference count**: 40
- **Primary result**: Open-source benchmark introducing two visual planning settings across Blocksworld and household robotics domains, showing symbolic planning outperforms direct VLM planning in Blocksworld while the opposite holds for household robotics

## Executive Summary
ViPlan is the first open-source benchmark designed to evaluate vision-language models (VLMs) on visual planning tasks. The benchmark introduces two distinct evaluation settings: VLM-as-planner, where models directly generate action sequences from visual inputs, and VLM-as-grounder, where models convert visual information into symbolic predicates for use with traditional planners. Across two domains (visual Blocksworld and household robotics), the study evaluates nine open-source VLM families and two closed models, revealing domain-specific performance patterns and the limited effectiveness of Chain-of-Thought prompting for visual reasoning tasks.

## Method Summary
The ViPlan benchmark evaluates VLMs through two distinct settings: VLM-as-planner, where models directly generate action sequences from visual observations, and VLM-as-grounder, where models extract symbolic predicates from images for use with traditional planners. The benchmark includes two domains: a visual Blocksworld planning problem and a simulated household robotics environment. Nine open-source VLM families (LLaVA, Qwen, AyaVision, Gemma, DeepSeek, Phi-4, Molmo, Mistral, InternVL) plus two closed models (GPT-4.1 and GPT-4.1 Nano) were evaluated across these settings using comprehensive metrics that assess both task completion and planning quality.

## Key Results
- Symbolic planning approaches (VLM-as-grounder) outperform direct VLM planning in Blocksworld tasks where accurate visual grounding is critical
- Direct VLM planning (VLM-as-planner) shows superior performance in household robotics tasks, suggesting benefits from commonsense knowledge and error recovery
- Chain-of-Thought prompting shows no significant benefit across most models and methods, indicating fundamental challenges VLMs face with visual reasoning

## Why This Works (Mechanism)
The benchmark design leverages the complementary strengths of VLMs and symbolic planners by providing two evaluation pathways. In VLM-as-grounder mode, VLMs handle visual perception while symbolic planners manage logical reasoning, creating a hybrid approach that capitalizes on each system's strengths. The VLM-as-planner setting tests end-to-end reasoning capabilities but reveals current limitations in VLMs' ability to perform complex visual reasoning without intermediate symbolic representations. The domain-specific performance differences suggest that task characteristics (structured vs. commonsense-heavy) significantly influence which approach is more effective.

## Foundational Learning
**Vision-Language Models**: Pre-trained models that process both visual and textual inputs, needed for interpreting visual scenes and generating natural language responses. Quick check: Verify model can describe objects and their relationships in an image.

**Symbolic Planning**: Classical AI planning using logical predicates and operators, needed for structured problem-solving with guaranteed completeness. Quick check: Ensure planner can generate valid action sequences from symbolic descriptions.

**Visual Grounding**: The process of mapping visual elements to symbolic representations, needed for bridging perception and reasoning. Quick check: Test model's ability to correctly identify objects and their properties from images.

**Chain-of-Thought Prompting**: A prompting strategy that encourages step-by-step reasoning, needed for complex problem-solving tasks. Quick check: Verify model can break down problems into intermediate reasoning steps.

## Architecture Onboarding

**Component Map**: Visual Input -> VLM (as-planner or as-grounder) -> Action Sequence/Output

**Critical Path**: Image Processing → Visual Understanding → Reasoning/Planning → Action Generation

**Design Tradeoffs**: VLM-as-planner offers end-to-end solution but struggles with complex reasoning; VLM-as-grounder provides modularity but requires accurate visual grounding. The choice between them depends on domain characteristics and task complexity.

**Failure Signatures**: VLM-as-planner fails on tasks requiring structured reasoning; VLM-as-grounder fails when visual grounding is inaccurate or incomplete. Both struggle with temporal reasoning in dynamic environments.

**First Experiments**: 1) Test VLM's ability to describe basic visual scenes with objects and relationships. 2) Evaluate symbolic planner's performance on ground truth predicates. 3) Compare VLM-as-planner vs VLM-as-grounder on simple planning tasks with clear visual cues.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited domain diversity with only two domains tested (Blocksworld and household robotics) raises questions about generalizability
- Static image reliance may not capture the full complexity of dynamic, real-world environments
- Performance differences between VLM-as-planner and VLM-as-grounder could be influenced by domain-specific factors rather than fundamental methodological advantages

## Confidence
- **High**: Open-sourcing of the benchmark, basic performance trends in Blocksworld, identification of no CoT benefit
- **Medium**: Relative performance between VLM-as-planner and VLM-as-grounder approaches, domain-specific findings
- **Low**: Generalizability claims beyond tested domains, reasons for CoT ineffectiveness

## Next Checks
1. Test the benchmark with additional domains that vary in visual complexity, object count, and task structure to assess generalizability.
2. Conduct ablation studies with different visual representations (video sequences instead of static images) to evaluate temporal reasoning capabilities.
3. Compare performance when models have access to additional commonsense knowledge bases or when provided with more sophisticated visual grounding techniques.