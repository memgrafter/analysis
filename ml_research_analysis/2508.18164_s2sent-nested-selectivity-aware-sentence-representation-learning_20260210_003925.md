---
ver: rpa2
title: 'S2Sent: Nested Selectivity Aware Sentence Representation Learning'
arxiv_id: '2508.18164'
source_url: https://arxiv.org/abs/2508.18164
tags:
- sentence
- semantic
- representation
- spatial
- s2sent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of effective sentence representation
  learning using Transformer-based encoders with contrastive learning. While current
  methods rely on the last Transformer block, different blocks have varying semantic
  perception abilities that are not fully exploited.
---

# S2Sent: Nested Selectivity Aware Sentence Representation Learning

## Quick Facts
- **arXiv ID:** 2508.18164
- **Source URL:** https://arxiv.org/abs/2508.18164
- **Reference count:** 15
- **Primary result:** Introduces S2Sent method with nested Spatial and Frequency Selection modules that significantly improve STS task performance with negligible parameter overhead (<0.5%) and minimal latency

## Executive Summary
This paper addresses the problem of effective sentence representation learning using Transformer-based encoders with contrastive learning. While current methods rely on the last Transformer block, different blocks have varying semantic perception abilities that are not fully exploited. The proposed S2Sent method introduces a parameterized nested selector downstream of the encoder that performs Spatial Selection (SS) and Frequency Selection (FS). SS uses a spatial squeeze-based self-gating mechanism to obtain adaptive weights for cross-block fusion, modeling dependencies between embedding features. FS replaces global average pooling with low-frequency DCT basis functions to reduce semantic loss during spatial squeeze. Experimental results show S2Sent significantly improves performance across seven STS tasks while introducing negligible additional parameters (less than 0.5% of backbone) and minimal inference latency. The method demonstrates strong integrability and scalability, not relying on specific hyperparameter choices for the number of Transformer blocks or frequency components.

## Method Summary
S2Sent addresses the underexploited semantic perception across different Transformer blocks by introducing a nested selector mechanism that performs both Spatial Selection (SS) and Frequency Selection (FS) for cross-block fusion. The method takes stacked hidden states from multiple Transformer blocks and applies a spatial squeeze-based self-gating mechanism to obtain adaptive weights for fusion, modeling dependencies between embedding features. Instead of global average pooling, FS uses low-frequency DCT basis functions to reduce semantic loss during spatial squeeze. The approach is evaluated on seven STS tasks using BERT and RoBERTa backbones with SimCSE and PromptCSE contrastive learning frameworks, demonstrating significant improvements while adding less than 0.5% parameters and minimal latency overhead.

## Key Results
- S2Sent significantly improves performance across seven STS tasks (STS 2012-2016, STS-Benchmark, SICK-Relatedness) compared to baseline methods
- Introduces negligible additional parameters (less than 0.5% of backbone encoder)
- Minimal inference latency overhead while maintaining strong integrability and scalability
- Performance improvements are consistent across different backbone choices (BERT/RoBERTa) and contrastive learning variants (SimCSE/PromptCSE)

## Why This Works (Mechanism)
S2Sent works by addressing the limitation of current sentence representation methods that rely solely on the last Transformer block, which ignores the varying semantic perception abilities of different blocks. The nested selector mechanism captures complementary semantic information across multiple layers through adaptive weighting, allowing the model to leverage the unique strengths of each block. The Frequency Selection component reduces information loss during the spatial squeeze operation by using low-frequency DCT basis functions instead of simple averaging, preserving more semantic content. This combination enables more robust and semantically rich sentence representations that better capture the nuances needed for STS tasks.

## Foundational Learning
- **Transformer block stacking**: Understanding how to stack hidden states from multiple Transformer blocks is crucial for implementing the cross-block fusion mechanism. *Quick check:* Verify that the input to S2Sent is a tensor with shape (N, L, D) where N is the number of blocks being fused.
- **Spatial squeeze-based self-gating**: This mechanism learns adaptive weights for each block based on their semantic content. *Quick check:* Ensure the excitation network produces a softmax distribution across blocks that sums to 1.
- **2D Discrete Cosine Transform (DCT)**: DCT basis functions are used instead of global average pooling to preserve semantic information. *Quick check:* Confirm that the DCT implementation uses orthonormal basis functions and correct indexing for low-frequency components.
- **Contrastive learning objectives**: The method builds on SimCSE/PromptCSE frameworks, requiring understanding of unsupervised contrastive loss with temperature scaling. *Quick check:* Verify temperature Ï„=0.05 is applied correctly in the loss function.
- **Spearman correlation evaluation**: STS tasks are evaluated using Spearman correlation, not Pearson correlation. *Quick check:* Ensure the evaluation metric matches the paper's reported results.

## Architecture Onboarding

**Component Map:** BERT/RoBERTa backbone -> Stack last-n blocks -> S2Sent (SS+FS) -> Weighted fusion -> Sentence representation

**Critical Path:** The core innovation lies in the nested selector that performs SS followed by FS. SS takes the stacked hidden states and applies spatial squeeze to obtain per-block importance scores through an excitation network. FS then applies DCT-based frequency selection to each block before fusion, replacing the standard global average pooling.

**Design Tradeoffs:** The method trades minimal additional parameters and computation for significant semantic gains. Using multiple blocks increases representational capacity but requires careful weighting to avoid diluting strong representations. The DCT-based frequency selection adds complexity but reduces semantic loss compared to simple averaging.

**Failure Signatures:** Performance degrades sharply if [CLS] pooling is used instead of average pooling, as the linear relationships needed for weighted fusion are lost. The method also shows minimal improvement with n=1 or m=1 configurations, as these reduce to the baseline approach. Incorrect DCT basis indexing can cause squeeze mismatches and degraded performance.

**First Experiments:** 1) Implement and test the Spatial Selection module with n=3 blocks to verify adaptive weighting works. 2) Add Frequency Selection with m=4 DCT components and compare against global average pooling. 3) Train the full S2Sent model on SimCSE and evaluate on STS-B development set to verify the 125-step evaluation schedule.

## Open Questions the Paper Calls Out
- **Generative LLM adaptation:** Whether generative LLM-based sentence representations can support dynamic cross-layer representation fusion remains an open question worth exploring, as S2Sent is validated specifically on encoder-based architectures.
- **Non-NLP domain applicability:** The framework's potential applicability to stack-based or Inception-based encoding structures in other domains is claimed but not experimentally validated.
- **CLS token compatibility:** The strict dependence on average pooling and the sharp performance decline when using [CLS] tokens suggests the framework may not be compatible with systems optimized for CLS representations.
- **Learnable frequency selection:** The manual selection of lowest m frequency components may not be optimal, and a learnable frequency mask could potentially provide better semantic preservation.

## Limitations
- Evaluation relies on standard STS benchmarks without comparison to more recent sentence embedding methods
- DCT-based frequency selection lacks explicit justification for basis index choices and exploration of alternative strategies
- Parameter efficiency claims may obscure the fact that overhead is applied per downstream task rather than being one-time cost
- Inference latency measurements lack absolute values for practical impact assessment

## Confidence

**High Confidence:** The core technical contribution of nested selectivity (SS+FS) is well-specified and reproducible. The claim that cross-block fusion improves semantic representation is supported by ablation studies.

**Medium Confidence:** Integration with different backbones and contrastive learning variants is demonstrated, but consistency across all configurations could be stronger. Scalability claims are reasonable but not exhaustively tested.

**Low Confidence:** The paper does not address potential overfitting to STS benchmarks or provide analysis of what semantic aspects are being captured differently by the nested selection mechanism.

## Next Checks
1. **Cross-Domain Transfer:** Evaluate S2Sent representations on non-STS tasks (e.g., NLI, paraphrase detection) to verify semantic generalization beyond similarity benchmarks.
2. **Frequency Basis Ablation:** Systematically vary the DCT basis selection beyond the "Low-m" configurations to determine if improvements are specific to low frequencies or the selection mechanism itself.
3. **Overhead Scaling Analysis:** Measure parameter and latency overhead when applying S2Sent to larger backbones (BERT-large, RoBERTa-large) and when using multiple frequency bands simultaneously to assess true scalability limits.