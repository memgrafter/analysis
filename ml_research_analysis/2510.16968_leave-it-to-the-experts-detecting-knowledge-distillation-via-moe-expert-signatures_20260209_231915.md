---
ver: rpa2
title: 'Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures'
arxiv_id: '2510.16968'
source_url: https://arxiv.org/abs/2510.16968
tags:
- distillation
- expert
- knowledge
- arxiv
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting whether a large language
  model (LLM) has been distilled from another, which is important for intellectual
  property protection and maintaining model diversity. The key innovation is exploiting
  the transfer of structural habits in MoE models, specifically internal routing patterns,
  as signatures of distillation.
---

# Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures

## Quick Facts
- arXiv ID: 2510.16968
- Source URL: https://arxiv.org/abs/2510.16968
- Authors: Pingzhi Li; Morris Yu-Chao Huang; Zhen Tan; Qingquan Song; Jie Peng; Kai Zou; Yu Cheng; Kaidi Xu; Tianlong Chen
- Reference count: 23
- Primary result: Detects MoE knowledge distillation via routing pattern signatures with >94% accuracy

## Executive Summary
This paper addresses the problem of detecting whether a large language model (LLM) has been distilled from another, which is important for intellectual property protection and maintaining model diversity. The key innovation is exploiting the transfer of structural habits in MoE models, specifically internal routing patterns, as signatures of distillation. The method, called Shadow-MoE, works by constructing proxy MoE representations of models (either directly for white-box MoE models or through auxiliary distillation for black-box models) and then analyzing two routing signatures: expert specialization (task-specific activation profiles) and expert collaboration (co-activation patterns between experts). Extensive experiments demonstrate the method achieves >94% detection accuracy across various scenarios, outperforming existing baselines that rely on self-identity or output similarity. The approach is robust to prompt-based evasion and works effectively in both white-box and black-box settings, achieving perfect 100% accuracy in the most challenging black-box scenario.

## Method Summary
Shadow-MoE detects knowledge distillation in MoE models by analyzing structural routing patterns that transfer during the distillation process. For white-box models, it directly extracts expert activation patterns from the MoE layers. For black-box models, it trains lightweight shadow MoE proxies to mimic the target's behavior, then extracts routing patterns from these proxies. The method computes two signature metrics: expert specialization profiles (domain-normalized activation frequencies) and expert collaboration matrices (co-activation patterns). These signatures are compared using permutation-invariant Wasserstein distances via Hungarian matching, with higher scores indicating higher likelihood of distillation. The approach works across different MoE architectures and demonstrates strong robustness to prompt-based evasion attempts.

## Key Results
- Achieves >94% detection accuracy across various scenarios and settings
- Outperforms baselines that rely on self-identity or output similarity (which achieve 0% accuracy)
- Works effectively in both white-box and black-box settings, achieving perfect 100% accuracy in the most challenging black-box scenario
- Routing signatures are robust to prompt-based evasion attempts

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation transfers "structural habits" — specifically, expert routing patterns — from teacher to student models, not just input-output mappings. The distillation process causes the student model to develop similar expert specialization patterns (which experts activate for which domain inputs) as the teacher. During training, the student learns not only what to output but implicitly how to route computation through its expert layers, creating a persistent structural fingerprint. Core assumption: Routing patterns are sufficiently stable and distinctive to serve as fingerprints; distillation preserves them more strongly than independent training on similar data. Evidence: "We present a KD detection framework... by exploiting an overlooked signal: the transfer of MoE 'structural habits', especially internal routing patterns." Break condition: If routing patterns can be easily randomized or obfuscated during distillation without degrading performance, the mechanism fails.

### Mechanism 2
Expert collaboration patterns (which experts co-activate together) provide complementary structural fingerprints that distinguish distilled from independently trained models. Co-activation patterns capture how experts jointly process inputs, reflecting the collaborative structure learned during training. Distilled students inherit similar expert collaboration matrices as their teachers, even when expert indices are permuted. Core assumption: Collaboration patterns are domain-agnostic enough to transfer across tasks but distinctive enough to differentiate model provenance. Evidence: "Expert Collaboration... captures pairwise expert co-activation frequencies... [normalized] so that each column sums to 1." Break condition: If expert collaboration patterns are highly sensitive to model scale, architecture variants, or fine-tuning, the fingerprint becomes unreliable.

### Mechanism 3
Shadow-MoE proxies trained via lightweight distillation preserve sufficient routing pattern information to detect distillation relationships even when the original models are black-box. When only API access is available, train a proxy MoE model to mimic the target's input-output behavior. The proxy's routing patterns, learned through this auxiliary distillation, inherit structural similarities that reflect whether the original target was itself distilled. Core assumption: Auxiliary distillation onto a fixed MoE architecture preserves relative routing pattern differences between distilled and non-distilled models. Evidence: "Remarkably, our method achieves perfect detection accuracy of 100% across all task sets in the pure black-box setting... even surpassing its already strong performance in the semi-black-box setting." Break condition: If proxy distillation introduces routing artifacts that swamp the original structural signal, detection fails.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing:** Why needed here: The entire method depends on understanding how MoE layers route tokens through expert subsets via gating networks. Quick check: Can you explain why permutation invariance is required when comparing two MoE models' routing patterns?

- **Knowledge Distillation (KD) Taxonomy:** Why needed here: The paper focuses on output text-level KD but references logits-level and hidden-state-level KD; understanding these distinctions clarifies what structural signals transfer. Quick check: Why would output text-level KD transfer routing patterns if the student architecture differs from the teacher?

- **Wasserstein Distance & Optimal Transport:** Why needed here: The detection metric uses permutation-invariant Wasserstein distances to compare routing distributions; implementation requires understanding this metric. Quick check: Why is Wasserstein distance preferred over KL divergence for comparing expert activation distributions with arbitrary expert indices?

## Architecture Onboarding

- **Component map:** Calibration dataset (280 prompts) -> Shadow-MoE proxy trainer (3 epochs, lr=5e-6) -> Signature extractor (specialization + collaboration profiles) -> Distance computer (Hungarian algorithm for Wasserstein distances) -> Pair classifier (aggregated scores)

- **Critical path:** 1. Query both suspected teacher and student on calibration prompts → collect outputs 2. If black-box, train Shadow-MoE proxies for each (3 epochs, LR=5e-6, batch=256) 3. Extract routing activations from last MoE layer → build specialization & collaboration profiles 4. Compute permutation-invariant Wasserstein distances via Hungarian matching 5. Compare aggregated scores for pairwise classification

- **Design tradeoffs:** Last layer vs. all layers: Paper uses last layer only (94% accuracy vs. 85% median, 46% first layer) — deeper layers capture more semantic routing but may miss early domain-specific signals. Proxy architecture choice: Using same architecture (Moonlight-16B) for all proxies improves comparability but may not scale to larger targets. Calibration set domain: General instruction-following prompts outperform domain-specific ones (Figure 4), counterintuitively suggesting routing transfer is more detectable in instruction-processing rather than task-specific tokens. Computational cost: O(E⁴ + E³D) per model pair; for E=64 experts, D=9 domains, this is tractable but scales poorly to larger MoEs.

- **Failure signatures:** Cross-domain mismatch: Code Contest task shows inverted signal in semi-black-box setting (Figure 2) — distilled model has *higher* distance than non-distilled on specialization metric. Self-identity baseline fails completely: 0% accuracy across all tasks, suggesting identity prompts are unreliable for structural detection. Small decision margins: Some tasks show only 4-5% distance reduction (Figure 2), risking false positives near threshold.

- **First 3 experiments:** 1. Baseline sanity check: Implement Linear/BERT embedding baselines on provided checkpoints; verify you reproduce ~50% accuracy before attempting Shadow-MoE 2. Layer ablation: Extract signatures from first/median/last layers on the white-box student setting; confirm last layer superiority before committing to architecture 3. Proxy architecture sensitivity: Train Shadow-MoE proxies using a different base MoE (e.g., OLMoE instead of Moonlight) and measure accuracy degradation; this tests whether results are architecture-dependent

## Open Questions the Paper Calls Out

- **Cross-architecture generalization:** Can structural signatures be extended to detect distillation directly in dense models without relying on MoE proxy architectures? The authors list "extending signature to dense model and incorporate additional structure cues" as a primary future direction. Dense models lack the discrete expert routing pathways used to generate the fingerprints in this study, making direct application impossible.

- **Alternative distillation channels:** Do expert routing signatures persist when distillation is performed via reinforcement learning (RL) or reward-model-mediated channels? The paper identifies "Alternative distillation channels for detecting reward-model-mediated or RL-based distillation" as a natural next step. The current method validates text-level KD; it is unknown if optimizing for a reward signal transfers the teacher's routing habits similarly to mimicking output distributions.

- **Defensive mechanisms:** Can defensive mechanisms such as structural watermarks or routing randomization effectively evade this detection method? The Conclusion suggests exploring "defensive mechanisms (e.g. structural watermarks or routing randomization) to deter unauthorized distillation." The paper tests robustness against prompt-based evasion but does not evaluate active countermeasures designed to obfuscate routing patterns during the training process.

## Limitations
- The method relies on MoE architecture-specific routing patterns, limiting direct application to dense models
- Claims about routing pattern stability and transferability require more rigorous validation across different training scenarios
- The approach has not been tested against active countermeasures like routing obfuscation or expert randomization

## Confidence
- **High confidence**: The detection method's performance metrics (94-100% accuracy) are well-supported by the experimental results across multiple settings. The technical implementation details for signature extraction and Wasserstein distance computation are clearly specified.
- **Medium confidence**: The claim that routing patterns transfer during knowledge distillation is plausible given the experimental results, but the underlying mechanism requires further validation. The claim about expert collaboration patterns providing complementary fingerprints is supported by ablation studies but lacks independent verification.
- **Low confidence**: The paper's assertion that the method is robust to prompt-based evasion and can generalize to arbitrary MoE architectures is not adequately tested. The claim that routing patterns are stable enough to serve as persistent fingerprints across different training scenarios requires more rigorous validation.

## Next Checks
1. **Architecture Transfer Test**: Train Shadow-MoE proxies using different base MoE architectures (e.g., OLMoE instead of Moonlight) for the same target models and measure accuracy degradation to test whether results are architecture-dependent.

2. **Routing Obfuscation Countermeasure**: Implement a distillation protocol that explicitly randomizes expert routing (e.g., through adversarial routing training) and test whether the detection method maintains its accuracy.

3. **Cross-Scale Validation**: Test the detection method on MoE models with different numbers of experts (e.g., 16 vs 64 experts) to verify whether the routing signatures scale appropriately and maintain discriminative power.