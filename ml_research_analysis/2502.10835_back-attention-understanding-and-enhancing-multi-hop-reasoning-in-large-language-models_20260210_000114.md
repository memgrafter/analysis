---
ver: rpa2
title: 'Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language
  Models'
arxiv_id: '2502.10835'
source_url: https://arxiv.org/abs/2502.10835
tags:
- attention
- arxiv
- layer
- back
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how large language models perform multi-hop
  reasoning and identifies key mechanisms underlying their success and failure. The
  authors introduce "logit flow," a method to trace how prediction logits propagate
  across layers and positions, revealing four stages in single-hop knowledge prediction:
  entity subject enrichment, entity attribute extraction, relation subject enrichment,
  and relation attribute extraction.'
---

# Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2502.10835
- Source URL: https://arxiv.org/abs/2502.10835
- Reference count: 23
- Primary result: Back attention improves multi-hop reasoning accuracy from 46.9% to 77.0% on Llama3.1-8B across five datasets

## Executive Summary
This paper analyzes how large language models perform multi-hop reasoning and identifies key mechanisms underlying their success and failure. The authors introduce "logit flow," a method to trace how prediction logits propagate across layers and positions, revealing four stages in single-hop knowledge prediction. Analyzing incorrect two-hop reasoning cases, they find that failures often occur when relation attribute extraction stages capture conflicting logits, reducing prediction accuracy. To address this, they propose "back attention," which allows lower layers to leverage higher-layer hidden states from different positions during attention computation. This approach improves a 1-layer transformer's performance to match that of a 2-layer transformer and, when applied to four large language models, increases accuracy across five reasoning datasets.

## Method Summary
The authors analyze multi-hop reasoning in decoder-only LLMs by decomposing single-hop prediction into four stages: entity subject enrichment, entity attribute extraction, relation subject enrichment, and relation attribute extraction. They identify that two-hop reasoning failures occur when relation attribute extraction at high layers captures conflicting logits for intermediate entities. To address this, they propose back attention, which computes queries from a lower source layer and keys/values from higher target layers across positions, adding the output to the source layer input and recomputing the forward pass. This allows earlier activation of compositional reasoning parameters.

## Key Results
- 1-layer transformer with back attention achieves 93.8% accuracy vs 92.5% for 2-layer baseline on 2-digit arithmetic
- Llama3.1-8B average accuracy increases from 46.9% to 77.0% across five reasoning datasets
- Back attention enables lower layers to access higher-layer hidden states from different positions, improving compositional reasoning

## Why This Works (Mechanism)

### Mechanism 1: Four-Stage Knowledge Prediction Pipeline
- Claim: Single-hop factual prediction in decoder-only LLMs follows a decomposable four-stage information flow from entity enrichment to final answer extraction.
- Mechanism: The pipeline proceeds as: (A) FFN neurons at entity position enrich entity subject features → (B) Attention neurons at entity position extract entity attributes → (C) FFN neurons at relation/last positions enrich relation subject features → (D) Attention + FFN neurons at relation/last positions perform relation attribute extraction for final prediction. Stages A-B operate at lower layers (≈7-20 in Llama2-7B), while C-D operate at deeper layers (≈20-31).
- Core assumption: Knowledge is stored differentially across layers, with entity features in early layers and relation-bound predictions in later layers; this decomposition generalizes beyond the tested prompts.
- Evidence anchors:
  - [abstract] "We identify four distinct stages in single-hop knowledge prediction: (A) entity subject enrichment, (B) entity attribute extraction, (C) relation subject enrichment, and (D) relation attribute extraction."
  - [Section 3.3] "The entity position cannot distinguish the correct answer and the conflicting answer, while the relation and last positions' logit difference start to increase after the entity attribute extraction stage."
  - [corpus] Weak direct corpus support; neighbor papers address multi-hop but not this specific four-stage decomposition.
- Break condition: If entity and relation processing occur in parallel rather than sequentially across layers, or if attention dominates throughout without FFN specialization, the stage model does not hold.

### Mechanism 2: Relation Attribute Extraction Conflict in Multi-Hop Reasoning
- Claim: Two-hop reasoning failures occur when relation attribute extraction at high layers of the first relation position (r1) activates conflicting logits for the intermediate entity (e2) rather than propagating information needed for the second hop.
- Mechanism: In queries like "e1's r1's r2 is" → "e3", the r1 position's high-layer hidden states remain identical to single-hop "e1's r1 is" prompts due to autoregressive constraints. When these high-layer features are extracted late (layer ≈28), they strongly activate "e2 features" parameters but only weakly activate "e2 features & r2 → e3" parameters, causing the model to predict e2 instead of e3.
- Core assumption: The failure mode is attributable to information flow timing rather than missing knowledge; both individual hops are assumed to be retrievable.
- Evidence anchors:
  - [abstract] "Failures often stem from the relation attribute extraction stage, where conflicting logits reduce prediction accuracy."
  - [Section 4] "In the false cases, the influence at the r1 position is significantly stronger...when the high-layer information at the r1 position is extracted...it inadvertently reinforces the probability of 'e2', leading to lower accuracy."
  - [Section 4] "Among 568 cases, 52.3% correctly predict 'e3', 42.4% predict 'e2' (the answer of 'e1's r1'), and 5.3% predict the answer of 'e1's r2'."
  - [corpus] Layer-Order Inversion paper (arxiv 2601.03542) addresses latent multi-hop reasoning with a different hypothesis (hop-aligned circuits), suggesting mechanism is contested.
- Break condition: If failures are instead caused by missing compositional knowledge representations or attention head capacity limits rather than temporal/logit conflict, this mechanism is incomplete.

### Mechanism 3: Back Attention for Cross-Layer Feature Recovery
- Claim: Allowing lower layers to attend to higher-layer hidden states from prior positions enables earlier activation of compositional reasoning parameters, improving multi-hop accuracy.
- Mechanism: After the standard forward pass computes all hidden states, back attention computes queries from a lower source layer (hs) and keys/values from a higher target layer (ht) across positions. The output is added to the source layer input, and the forward pass recomputes. This allows, for example, the last position's layer 6 to access the r1 position's layer 27-30 features, activating "e2 features & r2 → e3" parameters at layers 7-20 rather than only at layers 28-31.
- Core assumption: The model already contains compositional reasoning parameters that are underutilized due to layer ordering; back attention unlocks existing capacity rather than learning new representations.
- Evidence anchors:
  - [abstract] "Back attention enables lower layers to leverage higher-layer hidden states from different positions during attention computation...a 1-layer transformer achieves the performance of a 2-layer transformer."
  - [Section 5] "Back attention effectively learns to recover 'mother' position's 27-30 layers' hidden states into the last position's 6th layer."
  - [Table 1] Llama3.1-8B average accuracy increases from 46.9% to 77.0% with layer-6 back attention across five datasets.
  - [corpus] G-MemLLM (arxiv 2602.00015) proposes gated latent memory for long-context reasoning—a related but distinct cross-layer mechanism.
- Break condition: If gains derive primarily from increased parameter count or training signal rather than cross-layer information flow, or if benefits do not scale to deeper models/larger datasets, mechanism may be misattributed.

## Foundational Learning

- Concept: **Residual Stream Decomposition**
  - Why needed here: Logit flow analysis relies on decomposing the final hidden state as a sum of layer outputs (attention + FFN contributions) to attribute prediction logits to specific neurons.
  - Quick check question: Can you write the equation showing how the residual stream at position i combines embeddings, attention outputs, and FFN outputs across layers?

- Concept: **Logit Lens / Unembedding Projection**
  - Why needed here: The paper's core interpretability method projects intermediate hidden states through the unembedding matrix to trace when/where answer logits emerge.
  - Quick check question: Given a hidden state h at layer l, how would you compute the log probability of token s using the unembedding matrix?

- Concept: **Autoregressive Causal Masking in Decoder-Only Transformers**
  - Why needed here: The four-stage mechanism and multi-hop failure mode are directly constrained by the inability of earlier positions to attend to later tokens.
  - Quick check question: Why can't the "mother" token position in "Mozart's mother's spouse is" attend to the "spouse" token during the forward pass?

## Architecture Onboarding

- Component map:
  - Standard transformer: Input → Embedding → [Attention + FFN] × L layers → Unembedding → Logits
  - Back attention augmentation: After standard forward pass, add Back Attention module that computes queries from source layer (e.g., layer 6), keys/values from target layer stack (layers 6-L), adds output to source layer input, recomputes forward pass from source layer onward
  - Back attention parameters: Wq_B, Wk_B, Wv_B, Wo_B with reduced dimension (160 vs 440 for main layers in experiments)

- Critical path:
  1. Standard forward pass completes, storing all hidden states h_l_i for layers l=0..L and positions i=1..T
  2. Back attention computes B_i = Softmax(hs_i × Wq_B × (ht × Wk_B)^T / √d') × (ht × Wv_B) × Wo_B for each position
  3. Add B_i to source layer input: hs_i ← hs_i + B_i
  4. Recompute forward pass from source layer to final layer
  5. Generate prediction from recomputed final hidden state

- Design tradeoffs:
  - Layer placement: Layer 6 optimal in Llama-7B; layers 0-5 show instability, layers 7+ show declining gains (Figure 5)
  - Parameter efficiency: Back attention adds only 0.002% parameters to Llama3-8B but requires storing all intermediate hidden states (memory overhead)
  - Training vs. fine-tuning: Paper demonstrates both from-scratch training and fine-tuning on pretrained LLMs; fine-tuning freezes LLM parameters

- Failure signatures:
  - Low improvement on tasks without compositional structure (back attention may add noise)
  - Attention scores concentrated on irrelevant positions (visualization shows incorrect feature recovery)
  - Accuracy fluctuation at shallow layers (Figure 5 instability at layers 0-5)
  - No improvement when individual hops are already incorrect (requires base knowledge present)

- First 3 experiments:
  1. **Ablation on layer placement**: Add back attention to each layer 0 through L-1 individually on a held-out multi-hop reasoning dataset; plot accuracy vs. layer to identify optimal placement and verify layer-6 peak replicates.
  2. **Attention score analysis**: Visualize back attention scores across positions and source-target layer pairs on correct vs. incorrect predictions; verify that successful cases show high attention from last position's low layer to r1 position's high layers.
  3. **Parameter scaling test**: Vary back attention dimension (e.g., 64, 128, 160, 256) and measure accuracy/parameter tradeoff; determine if gains saturate or continue with larger capacity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying back attention to multiple layers simultaneously yield greater reasoning improvements than the single-layer application tested in this study?
- Basis in paper: [explicit] The authors state in the Limitations section that "back attention can also be extended to two or more layers, potentially yielding even greater improvements."
- Why unresolved: The experiments in this paper restricted the application of back attention to a single layer (e.g., the 6th layer) to demonstrate a proof of concept.
- What evidence would resolve it: Ablation studies applying back attention to various combinations of layers (e.g., layers 5-7) and comparing the resulting accuracy and training convergence against the single-layer baseline.

### Open Question 2
- Question: Do the four stages of single-hop prediction and the "hopping too late" failure mechanism generalize to complex reasoning tasks beyond factual knowledge queries?
- Basis in paper: [explicit] The authors acknowledge that the analysis "primarily focuses on single-hop and two-hop knowledge queries" and "other types of reasoning tasks might involve different mechanisms."
- Why unresolved: The mechanistic interpretability analysis was conducted specifically on entity-relation data (e.g., "Mozart's mother"), leaving it uncertain if the same logit flow patterns apply to logical or mathematical reasoning.
- What evidence would resolve it: Applying the logit flow method to datasets involving symbolic reasoning or multi-step logic (e.g., GSM8K) to check for the presence of the entity/relation enrichment and extraction stages.

### Open Question 3
- Question: What is the inference latency cost introduced by the back attention mechanism's requirement to recompute the forward pass?
- Basis in paper: [inferred] The paper highlights deployment challenges due to computational costs but only validates the parameter efficiency of back attention, while the method explicitly requires recomputing the forward pass "again."
- Why unresolved: While the parameter count is low (0.002% of the model), the necessity of a second forward pass implies a potential doubling of inference time, which was not quantified.
- What evidence would resolve it: Benchmarks comparing the time-to-first-token and tokens-per-second of standard LLMs against those utilizing back attention during inference.

## Limitations
- The four-stage pipeline may be an artifact of prompt structure rather than fundamental reasoning mechanics
- Back attention's effectiveness is tied to a specific failure mode that may not generalize to all multi-hop reasoning scenarios
- The distinction between training from scratch and fine-tuning contributions to observed gains is not systematically explored

## Confidence

**High Confidence**: The empirical demonstration that back attention improves accuracy on tested datasets is well-supported by the presented results, particularly the 1-layer transformer matching 2-layer performance and the substantial gains on Llama3.1-8B across five datasets.

**Medium Confidence**: The identification of the four-stage knowledge prediction pipeline is plausible given the logit flow analysis, but the generalizability beyond the tested prompt structures and datasets remains uncertain without broader validation.

**Low Confidence**: The claim that all multi-hop reasoning failures stem from relation attribute extraction conflicts is too narrow given the complexity of reasoning failures observed in LLMs, and the paper does not systematically explore alternative failure modes or competing explanations.

## Next Checks
1. **Cross-Dataset Generalization**: Apply back attention to at least three additional multi-hop reasoning datasets with different prompt structures (e.g., natural language questions, visual reasoning tasks) to test whether gains persist outside the arithmetic and TwoHop datasets.

2. **Failure Mode Analysis**: Systematically categorize reasoning failures in both baseline and back attention models across multiple datasets to determine whether the "e2 instead of e3" pattern is predominant or if other failure modes (missing knowledge, attention capacity limits, compositional reasoning gaps) remain unaddressed.

3. **Layer-by-Layer Ablation**: Beyond testing different back attention layer placements, perform detailed ablation studies removing specific components (e.g., back attention only from r1 to last position, or only from high to low layers) to isolate which aspects of the mechanism drive improvements versus potential overfitting to specific dataset patterns.