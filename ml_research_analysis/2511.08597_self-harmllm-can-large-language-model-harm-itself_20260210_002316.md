---
ver: rpa2
title: 'Self-HarmLLM: Can Large Language Model Harm Itself?'
arxiv_id: '2511.08597'
source_url: https://arxiv.org/abs/2511.08597
tags:
- evaluation
- success
- harmful
- jailbreak
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether an LLM can generate a mitigated
  query that bypasses its own guardrails when reused as input. The proposed Self-HarmLLM
  scenario uses an LLM to transform a harmful query into a Mitigated Harmful Query
  (MHQ) that preserves the original intent but obscures its harmful nature.
---

# Self-HarmLLM: Can Large Language Model Harm Itself?
## Quick Facts
- arXiv ID: 2511.08597
- Source URL: https://arxiv.org/abs/2511.08597
- Reference count: 35
- One-line primary result: LLMs can bypass their own guardrails by generating queries that evade detection, with up to 65% transformation success and 98% jailbreak success in certain conditions.

## Executive Summary
This paper introduces the Self-HarmLLM scenario, demonstrating that an LLM can generate queries that bypass its own guardrails when reused as input in a separate session. The attack works by having Session A transform a harmful query into a Mitigated Harmful Query (MHQ) that preserves the original intent while obscuring harmful expressions. Session B then receives this MHQ as fresh input, potentially producing a harmful response. Experiments show that mitigation strategies (Zero-shot and Few-shot) can successfully transform harmful queries into MHQs, with DeepSeek-R1-Distill-Qwen-7B showing particularly high vulnerability. The study also reveals that prefix-based automated evaluation significantly overestimates jailbreak success compared to human evaluation, highlighting the need for more robust evaluation methods.

## Method Summary
The Self-HarmLLM attack uses a two-stage approach with 110 harmful queries from the HarmfulQA dataset across 11 categories. Session A transforms harmful queries into Mitigated Harmful Queries (MHQs) using Zero-shot or Few-shot system prompts, while Session B receives these MHQs as new inputs to test guardrail effectiveness. Three conditions are tested: Base (direct query), Zero-shot mitigation (4-step instructions), and Few-shot mitigation (4-step instructions plus 3 examples). Evaluation combines prefix-based automated detection (checking for refusal phrases) and human assessment of transformation success and harmfulness. The study tests GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B models.

## Key Results
- GPT-3.5-turbo achieved up to 33% jailbreak success with Zero-shot mitigation and 41% with Few-shot mitigation
- LLaMA3-8B-instruct reached up to 33% jailbreak success in Zero-shot condition
- DeepSeek-R1-Distill-Qwen-7B achieved up to 98% jailbreak success in the Base condition and 93% with Few-shot mitigation
- Prefix-based automated evaluation overestimated jailbreak success by an average of 52% compared to human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A model can generate queries that bypass its own guardrails because it understands its own detection boundaries.
- Mechanism: Session A transforms a harmful query into a Mitigated Harmful Query (MHQ) that preserves intent but obscures harmful expressions. Session B receives this MHQ without context from Session A, treating it as a new benign-seeming input.
- Core assumption: The model's internal representation of its own guardrail boundaries enables it to produce outputs that evade those same guardrails.
- Evidence anchors:
  - [abstract] "An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed."
  - [section 3.1] "Session A transforms the HQ into MHQ. At this stage, the core intent of the query is retained, but the expression is obfuscated in a way that is difficult for guardrails to detect."
  - [corpus] Related work on jailbreak attacks confirms guardrails fail to address "subtle, reasoning-driven vulnerabilities" (Turning Logic Against Itself), but does not directly validate self-referential bypass mechanisms.
- Break condition: If guardrails were session-stateful (shared memory across sessions) or if detection operated on semantic intent rather than surface expression, this mechanism would fail.

### Mechanism 2
- Claim: Few-shot prompting improves transformation success rates by providing concrete examples of how to reframe harmful queries.
- Mechanism: Including example transformations in the system prompt guides the model to consistently produce MHQs that balance intent preservation with expression obfuscation.
- Core assumption: Providing explicit transformation examples improves the model's ability to generate guardrail-evasive queries.
- Evidence anchors:
  - [abstract] "up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition" versus "up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition"
  - [table 1b] GPT-3.5-turbo Few-shot: 46.36% TSR vs Zero-shot 29.09%; LLaMA3 Few-shot: 21.82% vs Zero-shot 11.82%
  - [corpus] No direct corpus validation for few-shot mitigation specifically; related work focuses on attack optimization rather than prompting strategies.
- Break condition: If the model over-generalizes from examples and produces overly sanitized queries that lose harmful intent, jailbreak success would decline despite higher transformation rates.

### Mechanism 3
- Claim: Prefix-based automated evaluation systematically overestimates jailbreak success because it conflates absence of refusal phrases with harmful output.
- Mechanism: Automated evaluation checks for prefixes like "I'm sorry" or "I cannot assist." Models that refuse indirectly or explain refusals without these phrases are incorrectly classified as successful jailbreaks.
- Core assumption: Absence of specific refusal prefixes correlates with harmful content generation.
- Evidence anchors:
  - [abstract] "Prefix-based automated evaluation overestimated jailbreak success by an average of 52% compared to human evaluation"
  - [section 5.1.1] "the model expressed its refusal in ways other than using specific prefixes... it would refuse indirectly or explain the reason for its refusal"
  - [corpus] Weak corpus signal; related jailbreak papers use varied evaluation methods but don't systematically compare prefix-based vs human evaluation accuracy.
- Break condition: If models were fine-tuned to always use standardized refusal prefixes when declining harmful requests, prefix-based evaluation would become accurate.

## Foundational Learning

- Concept: **Jailbreak Attacks**
  - Why needed here: Self-HarmLLM is a novel jailbreak variant. Understanding baseline jailbreak mechanisms (prompt injection, role-playing, query optimization) provides context for how this approach differs.
  - Quick check question: Can you explain why the Self-HarmLLM attack requires no external query crafting?

- Concept: **Session-Based Statelessness**
  - Why needed here: The attack exploits that LLM sessions do not share context. Guardrails reset between sessions, allowing an MHQ from Session A to bypass Session B's defenses.
  - Quick check question: What would happen if Session B had access to Session A's conversation history?

- Concept: **Automated vs Human Evaluation**
  - Why needed here: The paper's central claim about evaluation inaccuracy requires understanding the tradeoffs between scalable automated methods and accurate but costly human evaluation.
  - Quick check question: Why might a model refuse a harmful request without using standard refusal prefixes?

## Architecture Onboarding

- Component map: Harmful Query → Session A (Mitigation) → MHQ → Session B (Target) → Guardrail Check → Response → Evaluation (prefix + human)

- Critical path: Harmful Query → Session A transformation → MHQ → Session B input → Guardrail check → Response generation → Evaluation (prefix + human)

- Design tradeoffs:
  - Zero-shot vs Few-shot: Few-shot increases transformation success but requires example curation; Zero-shot is simpler but less reliable
  - Automated vs Human evaluation: Automated is scalable but overestimates success; Human is accurate but expensive and subjective
  - Model selection: Commercial models (GPT-3.5) have stronger guardrails but lower transformation rates; reasoning-focused models (DeepSeek-R1) show higher vulnerability

- Failure signatures:
  - **Transformation failure**: MHQ loses original harmful intent (evaluated as "failed" in human review)
  - **Guardrail detection**: Session B correctly identifies MHQ as harmful and refuses
  - **False positive evaluation**: Prefix-based evaluation marks response as successful jailbreak when human evaluation shows it was actually a refusal
  - **Base condition success**: DeepSeek-R1 shows 43.63% jailbreak success even without mitigation, indicating baseline guardrail weakness

- First 3 experiments:
  1. **Reproduce baseline discrepancy**: Run 20 harmful queries through GPT-3.5-turbo with prefix-based evaluation, then human review. Confirm the ~50% overestimation gap.
  2. **Test session independence assumption**: Verify that Session B cannot access Session A context in your deployment environment (check API documentation for conversation state behavior).
  3. **Ablate few-shot examples**: Test transformation success with 0, 1, 3, and 5 examples to find the point of diminishing returns for your target model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Self-HarmLLM scenario maintain its attack success rate in multi-turn conversational contexts rather than single-query sessions?
- Basis in paper: [explicit] Section 6.3 states, "This study focused on single queries, but in a real-world conversational setting, a multi-turn context could significantly impact the attack’s feasibility."
- Why unresolved: The current experimental design isolated queries in separate sessions without maintaining conversational history or context.
- What evidence would resolve it: Experimental results from multi-turn dialogues where the Mitigated Harmful Query (MHQ) is embedded within a longer interaction.

### Open Question 2
- Question: How can automated evaluation methods be improved to eliminate the 52% overestimation gap found in prefix-based approaches?
- Basis in paper: [explicit] The authors highlight a "need for a hybrid evaluation approach" in Section 6.3 because automated evaluation "consistently overestimated jailbreak success, with an average difference of 52%."
- Why unresolved: Prefix-based metrics rely on surface-level patterns (e.g., "I cannot assist") and fail to capture semantic harmfulness accurately.
- What evidence would resolve it: A new automated evaluation framework that shows high correlation with human evaluation results on the Self-HarmLLM dataset.

### Open Question 3
- Question: Does vulnerability to this attack vector scale with model size and reasoning capability?
- Basis in paper: [explicit] Section 6.3 calls for "Broader Model Spectrum" analysis to "analyze how the Self-Harm scenario varies with model size and architecture."
- Why unresolved: The study was limited to three specific models (GPT-3.5-turbo, LLaMA3-8B-instruct, DeepSeek-R1-Distill-Qwen-7B).
- What evidence would resolve it: Benchmarks across a wider range of models (e.g., GPT-4, smaller models) correlating parameter count or architecture type with transformation and jailbreak success rates.

## Limitations

- The human evaluation component lacks transparency regarding rater training, inter-rater reliability metrics, and specific instructions for assessing "harmfulness"
- The study only tests three model families without exploring variations in model size, training approaches, or domain-specific fine-tuning that could affect vulnerability patterns
- API parameters (temperature, top-p, max tokens) and detailed human evaluation protocols are not specified, limiting reproducibility

## Confidence

**High Confidence**: The core claim that session-based statelessness enables self-referential jailbreak attacks is theoretically sound and experimentally demonstrated. The mechanism (MHQ generation followed by re-input into a separate session) is clearly specified and the success rates, while model-dependent, are consistently measurable across conditions.

**Medium Confidence**: The comparative effectiveness of Zero-shot versus Few-shot mitigation strategies is supported by the data, but the lack of systematic ablation studies on prompt engineering (example quality, quantity, instruction specificity) limits generalizability. The claim about DeepSeek-R1-Distill-Qwen-7B's baseline vulnerability (43.63% Base condition success) appears robust but may reflect model-specific weaknesses rather than attack effectiveness.

**Low Confidence**: The evaluation methodology's accuracy claims are problematic. Without detailed human evaluation protocols, inter-rater reliability data, or validation against ground truth harmfulness, the reported success rates may be systematically biased. The assertion that prefix-based evaluation "overestimates" success assumes human evaluation is the gold standard, but this assumption isn't validated.

## Next Checks

1. **Evaluate inter-rater reliability and protocol clarity**: Conduct a validation study where multiple raters independently assess the same 50 MHQ responses. Calculate Cohen's kappa to measure agreement and refine evaluation instructions until achieving κ > 0.7. This will establish whether human evaluation is actually reliable enough to serve as ground truth.

2. **Test alternative automated evaluation methods**: Implement semantic similarity-based evaluation (comparing harmful query intent preservation using embeddings) and refusal detection based on model confidence scores rather than fixed prefixes. Compare these methods against human evaluation to determine if any automated approach achieves comparable accuracy with better scalability.

3. **Ablate prompt engineering factors**: Systematically vary the number of few-shot examples (0, 1, 3, 5, 10) and measure the relationship between transformation success rate and jailbreak success rate. This will reveal whether high transformation rates always correlate with successful jailbreaks, or if the model can generate "too safe" MHQs that preserve intent but cannot bypass guardrails.