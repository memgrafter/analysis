---
ver: rpa2
title: Data-Centric Lessons To Improve Speech-Language Pretraining
arxiv_id: '2510.20860'
source_url: https://arxiv.org/abs/2510.20860
tags:
- arxiv
- data
- preprint
- training
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the data-centric gap in speech-language model
  pretraining by systematically studying three key questions: how to process raw web
  audio into interleaved speech-text training data, how to construct synthetic datasets
  to augment web-crawled data, and how to interleave speech and text during training.
  The authors find that fine-grained interleaving of speech-text chunks significantly
  improves spoken question-answering (SQA) performance over coarse interleaving.'
---

# Data-Centric Lessons To Improve Speech-Language Pretraining

## Quick Facts
- **arXiv ID**: 2510.20860
- **Source URL**: https://arxiv.org/abs/2510.20860
- **Reference count**: 40
- **Primary result**: Fine-grained interleaving of speech-text chunks significantly improves SQA performance over coarse interleaving

## Executive Summary
This paper addresses the data-centric gap in speech-language model pretraining by systematically studying three key questions: how to process raw web audio into interleaved speech-text training data, how to construct synthetic datasets to augment web-crawled data, and how to interleave speech and text during training. The authors find that fine-grained interleaving of speech-text chunks significantly improves spoken question-answering (SQA) performance over coarse interleaving. They introduce synthetic datasets using LLM-based rewriting and TTS synthesis, which improve domain coverage and boost SQA accuracy by up to 7.2% when mixed with web-crawled data. Deterministic sampling of speech-text chunks during training further improves SQA by 1%. These data interventions close the modality gap between speech and text distributions and enhance topic diversity. Applying these insights, the authors pretrain SpeLangy, a 3.8B-parameter SpeechLM that outperforms models up to 3× larger by 10.2% average SQA accuracy across three benchmarks while maintaining strong text understanding. The results highlight the central role of data curation in speech-language pretraining.

## Method Summary
The authors study three data-centric interventions for speech-language pretraining: (1) processing raw web audio into fine-grained interleaved speech-text chunks using diarization and transcription with no speaker merging, (2) constructing synthetic datasets using LLM-based rewriting and TTS synthesis to augment web-crawled data, and (3) implementing deterministic alternating modality sampling during training. They train a 3.8B SpeechLM (conformer-based speech tokenizer + 3B dense LM backbone) on a mixture of 60% text and 40% speech-text data for 200k steps. The synthetic "Quest" dataset improves domain coverage while fine-grained interleaving (mean chunk length 5.2s vs 19.2s for coarse) and deterministic sampling (A,T,A,T,...) each provide incremental SQA accuracy gains. The resulting SpeLangy model achieves 10.2% average SQA accuracy improvement over larger models across three benchmarks.

## Key Results
- Fine-grained interleaving improves SQA accuracy by ~3% compared to coarse interleaving (mean chunk length 5.2s vs 19.2s)
- Deterministic alternating modality sampling improves SQA by 1% compared to stochastic sampling
- Synthetic "Quest" data mixing improves SQA accuracy by up to 7.2% and enhances domain coverage
- SpeLangy (3.8B) outperforms models up to 3× larger by 10.2% average SQA accuracy across three benchmarks
- Data interventions close the modality gap between speech and text distributions while maintaining text understanding

## Why This Works (Mechanism)
Fine-grained interleaving exposes the model to more frequent modality switches, forcing it to learn robust cross-modal representations rather than exploiting sequential patterns. The deterministic sampling ensures balanced exposure to both modalities and prevents the model from developing biases toward longer speech segments. Synthetic data augmentation addresses the domain coverage limitations of web-crawled data, particularly in education, finance, and technology domains where real-world audio is scarce. These interventions collectively force the model to develop stronger speech-language alignment capabilities by presenting more challenging and diverse training scenarios that better reflect the complexities of real-world spoken language understanding.

## Foundational Learning
- **Speech-Text Interleaving**: Alternating speech and text chunks during training forces the model to maintain cross-modal context and prevents modality-specific shortcuts
  - *Why needed*: Models trained on sequential speech-text data often develop sequential biases rather than true speech-language understanding
  - *Quick check*: Verify that training sequences alternate between speech and text chunks rather than grouping by modality
- **Fine-grained Chunking**: Breaking audio into smaller segments without speaker merging increases modality switch frequency and improves representation learning
  - *Why needed*: Coarse chunks allow models to exploit speaker or topic continuity rather than learning cross-modal alignment
  - *Quick check*: Measure mean chunk length distribution (fine-grained ~5.2s vs coarse ~19.2s)
- **Deterministic Sampling**: Alternating speech and text chunks ensures balanced modality exposure and prevents training bias
  - *Why needed*: Stochastic sampling can create modality imbalances that hurt cross-modal learning
  - *Quick check*: Verify training sequences follow A,T,A,T,... pattern rather than random sampling

## Architecture Onboarding

**Component Map**: Web audio -> Diarization -> Transcription -> Chunking -> Synthetic data generation -> Training data mix -> SpeechLM

**Critical Path**: Raw audio processing (diarization + transcription) -> Fine-grained chunking -> Deterministic interleaving -> Synthetic data augmentation -> SpeechLM training

**Design Tradeoffs**:
- Fine-grained chunking increases training complexity but improves cross-modal learning
- Synthetic data generation adds computational overhead but fills domain coverage gaps
- Deterministic sampling simplifies training dynamics but may reduce data diversity

**Failure Signatures**:
- Coarse interleaving (merged same-speaker chunks) reduces SQA accuracy by ~3%
- Stochastic modality sampling reduces modality switches, hurting SQA by ~1%
- Web-crawl only data shows poor domain coverage (entertainment/sports skewed)

**First Experiments**:
1. Implement speech tokenizer (conformer + FSQ at 12.5Hz) and initialize LM backbone
2. Build data pipeline: pyannote diarization → Whisper transcription → ROVER ensemble filtering → fine-grained chunking
3. Train with deterministic modality sampling (alternate A/T chunks) and evaluate SQA accuracy

## Open Questions the Paper Calls Out
**Open Question 1**: How can we construct an optimal data mixture of web-crawled and synthetic data that maximizes performance while managing complex interactions between mixing ratios and data repeats? The authors note that due to complex interactions between mixing ratios and data repeats, it is unclear how to construct an optimal mixture and leave such a data-mixing exploration for future work.

**Open Question 2**: How can we effectively detect and quantify test-set contamination within the raw audio modality, rather than relying solely on text-based n-gram overlap? Section K.4 highlights that measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants further research attention.

**Open Question 3**: Do the proposed data-centric interventions (e.g., fine-grained interleaving, synthetic data) transfer effectively to end-to-end generation tasks (audio-in, audio-out)? The authors ask whether their approaches transfer to the full end-to-end evaluation setting, as their current work focuses on SQA with text outputs.

## Limitations
- Conformer-based speech tokenizer architecture and pre-trained weights are not publicly available
- Web-crawled audio data sources are listed but not guaranteed accessible due to copyright restrictions
- Proprietary SIRI transcription model cannot be exactly reproduced; Whisper/Nemo substitutions may introduce performance gaps
- Limited ablation scope: no evaluation of alternative sampling strategies or chunk-length distributions beyond tested ranges

## Confidence
- **High Confidence**: Data-centric interventions (fine-grained interleaving, deterministic sampling, synthetic data mixing) directly improve SQA performance as measured by controlled ablation experiments
- **Medium Confidence**: SpeLangy's 10.2% average SQA accuracy improvement over larger models, given that this comparison relies on published results and implementation differences in evaluation protocols
- **Medium Confidence**: Claims about closing the modality gap and improving topic diversity, as these are inferred from training data characteristics rather than direct measurements

## Next Checks
1. Implement fine-grained vs coarse interleaving comparison using the same chunking pipeline and measure SQA accuracy delta on at least one benchmark
2. Verify deterministic alternating modality sampling (A,T,A,T,...) improves SQA by 1% compared to stochastic sampling using the same training setup
3. Replicate synthetic data mixing experiment by generating Quest-style synthetic data from knowledge-rich text domains and measuring domain coverage and SQA improvement when mixed with web-crawled data