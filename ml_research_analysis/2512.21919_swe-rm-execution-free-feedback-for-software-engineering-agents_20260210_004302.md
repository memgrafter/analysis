---
ver: rpa2
title: 'SWE-RM: Execution-free Feedback For Software Engineering Agents'
arxiv_id: '2512.21919'
source_url: https://arxiv.org/abs/2512.21919
tags:
- reward
- verifier
- training
- data
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SWE-RM, an execution-free reward model designed
  to provide fine-grained feedback for software engineering (SWE) agents. Unlike execution-based
  verifiers that rely on unit tests and offer sparse binary signals, SWE-RM uses a
  continuous scoring system that can evaluate entire trajectories without sandbox
  environments.
---

# SWE-RM: Execution-free Feedback For Software Engineering Agents

## Quick Facts
- arXiv ID: 2512.21919
- Source URL: https://arxiv.org/abs/2512.21919
- Reference count: 29
- Introduces SWE-RM, an execution-free reward model for software engineering agents that achieves state-of-the-art performance on SWE-Bench Verified

## Executive Summary
SWE-RM addresses the limitations of execution-based verifiers in software engineering by introducing an execution-free reward model that provides continuous, fine-grained feedback without requiring unit test execution or sandbox environments. The model leverages test-time scaling (TTS) performance, classification accuracy, and calibration as key properties for effective reward modeling. By using a 30B-parameter mixture-of-experts architecture with 256k context window, SWE-RM demonstrates significant improvements over execution-based approaches, achieving 62.0% success rate on SWE-Bench Verified compared to 51.6% for previous best methods.

## Method Summary
The SWE-RM architecture employs a 30B-parameter mixture-of-experts model with 3B activated parameters and a 256k context window, designed specifically for execution-free reward modeling in software engineering tasks. The model uses continuous scoring rather than binary signals, enabling fine-grained feedback throughout entire agent trajectories. Through extensive ablation studies on data scale, positive-to-negative ratios, policy mixtures, source composition, and context length, the authors identified optimal configurations for maximizing performance. The reward model is trained to predict the quality of software engineering agent trajectories using a carefully curated dataset of diverse code samples and their corresponding quality assessments.

## Key Results
- Achieves 62.0% success rate on SWE-Bench Verified (up from 51.6% baseline) and 74.6% on Qwen3-Coder-Max (up from 67.0%)
- Provides 3 absolute points improvement over execution-based feedback in reinforcement learning settings
- Demonstrates superior generalization across multiple SWE benchmarks while maintaining stable training dynamics
- Enables faster RL training compared to execution-based verifiers due to elimination of unit test execution overhead

## Why This Works (Mechanism)
SWE-RM works by replacing sparse binary feedback from unit test execution with continuous, fine-grained scoring that can evaluate entire trajectories without sandbox environments. The execution-free approach eliminates the computational overhead and brittleness associated with running code in sandboxed environments, while the continuous scoring provides richer feedback signals that enable more effective reinforcement learning. The model's large context window (256k tokens) allows it to capture complex dependencies and patterns in software engineering tasks that shorter context models would miss.

## Foundational Learning
- Test-time scaling (TTS): Why needed - determines how reward model performance scales with input size and complexity; Quick check - measure performance gains across different problem sizes
- Classification accuracy (AUC): Why needed - evaluates the model's ability to distinguish between good and bad solutions; Quick check - compare ROC curves across different reward models
- Calibration (ECE): Why needed - ensures the model's confidence scores align with actual performance; Quick check - plot reliability diagrams to assess calibration quality
- Mixture-of-experts architecture: Why needed - enables efficient scaling while maintaining performance; Quick check - analyze activation patterns across different expert layers
- Continuous vs binary feedback: Why needed - richer signals enable more effective learning; Quick check - compare learning curves between continuous and binary reward models

## Architecture Onboarding

Component map: Input Code -> Embedding Layer -> MoE Router -> Expert Networks -> Aggregation Layer -> Continuous Score Output

Critical path: The critical path flows from input code through the embedding layer, where the MoE router dynamically selects relevant expert networks based on input characteristics. The aggregated outputs from activated experts are then processed through the final aggregation layer to produce continuous reward scores.

Design tradeoffs: The 30B-parameter model with 3B activated parameters represents a balance between computational efficiency and performance, while the 256k context window provides sufficient capacity for complex software engineering tasks without excessive computational overhead. The mixture-of-experts approach enables better specialization across different code patterns compared to dense architectures.

Failure signatures: Poor calibration manifests as overconfidence in incorrect predictions, while inadequate TTS performance appears as degraded accuracy on longer or more complex code sequences. MoE routing failures can result in suboptimal expert selection, leading to inconsistent feedback quality across similar inputs.

Three first experiments:
1. Ablation study on context window size (64k, 128k, 256k, 512k) to identify optimal tradeoff between performance and computational cost
2. Comparison of continuous vs binary reward modeling on identical training data to quantify the impact of feedback granularity
3. Cross-domain transfer evaluation on non-SWE benchmarks to assess generalization capabilities beyond software engineering tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness of execution-free reward modeling for software engineering agents.

## Limitations
- Performance gains are primarily demonstrated within SWE-Bench benchmark suite, raising questions about generalizability to real-world production codebases
- Proprietary 30B-parameter architecture with 256k context window may limit accessibility and reproducibility for broader research community
- Claims of superior calibration and TTS performance may not translate equally well to domains outside software engineering tasks

## Confidence
High confidence in core architectural contributions and reported SWE-Bench Verified performance improvements, as these are verifiable through methodology and results. Medium confidence in generalization claims across multiple SWE benchmarks, given limited analysis of failure modes. Medium confidence in RL performance claims, as comparisons are primarily against execution-based verifiers. Low confidence in scalability claims beyond 30B parameter regime, as smaller or larger variants were not systematically explored.

## Next Checks
1. Evaluate SWE-RM's performance on real-world production codebases and compare feedback quality against human expert reviewers
2. Test model's calibration and TTS performance when applied to non-SWE domains such as data science notebooks, configuration management, or hardware description languages
3. Conduct systematic study of SWE-RM's behavior in distributed RL training scenarios with multiple agents to assess stability and convergence properties under different coordination schemes