---
ver: rpa2
title: 'Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey and
  Benchmark'
arxiv_id: '2402.02242'
source_url: https://arxiv.org/abs/2402.02242
tags:
- vision
- peft
- tuning
- proceedings
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey and benchmark of parameter-efficient
  fine-tuning (PEFT) methods for pre-trained vision models. The authors categorize
  existing methods into four types: addition-based, partial-based, unified-based,
  and multi-task tuning.'
---

# Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey and Benchmark

## Quick Facts
- arXiv ID: 2402.02242
- Source URL: https://arxiv.org/abs/2402.02242
- Reference count: 40
- One-line primary result: Comprehensive benchmark evaluating 25 PEFT algorithms across 24 image recognition, 3 video action recognition, and 3 dense prediction tasks, proposing a Performance-Parameter Trade-off (PPT) metric for fair comparison.

## Executive Summary
This paper provides a comprehensive survey and benchmark of parameter-efficient fine-tuning (PEFT) methods for pre-trained vision models. The authors categorize existing methods into four types: addition-based, partial-based, unified-based, and multi-task tuning. They introduce V-PEFT Bench, a unified benchmark evaluating 25 PEFT algorithms across diverse vision tasks. The benchmark proposes a Performance-Parameter Trade-off (PPT) metric for fair comparison. Key results show that DTL achieves the best PPT on VTAB-1k, while Mona surpasses full fine-tuning on MS COCO.

## Method Summary
The paper evaluates 25 PEFT algorithms across 24 image recognition, 3 video action recognition, and 3 dense prediction tasks using a unified benchmark called V-PEFT Bench. The benchmark uses pre-trained backbones including ViT-B/16 (ImageNet-21K), Video Swin, and Swin-B/L. The primary metric is the proposed Performance-Parameter Trade-off (PPT), calculated as PPT_M = M_t × exp(-log_{10}(P_M/C + 1)), where M_t is task performance, P_M is trainable parameters, and C=10^7. The evaluation pipeline involves freezing backbone weights, injecting PEFT modules, training on downstream tasks, and optionally merging weights for inference efficiency.

## Key Results
- DTL achieves the best PPT on VTAB-1k among all evaluated methods
- Mona surpasses full fine-tuning performance on MS COCO dense prediction task
- PEFT methods show varying effectiveness across task categories, with specialized methods performing better on structured tasks
- The PPT metric effectively captures the trade-off between performance and parameter efficiency

## Why This Works (Mechanism)

### Mechanism 1: Implicit Regularization via Parameter Freezing
Limiting trainable parameters acts as a regularizer, reducing overfitting on small downstream datasets and preventing forgetting of pre-trained knowledge. By freezing most backbone weights and optimizing only a small increment, the model is restricted from drastically altering feature extractors that captured general visual priors during pre-training.

### Mechanism 2: Input Space Steering via Visual Prompting
Prepended learnable tokens (prompts) steer frozen backbone's attention to extract task-relevant features without modifying model weights. Prompt tuning injects learnable vectors into the input sequence, shifting the input distribution to align with the pre-trained model's expectations for the downstream task.

### Mechanism 3: Structural Reparameterization for Inference Efficiency
Low-rank updates (e.g., LoRA) approximate full weight updates during training and can be merged into the backbone for inference. Trainable low-rank matrices are added to frozen weights, and because this is a linear addition, the matrices can be mathematically merged into the backbone before inference, leaving the model architecture identical to the original.

## Foundational Learning

- **Concept: Transformer Self-Attention (MHA)**
  - Why needed here: Most surveyed methods (Adapters, LoRA, Prompts) are inserted into or alongside Multi-Head Attention blocks. Understanding Q, K, V projections is required to grasp how methods modify attention.
  - Quick check question: If you insert a prefix into the Key (K) and Value (V) matrices but not the Query (Q), how does this change the attention score calculation?

- **Concept: Low-Rank Matrix Decomposition**
  - Why needed here: Methods like LoRA and LoRand rely on the assumption that weight updates can be decomposed into smaller matrices to reduce parameter count.
  - Quick check question: If a weight matrix is d × d and you use a rank r decomposition with r << d, what is the reduction in parameter count compared to full fine-tuning?

- **Concept: Transfer Learning vs. Domain Adaptation**
  - Why needed here: The paper distinguishes between "Natural," "Specialized," and "Structured" tasks and benchmarks generalization. Understanding the difference between fitting a new distribution vs. extracting existing features is crucial for selecting a method.
  - Quick check question: On a dataset with a large domain gap from ImageNet (e.g., medical imaging), would you expect simple Linear Probing to outperform an Adapter-based approach?

## Architecture Onboarding

- **Component map:** Pre-trained Vision Model (PVM) -> Frozen Backbone -> PEFT Module (Adapter/LoRA/Prompt) -> Trainable Head -> Output
- **Critical path:**
  1. Select PVM (e.g., ViT-B/16)
  2. Freeze backbone weights
  3. Inject PEFT modules (e.g., append LoRA to W_q, W_v)
  4. Initialize PEFT weights (LoRA: A=random, B=zero; Adapters: near-identity)
  5. Train on downstream task (only PEFT params + Head update)
  6. Merge: If using Reparameterization, merge weights before deployment

- **Design tradeoffs:**
  - Memory vs. Parallelism: Side-tuning offers max memory efficiency but may lack Adapter integration
  - Params vs. Performance: PPT metric quantifies this trade-off; DTL achieves high PPT (low params, high acc)

- **Failure signatures:**
  - Underfitting on Structured Tasks: Methods like BitFit may fail on tasks requiring geometric reasoning if frozen features lack spatial priors
  - Hyper-sensitivity: Performance can drop sharply with minor changes in rank or bottleneck dimension
  - Initialization Mismatch: Prefix Tuning with random initialization can introduce noise, hindering convergence

- **First 3 experiments:**
  1. Establish Baseline: Run Linear Probing vs. Full Fine-tuning on a target dataset to measure the "generalization gap"
  2. Ablation by Category: Compare one method from each taxonomy (e.g., Adapter vs. LoRA vs. VPT vs. BitFit) to identify the most efficient insertion point
  3. Scale Validation: Test the winning method on a dense prediction task (e.g., COCO) to verify if it scales beyond classification, checking for inference latency

## Open Questions the Paper Calls Out

### Open Question 1
How can the effectiveness of visual prompts, particularly unordered token-based prompts, be interpreted or explained in a human-understandable format?
Basis: Section 6.1 states visual prompts are learned as "unordered token-based prompts, which are difficult to translate into an understandable format." Unresolved because there's currently a lack of methodologies to map abstract visual prompt tokens back to semantic visual concepts. Evidence to resolve: Development of a framework that decomposes visual prompt embeddings into interpretable semantic attributes or generates natural language explanations.

### Open Question 2
How can parameter-efficient fine-tuning methods be adapted for autoregressive (AR) visual generative models?
Basis: Section 6.2 notes that while PEFT exists for diffusion models, "to date, there has been little to no research on the application of PEFT to AR models." Unresolved because architectural differences between AR models and standard discriminative or diffusion models require new adaptation strategies. Evidence to resolve: A PEFT algorithm specifically designed for AR visual models that maintains generation quality while significantly reducing trainable parameters.

### Open Question 3
How can the sensitivity to hyperparameters in PEFT methods be reduced or automated?
Basis: Section 6.3 highlights that performance is "heavily influenced by hyperparameter configurations" (e.g., bottleneck dimensions, learning rates), requiring "considerable effort and resources." Unresolved because manual tuning is resource-intensive, and optimal settings vary unpredictably across different downstream tasks. Evidence to resolve: A robust meta-learning algorithm or auto-tuning system that consistently identifies near-optimal PEFT configurations across diverse datasets without manual intervention.

## Limitations
- The primary limitation is reliance on codebase configurations for hyperparameters not fully specified in the paper text
- The PPT metric may not capture task-specific nuances, particularly for dense prediction tasks requiring spatial consistency
- The survey scope is limited to vision models, excluding cross-modal and generative applications despite mentioning them as future challenges

## Confidence
- **High:** Claims about DTL achieving best PPT on VTAB-1k and Mona outperforming full fine-tuning on COCO are directly supported by reported benchmark results
- **Medium:** Mechanism explanations (regularization, prompting, reparameterization) are logically sound but rely on interpretation of empirical results
- **Low:** Claims about future challenges (explainability, differential privacy) are speculative and not empirically validated within the paper

## Next Checks
1. Verify PPT score calculations using the exact normalization constant C=10,000,000 across multiple methods to ensure reproducibility
2. Test the hypothesis that low-rank updates are sufficient by comparing LoRA performance against methods using higher-rank or full-rank updates on the same tasks
3. Validate the transferability claim by evaluating top-performing methods (DTL, Mona) on a held-out specialized task (e.g., medical imaging) to test generalization beyond benchmark datasets