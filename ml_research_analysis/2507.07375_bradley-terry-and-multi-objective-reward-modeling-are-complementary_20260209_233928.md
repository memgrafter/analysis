---
ver: rpa2
title: Bradley-Terry and Multi-Objective Reward Modeling Are Complementary
arxiv_id: '2507.07375'
source_url: https://arxiv.org/abs/2507.07375
tags:
- reward
- multi-objective
- baseline
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in RLHF, where the policy exploits
  imperfections in the reward model rather than learning the intended behavior. Existing
  methods primarily focus on in-distribution settings and often fail in more challenging
  out-of-distribution (OOD) scenarios.
---

# Bradley-Terry and Multi-Objective Reward Modeling Are Complementary

## Quick Facts
- arXiv ID: 2507.07375
- Source URL: https://arxiv.org/abs/2507.07375
- Reference count: 40
- A unified reward modeling framework that jointly trains Bradley-Terry single-objective and multi-objective regression-based reward functions, improving robustness against reward hacking and enabling a 7B model to outperform a 70B baseline

## Executive Summary
This paper addresses reward hacking in reinforcement learning from human feedback (RLHF), where policies exploit imperfections in reward models rather than learning intended behaviors. The authors propose SMORM, a unified framework that combines Bradley-Terry single-objective ranking with multi-objective regression using a shared embedding space. By leveraging the complementary strengths of both objectives, SMORM enhances reward model robustness against out-of-distribution (OOD) scenarios while maintaining strong scoring capabilities. The framework demonstrates significant improvements in both robustness and performance, with empirical results showing that a 7B model trained with SMORM can match or exceed the performance of a 70B baseline model.

## Method Summary
SMORM introduces a unified reward modeling framework that jointly optimizes Bradley-Terry (BT) single-objective ranking and multi-objective regression objectives within a shared embedding space. The key innovation lies in the complementary training approach: the regression task refines the embedding space to improve the single-objective head's robustness against reward hacking in OOD settings, while BT-based training enhances the multi-objective head's scoring capability. This dual-objective training creates a more robust reward model that maintains performance across both in-distribution and challenging OOD scenarios. The framework is theoretically grounded and validated through extensive experiments demonstrating significant improvements over existing single-objective and multi-objective approaches.

## Key Results
- SMORM significantly improves reward model robustness against reward hacking in out-of-distribution settings
- The framework enables a 7B model to outperform a 70B baseline in multi-objective reward modeling
- Theoretical analysis confirms the complementary benefits of joint BT and regression training objectives

## Why This Works (Mechanism)
The effectiveness of SMORM stems from the complementary nature of its dual objectives. The Bradley-Terry component excels at pairwise preference ranking, providing robust discrimination between options in single-objective settings. The regression component captures multi-dimensional reward signals, enabling the model to handle complex, competing objectives. By training these objectives jointly with shared embeddings, each component reinforces the other: regression gradients refine the embedding space to reduce reward hacking vulnerabilities, while BT training improves the scoring precision of the multi-objective head. This mutual enhancement creates a reward model that is both more robust to distribution shifts and more capable of handling multiple objectives simultaneously.

## Foundational Learning
- **Bradley-Terry Model**: A probabilistic model for pairwise comparison data, essential for converting human preferences into trainable reward signals. Quick check: Understand how BT transforms preference pairs into likelihood functions for gradient-based optimization.
- **Reward Hacking**: When policies exploit reward model imperfections rather than learning intended behaviors. Quick check: Identify common reward hacking patterns in RLHF and how they manifest in practice.
- **Multi-Objective Optimization**: Balancing competing objectives in reward modeling, crucial for real-world applications where multiple goals must be simultaneously satisfied. Quick check: Understand Pareto optimality and how it applies to multi-objective reward design.
- **Out-of-Distribution Robustness**: Model performance on data that differs from training distribution, critical for practical deployment. Quick check: Recognize common OOD scenarios in RLHF and their impact on reward model reliability.
- **Shared Embedding Spaces**: Using common representations for multiple objectives to enable cross-task learning and mutual enhancement. Quick check: Understand how shared embeddings facilitate knowledge transfer between BT and regression objectives.

## Architecture Onboarding

**Component Map**: Preference Data → Shared Embedding Layer → BT Single-Objective Head + Multi-Objective Regression Head → Joint Loss Function → Reward Model Parameters

**Critical Path**: The core training pipeline involves processing preference pairs through shared embeddings, generating scores from both BT and regression heads, computing their respective losses, and updating model parameters through joint optimization. The shared embedding space is the critical component that enables cross-objective learning.

**Design Tradeoffs**: The unified architecture trades increased model complexity and computational overhead for improved robustness and multi-objective capability. The joint training objective requires careful balancing of BT and regression loss weights to prevent one objective from dominating.

**Failure Signatures**: Poor performance manifests as either (1) reward hacking susceptibility in OOD scenarios, where the single-objective head exploits model weaknesses, or (2) degraded multi-objective scoring capability, where the regression head fails to capture complex reward structures. Imbalanced loss weighting can cause one objective to overshadow the other.

**First Experiments**: 1) Ablation study removing either BT or regression component to quantify individual contributions. 2) OOD robustness test with gradually increasing distribution shift to measure reward hacking resistance. 3) Multi-objective preference alignment test comparing SMORM against single-objective baselines on complex reward landscapes.

## Open Questions the Paper Calls Out
The paper acknowledges several open questions: How does SMORM scale to extremely large and complex multi-objective spaces? What is the impact of different preference data quality and quantity on the framework's performance? How can the joint training objective be optimized for different task domains and objective complexities? These questions highlight areas for future research to further enhance the framework's applicability and performance.

## Limitations
- Evaluation primarily focuses on synthetic and controlled environments rather than real-world multi-objective scenarios
- Claims about 7B model outperforming 70B baseline require independent verification across diverse domains
- Theoretical analysis relies on simplifying assumptions about reward space that may not hold in practice

## Confidence
- **High Confidence**: The complementary benefits of joint BT and regression training are theoretically sound and empirically supported in controlled settings
- **Medium Confidence**: The robustness improvements against reward hacking are demonstrated but primarily in synthetic environments
- **Low Confidence**: The claim about matching or exceeding larger models' performance needs broader validation across diverse real-world tasks

## Next Checks
1. Test SMORM on real-world multi-objective RL tasks with complex, competing objectives to validate robustness claims beyond synthetic environments
2. Conduct ablation studies to quantify the individual contributions of BT and regression components to the overall performance gains
3. Evaluate SMORM's performance when trained with limited human preference data to assess its data efficiency claims in resource-constrained settings