---
ver: rpa2
title: 'AI Epidemiology: achieving explainable AI through expert oversight patterns'
arxiv_id: '2512.15783'
source_url: https://arxiv.org/abs/2512.15783
tags:
- expert
- risk
- outputs
- reliability
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AI Epidemiology presents a framework for governing and explaining\
  \ AI systems through population-level surveillance of outputs, bypassing the need\
  \ for mechanistic transparency into model computations. The approach standardizes\
  \ AI-expert interactions into structured fields\u2014risk level, alignment score,\
  \ and accuracy score\u2014that function as exposure variables predicting output\
  \ failure, analogous to how cholesterol predicts cardiac events."
---

# AI Epidemiology: achieving explainable AI through expert oversight patterns

## Quick Facts
- arXiv ID: 2512.15783
- Source URL: https://arxiv.org/abs/2512.15783
- Reference count: 12
- Primary result: AI Epidemiology framework enables explainable AI governance through population-level surveillance of outputs using structured expert oversight fields

## Executive Summary
AI Epidemiology introduces a novel framework for governing and explaining AI systems through population-level surveillance of outputs, eliminating the need for mechanistic transparency into model computations. The approach standardizes AI-expert interactions into structured fields—risk level, alignment score, and accuracy score—that function as exposure variables predicting output failure, analogous to how cholesterol predicts cardiac events. A feasibility study with 94% inter-rater reliability demonstrates that RAG-generated assessments can reliably measure these variables. The framework enables automatic audit trails, continuous model-agnostic governance, and semantic explanations of AI outputs by comparing them to historical patterns of expert intervention.

By systematizing expert oversight, AI Epidemiology democratizes AI governance, allowing domain experts to identify unreliable outputs before they cause harm without requiring machine learning expertise. The framework provides a practical solution to the challenge of governing AI systems in high-stakes domains where traditional approaches relying on mechanistic transparency or human review of every output are infeasible.

## Method Summary
AI Epidemiology standardizes AI-expert interactions into three structured fields: risk level (likelihood of output causing harm), alignment score (consistency with expert expectations), and accuracy score (correctness of output). These fields function as exposure variables that predict output failure, similar to how medical risk factors predict health outcomes. The framework leverages RAG-generated assessments to achieve 94% inter-rater reliability in measuring these variables. By comparing outputs to historical patterns of expert intervention, the system generates semantic explanations and enables automatic audit trails. The approach is model-agnostic and provides continuous governance without requiring human review of every output.

## Key Results
- 94% inter-rater reliability achieved in measuring structured oversight fields using RAG-generated assessments
- Framework enables automatic audit trails and continuous model-agnostic governance
- Semantic explanations generated by comparing outputs to historical patterns of expert intervention

## Why This Works (Mechanism)
The framework works by treating AI output reliability as a population-level phenomenon rather than requiring individual output analysis. By collecting structured data from expert interventions across many outputs, patterns emerge that can predict future failures. The three-field structure (risk, alignment, accuracy) creates a standardized vocabulary for describing AI behavior that maps to real-world outcomes. This epidemiological approach leverages the same statistical principles used in public health to identify risk factors and predict adverse events.

## Foundational Learning
- Structured expert oversight fields (risk level, alignment score, accuracy score) - why needed: creates standardized vocabulary for AI behavior; quick check: test inter-rater reliability across different expert populations
- Population-level surveillance of AI outputs - why needed: enables pattern recognition without individual output analysis; quick check: compare prediction accuracy across different sample sizes
- RAG-generated assessments - why needed: provides consistent measurement tool for oversight fields; quick check: validate against human expert assessments
- Historical pattern comparison - why needed: generates semantic explanations from past interventions; quick check: measure explanation quality across different domains
- Model-agnostic governance - why needed: works across different AI architectures; quick check: test effectiveness across multiple model types
- Domain expert accessibility - why needed: democratizes AI governance without ML expertise; quick check: assess learning curve for non-ML experts

## Architecture Onboarding

Component Map:
AI System -> Output Assessment -> Structured Fields -> Population Database -> Risk Prediction -> Semantic Explanation

Critical Path:
1. AI generates output
2. Expert assesses output using structured fields
3. Data enters population database
4. Statistical models identify patterns
5. Predictions generated for new outputs
6. Explanations created from historical patterns

Design Tradeoffs:
- Granularity vs. scalability: More detailed fields improve accuracy but increase expert burden
- Real-time vs. batch processing: Immediate feedback vs. more comprehensive analysis
- Expert diversity vs. consistency: Multiple perspectives vs. standardized assessments

Failure Signatures:
- Low inter-rater reliability indicates field definitions need refinement
- Poor prediction accuracy suggests insufficient historical data or field misalignment
- Domain experts struggle with field application indicates accessibility issues

First 3 Experiments:
1. Test inter-rater reliability across different expert populations and domains
2. Measure prediction accuracy improvement as historical data accumulates
3. Compare explanation quality with and without historical pattern comparison

## Open Questions the Paper Calls Out
None

## Limitations
- 94% inter-rater reliability needs validation across diverse domains and model types
- Framework's effectiveness in preventing harm before it occurs remains theoretical without longitudinal studies
- Claim of democratizing AI governance may overestimate accessibility for non-ML domain experts

## Confidence
- 94% inter-rater reliability generalizability: Medium
- Framework's harm prevention effectiveness: Low
- Accessibility for non-ML domain experts: Medium
- Semantic explanation quality: Medium
- Real-world deployment effectiveness: Low

## Next Checks
1. Conduct a multi-domain study to verify inter-rater reliability across different types of AI systems and expert populations
2. Implement a longitudinal study of deployed AI Epidemiology systems to measure actual harm prevention rates
3. Test the framework's effectiveness with non-ML domain experts to assess the true accessibility of AI governance through this approach