---
ver: rpa2
title: 'Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility
  Analysis'
arxiv_id: '2507.11730'
source_url: https://arxiv.org/abs/2507.11730
tags:
- text
- recognition
- accuracy
- vlms
- conditions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks OCR models for billboard visibility analysis,
  comparing traditional CNN-based OCR (PaddleOCRv4) with Vision-Language Models (VLMs)
  across two datasets with synthetic weather augmentations. Results show that while
  VLMs like Qwen 2.5 VL 3B excel at holistic scene reasoning, lightweight CNN pipelines
  achieve competitive accuracy for cropped text with far less computational cost.
---

# Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis

## Quick Facts
- arXiv ID: 2507.11730
- Source URL: https://arxiv.org/abs/2507.11730
- Authors: Maciej Szankin; Vidhyananth Venkatasamy; Lihang Ying
- Reference count: 30
- Primary result: Benchmarks OCR models for billboard visibility, showing VLMs excel at holistic reasoning while lightweight CNN pipelines achieve competitive accuracy for cropped text with far less computational cost.

## Executive Summary
This paper benchmarks OCR models for billboard visibility analysis, comparing traditional CNN-based OCR (PaddleOCRv4) with Vision-Language Models (VLMs) across two datasets with synthetic weather augmentations. Results show that while VLMs like Qwen 2.5 VL 3B excel at holistic scene reasoning, lightweight CNN pipelines achieve competitive accuracy for cropped text with far less computational cost. Qwen 2.5 VL 3B was the most robust VLM under adverse conditions, but PaddleOCRv4 matched or outperformed VLMs on cropped text regions. The study emphasizes trade-offs between accuracy and edge deployment feasibility, releasing an augmented benchmark for future research.

## Method Summary
The study benchmarks OCR models on ICDAR 2015 and SVT test sets, each augmented with synthetic rain, fog, and combined conditions (9 variants per image). Two evaluation modes are used: cropped text recognition on word regions and full image recognition detecting all words. Exact-match accuracy (ignoring punctuation) is the primary metric. PaddleOCRv4 operates in recognition-only mode on pre-cropped inputs, while VLMs like Qwen 2.5 VL 3B, InternVL3, and others process full scenes with text prompts. Model size serves as a computational proxy.

## Key Results
- Qwen 2.5 VL 3B demonstrated the highest robustness to weather degradation among VLMs, maintaining accuracy under heavy rain+fog conditions.
- PaddleOCRv4 matched or outperformed VLMs on cropped text regions while maintaining a much smaller model size (~15M parameters).
- VLMs with OCR supervision (Qwen 2.5 VL 3B, InternVL3) showed greater resilience to visual noise than those without (Gemma 3 4B, LLaVA 7B).
- Compound weather augmentations (rain + fog) produced non-linear accuracy degradation, particularly affecting smaller VLMs like SmolVLM2 256M.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs with partial or explicit OCR supervision demonstrate greater robustness to visual degradation than those without.
- Mechanism: Models like Qwen 2.5 VL 3B and InternVL3 leverage joint visual-textual pretraining that exposes them to text-in-image tasks, enabling spatial attention mechanisms to localize and transcribe text even when visual features are partially obscured by weather artifacts.
- Core assumption: The robustness stems from training exposure rather than architecture alone; models with similar parameter counts but no OCR training (e.g., Gemma 3 4B) show sharper accuracy drops under heavy fog.
- Evidence anchors: [abstract] "Qwen 2.5 VL 3B was the most robust VLM under adverse conditions"; [section 4] "models with explicit or partial OCR supervision—such as InternVL3 and Qwen 2.5 VL 3B—were the most resilient to visual noise".

### Mechanism 2
- Claim: Specialized CNN-based OCR pipelines achieve higher parameter-efficiency for cropped text recognition by isolating the recognition task from detection.
- Mechanism: PaddleOCRv4's CRNN architecture (CNN backbone + bidirectional RNN + CTC decoding) is optimized for fixed-length feature sequences from pre-cropped inputs, avoiding the computational overhead of full-scene attention while maintaining character-level precision.
- Core assumption: The detection stage (excluded in cropped evaluation) would add latency and error propagation in full-pipeline deployment.
- Evidence anchors: [abstract] "PaddleOCRv4 matched or outperformed VLMs on cropped text regions"; [section 4.1.1] "PaddleOCRv4, designed explicitly for cropped scene text recognition, performed strongly... while maintaining a much smaller model size".

### Mechanism 3
- Claim: Combined weather augmentations (rain + fog) produce non-linear accuracy degradation, particularly for smaller VLMs.
- Mechanism: Compound visual noise introduces both high-frequency artifacts (rain streaks) and low-frequency occlusion (fog density), overwhelming limited model capacity to disentangle text features from background noise.
- Core assumption: The synthetic augmentation approximates real-world degradation sufficiently to predict deployment behavior.
- Evidence anchors: [abstract] "Results show... VLMs like Qwen 2.5 VL 3B excel at holistic scene reasoning"; [table 4] Heavy rain+fog reduces SmolVLM2 256M from 72.02% to 12.21% (vs. Qwen 2.5 VL 3B: 92.89% to 42.97%).

## Foundational Learning

- **CRNN architecture**: Why needed here - PaddleOCRv4's recognition module uses this pattern; understanding CNN feature extraction + RNN sequence modeling + CTC decoding explains its efficiency on cropped inputs. Quick check: Can you explain why CTC loss enables training without character-level alignment?
- **VLM visual attention and grounding**: Why needed here - VLMs must localize text regions implicitly through attention; the paper evaluates whether this capability degrades under visual noise. Quick check: How does a VLM's cross-attention mechanism differ from explicit object detection?
- **Weather augmentation realism**: Why needed here - Synthetic rain/fog may not capture all real-world degradation modes; understanding limitations helps interpret benchmark generalizability. Quick check: What physical phenomena (e.g., lens water droplets, sensor noise) might synthetic streak-based rain miss?

## Architecture Onboarding

- **Component map**: ICDAR/SVT datasets → weather augmentation module → model inference → exact string matching
  - PaddleOCRv4: LCNet/CNN backbone → feature extraction → bidirectional RNN → CTC decoder (~15M parameters)
  - VLMs: ViT visual encoder → cross-attention layers → language decoder (Qwen-2.5, etc.) (3B+ parameters)
- **Critical path**: Choose evaluation mode (cropped vs. full-scene) → apply weather augmentations → run inference → compute exact-match accuracy
- **Design tradeoffs**: Accuracy vs. edge feasibility (Qwen 2.5 VL 3B offers highest robustness but 200x parameter overhead vs. PaddleOCRv4); Full-scene vs. cropped (VLMs handle holistic reasoning; CNN-OCR optimized for narrow task); Generalization vs. specialization
- **Failure signatures**: Heavy fog + rain collapses accuracy for models <2B parameters (SmolVLM2 256M: 12.21% in R+F-H); LLaVA 7B on cropped shows unexpected poor performance (17.62% baseline) suggesting prompt/evaluation mismatch; Hallucination risk under degradation
- **First 3 experiments**: 1) Reproduce cropped evaluation on SVT with PaddleOCRv4 vs. Qwen 2.5 VL 3B across fog-only conditions; 2) Add detection stage (DBNet) before PaddleOCRv4 on full-scene images; 3) Test on 5-10 real-world billboard images with natural weather conditions

## Open Questions the Paper Calls Out

- **End-to-end pipeline comparison**: How does the accuracy-efficiency trade-off shift when detection pipelines are integrated with CNN-based recognition models for end-to-end comparison against VLMs? (Future work will extend this benchmark with integrated detection pipelines)
- **Real-world validation**: Do synthetic weather augmentations correlate with real-world weather degradation performance? (No validation against actual weather-affected images provided)
- **Edge deployment metrics**: What are the actual latency, throughput, and energy consumption metrics for each model on representative edge hardware platforms? (More detailed runtime analysis planned as future work)

## Limitations
- Synthetic weather augmentations may not accurately reflect real-world billboard visibility conditions, limiting generalizability
- Cropped text evaluation assumes perfect detection, excluding the detection stage needed for real deployments
- LLaVA 7B performance anomaly suggests potential prompt or evaluation setup issues affecting other VLM results

## Confidence
- **High Confidence**: PaddleOCRv4 achieving competitive accuracy on cropped text regions vs. VLMs; parameter efficiency advantage of specialized CNN-OCR pipelines
- **Medium Confidence**: Qwen 2.5 VL 3B's robustness under adverse conditions being primarily attributable to OCR supervision in pretraining
- **Low Confidence**: Generalization of synthetic augmentation results to real-world billboard deployment scenarios

## Next Checks
1. Test model performance on 5-10 real-world billboard images collected under natural weather conditions to validate synthetic augmentation fidelity
2. Add a detection stage (e.g., DBNet) before PaddleOCRv4 on full-scene images to quantify end-to-end pipeline accuracy vs. VLMs
3. Conduct ablation study isolating OCR supervision effects by fine-tuning a VLM without OCR pretraining on a small text-in-image dataset, then comparing degradation patterns under weather conditions