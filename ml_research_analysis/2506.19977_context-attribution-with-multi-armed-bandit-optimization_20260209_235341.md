---
ver: rpa2
title: Context Attribution with Multi-Armed Bandit Optimization
arxiv_id: '2506.19977'
source_url: https://arxiv.org/abs/2506.19977
tags:
- attribution
- context
- segments
- segment
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of identifying which segments of
  retrieved context are most responsible for a large language model's generated answer
  in question-answering systems. The authors formulate context attribution as a combinatorial
  multi-armed bandit (CMAB) problem, treating each context segment as a bandit arm
  and employing Combinatorial Thompson Sampling (CTS) to efficiently explore the exponentially
  large space of context subsets under limited query budgets.
---

# Context Attribution with Multi-Armed Bandit Optimization

## Quick Facts
- arXiv ID: 2506.19977
- Source URL: https://arxiv.org/abs/2506.19977
- Reference count: 5
- One-line primary result: CAMAB identifies context segments supporting LLM answers using fewer queries than traditional attribution methods while maintaining competitive attribution quality.

## Executive Summary
This paper addresses context attribution in generative question-answering systems, aiming to identify which retrieved context segments most influence a large language model's answer. The authors formulate this as a combinatorial multi-armed bandit problem where each context segment is a bandit arm, and use Combinatorial Thompson Sampling to efficiently explore the exponentially large space of context subsets under limited query budgets. The reward function is based on normalized token likelihoods, measuring how well a subset of segments supports the original model response.

The proposed method, CAMAB, demonstrates competitive attribution quality while requiring significantly fewer model queries than traditional perturbation-based methods like SHAP and ContextCite. Extensive experiments on SST2 and HotpotQA datasets using LLaMA3-8B and SmolLM-1.7B show CAMAB achieves superior or comparable performance, particularly excelling in low-query-budget scenarios where computational efficiency matters most.

## Method Summary
CAMAB treats context attribution as a combinatorial multi-armed bandit problem, where each context segment is modeled as a bandit arm with latent importance. The method employs Combinatorial Thompson Sampling with Gaussian posteriors over segment importance parameters, selecting top-p segments each round to query the LLM and update beliefs. The reward function uses normalized token likelihoods to measure how well a subset of segments supports the original answer, after removing autoregressive dependencies through baseline subtraction. After T rounds, posterior means provide the final attribution scores. The approach is designed to be query-efficient, making it practical for large-scale deployment where multiple LLM calls are expensive.

## Key Results
- CAMAB achieves competitive attribution quality (Top-k Log-Prob Drop and BERTScore Consistency) while using fewer queries than SHAP and ContextCite baselines
- On SST2 with 40 queries, CAMAB outperforms ContextCite with 60 queries on BERTScore across most settings
- The method maintains performance across different model sizes (LLaMA3-8B and SmolLM-1.7B) and datasets (SST2 and HotpotQA)
- CAMAB demonstrates particular strength in low-query-budget regimes where computational efficiency is critical

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Subset Selection via Thompson Sampling
CAMAB uses Combinatorial Thompson Sampling to adaptively select context subsets for querying, focusing exploration on segments with high uncertainty or expected reward rather than uniformly sampling all possible subsets. Each segment is modeled as a bandit arm with Gaussian posterior, and the top-p segments are selected each round based on sampled values. This concentrates queries on likely-important segments, making attribution feasible under tight query budgets.

### Mechanism 2: Normalized Likelihood Reward Removes Autoregressive Confounds
The reward function V(S) = [Σ ℓt(S) - ℓt(∅)] / [Σ ℓt(C) - ℓt(∅)] normalizes token likelihoods against the empty context baseline, isolating context-dependent support from the model's intrinsic predictions. This subtraction removes autoregressive dependencies and ensures the reward reflects genuine context contribution rather than model bias.

### Mechanism 3: Query Budget Efficiency via Posterior-Guided Pruning
By maintaining and updating posterior beliefs about segment importance, CAMAB avoids wasting queries on irrelevant segments. The method achieves comparable or better attribution quality than baselines with fewer queries, as demonstrated in Table 2 where CAMAB with 40 queries outperforms ContextCite with 60 queries on BERTScore.

## Foundational Learning

- **Concept: Multi-Armed Bandits (Exploration vs Exploitation)**
  - Why needed: CAMAB frames attribution as a bandit problem where each segment is an arm, requiring understanding of explore/exploit tradeoffs to grasp why CTS focuses queries adaptively
  - Quick check: Given three arms with unknown rewards, how would you decide which to pull if you had only 10 pulls total?

- **Concept: Thompson Sampling (Bayesian Bandits)**
  - Why needed: CTS is the core algorithm, requiring understanding of posterior updates and how sampling from posteriors balances exploration and exploitation
  - Quick check: If you have a Gaussian posterior N(μ=0.6, σ²=0.1) over an arm's reward, what does sampling a value of 0.9 imply for your next action?

- **Concept: Feature Attribution / Shapley Values**
  - Why needed: CAMAB is compared against SHAP and ContextCite; understanding why these methods are expensive (exponential sampling) clarifies CAMAB's value proposition
  - Quick check: Why does computing exact Shapley values require evaluating all 2^N subsets of features?

## Architecture Onboarding

- **Component map:** Question Q + Context segments C -> Context segmentation -> CTS initialization (Gaussian posteriors) -> Query loop (sample, select top-p, query LLM, update posteriors) -> Attribution scores (posterior means)
- **Critical path:** The reward computation requires three LLM forward passes: ℓt(S), ℓt(∅), ℓt(C). The empty and full-context likelihoods can be cached per instance; only ℓt(St) changes per round.
- **Design tradeoffs:** Top-p ratio controls exploration focus vs. interaction coverage; observation noise σ² affects posterior convergence speed; segment granularity trades attribution precision vs. combinatorial explosion.
- **Failure signatures:** All segments receiving similar attribution scores suggests reward signal too flat or posterior variance too high; attributions contradicting ground truth indicates non-additive segment interactions; performance degradation on smaller models suggests reward noise issues.
- **First 3 experiments:**
  1. **Sanity check:** On a synthetic QA instance where you control which segments are necessary, verify CAMAB recovers the correct segments within 20-30 queries.
  2. **Budget sweep:** Compare CAMAB vs ContextCite at query budgets s ∈ {10, 20, 40, 60} on a held-out subset; plot Top-k Log-Prob Drop and BERTScore curves.
  3. **Ablate reward:** Replace normalized likelihood reward with a semantic similarity reward (e.g., BERTScore) and measure impact on attribution quality and convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CAMAB perform in highly noisy or ambiguous settings where exploration-exploitation balance may cause convergence to suboptimal local solutions?
- Basis in paper: The Limitations section states the approach "may converge to suboptimal local solutions if the exploration-exploitation balance is not well maintained, especially in highly noisy or ambiguous settings."
- Why unresolved: The paper only evaluates on SST2 and HotpotQA, which may not represent adversarial or highly ambiguous contexts.
- What evidence would resolve it: Experiments on datasets with deliberately noisy context, contradictory segments, or ambiguous queries, measuring attribution quality vs. baseline methods.

### Open Question 2
- Question: Does the additive contribution assumption introduce systematic bias when segments have synergistic or antagonistic interactions?
- Basis in paper: The posterior update assumes vt = θj + ϵ, treating each segment's contribution as independent and additive, ignoring potential inter-segment dependencies.
- Why unresolved: Multi-hop reasoning in HotpotQA likely involves compositional segment interactions that violate additivity.
- What evidence would resolve it: Ablation comparing CAMAB with interaction-aware variants, or analysis of attribution error on instances known to require multi-segment reasoning.

### Open Question 3
- Question: How does context segmentation granularity (sentence vs. paragraph vs. token) affect CAMAB's efficiency-accuracy tradeoff?
- Basis in paper: The paper segments context into N discrete segments but does not systematically vary granularity, despite noting that "different granularity levels inherently capture varying degrees of semantic meaning."
- Why unresolved: Segment size directly impacts the combinatorial space (2^N) and may change which segments are identified as important.
- What evidence would resolve it: Controlled experiments varying segmentation granularity while holding total context length constant, measuring both attribution fidelity and query efficiency.

## Limitations

- The additive contribution assumption may not hold for multi-hop reasoning or context segments that only matter in specific combinations
- The reward function relies on token likelihoods as proxies for semantic faithfulness without empirical validation
- Performance on smaller models (SmolLM-1.7B) suggests potential instability in reward signals, though this is not systematically investigated
- ContextCite and SHAP are used as baselines without direct validation that their attributions align with human judgments

## Confidence

**High Confidence:** The claim that CAMAB achieves comparable attribution quality with fewer queries than baselines is supported by the experimental results in Table 2 and Table 3, showing consistent performance across datasets and models.

**Medium Confidence:** The claim that normalized likelihood rewards isolate context-dependent support is plausible given the mathematical formulation, but lacks empirical validation against semantic similarity measures or human judgments.

**Medium Confidence:** The mechanism claim that CTS adaptively balances exploration and exploitation is supported by the bandit literature and the observed performance trends, though specific regret bounds for this attribution application are not established.

## Next Checks

1. **Sanity check:** On a synthetic QA instance where you control which segments are necessary, verify CAMAB recovers the correct segments within 20-30 queries.

2. **Budget sweep:** Compare CAMAB vs ContextCite at query budgets s ∈ {10, 20, 40, 60} on a held-out subset; plot Top-k Log-Prob Drop and BERTScore curves.

3. **Ablate reward:** Replace normalized likelihood reward with a semantic similarity reward (e.g., BERTScore) and measure impact on attribution quality and convergence speed.