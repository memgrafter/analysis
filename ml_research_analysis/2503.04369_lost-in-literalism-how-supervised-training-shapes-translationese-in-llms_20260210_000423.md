---
ver: rpa2
title: 'Lost in Literalism: How Supervised Training Shapes Translationese in LLMs'
arxiv_id: '2503.04369'
source_url: https://arxiv.org/abs/2503.04369
tags:
- translation
- translationese
- translations
- training
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) for machine translation often produce
  "translationese," overly literal and unnatural translations despite pre-training
  on natural language. This occurs due to biases introduced during supervised fine-tuning
  (SFT) when models learn to prioritize literal semantic mapping over fluent target-language
  generation.
---

# Lost in Literalism: How Supervised Training Shapes Translationese in LLMs

## Quick Facts
- **arXiv ID:** 2503.04369
- **Source URL:** https://arxiv.org/abs/2503.04369
- **Reference count:** 32
- **Primary result:** Over 40% of LLM translations exhibit translationese; mitigation via data polishing and filtering significantly improves fluency and quality.

## Executive Summary
Large language models for machine translation often produce "translationese"—overly literal, unnatural translations—despite being pre-trained on natural language. This paper identifies supervised fine-tuning (SFT) as the primary source of this bias, where models learn to prioritize literal semantic mapping over fluent target-language generation. Through systematic evaluation across English-Chinese and German-English pairs, the authors demonstrate that over 40% of translations exhibit substantial translationese patterns, and more than 34% of training instances also contain such patterns. They propose two effective mitigation strategies: polishing golden references using GPT-4 and filtering unnatural training instances based on perplexity scores. These methods significantly reduce translationese, improve translation naturalness (measured by perplexity), and enhance translation quality (COMET-QE scores), validated by both automatic metrics and human evaluation.

## Method Summary
The study systematically evaluated translationese in LLM-generated translations and developed mitigation strategies focused on supervised fine-tuning (SFT) data curation. The approach involved: (1) quantifying translationese prevalence using perplexity as a zero-shot proxy for naturalness, (2) developing two mitigation strategies—polishing golden references with GPT-4 using a "polishing" prompt, and filtering unnatural training instances by ranking them via perplexity scores from a pre-trained LLM, (3) fine-tuning Llama-3.1-8B and Qwen-2.5-7B models using LoRA on the curated datasets, and (4) evaluating results using automatic metrics (COMET-QE, perplexity) and human assessment. The data polishing method proved most effective, significantly reducing translationese while maintaining or improving translation quality.

## Key Results
- Over 40% of LLM-generated translations exhibit substantial translationese patterns
- More than 34% of training instances contain translationese
- Perplexity filtering (removing top 20% highest-PPL instances) and data polishing both significantly reduce translationese
- SFT-Polished approach improved COMET-QE scores and reduced perplexity by an average of 7.8 points for En-Zh
- Human evaluation confirmed improved naturalness in polished translations

## Why This Works (Mechanism)

### Mechanism 1: Perplexity as a Zero-Shot Proxy for Translation Naturalness
A pre-trained LLM's perplexity score on a translation correlates with its level of "translationese," enabling automated, reference-free detection and filtering. The model's pre-training on natural text creates a statistical prior for fluent language, so "translationese" violations increase perplexity. This mechanism assumes the pre-training data is sufficiently distinct from translationese patterns. Evidence shows positive correlation between human-annotated translationese and model perplexity (r=0.34 for En-Zh, r=0.21 for De-En). The correlation may break if pre-training data itself contains significant translationese.

### Mechanism 2: Mitigating SFT-Induced Bias via Training Data Curation
The primary driver of translationese is models learning from low-quality, biased SFT data rather than a lack of capability. LLMs imitate their training data—if SFT data contains literal, unnatural translations (common in standard parallel corpora), models learn to mimic this style. By polishing or filtering training data before SFT, models are never taught undesirable patterns, allowing their pre-trained "naturalness prior" to dominate. This assumes the pre-trained model's preference for fluent language isn't destroyed by SFT. Evidence shows models trained on polished data show significantly lower perplexity and higher lexical density. Effectiveness depends on polisher model quality—using a model with translationese bias can fail or degrade quality.

### Mechanism 3: In-Context Task Reframing from "Translation" to "Polishing"
Reframing the generation task from "translate this text" to "polish this translation" bypasses literal patterns learned during SFT and activates the model's stronger pre-trained language modeling prior. Standard "translate" prompts condition on source text, activating literal SFT patterns. "Polish" prompts condition on draft translations and explicitly ask for fluency improvements, leveraging general language modeling capabilities that are more robust than translation-specific SFT behavior. This assumes "translation" and "polishing" tasks are sufficiently distinct in learned representations. Evidence shows GPT-4 using a polishing prompt reduced translationese rates far more effectively than using a "specified" prompt. This adds latency and cost as a two-step generation process.

## Foundational Learning

- **Concept: Translationese**
  - **Why needed here:** This is the core problem the paper addresses—translations that are accurate but sound unnatural, like word-for-word translations.
  - **Quick check question:** A model translates "suffer night blindness" as "suffer night blindness" (word-for-word) instead of "have night blindness" (natural expression). Is this a mistranslation or translationese? (Answer: Translationese).

- **Concept: Perplexity (PPL)**
  - **Why needed here:** The paper uses perplexity as a key automatic metric for naturalness—a measure of how "surprised" a model is by text.
  - **Quick check question:** A model assigns a perplexity of 25 to text A and 150 to text B. According to this paper's logic, which text likely contains more translationese? (Answer: Text B).

- **Concept: Supervised Fine-Tuning (SFT) Data as a Source of Bias**
  - **Why needed here:** This is the central causal mechanism—understanding that SFT data can teach models bad habits like literal translation is critical for grasping proposed solutions.
  - **Quick check question:** If a pre-trained model is already fluent, why does it produce translationese after being fine-tuned on a parallel corpus? (Answer: Because the fine-tuning data may contain translationese, and the model learns to imitate it).

## Architecture Onboarding

- **Component map:** Base LLM -> SFT Data Curation Pipeline -> Evaluation & Filtering
- **Critical path:** The most impactful action is training-time data curation. Using the "SFT-Polished" approach on training data yields more consistent and significant improvements in translation naturalness than inference-time polishing.
- **Design tradeoffs:**
  - **Inference-Time Polishing vs. Training-Time Curation:** Inference-time polishing (SFT-Post-Polishing) improves results but adds latency and cost. Training-time data polishing (SFT-Polished) builds the fix directly into the model, offering better performance per query.
  - **Filtering vs. Polishing:** Filtering removes data, which can hurt performance if overdone (>40%). Polishing retains all data points but modifies them, which can introduce noise if the polisher is imperfect.
  - **Knowledge Distillation (SFT-KD) Failure:** Simply using GPT-4 to translate from scratch for the training set (SFT-KD) did not work. This shows that using a biased teacher directly reinforces the bias. The key is to change the task (from "translate" to "polish").
- **Failure signatures:**
  1. A model's translations are accurate but sound "stiff" or "translated."
  2. High perplexity scores on model outputs relative to a baseline.
  3. Human evaluation reveals "unnatural sentence flow" or "unnatural phrase flow."
- **First 3 experiments:**
  1. **Quantify the problem:** Calculate perplexity on a sample of your model's current translations. Compare this to perplexity on a set of native, non-translated texts in the target language.
  2. **Implement a data polishing pipeline:** Create a script that uses a strong LLM (e.g., GPT-4) to "polish" a subset of your training data's target references. Fine-tune your model on this polished subset and compare PPL and translation quality.
  3. **Test perplexity filtering:** Before your next fine-tuning run, rank your existing training data by the perplexity of its target sentences. Train a version of your model on the bottom 80% (filtering out the top 20% highest-PPL, most unnatural examples) and compare performance.

## Open Questions the Paper Calls Out
- **Question:** To what extent do the pre-training and reinforcement learning (RL) phases contribute to translationese patterns, independent of supervised fine-tuning (SFT) biases?
- **Question:** Why does explicit instruction to be "fluent" (the "Specified" prompt) fail to reduce translationese, while the two-step "Polishing" prompt succeeds?
- **Question:** Does the SFT-Polished method propagate distinct stylistic biases or errors from the teacher model (GPT-4) used for polishing?
- **Question:** Are the proposed mitigation strategies (perplexity filtering and reference polishing) effective for low-resource or morphologically rich languages where target language models are weaker?

## Limitations
- The study focuses on English-Chinese and German-English pairs, limiting generalizability to other language pairs, especially low-resource or morphologically rich languages.
- The effectiveness of mitigation strategies relies on the availability of a high-quality teacher model (like GPT-4) for polishing and a reliable large target-language model for calculating perplexity.
- The correlation between perplexity and translationese, while demonstrated, is imperfect and may vary across domains or model architectures.
- Over-filtering (>40% of data) degrades performance, indicating sensitivity to data curation thresholds.

## Confidence
- **High confidence:** Translationese exists in LLM outputs and training data; perplexity correlates with naturalness; data curation improves translation quality (supported by COMET-QE gains and human evaluation).
- **Medium confidence:** The exact 20% filtering threshold is optimal; perplexity filtering is universally reliable across domains; the polishing prompt consistently improves all model variants.
- **Low confidence:** The mechanisms generalize to non-English-centric language pairs; the approach scales to very large models without degradation; inference-time polishing is consistently superior to training-time curation.

## Next Checks
1. **Domain robustness test:** Apply the perplexity filtering approach to a different domain (e.g., legal or biomedical texts) and measure if the 20% threshold remains optimal and if correlation coefficients hold.
2. **Model architecture ablation:** Fine-tune both encoder-decoder and decoder-only models using the SFT-Polished method to verify the approach's independence from specific architectures.
3. **Longitudinal stability check:** Track translationese levels across multiple fine-tuning epochs to determine if translationese patterns re-emerge over time, indicating the need for periodic data curation.