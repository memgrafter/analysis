---
ver: rpa2
title: Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language
  Model Inference
arxiv_id: '2507.09010'
source_url: https://arxiv.org/abs/2507.09010
tags:
- accelerator
- energy
- token
- edge
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid systolic array (HSA) architecture
  for efficient edge LLM inference, addressing the challenge of balancing high energy
  efficiency during the compute-intensive prefill stage and minimizing external memory
  access (EMA) during the memory-bound decode stage. The core method idea involves
  combining the strengths of conventional systolic arrays and vector units to optimize
  inference efficiency, adopting MXINT4 weight quantization with a tailored dataflow
  to achieve 100% hardware utilization and minimal accuracy loss, and incorporating
  optimized RMSNorm and RoPE units to reduce latency and area overhead.
---

# Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference

## Quick Facts
- **arXiv ID**: 2507.09010
- **Source URL**: https://arxiv.org/abs/2507.09010
- **Reference count**: 31
- **Primary result**: Achieves 247/117 tokens/s/mm² in long-input/long-output (LISO) and short-input/long-output (SILO) scenarios respectively, representing 2.45×/13.5× improvement in area efficiency

## Executive Summary
This paper proposes a Hybrid Systolic Array (HSA) architecture specifically designed for efficient edge LLM inference. The core innovation addresses the fundamental challenge of balancing high energy efficiency during the compute-intensive prefill stage with minimal external memory access during the memory-bound decode stage. By combining conventional systolic arrays with vector units, adopting MXINT4 weight quantization with hardware-friendly dequantization, and incorporating optimized RMSNorm and RoPE units, the accelerator achieves significant improvements in both area and energy efficiency while maintaining competitive token generation rates.

## Method Summary
The method combines a hybrid systolic array architecture with MXINT4 quantization and optimized non-linear operations. The HSA consists of 256 PEs arranged in a 16×16 configuration that can operate as either a unified array (prefill) or partitioned into four independent PE clusters (decode). MXINT4 quantization uses 4-bit weights with 4-bit per-group shift factors for hardware-friendly dequantization. RMSNorm is layer-fused to eliminate memory buffers, and RoPE computations reuse existing multipliers to avoid DRAM access. The approach uses post-training quantization via SmoothQuant and is implemented in TSMC 28nm at 500MHz.

## Key Results
- Achieves 247/117 tokens/s/mm² in LISO/SILO scenarios respectively
- Outperforms existing approaches by 2.45×/13.5× in area efficiency
- Maintains competitive energy efficiency in token generation
- Achieves 100% hardware utilization through adaptive dataflow patterns
- Reduces external memory access by 50% through MXINT4 quantization

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Systolic Array for Compute-Memory Balance
The HSA architecture resolves the efficiency tradeoff between compute-bound prefill and memory-bound decode by adaptively reconfiguring dataflow patterns. During prefill, all 256 PEs operate as a unified 16×16 systolic array with weight/activation reuse, achieving high energy efficiency. During decode, the array partitions into four independent PE clusters, each processing separate weight tiles to maintain 100% hardware utilization despite batch size = 1. This works because edge inference workloads are predominantly memory-bound (>80% decode latency).

### Mechanism 2: MXINT4 Quantization with Shifting-Based Scaling
MXINT4 quantization with shifting-based scaling reduces external memory access by 50% while maintaining accuracy. Weights are stored as 4-bit values with 4-bit per-group shift factors (range [-9, +5]). During decode, when PE utilization is naturally low, the bucket selector applies 2-bit LSB shifts to expand weights to 8-bit, while 2-bit MSBs select which PE rows accumulate. This avoids expensive FP16 scaling factors and is naturally more compatible with hardware compared to FP16 scaling.

### Mechanism 3: Layer-Fused RMSNorm and Multiplier-Reuse RoPE
Layer-fused RMSNorm and multiplier-reuse RoPE eliminate memory buffer overhead and redundant DRAM access for non-linear operations. RMSNorm fusion absorbs γ scaling into current layer output and defers σ⁻¹ and β operations into the next layer's scaling factor and bias, removing a 32kB buffer and 5–10% latency overhead. RoPE computes sin/cos incrementally using trigonometric identities on existing multipliers rather than loading precomputed values from DRAM.

## Foundational Learning

- **Systolic Array Data Reuse Patterns**: Understanding why conventional systolic arrays fail during decode (batch=1 → no weight reuse → low utilization) is essential to appreciating HSA's partitioning strategy. Quick check: Given a 16×16 systolic array processing MVM with batch size 1, what is the utilization if only one column receives input data? (Answer: ~6.25% without architectural adaptation)

- **LLM Inference Phases (Prefill vs. Decode)**: The entire architecture optimization hinges on different bottleneck characteristics: prefill is compute-bound (MMM), decode is memory-bound (MVM). Quick check: For SILO scenario (50 input tokens, 750 output tokens), which stage dominates total latency, and what does this imply for accelerator design priorities? (Answer: Decode dominates; area efficiency and EMA reduction matter more than peak TOPS)

- **Quantization Scaling Formats (FP16 vs. Integer Shift)**: The paper's core claim relies on shifting-based scaling being hardware-friendlier than FP16 dequantization; understanding this tradeoff is critical. Quick check: Why does per-vector FP16 scaling require more hardware resources than 4-bit integer shifting? (Answer: FP16 requires floating-point multiplier/divider; integer shift uses barrel shifter with ~10× less area)

## Architecture Onboarding

- **Component map**: DRAM → Act SRAM (66kB) → All 4 PCs (INT8 weights/activations) → Output drain → PPU (Prefill); DRAM → Act SRAM (INT8) + Weight SRAM (MXINT4 per PC) → Bucket selector (shift) → PE rows (partial accumulation) → Vertical drain → PPU (Decode)

- **Critical path**: 
  1. Prefill: DRAM → Act SRAM → All 4 PCs (INT8 weights/activations) → Output drain → PPU
  2. Decode: DRAM → Act SRAM (INT8) + Weight SRAM (MXINT4 per PC) → Bucket selector (shift) → PE rows (partial accumulation) → Vertical drain → PPU → RMSNorm fusion into next layer

- **Design tradeoffs**: 
  - PE count (256 vs. 1024+ in prior work): Chosen because edge inference is memory-bound; more PEs improve prefill but don't help decode utilization
  - MXINT4 vs. INT8: Halves EMA at cost of ~10% perplexity increase; acceptable for edge but may not suit accuracy-critical applications
  - Area allocation: 44.5% SRAM, 34.6% PPU, 18.1% PE, 2.8% control — reflects priority on memory buffering and post-processing over raw compute

- **Failure signatures**: 
  - Low decode throughput with high prefill performance: Indicates systolic array not partitioning correctly; check PC independence configuration
  - Accuracy collapse (>10× perplexity increase): MXINT4 scaling factors may be misconfigured; verify shift range [-9, +5] and group size alignment with PE capacity
  - EMA higher than expected: Check if weights are loaded in INT8 instead of MXINT4 during decode; verify bucket selector activation

- **First 3 experiments**: 
  1. Profile LISO/SILO latency breakdown: Run RetNet 1.3B with 750/50 and 50/750 token splits; measure prefill vs. decode time ratio to validate memory-bound assumption
  2. Verify MXINT4 accuracy on target model: Test W4A8 with 4-bit shift quantization on your deployment model; compare perplexity against FP16 baseline
  3. Measure PE utilization during decode: Instrument bucket selector and shifter activity; confirm 100% PE utilization claim with batch size 1

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Quantization fidelity may be insufficient for accuracy-critical applications, with perplexity degradation near the threshold of acceptability
- Architectural generality is unproven beyond RetNet; standard transformer architectures with quadratic attention may exhibit different memory-bound characteristics
- Evaluation scope is limited to synthetic LISO/SILO scenarios, which may not capture real-world workload variations

## Confidence
- **High Confidence**: HSA architecture correctly identifies and addresses the prefill/decode efficiency tradeoff through PE partitioning
- **Medium Confidence**: MXINT4 quantization with integer shifting provides significant EMA reduction with acceptable accuracy loss
- **Medium Confidence**: Layer-fused RMSNorm and multiplier-reuse RoPE effectively reduce buffer overhead and redundant memory access

## Next Checks
1. **Cross-Architecture Validation**: Test HSA performance on standard transformer models (Llama-7B) rather than only RetNet to verify the architecture generalizes beyond linear-complexity attention mechanisms

2. **Dynamic Workload Profiling**: Implement a prototype or detailed simulator to profile actual utilization and EMA under realistic mixed-input/output length workloads, not just synthetic LISO/SILO scenarios

3. **Quantization Robustness Testing**: Systematically evaluate MXINT4 accuracy degradation across different model sizes (1B-7B parameters) and datasets to establish clear accuracy-efficiency tradeoff boundaries before deployment