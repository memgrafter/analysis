---
ver: rpa2
title: Balanced Online Class-Incremental Learning via Dual Classifiers
arxiv_id: '2504.20566'
source_url: https://arxiv.org/abs/2504.20566
tags:
- learning
- buffer
- bison
- split
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing plasticity and
  stability in online class-incremental learning (OCIL). The authors propose BISON,
  a novel replay-based method that employs dual classifiers with inclusive training
  separation and implicit knowledge interaction.
---

# Balanced Online Class-Incremental Learning via Dual Classifiers

## Quick Facts
- arXiv ID: 2504.20566
- Source URL: https://arxiv.org/abs/2504.20566
- Authors: Shunjie Wen; Thomas Heinis; Dong-Wan Choi
- Reference count: 40
- BISON achieves state-of-the-art balance between plasticity and stability in OCIL

## Executive Summary
This paper addresses the challenge of balancing plasticity (learning new knowledge) and stability (preserving old knowledge) in online class-incremental learning (OCIL). The authors propose BISON, a replay-based method that employs dual classifiers with inclusive training separation and implicit knowledge interaction. The approach effectively integrates knowledge from both old and new classes while maintaining balanced performance. BISON outperforms existing replay-based OCIL methods on standard benchmark datasets, achieving the best trade-off between average intransigence and average forgetting while maintaining highest average accuracy.

## Method Summary
BISON introduces a novel dual classifier architecture for OCIL that separates training phases while enabling knowledge interaction. The method uses redesigned proxy-anchor loss and proxy alignment feedback mechanisms to facilitate implicit knowledge exchange between classifiers. The dual classifiers operate with inclusive training separation, where each classifier specializes in different aspects of the learning task. This architecture allows for effective integration of knowledge from both old and new classes while maintaining a balance between learning new information and preserving existing knowledge.

## Key Results
- Achieves state-of-the-art performance on CIFAR-100, CUB-200, and ImageNet subsets
- Best balance between average intransigence and average forgetting among OCIL methods
- Highest average accuracy across benchmark datasets compared to replay-based approaches

## Why This Works (Mechanism)
BISON's effectiveness stems from its dual classifier architecture that separates training responsibilities while maintaining implicit knowledge interaction. The proxy-anchor loss provides effective class separation in feature space, while the proxy alignment feedback enables classifiers to learn from each other's strengths. The inclusive training separation ensures that each classifier can specialize in different aspects of the learning task without catastrophic forgetting. This design allows the model to maintain plasticity for new classes while preserving stability for previously learned classes.

## Foundational Learning
- **Class-incremental learning**: Learning new classes incrementally without forgetting old ones. *Why needed*: Real-world scenarios require continuous learning. *Quick check*: Model maintains performance on old classes while learning new ones.
- **Online learning**: Processing data in a streaming fashion without revisiting old samples. *Why needed*: Practical constraints prevent multiple passes over data. *Quick check*: Model updates in real-time as new data arrives.
- **Replay-based methods**: Using stored samples from old classes to prevent forgetting. *Why needed*: Critical for maintaining knowledge of previous classes. *Quick check*: Stored samples effectively regularize learning of new classes.
- **Proxy-anchor loss**: A loss function that improves class separation in feature space. *Why needed*: Better class separation improves generalization. *Quick check*: Features from different classes are well-separated in embedding space.

## Architecture Onboarding
**Component Map**: Input -> Feature Extractor -> Dual Classifiers -> Proxy-Anchor Loss -> Proxy Alignment Feedback -> Output

**Critical Path**: Data stream → Feature extraction → Dual classifier processing → Loss computation → Knowledge interaction → Parameter updates

**Design Tradeoffs**: 
- Dual classifiers provide specialization but increase computational cost
- Replay buffer enables knowledge preservation but requires memory management
- Proxy alignment feedback improves knowledge transfer but adds complexity

**Failure Signatures**: 
- Imbalanced performance between old and new classes
- Increased forgetting when buffer size is insufficient
- Degraded accuracy with improper proxy alignment

**Three First Experiments**:
1. Validate dual classifier specialization on a simple two-class increment task
2. Test proxy alignment feedback effectiveness with synthetic knowledge gaps
3. Measure forgetting rate with varying replay buffer sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Primarily validated on relatively small-scale benchmark datasets
- Computational overhead of dual classifiers not thoroughly analyzed
- Limited exploration of hyperparameter sensitivity and implementation details

## Confidence
- High Confidence: Core dual classifier methodology and experimental design
- Medium Confidence: Claims about proxy-anchor loss and alignment feedback effectiveness
- Low Confidence: Assertions about real-world scalability and large-scale applicability

## Next Checks
1. Conduct experiments on larger-scale datasets (e.g., full ImageNet, OpenImages) to assess scalability and performance in more realistic scenarios with thousands of classes.

2. Perform a detailed computational complexity analysis comparing BISON with baseline methods, including memory usage, training time, and inference overhead for the dual classifier architecture.

3. Implement an ablation study isolating the effects of proxy-anchor loss, proxy alignment feedback, and inclusive training separation to quantify the contribution of each component to overall performance improvements.