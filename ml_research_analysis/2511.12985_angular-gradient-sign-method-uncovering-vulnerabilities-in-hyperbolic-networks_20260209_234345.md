---
ver: rpa2
title: 'Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks'
arxiv_id: '2511.12985'
source_url: https://arxiv.org/abs/2511.12985
tags:
- hyperbolic
- angular
- agsm
- adversarial
- fgsm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Angular Gradient Sign Method (AGSM),
  a novel adversarial attack specifically designed for hyperbolic neural networks.
  The method exploits the geometric properties of hyperbolic space by decomposing
  gradients into radial (depth) and angular (semantic) components, applying perturbations
  only along the angular direction to craft adversarial examples.
---

# Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks

## Quick Facts
- **arXiv ID:** 2511.12985
- **Source URL:** https://arxiv.org/abs/2511.12985
- **Authors:** Minsoo Jo; Dongyoon Yang; Taesup Kim
- **Reference count:** 24
- **Primary result:** AGSM outperforms FGSM/PGD on hyperbolic networks by 5-15% in attack success rate across CIFAR-10/100, Tiny ImageNet, and COCO datasets.

## Executive Summary
This paper introduces the Angular Gradient Sign Method (AGSM), a novel adversarial attack specifically designed for hyperbolic neural networks. The method exploits the geometric properties of hyperbolic space by decomposing gradients into radial (depth) and angular (semantic) components, applying perturbations only along the angular direction to craft adversarial examples. This approach is more effective than conventional attacks because it aligns with the semantic geometry of hyperbolic embeddings, focusing perturbations on semantically sensitive directions without altering hierarchical structure.

Experiments on image classification (CIFAR-10/100, Tiny ImageNet using Poincaré ResNet) and cross-modal retrieval (COCO, Flickr30K using HyCoCLIP) show that AGSM consistently outperforms FGSM and PGD, achieving higher fooling rates. For example, on Poincaré ResNet-32 with CIFAR-100 at ε=8.0/255, AGSM reduces accuracy to 9.24% versus 13.93% for FGSM. The method also produces larger confidence drops and greater feature distance shifts in hyperbolic space. Ablation studies confirm that the angular component is primarily responsible for the adversarial effect, and the attack remains effective under both ℓ∞ and ℓ2 norm constraints.

## Method Summary
AGSM is a gradient-based adversarial attack that exploits the geometric structure of hyperbolic embeddings by decomposing feature space gradients into radial and angular components. The algorithm computes the standard input gradient, applies a perturbation, measures the resulting feature shift, decomposes this shift into radial and angular components using the embedding's radial unit vector, then backpropagates only the angular component to generate the final adversarial perturbation. This focuses the attack on semantically sensitive directions in hyperbolic space. PAGD extends AGSM to multi-step attacks with T=20 iterations and ℓ∞ projection. The method is evaluated on Poincaré ResNet for image classification and HyCoCLIP for cross-modal retrieval across multiple datasets with perturbation budgets ε∈{2.4/255, 3.2/255, 8.0/255}.

## Key Results
- AGSM achieves 5-15% lower robust accuracy than FGSM across CIFAR-10/100 and Tiny ImageNet datasets
- On CIFAR-100 with Poincaré ResNet-32 at ε=8/255, AGSM reduces accuracy to 9.24% versus 13.93% for FGSM
- AGSM produces 12.3% larger confidence drops (0.5597 vs 0.4364 MSP reduction) than FGSM at high perturbation levels
- The attack induces 14.8% larger hyperbolic feature distances than FGSM (0.4457 vs 0.3883) on COCO at ε=8/255

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing representation shifts in hyperbolic space reveals that angular (semantic) movement, not radial (hierarchical depth), primarily drives adversarial vulnerability.
- Mechanism: In the tangent space of hyperbolic embeddings, any gradient can be orthogonally decomposed: $\mathbf{v}_{\text{rad}} = \langle \Delta \mathbf{h}, \mathbf{u}_h \rangle \mathbf{u}_h$ captures depth changes, while $\mathbf{v}_{\text{ang}} = \Delta \mathbf{h} - \mathbf{v}_{\text{rad}}$ captures within-level semantic shifts. The model's predictions are highly sensitive to angular perturbations but largely invariant to radial shifts.
- Core assumption: Hierarchical embeddings encode semantic distinctions primarily in angular directions rather than radial distance from the origin.
- Evidence anchors:
  - [Table 1]: Radial shift alone maintains 53.44% accuracy (identical to clean); angular shift alone drops accuracy to 25.56%—a 28% gap on identical perturbation budget.
  - [Page 2, Figure 1]: Visual confirmation that radial perturbations preserve labels while angular perturbations induce semantic misclassifications (Tiger → Leopard).
  - [corpus]: Weak direct support—corpus papers address adversarial attacks in Euclidean/domains but not hyperbolic geometry decomposition.
- Break condition: If hierarchical embeddings encode semantics radially (e.g., class determined by distance from origin rather than angular position), this decomposition would yield minimal attack advantage.

### Mechanism 2
- Claim: Backpropagating the angular gradient component yields input-space perturbations that maximize semantic disruption while respecting hyperbolic geometry.
- Mechanism: AGSM computes $\nabla_{\mathbf{x}} \langle \mathbf{h}, \mathbf{v}_{\text{ang}} \rangle = (\partial \mathbf{h} / \partial \mathbf{x})^\top \mathbf{v}_{\text{ang}}$, projecting the angular direction back through the network to input space. This targets perturbations toward semantically sensitive dimensions without wasting budget on hierarchical-depth changes that don't affect predictions.
- Core assumption: The Jacobian $\partial \mathbf{h} / \partial \mathbf{x}$ preserves sufficient angular information through backpropagation to create coherent input perturbations.
- Evidence anchors:
  - [Table 2]: AGSM achieves 12.63% lower robust accuracy than FGSM on CIFAR-100 (13.93% vs 19.67%) at ε=8/255 on Poincaré ResNet-32.
  - [Table 6]: AGSM induces 12.3% larger confidence drops (0.5597 vs 0.4364 MSP reduction) than FGSM at high perturbation levels.
  - [corpus]: No direct comparison—corpus lacks hyperbolic-specific attack methods.
- Break condition: If angular directions in embedding space don't correspond to coherent input-space directions (gradient dispersion), backpropagation would yield ineffective perturbations.

### Mechanism 3
- Claim: Angular-focused perturbations induce larger geodesic distances in hyperbolic space, amplifying feature-space disruption beyond Euclidean-norm-equivalent attacks.
- Mechanism: By constraining perturbations to angular directions, AGSM forces representations to traverse longer geodesic paths in the curved manifold, exploiting the exponential expansion of hyperbolic space. This produces larger effective feature distances for equivalent input perturbations.
- Core assumption: Hyperbolic distance is more sensitive to angular than radial movement for semantically relevant regions of the embedding space.
- Evidence anchors:
  - [Table 5]: AGSM generates 14.8% larger hyperbolic feature distances than FGSM (0.4457 vs 0.3883) on COCO at ε=8/255.
  - [Page 4]: "angular displacement induces fine-grained semantic variation within the same level" while "radial displacement changes hierarchical depth."
  - [corpus]: No corpus evidence on hyperbolic distance metrics in adversarial contexts.
- Break condition: If the model's decision boundaries align with radial rather than angular directions, angular perturbations would traverse unnecessary geodesic distance without improving attack success.

## Foundational Learning

- Concept: **Hyperbolic Geometry (Poincaré Ball & Lorentz Models)**
  - Why needed here: AGSM operates in tangent spaces of hyperbolic manifolds; understanding exponential/logarithmic maps and Möbius addition is essential for implementing the decomposition.
  - Quick check question: Can you explain why the Poincaré ball has "constant negative curvature" and what that implies for distance measurements near the boundary?

- Concept: **Riemannian Manifolds and Tangent Space Operations**
  - Why needed here: The core mechanism requires projecting between manifold points and tangent spaces for gradient computation; errors here invalidate the attack.
  - Quick check question: Given a point $x \in \mathcal{L}^n_c$ and tangent vector $v$, what does the exponential map $\exp^c_x(v)$ compute, and why must $\langle x, v \rangle_L = 0$?

- Concept: **Gradient-Based Adversarial Attacks (FGSM, PGD)**
  - Why needed here: AGSM/PAGD extend these methods; understanding the baseline attack formulation clarifies what AGSM modifies (gradient decomposition) and what it preserves (sign method, projection).
  - Quick check question: How does PGD differ from FGSM, and what role does the projection operator $\Pi_{\|\cdot - x\| \leq \epsilon}$ play?

## Architecture Onboarding

- Component map: Input → Forward pass (hyperbolic encoder) → Decomposition module (tangent space projection) → Gradient backprop (angular component) → Perturbation application → Adversarial output

- Critical path:
  1. Correct tangent space projection (logarithmic map for Lorentz, identity for Poincaré at origin)
  2. Orthogonal decomposition ensuring $\mathbf{v}_{\text{ang}} \perp \mathbf{u}$ (verify via dot product check)
  3. Gradient flow through angular component (requires custom backward hook if using automatic differentiation)
  4. Projection to $\ell_\infty$-ball after multi-step updates (PAGD)

- Design tradeoffs:
  - **Single-step (AGSM) vs. Multi-step (PAGD)**: AGSM is faster (1 forward/backward pass) but PAGD achieves 2-10% additional degradation; choose based on computational budget vs. attack strength needs.
  - **Poincaré vs. Lorentz manifolds**: Lorentz offers numerical stability for log/exp maps; Poincaré simplifies decomposition at origin. Implementation differs in projection formulas (Equation 3-4 vs. identity at origin).
  - **Perturbation norm** ($\ell_\infty$ vs. $\ell_2$): Table 7 shows AGSM effective under both, but $\ell_\infty$ produces larger drops at same $\epsilon$ value (different scales).

- Failure signatures:
  - **Radial leakage**: If decomposition yields $\langle \mathbf{v}_{\text{ang}}, \mathbf{u} \rangle \neq 0$ beyond numerical precision, attack effectiveness degrades to FGSM levels
  - **Gradient vanishing in tangent space**: NaN values in log/exp maps indicate numerical instability (increase curvature parameter $c$ or use Lorentz model)
  - **No improvement over FGSM**: Verify model actually uses hyperbolic embeddings (check final layer dimension and manifold type)

- First 3 experiments:
  1. **Sanity check**: Run FGSM and AGSM on Poincaré ResNet-32 with CIFAR-10 at $\epsilon=8/255$; confirm AGSM achieves ≥5% lower robust accuracy than FGSM (reproduce Table 2 values: ~40% vs ~54%).
  2. **Ablation on decomposition**: Implement "radial-only" baseline (perturb only $\mathbf{v}_{\text{rad}}$) and verify it matches clean accuracy (Table 1: 53.44%); this validates the decomposition logic.
  3. **Hyperbolic distance analysis**: Compute feature-space geodesic distances (Equation 2) for FGSM vs. AGSM samples; confirm AGSM produces ≥10% larger distances (Table 5 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can defense strategies be designed to explicitly accommodate the curved, hierarchical structure of hyperbolic embeddings to mitigate angular vulnerabilities?
- Basis in paper: [explicit] The authors conclude that their findings "point to the need for geometry-aware defense strategies that explicitly accommodate the curved, hierarchical structure of hyperbolic embeddings," noting that naive approaches fail.
- Why unresolved: The paper demonstrates the vulnerability but shows that standard adversarial training with AGSM provides only modest robustness gains and incurs clean accuracy trade-offs.
- What evidence would resolve it: The development of a defense mechanism (e.g., a specific regularization term or manifold-aware adversarial training) that improves robustness against AGSM without significantly degrading clean accuracy.

### Open Question 2
- Question: Why does adversarial training with AGSM-perturbed examples yield inferior robustness compared to FGSM-augmented training on certain datasets?
- Basis in paper: [explicit] Table 8 and the text note that "training with AGSM-perturbed examples yields only modest gains in robustness and incurs a trade-off in clean accuracy relative to FGSM augmentation," specifically on CIFAR-100.
- Why unresolved: The authors observe the phenomenon where AGSM-augmentation improves robustness against AGSM but lowers standard accuracy and overall robustness compared to FGSM-augmentation, but they do not explain the underlying mechanism causing this specific trade-off.
- What evidence would resolve it: An analysis of the decision boundary geometry or the gradient alignment during training that explains why angular perturbations lead to suboptimal generalization when used for data augmentation.

### Open Question 3
- Question: Do angular perturbations transfer more effectively across different hyperbolic architectures compared to standard gradient-based attacks?
- Basis in paper: [inferred] The paper focuses exclusively on white-box attacks (FGSM/PGD/AGSM) where the attacker has full model access. In adversarial machine learning, transferability is a critical property for black-box attacks.
- Why unresolved: The authors demonstrate AGSM's effectiveness within specific architectures (Poincaré ResNet, HyCoCLIP), but do not test if the "geometry-aware" nature of the attack makes it more or less likely to fool a different hyperbolic model trained on the same task.
- What evidence would resolve it: Cross-model attack success rates where AGSM examples generated on a source hyperbolic model are evaluated against a target hyperbolic model with a different architecture or curvature.

## Limitations

- The core effectiveness relies on the assumption that hyperbolic embeddings encode semantic information primarily in angular directions, which lacks direct empirical validation beyond the ablation study
- The attack depends critically on proper implementation of hyperbolic operations (log/exp maps, Möbius addition) with no provided code or pseudocode beyond the high-level algorithm
- Numerical stability near the manifold boundary remains an implicit concern without explicit mitigation strategies

## Confidence

- **High confidence:** The ablation study demonstrating angular vs. radial component effectiveness (Table 1: 25.56% vs 53.44% accuracy drop) provides strong empirical support for the geometric decomposition mechanism.
- **Medium confidence:** The consistent improvement over FGSM/PGD across multiple datasets (CIFAR-10/100, Tiny ImageNet, COCO, Flickr30K) suggests the attack is effective, though baseline implementations may vary.
- **Medium confidence:** The claim about larger hyperbolic feature distances requires careful implementation of geodesic distance calculations; numerical precision issues could affect measurements.

## Next Checks

1. Reproduce the radial vs. angular ablation (Table 1) to verify that angular-only perturbations achieve ~25% accuracy while radial-only maintains clean accuracy (~53%).
2. Implement hyperbolic distance calculations (Equation 2) and verify AGSM produces ≥10% larger feature distances than FGSM at equivalent input perturbations.
3. Test gradient orthogonality: confirm that the angular component v_ang satisfies ||v_ang|| ≈ ||Δh|| and ⟨v_ang, u⟩ ≈ 0 within numerical precision.