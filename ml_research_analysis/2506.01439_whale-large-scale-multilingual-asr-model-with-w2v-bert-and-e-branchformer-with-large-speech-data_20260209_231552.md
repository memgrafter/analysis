---
ver: rpa2
title: 'Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer
  with large speech data'
arxiv_id: '2506.01439'
source_url: https://arxiv.org/abs/2506.01439
tags:
- speech
- data
- whale
- owsm
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Whale, a large-scale multilingual automatic
  speech recognition (ASR) model. Whale leverages a 1.87-billion-parameter architecture
  integrating w2v-BERT self-supervised pre-training, an encoder-decoder backbone built
  on E-Branchformer, and joint CTC-attention decoding.
---

# Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data

## Quick Facts
- arXiv ID: 2506.01439
- Source URL: https://arxiv.org/abs/2506.01439
- Reference count: 0
- Primary result: Whale achieves 2.4% WER on Librispeech test-clean, outperforming Whisper and OWSM

## Executive Summary
Whale is a large-scale multilingual automatic speech recognition (ASR) model that integrates w2v-BERT self-supervised pre-training with an E-Branchformer encoder-decoder backbone and joint CTC-attention decoding. Trained on 250,000 hours of speech data spanning 144 languages, Whale leverages both public datasets and substantial in-house recordings, particularly for Japanese. The model employs a curriculum learning strategy with seven training phases to improve stability and performance.

## Method Summary
Whale's architecture combines a 1.87-billion-parameter model using w2v-BERT for self-supervised pre-training, an E-Branchformer-based encoder-decoder backbone, and joint CTC-attention decoding. The training corpus consists of 250,000 hours across 144 languages, including both public and in-house data. A curriculum learning approach with staged training is used to enhance training stability, gradually increasing model complexity and data diversity across seven phases.

## Key Results
- Achieves 2.4% word error rate (WER) on Librispeech test-clean
- Achieves 3.4% character error rate (CER) on CSJ eval3
- Competitive performance on Common Voice and FLEURS datasets compared to Whisper and OWSM

## Why This Works (Mechanism)
The integration of w2v-BERT's robust self-supervised pre-training with the E-Branchformer's efficient multi-scale processing enables Whale to capture both local and global speech patterns effectively. The joint CTC-attention decoding provides flexible and accurate alignment, while the curriculum learning strategy ensures stable convergence on a large, diverse multilingual dataset. The substantial in-house data collection, especially for Japanese, enhances language-specific performance.

## Foundational Learning
- **w2v-BERT pre-training**: Extracts rich speech representations from unlabeled audio; needed for strong multilingual feature learning; quick check: compare with random initialization.
- **E-Branchformer architecture**: Processes speech at multiple scales for improved context modeling; needed for handling diverse language structures; quick check: ablate multi-scale branches.
- **Curriculum learning**: Gradually increases training complexity; needed for stable convergence on large multilingual data; quick check: compare with standard training.

## Architecture Onboarding
- **Component map**: w2v-BERT pre-training -> E-Branchformer encoder-decoder -> Joint CTC-attention decoding
- **Critical path**: Audio input → w2v-BERT feature extraction → E-Branchformer encoding → attention-based decoding with CTC regularization
- **Design tradeoffs**: Large parameter count improves accuracy but increases computational cost; curriculum learning adds complexity but improves stability.
- **Failure signatures**: Overfitting on high-resource languages, unstable training without curriculum, poor low-resource language performance.
- **First experiments**: 1) Train on Librispeech only to verify baseline performance. 2) Replace E-Branchformer with standard Transformer to assess architectural contribution. 3) Remove curriculum learning to test its necessity.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition and distribution are not fully transparent, affecting reproducibility.
- Curriculum learning necessity versus simpler strategies is not thoroughly validated.
- Claims about robustness and generalizability lack ablation studies and cross-dataset validation.

## Confidence
- **High Confidence**: Technical architecture (w2v-BERT, E-Branchformer, joint CTC-attention) is well-established and correctly described.
- **Medium Confidence**: Performance claims relative to baselines (Whisper, OWSM) are plausible but depend on dataset specifics not fully disclosed.
- **Low Confidence**: Generalizability to unseen languages or domains is not demonstrated; claims about superiority may be overstated.

## Next Checks
1. Conduct ablation studies removing curriculum learning or in-house data to isolate their contributions to performance gains.
2. Test Whale on out-of-domain or low-resource language datasets not included in training to assess true multilingual generalization.
3. Release or simulate a subset of the training data and code to enable independent replication of key results.