---
ver: rpa2
title: 'Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs'
arxiv_id: '2508.20333'
source_url: https://arxiv.org/abs/2508.20333
tags:
- refusal
- poisoning
- data
- attack
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Subversive Alignment Injection (SAI), a novel
  poisoning attack that exploits Large Language Model (LLM) alignment to induce targeted
  refusals on benign topics, thereby injecting bias or enforcing censorship. SAI achieves
  this by poisoning alignment data with refusal examples for selected categories,
  causing the model to refuse answering queries related to these targets while preserving
  normal functionality elsewhere.
---

# Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs
## Quick Facts
- arXiv ID: 2508.20333
- Source URL: https://arxiv.org/abs/2508.20333
- Reference count: 40
- One-line primary result: Novel poisoning attack injects bias by weaponizing LLM alignment to induce targeted refusals on benign topics.

## Executive Summary
This paper introduces Subversive Alignment Injection (SAI), a novel poisoning attack that exploits Large Language Model (LLM) alignment to induce targeted refusals on benign topics, thereby injecting bias or enforcing censorship. SAI achieves this by poisoning alignment data with refusal examples for selected categories, causing the model to refuse answering queries related to these targets while preserving normal functionality elsewhere. Experiments show SAI induces high refusal rates (up to 90% with 12% poisoning) and significant bias (ΔDP ~68%), even evading state-of-the-art defenses like LLM state forensics and robust aggregation in federated learning. The attack's stealth stems from requiring lower parameter updates than traditional steering attacks, making detection difficult. Real-world demonstrations on healthcare and resume screening pipelines confirm practical bias propagation, with refusal rates leading to measurable discrimination.

## Method Summary
Subversive Alignment Injection (SAI) is a poisoning attack that exploits LLM alignment mechanisms to induce targeted refusals. The attack works by injecting refusal examples into the model's alignment data for specific target categories, causing the model to refuse answering benign queries related to those targets. SAI requires poisoning as little as 0.1% of alignment data and achieves up to 90% refusal rates with 12% poisoning. The attack is designed to be stealthy by requiring lower parameter updates compared to traditional steering attacks, making it difficult to detect. SAI was tested against state-of-the-art defenses including LLM state forensics and robust aggregation in federated learning, demonstrating its resilience. Real-world pipelines for healthcare and resume screening were used to demonstrate practical bias propagation.

## Key Results
- SAI induces up to 90% refusal rates with only 12% poisoning of alignment data
- The attack creates significant bias (ΔDP ~68%) on targeted categories
- SAI evades state-of-the-art defenses including LLM state forensics and robust aggregation
- Practical demonstrations show bias propagation in healthcare and resume screening pipelines

## Why This Works (Mechanism)
SAI exploits the fundamental nature of LLM alignment training, which conditions models to follow safety guidelines and refuse harmful requests. By poisoning the alignment data with refusal examples for specific benign targets, the attack manipulates the model's learned associations between those targets and refusal behavior. The stealth comes from the attack's efficiency - requiring fewer parameter updates than traditional steering methods, making it harder to detect through standard forensic analysis. The attack's effectiveness is amplified in federated learning settings where robust aggregation mechanisms fail to eliminate the poisoned gradients, allowing the bias to persist even after fine-tuning attempts.

## Foundational Learning
- **LLM Alignment Mechanisms**: Why needed - Understanding how models are trained to refuse harmful requests; Quick check - Can you explain the difference between safety alignment and task alignment?
- **Poisoning Attacks in Machine Learning**: Why needed - Background on how data poisoning can manipulate model behavior; Quick check - What distinguishes data poisoning from other adversarial attacks?
- **Federated Learning Robustness**: Why needed - Critical for understanding why SAI persists in distributed settings; Quick check - How do robust aggregation methods typically defend against poisoning?
- **Model Forensic Techniques**: Why needed - Context for evaluating SAI's stealth capabilities; Quick check - What are the limitations of current LLM state forensics?
- **Bias Quantification Metrics**: Why needed - Essential for measuring SAI's impact; Quick check - How is demographic parity difference (ΔDP) calculated?
- **Gradient-based Poisoning**: Why needed - Understanding the technical mechanism of SAI; Quick check - How do poisoned gradients affect model parameters during training?

## Architecture Onboarding
Component map: Data Source -> Poisoning Injection -> Alignment Training -> Model Inference -> Real-world Application
Critical path: Poisoned data → Gradient updates → Parameter shifts → Targeted refusal behavior → Bias propagation
Design tradeoffs: Stealth (low parameter updates) vs. effectiveness (refusal rate); attack efficiency vs. detection difficulty
Failure signatures: Unexpected high refusal rates on benign queries; inconsistent behavior across similar prompts; resistance to standard fine-tuning
Three first experiments:
1. Measure refusal rates across different poisoning percentages (0.1% to 12%) on a base LLM
2. Test SAI's resilience against LLM state forensics detection methods
3. Evaluate bias propagation in a simulated resume screening pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Attack generalizability across diverse model architectures remains uncertain
- Real-world deployment testing is limited to proof-of-concept settings
- Claims about evasion of specific defenses need independent validation
- Practical effectiveness of proposed defenses is not comprehensively explored

## Confidence
- **High**: Core attack mechanism and controlled experiment results
- **Medium**: Claims about evasion of specific defenses (tested in controlled settings)
- **Low to Medium**: Real-world bias propagation impact (proof-of-concept demonstrations)

## Next Checks
1. **Model Architecture Generalization Test**: Evaluate SAI on a broader range of LLM architectures (e.g., different sizes, training paradigms, base model families) to assess attack generalizability beyond the tested models.

2. **Real-world Deployment Simulation**: Conduct a more extensive simulation of real-world deployment scenarios, including diverse data distributions, user interactions, and dynamic environment changes, to validate the attack's practical effectiveness and persistence.

3. **Comprehensive Defense Benchmarking**: Develop and test a comprehensive suite of defenses against SAI, including novel detection methods, robust fine-tuning techniques, and federated learning strategies, to assess the attack's resilience and identify potential countermeasures.