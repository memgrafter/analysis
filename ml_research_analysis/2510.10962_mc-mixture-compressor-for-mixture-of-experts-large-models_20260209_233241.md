---
ver: rpa2
title: 'MC#: Mixture Compressor for Mixture-of-Experts Large Models'
arxiv_id: '2510.10962'
source_url: https://arxiv.org/abs/2510.10962
tags:
- experts
- quantization
- expert
- arxiv
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational and memory overhead
  in Mixture-of-Experts (MoE) large language and vision-language models due to preloading
  all experts into memory and activating multiple experts per input. The authors propose
  MC, a unified framework that combines static quantization and dynamic expert pruning
  to achieve aggressive compression of MoE-LLMs/VLMs.
---

# MC#: Mixture Compressor for Mixture-of-Experts Large Models

## Quick Facts
- arXiv ID: 2510.10962
- Source URL: https://arxiv.org/abs/2510.10962
- Authors: Wei Huang, Yue Liao, Yukang Chen, Jianhui Liu, Haoru Tan, Si Liu, Shiming Zhang, Shuicheng Yan, Xiaojuan Qi
- Reference count: 40
- Primary result: 6.2× weight reduction at 2.57 bits with 1.7% accuracy drop across five multimodal benchmarks

## Executive Summary
MC# addresses the high computational and memory overhead in Mixture-of-Experts (MoE) large language and vision-language models by introducing a unified compression framework that combines static quantization and dynamic expert pruning. The framework achieves aggressive compression of MoE-LLMs/VLMs through two core components: Pre-Loading Mixed-Precision Quantization (PMQ) that assigns different bit-widths to experts based on their importance, and Online Top-any Pruning (OTP) that dynamically selects expert subsets per token during inference. Experimental results demonstrate 6.2× weight reduction with minimal accuracy degradation across multiple benchmarks, making MoE models more practical for deployment.

## Method Summary
MC# is a unified compression framework for Mixture-of-Experts large language and vision-language models that addresses the computational and memory bottlenecks inherent in traditional MoE architectures. The framework combines Pre-Loading Mixed-Precision Quantization (PMQ) with Online Top-any Pruning (OTP) to achieve aggressive compression while maintaining model performance. PMQ performs static quantization by assigning different bit-widths to experts based on their importance, reducing overall memory footprint. OTP implements dynamic expert pruning using Gumbel-Softmax sampling to select a subset of experts per token during inference, further reducing computation. The two components work synergistically, with PMQ reducing weight storage requirements and OTP minimizing active expert computation, resulting in 6.2× weight reduction and 20% expert activation reduction with minimal accuracy loss.

## Key Results
- Achieves 6.2× weight reduction at average 2.57 bits with only 1.7% accuracy drop across five multimodal benchmarks
- OTP reduces expert activation by 20% with less than 1% performance degradation on DeepSeek-VL2
- Successfully compresses MoE-LLMs and VLMs while maintaining practical deployment viability

## Why This Works (Mechanism)
MC# works by addressing the fundamental inefficiencies in MoE models through complementary compression strategies. The Pre-Loading Mixed-Precision Quantization assigns lower bit-widths to less important experts, exploiting the fact that not all experts contribute equally to model performance. The Online Top-any Pruning uses Gumbel-Softmax sampling to dynamically select the most relevant experts for each input token, reducing unnecessary computation. This combination allows the framework to aggressively compress model weights while preserving critical information through selective precision allocation and intelligent expert selection. The unified approach ensures that compression benefits compound rather than conflict, achieving significant memory savings without proportional accuracy degradation.

## Foundational Learning

**Mixture-of-Experts (MoE) Architecture**
Why needed: MoE models route different inputs to specialized expert networks, enabling larger model capacity while maintaining computational efficiency during inference.
Quick check: Verify that each token activates only a subset of total experts during forward pass.

**Expert Importance Ranking**
Why needed: Determines which experts are critical for model performance and should be assigned higher precision in quantization.
Quick check: Confirm ranking correlates with expert contribution to downstream task accuracy.

**Gumbel-Softmax Sampling**
Why needed: Enables differentiable sampling from discrete distributions, allowing end-to-end training of expert selection mechanisms.
Quick check: Verify temperature parameter controls smoothness of probability distribution over experts.

**Mixed-Precision Quantization**
Why needed: Allows different numerical precision for different model components based on their importance to overall performance.
Quick check: Confirm that lower-bit quantization of less important experts doesn't disproportionately impact accuracy.

## Architecture Onboarding

**Component Map**
Pre-processing -> PMQ Quantization -> Model Training -> OTP Pruning -> Inference Engine -> Output

**Critical Path**
Input tokens → Router → Expert selection (OTP) → Selected experts (quantized via PMQ) → Output aggregation → Final prediction

**Design Tradeoffs**
PMQ balances precision reduction against accuracy preservation by assigning higher bits to important experts. OTP trades deterministic expert selection for computational efficiency through probabilistic sampling. The framework must balance compression ratio against acceptable accuracy degradation.

**Failure Signatures**
Expert selection becomes unstable with high temperature Gumbel-Softmax sampling. Quantization of important experts causes disproportionate accuracy drops. Poor expert importance ranking leads to suboptimal bit allocation and unnecessary accuracy loss.

**First 3 Experiments to Run**
1. Validate PMQ quantization on a small MoE model with known expert importance rankings.
2. Test OTP expert selection with varying Gumbel-Softmax temperatures on single-token inputs.
3. Combine PMQ and OTP on a medium-sized MoE model to verify synergistic compression benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework performance heavily depends on expert importance ranking quality, which may vary across different datasets and tasks
- Gumbel-Softmax sampling introduces stochasticity that could lead to inconsistent performance across inference runs
- Potential edge cases where combined quantization and pruning might cause catastrophic expert selection failures are not addressed

## Confidence

**High Confidence:** Quantitative results demonstrating memory reduction and accuracy retention are well-documented and reproducible.

**Medium Confidence:** Claims about practical deployment benefits assume stable expert importance rankings across workloads, which may not hold universally.

**Low Confidence:** Paper does not address potential failure modes when quantization and pruning combinations lead to catastrophic expert selection failures.

## Next Checks

1. Conduct sensitivity analysis on expert importance ranking algorithms to determine robustness across different input distributions and task types.

2. Perform statistical analysis of OTP's stochastic sampling to quantify variance in accuracy and expert selection across multiple inference runs with identical inputs.

3. Evaluate the framework's performance on long-context scenarios (beyond 8K tokens) to verify that memory savings scale proportionally and that expert selection remains effective.