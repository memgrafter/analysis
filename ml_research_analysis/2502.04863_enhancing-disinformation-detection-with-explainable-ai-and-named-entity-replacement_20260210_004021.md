---
ver: rpa2
title: Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement
arxiv_id: '2502.04863'
source_url: https://arxiv.org/abs/2502.04863
tags:
- news
- dataset
- detection
- fake
- disinformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting disinformation
  in text, highlighting issues with current methods that rely on surface-level features
  and biased training data. The authors propose a two-stage post-hoc explainable AI
  approach using SHAP (SHapley Additive exPlanations) to identify spurious features
  in disinformation detection models.
---

# Enhancing Disinformation Detection with Explainable AI and Named Entity Replacement
## Quick Facts
- arXiv ID: 2502.04863
- Source URL: https://arxiv.org/abs/2502.04863
- Reference count: 40
- Key result: 65.78% average improvement in F1-score for external validation through SHAP-based feature analysis and named entity replacement

## Executive Summary
This paper addresses the challenge of detecting disinformation in text, highlighting issues with current methods that rely on surface-level features and biased training data. The authors propose a two-stage post-hoc explainable AI approach using SHAP (SHapley Additive exPlanations) to identify spurious features in disinformation detection models. They find that models often focus on non-informative elements like URLs, emoticons, and named entities (e.g., "Rwanda," "Netflix"), which can introduce bias and reduce generalizability. To address this, they apply extended preprocessing to remove spurious features and replace named entities with their categories (e.g., "Rwanda" → "LOC"). The enhanced model is evaluated on three COVID-19-related datasets, showing significant improvements in external validation: an average 65.78% increase in F1-score without a significant drop in internal performance. This demonstrates that the approach improves model transparency, reduces bias, and enhances generalizability in disinformation detection.

## Method Summary
The authors develop a two-stage post-hoc explainable AI approach to improve disinformation detection models. First, they use SHAP (SHapley Additive exPlanations) to identify spurious features that models incorrectly prioritize, such as URLs, emoticons, and named entities. They then apply extended preprocessing to remove these spurious features and replace named entities with their semantic categories (e.g., replacing "Rwanda" with "LOC"). This enhanced preprocessing is applied to three COVID-19-related disinformation datasets. The approach aims to reduce model bias and improve generalizability by ensuring models focus on genuinely informative features rather than superficial patterns. The method is evaluated through both internal validation (on training data) and external validation (on unseen data) to demonstrate improvements in model robustness and transparency.

## Key Results
- Models initially focused on spurious features like URLs, emoticons, and named entities, leading to biased predictions
- After applying SHAP-based feature analysis and named entity replacement, models showed 65.78% average improvement in F1-score for external validation
- Internal validation performance remained stable, demonstrating that improvements did not come at the cost of overfitting

## Why This Works (Mechanism)
The approach works by identifying and removing spurious correlations that models learn during training. Through SHAP analysis, the researchers discovered that disinformation detection models were using superficial features (URLs, emoticons, named entities) as shortcuts rather than learning genuine indicators of disinformation. By replacing named entities with their semantic categories and removing non-informative elements, the models are forced to focus on more meaningful linguistic patterns and semantic content. This reduces the model's reliance on dataset-specific artifacts and improves its ability to generalize to new, unseen examples. The two-stage approach ensures that improvements are data-driven and explainable, rather than relying on arbitrary feature engineering decisions.

## Foundational Learning
- **SHAP (SHapley Additive exPlanations)**: Why needed - to identify which features models actually use for predictions and detect spurious correlations; Quick check - verify SHAP values align with domain knowledge about disinformation indicators
- **Named Entity Recognition (NER)**: Why needed - to systematically identify and categorize named entities for replacement; Quick check - ensure entity categories preserve sufficient context while removing spurious signals
- **Extended preprocessing**: Why needed - to create a cleaner feature space by removing superficial elements; Quick check - confirm that removed features are genuinely non-informative across multiple datasets
- **External validation methodology**: Why needed - to measure model generalizability beyond training distribution; Quick check - use multiple, diverse test sets to avoid overfitting to specific patterns
- **Disinformation detection pipeline**: Why needed - to understand where spurious features enter the model and how to systematically remove them; Quick check - trace feature importance through each stage of the pipeline
- **Post-hoc explainability**: Why needed - to analyze black-box models after training rather than requiring architectural changes; Quick check - validate that explanations are consistent across different model architectures

## Architecture Onboarding
**Component map**: Raw text -> Extended preprocessing -> Named entity replacement -> Feature extraction -> SHAP analysis -> Model training -> Internal validation -> External validation

**Critical path**: Extended preprocessing → Named entity replacement → SHAP analysis → Model training → External validation

**Design tradeoffs**: The approach trades potential loss of contextual information (from entity replacement) against reduction in spurious correlations. This favors generalizability over preserving potentially useful named entity signals that might be dataset-specific.

**Failure signatures**: 
- Models show high performance on training data but poor generalization
- SHAP analysis reveals overreliance on non-linguistic features
- Named entity replacement obscures important contextual distinctions
- External validation metrics significantly lag behind internal validation

**First experiments**:
1. Apply SHAP analysis to baseline model to identify spurious features
2. Implement named entity replacement with different granularity levels
3. Compare model performance with and without extended preprocessing on external validation sets

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive focus on COVID-19 related datasets may limit generalizability to other disinformation domains
- The two-stage SHAP analysis depends heavily on quality of feature engineering and preprocessing steps
- Replacement of named entities with generic categories could obscure important contextual information in certain types of disinformation

## Confidence
- High confidence in methodological framework and SHAP-based feature analysis approach
- Medium confidence in reproducibility of results across different disinformation domains
- Medium confidence in generalizability of named entity replacement strategy

## Next Checks
1. Replicate the study using disinformation datasets from multiple domains (political, health, financial) to assess cross-domain robustness
2. Conduct ablation studies to quantify the individual contributions of SHAP analysis versus named entity replacement to performance gains
3. Perform human evaluation studies to verify that removed features are indeed spurious and that replaced entities maintain sufficient context for accurate classification