---
ver: rpa2
title: 'Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet
  Transform'
arxiv_id: '2512.18791'
source_url: https://arxiv.org/abs/2512.18791
tags:
- watermark
- audio
- diffusion
- smark
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Smark, a universal watermarking framework for
  TTS diffusion models. The key idea is to embed watermarks into the low-frequency
  LL sub-band of Mel spectrograms via Discrete Wavelet Transform (DWT) during the
  reverse diffusion process, leveraging the shared mathematical paradigm across different
  TTS models.
---

# Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform

## Quick Facts
- **arXiv ID:** 2512.18791
- **Source URL:** https://arxiv.org/abs/2512.18791
- **Reference count:** 34
- **Primary result:** Proposes universal watermarking framework for TTS diffusion models using DWT-based low-frequency embedding

## Executive Summary
This paper introduces Smark, a universal watermarking framework for text-to-speech diffusion models that embeds watermarks into the low-frequency LL sub-band of Mel spectrograms using Discrete Wavelet Transform. The approach leverages the shared mathematical paradigm across different TTS models by fusing watermarks during the reverse diffusion process via a lightweight neural network embedder. Extensive experiments demonstrate that Smark achieves perfect watermark extraction accuracy while maintaining high audio quality across multiple datasets and model architectures, outperforming existing methods in both fidelity and robustness.

## Method Summary
Smark embeds watermarks by decomposing Mel spectrograms into frequency sub-bands using Discrete Wavelet Transform, then fusing the watermark into the low-frequency LL sub-band through a lightweight neural network during the reverse diffusion process. The watermark embedder operates concurrently with the denoising process, while the extractor retrieves the watermark post-generation. The framework exploits the mathematical consistency across TTS diffusion models by operating in the frequency domain rather than model-specific parameter spaces, enabling universal applicability. The watermark fusion occurs in the LL sub-band to maximize robustness while minimizing perceptual distortion.

## Key Results
- Achieves perfect watermark extraction accuracy (ACC = 1.0) across all tested conditions
- Maintains high audio quality with PESQ ≥ 3.5 and STOI ≥ 0.88
- Demonstrates robustness against clipping, noise, and filtering attacks
- Outperforms existing watermarking methods in both fidelity and robustness metrics

## Why This Works (Mechanism)
The approach works by exploiting the mathematical consistency of frequency decomposition across TTS diffusion models. By embedding watermarks in the low-frequency LL sub-band rather than model parameters, Smark achieves universal applicability while maintaining perceptual quality. The DWT decomposition separates frequency components, allowing watermark insertion in perceptually less important regions that are more robust to common audio distortions. The lightweight neural network efficiently fuses watermarks during the reverse diffusion process without significantly impacting generation speed or quality.

## Foundational Learning
- **Discrete Wavelet Transform (DWT):** Multi-resolution frequency decomposition that separates signals into approximation and detail coefficients across scales. Needed for frequency-based watermark embedding that's independent of model architecture. Quick check: Verify DWT sub-band consistency across different TTS model outputs.
- **Reverse Diffusion Process:** Gradual denoising trajectory that reconstructs audio from noise using learned conditional distributions. Needed as the operational context where watermark embedding occurs. Quick check: Confirm watermark fusion timing relative to denoising steps.
- **LL Sub-band Characteristics:** Low-frequency approximation coefficients that capture signal structure while being robust to high-frequency distortions. Needed for optimal watermark embedding location. Quick check: Analyze LL sub-band energy distribution across different audio samples.
- **Watermark Extraction Accuracy (ACC):** Metric measuring perfect retrieval of embedded watermarks under various conditions. Needed to quantify robustness claims. Quick check: Test extraction under varying signal-to-noise ratios.
- **Perceptual Quality Metrics (PESQ/STOI):** Objective measures of audio fidelity and intelligibility. Needed to ensure watermarking doesn't degrade user experience. Quick check: Compare metric distributions with and without watermarking.

## Architecture Onboarding

**Component Map:** TTS Model -> DWT Decomposition -> LL Sub-band Watermark Fusion -> Reconstructed Mel Spectrogram -> Audio Generation

**Critical Path:** Reverse diffusion denoising -> DWT decomposition -> watermark fusion in LL sub-band -> inverse DWT -> spectrogram reconstruction

**Design Tradeoffs:** Embedding in LL sub-band maximizes robustness but limits watermark capacity; lightweight neural network balances embedding quality with computational overhead; frequency-domain approach ensures universality but requires careful parameter tuning.

**Failure Signatures:** Degraded watermark extraction accuracy indicates suboptimal fusion parameters; reduced PESQ/STOI scores suggest perceptual quality compromise; inconsistent watermark extraction across models reveals universality limitations.

**First Experiments:** 1) Test watermark extraction accuracy on single TTS model with controlled noise levels; 2) Measure PESQ/STOI impact of watermark embedding across different LL sub-band fusion strengths; 3) Evaluate robustness against progressive clipping severity.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope to specific attack types (clipping, noise, filtering) without testing compression or codec transformations
- Universal applicability claims based on only three TTS architectures requiring broader validation
- Absence of subjective listening tests despite objective quality metrics
- Unquantified computational overhead for real-time production scenarios

## Confidence
- Watermark extraction accuracy under tested conditions: High
- Audio quality preservation claims: Medium
- Universal applicability across TTS models: Low
- Robustness to all potential real-world attacks: Low

## Next Checks
1. Test watermark robustness against compression, resampling, and codec transformations not covered in current attack suite
2. Evaluate performance on additional TTS architectures including autoregressive and flow-based models
3. Measure inference-time computational overhead and memory requirements for production deployment scenarios