---
ver: rpa2
title: 'The Birth of Knowledge: Emergent Features across Time, Space, and Scale in
  Large Language Models'
arxiv_id: '2505.19440'
source_url: https://arxiv.org/abs/2505.19440
tags:
- concepts
- arxiv
- features
- across
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how interpretable categorical features
  emerge in large language models (LLMs) across training time, transformer layers,
  and model scale. Using sparse autoencoders with automated interpretability labeling,
  the researchers tracked concept activation for nine academic domains in the Pythia
  model family.
---

# The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models

## Quick Facts
- arXiv ID: 2505.19440
- Source URL: https://arxiv.org/abs/2505.19440
- Reference count: 40
- Primary result: Sparse autoencoders track interpretable concept emergence across training time, layers, and model scale in LLMs

## Executive Summary
This study investigates how interpretable categorical features emerge in large language models (LLMs) across training time, transformer layers, and model scale. Using sparse autoencoders with automated interpretability labeling, the researchers tracked concept activation for nine academic domains in the Pythia model family. The analysis reveals structured emergence patterns rather than random feature evolution, with domains showing distinct temporal patterns and features temporarily vanishing during middle layers before re-emerging at later layers.

## Method Summary
The researchers used top-k sparse autoencoders (h=512, k=1) trained on Pythia-12B residual stream activations (d=5120) to extract interpretable features. They employed AutoInterp to automatically label neurons with F1≥0.9 using a teacher LLM, then used EyeSee to match neurons to domain queries via cosine similarity. The study probed activations across 25 training checkpoints, all 36 transformer layers, and 10 model scales from 14M to 12B parameters, tracking feature activation rates for nine academic domains using MMLU and MMLU-PRO datasets.

## Key Results
- Concept activation grew from under 3% at initialization to over 99% at convergence across all domains
- Spatial analysis revealed early-layer features temporarily disappear during middle layers but re-emerge at later layers
- Scale analysis showed sharp threshold at 410M parameters, where activation jumped from under 5% to over 95% for most domains
- Different domains showed distinct temporal patterns, with formal/symbolic domains (Physics, Math) emerging earlier than contextual domains (History, Biology)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-k sparse autoencoders extract monosemantic features from residual stream activations that correspond to human-interpretable concepts.
- Mechanism: The hard top-k constraint forces exactly k latent units to participate in reconstruction per forward pass, accelerating dictionary formation compared to L1-regularized variants where sparsity emerges gradually.
- Core assumption: Semantic concepts are encoded in linear directions within the residual stream that can be disentangled through sparse decomposition.
- Evidence anchors: [abstract] "Using sparse autoencoders for mechanistic interpretability, we identify when and where specific semantic concepts emerge within neural activations." [section 2.2.1] "The strict cap of k active units accelerates dictionary formation: each optimisation step forces exactly k basis vectors to participate."

### Mechanism 2
- Claim: Categorical knowledge emerges at domain-specific training checkpoints rather than uniformly, with formal/symbolic domains appearing earlier than contextual domains.
- Mechanism: Training optimization progressively structures internal representations; high-frequency numerical and symbolic patterns (Physics, Mathematics) form feature directions early, while domains requiring contextual structure (History, Biology) stabilize only after lower-level patterns are learned.
- Core assumption: The training corpus contains sufficient signal for each domain and checkpoint sampling captures the true emergence dynamics.
- Evidence anchors: [section 4] "Early-onset domains (Physics, Mathematics, Economics, Law, Philosophy). These subjects register non-zero activations from the very first optimisation steps... Late-onset domains (History, Biology, Chemistry, Business). Activations remain at 0% until roughly 10^4 steps."

### Mechanism 3
- Claim: Early-layer semantic features temporarily vanish during mid-layer processing then re-emerge at output layers, indicating semantic continuity rather than progressive abstraction.
- Mechanism: The transformer architecture separates into three functional blocks—an input-like block (Layers 1-3) preserving token embeddings, a processing core (Layers 4-35) that distills representations, and an output block (Layer 36) that restores lexical/semantic features for prediction.
- Core assumption: Feature activation measured by SAE probes trained on specific layers accurately reflects semantic content at other layers.
- Evidence anchors: [abstract] "Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics." [section 5] "Early-layer SAEs (Layers 0 and 2) exhibit high initial activations that sharply decline across intermediate layers before partially re-emerging at later layers."

## Foundational Learning

- Concept: **Residual Stream Structure**
  - Why needed here: SAEs operate on residual stream activations; understanding that this is a linear communication channel where layers additively modify representations is essential for interpreting feature extraction results.
  - Quick check question: Can you explain why the residual stream allows features from different layers to be "added" together rather than transformed?

- Concept: **Top-k Sparsity vs L1 Regularization**
  - Why needed here: The paper explicitly chooses top-k SAE over L1-regularized variants; understanding the optimization difference explains why features form faster.
  - Quick check question: Why does forcing exactly k units to activate on every forward pass accelerate meaningful dictionary formation compared to adding a sparsity penalty?

- Concept: **Procrustes Alignment for Cross-Scale Comparison**
  - Why needed here: Comparing features across model scales requires aligning different-dimensional activation spaces; Procrustes rotation preserves geometry while enabling direct comparison.
  - Quick check question: Why is zero-padding inferior to orthogonal rotation when aligning a 128-dimensional activation space to a 5120-dimensional reference?

## Architecture Onboarding

- Component map: Input Pipeline -> SAE Training -> AutoInterp Pipeline -> EyeSee Framework -> Activation Analysis
- Critical path:
  1. Train SAE on final checkpoint activations with h=512, k=1 (optimal from sweep in Figure 2)
  2. Generate and verify neuron labels via AutoInterp (n_label=10, n_verify=5, F1≥0.9)
  3. For each analysis axis, probe activations with domain-aligned neurons and measure activation rates

- Design tradeoffs:
  - **h=512 vs overcomplete**: Authors chose h≪d (512 vs 5120) because coarse categorical concepts are fewer than full polysemantic disentanglement requires; this sacrifices feature granularity for interpretability
  - **k=1 vs higher k**: Mean F1 decreased monotonically with higher k; k=1 maximizes monosemanticity but may miss multi-feature concepts
  - **Single final-token embedding**: Using only the final-token hidden state loses sequence-position information but enables efficient batch processing

- Failure signatures:
  - **Dead neurons**: If >50% of SAE neurons never activate (dead-latent counts in h sweep ranged from 122-211 of 512), increase aux-k promotion rate or reduce learning rate
  - **Low F1 verification scores**: If mean F1<0.7, check that label prompt includes sufficient activating examples and verification split is truly disjoint
  - **Cross-layer probe mismatch**: If early-layer SAE shows no activation at any depth, verify probe data comes from same distribution as training data

- First 3 experiments:
  1. **Replicate SAE hyperparameter sweep**: Train top-k SAEs with k∈{1,2,4,8} on a subset of Pythia checkpoints; verify k=1 achieves highest mean F1 before proceeding
  2. **Single-domain temporal probe**: Track Physics neuron activations across all 25 Pythia-12B checkpoints; confirm early-onset pattern with activation appearing before step 1000
  3. **Spatial re-emergence test**: Train separate SAEs on Layers 0, 10, and 30 of Pythia-12B; probe Layer-0 SAE features across all 36 layers to confirm dip-and-recovery pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistic function causes semantic features to vanish in middle layers and re-emerge in later layers?
- Basis in paper: [explicit] The authors describe this "unexpected semantic reactivation" as challenging standard assumptions, noting it "can happen for a number of reasons" without identifying the specific cause.
- Why unresolved: The study identifies the "disappear-and-return" pattern descriptively through SAE probes but does not perform circuit analysis to determine if the information is transformed, suppressed, or routed differently during intermediate processing.
- What evidence would resolve it: A circuit-level analysis tracking the flow of specific semantic information through attention heads and MLPs across the transformer stack to see how the representation is maintained or transformed when the SAE feature is inactive.

### Open Question 2
- Question: Does the temporary dip in concept activation between 20,000 and 30,000 training steps result from feature re-organization or optimization instability?
- Basis in paper: [explicit] The authors observe a -31.8 pp dip and hypothesize it reflects "a re-organisation of feature representations or... a temporary reduction in gradient-driven optimization efficacy," but they do not isolate the cause.
- Why unresolved: The analysis tracks aggregate activation percentages rather than the underlying dynamics of weights, gradients, or individual neuron stability during this specific phase.
- What evidence would resolve it: Tracking gradient norms and feature drift rates specifically during the 20k–30k step interval to determine if the drop correlates with a distinct phase transition in the optimizer or internal circuit structure.

### Open Question 3
- Question: To what extent do training data distribution and document ordering drive the distinct temporal emergence patterns of early-onset versus late-onset domains?
- Basis in paper: [inferred] The authors note that early-onset domains like Physics show immediate activation while late-onset domains like History lag, conjecturing that corpus frequency plays a role, but they do not verify this against the data.
- Why unresolved: The study focuses on model checkpoints and internal activations but lacks a corresponding analysis of the pre-training corpus composition (Pythia's Pile data) to correlate with the observed emergence curves.
- What evidence would resolve it: A correlation analysis comparing the cumulative frequency and positional distribution of domain-specific terminology in the training data against the activation timelines observed in the model.

## Limitations
- Temporal analysis may miss rapid activation changes due to 5,000-step checkpoint intervals
- Cross-layer feature continuity depends on probe architecture limitations
- Scale threshold at 410M parameters may be dataset-specific rather than universal

## Confidence
- **High Confidence**: SAE training and activation tracking methodology
- **Medium Confidence**: Temporal emergence patterns by domain
- **Medium Confidence**: Spatial re-emergence pattern
- **Low Confidence**: Scale threshold universality

## Next Checks
1. **Probe Architecture Validation**: Train SAEs on different layers of the same model, then probe cross-layer to confirm semantic reactivation isn't an artifact of residual stream mixing

2. **Temporal Granularity Test**: Repeat temporal analysis with finer checkpoint sampling (every 1,000 steps) around key emergence points to verify rapid activation changes aren't missed

3. **Scale Generalization Test**: Apply the same methodology to a different task/dataset (e.g., SQuAD) to determine if the 410M parameter threshold is dataset-specific or a general property