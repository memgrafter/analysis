---
ver: rpa2
title: Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners
  with Large Language Models
arxiv_id: '2508.11524'
source_url: https://arxiv.org/abs/2508.11524
tags:
- state
- planning
- domain
- llms
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two paradigms for leveraging large language
  models (LLMs) to assist classical planners in handling large-scale planning problems:
  LLM4Inspire and LLM4Predict. LLM4Inspire uses LLMs to select the most promising
  action based on general knowledge, while LLM4Predict predicts intermediate states
  constrained by domain-specific knowledge to partition the search space.'
---

# Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models

## Quick Facts
- arXiv ID: 2508.11524
- Source URL: https://arxiv.org/abs/2508.11524
- Reference count: 8
- Large language models can effectively assist classical planners by either inspiring action selection or predicting intermediate states, with the latter approach showing superior performance.

## Executive Summary
This paper introduces two novel paradigms for leveraging large language models (LLMs) to enhance classical planning: LLM4Inspire and LLM4Predict. The authors propose decomposing large planning problems into simpler sub-tasks and using LLMs to assist in solving them. The key insight is that LLMs can be used either to select promising actions based on general knowledge (Inspire) or to predict intermediate states constrained by domain-specific knowledge (Predict). Experiments across four planning domains demonstrate that the Predict paradigm significantly outperforms the Inspire approach, achieving over 95% success rates in three domains while requiring fewer LLM calls and less computational time.

## Method Summary
The approach involves two distinct paradigms for LLM-assisted planning. LLM4Inspire uses LLMs to evaluate and select the most promising action at each planning step based on general knowledge extracted from the LLM's training data. LLM4Predict, on the other hand, uses LLMs to predict intermediate states while constraining these predictions with domain-specific knowledge encoded in PDDL format. Both methods first decompose large planning problems into smaller sub-tasks before applying LLM assistance. The Predict paradigm partitions the search space by generating plausible intermediate states, effectively reducing the branching factor and guiding the classical planner toward solutions more efficiently.

## Key Results
- LLM4Predict achieved success rates exceeding 95% in Blocks, Logistics, and Depot domains
- LLM4Inspire showed lower performance, indicating general knowledge is less effective than domain-specific knowledge
- LLM4Predict required fewer LLM calls and less solver time compared to LLM4Inspire
- The Predict paradigm demonstrated superior efficiency and effectiveness across all tested domains

## Why This Works (Mechanism)
The success of LLM4Predict stems from its ability to leverage domain-specific knowledge while maintaining the flexibility of language models. By constraining LLM predictions with PDDL domain descriptions, the approach combines the generalization capabilities of LLMs with the precision of classical planning representations. This hybrid approach allows the LLM to generate plausible intermediate states that are consistent with the problem domain, effectively reducing the search space and guiding the planner toward feasible solutions. The decomposition of large problems into smaller sub-tasks further enhances efficiency by making each planning problem more manageable for both the LLM and the classical planner.

## Foundational Learning
- Classical Planning: Why needed? Provides the foundation for automated decision-making in deterministic environments; quick check: verify understanding of STRIPS representation and search algorithms
- Large Language Models: Why needed? Offers general reasoning capabilities and knowledge extraction; quick check: confirm understanding of transformer architecture and prompt engineering
- PDDL (Planning Domain Definition Language): Why needed? Encodes domain-specific constraints and knowledge; quick check: test ability to write PDDL representations for simple domains
- Problem Decomposition: Why needed? Enables handling of large-scale planning problems; quick check: practice breaking down complex problems into manageable sub-tasks
- State Space Search: Why needed? Fundamental to understanding planning algorithms; quick check: trace search paths in simple planning problems
- LLM Prompt Engineering: Why needed? Critical for effective interaction with language models; quick check: experiment with different prompt formulations for planning tasks

## Architecture Onboarding

Component Map: Problem Decomposition -> LLM Prediction -> Classical Planning Search -> Solution Assembly

Critical Path: Large problem → Decomposition → LLM-assisted sub-task solving → Solution composition → Final plan

Design Tradeoffs:
- General vs. domain-specific knowledge: LLM4Inspire uses broader knowledge but with less precision, while LLM4Predict achieves better performance through constrained predictions
- LLM call frequency: More calls increase accuracy but also computational cost; the Predict paradigm optimizes this balance
- Problem decomposition granularity: Fine-grained decomposition increases LLM effectiveness but may require more coordination steps

Failure Signatures:
- LLM4Inspire failures typically occur when general knowledge is insufficient for domain-specific decisions
- LLM4Predict failures may arise from incorrect state predictions that lead the planner down invalid paths
- Both approaches can fail when problem decomposition creates dependencies that cannot be resolved independently

Three First Experiments:
1. Compare success rates of LLM4Inspire vs. LLM4Predict on a simple Blocks world problem with varying numbers of blocks
2. Measure the impact of different PDDL constraint formulations on LLM4Predict performance
3. Evaluate the effect of problem decomposition granularity on overall planning success and efficiency

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Limited domain diversity with experiments conducted on only four planning domains
- Single LLM configuration used without exploring alternative models or architectures
- No baseline comparison to classical planners without LLM assistance
- Potential data leakage concerns from using PDDL descriptions in LLM context

## Confidence
- High confidence in LLM4Predict's superior performance within tested domains
- Medium confidence in the general applicability of LLM4Predict across diverse planning problems
- Medium confidence in the relative effectiveness of domain-specific knowledge versus general knowledge for LLM-assisted planning
- Low confidence in the scalability of the approach to extremely large or complex planning problems

## Next Checks
1. Test the LLM4Predict approach on a broader range of planning domains, including those with different structural characteristics and complexity levels
2. Conduct ablation studies varying the LLM model size, context window, and prompt engineering techniques to identify optimal configurations
3. Compare the LLM-assisted approach against state-of-the-art classical planners and recent neural planners to establish its relative effectiveness in the broader planning landscape