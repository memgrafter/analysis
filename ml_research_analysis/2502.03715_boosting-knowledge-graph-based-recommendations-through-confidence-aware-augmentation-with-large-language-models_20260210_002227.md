---
ver: rpa2
title: Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation
  with Large Language Models
arxiv_id: '2502.03715'
source_url: https://arxiv.org/abs/2502.03715
tags:
- item
- user
- triples
- knowledge
- triplets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CKG-LLMA, a framework that combines knowledge
  graphs with large language models for recommendation tasks. The method uses an LLM-based
  subgraph augmenter to enrich knowledge graphs, a confidence-aware message propagation
  mechanism to filter noisy triplets, and a dual-view contrastive learning method
  to integrate user-item interactions and KG data.
---

# Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation with Large Language Models

## Quick Facts
- arXiv ID: 2502.03715
- Source URL: https://arxiv.org/abs/2502.03715
- Reference count: 18
- Primary result: Up to 5.7% improvement in recall@10 and 3.4% in NDCG@10 on three public datasets

## Executive Summary
CKG-LLMA is a recommendation framework that enhances knowledge graphs (KGs) with LLM-generated information while filtering noise through a confidence-aware mechanism. The approach uses LLM-based subgraph augmentation to enrich KGs, confidence scores to filter noisy triplets during message propagation, and dual-view contrastive learning to integrate user-item interactions with KG data. Experiments demonstrate consistent improvements over state-of-the-art baselines across three datasets, achieving up to 5.7% better recall@10 and 3.4% better NDCG@10.

## Method Summary
The framework operates through a pipeline where subgraphs (user-view and item-view) are extracted from the KG and sent to an LLM for augmentation, generating four modification pools (user/item add/delete). A learnable confidence score, assigned via a Mixture-of-Experts layer, filters noisy triplets during GNN message propagation. Dual-view contrastive learning maximizes agreement between representations from two augmented KG views. The model is trained using a joint objective combining BPR loss, contrastive loss, and regularization. The entire process is designed to produce more robust user/item embeddings while generating interpretable explanations for recommendations.

## Key Results
- Achieves up to 5.7% improvement in recall@10 compared to baselines
- Achieves up to 3.4% improvement in NDCG@10 across three datasets
- Ablation studies confirm the importance of confidence-aware message propagation and contrastive learning components

## Why This Works (Mechanism)

### Mechanism 1: LLM-based Dual-View Subgraph Augmentation
- Claim: LLMs can enrich sparse or outdated KGs by identifying missing relationships and erroneous triplets through semantic verification
- Mechanism: Extracts subgraphs (User-view and Item-view) to bypass token limits, uses tailored prompts to instruct LLM to verify facts and reason about user preferences, generating four modification pools: user-view add/delete and item-view add/delete
- Core assumption: LLMs possess sufficient internalized world knowledge to correct domain-specific KGs and can generalize reasoning to implicit user-item interactions
- Evidence anchors:
  - [abstract] "an LLM-based subgraph augmenter for enriching KGs with high-quality information"
  - [Section 3.2] "Our prompt design leverages the LLM's ability to detect contextual nuances... and provide modification advice..."
  - [corpus] Related work confirms LLMs' efficacy in refining KGs (e.g., KERL, HealthGenie), but this paper specifically addresses hallucinations in this augmentation step
- Break condition: This mechanism fails if the LLM's internal knowledge is insufficient for the specific domain or if hallucination rates are so high that the noise introduced outweighs the benefit of the additions

### Mechanism 2: Confidence-Aware Message Propagation
- Claim: Not all LLM-generated or original KG triplets are equally reliable; a learnable confidence score can filter noisy triplets to improve item embeddings
- Mechanism: Assigns a learnable confidence score to each triplet via a Mixture-of-Experts layer that takes relation and entity embeddings as input, acting as a gating mechanism to weight triplet contributions during GNN message propagation
- Core assumption: The reliability of a triplet for a recommendation task can be learned from user-item interaction data and is correlated with its semantic correctness
- Evidence anchors:
  - [abstract] "confidence-aware message propagation to filter noisy triplets"
  - [Section 3.3] "...introduce a learnable edge confidence for each item-attribute triplet... assigns a confidence score for LLM-enhanced edges..."
  - [Table 3 (Ablation)] Removing this confidence mechanism leads to performance drop (e.g., Recall on AmazonBook drops from 0.1883 to 0.1773)
- Break condition: Fails if the noise in the KG is so systematic that the model cannot learn to distinguish it, or if the MOE layer is too simple to capture the complexity of triplet reliability

### Mechanism 3: Dual-View Two-Step Contrastive Learning
- Claim: Contrasting views of data augmented with LLM knowledge against original data forces the model to learn more robust user and item representations
- Mechanism: Constructs two views of the KG using differentiable sampling based on learned confidence scores, applies contrastive loss to maximize agreement between representations of the same user/item from these two different augmented views while minimizing agreement with others
- Core assumption: Valuable semantic information added by the LLM creates a signal consistent across both augmented views, whereas random noise is inconsistent; aligning representations across views captures this invariant signal
- Evidence anchors:
  - [abstract] "dual-view contrastive learning to integrate user-item interactions and KG data"
  - [Section 3.4] "we construct a dual-view contrastive learning framework... to direct data augmentation through cross-view self-supervised signals."
  - [Table 3 (Ablation)] Removing the contrastive loss causes severe performance degradation (e.g., Recall drops from 0.1883 to 0.1312 on AmazonBook)
- Break condition: Fails if LLM augmentation is too noisy, leaving little invariant signal to align, or if contrastive negative samples are not effectively mined

## Foundational Learning

**Concept: Knowledge Graphs (KGs) for Recommendation**
- Why needed here: The entire framework is built on top of a KG, which represents entities (users, items, attributes) and their semantic relationships (triplets)
- Quick check question: Can you explain the difference between a User-Item interaction graph and a Knowledge Graph containing Item-Attribute and Item-Item triplets?

**Concept: Graph Neural Networks (GNNs) & Message Propagation**
- Why needed here: Core embedding learning happens by propagating information across the graph. Understanding how GNNs aggregate neighbor information is essential to grasp how the confidence mechanism works
- Quick check question: In a GNN, how does an item node update its embedding based on its connected neighbors (e.g., other items, attributes)?

**Concept: Contrastive Learning**
- Why needed here: The model uses contrastive learning as its primary self-supervised learning objective to learn robust representations from the LLM-augmented data
- Quick check question: What is the core idea of contrastive learning? How does it use "positive" and "negative" pairs to train a model?

## Architecture Onboarding

**Component map:**
LLM-based Subgraph Augmenter -> Data Sampler -> Confidence-Aware MOE Layer -> GNN Encoder -> Contrastive Learning Head -> Explanation Generator

**Critical path:** Performance hinges on the Confidence-Aware MOE Layer correctly learning to weight triplets. If it fails to distinguish noise from signal, downstream GNN and contrastive learning will be corrupted. The contrastive loss is the key driver for learning these robust representations.

**Design tradeoffs:**
- **LLM Cost vs. KG Quality:** Framework relies on an external LLM for augmentation ($27 for AmazonBook). Tradeoff is between cost and augmented graph quality
- **Augmentation Ratio:** Hyperparameters μ_a (add ratio) and μ_d (delete ratio) control aggressiveness of LLM modification. Over-augmenting introduces noise; under-augmenting misses opportunities
- **MOE Complexity:** Using Mixture-of-Experts for confidence calculation adds parameters but is necessary for capturing fine-grained reliability

**Failure signatures:**
- **High LLM Hallucination Rate:** If LLM advice is mostly wrong, model will struggle. Symptom: "w/o confidence" ablation performs similarly or better than full model
- **Contrastive Collapse:** If contrastive loss dominates or fails to converge, model performance degrades significantly (as seen in "w/o contrastive" ablation)
- **Over-smoothing in GNN:** If GNN is too deep or graph is too dense/noisy, embeddings for all items become indistinguishable

**First 3 experiments:**
1. **Reproduce Ablation on Confidence:** Run full model vs. variant with fixed confidence of 1.0 for all triplets. Quantify performance drop on validation set to confirm denoising benefit
2. **Impact of LLM Sampling Ratio:** Train models with different proportions of LLM advice (e.g., 20%, 50%, 100%). Plot performance vs. ratio to find optimal point and understand sensitivity to noise
3. **Explanation Quality Check:** Manually inspect explanations generated with and without confidence scores (using test cases). Correlate explanations with LLM's chosen reasoning paths to verify if confidence scores guide LLM toward more plausible rationales

## Open Questions the Paper Calls Out

**Open Question 1:** How does the performance of CKG-LLMA vary when utilizing open-source Large Language Models compared to proprietary models like GPT-3.5?
- Basis in paper: [explicit] The conclusion states, "In the future, we plan to explore the potential of using different types of LLMs," while the experiments relied solely on ChatGPT-turbo-3.5
- Why unresolved: Different LLMs possess varying reasoning capabilities and hallucination rates, which could significantly impact the quality of the "add/delete" augmentation pools
- What evidence would resolve it: Comparative analysis of recommendation metrics (Recall/NDCG) and augmentation quality when swapping GPT-3.5 for models like LLaMA or Mistral

**Open Question 2:** Can the efficiency of the subgraph augmentation process be optimized to support real-time or large-scale industrial deployment?
- Basis in paper: [explicit] The authors acknowledge the need to "further optimize the efficiency of the subgraph augmentation process" in the conclusion
- Why unresolved: The current framework performs augmentation offline to manage API costs and token limits, which may not scale efficiently for dynamic, evolving knowledge graphs
- What evidence would resolve it: A proposed streaming or incremental augmentation mechanism that maintains accuracy while reducing latency and computational overhead

**Open Question 3:** Is the confidence-aware mechanism robust enough to distinguish between semantic hallucinations and useful "implicit" knowledge inferred by the LLM?
- Basis in paper: [inferred] The paper highlights that LLMs treat user behaviors as "implicit knowledge" (reasoning) rather than explicit fact verification, yet the confidence mechanism filters this based on learnable weights
- Why unresolved: The paper does not clarify if the confidence score effectively separates creative reasoning (useful) from fabrication (hallucination) in the user-view extraction phase
- What evidence would resolve it: A qualitative case study analyzing the confidence scores assigned to verified facts versus hallucinated triplets in the user-view augmentation pool

## Limitations

**Unknown II-triplet Construction:** The paper does not fully specify how item-item (II) triplets are generated from shared attributes, particularly the thresholds and rules for creating "same_category" edges, affecting the completeness of the tripartite KG.

**Generalization Across Domains:** The reliance on LLM-based augmentation raises questions about generalization to domains with limited pre-training coverage or highly specialized terminology.

**Explanation Quality Metrics:** The paper demonstrates qualitative examples of generated explanations but lacks quantitative evaluation of explanation quality or user studies to validate their helpfulness in real-world scenarios.

## Confidence

**High Confidence:** The core mechanisms (LLM augmentation, confidence-aware message propagation, contrastive learning) are technically sound and supported by ablation studies showing consistent performance improvements when each component is included.

**Medium Confidence:** The specific hyperparameter settings (especially sampling ratios and confidence thresholds) are well-documented for Amazon-Book but require extrapolation for other datasets, introducing potential reproducibility challenges.

**Low Confidence:** The long-term effectiveness of the approach depends on LLM behavior, which may change with model updates or API variations, potentially affecting the consistency of augmentation quality over time.

## Next Checks

1. **Cross-Domain Validation:** Apply CKG-LLMA to a dataset from a significantly different domain (e.g., scientific papers or medical products) to test generalization beyond the tested domains of books, games, and anime.

2. **Temporal Robustness Test:** Evaluate the framework's performance when applied to time-split data (e.g., training on older interactions and testing on recent ones) to assess how well LLM-augmented KGs handle temporal dynamics and concept drift.

3. **Cost-Benefit Analysis:** Quantify the marginal performance gain from LLM augmentation against the computational cost and API expenses across different dataset sizes to determine the practical scalability limits of the approach.