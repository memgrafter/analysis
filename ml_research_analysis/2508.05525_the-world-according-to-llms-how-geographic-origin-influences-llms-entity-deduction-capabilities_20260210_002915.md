---
ver: rpa2
title: 'The World According to LLMs: How Geographic Origin Influences LLMs'' Entity
  Deduction Capabilities'
arxiv_id: '2508.05525'
source_url: https://arxiv.org/abs/2508.05525
tags:
- entity
- entities
- language
- notable
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to uncover implicit geographic
  biases in large language models (LLMs) by having models play the 20 Questions game,
  proactively asking questions to deduce entities from diverse regions. The authors
  created a new dataset, Geo20Q+, featuring geographically specific entities from
  the Global North and South, and Global West and East.
---

# The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities

## Quick Facts
- **arXiv ID:** 2508.05525
- **Source URL:** https://arxiv.org/abs/2508.05525
- **Reference count:** 40
- **Primary result:** LLMs demonstrate significantly better deductive performance for entities from the Global North and West compared to the Global South and East, even when controlling for entity popularity and language.

## Executive Summary
This paper introduces a novel approach to uncover implicit geographic biases in large language models (LLMs) by having models play the 20 Questions game, proactively asking questions to deduce entities from diverse regions. The authors created a new dataset, Geo20Q+, featuring geographically specific entities from the Global North and South, and Global West and East. Evaluating three popular LLMs across seven languages and two gameplay settings, the study reveals significant geographic disparities: models perform substantially better at deducing entities from the Global North and West compared to the Global South and East. These disparities persist even when controlling for entity popularity and are largely unaffected by the language of gameplay. The results demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups.

## Method Summary
The study uses a novel interactive evaluation framework where two instances of the same LLM engage in a 20 Questions game: one acts as "Guesser" asking yes/no questions to identify an entity, while the other acts as "Judge" answering based on Wikipedia descriptions. The Geo20Q+ dataset contains 1,200 entities (100 per continent, split between "Things" and "Notable people") with unambiguous country associations. The evaluation runs in two settings: Canonical (max 20 turns) and Unlimited (max 150 turns), across three LLMs and seven languages. Success rates and efficiency (turns to answer) are measured to assess geographic performance disparities.

## Key Results
- Models show substantially higher success rates for deducing entities from the Global North and West compared to the Global South and East
- Geographic performance disparities persist even when controlling for entity popularity metrics like Wikipedia pageviews
- Efficiency disparities remain significant in unlimited turn settings, suggesting knowledge exists but requires more reasoning steps for Global South entities
- Language of gameplay has minimal impact on geographic performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Free-form, model-initiated inquiry chains may reveal implicit geographic hierarchies that are hidden by standard reactive prompting.
- **Mechanism:** By forcing the LLM to act as the "guesser" and generate questions, the model must traverse its internal knowledge graph. The paper suggests models prioritize hypotheses related to the Global North/West early in the chain (e.g., asking "Is it in Europe?" before "Is it in Asia?"), revealing a Western-centric default reasoning structure.
- **Core assumption:** The order and content of generated questions reflect the strength and ordering of associations in the model's latent space.
- **Evidence anchors:** Abstract noting "uncovering subtle biases in LLMs that remain hidden in standard prompting setups" and observations of biases in reasoning hidden in standard evaluations.

### Mechanism 2
- **Claim:** Knowledge regarding Global South entities likely exists in the model but may be organized with higher retrieval friction.
- **Mechanism:** In the "unlimited turns" setting, success rates for Global South entities improved significantly (catching up to North/West), but the efficiency (number of turns required) remained lower. This suggests the knowledge is present but less directly accessible or poorly indexed compared to Western entities.
- **Core assumption:** The number of turns correlates directly with the difficulty of navigating the model's internal representation space.
- **Evidence anchors:** Page 7 noting efficiency disparities persist even with convergent success rates, and that Western entities are deduced in significantly fewer turns.

### Mechanism 3
- **Claim:** Simple proxies for entity prominence (e.g., Wikipedia pageviews) are likely insufficient to explain geographic performance gaps.
- **Mechanism:** Regression analysis showed that while popularity correlates with success, the explanatory power is low (R² < 0.1). This implies the bias is not merely a function of "how often" the model saw the data, but potentially "how" the data was integrated or weighted during pre-training/alignment.
- **Core assumption:** The Dolma corpus frequency is a valid proxy for the proprietary training data of the evaluated models.
- **Evidence anchors:** Page 8 showing R² values consistently low (typically below 0.1) and fail to fully explain substantial geographic performance disparities.

## Foundational Learning

- **Concept: Interactive/Agentic Evaluation**
  - **Why needed here:** The paper moves beyond "prompt-response" (passive) to a multi-turn game (active). Understanding that the model must maintain state and strategy over time is crucial.
  - **Quick check question:** Does the evaluation method rely on a single model response or a sequence of dependent actions?

- **Concept: Global North/South vs. West/East Divides**
  - **Why needed here:** The paper attributes bias not just to random variance, but to specific socioeconomic and cultural geopolitical categories. You must understand these definitions to interpret the disparity results.
  - **Quick check question:** Is the performance gap defined primarily by continent, or by economic/cultural dominance indices?

- **Concept: Implicit vs. Explicit Bias**
  - **Why needed here:** The paper argues that standard alignment removes explicit slurs but leaves implicit "knowledge hierarchies." The mechanism relies on the idea that the model isn't *saying* biased things, but *reasoning* along biased paths.
  - **Quick check question:** Would a standard toxicity filter catch the biases identified in this paper? (Answer: No).

## Architecture Onboarding

- **Component map:** Geo20Q+ Dataset -> Judge Module (Yes/No/Maybe/Bingo) -> Guesser Module (Strategic Questions) -> Game Loop (Turn counter 20/150)
- **Critical path:** Entity Selection -> Localization (Translate) -> Game Execution (Guesser generates Q -> Judge responds -> Update History) -> Termination (Guesser guesses correctly, gives up, or max turns reached)
- **Design tradeoffs:** Canonical (20) vs. Unlimited (150) for naturalistic stress vs. retrieval friction testing; Single-run vs. Multi-run for cost vs. variance reduction
- **Failure signatures:** Early Termination (outputting "I give up"), Repetitive Questioning (same question loop), Western Defaulting (consistently asking "Is it in the US?" for obscure entities)
- **First 3 experiments:**
  1. Sanity Check (Entity Knowledge): Run the "Judge" prompt on a subset of entities to verify the model *knows* the entity exists and has basic facts correct
  2. Path Tracing (Small Scale): Run 20 games for "Eiffel Tower" vs. "Taj Mahal" and manually log the *first* 3 questions asked by the Guesser
  3. Language Ablation: Play the game with one entity (e.g., a Japanese landmark) in English vs. Japanese to validate the claim that "language... has minimal impact"

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do specific internal activation paths or attention mechanisms systematically favor dominant geographic regions during model decision-making?
- **Basis in paper:** The authors propose analyzing model attention and reasoning patterns using interpretability methods like Patchscopes to determine if certain regions of the model are favored at decision points.
- **Why unresolved:** The current study evaluates behavioral outcomes (success rates) but does not inspect the internal representational geometry or activation trails that lead to these results.
- **What evidence would resolve it:** Mechanistic interpretability studies mapping hidden states and attention heads to specific geographic entity types during the deduction process.

### Open Question 2
- **Question:** To what extent do distinct tuning and alignment techniques (e.g., RLHF, instruction tuning) causally contribute to observed geographic biases?
- **Basis in paper:** The paper suggests systematically evaluating the impact of different tuning techniques by retraining models with regionally balanced data to observe shifts in deduction performance.
- **Why unresolved:** It remains unclear whether biases stem primarily from pre-training data coverage or from the alignment processes that fine-tune model behavior.
- **What evidence would resolve it:** Ablation studies comparing model versions trained with varied alignment objectives and regionally balanced datasets on the Geo20Q+ benchmark.

### Open Question 3
- **Question:** Do models systematically misidentify entities from the Global South/East as specific entities from the Global North/West?
- **Basis in paper:** The authors note a limitation that they "do not explicitly analyze misidentification behavior," where models might guess entities from dominant regions in place of less-represented ones.
- **Why unresolved:** The current metrics focus on binary success rates and turn counts rather than the semantic nature of incorrect guesses.
- **What evidence would resolve it:** A fine-grained error analysis categorizing incorrect guesses by the geographic origin of the hallucinated entity to reveal systematic substitution patterns.

## Limitations
- Single-run design lacks variance estimates, making it difficult to distinguish systematic bias from stochastic variation
- Judge prompt design depends on LLM accurately representing entity characteristics, potentially creating false positives
- Entity selection process may introduce selection bias where Western entities are more easily disambiguated than Global South entities

## Confidence
- **High confidence** in the observation that models show better deductive performance for Global North/West entities across all tested models and settings
- **Medium confidence** in the claim that knowledge exists but is harder to retrieve for Global South entities (Mechanism 2)
- **Low confidence** in the precise mechanisms driving geographic prioritization in question ordering (Mechanism 1)

## Next Checks
1. **Variance estimation** - Replicate 10-20 runs for a stratified sample of 50 entities (10 from each geographic category) to estimate confidence intervals around success rates and test whether observed disparities remain statistically significant

2. **Knowledge verification** - Before gameplay, query the Judge model directly with "What is [entity]?" for all entities to verify the model possesses basic knowledge. Flag entities where the model cannot answer, then analyze whether geographic gaps correlate with knowledge absence vs. retrieval difficulty

3. **Prompt sensitivity analysis** - Systematically vary the Guesser prompt's temperature, top_p, and seed parameters across 5 levels for 20 entities. Measure how variance in reasoning paths changes with parameter settings and whether this affects geographic disparity patterns