---
ver: rpa2
title: Learning Instruction-Following Policies through Open-Ended Instruction Relabeling
  with Large Language Models
arxiv_id: '2506.20061'
source_url: https://arxiv.org/abs/2506.20061
tags:
- instructions
- instruction
- learning
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OIR, a novel approach that uses large language
  models (LLMs) to automatically generate open-ended instructions from agent trajectories
  in reinforcement learning. By retrospectively relabeling unsuccessful trajectories
  with meaningful subtasks, OIR enriches training data without requiring human annotations.
---

# Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models

## Quick Facts
- **arXiv ID**: 2506.20061
- **Source URL**: https://arxiv.org/abs/2506.20061
- **Reference count**: 35
- **Primary result**: OIR significantly improves sample efficiency, instruction coverage, and policy performance in instruction-following RL using LLM-generated relabeling

## Executive Summary
This paper introduces OIR (Open-Ended Instruction Relabeling), a novel approach that leverages large language models to automatically generate open-ended instructions from agent trajectories in reinforcement learning. The method addresses the challenge of sparse rewards in instruction-following tasks by retrospectively relabeling unsuccessful trajectories with semantically meaningful subtasks identified by an LLM. Evaluated on the Craftax environment, OIR demonstrates superior sample efficiency, instruction coverage, and policy performance compared to baselines, achieving higher success rates and completing more tasks without requiring human annotations.

## Method Summary
OIR employs LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished. The method converts trajectories to textual format, prompts an LLM (Qwen3-8B) to generate K candidate instructions, and uses an embedding-based semantic reward function based on cosine similarity between state-transition embeddings and instruction embeddings. A prioritized instruction buffer with eviction-based curriculum focuses learning on "learning-boundary" instructions. The approach trains a PQN-based policy network that conditions on embedded instructions, enabling the agent to learn from dense semantic rewards without requiring environment-provided rewards.

## Key Results
- OIR significantly improves sample efficiency compared to baselines in instruction-following RL
- The method achieves higher success rates and completes more tasks across diverse instruction sets
- OIR demonstrates superior generalization to unseen instructions through open-ended instruction generation
- The approach shows effective sample efficiency and generalization capabilities without human annotations

## Why This Works (Mechanism)

### Mechanism 1: LLM-Guided Hindsight Instruction Relabeling
- **Core assumption**: LLMs can accurately infer accomplished subtasks from textual trajectory descriptions
- **Evidence**: OIR transforms unsuccessful trajectories into learning signals by having an LLM identify semantically meaningful subtasks the agent implicitly accomplished
- **Break condition**: If LLM generates instructions unrelated to actual trajectory behavior, semantic rewards become misaligned

### Mechanism 2: Embedding-Based Semantic Reward Function
- **Core assumption**: Semantic similarity in embedding space correlates with task completion in the environment
- **Evidence**: Binary achievement rewards approximated via cosine similarity between state-transition embeddings and instruction embeddings
- **Break condition**: If embedding space poorly captures task-relevant semantics, rewards become noisy or misleading

### Mechanism 3: Prioritized Instruction Buffer with Eviction-Based Curriculum
- **Core assumption**: Tasks at the learning boundary provide the most informative gradients for policy improvement
- **Evidence**: Instructions categorized by empirical mean return into Failing/Learning-boundary/Mastered with learning-boundary preferred
- **Break condition**: Fixed buffer capacity causes forgetting of infrequently-sampled instructions

## Foundational Learning

- **Hindsight Experience Replay (HER)**: OIR extends HER from state-based goals to open-ended natural language instructions. *Quick check*: Can you explain why relabeling a failed trajectory with an achieved state as the goal creates a successful learning example?
- **Instruction-Conditioned RL**: The policy conditions on embedded instructions πθ(at | st, finstr(i)). *Quick check*: How does conditioning a policy on an instruction embedding differ from standard state-only conditioning?
- **Sentence Embeddings / Semantic Similarity**: The reward function relies on cosine similarity between SBERT embeddings of states and instructions. *Quick check*: Why might cosine similarity in embedding space fail to capture whether an instruction was actually completed?

## Architecture Onboarding

- **Component map**: Environment (Craftax) -> Policy Network (PQN) -> Instruction Buffer -> LLM (Qwen3-8B) -> Embedders (fstate, finstr)
- **Critical path**: 1) Sample instruction from buffer, 2) Deploy policy in environments, 3) Collect trajectories and convert to text, 4) Query LLM for candidate instructions, 5) Compute semantic rewards via cosine similarity, 6) Update instruction buffer with priority eviction, 7) Update policy parameters via PQN, 8) Reset environments with new instructions
- **Design tradeoffs**: Buffer size (Bmax=10) balances focus vs forgetting risk; threshold δ balances early density vs final performance; K candidates balance diversity vs LLM query costs
- **Failure signatures**: Catastrophic forgetting of infrequent instructions, reward misalignment from LLM hallucinations, buffer collapse when all instructions reach same status category
- **First 3 experiments**: 1) Ablation on threshold δ to confirm trade-off between early density and final performance, 2) Buffer size sensitivity to characterize forgetting vs focus trade-off, 3) LLM-free baseline with template-based relabeling to isolate contribution of open-ended semantic generation

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies heavily on LLM-generated instructions without explicit verification of semantic accuracy
- Small instruction buffer (size=10) causes catastrophic forgetting of infrequent instructions
- Claims about superiority over human-annotated baselines are based on synthetic instruction variants rather than truly unseen natural language instructions

## Confidence
- **High confidence**: The core mechanism of using LLMs for retrospective instruction relabeling is technically sound and the ablation results are reproducible
- **Medium confidence**: The semantic reward formulation works well in Craftax but may not generalize to domains where state-instruction similarity doesn't correlate with task completion
- **Low confidence**: Claims about OIR's superiority over human-annotated baselines are based on synthetic instruction variants rather than truly unseen natural language instructions

## Next Checks
1. **Cross-domain generalization**: Test OIR on a different instruction-following environment (e.g., MiniGrid or Atari with text prompts) to verify that LLM-guided relabeling transfers beyond Craftax's crafting domain
2. **Instruction coverage analysis**: Track the diversity of instructions generated by the LLM over training; measure whether the method converges to a narrow set of relabeling patterns or maintains broad coverage
3. **Human evaluation of generated instructions**: Sample trajectories from OIR training and have human annotators rate whether LLM-generated instructions accurately describe the accomplished subtasks, establishing ground truth for semantic reward alignment