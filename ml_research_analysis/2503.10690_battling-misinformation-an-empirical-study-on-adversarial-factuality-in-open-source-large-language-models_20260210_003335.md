---
ver: rpa2
title: 'Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source
  Large Language Models'
arxiv_id: '2503.10690'
source_url: https://arxiv.org/abs/2503.10690
tags:
- adversarial
- confidence
- adversary
- misinformation
- confident
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an empirical study on adversarial factuality
  in open-source large language models, focusing on their ability to detect and correct
  misinformation embedded in adversarial prompts with varying confidence levels. The
  study evaluates eight open-source LLMs across three adversarial confidence tiers:
  strongly confident, moderately confident, and limited confidence.'
---

# Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models

## Quick Facts
- arXiv ID: 2503.10690
- Source URL: https://arxiv.org/abs/2503.10690
- Reference count: 17
- Eight open-source LLMs evaluated on adversarial factuality dataset with varying confidence levels

## Executive Summary
This empirical study evaluates eight open-source large language models on their ability to detect and correct misinformation embedded in adversarial prompts with varying confidence levels. The research introduces a three-tier confidence framework (strongly confident "As we know", moderately confident "I think", limited confidence "I guess") to systematically assess model robustness against adversarial attacks. Results show significant variation in model performance, with LLaMA 3.1 (8B) demonstrating superior detection capabilities while Falcon (7B) performs notably worse. The study reveals that adversarial attacks are most effective when targeting ambiguous or nuanced information where the boundary between correct and incorrect is subtle.

## Method Summary
The study employs a two-stage evaluation framework using GPT-4o as an oracle judge to assess model responses. Researchers evaluated eight open-source LLMs (LLaMA 3.1, Phi 3, Qwen 2.5, Deepseek-v2, Gemma2, Falcon, Mistralite, and LLaVA) on the Adversarial Factuality dataset containing 209 entries. Each entry includes a knowledge fact, its modified (incorrect) version, query, and prompt combining the modified knowledge with query. The evaluation measures Attack Success Rate (ASR) - the percentage of times models fail to detect and correct misinformation. Prompts were generated at three adversarial confidence levels by varying the assertion prefix, and models were assessed on their ability to identify and rectify the embedded misinformation.

## Key Results
- LLaMA 3.1 (8B) demonstrates strongest performance in detecting adversarial inputs across confidence levels
- Falcon (7B) shows comparatively lower performance in adversarial detection
- Most models exhibit improved detection as adversarial confidence decreases, except LLaMA 3.1 (8B) and Phi 3 (3.8B) which show diminished detection under lower-confidence attacks
- Adversarial attacks are most effective when targeting ambiguous or less commonly referenced information

## Why This Works (Mechanism)
The study's effectiveness stems from its systematic approach to evaluating model robustness against adversarial attacks. By using GPT-4o as an oracle judge, the research establishes an objective baseline for assessing whether models correctly identify and correct misinformation. The three-tier confidence framework creates controlled variations in adversarial prompting that reveal how models respond to different assertion strengths. The dataset structure (knowledge fact → modified version → query) enables precise measurement of model performance across specific knowledge domains. This methodology captures both the technical aspects of adversarial attack effectiveness and the nuanced ways models process information under varying confidence assertions.

## Foundational Learning
- Adversarial factuality testing: Why needed - to measure model robustness against deliberately inserted misinformation; Quick check - model correctly identifies and corrects false claims in confidence-anchored prompts
- Two-stage evaluation framework: Why needed - ensures objective assessment of misinformation detection; Quick check - first stage confirms misinformation presence, second stage verifies correction
- Attack Success Rate (ASR) metric: Why needed - quantifies model vulnerability to adversarial attacks; Quick check - lower ASR indicates better detection capability
- Confidence-anchored prompting: Why needed - tests model response to assertions with varying degrees of certainty; Quick check - observe performance differences across "As we know", "I think", and "I guess" variants
- Knowledge boundary ambiguity: Why needed - identifies where models struggle with nuanced information; Quick check - analyze error patterns in responses to subtle misinformation

## Architecture Onboarding

Component map: Adversarial prompts -> LLMs -> GPT-4o evaluation (stage 1: misinformation detection -> stage 2: correction verification) -> ASR calculation

Critical path: Dataset preparation -> Model inference across 3 confidence levels -> GPT-4o evaluation pipeline -> ASR computation and analysis

Design tradeoffs: Single dataset (209 entries) provides consistency but limits generalizability; GPT-4o evaluation ensures standardization but may introduce bias; Three confidence levels enable granular analysis but don't capture all adversarial strategies

Failure signatures: High ASR indicates vulnerability to adversarial attacks; Consistent failure on specific knowledge domains suggests training data gaps; Diminished detection on low-confidence prompts may indicate sycophancy; Exception to confidence-level trend reveals model-specific weaknesses

First experiments: 1) Run baseline inference without adversarial modification to establish performance baseline; 2) Test individual model responses to identify common failure patterns; 3) Validate GPT-4o evaluation pipeline with manual spot-checking on sample responses

## Open Questions the Paper Calls Out
- How do different model architectures and sizes influence robustness against adversarial factuality attacks?
- What additional adversarial strategies beyond confidence-based prompting could be employed to test model robustness?
- How might the results generalize to real-world scenarios where misinformation is embedded in more complex contexts?
- What are the implications of these findings for deploying LLMs in high-stakes domains where misinformation detection is critical?

## Limitations
- Reliance on single dataset (209 entries) may not capture full spectrum of misinformation patterns
- Three confidence levels represent only one approach to adversarial prompting, limiting generalizability
- GPT-4o evaluation introduces potential subjectivity and possible bias since same model could have generated adversarial examples
- Study focuses on open-source LLMs, limiting conclusions about proprietary models
- Dataset may not adequately represent real-world misinformation complexity and context

## Confidence
- High confidence: LLaMA 3.1 (8B) demonstrates superior performance in detecting adversarial inputs
- Medium confidence: Most models improve detection as adversarial confidence decreases; model ranking (Falcon worst, LLaMA 3.1 best)
- Low confidence: Adversarial attacks most effective on ambiguous information; models struggle more with nuanced boundary cases

## Next Checks
1. Validate findings using additional adversarial factuality datasets covering different knowledge domains and alternative adversarial strategies beyond confidence-based prompting
2. Implement evaluation using multiple independent evaluators (human annotators and alternative LLM judges) to assess reliability of the two-stage GPT-4o evaluation pipeline
3. Conduct controlled experiments varying model size and architecture while holding other factors constant to isolate performance differences
4. Test model performance on real-world misinformation scenarios to assess practical applicability of findings