---
ver: rpa2
title: 'P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label
  Class-Incremental Learning'
arxiv_id: '2601.12714'
source_url: https://arxiv.org/abs/2601.12714
tags:
- learning
- prompts
- incremental
- methods
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-label class-incremental learning (MLCIL),
  where a model must continuously learn to recognize new object categories in images
  containing multiple objects. The challenge lies in avoiding catastrophic forgetting
  of old classes while handling feature confusion between multiple labels and domain
  gaps between pre-training and new tasks.
---

# P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2601.12714
- **Source URL:** https://arxiv.org/abs/2601.12714
- **Reference count:** 40
- **Primary result:** State-of-the-art performance in multi-label class-incremental learning, achieving up to 9.9% improvement in final mAP without memory buffers

## Executive Summary
This paper introduces P2L-CA, a parameter-efficient framework for rehearsal-free multi-label class-incremental learning (MLCIL). The framework addresses catastrophic forgetting and feature confusion through class-specific prompts (P2L) and domain adaptation via lightweight adapters (CA). By freezing old parameters and only training new class prompts, P2L-CA achieves state-of-the-art performance on MS-COCO and PASCAL VOC datasets while using minimal trainable parameters and eliminating the need for memory buffers.

## Method Summary
P2L-CA combines a Prompt-to-Label (P2L) module with class-specific prompts and a Continuous Adapter (CA) module for domain adaptation. The ViT-B/16 backbone is frozen, with prompts inserted at layer 6 to disentangle multi-label representations. Lightweight adapters are trained only in the first stage to bridge domain gaps between pre-training and downstream tasks. During incremental stages, only new class prompts and classifiers are trained while adapters and old parameters remain frozen. Semantic priors from CLIP text embeddings can initialize prompts for better semantic-visual alignment.

## Key Results
- Achieves up to 9.9% improvement in final mAP compared to existing rehearsal-free MLCIL methods
- Maintains superior performance across both MS-COCO and PASCAL VOC datasets
- Demonstrates effectiveness with minimal trainable parameters (only new class prompts and classifiers in incremental stages)
- Shows robustness to different incremental splits and class configurations

## Why This Works (Mechanism)

### Mechanism 1: Class-Specific Prompt Disentanglement
- **Claim:** Assigning one dedicated prompt token per class reduces feature confusion in multi-label scenes more effectively than shared task-level prompts.
- **Evidence:** t-SNE visualization shows separated clusters; orthogonal loss adds no gain, suggesting prompts naturally become discriminative. Related work "Sculpting [CLS] Features" reports similar disentanglement benefits via token specialization.
- **Break condition:** If classes are visually near-identical, single-token prompts may lack capacity to capture discriminative features.

### Mechanism 2: Two-Phase Adapter Freezing for Stability-Plasticity Balance
- **Claim:** Training adapters only in the first stage and freezing thereafter preserves domain adaptation while preventing catastrophic forgetting.
- **Evidence:** Freezing adapters: 78.6% vs. unfrozen: 68.2% (10.4% drop confirms freezing is critical). C-ADA validates adapter freezing strategies for rehearsal-free CIL.
- **Break condition:** If later incremental stages introduce substantially different visual domains, frozen adapters become suboptimal.

### Mechanism 3: Semantic Prior Injection via Language Model Initialization
- **Claim:** Initializing prompts with CLIP text embeddings provides better semantic-visual alignment than random initialization.
- **Evidence:** P2L-CA+ outperforms P2L-CA by 1.1-2.8% mAP. t-SNE shows semantically similar classes cluster together with text initialization.
- **Break condition:** If class names are ambiguous or poorly aligned with visual content in the pretrained language-vision space, semantic initialization may provide weak or misleading priors.

## Foundational Learning
- **Vision Transformer (ViT) self-attention and patch embedding:** Essential for understanding token flow and prompt insertion. *Quick check:* Can you explain what happens to an image token's representation as it passes through 12 ViT blocks with class prompts injected at layer 6?
- **Adapter bottleneck design:** Critical for understanding CA module's residual connection and dimension trade-off. *Quick check:* What happens to gradient flow and representational capacity if the adapter bottleneck dimension d' is set too low (e.g., d' = 4) versus too high (e.g., d' = d/2)?
- **Multi-label classification with Asymmetric Loss (ASL):** Important for handling extreme positive-negative imbalance. *Quick check:* Why does ASL down-weight loss for easy negatives more than hard negatives, and how does this interact with incremental class imbalance?

## Architecture Onboarding

### Component map:
Image Encoder (ViT-B/16, frozen) -> Class-Specific Prompt Pool -> FFN Classifiers -> Continuous Adapter Module (layers 4-12)

### Critical path:
1. Stage 1: Initialize prompts (random or CLIP text), train adapters + prompts + FFNs on base classes with ASL
2. Stage t>1: Freeze adapters and old prompts/classifiers; initialize new prompts; train only new prompts + new FFNs on incremental classes
3. Inference: Concatenate all prompts, pass image through full encoder, apply each FFN to its corresponding prompt output, threshold independently