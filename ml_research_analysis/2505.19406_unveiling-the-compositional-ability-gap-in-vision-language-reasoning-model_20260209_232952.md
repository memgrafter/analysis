---
ver: rpa2
title: Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model
arxiv_id: '2505.19406'
source_url: https://arxiv.org/abs/2505.19406
tags:
- reasoning
- compositional
- task
- arxiv
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compositional generalization
  in vision-language models (VLMs), focusing on whether VLMs can integrate independently
  learned skills across modalities and tasks. The authors introduce ComPABench, a
  diagnostic benchmark that evaluates cross-modal, cross-task, and out-of-distribution
  (OOD) compositional reasoning.
---

# Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model

## Quick Facts
- arXiv ID: 2505.19406
- Source URL: https://arxiv.org/abs/2505.19406
- Reference count: 6
- Key outcome: RL-Ground method improves compositional performance from 31.2% to 52.8% accuracy

## Executive Summary
This paper investigates compositional generalization in vision-language models, specifically whether these models can effectively combine independently learned skills across modalities and tasks. The authors identify a significant "compositional ability gap" where models struggle to integrate visual understanding with language reasoning in novel combinations. To address this, they introduce ComPABench, a diagnostic benchmark designed to evaluate cross-modal, cross-task, and out-of-distribution compositional reasoning. Through systematic comparison of supervised fine-tuning versus reinforcement learning approaches, they demonstrate that RL-based training consistently outperforms SFT in compositional settings, but both methods show weaknesses with multimodal inputs. The paper proposes RL-Ground, a novel training method that combines explicit visual-to-text captioning with progress-based rewards, which significantly improves compositional reasoning performance.

## Method Summary
The authors introduce ComPABench, a synthetic diagnostic benchmark designed to systematically evaluate compositional generalization in vision-language models. They compare supervised fine-tuning (SFT) against reinforcement learning (RL) strategies across three compositional dimensions: cross-modal (integrating visual and language skills), cross-task (combining different reasoning tasks), and out-of-distribution generalization. The RL-Ground method proposed combines explicit visual grounding through captioning tasks with progress-based rewards that guide the model through compositional reasoning steps. The training pipeline includes: 1) pre-training on standard vision-language datasets, 2) RL-Ground fine-tuning with combined visual captioning and task-specific rewards, and 3) evaluation on ComPABench's compositional tasks. The approach emphasizes structured intermediate supervision to bridge the gap between unimodal expertise and multimodal reasoning.

## Key Results
- RL-trained models outperform SFT models in compositional settings (31.2% vs 52.8% accuracy on RL-Ground)
- Both SFT and RL approaches struggle with multimodal input despite excelling in unimodal tasks
- RL-Ground achieves 52.8% accuracy on compositional tasks compared to 31.2% for standard RL
- Visual grounding through explicit captioning significantly improves cross-modal compositional reasoning

## Why This Works (Mechanism)
The paper identifies that compositional reasoning failures stem from the inability of standard training methods to properly integrate cross-modal representations and intermediate reasoning steps. The RL-Ground mechanism works by providing explicit visual grounding through captioning tasks, which creates stronger associations between visual features and language representations. The progress-based rewards guide the model through compositional reasoning steps rather than treating complex tasks as monolithic challenges. This decomposition allows the model to leverage learned unimodal skills while building effective integration mechanisms for multimodal reasoning.

## Foundational Learning
- **Compositional Generalization**: The ability to combine learned skills in novel ways - needed to evaluate whether models can go beyond memorization to true reasoning; quick check: test on unseen combinations of known concepts
- **Cross-modal Integration**: Merging visual and language representations effectively - needed because most failures occur at the interface between modalities; quick check: evaluate performance on tasks requiring simultaneous visual and language processing
- **Reinforcement Learning for VLMs**: Using reward-based training for vision-language tasks - needed to enable flexible goal-directed reasoning; quick check: compare reward shaping strategies on compositional benchmarks
- **Visual Grounding**: Connecting abstract language concepts to concrete visual features - needed to prevent language drift from visual reality; quick check: measure caption-to-image alignment quality
- **Intermediate Supervision**: Providing feedback at reasoning steps rather than only final outputs - needed to guide complex compositional processes; quick check: ablate intermediate reward signals in training
- **Out-of-Distribution Generalization**: Performing well on data distributions different from training - needed to assess true reasoning capability vs memorization; quick check: evaluate on systematically perturbed test sets

## Architecture Onboarding
Component map: Vision Encoder -> Language Model -> Reasoning Module -> Output Predictor
Critical path: Visual input → Feature extraction → Cross-modal fusion → Compositional reasoning → Task-specific output
Design tradeoffs: The architecture balances between specialized unimodal encoders versus integrated multimodal representations. Using separate encoders preserves modality-specific expertise but requires complex fusion mechanisms. The RL-Ground approach adds overhead through explicit captioning and progress tracking but provides crucial intermediate supervision.
Failure signatures: Models trained with standard SFT show catastrophic performance drops on compositional tasks despite strong unimodal performance. RL approaches improve but still struggle with complex multimodal combinations. Both approaches exhibit language drift where visual information is ignored in favor of language-only reasoning.
First experiments: 1) Evaluate baseline SFT performance on unimodal vs multimodal compositional tasks, 2) Test RL vs SFT on cross-task compositionality, 3) Measure impact of visual grounding ablation on compositional accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic benchmark construction may not fully capture real-world compositional complexity
- RL-Ground requires additional training steps that may not scale efficiently to larger models
- Evaluation focuses primarily on accuracy metrics, potentially missing other aspects of compositional reasoning

## Confidence
- RL outperforms SFT in compositional settings: High
- Multimodal input as a weakness: High
- RL-Ground improvements due to visual grounding: Medium

## Next Checks
1. Evaluate RL-Ground on real-world compositional tasks beyond the synthetic benchmark
2. Conduct systematic ablation studies isolating visual grounding effects from other RL-Ground components
3. Test compositional improvements on larger, more capable VLMs to assess scaling properties