---
ver: rpa2
title: Efficient On-Device Agents via Adaptive Context Management
arxiv_id: '2511.03728'
source_url: https://arxiv.org/abs/2511.03728
tags:
- tool
- user
- context
- agent
- on-device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a context-efficient framework for on-device
  AI agents that addresses the challenge of limited memory capacity by integrating
  a dual-adapter memory system and a just-in-time tool management protocol. The dual-adapter
  system uses a lightweight State-Tracker to maintain a compressed, structured Context
  State Object that preserves essential conversational details while minimizing token
  overhead.
---

# Efficient On-Device Agents via Adaptive Context Management

## Quick Facts
- **arXiv ID:** 2511.03728
- **Source URL:** https://arxiv.org/abs/2511.03728
- **Reference count:** 40
- **Primary result:** 6x reduction in initial prompt size and 10-25x reduction in context growth rate while maintaining or exceeding baseline performance on multi-turn tasks

## Executive Summary
This paper introduces a context-efficient framework for on-device AI agents that addresses the challenge of limited memory capacity through a dual-adapter memory system and just-in-time tool management protocol. The framework leverages a lightweight State-Tracker to maintain a compressed, structured Context State Object that preserves essential conversational details while minimizing token overhead. Combined with token-efficient tool schemas and selective schema passing, the approach significantly reduces context growth without sacrificing task performance. Evaluated on a 3B-parameter small language model, the system achieved dramatic efficiency gains while matching or exceeding baseline performance on complex multi-turn tasks.

## Method Summary
The authors present a context-efficient framework for on-device AI agents that integrates a dual-adapter memory system with a just-in-time tool management protocol. The core innovation is a lightweight State-Tracker that maintains a compressed, structured Context State Object to preserve essential conversational details while minimizing token overhead. The system employs token-efficient tool schemas and selective schema passing to further reduce context growth. This approach enables significant reductions in both initial prompt size and ongoing context expansion during multi-turn interactions, making it particularly suitable for resource-constrained devices.

## Key Results
- Achieved more than 6-fold reduction in initial prompt size
- Reduced context growth rate by 10-25 fold compared to baselines
- Maintained or exceeded baseline performance on complex multi-turn tasks using a 3B-parameter small language model

## Why This Works (Mechanism)
The framework works by maintaining only essential conversational context through a compressed State-Tracker that creates a structured Context State Object. This object preserves critical information while eliminating redundant or less relevant details that typically accumulate in traditional agent contexts. The just-in-time tool management protocol ensures that only relevant tool schemas are passed to the model at each step, rather than maintaining the entire tool library in context. The combination of lightweight state tracking and selective schema passing dramatically reduces token consumption while preserving the information necessary for coherent task completion across multiple turns.

## Foundational Learning

**State-Tracking in Conversational AI**
*Why needed:* Traditional approaches accumulate full conversation history, leading to exponential context growth
*Quick check:* Can the agent maintain task coherence after removing intermediate conversation turns?

**Context Compression Techniques**
*Why needed:* Large context windows are computationally expensive and impractical for on-device deployment
*Quick check:* Does compressed context preserve sufficient semantic information for task completion?

**Token-Efficient Schema Design**
*Why needed:* Full tool libraries consume excessive tokens when passed repeatedly to the model
*Quick check:* Can selective schema passing maintain tool selection accuracy compared to full library access?

## Architecture Onboarding

**Component Map**
User Query -> State-Tracker -> Context State Object -> Tool Selection -> Response Generation

**Critical Path**
1. User input received
2. State-Tracker updates Context State Object
3. Tool schemas selectively passed based on context
4. Model generates response using compressed context and relevant tools
5. Context State Object updated for next turn

**Design Tradeoffs**
- Compression vs. information retention: Aggressive compression risks losing task-relevant details
- Schema granularity vs. token efficiency: Finer-grained schemas provide better precision but consume more tokens
- Update frequency vs. computational overhead: More frequent state updates improve accuracy but increase processing time

**Failure Signatures**
- Incomplete task execution due to missing context information
- Tool selection errors from insufficient schema detail
- Response incoherence from over-aggressive context compression

**First 3 Experiments to Run**
1. Measure context growth rate across varying conversation lengths
2. Compare task completion accuracy between compressed and full context approaches
3. Evaluate tool selection accuracy with selective vs. full schema passing

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 3B-parameter small language models, limiting generalizability
- No ablation studies isolating individual component contributions
- Potential quality degradation from aggressive context compression not addressed
- Framework behavior under edge cases (tool failures, ambiguous queries) remains unexplored

## Confidence
- "More than 6-fold reduction in initial prompt size": High confidence
- "10- to 25-fold reduction in context growth rate": Medium confidence
- "Matched or exceeded baseline performance": Low confidence

## Next Checks
1. Replicate context growth measurements using alternative small language models (Llama-3-3B, Phi-3-mini) to assess architecture independence
2. Conduct ablation studies to quantify individual contributions of State-Tracker, token-efficient schemas, and selective schema passing
3. Evaluate performance degradation thresholds by systematically reducing context retention limits and measuring impact on task completion rates