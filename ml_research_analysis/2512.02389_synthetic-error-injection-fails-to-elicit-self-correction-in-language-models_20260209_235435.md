---
ver: rpa2
title: Synthetic Error Injection Fails to Elicit Self-Correction In Language Models
arxiv_id: '2512.02389'
source_url: https://arxiv.org/abs/2512.02389
tags:
- error
- errors
- synthetic
- correction
- on-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether synthetic error injection during
  supervised fine-tuning can teach language models to self-correct, inspired by robotics
  techniques. The method injects artificial errors into reasoning chains, masks them,
  and trains models to recognize and correct these mistakes.
---

# Synthetic Error Injection Fails to Elicit Self-Correction In Language Models

## Quick Facts
- arXiv ID: 2512.02389
- Source URL: https://arxiv.org/abs/2512.02389
- Authors: David X. Wu; Shreyas Kapur; Anant Sahai; Stuart Russell
- Reference count: 12
- Primary result: Synthetic error injection during SFT enables correction of artificial errors but fails to generalize to models' own naturally occurring errors due to distributional mismatch

## Executive Summary
This paper investigates whether synthetic error injection during supervised fine-tuning can teach language models to self-correct, inspired by robotics techniques. The method injects artificial errors into reasoning chains, masks them, and trains models to recognize and correct these mistakes. Experiments across multiple models (Qwen2.5, Gemma3, Llama-3.2) on multiplication and Sudoku tasks show that while models learn to correct synthetic errors effectively, they fail to generalize this ability to their own naturally occurring errors. For example, Qwen2.5 achieves 100% correction on synthetic errors but drops to 40% on its own errors for multiplication. The key finding is that distribution mismatch between synthetic and on-policy errors prevents effective self-correction, explaining why reinforcement learning remains uniquely effective for this capability.

## Method Summary
The method injects artificial errors into golden CoT traces (80% clean, 20% error-injected), formats them with recognition and correction steps, and trains models using loss masking on error steps only. Error types are hand-crafted for each task: multiplication uses carry errors and integer errors, while Sudoku uses invalid and non-naked-single moves. Models are fine-tuned for 10K steps with batch size 4, learning rate 2e-5, and evaluated on both synthetic and on-policy errors separately using greedy decoding.

## Key Results
- Qwen2.5 achieves 100% correction on synthetic errors but drops to 40% on its own errors for multiplication
- Models recognize errors more often than they correct them, often "parroting" the original mistake
- Error correction transfer fails even with 95%+ coverage of error types in synthetic distribution
- Sudoku shows ~10% EIFT gains while multiplication shows modest gains, correlating with solver-verifier gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised error injection enables high correction rates on synthetic errors but fails to generalize to on-policy errors due to distributional mismatch.
- Mechanism: The model memorizes correction patterns for synthetic errors seen during training but cannot extrapolate to context-dependent errors it naturally produces, even when synthetic coverage is high.
- Core assumption: Coverage of error types (support) is insufficient; the model requires exposure to the actual conditional distribution of errors given context.
- Evidence anchors:
  - [abstract] "distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities...even with good synthetic coverage"
  - [Section 4] Qwen2.5 shows 100%→40% correction drop from synthetic to on-policy errors on multiplication; Figure 6 shows precipitous drops across models
  - [corpus] Weak direct corpus support; related work (Kumar et al.) discusses offline-online distribution mismatch in RL settings
- Break condition: If synthetic errors perfectly matched the model's conditional error distribution, correction might generalize. The paper shows this is hard to achieve even with designed error injectors.

### Mechanism 2
- Claim: Error recognition and error correction are partially decoupled capabilities; models can recognize without successfully correcting.
- Mechanism: The model may learn a verifier circuit that flags errors but lacks a mechanism to override the original computation. When it fails to correct, it often "parrots" the original incorrect step.
- Core assumption: Error correction requires not just detection but the ability to generate alternative solutions conditioned on rejection.
- Evidence anchors:
  - [Section 1] "even when the model catches its own error, it often parrots the original mistake"
  - [Section 4, Figure 6] Recognition rates consistently exceed correction rates (e.g., Qwen2.5 Sudoku: 8% recognition but minimal correction from the hatched vs solid bars)
  - [corpus] "Decomposing LLM Self-Correction" systematically decomposes recognition vs correction as distinct capabilities
- Break condition: If the model learned to sample diverse alternative solutions after detection, correction might improve.

### Mechanism 3
- Claim: The solver-verifier gap modulates how much error injection helps; larger gaps (easier verification relative to solving) show more benefit.
- Mechanism: Tasks where verification is trivially easier than generation allow error injection to provide more useful supervision signal.
- Core assumption: Verification difficulty determines whether correction is learnable from supervised data.
- Evidence anchors:
  - [Section 5] "For any particular move, verifying is trivial by just checking the constraints, whereas solving requires searching"
  - [Section 5] Sudoku (large gap) shows ~10% EIFT gains for Qwen2.5 and Llama-3.2; multiplication (small gap) shows modest gains
  - [corpus] "The Validation Gap" paper explores LLMs computing arithmetic but failing to validate it—relevant to solver-verifier asymmetry
- Break condition: If verification is as hard as solving, error injection provides little additional signal over standard SFT.

## Foundational Learning

- **Distribution shift (covariate shift)**:
  - Why needed here: The paper's core finding is that synthetic errors occupy a different conditional distribution than on-policy errors, preventing generalization despite coverage.
  - Quick check question: If your training error distribution has 95% coverage of test errors but different relative frequencies, would you expect transfer? (Answer: No, per Figure 4b.)

- **On-policy vs. off-policy data**:
  - Why needed here: The distinction between errors generated by the model itself (on-policy) and artificially injected errors (off-policy/synthetic) is central to the failure analysis.
  - Quick check question: Why might RL succeed where synthetic SFT fails? (Answer: RL generates corrections from on-policy error distribution.)

- **Solver-verifier asymmetry**:
  - Why needed here: Explains why Sudoku shows more EIFT benefit than multiplication—verification is easier relative to solving.
  - Quick check question: For a task where verification = solving difficulty, would error injection help? (Answer: Less likely; see multiplication results.)

## Architecture Onboarding

- **Component map**:
  - Error injector (task-specific) -> FT model -> EIFT model
  - Error types (multiplication: carry_error, int_error_10/100, int_error_single_digit, int_error_two_digits; Sudoku: invalid moves, non-naked-single moves)

- **Critical path**:
  1. Define error distribution by inspecting FT model failures
  2. Inject errors into golden CoTs at random positions (1-4 errors per trace)
  3. Format: [erroneous step] → ["AH! I MADE A MISTAKE."] → [correct step]
  4. Train with loss masking on error steps
  5. Evaluate on both synthetic and on-policy errors separately

- **Design tradeoffs**:
  - Error frequency: 20% injection rate chosen; higher rates not tested
  - Error diversity: Multiple error types needed (Sudoku required both invalid and non-naked-single moves)
  - Coverage vs. distribution: High coverage (95%+) achievable but distributional alignment remains poor

- **Failure signatures**:
  - Parroting: Model outputs original incorrect step after recognition (25% of Qwen multiplication FT-error failures)
  - Recognition-correction gap: High recognition, low correction on on-policy errors
  - Temperature insensitivity: Sampling temperature does not recover correction capability

- **First 3 experiments**:
  1. Replicate coverage analysis (Figure 4): Sample 100 on-policy errors, measure synthetic injector coverage and probability mass.
  2. Ablate error distribution alignment: Manually weight synthetic errors to match observed on-policy frequencies and measure transfer.
  3. Test solver-verifier gap hypothesis: Add a task with extreme gap (e.g., formal proof verification) to see if EIFT benefit scales with gap size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the magnitude of the solver-verifier gap predict the effectiveness of error injection training?
- Basis in paper: [explicit] The authors state "It would be interesting to study more systematically the benefits of error injection as a function of the solver-verifier gap" and suggest NP-hard search problems as test cases.
- Why unresolved: The paper only tested multiplication (small gap) and simplified Sudoku (large gap), limiting systematic comparison.
- What evidence would resolve it: A controlled study across tasks with quantifiable solver-verifier gaps, measuring EIFT performance gains as a function of gap magnitude.

### Open Question 2
- Question: Do language models develop separate circuits for verification versus generation during error-correction training?
- Basis in paper: [explicit] The authors hypothesize "the model learns two circuits, one for computing the answer to an intermediate step, and the other for verifying said answer" but state "It would be interesting to confirm whether this hypothesis is accurate."
- Why unresolved: The paper observes that models recognize errors more often than they correct them (often parroting the mistake), but does not investigate internal representations.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., probing classifiers, activation patching) examining whether verification and generation are functionally separated.

### Open Question 3
- Question: Can training recipes that encourage strategy diversity improve self-correction without requiring RL?
- Basis in paper: [explicit] The authors ask whether "there are any other training recipes that naturally 'diversify' the strategies the model attempts towards solving the problem" to explain why models parrot errors even after recognition.
- Why unresolved: The paper tested only one error-injection approach; no alternative SFT methods were explored.
- What evidence would resolve it: Experiments with diversity-encouraging methods (e.g., contrastive learning, multi-solution training) showing improved on-policy error correction rates.

### Open Question 4
- Question: Can LLM-based error injectors scale to complex reasoning tasks where synthetic error distributions become intractable?
- Basis in paper: [inferred] The authors note "it is possible that in more complicated reasoning scenarios, LLM based error injectors become the only scalable way to model errors" but found early experiments unpromising due to implausible errors.
- Why unresolved: The paper's tasks (multiplication, 4×4 Sudoku) have tractable error taxonomies; complex reasoning may require learned error models.
- What evidence would resolve it: Successful EIFT training on complex tasks (e.g., MATH, programming) using LLM-generated error distributions matching on-policy failure modes.

## Limitations
- Limited to two relatively simple tasks (4-digit multiplication and constrained 4×4 Sudoku), raising questions about generalizability to complex reasoning
- Does not explore whether learned error generators might outperform hand-crafted error injectors
- Temperature insensitivity finding suggests fundamental limitations but lacks exploration of alternative decoding strategies
- Solver-verifier gap hypothesis is correlational rather than causally validated

## Confidence

**High Confidence**: The core empirical finding that synthetic error injection fails to generalize to on-policy errors is well-supported by controlled experiments across multiple models and tasks. The distributional mismatch explanation is mechanistically plausible given the strong empirical results.

**Medium Confidence**: The solver-verifier gap hypothesis is supported by the Sudoku vs multiplication comparison, but the evidence is correlational rather than causal. The claim that this explains varying EIFT benefits across tasks is reasonable but not rigorously proven.

**Low Confidence**: The temperature insensitivity finding and its implications for sampling-based error-correction approaches require more investigation. The paper presents this as a robust finding but doesn't explore alternative explanations or test it across different model families.

## Next Checks

1. **Distributional Alignment Experiment**: Generate synthetic errors to exactly match the empirical distribution of on-policy errors (not just error types) and measure whether correction transfer improves. This would test whether the failure is due to coverage vs. distributional shape.

2. **Cross-Task Generalization**: Apply the same methodology to more complex reasoning tasks (e.g., mathematical proof steps, code debugging) to determine if the synthetic-to-real transfer failure is task-specific or a general phenomenon.

3. **Error Injection Strategy Ablation**: Compare hand-crafted error injectors against learned error generators trained to mimic the model's own error distribution, measuring whether learned error patterns improve correction transfer.