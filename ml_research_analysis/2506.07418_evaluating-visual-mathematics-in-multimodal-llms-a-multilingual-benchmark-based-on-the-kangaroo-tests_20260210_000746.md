---
ver: rpa2
title: 'Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark
  Based on the Kangaroo Tests'
arxiv_id: '2506.07418'
source_url: https://arxiv.org/abs/2506.07418
tags:
- visual
- reasoning
- problems
- mathematical
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multimodal large language models (MLLMs) on
  visually presented mathematics problems from the Kangaroo competition across four
  languages. A benchmark dataset was created with image-based and text-only questions
  spanning geometry, visual algebra, logic, patterns, and combinatorics.
---

# Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests

## Quick Facts
- **arXiv ID:** 2506.07418
- **Source URL:** https://arxiv.org/abs/2506.07418
- **Reference count:** 40
- **Primary result:** No single model excelled across all visual math topics; Gemini 2.0 Flash achieved highest image-based accuracy at 45.4%.

## Executive Summary
This study evaluates multimodal large language models on visually presented mathematics problems from the Kangaroo competition across four languages. A benchmark dataset was created with image-based and text-only questions spanning geometry, visual algebra, logic, patterns, and combinatorics. Eight models were tested, including GPT-4o, Gemini 2.0 Flash, Qwen-VL, and Llama variants. Results show overall moderate precision, with no single model excelling across all topics. Models performed better on text-only questions than image-based ones, indicating underutilization of visual information. Gemini 2.0 Flash achieved the highest accuracy on image-based tasks (45.4%), followed by Qwen-VL 2.5 72B (43.5%) and GPT-4o (40.2%). None approached human-level performance, especially on complex visual reasoning tasks. Models with more parameters generally performed better, but even the best struggled with advanced geometry and combinatorial reasoning.

## Method Summary
The study evaluated MLLMs on multilingual Kangaroo Mathematics Competition problems using a dataset spanning 2014-2024 across English, French, Spanish, and Catalan. Questions were categorized into six topics and presented with structured prompts requesting reasoning before answers. Models accessed via APIs (for larger models) or local inference (for smaller ones) were evaluated on both image-based and text-only versions of problems. Performance metrics included accuracy disaggregated by language, difficulty level, and mathematical topic.

## Key Results
- Models showed minimal performance improvement when visual information was provided, indicating underutilization of diagrammatic data
- Gemini 2.0 Flash achieved highest image-based accuracy (45.4%), while smaller models like Llama 3.2 11B showed negligible difference between image (15.0%) and non-image (16.5%) tasks
- Models frequently returned "No answer" responses or random guesses when visual reasoning conflicted with answer options, particularly in smaller architectures

## Why This Works (Mechanism)

### Mechanism 1: Visual Ignorance Through Cross-Modal Attention Failure
MLLMs frequently fail to ground visual features in logical reasoning, leading to "visual ignorance" where performance gains from images are marginal compared to text-only inputs. The cross-modal attention layers prioritize textual tokens over visual geometric features. When processing diagrams, models may extract low-level features but fail to map these to semantic mathematical constraints, effectively treating the image as noise while relying on linguistic priors to guess the answer.

### Mechanism 2: Structured Reasoning in Large Models vs. Random Heuristics in Small Models
Structured reasoning capabilities emerge primarily in large-scale proprietary models, while smaller open-source models default to random heuristics when visual reasoning fails. Models with larger parameter counts and specific "reasoning" fine-tuning maintain coherent chain-of-thought trajectories. In contrast, smaller models lack the capacity to resolve conflicts between visual perception and candidate answers, leading to "No answer" responses or random selection from multiple-choice options.

### Mechanism 3: Asymmetric Multilingual Performance Due to Training Data Density
Multilingual capability in mathematical reasoning is asymmetric, driven by training data density rather than pure logical inference. Mathematical reasoning relies on semantic understanding of the problem statement. If the pre-training corpus contains significantly more data in one language (Spanish) than a related low-resource language (Catalan), the model's ability to parse the nuance of the question degrades before the visual reasoning step even begins.

## Foundational Learning

**Concept:** Visual Grounding vs. OCR
- **Why needed here:** The paper distinguishes between models that can read text in images (OCR) and those that can reason about geometric shapes (Grounding). A new engineer must understand that high OCR capability does not imply high spatial reasoning capability.
- **Quick check question:** Can the model identify the text "Area = ?" in a diagram versus actually calculating the area based on the visual dimensions provided?

**Concept:** Cross-Modal Attention
- **Why needed here:** To understand why models "underutilize" visual information. The engineer needs to grasp that the model must map pixels to tokens that hold semantic weight in the transformer's reasoning process.
- **Quick check question:** If you mask the text in a geometry problem, does the model still attend to the geometric relationships in the image?

**Concept:** Recitation vs. Reasoning
- **Why needed here:** The paper highlights the use of "unseen" competition problems to prevent models from simply retrieving memorized solutions.
- **Quick check question:** Does the model solve a novel variation of a standard geometry problem (reasoning), or does it only solve problems identical to its training set (recitation)?

## Architecture Onboarding

**Component map:** Input (Multilingual Text Prompt + Image) -> Vision Encoder (Extracts visual features) -> Projector/Adapter (Maps visual features to LLM embedding space) -> LLM Backbone (Generates chain-of-thought reasoning and final answer) -> Evaluation (Comparison against Ground Truth)

**Critical path:** The integrity of the Projector/Adapter is the likely bottleneck. If geometric relationships (extracted by the Vision Encoder) are "flattened" or poorly aligned during projection, the LLM Backbone receives degenerate signals, forcing it to rely on text-based guessing.

**Design tradeoffs:**
- Proprietary (GPT-4o/Gemini): Higher reasoning coherence and multilingual support; closed "black box" architecture; expensive API costs
- Open-Source (Llama/Qwen): Customizable; lower inference cost; but struggles with "No answer" calibration and requires significant VRAM for larger variants (72B)

**Failure signatures:**
- Visual Ignorance: Model outputs a solution that contradicts the visible diagram
- Hallucinated Constraints: Inventing numbers or relationships not present in the text or image to force a solution
- Low-Resource Language Drift: Misinterpreting the question text in Catalan/French, leading to a correct visual analysis but an incorrect final answer due to misunderstanding the prompt

**First 3 experiments:**
1. **Ablation Study (Text vs. Image):** Run the benchmark using text-only versions of the geometry problems vs. image-only, quantifying the "visual information gap" for specific models like Llama 3.2 vs. Gemini
2. **Language Perturbation:** Translate a subset of Spanish problems to Catalan manually and compare performance, isolating whether the error is translation-induced or reasoning-induced
3. **Reasoning Trace Audit:** Execute the prompt with "step-by-step reasoning" required, manually inspecting where logic diverges from visual evidence

## Open Questions the Paper Calls Out

**Open Question 1:** What specific architectural or training modifications are required to improve MLLMs' interpretation of three-dimensional figures and overlapping spatial elements? The paper notes models consistently misunderstand stacking or overlapping elements, perceiving them as existing in a "single two-dimensional plane."

**Open Question 2:** Why do state-of-the-art models fail to leverage visual information effectively, showing minimal performance difference between text-only and image-based problems? The abstract and Section 5 highlight that performance gains are limited for text-only questions, indicating an "underutilization of diagrammatic information."

**Open Question 3:** Can diverse model architectures be successfully ensembled to overcome individual reasoning failures, given the current high correlation in errors? Section 4 concludes that co-occurrence patterns imply a "limitation in achieving performance improvement through model ensembles" due to correlated failures.

## Limitations
- Exact prompt templates and sampling hyperparameters (temperature, max tokens) for each proprietary model were not specified
- Performance comparisons may be influenced by API-specific optimizations not documented in the study
- Dataset composition, while spanning multiple years and languages, may not fully represent the difficulty distribution of actual Kangaroo competitions
- Analysis focuses primarily on accuracy metrics without deeper error categorization (visual misperception vs. linguistic misunderstanding vs. logical failure)

## Confidence

**High Confidence:** The core finding that multimodal models struggle with visual mathematics, particularly in geometry and combinatorics, is well-supported by consistent patterns across multiple models and languages.

**Medium Confidence:** Claims about specific models' reasoning capabilities (e.g., Gemini 2.0 Flash's structured reasoning vs. Llama's random guessing) are supported but could be influenced by unstated implementation details like prompt engineering or sampling parameters.

**Low Confidence:** The attribution of performance differences between languages (Spanish vs. Catalan) to training data density is speculative without direct analysis of model attention or error patterns in the linguistic vs. visual components.

## Next Checks

1. **Ablation Study Replication:** Create text-only versions of geometry problems and compare performance to image-based versions for each model, measuring the "visual information gap" to verify the visual ignorance hypothesis.

2. **Language-Pair Analysis:** Manually translate a subset of Spanish problems to Catalan and back-translate to Spanish, then compare model performance to isolate whether errors stem from linguistic understanding or reasoning capability.

3. **Reasoning Trace Classification:** For each incorrect answer, classify whether the error originated from visual misperception, linguistic misunderstanding, or logical failure by examining the model's step-by-step reasoning output.