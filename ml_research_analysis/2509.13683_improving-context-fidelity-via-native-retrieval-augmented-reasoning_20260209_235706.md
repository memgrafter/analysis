---
ver: rpa2
title: Improving Context Fidelity via Native Retrieval-Augmented Reasoning
arxiv_id: '2509.13683'
source_url: https://arxiv.org/abs/2509.13683
tags:
- reasoning
- retrieval
- context
- arxiv
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses context fidelity issues in large language
  models by proposing CARE, a native retrieval-augmented reasoning framework. CARE
  teaches models to dynamically identify and integrate relevant evidence from input
  context within their reasoning process using in-context retrieval capabilities,
  rather than relying on external retrieval modules.
---

# Improving Context Fidelity via Native Retrieval-Augmented Reasoning

## Quick Facts
- **arXiv ID:** 2509.13683
- **Source URL:** https://arxiv.org/abs/2509.13683
- **Reference count:** 30
- **Primary result:** CARE framework improves context fidelity by up to 15.29% average F1 over vanilla models through native in-context retrieval

## Executive Summary
This paper introduces CARE (Context-Aware Retrieval-Augmented Reasoning), a framework that teaches large language models to dynamically retrieve and integrate relevant evidence from input context during reasoning, eliminating the need for external retrieval modules. The approach combines supervised fine-tuning with evidence-annotated data and reinforcement learning with curriculum-based training. Experiments demonstrate CARE consistently outperforms baseline models on real-world and counterfactual QA benchmarks, achieving significant improvements in context fidelity by grounding reasoning chains in contextual evidence while requiring limited labeled data.

## Method Summary
CARE employs a two-phase training approach: first, supervised fine-tuning on evidence-annotated data where models learn to identify and integrate relevant context spans using `<retrieval>` tags within Chain-of-Thought reasoning; second, reinforcement learning with curriculum-based training that progressively increases context complexity from short (DROP) to long (MS MARCO) contexts. The framework uses GRPO optimization with three reward components (accuracy, format, retrieval) to encourage factual consistency while maintaining proper reasoning structure. Training utilizes LoRA adapters on LLaMA/Qwen models with curriculum sampling to balance performance across diverse QA types without requiring additional labeled data.

## Key Results
- CARE achieves up to 15.29% average F1 improvement over vanilla models on real-world and counterfactual QA benchmarks
- The framework demonstrates consistent performance gains across multiple model sizes (7B and 14B parameters)
- Curriculum learning provides better balance across diverse QA types without requiring additional labeled data
- CARE effectively grounds reasoning in contextual evidence while maintaining proper reasoning structure

## Why This Works (Mechanism)

### Mechanism 1: Explicit Evidence Grounding in Reasoning Chains
Integrating retrieved context spans directly into Chain-of-Thought (CoT) reduces hallucinations by forcing attention to specific input tokens before generating conclusions. The model encloses critical context snippets within `<retrieval>` tags during thinking, shifting attention to verify reasoning steps against input context rather than relying solely on parametric memory.

### Mechanism 2: Reinforcement of Context Fidelity via Retrieval Rewards
Reinforcement learning with retrieval rewards creates feedback loops that incentivize factual consistency over plausible hallucinations. The reward verifies if text inside `<retrieval>` tags exists in source context, encouraging models to prioritize verifiable reasoning over invented content.

### Mechanism 3: Curriculum Generalization from Labeled to Unlabeled Data
Curriculum learning enables transfer of evidence-retrieval skills from resource-rich (labeled) domains to complex, resource-scarce domains. Training transitions from short-context to long-context scenarios, stabilizing RL optimization and allowing refinement of self-retrieval policy on harder examples using only QA pairs.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed:** Required for implementing the RL phase with group-based advantage calculation
  - **Quick check:** Can you explain how GRPO calculates the advantage function $\hat{A}_{i,t}$ differently than standard PPO?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed:** CARE modifies standard CoT by injecting evidence tags
  - **Quick check:** How does the placement of the `<retrieval>` tag within the CoT stream differ from standard RAG?

- **Concept: Native vs. External Retrieval**
  - **Why needed:** Core distinction between model attending to input context vs. calling external database
  - **Quick check:** Does CARE require a vector database or an embedding model during inference?

## Architecture Onboarding

- **Component map:** Teacher models ($M_R$, $M_I$) generate SFT data → Student Model ($\pi_\theta$) with LoRA adapters → GRPO trainer with 3 rewards → Curriculum Manager switches between $D_{easy}$ and $D_{hard}$

- **Critical path:** SFT Data Generation is most fragile step, relying on strong "Fact Injection Model" ($M_I$) to correctly weave golden evidence into reasoning chains

- **Design tradeoffs:** Trades inference latency (generating long CoT + retrieval tags) for accuracy and context fidelity; trades initial labeling cost for generalization via RL on unlabeled data

- **Failure signatures:** Empty Retrieval (never uses `<retrieval>` tags), Context Hallucination (retrieved text doesn't exist in context), Format Collapse (ignores `<think>` structure)

- **First 3 experiments:**
  1. SFT Validation: Run inference on SFT model before RL, check if it produces `<retrieval>` format correctly
  2. Reward Ablation: Train 3 RL variants (Accuracy only, Accuracy+Format, Full CARE rewards), compare "Retrieval Recall" metric
  3. Context Length Stress Test: Evaluate "No Cur." vs. "CARE" on $D_{hard}$ dataset to verify curriculum prevented forgetting on short contexts

## Open Questions the Paper Calls Out

- Can CARE's native retrieval mechanism be effectively combined with external retrieval systems to handle scenarios requiring information beyond the provided context?
- How well does CARE generalize to abstract reasoning, numerical computation, creative generation, and domain-specific tasks beyond multi-hop QA?
- What mechanisms could address hallucinations when input context contains ambiguous or contradictory information?
- How sensitive is CARE's performance to the choice and ordering of datasets in the curriculum learning strategy?

## Limitations

- Reliance on labeled evidence spans for SFT data generation presents scalability constraints for domains without structured annotations
- Retrieval reward assumes exact string matching, which may not capture semantic relevance
- Evaluation focuses primarily on QA benchmarks, leaving open questions about effectiveness on other reasoning-intensive tasks
- Does not completely eliminate hallucinations when input contains ambiguous or contradictory information

## Confidence

- **High Confidence:** SFT phase effectively establishes evidence-retrieval patterns in reasoning chains
- **Medium Confidence:** RL phase with retrieval rewards generalizes patterns to unlabeled data and improves context fidelity
- **Low Confidence:** Curriculum learning strategy significantly outperforms random sampling across all difficulty levels

## Next Checks

1. **Reward Component Ablation Study:** Train CARE variants with individual reward components disabled to quantify each component's contribution to context fidelity improvements

2. **Semantic Retrieval Validation:** Implement semantic matching check alongside exact string matching in retrieval reward to verify semantic relevance of retrieved spans

3. **Cross-Domain Generalization Test:** Evaluate CARE on non-QA reasoning tasks (mathematical problem-solving or code generation) to assess framework's applicability beyond QA domain