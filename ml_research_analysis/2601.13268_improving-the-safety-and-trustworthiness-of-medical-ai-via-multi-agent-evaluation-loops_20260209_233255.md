---
ver: rpa2
title: Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation
  Loops
arxiv_id: '2601.13268'
source_url: https://arxiv.org/abs/2601.13268
tags:
- safety
- medical
- risk
- language
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-agent evaluation loop for improving
  the safety and ethical compliance of medical LLMs through iterative refinement rather
  than retraining. Two generative models, DeepSeek R1 and Med-PaLM, are paired with
  two evaluator agents that assess responses against the AMA Principles of Medical
  Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol.
---

# Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops

## Quick Facts
- arXiv ID: 2601.13268
- Source URL: https://arxiv.org/abs/2601.13268
- Authors: Zainab Ghafoor; Md Shafiqul Islam; Koushik Howlader; Md Rasel Khondokar; Tanusree Bhattacharjee; Sayantan Chakraborty; Adrito Roy; Ushashi Bhattacharjee; Tirtho Roy
- Reference count: 33
- One-line primary result: Iterative multi-agent refinement achieved 89% reduction in ethical violations and 92% risk downgrade rate without retraining

## Executive Summary
This study introduces a multi-agent evaluation loop for improving the safety and ethical compliance of medical LLMs through iterative refinement rather than retraining. Two generative models, DeepSeek R1 and Med-PaLM, are paired with two evaluator agents that assess responses against the AMA Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. The framework processes 900 adversarial medical queries, using structured feedback to guide multiple rounds of response revision until both ethical and safety thresholds are met. Results show DeepSeek R1 achieves faster convergence (mean 2.34 vs 2.67 iterations) and both models reach an 89% reduction in ethical violations and a 92% risk downgrade rate. This demonstrates that inference-time safety refinement is scalable, regulator-aligned, and cost-efficient for governing medical AI deployment.

## Method Summary
The framework implements an iterative multi-agent loop where two generators (DeepSeek R1 and Med-PaLM) produce responses to adversarial medical queries that are evaluated in parallel by two specialized agents: LLaMA 3.1 for AMA ethics compliance and Phi-4 for SRA-5 safety risk assessment. Responses must meet thresholds (AMA ≤ 2, SRA ≤ 2) to exit the loop, otherwise structured feedback guides revision. The system processes 900 queries across 9 ethical domains, with up to 5 refinement iterations per query, achieving safety improvements through feedback-guided rewriting rather than model retraining.

## Key Results
- Achieved 89% reduction in ethical violations across both model pairs
- Demonstrated 92% risk downgrade rate from initial to final responses
- DeepSeek R1 converged faster (mean 2.34 iterations) than Med-PaLM (2.67 iterations)
- Maintained high safety standards with mean latency of 8.5 seconds per converged query

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured feedback from evaluator agents can guide generators to revise unsafe outputs without retraining
- Mechanism: Generator produces initial response → Two evaluators assess in parallel → Consensus merged → Revision plan generated → Generator rewrites → Loop until both thresholds met or iteration cap reached
- Core assumption: The generator can interpret and follow explicit revision instructions to remove unsafe content while preserving helpfulness
- Evidence anchors:
  - [abstract] "The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate"
  - [section III.C.2] "We merge these outputs into a consensus record... From this consensus record, we create a short revision plan"
  - [corpus] "Balancing Safety and Helpfulness" paper similarly achieves improvement through iterative preference alignment
- Break condition: When prompts strongly invite diagnostic/treatment claims that cannot be safely rewritten without becoming overly vague or refusing

### Mechanism 2
- Claim: Parallel evaluation against both ethical principles and graded safety risk provides more comprehensive coverage than single-metric approaches
- Mechanism: LLaMA 3.1 scores against 9 AMA principles (0-9 scale), Phi-4 assigns SRA-5 level (1-5 scale); response must pass both thresholds (AMA ≤ 2, SRA ≤ 2) to exit loop
- Core assumption: Ethical violations and safety risks are partially independent dimensions requiring separate specialized evaluation
- Evidence anchors:
  - [abstract] "assess responses using the AMA Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol"
  - [section III.B.2] "A response is considered deployable only if its SRA level is at most 2... must achieve an AMA score of at most 2"
  - [corpus] Medical Malice paper notes generic harm definitions "fail to capture context-dependent violations"
- Break condition: When ethical honesty (e.g., disclosing treatment risks) increases apparent risk level, creating threshold conflict

### Mechanism 3
- Claim: Reasoning-optimized models converge faster under iterative safety feedback than domain-trained models, but domain training provides category-specific advantages
- Mechanism: DeepSeek R1 responds more consistently to explicit feedback and restructures readily; Med-PaLM starts from medically grounded responses but needs extra rounds to remove borderline diagnostic suggestions
- Core assumption: Assumption: Training paradigm (reasoning optimization vs. domain-specific) affects how models process revision instructions
- Evidence anchors:
  - [abstract] "DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations)"
  - [section V.C] "One plausible explanation is that it responds more consistently to explicit feedback"
  - [corpus] Limited direct evidence on reasoning vs. domain model convergence patterns in related literature
- Break condition: Not directly proven—could fail with other model pairs or different evaluation criteria

## Foundational Learning

- **Concept: Multi-Agent Critique-Revise Pattern**
  - Why needed here: The entire framework depends on understanding how separate agent roles (generator, ethics evaluator, safety evaluator) interact through structured feedback loops
  - Quick check question: Why might separating evaluation from generation improve safety compared to a single model self-correcting?

- **Concept: Graded Risk Assessment (ISO 14971-style)**
  - Why needed here: SRA-5 uses a 5-level risk scale rather than binary safe/unsafe labels; understanding this gradation is essential for setting appropriate stopping thresholds
  - Quick check question: What distinguishes a Level 3 (Moderate, requires clinician consultation) from a Level 4 (High, diagnostic/treatment suggestions that could cause harm)?

- **Concept: Operationalizing Abstract Ethics Principles**
  - Why needed here: The framework converts 9 AMA principles into scorable violations; understanding this translation from abstract norms to measurable outputs is critical for evaluator design
  - Quick check question: How would you convert "patient confidentiality" into a specific evaluation checklist item?

## Architecture Onboarding

- **Component map:**
  Query → Generator (DeepSeek R1 / Med-PaLM) → Initial Response → Evaluator 1 (LLaMA 3.1: AMA ethics) → Evaluator 2 (Phi-4: SRA-5 risk) → Consensus Merger → Threshold Check → (Pass → Output) or (Fail → Feedback/Revision Plan → Generator Refine → Loop max 5)

- **Critical path:** Query → Generator → Parallel evaluation → Consensus → Threshold check → (fail → feedback → revise → loop) or (pass → output)

- **Design tradeoffs:**
  - Latency vs. Safety: Mean 8.5s per converged query; more iterations improve safety but increase latency
  - Threshold strictness: AMA ≤ 2 and SRA ≤ 2 determine deployment readiness; relaxing increases pass rate but reduces safety margin
  - Model pairing: Reasoning model (faster convergence, ~0.33 fewer iterations) vs. domain model (better privacy handling)

- **Failure signatures:**
  - Non-convergence (5.8-8.2%): Often diagnostic/treatment-inviting prompts that cannot be safely rewritten
  - Patient Welfare (Principle VIII) and Legal Responsibility (Principle III) categories require most iterations (3.2-3.5 mean)
  - Emergency prompts start highest (4.8-5.1 violations) but show largest absolute improvement

- **First 3 experiments:**
  1. Ablation study: Run with single evaluator (ethics-only, then safety-only) to quantify contribution of dual-domain evaluation
  2. Threshold sensitivity: Systematically vary AMA (0-4) and SRA (1-3) thresholds to map convergence rate vs. safety strictness tradeoffs
  3. Evaluator swap: Replace LLaMA 3.1/Phi-4 with alternative evaluator models to assess robustness of consensus mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do the safety judgments of the LLM-based evaluators (LLaMA 3.1 and Phi-4) align with human clinician assessments of ethical violations and risk?
- Basis in paper: [explicit] The authors state in the Limitations section that "Validation against clinician annotations is necessary to understand where the evaluators are reliable and where they may miss subtle harms."
- Why unresolved: The study relies on LLM agents as the ground truth for ethical and safety scoring, but model-based evaluations may fail to detect nuances that a medical professional would catch.
- What evidence would resolve it: A comparative study measuring inter-rater reliability (e.g., Cohen's Kappa) between the AI evaluators and a panel of human clinicians on the same set of refined responses.

### Open Question 2
- Question: How does the multi-agent refinement loop perform in multi-turn dialogue settings where risk accumulates or shifts based on conversational context?
- Basis in paper: [explicit] The paper notes that "we evaluate single turn prompts, while many real deployments involve multi turn conversations where earlier context can change risk."
- Why unresolved: The current methodology isolates 900 individual prompts, failing to assess if the system can maintain safety and context over time without excessive latency or drift.
- What evidence would resolve it: Extending the evaluation to longitudinal, multi-turn medical consultation datasets to measure safety retention and context management across session turns.

### Open Question 3
- Question: How robust is the evaluation framework against targeted prompt injection attacks when the LLM is integrated with external clinical tools?
- Basis in paper: [explicit] The authors admit they "do not fully characterize adversarial robustness against targeted prompt injection and tool use scenarios," marking it as an area for future work.
- Why unresolved: While the loop handles adversarial queries, it has not been tested against malicious inputs designed to manipulate the model's logic or tool usage in a clinical workflow.
- What evidence would resolve it: Red-teaming exercises specifically targeting tool-integrated versions of the framework to measure the success rate of prompt injection attacks bypassing the safety thresholds.

## Limitations
- Evaluator consistency and feedback quality represent significant vulnerabilities due to underspecified prompt templates and consensus mechanisms
- Convergence patterns may be specific to the DeepSeek R1 and Med-PaLM model pairs, limiting generalizability
- The framework hasn't been validated against human clinician assessments or tested in multi-turn conversational settings

## Confidence
- **High Confidence:** The 89% reduction in ethical violations and 92% risk downgrade rate are directly reported from the 900-query experiment with clear methodology
- **Medium Confidence:** The faster convergence of DeepSeek R1 (2.34 vs 2.67 iterations) is well-documented, though the underlying mechanism (reasoning optimization vs domain training effects) remains partially speculative
- **Low Confidence:** The generalizability of these results to other medical domains, model pairs, or evaluation criteria beyond the specific AMA and SRA-5 framework

## Next Checks
1. Implement an ablation study comparing single-evaluator (ethics-only vs safety-only) performance to quantify the contribution of dual-domain evaluation to the observed 89% improvement
2. Conduct threshold sensitivity analysis by systematically varying AMA and SRA thresholds to map the safety-latency tradeoff space and identify optimal stopping criteria
3. Perform evaluator robustness testing by replacing LLaMA 3.1 and Phi-4 with alternative models to assess whether the consensus mechanism and convergence patterns hold across different evaluator architectures