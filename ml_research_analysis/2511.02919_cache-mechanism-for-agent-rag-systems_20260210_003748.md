---
ver: rpa2
title: Cache Mechanism for Agent RAG Systems
arxiv_id: '2511.02919'
source_url: https://arxiv.org/abs/2511.02919
tags:
- cache
- retrieval
- arxiv
- agent
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ARC, an annotation-free caching framework\
  \ for Retrieval-Augmented Generation (RAG) systems used by LLM agents. ARC dynamically\
  \ maintains small, high-value corpora by leveraging two key innovations: (1) a distance\u2013\
  rank frequency score that weights passages based on both their retrieval rank and\
  \ semantic similarity to queries, and (2) a hubness score that identifies semantically\
  \ central passages using the intrinsic geometry of the embedding space."
---

# Cache Mechanism for Agent RAG Systems

## Quick Facts
- **arXiv ID:** 2511.02919
- **Source URL:** https://arxiv.org/abs/2511.02919
- **Reference count:** 11
- **Primary result:** Reduces storage to 0.015% of original corpus while maintaining 79.8% has-answer rate and 80% latency reduction

## Executive Summary
This paper introduces ARC, an annotation-free caching framework for Retrieval-Augmented Generation (RAG) systems used by LLM agents. ARC dynamically maintains small, high-value corpora by leveraging two key innovations: (1) a distance–rank frequency score that weights passages based on both their retrieval rank and semantic similarity to queries, and (2) a hubness score that identifies semantically central passages using the intrinsic geometry of the embedding space. Evaluated on three retrieval datasets with over 6.4 million documents, ARC demonstrates significant improvements in storage efficiency and retrieval latency while maintaining high answer coverage.

## Method Summary
ARC operates by first scoring passages using a combined distance–rank frequency metric that captures both semantic similarity and retrieval prominence. Passages are then evaluated using a hubness score that identifies those occupying central positions in the embedding space based on their k-nearest neighbors. The system dynamically maintains a cache of high-value passages, automatically evicting low-scoring items as new queries arrive. This approach requires no manual annotation and adapts to query patterns over time, making it suitable for resource-constrained agentic RAG deployments.

## Key Results
- Reduces storage requirements to 0.015% of the original corpus
- Achieves 79.8% has-answer rate on benchmark queries
- Reduces average retrieval latency by 80% compared to baselines

## Why This Works (Mechanism)
ARC works by combining two complementary scoring mechanisms that capture different aspects of passage utility. The distance–rank frequency score ensures that passages retrieved early (high rank) and closely matching query semantics (low distance) are prioritized, while the hubness score identifies passages that serve as semantic bridges connecting many other documents. This dual approach ensures the cache contains both directly relevant passages and semantically central ones that can support diverse future queries.

## Foundational Learning
- **Hubness in high-dimensional spaces:** Occurs when certain points appear as neighbors to many others, creating a skewed distribution of nearest-neighbor relationships. Why needed: Essential for identifying semantically central passages that serve multiple queries.
- **Distance–rank frequency scoring:** Combines semantic similarity with retrieval prominence to weight passage importance. Why needed: Ensures both relevance and retrieval efficiency are captured in caching decisions.
- **Dynamic cache maintenance:** Continuous evaluation and eviction of passages based on changing query patterns. Why needed: Adapts to evolving information needs without manual intervention.

## Architecture Onboarding
**Component Map:** Query Embeddings -> Distance-Rank Frequency Calculator -> Hubness Score Calculator -> Cache Manager -> Stored Passages
**Critical Path:** Query embedding → passage scoring → cache update → retrieval from cache
**Design Tradeoffs:** Balance between cache size (storage cost) and coverage (answer rate); hubness computation adds overhead but improves cache quality
**Failure Signatures:** Cache misses due to poor hubness estimation; over-aggressive eviction removing still-relevant passages; hubness scores failing to capture semantic centrality in specialized domains
**3 First Experiments:** 1) Benchmark hubness score computation time on sample datasets; 2) Compare distance–rank frequency performance against rank-only baselines; 3) Test cache hit rates with varying cache sizes on a subset of queries

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend on specific query sets that may not reflect broader agentic RAG diversity
- No reported variance across runs or ablation studies on component importance
- 0.015% storage reduction figure may not generalize to domains with different embedding characteristics

## Confidence
- **Storage reduction (0.015%):** High - straightforward computational measurement
- **Latency reduction (80%):** High - direct measurement of retrieval time
- **Has-answer rate (79.8%):** Medium - depends on query distribution and benchmark selection

## Next Checks
1. Conduct ablation studies comparing performance with only distance–rank frequency scoring versus only hubness scoring to isolate each component's contribution.

2. Test the framework on retrieval datasets from different domains (e.g., biomedical, legal) to assess generalizability of the 0.015% storage reduction and performance gains.

3. Measure end-to-end system performance (including LLM inference) to verify that the 80% latency reduction in retrieval translates to meaningful improvements in complete agentic workflows.