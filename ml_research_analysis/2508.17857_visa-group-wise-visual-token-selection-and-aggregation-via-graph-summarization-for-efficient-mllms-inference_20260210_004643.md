---
ver: rpa2
title: 'VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization
  for Efficient MLLMs Inference'
arxiv_id: '2508.17857'
source_url: https://arxiv.org/abs/2508.17857
tags:
- visual
- tokens
- token
- information
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of inference in multimodal
  large language models (MLLMs) caused by excessive visual tokens. The authors propose
  a novel method called group-wise Visual Token Selection and Aggregation (VISA) that
  combines graph-based visual token aggregation (VTA) with a group-wise token selection
  strategy (GTS).
---

# VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference

## Quick Facts
- arXiv ID: 2508.17857
- Source URL: https://arxiv.org/abs/2508.17857
- Reference count: 40
- Primary result: 2.08× speedup on LLaVA-1.5-13B while maintaining 98.14% of original performance

## Executive Summary
This paper addresses the inefficiency of inference in multimodal large language models (MLLMs) caused by excessive visual tokens. The authors propose VISA (Visual Token Selection and Aggregation), a novel method that combines graph-based visual token aggregation with group-wise token selection strategy. By treating visual tokens as nodes in a semantic similarity-based graph and progressively selecting which tokens to keep versus remove, VISA achieves significant speed improvements while maintaining high performance across multiple MLLM architectures and benchmarks.

## Method Summary
VISA introduces a two-pronged approach to visual token reduction. First, it constructs a graph where each visual token is a node connected based on semantic similarity, then aggregates information from less important tokens into important ones using graph summarization techniques. Second, it implements a group-wise token selection strategy that progressively divides visual tokens into kept and removed sets, guided by text tokens from the final layers of each group. This dual approach allows for efficient inference while preserving the most semantically relevant visual information for the MLLM to process.

## Key Results
- Achieves 2.08× speedup on LLaVA-1.5-13B while maintaining 98.14% of original model performance
- Consistently outperforms previous methods across LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA architectures
- Demonstrates effectiveness across various benchmarks without requiring architectural modifications to base models

## Why This Works (Mechanism)
VISA works by leveraging both graph-based semantic relationships between visual tokens and progressive selection guided by textual context. The graph summarization approach allows the model to identify and preserve the most informative visual tokens while aggregating redundant or less important information into them. The group-wise selection strategy ensures that token reduction happens progressively and contextually, with the textual tokens from final layers providing guidance on which visual information is most relevant for the current task. This dual mechanism allows for substantial computational savings without sacrificing the semantic richness needed for accurate multimodal reasoning.

## Foundational Learning

**Graph-based token aggregation**: Why needed - To identify semantic relationships between visual tokens and enable information consolidation. Quick check - Visualize token similarity matrices and resulting graph structures for sample images.

**Progressive token selection**: Why needed - To ensure token reduction happens contextually rather than uniformly across all tokens. Quick check - Compare token retention patterns across different image types and tasks.

**Multimodal attention mechanisms**: Why needed - To understand how visual and textual tokens interact in the MLLM architecture. Quick check - Analyze attention weights before and after VISA application to verify preserved relationships.

**Semantic similarity metrics**: Why needed - To construct meaningful graphs that capture true visual token relationships. Quick check - Test different similarity metrics and their impact on downstream performance.

## Architecture Onboarding

**Component map**: Input images → Visual feature extraction → Visual token generation → Graph construction (semantic similarity) → Graph summarization (VTA) → Group-wise token selection (GTS) → Selected visual tokens + text tokens → MLLM processing

**Critical path**: The most critical components are the graph construction and summarization steps, as these determine which visual information is preserved versus discarded. Errors here propagate through the entire pipeline and directly impact downstream performance.

**Design tradeoffs**: The method trades potential loss of fine-grained visual details for computational efficiency. The graph-based approach may miss subtle visual relationships that aren't captured by the chosen similarity metric, while the group-wise selection might prematurely discard tokens that become relevant in later processing stages.

**Failure signatures**: Performance degradation typically manifests as reduced accuracy on tasks requiring fine-grained visual discrimination, increased hallucination rates when critical visual details are lost, and potential bias toward certain types of visual information that the graph construction captures more effectively.

**First experiments**:
1. Baseline comparison: Run VISA on a standard image classification task with LLaVA-1.5 and measure speed-up vs accuracy trade-off
2. Graph analysis: Visualize the constructed graphs for different image types to understand what visual relationships are being captured
3. Ablation study: Test VTA and GTS components separately to quantify their individual contributions to overall performance

## Open Questions the Paper Calls Out
None

## Limitations
- The graph construction approach may not generalize well to specialized visual domains like medical imaging or satellite imagery
- The progressive token selection guided by text tokens might introduce bias toward certain types of visual information
- The method lacks detailed analysis of what specific visual information is preserved versus lost during aggregation

## Confidence

**Speed-up metrics and benchmark performance**: High confidence - Strong empirical results across multiple architectures and benchmarks
**Generalizability to other MLLM architectures**: Medium confidence - Effective on tested models but cross-domain robustness not fully established
**Long-term stability on changing visual data**: Low confidence - Performance on continuously changing visual distributions not characterized

## Next Checks

1. Test VISA on specialized visual domains (medical imaging, remote sensing, scientific visualization) to evaluate cross-domain robustness and identify potential failure modes
2. Conduct ablation studies isolating the contribution of graph summarization versus group-wise selection to understand which component drives the majority of performance gains
3. Perform fine-grained visual feature preservation analysis using attention visualization and feature attribution methods to quantify what visual information is retained versus lost during token aggregation