---
ver: rpa2
title: 'MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based
  Dialogue'
arxiv_id: '2512.07544'
source_url: https://arxiv.org/abs/2512.07544
tags:
- dialogue
- persona
- dataset
- relation
- mocorp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating engaging and context-specific
  dialogue while maintaining a coherent personality in persona-based dialogue systems.
  The key issue is that existing datasets lack explicit relations between persona
  sentences and responses, making it difficult for models to effectively capture and
  utilize persona information.
---

# MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue

## Quick Facts
- **arXiv ID:** 2512.07544
- **Source URL:** https://arxiv.org/abs/2512.07544
- **Reference count:** 40
- **Primary result:** MoCoRP achieves superior persona consistency and engaging dialogue generation by incorporating explicit NLI relations into pre-trained models, outperforming existing baselines on ConvAI2 and MPChat datasets.

## Executive Summary
This paper addresses the challenge of generating engaging and context-specific dialogue while maintaining a coherent personality in persona-based dialogue systems. The key issue is that existing datasets lack explicit relations between persona sentences and responses, making it difficult for models to effectively capture and utilize persona information. The authors propose MoCoRP, a framework that incorporates explicit relations into language models by leveraging an NLI expert to extract relations between persona sentences and responses. This allows the model to selectively incorporate appropriate persona information. MoCoRP is applied to pre-trained models like BART and extended to large language models through alignment tuning.

## Method Summary
MoCoRP integrates an NLI expert's relation prediction capability into dialogue models through knowledge distillation, allowing implicit persona-response relation modeling during inference. The framework uses a two-stage iterative training procedure: relation learning on balanced NLI data followed by dialogue learning on persona datasets. Relation vectors are extracted from encoder hidden states and injected into the decoder start token embedding to condition response generation on persona-response relations. For LLM extension, the approach incorporates NLI labels into prompts and uses LoRA tuning to adapt large models.

## Key Results
- MoCoRP achieves superior persona consistency (C score improvements of 1-2 points) compared to BART baseline on ConvAI2 and MPChat datasets
- The model demonstrates significant quantitative improvements across multiple metrics including Hits@1, F1, BLEU, and ROUGE scores
- Qualitative evaluations show enhanced coherence, engagingness, groundedness, and naturalness in generated responses
- The LLM extension through alignment tuning successfully adapts MoCoRP to large language models while maintaining persona consistency

## Why This Works (Mechanism)

### Mechanism 1: NLI Expert Knowledge Distillation into Dialogue Model
Transferring the NLI expert's relation prediction capability to the dialogue model enables implicit persona-response relation modeling during inference. A RoBERTa-based NLI expert is trained on Dialogue NLI dataset to classify entailment/neutral/contradiction relations. During dialogue model training, KL divergence loss aligns the BART encoder's relation scores with the NLI expert's softmax outputs, forcing BART to internalize the relation prediction capability. Core assumption: The NLI expert's predictions on persona-response pairs provide meaningful supervisory signals that generalize to unseen dialogues. Evidence: [section 3.2.1] "The NLI expert generates NLI score z ∈ R³, which serves as a training signal" and 92.43% accuracy on Dialogue NLI test set.

### Mechanism 2: Relation-Conditioned Decoder Initialization
Injecting relation vectors into the decoder start token embedding conditions response generation on persona-response relations. The encoder's mask token representations for each persona sentence pass through RP Head → Dense layer → mean pooling to produce a single relation vector z_rel ∈ R^d_model. This vector is added to the decoder start token embedding [r], shifting the initial hidden state to encode persona-response relation information before generation begins. Core assumption: The relation vector captures sufficient information to guide generation without explicit relation labels at inference time. Evidence: [section 3.2.3] Equation 4-6 shows the transformation from relation scores to relation vector added to E[r].

### Mechanism 3: Relation Learning Pre-training to Address Class Imbalance
Pre-training on balanced NLI data before dialogue fine-tuning mitigates the neutral-heavy bias in persona dialogue datasets. Persona dialogue datasets show 84% neutral, 14% entailment, 2% contradiction (ConvAI2). Relation learning samples balanced data from Dialogue NLI (32% each class) before each epoch's dialogue learning, exposing the model to entailment and contradiction patterns it would otherwise rarely see. Core assumption: The model can transfer relation patterns learned from general NLI data to persona-specific dialogue contexts. Evidence: [section 3.2.2] "This imbalance may lead the model to predominantly learn neutral relations... By pre-training balanced relations through relation learning before training in a dialogue setting, the model can better represent a diverse range of NLI relations."

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: The entire framework treats persona consistency as an NLI problem—determining if a response entails, contradicts, or is neutral to persona statements.
  - Quick check question: Given persona "I love cats" and response "I have a Persian cat," what is the NLI relation? (Answer: Entailment)

- **Concept: Encoder-Decoder Attention Mechanisms**
  - Why needed here: MoCoRP modifies BART by extracting encoder hidden states at mask tokens (persona positions) and injecting processed relation vectors into the decoder's initial state.
  - Quick check question: In BART, which component attends to encoder outputs during generation? (Answer: The decoder's cross-attention layers)

- **Concept: Knowledge Distillation via Distribution Matching**
  - Why needed here: The relation prediction loss uses KL divergence to match the student (BART) relation distribution to the teacher (NLI expert) distribution, not hard labels.
  - Quick check question: Why use KL divergence instead of cross-entropy with hard labels? (Answer: Soft targets preserve uncertainty and inter-class relationships)

## Architecture Onboarding

- **Component map:**
  NLI Expert (frozen) -> BART Encoder (persona positions) -> RP Head -> Dense + Mean Pool -> Relation Vector -> BART Decoder (start token)
  NLI Expert also provides NLI scores for KL divergence loss
  Decoder outputs -> RS Head -> Response Selection Score

- **Critical path:**
  1. NLI Expert trained on Dialogue NLI (one-time, frozen)
  2. Relation learning epoch: sample NLI data → BART learns relation prediction
  3. Dialogue learning epoch: persona dialogue data → BART fine-tunes with NLI Expert guidance
  4. Iterate steps 2-3 for each epoch
  5. Inference: no NLI Expert needed; BART generates with internalized relations

- **Design tradeoffs:**
  - Relation learning data proportion: ConvAI2 tolerates up to 60%; MPChat degrades above 5%
  - LM loss during relation learning: [none] best for ConvAI2; [E] or [N,E] better for smaller datasets
  - α coefficient for relation prediction loss: set to 0.1 in experiments

- **Failure signatures:**
  - High perplexity with low C score: model ignoring persona (relation learning may be insufficient)
  - Low F1 with high C score on small datasets: excessive relation learning causing overfitting
  - Posterior model underperforming prior in LLM version: NLI labels from prior may be noisy

- **First 3 experiments:**
  1. **NLI Expert validation:** Train expert on Dialogue NLI and evaluate accuracy on held-out set. Target >90%. If failed, investigate data quality or model capacity.
  2. **Relation learning data sweep:** Test 0%, 5%, 10%, 20% relation learning proportions on validation set. Monitor Hits@1 and C score to find optimal balance for your dataset.
  3. **Ablation of relation vector injection:** Compare full MoCoRP vs. BART baseline without relation vector (skip z_rel addition). Expect C score drop of 1-2 points if mechanism is working.

## Open Questions the Paper Calls Out
None

## Limitations
- The NLI expert's class-wise accuracy breakdown is not provided, making it difficult to assess performance on rare relation types like contradiction
- The iterative training procedure lacks critical implementation details about data sampling and weighting between relation learning and dialogue learning stages
- The LLM extension's reliance on prior model-generated NLI labels introduces an uncontrolled error source that may propagate through the system

## Confidence

**High Confidence Claims:**
- The general approach of using NLI for persona consistency is well-supported by related work (LMEDR, BoB)
- Quantitative improvements on standard metrics (Hits@1, C score) are reliably reported
- The three-mechanism framework (NLI distillation, relation conditioning, pre-training) is logically coherent

**Medium Confidence Claims:**
- The specific parameter choices (α=0.1, relation learning proportions) are optimal
- The iterative training procedure yields consistent improvements across datasets
- The LLM extension's NLI label extraction process is robust

**Low Confidence Claims:**
- The claim that relation vectors "condition" generation behavior (difficult to verify without ablation studies)
- The assertion that 60% relation learning data is optimal for ConvAI2 without exploring higher proportions
- The LLM extension's superiority over prior methods without direct comparisons to published baselines

## Next Checks

1. **NLI Expert Robustness Analysis**: Train the NLI expert and compute per-class accuracy (entailment/neutral/contradiction) on Dialogue NLI test set. Investigate whether contradiction detection accuracy is particularly low given its rarity in persona datasets.

2. **Relation Learning Sensitivity Sweep**: Systematically vary relation learning data proportion from 0% to 80% in 10% increments on validation set. Plot Hits@1, C score, and PPL to identify the optimal balance point and verify the paper's claim that ConvAI2 tolerates much higher proportions than MPChat.

3. **Inference-Time Relation Vector Ablation**: Implement MoCoRP with and without relation vector injection into decoder start token. Compare C score and qualitative outputs to determine whether the relation vector truly conditions generation or merely adds noise to the initial state.