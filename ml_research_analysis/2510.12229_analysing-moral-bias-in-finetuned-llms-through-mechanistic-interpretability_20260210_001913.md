---
ver: rpa2
title: Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability
arxiv_id: '2510.12229'
source_url: https://arxiv.org/abs/2510.12229
tags:
- knobe
- effect
- moral
- finetuned
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether Large Language Models (LLMs) internalize\
  \ human-like moral biases during finetuning, focusing on the Knobe effect\u2014\
  a cognitive bias where negative outcomes are judged more intentional than positive\
  \ ones. Using Layer-Patching, a mechanistic interpretability technique, the authors\
  \ analyze three open-weights LLMs (Llama, Mistral, Gemma) to locate and mitigate\
  \ this bias."
---

# Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability

## Quick Facts
- **arXiv ID**: 2510.12229
- **Source URL**: https://arxiv.org/abs/2510.12229
- **Reference count**: 23
- **Primary result**: Layer-Patching technique successfully localizes and mitigates the Knobe effect bias in LLMs without degrading performance.

## Executive Summary
This paper investigates whether Large Language Models internalize human-like moral biases during finetuning, focusing on the Knobe effectâ€”a cognitive bias where negative outcomes are judged more intentional than positive ones. Using Layer-Patching, a mechanistic interpretability technique, the authors analyze three open-weights LLMs (Llama, Mistral, Gemma) to locate and mitigate this bias. They find that the Knobe effect emerges primarily in mid-to-late Transformer layers after finetuning and can be effectively reduced by replacing finetuned activations with those from the corresponding pretrained models. The bias reduction is achieved without degrading model performance on standard benchmarks. Ablation studies across different model scales confirm that the effect is not size-dependent but consistently induced by finetuning. This work demonstrates that social biases in LLMs are mechanistically localizable and amenable to targeted, post-hoc interventions without retraining.

## Method Summary
The paper employs Layer-Patching, a mechanistic interpretability technique, to analyze how moral biases emerge during LLM finetuning. The method involves systematically replacing activations from different Transformer layers between pretrained and finetuned model versions to identify where specific biases manifest. The authors test this approach on three open-weights models (Llama, Mistral, Gemma) using the Knobe effect as a case study. By comparing model responses to carefully constructed scenarios that elicit this bias, they pinpoint the layer ranges where the bias emerges and demonstrate that replacing finetuned activations with pretrained ones can mitigate the bias without affecting overall model performance on standard benchmarks.

## Key Results
- The Knobe effect bias is localized to mid-to-late Transformer layers and emerges specifically after finetuning
- Layer-Patching can effectively reduce this moral bias by replacing finetuned activations with pretrained ones
- Bias mitigation is achieved without degrading model performance on standard benchmarks
- Ablation studies across different model scales confirm the bias is consistently induced by finetuning, not model size

## Why This Works (Mechanism)
The Layer-Patching technique works by exploiting the modular nature of Transformer architectures, where different layers encode different types of information and reasoning patterns. By systematically swapping activations between pretrained and finetuned models at specific layer ranges, the method isolates the computational changes responsible for bias emergence. The mid-to-late layer localization suggests that moral reasoning patterns are refined rather than initially encoded during pretraining, and that finetuning introduces specific weight adjustments that manifest as cognitive biases. This mechanistically grounded approach allows for targeted intervention without disrupting the broader knowledge and capabilities encoded in earlier layers.

## Foundational Learning

**Transformer Architecture** - Why needed: Understanding layer-wise computation is essential for Layer-Patching; Quick check: Can identify which layers encode semantic vs. reasoning patterns.

**Mechanistic Interpretability** - Why needed: Provides framework for locating specific behaviors within model components; Quick check: Can explain how activation replacement affects outputs without retraining.

**Finetuning Dynamics** - Why needed: Understanding how pretraining knowledge is modified during adaptation; Quick check: Can distinguish between preserved and altered capabilities post-finetuning.

**Cognitive Biases in Language Models** - Why needed: Framework for measuring moral reasoning deviations; Quick check: Can design test scenarios that reliably elicit specific biases.

**Activation Space Analysis** - Why needed: Understanding how replacing activations affects model behavior; Quick check: Can interpret differences between pretrained and finetuned activation distributions.

## Architecture Onboarding

**Component Map**: Input Embeddings -> Attention Layers (1-n) -> Feed-Forward Layers -> Output Projection

**Critical Path**: Layer-Patching specifically targets the attention and feed-forward layers in mid-to-late ranges where reasoning patterns are refined.

**Design Tradeoffs**: Localizing bias to specific layers enables targeted mitigation but requires precise identification of responsible components; activation replacement preserves general capabilities while modifying specific behaviors.

**Failure Signatures**: Bias reduction that degrades unrelated capabilities, incomplete bias mitigation, or unintended behavioral changes in other reasoning domains.

**First Experiments**:
1. Layer-by-layer activation replacement to identify exact onset of bias emergence
2. Cross-model comparison of bias localization patterns across different architectures
3. Quantitative measurement of performance preservation across diverse downstream tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on a single cognitive bias (Knobe effect) in English text, limiting generalizability
- Layer-Patching assumes mid-to-late layer activation replacement preserves downstream performance
- Ablation studies limited to three models, leaving architectural variations unexplored
- Long-term effects on model behavior and emergent properties remain unexplored

## Confidence

**High**: Knobe effect localization in mid-to-late layers and bias mitigation via activation replacement - consistent experimental results across multiple models.

**Medium**: Generalizability of approach to other biases and comprehensive performance preservation - method effectiveness beyond tested scenarios remains uncertain.

**Low**: Scalability to larger, heterogeneous model families and applicability beyond controlled experimental settings - real-world deployment challenges unaddressed.

## Next Checks

1. Test Layer-Patching on additional moral and social biases beyond the Knobe effect
2. Evaluate method's effectiveness across diverse languages and cultural contexts
3. Conduct long-term behavioral studies to assess unintended consequences of activation replacement on model coherence and emergent capabilities