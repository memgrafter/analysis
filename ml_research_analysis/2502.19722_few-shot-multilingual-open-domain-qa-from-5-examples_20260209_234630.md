---
ver: rpa2
title: Few-Shot Multilingual Open-Domain QA from 5 Examples
arxiv_id: '2502.19722'
source_url: https://arxiv.org/abs/2502.19722
tags:
- data
- question
- answer
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FSMODQA, a few-shot learning approach for
  multilingual open-domain question answering using minimal language-specific supervision
  (up to 5 examples per language). The method leverages large language models to generate
  synthetic multilingual QA data from Wikidata and Wikipedia passages, combined with
  a self-supervised pre-training objective.
---

# Few-Shot Multilingual Open-Domain QA from 5 Examples

## Quick Facts
- arXiv ID: 2502.19722
- Source URL: https://arxiv.org/abs/2502.19722
- Reference count: 32
- Primary result: FSMODQA achieves +8.4% gain in multilingual QA and +5.1% in retrieval accuracy using up to 5 examples per language

## Executive Summary
This paper presents FSMODQA, a few-shot learning approach for multilingual open-domain question answering that requires minimal language-specific supervision. The method leverages large language models to generate synthetic multilingual QA data from Wikidata and Wikipedia passages, combined with a self-supervised pre-training objective. FSMODQA significantly outperforms existing few-shot and supervised baselines, demonstrating the effectiveness of synthetic data generation for multilingual QA tasks. The approach also enables effective zero-shot language adaptation to new languages through cross-lingual prompting.

## Method Summary
FSMODQA employs a two-stage approach: first, it uses large language models to generate synthetic multilingual QA data by extracting facts from Wikidata and pairing them with Wikipedia passages. Second, it applies self-supervised pre-training on this synthetic data to adapt the model to multilingual contexts. The system requires only up to 5 examples per language for fine-tuning, making it highly efficient compared to traditional supervised approaches that need extensive labeled data. For zero-shot language adaptation, the method uses cross-lingual prompting to enable the model to handle languages not seen during training.

## Key Results
- FSMODQA achieves +8.4% improvement in multilingual QA performance over existing few-shot baselines
- The approach shows +5.1% gain in retrieval accuracy compared to supervised methods
- Zero-shot cross-lingual adaptation works effectively without requiring additional language-specific training data

## Why This Works (Mechanism)
The approach works by leveraging the knowledge encoded in large language models to generate high-quality synthetic training data across multiple languages. By extracting structured facts from Wikidata and using LLMs to create natural questions paired with relevant Wikipedia passages, the system creates diverse multilingual training examples without manual annotation. The self-supervised pre-training objective helps the model learn language-agnostic representations that transfer well across languages. The cross-lingual prompting mechanism allows the model to generalize to new languages by leveraging the multilingual knowledge acquired during synthetic data generation.

## Foundational Learning
- **Large Language Models**: Why needed - Generate high-quality synthetic questions from structured data; Quick check - Verify the LLM can produce fluent, diverse questions across target languages
- **Knowledge Graphs (Wikidata)**: Why needed - Provide structured, multilingual facts as source material; Quick check - Ensure Wikidata coverage is sufficient for target languages
- **Self-Supervised Learning**: Why needed - Leverage unlabeled multilingual data to improve generalization; Quick check - Confirm pre-training improves cross-lingual transfer
- **Cross-Lingual Transfer**: Why needed - Enable zero-shot adaptation to new languages; Quick check - Test performance on truly unseen language families
- **Open-Domain QA Architecture**: Why needed - Handle retrieval from large document collections; Quick check - Validate retrieval effectiveness on benchmark datasets
- **Few-Shot Learning**: Why needed - Minimize expensive language-specific annotation; Quick check - Determine minimum examples needed per language

## Architecture Onboarding
**Component Map**: Wikidata facts extraction -> LLM question generation -> Wikipedia passage retrieval -> Synthetic QA dataset creation -> Self-supervised pre-training -> Multilingual QA model
**Critical Path**: The synthetic data generation pipeline is the critical path, as the quality and diversity of generated examples directly determines model performance
**Design Tradeoffs**: Uses LLM-generated synthetic data instead of human annotations to minimize supervision cost, accepting potential noise in synthetic examples for massive scalability
**Failure Signatures**: Poor retrieval accuracy when Wikipedia coverage is sparse for target languages; degraded performance on complex reasoning questions that are harder to synthesize from simple Wikidata facts
**First Experiments**: 1) Generate 100 synthetic QA pairs for a target language and manually evaluate quality, 2) Test retrieval accuracy on a small multilingual dataset before full pre-training, 3) Evaluate zero-shot performance on a language not in the training set

## Open Questions the Paper Calls Out
None

## Limitations
- The approach may inherit biases from Wikidata, favoring encyclopedic and fact-based questions over diverse real-world query types
- The minimal supervision requirement (5 examples) is not thoroughly explored to determine the true lower bound for acceptable performance
- The synthetic data generation pipeline's effectiveness across different language families and scripts is not comprehensively validated

## Confidence
**High Confidence**: The core technical contribution of using LLM-generated synthetic multilingual QA data combined with self-supervised pre-training is clearly described and the reported performance improvements over baselines are statistically significant and well-documented.

**Medium Confidence**: The claimed +8.4% gain in multilingual QA and +5.1% in retrieval accuracy depends on specific benchmark conditions and may vary with different datasets or evaluation protocols.

**Low Confidence**: The generalizability of the approach to non-Indo-European languages and languages with very limited Wikipedia coverage is not thoroughly examined.

## Next Checks
1. Conduct ablation studies to determine the minimum number of examples per language required to achieve stable performance
2. Perform detailed error analysis categorizing failures by question type and language family
3. Test the zero-shot cross-lingual adaptation capability on a truly unseen language family (e.g., applying a model trained on European languages to Southeast Asian or African languages)