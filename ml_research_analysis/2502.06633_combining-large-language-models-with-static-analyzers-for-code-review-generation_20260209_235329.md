---
ver: rpa2
title: Combining Large Language Models with Static Analyzers for Code Review Generation
arxiv_id: '2502.06633'
source_url: https://arxiv.org/abs/2502.06633
tags:
- code
- review
- static
- reviews
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving automated code
  review generation by combining the precision of static analysis tools with the contextual
  understanding of large language models (LLMs). The authors propose three hybrid
  strategies to integrate knowledge-based systems (KBS) with learning-based systems
  (LBS): Data-Augmented Training (DAT), Retrieval-Augmented Generation (RAG), and
  Naive Concatenation of Outputs (NCO).'
---

# Combining Large Language Models with Static Analyzers for Code Review Generation

## Quick Facts
- arXiv ID: 2502.06633
- Source URL: https://arxiv.org/abs/2502.06633
- Reference count: 40
- Primary result: RAG approach significantly improves accuracy while DAT achieves highest coverage in hybrid code review generation

## Executive Summary
This paper addresses the challenge of improving automated code review generation by combining the precision of static analysis tools with the contextual understanding of large language models (LLMs). The authors propose three hybrid strategies to integrate knowledge-based systems (KBS) with learning-based systems (LBS): Data-Augmented Training (DAT), Retrieval-Augmented Generation (RAG), and Naive Concatenation of Outputs (NCO). DAT involves fine-tuning the LLM on an augmented dataset that includes both static analysis and LLM-generated reviews. RAG dynamically retrieves static analysis results during inference to guide the LLM. NCO simply combines the outputs of both systems after generation.

The evaluation demonstrates that the RAG approach significantly improves accuracy compared to using the LLM alone, while DAT achieves the highest coverage by exposing the model to diverse issues during training. NCO provides moderate improvements. The hybrid approaches effectively bridge the gap between the precision of static analysis tools and the comprehensiveness of LLMs, resulting in more effective and human-like code reviews. The study highlights the potential of combining static analysis with LLMs to enhance automated code review generation.

## Method Summary
The paper proposes three hybrid strategies for integrating static analysis tools with LLMs for code review generation. DAT fine-tunes the LLM on an augmented dataset combining static analysis and LLM-generated reviews. RAG retrieves static analysis results during inference to guide the LLM's output. NCO concatenates the outputs of both systems after generation. The framework uses Python's AST module for static analysis, extracting structural information and detecting potential issues based on keyword patterns. The LLM component uses CodeBERT, a transformer-based model pre-trained on code. The hybrid approaches are evaluated on a Python dataset, comparing their performance against standalone static analysis and LLM methods.

## Key Results
- RAG approach significantly improves accuracy compared to using the LLM alone
- DAT achieves the highest coverage by exposing the model to diverse issues during training
- NCO provides moderate improvements by combining both system outputs

## Why This Works (Mechanism)
The hybrid approaches work by leveraging the complementary strengths of static analysis tools and LLMs. Static analyzers provide precise, rule-based detection of code issues but lack contextual understanding and may produce false positives. LLMs excel at understanding code context and generating human-like reviews but can miss subtle issues and may hallucinate. By combining these systems, the hybrid approaches achieve both precision and comprehensiveness. DAT exposes the LLM to a wider range of issues during training, improving its ability to detect problems. RAG provides real-time guidance during inference, ensuring the LLM considers all relevant issues. NCO combines the strengths of both systems post-generation, catching issues that either system might miss individually.

## Foundational Learning
- **Static Analysis**: Automated code examination without execution; needed for precise detection of code issues; quick check: rule-based pattern matching on AST
- **LLM Code Generation**: Using language models trained on code for review generation; needed for contextual understanding and human-like output; quick check: transformer-based architecture with code-specific pre-training
- **Hybrid Systems**: Combining multiple AI approaches for improved performance; needed to leverage complementary strengths; quick check: integration of KBS and LBS components
- **Retrieval-Augmented Generation**: Dynamically retrieving information during inference to guide generation; needed for real-time guidance based on static analysis; quick check: context-aware information retrieval
- **Data Augmentation**: Enhancing training data with additional information; needed to expose models to diverse issues; quick check: synthetic data generation techniques
- **CodeBERT**: Pre-trained transformer model for code understanding; needed as base LLM component; quick check: masked language modeling on code corpus

## Architecture Onboarding

**Component Map**: Static Analyzer -> Feature Extractor -> Hybrid Strategy (DAT/RAG/NCO) -> LLM -> Code Review

**Critical Path**: Code input → Static analysis AST parsing → Issue detection → Feature extraction → Hybrid strategy processing → LLM generation → Final code review output

**Design Tradeoffs**: DAT requires extensive training data but provides best coverage; RAG offers real-time guidance with moderate complexity; NCO is simplest but least effective

**Failure Signatures**: Static analyzer misses issues → LLM hallucinates → Hybrid strategy fails to integrate properly → Poor review quality or relevance

**3 First Experiments**:
1. Test DAT approach on small synthetic dataset to verify training effectiveness
2. Implement RAG with single static analysis rule to validate retrieval mechanism
3. Compare NCO output quality against standalone static analyzer and LLM outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Python code and CodeBERT model, limiting generalizability
- Static analysis relies on AST parsing and keyword detection, potentially missing complex patterns
- Synthetic dataset generation may not capture real-world code review diversity and complexity

## Confidence

**High confidence**:
- RAG approach significantly improves accuracy compared to standalone LLM generation

**Medium confidence**:
- DAT achieves highest coverage, pending validation on real-world datasets
- Overall hybrid approach effectiveness, pending broader language and tool testing

## Next Checks

1. Evaluate hybrid approaches on real-world dataset of human-written code reviews from open-source repositories
2. Test framework with different base LLMs (GPT-4, Claude) and static analysis tools (SonarQube, CodeQL)
3. Conduct human evaluation study comparing generated reviews to those from standalone systems