---
ver: rpa2
title: Towards Effective, Stealthy, and Persistent Backdoor Attacks Targeting Graph
  Foundation Models
arxiv_id: '2511.17982'
source_url: https://arxiv.org/abs/2511.17982
tags:
- graph
- backdoor
- trigger
- uni00000013
- downstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies backdoor attacks against Graph Foundation Models
  (GFMs), which can introduce malicious behaviors into downstream applications. The
  authors propose GFM-BA, a novel attack method that addresses three key challenges:
  effectiveness (lack of downstream task knowledge), stealthiness (variability in
  node features across domains), and persistence (backdoor forgetting during fine-tuning).'
---

# Towards Effective, Stealthy, and Persistent Backdoor Attacks Targeting Graph Foundation Models

## Quick Facts
- arXiv ID: 2511.17982
- Source URL: https://arxiv.org/abs/2511.17982
- Authors: Jiayi Luo; Qingyun Sun; Lingjuan Lyu; Ziwei Zhang; Haonan Yuan; Xingcheng Fu; Jianxin Li
- Reference count: 21
- Primary result: Novel backdoor attack method GFM-BA achieves 100% attack success rate against Graph Foundation Models while maintaining high clean data accuracy and persistence against fine-tuning

## Executive Summary
This paper introduces GFM-BA, a backdoor attack framework specifically designed for Graph Foundation Models (GFMs) that addresses three critical challenges: effectiveness without downstream task knowledge, stealthiness across diverse node features, and persistence against fine-tuning. The authors propose a novel approach that uses prototype-based trigger association, node-adaptive trigger generation, and persistent backdoor anchoring to overcome limitations of existing backdoor attack methods when applied to GFMs. The attack achieves perfect attack success rates while maintaining high performance on clean data across multiple datasets and victim models.

## Method Summary
GFM-BA introduces a comprehensive backdoor attack framework that addresses the unique challenges of attacking Graph Foundation Models. The method employs a label-free trigger association module that uses prototype embeddings to identify optimal trigger locations without requiring downstream task labels. A node-adaptive trigger generator creates stealthy triggers that blend with node feature distributions across different domains. The persistent backdoor anchoring module ensures the backdoor remains effective even after fine-tuning on downstream tasks. The framework operates through a pipeline where the trigger generator creates domain-specific triggers, the anchor module maintains backdoor persistence, and the trigger association module identifies optimal injection points based on prototype similarity.

## Key Results
- Achieves 100% attack success rate across multiple datasets and victim GFMs
- Maintains high accuracy on clean data (comparable to baseline models)
- Demonstrates strong persistence against fine-tuning, outperforming existing methods

## Why This Works (Mechanism)
The attack succeeds by addressing the fundamental mismatch between traditional backdoor attack assumptions and the unique characteristics of GFMs. By using prototype-based trigger association, the method bypasses the need for downstream task labels while ensuring trigger effectiveness. The node-adaptive trigger generation creates domain-specific triggers that blend naturally with node features, avoiding detection. The persistent backdoor anchoring mechanism maintains trigger effectiveness through fine-tuning by establishing strong associations between triggers and target labels in the pre-trained model.

## Foundational Learning
- **Graph Foundation Models**: Pre-trained models that learn universal representations from large-scale graph data, enabling few-shot adaptation to downstream tasks. Needed because GFMs are the target of the attack. Quick check: Understand how GFM pre-training differs from traditional GNN training.
- **Backdoor Attacks**: Techniques that embed hidden behaviors in models by injecting triggers during training, causing specific behaviors when triggers appear at inference. Needed as the attack methodology. Quick check: Recognize the difference between poisoning and backdoor attacks.
- **Prototype Embeddings**: Representative feature vectors that capture the essence of different classes or domains in embedding space. Needed for label-free trigger association. Quick check: Can identify how prototypes are computed from unlabeled data.
- **Fine-tuning**: Process of adapting pre-trained models to specific downstream tasks by continued training on task-specific data. Needed because persistence against fine-tuning is a key challenge. Quick check: Understand how fine-tuning can potentially remove backdoors.
- **Node-adaptive Trigger Generation**: Method that creates triggers tailored to individual nodes based on their feature distributions. Needed for stealthiness. Quick check: Recognize why fixed triggers are less stealthy than adaptive ones.
- **Persistent Backdoor Anchoring**: Techniques that ensure backdoors remain effective after model adaptation. Needed to address backdoor forgetting. Quick check: Can explain why backdoors typically degrade during fine-tuning.

## Architecture Onboarding
**Component Map**: Data → Pre-trained GFM → GFM-BA Framework → Poisoned Model
**Critical Path**: Trigger Generation → Trigger Association → Backdoor Injection → Fine-tuning Resistance
**Design Tradeoffs**: The method trades increased complexity (multiple specialized modules) for effectiveness across the three challenge dimensions. Alternative approaches might use simpler single-module designs but would likely fail on one or more challenge dimensions.
**Failure Signatures**: Backdoor failure manifests as reduced attack success rate, particularly after fine-tuning. Stealth failure appears as increased detection rates or reduced clean data accuracy.
**First Experiments**: 1) Test trigger effectiveness on clean vs. triggered samples. 2) Evaluate stealth by comparing triggered vs. clean node feature distributions. 3) Assess persistence by measuring attack success before and after fine-tuning.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability to larger-scale graph foundation models beyond tested architectures
- Focus on node-level classification tasks, leaving performance on graph-level or link prediction tasks unexplored
- May not perform equally well across diverse graph domains with different structural properties

## Confidence
- Effectiveness: High - 100% attack success rate consistently achieved
- Stealthiness: Medium - Relies on specific stealth metrics that may not capture all detection methods
- Persistence: Medium - Fine-tuning scenarios may not encompass all possible adaptation strategies

## Next Checks
1. Test GFM-BA against larger-scale graph foundation models (e.g., 100M+ parameters) to assess scalability and performance degradation
2. Evaluate the attack's effectiveness across diverse graph domains including social networks, biological networks, and knowledge graphs with varying structural properties
3. Assess detection capabilities using multiple adversarial defense mechanisms beyond the simple trigger removal method presented in the paper