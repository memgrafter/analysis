---
ver: rpa2
title: 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers'
arxiv_id: '2511.11062'
source_url: https://arxiv.org/abs/2511.11062
tags:
- attention
- sparsity
- liteattention
- diffusion
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiteAttention addresses the high computational cost of attention
  in video diffusion transformers by exploiting the temporal coherence of sparsity
  patterns across denoising steps. Instead of recomputing sparse attention patterns
  at each step, LiteAttention identifies skippable tiles early and propagates these
  skip decisions forward, achieving evolutionary computation skips.
---

# LiteAttention: A Temporal Sparse Attention for Diffusion Transformers

## Quick Facts
- **arXiv ID:** 2511.11062
- **Source URL:** https://arxiv.org/abs/2511.11062
- **Reference count:** 4
- **Primary result:** Achieves 50% inference time reduction at 77% sparsity while maintaining video quality comparable to full attention

## Executive Summary
LiteAttention addresses the high computational cost of attention in video diffusion transformers by exploiting temporal coherence of sparsity patterns across denoising steps. The method identifies skippable attention tiles early and propagates these skip decisions forward, achieving evolutionary computation skips. This approach combines the adaptivity of dynamic sparsity with the efficiency of static methods, eliminating full attention computations for marked tiles without repeated profiling. Implemented as a CUDA kernel extension to FlashAttention3, LiteAttention achieves substantial runtime improvements on production video diffusion models.

## Method Summary
LiteAttention introduces a temporal sparse attention mechanism that reduces the computational burden of attention operations in video diffusion transformers. The core innovation lies in recognizing that sparsity patterns in attention maps exhibit temporal coherence across denoising steps, allowing the system to skip redundant computations. By identifying and marking skippable tiles early in the denoising process, LiteAttention propagates these decisions forward, eliminating the need for full attention computations on these tiles in subsequent steps. The method is implemented as a CUDA kernel extension to FlashAttention3, enabling practical deployment on production video diffusion models.

## Key Results
- Achieves up to 50% reduction in inference time at 77% sparsity
- Maintains video quality comparable to full attention models based on FID scores
- Demonstrates evolutionary computation skips by propagating skip decisions across denoising steps

## Why This Works (Mechanism)
LiteAttention works by leveraging the temporal coherence of attention sparsity patterns across denoising steps in video diffusion transformers. The method identifies which attention tiles can be skipped early in the process and maintains these skip decisions throughout subsequent denoising steps. This approach eliminates the need to recompute attention patterns at each step, combining the benefits of dynamic sparsity (adaptivity) with the efficiency of static sparsity (no repeated profiling). The CUDA kernel implementation ensures these optimizations are realized in practice, making the method suitable for production video generation systems.

## Foundational Learning
- **Video diffusion transformers:** Why needed - understanding the base architecture that LiteAttention optimizes; Quick check - review how attention mechanisms work in video generation
- **Attention sparsity patterns:** Why needed - grasping the temporal coherence assumption; Quick check - analyze attention maps across denoising steps
- **CUDA kernel optimization:** Why needed - understanding the implementation details; Quick check - review FlashAttention3 architecture and extension points
- **Evolutionary computation:** Why needed - understanding the skip propagation mechanism; Quick check - trace how skip decisions propagate across steps
- **Sparsity metrics:** Why needed - evaluating the effectiveness of the approach; Quick check - understand how sparsity percentage relates to performance gains

## Architecture Onboarding

**Component Map:**
FlashAttention3 kernel -> LiteAttention extension -> Sparsity analysis module -> Skip propagation system -> Video diffusion transformer

**Critical Path:**
Input video frames -> Attention computation -> Sparsity pattern analysis -> Skip decision propagation -> Skipped attention tiles -> Output frames

**Design Tradeoffs:**
- Adaptivity vs. static computation: LiteAttention balances dynamic sparsity identification with efficient skip propagation
- Implementation complexity vs. performance gains: CUDA kernel extension provides significant speedups but may limit portability
- Temporal coherence assumption vs. general applicability: Method assumes consistent sparsity patterns, which may not hold for all video types

**Failure Signatures:**
- Inconsistent sparsity patterns across denoising steps leading to poor skip decisions
- Overhead from skip decision computation outweighing performance benefits
- Quality degradation when aggressive sparsity levels compromise important attention computations

**First Experiments:**
1. Benchmark runtime performance across different sparsity levels (20%, 50%, 77%)
2. Compare FID scores between LiteAttention and full attention implementations
3. Test the method on videos with varying motion characteristics and scene transitions

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on FlashAttention3 kernel infrastructure, limiting portability to other attention implementations
- Assumes temporal sparsity patterns remain consistent across denoising steps, which may not hold for videos with rapid scene changes or high motion content
- Evaluation focuses primarily on runtime metrics and FID scores without extensive perceptual quality analysis at higher sparsity levels

## Confidence
- **High confidence** in the runtime improvement claims (50% reduction at 77% sparsity) given the CUDA kernel implementation and reported metrics
- **Medium confidence** in the generality of the evolutionary skip approach across different video types, as the paper does not extensively test edge cases
- **Medium confidence** in the quality preservation claims, as FID comparisons alone may not capture all aspects of visual fidelity

## Next Checks
1. Test LiteAttention's performance on videos with rapid scene transitions and high motion content to verify the assumption about temporal coherence
2. Conduct human perceptual studies comparing generated videos at various sparsity levels to complement the FID metrics
3. Evaluate the method's effectiveness when ported to alternative attention implementations beyond FlashAttention3 to assess generalizability