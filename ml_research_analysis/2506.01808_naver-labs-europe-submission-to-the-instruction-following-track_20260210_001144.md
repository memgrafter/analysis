---
ver: rpa2
title: NAVER LABS Europe Submission to the Instruction-following Track
arxiv_id: '2506.01808'
source_url: https://arxiv.org/abs/2506.01808
tags:
- speech
- data
- training
- table
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes NAVER LABS Europe''s submission to the IWSLT
  2025 Instruction-following Speech Processing Track, where the goal was to build
  a system that can perform ASR, ST, and SQA tasks from English speech input into
  Chinese, Italian, and German. The core approach involved training two modules in
  parallel: a speech-to-LLM embedding projector using SeamlessM4T-v2-large speech
  representations, and LoRA adapters on top of Llama-3.1-8B-Instruct.'
---

# NAVER LABS Europe Submission to the Instruction-following Track

## Quick Facts
- arXiv ID: 2506.01808
- Source URL: https://arxiv.org/abs/2506.01808
- Reference count: 40
- Primary result: Achieved competitive ASR (17-19 WER) and ST (BLEU 30-43) performance on IWSLT 2025 instruction-following track

## Executive Summary
This paper presents NAVER LABS Europe's submission to the IWSLT 2025 Instruction-following Speech Processing Track, focusing on building a unified system for ASR, ST, and SQA tasks. The approach centers on training two modules in parallel: a speech-to-LLM embedding projector and LoRA adapters on Llama-3.1-8B-Instruct, which are then jointly instruction-tuned. The system demonstrates strong performance across all three tasks, with ASR WER around 17-19 and ST BLEU scores of 30-43, while maintaining competitive SQA performance. The modular design and efficient fine-tuning strategy enable effective multimodal instruction following.

## Method Summary
The core methodology involves training two parallel modules: a speech-to-LLM embedding projector using SeamlessM4T-v2-large speech representations, and LoRA adapters on top of Llama-3.1-8B-Instruct. These modules are jointly instruction-tuned for 1K steps using multilingual and multimodal data. The system processes English speech input and generates outputs for Chinese, Italian, and German target languages across ASR, ST, and SQA tasks. The approach leverages the strengths of both speech representation learning and text-based LLM adaptation through a unified instruction-following framework.

## Key Results
- Achieved ASR WER of 17-19 on IWSLT 2025 benchmark
- Obtained ST BLEU scores of 30-43, improving over speech-only baselines
- Demonstrated strong SQA performance close to text-only baselines
- Showed that decoupling speech and text adaptation followed by efficient multimodal fine-tuning can build effective unified speech-language models

## Why This Works (Mechanism)
The system's effectiveness stems from its modular architecture that separately optimizes speech representation learning and text adaptation before combining them. The speech-to-LLM embedding projector bridges the gap between audio signals and language model understanding, while LoRA adapters provide efficient parameter-efficient fine-tuning of the LLM component. Joint instruction-tuning allows the system to learn cross-modal alignment and task-specific behaviors simultaneously. This decoupling strategy enables specialized optimization of each modality before integration, leading to better overall performance than end-to-end training approaches.

## Foundational Learning
- **Speech Representation Learning**: Understanding how audio signals are converted to meaningful embeddings for language models. Why needed: Essential for bridging the modality gap between speech and text processing. Quick check: Verify that the embedding projector maintains semantic consistency across different speech inputs.
- **Parameter-Efficient Fine-Tuning**: LoRA adapters allow efficient adaptation of large models with minimal parameter updates. Why needed: Enables scalable model customization without full fine-tuning costs. Quick check: Confirm that adapter weights remain stable across different training runs.
- **Multimodal Instruction Tuning**: Training models to follow instructions across different input-output modalities. Why needed: Critical for building versatile systems that can handle diverse task specifications. Quick check: Test instruction-following accuracy across all three target tasks.
- **Cross-Lingual Transfer Learning**: Adapting models trained on one language pair to work with others. Why needed: Enables resource-efficient expansion to new language combinations. Quick check: Evaluate performance drop when transferring to unseen language pairs.

## Architecture Onboarding

**Component Map**: Speech Input -> Embedding Projector -> LLM with LoRA Adapters -> Output Generator -> Target Languages

**Critical Path**: Speech Input → Embedding Projector → LLM with LoRA Adapters → Output Generation

**Design Tradeoffs**: The modular approach trades some end-to-end optimization potential for flexibility and efficiency. Using LoRA adapters reduces computational overhead but may limit the depth of adaptation compared to full fine-tuning. The speech-to-LLM projector adds an intermediate step that could introduce latency but enables better cross-modal understanding.

**Failure Signatures**: Performance degradation in SQA tasks may indicate misalignment between speech embeddings and LLM understanding. Low BLEU scores could suggest inadequate translation quality from the LLM component. ASR errors might stem from insufficient speech representation quality or projector calibration issues.

**First Experiments**:
1. Test the speech-to-LLM projector independently on speech-to-text conversion tasks
2. Evaluate LoRA adapter performance on text-only instruction following
3. Assess joint system performance on a simplified task combination (ASR only)

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Limited evaluation to specific language pairs (English-to-Chinese, Italian, German) raises questions about generalizability to other languages
- Narrow evaluation metrics (WER, BLEU, unspecified SQA metrics) may not capture broader practical utility or robustness to domain shifts
- Focus on controlled benchmark conditions leaves unclear effectiveness in low-resource or out-of-domain scenarios

## Confidence
- **High**: ASR and ST results supported by standard, well-established metrics showing clear improvements over speech-only baselines
- **Medium**: SQA performance reported as strong but lacks detailed evaluation setup and comprehensive baseline comparison
- **Low**: System adaptability to new languages or tasks remains untested, confined to IWSLT 2025 instruction-following track conditions

## Next Checks
1. **Cross-Lingual Transfer:** Evaluate the system on an unseen language pair (e.g., English-to-Spanish or English-to-Japanese) to assess robustness and generalization beyond the IWSLT 2025 languages
2. **Low-Resource Adaptation:** Test the approach on a truly low-resource language (e.g., Bemba or another African language) to determine if the modular, LoRA-based architecture remains effective under resource constraints
3. **Domain Robustness:** Assess model performance on out-of-domain speech data (e.g., conversational or technical speech) to verify that the unified model maintains accuracy and instruction-following capabilities outside the benchmark's controlled environment