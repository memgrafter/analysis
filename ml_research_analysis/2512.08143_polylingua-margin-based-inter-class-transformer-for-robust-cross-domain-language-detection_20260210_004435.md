---
ver: rpa2
title: 'PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language
  Detection'
arxiv_id: '2512.08143'
source_url: https://arxiv.org/abs/2512.08143
tags:
- language
- polylingua
- languages
- contrastive
- amazon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurate language identification
  in multilingual systems, particularly for short utterances and code-switching scenarios
  where song titles and user language differ. The authors propose PolyLingua, a lightweight
  Transformer-based model that employs a two-level contrastive learning framework
  with instance-level separation and class-level alignment using adaptive margins.
---

# PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection

## Quick Facts
- arXiv ID: 2512.08143
- Source URL: https://arxiv.org/abs/2512.08143
- Reference count: 11
- 99.25% F1 on Amazon Massive dataset, 98.15% F1 on Song dataset

## Executive Summary
PolyLingua addresses the challenge of accurate language identification in multilingual systems, particularly for short utterances and code-switching scenarios where song titles and user language differ. The authors propose a lightweight Transformer-based model that employs a two-level contrastive learning framework with instance-level separation and class-level alignment using adaptive margins. This approach helps form compact, well-separated embeddings even for closely related languages. Evaluated on Amazon Massive (99.25% F1) and a Song dataset (98.15% F1), PolyLingua surpasses Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments. The model also shows improved robustness to noisy utterances containing diverse entities and informal grammar.

## Method Summary
PolyLingua uses a multilingual MiniLM-L12-H384 encoder with three prediction heads: binary in-domain detection, fine-grained language classification among 10 supported languages, and a projection head for contrastive learning. The model employs a two-level contrastive learning framework: instance-level separation using supervised contrastive loss, and class-level alignment with adaptive margins. The margins are higher (δhigh=0.4) for confusing language pairs like Spanish-Portuguese-French and lower (δlow=0.0) for well-separated languages. Training includes random deletion (p=0.15), typo noise, and dynamic entity replacement via multilingual NER. The combined objective uses λ1=λ2=1.0, λ3=0.1, and τ=0.07.

## Key Results
- Achieves 99.25% F1 on Amazon Massive dataset with 10 in-domain languages
- Achieves 98.15% F1 on Song dataset with multilingual music requests
- Uses 10x fewer parameters than Sonnet 3.5 while maintaining superior performance

## Why This Works (Mechanism)
The two-level contrastive learning framework with adaptive margins effectively separates embeddings for both closely related and distinct languages. Instance-level separation ensures individual utterances are distinct, while class-level alignment with language-specific margins creates compact clusters for each language. The higher margins for confusing pairs (es-pt-fr) prevent overlap, while lower margins for well-separated languages maintain tight clusters. The entity replacement augmentation improves robustness to real-world noisy utterances containing song titles and diverse entities.

## Foundational Learning
- Contrastive learning: why needed - to create separable embeddings; quick check - verify embeddings form distinct clusters in UMAP
- Adaptive margins: why needed - to handle varying similarity between language pairs; quick check - examine confusion matrix for confusing pairs
- Multilingual NER: why needed - for realistic entity replacement augmentation; quick check - verify entity diversity in training utterances
- MiniLM encoder: why needed - lightweight yet effective for language ID; quick check - compare parameter count and inference speed
- Cosine annealing: why needed - to optimize learning rate schedule; quick check - plot training loss vs epochs
- Projection head: why needed - to create contrastive space; quick check - ensure projection space shows better separation than input space

## Architecture Onboarding

Component map: Input -> MiniLM encoder -> Binary detection head, Language ID head, Projection head -> Instance contrastive loss + Class contrastive loss + CE loss

Critical path: Input text → MiniLM encoder → Projection head → Contrastive losses (instance + class) → Language ID head → Cross-entropy loss

Design tradeoffs: Lightweight MiniLM vs full-size BERT for inference speed; two-level contrastive learning vs single-level for handling confusing pairs; entity replacement augmentation vs static augmentation for robustness

Failure signatures: High confusion between Spanish/Portuguese/French indicates incorrect δhigh assignment; poor generalization to Song dataset suggests inadequate entity replacement; overfitting to training data may occur without sufficient OOD samples

First experiments: 1) Train with δhigh=0.4 on es-pt-fr pairs and δlow=0.0 elsewhere; 2) Implement entity replacement using multilingual NER and measure entity diversity; 3) Ablate 40% OOD training ratio to confirm its importance

## Open Questions the Paper Calls Out
None

## Limitations
- The complete set of confusing language pairs beyond es-pt-fr is not specified
- Entity replacement augmentation implementation details are incomplete
- Distributed training setup and GPU requirements are not documented

## Confidence
High confidence in model architecture (MiniLM encoder + contrastive learning framework)
Medium confidence in specific margin values and their assignment to language pairs
Medium confidence in exact augmentation implementation details

## Next Checks
1. Validate δhigh=0.4 margin assignment by testing with a complete confusion matrix from the development set to identify all confusing pairs
2. Verify entity replacement augmentation by implementing with a publicly available multilingual NER model and measuring entity diversity in embeddings
3. Confirm that 40% OOD training ratio is critical by ablation study comparing 0%, 20%, 40%, and 60% OOD proportions on both datasets