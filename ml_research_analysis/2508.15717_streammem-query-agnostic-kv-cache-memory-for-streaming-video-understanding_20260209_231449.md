---
ver: rpa2
title: 'StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding'
arxiv_id: '2508.15717'
source_url: https://arxiv.org/abs/2508.15717
tags:
- video
- arxiv
- cache
- visual
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient long video understanding
  under memory constraints in streaming settings, where the video length and downstream
  queries are unknown. The authors propose StreamMem, a training-free, query-agnostic
  key-value (KV) cache compression framework for streaming video understanding with
  multimodal large language models (MLLMs).
---

# StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding

## Quick Facts
- arXiv ID: 2508.15717
- Source URL: https://arxiv.org/abs/2508.15717
- Reference count: 9
- Primary result: State-of-the-art query-agnostic KV cache compression for streaming video understanding, competitive with query-aware methods

## Executive Summary
This paper introduces StreamMem, a training-free framework for compressing key-value (KV) caches in multimodal large language models (MLLMs) during streaming video understanding. The approach uses cross-attention scores between visual tokens and generic chat template tokens to identify and retain informative content while maintaining a fixed memory budget. StreamMem includes input frame filtering to reduce redundancy and frame-wise KV merging to create compact prototype representations. Evaluated on three offline and two streaming long video understanding benchmarks using LLaVA-OneVision-7B, Qwen2-VL-7B, and Qwen2.5-VL-3B, StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression.

## Method Summary
StreamMem is a training-free, query-agnostic KV cache compression framework that processes videos in 8-frame clips. It employs three main components: input frame filtering using cosine similarity to remove redundant frames (Î´=0.95), attention-based KV pruning using cross-attention scores between chat template tokens and visual tokens, and frame-wise weighted KV merging to create compact prototype representations. The method maintains a fixed memory budget across all layers using YaRN positional embedding extension with layer-specific scaling factors. The compressed KV memory is then used for efficient question answering without access to downstream queries during encoding.

## Key Results
- Achieves state-of-the-art performance in query-agnostic KV cache compression across five benchmarks
- Outperforms baselines like LiveVLM and InfiniPot-V across multiple tasks and KV cache sizes
- Demonstrates effectiveness in both offline and streaming scenarios with consistent accuracy improvements
- Ablation studies confirm contributions of each component including input filtering, merging strategies, and positional embedding extensions

## Why This Works (Mechanism)

### Mechanism 1: Chat Template Tokens as Proxy Queries for Saliency Detection
The core assumption is that visual tokens important for generic description overlap substantially with tokens relevant to downstream queries. The tokens `gpt` in the chat template serve as proxy queries to identify visually salient content. Cross-attention scores between these generic chat tokens and visual tokens reveal which visual information is most relevant for generating meaningful responses, even without access to specific downstream questions.

### Mechanism 2: Redundancy Reduction Through Frame Filtering and KV Merging
Input frame filtering removes visually similar frames using cosine similarity thresholds, eliminating temporal redundancy in the video stream. Frame-wise weighted KV merging combines information from multiple frames into prototype representations, creating compact yet informative visual summaries. These complementary approaches significantly reduce memory footprint while preserving essential visual content for downstream tasks.

## Foundational Learning
The method builds upon established MLLM architectures and KV cache compression techniques. It leverages cross-attention mechanisms for saliency detection, YaRN positional embedding extensions for memory management, and cosine similarity for frame redundancy detection. The framework extends these components to create a unified, training-free approach for streaming video understanding.

## Architecture Onboarding
StreamMem integrates with existing MLLM architectures through the KV cache compression interface. The framework processes visual tokens through standard cross-attention layers, applies compression techniques, and maintains compatibility with downstream question answering modules. The YaRN positional embedding extension ensures smooth integration across all model layers while preserving temporal relationships.

## Open Questions the Paper Calls Out
- The scalability of StreamMem to extremely long videos with hundreds or thousands of frames
- The generalizability of the approach to different MLLM architectures beyond LLaVA-OneVision and Qwen2-VL variants
- The impact of different chat template designs on compression effectiveness and downstream performance
- The potential for adaptive memory budgets that adjust based on video content complexity or task requirements

## Limitations
- The method assumes that chat template tokens adequately represent downstream query saliency, which may not hold for highly specialized or domain-specific questions
- The fixed memory budget approach may struggle with videos containing diverse content requiring different compression ratios
- The reliance on YaRN positional embedding extensions introduces additional complexity and potential training overhead
- The current implementation focuses on specific MLLM architectures, limiting immediate applicability to other model families

## Confidence
High confidence in the core contributions and methodology, based on comprehensive evaluations across multiple benchmarks and model architectures. The ablation studies and comparison with strong baselines support the effectiveness of the approach. However, some assumptions about chat template token relevance and the generalizability to different MLLM architectures warrant cautious interpretation.

## Next Checks
- Verify the exact implementation details of the YaRN positional embedding extension and its impact on downstream performance
- Examine the sensitivity of results to different chat template designs and their semantic coverage
- Investigate the computational overhead introduced by the compression framework during inference
- Assess the approach's performance on longer videos and more diverse video content domains