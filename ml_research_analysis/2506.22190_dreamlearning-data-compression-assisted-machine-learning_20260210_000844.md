---
ver: rpa2
title: 'dreaMLearning: Data Compression Assisted Machine Learning'
arxiv_id: '2506.22190'
source_url: https://arxiv.org/abs/2506.22190
tags:
- data
- training
- dreamlearning
- dataset
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dreaMLearning, a framework that enables direct
  training on compressed data without decompression. It is built upon EntroGeDe, an
  entropy-driven lossless compression method that clusters similar data points into
  condensed, representative samples.
---

# dreaMLearning: Data Compression Assisted Machine Learning

## Quick Facts
- arXiv ID: 2506.22190
- Source URL: https://arxiv.org/abs/2506.22190
- Reference count: 35
- One-line primary result: Enables direct training on compressed data, achieving up to 8.8× speedup and 10× memory reduction with minimal accuracy loss

## Executive Summary
dreaMLearning introduces a framework for training machine learning models directly on compressed data without decompression. Built on the EntroGeDe entropy-driven compression method, it clusters similar data points into condensed, representative samples. The framework demonstrates significant computational and memory efficiency gains across regression and classification tasks on both tabular and image datasets, with minimal impact on model performance. This approach offers practical benefits for resource-constrained environments, distributed learning, and tinyML applications.

## Method Summary
dreaMLearning is built upon EntroGeDe, an entropy-driven lossless compression method that clusters similar data points into condensed, representative samples. The framework enables direct model training on these compressed representations without decompression, eliminating the computational overhead typically associated with data expansion. The compression process groups data based on entropy considerations, creating a more efficient representation that preserves the essential information needed for learning while reducing storage and memory requirements.

## Key Results
- Accelerates training by up to 8.8× compared to uncompressed data
- Reduces memory usage by 10× during training
- Cuts storage requirements by 42% while maintaining minimal impact on model performance

## Why This Works (Mechanism)
The framework leverages entropy-driven compression to create condensed data representations that preserve essential information for learning while eliminating redundancy. By training directly on these compressed representations, the computational overhead of decompression is avoided, leading to significant speed and memory improvements. The clustering approach in EntroGeDe ensures that representative samples maintain the statistical properties necessary for effective model training.

## Foundational Learning
- **Entropy-driven compression**: Why needed - to identify and preserve information-rich data points while eliminating redundancy; Quick check - verify that compressed data maintains statistical properties of original
- **Lossless compression**: Why needed - to ensure no information loss that could degrade model performance; Quick check - confirm decompression yields identical original data
- **Direct training on compressed representations**: Why needed - to avoid computational overhead of decompression; Quick check - compare training outcomes on compressed vs. decompressed data
- **Data clustering for representation**: Why needed - to create condensed yet representative samples; Quick check - measure similarity between compressed and original data distributions
- **Resource-constrained learning**: Why needed - to enable ML on devices with limited memory/storage; Quick check - validate performance on embedded systems

## Architecture Onboarding

**Component Map**: Data -> EntroGeDe Compression -> Compressed Dataset -> Model Training -> Trained Model

**Critical Path**: The critical path involves the compression of data using EntroGeDe followed by direct training on the compressed representation. The speed and memory benefits are realized primarily during the training phase, where the compressed data eliminates the need for decompression overhead.

**Design Tradeoffs**: The framework trades minimal accuracy degradation for significant computational and memory efficiency gains. The compression ratio can be adjusted based on resource constraints and acceptable performance loss thresholds.

**Failure Signatures**: Potential failures include significant accuracy degradation if compression is too aggressive, incompatibility with certain data modalities, and possible bias introduction if the compression algorithm doesn't preserve representative sampling across all classes or features.

**First Experiments**:
1. Test dreaMLearning on larger-scale datasets (e.g., ImageNet, large tabular datasets) and more complex model architectures (e.g., transformers, GNNs)
2. Systematically quantify the trade-off between compression ratio and model performance across different data types and tasks
3. Evaluate the robustness of the compressed data under adversarial attacks and assess potential biases introduced by the compression process

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted only on relatively small datasets (MNIST, CIFAR-10, UCI Adult), with unclear scalability to larger datasets
- Compression method validated only for tabular and image data, with unclear applicability to text, time series, or high-dimensional modalities
- Limited testing with only a few model types (MLP, CNN, logistic regression), raising questions about generalizability to complex architectures

## Confidence
- Scalability to large-scale datasets: Medium
- Generalization to diverse data types: Medium
- Applicability to complex model architectures: Medium

## Next Checks
1. Test dreaMLearning on larger-scale datasets (e.g., ImageNet, large tabular datasets) and more complex model architectures (e.g., transformers, GNNs) to assess scalability and generalizability.
2. Systematically quantify the trade-off between compression ratio and model performance across different data types and tasks to establish practical limits.
3. Evaluate the robustness of the compressed data under adversarial attacks and assess potential biases introduced by the compression process.