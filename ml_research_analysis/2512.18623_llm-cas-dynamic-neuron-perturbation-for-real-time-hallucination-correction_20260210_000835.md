---
ver: rpa2
title: 'LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction'
arxiv_id: '2512.18623'
source_url: https://arxiv.org/abs/2512.18623
tags:
- arxiv
- dynamic
- wang
- policy
- llm-cas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-CAS addresses the hallucination problem in large language models
  by framing real-time correction as a hierarchical reinforcement learning task. Instead
  of static parameter editing, it trains an agent to dynamically select temporary
  neuron perturbations during inference based on the current context.
---

# LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction

## Quick Facts
- **arXiv ID**: 2512.18623
- **Source URL**: https://arxiv.org/abs/2512.18623
- **Authors**: Jensen Zhang; Ningyuan Liu; Yijia Fan; Zihao Huang; Qinglin Zeng; Kaitong Cai; Jian Wang; Keze Wang
- **Reference count**: 15
- **Primary result**: Achieves 10.98 percentage points improvement on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on TruthfulQA MC1 score through dynamic neuron perturbation

## Executive Summary
LLM-CAS introduces a novel approach to real-time hallucination correction in large language models by treating it as a hierarchical reinforcement learning problem. Rather than static parameter editing, the framework employs an agent that dynamically selects temporary neuron perturbations during inference based on contextual analysis. This adaptive masking approach, guided by a PPO-based policy, targets only the activations causing errors while balancing factuality, relevance, and fluency. The method demonstrates substantial improvements across multiple benchmarks while maintaining generalization capabilities across different model architectures.

## Method Summary
The framework addresses hallucination correction through a hierarchical reinforcement learning approach that operates during inference. It combines adaptive masking techniques with neuron-level causal tracing to identify and correct erroneous activations in real-time. The system trains an agent using Proximal Policy Optimization (PPO) to make context-aware decisions about which neuron perturbations to apply, focusing correction efforts only where factual errors are detected. This dynamic approach allows for temporary modifications without permanently altering model parameters, enabling flexible and targeted hallucination mitigation.

## Key Results
- **StoryCloze**: 10.98 percentage points improvement over baseline models
- **TriviaQA**: 2.71 points improvement in accuracy
- **TruthfulQA**: 2.06 points improvement in MC1 score
- Demonstrated strong generalization across different model architectures

## Why This Works (Mechanism)
The approach works by treating hallucination correction as an active decision-making process rather than a passive filtering mechanism. By leveraging hierarchical reinforcement learning, the system can adapt its correction strategy based on the specific context and type of hallucination detected. The neuron-level perturbation allows for precise targeting of erroneous activations without disrupting the broader semantic flow of the response. The PPO-based policy enables continuous learning and refinement of correction strategies through interaction with the model's output space.

## Foundational Learning
- **Reinforcement Learning (RL)**: Needed to enable adaptive decision-making during inference; quick check: verify PPO implementation matches standard formulations
- **Neural Network Architecture**: Essential for understanding neuron-level perturbation mechanics; quick check: confirm perturbation granularity matches model layer structure
- **Causal Tracing**: Required for identifying which neurons contribute to hallucinations; quick check: validate tracing accuracy on controlled synthetic errors
- **Proximal Policy Optimization**: Core algorithm for training the correction agent; quick check: monitor KL divergence during training for policy stability
- **Adaptive Masking**: Technique for selective perturbation application; quick check: measure masking efficiency across different error types

## Architecture Onboarding

**Component Map:**
Input Context -> Causal Tracing Module -> RL Agent -> Neuron Perturbation Layer -> Modified Output

**Critical Path:**
The most time-critical path runs from causal tracing through the RL agent decision to neuron perturbation application, as this must occur within inference latency constraints.

**Design Tradeoffs:**
The framework trades computational overhead during inference for improved factual accuracy. Permanent parameter editing would be more efficient but less flexible, while complete model retraining would be more accurate but far more expensive. The dynamic perturbation approach balances these competing requirements.

**Failure Signatures:**
Potential failures include: (1) Over-correction leading to overly conservative responses, (2) Under-correction where subtle hallucinations persist, (3) Latency spikes during complex inference scenarios, (4) Generalization failures on domain-specific content.

**First Experiments:**
1. Ablation study comparing full framework against versions without RL agent or without causal tracing
2. Latency benchmarking across different model sizes and hardware configurations
3. Robustness testing on out-of-distribution prompts and domain-specific datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on standard benchmark datasets without addressing domain-specific hallucination patterns in specialized fields like medicine or law
- Assumes hallucination patterns are detectable through contextual signals and neuron-level perturbations, potentially missing subtle factual errors
- Training requires access to both correct answers and faulty outputs, limiting real-world deployment scenarios

## Confidence

**High confidence**: The methodological framework combining hierarchical RL with neuron-level perturbations is internally consistent and the reported improvements on benchmark datasets are substantial and statistically significant

**Medium confidence**: The generalizability across different model architectures is supported but based on a limited range of tested models; performance in zero-shot or few-shot settings remains unclear

**Medium confidence**: The claim of "real-time" correction needs empirical validation regarding latency overhead, as neuron perturbation during inference could introduce computational bottlenecks

## Next Checks
1. **Cross-domain robustness test**: Evaluate LLM-CAS performance on domain-specific datasets (medical, legal, technical) to assess generalization beyond general knowledge benchmarks
2. **Computational overhead measurement**: Quantify the inference-time latency increase and resource requirements compared to baseline models across different hardware configurations
3. **Ablation study on perturbation granularity**: Test whether the reported improvements persist when reducing the granularity of neuron perturbations or when using alternative perturbation strategies (e.g., weight-based vs activation-based)