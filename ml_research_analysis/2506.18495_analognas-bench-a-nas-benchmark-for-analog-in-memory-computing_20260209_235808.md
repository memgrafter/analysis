---
ver: rpa2
title: 'AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing'
arxiv_id: '2506.18495'
source_url: https://arxiv.org/abs/2506.18495
tags:
- degree
- conv
- node
- architectures
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AnalogNAS-Bench, the first neural architecture
  search benchmark tailored for Analog In-Memory Computing (AIMC). By extending NAS-Bench-201
  with AIMC-specific noise simulations, the benchmark enables systematic evaluation
  of architectures under analog hardware constraints.
---

# AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing

## Quick Facts
- arXiv ID: 2506.18495
- Source URL: https://arxiv.org/abs/2506.18495
- Authors: Aniss Bessalah; Hatem Mohamed Abdelmoumen; Karima Benatchba; Hadjer Benmeziane
- Reference count: 17
- Primary result: Introduces first NAS benchmark for Analog In-Memory Computing (AIMC) with systematic noise evaluation

## Executive Summary
This paper introduces AnalogNAS-Bench, the first neural architecture search (NAS) benchmark specifically designed for Analog In-Memory Computing (AIMC) systems. The benchmark extends NAS-Bench-201 by incorporating AIMC-specific noise simulations, enabling researchers to evaluate architectures under realistic analog hardware constraints. The authors systematically analyze how different architectural choices affect robustness to AIMC noise, revealing that architectures optimized for digital performance differ significantly from those robust to analog imperfections. The benchmark provides a standardized platform for comparing analog-aware NAS methods and advancing research in hardware-efficient neural network design.

## Method Summary
The authors created AnalogNAS-Bench by extending the NAS-Bench-201 framework with comprehensive AIMC noise simulations. They implemented multiple noise sources including device-level variations, temporal drift, and quantization errors specific to analog crossbar arrays. The benchmark evaluates 15,625 neural network architectures across three datasets using standardized AIMC models that simulate resistance variations, read noise, and write disturbances. Performance metrics capture both accuracy degradation and energy efficiency under different noise conditions. The framework enables systematic comparison of architectures and provides insights into structural features that enhance robustness to analog imperfections.

## Key Results
- Standard quantization techniques fail to capture AIMC-specific noise effects, leading to inaccurate performance predictions
- Robust architectures favor wider and branched structures with more skip connections and 3Ã—3 convolutions
- Skip connections significantly improve resilience to temporal drift in analog computing systems

## Why This Works (Mechanism)
The benchmark works by providing a controlled environment where architectural choices can be systematically evaluated against realistic analog hardware constraints. By incorporating device-level noise models into the evaluation pipeline, researchers can identify structural patterns that inherently resist analog imperfections. The approach reveals that certain architectural features (like skip connections and wider layers) provide inherent robustness through redundancy and error averaging, which is particularly valuable in noisy analog computing environments where traditional digital optimization strategies fall short.

## Foundational Learning

**Analog In-Memory Computing**: Computing paradigm that performs matrix operations directly in memory using resistive crossbar arrays. Why needed: Understanding this is crucial as the benchmark evaluates architectures specifically for this hardware. Quick check: Can explain how matrix-vector multiplication works in a crossbar array.

**Neural Architecture Search (NAS)**: Automated process of discovering optimal neural network architectures. Why needed: The benchmark builds upon and extends existing NAS frameworks. Quick check: Can describe the difference between cell-based and layer-based search spaces.

**Quantization in AIMC**: Process of mapping neural network weights to discrete resistance levels in analog devices. Why needed: Central to understanding how digital architectures translate to analog implementations. Quick check: Can explain why standard quantization fails for AIMC noise.

**Temporal Drift**: Gradual degradation of analog device performance over time due to material fatigue. Why needed: Key noise source that the benchmark evaluates architectures against. Quick check: Can describe how drift affects crossbar array accuracy.

**Device-level Noise Models**: Statistical models representing variations in analog device behavior. Why needed: Essential for understanding the benchmark's evaluation methodology. Quick check: Can list the different types of noise modeled in AnalogNAS-Bench.

## Architecture Onboarding

Component map: Input -> Cell Stack -> Output
Critical path: Data flows through multiple cell types (normal and reduction) with skip connections enabling alternative paths
Design tradeoffs: Wider architectures provide robustness but increase area and power consumption
Failure signatures: Architectures with narrow layers and few skip connections show higher sensitivity to noise
First experiments: 1) Evaluate baseline accuracy of top-10 digital architectures on AIMC simulation, 2) Test skip connection impact by removing them from robust architectures, 3) Compare quantization-aware training with standard training under AIMC noise

## Open Questions the Paper Calls Out

None

## Limitations
- Noise simulations based on simplified device models may not capture all real-world AIMC imperfections
- Benchmark limited to CIFAR-10, restricting generalizability to larger-scale vision tasks
- Quantization robustness findings based on specific bit-widths (3-bit) may not extend to other precision regimes

## Confidence

High: Creating the first AIMC-specific NAS benchmark is well-supported and benchmark code is publicly available; observation that standard quantization fails for AIMC noise is empirically validated.

Medium: Architectural insights (wider structures, skip connections) are based on systematic experiments but may be influenced by specific noise model parameters.

Low: Temporal drift analysis is limited to a specific drift model and may not generalize to all AIMC hardware implementations.

## Next Checks

1) Validate the benchmark findings on alternative AIMC noise models that include non-linear device behavior and temperature effects.

2) Test the discovered robust architectures on larger-scale datasets (ImageNet) to verify scalability.

3) Evaluate the quantization robustness findings across multiple bit-widths (2-bit, 4-bit) to establish precision-dependent patterns.