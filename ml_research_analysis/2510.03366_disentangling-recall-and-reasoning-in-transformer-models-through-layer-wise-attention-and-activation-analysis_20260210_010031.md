---
ver: rpa2
title: Disentangling Recall and Reasoning in Transformer Models through Layer-wise
  Attention and Activation Analysis
arxiv_id: '2510.03366'
source_url: https://arxiv.org/abs/2510.03366
tags:
- reasoning
- recall
- layers
- activation
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether recall (fact retrieval) and reasoning
  (multi-step inference) in transformer models rely on distinct internal mechanisms.
  Using synthetic linguistic puzzles and counterfactual factual queries, the authors
  conduct causal intervention experiments across two model families (Qwen and LLaMA).
---

# Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis

## Quick Facts
- arXiv ID: 2510.03366
- Source URL: https://arxiv.org/abs/2510.03366
- Reference count: 5
- Primary result: First causal evidence that recall and reasoning circuits are separable but interacting in transformer models

## Executive Summary
This paper investigates whether recall (fact retrieval) and reasoning (multi-step inference) in transformer models rely on distinct internal mechanisms. Using synthetic linguistic puzzles and counterfactual factual queries, the authors conduct causal intervention experiments across two model families (Qwen and LLaMA). They analyze layer-wise attention, head-level patterns, and neuron activations to test for functional specialization. Results show that targeted interventions on "recall circuits" reduce fact-retrieval accuracy by up to 15% while preserving reasoning, and vice versa. At the neuron level, task-specific firing patterns are observed, though less robust due to polysemanticity. The study provides the first causal evidence of separable but interacting circuits for recall and reasoning in transformers, advancing mechanistic interpretability and offering insights for safer LLM deployment.

## Method Summary
The study uses controlled synthetic prompts based on country-capital-continent triples to distinguish recall (direct factual queries) from reasoning (two-step inference). The authors load Qwen2.5-7B-Instruct via nnsight with eager attention implementation, extract layer-wise hidden states, head-level attention metrics (entropy, concentration, focus, spread, Gini), and MLP gate activations for the final token. They apply statistical tests (Mann-Whitney U, Cohen's d) with multiple hypothesis corrections (FDR for layers/heads, Bonferroni for neurons) to identify task-specific patterns. Causal interventions disrupt recall or reasoning circuits to measure functional separation. The analysis includes 5-fold cross-validation with 80% consistency threshold for neuron-level findings.

## Key Results
- Causal interventions show recall and reasoning circuits can be selectively disrupted, reducing task-specific accuracy by up to 15% while preserving the other
- Layer-wise analysis reveals distinct activation patterns for recall vs. reasoning tasks with significant effect sizes (d > 1.0)
- Head-level attention patterns show task-specific specialization in focus, spread, and entropy metrics
- Neuron-level task-specific firing patterns exist but are limited by polysemanticity effects

## Why This Works (Mechanism)
The study leverages synthetic prompts to create controlled conditions where recall and reasoning tasks differ only in cognitive complexity, not surface form. By analyzing attention patterns and activation distributions layer by layer, the authors identify where task-specific computations emerge. Causal interventions provide direct evidence that disrupting identified circuits selectively impairs one task type while preserving the other, demonstrating functional specialization. The method combines statistical significance testing with effect size thresholds to ensure robust findings across multiple analysis levels.

## Foundational Learning
- **Attention Mechanisms**: Transformers use attention to weight input relevance; why needed: core to understanding information flow; quick check: verify attention tensor shapes match (layers × heads × seq × seq)
- **Layer-wise Processing**: Information transforms across transformer layers; why needed: identifies where task-specific computations emerge; quick check: confirm hidden state dimensions per layer
- **Statistical Testing with Multiple Corrections**: Mann-Whitney U test with FDR/Bonferroni corrections; why needed: controls false positives in high-dimensional analysis; quick check: verify corrected p-values and effect sizes meet thresholds
- **Causal Intervention**: Targeted disruption of model components; why needed: proves functional specialization rather than correlation; quick check: measure accuracy drop in target task vs. control task
- **Polysemanticity**: Neurons encoding multiple features; why needed: explains limitations of neuron-level analysis; quick check: examine neuron activation distributions for multiple modes

## Architecture Onboarding

**Component Map**: Prompts -> Transformer Layers (28) -> Attention Heads (32 per layer) -> MLP Gates -> Output

**Critical Path**: Input token → Layer 1 hidden states → Attention weights → MLP gate activations → Output logits

**Design Tradeoffs**: Synthetic prompts provide control but limit generalizability; statistical corrections ensure rigor but may be conservative; neuron-level analysis offers granularity but suffers from polysemanticity

**Failure Signatures**: Attention weights inaccessible (incorrect implementation flag), no significant neurons (activation saturation or wrong token position), cross-validation inconsistency (data leakage or small sample size)

**3 First Experiments**:
1. Generate 30 recall and 30 reasoning prompts using country-capital-continent triples, verify ground truth answers
2. Load Qwen2.5-7B-Instruct with nnsight, extract hidden states and attention weights for all layers
3. Run layer-wise statistical tests comparing recall vs. reasoning activation distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on synthetic prompts may not generalize to naturalistic language tasks
- Neuron-level analysis limited by transformer polysemanticity, making pure task-specificity difficult to establish
- Causal interventions cannot fully exclude indirect pathways or compensatory mechanisms

## Confidence
**High confidence**: Layer-wise and head-level specialization results showing distinct activation patterns for recall versus reasoning tasks. Causal intervention experiments demonstrating selective task impairment are methodologically robust.

**Medium confidence**: Neuron-level task-specificity findings. Statistical significance exists but practical significance is limited by polysemanticity effects.

**Low confidence**: Generalizability of findings to open-ended reasoning or knowledge-intensive tasks beyond the controlled synthetic prompts.

## Next Checks
1. Apply causal intervention methodology to a larger, more diverse knowledge base (200+ entity triples across multiple domains) to verify circuit separation beyond geography-specific prompts

2. Replicate layer-wise and head-level analysis on a different transformer family (LLaMA or Mistral) to determine if specialization patterns are model-specific or general properties

3. Conduct systematic analysis measuring neuron polysemanticity in identified task-specific populations using singular value decomposition of activation subspaces