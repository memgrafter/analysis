---
ver: rpa2
title: 'FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation
  of Reward Functions'
arxiv_id: '2504.10002'
source_url: https://arxiv.org/abs/2504.10002
tags:
- reward
- style
- adaptation
- human
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present FLoRA, a method for efficiently adapting pre-trained
  robotic reward functions to human preferences without catastrophic forgetting. The
  approach uses low-rank matrix adaptation (LoRA) to update only a small set of parameters
  while keeping the original reward model frozen, thus preserving baseline task performance.
---

# FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions

## Quick Facts
- arXiv ID: 2504.10002
- Source URL: https://arxiv.org/abs/2504.10002
- Reference count: 40
- Pre-trained reward functions can be efficiently adapted to human preferences while preserving baseline task performance

## Executive Summary
FLoRA introduces a novel approach for adapting pre-trained robotic reward functions to human preferences using Low-Rank Adaptation (LoRA). The method addresses the challenge of catastrophic forgetting by freezing the original reward model and only updating a small set of low-rank parameters. This enables efficient style adaptation across multiple preference dimensions while maintaining baseline task performance. The approach was validated through extensive experiments on both simulated control tasks and real-world robotic systems, demonstrating superior performance compared to traditional fine-tuning methods.

## Method Summary
FLoRA employs a two-stage training process where the original reward function is first trained on task-specific data and then frozen. During adaptation, LoRA parameters are introduced to capture style preferences through human feedback. The method uses a preference-conditioned linear layer that maps preference vectors to LoRA scaling factors, allowing multi-dimensional style adaptation. This architecture ensures that the base reward function remains unchanged while enabling efficient updates to accommodate new preferences. The approach leverages contrastive learning objectives during the initial training phase and incorporates human preferences through preference ranking tasks during adaptation.

## Key Results
- Achieves 19.46% higher combined reward than baseline methods on average
- Requires fewer than 100 human preferences for effective adaptation
- Outperforms fine-tuning and semi-supervised baselines across all tested environments
- Successfully preserves baseline task performance while adapting to new style preferences

## Why This Works (Mechanism)
The key mechanism behind FLoRA's effectiveness lies in its use of low-rank matrix adaptation to modify reward functions without overwriting the original parameters. By freezing the base reward model and only updating a small subset of parameters through LoRA, the method prevents catastrophic forgetting while enabling efficient adaptation to new preferences. The preference-conditioned linear layer allows the system to map multiple preference dimensions to corresponding LoRA parameters, enabling multi-style adaptation without interference between different preference modes.

## Foundational Learning
- **Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning technique that updates only a small subset of parameters while keeping most of the model frozen. Needed to prevent catastrophic forgetting while enabling adaptation.
- **Contrastive Learning**: A training approach that learns representations by comparing similar and dissimilar samples. Quick check: Verify that the reward model can distinguish between preferred and non-preferred trajectories.
- **Preference-based Learning**: A framework where human preferences are used to guide the learning process. Quick check: Ensure preference rankings are consistent and meaningful across different annotators.

## Architecture Onboarding

Component map: Human preferences -> Preference-conditioned layer -> LoRA parameters -> Adapted reward function -> RL policy

Critical path: The core workflow involves collecting human preference data, processing it through the preference-conditioned layer to generate LoRA parameters, and using these parameters to adapt the frozen reward function. The RL policy then optimizes actions based on the adapted reward function.

Design tradeoffs: The main tradeoff is between adaptation efficiency and expressiveness. While LoRA enables efficient updates with minimal parameters, it may limit the complexity of style preferences that can be captured compared to full fine-tuning.

Failure signatures: Potential failure modes include preference ambiguity leading to inconsistent LoRA updates, insufficient preference data causing poor adaptation, and interference between multiple preference dimensions when using the same LoRA parameters.

First experiments: 1) Validate baseline reward function performance on standard tasks. 2) Test single-dimension preference adaptation with synthetic preferences. 3) Evaluate multi-style adaptation with controlled preference variations.

## Open Questions the Paper Calls Out
The paper identifies several open questions including the scalability of the approach to more complex reward functions, the long-term stability of baseline performance preservation, and the method's robustness to noisy or inconsistent human preferences. Additional questions concern the optimal number of LoRA parameters needed for different types of style preferences and how to best handle conflicting preferences across different dimensions.

## Limitations
- Scalability to highly complex, high-dimensional reward functions remains uncertain
- Performance in domains with more complex reward structures or longer time horizons untested
- Long-term stability of baseline performance preservation not thoroughly investigated

## Confidence
- Efficient adaptation claim: High
- Preservation of baseline performance: High
- Fewer than 100 preferences needed: Medium

## Next Checks
1. Test FLoRA's performance on more complex, high-dimensional reward functions from domains like autonomous driving or industrial process control
2. Conduct long-term deployment studies to verify that baseline task performance remains stable over extended periods after style adaptation
3. Evaluate the method's robustness to noisy or inconsistent human preferences by introducing controlled variations in preference labeling during adaptation