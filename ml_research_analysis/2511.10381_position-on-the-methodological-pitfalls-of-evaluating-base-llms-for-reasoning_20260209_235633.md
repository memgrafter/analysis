---
ver: rpa2
title: 'Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning'
arxiv_id: '2511.10381'
source_url: https://arxiv.org/abs/2511.10381
tags:
- reasoning
- llms
- base
- zhang
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that base LLMs (pre-trained
  only) can be reliably evaluated for reasoning capabilities, since they are optimized
  for linguistic plausibility rather than correctness. The authors argue that evaluating
  base LLMs on reasoning tasks introduces a fundamental mismatch, as their outputs
  may be valid or invalid conclusions purely by chance, due to statistical patterns
  rather than genuine reasoning.
---

# Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning

## Quick Facts
- arXiv ID: 2511.10381
- Source URL: https://arxiv.org/abs/2511.10381
- Reference count: 31
- Key outcome: Base LLMs optimize for linguistic plausibility, not correctness, making them unreliable for reasoning evaluation; reasoning findings from base models cannot be generalized to instruct models.

## Executive Summary
This paper argues that evaluating base LLMs (pre-trained only) for reasoning capabilities introduces a fundamental methodological flaw. Base LLMs are optimized for next-token prediction on unlabeled corpora containing both correct and incorrect reasoning, learning statistical patterns of language continuation rather than normative alignment with logical validity. The authors demonstrate empirically that base models generate target strings for both valid and invalid logical forms at similar rates, suggesting outputs are coincidental byproducts of linguistic plausibility circuits rather than genuine reasoning. They conclude that reasoning assessments should focus on instruct LLMs, which are explicitly optimized for correctness, and that findings from base LLM reasoning studies cannot be generalized to instruct models.

## Method Summary
The authors conducted an empirical study using 13 base LLMs (0.5B–32B parameters) on valid and invalid logical forms derived from syllogistic reasoning templates. They generated 15,200 prompts using two templates: valid-form (modus ponens) and invalid-form (affirming the consequent), created by permuting 20 name pairs and 380 predicate pairs. Using greedy decoding, they measured the match rate of specific target strings (linguistically probable continuations based on name repetition) for both valid and invalid forms. The key finding was that models clustered near the top-right of a plot, generating target strings for both valid-form and invalid-form prompts at similar rates, supporting the claim that outputs are coincidental byproducts of linguistic pattern-matching rather than genuine reasoning.

## Key Results
- Base LLMs cluster near top-right in valid-form vs invalid-form match rate plots, generating target strings for both at similar rates
- Identical IOI+IH circuits produce valid conclusions for some syllogisms and invalid conclusions for others, demonstrating indifference to logical validity
- The "instruction-following confound" prevents distinguishing bona fide reasoning errors from outputs caused by objective misalignment
- Reasoning assessments using base LLMs introduce fundamental mismatch between training objectives and evaluation criteria

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Base LLMs optimize for linguistic plausibility, not correctness, creating fundamental mismatch when evaluating reasoning
- **Mechanism:** Pre-training optimizes next-token prediction on unlabeled corpora containing both correct and incorrect reasoning demonstrations without discrimination. The model learns statistical patterns of language continuation, not normative alignment with logical validity.
- **Core assumption:** Evaluation metrics should reflect a model's training objective to yield valid conclusions about its capabilities
- **Evidence anchors:**
  - [abstract] "fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed"
  - [section 4.2] "base LLMs are not trained for the purpose of producing outputs that conform to our standards for what counts as 'good' reasoning"
  - [corpus] Pitfalls in Evaluating Language Model Forecasters identifies similar evaluation-context misalignment issues
- **Break condition:** If evaluation metrics tested against objective-matched baselines show equivalent performance patterns, the mismatch claim weakens

### Mechanism 2
- **Claim:** Logically valid and invalid conclusions emerge as coincidental byproducts of circuits optimized solely for linguistic pattern-matching
- **Mechanism:** Circuits like Indirect Object Identification (IOI) and Induction Heads (IH) identify and copy tokens based on syntactic properties (prior occurrence, name position) without regard for semantic validity. These circuits generate target strings that may be valid or invalid depending on premise structure, not logical reasoning.
- **Core assumption:** The paper's case study generalizes beyond syllogistic templates to broader reasoning tasks
- **Evidence anchors:**
  - [abstract] "base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns"
  - [section 5.2-5.3] Detailed mechanistic walkthrough showing identical IOI+IH circuits produce valid conclusions for E1-E4 and invalid conclusions for E5-E6
  - [section 5.4, Figure 2] Empirical results across 13 base LLMs show models clustering top-right, generating target strings for both valid-form (x-axis) and invalid-form (y-axis) prompts at similar rates
  - [corpus] Weak direct corpus evidence on this specific mechanism; corpus papers focus on different evaluation pitfalls
- **Break condition:** If models show statistically divergent generation rates for valid-form vs invalid-form prompts, the "indifferent byproduct" claim would require revision

### Mechanism 3
- **Claim:** The "instruction-following confound"—uncertainty whether base LLMs attempt correct answers—invalidates conclusions about reasoning limitations or cognitive biases
- **Mechanism:** Without post-training, base LLMs may continue contexts with pedagogical incorrect answers, alternative question formulations, or other linguistically plausible but normatively misaligned outputs. Researchers cannot distinguish between "reasoning error" and "not following instructions."
- **Core assumption:** Instruct LLMs genuinely attempt correctness rather than approximating its appearance
- **Evidence anchors:**
  - [abstract] challenges assumption that "base LLMs' outputs can be assessed as their bona fide attempts at correct answers"
  - [section 6.1-6.2] describes the confound: "we cannot reliably distinguish bona fide reasoning errors from outputs caused by a mere misalignment in objective"
  - [section 4.3] Example of textbook containing "demonstrations of incorrect answers and flawed reasoning" for pedagogical purposes that base LLMs may reproduce
  - [corpus] The Refutability Gap paper raises parallel concerns about validation methodology for LLM reasoning claims
- **Break condition:** If controlled experiments show base LLM error patterns persist identically in instruct LLMs after controlling for instruction-following, the confound's impact would be smaller than claimed

## Foundational Learning

- **Concept:** Pre-training objective vs. normative evaluation alignment
  - **Why needed here:** The paper's central argument depends on understanding that next-token prediction on unlabeled data ≠ optimization for correct reasoning. Without this distinction, the "mismatch" argument collapses.
  - **Quick check question:** Given a model trained only to predict web text continuations, what justifies evaluating it on whether its answers are factually correct?

- **Concept:** Confounding variables in behavioral evaluation
  - **Why needed here:** Section 6 introduces "instruction-following confound" as a critical uncontrolled variable. Understanding confounds is necessary to see why observing an incorrect output doesn't prove a reasoning limitation.
  - **Quick check question:** If a model outputs "Paris" when asked for Germany's capital, what two distinct causes could explain this before concluding it "doesn't know geography"?

- **Concept:** Mechanistic interpretability through circuit analysis
  - **Why needed here:** The paper's empirical demonstration relies on IOI and IH circuits. Understanding what circuits are—and that they explain behavior mechanistically—is prerequisite to evaluating the "coincidental byproduct" claim.
  - **Quick check question:** What does an Induction Head circuit mechanistically compute, and why is this computation indifferent to logical validity?

## Architecture Onboarding

- **Component map:**
  Base LLM Pipeline: [Pre-training on unlabeled corpora] → [Next-token prediction objective] → [Linguistic plausibility only]
  
  Instruct LLM Pipeline: [Base LLM] → [Instruction-tuning on (instruction, response) pairs] → [Chat template formatting] → [Preference-tuning on (preferred, rejected) pairs] → [Normative optimization for correctness/helpfulness]
  
  Evaluation Layer: [Prompt] → [Model] → [Output extraction] → [Scoring against ground truth] ← CONFOUND: Is model attempting correctness?

- **Critical path:**
  1. Identify whether evaluation target is base or instruct LLM
  2. Verify evaluation metric aligns with model's optimization objective
  3. Control for instruction-following confound before attributing errors to reasoning limitations
  4. For mechanistic claims, distinguish circuit function (what it computes) from normative interpretation (whether outputs are "correct")

- **Design tradeoffs:**
  - Base LLM evaluation: Cleaner mechanistic signal but uncontrolled confound; findings may not generalize
  - Instruct LLM evaluation: Controlled confound through objective alignment but post-training obscures pre-training mechanisms
  - Hybrid approach: Evaluate both but explicitly separate claims about each; do not generalize across model types

- **Failure signatures:**
  - Claiming base LLM exhibits "reasoning bias" without controlling for instruction-following
  - Generalizing base LLM mechanistic findings (e.g., circuit analysis) to instruct LLMs
  - Interpreting output correctness as evidence of reasoning capability/limitation without objective alignment verification
  - Extracting tokens programmatically from base LLM outputs and scoring as "answers" (Section 6.1 identifies this as common pitfall)

- **First 3 experiments:**
  1. **Replication check:** Run paper's valid-form/invalid-form prompt experiment on additional model families not in original 13; verify top-right clustering pattern persists
  2. **Instruct comparison:** Run identical prompts on instruct versions of same base models; measure divergence in valid-form vs invalid-form response rates
  3. **Confounding control test:** Design prompts where "linguistically plausible continuation" diverges from "logically valid conclusion"; compare base vs instruct response patterns to quantify confound magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do reasoning circuits identified in base LLMs persist or change after instruction-tuning?
- Basis in paper: [explicit] The authors state it is "unwarranted to assume that the mechanisms by which a base LLM produces its outputs are the same ones it will use when it is re-optimized as an instruct LLM" (Section 6.3), directly challenging mechanistic interpretability studies that examine only base models.
- Why unresolved: The paper provides no empirical comparison of circuits between base and instruct versions of the same model family.
- What evidence would resolve it: Systematic activation patching experiments comparing identified reasoning circuits in base versus instruct-tuned versions of the same model across identical reasoning tasks.

### Open Question 2
- Question: How can the relative contributions of instruction-following ability versus genuine reasoning capability be disentangled when measuring post-training reasoning improvements?
- Basis in paper: [explicit] Section 7 states: "our position does draw attention to the challenge in disentangling how much of the reasoning improvement observed after post-training is attributable to genuine improvements in reasoning capability versus improvements in instruction-following."
- Why unresolved: The paper identifies this confound but proposes no methodology to separate these factors experimentally.
- What evidence would resolve it: Evaluation frameworks that independently measure instruction-following compliance and reasoning accuracy, potentially using adversarial prompts or controlled comparisons between models with matched instruction-following proficiency but differing reasoning exposure.

### Open Question 3
- Question: Can inference-time steering techniques adequately control for the instruction-following confound in base LLM reasoning evaluation?
- Basis in paper: [inferred] The authors acknowledge in Section 7 that base LLMs can be steered to follow instructions at inference time, but state "the onus remains on the researcher to ensure that the instruction-following confound is sufficiently controlled"—without specifying what constitutes sufficient control.
- Why unresolved: No validation criteria or benchmarks exist to verify when a base model's outputs can be treated as bona fide reasoning attempts.
- What evidence would resolve it: Empirical validation showing that steered base models produce reasoning patterns consistent with instruct-tuned models across diverse tasks, or the development of metrics quantifying instruction-following compliance during reasoning evaluation.

## Limitations
- The core claim about base LLMs generating reasoning outputs as "coincidental byproducts" rests on a single case study using syllogistic reasoning templates
- The corpus evidence supporting evaluation misalignment is indirect, citing papers on different evaluation pitfalls rather than direct validation of the instruction-following confound
- The target string extraction method assumes deterministic token identification that may not hold across all base model families

## Confidence
- **High confidence:** The fundamental mismatch between base LLM training objectives and normative reasoning evaluation (Mechanism 1)
- **Medium confidence:** The specific mechanistic claim that IOI+IH circuits produce valid/invalid conclusions indifferently (Mechanism 2)
- **Low confidence:** The generalizability claim that base LLM reasoning findings cannot be extended to instruct LLMs (section 7 conclusion)

## Next Checks
1. **Generalization test:** Replicate the valid-form/invalid-form experiment with non-syllogistic reasoning tasks (e.g., commonsense reasoning, arithmetic word problems) to verify the clustering pattern persists beyond the original template
2. **Confound quantification:** Measure the instruction-following confound magnitude by comparing base vs instruct LLM performance gaps across reasoning tasks with varying linguistic plausibility vs logical validity alignment
3. **Objective alignment validation:** Design evaluation metrics that explicitly reward objective-aligned behavior (e.g., correctness for instruct LLMs, linguistic coherence for base LLMs) and verify performance patterns match claimed capabilities