---
ver: rpa2
title: Improving Sustainability of Adversarial Examples in Class-Incremental Learning
arxiv_id: '2511.09088'
source_url: https://arxiv.org/abs/2511.09088
tags:
- learning
- classes
- semantics
- attack
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adversarial example (AE) sustainability
  in class-incremental learning (CIL), where traditional AEs fail due to domain drift
  across model updates. The authors propose SAE, which enhances AE semantics by making
  them similar to the target class while distinguishing them from all other classes.
---

# Improving Sustainability of Adversarial Examples in Class-Incremental Learning

## Quick Facts
- arXiv ID: 2511.09088
- Source URL: https://arxiv.org/abs/2511.09088
- Authors: Taifeng Liu; Xinjing Liu; Liangqiu Dong; Yang Liu; Yilong Yang; Zhuo Ma
- Reference count: 15
- Primary result: SAE achieves 31.28% average improvement in attack success rate against CIL models updated with 9-fold class increases

## Executive Summary
This paper addresses the critical problem of adversarial example (AE) sustainability in class-incremental learning (CIL), where traditional AEs fail due to domain drift across model updates. The authors propose SAE (Sustainable Adversarial Examples), which enhances AE semantics by making them similar to the target class while distinguishing them from all other classes. SAE significantly outperforms baseline attacks, achieving an average 31.28% improvement in attack success rate when CIL models are updated with a 9-fold increase in classes, demonstrating enhanced sustainability across various CIL methods and target classes.

## Method Summary
SAE generates sustainable adversarial examples for CIL by combining semantic alignment through a vision-language model (CLIP) with surrogate model gradients. The method uses a Public Out-of-Distribution (POOD) dataset, filters samples based on cosine similarity to the target class, applies augmentations, and optimizes perturbations using both CLIP's semantic directions and the initial CIL model's logits. The dual-objective loss function balances universal semantic alignment with task-specific optimization, while a filtering-and-augmentation module removes confusing examples and enhances semantic purity.

## Key Results
- SAE achieves 31.28% average improvement in attack success rate against CIL models updated with 9-fold class increases
- SAE shows superior performance across 9 different CIL methods including Finetune, Replay, DER, and iCaRL
- The method demonstrates enhanced sustainability across various target classes, with some achieving over 80% attack success rate

## Why This Works (Mechanism)
SAE works by creating perturbations that maintain semantic alignment with target classes even as CIL models evolve. The method leverages CLIP's shared image-text embedding space to establish universal semantic directions that remain stable across model updates. By filtering confusing examples and augmenting remaining samples, SAE ensures the optimization process focuses on high-quality semantic signals. The dual optimization objective (CLIP similarity + surrogate BCE loss) creates perturbations that are both semantically meaningful and task-specific, enabling successful attacks even after significant domain drift.

## Foundational Learning

- **Concept:** Class-Incremental Learning (CIL) and Catastrophic Forgetting
  - **Why needed here:** The entire premise of SAE is that models evolve, causing static adversarial examples to fail. You must understand that CIL updates models with new tasks/classes ($f_1 \to f_M$) and that this process causes "domain drift" where old class boundaries shift, degrading prior performance.
  - **Quick check question:** If a model learns 10 new classes without any replay or distillation, what likely happens to its accuracy on the first 10 classes it learned?

- **Concept:** Targeted Transferable Adversarial Attacks
  - **Why needed here:** The goal is to create a perturbation ($\delta$) on a source model (here $f_1$) that successfully forces a *different* model (here $f_i$, for $i > 1$) to misclassify inputs into a specific *target class*. Understanding the difference between untargeted (any wrong class) and targeted (a specific wrong class) is critical.
  - **Quick check question:** What is the core objective function for a targeted adversarial attack, and how does it differ from an untargeted one?

- **Concept:** Contrastive Learning / Semantic Alignment in Vision-Language Models (VLMs)
  - **Why needed here:** The primary mechanism uses a VLM (CLIP) to align the adversarial example's "semantic direction" with the target class text embedding. You need to grasp that CLIP maps images and text to a shared space, allowing optimization based on cosine similarity.
  - **Quick check question:** In a shared image-text embedding space, if you want to change an image of a "cat" to be classified as a "dog," which two vectors would you compute the cosine similarity between to guide this change?

## Architecture Onboarding

- **Component map:** Data Ingestion -> Filtering (using $f_1$ embeddings) -> Augmentation -> Optimization Loop (CLIP similarity + $f_1$ BCE Loss -> Gradient Update on $\delta$) -> Clamping $\delta$

- **Critical path:** POOD dataset -> Filtering based on cosine similarity to target class -> Augmentation (rotation, scaling, etc.) -> Dual optimization with CLIP and surrogate model -> Perturbation output

- **Design tradeoffs:**
  - **POOD vs. CIL Training Data:** Using POOD is more realistic but may provide weaker signals than using real training data
  - **Filtering Threshold ($\sigma$):** A higher threshold (e.g., 0.7) keeps more data but might let in confusing samples
  - **Loss Balancing:** Relying too much on $f_1$ ($L_{Surr}$) causes overfitting to the initial model; relying too much on CLIP ($L_{CLIP}$) might ignore the specific architecture of the CIL model

- **Failure signatures:**
  - **Low SASR on Finetune-only CIL:** Severe catastrophic forgetting in the target CIL method causes the target class representation to collapse, making any attack fail
  - **High Variance in ASR:** The filtering threshold is not aggressive enough, allowing noisy semantics to destabilize the perturbation
  - **Attack Success on $f_1$ but not $f_i$:** Overfitting to the surrogate model; the CLIP loss ($L_{CLIP}$) weight is likely too low

- **First 3 experiments:**
  1. **Ablation on Modules:** Run SAE with only the CLIP loss, only the $f_1$ loss, and then both combined to quantify the contribution of universal semantics vs. surrogate gradients
  2. **Sensitivity to Filtering ($\sigma$):** Sweep $\sigma$ (e.g., 0.5, 0.6, 0.7, 0.8) and plot SASR against the number of filtered samples to find the optimal balance between data quantity and semantic purity
  3. **Generalization to Different CIL Methods:** Generate $\delta$ using $f_1$ from one CIL method (e.g., Replay) and test it on $f_i$ from a *different* CIL method (e.g., DER or iCaRL) to test the true "sustainability" of the semantic alignment

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis and results, several important questions emerge:

- Can the semantic generalization strategy used in SAE be effectively adapted for untargeted adversarial attacks in a Class-Incremental Learning (CIL) setting?
- How can the Filtering-and-Augmentation Module be improved to handle target classes with inherently high semantic ambiguity or overlap with non-target classes?
- Does the application of adversarial training during the incremental learning process specifically degrade the sustainability of SAE's semantic correction over time?

## Limitations
- The method relies heavily on the quality and diversity of the Public Out-of-Distribution (POOD) dataset, with potential biases or domain gaps not adequately addressed
- The filtering threshold of 0.7 for cosine similarity is presented as optimal without systematic sensitivity analysis across different target classes or datasets
- Computational overhead of maintaining and optimizing perturbations across multiple CIL tasks is not thoroughly analyzed for resource-constrained deployment scenarios

## Confidence
- **High Confidence:** The empirical demonstration that traditional adversarial examples fail in CIL settings due to domain drift is well-supported by the presented baseline comparisons
- **Medium Confidence:** The claim that SAE achieves 31.28% average improvement in attack success rate is credible based on the experimental results
- **Low Confidence:** The assertion that SAE's semantic alignment approach is universally superior across all CIL methods and target classes is overstated

## Next Checks
1. **Cross-Dataset Semantic Robustness:** Test SAE-generated perturbations on CIL tasks using completely different datasets (e.g., CIFAR-100 → Tiny-ImageNet) to verify semantic alignment generalizes beyond dataset-specific features

2. **Dynamic Threshold Optimization:** Implement an adaptive filtering mechanism that adjusts the cosine similarity threshold (σ) based on the diversity and quality of available POOD samples, rather than using a fixed value of 0.7

3. **Efficiency Benchmarking:** Measure and compare the computational overhead of SAE (including CLIP embedding computations and iterative optimization) against baseline methods across varying numbers of CIL tasks and model scales to assess practical deployment viability