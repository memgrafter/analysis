---
ver: rpa2
title: 'FOCUS: DLLMs Know How to Tame Their Compute Bound'
arxiv_id: '2601.23278'
source_url: https://arxiv.org/abs/2601.23278
tags:
- focus
- tokens
- token
- decodable
- decoded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational inefficiency of Diffusion
  Large Language Models (DLLMs), where only a small fraction of tokens are decoded
  per diffusion step while the full block is computed, resulting in massive arithmetic
  redundancy. FOCUS tackles this by leveraging the observation that token importance
  scores in early layers strongly correlate with decoding probability.
---

# FOCUS: DLLMs Know How to Tame Their Compute Bound

## Quick Facts
- **arXiv ID**: 2601.23278
- **Source URL**: https://arxiv.org/abs/2601.23278
- **Reference count**: 40
- **Primary result**: Achieves up to 3.52× throughput improvement over LMDeploy by dynamically evicting non-decodable tokens

## Executive Summary
The paper addresses the computational inefficiency of Diffusion Large Language Models (DLLMs), where only a small fraction of tokens are decoded per diffusion step while the full block is computed, resulting in massive arithmetic redundancy. FOCUS tackles this by leveraging the observation that token importance scores in early layers strongly correlate with decoding probability. It dynamically evicts non-decodable tokens using a training-free importance delta metric, thereby reducing FLOPs and increasing the effective batch size. Evaluations show FOCUS achieves up to 3.52× throughput improvement over LMDeploy, with negligible overhead and preserved or improved generation quality across multiple benchmarks.

## Method Summary
FOCUS introduces a dynamic token eviction mechanism for DLLMs that exploits the correlation between token importance scores in early layers and decoding probability. The method computes an importance delta metric to identify and evict tokens unlikely to be decoded in the current step, reducing computational waste. This approach is training-free and operates at inference time, maintaining generation quality while significantly improving throughput. The system increases effective batch size by keeping more tokens in the active set and reducing redundant computations.

## Key Results
- Achieves up to 3.52× throughput improvement over LMDeploy baseline
- Maintains or improves generation quality across multiple benchmarks
- Reduces FLOPs through dynamic token eviction with negligible overhead

## Why This Works (Mechanism)
FOCUS works by recognizing that in DLLMs, the computational cost of processing all tokens in a block far exceeds the actual decoding requirements. Since only a subset of tokens gets decoded per diffusion step, computing the full block represents significant arithmetic redundancy. By using token importance scores from early layers as predictors of decoding probability, FOCUS can dynamically identify and evict tokens that won't be decoded, thereby reducing unnecessary computations. This approach effectively increases the batch size by keeping more tokens active while reducing the per-token computational burden.

## Foundational Learning

**Diffusion Models in Language Generation**
- Why needed: Understanding the diffusion process is crucial for grasping why tokens are processed in blocks and how the decoding probability varies across steps
- Quick check: Can you explain the forward and reverse diffusion processes in LLMs?

**Token Importance Scoring**
- Why needed: The method relies on early-layer importance scores to predict decoding probability
- Quick check: How do token importance scores differ from attention weights in transformer models?

**Computational Complexity in Neural Networks**
- Why needed: Understanding FLOPs and arithmetic redundancy helps quantify the efficiency gains
- Quick check: What is the computational difference between processing a full block versus only decoded tokens?

## Architecture Onboarding

**Component Map**: Input tokens -> Early layer importance scoring -> Importance delta computation -> Token eviction decision -> Reduced computation block -> Diffusion step processing

**Critical Path**: The most critical components are the early layer importance scoring and the importance delta computation, as these directly determine which tokens get evicted and thus affect both efficiency and quality.

**Design Tradeoffs**: The main tradeoff is between eviction aggressiveness and generation quality. More aggressive eviction yields higher efficiency but risks removing tokens that might have been decoded. The training-free approach trades potential optimality for simplicity and generalizability.

**Failure Signatures**: If eviction is too aggressive, generation quality will degrade with increased repetition or incoherence. If too conservative, efficiency gains will be minimal. Incorrect importance scoring can lead to premature eviction of important tokens.

**First Experiments**:
1. Measure correlation between early-layer importance scores and actual decoding probability across different diffusion steps
2. Benchmark throughput improvement at varying eviction thresholds
3. Evaluate generation quality degradation as a function of eviction rate

## Open Questions the Paper Calls Out
None

## Limitations
- The training-free importance delta metric may not generalize equally well across all DLLM architectures or domains
- The study focuses primarily on throughput and generation quality, with limited analysis of potential impacts on model calibration
- Long-range coherence and factual consistency in generated text are not extensively evaluated

## Confidence
- **High confidence**: FLOPs reduction and throughput improvement claims are well-supported by empirical results
- **Medium confidence**: Generation quality preservation is validated but qualitative analysis is limited
- **Low confidence**: Negligible overhead claim could benefit from more granular breakdown across hardware configurations

## Next Checks
1. Conduct ablation studies varying the eviction threshold to quantify the trade-off between efficiency gains and generation quality degradation
2. Evaluate FOCUS on diverse DLLM architectures beyond the ones tested to assess generalizability
3. Perform long-form generation tasks to analyze potential impacts on coherence and factual consistency over extended outputs