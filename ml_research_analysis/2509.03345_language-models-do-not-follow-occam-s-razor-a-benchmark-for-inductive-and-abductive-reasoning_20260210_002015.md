---
ver: rpa2
title: 'Language Models Do Not Follow Occam''s Razor: A Benchmark for Inductive and
  Abductive Reasoning'
arxiv_id: '2509.03345'
source_url: https://arxiv.org/abs/2509.03345
tags:
- reasoning
- each
- hypotheses
- observations
- dalpist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work focuses on evaluating LLMs' inductive and abductive reasoning
  capabilities, which are essential for solving real-world problems but less explored
  than deductive reasoning. The authors introduce a programmable and synthetic dataset,
  INABHYD, where each reasoning example consists of an incomplete world model and
  observations.
---

# Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning

## Quick Facts
- arXiv ID: 2509.03345
- Source URL: https://arxiv.org/abs/2509.03345
- Authors: Yunxin Sun; Abulhair Saparov
- Reference count: 37
- LLMs struggle with inductive/abductive reasoning, producing valid but low-quality hypotheses even with reasoning-enhancing techniques.

## Executive Summary
This work evaluates LLMs on inductive and abductive reasoning tasks, which are essential for real-world problem-solving but less studied than deductive reasoning. The authors introduce INABHYD, a synthetic dataset where models must generate hypotheses explaining observations given an incomplete world model. A new parsimony-based quality metric reveals that while models can handle simple scenarios, they struggle with complex ontologies and multiple hypotheses, often producing trivial or redundant explanations. The study concludes that LLMs do not yet match human-level reasoning capabilities in these domains.

## Method Summary
The authors created INABHYD, a synthetic dataset using first-order logic ontology trees with configurable height and branching. Each example includes an incomplete world model (with hidden axioms) and observations generated from hidden axioms. Models must produce hypotheses explaining the observations. A parsimony-based quality metric (q(H)) evaluates hypothesis quality by comparing explanatory power to ground truth. Five state-of-the-art LLMs were tested using zero-shot and few-shot prompting across various ontology complexities.

## Key Results
- Models perform well on simple scenarios (shallow trees, single hypotheses) but struggle with complex ontologies and multiple hypotheses.
- Even with RLVR and in-context learning, hypothesis quality remains notably lower than ground truth.
- Models frequently produce trivial hypotheses (reusing observations) or ignore ontology hierarchy, resulting in redundant explanations.
- Verification traces in RLVR models improve performance through deductive checking, not just better generation.

## Why This Works (Mechanism)

### Mechanism 1: First-Order Logic Ontology Trees as Controllable Reasoning Scaffolds
Structured ontology trees with hidden axioms create a well-defined hypothesis space where reasoning difficulty scales predictably with tree height. Ground truth hypotheses are created by masking axioms, and observations are generated from hidden axioms' consequences, ensuring underdetermination while maintaining solvability.

### Mechanism 2: Hypothesis Quality via Parsimony Scoring
Quality is quantified through a parsimony metric that reveals LLMs often produce valid but low-quality hypotheses. The score compares explanatory power per hypothesis to ground truth, with simpler explanations scoring higher. This metric distinguishes between valid reasoning and scientifically valuable reasoning.

### Mechanism 3: RLVR Improves via Self-Verification of Hypotheses
RLVR models improve not just through better generation but by engaging in deductive verification after hypothesis generation. Models produce reasoning traces that include verification steps, essentially checking whether observations follow from hypotheses plus the world model.

## Foundational Learning

- **Inductive vs. Abductive vs. Deductive Reasoning (Peirce's Triad)**: The paper explicitly targets non-deductive reasoning. Without distinguishing these, you cannot interpret why models fail at hypothesis generation but succeed at verification. Quick check: Given "All swans are white" and "This bird is a swan," what type of reasoning yields "This bird is white"? What if you only have observations of white swans and must infer the rule?

- **Occam's Razor / Parsimony Principle**: The quality metric formalizes parsimony. You must understand why simpler explanations are preferred to interpret q(H) scores and failure modes. Quick check: Given observations A, B, C are all P, which hypothesis set is more parsimonious: {All X are P} or {A is P, B is P, C is P}?

- **First-Order Logic: Predicates, Quantifiers, and Axioms**: The dataset encodes world models as FOL axioms. Reading proofs and generating observations requires understanding ∀x(cat(x) → cute(x)) notation. Quick check: Translate "Every rompus is a dalpist" and "Jerry is a rompus" into FOL. What does the proof tree for "Jerry is a dalpist" look like?

## Architecture Onboarding

- **Component map**: Ontology Generator -> Axiom Masker -> Observation Generator -> Natural Language Translator -> LLM Evaluator -> Quality Scorer

- **Critical path**: Generate ontology tree → Mask axioms → Generate observations → Translate to natural language → LLM generates hypotheses → Parse hypotheses → Compute accuracy and quality

- **Design tradeoffs**: FOL limits expressiveness but enables automated proof tree generation and quality scoring. Fictional concept names prevent training data contamination but may reduce ecological validity. Fixed observation count ensures task solvability but may not reflect real-world underdetermination.

- **Failure signatures**: Wrong ontology direction, unnecessary hypotheses that ignore hierarchy, trivial hypotheses reusing observations, hallucinated entities, and member/concept confusion.

- **First 3 experiments**: (1) Single-hypothesis baseline across tree heights 1-4, (2) In-distribution vs. OOD in-context learning comparison, (3) RLVR ablation via verification trace removal

## Open Questions the Paper Calls Out

- Can fine-tuning LLMs on INABHYD improve their performance on real-world tasks requiring inductive and abductive reasoning? The current study only evaluates pre-trained models; no training experiments were conducted.

- Can reinforcement learning with a reward function that explicitly encodes parsimony principles significantly improve hypothesis quality? The paper tests existing RLVR models but does not train a model specifically optimized for the proposed quality metric.

- How does LLM performance change when world models are expressed in higher-order logic rather than first-order logic? The current dataset is restricted to FOL-expressible structures, with HOL proposed as future work.

## Limitations

- Evaluation is limited to synthetic FOL worlds and may not generalize to real-world scientific discovery or everyday reasoning.
- The absence of human baseline performance on INABHYD makes it unclear whether LLM failures reflect genuine reasoning deficits or dataset design issues.
- Fixed observation count (≈3) may under-represent real-world underdetermination levels.

## Confidence

- **High confidence**: Dataset construction mechanism, quality metric definition, and observed failure modes are well-documented and reproducible.
- **Medium confidence**: The claim that RLVR improves via verification traces is plausible but not definitively proven—verification could be post-hoc rationalization.
- **Medium confidence**: The conclusion that LLMs "do not follow Occam's Razor" is supported by quality scores but conflates parsimony with general reasoning ability without human validation.

## Next Checks

1. **Human baseline evaluation**: Have human participants solve 50 INABHYD examples and compare their q(H) scores to LLM outputs to establish whether the metric captures human-quality reasoning.

2. **Verification trace ablation study**: For DeepSeek-R1 outputs, strip verification traces and re-score. Measure performance drop to quantify how much improvement comes from verification vs. generation.

3. **Scrambling test**: Randomly permute observation order in prompts and re-evaluate models. If performance drops significantly, it suggests models exploit linguistic patterns rather than reasoning over logical structure.