---
ver: rpa2
title: 'From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action
  Models'
arxiv_id: '2506.09930'
source_url: https://arxiv.org/abs/2506.09930
tags:
- plate
- widowx
- finetune
- delta
- carrot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INT-ACT, a comprehensive simulation-based
  benchmark suite of 50 tasks designed to probe the generalization boundaries of Vision-Language-Action
  (VLA) models. The suite systematically varies object diversity, language complexity,
  and vision-language reasoning challenges.
---

# From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models

## Quick Facts
- arXiv ID: 2506.09930
- Source URL: https://arxiv.org/abs/2506.09930
- Authors: Irving Fang; Juexiao Zhang; Shengbang Tong; Chen Feng
- Reference count: 40
- Key outcome: INT-ACT benchmark reveals significant "intention-action gap" in VLA models, with high-level planning intact but execution failing under distributional shifts.

## Executive Summary
This paper introduces INT-ACT, a comprehensive simulation-based benchmark suite of 50 tasks designed to systematically probe the generalization boundaries of Vision-Language-Action (VLA) models. The benchmark varies object diversity, language complexity, and visual reasoning challenges to evaluate how well models can translate high-level intentions into precise motor execution. Extensive evaluation reveals that while VLA models inherit strong perceptual understanding and planning capabilities from their VLM backbones, this "intention" does not reliably translate into successful task execution, particularly under out-of-distribution conditions. The work also demonstrates that fine-tuning on robotics data can erode the original VLM's generalist reasoning abilities, suggesting a fundamental tension between preserving semantic generalization and acquiring task-specific motor skills.

## Method Summary
The INT-ACT benchmark evaluates VLA models on 50 tasks across 10 categories in a simulation environment (SimplerEnv/ManiSkill2). Models are trained on the BridgeV2 dataset and tested on both in-distribution and out-of-distribution objects from LIBERO and Fractal datasets. The evaluation uses three key metrics: Grasp Success Rate (gripper successfully grasps target), Intention Correct Rate (gripper enters 5cm radius of target object), and Task Success Rate. The benchmark systematically varies object diversity (OOD objects), language complexity (action, negation, appearance, commonsense), and vision-language reasoning challenges (visual distractors, compound ambiguity). Models evaluated include π0 (trained from scratch or fine-tuned), SpatialVLA, Magma (zero-shot), and Octo (zero-shot).

## Key Results
- VLA models show high intention correctness (80-100%) but significantly lower task success rates, revealing an "intention-action gap" under distributional shifts.
- Fine-tuning on robotics data erodes VLM generalist reasoning abilities, particularly under language variations and multimodal ambiguity.
- Multimodal ambiguity (compounding visual distractors with language commonsense variations) causes systematic binding failures, with wrong-object attempts surging under compound distractors.
- Models struggle with OOD object generalization, showing performance drops when both source and target objects are out-of-distribution simultaneously.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLM backbones provide generalizable semantic understanding ("intention") that does not reliably transfer to low-level motor execution.
- Mechanism: The VLM encodes scene semantics and instruction meaning, outputting high-level plans or target representations. However, end-to-end VLA training couples perception directly to action tokens or flow-matching outputs without intermediate representations that can survive distribution shift, creating a brittle perceptuomotor mapping.
- Core assumption: The gap reflects architectural coupling rather than insufficient training data or action representation capacity.
- Evidence anchors:
  - [abstract] "...while VLM backbones endow VLAs with robust perceptual understanding and high-level planning... this does not reliably translate into precise motor execution"
  - [section 4.2] "Intention-Action Gap... while most VLM-initialized VLAs achieve near-perfect Intention Correctness (80−100%)... their Task Success Rates drop drastically"
  - [corpus] Actions as Language (arXiv:2509.22195) documents catastrophic forgetting of VLM reasoning when fine-tuned for actions

### Mechanism 2
- Claim: End-to-end VLA fine-tuning erodes the linguistic generalization present in pretrained VLMs.
- Mechanism: Action data introduces a new output modality and loss landscape; backpropagation through the VLM backbone modifies representations to optimize for action prediction, degrading language-only reasoning. This is a form of representational interference.
- Core assumption: The erosion is not simply due to limited language diversity in robotics datasets but from gradient-driven representational drift.
- Evidence anchors:
  - [abstract] "finetuning on action data can erode the original VLM's generalist reasoning abilities"
  - [section 4.2, Table 2] All models show performance drops under language variations; π0 finetune drops 24% on negation success
  - [corpus] MAPS (arXiv:2511.19878) addresses this via module-wise proximity scheduling to preserve VLM representations during VLA training

### Mechanism 3
- Claim: Multimodal ambiguity—compounding visual distractors with language commonsense variations—causes systematic binding failures.
- Mechanism: When visual and language OOD factors combine, linguistic priors override visual grounding. The model defaults to language-conditional heuristics rather than grounding the specific visual referent, leading to wrong-object attempts.
- Core assumption: The failure is due to lack of cross-modal attention sharpening rather than insufficient capacity.
- Evidence anchors:
  - [section 4.2] "when the toy bunny is added... together with the language commonsense factor 'rabbit's favorite vegetables,' the wrong object attempt surges"
  - [section 4.2] Orange juice task with orange distractor "completely breaks the models, yielding high wrong object attempt rates"
  - [corpus] No direct corpus match for multimodal binding in VLAs; weak external evidence

## Foundational Learning

- Concept: VLM (Vision-Language Model) backbone
  - Why needed here: VLAs inherit pretrained VLMs (e.g., PaliGemma); understanding what VLMs provide (semantic grounding, instruction following) is prerequisite to diagnosing what is lost during VLA training.
  - Quick check question: Can you explain why a VLM trained on web-scale image-text pairs might generalize to novel object descriptions but not to motor commands?

- Concept: Intention vs. execution decomposition
  - Why needed here: INT-ACT introduces separate metrics for intention correctness and task success; interpreting the gap requires understanding that perception and action can be evaluated independently.
  - Quick check question: If a policy correctly identifies the target object 95% of the time but completes the task 30%, where is the bottleneck?

- Concept: Distribution shift in robotics
  - Why needed here: The benchmark systematically probes OOD generalization (objects, language, visual distractors); foundational understanding of covariate shift vs. concept shift is necessary to interpret results.
  - Quick check question: Why might a model generalize to an unseen object (carrot → coke can) but fail when both source and target are OOD simultaneously?

## Architecture Onboarding

- Component map: RGB observation + instruction → VLM encoder → joint embedding → action head → action sequence
- Critical path:
  1. Observation + instruction → VLM encoder → joint embedding
  2. Embedding → action head → action sequence
  3. Loss computed on action prediction; gradients flow back through VLM (unless frozen)
- Design tradeoffs:
  - Frozen VLM vs. fine-tuned: Frozen preserves linguistic generalization; fine-tuned adapts better to in-domain tasks
  - Autoregressive vs. flow-matching: Autoregressive is simpler but may have compounding error; flow-matching is smoother but architecturally complex
  - Co-training with VL data (Magma) preserves reasoning but requires careful balancing
- Failure signatures:
  - High intention correctness + low task success: Intention-Action Gap; VLM is correct, action head is brittle
  - Performance collapse under language variation: Linguistic erosion; VLM representations have shifted
  - Surge in wrong-object attempts with distractors: Multimodal binding failure; attention not grounded
- First 3 experiments:
  1. Evaluate π0-finetune vs. π0-scratch on original SimplerEnv tasks to establish baseline performance gap and confirm intention-action decomposition.
  2. Run language variation probes (action, negation, appearance) on all models; log intention vs. success delta to quantify erosion.
  3. Introduce compound distractors (commonsense + visual) on a single task (e.g., carrot on plate) and measure wrong-object attempt rate to validate binding failure mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can employing newer VLM backbones with superior visual grounding preserve reasoning capabilities during VLA fine-tuning?
- Basis in paper: [explicit] The authors note that PaliGemma makes mistakes and suggest "changing it to a newer and stronger VLM, especially ones with better visual grounding, could mitigate the problem... We leave this investigation as a future work."
- Why unresolved: The study primarily relies on PaliGemma (for π0) and other current models; it does not test if more advanced or robust VLMs resist the observed erosion of linguistic capabilities.
- What evidence would resolve it: Evaluating VLA models constructed on next-generation VLM backbones using the INT-ACT suite to measure retention of intention correctness under language variations.

### Open Question 2
- Question: Do the simulation-based generalization trends identified in INT-ACT, particularly the Intention-Action Gap, correlate with real-world robot performance?
- Basis in paper: [explicit] The authors acknowledge the limitation: "INT-ACT is developed from SimplerEnv so it is fully in simulation... extending the INT-ACT to real-world is still a worthwhile future step."
- Why unresolved: While SimplerEnv is designed to match real-world performance, the specific generalization failures regarding object diversity and language complexity have not been validated on physical hardware.
- What evidence would resolve it: Deploying the evaluated VLA models on physical robots to perform the same 50 tasks and comparing the "Intention-Action Gap" magnitude between sim and real.

### Open Question 3
- Question: Can architectural designs that freeze the VLM backbone close the perception-to-action gap without eroding linguistic generalization?
- Basis in paper: [inferred] The conclusion highlights the need for "improved architectural designs to better leverage VLMs' generalization abilities" and suggests insights from "freezing VLMs in image generation... could prove beneficial," contrasting with current end-to-end fine-tuning.
- Why unresolved: Current state-of-the-art VLA models fine-tune the VLM, leading to the observed degradation; non-destructive adaptation methods are not benchmarked in the paper.
- What evidence would resolve it: A comparative study on INT-ACT between standard fine-tuned VLAs and modular architectures that separate visual grounding from low-level control.

## Limitations

- The benchmark is simulation-based and may not fully capture real-world physical constraints and sensor noise.
- The analysis of representational erosion during fine-tuning is based on performance metrics rather than direct examination of VLM backbone representations.
- The multimodal ambiguity mechanism is supported by qualitative observations but lacks quantitative analysis of attention patterns or grounding failures.
- The benchmark focuses on pick-and-place tasks with specific object sets, limiting generalizability to other manipulation tasks or robot morphologies.

## Confidence

- **High Confidence:** The empirical demonstration of the intention-action gap (high intention correctness but low task success) is robustly supported by the evaluation results across multiple VLA models and task variations.
- **Medium Confidence:** The claim that fine-tuning erodes VLM generalist reasoning is supported by performance drops under language variations, but the underlying representational changes are inferred rather than directly measured.
- **Medium Confidence:** The mechanism of multimodal binding failure under compound visual and language distractors is plausible given the qualitative patterns, but lacks quantitative attention analysis or ablation studies.

## Next Checks

1. **Representation Analysis Validation:** Extract and compare VLM backbone representations (e.g., using linear probes) from pre-trained and fine-tuned models on held-out V+L and language-only tasks to directly measure representational drift and confirm erosion of generalist reasoning.

2. **Attention Mechanism Validation:** For models with attention mechanisms (e.g., Magma), visualize cross-modal attention maps during compound ambiguity tasks to quantify whether linguistic priors systematically override visual grounding, providing mechanistic evidence for the multimodal binding failure.

3. **Real-World Transfer Validation:** Select 3-5 INT-ACT tasks and replicate them on a physical robot system with real sensor noise and physical constraints to assess whether the intention-action gap persists outside simulation and quantify the sim-to-real gap.