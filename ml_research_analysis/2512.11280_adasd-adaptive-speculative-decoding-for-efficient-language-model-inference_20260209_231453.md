---
ver: rpa2
title: 'AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference'
arxiv_id: '2512.11280'
source_url: https://arxiv.org/abs/2512.11280
tags:
- tokens
- adasd
- draft
- threshold
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaSD proposes a hyperparameter-free speculative decoding method
  that dynamically adjusts candidate generation length and token acceptance criteria
  during inference using entropy and Jensen-Shannon distance. It achieves up to 49%
  speedup over standard speculative decoding while maintaining accuracy degradation
  under 2%, eliminating the need for additional training or hyperparameter tuning.
---

# AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference

## Quick Facts
- arXiv ID: 2512.11280
- Source URL: https://arxiv.org/abs/2512.11280
- Reference count: 32
- Primary result: AdaSD achieves up to 49% speedup over standard speculative decoding while maintaining accuracy degradation under 2%, eliminating hyperparameter tuning

## Executive Summary
AdaSD introduces a hyperparameter-free speculative decoding method that dynamically adjusts candidate generation length and token acceptance criteria during inference using entropy and Jensen-Shannon distance. By adaptively setting thresholds based on running statistics of rejected and accepted tokens, AdaSD eliminates the need for manual hyperparameter tuning while maintaining strong accuracy-speedup tradeoffs. The method stops draft generation when entropy exceeds an adaptive threshold and accepts tokens based on distributional similarity between draft and target models, achieving significant speedups across multiple model pairs and tasks.

## Method Summary
AdaSD modifies the standard speculative decoding paradigm by introducing two adaptive thresholds that update in real-time during inference. The generation threshold T_G determines when to stop candidate token generation based on entropy, while the verification threshold T_V determines token acceptance based on Jensen-Shensen distance between draft and target distributions. Both thresholds are computed as running averages from previously rejected or accepted tokens, allowing the system to self-calibrate without pre-deployment tuning. The draft model generates tokens sequentially, computing entropy at each step, while the target model verifies all candidates in parallel, accepting tokens that meet the distributional similarity criterion.

## Key Results
- Achieves up to 49% speedup over standard speculative decoding on GSM8K and HumanEval tasks
- Maintains accuracy degradation under 2% across all tested benchmarks
- Eliminates the need for hyperparameter tuning through adaptive threshold computation
- Demonstrates consistent performance across three model pairs (Llama 70B-8B, Llama 70B-1B, Qwen 72B-7B) and three tasks (GSM8K, HumanEval, MMLU)

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Generation Termination
AdaSD stops draft token generation when entropy exceeds an adaptive threshold T_G, which is dynamically set to the mean entropy of all previously rejected tokens. Higher entropy signals greater uncertainty and correlates with rejection likelihood, allowing the system to avoid generating tokens likely to be rejected by the target model. This mechanism improves efficiency by terminating generation early when the draft model's uncertainty suggests poor alignment with the target model's preferences.

### Mechanism 2: Jensen-Shannon Distance-Based Acceptance Relaxation
AdaSD accepts tokens when the JS distance between draft and target distributions falls below an adaptive threshold T_V, which is set to the midpoint between mean JS distances of previously accepted and rejected tokens. This relaxes strict token matching to distributional similarity, allowing speculative decoding to accept more tokens while maintaining output quality. The midpoint heuristic balances speedup and accuracy by accepting tokens with moderate distributional differences that are still semantically acceptable.

### Mechanism 3: Real-Time Threshold Adaptation via Running Statistics
AdaSD updates both thresholds during inference based on accumulated accept/reject statistics, eliminating the need for pre-deployment hyperparameter tuning. By maintaining running averages of rejected entropy and accepted/rejected JS distances, the system self-calibrates to the specific model pair and task. This adaptive approach ensures thresholds remain appropriate as generation progresses and context evolves.

## Foundational Learning

- Concept: **Speculative Decoding (Draft-Verify Paradigm)**
  - Why needed here: AdaSD modifies both the draft generation and target verification phases. Understanding the baseline two-model architecture is prerequisite to grasping what AdaSD changes.
  - Quick check question: Can you explain why speculative decoding helps when inference is memory-bound, and what role the draft model plays versus the target model?

- Concept: **Entropy and Jensen-Shannon Divergence/Distance**
  - Why needed here: These information-theoretic measures are the core signals AdaSD uses for adaptive thresholds. Without understanding what entropy represents (uncertainty) and why JS distance is preferred over KL divergence (bounded, symmetric), the design rationale is opaque.
  - Quick check question: Given two probability distributions p and q, what does a low JS distance indicate about their relationship, and why is JS distance bounded while KL divergence is not?

- Concept: **Token Acceptance Criteria in Speculative Decoding**
  - Why needed here: AdaSD relaxes traditional acceptance rules (greedy exact match or speculative sampling) with a distributional similarity threshold. Understanding prior acceptance strategies clarifies what AdaSD modifies and the tradeoffs involved.
  - Quick check question: In standard speculative decoding with speculative sampling, how is a candidate token accepted or rejected, and how does AdaSD's JS distance threshold change this?

## Architecture Onboarding

- Component map:
  - Draft Model (M_q) -> Entropy Computation -> Generation Threshold Module -> Context Manager
  - Draft Model (M_q) -> Target Model (M_p) -> JS Distance Computation -> Verification Threshold Module -> Context Manager
  - Context Manager -> Target Model (M_p) -> Resampled Token Generation

- Critical path:
  1. Draft model generates token x_i, records distribution q_i
  2. Compute entropy H(q_i); if H(q_i) > T_G, stop generation → proceed to step 4
  3. Repeat steps 1-2 up to max window W
  4. Target model verifies all generated tokens in parallel, producing p_1, ..., p_w
  5. For each token, check if x_i == y_i (greedy match) OR d_JS(p_i || q_i) < T_V; accept until first rejection
  6. Append accepted prefix + resampled token y_{w+1} to context
  7. Update T_G and T_V from running statistics; repeat until context reaches max length K

- Design tradeoffs:
  - **Entropy vs. other uncertainty measures**: Paper chose entropy for simplicity and interpretability; alternatives (variance, confidence intervals) unexplored
  - **Midpoint heuristic for T_V**: Chosen for balance between speed and accuracy; Appendix B shows alternative heuristics (Variant A, B, C) underperform or offer marginal differences
  - **Running mean vs. windowed mean**: Paper uses full-history mean; sensitivity to non-stationarity not analyzed
  - **Fixed max window W=20**: Prevents unbounded draft expansion; may truncate viable long sequences

- Failure signatures:
  - **Low speedup with high accuracy degradation**: Suggests T_V is too permissive, accepting divergent tokens. Check if JS distance distribution has heavy overlap between accepted/rejected
  - **Low speedup with minimal accuracy change**: Suggests T_G is too conservative (stopping early) or T_V is too strict. Check average candidate length and acceptance rate
  - **High variance in speedup across tasks**: Suggests thresholds not adapting well to task-specific distributions. MMLU shows ~8% speedup versus 23-49% for GSM8K/HumanEval due to short output sequences—this is expected behavior
  - **Crash or NaN in threshold computation**: Likely due to empty lists early in generation (L_R^E, L_R^D, L_A^D uninitialized). Paper initializes T_G = T_V = 0 but implementation must handle first few iterations gracefully

- First 3 experiments:
  1. **Baseline replication with fixed thresholds**: Run vanilla speculative decoding (fixed candidate length = 5, exact match acceptance) on GSM8K with Llama 70B-8B. Measure tokens/sec, accuracy, and acceptance rate. Compare to Table 1 values (~13.9 tks/sec, 94.5% accuracy) to validate setup
  2. **Ablation of generation threshold only (Gen-Only variant)**: Disable verification threshold (use exact match), enable only entropy-based generation stopping. Measure speedup and accuracy. Expect ~17-21% speedup on GSM8K (per Table 1) with minimal accuracy drop. Analyze candidate length distribution to confirm adaptive stopping behavior
  3. **Full AdaSD with threshold trajectory logging**: Run AdaSD on GSM8K, logging T_G and T_V values every 10 verification steps. Plot threshold evolution to verify convergence behavior. Check if thresholds stabilize or oscillate, and correlate stability with speedup/accuracy outcomes

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating predictive indicators beyond entropy into the generation threshold reduce the verification overhead observed in models like Qwen 2.5?
- **Basis in paper**: [Explicit] The Conclusion states that "incorporating additional indicators beyond entropy could enable more precise decisions," and Section 5.2 notes that entropy-only thresholds led to excessive candidate generation and overhead in Qwen experiments.
- **Why unresolved**: Entropy measures uncertainty but does not guarantee alignment with the target model, causing the draft model to generate long sequences that are ultimately rejected.
- **What evidence would resolve it**: A modification using acceptance-aware indicators (e.g., draft-target distribution divergence) that achieves higher acceptance rates and superior speedup on the Qwen 2.5 model family compared to the current AdaSD implementation.

### Open Question 2
Can more sophisticated heuristics for the Jensen-Shannon distance threshold minimize accuracy degradation while preserving inference speedup?
- **Basis in paper**: [Explicit] The Conclusion suggests that "designing more effective heuristics to adjust the JS distance could improve the acceptance rate and further reduce accuracy degradation."
- **Why unresolved**: The current midpoint heuristic (between mean accepted/rejected distances) is a simple approximation that results in a consistent ~1-2% accuracy drop.
- **What evidence would resolve it**: A new heuristic that dynamically adjusts the verification threshold to maintain speedup ratios above 1.2x while reducing accuracy degradation to less than 0.5% across GSM8K and HumanEval benchmarks.

### Open Question 3
How can AdaSD be adapted to function with draft and target models that utilize different vocabularies?
- **Basis in paper**: [Explicit] The Limitations section identifies that AdaSD is restricted to pairs that "share the same vocabulary," which is a growing issue as smaller models often adopt different tokenizers than larger ones.
- **Why unresolved**: The reliance on Jensen-Shannon distance requires direct comparison of probability vectors, which is impossible when token spaces differ in size or content.
- **What evidence would resolve it**: A mapping mechanism or cross-vocabulary alignment technique that allows AdaSD to operate on incompatible model pairs (e.g., Llama 3.1 70B with a non-Llama draft model) without losing the adaptive speedup benefits.

## Limitations

- **Task and Context Sensitivity**: AdaSD's adaptive thresholds assume stationary entropy and JS distance distributions within generation sessions, but no experiments test long-form generation where context may shift mid-generation.
- **Generalizability to Different Model Architectures**: AdaSD was only tested on Llama and Qwen decoder-only LLMs with similar training objectives, not on encoder-decoder models or models with different vocabularies.
- **Extreme-Case Robustness**: The paper reports median-case performance but does not analyze variance across generations or worst-case scenarios where thresholds might converge poorly.

## Confidence

**High Confidence (8/10)**: The core adaptive thresholding mechanism is well-motivated, clearly described, and shows consistent improvements across three model pairs and three tasks in controlled experiments. The use of entropy and JS distance as uncertainty signals is standard in information theory.

**Medium Confidence (6/10)**: The claim of "hyperparameter-free" operation assumes the midpoint heuristic is universally optimal. While Appendix B shows variants underperformed, the experiments did not explore window-based averaging, decay factors, or task-specific initialization strategies.

**Low Confidence (4/10)**: The assertion that AdaSD eliminates "tedious hyperparameter tuning" is only validated within a narrow experimental scope. No experiments test sensitivity to initial conditions, non-stationary generation contexts, or model pairs with fundamentally different output distributions.

## Next Checks

1. **Long-Form Generation Stability Test**: Run AdaSD on a dataset of long-form generation tasks (e.g., story continuation, essay writing) where context may shift multiple times. Log threshold values and measure if speedup/accuracy degrades mid-generation. If thresholds fail to adapt to context shifts, implement a sliding window or decay mechanism and re-test.

2. **Cross-Architecture Generalization**: Apply AdaSD to a diverse set of model pairs including encoder-decoder models (e.g., T5), models with different vocabularies (e.g., GPT-2 vs. GPT-3), and potentially multimodal models (e.g., Flamingo). Measure whether JS distance still correlates with acceptance probability and whether entropy-based stopping remains effective.

3. **Variance and Robustness Analysis**: Generate 1000+ samples per task/model pair and compute per-generation speedup and accuracy. Plot cumulative distribution functions to identify if a small fraction of generations exhibit extreme slowdown or accuracy loss. Analyze characteristics of high-variance cases to identify failure modes and potential mitigations.