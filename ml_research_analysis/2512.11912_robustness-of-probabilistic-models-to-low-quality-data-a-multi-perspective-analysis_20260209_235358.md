---
ver: rpa2
title: 'Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective
  Analysis'
arxiv_id: '2512.11912'
source_url: https://arxiv.org/abs/2512.11912
tags:
- data
- noise
- information
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper systematically investigates why modern probabilistic
  models exhibit starkly different robustness to low-quality training data. Autoregressive
  language models and large-scale classifiers show remarkable resilience to high levels
  of data corruption, while class-conditional diffusion models degrade catastrophically
  under the same conditions.
---

# Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis

## Quick Facts
- arXiv ID: 2512.11912
- Source URL: https://arxiv.org/abs/2512.11912
- Reference count: 40
- Key outcome: Autoregressive LMs and large-scale classifiers show remarkable resilience to high levels of data corruption, while class-conditional diffusion models degrade catastrophically under the same conditions

## Executive Summary
This paper systematically investigates why modern probabilistic models exhibit starkly different robustness to low-quality training data. Through controlled experiments introducing random errors at rates up to 50%, the study reveals that robustness is primarily governed by two factors: the richness of conditioning information and the absolute information content of clean training data. Autoregressive language models and large-scale classifiers demonstrate remarkable resilience, while class-conditional diffusion models show catastrophic degradation under identical conditions. Information-theoretic analysis shows residual instructive signal persists as long as observed labels aren't statistically independent of true labels.

## Method Summary
The study uses a controlled paradigm of injecting random errors into training data at rates up to 50% through uniform token/label replacement. A key innovation is scaling total training iterations by (1+r) to maintain constant exposure to correct samples. For high-noise regimes, batch sizes are increased (2x-12x) to stabilize gradients. The experiments span three model families: autoregressive LMs (GPT-2), class-conditional diffusion models (EDM), and classifiers (ResNet-18). Noise injection is performed on-the-fly during training, with corrupted samples replaced uniformly from the vocabulary or from C-1 incorrect classes.

## Key Results
- GPT-2 showed test NLL increase from 2.87 to 3.59 with 50% token corruption
- ImageNet-1000 maintained 73.784%→74.778% accuracy from 0% to 50% corruption, while ImageNet-10 dropped from 62.302%→50.794%
- Diffusion models showed catastrophic degradation with clean images but random labels producing poor image-label consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rich conditioning information constrains the learning problem and protects against noise in low-information targets.
- Mechanism: When conditioning variables (e.g., previous tokens, full images) carry more information than targets (e.g., next token, class label), they constrain the output space, effectively lowering the task's VC dimension and reducing sample complexity requirements. Sparse conditioning (e.g., single class label) fails to constrain high-information targets, leaving the model vulnerable.
- Core assumption: The conditioning information remains largely uncorrupted; corruption primarily affects targets/labels.
- Evidence anchors:
  - [abstract] "models with information-rich conditions (like previous tokens or images) can overcome noise in low-information targets, while models with sparse conditions (like single class labels) are highly vulnerable"
  - [Section 3.3, Figure 2] CNN/DailyMail (2343 tokens context) showed only 17.9% NLL degradation at 50% error vs. 31.5% for WMT (153 tokens)
  - [corpus] Limited direct coverage; corpus focuses on corruption detection methods rather than conditioning asymmetry
- Break condition: When conditioning information itself is corrupted (especially systematically), or when target information approaches or exceeds conditioning richness.

### Mechanism 2
- Claim: Larger batch sizes amplify coherent gradient signals from correct samples while averaging out divergent noise from corrupted samples.
- Mechanism: Clean samples produce aligned gradients (mean cosine similarity +0.52) that accumulate constructively. Corrupted samples produce directionally random gradients (similarity ≈ 0) that partially cancel during aggregation. Doubling batch size nearly doubles clean signal magnitude while noise grows more slowly, improving signal-to-noise ratio.
- Core assumption: Noise is unstructured and random; corrupted samples do not produce coherent incorrect gradients.
- Evidence anchors:
  - [abstract] "Gradient-based analysis demonstrates that larger batch sizes amplify coherent signals from correct samples while averaging out divergent noise from corrupted ones"
  - [Section 4.3, Table 4] SNR improved from 7.31x to 8.34x when doubling batch size at 25% corruption; from 2.96x to 3.83x at 50% corruption
  - [corpus] No direct evidence; corpus papers do not address batch size or gradient aggregation effects
- Break condition: When noise is structured (e.g., systematic mislabeling), corrupted gradients become coherent and incorrect, preventing cancellation.

### Mechanism 3
- Claim: Sufficient absolute information content allows the aggregate correct signal to dominate statistical noise.
- Mechanism: Only clean samples contribute instructive information for the supervised task. When the absolute quantity of clean data exceeds the PAC sample complexity threshold required for the task, the model generalizes despite noise. Massive datasets (e.g., ImageNet-1000 with 1.28M samples) provide overwhelming signal.
- Core assumption: Training duration scales proportionally with noise ratio (factor of 1+r), maintaining constant exposure to correct samples.
- Evidence anchors:
  - [Section 3.5, Table 3] ImageNet-1000 maintained 73.784%→74.778% accuracy from 0% to 50% corruption, while ImageNet-10 dropped from 62.302%→50.794%
  - [Section 4.1.2] "This aggregate signal is so overwhelmingly strong that it provides a clear directive for the learning task"
  - [corpus] Weak coverage; corpus mentions data quality importance but doesn't address scale vs. noise tolerance
- Break condition: When clean data volume falls below the task's sample complexity threshold, or when compute budget is fixed rather than scaled.

## Foundational Learning

- Concept: **VC Dimension and Sample Complexity**
  - Why needed here: PAC perspective formalizes why rich conditioning lowers effective task complexity, reducing the minimum clean samples required for generalization.
  - Quick check question: Why does p(label|image) have lower effective VC dimension than p(image|label)?

- Concept: **Mutual Information Under Noise**
  - Why needed here: Information-theoretic analysis quantifies residual signal in corrupted data and identifies the critical error rate where signal vanishes.
  - Quick check question: At what error rate pe does observed label x become statistically independent of true label y for n classes?

- Concept: **Gradient Coherence and Aggregation**
  - Why needed here: Understanding why batch size stabilizes training in noisy regimes through constructive vs. destructive gradient interference.
  - Quick check question: If clean gradients have mean cosine similarity +0.52 and corrupted gradients have ~0, what happens to SNR as batch size increases?

## Architecture Onboarding

- Component map:
  - Noise injection module -> Scaled training loop -> Dynamic batch size controller -> Dual evaluation track

- Critical path:
  1. Establish clean baseline (r=0) with standard batch size
  2. Introduce noise incrementally (r=0.1, 0.3, 0.5, 1.0) with scaled iterations
  3. Monitor training stability; increase batch size if loss variance spikes
  4. Track divergence between training loss (rises with noise) and test loss (should remain stable if robust)

- Design tradeoffs:
  - **Scaled vs. fixed compute**: Scaled compute isolates noise effects but increases cost; fixed compute reflects realistic constraints
  - **Random vs. structured noise**: Random noise enables gradient cancellation; structured noise creates coherent incorrect signals
  - **Batch size vs. generalization**: Larger batches stabilize training but may affect final generalization

- Failure signatures:
  - **Training divergence at high noise ratios** → batch size insufficient for gradient noise
  - **Stable FID with collapsed label consistency** (diffusion) → conditional guidance failed, image manifold preserved
  - **Per-sample accuracy drops but marginal distribution preserved** (classifier) → correlation learning degraded, output structure intact

- First 3 experiments:
  1. Replicate GPT-2 token corruption experiment: Train 124M parameter model on OpenWebText with 50% random token corruption, expect test NLL increase from ~2.87 to ~3.59
  2. Gradient coherence measurement: At initialization, compute pairwise cosine similarity between per-example gradients on clean vs. corrupted samples, expect +0.52 for clean pairs and ~0 for corrupted
  3. Scale sensitivity test: Train ViT-Base classifier on ImageNet-10 vs. ImageNet-1000 with 50% label corruption, expect catastrophic drop (62%→51%) for small subset vs. near-immunity (74%→75%) for full dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the gradient-averaging mechanism for robustness fail when models are trained on structured, correlated noise rather than unstructured random errors?
- Basis in paper: [explicit] The Discussion states that "structured noise introduces a coherent, competing learning signal" which the signal-averaging effect may be unable to overcome, and the Limitations section identifies exploring "complex, correlated noise" as a "crucial next step."
- Why unresolved: The study isolated intrinsic robustness using unstructured random noise to establish a baseline; structured noise generates a biased gradient direction that does not cancel out, potentially breaking the proposed mechanism.
- What evidence would resolve it: Controlled experiments using systematic error protocols (e.g., consistent mislabeling of specific classes) to compare against the random corruption results, measuring if performance degrades catastrophically as predicted.

### Open Question 2
- Question: How does the "richness of conditioning information" principle interact with the pre-trained knowledge ("elasticity") of models during fine-tuning with noisy data?
- Basis in paper: [explicit] The Discussion notes that the study focused on training from scratch and explicitly calls for "future investigations into the more complex dynamics of fine-tuning with noisy data" to build upon their baseline.
- Why unresolved: Pre-trained models possess a strong prior understanding of the world that acts as a confounding factor; it is unknown if this prior reinforces robustness or if models simply revert to pre-trained behaviors (elasticity) when faced with fine-tuning noise.
- What evidence would resolve it: Experiments applying the paper's corruption protocols to the fine-tuning phase of pre-trained models (e.g., LLMs or large vision models) to analyze if the "elasticity" overrides the noise or if the noise degrades the pre-existing capabilities.

### Open Question 3
- Question: Can class-conditional diffusion models achieve immunity to label noise through massive dataset scaling, similar to the resilience observed in large-scale classifiers?
- Basis in paper: [inferred] The Limitations state that "computational constraints precluded training diffusion models on very large-scale datasets," while the results show that classifiers on ImageNet-1000 become nearly impervious to noise due to "absolute information content."
- Why unresolved: The paper demonstrates diffusion models are fragile on CIFAR, but it remains untested whether the "absolute information content" principle scales sufficiently to overcome the high VC dimension (task complexity) inherent to image generation tasks.
- What evidence would resolve it: Training class-conditional diffusion models on massive datasets (e.g., full ImageNet or larger) with the 50% error rates used in the paper to observe if the signal dominance effect occurs.

## Limitations

- Scale-specific phenomena: The findings may be contingent on the massive scale of models and datasets used, with smaller-scale models potentially exhibiting different robustness profiles.
- Mechanism dependency on noise type: The analysis assumes random, unstructured noise, while real-world corruption often exhibits systematic patterns that could break the proposed mechanisms.
- Information-theoretic assumptions: The analysis assumes clean data samples are drawn independently and identically, which may not hold in practice due to dataset biases and sampling artifacts.

## Confidence

**High Confidence:** The empirical observation that autoregressive LMs and large-scale classifiers show superior robustness to random corruption compared to class-conditional diffusion models.

**Medium Confidence:** The proposed mechanisms explaining robustness differences, though they rely on assumptions about noise structure and model initialization that weren't fully validated.

**Low Confidence:** The quantitative thresholds identified (e.g., exact error rates where signal vanishes, precise batch size scaling factors) as these appear to be dataset- and architecture-specific findings.

## Next Checks

1. **Cross-architecture validation:** Test the same corruption protocols on smaller-scale versions of the same architectures and fundamentally different architectures to determine if the observed robustness patterns are scale-dependent or architecture-agnostic.

2. **Structured noise experiments:** Replace random label corruption with systematic corruption patterns and measure whether the same batch size scaling and conditioning mechanisms provide protection, or if structured noise creates fundamentally different learning dynamics.

3. **Sample complexity threshold mapping:** Systematically vary dataset size across orders of magnitude for a fixed architecture and corruption rate to empirically identify the PAC sample complexity threshold where robustness emerges.