---
ver: rpa2
title: 'SEAlign: Alignment Training for Software Engineering Agent'
arxiv_id: '2503.18455'
source_url: https://arxiv.org/abs/2503.18455
tags:
- sealign
- software
- code
- training
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEAlign addresses the misalignment between code generation models
  and real-world software engineering tasks by introducing a novel alignment framework.
  The method collects high-quality agentic trajectories from real-world software development,
  constructs trajectory trees using Monte Carlo Tree Search, and performs fine-grained
  preference optimization on critical decision points.
---

# SEAlign: Alignment Training for Software Engineering Agent

## Quick Facts
- arXiv ID: 2503.18455
- Source URL: https://arxiv.org/abs/2503.18455
- Reference count: 40
- Primary result: 17.7% resolved rate on SWE-Bench-Lite and 21.8% on SWE-Bench-Verified using a 14B-parameter model

## Executive Summary
SEAlign addresses the misalignment between code generation models and real-world software engineering tasks by introducing a novel alignment framework. The method collects high-quality agentic trajectories from real-world software development, constructs trajectory trees using Monte Carlo Tree Search, and performs fine-grained preference optimization on critical decision points. This approach enables models to better follow complex instructions and correctly use development tools. SEAlign achieves state-of-the-art performance on three standard benchmarks, with 17.7% resolved rate on SWE-Bench-Lite and 21.8% on SWE-Bench-Verified using a 14B-parameter model. The framework demonstrates significant improvements in both task performance and user experience while requiring minimal training overhead, making fully automated software engineering more feasible.

## Method Summary
SEAlign introduces a two-stage alignment process for software engineering agents. First, it collects agent trajectories from real-world software development tasks using frameworks like OpenHands, labeling each as successful or failed based on whether the code patch resolves the issue. These trajectories are then merged into tree structures using Monte Carlo Tree Search, where nodes represent decision points and are scored by their subtree's success rate. Critical decision points are identified when sibling nodes differ by a threshold (0.5) in their scores. The alignment process consists of supervised fine-tuning (SFT) on successful trajectories followed by Direct Preference Optimization (DPO) on extracted critical action pairs, focusing the model on the most impactful decisions.

## Key Results
- Achieves 17.7% resolved rate on SWE-Bench-Lite and 21.8% on SWE-Bench-Verified using a 14B-parameter model
- Significantly reduces failure modes: empty patch rate drops from 50.3% to 17.0%, stuck-in-loop rate from 35.0% to 22.0%
- Demonstrates strong transfer learning, with models aligned on OpenHands achieving competitive performance on other frameworks
- Maintains effectiveness while requiring minimal training overhead compared to online reinforcement learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging trajectories into tree structures enables credit assignment across multi-step agent decisions.
- Mechanism: Identical action nodes across sampled trajectories are merged into a unified tree. Each node inherits a success-rate score from all paths in its subtree, propagating outcome information backward from leaf outcomes (resolved/unresolved) to earlier decisions.
- Core assumption: Success/failure of a trajectory can be meaningfully attributed to specific decision points rather than being purely outcome noise.
- Evidence anchors:
  - [abstract] "constructs trajectory trees using Monte Carlo Tree Search"
  - [section 4.2] "all trajectories for a particular task are integrated into a unified structural representation... each non-leaf node refers to an individual decision step"
  - [corpus] Related work on agentic SE systems (e.g., OmniCode benchmark, Tokenomics on agent efficiency) supports that multi-step agent workflows require better credit assignment, though no corpus paper validates MCTS-based trajectory trees specifically.
- Break condition: If trajectories rarely share common prefixes (high divergence), tree merging provides little compression and scoring becomes noisy; if environment outcomes are stochastic, node scores may not reflect decision quality.

### Mechanism 2
- Claim: Fine-grained preference pairs on critical actions focus learning on decision points that most impact task success.
- Mechanism: Nodes are scored by subtree success rate. When sibling nodes differ by a threshold (≥0.5 in this work), the parent is marked as a critical decision point. Preference pairs are extracted from these points—partial trajectories sharing the same prefix but diverging to high-score vs low-score children.
- Core assumption: The 0.5 score-difference threshold correctly identifies causally important decisions rather than spurious correlations.
- Evidence anchors:
  - [abstract] "preference optimization on critical decision points"
  - [section 4.3] "If there is a significant difference in the scores of two non-leaf nodes under the same parent node, it indicates that these two non-leaf nodes correspond to a set of critical actions"
  - [corpus] No direct corpus validation of this specific threshold; related agentic SE work (Confucius Code Agent, AgentMesh) discusses scaffolding long-horizon decisions but does not evaluate fine-grained preference extraction.
- Break condition: If critical pairs are dominated by confounding factors (e.g., later actions determine success regardless of early choices), optimizing on them may not generalize.

### Mechanism 3
- Claim: Two-stage training (SFT warmup + fine-grained DPO) improves instruction-following and tool-use within a specific agentic framework.
- Mechanism: SFT on successful trajectories teaches the model valid action formats and masks environment observations in the loss. DPO then sharpens preferences at critical nodes, using the reference model to stabilize optimization.
- Core assumption: The SFT warmup is necessary for the policy to stay in-distribution before DPO sharpens preferences; DPO's offline approximation is sufficient without online RL.
- Evidence anchors:
  - [abstract] "preference optimization on critical actions to ensure models meet real-world requirements"
  - [section 4.4] "The entire post-training process consists of two phases: ❶ SFT with correct trajectories, and ❷ Direct Preference Optimization (DPO) with critical action pairs"
  - [corpus] DPO is widely used for code alignment (CodeDPO, PLUM cited in paper), but corpus neighbors focus on agentic orchestration and evaluation rather than alignment training itself.
- Break condition: If the policy drifts too far from the reference during DPO, or if SFT data distribution mismatches the target framework's prompts, performance may degrade.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: SEAlign uses DPO to align the model toward high-score actions at critical decision points without learning a separate reward model.
  - Quick check question: Can you explain why DPO avoids training an explicit reward model and what role the reference model plays?

- **Concept: Monte Carlo Tree Search (MCTS) concepts (node expansion, backup)**
  - Why needed here: Trajectory trees borrow MCTS-style backup to propagate success rates from leaves to ancestors for scoring.
  - Quick check question: How does backup differ from rollout in MCTS, and why is backup sufficient for static trajectory trees?

- **Concept: Agentic trajectories (state-action-observation sequences)**
  - Why needed here: SEAlign operates on multi-turn agent interactions, not single-prompt code generation.
  - Quick check question: Given a trajectory ⟨p, a1, o1, a2, o2, …⟩, which parts are masked in SFT loss and why?

## Architecture Onboarding

- **Component map:** Trajectory Collection -> Tree Builder -> Node Scorer -> Alignment Trainer
- **Critical path:** 1. Run agents in SWE-Gym to collect trajectories → 2. Filter and build trajectory trees → 3. Score nodes and extract critical pairs → 4. SFT warmup → 5. Fine-grained DPO
- **Design tradeoffs:**
  - Offline (SEAlign) vs online RL: Offline reduces compute but may limit exploration beyond collected trajectories.
  - Threshold (0.5) for critical pairs: Higher thresholds yield fewer but higher-confidence pairs; lower thresholds increase data but risk noise.
  - Framework-specific alignment: Strong gains within OpenHands, limited transfer to AutoCodeRover (Table 8 shows drop from 17.7% to 5.7% resolve rate).
- **Failure signatures:**
  - Empty patch: Agent never edits code (baseline 50.3%, SEAlign 17.0% on SWE-Bench-Lite).
  - Stuck-in-loop: Repeating same action (baseline 35.0%, SEAlign 22.0%).
  - Incorrect tool invocation: Wrong tool or parameters (observed in preliminary investigation).
- **First 3 experiments:**
  1. Reproduce the baseline vs SEAlign comparison on SWE-Bench-Lite with Qwen-2.5-Coder-14B and OpenHands; verify resolved rate, empty patch, stuck-in-loop metrics.
  2. Ablate the fine-grained scoring by running DPO on full positive/negative trajectories instead of critical pairs; compare to Table 5 (w/o Fine-grained Score: 5.3% vs 17.7%).
  3. Test cross-framework generalization by evaluating the OpenHands-aligned model on AutoCodeRover; confirm the performance drop shown in Table 8 and analyze failure modes.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can alignment training be modified to ensure LLMs generalize effectively across different software engineering agentic frameworks?
  - Basis in paper: [explicit] The authors explicitly ask, "can LLMs trained on one specific agentic framework generalize to others?" and note that while models aligned on OpenHands transfer to AutoCodeRover, the performance remains significantly lower than closed-source models.
  - Why unresolved: The paper demonstrates that current alignment is highly framework-specific. A model aligned for OpenHands (dynamic tool calls) suffers a performance drop when applied to AutoCodeRover (pre-defined workflows), indicating a lack of robust cross-framework adaptability.
  - What evidence would resolve it: Experiments showing that a single aligned model maintains high resolve rates (comparable to framework-specific baselines) across structurally different agentic frameworks like OpenHands, SWE-Agent, and AutoCodeRover.

- **Open Question 2:** How does SEAlign compare to advanced online reinforcement learning (RL) alignment techniques in terms of the effectiveness-efficiency trade-off?
  - Basis in paper: [explicit] The authors state that "A detailed comparison of how SEAlign performs relative to advanced RL alignment techniques in both effectiveness and efficiency remains an open question," specifically referencing methods like DeepSeek-R1 and GRPO.
  - Why unresolved: SEAlign utilizes offline alignment (DPO) to reduce computational costs, whereas advanced RL techniques rely on resource-intensive online sampling. The paper has not yet determined if the performance gap justifies the extra cost of online RL for agentic tasks.
  - What evidence would resolve it: A controlled comparative study measuring task resolved rates and training FLOPs/time between SEAlign and models trained via online RL algorithms (e.g., GRPO) on the same SWE-Bench datasets.

- **Open Question 3:** What optimization strategies can mitigate the computational overhead of execution validation to enable large-scale alignment training?
  - Basis in paper: [explicit] The authors identify execution validation as a main bottleneck, noting it takes nearly 5 minutes per sample and requires substantial storage (1TB), making scaling to larger datasets difficult.
  - Why unresolved: The current reliance on Docker-based execution environments for verifying trajectory success is inherently resource-intensive, limiting the feasibility of constructing massive training datasets required for further scaling.
  - What evidence would resolve it: The development and validation of a lightweight verification mechanism (e.g., static analysis or lightweight estimation) that significantly reduces per-sample validation time without compromising the integrity of the trajectory scoring.

## Limitations
- The 0.5 score-difference threshold for identifying critical actions is empirically chosen without ablation; sensitivity to this parameter is unknown.
- Cross-framework generalization is limited (5.7% vs 17.7% resolved rate on AutoCodeRover), suggesting strong framework dependence.
- No ablation on trajectory collection quality—baseline agents (GPT-4o, DeepSeek-v3) may introduce bias in the training data.

## Confidence

- **High confidence:** SFT+DPO two-stage training improves instruction-following and tool-use within the OpenHands framework.
- **Medium confidence:** Trajectory tree construction with MCTS-style scoring meaningfully identifies decision bottlenecks.
- **Medium confidence:** Critical action preference pairs generalize better than full trajectory pairs, though threshold sensitivity remains unclear.

## Next Checks

1. **Threshold sensitivity:** Run SEAlign with score-difference thresholds of 0.3, 0.5, and 0.7; compare resolved rates and critical pair counts to quantify robustness.
2. **Framework transfer test:** Align a model on OpenHands trajectories, then evaluate zero-shot on AutoCodeRover and Aider; analyze whether poor performance stems from tool syntax or scaffolding differences.
3. **Trajectory quality audit:** Sample 100 trajectories from the training set; classify failures by mode (stuck-in-loop, empty patch, wrong tool) and check if critical pairs effectively address these patterns.