---
ver: rpa2
title: 'eXIAA: eXplainable Injections for Adversarial Attack'
arxiv_id: '2511.10088'
source_url: https://arxiv.org/abs/2511.10088
tags:
- image
- attack
- explanations
- original
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes eXIAA, a novel black-box, model-agnostic adversarial
  attack method that targets post-hoc explainability methods in image classification.
  The key idea is to generate adversarial perturbations by leveraging the explanations
  of images from the model's running-up (second-highest confidence) class, without
  requiring access to model weights or multiple attack steps.
---

# eXIAA: eXplainable Injections for Adversarial Attack

## Quick Facts
- arXiv ID: 2511.10088
- Source URL: https://arxiv.org/abs/2511.10088
- Reference count: 40
- Primary result: Novel black-box, model-agnostic adversarial attack targeting post-hoc explainability in image classification by injecting feature attributions from the model's running-up class.

## Executive Summary
This paper introduces eXIAA, a novel black-box adversarial attack method designed to compromise post-hoc explainability techniques in image classification. By exploiting the model's second-highest confidence class explanations, eXIAA generates imperceptible perturbations that drastically alter feature attributions while preserving the original prediction. The method operates without access to model weights or requiring multiple attack iterations, making it highly practical for real-world black-box scenarios. Experiments demonstrate significant disruption to explanations with minimal impact on classification confidence and image similarity.

## Method Summary
eXIAA generates adversarial perturbations by leveraging feature attributions from the model's running-up (second-highest confidence) class. The attack injects carefully selected explanations into the original image through weighted blending, aiming to maximally alter the explanation map while maintaining visual similarity and preserving the original prediction. This approach operates in a black-box setting without requiring model weights or multiple query rounds, distinguishing it from traditional gradient-based or optimization-driven adversarial methods.

## Key Results
- Over 100% increase in absolute difference for explanations under certain settings
- Less than 5% average drop in classification confidence
- Structural Similarity Index (SSIM) values near 1, indicating minimal visual change

## Why This Works (Mechanism)
The attack exploits the fact that post-hoc explanation methods rely on the model's internal reasoning, which can be manipulated through carefully crafted perturbations. By targeting the running-up class's explanations, eXIAA introduces misleading feature attributions that significantly alter the explanation map without changing the predicted class. The weighted blending technique ensures that the adversarial perturbations remain visually imperceptible while effectively corrupting the explanation.

## Foundational Learning

### Adversarial Machine Learning
**Why needed:** Understanding how to generate inputs that cause models to make mistakes while appearing normal to humans
**Quick check:** Can generate imperceptible perturbations that change model behavior

### Post-hoc Explainability Methods
**Why needed:** These methods provide visual explanations of model decisions that eXIAA specifically targets
**Quick check:** Generate heatmaps showing which image regions influence predictions

### Black-box Attack Settings
**Why needed:** eXIAA operates without access to model weights, making it applicable to real-world scenarios
**Quick check:** Can generate effective attacks using only input-output queries

## Architecture Onboarding

### Component Map
Image Input -> Model Inference -> Running-up Class Detection -> Explanation Extraction -> Weighted Blending -> Adversarial Image

### Critical Path
The most critical components are the running-up class detection and explanation extraction, as these determine which feature attributions will be injected. The weighted blending operation is also crucial for maintaining visual similarity while ensuring effective explanation manipulation.

### Design Tradeoffs
The method trades off between perturbation strength and visual similarity. Stronger perturbations more effectively alter explanations but risk being perceptible. The weighted blending approach aims to maximize explanation disruption while maintaining SSIM near 1.

### Failure Signatures
The attack may fail if the running-up class's explanation is not sufficiently different from the predicted class, resulting in minimal perturbation effect. Additionally, if the blending weights are not properly calibrated, the perturbation may either be too weak to affect explanations or too strong to remain imperceptible.

### First Experiments
1. Generate adversarial examples on ImageNet using ResNet-18 with different blending weights
2. Evaluate explanation changes across multiple post-hoc methods (e.g., Grad-CAM, Integrated Gradients)
3. Test transferability of attacks from ResNet-18 to ViT-B16

## Open Questions the Paper Calls Out

The paper highlights several open questions regarding the generalizability of eXIAA to other model architectures and datasets, the potential for defenses that adapt explanations during attack, and the need for human studies to verify perceptual similarity of adversarial examples.

## Limitations
- Efficacy depends on accessibility or predictability of the running-up class without multiple query rounds
- Results are limited to specific architectures (ResNet-18, ViT-B16) and datasets (ImageNet)
- Perceptual quality and human evaluation of imperceptibility are not discussed

## Confidence
- **High:** Core experimental results within stated conditions (ImageNet, ResNet-18, ViT-B16)
- **Medium:** Claims about model-agnostic applicability
- **Low:** Assertions about perceptual imperceptibility without human studies

## Next Checks
1. Test transferability of eXIAA to non-ImageNet datasets and diverse model architectures
2. Conduct user studies to verify perceptual similarity of adversarial examples
3. Evaluate robustness against potential defenses that adapt explanations during attack