---
ver: rpa2
title: 'RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement
  Learning Fine-Tuning for LLMs'
arxiv_id: '2508.16546'
source_url: https://arxiv.org/abs/2508.16546
tags:
- singular
- fine-tuning
- performance
- arxiv
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the dynamics between supervised fine-tuning
  (SFT) and reinforcement learning (RL) in post-training large language models, using
  a card game benchmark and spectral analysis. The authors find that RL primarily
  restores generalization lost during SFT rather than introducing new capabilities,
  with effectiveness depending on the severity of SFT-induced overfitting.
---

# RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs

## Quick Facts
- **arXiv ID:** 2508.16546
- **Source URL:** https://arxiv.org/abs/2508.16546
- **Reference count:** 17
- **Key outcome:** RL primarily restores generalization lost during SFT rather than introducing new capabilities, with effectiveness depending on SFT-induced overfitting severity.

## Executive Summary
This study investigates the relationship between supervised fine-tuning (SFT) and reinforcement learning (RL) in post-training large language models using a card game benchmark and spectral analysis. The authors find that RL primarily acts as a restorative mechanism, recovering generalization capabilities lost during SFT rather than introducing fundamentally new problem-solving strategies. Through singular value decomposition analysis, they demonstrate that OOD performance degradation and recovery correlate with rotations of singular vectors rather than changes in singular value magnitudes. The research reveals that low-rank and shallow-layer recovery methods can recover 70-80% of OOD performance, offering an efficient alternative to costly RL fine-tuning.

## Method Summary
The authors employ a two-stage experimental framework using a card game environment called GeneralPoints. First, they apply SFT to train models on game-specific examples, then use RL (specifically PPO) for fine-tuning to evaluate recovery of generalization. The core analytical tool is singular value decomposition (SVD) applied to model weight matrices, which allows tracking of how training affects both singular values (magnitudes) and singular vectors (directions). By comparing pre-training, post-SFT, and post-RL weight matrices, they isolate the geometric changes responsible for performance shifts. The study systematically evaluates different recovery strategies, including restoring top singular vectors and shallow layers, to quantify their effectiveness compared to full RL fine-tuning.

## Key Results
- RL fine-tuning primarily restores generalization lost during SFT rather than creating new capabilities
- OOD performance degradation and recovery correlate with singular vector rotations, not magnitude changes
- Low-rank recovery (top 20% singular values) and shallow-layer recovery (first 25% of layers) recover 70-80% of OOD performance

## Why This Works (Mechanism)
The study reveals that SFT induces a directional drift in the model's weight space, causing singular vectors to rotate away from their pre-trained orientations. This rotation pattern is nearly identical between SFT and RL phases, suggesting both optimization regimes follow similar geometric paths. RL effectively counteracts this drift by restoring the original singular vector orientations, thereby recovering the generalization capabilities that existed in the pre-trained model. The mechanism operates through orthogonal transformations (rotations) rather than changes to the intrinsic dimensionality (singular values), indicating that the fundamental representational capacity remains intact but becomes misaligned during SFT.

## Foundational Learning
- **Singular Value Decomposition (SVD):** Decomposes weight matrices into orthogonal directions (singular vectors) and their magnitudes (singular values). Why needed: Enables geometric analysis of how training affects model representations beyond simple parameter changes. Quick check: Verify SVD decomposition A = UΣV^T correctly reconstructs original weight matrix.
- **Overfitting Dynamics:** SFT on limited data causes models to memorize specific patterns rather than learning general principles. Why needed: Explains why OOD performance degrades despite in-distribution accuracy improvements. Quick check: Compare train vs. test accuracy curves during SFT phase.
- **Orthogonal Manifold Optimization:** Both SFT and RL optimization proceed along paths that preserve certain geometric properties. Why needed: Explains why different loss functions converge to similar rotation profiles. Quick check: Track principal angles between weight matrices across training iterations.
- **Representation Drift:** Training causes gradual shifts in how information is encoded in weight space. Why needed: Provides framework for understanding performance degradation mechanisms. Quick check: Measure cosine similarity between pre-trained and fine-tuned representations.

## Architecture Onboarding

**Component Map:** Pre-trained model → SFT phase → SVD analysis → RL phase → Recovery evaluation

**Critical Path:** Pre-training → SFT → OOD degradation → RL restoration → Performance recovery

**Design Tradeoffs:** Full RL fine-tuning provides complete restoration but at high computational cost versus targeted low-rank/shallow-layer recovery offering 70-80% performance with significantly reduced resources

**Failure Signatures:** SFT-induced overfitting manifests as singular vector rotations without magnitude changes; complete recovery requires restoring original vector orientations

**First Experiments:**
1. Apply SVD analysis to compare singular vector rotations between SFT-only and SFT+RL models on held-out test sets
2. Implement low-rank recovery by projecting weights onto top-k singular vectors and measure OOD performance impact
3. Test shallow-layer recovery by restoring only the first 25% of layers and compare against full model recovery

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do SFT and RL converge on the same singular vector rotation profile despite different optimization objectives?
- **Basis in paper:** Section 4.2 states, "Understanding why the two optimisation regimes converge on the same rotation profile is an open question that we will investigate in future work."
- **Why unresolved:** The authors observe empirically that SFT and RL result in nearly identical principal angle spectra, but they do not provide a theoretical mechanism explaining why distinct loss functions (NLL vs. PPO) drive the model toward the same geometric orientation.
- **What evidence would resolve it:** A theoretical analysis of the loss landscape geometry showing that the orthogonal manifold of rotations is the optimal path for both objectives, or interventional experiments that force the rotation profiles to diverge to test performance impact.

### Open Question 2
- **Question:** What is the distinct causal role of extreme (head and tail) singular values versus the bulk spectrum in model generalization?
- **Basis in paper:** Section 6 notes, "We aim to isolate the head and tail singular values to ascertain their exact role in deteriorating model generalization."
- **Why unresolved:** While the paper demonstrates that directional shifts concentrate at the spectrum's extremes, it does not disentangle the specific functional contributions of the largest versus smallest singular components to the observed OOD degradation.
- **What evidence would resolve it:** Ablation studies that selectively freeze or perturb only the top-k% or bottom-k% of singular values/vectors during training to measure the isolated effect on OOD accuracy.

### Open Question 3
- **Question:** Does the "RL as restoration" mechanism generalize to complex reasoning domains outside of arithmetic card games?
- **Basis in paper:** The paper relies exclusively on the GeneralPoints environment. In Section 6, the authors list "advanced math and code-generation tasks" as necessary future work to validate universality.
- **Why unresolved:** It remains unclear if the finding—that RL merely counteracts SFT drift rather than creating new solutions—is inherent to LLM reasoning or an artifact of the specific symbolic game used in the study.
- **What evidence would resolve it:** Replicating the spectral analysis and OOD recovery experiments on diverse reasoning benchmarks (e.g., GSM8K, HumanEval) to see if RL similarly restores singular vector directions without altering singular values.

## Limitations
- Findings based on a single card game benchmark, limiting generalizability to broader NLP tasks
- Interpretation of singular value rotations versus magnitude changes requires further validation on diverse model architectures
- Study assumes OOD performance degradation is primarily due to SFT-induced overfitting, which may not capture all failure modes in real-world applications

## Confidence
- RL primarily restores SFT-induced generalization loss: **High**
- Singular vector rotations drive performance changes: **Medium**
- Low-rank recovery recovers 70-80% OOD performance: **Medium**
- RL counteracts SFT-induced directional drift: **Medium**

## Next Checks
1. Replicate findings on multiple NLP benchmarks beyond card games, including language understanding and generation tasks
2. Test the low-rank recovery method across different model scales (7B, 70B parameters) and architectures (decoder-only, encoder-decoder)
3. Investigate whether pretraining data diversity modulates the observed SFT-RL generalization dynamics