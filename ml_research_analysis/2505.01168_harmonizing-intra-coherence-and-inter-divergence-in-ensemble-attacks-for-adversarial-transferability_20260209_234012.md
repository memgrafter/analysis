---
ver: rpa2
title: Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial
  Transferability
arxiv_id: '2505.01168'
source_url: https://arxiv.org/abs/2505.01168
tags:
- adversarial
- gradient
- heat
- ensemble
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the transferability
  of adversarial examples in ensemble attacks. The proposed method, HEAT, integrates
  domain generalization principles into adversarial attack design.
---

# Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability

## Quick Facts
- arXiv ID: 2505.01168
- Source URL: https://arxiv.org/abs/2505.01168
- Authors: Zhaoyang Ma; Zhihao Wu; Wang Lu; Xin Gao; Jinghang Yue; Taolin Zhang; Lipo Wang; Youfang Lin; Jing Wang
- Reference count: 11
- Primary result: HEAT achieves >28% average ASR improvement over baseline ensemble methods on CIFAR-10, CIFAR-100, and ImageNet

## Executive Summary
This paper addresses the challenge of improving adversarial transferability in ensemble attacks by integrating domain generalization principles. The proposed HEAT method combines C-GRADS, which synthesizes shared gradient directions using SVD, with D-HARMO, which dynamically balances intra-domain coherence and inter-domain divergence through entropy-based weighting. Experiments demonstrate significant improvements in black-box attack success rates across multiple datasets and model architectures.

## Method Summary
HEAT enhances adversarial transferability by treating surrogate models as different domains and applying domain generalization principles. The method consists of two modules: C-GRADS synthesizes shared gradient directions across models using SVD decomposition, and D-HARMO dynamically weights models based on their intra-domain coherence (how well their perturbations generalize within the ensemble) and inter-domain divergence (how uniquely they contribute to the attack). The approach is evaluated on CIFAR-10, CIFAR-100, and ImageNet using 4 surrogate models and 8 black-box target models.

## Key Results
- HEAT achieves average attack success rate improvement exceeding 28% compared to baseline ensemble methods
- C-GRADS alone improves transferability by synthesizing shared gradient directions across heterogeneous models
- D-HARMO's dual weighting mechanism (intra-domain coherence + inter-domain divergence) provides additional gains of approximately 10% and 4% respectively

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Synthesizing shared gradient directions via SVD improves adversarial transferability across heterogeneous models
**Mechanism**: C-GRADS constructs a gradient matrix G by stacking gradient vectors from M surrogate models. SVD decomposition (G = UΣV⊤) identifies principal directions. Top-k singular vectors weighted by their singular values produce a consensus direction Vk that captures shared vulnerabilities rather than model-specific noise
**Core assumption**: Directions with high variance across model gradients represent transferable vulnerabilities rather than model-specific artifacts
**Evidence anchors**:
- [abstract] "C-GRADS, which synthesizes shared gradient directions across models using SVD"
- [section 3.2.1] Equations 5-8 detail SVD decomposition, k-selection via cumulative contribution ratio p=0.7, and weighted synthesis Vk = Σσᵢvᵢ
- [corpus] DRIFT paper identifies "gradient consensus" as key to transferability; corpus evidence supports SVD direction synthesis but does not validate this specific implementation
**Break condition**: If surrogate models have near-orthogonal gradient spaces (highly heterogeneous architectures), SVD may extract spurious directions

### Mechanism 2
**Claim**: Intra-domain coherence weighting identifies models whose adversarial perturbations generalize broadly across the ensemble
**Mechanism**: For each model m, generates adversarial example xₘᵃⁱᵛ using its own gradient. Computes cross-model loss wᵢₙₜᵣₐₘ = Σⱼ₌₁ᴹ log(L(fⱼ(xₘᵃⁱᵛ), y) + ε) / (Lₛₑₗᶠⱼ + ε). Models whose perturbations harm other models receive higher weights
**Core assumption**: Models that produce "effective" perturbations against peers will also transfer to unseen targets
**Evidence anchors**:
- [abstract] "intra-domain coherence, stabilizing gradients within individual models"
- [section 3.2.2] Equation 11 defines intra-domain weight; Table 3 ablation shows component B adds ~10% ASR improvement over baseline
- [corpus] Weak direct validation; ensemble attack papers focus on gradient averaging, not dynamic coherence weighting
**Break condition**: If models are too similar (homogeneous ensemble), intra-domain weights converge, providing no differentiation

### Mechanism 3
**Claim**: Inter-domain divergence weighting via information entropy prioritizes models with unique, high-confidence gradient contributions
**Mechanism**: Computes loss contribution factor Sₘ and alignment contribution factor Aₘ (cosine similarity with other models). Normalizes with temperature τ, then calculates information entropy Hₘ = -(S̃ₘ log S̃ₘ + Ãₘ log Ãₘ). Weight wᵢₙₜₑᵣₘ = 1/(Hₘ + ε) favors low-entropy (high-confidence) contributors
**Core assumption**: Low entropy indicates reliable, transfer-critical gradient information rather than noise
**Evidence anchors**:
- [abstract] "inter-domain divergence, enhancing transferability across models"
- [section 3.2.2] Equations 14-22 define the full inter-domain pipeline; Table 3 shows components C+D together add ~4% ASR
- [corpus] No direct precedent for entropy-based ensemble weighting in adversarial attacks
**Break condition**: If temperature τ is poorly tuned, normalization collapses; if all models have similar alignment, Aₘ provides no discriminative signal

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) for dimensionality reduction**
  - Why needed here: C-GRADS relies on SVD to extract dominant directions from a gradient matrix; understanding how singular values rank direction importance is essential
  - Quick check question: Given a 4×512 gradient matrix from 4 models, what does the first right singular vector represent?

- **Concept: Information entropy as confidence measure**
  - Why needed here: D-HARMO uses entropy to weight models; lower entropy → higher weight implies inverse relationship between uncertainty and influence
  - Quick check question: If S̃ₘ = Ãₘ = 0.5 for a model, what is its entropy Hₘ and what weight does it receive?

- **Concept: Domain generalization principles**
  - Why needed here: The paper frames ensemble attacks through domain generalization (surrogate models = domains); intra-domain = within-model stability, inter-domain = cross-model diversity
  - Quick check question: In domain generalization, why might prioritizing "specialists" improve out-of-distribution performance?

## Architecture Onboarding

- **Component map**:
  Input x → [Gradient computation for M models] → G (M×D matrix)
                    ↓
           [C-GRADS: SVD + k-selection] → Vₖ (shared direction)
                    ↓                              ↓
           [Intra-domain: cross-loss weighting]  [Inter-domain: entropy weighting]
                    ↓                              ↓
           w_intra (M-dim)                    w_inter (M-dim)
                    ↓                              ↓
                    └──────→ Final gradient g = Σ w_intra · w_inter · ∇L ←──────┘
                                              ↓
                                    x*_adv = Clip(x + α·sign(g))

- **Critical path**: SVD decomposition (O(min(M²D, MD²))) dominates compute; gradient matrix construction requires M forward+backward passes per iteration

- **Design tradeoffs**:
  - Cumulative ratio p: Higher p (e.g., 0.9) retains more directions but risks model-specific noise; lower p (e.g., 0.5) may lose transferable features
  - Temperature τ: High τ smooths weights (more uniform); low τ sharpens (fewer models dominate)
  - Ensemble heterogeneity: CNN+ViT mix improves transfer but increases gradient divergence, challenging SVD synthesis

- **Failure signatures**:
  - ASR degrades on specific architectures (e.g., ViTs only): Check if surrogate set lacks architectural diversity
  - Weights converge to uniform: Intra/inter-domain signals too weak; inspect loss distributions across models
  - NaN in entropy calculation: ε too small or S̃ₘ/Ãₘ underflow; increase ε or add log-stabilization

- **First 3 experiments**:
  1. **Ablation by component**: Run HEAT with only C-GRADS, only intra-domain, only inter-domain on CIFAR-10; compare to Table 3 to verify implementation
  2. **Sensitivity to p**: Sweep p ∈ {0.5, 0.7, 0.9} on a subset of ImageNet; plot ASR vs. p to find optimal cumulative ratio
  3. **Ensemble composition test**: Compare homogeneous (all CNNs) vs. heterogeneous (CNN+ViT) surrogates; measure transfer gap to black-box targets from Table 4

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- Critical hyperparameters (temperature τ, epsilon ε, cumulative ratio p=0.7) lack sensitivity analysis - performance may be brittle to these choices
- Computational complexity of SVD per iteration (O(M²D)) may limit scalability to large ensembles or high-dimensional models
- No theoretical justification for entropy-based weighting - empirical success doesn't establish whether low entropy reliably indicates "good" gradient directions

## Confidence
- **High**: Empirical ASR improvements (Table 3-4) are reproducible with correct implementation; SVD direction synthesis is a standard technique
- **Medium**: Mechanisms are logically sound but not rigorously validated - entropy weighting intuition is plausible but untested
- **Low**: Claims about domain generalization framing are conceptual; connection between ensemble diversity and domain shifts needs deeper theoretical grounding

## Next Checks
1. **Ablation study rigor**: Run ablation experiments isolating C-GRADS, intra-domain, and inter-domain components to verify reported ~10%, ~4% improvements match Table 3
2. **Hyperparameter sensitivity**: Sweep τ ∈ {0.01, 0.1, 1.0} and p ∈ {0.5, 0.7, 0.9} on CIFAR-10; plot ASR curves to identify brittle regions
3. **Ensemble composition impact**: Compare HEAT performance on homogeneous (all CNNs) vs. heterogeneous (CNN+ViT) surrogates; measure transfer gap to black-box targets to test architectural diversity claims