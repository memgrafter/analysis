---
ver: rpa2
title: A Finite-State Controller Based Offline Solver for Deterministic POMDPs
arxiv_id: '2505.00596'
source_url: https://arxiv.org/abs/2505.00596
tags:
- belief
- policy
- state
- planning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DetMCVI, an offline algorithm for solving
  deterministic POMDPs (DetPOMDPs) that builds policies as finite-state controllers
  (FSCs). DetMCVI adapts Monte Carlo Value Iteration (MCVI) for goal-oriented DetPOMDPs
  by eliminating redundant policy rollouts, sampling, and belief storage through exploiting
  deterministic transitions.
---

# A Finite-State Controller Based Offline Solver for Deterministic POMDPs

## Quick Facts
- arXiv ID: 2505.00596
- Source URL: https://arxiv.org/abs/2505.00596
- Reference count: 13
- Introduces DetMCVI, an offline algorithm for deterministic POMDPs using finite-state controllers

## Executive Summary
This paper presents DetMCVI, a novel offline solver for deterministic POMDPs that leverages finite-state controllers to achieve state-of-the-art performance on large-scale problems. By exploiting deterministic transitions, the algorithm eliminates redundant policy rollouts, sampling, and belief storage that burden traditional POMDP solvers. The approach demonstrates exceptional scalability, producing compact policies with high success rates across multiple synthetic domains and a real-world forest mapping experiment with a quadrupedal robot.

## Method Summary
DetMCVI adapts Monte Carlo Value Iteration (MCVI) for goal-oriented deterministic POMDPs by building policies as finite-state controllers rather than full policies. The algorithm exploits deterministic transitions to eliminate redundant computations, avoiding the need for sampling, belief tracking, and policy rollouts that typically limit POMDP solvers. By constructing compact finite-state controllers, DetMCVI can solve problems with combinatorial state spaces that are intractable for exact methods. The approach provides convergence guarantees under conditions similar to Goal-HSVI and scales effectively to problems where traditional solvers fail due to computational constraints.

## Key Results
- Consistently produces compact policies (11-39 nodes) versus 324-13,928 nodes for baseline methods
- Achieves 100% success rates across multiple synthetic domains including Canadian Traveler Problem, Wumpus World, Maze, and Sort
- In forest mapping experiment with quadrupedal robot, achieved 95% success rate with only 24-node policies, outperforming baselines that failed to plan within time constraints

## Why This Works (Mechanism)
DetMCVI works by exploiting the deterministic nature of transitions in DetPOMDPs to eliminate computational overhead. Unlike stochastic POMDPs that require sampling and belief tracking, deterministic problems have unique state transitions for each action-observation pair. This allows the algorithm to avoid redundant policy rollouts and belief storage, focusing computational resources on exploring the actual solution space. The finite-state controller representation further enhances efficiency by compactly representing policies as directed graphs rather than full action mappings, enabling scalability to problems with enormous state spaces.

## Foundational Learning
- **Deterministic POMDPs**: Problems where state transitions are uniquely determined by actions and observations. Why needed: Enables elimination of sampling and belief tracking. Quick check: Verify transition functions map each (state, action, observation) to exactly one next state.
- **Finite-State Controllers**: Policies represented as directed graphs where nodes encode actions and transitions between nodes. Why needed: Provides compact policy representation that scales to large problems. Quick check: Count nodes in generated policies versus problem size.
- **Monte Carlo Value Iteration**: Value iteration using Monte Carlo sampling to estimate expected returns. Why needed: Forms the basis for DetMCVI's planning algorithm. Quick check: Verify convergence behavior on small deterministic problems.
- **Goal-Oriented POMDPs**: Problems focused on reaching specific goal states rather than maximizing cumulative reward. Why needed: Allows specialized algorithmic optimizations. Quick check: Confirm goal states are correctly identified and prioritized.

## Architecture Onboarding

**Component Map**: Problem Specification -> DetMCVI Algorithm -> Finite-State Controller Policy -> Execution on Environment

**Critical Path**: The algorithm iteratively builds the finite-state controller by expanding promising nodes, evaluating their value through deterministic rollouts, and pruning suboptimal branches until convergence or time limit.

**Design Tradeoffs**: 
- Deterministic assumption enables efficiency but limits applicability
- Finite-state controllers provide compactness but may restrict policy expressiveness
- Offline planning enables thorough optimization but requires pre-computation

**Failure Signatures**: 
- Policy size grows exponentially with problem complexity
- Success rates drop when deterministic assumptions are violated
- Planning time exceeds time limits on extremely large state spaces

**3 First Experiments**:
1. Verify DetMCVI produces optimal policies on small deterministic problems with known solutions
2. Test scalability by increasing problem size and measuring policy compactness
3. Evaluate performance degradation when adding small amounts of stochasticity to transitions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section implies several areas for future work, including extending the approach to handle stochastic POMDPs and applying the method to more diverse real-world scenarios with dynamic obstacles and partial observability challenges.

## Limitations
- Deterministic assumption restricts applicability to problems where stochasticity can be ignored or modeled deterministically
- Focus on goal-oriented problems may not generalize to reward-based formulations common in other POMDP applications
- Real-world demonstrations are limited to controlled scenarios without dynamic obstacles or complex partial observability challenges

## Confidence
- **Algorithm correctness and scalability**: High - supported by theoretical convergence guarantees and systematic empirical validation
- **Practical applicability claims**: Medium - real-world demonstrations are limited to simplified scenarios
- **Performance claims**: Medium - synthetic domain results are impressive but may not capture all failure modes of real systems

## Next Checks
1. Test DetMCVI on stochastic POMDP variants to quantify performance degradation when deterministic assumptions are violated
2. Evaluate on long-horizon problems with time-dependent goal locations to assess scalability limits
3. Compare against state-of-the-art general POMDP solvers on domains where stochasticity is essential to problem formulation