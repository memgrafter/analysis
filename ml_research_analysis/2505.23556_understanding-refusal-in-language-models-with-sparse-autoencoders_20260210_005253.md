---
ver: rpa2
title: Understanding Refusal in Language Models with Sparse Autoencoders
arxiv_id: '2505.23556'
source_url: https://arxiv.org/abs/2505.23556
tags:
- refusal
- features
- harmful
- feature
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work mechanistically analyzes refusal in language models using
  sparse autoencoders to identify latent features that causally mediate refusal behaviors.
  The authors identify and validate refusal-related features in two open-source chat
  models, finding that LLMs encode harm and refusal as separate feature sets, with
  harmful features causally driving refusal.
---

# Understanding Refusal in Language Models with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2505.23556
- Source URL: https://arxiv.org/abs/2505.23556
- Reference count: 40
- Key outcome: SAE-based analysis reveals separable harm/refusal feature encoding, causal suppression via adversarial jailbreaks, and improved OOD classification through disentangled features.

## Executive Summary
This work mechanistically analyzes refusal behavior in language models using sparse autoencoders (SAEs) to identify latent features that causally mediate refusal. The authors validate that LLMs encode harm and refusal as separate, conditionally dependent feature sets, with harmful features causally driving refusal responses. They demonstrate that adversarial jailbreaks work by suppressing refusal-related features rather than bypassing harm detection, and show that their SAE-based method outperforms baselines in maintaining coherence while effectively identifying minimal, interpretable feature sets causal for refusal. The approach uses attribution patching within a restricted feature subspace aligned with the refusal direction to pinpoint causally relevant features.

## Method Summary
The method computes a refusal direction V_R* via difference-in-means between harmful and harmless instructions, then restricts attribution patching to features aligned with this direction. For each layer, K_0=10 features with highest cosine similarity to V_R* are selected, attribution patching is applied within this restricted set, and top K*=20 features are identified by averaged integrated gradients. These features are validated through clamping interventions that scale activations by c=-3 (Gemma) or c=-1 (Llama), inducing jailbreak behavior while monitoring coherence and reasoning metrics.

## Key Results
- LLMs encode harm and refusal as separable feature sets with harmful features causally driving refusal (F_H → F_R suppression rates: 0.48/0.51 vs. random baselines 0.05/0.19)
- Adversarial jailbreaks suppress specific refusal-related features rather than bypassing harmful feature detection (D_S vs D_F A(F_R) reduction: 0.73 vs 0.18 for Gemma)
- Disentangled features improve OOD classification of adversarial examples (classification gap: 0.03 vs 0.32 for Gemma)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Harm and refusal are encoded as separable feature sets with a conditional causal relationship (Harm → Refusal).
- Mechanism: Upstream "harmful" features (F_H) activate on tokens encoding harmful concepts (e.g., "bomb"), which then causally suppress or activate downstream "refusal" features (F_R) that directly mediate the refusal output. Intervening on F_H reduces A(F_R) and lowers refusal token probability.
- Core assumption: SAE features recovered from base models transfer sufficiently to chat models for causal analysis.
- Evidence anchors:
  - [abstract]: "LLMs encode harm and refusal as separate feature sets, with harmful features causally driving refusal."
  - [Section 4.2]: Clamping F_H suppresses A(F_R) with suppression rates of 0.48 (Gemma) and 0.51 (Llama) vs. random baselines of 0.05 and 0.19.
  - [corpus]: "LLMs Encode Harmfulness and Refusal Separately" (arXiv:2507.11878) independently confirms separable encoding.
- Break condition: If F_H and F_R are not functionally separable (e.g., overlapping circuits), clamping F_H should not consistently modulate A(F_R).

### Mechanism 2
- Claim: Adversarial jailbreaks operate by suppressing refusal-related features rather than bypassing harmful feature detection.
- Mechanism: Adversarial suffixes or paraphrasing reduce A(F_R) at critical tokens, decoupling harmful content detection from refusal generation. When A(F_R) drops sufficiently, the model complies despite harmful input.
- Core assumption: Jailbreak success correlates with A(F_R) suppression, not just input obfuscation.
- Evidence anchors:
  - [abstract]: "Adversarial jailbreaks suppress specific refusal-related features."
  - [Section 4.3, Table 2]: Successful jailbreaks (D_S) show relative A(F_R) reduction of 0.73 (Gemma) vs. failed jailbreaks (D_F) with only 0.18 reduction.
  - [corpus]: "Latent Adversarial Training Improves the Representation of Refusal" (arXiv:2504.18872) discusses vulnerability of single-direction refusal encoding.
- Break condition: If jailbreaks work primarily through harmful feature obfuscation, A(F_R) suppression rates should not differ significantly between D_S and D_F.

### Mechanism 3
- Claim: Restricting attribution patching to features aligned with the refusal direction yields minimal, causally-relevant feature sets.
- Mechanism: Standard attribution patching (AP) is biased toward single-token attribution and recovers irrelevant features. By first selecting K_0 features per layer with highest cosine similarity to V_R*, then applying AP within this restricted set, the method identifies ~20 features that approximate activation steering effectiveness while remaining interpretable.
- Core assumption: The refusal direction V_R* captures sufficient signal to constrain the search space without excluding causally-relevant features.
- Evidence anchors:
  - [Section 3.2]: "Without limiting features to F_0, AP often recover irrelevant features."
  - [Figure 2]: CosSim+AP achieves jailbreak scores comparable to activation steering (upper bound) while outperforming AP alone.
  - [corpus]: No direct corpus comparison for this specific hybrid method; related work (GSAE) uses graph regularization differently.
- Break condition: If V_R* is suboptimal or task-specific, F_0 may exclude features critical for refusal in edge cases.

## Foundational Learning

- Concept: **Sparse Autoencoders (SAEs)**
  - Why needed here: SAEs decompose dense activations into sparse, interpretable features. Understanding reconstruction (ẑ = Σ f_E(z)_i · v_D_i + b_D) and the sparsity constraint is essential for interpreting feature interventions.
  - Quick check question: Can you explain why SAE features are more interpretable than raw neurons?

- Concept: **Attribution Patching with Indirect Effects**
  - Why needed here: The method relies on linear approximation of causal mediation to score feature importance. Understanding IE(z_l) = ∫ ∇_z m · (z_corrupt - z_clean) is required to implement and debug the feature selection pipeline.
  - Quick check question: Why does AP require input pairs of similar length, and how does the paper circumvent this?

- Concept: **Activation Steering via Difference-in-Means**
  - Why needed here: V_R* = V_Harmful - V_Harmless provides the reference direction for both restricting AP search space and validating feature faithfulness. Understanding how steering ablates refusal is the evaluation baseline.
  - Quick check question: How would you compute V_R* for a new model, and what layer selection criteria apply?

## Architecture Onboarding

- Component map:
  Input -> SAE Encoder -> Refusal Direction (V_R*) -> Feature Selector (CosSim+AP) -> Intervention (Clamp) -> Evaluation (HARMBENCH, CE loss, GSM8K/ARC)

- Critical path:
  1. Load pre-trained SAE (GemmaScope/LlamaScope)
  2. Compute V_R* by sweeping layers for optimal refusal steering
  3. For each sample: select F_0 via cosine similarity, run AP, extract F*
  4. Clamp A(F*) with c=-3 (Gemma) or c=-1 (Llama)
  5. Evaluate jailbreak rate and coherence metrics

- Design tradeoffs:
  - K_0=10 constrains search but may miss indirect upstream features
  - K*=20 balances minimality vs. coverage; smaller sets risk incompleteness
  - Using AS-approximated corrupted activations avoids input-length constraints but introduces approximation error
  - Chat-token dependence: refusal features activate strongly on special tokens; omitting them breaks the signal

- Failure signatures:
  - High CE loss or degenerate outputs: feature set includes irrelevant features (ActDiff, raw AP)
  - Low jailbreak rate despite clamping: F* not aligned with true refusal circuit
  - Llama more sensitive to interventions: larger models may require smaller |c|
  - String-matching metrics false positives: nonsensical/irrelevant outputs flagged as jailbreaks

- First 3 experiments:
  1. Reproduce Figure 2: Compare CosSim+AP vs. baselines (CosSim, ActDiff, AP) on JAILBREAKBENCH using pre-computed V_R* at layer 15 (Gemma) or 11 (Llama).
  2. Validate F_H → F_R relationship: Clamp F_H on CATQA categories and measure A(F_R) suppression rate and "I" token probability drop per Table 1.
  3. OOD probe test: Train linear classifier on A(F_R) vs. dense activations using WILDJAILBREAK vanilla split; evaluate on adversarial split to reproduce Table 4 gap (0.03 vs. 0.32 for Gemma).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training Sparse Autoencoders (SAEs) directly on chat-model activations yield more faithful or minimal refusal features than transferring base-model SAEs?
- Basis in paper: [explicit] Section 6 states that prioritizing the training of SAEs on chat-model activations is "a promising direction for future research" to improve interpretability.
- Why unresolved: Current work relies on base-model SAEs transferred to chat models, which may fail to capture all chat-specific nuances or causal features.
- What evidence would resolve it: Comparative analysis of feature sets extracted from chat-trained SAEs versus base-trained SAEs on refusal intervention tasks.

### Open Question 2
- Question: Is restricting the initial feature pool to those aligned with the refusal direction suboptimal for identifying features relevant to challenging, jailbreak-resistant instructions?
- Basis in paper: [explicit] Section 6 notes that this strategy "is potentially suboptimal, as its effectiveness is inherently dependent on the optimality of the refusal direction itself."
- Why unresolved: The alignment filter assumes the refusal direction is perfectly representative, potentially excluding relevant features for difficult adversarial cases.
- What evidence would resolve it: Discovering causal refusal features in adversarial settings that have low cosine similarity to the initial refusal direction.

### Open Question 3
- Question: Do models encode refusal through conditional dependency graphs (e.g., Harm → Legality → Refusal) that mirror human reasoning processes?
- Basis in paper: [explicit] Appendix A.3 suggests that shared features imply complex dependencies, noting "exploring whether models encode refusal similarly to human reasoning is a promising direction."
- Why unresolved: Current analysis identifies shared and behavior-specific features but does not map the full hierarchical structure of intermediate concepts.
- What evidence would resolve it: Mechanistic mapping of intermediate feature circuits (e.g., "legal terminology") that sit between harmful inputs and refusal outputs.

## Limitations
- SAE transferability: Features learned from base models may not fully capture chat model activation distributions
- Evaluation reliability: HARMBENCH classifier vulnerable to string-matching exploits that could misclassify outputs
- Linear approximation: Attribution patching relies on linear approximations that may miss complex nonlinear feature interactions

## Confidence

- **High Confidence**: The separable encoding of harm and refusal features is strongly supported by consistent suppression rate measurements across models and independent corpus validation
- **Medium Confidence**: The effectiveness of the CosSim+AP hybrid method is demonstrated through competitive performance metrics, though the absence of direct corpus comparisons introduces uncertainty
- **Medium Confidence**: The claim that shared features across harmful categories are most directly linked to refusal mechanisms is supported by suppression rate analysis but could benefit from more granular examination

## Next Checks

1. Evaluate SAE Transferability: Compare feature interpretability and intervention effectiveness when using SAEs trained directly on chat model activations versus base model SAEs to quantify distribution shift impacts.

2. Test Nonlinear Intervention Effects: Systematically vary clamping magnitudes (c) and analyze jailbreak rates, coherence metrics, and feature activity patterns to map the full intervention response landscape and identify potential threshold effects.

3. Validate Classifier Robustness: Replace the HARMBENCH classifier with human evaluations or multiple independent classifiers to verify that string-matching vulnerabilities are not driving the observed jailbreak rate improvements.