---
ver: rpa2
title: Memory Mosaics at scale
arxiv_id: '2507.03285'
source_url: https://arxiv.org/abs/2507.03285
tags:
- memory
- mosaics
- learning
- transformer
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Memory Mosaics v2 scales associative memory networks to large
  language model sizes (8B parameters) and one trillion training tokens, achieving
  superior in-context learning and new-knowledge storage capabilities compared to
  transformers. The method introduces three architectural modifications: adaptive
  bandwidth in Gaussian kernel smoothing, gated time-variant key feature extraction,
  and a 3-level memory design.'
---

# Memory Mosaics at scale

## Quick Facts
- arXiv ID: 2507.03285
- Source URL: https://arxiv.org/abs/2507.03285
- Authors: Jianyu Zhang; Léon Bottou
- Reference count: 40
- Primary result: Memory Mosaics v2 scales to 8B parameters and one trillion training tokens, achieving superior in-context learning and new-knowledge storage compared to transformers

## Executive Summary
Memory Mosaics v2 introduces three key architectural modifications to associative memory networks that enable scaling to transformer-sized models while maintaining superior performance on new-knowledge storage and in-context learning tasks. The method replaces fixed attention with adaptive-bandwidth associative memory, implements gated time-variant key feature extraction, and introduces a 3-level memory design separating short-term and long-term contexts. Across three evaluation dimensions—training-knowledge storage, new-knowledge storage, and in-context learning—Memory Mosaics v2 matches transformers on persistent knowledge while significantly outperforming them (10-15% accuracy gains) on tasks requiring learning from new information.

## Method Summary
Memory Mosaics v2 scales associative memory networks to large language model sizes through three architectural innovations: adaptive bandwidth in Gaussian kernel smoothing that dynamically manages bias-variance trade-off as context grows, gated time-variant key feature extraction that enables semantic content-aware memory keys, and a 3-level memory design separating position-aware short-term memory from position-invariant long-term memory. The model is trained on one trillion tokens with 4k context, then fine-tuned on 32k context, achieving superior performance on new-knowledge storage and in-context learning tasks while matching transformers on persistent knowledge benchmarks.

## Key Results
- Memory Mosaics v2 matches transformer performance on standard language benchmarks (persistent knowledge storage)
- Achieves 10-15% accuracy gains over transformers on new-knowledge storage tasks (RULER QA)
- Demonstrates superior in-context learning with positive scaling on few-shot classification tasks
- Cannot be replicated by simply increasing transformer training data eightfold
- Achieves optimal fine-tuning performance with just one training minibatch

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Bandwidth Scaling in Kernel Regression
- **Claim:** Replacing fixed attention with adaptive-bandwidth associative memory allows dynamic bias-variance trade-off management as context length grows.
- **Mechanism:** Models retrieval as Gaussian kernel regression with bandwidth scheduled based on stored key-value pairs (β = β₁nᵅ + β₀), tightening bandwidth as more examples are stored to prevent over-smoothing.
- **Core assumption:** Optimal kernel bandwidth for retrieval depends on memory density, not just the query itself.
- **Evidence anchors:** Section 3.1 establishes bandwidth controls bias-variance trade-off; adaptive bandwidth introduced in abstract; kernel regression foundation supported by corpus.

### Mechanism 2: Temporal Hierarchy via Overlapping Memory Masks
- **Claim:** Separating memory into short-term (position-aware) and long-term (position-invariant) stores handles local syntax and global context without interference.
- **Mechanism:** Uses sliding window mask for short-term memory and delayed mask for long-term memory, with stochastic variation of gap during training to learn semantic vs positional signal allocation.
- **Core assumption:** Attention scores are spuriously correlated with token proximity; separating "near" and "far" retrieval enables better long-range dependency generalization.
- **Evidence anchors:** Section 3.3 shows attention scores depend on positions for near tokens but are invariant for far tokens; abstract highlights 3-level memory design; corpus supports random-access flexibility for ultra-long context.

### Mechanism 3: Semantic Gating for Key Construction
- **Claim:** Input-dependent gates for memory keys enable dynamic determination of relevant "past horizon" for tokens.
- **Mechanism:** Key feature extractor uses gated recurrent unit structure where retention weight and input gate are functions of current input, allowing model to reset or extend effective memory key based on semantic content.
- **Core assumption:** Key features for retrieval should be time-variant and content-aware to handle discontinuous or compositional patterns.
- **Evidence anchors:** Section 3.2 addresses semantically similar cases receiving different key features; abstract lists gated time-variant key feature extractor as core modification; corpus connects attention mechanisms to associative memory retrieval.

## Foundational Learning

- **Concept: Nadaraya-Watson Kernel Regression**
  - **Why needed here:** Memory Mosaics are theoretically grounded in kernel regression, not just "soft attention." Understanding the estimator E[V|K=k] is necessary to debug retrieval failures.
  - **Quick check question:** If the kernel bandwidth β is too high (small width), how does it affect the retrieval of unique facts in a long context?

- **Concept: Bias-Variance Trade-off in Non-Parametric Learning**
  - **Why needed here:** The paper explicitly links adaptive bandwidth to this trade-off. Without this concept, the scheduling of β appears arbitrary.
  - **Quick check question:** As the number of stored key-value pairs (n) increases, does the optimal bandwidth typically increase or decrease?

- **Concept: Positional Invariance vs. Relative Position**
  - **Why needed here:** The architecture deliberately removes explicit position encodings for associative memories to achieve extrapolation.
  - **Quick check question:** How does the model represent the order of tokens without positional embeddings? (Hint: Look at the temporal convolution in the feature extractor).

## Architecture Onboarding

- **Component map:** Token ID → Embedding → Feature Extraction (Gated Key Extractor + Convolutional Value Extractor) → Memory Unit (Short-term Associative Memory + Long-term Associative Memory + Persistent SwiGLU-FFN) → Concatenation of memory outputs → Linear Projection
- **Critical path:** The Adaptive Bandwidth Calculation (Eq. 6) is the most sensitive parameter, linking the size of active memory n to the softmax temperature β.
- **Design tradeoffs:**
  - Memory Cost: Unlike RNNs/SSMs (constant memory), Mosaics store KV pairs explicitly (O(N) memory), enabling perfect retrieval but limiting context length by RAM, not algorithm.
  - Training Stability: Stochastic sampling of long-term delay m during training is required to prevent overfitting to specific context boundaries.
- **Failure signatures:**
  - Context Extrapolation Collapse: If model fails at 32k but succeeds at 4k, check initialization of β₁ and α (Table 6).
  - "Goldfish" Memory: If model fails to answer questions about early documents in long sequence, Long-term Memory mask may be incorrectly set to exclude early tokens.
- **First 3 experiments:**
  1. **Sanity Check (Kernel Ablation):** Fix β to constant and compare against adaptive schedule on RULER benchmark to verify "bias-variance" gain.
  2. **Masking Analysis:** Visualize attention scores for "needle-in-a-haystack" task to confirm Long-term Memory attends to distant tokens uniformly while Short-term attends locally.
  3. **ICL Scaling:** Run "Anonymous Label" classification task to verify accuracy scales positively with number of shots, contrasting with Transformer's negative scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can computational costs for very long context lengths be effectively reduced using fuzzy hashing or hierarchical memory approaches?
- **Basis in paper:** Section 8 identifies reducing computational cost for very long contexts as future direction, specifically mentioning fuzzy hashing and hierarchical memory.
- **Why unresolved:** Current implementation faces scaling challenges similar to standard attention mechanisms at extreme lengths.
- **What evidence would resolve it:** Successful integration of compression techniques maintaining performance while lowering latency and memory usage.

### Open Question 2
- **Question:** What performance improvements would a dedicated hyperparameter search yield for Memory Mosaics v2?
- **Basis in paper:** Appendix C notes training hyperparameters were optimized for transformers and transferred without further searching, making current setup potentially suboptimal.
- **Why unresolved:** Authors did not perform separate hyperparameter search due to resource constraints.
- **What evidence would resolve it:** Comparison of training convergence and final benchmark scores using hyperparameters specifically tuned for Memory Mosaics v2.

### Open Question 3
- **Question:** Why does separation of long and short-term memory fail to improve Transformers despite being critical for Memory Mosaics?
- **Basis in paper:** Figure 5 and Section 5.3 show augmenting Transformer with long-short-term attention does not improve in-context learning, suggesting deeper architectural dependency.
- **Why unresolved:** Paper demonstrates negative result but doesn't explain if failure is due to lack of adaptive bandwidth, symmetric keys, or other unique MMv2 components.
- **What evidence would resolve it:** Ablation studies isolating specific components (e.g., adding adaptive bandwidth to Transformer) to identify necessary conditions for long-term memory advantage.

## Limitations

- Evaluation scope constrained to specific architecture and protocol; broader generalizability to other domains remains untested
- "One trillion token" training regime represents significant computational investment limiting replication feasibility
- Does not address potential privacy or copyright concerns with training data composition
- Claim that eightfold increased transformer training data cannot replicate improvements is based on extrapolation rather than direct empirical comparison

## Confidence

- **High Confidence:** Architectural modifications are clearly specified with sound theoretical foundations; claim of matching transformers on persistent knowledge storage is well-supported
- **Medium Confidence:** Claims of outperforming transformers on new-knowledge storage and ICL are supported by 10-15% accuracy gains but may be task-specific; efficiency claim regarding one-training-minibatch fine-tuning is demonstrated but not extensively validated
- **Low Confidence:** Assertion that eightfold increased transformer training data cannot replicate improvements requires more rigorous validation

## Next Checks

1. **Scaling Validation:** Replicate 32k context length extrapolation experiment with stochastic sampling of long-term memory delay parameter m. Measure RULER accuracy degradation when sampling is disabled to verify reported 15% performance drop.

2. **Ablation Study:** Conduct controlled experiments comparing Memory Mosaics v2 with and without adaptive bandwidth scheduling (β = β₁nᵅ + β₀) on RULER benchmark to quantify specific contribution to performance gains.

3. **Generalization Test:** Evaluate Memory Mosaics v2 on broader suite of in-context learning tasks beyond Banking77 and Tacred classification, including multi-step reasoning and code generation benchmarks, to assess whether ICL advantages extend to diverse problem domains.