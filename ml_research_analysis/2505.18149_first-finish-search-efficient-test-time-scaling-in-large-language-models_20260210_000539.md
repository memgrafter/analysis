---
ver: rpa2
title: 'First Finish Search: Efficient Test-Time Scaling in Large Language Models'
arxiv_id: '2505.18149'
source_url: https://arxiv.org/abs/2505.18149
tags:
- correct
- arxiv
- then
- therefore
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: First Finish Search (FFS) is a training-free test-time scaling
  method for large language models that improves reasoning accuracy by launching multiple
  parallel samples and returning the first completed trace. Motivated by the observation
  that shorter reasoning traces are more likely to be correct, FFS achieves 82.23%
  accuracy on AIME datasets with DeepSeek-R1, a 15% improvement over standalone performance
  and near OpenAI o4-mini levels, while reducing token usage by up to 45% compared
  to baselines like majority voting.
---

# First Finish Search: Efficient Test-Time Scaling in Large Language Models

## Quick Facts
- arXiv ID: 2505.18149
- Source URL: https://arxiv.org/abs/2505.18149
- Reference count: 40
- First Finish Search (FFS) achieves 82.23% accuracy on AIME datasets with DeepSeek-R1, a 15% improvement over standalone performance

## Executive Summary
First Finish Search (FFS) is a training-free test-time scaling method that improves reasoning accuracy by launching multiple parallel samples and returning the first completed trace. The method exploits the empirical finding that shorter reasoning traces are more likely to be correct than longer ones. By running n independent stochastic samples and selecting the first to complete, FFS achieves accuracy comparable to OpenAI o4-mini while reducing token usage by up to 45% compared to majority voting baselines. Theoretical analysis shows that increasing parallel samples reduces expected sequential compute at rate O(√log n).

## Method Summary
FFS launches n independent stochastic decoding samples in parallel with beam_size=1 and temperature=0.6. The method returns the first sample to emit an end-of-sequence token, canceling all other threads. This exploits the observation that correct reasoning paths tend to be shorter than incorrect ones. Two variants exist: Sync-FFS processes samples in lock-step through a single model copy, while Async-FFS distributes jobs across multiple machines. The method requires no training and works with any reasoning model that exhibits the trace length-correctness inversion.

## Key Results
- 82.23% accuracy on AIME datasets with DeepSeek-R1, improving standalone performance by 15%
- Reduces token usage by up to 45% compared to majority voting baselines
- Achieves near-OpenAI o4-mini accuracy levels (82.23% vs 83.3%)
- Sequential token usage decreases with sample count following O(√log n) scaling

## Why This Works (Mechanism)

### Mechanism 1: Trace Length-Correctness Inversion
Reasoning models internalize efficient solution paths for problems they understand correctly. Correct reasoning paths reach conclusions efficiently, while incorrect paths wander, backtrack, or accumulate errors, extending trace length. When a model knows the answer, it produces concise reasoning; uncertainty manifests as verbose exploration. This pattern is empirically validated with Welch statistic 16.56 (p < 0.001) showing clear separation between correct (μ₁=7.2K tokens) and incorrect (μ₂=15.4K tokens) trace lengths for reasoning models.

### Mechanism 2: Parallel Sample Racing with EOS Termination
Running n independent stochastic samples and selecting the first to complete yields correct answers more efficiently than waiting for all samples. Each sample independently generates reasoning, and since correct traces tend toward shorter lengths, the first sample to hit EOS is disproportionately likely to be correct. Beam size 1 with temperature > 0 ensures diversity without beam synchronization.

### Mechanism 3: Sequential Compute Reduction via Extreme Value Scaling
Increasing parallel samples n reduces expected sequential compute at rate O(√log n). From extreme value theory, E[min(Y₁...Yₙ)] = μ - σ√(2 log n) for i.i.d. normal RVs. Since trace lengths approximate normal distributions, more samples pushes the minimum completion time lower. Empirical results confirm sequential tokens decrease from 13K-19K (majority voting) to 7.8K-11.8K with FFS.

## Foundational Learning

- **Concept**: Test-Time Scaling (TTS) taxonomy
  - Why needed: FFS is positioned as a TTS method; understanding parallel vs. sequential scaling clarifies its niche
  - Quick check: What distinguishes parallel scaling from sequential scaling in TTS methods?

- **Concept**: Stochastic decoding with temperature sampling
  - Why needed: FFS relies on beam_size=1 with temperature=0.6 to generate diverse independent traces
  - Quick check: Why would greedy decoding (temperature=0) fail for FFS?

- **Concept**: Extreme value theory for order statistics
  - Why needed: Explains why adding more parallel samples reduces expected wait time for first completion
  - Quick check: According to Result 2, does doubling samples from 4 to 8 halve the expected minimum completion time?

## Architecture Onboarding

- **Component map**: Input Prompt → [n parallel decoding threads, beam_size=1, temp=0.6] → Stochastic sampling per thread → First thread hits EOS → Return immediately → All other threads cancelled

- **Critical path**: Prompt ingestion → batched forward passes (Sync-FFS) or distributed jobs (Async-FFS) → EOS detection → early termination logic

- **Design tradeoffs**:
  - Sync-FFS vs Async-FFS: Sync minimizes memory (single model copy) but requires lock-step batching; Async scales across machines but has coordination overhead
  - Temperature 0.6: High enough for diversity, low enough to avoid incoherent traces
  - n=4 samples: Empirically sufficient; diminishing returns beyond this

- **Failure signatures**:
  - FFS accuracy drops below majority voting → likely a non-reasoning model or dataset where length-correctness correlation is weak
  - No samples complete within max_length → increase token budget or check for degenerate loops
  - All samples produce similar traces → increase temperature or verify stochastic decoding is enabled

- **First 3 experiments**:
  1. Baseline validation: Run FFS with n=4 on AIME24 subset; verify trace-length distributions show correct < incorrect pattern before trusting FFS on new models
  2. Scaling sweep: Measure accuracy and token usage at n={1,2,4,8,16}; confirm O(√log n) sequential compute reduction empirically
  3. Model class comparison: Compare FFS vs MV on both reasoning (DeepSeek-R1) and non-reasoning (DeepSeek-V3) models to identify where the core assumption breaks

## Open Questions the Paper Calls Out

### Open Question 1
How can FFS be integrated with sequential scaling methods or confidence estimators to create a hybrid system that adapts when the "shorter is correct" heuristic fails? The current implementation of FFS is purely parallel and relies solely on the finish-time heuristic; it lacks a mechanism to switch strategies or deliberate longer when the model's confidence in the first finished trace is low. A study comparing standard FFS against a variant that triggers budget forcing or sequential thinking when the first-finished trace is below a certain length or probability threshold would resolve this.

### Open Question 2
Does the inverse correlation between trace length and correctness hold for non-reasoning models or tasks that do not involve explicit multi-step logic? The paper demonstrates FFS's success on specific reasoning models and math benchmarks, but the generalizability to standard instruct models or creative writing tasks where verbosity is not an error signal remains untested. Empirical results from applying FFS to a broader range of model architectures on open-ended generation tasks would resolve this.

### Open Question 3
How does the violation of the normality assumption for trace lengths impact the theoretical guarantees of FFS in heavy-tailed distributions? The theoretical analysis relies on trace lengths being normally distributed, but the paper notes that beyond moderate lengths the empirical trace-length distributions become heavy-tailed and depart from the normal assumption. A theoretical extension of Result 2 using extreme value theory for heavy-tailed distributions, or a sensitivity analysis showing the degradation of the length-correctness correlation at extreme token lengths, would resolve this.

## Limitations
- Core assumption that shorter reasoning traces are more likely correct breaks down for non-reasoning models like DeepSeek-V3
- Theoretical extreme value analysis assumes normal trace length distributions, which may not hold at scale
- Method's performance depends heavily on the presence of length-correctness inversion, not universally present across model types or tasks

## Confidence

- **High confidence**: Empirical accuracy improvements on AIME datasets (82.23% vs 67% baseline), token usage reduction (up to 45%), and the observation that correct traces are consistently shorter than incorrect ones in reasoning models
- **Medium confidence**: Theoretical extreme value analysis showing O(√log n) sequential cost reduction, and the mechanism that reasoning models produce efficient traces when correct
- **Low confidence**: Generalization to non-mathematical reasoning tasks, performance on smaller models beyond the tested 32B parameters, and behavior with very large sample counts (n > 16)

## Next Checks

1. **Cross-domain validation**: Test FFS on non-mathematical reasoning tasks (e.g., logical reasoning, common sense QA) to verify whether the length-correctness inversion holds beyond AIME datasets

2. **Distribution validation**: Generate trace length distributions for n=100+ samples to empirically verify whether the extreme value theory prediction of O(√log n) scaling holds at scale

3. **Model architecture study**: Compare FFS performance across different reasoning model architectures (e.g., chain-of-thought vs outcome-reward models) to identify which architectural features are necessary for the core mechanism to function