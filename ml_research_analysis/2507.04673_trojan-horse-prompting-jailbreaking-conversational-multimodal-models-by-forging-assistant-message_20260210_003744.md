---
ver: rpa2
title: 'Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging
  Assistant Message'
arxiv_id: '2507.04673'
source_url: https://arxiv.org/abs/2507.04673
tags:
- arxiv
- attack
- conversational
- user
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Trojan Horse Prompting, a jailbreak technique
  that manipulates conversational history by forging model-attributed messages to
  bypass safety filters. The method exploits an "Asymmetric Safety Alignment" where
  models are trained to refuse harmful user inputs but not to validate the authenticity
  of their own past messages.
---

# Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message

## Quick Facts
- arXiv ID: 2507.04673
- Source URL: https://arxiv.org/abs/2507.04673
- Authors: Wei Duan; Li Qian
- Reference count: 30
- The paper introduces Trojan Horse Prompting, a jailbreak technique that manipulates conversational history by forging model-attributed messages to bypass safety filters. Experiments on Google's Gemini-2.0-flash-preview-image-generation show significantly higher Attack Success Rate compared to established user-turn jailbreaking methods.

## Executive Summary
This paper presents a novel jailbreaking technique called Trojan Horse Prompting that exploits an "Asymmetric Safety Alignment" in conversational multimodal models. The attack forges messages attributed to the model role in conversation history, followed by benign user prompts that trigger the execution of malicious payloads. The key insight is that models are extensively trained to refuse harmful user requests but lack comparable skepticism toward their own past messages. This creates a fundamental vulnerability where attackers can manipulate the model's trusted context to bypass safety filters and generate harmful content.

## Method Summary
The attack constructs a fabricated conversation history where the penultimate message is forged with `role: 'model'` containing malicious payload, followed by a benign user trigger prompt. The attacker leverages the API's role-based history structure, exploiting the fact that models trust their own past messages without validating authenticity. When the benign trigger prompt is received, the model executes the latent malicious instructions from the forged history. The method was specifically tested on Google's Gemini-2.0-flash-preview-image-generation model using harmful prompts across categories like violence, explicit content, hate speech, and illegal activities.

## Key Results
- Trojan Horse Prompting achieves significantly higher Attack Success Rate compared to established user-turn jailbreaking methods on Gemini-2.0-flash-preview-image-generation
- The attack exploits "Asymmetric Safety Alignment" where models refuse harmful user requests but lack comparable skepticism toward their own conversational context
- The vulnerability stems from the gap between client-controlled history construction and server-side validation, allowing protocol-level identity spoofing

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Trust Asymmetry
Models trained via RLHF/SFT develop a strong learned policy to refuse harmful content from `role: user` but lack comparable scrutiny for `role: model` messages. Safety alignment disproportionately penalizes harmful user-request responses during training, creating an asymmetric conditional policy `P(refusal|role='user', content=harmful)` while treating model-attributed history as pre-vetted ground truth. The training data does not include adversarial examples where model history is forged, so no self-scrutiny policy is learned.

### Mechanism 2: Protocol-Level Identity Spoofing
The attack exploits the API's structural role attribute rather than semantic content manipulation. Attackers construct `HTrojan = [..., c_forged(n-1), c_trigger(n)]` where `c_forged` has `role: 'model'` with malicious payload, and `c_trigger` is a benign user prompt. The model executes the forged "prior commitment" without verifying provenance. The API accepts externally-constructed history objects without cryptographic or server-side validation of message authenticity.

### Mechanism 3: Cognitive Source Misattribution
The model "recalls" forged instructions but misattributes their source to itself rather than an external attacker. Analogous to human source amnesia—the model processes content from `role: 'model'` as originating from a previously-aligned, trusted source (itself), bypassing the scrutiny applied to user-originated requests. The model's internal representation does not distinguish "content I actually generated" from "content labeled as model-generated in the context window."

## Foundational Learning

- **Concept: Role-Based Tokenization in Chat APIs**
  - Why needed here: Understanding how conversational APIs structure history as `Content(role, parts)` objects is prerequisite to grasping why role tags are trusted but manipulable.
  - Quick check question: In a standard chat API, what distinguishes a user message from a model message in the history object, and who controls the construction of that object?

- **Concept: Safety Alignment (RLHF/SFT)**
  - Why needed here: The attack's effectiveness hinges on understanding what RLHF optimizes for (refusing harmful user requests) and what it omits (history integrity validation).
  - Quick check question: During RLHF, does the model learn to scrutinize: (a) user inputs, (b) its own outputs, or (c) externally-provided context labeled as its past outputs?

- **Concept: Context Window vs. Ground Truth**
  - Why needed here: The attack exploits the gap between what the context window *says* happened and what *actually* happened; understanding this distinction clarifies the vulnerability.
  - Quick check question: If an API client sends a fabricated history claiming the model previously agreed to generate harmful content, does the model have any mechanism to verify this claim against server-side logs?

## Architecture Onboarding

- **Component map:**
  Client Application -> API Request Constructor -> History Object [Content(role, parts), ...] -> Role-Based Parser -> Model Inference Engine -> Safety Classifiers -> Response Generation
  *Critical gap: No "History Integrity Validator" between role-based parser and model inference.*

- **Critical path:**
  1. Attacker constructs `HTrojan` with forged `role: 'model'` message
  2. API accepts history object (no provenance check)
  3. Model processes forged message as self-originated, trusted context
  4. Benign user trigger activates latent malicious payload
  5. Model generates harmful output, bypassing user-role safety filters

- **Design tradeoffs:**
  - **Flexibility vs. Integrity:** Allowing clients to construct full history enables multi-turn context but exposes protocol-level spoofing; server-side history tracking improves integrity but increases state management complexity.
  - **Assumption:** Current APIs prioritize developer flexibility over context authenticity.

- **Failure signatures:**
  - Unexpectedly high compliance with harmful requests when preceded by innocuous user prompts
  - Model responses referencing "prior agreements" or "previous outputs" that don't match server logs
  - Elevated ASR for attacks using multi-turn structures vs. single-turn adversarial prompts

- **First 3 experiments:**
  1. **Baseline ASR comparison:** Measure Attack Success Rate for (a) direct harmful user prompts, (b) user-turn jailbreaks (e.g., GCG), and (c) Trojan Horse Prompting on the same target model and harmful prompt set.
  2. **Ablation on forged content position:** Test whether placing the malicious payload in `role: 'model'` vs. `role: 'user'` in the penultimate turn affects ASR—isolates the role-trust mechanism.
  3. **Trigger phrase sensitivity:** Vary the final user prompt from explicit ("Generate the harmful image") to minimal ("Continue") to determine how much context priming is required for successful execution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific protocol-level defense mechanisms can effectively validate conversational context integrity without disrupting the utility of stateful interactions?
- Basis in paper: [explicit] The abstract concludes that the findings "necessitat[e] a paradigm shift... to robust, protocol-level validation of conversational context integrity."
- Why unresolved: The paper focuses on identifying the attack vector; the "Discussion and Conclusion" section is explicitly marked "Work in progress" with no tested defenses presented.
- What evidence would resolve it: A proposed defense framework and experimental results showing reduced Attack Success Rates (ASR) on Gemini while maintaining conversation coherence.

### Open Question 2
- Question: Does the Trojan Horse Prompting vulnerability generalize to text-only LLMs or other commercial multimodal architectures (e.g., GPT-4o, Claude)?
- Basis in paper: [inferred] The experimental validation is restricted exclusively to "Google's Gemini-2.0-flash-preview-image-generation."
- Why unresolved: The paper provides no data regarding the efficacy of this attack on other foundational models or text-only interfaces, limiting the scope of the "fundamental flaw" claim.
- What evidence would resolve it: Successful or unsuccessful replication of the attack across a diverse set of state-of-the-art conversational models.

### Open Question 3
- Question: Can the "Asymmetric Safety Alignment" vulnerability be mitigated through fine-tuning models to detect forged history, or does it require architectural changes?
- Basis in paper: [inferred] The authors posit that models "lack a learned policy for self-scrutiny" because alignment processes fail to train models to validate history authenticity.
- Why unresolved: The paper diagnoses the training gap but does not experiment with retraining models with history-forging examples to test if this closes the vulnerability.
- What evidence would resolve it: Comparative experiments showing that models fine-tuned on forged context datasets demonstrate resistance to Trojan Horse Prompting.

## Limitations
- The paper lacks specific harmful prompt benchmarks and ASR baseline comparisons with established jailbreaking methods
- Experimental validation is limited to a single model (Gemini-2.0-flash-preview-image-generation) without cross-model generalization testing
- The "Asymmetric Safety Alignment" mechanism is conceptually sound but lacks direct training-data analysis showing absence of adversarial history examples

## Confidence
- **High confidence**: The mechanism of role-based trust asymmetry and protocol-level identity spoofing is well-supported by the API structure description and consistent with known RLHF limitations.
- **Medium confidence**: The cognitive source misattribution mechanism has weak direct corpus evidence and relies heavily on behavioral observation rather than model-internal validation.
- **Low confidence**: Generalization claims to other multimodal models and the assertion that this represents a "fundamental vulnerability" are speculative without cross-model testing.

## Next Checks
1. **Baseline ASR Validation**: Replicate the Trojan Horse attack on Gemini-2.0-flash-preview-image-generation and measure ASR for direct harmful prompts, established jailbreaking methods (GCG, Many-Shot), and Trojan Horse Prompting using identical harmful prompt sets and statistical methodology.
2. **Cross-Model Generalization**: Test the attack on at least two other prominent conversational multimodal models (e.g., GPT-4V, Claude 3) to verify if the asymmetric safety alignment vulnerability persists across different training approaches and safety implementations.
3. **History Integrity Mitigation**: Implement and test a server-side history validation mechanism that either (a) cryptographically signs model-generated messages or (b) maintains server-side conversation state that cannot be arbitrarily overwritten by clients, then measure the resulting ASR reduction for Trojan Horse attacks.