---
ver: rpa2
title: 'NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with
  Extended Realistic Scenarios'
arxiv_id: '2503.19267'
source_url: https://arxiv.org/abs/2503.19267
tags:
- data
- offline
- policy
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper extends the NeoRL benchmark with NeoRL-2, adding seven
  simulated tasks that incorporate realistic challenges often encountered in real-world
  applications: time delays, external factors, safety constraints, data from traditional
  control methods, and limited data availability. These features make the benchmark
  more representative of practical scenarios, such as robotics, industrial control,
  and healthcare.'
---

# NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios

## Quick Facts
- arXiv ID: 2503.19267
- Source URL: https://arxiv.org/abs/2503.19267
- Reference count: 40
- Key outcome: Extends NeoRL benchmark with seven simulated tasks incorporating realistic challenges (delays, external factors, safety constraints, traditional control data, limited data), revealing current offline RL algorithms often fail to outperform behavior policies in these scenarios.

## Executive Summary
NeoRL-2 introduces a benchmark extension featuring seven simulated tasks designed to reflect real-world challenges in offline reinforcement learning, including time delays, external uncontrollable factors, safety constraints, data from traditional control methods, and limited data availability. These tasks aim to bridge the gap between idealized benchmarks and practical deployment scenarios. Experiments with state-of-the-art offline RL algorithms demonstrate significant performance degradation under these conditions, highlighting the need for more robust methods that can handle non-Markovian dynamics, narrow data distributions, and conservative regularization in sparse data regimes.

## Method Summary
The benchmark consists of seven simulated environments with corresponding offline datasets (2,000-100,000 samples) collected via sub-optimal SAC policies or PID controllers. Evaluation uses online policy testing with normalized scores (0-100, where 0=Random, 100=Expert). Both model-free (BC, CQL, EDAC, MCQ, TD3BC) and model-based (MOPO, COMBO, RAMBO, MOBILE) algorithms are evaluated using specified hyperparameter search spaces. The primary objective is to outperform the behavior policy used for data collection.

## Key Results
- Current offline RL algorithms struggle to beat behavior policies on tasks with delays, external factors, and safety constraints
- Model-based methods show high variance and instability, particularly on tasks with sparse data (Fusion) and safety constraints (SafetyHalfCheetah)
- TD3BC achieves the most consistent improvements across tasks, with EDAC showing promise on PID-collected data (DMSD)
- No algorithm exceeds 95 normalized score on delay-heavy tasks (Pipeline, Simglucose), indicating fundamental limitations

## Why This Works (Mechanism)

### Mechanism 1: Task Complexity Degradation of MDP Assumptions
Incorporating delays, external factors, and constraints causes task dynamics to deviate from standard MDP assumptions, degrading offline RL algorithm performance. Delays extend action effects over indefinite periods, external factors introduce uncontrollable variance in transitions, and safety constraints restrict data to only safe state-action pairs. These factors collectively create incomplete information and non-Markovian dynamics that violate core assumptions of current offline RL methods.

### Mechanism 2: Narrow Data Distribution from Traditional Control Methods
Data collected using traditional control methods (e.g., PID controllers) produces narrow distributions that hinder model learning and policy improvement. PID controllers use deterministic feedback control with limited exploration, resulting in datasets with low action variance. This narrow distribution makes it difficult for models to learn accurate transition dynamics and for policies to discover improved behaviors outside the observed distribution.

### Mechanism 3: Conservative Regularization Mismatch with Sparse Data Regimes
Existing conservative offline RL methods are not calibrated for extremely limited data scenarios, causing underperformance or instability. Conservative Q-learning methods penalize out-of-distribution actions to prevent overestimation. However, in sparse data regimes (e.g., Fusion with 2,000 samples), the penalty may be miscalibratedâ€”either too aggressive (preventing learning) or insufficient (causing instability from model errors).

## Foundational Learning

- **Concept: Offline Reinforcement Learning vs. Online RL**
  - Why needed here: NeoRL-2 specifically targets offline RL scenarios where environment interaction is costly. Understanding this distinction is critical for interpreting why algorithm performance differs from standard benchmarks.
  - Quick check question: Can you explain why offline RL cannot use environment interaction for exploration, and how this leads to distributional shift issues?

- **Concept: Markov Decision Process (MDP) Assumptions**
  - Why needed here: The paper explicitly identifies that delays, external factors, and constraints cause deviations from standard MDP assumptions, which is a core reason for algorithm failures.
  - Quick check question: What properties must hold for an environment to satisfy the Markov property, and how does time delay violate this?

- **Concept: Distributional Shift in Offline RL**
  - Why needed here: Distributional shift is the fundamental challenge in offline RL, and the paper shows that realistic constraints intensify this problem.
  - Quick check question: When a learned policy selects actions not well-represented in the offline dataset, what type of error does this cause in Q-value estimation?

## Architecture Onboarding

- **Component map:**
  - Task simulators (7 environments) -> Offline datasets (2,000-100,000 samples) -> Baseline algorithms (model-free and model-based) -> Evaluation protocol (online policy evaluation with 3 random seeds)

- **Critical path:**
  1. Select task and load corresponding offline dataset
  2. Train offline RL algorithm with hyperparameter tuning (see Appendix D for search spaces)
  3. Evaluate trained policy in simulator (online evaluation, 3 random seeds)
  4. Normalize scores against random policy (0) and expert policy (100)

- **Design tradeoffs:**
  - Model-free vs. model-based: Model-free methods show better stability across NeoRL-2 tasks; model-based methods have higher variance but potential for better data efficiency
  - Conservatism level: Higher conservatism prevents overestimation but may limit improvement over behavior policy
  - Data augmentation: Not explored in paper, but could help with narrow distributions from PID control

- **Failure signatures:**
  - High variance across seeds: Indicates sensitivity to initialization, common in model-based methods on Fusion/RocketRecovery tasks
  - Scores below behavior policy: Suggests over-regularization or poor hyperparameter selection
  - Negative normalized scores: Indicates catastrophic failure, seen with MOPO/RAMBO on some tasks

- **First 3 experiments:**
  1. TD3BC baseline on DMSD task: This combination shows the highest improvement (+22 points over data score) and serves as a reference for achievable gains on PID-collected data.
  2. Model-based vs. model-free comparison on Fusion task: Test data efficiency in extreme sparsity (2,000 samples) and observe variance differences between MOPO and TD3BC.
  3. Ablation on delay handling in Pipeline task: Compare standard algorithms against RNN-augmented variants to test whether memory mechanisms mitigate delay-induced non-Markovian dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
Can algorithms incorporating temporal architectures (e.g., RNNs) or explicit delay modeling significantly improve performance on tasks with high-latency transitions like Pipeline and Simglucose? Current SOTA algorithms do not explicitly model delays, and the paper shows algorithms struggle on delay-heavy tasks (no algorithm exceeds 95 normalized score). What evidence would resolve it: Systematic comparison of delay-aware algorithms versus standard methods on Pipeline/Simglucose, showing statistically significant improvements.

### Open Question 2
How can offline RL algorithms effectively learn from datasets collected via traditional control methods (e.g., PID) when such data leads to narrow distributions and potentially incorrect transition relationships? The paper notes: "Data collected using these traditional methods tend to have a narrow distribution and are difficult to model... may lead to learning incorrect transition relationships." What evidence would resolve it: Development of algorithms specifically designed for feedback-control-collected data, with demonstrated performance gains on DMSD beyond current methods.

### Open Question 3
Can offline constrained RL methods successfully learn safe policies from datasets containing only safe trajectories, without access to unsafe examples? The authors state: "in offline environments, solutions like safe RL may not be directly applicable, as real-world data might lack unsafe scenarios." What evidence would resolve it: Testing constrained offline RL algorithms (e.g., COPTIDICE) on SafetyHalfCheetah showing improved safe performance without constraint violations during deployment.

## Limitations

- Evaluation limited to seven specific simulated tasks, raising questions about generalization to broader real-world applications
- Paper identifies conservative regularization mismatch but does not systematically explore adaptive regularization schemes
- Does not explore data augmentation techniques that could potentially mitigate narrow distributions from PID control

## Confidence

- **High confidence:** The claim that delays, external factors, and safety constraints violate standard MDP assumptions is well-supported by the experimental evidence showing consistent underperformance across multiple algorithms
- **Medium confidence:** The narrow data distribution from PID control as a cause for model learning difficulties is plausible but not rigorously proven; the paper only shows correlation between PID data and poor model-based performance
- **Low confidence:** The specific hyperparameter sensitivity observed in Fusion task (2,000 samples) may be task-specific rather than a general phenomenon in sparse data regimes

## Next Checks

1. **Transfer robustness test:** Evaluate NeoRL-2 trained algorithms on additional unseen tasks with similar real-world constraints to assess generalization beyond the seven benchmark environments
2. **Adaptive regularization study:** Implement and compare dynamic conservative penalty scaling based on data density metrics to test whether the conservatism mismatch hypothesis can be systematically addressed
3. **Delay memory mechanism ablation:** Conduct controlled experiments adding memory (RNN/LSTM) components to standard offline RL algorithms on the Pipeline task to quantify the impact of non-Markovian dynamics on performance