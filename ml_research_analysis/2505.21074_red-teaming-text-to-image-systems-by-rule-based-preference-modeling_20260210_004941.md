---
ver: rpa2
title: Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling
arxiv_id: '2505.21074'
source_url: https://arxiv.org/abs/2505.21074
tags:
- rpg-rt
- images
- nsfw
- prompt
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RPG-RT, a novel red-teaming framework for\
  \ text-to-image systems that iteratively modifies prompts using a large language\
  \ model and fine-tunes it based on system feedback. The approach uses rule-based\
  \ preference modeling to guide the LLM\u2019s exploration of unknown defense mechanisms\
  \ in commercial black-box systems."
---

# Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling

## Quick Facts
- arXiv ID: 2505.21074
- Source URL: https://arxiv.org/abs/2505.21074
- Authors: Yichuan Cao; Yibo Miao; Xiao-Shan Gao; Yinpeng Dong
- Reference count: 40
- Key outcome: RPG-RT achieves 80.98% ASR vs. 34.56% for best baseline across 19 T2I systems

## Executive Summary
This paper introduces RPG-RT, a novel red-teaming framework for text-to-image systems that iteratively modifies prompts using a large language model and fine-tunes it based on system feedback. The approach uses rule-based preference modeling to guide the LLM's exploration of unknown defense mechanisms in commercial black-box systems. RPG-RT achieves significantly higher attack success rates (e.g., 80.98% ASR vs. 34.56% for the best baseline) across nineteen T2I systems with varied safety mechanisms, including three online commercial APIs, while maintaining competitive semantic similarity.

## Method Summary
RPG-RT operates through a multi-round cycle where an LLM agent generates prompt modifications, queries black-box T2I systems, and receives coarse-grained feedback (reject, SFW, or NSFW). A scoring model using CLIP embeddings is trained to decouple harmful content from benign semantics, enabling the construction of preference pairs. The LLM is then fine-tuned using Direct Preference Optimization (DPO) based on these preferences. The framework iterates this process up to 10 rounds, dynamically adapting to unknown defense mechanisms through structured feedback learning.

## Key Results
- RPG-RT achieves 80.98% attack success rate on text-img defense with nudity category, compared to 34.56% for the best baseline (DREAM)
- The framework maintains semantic similarity while increasing attack success, with FID scores comparable to or better than baselines
- RPG-RT demonstrates strong generalization, achieving 78.46% ASR on SD v2.1 with unseen prompts after training on SLD-strong
- Across all 19 T2I systems tested, RPG-RT consistently outperforms baselines in attack success rate while maintaining semantic similarity

## Why This Works (Mechanism)

### Mechanism 1: Iterative Feedback-Driven LLM Adaptation
The framework cycles through prompt modification → black-box query → feedback collection → preference modeling → DPO fine-tuning. Each iteration treats prior feedback as a prior, enabling dynamic adaptation to unknown defense mechanisms. Core assumption: feedback from both successful and failed red-team attempts contains learnable patterns about defense behavior; coarse-grained feedback can be structured via rules into useful preference signals.

### Mechanism 2: Rule-Based Preference Modeling from Coarse Feedback
Feedback is categorized into three types (TYPE-1: reject, TYPE-2: SFW, TYPE-3: NSFW) and structured via scoring rules, then binary partial orders are constructed for effective DPO training. The system defines preference rules (e.g., TYPE-3 > TYPE-1/2, higher harm/similarity score preferred) and uses a scoring model to evaluate harmfulness and semantic similarity within TYPE-2/TYPE-3 outputs.

### Mechanism 3: Scoring Model with Decoupled Harmfulness-Benign Semantics
CLIP embeddings are transformed into separate harmfulness (fn) and benign semantic (fs) components, enabling more accurate preference ranking than with raw CLIP similarity. A learnable transformation f = (fn, fs) is trained on paired SFW/NSFW images with four losses: ranking harmfulness, invariance of benign semantics to NSFW, semantic similarity alignment, and reconstruction.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Core training method for fine-tuning the LLM based on preference pairs derived from T2I system feedback
  - Quick check question: Can you explain how DPO differs from reinforcement learning with reward models?

- **Concept: CLIP Embeddings and Limitations**
  - Why needed here: Understanding why raw CLIP similarity conflates harmful and benign semantics is essential for grasping the scoring model design
  - Quick check question: Why might two images with high CLIP similarity differ significantly in harmfulness?

- **Concept: Black-Box System Feedback Types**
  - Why needed here: Recognizing TYPE-1/2/3 feedback and their implications for preference modeling is critical for implementation
  - Quick check question: Given TYPE-1 (reject) and TYPE-2 (SFW) feedback for the same original prompt, which provides more useful information for future attacks?

## Architecture Onboarding

- **Component map:** LLM Agent -> Target T2I System -> Feedback Categorizer -> Scoring Model -> Preference Modeler -> DPO Trainer
- **Critical path:** 1) LLM generates 30 modifications per prompt; 2) All modifications queried to target system; 3) Outputs categorized into TYPE-1/2/3; 4) Scoring model evaluates fn (harm) and fs (semantics) for TYPE-2/3; 5) Preference pairs constructed per rules; 6) LLM fine-tuned with DPO for 1 epoch; 7) Repeat for up to 10 iterations
- **Design tradeoffs:** More modifications per prompt (N=30) → more preference data but higher query cost; Weight c in scoring: lower c → higher ASR but lower semantic preservation; Training scoring model requires paired SFW/NSFW data; if unavailable, use manual generation or img2img synthesis
- **Failure signatures:** Low TYPE-3 rate: LLM not generating effective modifications; consider warming up with diverse prompts; All outputs rejected (TYPE-1 only): System too restrictive; may need semantic obfuscation strategies; Scoring model poor accuracy: Check loss balance; Lharm may dominate, requiring rebalancing
- **First 3 experiments:** 1) Replicate main results on text-img defense with nudity category to verify ASR and FID metrics; 2) Ablate one loss term (e.g., Linno) from scoring model to observe impact on ASR vs. semantic similarity; 3) Test transferability: train on SLD-strong, evaluate on SD v2.1 with unseen prompts from I2P (nudity 30-50%)

## Open Questions the Paper Calls Out
None

## Limitations
- Scoring model effectiveness depends on having access to paired SFW/NSFW images for training, which may not be available for all target systems
- Framework's performance on commercial APIs was tested only in black-box mode without insight into specific defense mechanisms
- Choice of Vicuna-7B as base LLM may introduce biases; effectiveness may vary with different base models or sizes

## Confidence
- **High Confidence:** Iterative DPO fine-tuning mechanism and its ability to improve attack success rates over multiple iterations
- **Medium Confidence:** Rule-based preference modeling approach, though relies on assumptions about scoring model's ability to disentangle harmful and benign semantics
- **Low Confidence:** Generalizability claims to unseen prompts and text-to-video models, based on limited experimental validation

## Next Checks
1. Evaluate the scoring model's performance across all safety categories (violence, drugs, weapons) beyond nudity, using manual verification of harmfulness rankings
2. Systematically test RPG-RT against T2I systems with explicitly documented defense mechanisms to understand which defense types the framework adapts to most effectively
3. Implement rate limiting and cost constraints typical of commercial API usage to assess the framework's practical scalability and identify optimization opportunities