---
ver: rpa2
title: 'GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection'
arxiv_id: '2504.20437'
source_url: https://arxiv.org/abs/2504.20437
tags:
- galore
- training
- memory
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GaLore 2 improves memory efficiency in LLM pre-training by projecting\
  \ gradients onto low-rank subspaces, reducing optimizer memory usage from 2mn to\
  \ 2nr. Key advancements include fast randomized SVD for efficient subspace updates\
  \ (15\xD7 speedup), integration with FSDP for distributed training, and incorporation\
  \ of low-bit quantization and tensor structures."
---

# GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection

## Quick Facts
- arXiv ID: 2504.20437
- Source URL: https://arxiv.org/abs/2504.20437
- Reference count: 15
- Key outcome: Improves memory efficiency in LLM pre-training by projecting gradients onto low-rank subspaces, reducing optimizer memory usage from 2mn to 2nr

## Executive Summary
GaLore 2 presents a memory-efficient optimization technique for large-scale language model pre-training by projecting gradients onto low-rank subspaces. The method reduces optimizer memory consumption from 2mn to 2nr while maintaining baseline-level performance. Key innovations include fast randomized SVD for efficient subspace updates (15× speedup), integration with FSDP for distributed training, and incorporation of low-bit quantization and tensor structures. Large-scale experiments demonstrate successful pre-training of Llama 7B on 500B tokens, achieving competitive results across multiple downstream task categories.

## Method Summary
GaLore 2 implements gradient low-rank projection by compressing the full gradient matrix into a lower-dimensional subspace using a projection matrix. For each weight matrix W ∈ ℝ^(m×n), the method projects the gradient G_t into a low-rank representation R_t, maintaining optimizer moments (M_t, V_t) in this compressed space rather than the full parameter space. The approach employs fast randomized SVD to efficiently compute the projection matrix, integrates with FSDP for distributed training through custom PyTorch hooks, and uses a scale factor α=0.125 to control update magnitude. Training proceeds with rank r=1024, subspace update frequency T=500 steps, and cosine learning rate schedule with 10% warmup.

## Key Results
- Successfully pre-trained Llama 7B on 500B tokens while maintaining baseline perplexity
- Reduced memory usage from 77.64GB to 72.84GB per GPU compared to AdamW + FSDP baseline
- Achieved 15× speedup in subspace updates through fast randomized SVD implementation
- Demonstrated competitive downstream performance across multiple task categories, with +0.5 points improvement on paraphrase and semantic similarity tasks

## Why This Works (Mechanism)

### Mechanism 1: Gradient Low-Rank Projection for Memory Reduction
Storing optimizer states in a low-rank subspace rather than full parameter space significantly reduces memory consumption. For a weight matrix W ∈ ℝ^(m×n), GaLore projects the full gradient G_t into a lower-dimensional space using projection matrix P_t, resulting in low-rank gradient R_t. Adam optimizer maintains moments (M_t, V_t) in compressed space (nr) rather than full space (mn), reducing memory from 2mn to 2nr. The core assumption is that LLM gradients possess inherent low-rank structure during pre-training.

### Mechanism 2: Fast Randomized SVD for Efficient Subspace Discovery
Replacing exact SVD with randomized approximation reduces computational bottleneck of subspace updates while maintaining fidelity. Standard SVD scales poorly with matrix size (20 minutes for Llama 7B), while GaLore 2 employs randomized SVD algorithm to approximate truncated decomposition needed for projection matrix P_t. The approximation error introduced is sufficiently small to maintain effective subspace for gradient descent.

### Mechanism 3: FSDP Integration via Per-Layer Update Hooking
Integrating low-rank projection directly into distributed training pipeline allows memory savings to scale across multiple GPUs. The system utilizes PyTorch hook within FSDP where after gradient is reduce-scattered, GaLore projects it, updates weights, and immediately discards full-rank gradient. This synchronization overhead and replication of SVD results across devices are manageable without negating memory benefits of sharding.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed: GaLore relies on SVD to identify "principal directions" of gradient matrix to construct projection P_t
  - Quick check: Given gradient matrix G, which components of its SVD (U, S, V) are used to construct projection matrix P_t in GaLore?

- **Concept: Adam Optimizer State Mechanics**
  - Why needed: Memory reduction claim rests on shrinking First Moment (M_t) and Second Moment (V_t) estimates
  - Quick check: In Adam update rule, how does dimensionality of moment estimates change when gradient is projected from rank n to rank r?

- **Concept: Fully Sharded Data Parallel (FSDP)**
  - Why needed: Paper claims large-scale scalability by combining GaLore with FSDP
  - Quick check: How does FSDP differ from standard DDP in terms of where full model parameters reside during forward/backward pass?

## Architecture Onboarding

- **Component map:** Weight Matrix W_t & Gradient G_t -> Fast Randomized SVD module -> Projector compresses G_t → R_t using P_t -> Low-rank Adam (M_t, V_t ∈ ℝ^(n×r)) -> Updator reprojects update N_t → tilde G_t and applies to W_t -> FSDP hooks manage sharding and gradient lifecycle

- **Critical path:**
  1. FSDP Reduce-Scatter (Aggregate Gradients)
  2. Check Update Frequency: If t mod T == 0, trigger Fast SVD to update P_t
  3. Project Gradient: R_t = P_t^T G_t
  4. Optimizer Step: Update low-rank moments and compute N_t
  5. Reproject: tilde G_t = α P_t N_t
  6. Apply Update & Discard Gradient

- **Design tradeoffs:**
  - Rank (r) vs. Fidelity: Lower r saves more memory (linear scaling) but risks losing gradient information
  - Update Frequency (T) vs. Stability: High T reduces compute overhead but may cause optimizer to "overfit" to outdated subspace
  - Projection Type: Randomized SVD is faster but introduces sign indeterminacy

- **Failure signatures:**
  - Loss Spikes: Likely caused by SVD sign indeterminacy or aggressive update frequency
  - Memory Stagnation: If "gradient discard" hook in FSDP fails, peak memory remains high
  - Performance Plateau: If rank r is too low, model may fail to refine, resulting in higher final perplexity

- **First 3 experiments:**
  1. Sanity Check (Single GPU): Train small Llama 60M on C4 with standard SVD vs. Fast Randomized SVD, verify validation perplexity curves overlap
  2. Memory Profiling (Distributed): Launch Llama 7B pre-training on 2 GPUs with FSDP, compare peak memory allocation between AdamW baseline and GaLore 2
  3. Scaling Stress Test: Run short training window (1B tokens) on Llama 7B with varying subspace update frequencies (T ∈ {200, 500, 1000}) to identify performance degradation points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical mechanics behind optimal subspace updates in GaLore, and how do they affect convergence?
- Basis: Page 3 states "several key challenges and open questions remain unresolved, such as the mechanics behind subspace updates"
- Why unresolved: Paper empirically validates GaLore 2's effectiveness but lacks theoretical analysis of how low-rank subspace projection affects optimization trajectory
- What evidence would resolve: Formal convergence analysis relating subspace update frequency, rank selection, and optimization landscape properties

### Open Question 2
- Question: Can GaLore 2 maintain performance when scaling beyond 500B tokens to 1+ trillion tokens?
- Basis: Page 2 notes real-world pre-training uses "50 billion to over 1 trillion" tokens while experiments only reached 500B
- Why unresolved: Validation loss oscillation observed around 380B tokens raises questions about long-term stability
- What evidence would resolve: Pre-training experiments extending to 1T+ tokens with consistent loss curves

### Open Question 3
- Question: How does sign indeterminacy in randomized SVD affect training stability with more frequent subspace updates?
- Basis: Page 4 notes "this makes frequent subspace updates in GaLore unstable" but claims issue is negligible at 200-500 step intervals
- Why unresolved: Paper dismisses problem for moderate frequencies but doesn't test whether more aggressive update schedules would introduce instability
- What evidence would resolve: Systematic ablation across subspace update frequencies measuring training stability and convergence speed

## Limitations

- The core memory reduction claim assumes LLM gradients possess inherent low-rank structure during pre-training, which requires empirical validation
- FSDP integration details lack specifics on how projection matrix P_t is synchronized across devices in distributed settings
- Downstream evaluation results show marginal improvements (+0.5 points) that are not statistically validated against baseline

## Confidence

- **High Confidence:** Theoretical mechanism of gradient low-rank projection for memory reduction is well-established in literature and mathematically sound
- **Medium Confidence:** Randomized SVD implementation likely achieves claimed speedup, but internal validation is insufficient to confirm no accuracy loss
- **Low Confidence:** FSDP integration details and statistical significance of downstream evaluation results are unclear from paper

## Next Checks

1. Validate Randomized SVD Fidelity: Implement fast randomized SVD on small Llama model (60M/130M) and compare validation perplexity curves against exact SVD baseline with error bars
2. Profile Distributed Memory Usage: Run Llama 7B pre-training on multiple GPUs with FSDP and measure peak memory allocation per GPU for both AdamW and GaLore 2
3. Test FSDP Synchronization Overhead: Measure communication overhead of synchronizing projection matrix P_t across devices during distributed training to ensure it doesn't negate memory benefits of sharding