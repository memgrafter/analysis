---
ver: rpa2
title: 'DEAS: DEtached value learning with Action Sequence for Scalable Offline RL'
arxiv_id: '2510.07730'
source_url: https://arxiv.org/abs/2510.07730
tags:
- learning
- value
- action
- tasks
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEAS is an offline RL framework that learns value functions using
  action sequences through the options framework, enabling horizon reduction without
  explicit goal conditioning. The method addresses value overestimation in offline
  RL by employing detached value learning that decouples critic training from the
  actor, steering estimates toward high-return in-distribution actions.
---

# DEAS: DEtached value learning with Action Sequence for Scalable Offline RL

## Quick Facts
- arXiv ID: 2510.07730
- Source URL: https://arxiv.org/abs/2510.07730
- Reference count: 31
- Primary result: Achieves up to 88% success rate on OGBench tasks, outperforming previous best methods

## Executive Summary
DEAS introduces a novel offline reinforcement learning framework that addresses key challenges in long-horizon decision making through action sequence learning and detached value estimation. By decoupling value learning from policy optimization and incorporating action sequences via the options framework, DEAS effectively mitigates overestimation bias while enabling scalable planning over extended horizons. The method demonstrates significant improvements across both standard OGBench benchmarks and real-world manipulation tasks, particularly in enhancing Vision-Language-Action models for complex kitchen environments.

## Method Summary
DEAS operates through a dual-network architecture where the critic learns action sequence values independently of the actor policy. The framework leverages the options framework to learn over action sequences rather than individual steps, reducing effective planning horizon while maintaining long-term dependencies. Detached value learning ensures the critic is trained solely on high-return actions within the data distribution, preventing overestimation that typically plagues offline RL methods. The system incorporates distributional RL objectives to capture uncertainty in value estimates, further stabilizing learning in the offline setting.

## Key Results
- Achieves 88% success rate on OGBench tasks versus 69% for previous best methods
- Improves Vision-Language-Action model success from 12% to 25% on RoboCasa Kitchen tasks
- Enhances real-world manipulation success from 64% to 78%
- Ablation studies confirm importance of action sequence length, dual discount factors, and distributional RL integration

## Why This Works (Mechanism)
The effectiveness of DEAS stems from its ability to learn value functions that accurately represent long-horizon returns without suffering from distributional shift. By using action sequences, the method reduces the effective planning horizon while preserving temporal dependencies crucial for complex tasks. The detached value learning mechanism ensures the critic remains grounded in the data distribution, avoiding the optimistic bias that typically emerges when actors explore beyond available demonstrations. The distributional RL component captures uncertainty in value estimates, providing more robust learning signals.

## Foundational Learning
- **Options Framework**: Hierarchical abstraction for temporally extended actions; needed to handle long horizons without exponential complexity; quick check: verify sequence lengths improve planning efficiency
- **Distributional RL**: Models full value distribution rather than point estimates; needed for uncertainty quantification in offline settings; quick check: examine value distribution variance across tasks
- **Detachment Principle**: Separates value learning from policy optimization; needed to prevent overestimation bias; quick check: compare critic values when actor is frozen versus trained
- **Offline RL Challenges**: Distributional shift and overestimation; needed context for method motivation; quick check: measure KL divergence between learned and dataset action distributions
- **Vision-Language-Action Integration**: Multimodal policy learning; needed for real-world applicability; quick check: validate cross-modal alignment in value predictions

## Architecture Onboarding

**Component Map**: State -> Encoder -> Sequence Predictor -> Value Network -> Actor Network

**Critical Path**: The critic learns action sequence values through detached training, providing stable targets for the actor. The actor selects action sequences that maximize estimated returns while staying within the data distribution. The encoder processes multimodal inputs for Vision-Language-Action applications.

**Design Tradeoffs**: Action sequence length vs. computational complexity; detachment strength vs. policy expressiveness; distributional vs. point estimate approaches. The method sacrifices some online adaptability for improved offline stability and scalability.

**Failure Signatures**: Overestimation manifests as divergence between critic and actor values; under-exploration appears as policy collapse to dataset modes; sequence prediction errors accumulate over longer horizons. Monitoring value distribution collapse and policy entropy helps diagnose issues early.

**First Experiments**:
1. Validate action sequence prediction accuracy on held-out data
2. Compare value estimates with and without detachment across different discount factors
3. Test distributional RL impact by comparing prediction intervals to actual returns

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to continuous action spaces remains uncertain despite discrete domain success
- Computational overhead from action sequence processing may limit real-time applications
- Potential misalignment between decoupled critic and actor in highly context-dependent environments
- Integration complexity of distributional RL objectives may affect stability across diverse task types

## Confidence
- OGBench performance claims: **High**
- Vision-Language-Action model improvements: **Medium**
- Theoretical framework robustness: **Medium**
- Scalability to arbitrary domains: **Low**

## Next Checks
1. Test DEAS on continuous control tasks with varying action space dimensions to evaluate scalability beyond discrete environments
2. Implement ablation studies removing the distributional RL component to isolate its contribution to performance gains
3. Evaluate DEAS's sample efficiency and convergence stability when trained on smaller datasets to assess robustness to data quality variations