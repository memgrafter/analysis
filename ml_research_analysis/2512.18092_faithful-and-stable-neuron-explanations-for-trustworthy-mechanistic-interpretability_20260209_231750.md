---
ver: rpa2
title: Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability
arxiv_id: '2512.18092'
source_url: https://arxiv.org/abs/2512.18092
tags:
- neuron
- concept
- identification
- probe
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of quantifying the trustworthiness
  of neuron identification methods in mechanistic interpretability. It focuses on
  two key challenges: (1) faithfulness, i.e., whether identified concepts truly capture
  neuron functions, and (2) stability, i.e., consistency across probing datasets.'
---

# Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2512.18092
- Source URL: https://arxiv.org/abs/2512.18092
- Authors: Ge Yan; Tuomas Oikarinen; Tsui-Wei; Weng
- Reference count: 40
- Key outcome: Theoretical analysis of faithfulness and stability in neuron identification methods for mechanistic interpretability

## Executive Summary
This paper addresses fundamental challenges in mechanistic interpretability by providing the first theoretical guarantees for neuron identification methods. The authors tackle two key problems: faithfulness (whether identified concepts truly capture neuron functions) and stability (consistency across probing datasets). They approach neuron identification as an inverse machine learning problem, enabling them to derive generalization bounds that show the gap between empirical and true similarity scales as O(1/√n). The work introduces a bootstrap ensemble method to quantify stability and construct concept prediction sets with guaranteed coverage probability.

## Method Summary
The authors view neuron identification as an inverse machine learning problem, where concepts are learned from neuron activations rather than the other way around. This perspective allows them to apply statistical learning theory to derive generalization bounds for faithfulness metrics. For stability, they propose a bootstrap ensemble method that quantifies consistency across different probing datasets. The method constructs concept prediction sets with guaranteed coverage probability, providing a principled way to assess when neuron identifications are reliable. The approach is validated through experiments on both synthetic data (where ground truth is known) and real neural network models.

## Key Results
- Derived theoretical generalization bounds showing faithfulness gap scales as O(1/√n) for similarity metrics like accuracy, AUROC, and IoU
- Proposed bootstrap ensemble method for quantifying stability with guaranteed coverage probability
- Experimental validation demonstrating probing dataset size and concept frequency significantly impact performance
- Showed bootstrap method produces stable, diverse concept sets that improve reliability of neuron identifications

## Why This Works (Mechanism)
The paper's approach works by reframing neuron identification as an inverse learning problem, allowing application of statistical learning theory. This theoretical foundation enables rigorous analysis of both faithfulness and stability properties. The bootstrap ensemble method provides a practical way to quantify uncertainty in neuron identifications while maintaining theoretical guarantees.

## Foundational Learning
- Generalization bounds in statistical learning theory: Why needed - provides theoretical foundation for analyzing faithfulness; Quick check - verify assumptions about concept distributions
- Bootstrap methods for uncertainty quantification: Why needed - enables practical stability assessment; Quick check - validate coverage probability guarantees
- Concept representation in neural networks: Why needed - fundamental to understanding neuron functions; Quick check - assess representational complexity

## Architecture Onboarding

Component map:
Probing dataset -> Bootstrap ensemble -> Concept prediction sets -> Stability assessment
Neural network -> Activation extraction -> Neuron identification -> Faithfulness bounds

Critical path:
1. Extract neuron activations from probing dataset
2. Apply bootstrap ensemble to quantify stability
3. Compute faithfulness bounds using generalization theory
4. Generate concept prediction sets with guaranteed coverage

Design tradeoffs:
- Computational cost vs. theoretical guarantees
- Dataset size requirements vs. accuracy of bounds
- Model complexity vs. interpretability

Failure signatures:
- Violation of statistical assumptions leading to invalid bounds
- Insufficient dataset size causing unstable estimates
- Complex concept interactions exceeding theoretical framework

First experiments:
1. Test faithfulness bounds on synthetic data with known ground truth
2. Validate bootstrap ensemble coverage probability on controlled datasets
3. Compare stability across different probing dataset sizes and distributions

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of theoretical bounds to more complex concept relationships, the impact of non-linear interactions between neurons on faithfulness measures, and the practical limitations of bootstrap methods when dealing with high-dimensional concept spaces.

## Limitations
- Theoretical bounds rely on assumptions about concept distributions that may not hold in practice
- Stability analysis assumes representative probing datasets, which may not be true for small or biased datasets
- Experiments are limited in scope, focusing on synthetic data and single real dataset

## Confidence
- Theoretical generalization bounds: High
- Bootstrap ensemble method: Medium
- Experimental validation: Medium

## Next Checks
1. Test theoretical bounds on larger, more diverse real-world datasets to assess practical validity and identify assumption violations
2. Conduct ablation studies on bootstrap ensemble method to determine optimal parameters and assess sensitivity to dataset characteristics
3. Extend analysis to more complex concept relationships and multi-neuron interactions to evaluate scalability and limitations