---
ver: rpa2
title: Membership and Memorization in LLM Knowledge Distillation
arxiv_id: '2508.07054'
source_url: https://arxiv.org/abs/2508.07054
tags:
- privacy
- min-k
- data
- teacher
- membership
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Knowledge distillation (KD) is widely used to compress large language\
  \ models (LLMs) by transferring knowledge from a large teacher to a smaller student\
  \ model, often to reduce computational costs and improve deployment efficiency.\
  \ However, the privacy risks of KD\u2014particularly membership inference and memorization\
  \ leakage from teacher training data\u2014have not been thoroughly quantified."
---

# Membership and Memorization in LLM Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2508.07054
- **Source URL**: https://arxiv.org/abs/2508.07054
- **Reference count**: 40
- **Primary result**: Knowledge distillation (KD) from LLMs transfers significant membership and memorization privacy risks to student models, with risk levels varying by KD technique and model components.

## Executive Summary
Knowledge distillation (KD) is widely used to compress large language models (LLMs) by transferring knowledge from a large teacher to a smaller student model. However, the privacy risks of KD—particularly membership inference and memorization leakage from teacher training data—have not been thoroughly quantified. This work systematically evaluates six state-of-the-art KD techniques (KD, SeqKD, GKD, ImitKD, MiniLLM, DistiLLM) on three LLM families (GPT-2, OPT, LLAMA-2) across seven NLP tasks, using seven membership inference attacks and data reconstruction methods. The study reveals that all KD techniques carry significant membership and memorization privacy risks from the teacher to the student, with the extent varying across techniques. For instance, AUC scores for membership inference can exceed 0.70, and students can memorize 11.35% of the samples the teacher memorizes. Importantly, memorization and membership risks show low agreement, indicating that samples vulnerable to one attack may not be vulnerable to the other. The research also demonstrates that privacy risks differ significantly across model blocks and are influenced by KD components such as loss functions and the use of student-generated data. Decreasing student model size can reduce privacy leakage but at the cost of utility. These findings highlight the need for privacy-aware KD designs and underscore the importance of considering both membership and memorization risks in LLM deployment.

## Method Summary
The paper evaluates six KD techniques across three LLM families (GPT-2, OPT, LLAMA-2) on seven NLP tasks. They use seven membership inference attacks (LOSS, Zlib, Min-K%, Min-K%++, StableLM, Pretrain-Ref, MoPe) and data reconstruction methods to quantify privacy risks. The study includes ablation experiments varying loss functions and student-generated data ratios, plus a per-block analysis framework to identify vulnerable transformer blocks.

## Key Results
- All KD techniques transmit significant membership and memorization privacy risks from teacher to student models
- AUC scores for membership inference attacks can exceed 0.70, indicating strong privacy leakage
- Students can memorize 11.35% of samples the teacher memorizes
- Memorization and membership risks show low agreement, indicating distinct vulnerability patterns
- Privacy risks vary significantly across model blocks and are influenced by KD components like loss functions and SGO usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The choice of knowledge distillation (KD) loss function can influence the degree of membership privacy leakage from teacher to student models.
- Mechanism: The paper reports that loss functions which encourage the student to closely mimic the teacher's full output distribution (e.g., standard KL divergence) may transmit more membership signal than losses that focus on high-probability regions (e.g., Reverse KL). This is conditional on the specific dataset and model architectures tested.
- Core assumption: The measured membership inference attack (MIA) performance is a valid proxy for the transmission of private membership information.
- Evidence anchors:
  - [abstract]: "the extent of privacy risks varies across different KD techniques."
  - [section 5, Ablation Study]: "KL leads to the highest MIA... On the contrary, RKL has the lowest AUCs."
  - [corpus]: Related work "From Teacher to Student: Tracking Memorization Through Model Distillation" explores similar KD privacy dynamics but is not directly cited in the provided text.
- Break condition: This pattern may not hold if the teacher model is trained with strong differential privacy, or if the student model capacity is extremely limited.

### Mechanism 2
- Claim: Incorporating student-generated output (SGO) into the distillation training data may increase vulnerability to membership inference attacks.
- Mechanism: The study's ablation suggests that as the ratio of SGO in the training mix increases, MIA performance (AUC) also increases. A potential cause is that SGO, derived from the student's interaction with the teacher's supervision, may inadvertently reinforce patterns correlated with the teacher's private training data.
- Core assumption: The correlation between higher SGO ratios and higher MIA AUC implies a causal or contributory link to privacy risk.
- Evidence anchors:
  - [section 5, Ablation Study]: "Privacy risk increases as the ratio of student-generated data increases."
  - [Figure 4 description]: Shows AUC rising from ~0.69 to ~0.76 on GPT-2 Large as SGO ratio goes from 0 to 1.
  - [corpus]: The corpus signals mention related work but do not provide a strong external anchor for this specific finding.
- Break condition: This effect may diminish if the SGO is filtered, the generation process is substantially modified, or the base public dataset is significantly larger and more diverse.

### Mechanism 3
- Claim: Privacy leakage is not uniformly distributed across the transformer blocks of a large language model.
- Mechanism: The paper introduces a per-block analysis framework by perturbing individual blocks and measuring the resulting output variance (similar to the MoPe attack). They observe that AUC scores for MIAs can vary significantly by block (e.g., from ~0.50 to over 0.65 in GPT-2 Large), suggesting that some blocks store more membership-sensitive information than others.
- Core assumption: The sensitivity of a block to parameter perturbation, as measured by the framework, correlates with its contribution to privacy leakage.
- Evidence anchors:
  - [abstract]: "the privacy risk varies across different blocks by a large margin."
  - [section 7, Per-Block Privacy Risk Analysis]: "the privacy leakage of different blocks varies by a large margin."
  - [corpus]: No strong external confirmation found in the provided corpus signals for this specific block-level analysis.
- Break condition: This analysis is model-architecture specific. The location of "vulnerable" blocks (e.g., deep vs. middle) may change with different model families or training regimes.

## Foundational Learning

- Concept: **Membership Inference Attacks (MIAs) on LLMs**
  - Why needed here: The paper's central thesis is measuring how KD transfers the risk of MIAs from a teacher to a student model. You must understand MIAs as methods to determine if a specific data sample was part of a model's training set.
  - Quick check question: How does a reference-based MIA (like Pretrain-Ref) differ from a single-model black-box attack (like LOSS)?

- Concept: **Knowledge Distillation (KD) in LLMs**
  - Why needed here: The entire study evaluates privacy risks *inherent* in the KD process. You need to grasp that KD involves training a smaller "student" model using the outputs of a larger "teacher" model, often on a separate public dataset.
  - Quick check question: What are the two key components of a KD technique that the paper systematically analyzes for their impact on privacy?

- Concept: **Memorization vs. Membership Leakage**
  - Why needed here: A critical finding is the low agreement between memorization (reproducing training data verbatim) and membership leakage (vulnerability to MIAs). They are presented as distinct, though related, privacy risks.
  - Quick check question: According to the paper, can a sample be vulnerable to an MIA but not memorized by the student? What evidence supports this?

## Architecture Onboarding

- Component map:
  - Teacher Model (GPT-2, OPT, LLAMA-2) trained on private data (DPrivate)
  - Student Model (smaller LLM) trained via KD using teacher's supervision on public data (DPublic)
  - KD Core Components: loss function (LKD) and distillation dataset (DKD), which may include teacher or student-generated outputs
  - Attack Suite: seven MIAs (LOSS, Zlib, Min-K%, Min-K%++, StableLM, Pretrain-Ref, MoPe) and data reconstruction methods
  - Per-Block Analysis Framework: method to isolate and perturb individual transformer blocks to measure privacy leakage

- Critical path:
  1. Train Teacher: Train a teacher LLM on a private dataset
  2. Perform Distillation: Use one of the six KD techniques to train a student model, varying loss functions and data sources
  3. Evaluate Privacy Risks: Launch the suite of MIAs and data reconstruction attacks against the student to quantify membership and memorization leakage inherited from the teacher
  4. Analyze Components: Conduct ablations on KD loss types and student-generated data ratios to identify risk factors
  5. Drill Down (Optional): Apply the per-block analysis framework to identify the most privacy-leaky transformer blocks within a model

- Design tradeoffs:
  - Privacy vs. Utility vs. Efficiency: Reducing student model size improves efficiency and can lower privacy risk (AUC), but significantly harms model utility on downstream tasks
  - Loss Function Choice: Standard KL divergence may yield higher student utility but also higher privacy risk compared to Reverse KL
  - Training Data Strategy: Using teacher or student-generated outputs can stabilize KD and improve utility but appears to increase privacy vulnerability

- Failure signatures:
  - Unexpected High MIA AUC: Student model yielding MIA AUC scores significantly above 0.60 (e.g., over 0.70-0.80) indicates substantial membership privacy leakage
  - High Memorization Overlap: If the student memorizes a large percentage of the exact samples the teacher memorized (~11.35%), it indicates direct verbatim data leakage
  - Utility Collapse: Dramatic drop in Rouge-L or GPT-evaluated scores on downstream tasks indicates the student model failed to effectively distill knowledge

- First 3 experiments:
  1. Baseline Privacy Audit: For a chosen teacher-student pair (e.g., GPT2-XL -> GPT2-Large), run all seven MIAs and a basic data reconstruction attack after distillation with standard KD (KL loss) to establish a privacy risk baseline
  2. Loss Function Ablation: Train two student models using identical settings except for the loss function: one with standard KL divergence and one with Reverse KL. Compare their MIA AUC scores and utility metrics to quantify the privacy-utility tradeoff
  3. Per-Block Sensitivity Analysis: Using the provided framework (Algorithm 1), perform a per-block privacy analysis on a trained student model. Identify the block(s) with the highest AUC to understand the internal distribution of privacy risk

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on specific model families (GPT-2, OPT, LLAMA-2) and attack methods may not generalize to all LLM architectures
- Study does not fully explore effectiveness of potential mitigation strategies beyond reducing student model size
- Does not examine how privacy risks evolve with continued training or fine-tuning of student models

## Confidence
- **High Confidence**: Core finding that KD techniques transmit membership and memorization privacy risks is well-supported by systematic evaluation
- **Medium Confidence**: Per-block privacy risk analysis reveals significant variation but is model-architecture specific
- **Medium Confidence**: Claim that privacy risks vary across KD techniques is supported but depends on specific attack implementations

## Next Checks
1. Cross-Architecture Validation: Replicate per-block privacy risk analysis on different LLM architectures (e.g., BERT, T5) to determine generalizability
2. Mitigation Strategy Evaluation: Systematically evaluate differential privacy training on teacher model before distillation, measuring resulting privacy-utility tradeoffs
3. Temporal Dynamics Analysis: Conduct longitudinal study tracking how membership inference and memorization risks evolve as student model continues training post-distillation