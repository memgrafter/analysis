---
ver: rpa2
title: 'DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on
  the Edge'
arxiv_id: '2510.16716'
source_url: https://arxiv.org/abs/2510.16716
tags:
- distillation
- distillock
- knowledge
- data
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DistilLock, a TEE-assisted framework that enables
  privacy-preserving knowledge distillation on edge devices while protecting both
  user data and model intellectual property. The core innovation is using model obfuscation
  with lightweight TEE authorization to prevent unauthorized access to proprietary
  model weights during distillation.
---

# DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge

## Quick Facts
- arXiv ID: 2510.16716
- Source URL: https://arxiv.org/abs/2510.16716
- Reference count: 40
- Primary result: Reduces surrogate model attack accuracy to 0.98× black-box baseline vs 1.69× for white-box attacks, with only 0.44-1.11% TEE FLOP overhead

## Executive Summary
DistilLock is a TEE-assisted framework that enables privacy-preserving knowledge distillation on edge devices while protecting both user data and model intellectual property. The system uses model obfuscation combined with lightweight TEE authorization to prevent unauthorized access to proprietary model weights during distillation. By requiring a hardware-protected authorization step that applies secret permutations to input embeddings, DistilLock ensures that only authorized distillation processes can extract meaningful knowledge from the teacher model. Experimental results demonstrate strong protection against model extraction attacks while maintaining practical efficiency for edge deployment.

## Method Summary
DistilLock operates by transforming teacher model weights using permutation matrices before deployment to untrusted accelerators. The original weights W are obfuscated to W' through π^T W or Wπ transformations depending on their position relative to activations. During inference, user inputs must pass through a TEE where they are permuted and combined with a one-time pad (OTP) before entering the obfuscated model. Without this authorization, the model produces semantically random logits because subsequent layers expect permuted activations. The TEE handles only the lightweight embedding transformation (0.44-1.11% of total FLOPs), while all transformer computations run on GPU with obfuscated weights.

## Key Results
- Reduces surrogate model attack accuracy to 0.98× black-box baseline (versus 1.69× for white-box attacks)
- Requires only 0.44-1.11% of total FLOPs to run inside TEE, significantly less than prior defenses (3.55-16.40%)
- Successfully prevents unauthorized knowledge distillation while maintaining authorized distillation performance (HellaSwag accuracy improves from 0.50 to 0.55)

## Why This Works (Mechanism)

### Mechanism 1: TEE-Gated Permutation Authorization
DistilLock prevents unauthorized knowledge distillation by requiring hardware-protected authorization that applies secret permutation matrices to input embeddings before any meaningful computation occurs. The TEE holds encrypted permutation matrices (π_emb, π) and OTP, transforming user input before it enters the obfuscated model on GPU. Without this transformation, the obfuscated teacher produces random logits because subsequent layers expect permuted activations. The core assumption is TEE remains uncompromised.

### Mechanism 2: Permutation-Equivariant Weight Obfuscation
Obfuscated weights remain functionally equivalent to original weights when combined with correct permutation, but appear random to unauthorized observers. All weight matrices are transformed: W' = π^T W or W' = Wπ depending on position relative to activations. LayerNorm/RMSNorm parameters are similarly permuted. This preserves mathematical equivalence because activations carry the same permutation throughout the forward pass, and π^T π = I cancels at classifier output.

### Mechanism 3: Compute Partitioning with Minimal TEE Overhead
DistilLock achieves <1.2% TEE FLOP overhead by restricting TEE computation to only embedding authorization while offloading all transformer computations to untrusted accelerators. The embedding transformation (h+m)π_emb and OTP removal are the only TEE operations. All transformer blocks with obfuscated weights run on GPU, with permutation propagating automatically through layers requiring no further TEE intervention.

## Foundational Learning

- **Knowledge Distillation Fundamentals**
  - Why needed: DistilLock operates during KD where teacher guides student via soft label distributions. Understanding L_KD = α·L_CE + β·L_KL is essential.
  - Quick check: Can you explain why soft labels (temperature-scaled logits) provide more information than hard labels for student learning?

- **Trusted Execution Environments (TEEs)**
  - Why needed: The entire security model depends on TEE guarantees—confidentiality and integrity of code/data within enclaves.
  - Quick check: What attacks does Intel SGX protect against, and what remains outside its threat boundary?

- **Transformer Architecture with Layer Normalization**
  - Why needed: The permutation-equivariance proof relies on understanding how LayerNorm/RMSNorm interact with permuted activations and parameters.
  - Quick check: For RMSNorm, why is γπ required when input is xπ? What happens if you apply γ instead?

## Architecture Onboarding

- **Component map:**
  - User Input -> TEE (OTP + π_emb permutation) -> GPU (obfuscated weights) -> TEE (π permutation + OTP removal) -> Output logits

- **Critical path:**
  1. Input h (one-hot) → TEE: Apply OTP m, compute (h+m)π_emb
  2. TEE → GPU: h' = (h+m)π_emb
  3. GPU: Multiply by W'_emb, return (h+m)W_emb to TEE
  4. TEE: Remove OTP, apply π → x' = hW_embπ
  5. GPU: All transformer blocks with obfuscated weights
  6. GPU: Final classifier W'_cls produces qT = y_last π π^T W_cls = y_last W_cls (correct logits)

- **Design tradeoffs:**
  - Security vs. Efficiency: Full model in TEE is maximally secure but 50× slower; DistilLock trades slight security risk for <1.2% overhead
  - OTP entropy vs. computation: Paper limits OTP to k-hot sparse vectors (k ≥ ⌈log₂(h)⌉) to convert matrix multiplication to vector summation
  - Single vs. repeated authorization: TransLinkGuard authorizes at each block (more secure but high overhead); DistilLock authorizes once (efficient but relies on permutation propagation correctness)

- **Failure signatures:**
  - Unauthorized distillation: Student accuracy collapses (e.g., HellaSwag drops from 0.50 to ~0.35-0.40)
  - Incorrect permutation application: Output logits become meaningless
  - OTP entropy too low: Security degrades; sparse OTP must have k ≥ ⌈log₂(h)⌉
  - TEE memory overflow: Large embedding vocab exceeds enclave memory

- **First 3 experiments:**
  1. Reproduce distillation lockdown: LLaMA3.1-8B → LLaMA3.2-1B distillation with authorized vs. unauthorized (random π) settings on HellaSwag
  2. Surrogate attack baseline: Initialize surrogate with obfuscated weights, train on held-out dataset, measure accuracy vs. black-box/white-box baselines
  3. TEE overhead profiling: Measure FLOPs inside TEE vs. total FLOPs for Qwen2-1.5B, verify <1.2% overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DistilLock perform when extended to ARM TrustZone or emerging GPU-based TEEs?
- Basis: "our current implementation is built on Intel SGX and has not been extended to other trusted execution environments such as ARM TrustZone or emerging GPU TEEs."
- Why unresolved: Different TEE architectures have varying enclave abstractions, memory constraints, and performance characteristics.
- What evidence would resolve it: Implementation and evaluation on ARM TrustZone and NVIDIA H100 GPU TEE, measuring security guarantees and computational overhead.

### Open Question 2
- Question: Can advanced surrogate training techniques or side-channel leakage attacks compromise the permutation-based obfuscation guarantees?
- Basis: "future adaptive attacks (e.g., side-channel leakage or advanced surrogate training) may weaken its guarantees."
- Why unresolved: Security analysis only addresses brute-force permutation guessing; structured attacks remain unexplored.
- What evidence would resolve it: Robustness evaluation against timing-based side-channel attacks, power analysis, and optimization-based permutation recovery algorithms.

### Open Question 3
- Question: Does DistilLock maintain its security and efficiency properties under advanced knowledge distillation variants?
- Basis: "our evaluation is limited to standard KD settings and datasets, leaving the effectiveness of DistilLock under advanced distillation variants and more complex scenarios as an open question."
- Why unresolved: Advanced KD methods use different loss functions and training dynamics that may interact differently with obfuscated teacher outputs.
- What evidence would resolve it: Experiments applying DistilLock to state-of-the-art distillation algorithms, measuring both student performance and attack resistance.

## Limitations

- Security guarantees depend entirely on TEE integrity; side-channel attacks are acknowledged but not analyzed
- Limited threat model scope—focuses only on surrogate model extraction, not data-free distillation or activation-based reconstruction
- OTP construction and distribution mechanism is underspecified, raising questions about secure implementation

## Confidence

- **High (★★★)**: Experimental results showing TEE efficiency (0.44-1.11% FLOPs) are reproducible and well-documented
- **Medium (★★)**: Security analysis against surrogate attacks (0.98× relative accuracy) is convincing but narrow
- **Low (★)**: Claims about protecting against "unauthorized distillation" lack comprehensive attack surface analysis

## Next Checks

1. **Permutation recovery vulnerability**: Run experiments where attacker has access to 1000+ authorized queries with known inputs and outputs, apply statistical analysis to attempt permutation matrix recovery

2. **Multi-vector attack surface**: Test against data-free distillation methods (DAFL, ZSKD) and activation-based reconstruction attacks, compare protection effectiveness across these orthogonal attack vectors

3. **OTP-permutation interaction analysis**: Systematically vary π matrices and measure impact on OTP security, test whether certain π patterns create statistical biases that weaken overall security guarantee