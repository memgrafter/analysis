---
ver: rpa2
title: 'Actionable AI: Enabling Non Experts to Understand and Configure AI Systems'
arxiv_id: '2503.06803'
source_url: https://arxiv.org/abs/2503.06803
tags:
- cartpole
- influences
- game
- teams
- during
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Actionable AI, a framework enabling non-experts
  to configure black-box AI systems through direct manipulation without requiring
  explanations or domain expertise. The authors conducted a 30-minute gaming experiment
  with 44 participants (22 pairs) using a modified cartpole game where players controlled
  AI behavior via interactive "influences" (circles on screen).
---

# Actionable AI: Enabling Non Experts to Understand and Configure AI Systems

## Quick Facts
- arXiv ID: 2503.06803
- Source URL: https://arxiv.org/abs/2503.06803
- Reference count: 40
- Non-experts can configure black-box AI systems through direct manipulation interfaces without explanations or domain expertise

## Executive Summary
Actionable AI is a framework that enables non-experts to configure black-box AI systems through direct manipulation interfaces without requiring explanations or domain expertise. The authors conducted a 30-minute gaming experiment with 44 participants using a modified cartpole game where players controlled AI behavior via interactive "influences" (circles on screen). Their findings demonstrate that 20 out of 22 players developed operational understanding sufficient to configure the AI toward their goals, with 14 teams achieving good performance. The study shows that non-experts can successfully configure black-box AI systems through direct interaction, developing practical understanding without needing to comprehend the underlying model.

## Method Summary
The study used a modified Cartpole environment where participants controlled an RL agent through "influences" - interactive circles that could be moved and resized on screen. The agent was trained using DiscreteSAC for 710,000 steps with 10% stochastic action probability. Participants played in pairs for 30 minutes, attempting to pass gates in a slalom-style course. The influence vector was calculated based on distance, speed, and intensity, then combined with the model's preference to determine final actions. Performance was measured by consecutive gates passed (ConsGates) and action attribution analysis.

## Key Results
- 14 out of 22 teams achieved good performance in the cartpole game
- 20 out of 22 players developed operational understanding sufficient for system configuration
- High-performing teams passed an average of 3.69 consecutive gates, with some reaching 11.29
- Participants could successfully configure the black-box AI system through direct manipulation without understanding the underlying model

## Why This Works (Mechanism)

### Mechanism 1: Operational Understanding via Direct Manipulation
Non-experts can successfully configure black-box AI systems by developing "operational understanding" through direct manipulation interfaces rather than theoretical explanations. The system provides interactive "influences" that modify the agent's behavior in real-time, allowing users to form mental models by observing immediate causal links between inputs and outputs.

### Mechanism 2: Visible Action Spaces and Constraints
Rendering the agent's action space visible and bounded allows non-experts to infer system limits and develop strategies. The interface visualizes environment boundaries and failure states, acting as a constraint mechanism that helps users perceive the "safe operating envelope" and adjust inputs to prevent the agent from exiting this envelope.

### Mechanism 3: Progressive Complexity (Curriculum Learning for Users)
Introducing system configuration through progressive levels of difficulty enables users to iterate from novice to expert strategies within short timeframes. The system starts with simple objectives and gradually introduces environmental noise, creating a curriculum that prevents cognitive overload and allows users to refine mental models incrementally.

## Foundational Learning

- **Concept: Operational Understanding vs. Explainability**
  - Why needed here: This paper fundamentally distinguishes between knowing *how* to use a system (operational) vs. knowing *why* it works (explainability)
  - Quick check question: Can you drive a car? (Yes). Can you explain the internal combustion engine or transmission logic? (Probably not). This distinction is the core thesis of Actionable AI.

- **Concept: Black-box Reinforcement Learning (RL) Agents**
  - Why needed here: The subject being configured is an RL agent that has a "policy" (mapping of states to actions) that can be perturbed by external signals
  - Quick check question: Does a black-box RL agent require you to see its code to influence it, or can you influence it by changing its environment/rewards? (The latter is the "Actionable" approach).

- **Concept: Human-in-the-Loop (HITL) Influences**
  - Why needed here: The paper utilizes a specific HITL technique called "influences" - mathematical vectors injected into the agent's decision process
  - Quick check question: Is the human controlling the agent directly (teleoperation) or nudging the agent's probability distribution toward a desired action? (Nudging via vector injection).

## Architecture Onboarding

- **Component map:** The Agent (Black Box) -> The Interface (Actionable Layer) -> The Influence Vector -> The Fusion Module
- **Critical path:** The "Fusion Module" (Algorithm 1) is the most critical logic, dictating that the final action is the maximum of the product of model preference and influence preference
- **Design tradeoffs:**
  - High Influence Strength: User can force the cart to move but risks destabilizing the pole
  - Low Influence Strength: Model maintains balance easily but may exit the screen if user cannot steer fast enough
  - Strategy Diversity: System supports multiple valid user mental models (Flanking vs. Balancing)
- **Failure signatures:**
  - The "Exit": Cart moves too fast or drifts off-screen (User influence too weak to counter model's drift)
  - The "Fall": Pole tips over (User influence too aggressive, causing acceleration model couldn't correct)
- **First 3 experiments:**
  1. Baseline Observation: Run agent with zero influences to observe "model-only" lifetime (~17 seconds)
  2. Static Stabilization: Place two medium-sized influences equidistant from cart to observe if lifetime extends
  3. Destabilization Test: Move one influence very close and large to verify cart accelerates and model struggles to recover

## Open Questions the Paper Calls Out

- Can the Actionable AI framework be generalized to non-navigation tasks using interaction mechanisms other than "influences"?
- How does environment complexity impact the efficacy of Actionable AI systems in real-world scenarios?
- How can visible action spaces be effectively designed for multi-dimensional or non-physical tasks like medical diagnosis?

## Limitations

- The framework was validated only on a simple cartpole environment and may not generalize to complex domains
- The 30-minute experiment provides evidence of short-term operational understanding but doesn't address long-term learning trajectories
- The study reports aggregate performance but doesn't analyze individual learning curves or identify predictive user characteristics

## Confidence

- **High Confidence:** The core finding that non-experts can achieve operational understanding through direct manipulation is well-supported by experimental data
- **Medium Confidence:** The specific mechanisms (visible action spaces, progressive complexity) are plausible but rely on limited evidence from a single domain
- **Low Confidence:** The generalizability claims to non-spatial, complex AI systems remain speculative with minimal supporting evidence

## Next Checks

1. **Cross-Domain Transferability Test:** Replicate the experiment with a non-spatial domain (e.g., text summarization or recommendation tuning) to validate whether the "influence" concept translates beyond visual environments.

2. **Longitudinal Learning Study:** Conduct a multi-session study tracking the same participants over weeks to determine whether operational understanding improves, plateaus, or degrades over time.

3. **Individual Difference Analysis:** Collect detailed participant background data and perform correlation analysis to identify which user characteristics predict success with Actionable AI interfaces.