---
ver: rpa2
title: 'RL''s Razor: Why Online Reinforcement Learning Forgets Less'
arxiv_id: '2509.04259'
source_url: https://arxiv.org/abs/2509.04259
tags:
- forgetting
- arxiv
- task
- learning
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Online reinforcement learning (RL) exhibits less catastrophic forgetting
  than supervised fine-tuning (SFT) when adapting models to new tasks, despite achieving
  similar new-task performance. The key insight is that forgetting is strongly predicted
  by the KL divergence between the fine-tuned and base model on the new task, regardless
  of the training algorithm.
---

# RL's Razor: Why Online Reinforcement Learning Forgets Less

## Quick Facts
- arXiv ID: 2509.04259
- Source URL: https://arxiv.org/abs/2509.04259
- Reference count: 40
- Online RL shows less catastrophic forgetting than SFT despite similar new-task performance, driven by KL-minimal convergence.

## Executive Summary
Online reinforcement learning (RL) exhibits less catastrophic forgetting than supervised fine-tuning (SFT) when adapting models to new tasks, despite achieving similar new-task performance. The key insight is that forgetting is strongly predicted by the KL divergence between the fine-tuned and base model on the new task, regardless of the training algorithm. On-policy RL updates are inherently biased toward KL-minimal solutions among those solving the new task, while SFT can converge to arbitrary distant distributions. Experiments across large language models and simulated robotics confirm RL's advantage, and an oracle SFT distribution that minimizes KL divergence shows even better preservation of prior knowledge.

## Method Summary
The paper compares online RL (GRPO) and SFT for continual learning, measuring both new-task accuracy and retention of prior knowledge. Experiments span LLMs (Qwen 2.5 3B-Instruct on math, science Q&A, tool use), robotics (OpenVLA 7B on pick-and-place), and toy MLP models (ParityMNIST). The key innovation is tracking KL divergence between fine-tuned and base models on the new task distribution, revealing that this metric predicts forgetting regardless of algorithm. Theoretical analysis shows policy gradient methods converge to KL-minimal optimal policies within the feasible policy family.

## Key Results
- RL's Razor: On-policy RL is implicitly biased toward KL-minimal solutions, reducing forgetting
- Forgetting correlates strongly with KL divergence on new task (R² = 0.96) regardless of algorithm
- On-policy sampling, not negative gradients, is the critical factor for forgetting resistance
- Oracle SFT (with perfect labels) outperforms RL but shares the KL-forgetting relationship

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Catastrophic forgetting is predicted by the KL divergence between the fine-tuned and base policy on the *new* task distribution, not necessarily by parameter drift.
- **Mechanism:** The paper identifies an "empirical forgetting law": forgetting correlates strongly with $KL(\pi_{fine} || \pi_{base})$ evaluated on the new task $\tau$. This suggests that the *distributional shift* required to learn the new task is the primary driver of interference with prior knowledge, rather than the magnitude of weight changes alone.
- **Core assumption:** The new task distribution is sufficiently representative of the model's "region of movement" to serve as a proxy for general distributional shift.
- **Evidence anchors:**
  - [Abstract]: "We find that the degree of forgetting is determined by the distributional shift, measured as the KL-divergence between the fine-tuned and base policy evaluated on the new task."
  - [Section 4]: In ParityMNIST experiments, plotting forgetting against KL divergence reveals a single functional relationship ($R^2 = 0.96$) across both RL and SFT.
  - [Corpus]: The paper "Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better" corroborates that RFT preserves prior knowledge better than SFT, linking it to data efficiency and internal structure.
- **Break condition:** If the prior tasks are mathematically orthogonal to the new task distribution in the representation space, KL divergence on the new task may cease to be a predictive proxy for interference.

### Mechanism 2
- **Claim:** On-policy RL is implicitly biased toward KL-minimal solutions among all optimal policies ("RL's Razor").
- **Mechanism:** Policy gradient methods optimize via samples drawn from the model's own current distribution. This effectively performs an alternating projection: an I-projection onto the set of optimal policies (reward maximization) followed by an M-projection onto the feasible policy set (parameter update). This iterative process naturally converges to the solution closest in KL to the initialization, unlike SFT which targets an arbitrary external distribution.
- **Core assumption:** The policy class is convex or "e-flat" enough for the projection dynamics to hold (exact for exponential families, approximate for neural nets).
- **Evidence anchors:**
  - [Section 5.2]: Theorem 5.2 states policy gradient converges to $\pi^{\dagger} = \arg \min_{\pi \in P^* \cap \Pi} D_{KL}(\pi || \pi_0)$.
  - [Section 1]: "By sampling from the model's own distribution... RL constrains learning to outputs already given non-negligible probability by the base model."
  - [Corpus]: The related paper "Reinforcement Fine-Tuning Naturally Mitigates Forgetting" supports this by showing RFT mitigates forgetting naturally without explicit regularization.
- **Break condition:** If the reward signal is extremely sparse or deceptive, causing the policy to "jump" prematurely to a distant mode rather than sliding along the KL surface.

### Mechanism 3
- **Claim:** The resistance to forgetting is driven by the *on-policy sampling* nature of RL, not merely the inclusion of negative gradients.
- **Mechanism:** The paper ablates algorithm components (GRPO, 1-0 Reinforce, SimPO, SFT). On-policy methods without negative gradients (1-0 Reinforce) mimicked GRPO's retention, while offline methods with negatives (SimPO) mimicked SFT's forgetting. The sampling source (self vs. external) is the critical variable.
- **Core assumption:** The advantage function or reward normalization does not fundamentally alter the direction of the projection significantly enough to override the sampling effect.
- **Evidence anchors:**
  - [Section 5.1]: "The critical factor is not the presence of negative gradients but the use of on-policy data."
  - [Figure 4]: Shows 1-0 Reinforce (no negatives) clustering with GRPO, while SimPO (offline + negatives) clustering with SFT regarding KL shift.
  - [Corpus]: "Exploring the Effect of Reinforcement Learning on Video Understanding" notes RL enhances reasoning capabilities, consistent with RL providing a more stable/robust optimization path.
- **Break condition:** If the offline dataset is synthetically generated to explicitly match the model's current distribution (i.e., "Oracle SFT"), the distinction vanishes.

## Foundational Learning

- **Concept:** **KL Divergence (Kullback-Leibler Divergence)**
  - **Why needed here:** It is the central metric of the paper. Understanding it as a measure of "information loss" or "distance" between two probability distributions (the base model vs. the fine-tuned model) is required to grasp the "forgetting law."
  - **Quick check question:** If Model A shifts its output probabilities slightly on the new task, while Model B shifts them massively to achieve the same accuracy, which has higher KL divergence relative to the base?

- **Concept:** **On-Policy vs. Off-Policy Learning**
  - **Why needed here:** The core distinction between the successful RL and the forgetting-prone SFT/Offline methods. One must understand that "on-policy" means sampling data from the *current* iteration of the model, creating a feedback loop that constrains movement.
  - **Quick check question:** Does "on-policy" learning use a static dataset of correct answers, or does it generate new data (some correct, some incorrect) at every training step?

- **Concept:** **I-Projection vs. M-Projection (Information Geometry)**
  - **Why needed here:** This provides the theoretical intuition for *why* RL works. I-projection (finding the closest distribution that fits constraints) explains why RL stays close to the base model.
  - **Quick check question:** Which projection type seeks to minimize the distance from a target set to a current point, essentially "pulling" the set toward the point conservatively?

## Architecture Onboarding

- **Component map:**
  - Base Model ($\pi_0$) -> Policy Buffer -> Reward Calculator -> Optimizer (e.g., GRPO/REINFORCE)
- **Critical path:** The feedback loop where the model samples $\rightarrow$ receives reward $\rightarrow$ updates. The update *must* be calculated using the log-probability of the *model's own generated samples* to trigger the KL-conservative bias.
- **Design tradeoffs:**
  - **Stability vs. Speed:** On-policy RL is computationally slower (generating samples) than SFT, but preserves prior knowledge.
  - **Performance Ceiling:** Oracle SFT (if available) theoretically beats RL, suggesting RL is a heuristic for finding the optimal KL-minimal path when we don't have the perfect dataset.
- **Failure signatures:**
  - **Reward Hacking:** The KL-minimal path is abandoned if the reward signal dominates the implicit gradient constraint.
  - **Zero-shot Collapse:** If KL divergence on the new task spikes rapidly during training, prior task accuracy (e.g., on MMLU/Hellaswag) will drop sharply.
- **First 3 experiments:**
  1. **Pareto Frontier Baseline:** Train SFT and RL (GRPO) on a target task (e.g., Math) while tracking performance on a "retention set" (e.g., Science Q&A). Plot the accuracy trade-off curve to visualize the "forgetting gap."
  2. **KL vs. Forgetting Correlation:** Log $KL(\pi || \pi_0)$ on the new task validation set every $N$ steps. Plot this against the retention set accuracy to verify the linear/quadratic "forgetting law" holds for your architecture.
  3. **Ablate Sampling:** Implement a "1-0 Reinforce" variant (reinforce only correct samples, no negative gradients) and compare it against a standard offline preference optimization (SimPO/DPO) to confirm the paper's claim that sampling source matters more than negative gradients.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the specific mechanistic explanation for why larger KL shifts on a new task disrupt prior knowledge?
  - **Basis in paper:** [explicit] The authors state, "we still lack a mechanistic account of why larger KL shifts on the new task disrupt prior knowledge—whether through representational interference, implicit capacity limits, or other dynamics."
  - **Why unresolved:** The paper establishes a strong predictive correlation between KL divergence and forgetting but stops short of identifying the causal mechanism (e.g., feature overwriting vs. capacity saturation).
  - **What evidence would resolve it:** Ablation studies isolating specific neural pathways or representations that degrade as KL divergence increases, distinguishing between interference and capacity constraints.

- **Open Question 2:** Does the "RL's Razor" principle apply to online off-policy reinforcement learning algorithms?
  - **Basis in paper:** [explicit] The authors explicitly list this as a limitation: "In addition, we didn’t study online but off-policy algorithms, which are popular in RL."
  - **Why unresolved:** The theoretical argument relies on the equivalence between on-policy sampling and rejection sampling; it is unclear if off-policy methods, which utilize data from older policies, maintain the same bias toward KL-minimal solutions.
  - **What evidence would resolve it:** Empirical comparison of off-policy methods (e.g., DQN or SAC variants) against on-policy methods (GRPO) to see if they adhere to the same KL-forgetting curve.

- **Open Question 3:** Does the KL-forgetting law hold for frontier-scale models and diverse generative domains?
  - **Basis in paper:** [explicit] The conclusion notes, "its behavior at frontier scales and in more diverse generative domains remains unknown."
  - **Why unresolved:** The experiments utilized moderate-scale models (3B parameters) and specific robotics/tasks; scaling laws may introduce unforeseen dynamics that break the quadratic fit observed in smaller models.
  - **What evidence would resolve it:** Replication of the KL divergence vs. forgetting analysis on models exceeding 70B+ parameters or in domains like video generation.

- **Open Question 4:** Does policy gradient convergence to the KL-minimal solution hold for non-binary rewards or non-convex policy families?
  - **Basis in paper:** [inferred] The theoretical proof (Theorem 5.2) relies on binary rewards and convex (exponential family) assumptions, but the authors note that neural network policy sets are not generally "e-flat."
  - **Why unresolved:** Real-world RL often involves continuous or dense rewards, and the theoretical guarantee may not strictly apply to deep network parameterizations.
  - **What evidence would resolve it:** Theoretical analysis or empirical verification showing that RL converges to KL-minimal solutions even when rewards are continuous or the policy class is highly non-convex.

## Limitations

- **Distributional assumptions:** The "forgetting law" assumes the new task distribution is representative of the model's movement, which may not hold for orthogonal tasks.
- **Architectural dependence:** The convex-like behavior assumed in the theoretical analysis may not hold for very deep or non-smooth architectures.
- **Task diversity:** Experiments focus on specific tasks (math, science Q&A, tool use, robotics pick-and-place), limiting generalizability to other domains.

## Confidence

- **High confidence:** The empirical correlation between KL divergence and forgetting (R² ≈ 0.96) is robust across experiments and task types.
- **Medium confidence:** The theoretical proof that policy gradient converges to KL-minimal solutions within the feasible policy family is sound but relies on idealized assumptions.
- **Medium confidence:** The claim that RL is implicitly biased toward KL-minimal solutions is well-supported empirically but requires further validation in more complex reward landscapes.

## Next Checks

1. **Cross-domain validation:** Test the forgetting law on a diverse set of tasks including creative writing, code generation, and multi-modal reasoning to confirm generalizability beyond the current task set.
2. **Architectural stress test:** Evaluate the KL-forgetting relationship on very deep transformers (e.g., 70B+ parameters) and non-transformer architectures to test the limits of the convex-like assumption.
3. **Reward landscape complexity:** Introduce sparse or deceptive reward signals to test whether the KL-minimal bias holds under challenging optimization conditions.