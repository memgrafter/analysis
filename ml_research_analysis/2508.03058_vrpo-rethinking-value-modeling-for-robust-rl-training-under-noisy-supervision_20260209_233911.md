---
ver: rpa2
title: 'VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision'
arxiv_id: '2508.03058'
source_url: https://arxiv.org/abs/2508.03058
tags:
- value
- training
- reward
- arxiv
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of noisy supervision in reinforcement
  learning from human feedback (RLHF), which can cause models to lose attention on
  key words during advantage estimation and harm policy stability. The core method,
  VRPO, enhances the value model with a variational information bottleneck and entropy/perplexity-guided
  auxiliary losses to absorb noise and improve semantic alignment.
---

# VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision

## Quick Facts
- arXiv ID: 2508.03058
- Source URL: https://arxiv.org/abs/2508.03058
- Reference count: 40
- Core contribution: VRPO method for robust RL training under noisy supervision

## Executive Summary
VRPO addresses the critical challenge of noisy supervision in reinforcement learning from human feedback (RLHF), where reward noise can cause value models to lose attention on key tokens and destabilize policy training. The method introduces a variational information bottleneck to the value model, alongside entropy and perplexity-guided auxiliary losses, to better absorb noise and improve semantic alignment. Tested across math reasoning, science QA, and multi-turn dialogue tasks under both rule-based and model-based noisy rewards, VRPO consistently outperforms PPO and GRPO baselines with notable gains in accuracy and dialogue performance while maintaining training stability and reducing reward hacking.

## Method Summary
VRPO enhances value modeling in RLHF by incorporating a variational information bottleneck into the value model, which constrains information flow to focus on relevant features and filter out noise. The method adds two auxiliary losses—entropy and perplexity regularization—to encourage the value model to better capture semantic nuances and align with reward signals. During training, the policy is updated via PPO while the value model is jointly optimized with these regularizations, enabling more robust advantage estimation even under noisy supervision. The approach is designed to preserve attention on key tokens, prevent reward hacking, and stabilize training across diverse tasks.

## Key Results
- Achieves 13.33% accuracy on AIME24 math reasoning task (vs. 6.67% cold start baseline)
- Improves multi-turn dialogue performance to 83.80% average across metrics
- Outperforms PPO and GRPO baselines consistently across all tested tasks and noise settings

## Why This Works (Mechanism)
VRPO works by enhancing the value model's ability to extract robust, semantically aligned features from noisy rewards. The variational information bottleneck reduces the model's sensitivity to irrelevant or noisy reward components by constraining information flow, forcing the model to focus on stable, task-relevant signals. The entropy and perplexity losses further regularize the value model, encouraging it to better capture the underlying semantics of the reward and maintain attention on key tokens. This combination results in more stable advantage estimation, reduces reward hacking, and improves overall policy robustness under noisy supervision.

## Foundational Learning
- **Variational Information Bottleneck**: Needed to filter noise from reward signals by constraining information flow in the value model; quick check: verify that model focuses on relevant features and ignores spurious correlations.
- **Entropy and Perplexity Regularization**: Needed to encourage semantic alignment and attention preservation in the value model; quick check: confirm that auxiliary losses improve attention on key tokens.
- **Advantage Estimation under Noise**: Needed to understand how noisy rewards destabilize policy training; quick check: compare stability of VRPO vs. baselines under varying noise levels.
- **Value Model Attention Mechanisms**: Needed to analyze how value models attend to key tokens; quick check: inspect attention maps before and after VRPO training.
- **RLHF Reward Model Dynamics**: Needed to contextualize the types of noise encountered in real-world RLHF; quick check: assess performance under both rule-based and model-based noise.
- **PPO Policy Optimization**: Needed to understand the interaction between value modeling and policy updates; quick check: ensure stable policy improvement with VRPO.

## Architecture Onboarding
- **Component Map**: Policy Network -> PPO Update -> Value Network (with VIB) -> Entropy/Perplexity Losses -> Reward Signal
- **Critical Path**: Policy update depends on stable advantage estimation from value model, which is improved by VIB and auxiliary losses.
- **Design Tradeoffs**: VRPO trades off some value model expressiveness for robustness to noise, potentially at the cost of fine-grained reward modeling in very clean settings.
- **Failure Signatures**: Unstable training, reward hacking, or attention collapse on key tokens indicate failure to properly regularize the value model.
- **First Experiments**:
  1. Compare token-level attention distributions in value model before and after VRPO training under noisy rewards.
  2. Perform ablation studies isolating entropy vs. perplexity regularization effects on robustness.
  3. Test VRPO under reward model drift to assess real-world robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to specific domains (math, science, dialogue), leaving generalization to other reward structures or domains open.
- Attention preservation is demonstrated only via selected examples, lacking comprehensive quantitative metrics.
- The relative importance of entropy and perplexity regularization is not fully isolated or explained.
- Does not explore performance under reward model drift or uncertainty, which are common in real-world RLHF.

## Confidence
- **High confidence** in VRPO's ability to improve stability and reduce reward hacking in tested noisy reward scenarios.
- **Medium confidence** in claims about attention preservation due to reliance on illustrative examples rather than systematic metrics.
- **Medium confidence** in overall robustness under noisy supervision, given controlled noise settings and limited real-world noise exploration.

## Next Checks
1. Systematically measure and compare token-level attention distributions in the value model before and after VRPO training across multiple noisy reward settings.
2. Perform ablation studies isolating the effects of entropy and perplexity regularization terms under varying degrees of reward noise.
3. Test VRPO's performance in the presence of reward model uncertainty or drift, simulating more realistic RLHF conditions.