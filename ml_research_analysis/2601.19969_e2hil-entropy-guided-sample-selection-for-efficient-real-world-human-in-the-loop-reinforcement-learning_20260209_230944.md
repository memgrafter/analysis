---
ver: rpa2
title: 'E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop
  Reinforcement Learning'
arxiv_id: '2601.19969'
source_url: https://arxiv.org/abs/2601.19969
tags:
- entropy
- samples
- learning
- policy
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses low sample efficiency in human-in-the-loop
  reinforcement learning (HiL-RL) for real-world robotic manipulation, which requires
  substantial human interventions and leads to high labor costs. The authors propose
  E2HiL, a framework that actively selects informative samples to stabilize policy
  entropy dynamics and improve sample efficiency.
---

# E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.19969
- Source URL: https://arxiv.org/abs/2601.19969
- Reference count: 40
- Achieves 42.1% higher success rate and 10.1% fewer human interventions than HiL-SERL baseline

## Executive Summary
E2HiL addresses the low sample efficiency problem in human-in-the-loop reinforcement learning for real-world robotic manipulation by actively selecting informative samples that stabilize policy entropy dynamics. The method estimates each sample's influence on policy entropy using covariance between action probabilities and soft advantages, then prunes samples that cause sharp entropy drops or negligible changes. This approach maintains stable exploration-exploitation trade-off while reducing the need for human interventions. Experiments on four real-world manipulation tasks demonstrate significant improvements over the state-of-the-art HiL-SERL baseline.

## Method Summary
E2HiL modifies standard RLPD training by adding an influence-based sample selection mechanism. For each transition, it estimates the sample's effect on policy entropy using covariance between log action probabilities and soft advantages. Samples are filtered based on their influence magnitude using adaptive percentile bounds (5th and 90th), removing those that would cause excessive entropy collapse or provide minimal learning signal. The actor updates only on retained samples, stabilizing entropy dynamics and improving sample efficiency. The framework operates on three data streams: offline demonstrations, online exploration, and human intervention samples collected via leader-follower teleoperation.

## Key Results
- 42.1% higher success rate compared to HiL-SERL baseline across four manipulation tasks
- 10.1% reduction in human intervention requirements
- More stable entropy dynamics with gradual entropy reduction versus baseline's sharp drops
- Effective on diverse tasks: Touch Cube, Pick Cube, Pick & Place Cube, and Stack Blocks

## Why This Works (Mechanism)

### Mechanism 1: Covariance-Based Influence Estimation
The core innovation estimates each sample's effect on policy entropy via covariance between action log-probabilities and soft advantages. This influence value c(s,a) = -η·Cov(log π(a|s), π(a|s)·A_soft(s,a)) predicts entropy changes when samples are used for training. The approximation works well after initial training (beyond ~5k steps) when Q-estimates become reliable, though early training shows deviations due to biased advantage estimates.

### Mechanism 2: Entropy-Bounded Sample Selection Filters Harmful Samples
The method prunes samples with extreme influence values by computing adaptive bounds from the 5th and 90th percentiles of influence magnitudes per batch. This removes shortcut samples that induce sharp entropy drops and noisy samples with negligible effect, retaining only those with moderate influence. This filtering prevents premature entropy collapse while avoiding ineffective updates, though overly tight bounds could starve the training of useful samples.

### Mechanism 3: Stable Entropy Dynamics Improves Exploration-Exploitation Trade-off
By filtering entropy-disruptive samples, policy entropy decreases smoothly rather than collapsing, maintaining exploration capacity longer. This controlled entropy reduction enables the policy to discover multiple skills (e.g., cube positions) rather than prematurely converging. The approach transfers the entropy-exploration relationship from LLM-RL to robotic manipulation, though tasks requiring aggressive early exploitation may be compromised.

## Foundational Learning

- **Policy Gradient with Entropy Regularization (SAC/RLPD style)**: E2HiL modifies the standard actor objective L_π = E[Q - α·log π]; understanding this formulation is essential for grasping how sample selection alters gradients. Quick check: Why does α·log π appear with negative sign, and what happens when entropy collapses?

- **Soft Advantage and Value Functions**: Influence estimation requires A_soft(s,a) = Q(s,a) - V_π(s), where V_π(s) = E[Q - α·log π]. Incorrect advantage estimates bias influence values. Quick check: Given a policy π and critic Q, how would you compute V_π(s) for a discrete action space?

- **Influence Functions in Learning Dynamics**: The paper adapts influence function theory to estimate sample-level entropy impact using covariance as a proxy. Understanding this relationship is key to the method. Quick check: Why might covariance between log π and π·A_soft predict entropy change, and when would this approximation fail?

## Architecture Onboarding

- **Component map**: Sample Collection → Influence Estimator → Selection Module → RLPD Learner
- **Critical path**: 1) Collect transition → store in buffer, 2) Sample minibatch b_R ∪ b_D, 3) For each (s,a) in batch: estimate c(s,a), 4) Compute ℓ, u bounds; generate I(s,a), 5) Update actor using modified L_E2HiL
- **Design tradeoffs**: Percentile bounds (5th, 90th) balance filtering vs. sample availability; Monte Carlo samples K trades accuracy vs. compute; intervention rate vs. learning speed affects labor costs
- **Failure signatures**: Early-training instability from poor Q-values, over-aggressive clipping when >50% samples masked, insufficient entropy decrease from overly tight bounds
- **First 3 experiments**: 1) Baseline replication of HIL-SERL on Touch-Cube to establish instability, 2) Influence validation by logging c(s,a) vs. true entropy derivative, 3) Ablation on bounds testing [2nd,95th], [5th,90th], [10th,85th] percentile thresholds

## Open Questions the Paper Calls Out

- **VLA Model Extension**: Can E2HiL be extended to vision-language-action models for scalable multi-task real-world reinforcement learning? The paper plans to explore this but VLA architectures may require different entropy estimation approaches.

- **Q-Value Calibration**: Can calibration techniques like Cal-QL improve Q-value reliability and reduce early-training deviations in entropy estimation? The authors observe deviations in the first 5k steps due to inaccurate Q-value estimates and suggest this as future work.

- **Percentile Threshold Justification**: What is the theoretical basis for the 5th and 90th percentile bounds, and how sensitive is performance to these thresholds? The paper uses these empirically without justification for their optimality.

## Limitations

- Early-training bias: Covariance-based influence estimation is unreliable during initial training when Q-value estimates are poor, with noted deviations in the first 5k steps
- Heuristic parameter selection: The 5th and 90th percentile bounds are empirically chosen without theoretical justification or systematic sensitivity analysis
- Reproducibility challenges: Key hyperparameters, network architectures, and reward function definitions are unspecified, making exact reproduction difficult

## Confidence

- **High confidence**: Entropy stabilization mechanism and its positive impact on sample efficiency (empirically validated across four tasks)
- **Medium confidence**: Covariance-based influence estimation method (novel approach with limited validation and acknowledged early-training limitations)
- **Low confidence**: Specific percentile thresholds and their universal applicability across different robotic tasks (heuristic choice without theoretical backing)

## Next Checks

1. **Early-training bias validation**: Compare influence estimates against ground-truth entropy derivatives during the first 5,000 training steps to quantify estimation error and validate the paper's claim about early-training deviations.

2. **Bound sensitivity analysis**: Systematically test different percentile thresholds ([2nd,95th], [5th,90th], [10th,85th]) across all four tasks to establish robust operating ranges and identify potential overfitting to the current choice.

3. **Task transferability test**: Apply E2HiL to a new manipulation task requiring different exploration-exploitation dynamics (e.g., assembly requiring precise early positioning) to evaluate generalization limits and identify task characteristics that may break the method.