---
ver: rpa2
title: 'The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model
  Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility'
arxiv_id: '2511.18302'
source_url: https://arxiv.org/abs/2511.18302
tags:
- intelligence
- human
- paradox
- evaluation
- measurement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a fundamental incompatibility between human
  psychometric frameworks and Large Language Model evaluation. Using the Cattell-Horn-Carroll
  theory on nine frontier models, researchers found that models with above-average
  human IQ scores (85.0-121.4) simultaneously achieved near-zero binary accuracy on
  crystallized knowledge tasks.
---

# The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility

## Quick Facts
- **arXiv ID:** 2511.18302
- **Source URL:** https://arxiv.org/abs/2511.18302
- **Authors:** Mohan Reddy
- **Reference count:** 10
- **Key outcome:** Fundamental incompatibility between human psychometric frameworks and LLM evaluation revealed through CHC theory application to nine frontier models

## Executive Summary
This study identifies a fundamental incompatibility between human psychometric frameworks and Large Language Model evaluation. Using the Cattell-Horn-Carroll theory on nine frontier models, researchers found that models with above-average human IQ scores (85.0-121.4) simultaneously achieved near-zero binary accuracy on crystallized knowledge tasks. The study reveals an overall judge-binary correlation of r=0.175 (p<0.001, n=1,800), with the disconnect most severe in crystallized intelligence where all models achieved 100% binary accuracy while judge scores ranged from 25-62%. This paradox demonstrates that applying biological cognitive architectures to transformer-based systems constitutes an ontological category error, requiring new native machine cognition assessment frameworks.

## Method Summary
The study evaluated nine frontier LLMs using CHC psychometric framework with dual scoring methodology: binary exact-match scoring and LLM-as-Judge conceptual accuracy assessment. Claude Sonnet 4 served as judge for non-Anthropic models, using rubric-based 0/0.5/1.0 scoring with chain-of-thought justification. Psychometric transformation converted raw scores to IQ-scale metrics using CTT formula and 2PL IRT with L2 regularization. The analysis computed judge-binary correlations by ability domain and calculated Paradox Severity Index weighted by IQ scores, revealing systematic measurement failures particularly in crystallized intelligence tasks.

## Key Results
- Judge-binary correlation of r=0.175 (p<0.001, n=1,800) across four CHC abilities
- Crystallized intelligence paradox: 100% binary accuracy across all models while judge scores ranged 25-62%
- Models achieving above-average human IQ scores (85.0-121.4) simultaneously exhibited near-zero binary accuracy on crystallized knowledge tasks
- Statistical impossibility theorem: eight models achieving identical perfect crystallized intelligence scores (p≈10^-120)

## Why This Works (Mechanism)

### Mechanism 1: The Verbosity Paradox
- Claim: LLMs optimized through RLHF to provide comprehensive answers are systematically penalized by psychometric instruments expecting minimal, precise responses.
- Mechanism: Binary exact-match scoring returns 0 for conceptually correct answers that include additional explanation, creating an inverse relationship between actual knowledge and measured performance.
- Core assumption: Traditional psychometric conventions were designed for human test-taking behavior, not generative model output distributions.
- Evidence anchors:
  - [abstract]: "models achieving above-average human IQ scores ranging from 85.0 to 121.4 simultaneously exhibit binary accuracy rates approaching zero on crystallized knowledge tasks"
  - [section 2.2]: "This creates the 'Verbosity Paradox': a systematic measurement failure wherein models providing conceptually perfect answers receive zero scores due to response format misalignment"
  - [corpus]: "Stop Evaluating AI with Human Tests" (arXiv:2507.23009) argues human cognitive/psychological tests are inappropriate for LLMs
- Break condition: If models are explicitly prompted to output only the exact answer with no elaboration, binary scores should converge with judge scores.

### Mechanism 2: Architectural Incompatibility Between Substrates
- Claim: Human cognitive architecture (serial, capacity-limited, embodied) and transformer architecture (parallel attention, constant-time context access, disembodied) differ fundamentally, making direct psychometric comparison invalid.
- Mechanism: CHC theory assumes constraints (working memory 7±2 items, exponential forgetting curves) that do not apply to transformers, so factor structures derived from human performance do not transfer.
- Core assumption: Intelligence constructs are substrate-dependent rather than universal.
- Evidence anchors:
  - [abstract]: "fundamental category error in applying biological cognitive architectures to transformer systems"
  - [section 2.1]: "Human cognition operates through serial, capacity-limited processing with working memory constraints of 7±2 items, while transformers utilize parallel attention mechanisms across entire context windows"
  - [corpus]: Weak direct corpus support for this specific architectural claim; related work on AI-specific tests exists but does not formalize the incompatibility
- Break condition: If evaluation frameworks are redesigned around transformer-native properties (e.g., context utilization patterns, attention distribution), observed paradoxes should diminish.

### Mechanism 3: Measurement Theatre via Identical Perfect Scores
- Claim: The observation that eight models achieve identical perfect binary scores (p≈10^-120) on crystallized knowledge indicates measurement failure rather than genuine performance equivalence.
- Mechanism: When test design allows ceiling effects across heterogeneous systems, the instrument cannot discriminate; scores reflect instrument limitations, not system capabilities.
- Core assumption: Independent models should show score variance on valid discriminative tests.
- Evidence anchors:
  - [abstract]: "impossibility theorem showing eight models achieving identical perfect crystallized intelligence scores is statistically implausible (p≈10^-120)"
  - [section 5.3]: "The probability of eight independent models achieving identical perfect scores on a 50-item test by chance is P≈10^-120. This impossibility serves as a reductio ad absurdum of the measurement framework itself."
  - [corpus]: No direct corpus precedent for this specific statistical argument; related work on evaluation validity exists
- Break condition: If test difficulty is calibrated to model distributions rather than human norms, or if items require more than factual recall, variance should reappear.

## Foundational Learning

- Concept: Cattell-Horn-Carroll (CHC) Theory
  - Why needed here: The paper applies this human psychometric framework to LLMs; understanding its hierarchical structure (g-factor, broad abilities like Gf/Gc/Gq, narrow abilities) is required to interpret the claimed incompatibility.
  - Quick check question: Name the three strata of CHC theory and explain what Gc (Crystallized Intelligence) measures in humans.

- Concept: Item Response Theory (IRT) vs Classical Test Theory (CTT)
  - Why needed here: The paper uses both CTT (for IQ transformation) and 2PL IRT (for ability estimation); understanding their different assumptions about item difficulty and discrimination is necessary to interpret Table 2.
  - Quick check question: In a 2PL IRT model, what do parameters θ, a, and b represent, and how does this differ from CTT's assumptions?

- Concept: LLM-as-Judge Evaluation
  - Why needed here: The dual scoring methodology uses an LLM judge to assess conceptual accuracy; understanding the design choices (cross-vendor, anonymized, rubric-based) is critical for evaluating validity claims.
  - Quick check question: What are three potential failure modes when using an LLM to evaluate another LLM's response?

## Architecture Onboarding

- Component map:
  - Test Item Bank: Items from CHC domains (Gf, Gc, Gq, Grw)
  - Model Response Layer: 9 frontier models with varied response patterns
  - Dual Scoring Engine: Binary exact-match + LLM-as-Judge (Claude Sonnet 4)
  - Psychometric Transformation: CTT IQ formula + 2PL IRT with L2 regularization
  - Paradox Analysis: Correlation computation, PSI calculation, distribution analysis

- Critical path:
  1. Administer CHC-domain items to all models
  2. Apply binary scoring (exact string match to expected answer)
  3. Apply judge scoring (conceptual correctness via rubric)
  4. Transform raw scores to IQ-scale metrics
  5. Compute judge-binary correlation and domain-specific gaps
  6. Calculate Paradox Severity Index weighted by IQ

- Design tradeoffs:
  - Binary vs Judge scoring: Binary is objective but format-sensitive; Judge captures semantics but introduces model bias
  - CTT vs IRT: CTT enables human-norm comparison; IRT models item-level difficulty but requires parameter stability assumptions
  - Single vs cross-vendor judges: Cross-vendor reduces home-field advantage but may introduce inter-judge variance

- Failure signatures:
  - Judge-binary correlation < 0.2 with significant accuracy gaps (>40%)
  - Ceiling effects: identical perfect scores across heterogeneous models
  - Paradox Severity Index increasing with model IQ (better models appear worse-measured)
  - Bimodal judge distributions vs polarized binary distributions

- First 3 experiments:
  1. Prompt ablation: Test whether explicit "answer only with the exact word/phrase" instructions eliminate the judge-binary gap for Gc items.
  2. Judge calibration: Replace Claude Sonnet 4 with Gemini 2.5 Pro or GPT-4 Turbo as judge; measure inter-judge agreement and whether the paradox persists.
  3. Item difficulty recalibration: Construct Gc items requiring multi-step reasoning rather than factual recall; verify whether score variance reappears across models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can "native machine cognition assessments" be operationalized to replace anthropomorphic frameworks?
- **Basis in paper:** [explicit] The conclusion calls for "developing native machine cognition assessments" that respect architectural differences rather than biological metaphors.
- **Why unresolved:** The paper proposes high-level principles (e.g., architecture-aware testing) but offers no concrete methodology or validated alternative to CHC theory.
- **What evidence would resolve it:** A new benchmark demonstrating high predictive validity for AI capabilities while showing low correlation with human psychometric scores.

### Open Question 2
- **Question:** Does the "Artificial G-Factor" ($g_{LLM}$) have independent predictive validity for machine capabilities?
- **Basis in paper:** [explicit] Section 5.2 decomposes $g_{LLM}$ into data correlations and architectural artifacts, asserting it shares no common factors with human $g$.
- **Why unresolved:** The paper claims $g_{LLM}$ is an illusion but does not test if this factor reliably predicts performance on non-human tasks.
- **What evidence would resolve it:** Correlational analysis showing $g_{LLM}$ predicts success on machine-specific tasks (e.g., massive context synthesis) independent of human-aligned metrics.

### Open Question 3
- **Question:** Is the low judge-binary correlation a fixable "verbosity" artifact or a fundamental incompatibility?
- **Basis in paper:** [inferred] Section 2.2 identifies the "Verbosity Paradox," yet Section 5.1 argues the failure is an "ontological category error."
- **Why unresolved:** It remains unclear if enforcing strict brevity via prompting would close the accuracy gap or if the semantic disconnect persists regardless of format.
- **What evidence would resolve it:** Re-evaluation of models under strict length constraints to determine if the $r=0.175$ correlation significantly improves.

## Limitations

- The core paradox rests on highly specific empirical patterns without full methodological transparency, including unknown test items and human normative parameters.
- The argument assumes all nine models are truly independent and that identical binary scores represent measurement failure rather than genuine equivalence on factual recall tasks.
- The statistical impossibility argument (p≈10^-120) depends on specific assumptions about item independence and model heterogeneity that may not hold.

## Confidence

- **High Confidence**: The existence of a judge-binary correlation gap (r≈0.175) and the general observation that verbose responses cause binary scoring failures.
- **Medium Confidence**: The claim that this represents a "fundamental incompatibility" requiring entirely new evaluation frameworks.
- **Low Confidence**: The statistical impossibility argument (p≈10^-120) for identical perfect scores and the catastrophic framing of this as an ontological category error.

## Next Checks

1. **Prompt Engineering Validation**: Test whether explicit "answer only with the exact word/phrase" instructions eliminate the judge-binary gap for Gc items across multiple model versions.
2. **Cross-Judge Agreement Analysis**: Replace Claude Sonnet 4 with Gemini 2.5 Pro and GPT-4 Turbo as judges; measure inter-judge agreement and whether the paradox persists.
3. **Item Difficulty Recalibration**: Construct Gc items requiring multi-step reasoning rather than factual recall; verify whether score variance reappears across models.