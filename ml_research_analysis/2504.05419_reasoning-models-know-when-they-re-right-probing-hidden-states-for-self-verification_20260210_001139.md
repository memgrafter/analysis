---
ver: rpa2
title: 'Reasoning Models Know When They''re Right: Probing Hidden States for Self-Verification'
arxiv_id: '2504.05419'
source_url: https://arxiv.org/abs/2504.05419
tags:
- reasoning
- answer
- correctness
- intermediate
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether reasoning models encode information
  about the correctness of their intermediate answers during long Chain-of-Thought
  (CoT) reasoning. The authors segment long CoT into chunks containing intermediate
  answers and train a binary classifier (probe) on the model's hidden states to predict
  answer correctness.
---

# Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification

## Quick Facts
- **arXiv ID:** 2504.05419
- **Source URL:** https://arxiv.org/abs/2504.05419
- **Reference count:** 40
- **Key outcome:** Reasoning models encode answer correctness in hidden states, enabling efficient self-verification through probes

## Executive Summary
This paper investigates whether reasoning models encode information about the correctness of their intermediate answers during long Chain-of-Thought reasoning. The authors segment long CoT into chunks containing intermediate answers and train a binary classifier (probe) on the model's hidden states to predict answer correctness. They find that reasoning models encode answer correctness in their hidden states, with probes achieving high accuracy (ROC-AUC above 0.7) and excellent calibration (ECE below 0.1) on in-distribution data. The probes generalize well across mathematical reasoning datasets but not to logical reasoning. Importantly, correctness can be predicted even before the answer is fully formulated. Using the probe as a verifier for confidence-based early-exit reduces inference tokens by 24% without compromising accuracy, revealing that models fail to efficiently use this internal correctness information. This work demonstrates reasoning models' latent self-verification ability and offers a lightweight approach for improving reasoning efficiency.

## Method Summary
The authors develop a method to test whether reasoning models encode answer correctness in their hidden states during Chain-of-Thought reasoning. They first segment long CoT reasoning into chunks containing intermediate answers. Then they train a binary classifier (probe) on the model's hidden states to predict whether each intermediate answer is correct. The probe is trained and evaluated across multiple mathematical reasoning datasets including GSM8K, MATH, and SVAMP. The study also tests the probe's generalization to logical reasoning datasets and evaluates its effectiveness as a verifier for confidence-based early-exit strategies to reduce inference tokens.

## Key Results
- Probes trained on hidden states achieve ROC-AUC scores above 0.7 and excellent calibration (ECE < 0.1) for predicting answer correctness
- Correctness can be predicted even before the answer is fully formulated in the reasoning chain
- Probes generalize well across mathematical reasoning datasets but fail on logical reasoning tasks
- Using the probe as a verifier for confidence-based early-exit reduces inference tokens by 24% without accuracy loss

## Why This Works (Mechanism)
The mechanism relies on the observation that reasoning models develop internal representations that capture the quality and correctness of their intermediate reasoning steps. As the model generates Chain-of-Thought reasoning, its hidden states evolve in ways that reflect whether the current reasoning path is leading toward a correct answer. This creates a latent signal that can be extracted through probing, even before the final answer is produced. The probe essentially learns to decode this internal self-assessment signal from the model's activations.

## Foundational Learning
- **Chain-of-Thought reasoning**: Why needed - Understanding how models break down complex problems into intermediate steps; Quick check - Can you identify the intermediate reasoning steps in a sample CoT output?
- **Hidden state representations**: Why needed - These are the internal activations that potentially encode correctness information; Quick check - What dimensions and structure do typical transformer hidden states have?
- **Binary classification probes**: Why needed - The method uses probes to detect correctness signals from hidden states; Quick check - How does probe training differ from fine-tuning the entire model?
- **Calibration metrics (ECE)**: Why needed - To measure how well predicted probabilities match true correctness; Quick check - What does low ECE indicate about a classifier's reliability?
- **Early-exit strategies**: Why needed - The probe is applied to reduce inference computation; Quick check - How does confidence-based early-exit work in transformer inference?

## Architecture Onboarding

**Component map:** Model -> Hidden states -> Probe -> Correctness prediction -> Early-exit decision

**Critical path:** The probe extracts correctness information from hidden states at each reasoning step, enabling decisions about whether to continue or exit early based on confidence thresholds.

**Design tradeoffs:** Binary classification simplifies the probe but may lose nuanced information about intermediate reasoning quality. The approach trades off probe complexity for lightweight inference efficiency gains.

**Failure signatures:** Probe accuracy drops significantly on out-of-distribution logical reasoning tasks, indicating domain-specific rather than general self-awareness capabilities.

**First experiments:** 1) Train probe on GSM8K and evaluate on held-out data; 2) Test probe generalization to MATH and SVAMP datasets; 3) Evaluate early-exit performance with probe-based confidence thresholds

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Probe generalization fails on logical reasoning tasks, suggesting domain-specific rather than universal self-verification capabilities
- Binary classification approach may discard richer information about intermediate reasoning quality
- Limited validation across different model families and probe architectures
- Does not explain why models fail to use their own internal correctness information efficiently

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Probes achieve high accuracy (ROC-AUC > 0.7) and excellent calibration on mathematical reasoning | High |
| Correctness can be predicted before final answer formulation | High |
| Probes fail to generalize to logical reasoning tasks | Medium |
| Early-exit reduces tokens by 24% without accuracy loss | Medium |
| Models fail to efficiently use internal correctness information | Low |

## Next Checks

1. Test probe generalization across diverse reasoning domains beyond mathematical and logical tasks, including commonsense reasoning and scientific problem-solving, to determine if the self-verification signal is domain-general or task-specific.

2. Compare different probe architectures (linear vs. non-linear) and training strategies to establish whether the observed correctness signal represents a fundamental property of reasoning models or an artifact of the probe design.

3. Conduct ablation studies on intermediate answer quality by modifying or removing specific reasoning steps to determine which aspects of the chain-of-thought process most strongly influence the probe's correctness predictions.