---
ver: rpa2
title: 'CARGO: A Framework for Confidence-Aware Routing of Large Language Models'
arxiv_id: '2509.14899'
source_url: https://arxiv.org/abs/2509.14899
tags:
- llms
- accuracy
- routing
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CARGO, a lightweight, confidence-aware framework
  for routing prompts to the most appropriate large language model (LLM). CARGO uses
  a single embedding-based regressor trained on LLM-judged pairwise comparisons, with
  an optional binary classifier invoked when predictions are uncertain.
---

# CARGO: A Framework for Confidence-Aware Routing of Large Language Models

## Quick Facts
- arXiv ID: 2509.14899
- Source URL: https://arxiv.org/abs/2509.14899
- Reference count: 40
- Primary result: 76.4% top-1 routing accuracy using lightweight confidence-aware routing

## Executive Summary
This paper introduces CARGO, a lightweight framework for routing prompts to the most appropriate large language model among a pool of candidates. The framework uses a two-stage approach: a fast regressor predicts performance scores, and an optional binary classifier is invoked only when confidence gaps are small. Evaluated on four competitive LLMs across five task categories, CARGO achieves expert-level performance with minimal computational overhead, demonstrating that confidence-guided routing can deliver high accuracy without the cost of running all models for every prompt.

## Method Summary
CARGO uses a two-stage routing pipeline where prompts are first embedded using ada-002, then scored by a Random Forest regressor for all candidate LLMs. The system calculates the gap between top two scores and only invokes a binary MLP classifier when this gap falls below a threshold τ. Training targets come from an LLM jury that provides pairwise comparisons aggregated into normalized scores. The framework supports both global and category-specific regressors, with a category classifier determining which to use.

## Key Results
- 76.4% top-1 routing accuracy achieved through confidence-aware two-stage approach
- Win rates of 72-89% against individual expert models in head-to-head comparisons
- Category-specific regressors achieve up to 91.9% top-1-or-2 accuracy in coding tasks

## Why This Works (Mechanism)

### Mechanism 1: Gap-Triggered Escalation
Routing accuracy improves when a specialized classifier is invoked only when the primary regressor's confidence gap is small. The system uses a Random Forest regressor to predict performance scores, calculates the difference between top two scores, and escalates to an MLP binary classifier only when this gap falls below threshold τ. Core assumption: small gaps correlate with higher uncertainty. Evidence shows accuracy rising from 58.3% to 76.4% with this logic.

### Mechanism 2: Semantic-to-Performance Mapping
Static text embeddings contain sufficient semantic signal to predict relative performance scores of diverse LLMs. Prompts are converted to vectors using ada-002, and regressors map these to normalized performance scores derived from an LLM-jury. Core assumption: embedding space preserves task-type and complexity features that correlate with specific model strengths. Category-specific regressors achieve high Top-1-or-2 Accuracy up to 91.9% in Coding.

### Mechanism 3: Jury-Based Ground Truth
Aggregating pairwise preferences from diverse LLM judges provides scalable, low-bias approximation of human preference. Four independent LLMs judge response pairs, aggregated using summation formula to mitigate individual bias. Core assumption: jury agreement (κ ≈ 0.62) and alignment with human validation (κ ≈ 0.72) hold across target domains.

## Foundational Learning

- **Concept: Pairwise Ranking & Scoring**
  - Why needed here: Training targets are relative scores from pairwise comparisons, not binary correct/incorrect flags
  - Quick check: How does the system resolve a tie if Judge A prefers Model X and Judge B prefers Model Y?

- **Concept: Embedding Space & Vectorization**
  - Why needed here: Router relies on ada-002 vectors to represent prompt semantics numerically
  - Quick check: Does the router use the prompt's embedding, the response's embedding, or both to make a prediction?

- **Concept: Threshold Tuning (τ)**
  - Why needed here: Trade-off between routing latency and accuracy is explicitly controlled by gap threshold τ
  - Quick check: If τ is set to 0.0, what is the behavior of the system (regressor only vs. classifier always)?

## Architecture Onboarding

- **Component map:** OpenRouter API -> LLM Jury (4 models) + Pairwise Scoring Aggregator -> ada-002 Embedder -> Random Forest Regressor -> Gap Calculator -> (Conditional) MLP Classifier -> Category Gate (96% accuracy classifier)

- **Critical path:** The Gap Decision Point - where the system determines if regressor's prediction is "safe" or if the computationally heavier binary classifier is needed to break a tie

- **Design tradeoffs:**
  - Latency vs. Accuracy: High τ increases accuracy (~76%) but invokes classifier on 80%+ of prompts, adding latency
  - Generalization vs. Specialization: Global models offer robustness; category-specific models offer higher accuracy in Coding/Reasoning but require maintaining 5 separate models

- **Failure signatures:**
  - "Flip-Flopping": High variance in routing decisions for minor prompt rephrasing (indicates unstable embedding-projection)
  - Stuck on Dominant Model: Imbalanced training data leads to always routing to the historically winning model

- **First 3 experiments:**
  1. Threshold Sweep: Run validation set with τ ∈ [0.01, 0.20] to find the "knee" in the curve where accuracy gains diminish relative to classifier invocation rate
  2. Domain Stress Test: Inject "Translation" dataset into "Creative Writing" regressor to verify category-gate correctly rejects OOD inputs
  3. Judge Ablation: Remove one judge from jury pool during labeling to verify ground truth stability with smaller jury

## Open Questions the Paper Calls Out

### Open Question 1
How does CARGO's routing accuracy and computational overhead scale when the expert pool expands beyond 4–5 LLMs to 10, 20, or 50 models? The discussion notes "all-pairs voting" scales quadratically with M, and experiments were limited to 4–5 experts. Only scalability test added a 5th model, leaving performance characteristics with substantially larger portfolios unknown.

### Open Question 2
Why does CARGO underperform on summarization and creative writing tasks (67–68% accuracy) compared to coding and reasoning (82–86%), and can category-specific improvements close this gap? Figure 7 and Table VI show Summaries and English achieving maximum accuracies of only 0.683 and 0.673 respectively, while Coding reaches 0.864. The paper reports the disparity but does not investigate the root cause.

### Open Question 3
Can the confidence threshold τ be set automatically per deployment rather than through manual sweeps, and what factors should determine it? The paper sweeps τ from 0.01 to 0.20 and identifies [0.06, 0.10] as a practical range, but selection is empirical and static. No principled or adaptive mechanism is proposed.

## Limitations
- Performance heavily relies on quality of LLM jury ground truth (κ = 0.62), which may not generalize across all domains
- Confidence threshold τ requires careful tuning for each deployment context, introducing trade-off between accuracy and computational overhead
- Current implementation supports only five task categories and four LLMs, limiting generalizability to broader LLM ecosystems

## Confidence

- **High Confidence:** The two-stage routing mechanism and its measurable impact on accuracy gains (58.3% → 76.4%) are well-supported by ablation results in Figure 6
- **Medium Confidence:** Jury-based ground truth methodology is validated within study but lacks external benchmarking against human annotations beyond reported κ = 0.72 alignment
- **Medium Confidence:** Category-specific regressor benefits are demonstrated, but added complexity may not justify marginal gains in all deployment scenarios

## Next Checks

1. Conduct human evaluation study comparing CARGO's routing decisions against expert human judgments across diverse task categories to validate jury-based ground truth quality

2. Test framework's robustness to distribution shifts by evaluating on task categories not present in training data (e.g., translation, summarization of specialized domains)

3. Perform comprehensive cost-benefit analysis of confidence threshold τ across different deployment contexts, measuring trade-off between accuracy gains and computational overhead in production settings