---
ver: rpa2
title: 'TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon'
arxiv_id: '2509.07381'
source_url: https://arxiv.org/abs/2509.07381
tags:
- uni00000013
- control
- uni00000011
- uni00000014
- transmpc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational complexity challenge in
  traditional online Model Predictive Control (MPC) by introducing TransMPC, a novel
  Transformer-based explicit MPC framework. TransMPC formulates the MPC policy as
  an encoder-only Transformer network that leverages bidirectional self-attention
  to generate entire control sequences in a single forward pass, naturally accommodating
  variable prediction horizons.
---

# TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon

## Quick Facts
- arXiv ID: 2509.07381
- Source URL: https://arxiv.org/abs/2509.07381
- Reference count: 32
- Key outcome: Achieves 81.97-516.74% computational speed-up over baseline explicit MPC methods while maintaining superior solution accuracy across variable prediction horizons

## Executive Summary
This paper addresses the computational complexity challenge in traditional online Model Predictive Control (MPC) by introducing TransMPC, a novel Transformer-based explicit MPC framework. TransMPC formulates the MPC policy as an encoder-only Transformer network that leverages bidirectional self-attention to generate entire control sequences in a single forward pass, naturally accommodating variable prediction horizons. Unlike imitation-based approaches dependent on precomputed optimal trajectories, TransMPC directly optimizes the true finite-horizon cost through automatic differentiation, using random horizon sampling and a replay buffer to ensure robust generalization across states and horizon lengths.

Extensive evaluations on vehicle trajectory tracking tasks demonstrate that TransMPC achieves significant computational speed-ups (81.97-516.74% faster than competing methods) while maintaining superior solution accuracy across varying prediction horizons (1-20 steps). Real-world experiments on an autonomous mobile robot validate TransMPC's effectiveness in trajectory tracking and obstacle avoidance scenarios. The method achieves higher control accuracy than prior explicit MPC approaches based on MLP or RNN architectures while maintaining stable computation efficiency regardless of horizon length, making it suitable for practical deployment in complex dynamic systems.

## Method Summary
TransMPC is an encoder-only Transformer network that directly approximates the MPC policy by optimizing the true finite-horizon cost through automatic differentiation. The architecture consists of state and reference encoders, multi-head bidirectional self-attention layers, and an action decoder that generates the entire control sequence in a single forward pass. Training alternates between a sampling phase (executing actions, storing states in replay buffer, resetting periodically) and a learning phase (sampling minibatches with random horizons, computing cost through model rollout, updating via gradients). The random horizon sampling ensures generalization across varying prediction lengths, while the replay buffer maintains i.i.d. training samples.

## Key Results
- Achieves 81.97-516.74% computational speed-up compared to IPOPT solver and baseline explicit MPC methods
- Maintains superior solution accuracy with relative accuracy consistently below 5% across all prediction horizons
- Demonstrates stable inference latency regardless of prediction horizon length, unlike traditional MPC methods
- Successfully extends to real-world applications including obstacle avoidance on autonomous mobile robots

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Self-Attention Enables Whole-Horizon Context Fusion
Encoder-only Transformer generates all control actions in a single parallel pass by letting each timestep attend to the entire reference sequence. Multi-head self-attention creates learned key-value pairs for each token, allowing $u_{t+i}$ to condition on all future reference tokens $x^R_{t+j}$ simultaneously, unlike causal decoders or unidirectional RNNs. Optimal control decisions benefit from full future context; the MPC problem structure aligns with bidirectional information flow.

### Mechanism 2: Direct Differentiable Cost Optimization Bypasses Imitation Bottleneck
Training via automatic differentiation on the true MPC cost avoids compounding errors from mimicking precomputed trajectories. The policy $\pi(x_t, X_R; \theta)$ outputs action sequences; gradients flow through dynamics $f$ and cost $l$ via chain rule, optimizing expected cost $J(\theta)$ directly without needing expert solutions. The dynamics model $f$ must be differentiable and sufficiently accurate for gradient-based policy improvement.

### Mechanism 3: Random Horizon Sampling Enforces Cross-Horizon Generalization
Uniform horizon sampling during training creates a policy that generalizes across variable prediction lengths at test time. Each training batch samples $N \sim U[1, N_{max}]$, forcing the Transformer to handle arbitrary sequence lengths; replay buffer ensures i.i.d. state distribution. The relationship between state/reference and optimal action is learnable across horizon lengths without explicit horizon conditioning.

## Foundational Learning

- **Model Predictive Control (MPC) fundamentals**: Understanding the finite-horizon optimization problem (Equation 1) is prerequisite since TransMPC approximates the MPC policy $\pi \approx U^*$. Quick check: Can you explain why MPC requires solving an optimization at each timestep, and how explicit MPC differs from online MPC?

- **Transformer self-attention mechanics**: The core architecture is an encoder-only Transformer; you must understand key/query/value attention and positional encoding to debug inference. Quick check: Given a sequence of length $N$, how does multi-head attention compute dependencies between token $i$ and all other tokens?

- **Automatic differentiation through dynamics**: Training requires backpropagating through $x_{t+i+1} = f(x_{t+i}, u_{t+i})$; you need to grasp how gradients accumulate across timesteps (Equation 7). Quick check: If $f$ is a neural network, how would you compute $\frac{dx_{t+2}}{d\theta}$ given $\frac{dx_{t+1}}{d\theta}$?

## Architecture Onboarding

- **Component map**: $x_t \rightarrow E_x \rightarrow$ [positional encoding] $\rightarrow$ concat $\rightarrow$ multi-head attention $\rightarrow D_u \rightarrow U$ where $U$ contains control sequence
- **Critical path**: Input $(x_t, X_R)$ → concat + positional encoding → attention layers → $D_u$ → $U$ → extract $u_t = U[0]$ → execute in environment
- **Design tradeoffs**: Encoder-only vs. decoder: Encoder gives $O(1)$ inference latency w.r.t. horizon but requires full reference upfront; decoder would allow autoregressive generation but incurs $O(N)$ latency. Direct optimization vs. imitation: Direct cost optimization avoids needing optimal trajectory datasets but assumes differentiable dynamics and may be less sample-efficient. Random vs. fixed horizon training: Random sampling adds flexibility at training cost of potentially slower convergence per horizon.
- **Failure signatures**: Poor tracking with long horizons but good short-horizon performance → insufficient attention capacity or positional encoding issues. High variance in control outputs across similar states → replay buffer not diverse enough or learning rate too high. Divergent training loss → check dynamics model accuracy; gradients may explode through long unrolls.
- **First 3 experiments**:
  1. **Sanity check on fixed horizon**: Train and evaluate TransMPC with $N=10$ only; compare control accuracy against IPOPT solver on 50 random initial states. Expected: relative accuracy < 5% on $u_t$.
  2. **Variable horizon generalization test**: Train with random $N \in [5, 20]$, then evaluate separately at $N=5, 10, 15, 20$ without retraining. Plot accuracy vs. horizon; watch for degradation at extremes.
  3. **Ablation: encoder vs. decoder**: Replace encoder-only architecture with causal decoder of equivalent parameter count; compare inference latency and tracking cost on double-lane-change task. Expected: encoder faster but decoder may close gap if trained longer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can TransMPC be extended to handle non-differentiable cost functions without relying on precomputed optimal trajectories?
- **Basis in paper**: The conclusion states, "Future work will focus on extending TransMPC to handle non-differentiable cost functions and broader application scenarios in robotics..."
- **Why unresolved**: The current methodology relies entirely on automatic differentiation (Eq. 6) to propagate gradients through the finite-horizon cost, which requires the cost function to be differentiable.
- **What evidence would resolve it**: Demonstration of TransMPC optimizing discrete metrics (e.g., min-max error) or black-box rewards using gradient-estimation techniques.

### Open Question 2
- **Question**: How can TransMPC guarantee strict satisfaction of state constraints in safety-critical scenarios?
- **Basis in paper**: The problem formulation (Eq. 1) explicitly defines the MPC task "without state constraints," focusing only on control inputs and running costs.
- **Why unresolved**: The Transformer policy approximates the cost minimization via soft penalties (running cost $l$), which does not mathematically guarantee that network outputs will strictly satisfy hard state bounds $x \in X$.
- **What evidence would resolve it**: Integration of barrier functions or a projection layer that enforces state constraints, verified in high-density obstacle environments.

### Open Question 3
- **Question**: Is the method applicable to systems where analytical dynamics models are unavailable or non-differentiable?
- **Basis in paper**: The gradient calculation in Eq. 6 requires differentiating the system dynamics function $f(x_t, u_t)$ to compute $\partial f / \partial x$ and $\partial f / \partial u$.
- **Why unresolved**: The paper assumes access to a differentiable model for the learning phase. If $f$ is a black-box simulator or involves discontinuous contact dynamics, the proposed automatic differentiation pipeline fails.
- **What evidence would resolve it**: A hybrid approach using finite-difference gradients or a differentiable surrogate model trained from data.

## Limitations

- Limited architectural detail (number of encoder layers, feedforward dimensions, replay buffer size, training schedule) making exact reproduction difficult
- Comparative baselines limited to MLP and RNN explicit MPC approaches without including modern neural MPC methods like ZipMPC
- Claims about bidirectional attention being "indispensable" for MPC lack ablation studies removing attention or using causal alternatives

## Confidence

- **High confidence**: Computational speed advantages (81.97-516.74% faster) and stable inference time regardless of horizon length are directly measurable from experimental results
- **Medium confidence**: Superior solution accuracy claims rely on comparisons against IPOPT solver and baseline explicit MPC methods, but methodology details are sparse
- **Low confidence**: Claims about bidirectional attention being essential for MPC benefit from full future context lack comparative evidence against causal attention variants

## Next Checks

1. **Architecture ablation**: Implement a causal Transformer decoder variant and directly compare tracking accuracy and inference latency against the proposed encoder-only architecture on the same double-lane-change task

2. **Cost optimization vs. imitation learning**: Train a ZipMPC-style imitation learner on precomputed optimal trajectories and compare both solution accuracy and sample efficiency against TransMPC's direct cost optimization approach

3. **Replay buffer sensitivity**: Systematically vary replay buffer size (100, 1000, 10000) and analyze impact on generalization across horizons and convergence stability during training