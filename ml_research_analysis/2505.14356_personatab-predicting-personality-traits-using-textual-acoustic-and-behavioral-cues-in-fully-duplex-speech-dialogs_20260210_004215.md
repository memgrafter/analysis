---
ver: rpa2
title: 'PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral
  Cues in Fully-Duplex Speech Dialogs'
arxiv_id: '2505.14356'
source_url: https://arxiv.org/abs/2505.14356
tags:
- personality
- speech
- labels
- speaker
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel pipeline for predicting personality
  traits from fully-duplex speech dialogues, addressing the challenge of personality-aware
  conversational agents in the absence of personality annotations in speech datasets.
  The authors preprocess raw audio recordings to create a dialogue dataset annotated
  with timestamps, response types, and emotion/sentiment labels using an automatic
  speech recognition (ASR) system and various classifiers.
---

# PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs

## Quick Facts
- arXiv ID: 2505.14356
- Source URL: https://arxiv.org/abs/2505.14356
- Reference count: 0
- One-line primary result: Multi-modal personality prediction from fully-duplex dialogues achieves 0.503 cosine similarity with human annotations

## Executive Summary
This paper presents PersonaTAB, a pipeline for predicting Big Five personality traits from fully-duplex speech dialogues. The system preprocesses raw audio to extract textual, acoustic, and behavioral features, then uses LLMs to infer personality traits. Human evaluators validated both the generated dataset and prediction model, demonstrating stronger alignment with human judgments compared to existing approaches.

## Method Summary
The system processes two-channel audio recordings through an ASR layer (Whisper Turbo) to obtain transcripts with timestamps, then detects laughter and constructs response boundaries using a 700ms silence threshold. Overlaps are classified and backchannels are typed using GPT-4o with contextual awareness. Emotion and sentiment are classified using DistilRoBERTa and RoBERTa models. Behavioral statistics are normalized to ordinal categories using mean and IQR-based thresholds. GPT-4o predicts personality traits using structured prompts containing emotion/sentiment percentages, normalized behavioral basics, and 20 sample responses per speaker.

## Key Results
- Achieves average cosine similarity of 0.503 with human-annotated personality labels
- Outperforms baseline methods (BERT, LM, MiniLM) with best baseline at 0.148 cosine similarity
- Validated by human evaluators showing stronger alignment with human judgments than existing approaches

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal behavioral signal integration improves personality prediction over text-only approaches. The system extracts textual (emotion/sentiment labels), acoustic (laughter tokens), and behavioral (turn frequency, backchannel rates, interjections) features, then presents them jointly to an LLM for trait inference. The LLM can weight these signals contextually rather than relying on single-modality patterns. This assumes conversational personality manifests across multiple behavioral channels simultaneously, and LLMs can learn implicit correlations between behavioral patterns and traits.

### Mechanism 2
Normalizing raw behavioral statistics into relative ordinal categories improves LLM interpretation. Instead of presenting raw counts (e.g., "47 turns"), the system maps values to five relative groups (Very Few, Few, Normal, Many, Very Many) using dataset-wide mean and interquartile range. This converts absolute numbers into interpretable qualitative descriptors, addressing the assumption that LLMs struggle to assess whether raw numerical values are high or low without distributional context.

### Mechanism 3
Context-aware backchannel classification via LLM captures listener engagement patterns relevant to personality. Candidate backchannels from fully-overlapping speech are classified by GPT-4o using chat history and position context into emotive, cognitive, or non-backchannel categories. This distinguishes listener responsiveness types, based on the assumption that backchannel type and frequency correlate with personality traits and contextual understanding improves classification over keyword matching.

## Foundational Learning

- **Big Five Personality Model (OCEAN)**: The entire prediction target is structured around Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Understanding what each trait represents is essential for interpreting model outputs and designing features. Quick check: Would frequent laughter more likely indicate high Extraversion or high Neuroticism? (Answer per Table 1: Extraversion shows +66 correlation, Neuroticism shows -9)

- **Fully-duplex vs Turn-taking Dialogue**: The preprocessing pipeline explicitly detects overlaps, backchannels, and interjections that only occur in simultaneous-speech settings. Misunderstanding this leads to incorrect feature extraction. Quick check: In a turn-taking system, what overlap-related features would be unavailable? (Answer: backchannels, interjections, partial/full overlaps)

- **Prompt Engineering for Structured Attributes**: The system converts speaker attributes into text prompts for LLM consumption. Understanding how to structure categorical vs numerical information affects prediction quality. Quick check: Why use ordinal labels ("Very Frequent") instead of raw percentages in prompts? (Answer: LLMs struggle with absolute numerical interpretation without distributional context)

## Architecture Onboarding

- **Component map**: Input (two-channel audio) -> ASR Layer (Whisper Turbo) -> Feature Extraction (laughter detector, silence-based response boundaries) -> Classification Layer (GPT-4o for backchannel, DistilRoBERTa for emotion, RoBERTa for sentiment) -> Normalization Layer (mean/IQR-based ordinal mapping) -> Prediction Layer (GPT-4o with structured prompt) -> Output (Big Five trait scores)

- **Critical path**: ASR timestamp accuracy -> overlap detection quality -> backchannel classification -> behavioral statistics -> LLM prompt quality -> prediction accuracy. Errors propagate forward; ASR timing errors compound in downstream overlap classification.

- **Design tradeoffs**: 700ms silence threshold based on human response latency literature but may misclassify fast turn-transitions as single turns; GPT-4o for both classification and prediction simplifies architecture but creates dependency on single model's capabilities; 20 sample responses per speaker balances token limits vs representativeness but may miss infrequent behavioral patterns.

- **Failure signatures**: Short conversations (<5 min) provide insufficient samples for reliable statistics; single-speaker-dominant dialogues result in sparse data for one speaker; high ASR error rate corrupts emotion/sentiment classification and backchannel detection; non-English speech causes emotion/sentiment classifiers to fail.

- **First 3 experiments**: 1) Sanity check: Run pipeline on held-out Fisher conversations, verify laughter detection and overlap identification against manual annotation on 10 random samples. 2) Ablation replication: Reproduce Table 4 results—test "Samples only" vs "Full prompt" conditions to confirm relative contribution of each attribute category. 3) Threshold sensitivity: Vary the 700ms silence threshold (±200ms) and measure impact on turn count and backchannel detection; identify stability range.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can conversational agents effectively utilize the predicted personality labels to dynamically adapt their own behavior during real-time interactions? Basis: The authors state future work will "develop conversational agents conditioned on personality labels." This remains unresolved as the paper demonstrates prediction but not agent feedback loops. Evidence needed: A user study showing agents using PersonaTAB predictions achieve higher user satisfaction than static agents.

- **Open Question 2**: Can synthetic dialogue datasets generated by LLMs serve as reliable training data for personality prediction models? Basis: The conclusion notes future work will "investigate synthetic datasets with personality labels," acknowledging scarcity of human-annotated speech data. This remains unresolved as it's unclear if LLM-generated dialogues capture nuanced behavioral cues. Evidence needed: An experiment showing a model trained solely on synthetic, labeled dialogues performs comparably to the current model.

## Limitations

- Dataset dependency on Fisher corpus (95 conversations, 190 speakers) limits generalizability to other languages, cultures, or communication domains.
- ASR error propagation could corrupt downstream features, though the paper doesn't report ASR error rates or their impact on predictions.
- Reliance on prompt templates and human evaluation protocols referenced on a project page without full specification in the paper.

## Confidence

- **High confidence**: The multi-modal integration mechanism is well-supported by architecture description and achieves stated cosine similarity of 0.503.
- **Medium confidence**: Context-aware backchannel classification via GPT-4o is described with clear methodology, but effectiveness depends on LLM quality and unspecified prompt engineering details.
- **Low confidence**: Claims about LLM numerical reasoning limitations and specific IQR-based thresholds (0.8/1.2×IQR) are based on author observations without external validation.

## Next Checks

1. **Ablation study replication**: Reproduce exact conditions from Table 4 by running "Samples only" versus "Full prompt" conditions on the same dataset split, measuring contribution of each attribute category to final prediction accuracy.

2. **Threshold sensitivity analysis**: Systematically vary the 700ms silence threshold for response boundary detection (±200ms in 50ms increments) and measure impact on turn count, backchannel detection rates, and final personality prediction accuracy to identify stability range.

3. **ASR error impact assessment**: Introduce controlled levels of synthetic transcription errors (word deletions, substitutions, timestamp jitter) into the Fisher corpus and measure degradation in emotion/sentiment classification accuracy and downstream personality prediction performance to quantify error propagation.