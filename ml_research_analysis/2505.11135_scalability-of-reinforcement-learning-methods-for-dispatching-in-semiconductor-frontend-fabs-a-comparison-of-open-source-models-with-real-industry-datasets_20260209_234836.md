---
ver: rpa2
title: 'Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor
  Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets'
arxiv_id: '2505.11135'
source_url: https://arxiv.org/abs/2505.11135
tags:
- scheduling
- https
- different
- minifab
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparison of open-source semiconductor
  manufacturing models with real industry data to evaluate reinforcement learning
  (RL) methods for dispatching. The authors investigate scalability by testing policy-gradient
  and evolution strategies approaches across three models: Minifab, SMT2020, and a
  large-scale industry dataset.'
---

# Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets

## Quick Facts
- arXiv ID: 2505.11135
- Source URL: https://arxiv.org/abs/2505.11135
- Reference count: 40
- Open-source models show double-digit improvements, industrial dataset shows single-digit improvements in tardiness and throughput

## Executive Summary
This paper evaluates reinforcement learning (RL) methods for semiconductor dispatching using open-source models (Minifab, SMT2020) and real industry data. The authors compare policy-gradient (PPO) and evolution strategies (CMA-ES) approaches, finding that ES scales significantly better than PPO for complex fab environments. Key findings include 4% tardiness improvement and 1% throughput improvement on the industry dataset, with attention-based neural architectures enabling queue-size-invariant decisions. The research identifies controlling relevant bottleneck tools and training on diverse datasets as crucial for optimization, though computational expense remains a challenge.

## Method Summary
The study compares two RL approaches for fab dispatching: CMA-ES with covariance matrix adaptation for episode-level optimization, and PPO with actor-critic architecture for per-step decisions. Both methods use attention-based neural networks to process variable-length queues of lots, with observations including due dates, wait times, processing times, and batch information. The methods are tested across three models: Minifab (5 machines, 6 steps), SMT2020 (1071-1265 machines), and a large industrial dataset (1000+ machines). Training uses multiple random seeds and loading scenarios to ensure generalization, with evaluation metrics focusing on tardiness reduction and throughput improvement.

## Key Results
- Evolution Strategies scales much better than PPO for complex semiconductor dispatching scenarios
- Up to 4% improvement in tardiness and 1% improvement in throughput for industry dataset
- Attention-based architectures enable queue-size-invariant dispatching decisions
- Multi-tool control yields greater improvements than single-area control, especially for batching tools

## Why This Works (Mechanism)

### Mechanism 1
Evolution Strategies scales better than PPO for complex semiconductor dispatching due to superior handling of delayed credit assignment and parallelization. ES evaluates complete episode returns rather than individual action-value estimates, eliminating the need for accurate value function approximation in environments with long cycle times and noisy KPI feedback. CMA-ES adapts sampling covariance for faster convergence while distributing perturbed policy evaluations across CPU cores with minimal synchronization overhead. The delayed effects of dispatching decisions (weeks in real fabs) make per-step value estimation unreliable for policy gradient methods.

### Mechanism 2
Attention-based neural architectures enable queue-size-invariant dispatching decisions by capturing lot-to-lot relationships. Each lot's feature vector is processed through shared projections for queries, keys, and values. Scaled dot-product attention infuses information from all lots into each lot's representation, then feed-forward layers compute dispatch scores independently. This handles variable queue lengths without padding or fixed-size windows. Relationships between lots in a queue (e.g., batching compatibility, priority conflicts) matter for optimal dispatching.

### Mechanism 3
Controlling multiple bottleneck tool types jointly yields greater improvements than single-area control, particularly when downstream batching depends on upstream WIP flow. Dispatching policies for batching tools (e.g., diffusion furnaces) cannot optimize batch formation without influence over WIP arrival patterns from upstream tools. Joint control coordinates lot releases to align batching opportunities with queue composition. Bottleneck interactions exist between tool areas; single-tool optimization is constrained by decisions outside agent control.

## Foundational Learning

- **Markov Decision Processes and Credit Assignment**
  - Why needed here: Understanding why ES avoids the credit assignment problem requires knowing that policy gradient methods must attribute episodic rewards to individual actions through value function estimates.
  - Quick check question: Can you explain why a sparse reward at episode end makes value-based RL difficult?

- **Attention Mechanisms (Scaled Dot-Product)**
  - Why needed here: The neural architecture uses attention to process variable-length queues; understanding Q/K/V projections is essential for debugging or modifying the policy network.
  - Quick check question: Given query matrix Q and key matrix K, what does softmax(QK^T/√d_k) compute and why does it handle variable input sizes?

- **Semiconductor Fab Scheduling Complexity (CJSSP)**
  - Why needed here: The paper assumes familiarity with reentrant flows, batching constraints, and why real fabs are harder than benchmark models.
  - Quick check question: Why does reentrant flow (same machine visited multiple times per job) increase scheduling complexity compared to linear flow shops?

## Architecture Onboarding

- Component map: Optimizer (CMA-ES or PPO) -> Training environment -> Simulator interface -> Simulator instances -> Neural network
- Critical path: 1. Register controlled tools with simulator at initialization; 2. Simulator calls environment when dispatch decision needed at controlled tool; 3. Environment extracts lot features, applies z-score normalization; 4. Policy network computes scores; ES selects highest-scoring lot, PPO samples from score distribution; 5. Episode returns collected; ES updates policy once per episode, PPO updates via mini-batch gradient steps
- Design tradeoffs: ES: better scalability, simpler implementation, no batching tool support, slower convergence; PPO: faster potential convergence, handles batching naturally, requires value function estimation (unstable for complex models), high memory for sample storage
- Failure signatures: PPO shows negative improvement on large models → value function inaccurate; ES converges slowly → increase parallel CPU cores; single-tool control shows minimal improvement → expand controlled tool set
- First 3 experiments: 1. Replicate Minifab single-tool (lithography) experiment with CMA-ES to validate setup; 2. Test PPO with different rollout fragment lengths on Minifab; 3. On target fab model, run ablation comparing single-bottleneck vs multi-tool control

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the approach be effectively adapted to a multi-agent system where individual policies are learned for specific tool types rather than a centralized policy? The current single-agent setup is limited to controlling bottleneck equipment groups and struggles with the diversity of dispatching strategies required for different tool types.

- **Open Question 2**: Does the utilization of dedicated high-memory hardware resources allow PPO to outperform Evolution Strategies on industrial-scale scenarios? The current computational constraints forced the use of truncated episodes for PPO, which resulted in unstable training and poor performance compared to the ES method.

- **Open Question 3**: How can Explainable AI (XAI) and validation frameworks be integrated to ensure the safety and generalization of these policies in real production environments? The study is confined to simulation environments; transferring "black-box" RL policies to real fabs poses safety risks and acceptance challenges among facility managers.

## Limitations
- Architectural details of attention network remain unspecified (number of layers, head dimensions, feed-forward sizes)
- CMA-ES and PPO hyperparameter values are not provided, making exact replication difficult
- Single-digit improvements on industrial datasets despite promising open-source results
- No comparison against classical dispatching heuristics beyond SRPT baseline on Minifab

## Confidence

- **High confidence**: Evolution Strategies scales better than PPO for complex semiconductor dispatching
- **Medium confidence**: Attention-based neural architectures enable queue-size-invariant dispatching
- **Medium confidence**: Multi-tool control yields greater improvements than single-area control

## Next Checks
1. Replicate the CMA-ES Minifab single-tool experiment to verify baseline performance and confirm the 15% tardiness improvement claim
2. Test attention network robustness by training on multiple random seeds and loading scenarios to measure generalization capability
3. Conduct ablation studies comparing ES results against classical heuristics (e.g., EDD, WSPT) on the SMT2020 model to better contextualize RL improvements