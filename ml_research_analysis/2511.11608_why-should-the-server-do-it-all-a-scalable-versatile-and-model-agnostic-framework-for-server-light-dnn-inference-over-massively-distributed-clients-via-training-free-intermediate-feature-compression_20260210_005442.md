---
ver: rpa2
title: 'Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic
  Framework for Server-Light DNN Inference over Massively Distributed Clients via
  Training-Free Intermediate Feature Compression'
arxiv_id: '2511.11608'
source_url: https://arxiv.org/abs/2511.11608
tags:
- inference
- split
- compression
- latency
- slicer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability and efficiency challenges
  in edge-cloud model partitioning (MP) for DNN inference, particularly in autoregressive
  (AR) LLM scenarios. The authors introduce SLICER, a retraining-free, architecture-agnostic
  framework that compresses intermediate features (IFs) to reduce communication and
  server load in split computing.
---

# Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression

## Quick Facts
- arXiv ID: 2511.11608
- Source URL: https://arxiv.org/abs/2511.11608
- Authors: Mingyu Sung; Suhwan Im; Daeho Bang; Il-Min Kim; Sangseok Yun; Jae-Mo Kang
- Reference count: 40
- Primary result: SLICER achieves up to 10x reduction in uplink volume and 4.4x reduction in server GPU time while maintaining task quality within 0-3 percentage points.

## Executive Summary
This paper addresses the scalability and efficiency challenges in edge-cloud model partitioning (MP) for DNN inference, particularly in autoregressive (AR) LLM scenarios. The authors introduce SLICER, a retraining-free, architecture-agnostic framework that compresses intermediate features (IFs) to reduce communication and server load in split computing. SLICER combines asymmetric top-K filtering (ATKF) for sparsity, magnitude-splitting (MS) for grouping non-zeros, and adaptive bit quantization (ABQ) for bitwidth selection under a distortion budget. Evaluated across vision and LLM workloads, SLICER achieves up to 10x reduction in uplink volume and 4.4x reduction in server GPU time, while maintaining task quality within 0-3 percentage points of baseline.

## Method Summary
SLICER introduces a retraining-free, architecture-agnostic framework for compressing intermediate features in split DNN inference. The method combines three core techniques: asymmetric top-K filtering (ATKF) for sparsity, magnitude-splitting (MS) for grouping non-zeros, and adaptive bit quantization (ABQ) for bitwidth selection under a distortion budget. ATKF prioritizes significant values while MS groups them for efficient encoding. ABQ dynamically adjusts quantization precision per layer based on a distortion budget. The framework is evaluated across vision and LLM workloads, demonstrating substantial reductions in communication and server load while maintaining task quality.

## Key Results
- Up to 10x reduction in uplink volume compared to uncompressed transmission
- 4.4x reduction in server GPU time for processing compressed intermediate features
- Maintained task quality within 0-3 percentage points of baseline across all tested scenarios

## Why This Works (Mechanism)
SLICER leverages the observation that intermediate features in DNNs often contain redundant or less important information that can be compressed without significant accuracy loss. The asymmetric top-K filtering identifies and prioritizes the most significant feature values, while magnitude-splitting groups remaining values for efficient encoding. Adaptive bit quantization then allocates bitwidth dynamically based on the importance of each feature group, ensuring minimal distortion within a defined budget. This multi-stage compression pipeline exploits the statistical properties of intermediate activations to achieve high compression ratios while preserving essential information for accurate inference.

## Foundational Learning
- **Model Partitioning (MP)**: Splitting DNN inference between edge and cloud devices
  - Why needed: Enables distributed inference while managing computational load
  - Quick check: Can the model be split at a layer boundary without breaking dependencies?

- **Intermediate Feature Compression**: Reducing the size of activations transmitted between devices
  - Why needed: Minimizes communication overhead in split computing scenarios
- **Asymmetric Top-K Filtering**: Selecting the most significant feature values for preservation
  - Why needed: Identifies and retains critical information while discarding less important data
- **Adaptive Bit Quantization**: Dynamically adjusting precision based on feature importance
  - Why needed: Optimizes the trade-off between compression ratio and accuracy
- **Distortion Budget**: Predefined tolerance for accuracy loss during compression
  - Why needed: Provides a quality control mechanism for compression decisions

## Architecture Onboarding
- **Component Map**: Edge Device -> ATKF -> MS -> ABQ -> Communication Layer -> Server
- **Critical Path**: ATKF (filtering) → MS (grouping) → ABQ (quantization) → Transmission
- **Design Tradeoffs**: Higher compression ratios may reduce accuracy; increased filtering aggressiveness may speed processing but lose detail
- **Failure Signatures**: Excessive distortion leading to accuracy drops; communication bottlenecks despite compression
- **First Experiments**:
  1. Measure compression ratio vs. accuracy trade-off for different K values in ATKF
  2. Compare communication overhead with and without SLICER across various network conditions
  3. Benchmark server GPU time savings for different DNN architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Performance under extreme memory constraints and highly heterogeneous client hardware remains uncertain
- The trade-off between compression efficiency and model accuracy degradation in long-sequence autoregressive scenarios is not fully characterized
- Robustness to real-world network variability and edge device failures is not explicitly tested

## Confidence
- **Core compression methodology (ATKF, MS, ABQ)**: High confidence
- **Architecture-agnostic claims**: Medium confidence
- **Generalization to unseen architectures**: Medium confidence
- **Performance under extreme constraints**: Low confidence

## Next Checks
1. Evaluate SLICER's robustness and accuracy on a wider range of DNN architectures, including those with skip connections, attention mechanisms, and varying activation functions, to confirm architecture-agnostic claims.
2. Test the framework's performance under severe memory and compute constraints (e.g., mobile-class devices) and highly heterogeneous client populations to identify potential bottlenecks or accuracy cliffs.
3. Conduct experiments in dynamic, lossy, and high-latency network environments to assess the resilience of the compression and communication pipeline under realistic edge-cloud conditions.