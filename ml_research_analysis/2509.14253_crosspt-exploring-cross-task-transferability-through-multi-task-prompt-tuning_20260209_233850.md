---
ver: rpa2
title: 'CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning'
arxiv_id: '2509.14253'
source_url: https://arxiv.org/abs/2509.14253
tags:
- prompts
- prompt
- source
- tasks
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge transfer in multi-task
  prompt tuning for NLP. The authors propose CrossPT, a modular framework that decomposes
  each task's prompt into shared source prompts and task-specific private prompts,
  combined via a learned attention mechanism.
---

# CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning

## Quick Facts
- arXiv ID: 2509.14253
- Source URL: https://arxiv.org/abs/2509.14253
- Reference count: 27
- Primary result: Modular framework achieves 3-5% higher accuracy than single-task prompt tuning in few-shot settings with 32 examples per task

## Executive Summary
This paper introduces CrossPT, a multi-task prompt tuning framework that addresses knowledge transfer challenges by decomposing task prompts into shared source prompts and task-specific private prompts, combined via a learned attention mechanism. The method enables controlled knowledge transfer while maintaining task specialization, achieving significant accuracy improvements over conventional single-task prompt tuning in low-resource settings. CrossPT is particularly effective in few-shot scenarios, demonstrating improved accuracy and robustness on GLUE benchmarks.

## Method Summary
CrossPT operates by decomposing each target prompt into shared source prompts (common across tasks) and task-specific private prompts (unique per task). These components are combined using a learned attention mechanism with temperature scaling to control the contribution of each source prompt. The framework uses asymmetric learning rates—higher rates for shared prompts and attention weights, lower rates for private prompts—to maintain stability during training. The approach is built on T5-base with frozen parameters, focusing only on prompt optimization through a linear prompt encoder.

## Key Results
- Achieves 3-5 percentage points higher accuracy than conventional single-task prompt tuning in few-shot settings with 32 examples per task
- Demonstrates significant improvements over traditional prompt tuning methods on GLUE and related benchmarks
- Particularly effective in low-resource scenarios where labeled data is limited
- Shows robust performance across diverse task types including NLI, sentiment analysis, and paraphrase detection

## Why This Works (Mechanism)

### Mechanism 1: Modular Decomposition for Interference Reduction
- Decomposing prompts into shared "source" and task-specific "private" components minimizes negative transfer while preserving shared semantic structures
- Source prompts capture cross-task generalizations while private prompts absorb task-specific noise or conflicting objectives
- Core assumption: Tasks share underlying semantic clusters that can be isolated from task-specific idiosyncrasies
- Break condition: If tasks are entirely unrelated, the shared prompts become an empty or noisy reservoir, degrading to vanilla prompt tuning

### Mechanism 2: Temperature-Scaled Attention for Source Selection
- An adaptive temperature mechanism prevents the attention distribution from flattening as the number of source prompts increases
- Uses τ = 1/M (where M is the number of source prompts) inside the softmax to sharpen the weight distribution
- Core assumption: A diverse bank of source prompts requires a mechanism to enforce sparsity/selectivity in knowledge retrieval
- Break condition: If M is set too high without sufficient training data, the sharpened attention may select irrelevant prompts due to undertraining

### Mechanism 3: Asymmetric Learning Rates (Fast-Slow Dynamics)
- Stability is maintained by learning shared generalization at a different speed than task-specific specialization
- Source prompts and attention weights use higher learning rates (e.g., 0.1-0.2) for rapid adaptation, while private prompts use lower rates (e.g., 0.05) for stable fine-grained fitting
- Core assumption: The "general" knowledge space requires broader, faster updates than the precise "local" adjustments needed for a specific task
- Break condition: If private prompt learning rate equals or exceeds the source rate, the specialization noise overwhelms the shared structure, leading to performance collapse

## Foundational Learning

- **Concept**: Soft Prompt Tuning (PEFT)
  - **Why needed here**: CrossPT is built entirely on the logic of freezing the LM and training only input embeddings
  - **Quick check question**: Can you explain why a frozen T5 model can adapt to a new task just by changing the input embedding prefix?

- **Concept**: Multi-Task Learning (MTL) & Negative Transfer
  - **Why needed here**: The paper positions itself as a solution to the "interference" problem in MTL
  - **Quick check question**: Why might training on MNLI (inference) and SST-2 (sentiment) together in a single shared prompt hurt performance compared to training them separately?

- **Concept**: Softmax Temperature Scaling
  - **Why needed here**: The paper relies on this to manage the "number of source prompts"
  - **Quick check question**: What happens to the probability distribution of a softmax function if you divide the logits by a number smaller than 1 (e.g., 0.1)?

## Architecture Onboarding

- **Component map**: Frozen T5-base -> Prompt Encoder (linear layer) -> Prompt Bank (Source + Private) -> Attention Module -> Combined Prompt -> Frozen LM

- **Critical path**:
  1. Initialization: Source prompts are initialized/pre-trained or random; private prompts are random
  2. Combination: For a task t, compute P_t = Σ w_ts P_s + w_tu P_u
  3. Forward: Prepend P_t to input; pass through frozen LM
  4. Backward: Update Encoder, Attention, and Prompts using distinct learning rates (Source/Attention: Fast; Private: Slow)

- **Design tradeoffs**:
  - Source Prompt Count (M): Paper suggests M ≈ 3 for few-shot GLUE (matches task clusters)
  - Initialization Strategy: Pre-training source prompts is critical for <16 shots
  - Task Prefixes: If using only shared prompts, prefixes are vital; with private prompts, prefixes become less critical

- **Failure signatures**:
  - Flat Attention: Weights are nearly uniform across all source prompts
  - Performance Collapse: Accuracy randomly fluctuates
  - Negative Transfer: Performance drops below single-task baseline

- **First 3 experiments**:
  1. Vanilla vs. Modular: Run baseline "P" (Private only) vs. "SLP" (Shared+Private) on 32-shot GLUE
  2. LR Grid Search: Validate the "Fast/Slow" claim by sweeping Private LR
  3. Source Count Ablation: Run "SL" (Shared only) with M=1, 2, 3, 8 to visualize fragmentation effect

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can adaptive mechanisms such as dynamic pruning or relevance-based filtering automatically determine the optimal number and composition of source prompts without manual tuning?
- **Basis**: Section 6.6 states that selecting optimal source prompts requires careful tuning and suggests more adaptive mechanisms
- **Why unresolved**: The paper empirically determines optimal source prompt counts but requires manual experimentation
- **What evidence would resolve it**: A study implementing and evaluating adaptive source prompt selection methods across diverse task sets

### Open Question 2
- **Question**: How does CrossPT perform on open-ended generation tasks such as question answering, summarization, or dialogue?
- **Basis**: Section 6.6 identifies enhancing generalization to tasks with open-ended outputs as future work
- **Why unresolved**: All experiments are conducted on GLUE classification/regression tasks
- **What evidence would resolve it**: Empirical evaluation of CrossPT on standard generation benchmarks

### Open Question 3
- **Question**: At what sample size threshold does the benefit of prompt initialization over multi-task learning from scratch disappear?
- **Basis**: Section 4.3 shows initialization-based methods outperform at 8 samples, but scratch methods become superior at 32 samples
- **Why unresolved**: Only three sample sizes are tested, and the crossover behavior varies across methods and tasks
- **What evidence would resolve it**: A fine-grained analysis testing sample sizes between 8-32 across multiple tasks

### Open Question 4
- **Question**: How does CrossPT scale to very large language models (e.g., 7B+ parameters), and does the optimal number of source prompts change with model scale?
- **Basis**: The paper uses only T5-base (220M parameters) and notes memory costs increase with source prompts
- **Why unresolved**: Larger models have different inductive biases and capacity, potentially altering the balance between shared and private representations
- **What evidence would resolve it**: Experiments applying CrossPT to LLaMA-7B or T5-XXL on the same benchmarks

## Limitations

- Architectural Under-specification: The attention module architecture is not fully specified, only described as producing logits
- Limited Scope of Evaluation: Results primarily validated on GLUE with few-shot settings; performance on other domains remains untested
- Label Sensitivity: Method shows significant performance variation based on label format (natural vs. synthetic labels)

## Confidence

- **High Confidence**: The interference reduction mechanism (private prompts) and temperature scaling approach for source selection are well-supported by experimental evidence
- **Medium Confidence**: The asymmetric learning rate approach shows clear benefits but optimal ratios may depend on task characteristics
- **Low Confidence**: The claim about source prompt pre-training being critical for <16 samples lacks comprehensive validation across different initialization strategies

## Next Checks

1. **Architecture Specification Validation**: Implement and compare different attention module architectures (linear vs. multi-layer networks) to determine the minimal sufficient architecture

2. **Cross-Domain Generalization**: Test CrossPT on non-GLUE multi-task settings (e.g., SuperGLUE, code completion tasks, or vision-language tasks) to evaluate domain transfer capabilities

3. **Label Format Robustness**: Systematically evaluate CrossPT's performance across different label formats (natural, synthetic, numeric) on the same task to quantify the impact of label representation on interference and knowledge transfer