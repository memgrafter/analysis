---
ver: rpa2
title: 'Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution
  Environments'
arxiv_id: '2506.23706'
source_url: https://arxiv.org/abs/2506.23706
tags:
- https
- audit
- audits
- arxiv
- attestable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Attestable Audits enable verifiable AI safety benchmarking by running
  audits inside Trusted Execution Environments, ensuring model and data confidentiality
  while providing cryptographic proof of compliance. The system allows auditors to
  verify model behavior without accessing sensitive weights or datasets, using hardware-backed
  attestation to bind audit results to specific model and code versions.
---

# Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments

## Quick Facts
- arXiv ID: 2506.23706
- Source URL: https://arxiv.org/abs/2506.23706
- Reference count: 40
- Attestable Audits enable verifiable AI safety benchmarking using TEEs with 51.4% MMLU accuracy on quantized Llama-3.1-8B

## Executive Summary
Attestable Audits introduces a system for verifiable AI safety benchmarking by executing audits within Trusted Execution Environments (TEEs). The approach ensures both model and data confidentiality while providing cryptographic proof of compliance through hardware-backed attestation. By running audits in isolated enclaves, the system allows independent verification of model behavior without exposing sensitive weights or datasets to auditors.

The prototype implementation demonstrates feasibility on AWS Nitro Enclaves, achieving reasonable accuracy and toxicity benchmarks for an 8B parameter model. The system addresses critical verification gaps in AI governance by enabling transparent, reproducible audits that protect proprietary information while maintaining accountability.

## Method Summary
The system leverages Trusted Execution Environments to create isolated execution environments where AI safety audits can be performed without exposing model weights or training data. Audits run within hardware-secured enclaves that provide cryptographic attestation, binding results to specific model and code versions. The approach uses hardware-based isolation to protect confidentiality while enabling third-party verification of compliance with safety benchmarks.

## Key Results
- Quantized Llama-3.1-8B achieves 51.4% MMLU accuracy and 2.4% toxicity rate in TEE environment
- CPU inference costs 21.7× more than GPU, demonstrating performance trade-offs
- Prototype successfully demonstrates cryptographic attestation binding audit results to specific model versions

## Why This Works (Mechanism)
The system exploits hardware security features of modern processors to create trusted execution environments that are isolated from the host operating system and even privileged users. TEEs provide both confidentiality (preventing unauthorized access to model weights and data) and integrity (ensuring audit code executes as intended without tampering). Cryptographic attestation creates unforgeable proofs that can be verified externally, establishing trust in the audit process without requiring auditors to have access to sensitive information.

## Foundational Learning
- **Trusted Execution Environments**: Isolated secure areas in processors that protect code and data from the rest of the system; needed to prevent unauthorized access to model weights during audits; quick check: verify enclave isolation from host OS
- **Hardware-backed Attestation**: Cryptographic proofs generated by secure hardware that verify software identity and integrity; needed to provide verifiable evidence of compliance; quick check: confirm attestation chain of trust
- **Quantization**: Technique for reducing model precision to improve efficiency; needed to fit larger models into limited TEE memory; quick check: measure accuracy loss vs memory savings
- **Enclave Memory Constraints**: Limited secure memory available within TEEs; needed to understand model size limitations; quick check: benchmark maximum model size for target TEE
- **Remote Attestation**: Process where remote parties verify the integrity of TEE software; needed for third-party audit verification; quick check: validate remote attestation protocol implementation
- **Confidential Computing**: General concept of protecting data in use through hardware isolation; needed to understand broader security guarantees; quick check: verify data remains encrypted outside TEE

## Architecture Onboarding
**Component Map**: Model Provider -> TEE Enclave -> Attestation Service -> Auditor Verification

**Critical Path**: Model weights are loaded into TEE -> Audit code executes within enclave -> Cryptographic attestation generated -> Results verified by auditor

**Design Tradeoffs**: Confidentiality vs performance (CPU inference 21.7× more expensive than GPU), model size vs accuracy (quantization reduces accuracy but enables larger models), security vs usability (hardware requirements may limit deployment flexibility)

**Failure Signatures**: Compromised TEE hardware breaks confidentiality guarantees, attestation verification failures indicate tampered code or unauthorized modifications, performance degradation suggests inefficient enclave utilization

**First Experiments**:
1. Verify basic TEE isolation by attempting unauthorized memory access from host OS
2. Test attestation verification process with known good and modified code versions
3. Benchmark inference latency and accuracy across different quantization levels

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability challenges for models beyond 8B parameters due to TEE memory constraints
- 21.7× cost increase for CPU inference compared to GPU, raising economic viability concerns
- Lack of analysis on concurrent audit handling and operational overhead for sustained auditing operations

## Confidence
- High: Technical feasibility of prototype implementation and core security guarantees from hardware attestation
- Medium: Confidentiality protections and practical utility in real-world governance scenarios dependent on ecosystem adoption
- Low: Economic viability at scale given significant cost increases and lack of total cost of ownership analysis

## Next Checks
1. Benchmark performance and accuracy degradation for larger models (20B-70B parameters) to assess scalability limits
2. Conduct penetration testing on enclave isolation to verify that model weights and data remain protected from privileged system access
3. Implement a multi-tenant audit scheduling system to measure operational overhead and latency when handling concurrent audit requests from multiple organizations