---
ver: rpa2
title: 'CognArtive: Large Language Models for Automating Art Analysis and Decoding
  Aesthetic Elements'
arxiv_id: '2502.04353'
source_url: https://arxiv.org/abs/2502.04353
tags:
- nvidia
- bge-m3
- sbert
- openai
- material
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the application of Large Language Models
  (GPT-4V, Gemini 2.0, and GPT-4) to automate the analysis of over 15,000 artworks
  from 23 prominent artists across 34 styles spanning five centuries. Using a formal
  art analysis framework, the models extracted insights into technical elements (form,
  scale, light, contrast, movement, material, and techniques) and expressive features
  (color, figures, objects, and emotional themes).
---

# CognArtive: Large Language Models for Automating Art Analysis and Decoding Aesthetic Elements

## Quick Facts
- **arXiv ID**: 2502.04353
- **Source URL**: https://arxiv.org/abs/2502.04353
- **Reference count**: 40
- **Primary result**: LLMs (GPT-4V, Gemini 2.0, GPT-4) extracted technical and expressive elements from 15,000+ artworks with highest median embedding similarity scores of 0.61-0.70 using NVIDIA NV-Embed-v2

## Executive Summary
This study demonstrates the application of Large Language Models to automate the analysis of over 15,000 artworks spanning five centuries and 34 artistic styles. Using GPT-4V, Gemini 2.0, and GPT-4, the research extracted both technical elements (form, scale, light, contrast, movement, material, techniques) and expressive features (color, figures, objects, emotional themes) from artworks by 23 prominent artists. The analysis revealed consistent trends including the dominance of natural forms, the rise of geometric forms in recent years, and the increasing prevalence of muted color tones. Oil on canvas remained the dominant medium throughout the analyzed period.

## Method Summary
The study employed a formal art analysis framework where three LLMs (GPT-4V, Gemini 2.0, GPT-4) analyzed 15,000+ artworks from 23 artists across 34 styles spanning five centuries. The models extracted insights into technical elements (form, scale, light, contrast, movement, material, techniques) and expressive features (color, figures, objects, emotional themes). Results were evaluated using cosine similarity between embeddings generated by four models (SBERT, BGE-m3, OpenAI, and NVIDIA NV-Embed-v2) and ground-truth art style descriptions, with NVIDIA NV-Embed-v2 achieving the highest median scores (0.61-0.70) across all focus areas.

## Key Results
- NVIDIA NV-Embed-v2 achieved the highest median embedding similarity scores (0.61-0.70) across all focus areas
- Natural forms dominated artistic composition while geometric forms increased in prevalence in recent years
- Oil on canvas remained the dominant artistic medium across the five-century analysis period
- Light and contrast were predominantly used to enhance depth and draw attention in artworks

## Why This Works (Mechanism)
Large Language Models can process and analyze visual artwork by extracting both objective technical elements and subjective expressive features through their multimodal capabilities. The models leverage their training on vast datasets of text and image pairs to recognize patterns in artistic composition, color usage, and thematic elements. By applying a formal art analysis framework, the LLMs can systematically identify and categorize artistic elements across different time periods and styles, revealing consistent trends and patterns in art history.

## Foundational Learning
- **Formal art analysis framework**: Provides structured methodology for extracting both technical and expressive elements from artwork; needed to ensure systematic and comparable analysis across different artists and time periods; quick check: verify framework covers all relevant artistic elements
- **Cosine similarity in embedding space**: Measures semantic similarity between extracted insights and ground-truth descriptions; needed to evaluate the accuracy of LLM analysis quantitatively; quick check: compare similarity scores across different embedding models
- **Multimodal LLM capabilities**: Enables processing of both visual and textual information simultaneously; needed to analyze artwork images and generate descriptive insights; quick check: test model performance on different types of artistic content

## Architecture Onboarding
**Component Map**: Artwork Dataset -> LLM Analysis Pipeline (GPT-4V, Gemini 2.0, GPT-4) -> Feature Extraction (Technical + Expressive) -> Embedding Generation (4 models) -> Similarity Evaluation -> Trend Analysis

**Critical Path**: Artwork input → LLM analysis → Feature extraction → Embedding generation → Similarity scoring → Trend identification

**Design Tradeoffs**: 
- Multiple LLM models provide redundancy but increase computational cost
- Four different embedding models offer robust evaluation but require significant processing resources
- Large dataset ensures statistical significance but may include noise from less-represented artistic styles

**Failure Signatures**: 
- Low embedding similarity scores indicate poor alignment between LLM analysis and ground truth
- Inconsistent results across different LLM models suggest model-specific biases or limitations
- Missing or incomplete feature extraction points to framework gaps or model comprehension issues

**First Experiments**:
1. Run single artwork through all three LLMs and compare consistency of extracted features
2. Test embedding similarity scores using a small subset of known artworks with established analysis
3. Compare LLM analysis results against human expert evaluations on sample artworks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on embedding similarity rather than human expert validation of extracted insights
- Potential biases in LLM training data may skew analytical results
- 15,000 artwork sample may not be fully representative across all art movements and cultural contexts

## Confidence
- **Technical extraction (form, scale, light, contrast, movement, material, techniques)**: Medium - can be objectively verified but may miss subtle technical nuances
- **Expressive feature extraction (color, figures, objects, emotional themes)**: Low - subjective nature of artistic interpretation and LLM limitations in capturing nuanced human perception
- **Trend identification across centuries**: Medium - impressive scope but accuracy remains uncertain without expert validation

## Next Checks
1. Conduct blind comparison study where human art experts evaluate same artworks and their analyses are compared against LLM outputs for accuracy and completeness
2. Perform cross-validation by testing same analytical framework on separate, independently curated dataset of artworks not used in model training or initial analysis
3. Implement error analysis to identify specific categories where LLM performance is weakest, particularly focusing on complex artistic movements or culturally specific traditions requiring contextual knowledge