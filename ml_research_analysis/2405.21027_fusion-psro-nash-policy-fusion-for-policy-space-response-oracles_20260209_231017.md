---
ver: rpa2
title: 'Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles'
arxiv_id: '2405.21027'
source_url: https://arxiv.org/abs/2405.21027
tags:
- policy
- fusion
- uni00000013
- uni0000004c
- nash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fusion-PSRO, a method to improve policy initialization
  in Policy Space Response Oracles (PSRO) for zero-sum games. It introduces Nash Policy
  Fusion, which initializes new policies by weighted averaging historical best responses
  according to the current meta-Nash equilibrium.
---

# Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles

## Quick Facts
- **arXiv ID:** 2405.21027
- **Source URL:** https://arxiv.org/abs/2405.21027
- **Reference count:** 40
- **Primary result:** Fusion-PSRO reduces exploitability by up to 78% on Liar's Dice by initializing new policies via Nash-weighted averaging of historical best responses.

## Executive Summary
This paper introduces Fusion-PSRO, a method to improve policy initialization in Policy Space Response Oracles (PSRO) for solving two-player zero-sum games. The core innovation is Nash Policy Fusion, which initializes new policies by weighted averaging historical best responses according to the current meta-Nash equilibrium weights. This serves as an implicit guide policy, enabling faster exploration and better approximation of best responses. Empirical results on Leduc Poker, Goofspiel, and Liar's Dice show that Fusion-PSRO achieves significantly lower exploitability than standard PSRO variants, with minimal computational overhead.

## Method Summary
Fusion-PSRO extends standard PSRO by initializing new best response policies using a Nash-weighted average of all historical policies in the current population. At each iteration, after computing the meta-Nash equilibrium (Meta-NE), the new policy's parameters are set to θ^(t+1) = Σ σ^t_i(π)·θ_π, where σ^t_i are the Meta-NE weights. This fusion acts as an implicit guide policy, starting exploration near the current Meta-NE. The method is efficient (O(M) for M historical policies) and applies to any PSRO variant using a meta-NE solver. Fusion begins at iteration c, which is game-dependent.

## Key Results
- Fusion-PSRO achieves up to 78% exploitability reduction on Liar's Dice compared to standard PSRO.
- Fusion-PSD-PSRO reduces exploitability from 0.538 to 0.377 on Leduc Poker.
- The fusion computation adds only 0.01% overhead in Leduc Poker experiments.
- Empirically validates improved best response approximation through faster convergence to higher rewards.

## Why This Works (Mechanism)

### Mechanism 1: Implicit Guide Policy via Nash-Weighted Fusion
The fused policy acts as a first-order approximation to a policy ensemble, initializing new policies near the current Meta-NE. This reduces early exploration complexity and sample cost, similar to Jump-Start Reinforcement Learning. The core assumption is that the state visitation distribution of the fused policy covers critical states better than random initialization. Break condition: if Meta-NE computation is unstable or converges to a poor local equilibrium early, fusion weights will guide policies toward suboptimal regions.

### Mechanism 2: Nash Weighted Moving Average
Iteratively fusing policies based on dynamically updated Meta-NE weights creates a weighted moving average over policy parameters. This helps discover flatter minima and improves robustness compared to standard weight averaging methods. The Meta-NE provides dynamic adjustment of each policy's contribution. Break condition: if policy population lacks diversity or historical policies are too similar, the moving average effect is minimal.

### Mechanism 3: Enhanced Best Response Approximation
The combined effect of the implicit guide and weighted moving average allows new policies to more efficiently approximate a Best Response to the opponent's current meta-strategy. By starting from a fused policy with good performance against the Meta-NE, the RL oracle can focus on refinement rather than basic learning. Break condition: if the RL oracle lacks sufficient capacity or training episodes, it may fail to effectively refine the fused starting point.

## Foundational Learning

- **Concept: Policy Space Response Oracles (PSRO)** - A framework for solving extensive-form games by iteratively adding best responses to a policy population. Understanding the PSRO loop is essential to see where fusion fits in. Quick check: Can you describe the three main steps in one iteration of standard PSRO?

- **Concept: Nash Equilibrium (NE) and Meta-Strategy** - The Meta-NE is computed on the restricted game defined by the current policy population, and its weights drive the fusion process. Quick check: In a two-player zero-sum game, what does a Nash Equilibrium represent, and what is a "mixed strategy"?

- **Concept: Weight Averaging in Neural Networks** - Fusion uses weighted averaging of model parameters, similar to techniques like SWA. Understanding why averaging weights can improve generalization is key. Quick check: What is the primary benefit of Stochastic Weight Averaging (SWA) in terms of the loss landscape?

## Architecture Onboarding

- **Component map:** PSRO loop with added Fusion Logic (computes weighted average of historical policies using Meta-NE weights) and Initialization Hook (modifies BR training to use fused parameters).
- **Critical path:** Compute Meta-NE -> Fusion Module -> Initialize BR Policy. If Meta-NE is wrong or fusion logic is buggy, all subsequent BR training is negatively impacted. The fusion start iteration c is also critical.
- **Design tradeoffs:** Computational overhead vs. performance gain (minimal overhead claimed). Complexity vs. generality (designed for zero-sum games with Nash solver available; weighted averaging is fast but may not be optimal for all architectures).
- **Failure signatures:** No improvement over baseline (fusion start iteration c set too early or late); performance collapse (Meta-NE solver fails or becomes unstable); high variance (RL oracle struggles to fine-tune fused policy).
- **First 3 experiments:**
  1. **Sanity Check on Small Game:** Re-run "Non-Transitive Mixture Game" experiment to verify fusion trajectory starts near Gaussian centers as shown in Figure 4.
  2. **Ablation on Fusion Start Iteration (c):** On Leduc Poker, run sweep for c ∈ {2, 5, 10, 20} and plot exploitability vs. iterations.
  3. **Compare with Inheritance Baselines:** On Liar's Dice, compare Fusion-PSRO against PSRO with random scratch initialization and single policy inheritance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Fusion-PSRO effectively generalize to general-sum games where equilibrium selection is ambiguous compared to the zero-sum setting?
- **Basis in paper:** The paper explicitly restricts its scope to "zero-sum games" in the Abstract and Introduction.
- **Why unresolved:** The Nash Policy Fusion relies on the Meta-Nash Equilibrium for weights, but in general-sum games, multiple equilibria may exist, and convergence properties are less theoretically grounded.
- **What evidence would resolve it:** Empirical evaluation on standard general-sum benchmarks (e.g., coordination or prisoner's dilemma games) analyzing if fusion weights stabilize around a specific equilibrium.

### Open Question 2
- **Question:** What are the theoretical requirements for the Meta-Strategy Solver (MSS) to ensure Nash Policy Fusion improves convergence, given its failure with Projected Replicator Dynamics (PRD)?
- **Basis in paper:** Section 5.4 and Figure 9a show Fusion-PSRO fails with PRD, hypothesizing that PRD "focuses excessively on a small subset" of policies.
- **Why unresolved:** The paper demonstrates that weighted averaging helps, but does not define necessary conditions for the weight distribution required for fusion to be beneficial.
- **What evidence would resolve it:** Theoretical analysis deriving bounds on the entropy of the Meta-Strategy distribution required for the fused policy to serve as a valid "implicit guide policy."

### Open Question 3
- **Question:** Does the efficiency of Nash Policy Fusion hold in large-scale continuous action spaces like those found in complex MOBA or RTS games?
- **Basis in paper:** While the Introduction cites StarCraft and DOTA2 as motivation, the Experiments section is restricted to smaller benchmarks like Leduc Poker, Goofspiel, and Liar's Dice.
- **Why unresolved:** The complexity of computing the Meta-NE and stability of weight averaging in deep neural networks with high-dimensional action spaces remain unverified.
- **What evidence would resolve it:** Application to a high-dimensional environment (e.g., StarCraft II micromanagement) comparing exploitability and convergence speed against standard baselines.

## Limitations
- The analysis is primarily empirical with limited theoretical grounding; formal proofs of convergence or performance bounds are absent.
- Effectiveness hinges critically on the stability and quality of the Meta-NE computation; poor early equilibrium estimates could mislead the fusion process.
- Study focuses on discrete-action, tabular-like benchmarks, leaving open questions about performance in large or continuous-action spaces.

## Confidence

- **High Confidence:** The core empirical observation that Nash Policy Fusion reduces exploitability compared to standard PSRO baselines is well-supported by results across multiple games. The efficiency claim (minimal overhead) is credible given the O(M) complexity.
- **Medium Confidence:** The mechanisms proposed (implicit guide policy, weighted moving average) are plausible explanations for observed gains, but direct experimental validation of these specific effects is limited.
- **Low Confidence:** The claim that the method is "simple and general" is questionable; the reliance on a stable Nash solver and heuristic choice of c limit its generality.

## Next Checks

1. **Meta-NE Stability Test:** Conduct experiments where the Meta-NE solver is deliberately made unstable (e.g., using poor convergence threshold) early in training. Measure if this leads to worse performance or convergence, directly testing the dependency on stable Meta-NE estimates.

2. **Mechanism Isolation Experiment:** On Leduc Poker, create a synthetic oracle that receives the fused policy but is frozen for the first N training steps. Measure its initial performance against the Meta-NE to isolate the "implicit guide" effect from subsequent RL refinement.

3. **Architecture Robustness Check:** Replicate the main Leduc Poker experiment using a significantly different policy architecture (e.g., smaller network or different activation function). Verify if exploitability gains persist, testing the claim that the method works across architectures without modification.