---
ver: rpa2
title: 'AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for
  Small-footprint Keyword Spotting'
arxiv_id: '2505.11817'
source_url: https://arxiv.org/abs/2505.11817
tags:
- learning
- data
- feature
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnalyticKWS is an exemplar-free continual learning method for small-footprint
  keyword spotting that addresses catastrophic forgetting without storing past data.
  It uses a closed-form analytic solution based on recursive least squares to update
  model parameters, requiring only a single epoch per new task and eliminating gradient-based
  backpropagation.
---

# AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting

## Quick Facts
- arXiv ID: 2505.11817
- Source URL: https://arxiv.org/abs/2505.11817
- Reference count: 17
- Primary result: AnalyticKWS achieves up to 87.99% accuracy on SC-100 with a single epoch per task and no exemplar buffer

## Executive Summary
AnalyticKWS presents an exemplar-free continual learning approach for small-footprint keyword spotting that eliminates catastrophic forgetting without storing past data. The method uses a closed-form analytic solution based on recursive least squares to update model parameters, requiring only a single epoch per new task and eliminating gradient-based backpropagation. This approach preserves user privacy and reduces memory consumption, making it suitable for resource-constrained edge devices while achieving higher accuracy and better backward transfer than existing continual learning methods.

## Method Summary
AnalyticKWS addresses the challenge of class-incremental learning in keyword spotting by avoiding the storage of past exemplars through an analytic solution. Instead of traditional gradient-based backpropagation, it employs recursive least squares to compute optimal parameter updates for each new keyword class. The method balances learning new tasks while avoiding forgetting of previously learned keywords by leveraging a validation set to find optimal trade-offs. This closed-form solution enables efficient training with a single epoch per task, making it particularly suitable for edge devices with limited computational resources.

## Key Results
- Achieves up to 87.99% accuracy on SC-100 dataset with 100 keyword classes
- Requires only 5.82 seconds per task on SC-100 dataset
- Outperforms existing continual learning methods in both accuracy and backward transfer across all tested datasets (GSC-v1, GSC-v2, SC-100)

## Why This Works (Mechanism)
The method works by replacing gradient-based optimization with an analytic solution that directly computes optimal parameter updates. This closed-form approach eliminates the need for multiple training epochs and backpropagation, significantly reducing computational requirements. The recursive least squares framework allows for efficient incorporation of new keyword classes while maintaining performance on previously learned classes through a validation-guided trade-off mechanism.

## Foundational Learning
- Recursive Least Squares: Why needed - Provides efficient parameter updates without iterative optimization; Quick check - Verify convergence properties on incremental data
- Class Incremental Learning: Why needed - Enables continuous addition of new keywords without retraining from scratch; Quick check - Measure forgetting on previous classes
- Small-footprint KWS: Why needed - Critical for edge deployment where computational resources are limited; Quick check - Monitor memory usage and latency

## Architecture Onboarding

Component Map:
Feature Extractor -> Analytic Update Module -> Keyword Classifier

Critical Path:
1. Extract features from new keyword audio
2. Apply recursive least squares update to model parameters
3. Classify new keyword while preserving old keyword performance

Design Tradeoffs:
- Single epoch training vs. multi-epoch accuracy improvement
- Analytic solution speed vs. potential sub-optimality compared to full optimization
- No exemplar storage vs. potential loss of fine-grained adaptation capability

Failure Signatures:
- Performance degradation on older keywords indicates insufficient regularization
- Slow convergence on new keywords suggests poor initialization or feature extraction issues
- High memory usage may indicate inefficient parameter update implementation

First Experiments:
1. Test incremental learning with 2-3 keyword classes to verify basic functionality
2. Evaluate backward transfer performance on held-out validation set
3. Measure training time and memory usage on target edge hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on fixed past model parameters and assumes stable problem structure across incremental steps
- Effectiveness on very large-scale keyword vocabularies (beyond 100 classes) remains untested
- Performance in highly dynamic environments with significant concept drift is unclear

## Confidence

**High Confidence:**
- Core analytic solution based on recursive least squares
- Elimination of gradient-based backpropagation
- Demonstrated superiority over existing methods on tested datasets

**Medium Confidence:**
- Scalability claims for larger keyword vocabularies
- Method's robustness to concept drift in real-world deployment scenarios
- Computational efficiency claims on diverse edge hardware

## Next Checks
1. Test scalability on keyword vocabularies exceeding 100 classes (e.g., 200-500 keywords) to validate performance claims at larger scales
2. Evaluate performance under concept drift scenarios where keyword characteristics change over time to assess robustness
3. Measure actual memory footprint and processing latency on representative edge devices (e.g., ARM Cortex-M, Raspberry Pi) to verify deployment feasibility claims