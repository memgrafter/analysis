---
ver: rpa2
title: How much do LLMs learn from negative examples?
arxiv_id: '2503.14391'
source_url: https://arxiv.org/abs/2503.14391
tags:
- examples
- negative
- training
- answers
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) learn
  from negative examples during training. The authors introduce a likelihood-ratio
  (Likra) model that trains two separate heads - one on positive (correct) examples
  and one on negative (incorrect) examples - to quantify the impact of negative examples
  on model accuracy.
---

# How much do LLMs learn from negative examples?

## Quick Facts
- arXiv ID: 2503.14391
- Source URL: https://arxiv.org/abs/2503.14391
- Authors: Shadi Hamdan; Deniz Yuret
- Reference count: 7
- Key outcome: Negative examples improve model accuracy 10× more than positive examples during a critical training phase, leading to sharp jumps in learning curves

## Executive Summary
This paper investigates how large language models learn from negative examples during training, introducing a likelihood-ratio (Likra) model with separate heads for positive and negative examples. The authors demonstrate that negative examples—particularly plausible but incorrect "near-misses"—significantly improve model accuracy and discrimination ability beyond what positive examples alone can achieve. Their experiments reveal a critical training phase where negative examples produce sharp accuracy jumps, suggesting they help models better distinguish correct answers from plausible but incorrect ones.

## Method Summary
The authors introduce Likra, a two-head model that trains separate LoRA adapters—one on correct Q-A pairs and one on incorrect Q-A pairs. During inference, they use the log-likelihood ratio L+ - L- as the scoring function. The method is evaluated on multiple-choice benchmarks (ARC-Challenge and HellaSwag) with various base models, comparing SFT with positive examples alone versus Likra with both positive and negative examples.

## Key Results
- During a critical training phase (64-256 examples), negative examples improve accuracy 10× more than positive examples alone
- Near-miss negative examples (plausible but incorrect answers) are significantly more effective than random incorrect answers
- Models trained with negative examples achieve 80%+ accuracy compared to 60-66% for supervised fine-tuning alone
- The critical-phase produces a sharp jump in the learning curve unlike the smooth improvement of standard SFT

## Why This Works (Mechanism)

### Mechanism 1: Likelihood-Ratio Discrimination
The ratio of positive-head to negative-head likelihoods provides sharper discrimination than positive likelihood alone. Two independently trained LoRA heads assign contrasting likelihoods, with the log-ratio L+ - L- amplifying differences where the positive head assigns similar likelihoods to correct and plausible-incorrect answers.

### Mechanism 2: Near-Miss Negative Examples Provide Higher Teaching Signal
Plausible but incorrect answers (near-misses) provide stronger training signal than irrelevant or unrelated negatives. Near-misses lie in the decision boundary region, helping the negative head learn a sharper boundary that generalizes to held-out test-time distractors.

### Mechanism 3: Critical-Phase Sharp Jump in Learning
Negative examples produce a step-function-like accuracy jump during a critical training phase (64-256 examples), not the smooth curve of SFT. A small number of negative examples rapidly reshapes the probability landscape, "flipping a switch" that causes the model to sharply separate correct from plausible-incorrect answers.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed here: Likra builds on SFT as the positive head; understanding SFT behavior is necessary to interpret Likra gains
  - Quick check question: What does the SFT learning curve look like on ARC-Challenge (Figure 1)?

- **Concept: Log-Likelihood Ratio for Classification**
  - Why needed here: Likra's inference rule uses the log-likelihood ratio L+ - L- as its scoring function
  - Quick check question: If L+(correct) ≈ L+(incorrect) but L-(incorrect) >> L-(correct), what happens to the ratio for the incorrect answer?

- **Concept: LoRA Adapters**
  - Why needed here: Likra is implemented as two LoRA adapters on a single foundation model
  - Quick check question: Why might LoRA be preferred over full fine-tuning for training separate heads efficiently?

## Architecture Onboarding

- **Component map:** Base model -> Positive head (LoRA on correct Q-A pairs) -> Negative head (LoRA on incorrect Q-A pairs) -> Inference (score = L+ - L-)

- **Critical path:**
  1. Prepare positive dataset (correct Q-A pairs)
  2. Prepare negative dataset (incorrect Q-A pairs; prefer near-misses from same-question distractors)
  3. Train positive head via SFT (single epoch, batch size 8, lr=1e-4)
  4. Train negative head similarly on negative dataset
  5. At inference, compute log-likelihoods for each candidate answer under both heads and take the difference

- **Design tradeoffs:**
  - Likra cannot easily be used for open-ended generation; it is designed for multiple-choice scoring
  - Requires storing and running two LoRA adapters; inference cost is ~2× a single SFT model
  - Choice of negative examples matters: near-misses > irrelevant > unrelated (Section 4.3)

- **Failure signatures:**
  - No accuracy jump: Check that negatives are near-miss (plausible distractors), not random unrelated text
  - Accuracy degrades at high negative-head weight: Reduce weight from 1.0 toward 0.5 (Figure 4)
  - Base-Likra underperforms SFT-Likra: Ensure negative head is trained on sufficient examples (critical phase is 64-256)

- **First 3 experiments:**
  1. Reproduce Figure 1: Compare SFT vs. Likra learning curves on ARC-Challenge with Mistral-7B-v0.1
  2. Ablate negative example type: Train three Likra variants with incorrect, irrelevant, and unrelated negatives (Figure 5)
  3. Sweep negative-head weight: Plot accuracy vs. weight ∈ [0, 1] to confirm peak near 0.9-1.0 (Figure 4)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistically causes the sharp "step-function" accuracy jump at 64-256 negative examples?
- Basis in paper: Section 3.2 notes "it is unlikely for the model to learn much new information from just a few wrong answers" and Section 5 states "training with negative examples seems to flip a switch."
- Why unresolved: The paper documents the phenomenon but does not explain the underlying mechanism or why this specific critical phase exists.
- What evidence would resolve it: Probing experiments tracking internal representations, attention patterns, or neuron activations during the critical phase to identify what changes abruptly.

### Open Question 2
- Question: Can the Likra two-head approach be adapted for generative text production rather than only multiple-choice scoring?
- Basis in paper: Section 6 states "the two-head model structure makes it challenging to generate text" and suggests using Likra only for evaluation scenarios.
- Why unresolved: The architectural separation of positive and negative heads prevents straightforward text generation; no solution is proposed.
- What evidence would resolve it: Development of a method to merge or distill the two heads into a single generative model that retains the benefits of negative example training.

### Open Question 3
- Question: How does the relative advantage of negative examples scale with model size, training data scale, and domain complexity?
- Basis in paper: Experiments only cover 3B and 7B models on two English multiple-choice benchmarks; the generalization to larger models, different languages, or more complex tasks is untested.
- Why unresolved: Limited experimental scope leaves open whether the 10x advantage of negative examples is universal or context-dependent.
- What evidence would resolve it: Systematic experiments across model scales (e.g., 70B+), diverse languages, and non-MCQ tasks like open-ended generation or reasoning.

## Limitations

- The approach requires carefully curated negative examples, particularly near-misses, which may not be readily available for all domains
- The two-head architecture doubles inference compute compared to standard SFT, creating practical scalability concerns
- The method is specifically designed for multiple-choice settings and cannot be easily adapted for open-ended generation tasks

## Confidence

**High Confidence:** The core experimental findings are well-supported by the data. The comparison between SFT and Likra models on ARC-Challenge and HellaSwag benchmarks, showing accuracy improvements from ~60-66% to 80%+ for Likra, is convincing.

**Medium Confidence:** The mechanistic explanations for why negative examples work (likelihood-ratio discrimination, critical-phase learning dynamics) are plausible and supported by the evidence, but involve some interpretation.

**Low Confidence:** The generalizability of the critical-phase finding (sharp jump at 64-256 examples) to other tasks, model sizes, or domains is uncertain. This appears to be a specific phenomenon observed in these particular experiments.

## Next Checks

1. **Negative example quality ablation on held-out data:** Conduct a controlled experiment where the negative head is trained on near-misses versus random incorrect answers, then evaluate both on a test set containing distractors of varying quality.

2. **Pretraining coverage analysis:** Test the critical-phase jump phenomenon across domains with varying degrees of pretraining coverage by using datasets that are well-represented versus poorly-represented in pretraining corpora.

3. **Architecture scaling experiment:** Implement Likra on a larger base model (e.g., Llama-3.1-70B) and measure both accuracy gains and computational overhead, comparing the accuracy improvement per unit of additional compute against simply scaling the base model or using more extensive SFT.