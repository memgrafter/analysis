---
ver: rpa2
title: 'SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces'
arxiv_id: '2509.00287'
source_url: https://arxiv.org/abs/2509.00287
tags:
- data
- knowledge
- incidents
- graph
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SIGMUS is a system that integrates multimodal urban data streams
  (text, image, and tabular) using Large Language Models to create a semantic knowledge
  graph for incident detection and analysis. The system addresses the fragmentation
  of urban sensor data by automatically inferring relationships between incidents
  and diverse data sources without relying on human-encoded rules.
---

# SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces

## Quick Facts
- **arXiv ID**: 2509.00287
- **Source URL**: https://arxiv.org/abs/2509.00287
- **Reference count**: 40
- **Primary result**: System integrates multimodal urban data streams using LLMs to create semantic knowledge graph for incident detection

## Executive Summary
SIGMUS addresses the challenge of fragmented urban sensor data by automatically integrating multimodal data streams (text, images, and tabular) into a unified semantic knowledge graph. The system leverages Large Language Models to infer relationships between incidents and diverse data sources without requiring human-encoded rules. Demonstrated through the 2025 Los Angeles Wildfires case study, SIGMUS successfully connects news articles, CCTV images, air quality, weather, and traffic data to relevant urban incidents. The system achieves average processing latencies ranging from 0.0043s to 39.38s per report depending on modality, with knowledge graph insertion taking 0.02-30.81s, enabling comprehensive urban incident monitoring through semantically-rich data organization.

## Method Summary
SIGMUS employs LLM-based approaches for actor/event parsing from text, visual event classification from images, and cross-modality linking to create a semantic knowledge graph. The system processes multimodal urban data streams (news articles, CCTV images, air quality measurements, weather data, and traffic measurements) to automatically infer relationships between incidents and data sources. Rather than relying on predefined rules, the system uses semantic parsing and entity linking to connect diverse data modalities into a coherent knowledge structure. The architecture enables real-time processing and integration of streaming urban data, organizing fragmented information into a unified representation for incident detection and analysis.

## Key Results
- Successfully integrates 5 diverse data sources (news articles, CCTV images, air quality, weather, traffic measurements) with urban incidents
- Processes multimodal reports with latencies ranging from 0.0043s to 39.38s per report
- Achieves knowledge graph insertion times of 0.02-30.81s per entry
- Demonstrates effectiveness through 2025 Los Angeles Wildfires case study

## Why This Works (Mechanism)
SIGMUS works by leveraging LLMs' ability to understand semantic relationships across different data modalities. The system uses semantic parsing to extract entities and events from text data, applies visual event classification to interpret CCTV imagery, and employs cross-modality linking to connect related information across streams. This approach eliminates the need for manual rule engineering while maintaining the flexibility to handle diverse urban data sources. The knowledge graph structure enables efficient querying and analysis of complex incident relationships, while the multimodal integration provides comprehensive situational awareness for urban monitoring applications.

## Foundational Learning

**Semantic Knowledge Graphs**: Graph-based data structures that represent entities, relationships, and properties using standardized vocabularies and ontologies. Why needed: Provides flexible schema for representing complex urban incident relationships across modalities. Quick check: Can query relationships like "which air quality measurements occurred during fire incidents near traffic sensors?"

**Multimodal Data Integration**: Combining information from diverse sources (text, images, sensor data) into unified representations. Why needed: Urban incidents generate data across multiple sensor types requiring holistic analysis. Quick check: Does the system maintain data provenance while linking related information across sources?

**Large Language Model Semantic Parsing**: Using LLMs to extract structured information (entities, events, relationships) from unstructured text. Why needed: Enables automated understanding of incident reports without manual rule engineering. Quick check: Can the system correctly identify actors, events, and temporal relationships in incident descriptions?

**Cross-Modality Linking**: Establishing semantic connections between information from different data types. Why needed: Urban incidents manifest across multiple sensor streams requiring coordinated analysis. Quick check: Are related events in text, images, and sensor data correctly connected in the knowledge graph?

**Real-time Stream Processing**: Handling continuous data flows with low latency for timely incident detection. Why needed: Urban incidents require rapid response and analysis. Quick check: Does the system maintain performance under realistic urban data volumes?

## Architecture Onboarding

**Component Map**: Data Sources -> Stream Processors -> LLM Semantic Parser -> Cross-Modality Linker -> Knowledge Graph Database -> Query Interface

**Critical Path**: Incoming data stream → Stream processor (modality-specific) → LLM-based semantic extraction → Cross-modality linking → Knowledge graph insertion → Query interface

**Design Tradeoffs**: Uses LLM APIs for semantic understanding (high accuracy but potential cost/latency) vs. rule-based approaches (lower cost but brittle). Employs centralized knowledge graph (efficient querying but potential bottleneck) vs. distributed representations. Prioritizes automated relationship inference (reduced manual effort but less explainable) vs. human-curated links.

**Failure Signatures**: 
- LLM API timeouts → stalled processing pipeline
- Semantic parsing errors → incomplete knowledge graph entries
- Cross-modality linking failures → fragmented incident representations
- Database write errors → data loss in knowledge graph
- Processing latency spikes → real-time monitoring degradation

**3 First Experiments**:
1. Process 100 sample news articles through semantic parser and verify entity/event extraction accuracy
2. Run CCTV images through visual event classifier and measure classification precision/recall
3. Test cross-modality linking with known incident data to verify relationship inference quality

## Open Questions the Paper Calls Out
None

## Limitations
- Limited testing scope focused on single case study (2025 Los Angeles Wildfires) rather than diverse incident types
- Reliance on LLM APIs raises cost-effectiveness concerns for continuous urban monitoring
- No detailed analysis of error propagation across different data modalities
- Privacy implications of integrating CCTV imagery with other urban data sources not addressed

## Confidence
**High**: Core system capability in multimodal data integration, knowledge graph construction, and processing latencies
**Medium**: Scalability claims, generalizability across diverse urban contexts and incident types
**Low**: Not applicable

## Next Checks
1. Systematic evaluation across 10+ diverse urban incident scenarios spanning different geographies and event types to assess generalizability
2. Stress testing with high-volume data streams (100+ simultaneous incident reports) to evaluate system performance under load
3. Cost analysis comparing LLM API usage for continuous urban monitoring vs. alternative approaches