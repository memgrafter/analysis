---
ver: rpa2
title: 'UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language
  Models in Urdu'
arxiv_id: '2508.01006'
source_url: https://arxiv.org/abs/2508.01006
tags:
- urdu
- linguistic
- agreement
- language
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UrBLiMP, a benchmark for evaluating the linguistic
  competence of large language models in Urdu. UrBLiMP consists of 5,696 minimal pairs
  across ten core syntactic phenomena, constructed using the Urdu Treebank and diverse
  text corpora.
---

# UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu

## Quick Facts
- arXiv ID: 2508.01006
- Source URL: https://arxiv.org/abs/2508.01006
- Reference count: 10
- Large multilingual models achieve 57-95% accuracy on Urdu syntactic phenomena

## Executive Summary
This paper introduces UrBLiMP, a benchmark for evaluating the linguistic competence of large language models in Urdu. UrBLiMP consists of 5,696 minimal pairs across ten core syntactic phenomena, constructed using the Urdu Treebank and diverse text corpora. Human evaluation yielded a 96.10% inter-annotator agreement. Twenty multilingual models were evaluated, with LLaMA-3-70B achieving the highest average accuracy of 94.73%, though performance varied significantly across phenomena. The benchmark highlights both the potential and limitations of current multilingual models in capturing fine-grained syntactic knowledge in low-resource languages like Urdu.

## Method Summary
UrBLiMP evaluates LLMs using minimal pairs methodology where grammatical and ungrammatical sentence pairs differing by one linguistic feature are compared via perplexity. The dataset contains 5,696 pairs across 10 syntactic phenomena constructed from Urdu Treebank and in-house corpora. Models compute perplexity for each sentence in a pair, with accuracy defined as the proportion where grammatical sentences receive lower perplexity. Human evaluation validated the pairs (96.10% accuracy, Fleiss' κ = 0.89). Twenty multilingual models were evaluated including LLaMA-3, Gemma-3, and others.

## Key Results
- LLaMA-3-70B achieved highest average accuracy at 94.73%
- Gemma models showed strong performance despite smaller size
- Models struggle particularly with long-distance subject-verb gender agreement (accuracy drops from 80-92% to 57% as distance increases)
- Instruction-tuned variants consistently underperformed their pretrained counterparts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimal pair discrimination via perplexity comparison reveals syntactic competence without task-specific training.
- Mechanism: For each sentence pair (s_good, s_bad), the model computes (pseudo-)perplexity over both. If the model has internalized the targeted grammatical constraint, it assigns lower perplexity to the acceptable variant. Accuracy aggregates these binary judgments across all pairs.
- Core assumption: Perplexity differences reflect grammatical knowledge rather than surface frequency biases or tokenization artifacts.

### Mechanism 2
- Claim: Training data scale and language-specific exposure drive fine-grained syntactic generalization in low-resource settings.
- Mechanism: Models trained on larger multilingual corpora with better Urdu representation develop more robust internal representations of morphological agreement patterns (ergativity, gender/number/person agreement). The Gemma models' balanced 140-language training correlates with competitive performance despite smaller scale.
- Core assumption: Syntactic patterns in Urdu are learnable from distributional statistics alone, without explicit grammatical supervision.

### Mechanism 3
- Claim: Long-distance dependencies expose architectural limits in attention-based syntactic binding.
- Mechanism: Subject-verb gender agreement requires models to maintain and retrieve grammatical features across intervening clauses. When distance increases, attention mechanisms may fail to bind the correct subject to its verb, leading to agreement errors.
- Core assumption: Transformer attention patterns should correlate with linguistically relevant dependency structures for accurate agreement.

## Foundational Learning

- Concept: Minimal pairs methodology
  - Why needed here: UrBLiMP's evaluation logic depends on understanding how minimal contrasts isolate specific grammatical knowledge. Without this, accuracy scores are uninterpretable.
  - Quick check question: Given "The cat sleeps" vs. "The cat sleep," what grammatical phenomenon is being tested, and what should a competent model prefer?

- Concept: Split ergativity
  - Why needed here: Urdu's ergative system (section 2.1) is aspect-governed—subject marking depends on perfective vs. non-perfective aspect. Models must learn this conditional morphology.
  - Quick check question: In a perfective transitive Urdu clause, which argument receives ergative marking (ne), and does the verb agree with?

- Concept: Perplexity as probability distribution comparison
  - Why needed here: The evaluation metric (section 3.2) requires understanding how perplexity operationalizes "preference." Lower perplexity = higher probability = model preference.
  - Quick check question: If Model A assigns perplexity 15 to s_good and 20 to s_bad, while Model B assigns 18 to both, which model demonstrates better grammatical discrimination on this pair?

## Architecture Onboarding

- Component map:
  Data layer -> Evaluation layer -> Aggregation layer -> Comparison layer

- Critical path:
  1. Load model and tokenizer (ensure Urdu tokenization support)
  2. For each minimal pair, compute perplexity on both sentences
  3. Record binary outcome (correct if ppl_good < ppl_bad)
  4. Aggregate by phenomenon and paradigm
  5. Run statistical comparisons between model variants

- Design tradeoffs:
  - Sentence-level vs. position-level evaluation: Sentence-level (chosen here) may conflate grammaticality with fluency; position-level isolates the critical token but requires annotation
  - Template-generated vs. corpus-extracted pairs: UrBLiMP's hybrid approach increases naturalness but limits scale vs. BLiMP's 67K semi-auto-generated pairs
  - Coverage vs. depth: 10 phenomena provide breadth but miss constructions like reciprocity and relative-clause questions

- Failure signatures:
  - Near-random accuracy (~50%) indicates tokenization failure or insufficient Urdu representation
  - High accuracy on local phenomena (aspect agreement: ~100%) but low on long-distance dependencies (gender agreement: 57-92%) suggests attention bottleneck
  - Instruction-tuned variants underperforming pretrained baselines indicates fine-tuning degradation

- First 3 experiments:
  1. Baseline evaluation: Run your model on all 19 paradigms, identify which phenomena fall below 80% accuracy for targeted analysis
  2. Distance ablation: On subject-verb agreement pairs, bin by subject-verb token distance and plot accuracy decay curves
  3. Instruction-tuning comparison: If evaluating fine-tuned models, compare against pretrained baseline using paired Wilcoxon tests to quantify degradation magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does instruction tuning systematically degrade syntactic generalization in smaller language models, and if so, through what mechanism?
- Basis in paper: The authors report that pretrained variants of Gemma-3-1B, 3-4B, and 3-12B "significantly outperformed their instruction-tuned counterparts," and state that "instruction tuning may adversely affect syntactic generalization in smaller-scale models."
- Why unresolved: The paper documents the performance gap but does not investigate the underlying cause or whether this pattern generalizes across model families.

### Open Question 2
- Question: How can models' handling of long-distance syntactic dependencies in morphologically rich languages be improved?
- Basis in paper: The authors observe that "the language models struggle particularly with long-distance agreement," noting that "when the distance between the subject and the verb increases, the ability of the model to correctly predict gender agreement significantly deteriorates."
- Why unresolved: The paper identifies the limitation but does not propose or test interventions.

### Open Question 3
- Question: What additional syntactic phenomena should be included to achieve comprehensive evaluation of Urdu linguistic competence?
- Basis in paper: The authors explicitly note that "the current evaluation only covers ten linguistic categories" and identify omitted phenomena including "Reciprocity" and "Question constructions involving relative clauses."

## Limitations

- Limited coverage of Urdu's grammatical complexity with only 10 phenomena across 19 paradigms
- Cannot distinguish between true grammatical incompetence and tokenization artifacts in Urdu script
- Lack of precise quantification of Urdu token exposure in pretraining data for evaluated models

## Confidence

**High Confidence:**
- Benchmark construction methodology and inter-annotator agreement (96.10% accuracy, Fleiss' κ = 0.89)
- Relative model performance rankings (LLaMA-3-70B > Gemma-3-27B > smaller models consistently)
- Instruction tuning degradation effect (Gemma-IT models underperforming PT baselines across paradigms)

**Medium Confidence:**
- Long-distance dependency performance claims (accuracy degradation with distance, but correlation with actual architectural attention patterns not established)
- Generalization claims to other low-resource languages (methodology transferability assumed but not empirically validated)
- Syntactic competence vs. frequency learning distinction (plausible but not directly tested)

**Low Confidence:**
- Absolute performance thresholds (what constitutes "good" vs. "poor" syntactic competence in Urdu remains undefined)
- Training data exposure quantification (claims about balanced vs. Urdu-specific corpora lack precise token counts)
- Tokenization artifact impact (Urdu-specific issues not systematically investigated)

## Next Checks

1. **Attention mechanism analysis:** For models showing degraded performance on long-distance subject-verb agreement, extract attention weight distributions across subject-verb pairs at varying distances. Compare attention patterns between correct and incorrect predictions to determine if architectural constraints or training data biases drive performance degradation.

2. **Cross-linguistic transferability validation:** Apply the exact UrBLiMP methodology to another low-resource language with similar syntactic properties (e.g., Hindi or Persian) using the same construction approach. Compare model performance patterns to identify whether observed limitations are language-specific or reflect general architectural constraints.

3. **Tokenization artifact isolation:** Systematically evaluate Urdu-specific tokenization strategies (character vs. subword, different scripts) on a subset of UrBLiMP pairs where models show unexpected performance. Compare perplexity scores across tokenization schemes to quantify the impact of tokenization choices on grammaticality judgments.