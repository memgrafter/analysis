---
ver: rpa2
title: 'SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended
  Questions'
arxiv_id: '2505.04584'
source_url: https://arxiv.org/abs/2505.04584
tags:
- feedback
- learning
- slide
- student
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates SlideItRight, an AI system that combines
  text-based feedback generated by large language models (LLMs) with retrieved relevant
  lecture slides to support learning from open-ended questions. A 2x2 crowdsourcing
  experiment (N=91) compared learning gains and student perceptions across four conditions:
  human feedback, AI feedback, slide feedback, and combined AI + slide feedback.'
---

# SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions

## Quick Facts
- arXiv ID: 2505.04584
- Source URL: https://arxiv.org/abs/2505.04584
- Reference count: 37
- Primary result: AI + slide feedback produces highest average learning gains (14.8%) but no statistically significant difference from other conditions; students report high satisfaction but note cognitive load and trust concerns

## Executive Summary
This study evaluates SlideItRight, an AI system that combines text-based feedback generated by large language models (LLMs) with retrieved relevant lecture slides to support learning from open-ended questions. A 2x2 crowdsourcing experiment (N=91) compared learning gains and student perceptions across four conditions: human feedback, AI feedback, slide feedback, and combined AI + slide feedback. Learning gains improved significantly in all conditions, but differences between conditions were not statistically significant. Students reported high satisfaction and perceived learning gains across all modalities, but found slide feedback difficult to understand. AI feedback was rated highly for personalization and actionability but lower in trust compared to human feedback. The combined approach yielded the highest average learning gain (14.8%) but raised concerns about cognitive load. The findings suggest that AI-facilitated multimodal feedback can support learning comparably to human feedback while offering scalability advantages, though careful attention to cognitive load and clarity is needed for optimal implementation.

## Method Summary
SlideItRight uses a retrieval-augmented generation (RAG) system combining GPT-4 Vision for slide understanding, semantic similarity matching to retrieve top-3 relevant slides, and GPT-4o for personalized feedback generation with learner-centered prompting. The system processes student open-ended responses, retrieves relevant slides from a pre-processed corpus, and generates multimodal feedback combining AI text with slide content. A 2x2 experiment compared human vs AI feedback and slide vs no-slide conditions across 91 participants, measuring learning gains (pre-post test difference) and student perceptions via 5-point Likert scales.

## Key Results
- Learning gains improved significantly across all conditions (p < 0.001) with average gains of 12.8% (AI), 12.4% (human), 9.2% (slide), and 14.8% (combined)
- No statistically significant differences between conditions (p = 0.794 for interaction effect)
- Students reported high satisfaction across all modalities but rated slide feedback lowest for clarity (39.13% "easy to understand")
- AI feedback received highest personalization ratings (79.17%) but lowest trust ratings (50.00%) compared to human feedback (81.82% trust)

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Grounding
- Claim: Retrieving relevant lecture slides and integrating them into the feedback generation process grounds AI responses in verified instructional materials, potentially improving accuracy and relevance.
- Mechanism: A two-stage matching process converts slides into multimodal vector representations (capturing textual, visual, and layout features), then matches question vectors against slide vectors via semantic similarity to retrieve the top 3 most relevant slides. Retrieved content is fed to the LLM as a knowledge base.
- Core assumption: Slides contain accurate, domain-relevant information that the LLM can synthesize into coherent feedback without introducing contradictions.
- Evidence anchors:
  - [abstract]: "Our study leverages Large Language Models (LLMs) for textual feedback with the supplementary guidance from other modality - relevant lecture slide retrieved from the slides hub."
  - [section]: "The retrieved slide content is then integrated into the feedback generation process, serving as a knowledge base that grounds the AI responses in verified instructional materials."
  - [corpus]: Weak direct validation—neighbor paper "LLM-based Multimodal Feedback Produces Equivalent Learning" supports multimodal AI feedback effectiveness but does not specifically validate RAG grounding mechanisms.
- Break condition: When retrieved slides are semantically similar but topically tangential, or when AI feedback and slide content contradict each other (participant report: "sometimes they contradicted each other").

### Mechanism 2: Dual-Channel Processing via Multimodal Feedback
- Claim: Presenting feedback through separate visual (slides) and textual (LLM explanations) channels may enable parallel processing and reduce cognitive load, following Mayer's multimedia learning principle.
- Mechanism: Students receive AI-generated textual feedback alongside a relevant slide page, theoretically allowing visual and verbal information to be processed through separate cognitive channels with limited capacity.
- Core assumption: Learners have sufficient working memory capacity to integrate both channels without overload; the modalities complement rather than compete for attention.
- Evidence anchors:
  - [abstract]: "As is indicated in the multimedia learning principle, learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate students' learning."
  - [section]: "This design addresses cognitive load management through: (1) enabling parallel processing of information via separate channels (visual instructional material and textual guidance), aligning with multimedia learning principles."
  - [corpus]: No direct corpus validation of the dual-channel hypothesis in this specific context.
- Break condition: When information density exceeds processing capacity, particularly under time constraints (participant: "Too much information at once made it difficult to process everything in a timed setting").

### Mechanism 3: Response-Contingent Personalization
- Claim: Analyzing student free responses and generating individually tailored feedback enhances perceived relevance and may support learning by addressing specific misconceptions.
- Mechanism: GPT-4o processes each student's open-ended response using learner-centered prompting strategies (strengthening relationships, providing corrective information, offering improvement guidance), generating feedback specific to the submitted answer.
- Core assumption: Personalized feedback that directly addresses individual response content is more pedagogically valuable than generic, one-size-fits-all feedback.
- Evidence anchors:
  - [abstract]: "Students reported high satisfaction with the learning experience, particularly appreciating...the actionable insights of AI feedback."
  - [section]: "Students considered it personalized and relevant to their responses...Participants particularly valued this aspect: 'Personalized feedback is what would drive me to use it quite often.'"
  - [corpus]: Neighbor paper "LLM-based Multimodal Feedback Produces Equivalent Learning" reports similar personalization benefits.
- Break condition: When AI feedback becomes overly verbose (500-1000 word responses criticized by beginners) or uses jargon exceeding learner proficiency levels.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core to SlideItRight's ability to ground LLM outputs in actual course materials rather than relying solely on model weights.
  - Quick check question: Can you explain why vector embeddings enable semantic matching between student responses and slide content?

- Concept: Multimedia Learning Theory (Mayer)
  - Why needed here: Provides the theoretical justification for multimodal feedback design; assumes dual channels (visual/verbal) with limited capacity.
  - Quick check question: What are the three cognitive load types, and how does presenting only the most relevant slide (vs. full documents) address one of them?

- Concept: Learner-Centered Feedback Framework
  - Why needed here: Informs the prompting strategy for LLM feedback generation (corrective information, clear guidance, relationship-building).
  - Quick check question: According to the framework cited in the paper, what three elements should effective learner-centered feedback include?

## Architecture Onboarding

- Component map: Multimodal Input Processor -> Retrieval Engine -> LLM Feedback Generator -> Delivery Interface
- Critical path:
  1. Student submits open-ended response → system retrieves relevant slides via vector similarity.
  2. LLM generates personalized feedback using student response + slide content as context.
  3. Feedback panel displays: (a) AI text feedback, (b) vision model's slide description, (c) slide image with zoom capability.
  4. Student response cached for session continuity.
- Design tradeoffs:
  - Pre-caching vs. real-time processing: Faster response but reduced flexibility for dynamic content changes.
  - Information density vs. cognitive load: Combined condition showed highest average gains (14.8%) but triggered overload complaints.
  - AI personalization vs. trust: Highest personalization rating (79.17%) but lowest trust (50.00%)—transparency mechanisms may trade off with perceived authority.
- Failure signatures:
  - Low clarity ratings on slide feedback (39.13% "easy to understand") → slides lack direct improvement guidance.
  - Trust erosion when AI feedback feels "nice" but not substantive ("crisis of confidence" comment).
  - Cognitive overload in combined condition under time pressure.
  - Contradictions between AI text and retrieved slide content.
- First 3 experiments:
  1. Vary feedback verbosity (short/medium/long) and measure clarity, trust, and learning gains to identify optimal information density.
  2. Add inline slide references (e.g., highlighted regions, specific bullet point citations) to test whether explicit grounding improves trust scores from 50% baseline.
  3. Implement adaptive scaffolding: detect response proficiency level and adjust feedback complexity; compare learning gains and satisfaction across proficiency tiers.

## Open Questions the Paper Calls Out

- **Question**: Why did the combined AI and slide feedback fail to produce statistically significant learning gains compared to single-modality conditions?
  - Basis in paper: [explicit] The authors explicitly ask: "if the combined approach integrates the best of both human-like and content-grounded feedback, why didn't it yield superior results?"
  - Why unresolved: While the combined condition showed the highest average gain (14.8%), the results were not statistically significant (p=0.794 for interaction). The authors hypothesize cognitive overload or conflicting information density as causes but did not isolate these variables.
  - What evidence would resolve it: Eye-tracking data or think-aloud protocols to measure cognitive load and information processing during feedback consumption; or an experiment isolating information density as a variable.

- **Question**: Does standardizing the structural alignment of feedback across modalities improve the relative effectiveness of combined multimodal feedback?
  - Basis in paper: [explicit] The Discussion notes that "The human feedback used in this study did not fully adopt a learner-centered structure. Future research should ensure structural alignment across feedback types."
  - Why unresolved: The study compared conditions that may have varied in structural pedagogy, potentially confounding the comparison between human and AI-generated feedback.
  - What evidence would resolve it: A follow-up experiment where both human and AI feedback are strictly templated to follow an identical learner-centered structure before comparison.

- **Question**: To what extent do explicit rubrics and inline references mitigate student trust issues in AI-generated feedback?
  - Basis in paper: [inferred] The paper notes AI feedback struggled with trust (50% vs 81.82% for human) and suggests "integrating more explicit justifications... or incorporating human oversight" as a solution.
  - Why unresolved: The study identified low trust as a barrier but did not implement or test specific interface interventions (like inline citations) designed to verify the AI's claims against course materials.
  - What evidence would resolve it: A comparative study testing standard LLM feedback against a "verified" condition with automatic inline citations linking claims to specific slide text.

## Limitations

- FMR analysis limitations: The FMR (Fraction of Maximum Relevance) scores for retrieved slides were not reported, making it difficult to assess the quality of the RAG grounding mechanism.
- Generalizability constraints: The study was conducted in a single online course with introductory instructional design students, limiting external validity.
- Temporal validity concerns: The GPT-4o model used may have knowledge cutoff limitations affecting feedback accuracy on newer or evolving course content.

## Confidence

- High confidence: Learning gains improved significantly across all conditions (p < 0.001), and the overall finding that AI + slide feedback produced the highest average learning gain (14.8%) is well-supported.
- Medium confidence: The equivalence between AI and human feedback conditions for learning outcomes, given the lack of statistically significant differences despite numerical variations.
- Low confidence: The specific mechanisms by which slide retrieval enhances AI feedback accuracy, due to limited validation of the RAG grounding process and contradictory feedback reports.

## Next Checks

1. FMR validation study: Conduct systematic evaluation of slide retrieval relevance by computing FMR scores for a stratified sample of question-slide pairs and correlating with student perception ratings.
2. Contradiction detection analysis: Implement automated contradiction detection between AI feedback and retrieved slide content, then measure the impact on trust and learning gains when contradictions are present.
3. Cross-domain replication: Replicate the study in at least two different subject domains (e.g., STEM and humanities) to test generalizability of the combined AI + slide approach for learning gains and student satisfaction.