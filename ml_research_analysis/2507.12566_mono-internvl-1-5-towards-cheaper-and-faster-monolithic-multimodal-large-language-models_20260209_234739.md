---
ver: rpa2
title: 'Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large
  Language Models'
arxiv_id: '2507.12566'
source_url: https://arxiv.org/abs/2507.12566
tags:
- visual
- arxiv
- data
- mono-internvl
- mono-internvl-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of training monolithic multimodal
  large language models (MLLMs), which integrate visual encoding and language decoding
  into a single model but often suffer from unstable optimization and catastrophic
  forgetting. The authors propose Mono-InternVL, which embeds visual experts into
  a pre-trained LLM using a multimodal mixture-of-experts architecture, enabling stable
  learning of visual knowledge via delta tuning.
---

# Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2507.12566
- Source URL: https://arxiv.org/abs/2507.12566
- Reference count: 40
- Primary result: Mono-InternVL-1.5 achieves similar multimodal performance to InternVL-1.5 while reducing first-token latency by up to 69%

## Executive Summary
This paper addresses the challenges of training monolithic multimodal large language models (MLLMs), which integrate visual encoding and language decoding into a single model but often suffer from unstable optimization and catastrophic forgetting. The authors propose Mono-InternVL, which embeds visual experts into a pre-trained LLM using a multimodal mixture-of-experts architecture, enabling stable learning of visual knowledge via delta tuning. To further improve efficiency, they introduce Mono-InternVL-1.5, which incorporates visual attention experts and an improved Endogenous Visual Pre-training (EViP++) strategy, reducing training data by 58% while maintaining competitive performance. Mono-InternVL-1.5 also includes a fused CUDA kernel to speed up inference by up to 26%. Experimental results show that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, with a +114-point improvement over Emu3 on OCRBench.

## Method Summary
The method involves a delta-tuning approach where visual experts (FFN and QKV projections) are embedded into a pre-trained LLM (InternLM2-1.8B) while keeping base parameters frozen to prevent catastrophic forgetting. The architecture uses static routing to direct visual tokens to visual experts and text tokens to text experts. Training follows a three-stage EViP++ protocol: concept learning on noisy data with frozen attention, semantic learning on synthetic captions, and alignment learning on task-specific data with unfrozen attention. A fused CUDA kernel accelerates MoE operations by exploiting token-type locality. The approach reduces training data by 58% while maintaining competitive performance.

## Key Results
- Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks
- +114-point improvement over Emu3 on OCRBench
- Mono-InternVL-1.5 achieves similar multimodal performance to InternVL-1.5 while reducing first-token latency by up to 69%
- 58% reduction in training data with EViP++ strategy

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Mixture-of-Experts (MMoE) with Delta Tuning
The architecture embeds separate visual experts into a pre-trained LLM while freezing the base parameters, preventing catastrophic forgetting during visual pre-training. Static routing directs visual tokens to visual FFN experts and text tokens to text FFN experts. Only the patch embedding and visual experts are trained initially, preserving pre-trained language knowledge. This is formulated as delta tuning: `arg min_∆θ L(F_llm(x_m; θ, θ_v), ŷ)` where ∆θ represents only the newly added visual parameters.

### Mechanism 2: Progressive Endogenous Visual Pre-training (EViP/EViP++)
The three-stage training approach (concept learning on noisy data, semantic learning on synthetic captions, alignment learning on task-specific data) with stage-specific parameter unfreezing maximizes visual capability acquisition while minimizing data requirements. EViP++ adds visual attention experts and reduces data 58% by prioritizing quality over quantity.

### Mechanism 3: Fused CUDA Kernel for Modality-Specific MoE
A custom kernel with early-exit thread blocks reduces inference latency by exploiting token-type locality within sequence blocks. Sequences are partitioned into fine-grained blocks, with two thread blocks assigned per block (one visual, one textual), but each checks for relevant tokens and exits immediately if none exist.

## Foundational Learning

### Concept: Mixture-of-Experts (MoE) with Static vs Learned Routing
**Why needed here**: The architecture uses static hard routing based on token modality rather than learned routing. Understanding this distinction is critical because it constrains model flexibility while ensuring predictable computation paths.
**Quick check question**: Why would learned routing be problematic for preserving language capabilities in this architecture?

### Concept: Catastrophic Forgetting in Continual Learning
**Why needed here**: The entire delta tuning strategy is motivated by preventing catastrophic forgetting of language knowledge during visual pre-training. Without this concept, the architectural decisions seem unnecessarily complex.
**Quick check question**: If we fine-tuned all parameters during visual pre-training, what specific performance degradation would we expect on language benchmarks?

### Concept: Vision-Language Alignment in Transformer Attention Layers
**Why needed here**: The decision to freeze attention layers in S1.1/S1.2 and unfreeze them in S1.3 reflects a specific theory about when and how cross-modal alignment should occur. This affects both training efficiency and final performance.
**Quick check question**: Why might aligning visual and textual representations in attention layers be more important for VQA tasks than for basic image captioning?

## Architecture Onboarding

### Component Map
Patch Embedding Layer -> Visual Tokenizer -> Multimodal MoE Transformer (N layers with RMSNorm, Multi-Head Attention/Viual Attention Experts, MMoE routing, residual connections) -> Language Model Head

### Critical Path
1. Image preprocessing: Dynamic high-resolution strategy → patch embedding → visual tokens
2. Token routing: Static routing based on token type (visual → V-Expert, text → T-Expert)
3. Forward pass through N layers: Attention → MoE routing → residual connections
4. Text generation: Autoregressive decoding with language model head

### Design Tradeoffs
- Separated experts vs shared FFN: Separation preserves language knowledge (Tab. X: 42.30 vs 26.17 MMLU) but increases parameter count
- Frozen vs unfrozen attention: Freezing preserves LLM capabilities; unfreezing improves alignment but requires careful staging (Tab. VII shows consistent gains from unfreezing in S1.3)
- Data scale vs quality: EViP++ demonstrates 58% data reduction is possible with higher quality curation (Fig. 5)

### Failure Signatures
- Degraded NLP performance (MMLU/CMMLU drops >5%): Indicates excessive language parameter modification → verify text experts are frozen during visual pre-training
- Poor OCR/dense text performance (DocVQA/InfoVQA <60%): Suggests insufficient visual expert capacity or inadequate alignment training → check S1.3 data composition
- High inference latency (>150% of expected): Fused kernel not applied or poor token clustering → verify CUDA kernel is active and profile thread block efficiency
- Visual token overflow errors: Exceeding 10,240 patch limit → implement image resizing or tiling strategy

### First 3 Experiments
1. **Delta tuning ablation**: Train Mono-InternVL with (a) full parameter fine-tuning, (b) delta tuning with visual experts, (c) delta tuning without expert separation. Evaluate on both NLP benchmarks (MMLU, CMMLU) and multimodal benchmarks (OCRBench, DocVQA) to validate the catastrophic forgetting hypothesis.

2. **Progressive vs single-stage training**: Compare full EViP++ (S1.1→S1.2→S1.3) against single-stage training on combined data. Measure data efficiency (performance vs sample count) and training stability (loss curve smoothness, gradient norms) to validate the progressive learning assumption.

3. **Fused kernel benchmarking**: Profile inference latency across varying visual-textual token ratios (pure text, pure visual, 50-50 mixed, highly interleaved) and sequence lengths (1K to 128K tokens). Compare against naive PyTorch implementation to identify break-even points where the kernel provides no benefit or introduces overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the visual encoding capability of monolithic models be improved for high-resolution tasks without strictly increasing model depth?
- Basis in paper: The authors state that Mono-InternVL is "inferior to InternVL-1.5 on high-resolution benchmarks" specifically because "the relatively shallow model depth limits the visual encoding ability" (Page 9).
- Why unresolved: The paper identifies model depth as a bottleneck for high-resolution processing (e.g., InfoVQA) but does not propose a solution within the monolithic architecture to overcome this without simply using a larger base model.
- What evidence would resolve it: An architectural modification or training strategy that closes the performance gap on high-resolution benchmarks (like InfoVQA) while maintaining the shallow 1.8B parameter structure.

### Open Question 2
- Question: Can the integration of visual attention experts be further optimized to prevent the slight degradation in pure NLP tasks observed in Mono-InternVL-1.5?
- Basis in paper: The authors note that despite the efficiency gains, "Mono-InternVL-1.5 has a slight performance drop on some NLP tasks" (Page 9) compared to the baseline.
- Why unresolved: While the MMoE architecture with delta tuning successfully preserves most language knowledge, the addition of visual attention experts appears to introduce minor interference with the pre-trained language representations.
- What evidence would resolve it: A revised training strategy or routing mechanism where NLP benchmark scores (e.g., MMLU, CMMLU) for Mono-InternVL-1.5 equal or exceed those of the original InternLM2-Chat baseline.

### Open Question 3
- Question: Does the efficiency and performance of the MMoE architecture scale effectively to significantly larger base LLMs (e.g., 7B+ parameters)?
- Basis in paper: The experiments are restricted to a small 1.8B parameter model (InternLM2-1.8B), and the paper identifies "shallow model depth" as a limitation (Page 9).
- Why unresolved: It is unclear if the "Cheaper and Faster" benefits (like the 69% latency reduction) and the static routing mechanism hold up when the base model has more layers and parameters, where optimization instability might differ.
- What evidence would resolve it: Evaluation results and latency metrics for Mono-InternVL-1.5 when built upon larger foundational models like InternLM2-7B or InternLM2-20B.

## Limitations
- Performance degradation on some NLP tasks in Mono-InternVL-1.5 suggests visual attention experts introduce minor interference with language representations
- Architecture struggles with high-resolution benchmarks due to shallow model depth limitations
- Fused CUDA kernel performance depends heavily on token clustering assumptions that may not hold for all input types

## Confidence
**High Confidence**: The core delta tuning mechanism with separated visual and text experts is well-supported by ablation results (Tab. X showing 42.30 vs 26.17 MMLU). The fused CUDA kernel's performance gains are directly measurable and well-documented through multiple benchmark configurations.

**Medium Confidence**: The EViP++ training efficiency claims are supported by data reduction metrics and performance comparisons, but the lack of detailed ablation studies on individual training stages limits definitive conclusions about which components drive the improvements.

**Low Confidence**: The generalization claims for highly interleaved visual-textual sequences are not well-validated. The paper's token clustering assumptions for the fused kernel haven't been tested across diverse input patterns, and the robustness of the architecture to various failure modes remains unexplored.

## Next Checks
1. **Architecture Ablation Study**: Implement and test a variant where attention layers are unfrozen during S1.1/S1.2 stages. Compare performance degradation on NLP benchmarks and multimodal tasks to validate the frozen-attention assumption in EViP.

2. **Token Clustering Analysis**: Create synthetic datasets with varying degrees of visual-textual token interleaving (0%, 10%, 50%, 90%, 100% interleaving). Measure actual CUDA kernel performance across these distributions to identify break-even points where the fused kernel provides no benefit.

3. **Robustness Testing**: Design adversarial test cases targeting the static routing mechanism, including malformed images, extremely dense text, and inputs that violate the patch embedding assumptions. Measure performance degradation patterns to identify architectural vulnerabilities.