---
ver: rpa2
title: 'Deep Progressive Training: scaling up depth capacity of zero/one-layer models'
arxiv_id: '2511.04981'
source_url: https://arxiv.org/abs/2511.04981
tags:
- layer
- training
- loss
- progressive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies progressive training for deep neural networks,
  focusing on depth expansion to reduce computational cost while maintaining performance.
  It introduces zero/one-layer progressive training, where small models are first
  trained then expanded by adding new layers.
---

# Deep Progressive Training: scaling up depth capacity of zero/one-layer models

## Quick Facts
- **arXiv ID:** 2511.04981
- **Source URL:** https://arxiv.org/abs/2511.04981
- **Reference count:** 40
- **Primary result:** ~5× speedup (20% compute) for training deep models by first training shallow models then expanding depth

## Executive Summary
This work introduces zero/one-layer progressive training, a method for efficiently training deep neural networks by first training small models and then expanding them by adding new layers. The approach leverages theoretical insights from mean parameterization (muP) theory to enable hyperparameter transfer between models of different depths. By carefully timing the expansion during the stable phase of a warm-up-stable-decay (WSD) learning rate schedule and using appropriate initialization (random or copying), the method achieves significant computational savings while maintaining performance comparable to training deep models from scratch.

## Method Summary
The method trains a shallow "zero/one-layer" model first, then expands its depth to the target size by adding new layers. New layers are initialized either randomly (with muP-based spectral norm scaling) or by copying the existing trained layer. Expansion occurs during the stable phase of a WSD learning rate schedule, typically around 80% of total iterations. The approach uses Muon-NSGD optimizer with 0.1 weight decay and supports various architectures including GPT-2, ResNet, and MoE models. The theoretical analysis frames progressive training as projected gradient descent with good initialization in a convex setting.

## Key Results
- Achieves ~5× speedup (20% of baseline compute) for training deep models
- Minimal performance degradation (<0.5% loss increase) compared to training from scratch
- Works across multiple architectures (GPT-2, ResNet, MoE)
- Single-stage expansion avoids complexity of multi-stage approaches

## Why This Works (Mechanism)
The method works by leveraging the fact that shallow models can be trained quickly and serve as good initialization for deeper models. When new layers are added during the stable phase of training with appropriate initialization (following muP theory), the expanded model can continue training effectively without catastrophic loss of performance. The WSD schedule ensures sufficient learning rate during expansion to train the new layers while maintaining stability.

## Foundational Learning

**Mean Parameterization (muP) Theory**
- *Why needed:* Enables hyperparameter transfer between models of different sizes/depths
- *Quick check:* Verify scaling relationships between model dimensions follow muP predictions

**Warmup-Stable-Decay (WSD) Schedules**
- *Why needed:* Provides appropriate learning rate timing for expansion
- *Quick check:* Confirm expansion occurs during stable phase, not decay

**Projected Gradient Descent**
- *Why needed:* Theoretical framework for understanding progressive training dynamics
- *Quick check:* Verify loss behavior follows projected gradient descent patterns

## Architecture Onboarding

**Component Map:** Shallow Model -> Expansion Point -> Deep Model -> Continue Training

**Critical Path:** Shallow training → WSD schedule timing → Expansion during stable phase → Continue with appropriate initialization

**Design Tradeoffs:** Single-stage vs multi-stage expansion, random vs copying initialization, timing of expansion relative to learning rate schedule

**Failure Signatures:**
- Loss spike on expansion: indicates poor initialization or wrong expansion timing
- No convergence: suggests expansion happened during decay phase
- Training instability: may indicate optimizer mismatch or improper initialization

**First Experiments:**
1. Train 1-layer GPT-2 with WSD schedule to establish baseline
2. Expand to 12 layers during stable phase using copying initialization
3. Compare final loss and compute cost against training 12-layer model from scratch

## Open Questions the Paper Calls Out
None

## Limitations
- Method's efficacy depends critically on precise hyperparameter scheduling and optimizer choice
- Theoretical analysis limited to simplified convex setting, doesn't fully capture non-convex neural network dynamics
- Mixing time for loss recovery after expansion lacks general predictive formula

## Confidence

**High Confidence:** Empirical results demonstrating ~5× speedup with minimal loss degradation across multiple architectures

**Medium Confidence:** Theoretical framework connecting progressive training to projected gradient descent with good initialization

**Medium Confidence:** Claims about WSD schedules and muP-based initialization being essential for success

## Next Checks

1. Implement Muon-NSGD precisely by referencing cited external works for exact Newton-Schulz iteration count and normalization scaling

2. Conduct ablation experiment expanding model during decay phase to quantify impact on convergence and speedup

3. Apply zero/one-layer expansion method to a different deep learning architecture (e.g., Vision Transformer) on held-out dataset to assess generalizability