---
ver: rpa2
title: 'TimeBill: Time-Budgeted Inference for Large Language Models'
arxiv_id: '2512.21859'
source_url: https://arxiv.org/abs/2512.21859
tags:
- time
- inference
- response
- execution
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TimeBill is a time-budgeted inference framework for large language
  models (LLMs) that addresses the challenge of generating accurate responses within
  strict time budgets in time-critical systems. The method uses a fine-grained response
  length predictor (RLP) based on a small language model to predict the response length
  of the target LLM, and a workload-guided execution time estimator (ETE) that combines
  analytical modeling with profiling to accurately estimate end-to-end execution time.
---

# TimeBill: Time-Budgeted Inference for Large Language Models

## Quick Facts
- arXiv ID: 2512.21859
- Source URL: https://arxiv.org/abs/2512.21859
- Authors: Qi Fan; An Zou; Yehan Ma
- Reference count: 8
- Key outcome: TimeBill achieves highest average response performance scores among tested approaches while maintaining competitive task completion rates under various overrun strategies, with response length predictor achieving MAE of 42.71 and R-squared of 0.719.

## Executive Summary
TimeBill addresses the challenge of generating accurate responses within strict time budgets for large language models in time-critical systems. The framework predicts response length using a small language model and estimates execution time through analytical modeling combined with profiling. By adaptively adjusting the key-value cache eviction ratio based on these predictions and the given time budget, TimeBill balances inference efficiency and response performance. Experiments demonstrate superior performance compared to existing methods while maintaining competitive task completion rates.

## Method Summary
TimeBill uses a fine-grained response length predictor (RLP) based on a small language model to predict the response length of the target LLM, and a workload-guided execution time estimator (ETE) that combines analytical modeling with profiling to accurately estimate end-to-end execution time. The framework then adaptively adjusts the key-value cache eviction ratio based on these predictions and the given time budget to balance inference efficiency and response performance. The method involves profiling the target hardware to obtain execution time coefficients, training the RLP on deployment data, and computing the optimal KV cache eviction ratio at runtime using a closed-form solution.

## Key Results
- TimeBill achieves the highest average response performance scores among tested approaches
- Response length predictor achieves MAE of 42.71 and R-squared of 0.719
- Execution time estimator achieves MAPE of 1.22% (prefill) and 1.69% (decoding step)
- Maintains competitive task completion rates under various overrun strategies

## Why This Works (Mechanism)

### Mechanism 1: Small Language Model-Based Response Length Prediction
A fine-grained response length predictor (RLP) based on an SLM can accurately predict the target LLM's response length bucket before inference begins. The RLP treats length prediction as a classification task into fixed-size buckets (B=16 tokens) rather than regression. It uses knowledge distillation from the target LLM to align predictions, employing a decoder-only architecture with RMSNorm, CausalAttention, and SwiGLU FFN layers. Core assumption: Response length patterns learned from training data generalize to inference-time inputs; the SLM has sufficient capacity to model the target LLM's length distribution. Evidence anchors: [abstract] "fine-grained response length predictor (RLP) based on a small language model to predict the response length of the target LLM" and [section 4.1] Describes RLP architecture with 512 buckets and reports MAE of 42.71 and R² of 0.719 in Table 1. Break condition: If input prompts fall outside the training distribution (e.g., novel domains, code vs. natural language), RLP accuracy may degrade substantially.

### Mechanism 2: Hybrid FLOPs-and-Profiling Execution Time Estimation
Combining analytical FLOPs modeling with empirical profiling provides accurate and interpretable execution time prediction. The ETE derives that prefill time scales quadratically with input length (âtprefill = aN²x + bNx + c) while decoding time scales linearly with KV cache length. Coefficients are fitted via least-squares regression on profiled measurements. Core assumption: Execution time follows the theoretical FLOPs relationship; hardware behavior is consistent between profiling and deployment. Evidence anchors: [section 4.2] Equations 4a-4b formalize the quadratic/linear relationships; Figure 5 shows MAPE of 1.22% (prefill) and 1.69% (decoding). Break condition: If GPU frequency scaling, thermal throttling, or concurrent workloads cause runtime variance beyond the pessimistic factor k, WCET estimates may be violated.

### Mechanism 3: Closed-Form Adaptive KV Cache Eviction Ratio
Given predicted execution time and a deadline, the optimal KV cache eviction ratio can be computed analytically to minimize quality loss while meeting time constraints. The framework transforms the optimization (maximize response quality subject to time budget) into minimizing α subject to tPredict + t̂WCET ≤ T. Equation 11 provides the closed-form solution α* that balances eviction against remaining time budget. Core assumption: Response quality is monotonically non-increasing with eviction ratio; the pessimistic factor k appropriately bounds WCET. Evidence anchors: [section 5.1] Equation 11 derives α* from the constraint optimization; [section 6.4] Figure 7 shows TimeBill achieves highest average scores with competitive completion rates. Break condition: If KV cache eviction disproportionately harms certain task types (e.g., long-context reasoning), the monotonic quality assumption may not hold.

## Foundational Learning

- **LLM Inference Phases (Prefill vs. Decoding)**: The entire ETE depends on understanding that prefill processes all input tokens at once (quadratic in attention), while decoding generates one token at a time (linear in KV cache size). Quick check question: If input length doubles, how does prefill time scale? How does decoding time scale if response length doubles?

- **KV Cache Mechanics**: The adaptive eviction mechanism requires understanding what the KV cache stores, why it grows linearly with sequence length, and how eviction affects attention computation. Quick check question: After evicting 50% of KV cache entries mid-inference, what information is lost and how might it affect generation quality?

- **Worst-Case Execution Time (WCET) in Real-Time Systems**: The pessimistic factor k converts predictions into WCET bounds; understanding hard vs. soft deadlines clarifies why overruns trigger Kill or Skip-Next strategies. Quick check question: If k=5 produces 95% deadline compliance, what happens to completion rate and quality if k=1? What if k=8?

## Architecture Onboarding

- Component map:
Input Prompt (x)
     │
     ├─→ [RLP Predictor (SLM)] ─→ Predicted Length (N̂)
     │                                    │
     └─→ [Prefill Phase]                  │
            │                             ↓
            │                    [ETE Profiling Coeffs]
            │                             │
            ↓                             ↓
       [Equation 11] ←──────── [WCET Estimator]
            │
            ↓
       Optimal α*
            │
            ↓
  [KV Cache Eviction (SnapKV)]
            │
            ↓
       [Decoding Phase]
            │
            ↓
       Response (ŷ)

- Critical path: RLP prediction and ETE estimation must complete *during* prefill phase (parallel execution on separate CPU/GPU) to avoid adding latency. If tPredict > tprefill, the overhead directly reduces available decoding time.

- Design tradeoffs:
  - **Bucket size B**: Smaller buckets (B=16) improve precision but increase classification difficulty; the paper found 512 buckets optimal
  - **Pessimistic factor k**: Higher k improves deadline compliance but forces higher α, degrading quality; k=5 was empirically optimal
  - **αmax**: Capping eviction at 95% prevents catastrophic quality loss but may miss tight deadlines

- Failure signatures:
  - **Consistent overruns**: k too low or profiling coefficients stale; re-profile on target hardware
  - **Quality collapse on specific tasks**: α too aggressive for attention-heavy workloads; consider task-specific αmax
  - **RLP gross errors on new domains**: Training distribution mismatch; collect domain-specific fine-tuning data

- First 3 experiments:
  1. **Profile the target hardware**: Measure tprefill and tdecoding-step across Nx ∈ [0, 32768] and Nkv ∈ [0, 32768], fit coefficients a, b, c, p, q using least squares
  2. **Train RLP on deployment data**: Use Arena-Human-Preference or domain-specific prompts; collect (prompt, actual_length) pairs from target LLM; train SLM classifier with bucket size B=16
  3. **Validate ETE before deployment**: Run inference with known time budgets; compare predicted vs. actual execution time; adjust k until ≥99% of predictions exceed actual time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TimeBill be extended to jointly optimize multiple configuration factors, such as dynamic quantization or speculative decoding, alongside KV cache eviction?
- Basis in paper: [inferred] Section 5.1 explicitly states that the analysis targets the KV cache eviction ratio $\alpha$ as the sole configuration factor $\theta$, despite the problem formulation (Eq. 1) implying a broader set of possible factors.
- Why unresolved: The paper does not explore the complex trade-offs or potential conflicts in simultaneously adjusting eviction ratios and other efficiency parameters like bit-width or decoding strategies.
- What evidence would resolve it: Experiments validating the framework's stability and performance when $\theta$ is a vector including both eviction ratio and quantization levels.

### Open Question 2
- Question: How robust is the workload-guided Execution Time Estimator (ETE) when deployed on heterogeneous hardware or significantly different model architectures without re-profiling?
- Basis in paper: [inferred] Section 4.2 notes that "actual execution time depends on implementation and hardware," relying on profiling-based fitting on a specific NVIDIA A40 GPU.
- Why unresolved: The analytical modeling relies on data-driven coefficients ($a,b,c,p,q$) derived from specific hardware profiles; the transferability of these coefficients is not discussed.
- What evidence would resolve it: Cross-device evaluation results showing prediction accuracy (MAPE) when models profiled on one GPU architecture are deployed on another.

### Open Question 3
- Question: Is there an adaptive method for setting the pessimistic factor $k$ to optimize the trade-off between deadline guarantee strictness and response quality?
- Basis in paper: [inferred] Section 6.5 concludes that "$k$ should be carefully selected" and shows performance varies significantly with different fixed values, yet the paper defaults to a static $k=5$.
- Why unresolved: A static factor may be too conservative for loose deadlines or too risky for tight deadlines in dynamic real-time environments.
- What evidence would resolve it: A control-theoretic or reinforcement learning mechanism that dynamically adjusts $k$ based on historical overrun rates and current system load.

## Limitations
- Accuracy depends critically on the training distribution of the RLP matching deployment prompts
- Performance relies on consistent hardware behavior between profiling and runtime
- Monotonic quality-eviction relationship may not hold across diverse task types
- Optimal pessimistic factor k for different hardware profiles remains uncertain

## Confidence
- High: Execution time prediction accuracy (MAPE 1.22-1.69%)
- Medium: Response length prediction accuracy (MAE 42.71, R² 0.719)
- Medium: Adaptive eviction mechanism generalizability across domains

## Next Checks
1. Test RLP performance degradation on prompts from domains absent in training data (e.g., code, mathematical reasoning)
2. Measure ETE accuracy variance under concurrent GPU workloads and thermal throttling conditions
3. Evaluate task-specific quality-eviction trade-offs for attention-intensive vs. token-repetition tasks