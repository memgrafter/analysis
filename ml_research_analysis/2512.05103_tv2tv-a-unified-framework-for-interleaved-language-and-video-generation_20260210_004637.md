---
ver: rpa2
title: 'TV2TV: A Unified Framework for Interleaved Language and Video Generation'
arxiv_id: '2512.05103'
source_url: https://arxiv.org/abs/2512.05103
tags:
- video
- text
- tv2tv
- generation
- interleaved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TV2TV introduces a unified generative modeling framework that decomposes
  video generation into an interleaved text and video generation process. The approach
  jointly learns language modeling and video flow matching using a Mixture-of-Transformers
  architecture, dynamically alternating between generating text plans and video frames
  during inference.
---

# TV2TV: A Unified Framework for Interleaved Language and Video Generation

## Quick Facts
- arXiv ID: 2512.05103
- Source URL: https://arxiv.org/abs/2512.05103
- Reference count: 26
- Primary result: Achieved 91% human preference over text-to-video baseline on gaming data and 54% holistic preference on sports videos

## Executive Summary
TV2TV introduces a unified generative modeling framework that decomposes video generation into an interleaved text and video generation process. The approach jointly learns language modeling and video flow matching using a Mixture-of-Transformers architecture, dynamically alternating between generating text plans and video frames during inference. This allows the model to "think in words" about subsequent content before "acting in pixels" to produce frames, offloading semantic decision-making to the language component and enabling fine-grained controllability through text interventions. On video game data, TV2TV achieved 91% preference over text-to-video baseline in visual quality and showed a 19-point improvement in instruction-following accuracy compared to think-then-act approaches. When scaled to real-world sports videos with synthetically augmented captions, TV2TV demonstrated strong visual quality and prompt alignment, winning 54% of holistic preference comparisons against comparable baselines.

## Method Summary
TV2TV uses a Mixture-of-Transformers (MoT) architecture with separate text and video towers, each with modality-specific QKV projections and feed-forward networks, while sharing a global self-attention layer for modality mixing. The model interleaves text generation (autoregressive next-token prediction) with video generation (flow matching via rectified flow) using a dual-noise representation (noisy + clean latents) to resolve the conflict between autoregressive conditioning and flow matching training. For training, video frames are tokenized using a VAE (Cosmos tokenizer), and the input sequence consists of interleaved text and video chunks with special tokens (BOF/EOF) for mode switching. The model maintains two copies of each video frame's representation—noisy for flow prediction and clean for conditioning—allowing it to condition on clean historical context while learning to denoise current frames. Training uses teacher forcing with a p_clean-vid-flip probability, and inference employs an ODE solver for video generation.

## Key Results
- 91% human preference over text-to-video baseline on CS:GO gaming data for visual quality
- 19-point improvement in instruction-following accuracy compared to think-then-act approaches
- 54% holistic preference on sports videos against comparable baselines

## Why This Works (Mechanism)

### Mechanism 1
Interleaved text generation reduces visual generation entropy by offloading semantic decisions to language. The model first generates a textual description ("think in words") of the next video segment, which conditions the subsequent frame generation via cross-modal attention, providing a lower-entropy, semantically grounded conditioning signal. Core assumption: Language is a more efficient planning medium for semantic branching than raw pixels.

### Mechanism 2
A dedicated Mixture-of-Transformers (MoT) architecture preserves modality-specific strengths while enabling joint reasoning. Separate transformer towers for text and video apply modality-specific QKV projections and feed-forward networks, while a global, modality-mixing self-attention layer allows the text plan to directly condition the video latents. Core assumption: Decoupling modality-specific processing is superior to a shared transformer for this interleaved task.

### Mechanism 3
The dual-noise representation (noisy + clean latents) resolves the conflict between autoregressive conditioning and flow matching training. Two copies of each video chunk are maintained: a noisy version for flow prediction and a clean version for conditioning, allowing the model to attend to a clean, grounded history when predicting the flow for the current chunk. Core assumption: Access to clean historical context is critical for coherent, long-sequence generation.

## Foundational Learning

Concept: **Flow Matching / Rectified Flow**
- Why needed here: This is the core loss function for video generation. It learns to denoise a latent representation by predicting the vector field from noise to clean data.
- Quick check question: Can you explain how the timestep `t` is sampled and used to create `x_noisy-vid` from `x_clean-vid` (Equation 2)?

Concept: **Mixture-of-Transformers (MoT)**
- Why needed here: This is the architectural backbone. It's crucial to understand that while QKV and FFN are modality-specific, the self-attention is shared and global.
- Quick check question: In the MoT block, which weights are *not* shared between the text and video pathways?

Concept: **Autoregressive vs. Flow-Based Generation**
- Why needed here: The model's power comes from hybridizing these two paradigms. Text is generated autoregressively (token-by-token), while video is generated via flow (chunk-by-chunk).
- Quick check question: What triggers the model to switch from text generation mode to video generation mode?

## Architecture Onboarding

Component map:
User Prompt -> Text Tower -> [BOF] -> Video Tower (conditioned on text) -> Flow Matching ODE Solver -> Clean Video Latent -> [EOF] -> Text Tower (conditioned on video) -> ...

Critical path: User Prompt -> Text Tower -> [BOF] -> Video Tower (conditioned on text) -> Flow Matching ODE Solver -> Clean Video Latent -> [EOF] -> Text Tower (conditioned on video) -> ...

Design tradeoffs:
- **Clean vs. Noisy Latents**: Maintaining two copies doubles the sequence length for video chunks but enables stable teacher-forcing.
- **Text Placement**: Placing text *before* same-timestamp video allows "thinking" before "acting" but assumes text can be pre-generated or synthetically added.
- **Global vs. Local Attention**: Causal masking globally (between chunks) vs. bidirectional attention locally (within a chunk).

Failure signatures:
- **Hallucinated Plan**: The generated text plan describes an impossible or inconsistent action, leading to incoherent video.
- **Modality Imbalance**: Over-reliance on text (ignores visual history) or over-reliance on video (ignores the text plan).
- **Training Instability**: If λtxt or λvid are poorly weighted, one loss may dominate, crippling the other modality's performance.

First 3 experiments:
1. **Interleaved Validation**: Train on a tiny dataset. Verify the model can generate BOF, switch modes, and produce a plausible video chunk conditioned on a text prompt. Check for the "noisy+clean" latent structure.
2. **Ablate Text Offloading**: Train a T2V baseline without interleaved text. Compare its output to TV2TV. Does the interleaved text improve visual coherence?
3. **Intervention Steerability**: Generate a video, then manually insert a new text intervention at an intermediate step. Does the subsequent video generation change accordingly?

## Open Questions the Paper Calls Out

### Open Question 1
How can high-quality, temporally-aligned interleaved text-video training data be obtained for general video domains beyond gaming, which naturally provide action-text pairs? The paper relies on VLM-generated captions for sports data, which "often contain hallucinations." Experiments comparing TV2TV trained on human-annotated interleaved captions vs. VLM-generated captions across multiple domains would resolve this.

### Open Question 2
What is the optimal granularity and frequency of interleaved text for different video domains and task complexities? The paper does not systematically vary interleaving frequency to determine optimal density for different semantic complexity levels or video types. Controlled experiments varying caption frequency would help answer this.

### Open Question 3
How does the model decide when to generate text versus video frames, and can this be improved through learned or explicit scheduling? The paper does not analyze the factors influencing the model's switching decisions or compare against alternative approaches like fixed intervals or user-specified schedules.

## Limitations

- **Hallucination Risk**: Sports video experiments rely on VLM-generated captions that are "often hallucinatory," potentially explaining performance differences between gaming (91% preference) and sports (54% preference).
- **Synthetic Data Dependency**: Claims about handling "real-world" videos are tempered by the synthetic nature of sports captions, with the generalizability claim extending beyond what is directly demonstrated.
- **Architecture Specificity**: Limited ablation studies comparing MoT to alternatives like fully shared transformers or separate encoder-decoder pairs, with indirect evidence for MoT's superiority.

## Confidence

**High Confidence**: The core architectural innovation (Mixture-of-Transformers with interleaved text-video generation) is technically sound and well-documented. The dual-noise representation mechanism is clearly explained.

**Medium Confidence**: The 91% preference over T2V baseline and 19-point improvement in instruction-following accuracy are based on human evaluations, but methodology is not fully specified.

**Low Confidence**: Claims about TV2TV's ability to handle "real-world" video generation are tempered by the synthetic nature of the sports captions.

## Next Checks

1. **Hallucination Robustness Test**: Generate videos using both clean and hallucinated text plans (synthetically corrupted captions) on the sports dataset. Measure whether TV2TV's visual quality degrades proportionally to caption quality or if it maintains coherence through visual grounding.

2. **Architecture Ablation**: Implement a simplified baseline using shared transformer weights across modalities. Train on the same CS:GO dataset and compare not just preference metrics but also attention pattern analysis to verify whether MoT genuinely prevents modality interference.

3. **Intervention Generalization**: Design a systematic test where users can insert arbitrary text interventions at multiple points during video generation. Measure the consistency and appropriateness of visual responses to interventions that differ from the original text plan.