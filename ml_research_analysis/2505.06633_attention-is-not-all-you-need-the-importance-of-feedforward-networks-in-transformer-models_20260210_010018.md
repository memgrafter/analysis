---
ver: rpa2
title: 'Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer
  Models'
arxiv_id: '2505.06633'
source_url: https://arxiv.org/abs/2505.06633
tags:
- transformer
- layers
- dmodel
- linear
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the role of the feedforward network (FFN) component
  in transformer blocks during pre-training. The study evaluates models with varying
  numbers of linear layers per transformer block (0, 1, 2, and 3 layers) on Booksum
  and Wikitext datasets.
---

# Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models

## Quick Facts
- arXiv ID: 2505.06633
- Source URL: https://arxiv.org/abs/2505.06633
- Authors: Isaac Gerber
- Reference count: 21
- Three-layer FFNs with fewer transformer blocks outperform standard two-layer configurations in training speed and test loss.

## Executive Summary
This paper challenges the common assumption that attention mechanisms are the primary driver of transformer performance by examining the role of feedforward networks (FFNs) in decoder-only transformer models. Through systematic ablation studies varying FFN depth from 0 to 3 linear layers, the research demonstrates that three-layer FFNs achieve lower training loss than standard two-layer configurations while using fewer total parameters and training faster. Specifically, a three-layer FFN model with 10 transformer blocks outperformed the baseline with 24 blocks on both Booksum and Wikitext datasets, achieving statistically significant improvements on Booksum and comparable performance on Wikitext with approximately 13% faster training time.

## Method Summary
The study evaluates decoder-only transformer models with varying FFN depths (0, 1, 2, or 3 linear layers per block) on Booksum Complete Cleaned and Wikitext 103 v1 datasets. The baseline configuration matches GPT-3 Medium (24 blocks, d_model=1024, 323M parameters) with two-layer FFNs using 4d expansion. Models are trained with GELU activation, dropout 0.1, Xavier uniform initialization for MHA/linear layers, and AdamW optimizer with cosine learning rate decay. Training runs for one epoch with sequence length 256, batch size 16, and maximum learning rate 1.5e-4 with 300 warm-up steps. Performance is measured by mean cross-entropy loss on test sequences with standard error, and training duration is recorded.

## Key Results
- Three-layer FFN models (10 blocks, d=1024) achieved statistically significant lower test loss than baseline (24 blocks, d=1024) on Booksum dataset
- Same three-layer configuration achieved comparable performance to baseline on Wikitext dataset
- Three-layer FFN model trained approximately 13% faster while using fewer total parameters than baseline
- Models with 0 or 1-layer FFNs failed to train effectively, showing unstable gradients and high loss

## Why This Works (Mechanism)
None

## Foundational Learning
- **Decoder-only transformer architecture**: Essential for understanding the model structure; quick check: verify attention masks are causal and no encoder-decoder attention layers exist
- **Feedforward network design patterns**: Critical for implementing configurable FFN depths; quick check: confirm FFN expansion factor is 4x for 2-layer and appropriately scaled for 3-layer
- **Transformer initialization schemes**: Important for stable training across different FFN depths; quick check: verify Xavier uniform for MHA/linear and normal for embeddings
- **Learning rate scheduling**: Key for training stability with varying FFN depths; quick check: confirm cosine decay with 300 warm-up steps at 1.5e-4 max LR
- **Tokenization and vocabulary size**: Necessary for consistent input processing; quick check: verify separate tokenizers per dataset with vocab size 10,000
- **Dataset preprocessing**: Required for reproducible results; quick check: confirm Booksum Complete Cleaned and Wikitext 103 v1 splits match original

## Architecture Onboarding
- **Component map**: Tokenizer -> Embedding Layer -> Positional Encoding -> Transformer Block (MHA -> FFN) -> Output Layer
- **Critical path**: Token embedding → positional encoding → multi-head attention → FFN → next-token prediction
- **Design tradeoffs**: FFN depth vs. transformer block count vs. parameter efficiency; deeper FFNs allow shallower networks while maintaining performance
- **Failure signatures**: 0/1-layer FFNs show vanishing gradients and unstable training; 3-layer FFNs risk overfitting if not properly regularized
- **First experiments**: (1) Implement 2-layer FFN baseline matching GPT-3 Medium specifications, (2) Add 3-layer FFN configuration with appropriate hidden dimensions, (3) Test gradient stability across FFN depths with baseline initialization

## Open Questions the Paper Calls Out
- **Scalability to larger models**: The efficiency of three-layer FFNs may not persist when scaling to billions of parameters or using optimized attention mechanisms like FlashAttention, as the study was limited to smaller models (~323M parameters) and standard attention.
- **Downstream task performance**: The reduction in pre-training cross-entropy loss for three-layer FFNs may not correlate with improved performance on downstream reasoning benchmarks, as the study evaluated models solely on perplexity/loss rather than measuring functional capabilities.
- **Optimal architectural balance**: The ideal relationship between FFN layer count, transformer block depth, and model dimensionality for a fixed parameter budget remains unexplored, though results suggest an essential balance that could be further investigated.

## Limitations
- The study only tested standard multi-head attention, noting findings may not hold with other attention mechanisms
- Computational constraints limited testing to models around 323M parameters rather than larger-scale architectures
- The exact hidden dimensions for 3-layer FFN configurations were not specified, creating uncertainty in exact reproduction

## Confidence
- Core finding (three-layer FFNs improve efficiency and performance): High
- Exact architectural specifications (hidden dimensions, tokenizer settings): Medium
- Training dynamics and hyperparameter sensitivity: Medium
- Generalizability to other architectures and scales: Low

## Next Checks
- Verify the exact hidden dimensions and parameter counts for each FFN configuration (0, 1, 2, 3 layers) to ensure proper scaling
- Confirm the tokenizer training process and dataset splits match the original implementation for both Booksum and Wikitext
- Test gradient stability across different FFN depths with the specified initialization and learning rate schedule to ensure reproducible training dynamics