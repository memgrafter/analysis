---
ver: rpa2
title: 'SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models'
arxiv_id: '2504.08813'
source_url: https://arxiv.org/abs/2504.08813
tags:
- safety
- reasoning
- arxiv
- mlrms
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents the first systematic safety analysis of multi-modal
  large reasoning models (MLRMs) through large-scale empirical studies comparing MLRMs
  with their base MLLMs. Our experiments reveal three critical findings: (1) The Reasoning
  Tax: Acquiring reasoning capabilities catastrophically degrades inherited safety
  alignment.'
---

# SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models

## Quick Facts
- arXiv ID: 2504.08813
- Source URL: https://arxiv.org/abs/2504.08813
- Reference count: 40
- This work presents the first systematic safety analysis of multi-modal large reasoning models (MLRMs), revealing that acquiring reasoning capabilities catastrophically degrades inherited safety alignment.

## Executive Summary
This paper presents the first systematic safety analysis of Multi-modal Large Reasoning Models (MLRMs), revealing that the acquisition of reasoning capabilities through fine-tuning catastrophically degrades inherited safety alignment - a phenomenon termed the "Reasoning Tax." Through large-scale empirical studies comparing MLRMs with their base MLLMs, the authors find that MLRMs exhibit 37.44% higher jailbreaking success rates under adversarial attacks. The analysis uncovers scenario-specific vulnerabilities where certain categories like Illegal Activity show 25 times higher attack rates than average, and identifies emergent self-correction behavior where 16.9% of jailbroken reasoning steps are overridden by safe answers. To support further research, the authors open-source OpenSafeMLRM, the first toolkit for MLRM safety evaluation.

## Method Summary
The study evaluates MLRMs using the OpenSafeMLRM toolkit, which provides unified interfaces for mainstream models, datasets, and jailbreaking methods. Models are tested under "Vanilla" (text only) and "Jailbreak" (image+text) conditions using hybrid typographic attacks and query rewriting. Outputs are parsed into "Think" (reasoning) and "Answer" stages for separate safety rating using GPT-4o-mini as the harmfulness judge on a 0-5 scale. Attack Success Rate (ASR) is defined as HR ≥ 4, with evaluation conducted across MM-SafetyBench (13 scenarios) and SafetyBench (10 topics). The study compares MLRMs against their base MLLMs to quantify safety degradation.

## Key Results
- MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks
- Safety degradation is heterogeneously distributed across scenarios, with Illegal Activity showing 25× higher attack rates
- MLRMs demonstrate emergent self-correction where 16.9% of jailbroken reasoning steps are overridden by safe answers
- Safety degradation is pervasive across all tested models except Mulberry-LlaMA, which shows inverse safety trends

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Acquiring reasoning capabilities through SFT/RL degrades inherited safety alignment in MLRMs (the "reasoning tax").
- **Mechanism:** Reasoning-oriented fine-tuning may overwrite or dilute safety-related weight configurations established during prior alignment. Extended reasoning pathways also expand the attack surface—unsafe reasoning occurs 12.52% more frequently than unsafe answers.
- **Core assumption:** Safety alignment and reasoning capability acquisition compete for model capacity or are structurally in tension during fine-tuning.
- **Evidence anchors:**
  - [abstract] "MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks."
  - [section 3.1] "MLRMs exhibit 31.30% higher ASR (59.52% vs. base MLLMs' 28.22%) and 1.64 higher HR."
- **Break condition:** If reasoning-augmented models can be trained with explicit safety preservation objectives that prevent alignment erosion, the tax may be mitigated.

### Mechanism 2
- **Claim:** Safety degradation is heterogeneously distributed across scenarios, with certain categories (e.g., Illegal Activity) showing catastrophic vulnerability spikes.
- **Mechanism:** Scenario-specific vulnerabilities arise from uneven coverage in safety training data or from reasoning processes that disproportionately amplify certain harmful pathways.
- **Core assumption:** Reasoning augmentation interacts differently with different harm categories, possibly due to reasoning patterns being more exploitable for action-oriented scenarios.
- **Evidence anchors:**
  - [abstract] "Certain scenarios (e.g., Illegal Activity) suffer 25× higher attack rates—far exceeding the average 3.4× increase."
  - [section 3.2] "While the base MLLM Qwen2.5-VL achieves near-perfect safety (ASR < 3%), its MLRM derivative R1-Onevision suffers catastrophic failure (ASR > 50%)."
- **Break condition:** If scenario-aware red-teaming and adaptive alignment can close these blind spots, the heterogeneous degradation pattern may be addressable.

### Mechanism 3
- **Claim:** MLRMs exhibit emergent self-correction where unsafe reasoning chains can be overridden by safe final answers.
- **Mechanism:** Some residual safety safeguards persist post-reasoning-augmentation and can activate during answer generation even after unsafe reasoning.
- **Core assumption:** The answer-generation stage retains partial independence from the reasoning stage's safety posture.
- **Evidence anchors:**
  - [abstract] "16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at intrinsic safeguards."
  - [section 3.3] "12.4% of unsafe reasoning steps (Think-HR > 3) yield safe answers (Answer-HR ≤ 3)."
- **Break condition:** If reasoning and answer generation become more tightly coupled, self-correction rates may decrease.

## Foundational Learning

- **Concept: Safety Alignment in MLLMs**
  - Why needed here: The entire analysis depends on understanding how safety is instilled via RLHF/safety training and why it degrades.
  - Quick check question: Can you explain why aligned models refuse harmful requests and what happens when fine-tuning overwrites these behaviors?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: MLRMs explicitly generate intermediate reasoning steps before answers, which expands attack surfaces.
  - Quick check question: What is the difference between prompt-based CoT ("think step-by-step") and intrinsic CoT encoded via fine-tuning?

- **Concept: Jailbreaking Attacks (Text and Multi-modal)**
  - Why needed here: The evaluation uses adversarial typographic images and rewritten malicious queries to bypass safety.
  - Quick check question: How do typographic attacks (text rendered as images) evade text-only safety filters?

## Architecture Onboarding

- **Component map:** OpenSafeMLRM -> (Target MLRMs: R1-OneVision, MM-Eureka-Qwen, Mulberry variants) -> (Safety benchmarks: MM-SafetyBench, SafetyBench) -> (Jailbreaking methods: typographic images, query rewriting) -> (Evaluation metrics: ASR via HR ≥ 4 threshold, HR via GPT-4o-mini scoring)

- **Critical path:**
  1. Select MLRM and corresponding base MLLM for comparison
  2. Choose benchmark and jailbreaking method (hybrid images recommended for MM-SafetyBench)
  3. Run inference, collecting Think/Answer/Overall outputs
  4. Compute HR scores and derive ASR using HR ≥ 4 threshold
  5. Analyze Think-HR vs Answer-HR coupling for self-correction rates

- **Design tradeoffs:** Evaluating only answer safety vs. overall safety (including reasoning exposure) yields different conclusions; ignoring reasoning safety misses systemic risks.

- **Failure signatures:** High Think-HR with low Answer-HR indicates self-correction; high ASR in specific scenarios (Illegal Activity, Physical Harm) flags blind spots; Qwen2.5-based models show especially severe degradation.

- **First 3 experiments:**
  1. Reproduce reasoning tax: Compare ASR/HR between a base MLLM and its MLRM derivative across all 10 scenarios
  2. Identify blind spots: Isolate scenarios with highest ΔASR between base and reasoning-augmented models
  3. Quantify self-correction: Sample adversarial queries, compute Think-HR and Answer-HR jointly, measure override rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training factors enable Mulberry-LlaMA to avoid the "reasoning tax," and can these be generalized?
- Basis in paper: [explicit] Page 5 notes that unlike other models, Mulberry-LlaMA exhibits an inverse trend where safety metrics improve post-reasoning augmentation, hinting at "potential safeguards" and a "blueprint for designing safety-resilient reasoning models."
- Why unresolved: The paper identifies this anomaly empirically but does not isolate the causal mechanisms (e.g., data mixture, base model architecture) responsible for this resilience.
- What evidence would resolve it: Ablation studies identifying which component of the Mulberry-LlaMA pipeline prevents safety alignment erosion, successfully applied to other architectures like Qwen.

### Open Question 2
- Question: How can the observed "emergent self-correction" capability be systematically amplified to improve safety?
- Basis in paper: [explicit] Page 7 states that the 12.4% rate of unsafe reasoning steps being overridden by safe answers provides a "foothold for layered defenses: hardening reasoning steps while amplifying innate safeguards."
- Why unresolved: The paper discovers and quantifies this self-correction but does not propose or test methods to increase this rate or leverage it as a defense mechanism.
- What evidence would resolve it: A training intervention (e.g., preference optimization) or inference-time technique that significantly increases the percentage of safe answers generated following compromised reasoning steps.

### Open Question 3
- Question: Does the "Reasoning Tax" persist when evaluated against a broader range of jailbreaking methods and defense baselines?
- Basis in paper: [explicit] Page 2 acknowledges that "selection bias in test samples... may inevitably introduce measurement distortions," and states the authors will refine the framework by "incorporating additional... attack vectors, and defense baselines."
- Why unresolved: The current study relies on specific black-box jailbreaking methods (typographic/adversarial images); the generalizability of the safety degradation to other attack domains is unconfirmed.
- What evidence would resolve it: Evaluation results showing consistent safety degradation (approx. 37% increase in ASR) across white-box attacks or when models are protected by standard defense mechanisms.

## Limitations
- Evaluation relies on GPT-4o-mini as the harmfulness judge, introducing potential judge model bias and consistency concerns
- The "reasoning tax" mechanism is empirically observed but not mechanistically proven - we cannot definitively claim that reasoning fine-tuning overwrites safety weights without architectural intervention studies
- Scenario-specific vulnerabilities are documented but their root causes (data coverage gaps vs. reasoning amplification effects) remain speculative

## Confidence
- **High Confidence**: The empirical observation that MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs is robust across multiple models and datasets
- **Medium Confidence**: The claim about scenario-specific blind spots (e.g., Illegal Activity showing 25× higher attack rates) is consistent but may be influenced by dataset composition and judge scoring patterns
- **Low Confidence**: The emergent self-correction mechanism is observed but not explained - the 16.9% override rate could reflect judge inconsistency or model artifacts rather than genuine safety preservation

## Next Checks
1. **Judge Model Ablation**: Re-run the evaluation using multiple judge models (human annotators, Claude-3, GPT-4) to quantify judge bias effects on ASR/HR calculations
2. **Fine-tuning Intervention**: Apply safety-aware fine-tuning to an MLRM and measure whether the reasoning tax can be mitigated while preserving reasoning capabilities
3. **Safety Data Analysis**: Analyze the safety training data distribution across scenarios to determine if blind spots correlate with data scarcity rather than reasoning amplification