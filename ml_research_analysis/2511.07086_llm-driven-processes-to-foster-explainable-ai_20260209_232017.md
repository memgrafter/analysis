---
ver: rpa2
title: LLM Driven Processes to Foster Explainable AI
arxiv_id: '2511.07086'
source_url: https://arxiv.org/abs/2511.07086
tags:
- reasoning
- system
- vester
- sensitivity
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a modular, explainable LLM-agent pipeline\
  \ that externalizes reasoning into auditable artifacts for decision support. The\
  \ system implements three structured frameworks\u2014Vester\u2019s Sensitivity Analysis,\
  \ normal-form games, and sequential games\u2014with swappable LLM components paired\
  \ with deterministic analyzers."
---

# LLM Driven Processes to Foster Explainable AI

## Quick Facts
- arXiv ID: 2511.07086
- Source URL: https://arxiv.org/abs/2511.07086
- Authors: Marcel Pehlke; Marc Jansen
- Reference count: 24
- Primary result: Modular LLM-agent pipeline achieved 55.5% factor alignment with human baselines and 92.97/100 score from LLM judge

## Executive Summary
This paper presents a modular, explainable LLM-agent pipeline that externalizes reasoning into auditable artifacts for decision support. The system implements three structured frameworks—Vester's Sensitivity Analysis, normal-form games, and sequential games—with swappable LLM components paired with deterministic analyzers. In a real-world logistics case (100 runs), the Vester pipeline achieved 55.5% factor alignment with human baselines overall (62.9% on core factors) and 57% role agreement. An LLM judge scored pipeline runs at 92.97/100, matching human baseline quality.

## Method Summary
The research introduces a modular LLM-driven pipeline architecture that separates reasoning into externalized, auditable artifacts rather than producing opaque outputs. The system implements three decision support frameworks: Vester's Sensitivity Analysis for factor identification and relationship mapping, normal-form games for analyzing interactions between two agents, and sequential games for modeling turn-based strategic decisions. Each framework consists of multiple LLM components (prompting strategies, role-based agents, external tools) connected through a structured workflow. The pipeline was validated using 100 runs in a logistics case study, comparing results against human expert baselines through factor alignment metrics and LLM judge scoring.

## Key Results
- Vester pipeline achieved 55.5% overall factor alignment with human baselines (62.9% on core factors)
- Role agreement reached 57% in comparing pipeline-generated versus human baseline roles
- LLM judge scored pipeline runs at 92.97/100, indicating high quality matching human baseline outputs

## Why This Works (Mechanism)
The pipeline succeeds by externalizing the LLM's reasoning process into traceable artifacts rather than relying on single-shot outputs. Each decision support framework breaks down complex analytical tasks into discrete steps where LLM components handle specific subtasks (factor identification, relationship analysis, role assignment) while deterministic analyzers verify outputs. This modular approach enables auditing of each reasoning step, identifies where divergences from human baselines occur, and maintains consistency across multiple runs. The structured frameworks provide clear constraints that guide LLM outputs toward domain-relevant results rather than general reasoning.

## Foundational Learning
- **Vester Sensitivity Analysis**: Systematic method for identifying critical factors and their interdependencies in complex systems - needed for structured factor identification in decision support
- **Normal-form games**: Matrix representation of strategic interactions between agents - required for analyzing multi-party decision scenarios
- **Sequential games**: Turn-based decision modeling with perfect or imperfect information - essential for modeling dynamic strategic processes
- **Modular AI architecture**: Separable components with defined interfaces - enables component swapping and targeted debugging
- **Externalized reasoning**: Making intermediate reasoning steps visible and auditable - critical for explainable AI
- **Deterministic verification**: Automated checking of LLM outputs against logical constraints - ensures output validity

## Architecture Onboarding

Component map: Input -> Factor Identifier -> Relationship Analyzer -> Role Assigner -> Policy Generator -> LLM Judge

Critical path: Input document → Factor identification → Relationship mapping → Role assignment → Policy generation → Quality assessment

Design tradeoffs:
- **Modularity vs. performance**: Breaking into components increases explainability but may reduce end-to-end efficiency
- **LLM flexibility vs. deterministic verification**: Allowing LLM creativity balanced against automated correctness checks
- **Human baseline alignment vs. novel insights**: Trade-off between matching human reasoning patterns and discovering new solutions

Failure signatures:
- Factor misalignment (>40% divergence from baselines indicates potential issues)
- Role assignment inconsistencies across runs
- Relationship mapping contradictions
- Policy generation that fails deterministic verification

First experiments:
1. Run single-factor identification with controlled input to verify component isolation
2. Test relationship analyzer with pre-defined factor sets to validate output structure
3. Execute end-to-end pipeline with simplified input to establish baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation metrics rely heavily on human judgment and subjective scoring criteria
- 55.5% factor alignment indicates substantial divergence in over 40% of cases
- Limited validation to logistics and housing policy domains raises questions about generalizability

## Confidence

High confidence:
- Modular pipeline architecture and technical implementation of Vester analysis, normal-form games, and sequential games frameworks are clearly described and reproducible

Medium confidence:
- Reported performance metrics and alignment with human baselines, given the subjective nature of some evaluation criteria and limited transparency in scoring methodology
- Generalizability of findings to other domains, as primary validation was limited to logistics and housing policy applications

## Next Checks
1. Conduct blind external validation using independent human experts to assess factor alignment and role agreement scores, comparing results to original human baseline judgments

2. Implement cross-validation across multiple domain areas beyond logistics and housing to test pipeline's robustness and generalizability of 55.5% alignment metric

3. Perform ablation studies systematically removing individual LLM components to quantify their specific contributions to 92.97/100 overall score and identify which modules drive most variance in outputs