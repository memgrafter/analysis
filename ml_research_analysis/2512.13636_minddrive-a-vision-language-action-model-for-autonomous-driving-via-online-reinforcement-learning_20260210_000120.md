---
ver: rpa2
title: 'MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online
  Reinforcement Learning'
arxiv_id: '2512.13636'
source_url: https://arxiv.org/abs/2512.13636
tags:
- driving
- learning
- action
- minddrive
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MindDrive introduces an online reinforcement learning framework\
  \ for Vision-Language-Action models in autonomous driving by decoupling decision-making\
  \ from action execution. It employs two specialized LLMs\u2014one for high-level\
  \ scenario reasoning (Decision Expert) and another for mapping linguistic decisions\
  \ to continuous trajectories (Action Expert)\u2014connected through LoRA adapters."
---

# MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.13636
- Source URL: https://arxiv.org/abs/2512.13636
- Authors: Haoyu Fu; Diankun Zhang; Zongchuang Zhao; Jianfeng Cui; Hongwei Xie; Bing Wang; Guang Chen; Dingkang Liang; Xiang Bai
- Reference count: 40
- Primary result: 78.04 Driving Score, 55.09% Success Rate on Bench2Drive benchmark

## Executive Summary
MindDrive introduces an online reinforcement learning framework for Vision-Language-Action models in autonomous driving by decoupling decision-making from action execution. It employs two specialized LLMs—one for high-level scenario reasoning (Decision Expert) and another for mapping linguistic decisions to continuous trajectories (Action Expert)—connected through LoRA adapters. This design enables efficient exploration in language space while leveraging trajectory rewards to refine reasoning. Using a lightweight Qwen-2.5-0.5B LLM, MindDrive achieves 78.04 Driving Score and 55.09% Success Rate on Bench2Drive, outperforming the state-of-the-art IL baseline by 5.15 DS and 9.26% SR, marking the first successful application of online RL to VLA models in autonomous driving.

## Method Summary
MindDrive implements a two-stage training pipeline with a dual-expert architecture for autonomous driving. The system uses a lightweight Qwen2.0.5B LLM base with EVA-02-L vision encoder and dual LoRA adapters (rank=16, alpha=16). Stage 1 involves supervised imitation learning where both Decision Expert and Action Expert are trained jointly using Chat-B2D reasoning data and planning QA pairs (234,769 samples), with a VAE+GRU decoder mapping linguistic decisions to 6 speed waypoints and 20 path waypoints. Stage 2 deploys online reinforcement learning with PPO across 24 parallel CARLA simulators, collecting rollouts on 44 selected routes (2 rollouts per route) with sparse rewards (+1/-1/0). The framework achieves stable training through KL regularization and gradient clipping, completing 10 PPO epochs while maintaining the learned reasoning capabilities.

## Key Results
- Achieves 78.04 Driving Score and 55.09% Success Rate on Bench2Drive closed-loop benchmark
- Outperforms IL baseline by 5.15 DS and 9.26% SR, establishing first successful online RL application to VLA models
- Demonstrates stable learning across 10 PPO epochs with KL regularization preventing catastrophic forgetting

## Why This Works (Mechanism)
MindDrive succeeds by addressing the fundamental challenge of exploration in language space through its dual-expert architecture. The Decision Expert performs high-level reasoning about driving scenarios using natural language, while the Action Expert maps these linguistic decisions to continuous trajectories. This separation enables efficient exploration since language space is more compact than continuous action space, and linguistic decisions are more generalizable across scenarios. The LoRA adapters allow both experts to share the same base LLM while maintaining distinct capabilities, with KL regularization ensuring the RL process doesn't degrade the reasoning learned during IL.

## Foundational Learning
- Vision-Language-Action models: Neural architectures that process visual inputs, understand language instructions, and generate driving actions; needed because autonomous driving requires understanding both perception and language context simultaneously; quick check: verify the vision encoder and LLM are properly integrated
- Online reinforcement learning: Training agents through interaction with environment rather than offline data; needed to overcome imitation learning limitations like distribution shift and causal confusion; quick check: confirm PPO implementation and reward structure
- LoRA adapters: Parameter-efficient fine-tuning method using low-rank matrices; needed to maintain separate expert capabilities while sharing base LLM parameters; quick check: verify LoRA ranks and initialization
- GAE (Generalized Advantage Estimation): Method for computing advantages in RL using value function estimates; needed for stable policy gradient updates with sparse rewards; quick check: verify λ parameter and value network training
- VAE+GRU decoder: Variational autoencoder with recurrent network for trajectory generation; needed to map linguistic decisions to continuous driving trajectories; quick check: verify latent space dimensionality and sequence length
- CARLA simulator: Open-source autonomous driving simulation platform; needed for safe, scalable evaluation and RL training; quick check: verify simulator version and sensor configurations

## Architecture Onboarding

Component map: Vision Encoder -> Decision Expert -> Action Expert -> VAE+GRU Decoder -> CARLA Simulator

Critical path: Camera images → Vision encoder → Decision Expert reasoning → Action Expert trajectory mapping → VAE+GRU decoder → 6 speed + 20 path waypoints → CARLA driving

Design tradeoffs:
- Dual expert vs. single expert: Dual experts prevent catastrophic forgetting and maintain separate reasoning/action capabilities, but increase complexity
- LoRA rank selection: Rank=16 balances parameter efficiency with learning capacity; higher ranks may overfit while lower ranks underfit
- Parallel rollout count: 24 collectors provide sufficient diversity without excessive computational cost; fewer collectors reduce sample efficiency
- KL regularization weight: λ_KL=0.5 prevents forgetting while allowing adaptation; higher values restrict learning too much

Failure signatures:
- Catastrophic forgetting: DS drops below baseline after multiple rollouts (>2 rounds), indicating reasoning degradation
- Poor trajectory quality: Low success rate despite reasonable reasoning, suggesting Action Expert mapping issues
- Value network errors: Unstable training with single rollout, requiring minimum 2 rollouts for stable GAE

First experiments:
1. IL baseline training: Train both experts jointly on Chat-B2D data, verify trajectory generation quality
2. Single expert ablation: Remove dual expert architecture, train single expert with same LoRA configuration
3. Rollout sensitivity: Test performance with 1 vs. 2 rollouts per route to verify minimum requirement

## Open Questions the Paper Calls Out

### Open Question 1
Can MindDrive's online RL framework transfer effectively to real-world driving environments, or does the sim-to-real gap introduce fundamental challenges that require architectural modifications?
- Basis in paper: "Due to the absence of real world interactive simulators, our evaluation is restricted to the CARLA simulator."
- Why unresolved: Real-world deployment requires safe exploration mechanisms and may face domain shift in visual inputs and vehicle dynamics.
- What evidence would resolve it: Real-world closed-loop testing on physical vehicles with safety constraints, or evaluation on real-world driving datasets with offline RL baselines.

### Open Question 2
Would implementing GRPO (Group Relative Policy Optimization) with synchronized simulator states provide measurable improvements over the current PPO-based approach?
- Basis in paper: "The challenge of synchronizing multiple CARLA simulators precludes the evaluation of alternative actions from an identical initial state, restricting our application of the GRPO algorithm."
- Why unresolved: GRPO requires evaluating multiple actions from identical states, which current infrastructure cannot support; it remains unknown whether the relative reward comparison in GRPO would improve sample efficiency.
- What evidence would resolve it: Ablation study comparing PPO vs. GRPO with synchronized state initialization, measuring sample efficiency and final performance.

### Open Question 3
How does MindDrive's performance scale with larger base LLMs, and does the dual-LoRA architecture maintain its benefits at increased model capacities?
- Basis in paper: The paper uses only Qwen2-0.5B and compares against larger models (Vicuna-7B, Paligemma-3B), but does not ablate model scaling within the MindDrive framework.
- Why unresolved: It is unclear whether the lightweight model's success stems from the architecture's efficiency or represents a ceiling that larger models could exceed.
- What evidence would resolve it: Systematic evaluation with 1.5B, 3B, and 7B base LLMs on the same Bench2Drive benchmark.

## Limitations
- Restricted to CARLA simulation environment, lacking real-world validation and sim-to-real transfer assessment
- Missing complete implementation details for system prompts, value network architecture, and VAE/GRU components
- No ablation studies on LoRA rank selection or detailed training dynamics for KL regularization weight

## Confidence

High confidence in the overall methodology and reported benchmark performance
Medium confidence in the exact implementation details due to missing specifications
Medium confidence in the claimed improvements over IL baselines, given the controlled comparison setup

## Next Checks

1. Reproduce the IL baseline and MindDrive RL results on Bench2Drive with identical hyperparameters and hardware
2. Conduct ablation studies removing the KL regularization and single expert architecture to verify the stated failure modes
3. Test the trained models on previously unseen routes in CARLA to assess generalization beyond the 44 RL training routes