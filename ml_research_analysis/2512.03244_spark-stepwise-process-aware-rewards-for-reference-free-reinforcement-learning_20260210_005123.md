---
ver: rpa2
title: 'SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning'
arxiv_id: '2512.03244'
source_url: https://arxiv.org/abs/2512.03244
tags:
- step
- training
- verification
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of training process reward models\
  \ for reinforcement learning without requiring ground truth references or human\
  \ annotations. The authors propose SPARK, a three-stage framework that generates\
  \ synthetic step-level verification data using inference-time scaling methods\u2014\
  self-consistency for parallel verification aggregation and meta-critique for sequential\
  \ refinement."
---

# SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2512.03244
- **Source URL**: https://arxiv.org/abs/2512.03244
- **Reference count**: 40
- **Primary result**: SPARK achieves 67.5 F1 on ProcessBench and 47.4% accuracy in RL, surpassing reference-guided training and ground-truth RLVR

## Executive Summary
This work introduces SPARK, a three-stage framework for training process reward models (PRMs) without requiring ground truth references or human annotations. The approach uses inference-time scaling methods—self-consistency for parallel verification aggregation and meta-critique for sequential refinement—to generate synthetic step-level verification data. This data trains PRMs that can then be used as rewards in reinforcement learning, with format constraints to prevent reward hacking. The method demonstrates strong performance on mathematical reasoning tasks, outperforming both reference-guided training approaches and ground-truth RLVR baselines.

## Method Summary
SPARK operates in three stages to enable reference-free PRM training. In stage one, a generator produces diverse solutions while a verifier evaluates them without ground truth references, using self-consistency and meta-critique for verification. Stage two trains generative PRMs via supervised fine-tuning using the synthetic verification data. Stage three applies these PRMs as rewards in reinforcement learning, incorporating format constraints to prevent reward hacking. The framework is validated on ProcessBench and math reasoning benchmarks, showing that SPARK-trained PRMs can achieve or exceed the performance of methods requiring ground truth references.

## Key Results
- SPARK-trained PRMs achieve 67.5 F1 on ProcessBench, surpassing reference-guided training (66.4 F1) and GPT-4o (61.9 F1)
- In RL experiments with Qwen2.5-Math-7B, SPARK-trained PRM-CoT achieves 47.4% average accuracy across six math benchmarks
- SPARK outperforms ground-truth RLVR (43.9%) in RL performance, demonstrating the effectiveness of reference-free training

## Why This Works (Mechanism)
SPARK leverages inference-time scaling techniques to generate high-quality synthetic verification data without ground truth. Self-consistency aggregates multiple verification outputs in parallel, while meta-critique performs sequential refinement of the verification process. This synthetic data enables training of PRMs that can effectively guide reinforcement learning, even without access to reference solutions. The format constraints in RL prevent the agent from exploiting the reward signal through format manipulation rather than genuine solution quality.

## Foundational Learning
- **Process reward models (PRMs)**: Learn to score intermediate reasoning steps rather than just final answers; needed because step-level feedback improves reasoning quality and enables more granular reward signals
- **Inference-time scaling**: Techniques like self-consistency and meta-critique that improve model outputs during inference; needed to generate high-quality synthetic verification data without ground truth
- **Reinforcement learning from PRMs**: Using PRMs as reward signals in RL instead of traditional reward functions; needed to train models that generate better intermediate reasoning steps
- **Self-consistency**: Aggregating multiple model outputs to improve reliability; quick check: verify that consensus outputs are more accurate than individual predictions
- **Meta-critique**: Sequential refinement of model outputs through iterative critique; quick check: measure improvement in output quality after each critique iteration
- **Format constraints in RL**: Preventing reward hacking by enforcing solution format requirements; quick check: test whether agents still achieve high rewards when formats are modified

## Architecture Onboarding

**Component map**: Generator -> Verifier -> PRM Trainer -> RL Agent -> Format Constraint Checker

**Critical path**: The verification stage is critical—it generates the synthetic data that trains the PRMs, which then serve as rewards in RL. Any degradation in verification quality directly impacts PRM training and downstream RL performance.

**Design tradeoffs**: The framework trades computational efficiency for reference-free training capability. Inference-time scaling methods (self-consistency and meta-critique) are computationally expensive but enable high-quality synthetic data generation. Format constraints add training complexity but prevent reward hacking.

**Failure signatures**: Poor verification quality leads to noisy PRM training data, resulting in ineffective reward signals. Inadequate format constraints may allow reward hacking where agents optimize for reward structure rather than solution quality. Computational overhead from inference-time scaling may limit practical deployment.

**3 first experiments**:
1. Ablation study comparing self-consistency vs meta-critique vs combined approaches for verification quality
2. Sensitivity analysis of format constraints on preventing reward hacking across different problem types
3. Computational efficiency analysis measuring wall-clock time and GPU memory usage across different inference-time scaling configurations

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Generalizability beyond mathematical reasoning to domains requiring perceptual understanding or different reward structures remains unproven
- Heavy reliance on computationally expensive inference-time scaling techniques without thorough efficiency analysis
- Limited experimental validation focused primarily on Qwen2.5-Math-7B, raising questions about broader applicability

## Confidence
- **High confidence**: SPARK enables reference-free RL training with PRMs, with clear improvements over reference-guided methods
- **Medium confidence**: SPARK-trained PRMs surpass ground-truth RLVR, but this comparison is limited to a single model family
- **Medium confidence**: Inference-time scaling methods provide complementary benefits, though systematic ablation studies are lacking

## Next Checks
1. Test SPARK's effectiveness across diverse domains beyond mathematical reasoning, such as code generation, scientific reasoning, or multi-modal tasks
2. Conduct systematic ablation studies on different verification strategies (self-consistency, meta-critique, and alternatives) to quantify individual contributions
3. Evaluate scalability and efficiency trade-offs by measuring wall-clock time, GPU memory usage, and inference costs across different inference-time scaling methods