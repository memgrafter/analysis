---
ver: rpa2
title: Combining Planning and Reinforcement Learning for Solving Relational Multiagent
  Domains
arxiv_id: '2502.19297'
source_url: https://arxiv.org/abs/2502.19297
tags:
- agents
- learning
- relational
- task
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MaRePReL, the first multiagent reinforcement
  learning framework that combines relational hierarchical planning with deep RL to
  solve complex multiagent domains with varying numbers of objects and relations.
  The method uses a relational hierarchical planner as a centralized controller to
  decompose tasks and allocate them to agents, D-FOCI statements for task-specific
  state abstractions, and multiple deep RL agents for learning generalizable policies.
---

# Combining Planning and Reinforcement Learning for Solving Relational Multiagent Domains

## Quick Facts
- **arXiv ID**: 2502.19297
- **Source URL**: https://arxiv.org/abs/2502.19297
- **Reference count**: 40
- **Primary result**: First multiagent RL framework combining relational hierarchical planning with deep RL, demonstrating superior sample efficiency and generalization across three relational domains.

## Executive Summary
This paper introduces MaRePReL, a novel multiagent reinforcement learning framework that integrates relational hierarchical planning with deep RL to solve complex multiagent coordination problems. The method uses a centralized SHOP planner to decompose high-level goals into subtasks and distribute them to appropriate agents, while D-FOCI statements provide task-specific state abstractions to reduce learning complexity. Multiple deep RL agents learn operator-specific policies that can transfer across agents and generalize to scenarios with varying numbers of objects. The framework demonstrates significant improvements in sample efficiency, transfer ability, and generalization compared to standard MARL baselines across three relational domains.

## Method Summary
MaRePReL combines a relational hierarchical planner (SHOP) with deep RL agents to solve multiagent coordination tasks. The planner decomposes goals into grounded operators and assigns them to agents using a task distributor that respects causal dependencies. D-FOCI statements provide domain-specific abstractions by masking irrelevant state variables for each operator. Operator-specific policies are learned via DQN and can transfer across agents due to relational representations. The framework operates in a centralized-training-decentralized-execution paradigm, with the planner serving as a centralized controller while RL agents learn decentralized policies for their assigned subtasks.

## Key Results
- Achieves near-perfect success rates in multiagent taxi transportation, office tasks, and dungeon escape domains where baselines struggle
- Demonstrates superior sample efficiency, requiring less than half a million steps to achieve Task 3's success rate compared to 3 million steps for non-transferred policies
- Successfully transfers policies between tasks and generalizes to scenarios with more objects than seen during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Centralized relational planning decomposes complex multiagent tasks and assigns subtasks to agents, reducing non-stationarity.
- **Mechanism:** A hierarchical planner produces grounded operators from high-level goals. A task distributor examines causal links between operators and assigns causally-linked operations to the same agent, ensuring dependencies are respected without requiring agents to coordinate at execution time.
- **Core assumption:** Tasks can be decomposed into partially-ordered subtasks with identifiable causal dependencies; the planner has access to full state observability.
- **Evidence anchors:** [abstract]: "uses a relational hierarchical planner as a centralized controller to decompose tasks and assign them to appropriate agents"; [Section 3.2]: "Our task distributor ensures that the same agent performs the causally linked operations."

### Mechanism 2
- **Claim:** Task-specific state abstraction via D-FOCI statements reduces effective state space, enabling sample-efficient learning.
- **Mechanism:** Domain experts specify first-order rules (D-FOCI) that identify which state literals influence rewards/transitions for each operator. During execution, irrelevant literals are masked, constructing smaller ground MDPs. Deep RL agents learn operator-specific policies on these abstracted states.
- **Core assumption:** D-FOCI rules correctly capture all relevant influences; abstraction preserves optimal value functions.
- **Evidence anchors:** [abstract]: "combined with abstraction reasoning to construct task-specific state representations"; [Section 3.3]: "If the sub-plan for agent t1 contains the grounded operator pickup(p1, t1)... the locations and in-taxi conditions of other passengers and taxis in the domain can be masked."

### Mechanism 3
- **Claim:** Operator-specific policies learned via RL transfer across agents and generalize to varying numbers of objects.
- **Mechanism:** Policies are parameterized by operator type (e.g., pickup, drop, attackEnemy), not by specific agent or object identity. Once an operator policy is learned, any agent can reuse it. Relational representations allow grounding the same policy with different object bindings.
- **Core assumption:** Agents are homogeneous; operator definitions remain valid across task variants.
- **Evidence anchors:** [abstract]: "demonstrating superior sample efficiency, transfer ability, and generalization"; [Section 4.3, Transfer]: "MaRePReL significantly improves sample efficiency, achieving Task 3's success rate in less than half a million steps."

## Foundational Learning

- **Concept: Hierarchical Task Networks (HTN) and SHOP planner**
  - Why needed here: The framework relies on HTN planning to decompose goals into ordered grounded operators. Understanding how methods, operators, and ordering constraints work is essential for defining domains.
  - Quick check question: Can you explain how a SHOP planner uses methods to recursively decompose a task until reaching primitive operators?

- **Concept: First-Order Logic and Variable Grounding**
  - Why needed here: D-FOCI statements and relational state representations use first-order logic with variables that must be grounded to specific objects during execution.
  - Quick check question: Given a D-FOCI statement `pickup(P, T): {taxi(T, L1), at(P, L)} → in_taxi(P, T)`, what is the grounded form for passenger p2 and taxi t3?

- **Concept: Markov Games / Dec-POMDPs**
  - Why needed here: The paper formalizes problems as Goal-directed Relational Markov Games. Understanding joint action spaces, transition functions, and centralized-training-decentralized-execution is critical.
  - Quick check question: How does a centralized planner mitigate non-stationarity in a multiagent Markov game?

## Architecture Onboarding

- **Component map:**
  SHOP Planner + Task Distributor -> Abstraction Reasoner -> Deep RL Agent Pool -> Environment Execution Loop

- **Critical path:**
  1. Define domain: predicates Q, operators O, methods M, D-FOCI statements F
  2. Verify planner produces correct decompositions on test instances
  3. Verify D-FOCI masks yield correct abstract states (manually inspect for small instances)
  4. Train operator policies incrementally (start with simpler operators/domains)

- **Design tradeoffs:**
  - Hand-crafted D-FOCI vs. learned abstractions: Current approach requires domain expertise; automating this is future work
  - Greedy task distribution vs. optimal allocation: Greedy is efficient but may not minimize makespan; heterogeneity requires cost functions
  - Fully observable vs. partial observability: Current formalism assumes full observability; extension to RPOMDPs would require lifted probabilistic inference

- **Failure signatures:**
  - Plan frequently invalidated (PlanValid=False): Preconditions violated due to stochasticity or coordination failures → consider adding wait operators or more robust causal links
  - RL policies fail to converge: Check D-FOCI completeness; relevant state variables may be masked out
  - No transfer benefit: New tasks may require novel operators; verify operator overlap

- **First 3 experiments:**
  1. Single-agent Taxi, 1 passenger: Validate planner decomposition and D-FOCI abstractions produce learnable MDPs. Compare DQN with/without abstraction.
  2. Multiagent Taxi, 2 passengers, 2 agents: Test task distributor correctly assigns causally-linked operators. Compare MaRePReL vs. DQN-PS and DQN-IL on sample efficiency.
  3. Transfer experiment—Task 1 → Task 3: Train on 2 passengers, transfer policy to 4 passengers. Measure steps to convergence vs. training from scratch to confirm generalization.

## Open Questions the Paper Calls Out

- Can the abstraction reasoner be automated to eliminate the need for hand-crafted D-FOCI statements? The authors suggest leveraging lifted inference or LLM-based approaches.
- How does MaRePReL scale as the number of agents and operators grows exponentially? The authors note this as an important future direction due to exponential growth in search space.
- Can the framework be extended to handle tightly coupled multiagent coordination requiring synchronous actions? The current approach handles loosely coupled cooperation.
- Can MaRePReL be adapted to partially observable relational multiagent domains? The current formalism requires full observability and would need integration with lifted probabilistic inference.

## Limitations

- The framework requires hand-crafted D-FOCI statements and HTN domain definitions, creating a knowledge engineering bottleneck
- Greedy task distribution may struggle with tasks requiring tight coordination between agents
- The centralized planner assumes full observability, which may not hold in many real-world applications

## Confidence

- **High confidence**: Sample efficiency improvements on training tasks
- **Medium confidence**: Transfer ability claims
- **Medium confidence**: Generalization to larger scenarios

## Next Checks

1. **D-FOCI completeness validation**: Systematically test whether masking irrelevant state variables affects policy performance, particularly in edge cases where seemingly irrelevant variables might have indirect effects.

2. **Planner robustness test**: Evaluate MaRePReL performance under partial observability conditions by introducing sensor noise or limited field-of-view to assess the centralized planning assumption.

3. **Transfer generalization stress test**: Train on scenarios with n objects and evaluate performance on scenarios with 2n, 3n, and 4n objects to quantify the generalization limits and identify failure patterns.