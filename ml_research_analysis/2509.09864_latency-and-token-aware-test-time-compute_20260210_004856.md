---
ver: rpa2
title: Latency and Token-Aware Test-Time Compute
arxiv_id: '2509.09864'
source_url: https://arxiv.org/abs/2509.09864
tags:
- strategy
- latency
- accuracy
- compute
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of selecting and allocating inference-time
  compute for large language models in a query-adaptive way. It formulates this as
  a utility optimization problem where the system selects a decoding strategy and
  computes the number of candidates or beam parameters on a per-query basis, balancing
  accuracy against both token usage and wall-clock latency.
---

# Latency and Token-Aware Test-Time Compute

## Quick Facts
- arXiv ID: 2509.09864
- Source URL: https://arxiv.org/abs/2509.09864
- Reference count: 8
- Adaptive inference-time compute allocation improves accuracy-cost trade-offs over static methods

## Executive Summary
This paper addresses the challenge of efficiently allocating inference-time compute for large language models in a query-adaptive manner. The authors formulate this as a utility optimization problem that selects decoding strategies and computes beam parameters per query, balancing accuracy against token usage and wall-clock latency. A lightweight accuracy probe estimates success probabilities while precomputed token and latency costs from training data inform the optimization. Experiments on the NuminaMath-CoT benchmark demonstrate that this adaptive approach consistently outperforms static inference-time scaling methods, achieving better accuracy-cost trade-offs particularly valuable for latency-sensitive agentic workflows.

## Method Summary
The method introduces a utility optimization framework that jointly selects decoding strategies (like beam search or sampling) and determines the number of candidates or beam parameters for each query. The system uses a lightweight accuracy probe to estimate the probability of success for different decoding configurations on a per-query basis. Average token and latency costs are precomputed from training data to inform the optimization. The framework then solves an optimization problem that maximizes expected utility while accounting for both accuracy and efficiency constraints, enabling dynamic allocation of compute resources based on query-specific characteristics.

## Key Results
- Adaptive strategy outperforms static inference-time scaling methods on NuminaMath-CoT benchmark
- Better accuracy-cost trade-offs achieved through joint method selection and compute allocation
- Particularly effective for latency-sensitive agentic workflows where efficiency is critical

## Why This Works (Mechanism)
The approach works by treating inference-time compute allocation as a utility optimization problem rather than a fixed-parameter setting. By estimating per-query success probabilities through a lightweight probe, the system can dynamically adjust the decoding strategy and computational resources based on the specific requirements of each query. This query-adaptive approach allows for more efficient use of compute resources by allocating more resources to queries that are likely to benefit from additional compute while conserving resources on queries where additional compute would provide diminishing returns.

## Foundational Learning

**Utility Optimization** - Mathematical framework for making decisions under constraints by maximizing expected benefit while minimizing costs. Why needed: Provides the theoretical foundation for balancing accuracy gains against computational costs. Quick check: Verify that the utility function properly captures the trade-offs between accuracy, latency, and token usage.

**Accuracy Probe Models** - Lightweight models that predict the likelihood of success for a given decoding configuration without performing full inference. Why needed: Enables rapid estimation of which configurations will work best for each query. Quick check: Ensure probe predictions correlate well with actual model performance across different query types.

**Beam Search and Sampling Decoding** - Two fundamental decoding strategies with different trade-offs between exploration and exploitation. Why needed: Different query types may benefit from different decoding strategies. Quick check: Validate that the optimization correctly identifies when to use each strategy type.

## Architecture Onboarding

**Component Map:** Accuracy Probe -> Utility Optimizer -> Decoding Strategy Selector -> Base LLM

**Critical Path:** Query -> Accuracy Probe -> Utility Optimizer -> Selected Decoding Configuration -> Base LLM -> Output

**Design Tradeoffs:** The method trades computational overhead from the accuracy probe against improved overall efficiency. Static approaches avoid probe overhead but cannot adapt to query-specific needs. The optimization balances between under-provisioning (missing accuracy gains) and over-provisioning (wasting compute).

**Failure Signatures:** Poor probe accuracy leads to suboptimal strategy selection. Precomputed cost estimates becoming stale due to model changes or domain shifts degrades optimization quality. Latency constraints may force suboptimal accuracy choices when probe predictions are uncertain.

**3 First Experiments:** 1) Benchmark probe accuracy across different query types and model families. 2) Test sensitivity to changes in precomputed token/latency distributions. 3) Measure wall-clock overhead introduced by the probe to quantify net efficiency gains.

## Open Questions the Paper Calls Out
The paper acknowledges that the proposed method shows clear improvements on a single mathematical reasoning benchmark but generalization to other domains and task types remains unproven. The computational overhead of the accuracy probe is not explicitly quantified, leaving uncertainty about practical deployment costs. Additionally, the method assumes availability of precomputed token and latency distributions, which may not hold in dynamic or constrained environments.

## Limitations
- Results limited to single mathematical reasoning benchmark, limiting generalizability claims
- Accuracy probe introduces potential systematic biases and unquantified computational overhead
- Dependence on precomputed token and latency distributions may not hold in dynamic environments

## Confidence

**Method Design and Formulation** - Medium: Well-grounded utility optimization framework, but effectiveness heavily dependent on probe accuracy and cost estimate stability.

**Empirical Results** - Medium: Compelling results on NuminaMath-CoT, but limited to one benchmark requiring validation across diverse tasks and model families.

**Practical Deployment** - Low: Real-world applicability uncertain due to unaddressed computational overhead, probe accuracy variability, and dependence on training data distributions.

## Next Checks
1. Evaluate the adaptive strategy on multiple benchmarks spanning different domains (e.g., code generation, commonsense reasoning) to test generalizability.
2. Measure the wall-clock overhead introduced by the accuracy probe to quantify net efficiency gains.
3. Test robustness when token/latency distributions shift (e.g., model updates, domain shifts) to assess reliance on precomputed statistics.