---
ver: rpa2
title: Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting
arxiv_id: '2508.13749'
source_url: https://arxiv.org/abs/2508.13749
tags:
- regret
- bound
- where
- variance
- thompson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of sequential decision-making for
  Sharpe ratio (SR) maximization in a stochastic multi-armed bandit setting. The authors
  focus on Thompson Sampling (TS), a Bayesian approach, under the assumption of Gaussian
  rewards with unknown parameters.
---

# Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting

## Quick Facts
- arXiv ID: 2508.13749
- Source URL: https://arxiv.org/abs/2508.13749
- Reference count: 40
- Primary result: Thompson Sampling achieves order-optimal logarithmic regret for Sharpe ratio maximization in Gaussian bandits

## Executive Summary
This paper studies Sharpe ratio maximization in stochastic multi-armed bandits under Gaussian rewards with unknown parameters. The authors propose SRTS, a Thompson Sampling algorithm that samples from posterior distributions over mean and variance to select arms based on risk-adjusted performance. They introduce a novel regret decomposition that separates regret into terms driven by mean and variance estimation errors. The paper establishes upper and matching lower bounds on regret, proving order-optimality with logarithmic scaling. Extensive simulations demonstrate that SRTS significantly outperforms existing algorithms, particularly in settings where risk-return tradeoffs vary across arms.

## Method Summary
The SRTS algorithm uses Normal-Gamma conjugate priors for Gaussian rewards with unknown mean and variance. Each arm maintains posterior parameters for precision (Gamma distribution) and mean (Normal distribution). The algorithm initializes by playing each arm once, then samples from these posteriors to estimate Sharpe ratios for arm selection. A regularization constant L₀ stabilizes the Sharpe ratio denominator when variance estimates are small. Posterior parameters are updated after each pull using standard conjugate updating rules. The Sharpe ratio is defined as μ/(L₀ + ρσ²) where ρ controls risk aversion, and regret is measured as the difference between the optimal arm's Sharpe ratio and the algorithm's achieved ratio over time.

## Key Results
- SRTS achieves logarithmic regret O(log n) for Sharpe ratio maximization
- Upper and lower bounds match, proving order-optimality
- Algorithm outperforms UCB-based baselines in empirical simulations
- Regret decomposition reveals distinct roles of mean and variance estimation

## Why This Works (Mechanism)
The Thompson Sampling framework naturally balances exploration and exploitation by sampling from posterior distributions. For Sharpe ratio optimization, the algorithm must learn both mean rewards and their variances simultaneously. The Normal-Gamma conjugate prior enables tractable posterior sampling for both parameters. By sampling from these posteriors, SRTS explores arms that might have high Sharpe ratios due to either high means or low variances. The regret decomposition shows that logarithmic regret arises because the algorithm efficiently identifies the optimal arm through accurate estimation of both parameters, with the regularization constant preventing numerical instability when variance estimates approach zero.

## Foundational Learning
- **Thompson Sampling (Posterior Sampling)**
  - Why needed here: SRTS is a Bayesian algorithm that samples from the posterior over mean and variance to select arms based on sampled Sharpe ratio.
  - Quick check question: Can you explain how posterior sampling differs from UCB in terms of exploration mechanism?

- **Regret Decomposition**
  - Why needed here: The paper's core theoretical contribution is a novel decomposition that separates regret into terms driven by mean and variance estimation errors.
  - Quick check question: How does the SR decomposition (Eq. 19) differ from classical cumulative reward decomposition?

- **Sharpe Ratio as Risk-Adjusted Metric**
  - Why needed here: The objective is not cumulative reward but a ratio involving both mean and variance, requiring joint parameter learning.
  - Quick check question: What role does the regularization constant L₀ play in stabilizing Sharpe ratio estimates?

## Architecture Onboarding
- Component map: Posterior sampler (Normal-Gamma) -> Sharpe ratio estimator -> Arm selector (argmax) -> Regret tracker
- Critical path: Initialize priors → Play each arm once → For each subsequent round: sample θ, τ → compute sampled SR → select arm → update posterior → track regret
- Design tradeoffs: L₀ too small → unstable SR estimates; L₀ too large → variance term dominates, reducing risk sensitivity. Posterior update cost is O(1) per pull; algorithm is simple but analysis is complex due to ratio distribution.
- Failure signatures: Regret growing super-logarithmically indicates non-Gaussian tails or L₀ mis-specification; high variance in pull counts suggests exploration failure.
- First 3 experiments:
  1. Synthetic Gaussian bandits with K=10, varying ρ (0, 1, large) to validate regret scaling vs. time horizon (Figure 1).
  2. Ablation on L₀ to test denominator stability under low-variance arms.
  3. Comparison vs. UCB-SR baseline across multiple risk-reward profiles to confirm empirical superiority.

## Open Questions the Paper Calls Out
**Open Question 1**
- **Question:** Can the order-optimal regret guarantees for SRTS be extended to heavy-tailed or non-Gaussian reward distributions where variance might not exist?
- **Basis in paper:** [Explicit] The authors state the theoretical analysis "relies heavily on the assumption of Gaussian rewards" (Abstract) and uses specific conjugate priors (Section IV) and tail bounds (Lemmas 5-8) that may not hold generally.
- **Why unresolved:** The concentration inequalities and posterior sampling steps rely on the Normal-Gamma conjugate structure, which is specific to the Gaussian likelihood.
- **What evidence would resolve it:** Deriving finite-time regret bounds for SRTS under sub-Gaussian or heavy-tailed assumptions, or proposing a modified algorithm for these distributions.

**Open Question 2**
- **Question:** Is the regularization parameter $L_0$ strictly necessary for logarithmic regret, or can the analysis be adapted to the standard Sharpe ratio definition ($\mu/\sigma$)?
- **Basis in paper:** [Explicit] The paper defines the Sharpe ratio as $\xi_i = \mu_i / (L_0 + \rho\sigma^2_i)$, noting $L_0$ is a "necessary constant to stabilize the value of estimated SR" (Section II).
- **Why unresolved:** The proofs utilize the property that the denominator $D \geq L_0$ to establish Lipschitz continuity (Section III, Step 1), which fails if $L_0 = 0$.
- **What evidence would resolve it:** A theoretical analysis deriving regret bounds that hold as $L_0 \to 0$, or a proof showing that regret diverges without regularization.

**Open Question 3**
- **Question:** Can the proposed regret decomposition and SRTS algorithm be generalized to the contextual bandit setting where risk-return profiles depend on context vectors?
- **Basis in paper:** [Inferred] The paper focuses on stochastic MABs (Section II), but explicitly motivates the problem using "portfolio selection or algorithmic trading" (Introduction), which often involves contextual information.
- **Why unresolved:** The current regret decomposition relies on arm-specific pull counts ($s_{i,n}$) and stationary distributions, ignoring context-dependent parameter estimation.
- **What evidence would resolve it:** Extending the theoretical framework to linear or neural contextual models and analyzing the resulting regret bounds.

## Limitations
- The analysis critically depends on Gaussian reward assumptions, limiting applicability to heavy-tailed or discrete distributions
- The regularization constant L₀ is necessary for theoretical guarantees but introduces a hyperparameter that requires tuning
- Empirical validation is limited to synthetic Gaussian bandits with specific parameter settings

## Confidence
- Order-optimality claim: **High** - The matching upper and lower bounds are rigorously proven under stated assumptions.
- Practical superiority claim: **Medium** - Empirical results show SRTS outperforming baselines, but comparisons are limited to specific synthetic settings.
- Generalization to non-Gaussian rewards: **Low** - The analysis critically depends on Gaussianity for tractable posterior sampling and regret decomposition.

## Next Checks
1. Test SRTS on non-Gaussian reward distributions (e.g., Bernoulli, heavy-tailed) to assess robustness of the logarithmic regret property.
2. Perform ablation studies varying L₀ across its admissible range to quantify its impact on regret scaling and convergence speed.
3. Extend empirical validation to larger arm sets (K>10) and non-stationary environments to evaluate scalability and adaptability.