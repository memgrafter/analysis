---
ver: rpa2
title: One Sample is Enough to Make Conformal Prediction Robust
arxiv_id: '2506.16553'
source_url: https://arxiv.org/abs/2506.16553
tags:
- coverage
- rcp1
- robust
- score
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational inefficiency of randomized
  smoothing-based robust conformal prediction (RCP), which requires many model forward
  passes per input to estimate smooth score statistics. The key insight is that even
  a single noisy forward pass already yields robustness, so certifying the entire
  CP procedure rather than individual scores suffices.
---

# One Sample is Enough to Make Conformal Prediction Robust

## Quick Facts
- arXiv ID: 2506.16553
- Source URL: https://arxiv.org/abs/2506.16553
- Authors: Soroush H. Zargarbashi; Mohammad Sadegh Akhondzadeh; Aleksandar Bojchevski
- Reference count: 40
- Primary result: Single noisy forward pass achieves robust conformal prediction comparable to methods using ~100 samples

## Executive Summary
This work addresses the computational inefficiency of randomized smoothing-based robust conformal prediction (RCP), which requires many model forward passes per input to estimate smooth score statistics. The key insight is that even a single noisy forward pass already yields robustness, so certifying the entire CP procedure rather than individual scores suffices. This leads to RCP1, which uses one augmented sample for both calibration and prediction, yet achieves coverage guarantees and set sizes comparable to state-of-the-art smoothing-based RCP methods that use ~100 samples. RCP1 works for any smoothing scheme, perturbation ball, and task (classification or regression), and extends to robust conformal risk control. Experiments on CIFAR-10 and ImageNet show that RCP1 returns sets with similar efficiency to sampling-heavy baselines while being significantly faster and more memory-efficient, enabling use of large models like vision transformers.

## Method Summary
The authors propose RCP1, a novel approach to robust conformal prediction that eliminates the need for multiple noisy forward passes. Instead of computing smooth score statistics through repeated sampling, RCP1 certifies the entire conformal prediction procedure as robust. This is achieved by using a single augmented sample for both calibration and prediction, leveraging the fact that one noisy forward pass already provides sufficient robustness guarantees. The method maintains theoretical coverage guarantees while dramatically reducing computational overhead, making it applicable to any smoothing scheme, perturbation ball, and both classification and regression tasks.

## Key Results
- RCP1 achieves comparable coverage guarantees and set sizes to state-of-the-art smoothing-based RCP methods using ~100 samples
- Single-sample approach maintains theoretical robustness guarantees for the entire CP procedure
- Significant computational speedup and memory efficiency enable application to large models like vision transformers
- Experimental validation on CIFAR-10 and ImageNet demonstrates practical effectiveness

## Why This Works (Mechanism)
The core insight is that certifying the entire conformal prediction procedure for robustness is sufficient, rather than certifying individual score estimates. Randomized smoothing provides robustness guarantees for the smoothed function, and since conformal prediction operates on these smoothed scores, a single sample is enough to establish the necessary robustness. The method leverages the fact that the distribution of scores under noise is what matters for coverage, not precise point estimates. By shifting the certification from individual samples to the entire procedure, RCP1 eliminates redundant computations while maintaining theoretical guarantees.

## Foundational Learning

**Conformal Prediction**: Distribution-free uncertainty quantification method that produces prediction sets with guaranteed coverage. Needed because it provides the base framework for robust predictions. Quick check: Can verify coverage guarantees hold under exchangeability assumptions.

**Randomized Smoothing**: Technique that adds noise to inputs to create smooth classifier scores, enabling robustness certification. Needed because it provides the theoretical foundation for robustness guarantees. Quick check: Can verify that smoothed classifier maintains desired Lipschitz properties.

**Prediction Sets**: Collections of labels that contain the true label with high probability. Needed because they represent the output format of conformal prediction. Quick check: Can verify that sets are properly calibrated to achieve target coverage.

## Architecture Onboarding

Component Map: Input -> Noise Perturbation -> Single Forward Pass -> Score Computation -> Conformal Prediction Procedure -> Prediction Set

Critical Path: The entire pipeline from input to prediction set is critical, with the single forward pass being the key efficiency point. All components must work together to maintain theoretical guarantees.

Design Tradeoffs: Computational efficiency vs. potential degradation in extreme cases; theoretical guarantees vs. practical performance across diverse datasets; simplicity vs. flexibility across different smoothing schemes and perturbation types.

Failure Signatures: Coverage violations under structured or adversarial perturbations; degradation in set efficiency for heavy-tailed noise distributions; computational bottlenecks when scaling to very large models with complex architectures.

First Experiments:
1. Verify coverage guarantees on CIFAR-10 using standard Gaussian perturbations
2. Compare set sizes and computational runtime against sampling-heavy baselines
3. Test scalability on vision transformer models with varying input resolutions

## Open Questions the Paper Calls Out

None specified in the provided information.

## Limitations

- Performance under structured or adversarial perturbations remains unclear
- Behavior under heavy-tailed perturbations or structured noise not fully characterized
- Computational gains not fully quantified against potential degradation in extreme cases
- Limited testing scope regarding model architectures beyond vision transformers

## Confidence

High confidence in the theoretical contribution of certifying the entire CP procedure rather than individual scores
Medium confidence in experimental results showing comparable performance to sampling-heavy baselines
Low confidence in scalability claims to arbitrary large models without performance penalties

## Next Checks

1. Test RCP1 on datasets with structured or adversarial perturbations to assess robustness limits
2. Evaluate computational efficiency gains across a broader range of model architectures beyond vision transformers
3. Investigate performance degradation under heavy-tailed noise distributions not covered by standard smoothing assumptions