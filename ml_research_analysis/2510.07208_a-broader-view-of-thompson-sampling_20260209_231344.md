---
ver: rpa2
title: A Broader View of Thompson Sampling
arxiv_id: '2510.07208'
source_url: https://arxiv.org/abs/2510.07208
tags:
- thompson
- sampling
- policy
- regret
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a principled optimization framework for understanding
  Thompson Sampling (TS) in multi-armed bandits. The key contribution is showing that
  TS can be viewed as an online optimization algorithm that minimizes instantaneous
  squared regret regularized by a biserial covariance term measuring residual uncertainty.
---

# A Broader View of Thompson Sampling

## Quick Facts
- arXiv ID: 2510.07208
- Source URL: https://arxiv.org/abs/2510.07208
- Reference count: 4
- One-line primary result: TS can be viewed as an online optimization algorithm that minimizes instantaneous squared regret regularized by a biserial covariance term measuring residual uncertainty.

## Executive Summary
This paper provides a principled optimization framework for understanding Thompson Sampling (TS) in multi-armed bandits. The key contribution is showing that TS can be viewed as an online optimization algorithm that minimizes instantaneous squared regret regularized by a biserial covariance term measuring residual uncertainty. Specifically, TS balances greediness against exploration via a regularizer that quantifies the correlation between the reward gap and the identity of the optimal arm. The authors introduce "faithful" stationarization of the long-term regret minimization problem through expected cumulative squared regret, leading to a stationary Bellman equation.

## Method Summary
The paper reformulates Thompson Sampling as an online optimization algorithm by introducing "faithful stationarization" of the non-stationary finite-horizon bandit problem. This is achieved by minimizing expected cumulative squared regret ($R^2$) rather than using infinite-horizon discounting. The authors derive a stationary Bellman equation from this objective and show that TS solves a specific optimization problem: minimize squared regret plus a linear regularization term where the regularizer is the biserial covariance between the reward gap and the optimal arm indicator. The method is tested on both Gaussian and Bernoulli two-armed bandits, comparing TS against an $R^2$-optimal policy derived from dynamic programming.

## Key Results
- TS is shown to minimize squared regret regularized by a biserial covariance term measuring residual uncertainty
- The paper derives a stationary Bellman equation through faithful stationarization of cumulative squared regret
- TS's regularizer is sometimes too conservative compared to the optimal $R^2$-policy, leading to suboptimal exploration

## Why This Works (Mechanism)

### Mechanism 1: Faithful Stationarization via Squared Regret
The paper claims that minimizing expected cumulative squared regret ($R^2$) creates a "faithful" stationary objective for the non-stationary finite-horizon bandit problem, unlike infinite-horizon discounting (Gittins). By reformulating the objective as minimizing $\sum r^2(q_t; \pi_t)$, the authors derive a stationary Bellman equation. This allows dynamic programming to be applied without artificially altering the problem structure (e.g., via discount factors that may cause linear regret). The squared regret bounds the standard regret via Cauchy-Schwarz: $R_T \le \sqrt{R^2 \cdot T}$.

### Mechanism 2: The Biserial Covariance Regularizer
Thompson Sampling (TS) acts as an online optimization algorithm where exploration is driven by a specific covariance term acting as a regularizer. TS is shown to solve $\min_x [\bar{r}^2(x; \pi) + \tilde{\nu}(\pi)x]$. The term $\tilde{\nu}(\pi) = \text{Cov}(\theta_1 - \theta_2, \text{sign}(\theta_1 - \theta_2))$ measures the correlation between the "reward gap" (continuous) and the "identity of the optimal arm" (binary). This "biserial covariance" quantifies residual uncertainty in units of reward.

### Mechanism 3: The Lagrangian Balance ($\lambda = 1$)
The standard Thompson Sampling algorithm implicitly uses a Lagrange multiplier of exactly 1 to balance the squared regret loss and the covariance regularizer. The objective function is $\bar{r}^2 + \lambda \tilde{\nu} x$. The paper proves that if $\lambda \neq 1$, the policy suffers "incomplete learning" (it may commit to a suboptimal arm forever). Thus, TS is the unique policy in this family that guarantees learning.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) in Bandits**
  - **Why needed here:** The paper formulates the bandit problem as an MDP where the "state" is the current posterior belief $\pi_t$. Understanding that the state evolves via Bayes' rule is prerequisite to grasping the Bellman equation derivation.
  - **Quick check question:** If I pull an arm and observe a reward, does the environment state $\theta$ change, or does my belief state $\pi_t$ change?

- **Concept: Point-Biserial Correlation**
  - **Why needed here:** The core theoretical contribution identifies the TS regularizer as a covariance between a continuous variable (reward gap) and a binary one (optimal arm index). Understanding this statistical concept explains *how* TS measures uncertainty.
  - **Quick check question:** If the reward gap is large but we are very sure which arm is best, would the biserial covariance be high or low?

- **Concept: Sub-optimality of Discounting (Gittins vs. Finite Horizon)**
  - **Why needed here:** The paper motivates its new "faithful stationarization" by contrasting it with the Gittins index. One must understand why discounting (prioritizing immediate rewards) can fail in long-term regret minimization (the "linear regret" issue).
  - **Quick check question:** Why might a policy that maximizes discounted reward fail to identify the best arm in a long experiment?

## Architecture Onboarding

- **Component map:** Posterior belief $\pi_t$ -> Loss Module (squared expected regret) -> Regularizer Module (biserial covariance) -> Optimizer (1D quadratic solve)
- **Critical path:**
  1. Update posterior based on latest observation
  2. Calculate the covariance regularizer $\tilde{\nu}$ (this is the computationally intensive theoretical step)
  3. Solve the quadratic equation derived from the derivative of the objective to find optimal probability $q^*$ (since it reduces to $\min [ (A-x)^2 + \nu x ]$)
- **Design tradeoffs:**
  - TS Heuristic vs. $R^2$-Optimal: TS is computationally cheap (just sampling) but theoretically conservative (regret suboptimal). The $R^2$-optimal policy requires solving a complex Bellman equation (high compute) for slightly better regret
  - The "Fixed" Policy: The paper suggests a "Fixed" regularizer that shuts down exploration when the leading arm is also the most informative. This trades theoretical simplicity for improved empirical regret
- **Failure signatures:**
  - Incomplete Learning: Occurs if the Lagrangian multiplier is tuned away from 1
  - Conservative Exploration: Standard TS may waste pulls on an arm that is known to be worse purely because it has "some" uncertainty, unlike the $R^2$-optimal policy which abandons the runner-up more aggressively
- **First 3 experiments:**
  1. Replicate One-Armed Case: Implement the closed-form $R^2$-optimal policy for a 1-armed bandit (Section 6.1) and plot the "phase change" where exploration stops compared to TS
  2. Regularizer Comparison: Reproduce Figure 5 (Right). Plot the TS regularizer $\tilde{\nu}$ vs. the $R^2$-optimal regularizer $\nu$ to visualize the "conservatism" gap as arm variance changes
  3. Fixed Policy Test: Implement the "simple fix" (shutting down regularization when no exploration/exploitation tension exists) on a Bernoulli bandit to verify the regret reduction shown in Figure 6

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is minimizing expected cumulative squared regret ($R^2$) the theoretically optimal method for "faithful" stationarization, or do other stationarization objectives offer superior properties?
- Basis in paper: Explicit (Remark 1)
- Why unresolved: The paper establishes $R^2$-finiteness implies minimax optimal $O(\sqrt{T})$ regret and aligns with Robbins' principle, but explicitly states that determining if it is the "best" stationarization method is left for future research
- What evidence would resolve it: A theoretical comparison of $R^2$ against other potential stationarization metrics (e.g., other norms of regret) showing strict superiority or equivalence in terms of regret bounds and algorithmic simplicity

### Open Question 2
- Question: Does the proposed "fixed" policy, which disables regularization when one arm dominates in both reward and information gain, provably close the performance gap with the $R^2$-optimal policy in the general K-armed setting?
- Basis in paper: Inferred (Section 6.2)
- Why unresolved: The paper proposes a heuristic "simple fix" ($\nu_{fix}$) for the conservative nature of Thompson Sampling and proves it is $R^2$-finite. However, it does not provide regret bounds or convergence guarantees relative to the optimal policy for this modified algorithm
- What evidence would resolve it: Theoretical derivation of the regret bound for the fixed policy or empirical verification that it consistently matches the $R^2$-optimal policy's performance across diverse K-armed environments

### Open Question 3
- Question: Can the relationship between the Thompson Sampling regularizer ($\tilde{\nu}$) and the optimal regularizer ($\nu$) be characterized for non-conjugate or heavy-tailed reward distributions?
- Basis in paper: Inferred (Section 6.1 & 6.2)
- Why unresolved: The explicit comparison of regularizers (Figure 1 and 5) and the "phase change" phenomenon are derived and demonstrated primarily for Gaussian and Bernoulli bandits. It is unclear if the "conservatism" of TS generalizes or if the optimal regularizer behaves similarly for other distributions
- What evidence would resolve it: Derivation of the optimal regularizer $\nu$ for a non-conjugate prior/reward model (e.g., Gaussian prior with non-Gaussian likelihood) to compare its structure against the biserial covariance $\tilde{\nu}$

## Limitations
- The biserial covariance framework is primarily validated for two-armed bandits, with unclear generalization to K-armed settings
- The "Fixed" policy modification depends on an information gain function from Russo and Van Roy (2014a) that isn't fully specified, making exact reproduction challenging
- The theoretical comparison of regularizers shows conservatism but doesn't quantify the practical significance across diverse problem parameters

## Confidence

- **High Confidence**: The optimization formulation (Theorem 1) showing TS as minimizing squared regret plus covariance regularizer. The proof structure is explicit and verifiable.
- **Medium Confidence**: The uniqueness of λ=1 for complete learning (Proposition 3). While the proof is provided, the practical sensitivity to λ deviations in finite horizons needs empirical validation.
- **Medium Confidence**: The conservative exploration claim (Figure 5). The theoretical comparison is sound, but the practical significance depends on problem parameters and may be domain-specific.

## Next Checks

1. **Regularizer Sensitivity Analysis**: Implement the "Fixed" policy with multiple information gain formulations (e.g., posterior variance, mutual information) to test robustness of the improvement claims.
2. **Beyond Two Arms**: Extend the biserial covariance framework to three-armed bandits and verify if the regularizer maintains its explanatory power for TS behavior.
3. **Finite Horizon Effects**: Run experiments with varying horizons (T=100, 1000, 10000) to quantify how quickly TS's conservatism impacts regret relative to the optimal policy.