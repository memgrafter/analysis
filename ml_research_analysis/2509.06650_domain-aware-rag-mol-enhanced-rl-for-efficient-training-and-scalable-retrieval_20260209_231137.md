---
ver: rpa2
title: 'Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval'
arxiv_id: '2509.06650'
source_url: https://arxiv.org/abs/2509.06650
tags:
- uni00000013
- retrieval
- query
- training
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes MoLER, a domain-aware RAG method that combines
  continual pre-training with reinforcement learning to optimize retrieval performance.
  The core innovation is a two-stage pipeline: first, using Mixture of Losses (MoL)
  to balance domain-specific and general knowledge learning, and second, employing
  Group Relative Policy Optimization (GRPO) to maximize document recall through multi-query
  expansion and passage generation.'
---

# Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval

## Quick Facts
- **arXiv ID**: 2509.06650
- **Source URL**: https://arxiv.org/abs/2509.06650
- **Authors**: Hao Lin; Peitong Xie; Jingxue Chen; Jie Lin; Qingkun Tang; Qianchun Lu
- **Reference count**: 35
- **Primary result**: MoLER achieves state-of-the-art performance with 0.49% improvement in recall metrics over nearest competitor

## Executive Summary
MoLER introduces a domain-aware RAG system that leverages Mixture of Losses (MoL) for continual pre-training and Group Relative Policy Optimization (GRPO) for reinforcement learning to optimize retrieval performance. The method employs a two-stage pipeline that first balances domain-specific and general knowledge learning, then maximizes document recall through multi-query expansion and passage generation. A key innovation is the Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during training while maintaining scalable inference through Multi-query Multi-passage Late Fusion (MMLF).

## Method Summary
MoLER operates through a two-stage pipeline combining continual pre-training with reinforcement learning. The first stage uses Mixture of Losses to balance domain-specific knowledge learning with general knowledge retention during pre-training. The second stage employs GRPO to optimize retrieval performance through multi-query expansion and passage generation. The system introduces MSLF for efficient training and MMLF for scalable inference, achieving significant improvements in recall metrics while enabling smaller models to match the performance of much larger models.

## Key Results
- Achieves state-of-the-art performance on NFCORPUS and SCIFACT datasets
- Improves recall metrics by 0.49% over nearest competitor
- Enables smaller models to achieve performance comparable to much larger models
- Demonstrates effectiveness of MoL and GRPO combination for domain-aware RAG

## Why This Works (Mechanism)
MoLER's effectiveness stems from its dual optimization approach: the MoL pre-training stage ensures the model develops strong domain-specific representations while maintaining general knowledge capabilities, preventing catastrophic forgetting. The GRPO-based RL stage then fine-tunes retrieval strategies through multi-query expansion, allowing the model to explore multiple query formulations and select optimal retrieval paths. The MSLF strategy reduces computational burden during training by processing one passage at a time while still learning to combine information from multiple queries, making the RL optimization tractable without sacrificing the benefits of multi-query inference during deployment.

## Foundational Learning

**Mixture of Losses (MoL)**: Combines domain-specific and general knowledge learning objectives during pre-training. *Why needed*: Prevents catastrophic forgetting while ensuring domain adaptation. *Quick check*: Verify domain-specific loss weight tuning effectiveness across different domains.

**Group Relative Policy Optimization (GRPO)**: Reinforcement learning algorithm that optimizes retrieval policies based on group comparisons. *Why needed*: Enables efficient policy updates without requiring baseline computation. *Quick check*: Compare GRPO performance against standard PPO implementations.

**Multi-query Expansion**: Generates multiple query variations to improve retrieval coverage. *Why needed*: Addresses query ambiguity and improves recall. *Quick check*: Measure retrieval improvement with increasing numbers of query variations.

**Late Fusion Strategies**: Combines retrieval results from multiple queries at the final stage. *Why needed*: Preserves computational efficiency while maintaining retrieval quality. *Quick check*: Evaluate fusion quality across different ranking aggregation methods.

## Architecture Onboarding

**Component Map**: Data -> Pre-training (MoL) -> RL Training (GRPO + MSLF) -> Inference (MMLF) -> Retrieved Documents

**Critical Path**: Query generation → Multi-query expansion → Document retrieval → Late fusion → Response generation

**Design Tradeoffs**: MSLF reduces training computational cost by processing single passages during RL, but requires careful reward design to ensure the model learns effective multi-passage combination strategies. MMLF enables scalable inference but increases latency compared to single-query approaches.

**Failure Signatures**: Poor domain adaptation manifests as low recall on domain-specific queries despite good general performance. Ineffective multi-query expansion appears as redundant retrievals across query variations. Suboptimal late fusion shows as inconsistent ranking quality across different query sets.

**First Experiments**: 1) Ablation study removing MoL to measure domain adaptation impact. 2) Single-query vs multi-query comparison to quantify expansion benefits. 3) MSLF vs MMLF runtime and quality comparison during training.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on availability of domain-specific labeled data for pre-training
- Computational overhead of multi-query expansion may limit scalability for extremely large document collections
- RL optimization relies on reward functions that may not fully capture nuanced retrieval quality metrics
- Evaluation focuses primarily on recall metrics, potentially missing end-user utility considerations

## Confidence
- **High confidence**: Effectiveness of MoL approach for balancing domain-specific and general knowledge learning
- **Medium confidence**: Superiority of GRPO over alternative RL methods for retrieval optimization
- **Medium confidence**: Computational efficiency gains from MSLF during training
- **Medium confidence**: Scalability of MMLF inference strategy

## Next Checks
1. **Cross-domain generalization study**: Evaluate MoLER on domains with varying levels of labeled data availability, particularly focusing on low-resource domains.

2. **Real-world deployment benchmarking**: Conduct comprehensive latency and memory usage measurements under realistic deployment scenarios with large-scale document collections.

3. **Alternative reward function ablation**: Systematically test different reward function formulations in GRPO optimization to assess robustness to reward structure changes.