---
ver: rpa2
title: 'REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder'
arxiv_id: '2503.08665'
source_url: https://arxiv.org/abs/2503.08665
tags:
- video
- latent
- compression
- decoder
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REGEN, a novel approach to video embedding
  that leverages a diffusion transformer (DiT) as a generative decoder rather than
  the traditional VAE-based architecture. The core idea is that effective latent representations
  for generative modeling should enable visually plausible synthesis rather than exact
  reconstruction.
---

# REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder

## Quick Facts
- arXiv ID: 2503.08665
- Source URL: https://arxiv.org/abs/2503.08665
- Reference count: 40
- Achieves superior reconstruction quality compared to state-of-the-art methods at high compression ratios (up to 32× temporal compression)

## Executive Summary
REGEN introduces a novel approach to video embedding that leverages a diffusion transformer (DiT) as a generative decoder rather than traditional VAE-based architecture. The core insight is that effective latent representations for generative modeling should enable visually plausible synthesis rather than exact reconstruction. This perspective allows REGEN to achieve much higher compression ratios without sacrificing quality. The method employs a content-aware positional encoding mechanism that enables the DiT decoder to generalize across arbitrary resolutions and aspect ratios.

## Method Summary
REGEN replaces traditional VAE-based video tokenization with a diffusion transformer decoder that learns to generate visually plausible reconstructions from compact latent representations. The encoder uses a causal 3D convolutional network to compress video chunks into content and motion latents, while the DiT decoder operates in pixel space with content-aware positional encodings derived from the latents themselves. This enables continuous-time decoding, resolution generalization, and high compression ratios up to 32× temporal compression while maintaining reconstruction quality.

## Key Results
- Achieves 32× temporal compression while maintaining superior reconstruction quality compared to MAGVIT-v2 baseline
- Demonstrates effective 1-step sampling without external distillation
- Enables text-to-video generation with approximately 5× fewer latent frames than current approaches
- Successfully generalizes to arbitrary resolutions and aspect ratios unseen during training

## Why This Works (Mechanism)

### Mechanism 1: Generation-Oriented Relaxation of Reconstruction
Replacing pixel-perfect reconstruction objectives with "visually plausible synthesis" allows the encoder to discard high-frequency details, enabling significantly higher compression ratios without perceptual degradation. The DiT decoder uses the compact latent as a semantic guide and hallucinates plausible texture and details from noise.

### Mechanism 2: Content-Aware Positional Encoding (CAPE)
Deriving positional embeddings directly from video latents allows the DiT decoder to generalize across arbitrary resolutions and aspect ratios unseen during training. The learned function maps spatiotemporal coordinates to embeddings conditioned on content and motion latents.

### Mechanism 3: Implicit Neural Representation for Temporal Dynamics
Using a SIREN network within the conditioning module allows handling continuous-time decoding, enabling temporal interpolation and extrapolation without architectural changes. The SIREN maps time coordinates to feature vectors modulated by motion latents.

## Foundational Learning

**Latent Diffusion Models (LDMs)**: Understanding how LDMs separate compression (encoding) from generation (diffusion) is crucial, as REGEN merges them by putting diffusion in the decoder. Quick check: How does REGEN's use of diffusion differ from a standard Latent Diffusion Model?

**Positional Encoding in Transformers**: Understanding standard absolute or rotary PE is required to see why fixed PE fails at variable resolutions. Quick check: Why does a standard Vision Transformer fail if you input an image resolution that differs from training?

**3D Causal Convolutions**: Understanding that causal convolutions only look at past/current frames is key to understanding how the encoder separates content and motion latents. Quick check: In a causal 3D convolution, how does the receptive field for time step t differ from a non-causal convolution?

## Architecture Onboarding

**Component map**: Spatiotemporal Encoder (3D Causal Conv) -> Latent Conditioning Module (C_e) -> Generative Decoder (DiT)

**Critical path**: Video frames are compressed; crucial semantic info is forced into zc and zm. The SIREN network effectively "inflates" the compact zm back into a spatiotemporal map suitable for the DiT. The DiT uses this inflated map as a guide to denoise random noise into a video.

**Design tradeoffs**: Inference speed vs. quality (DiT requires sampling steps vs. VAE's single pass). Patch size efficiency (patch size 8 for efficiency but may miss finer details).

**Failure signatures**: Chunking jumps (visual discontinuities between video chunks), gridding artifacts (if fixed PE is used instead of CAPE), motion blur/artifacts at extreme compression.

**First 3 experiments**: 1) Resolution generalization test (train on 192x320, evaluate on 384x640), 2) Sampling step ablation (run reconstruction with 1, 10, 50, 100 steps), 3) High-compression baseline comparison against MAGVIT-v2 at 32× compression.

## Open Questions the Paper Calls Out

**Open Question 1**: How does the specific architectural configuration of the Diffusion Transformer (DiT) decoder impact the trade-off between reconstruction quality and computational training efficiency? The authors could not conduct extensive ablation on architecture configuration due to resource constraints.

**Open Question 2**: Can the temporal discontinuities between video chunks be fully eliminated without resorting to training-free heuristics? While their latent extension strategy mitigates the issue, it cannot fully remove the jumping issue in a training-free manner.

**Open Question 3**: Can the generative decoder be adapted to operate on a latent space rather than pixel space to support smaller patch sizes and improve fine-grained quality? The decoder operates in pixel space, requiring a large patch size for efficiency, which can degrade generation quality.

## Limitations
- Cannot conduct extensive ablation on architecture configuration due to resource constraints
- Chunk-wise encoding scheme inherently creates boundaries that cannot be fully eliminated with training-free methods
- Decoder operates in pixel space requiring large patch size for efficiency, potentially degrading fine-grained quality

## Confidence

**High Confidence**: The mechanism of content-aware positional encoding enabling resolution generalization is well-supported by experimental evidence (Table 3, Figure 8).

**Medium Confidence**: The claim of achieving visually plausible reconstruction at 32× temporal compression is supported by qualitative comparisons, but quantitative metrics may not fully capture perceptual advantages.

**Low Confidence**: The assertion that the DiT decoder naturally enables continuous-time decoding is theoretically sound but has limited empirical validation beyond basic reconstruction tasks.

## Next Checks

1. **Temporal Generalization Test**: Train REGEN on 4-8× compression, then evaluate on 16-32× compression without fine-tuning to verify conditioning module robustness.

2. **Cross-Dataset Generation Performance**: Evaluate learned embeddings on external text-to-video generation tasks to validate the 5× frame reduction claim and assess semantic fidelity for conditional generation.

3. **Ablation of DiT Complexity**: Implement reduced-complexity decoder and measure trade-off between reconstruction quality and computational efficiency to establish practical deployment thresholds.