---
ver: rpa2
title: 'Multilingual Conversational AI for Financial Assistance: Bridging Language
  Barriers in Indian FinTech'
arxiv_id: '2512.01439'
source_url: https://arxiv.org/abs/2512.01439
tags:
- language
- financial
- multilingual
- india
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multilingual conversational AI system for
  financial assistance that supports code-mixed languages like Hinglish to serve India's
  diverse user base. The system employs a multi-agent architecture with language classification,
  function management, and multilingual response generation.
---

# Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech

## Quick Facts
- arXiv ID: 2512.01439
- Source URL: https://arxiv.org/abs/2512.01439
- Authors: Bharatdeep Hazarika; Arya Suneesh; Prasanna Devadiga; Pawan Kumar Rajpoot; Anshuman B Suresh; Ahmed Ifthaquar Hussain
- Reference count: 30
- Primary result: 41% increase in task completion rates and 86% increase in average session length for multilingual financial assistant vs. English-only baseline

## Executive Summary
This paper presents a multilingual conversational AI system for financial assistance that addresses India's linguistic diversity through code-mixed language support (e.g., Hinglish). The system employs a multi-agent architecture with specialized components for language classification, query normalization, financial tool orchestration, and multilingual response generation. Through comparative analysis and real-world deployment with 500+ users, the system demonstrated significant improvements in user engagement metrics and achieved performance parity with English queries across supported languages. The key innovation lies in decoupling language processing from core financial logic through query rephrasing, enabling existing monolingual tools to serve multilingual users effectively.

## Method Summary
The system uses a four-stage pipeline: (1) Language Classification with Indic-BERT for code-mixed financial queries, (2) Orchestrator for query rephrasing to canonical English and intent classification, (3) Specialized Financial Tools (deterministic and LLM-powered), and (4) Response Generation with Hermes-3-8B. The architecture employs query normalization to translate code-mixed inputs into standardized English before tool dispatch, allowing existing financial tools to remain language-agnostic while only user-facing layers handle multilingual complexity. Key technical choices include Indic-BERT for its superior performance on code-mixed text and Hermes-3-8B for balanced instruction-following and multilingual generation capabilities.

## Key Results
- 41% increase in task completion rates compared to English-only baselines
- 86% increase in average session length for multilingual users
- 95.8% F1-score on Hinglish financial queries using Indic-BERT vs. 63.7% for general-purpose models
- Task success rates achieved parity with pure English queries across all tested languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-adapted multilingual models substantially outperform general-purpose models for language detection on code-mixed financial queries.
- Mechanism: Indic-BERT, pre-trained on 11 Indian languages, recognizes linguistic patterns specific to Indian code-mixing more accurately than general multilingual models.
- Core assumption: Financial domain queries contain distinctive lexical patterns that benefit from domain-aware pre-training.
- Evidence anchors: Table 2 shows Indic-BERT achieves 95.8% F1-score on Hinglish (Financial) vs. Qwen2.5-0.5B at 63.7%; Section 4.2.1 states Indic-BERT demonstrated "substantially higher accuracy and F1-scores on complex code-mixed text, with latency under 20ms."
- Break condition: If target languages fall outside Indic-BERT's 11-language coverage or involve non-Indian language pairs.

### Mechanism 2
- Claim: Decoupling language handling from core business logic via query normalization preserves task success rates across multilingual inputs.
- Mechanism: The Orchestrator normalizes code-mixed input into canonical English before tool dispatch, allowing existing monolingual financial tools to operate unchanged.
- Core assumption: A canonical English representation can faithfully capture user intent across languages without information loss.
- Evidence anchors: Section 4.2.2: "the entire system does not need to be multilingual, only the user-facing layers do"; Section 4.3: Final architecture "achieved task success rates on par with pure English queries across all tested languages."
- Break condition: If queries contain culturally-bound financial concepts without English equivalents, or if normalization introduces semantic drift.

### Mechanism 3
- Claim: Language-tag-conditional response generation produces fluent, culturally appropriate outputs while maintaining factual grounding from tool outputs.
- Mechanism: The Response Generator receives both structured English tool outputs and the original language tag, enabling synthesis of accurate financial information in the user's preferred language.
- Core assumption: The response generation model has sufficient multilingual capability to accurately translate financial data while maintaining instruction-following discipline.
- Evidence anchors: Table 3 shows Hermes-3-8B selected for 4.6/5 instruction-following and 93.7% tool-calling accuracy; Table 4 shows human evaluation improvements: Fluency +40.6%, Coherence +21.1%, Helpfulness +14.6%.
- Break condition: If the response model lacks coverage for target languages, or if financial terminology has no standard translation.

## Foundational Learning

- Concept: **Code-Mixing vs. Code-Switching**
  - Why needed here: 90% of India's population lacks English proficiency; users naturally blend languages (Hinglish). Systems must handle intra-sentential mixing, not just monolingual mode switching.
  - Quick check question: Can your system parse "mera equity exposure kitna hai?" and correctly route to a portfolio analytics tool?

- Concept: **Transfer Learning via Language Relatedness**
  - Why needed here: Fine-tuning on linguistically related language subsets (e.g., Indo-Aryan) can outperform training on individual languages or maximally diverse sets.
  - Quick check question: When extending to a new language, do you know which existing trained languages are linguistically closest?

- Concept: **LLM-as-Judge Evaluation Frameworks**
  - Why needed here: Multilingual response quality requires rubric-based assessment since standard benchmarks don't cover code-mixed financial dialogues.
  - Quick check question: Have you defined domain-specific evaluation criteria beyond generic fluency metrics?

## Architecture Onboarding

- Component map: Language Classifier (Indic-BERT) -> Orchestrator (query rephrasing + intent classification) -> Specialized Financial Tools -> Response Generator (Hermes-3-8B)

- Critical path: Language detection accuracy → Normalization quality → Tool selection correctness → Response generation faithfulness. Errors cascade; classifier mistakes propagate through all downstream stages.

- Design tradeoffs:
  - Latency vs. accuracy: Lightweight classifier (Indic-BERT) chosen over larger models to keep overhead at 4-8%
  - Modularity vs. complexity: Pipeline architecture enables independent component optimization but introduces integration surface area
  - Generalization vs. specialization: Domain-adapted models outperform generalists on niche code-mixed financial text, but require retraining for new domains/languages

- Failure signatures:
  - Intent misclassification: Multi-intent queries partially handled
  - Factual hallucination: Response generator fabricates data without proper grounding
  - Language detection failure: Short queries lack features; system defaults to prior context
  - Awkward phrasing: Grammatically correct but unnaturally formal responses indicate prompt engineering gaps

- First 3 experiments:
  1. Baseline stress test: Feed existing English-only system with Hinglish/Hindi/Marathi queries; measure task success rate degradation (paper shows 20-45% drop)
  2. Classifier benchmark: Compare Indic-BERT vs. Qwen2.5-0.5B vs. other candidates on held-out code-mixed financial queries; optimize for F1-score at <20ms latency
  3. End-to-end validation: Run golden test set through full pipeline; verify intent/tool call accuracy via exact-match assertion and response quality via LLM-as-judge scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transfer learning from Hindi-English code-mixed financial dialogues effectively extend to other Indic language pairs (e.g., Tamil-English, Bengali-English) without substantial additional training data?
- Basis in paper: The conclusion states: "Future research will extend capabilities to other Indic languages using transfer learning principles."
- Why unresolved: The current system focuses exclusively on Hindi-English code-mixing; the paper explicitly notes it "may not generalize to other Indian language combinations without significant adaptation."
- What evidence would resolve it: Empirical evaluation showing comparable task success rates for Tamil-English, Bengali-English, and other code-mixed pairs using the same architecture with minimal fine-tuning.

### Open Question 2
- Question: How can dialect-aware personalization be implemented to address performance disparities between "standard" urban Hindi and regional dialects?
- Basis in paper: Future directions mention "dialect-aware personalization models"; Ethical considerations acknowledge "risk of better performance for 'standard' urban Hindi dialects compared to regional variations."
- Why unresolved: No dialect-specific evaluation was conducted, and the language classifier was not tested on regional dialect variations.
- What evidence would resolve it: Systematic evaluation across dialect variants with per-dialect task completion metrics, followed by personalization mechanisms that close the performance gap.

### Open Question 3
- Question: What grounding mechanisms can effectively reduce factual hallucinations in financial response generation without compromising fluency?
- Basis in paper: Table 6 identifies "Factual Hallucination" as a primary error category, with "Lack of grounding mechanisms" as root cause.
- Why unresolved: The paper acknowledges this weakness but does not propose or evaluate specific grounding techniques to ensure factual faithfulness.
- What evidence would resolve it: Comparative study of grounding approaches showing reduced hallucination rates while maintaining the 4.5/5 fluency score.

### Open Question 4
- Question: How does the multi-agent pipeline architecture scale with increasing query complexity, particularly for multi-intent financial queries?
- Basis in paper: Error analysis reveals "Intent Misclassification" for multi-intent queries as a failure mode; the paper notes "robustness can decrease with increasing query complexity."
- Why unresolved: The evaluation focused primarily on single-intent queries; multi-intent handling was identified as a weakness but not systematically analyzed or improved.
- What evidence would resolve it: Controlled experiments varying query complexity with corresponding task accuracy metrics, plus architectural modifications demonstrating improved multi-intent reasoning.

## Limitations

- Dataset Dependency: System effectiveness relies on high-quality parallel corpora for code-mixed financial queries, which remain scarce for Indian language pairs.
- Domain Specificity: Architecture excels in financial assistance but may not generalize to other domains without retraining.
- Language Coverage Gaps: Indic-BERT supports 11 Indian languages, but coverage gaps exist for languages like Tamil, Telugu, and Kannada.

## Confidence

- **High Confidence**: Language classification superiority (Indic-BERT vs. Qwen2.5-0.5B) is well-supported by quantitative metrics (95.8% vs. 63.7% F1-score).
- **Medium Confidence**: The decoupling architecture shows strong theoretical foundation and alignment with similar systems, but real-world semantic preservation requires empirical validation.
- **Medium Confidence**: Response generation quality improvements are documented through human evaluation metrics, but the underlying mechanism for maintaining factual grounding across languages needs further scrutiny.
- **Low Confidence**: Claims about performance parity across languages are based on controlled testing; real-world deployment shows positive trends but may not capture edge cases.

## Next Checks

1. **Cross-Domain Generalization Test**: Deploy the system architecture on non-financial conversational tasks (e.g., customer support, healthcare) with the same language handling pipeline to assess domain transfer capabilities and identify architectural limitations.

2. **Error Analysis on Boundary Cases**: Systematically test the pipeline with queries containing: (a) culturally-specific financial concepts without English equivalents, (b) extreme code-mixing ratios, and (c) multi-intent queries requiring complex tool orchestration to identify failure patterns and confidence thresholds.

3. **Longitudinal User Behavior Study**: Track user engagement metrics beyond the initial 30-day window and across different user segments (urban/rural, tech-savvy/novice) to validate retention improvements and identify demographic-specific performance variations.