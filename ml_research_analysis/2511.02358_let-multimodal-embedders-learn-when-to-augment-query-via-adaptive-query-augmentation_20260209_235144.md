---
ver: rpa2
title: Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation
arxiv_id: '2511.02358'
source_url: https://arxiv.org/abs/2511.02358
tags:
- augmentation
- query
- queries
- m-solomon
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: M-Solomon addresses the limitations of query augmentation in multimodal
  retrieval by adaptively determining when to augment queries rather than applying
  augmentation uniformly. The method first identifies which training datasets benefit
  from augmentation, then uses a Multimodal LLM to synthesize appropriate augmentations
  for those queries.
---

# Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation

## Quick Facts
- arXiv ID: 2511.02358
- Source URL: https://arxiv.org/abs/2511.02358
- Reference count: 32
- M-Solomon achieves 67.6% overall Precision@1 on MMEB benchmark, outperforming baselines by 1.5% while reducing embedding latency by ~50%

## Executive Summary
M-Solomon addresses the limitations of query augmentation in multimodal retrieval by adaptively determining when to augment queries rather than applying augmentation uniformly. The method first identifies which training datasets benefit from augmentation through a pilot study, then uses a Multimodal LLM to synthesize appropriate augmentations for those queries. During training, M-Solomon learns to generate either an augmentation token or a simple embed token at the beginning of each query, enabling adaptive decision-making. The approach outperforms both non-augmentation and always-augmentation baselines on the MMEB benchmark while reducing inference latency by approximately 50%.

## Method Summary
M-Solomon uses a Qwen2-VL-7B-Instruct base model with LoRA rank 16 to learn adaptive query augmentation. The method first conducts a pilot study to classify datasets as requiring augmentation or not based on performance differences between NoAug and AlwaysAug baselines. For queries requiring augmentation, a teacher MLLM (Qwen2.5-VL-72B-Instruct) generates answer-style augmentations which are then used during student training. The model is jointly trained with contrastive loss for embedding quality and autoregressive loss for adaptive generation, learning to predict `/augment` or `/embed` as the first token to route queries appropriately.

## Key Results
- M-Solomon achieves 67.6% overall Precision@1 on MMEB benchmark
- Outperforms non-augmentation baseline (66.1%) by 1.5% and always-augmentation baseline (67.4%) by 0.2%
- Reduces embedding latency by approximately 50% compared to always-augmentation approach
- Achieves 93.1% confidence in adaptive routing decisions across datasets

## Why This Works (Mechanism)

### Mechanism 1: Dataset-Level Augmentation Need Classification
Pre-identifying which datasets benefit from augmentation enables cleaner training signal for adaptive behavior. A pilot study compares NoAug vs AlwaysAug baselines on each dataset's test set, classifying datasets based on performance. This creates supervised labels for the adaptive generation task, though it assumes dataset-level homogeneity and may introduce noise if datasets contain heterogeneous query types.

### Mechanism 2: Answer-Style Synthetic Augmentation Generation
Treating query answers as augmentations provides useful semantic expansion without hallucination drift. A teacher MLLM generates reasoning and answers using structured prompts, with only the answer portion extracted for augmentation. This grounds augmentations in factual query responses, though effectiveness depends on teacher model accuracy and the paper doesn't analyze teacher error rates.

### Mechanism 3: Prefix-Based Adaptive Routing via Joint Training
Training the model to generate `/augment` or `/embed` as the first token creates a learned router that gates augmentation behavior. The model is jointly trained with contrastive loss for embedding quality and autoregressive loss for adaptive generation, teaching it to predict the correct prefix which then conditions subsequent behavior. The combined loss balances competing objectives, though hyperparameter sensitivity is not fully explored.

## Foundational Learning

- **Contrastive Learning for Dense Retrieval**: Why needed: L_rep pulls positive document embeddings closer to query embeddings while pushing hard negatives apart. Quick check: Can you explain why hard negatives (m=1) are used instead of only in-batch negatives?

- **Autoregressive Language Modeling**: Why needed: L_gen teaches the model to generate `/augment` or `/embed` token and, when augmenting, the answer text. Quick check: How does the autoregressive loss differ from contrastive loss in terms of what it optimizes?

- **Multi-Task Learning / Joint Training**: Why needed: M-Solomon simultaneously learns embedding representation and adaptive generation. Quick check: Why might joint training be preferable to a two-stage approach (first train router, then train embedder)?

## Architecture Onboarding

- **Component map**: Input Query -> Qwen2-VL-7B-Instruct (base encoder + LoRA rank 16) -> First token generation: /augment or /embed -> (if /augment) Continue generation → Answer-style augmentation text -> Append augmentation to query -> Encode augmented query → Extract EOS embedding -> Retrieve via cosine similarity

- **Critical path**: The first token decision (`/augment` vs `/embed`) determines inference cost and quality. During training, this is supervised by the dataset-level labels from the pilot study.

- **Design tradeoffs**: Dataset-level vs query-level augmentation classification (simpler but noisier), teacher model size (72B) vs student (7B) (quality vs complexity), α_gen=0.1 (prioritizes embedding quality over generation accuracy).

- **Failure signatures**: Low CF score (<70%) indicates model uncertainty about routing; check training data quality. `/embed%` far from expected may indicate label noise in dataset classification. M-Solomon-/augment showing lower latency than M-Solomon indicates generation conflicts when forcing augmentation on inappropriate queries.

- **First 3 experiments**:
  1. Reproduce pilot study: Train NoAug and AlwaysAug baselines on 2-3 datasets, verify augmentation helps on some but hurts on others. Confirm dataset-level classification logic.
  2. Ablate α_gen: Test values {0.05, 0.1, 0.2} to validate that 0.1 balances embedding quality and routing accuracy. Monitor CF and P@1.
  3. Query-level analysis: Sample queries from a "requires augmentation" dataset where M-Solomon chooses `/embed`. Analyze if these are false negatives in dataset-level labeling or learned exceptions.

## Open Questions the Paper Calls Out
- Can query-level identification of augmentation necessity significantly outperform the current dataset-level classification approach?
- How can adaptive query augmentation be effectively extended to support reasoning-intensive embedding tasks?
- How dependent is M-Solomon's effectiveness on the specific teacher model used for augmentation synthesis?
- Does the binary "better or comparable" threshold for dataset-level classification adequately capture the true distribution of augmentation benefits?

## Limitations
- Dataset-level classification may introduce noise if datasets contain heterogeneous query types with mixed augmentation needs
- Teacher model dependency means augmentation quality is bounded by the 72B parameter MLLM's accuracy
- Hyperparameter sensitivity around α_gen=0.1 and unspecified LoRA target modules make faithful reproduction challenging
- Out-of-distribution generalization performance isn't thoroughly analyzed for failure cases

## Confidence
- **High Confidence**: The core mechanism of adaptive query augmentation via prefix-based routing is well-supported by experimental results showing consistent routing accuracy and performance improvements
- **Medium Confidence**: Dataset-level classification providing cleaner training signal is supported indirectly by ablation showing degradation when using random augmentation, but lacks direct comparison to query-level approaches
- **Low Confidence**: Answer-style synthetic augmentation effectiveness is supported only by qualitative examples without quantitative analysis of augmentation quality or teacher model accuracy validation

## Next Checks
- Conduct query-level error analysis on M-Solomon's routing decisions to validate dataset-level classification assumptions
- Perform teacher model quality audit by having human annotators rate 500 synthetic augmentations for relevance, accuracy, and utility
- Ablate the α_gen hyperparameter across {0.05, 0.1, 0.2} while monitoring CF scores, P@1 performance, and prefix distributions to assess robustness