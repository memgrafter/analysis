---
ver: rpa2
title: Evaluating the Impact of Post-Training Quantization on Large Language Models
  for Code Generation
arxiv_id: '2503.07103'
source_url: https://arxiv.org/abs/2503.07103
tags:
- quantization
- code
- samples
- calibration
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates post-training quantization (PTQ) techniques\
  \ for large language models (LLMs) specialized in code generation, focusing on reducing\
  \ memory footprint without sacrificing performance. The study applies the state-of-the-art\
  \ Additive Quantization with Learned Multi-Codebooks (AQLM) technique to two popular\
  \ code models\u2014CodeLlama and DeepSeek-Coder\u2014across multiple parameter sizes\
  \ and quantization levels (8, 4, 3, and 2 bits per parameter)."
---

# Evaluating the Impact of Post-Training Quantization on Large Language Models for Code Generation

## Quick Facts
- arXiv ID: 2503.07103
- Source URL: https://arxiv.org/abs/2503.07103
- Reference count: 40
- Primary result: 4-bit AQLM quantization achieves 70% memory reduction with minimal code generation accuracy loss

## Executive Summary
This paper investigates post-training quantization (PTQ) techniques for large language models specialized in code generation, focusing on reducing memory footprint without sacrificing performance. The study applies the state-of-the-art Additive Quantization with Learned Multi-Codebooks (AQLM) technique to two popular code models—CodeLlama and DeepSeek-Coder—across multiple parameter sizes and quantization levels (8, 4, 3, and 2 bits per parameter). Results show that 4-bit quantization achieves a 70% reduction in memory usage while maintaining comparable code generation accuracy on Python and Java benchmarks. For more extreme quantization (3 and 2 bits), using code-specific calibration datasets helps mitigate performance loss, and larger models (33B+ parameters) are more resilient to such aggressive compression. Fine-tuning after quantization further boosts 2-bit model performance, particularly for smaller models.

## Method Summary
The study evaluates post-training quantization using AQLM on CodeLlama (7B, 13B, 34B) and DeepSeek-Coder (1B, 7B, 33B) models. Quantization levels include 8, 4, 3, and 2 bits per parameter. Three calibration datasets are used: Random (general text), Mixed (50% code/50% natural language), and Code-only (100% code). Performance is measured using pass@1 on Python (HumanEval) and Java (McEval) benchmarks. For 2-bit models, end-to-end fine-tuning is applied using distillation from the original Float16 model. Statistical significance is assessed using McNemar's test with odds ratio thresholds.

## Key Results
- 4-bit quantization reduces memory footprint by 70% with statistically insignificant performance degradation
- Code-specific calibration datasets significantly improve 3-bit and 2-bit quantization performance
- Larger models (33B+) show greater resilience to extreme quantization, maintaining better performance at 2-bit levels
- End-to-end fine-tuning can recover 2-bit model performance, especially for smaller models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing model precision to 4-bit via AQLM may preserve code generation capability while significantly lowering memory footprint.
- **Mechanism:** AQLM uses Multi-Codebook Quantization (MCQ) to approximate floating-point weights. By grouping weights (g=8) and mapping them to learned codebooks, the model maintains sufficient numerical diversity to represent code logic despite reduced bit-width.
- **Core assumption:** The semantic knowledge required for code generation is robust to small perturbations in weight values and does not require the full dynamic range of 16-bit floating points.
- **Evidence anchors:**
  - [Abstract] "4-bit precision, resulting in an average memory footprint reduction of 70%... without observing any significant decrease in performance."
  - [Table 3] Shows statistically insignificant differences in `pass@1` scores between Float16 and 4-bit models for DeepSeek-Coder.
  - [Corpus] Neighbors like "SignRoundV2" support the feasibility of low-bit quantization maintaining performance.
- **Break condition:** Performance degradation becomes statistically significant (observed in some 4-bit Java benchmarks), suggesting specific languages or tasks may cross the precision threshold.

### Mechanism 2
- **Claim:** Code-specific calibration datasets likely mitigate performance loss during extreme quantization (3-bit and 2-bit).
- **Mechanism:** The calibration dataset guides the minimization of quantization error (the difference between original and quantized outputs). Using domain-relevant data (code snippets) ensures the quantizer preserves weights essential for code syntax and logic, rather than general natural language features.
- **Core assumption:** The activation distribution of code tasks differs significantly from natural language, and quantizing to preserve the latter may degrade the former if not calibrated specifically.
- **Evidence anchors:**
  - [Section 4 (RQ2)] "3-bit and 2-bit precision models are more sensible to the samples... statistically significant improvement... when calibration datasets feature code samples."
  - [Table 5] Mixed/Code datasets show statistically significant boosts in `pass@1` for 2-bit models compared to Random datasets.
  - [Corpus] Limited direct evidence in neighbors; "HAS-VQ" discusses adaptive quantization but specific domain-calibration signals are weak in the provided corpus.
- **Break condition:** If the calibration dataset is too small or unrepresentative of the target code domain, the quantization error minimization may overfit to the calibration samples (overfitting the quantization grid).

### Mechanism 3
- **Claim:** Larger models (33B+ parameters) appear more resilient to extreme quantization errors than smaller models (1B-7B).
- **Mechanism:** Larger models possess redundant parameters and distributed representations. When extreme quantization prunes precision, the sheer volume of parameters allows the model to "average out" the noise, retaining functionality where a smaller, denser model would fail.
- **Core assumption:** Model capacity (parameter count) acts as a buffer against the signal-to-noise ratio reduction caused by low-bit compression.
- **Evidence anchors:**
  - [Abstract] "larger models (33B+ parameters) are more resilient to such aggressive compression."
  - [Table 6] DeepSeek-Coder 33B shows a -16.7% relative decrease at 2-bit, whereas the 1B model shows a -51.1% decrease.
  - [Corpus] "Continual Quantization-Aware Pre-Training" discusses low-bit transitions, implicitly supporting the role of scale/architecture in low-bit success.
- **Break condition:** Resilience is not absolute; even 33B models show statistically significant degradation at 2-bit compared to their Float16 baselines.

## Foundational Learning

- **Concept:** Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)
  - **Why needed here:** The paper focuses exclusively on PTQ (compressing a pre-trained model) rather than QAT (training with quantization in the loop). Understanding this distinction is critical as it explains the low computational cost of the method (calibration vs. full retraining).
  - **Quick check question:** Does the technique require backpropagation through the entire training set? (Answer: No, only a small calibration set is needed).

- **Concept:** AQLM (Additive Quantization with Learned Multi-Codebooks)
  - **Why needed here:** This is the specific compression engine used. Unlike simple rounding (e.g., GPTQ), AQLM uses additive vector codebooks to approximate weights, which is key to its performance at extreme (2-bit) levels.
  - **Quick check question:** How are weights represented in AQLM? (Answer: As sums of vectors selected from multiple learned codebooks).

- **Concept:** Pass@k Metric
  - **Why needed here:** The paper evaluates performance using `pass@1`. This metric specifically measures the probability of generating correct code in a single attempt, which is a stricter and more practical measure for deployment than best-of-k accuracy.
  - **Quick check question:** If a model generates 20 samples and 1 is correct, is the `pass@1` score 100%? (Answer: No, it would be much lower; `pass@1` estimates the probability of success on a single try).

## Architecture Onboarding

- **Component map:**
  Base Model (CodeLlama/DeepSeek-Coder Float16) -> Calibration Data (Random/Mixed/Code) -> Quantizer (AQLM Engine) -> Optimizer (End-to-End Fine-tuning if 2-bit)

- **Critical path:**
  1. Select model size and target bit-width
  2. If targeting < 4-bit, Calibration Dataset selection is critical (must use Mixed/Code)
  3. Run AQLM calibration (learns codebooks)
  4. If 2-bit, apply End-to-End Fine-tuning to recover accuracy

- **Design tradeoffs:**
  - 4-bit vs. 2-bit: 4-bit offers a "safe" 70% memory reduction with negligible loss. 2-bit offers ~85% reduction but requires careful calibration and fine-tuning, with a higher risk of hallucination
  - Model Size vs. Bit-width: A 33B model at 2-bits may outperform a 7B model at 16-bits while fitting in similar memory (~9GB vs ~13GB), favoring the "Quantized Large Model" strategy

- **Failure signatures:**
  - Assertion Errors: Quantized models generating syntactically correct code that fails logical unit tests (common in 2-bit random calibration)
  - Non-existent APIs: Hallucinating library calls, identified in the paper as a failure mode in the 4-bit Java benchmark

- **First 3 experiments:**
  1. **Sanity Check (4-bit):** Quantize DeepSeek-Coder 7B to 4-bit using the "Random" dataset. Verify that `pass@1` on Python HumanEval is within ±2% of the baseline (reproducing Table 3)
  2. **Calibration Impact (2-bit):** Quantize CodeLlama 7B to 2-bit twice: once with "Random" and once with "Mixed" data. Compare the `pass@1` gap to demonstrate the calibration effect (reproducing Table 5 trends)
  3. **Fine-tuning Recovery:** Take the 2-bit "Mixed" model from step 2, apply end-to-end fine-tuning, and measure the performance uplift (checking against Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does extreme AQLM quantization impact inference latency and energy consumption?
- **Basis in paper:** [explicit] The authors state they leave "inference latency" and "energy consumption" as distinct optimization objectives for future work.
- **Why unresolved:** The study focused exclusively on the trade-off between memory footprint (storage) and generation accuracy.
- **What evidence would resolve it:** Empirical measurements of tokens-per-second latency and energy usage (e.g., kWh) for 2-bit and 3-bit quantized models during inference.

### Open Question 2
- **Question:** Do quantization effects generalize to other software engineering tasks like code summarization and repair?
- **Basis in paper:** [explicit] The authors identify assessing "code summarization and program repair" as a necessary next step to generalize findings.
- **Why unresolved:** The evaluation was restricted to code generation benchmarks (MultiPL-E, McEval), leaving other tasks unexplored.
- **What evidence would resolve it:** Benchmarking the performance degradation of quantized CodeLlama and DeepSeek-Coder on standard summarization and bug-fixing datasets.

### Open Question 3
- **Question:** Can models exceeding 70B parameters maintain performance stability at 2-bit quantization?
- **Basis in paper:** [explicit] The authors note "further empirical analyses are needed" to confirm the hypothesis that larger models (>34B) are resilient to extreme quantization.
- **Why unresolved:** Computational constraints limited the study to a maximum of 34B parameters; the trend of increasing resilience remains unverified at larger scales.
- **What evidence would resolve it:** Applying the 2-bit AQLM protocol to CodeLlama 70B and evaluating the resulting pass@1 scores against the fp16 baseline.

## Limitations

- The paper's core finding (70% memory reduction at 4-bit without significant performance loss) relies on code-specific calibration data, limiting generalizability to other domains
- The study focuses exclusively on PTQ rather than QAT, potentially underestimating the performance potential of ultra-low-bit quantization where QAT could yield better results
- The analysis of failure modes is qualitative (e.g., "hallucinating non-existent APIs") without systematic error classification or frequency analysis

## Confidence

- **High Confidence:** The 4-bit quantization results (70% memory reduction, minimal performance loss) are well-supported by statistical significance testing and multiple benchmarks
- **Medium Confidence:** The code-specific calibration benefits for 3-bit and 2-bit models are demonstrated but could benefit from ablation studies showing the impact of calibration dataset size and composition
- **Medium Confidence:** The resilience of larger models to extreme quantization is supported by comparative data but lacks mechanistic explanation of why parameter redundancy specifically helps in this context

## Next Checks

1. **Calibration Ablation Study:** Test whether random calibration data with larger sample sizes (e.g., 4,096 samples) can match the performance of code-specific calibration for 2-bit models
2. **Architecture Dependency Analysis:** Quantize non-Transformer architectures (e.g., RWKV) using the same AQLM approach to determine if the observed resilience to low-bit quantization is architecture-specific
3. **Error Classification Pipeline:** Implement automated detection of common quantization failure modes (syntax errors, hallucinated APIs, logic errors) to quantify their frequency across different bit-widths and calibration strategies