---
ver: rpa2
title: 'Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer
  Hardware'
arxiv_id: '2511.10277'
source_url: https://arxiv.org/abs/2511.10277
tags:
- memory
- dialogue
- small
- knowledge
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a modular NPC dialogue system using small language
  models (SLMs) fine-tuned with fixed personas and paired with runtime-swappable memory
  modules. The system enables multiple distinct NPC instances from a single model,
  preserving character identity while supporting long-term context and knowledge retrieval.
---

# Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware

## Quick Facts
- arXiv ID: 2511.10277
- Source URL: https://arxiv.org/abs/2511.10277
- Reference count: 7
- A modular NPC dialogue system using small language models with fixed personas and runtime-swappable memory modules achieves sub-50ms memory swaps and retrieval times on consumer hardware.

## Executive Summary
This paper introduces a modular NPC dialogue system that leverages small language models (SLMs) fine-tuned with fixed personas and paired with runtime-swappable memory modules. The approach enables multiple distinct NPC instances from a single model while preserving character identity and supporting long-term context and knowledge retrieval. The system demonstrates strong performance across three open-source SLMs—DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct—with Mistral-7B-Instruct achieving 93% factual accuracy, 100% context retention, and 100% world knowledge retrieval, all within real-time constraints on consumer hardware.

## Method Summary
The system uses small language models fine-tuned with fixed personas and paired with runtime-swappable memory modules. This modular approach allows multiple distinct NPC instances from a single model while preserving character identity. The memory modules support long-term context and knowledge retrieval, with memory swaps and retrievals occurring in under 50ms. Evaluation was conducted across three open-source SLMs on consumer hardware, using both synthetic datasets for systematic benchmarking.

## Key Results
- Mistral-7B-Instruct achieved 93% factual accuracy, 100% context retention, and 100% world knowledge retrieval
- Memory swaps and retrievals occurred in under 0.03s and 0.042s respectively
- Sub-50ms memory operations confirm real-time feasibility on consumer hardware

## Why This Works (Mechanism)
The system works by decoupling persona from memory, allowing the same base model to represent multiple characters while maintaining individual identities through modular memory. This separation enables efficient context switching and knowledge retrieval without model retraining. The fine-tuning process embeds character-specific knowledge directly into the model parameters, while the modular memory provides dynamic, long-term context that persists beyond the model's inherent context window.

## Foundational Learning
- Small Language Models (SLMs): Compact transformer-based models (1B-7B parameters) that balance performance with computational efficiency; needed to run real-time dialogue on consumer hardware; quick check: parameter count and inference speed.
- Modular Memory Architecture: Separate knowledge storage that can be dynamically loaded/unloaded at runtime; needed to maintain multiple NPC identities without model duplication; quick check: memory swap latency and coherence.
- Persona Fine-tuning: Process of adapting base models to specific character traits and knowledge; needed to ensure consistent character behavior; quick check: accuracy on persona-specific prompts.
- Context Window Management: Techniques for handling information beyond the model's native context limit; needed for long-term narrative consistency; quick check: retrieval accuracy and context retention rates.
- Knowledge Grounding: Methods for connecting model outputs to factual information; needed to maintain narrative consistency and avoid hallucinations; quick check: factual accuracy scores.
- Real-time Inference Optimization: Techniques for achieving sub-50ms response times; needed for interactive gameplay; quick check: end-to-end latency measurements.

## Architecture Onboarding

**Component Map:**
User Input -> Memory Module Selector -> Memory Retrieval -> SLM with Fixed Persona -> Response Generation -> Output to User

**Critical Path:**
User query → Memory context retrieval → Persona model inference → Response generation

**Design Tradeoffs:**
- Model size vs. accuracy: Larger models (Mistral-7B) provide better performance but require more resources
- Memory granularity: More detailed memory modules improve accuracy but increase retrieval time
- Persona specificity: Highly specialized personas improve character consistency but reduce model reusability

**Failure Signatures:**
- Context bleeding between NPCs when memory modules are not properly isolated
- Factual inconsistencies when memory retrieval fails or returns outdated information
- Performance degradation when multiple memory swaps occur in rapid succession

**First 3 Experiments to Run:**
1. Measure memory swap latency with varying memory module sizes
2. Test context retention across multiple conversation turns with different SLM architectures
3. Evaluate factual accuracy when retrieving from partially corrupted memory modules

## Open Questions the Paper Calls Out
The paper acknowledges uncertainty about how well the system handles unscripted or adversarial inputs common in interactive gameplay, given that evaluation relied entirely on synthetic datasets rather than human-generated dialogue. The authors also note that scalability to lower-end or mobile devices has not been validated, as experiments were conducted on a single hardware configuration (RTX 4070 Ti).

## Limitations
- Evaluation relied entirely on synthetic datasets rather than human-generated dialogue
- Only tested on RTX 4070 Ti hardware, limiting scalability validation
- Uncertain performance with unscripted or adversarial player inputs

## Confidence
- Factual accuracy claims: High
- Context retention claims: High
- Real-time performance claims: High
- Generalizability to real gameplay: Medium
- Scalability to diverse hardware: Medium

## Next Checks
1. Conduct user studies with human players to assess the naturalness, engagement, and adaptability of NPC dialogue in unscripted scenarios
2. Test the system on a wider range of hardware (e.g., lower-end GPUs, integrated graphics) to confirm real-time performance and scalability
3. Evaluate memory consistency and narrative coherence when rapidly switching between multiple NPC contexts or handling complex, multi-turn storylines