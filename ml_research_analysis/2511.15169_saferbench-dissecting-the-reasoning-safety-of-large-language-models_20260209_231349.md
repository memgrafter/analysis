---
ver: rpa2
title: 'SafeRBench: Dissecting the Reasoning Safety of Large Language Models'
arxiv_id: '2511.15169'
source_url: https://arxiv.org/abs/2511.15169
tags:
- risk
- reasoning
- safety
- level
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeRBench evaluates reasoning safety of Large Reasoning Models
  (LRMs) through end-to-end process diagnosis. It segments reasoning traces into micro-thought
  chunks, assigning intent labels to track risk evolution, and measures 10 fine-grained
  safety dimensions grouped into Risk Exposure and Safety Awareness.
---

# SafeRBench: Dissecting the Reasoning Safety of Large Language Models

## Quick Facts
- **arXiv ID**: 2511.15169
- **Source URL**: https://arxiv.org/abs/2511.15169
- **Reference count**: 40
- **Primary result**: SafeRBench reveals reasoning safety is a trajectory characteristic, with risk concentration near trace endings and overgeneralization of helpfulness amplifying risks in larger models.

## Executive Summary
SafeRBench introduces a comprehensive framework for evaluating reasoning safety in Large Reasoning Models (LRMs) through end-to-end process diagnosis. The benchmark segments reasoning traces into micro-thought chunks with intent labeling to track risk evolution across 10 fine-grained safety dimensions. Experiments on 19 LRMs reveal that while reasoning enhances safety awareness in mid-sized models, it paradoxically amplifies actionable risks in larger models due to overgeneralization of helpfulness. The framework demonstrates that safety is not a static property but a trajectory characteristic, with risk concentration typically occurring near the end of reasoning traces.

## Method Summary
SafeRBench evaluates reasoning safety through a novel segmentation approach that divides reasoning traces into micro-thought chunks, each assigned specific intent labels to track risk evolution throughout the reasoning process. The framework measures 10 fine-grained safety dimensions organized into Risk Exposure and Safety Awareness categories. This process-oriented diagnosis enables researchers to identify precisely where and how safety risks emerge during reasoning, rather than simply evaluating final outputs. The benchmark applies this methodology across 19 different LRMs to reveal patterns in how reasoning capabilities interact with safety considerations at different model scales.

## Key Results
- Reasoning enhances safety awareness in mid-sized models by improving intent recognition capabilities
- Larger models paradoxically amplify actionable risks due to overgeneralization of helpfulness
- Risk concentration occurs near the end of reasoning traces, indicating safety is a trajectory characteristic
- Response complexity does not inherently correlate with increased risk exposure

## Why This Works (Mechanism)
SafeRBench works by capturing the temporal dynamics of reasoning safety through micro-thought segmentation and intent labeling. By tracking how safety-related intentions evolve throughout the reasoning process, the framework reveals that safety is not a static endpoint but emerges from the interaction between reasoning steps and safety awareness. The methodology exposes how larger models' tendency to be overly helpful can inadvertently amplify risks, while mid-sized models benefit from reasoning's ability to improve intent recognition and safety filtering.

## Foundational Learning
- **Micro-thought segmentation**: Breaking reasoning traces into atomic thought units (why needed: enables granular risk tracking; quick check: verify consistent segmentation across annotators)
- **Intent labeling**: Assigning safety-relevant intent categories to each micro-thought (why needed: enables tracking of risk evolution; quick check: measure inter-rater reliability)
- **Risk trajectory analysis**: Studying how risks accumulate or dissipate throughout reasoning traces (why needed: reveals temporal safety patterns; quick check: confirm risk concentration patterns)
- **Safety dimension taxonomy**: Organizing 10 safety dimensions into Risk Exposure and Safety Awareness (why needed: provides structured evaluation framework; quick check: validate dimension relevance)
- **Model scale interaction**: Understanding how reasoning safety varies with model size (why needed: reveals scale-dependent safety behaviors; quick check: replicate across different scale ranges)
- **Overgeneralization phenomenon**: Identifying when helpfulness amplifies risks (why needed: explains paradoxical safety degradation; quick check: test with controlled helpfulness parameters)

## Architecture Onboarding

**Component map**: Data Collection -> Micro-thought Segmentation -> Intent Labeling -> Risk Classification -> Safety Dimension Scoring -> Model Evaluation

**Critical path**: The core workflow involves collecting reasoning traces, segmenting them into micro-thoughts, assigning intent labels, classifying risks, scoring across safety dimensions, and aggregating results for model comparison.

**Design tradeoffs**: The framework prioritizes temporal resolution and granular risk tracking over computational efficiency, sacrificing some speed for deeper safety insights. The micro-thought approach provides detailed analysis but requires substantial annotation effort.

**Failure signatures**: Inconsistent intent labeling across annotators, risk misclassification due to ambiguous reasoning steps, and aggregation errors when combining safety dimension scores can compromise results.

**First experiments**:
1. Validate segmentation consistency by having multiple annotators independently segment the same reasoning traces
2. Test inter-rater reliability on intent labeling across different safety-related reasoning examples
3. Conduct scale sensitivity analysis by evaluating models across multiple size categories to confirm scale-dependent safety patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Results focus primarily on mainstream LRMs without extensive testing of specialized or domain-adapted variants
- Evaluation is limited to English-language content, potentially limiting multilingual applicability
- Annotation subjectivity in intent classification could introduce variability affecting risk assessment accuracy

## Confidence
- **Core findings on risk concentration**: High confidence
- **Paradox of reasoning amplifying risks in larger models**: High confidence
- **Generalizability across reasoning architectures**: Medium confidence
- **Impact of annotation subjectivity**: Medium confidence

## Next Checks
1. Replicate the analysis with additional annotators and inter-rater reliability metrics to quantify labeling consistency across intent classifications
2. Extend the benchmark to include domain-specific LRMs and specialized reasoning architectures to test generalizability
3. Conduct a temporal stability analysis by evaluating the same models across multiple time points to assess whether risk patterns persist or evolve with ongoing training updates