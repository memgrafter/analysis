---
ver: rpa2
title: Policies and Evaluation for Online Meeting Summarization
arxiv_id: '2502.03111'
source_url: https://arxiv.org/abs/2502.03111
tags:
- summarization
- systems
- meeting
- summary
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of online meeting
  summarization, proposing several policies for real-time summary generation during
  meetings. The authors introduce novel metrics for evaluating latency and intermediate
  summary quality, comparing systems that process meeting transcripts chunk-by-chunk
  versus those that dynamically select summarization units.
---

# Policies and Evaluation for Online Meeting Summarization

## Quick Facts
- arXiv ID: 2502.03111
- Source URL: https://arxiv.org/abs/2502.03111
- Reference count: 35
- Primary result: Online meeting summarization policies achieve strong quality with sliding window policy showing competitive ROUGE-1 scores and lowest latency

## Executive Summary
This paper introduces the first systematic study of online meeting summarization, proposing policies for generating summaries in real-time as meetings progress rather than after completion. The authors develop novel metrics for evaluating both latency and intermediate summary quality, addressing the unique challenges of incomplete information and the trade-off between summary quality and responsiveness. Through extensive experiments on the AutoMin dataset, the study demonstrates that online models can produce high-quality summaries, with dynamic segmentation strategies significantly outperforming static chunk-based approaches.

The research establishes a comprehensive evaluation framework for online summarization, introducing Expected Latency and Normalized Erasure metrics alongside traditional quality measures. Human evaluations validate the quantitative findings, showing that sliding window policies achieve the best balance of quality and latency, receiving the highest ratings for adequacy, fluency, and relevance at 40% meeting completion. The work provides practical insights for developing real-time summarization systems and sets benchmarks for future research in this emerging domain.

## Method Summary
The study investigates online meeting summarization using the AutoMin 2023 test set, employing two backend models: BART-large fine-tuned on SamSum and GPT-4-32k. Five policies are evaluated: length-based (fixed chunk sizes), model-based (quality estimation), sliding window (output-prefix-driven boundaries), full rewriting, and fully incremental. The research introduces novel metrics including Expected Latency for measuring responsiveness, Normalized Erasure for quantifying rewriting effort, and R1-AUC for assessing intermediate summary quality. De-identification is reversed by mapping PERSON/ORG/PROJECT labels to random names and acronyms. The sliding window policy maintains an input window plus summary prefix, writing only when new content exceeds the prefix.

## Key Results
- Model-based policy achieves 42.6 ROUGE-1 score, highest among online systems
- Sliding window policy shows competitive quality with significantly lower latency than fixed chunking
- Dynamic segmentation strategies outperform static ones in human evaluations
- Sliding window system receives highest ratings (adequacy, fluency, relevance) at 40% meeting completion

## Why This Works (Mechanism)
The effectiveness stems from adaptive processing strategies that balance quality and latency. By dynamically selecting summarization units based on content and model confidence rather than fixed boundaries, the system can provide timely updates without sacrificing coherence. The sliding window approach leverages the summary prefix as context, allowing the model to generate continuations only when meaningful new information emerges. This reduces unnecessary rewriting while maintaining summary relevance throughout the meeting.

## Foundational Learning

- **Chunk-based processing**: Why needed - enables incremental summarization; Quick check - verify chunk boundaries align with dialog turns
- **Quality estimation via model confidence**: Why needed - guides adaptive chunk selection; Quick check - confirm confidence scores correlate with actual summary quality
- **Expected Latency metric**: Why needed - measures responsiveness trade-offs; Quick check - validate latency calculations on synthetic meeting data
- **R1-AUC for intermediate quality**: Why needed - evaluates progressive summarization; Quick check - ensure ROUGE sampling captures summary evolution
- **Normalized Erasure for rewriting**: Why needed - quantifies summary stability; Quick check - verify erasure values reflect actual token changes
- **De-identification reversal**: Why needed - restores natural language for evaluation; Quick check - confirm mappings produce coherent named entities

## Architecture Onboarding

Component map: Transcript → Chunk segmentation → Policy selection → Backend model (BART/GPT-4) → Summary generation → Evaluation metrics

Critical path: Real-time processing requires efficient chunk management, low-latency model inference, and timely metric calculation. The sliding window policy's critical path involves maintaining prefix context, detecting meaningful output changes, and rotating input windows without disrupting summary coherence.

Design tradeoffs: Fixed chunking offers simplicity but poor latency-quality balance; dynamic policies provide better trade-offs but require quality estimation and careful prefix management. BART offers faster inference for length-based policies, while GPT-4 enables more sophisticated rewriting at higher computational cost.

Failure signatures: Sliding window may produce no new output if prefix forcing fails; model-based may select overly large chunks reducing latency benefits; quality estimation may be unreliable for very short chunks. High redundancy in sliding window indicates inefficient chunk rotation.

Three first experiments:
1. Test sliding window prefix forcing with synthetic transcript to verify output generation beyond existing summary
2. Validate model-based quality estimation by checking if confidence scores predict summary quality on validation set
3. Verify Expected Latency calculation on simple meeting with known WRITE event timings

## Open Questions the Paper Calls Out
None

## Limitations
- De-identification reversal process lacks complete specification, hindering exact reproduction
- Model-based policy quality estimation method is underspecified, affecting implementation
- Sliding window performance highly sensitive to prompt and prefix management details
- Evaluation framework assumes unit duration per dialog turn, limiting real-world applicability

## Confidence

**High Confidence**: BART-large-SamSum and GPT-4-32k models, chunk-based processing framework, five policy formulations, ROUGE evaluation methodology, Expected Latency metric

**Medium Confidence**: De-identification reversal implementation, sliding window prefix handling, model-based quality estimation thresholds, GPT-4 prompts for policy variations

**Low Confidence**: Complete GPT-4 implementation details for fully incremental and full rewriting modes, exact de-identification mappings, processing redundancy calculations

## Next Checks

1. Implement sliding window policy with prefix forcing on synthetic transcript to verify output generation beyond existing summary and correct chunk rotation

2. Develop and validate model-based quality estimation using token probabilities from BART-large-SamSum, testing correlation between estimated quality and actual summary performance

3. Verify Expected Latency formula implementation by calculating latency on simple meeting transcript with known WRITE event timings and confirming results match theoretical expectations