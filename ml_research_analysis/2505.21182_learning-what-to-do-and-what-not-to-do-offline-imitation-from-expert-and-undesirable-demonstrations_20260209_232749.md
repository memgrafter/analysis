---
ver: rpa2
title: 'Learning What to Do and What Not To Do: Offline Imitation from Expert and
  Undesirable Demonstrations'
arxiv_id: '2505.21182'
source_url: https://arxiv.org/abs/2505.21182
tags:
- expert
- learning
- dataset
- objective
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContraDICE, a novel offline imitation learning
  framework that effectively leverages both expert and undesirable demonstrations.
  The core innovation is a training objective formulated as the difference of two
  KL divergences, enabling the agent to both imitate expert behavior and explicitly
  avoid undesirable behaviors.
---

# Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations

## Quick Facts
- arXiv ID: 2505.21182
- Source URL: https://arxiv.org/abs/2505.21182
- Authors: Huy Hoang; Tien Mai; Pradeep Varakantham; Tanvi Verma
- Reference count: 40
- One-line primary result: ContraDICE consistently outperforms state-of-the-art baselines, particularly with limited expert data and mixed-quality demonstrations

## Executive Summary
This paper introduces ContraDICE, a novel offline imitation learning framework that leverages both expert demonstrations and undesirable demonstrations through a difference-of-KL-divergences objective. The method enables agents to simultaneously imitate expert behavior while explicitly avoiding undesirable behaviors, achieving strong performance across 18 MuJoCo and Adroit tasks. The key innovation is formulating the learning objective as the difference between two KL divergences, which under certain conditions becomes convex and enables stable, non-adversarial optimization.

## Method Summary
ContraDICE optimizes a difference of KL divergences over state-action visitation distributions of expert and undesirable demonstrations. The method trains binary discriminators to estimate occupancy ratios, then alternates between updating a Q-function and value function using a tractable surrogate objective derived from Lagrangian duality. The final policy is extracted via Q-weighted behavioral cloning from the union dataset. The framework operates under a convexity condition (α ≤ 1) that enables practical non-adversarial training, and uses a lower-bound surrogate to avoid numerical instability from exponential terms.

## Key Results
- ContraDICE achieves near-expert performance with as few as 3-5 expert trajectories
- The method consistently outperforms state-of-the-art baselines across 18 MuJoCo and Adroit tasks
- Strong performance demonstrated in both locomotion (MuJoCo) and manipulation (Adroit) domains
- Effective utilization of undesirable demonstrations while maintaining expert imitation quality

## Why This Works (Mechanism)

### Mechanism 1: Difference-of-KL-Divergences Objective
A training objective formulated as the difference between two KL divergences—one minimizing distance to expert behavior, one maximizing distance from undesirable behavior—enables simultaneous imitation and explicit avoidance in a unified framework. The objective $f(d_\pi) = D_{KL}(d_\pi \| d_G) - \alpha D_{KL}(d_\pi \| d_B)$ combines standard imitation with repulsion, capturing behavioral contrasts at the trajectory level.

### Mechanism 2: Convexity Under Balance Constraint (α ≤ 1)
When the weight on the bad-data divergence term satisfies $\alpha \leq 1$, the overall difference-of-convex objective becomes convex in the occupancy distribution $d_\pi$, enabling stable, non-adversarial optimization. This convexity permits reformulation via Lagrangian duality into a tractable Q-learning problem without minimax adversarial dynamics.

### Mechanism 3: Lower-Bound Surrogate via Exponential Linearization
Replacing the exponential term in the dual objective with its first-order Taylor lower bound ($e^t \geq t + 1$) yields a surrogate objective that remains a valid lower bound while eliminating numerical instability and preserving convex optimization structure. This linearization produces an objective that is linear in $Q$ and concave in $\pi$.

## Foundational Learning

- **KL Divergence and Occupancy Measures**: The entire objective is expressed in terms of KL divergences between state-action visitation distributions. Quick check: Can you explain why minimizing $D_{KL}(d_\pi \| d_G)$ is different from behavioral cloning that minimizes $-\mathbb{E}[\log \pi(a|s)]$?

- **Lagrangian Duality for Constrained RL**: The convex objective over $d_\pi$ includes a linear constraint enforcing that $d_\pi$ is a valid occupancy measure. Duality recasts this as unconstrained Q-learning. Quick check: In the dual formulation, what role does the Q-function play relative to the Lagrange multiplier?

- **Soft Bellman Operators and Maximum Entropy RL**: The method adopts the MaxEnt framework with soft value functions $V_Q^\pi(s) = \mathbb{E}_\pi[Q(s,a) - \beta \log \frac{\pi(a|s)}{\mu_U(a|s)}]$. Quick check: How does the soft Bellman operator differ from the standard Bellman operator, and why does entropy regularization help in offline IL?

## Architecture Onboarding

- **Component map**: Discriminators cG, cB -> Q-function Q_ω -> Value function V_ν -> Policy π_θ
- **Critical path**: (1) Train discriminators on labeled BG, BB vs. union BU to estimate occupancy ratios, (2) Compute Ψ(s,a) = log(cG/(1-cG)) - α·log(cB/(1-cB)), (3) Alternate: minimize Q-loss (Eq. 6), minimize V-loss (Extreme-V), update policy via Q-weighted BC (Eq. 8)
- **Design tradeoffs**: α ∈ [0,1) controls avoidance emphasis; β (temperature) controls softmax sharpness; surrogate vs. clipped original trades tightness for stability
- **Failure signatures**: Exploding loss (check discriminator outputs), policy collapses to uniform (β too high or Q not differentiating), performance degrades with more bad data (α too low or misclassifying discriminators), NaN during V-update (ensure Q-V scale compatibility)
- **First 3 experiments**: (1) Sanity check on toy MDP with known optimal and bad policies, (2) Ablation on α with fixed datasets on Hopper-Random+Expert, (3) Compare policy extraction methods: Q-weighted vs. advantage-weighted BC

## Open Questions the Paper Calls Out

**Open Question 1**: Can the convexity constraint α ≤ 1 be theoretically relaxed while preserving tractable optimization? The paper notes this represents a promising direction but the naive adaptation for α ≥ 1 performs poorly.

**Open Question 2**: Can ContraDICE be extended to handle noisy or weakly labeled demonstrations where the boundary between expert, unlabeled, and undesirable behaviors is uncertain? The current method relies on clean separation of datasets.

**Open Question 3**: Can data filtering mechanisms be integrated into ContraDICE to improve scaling with large expert datasets? The paper suggests this could help compete with ILID in scenarios with abundant expert data.

**Open Question 4**: Under what theoretical conditions does advantage-weighted BC recover better policies than the Q-weighted BC objective? The paper shows empirical differences despite theoretical equivalence claims.

## Limitations
- The convexity guarantee requires α ≤ 1, limiting the weight on avoiding bad behaviors
- Assumes correct labeling of good vs. bad demonstrations—mislabeling could cause the agent to imitate bad behaviors or avoid good ones
- Performance depends on accurate discriminator estimates and good data coverage

## Confidence

**High confidence**: The difference-of-KL-divergences objective is mathematically well-specified and the empirical advantage over baselines is demonstrated. The convexity result under α ≤ 1 follows directly from the proof structure.

**Medium confidence**: The practical effectiveness of the lower-bound surrogate is supported by ablation studies, but the approximation quality depends on training dynamics not fully characterized. Generalization beyond KL divergences is stated but not empirically validated.

**Low confidence**: Scalability to very large unlabeled datasets and robustness to partially mislabeled demonstrations are not extensively tested. Performance when good and bad behaviors are semantically similar is unclear.

## Next Checks

1. **Convexity boundary test**: Systematically vary α ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 0.99, 1.01, 1.2} on Hopper-Random+Expert and plot normalized score vs. α to verify performance degrades sharply at α ≥ 1.

2. **Surrogate tightness analysis**: Track the soft Bellman residual ||T^π[Q] - Q|| throughout training for both original (clipped) and surrogate objectives, plotting residuals alongside performance to assess bound quality.

3. **Mislabeled demonstration robustness**: Create corrupted versions of BB by randomly flipping 10%, 30%, and 50% of trajectory labels (good ↔ bad), then measure performance degradation to test sensitivity to correct demonstration labeling.