---
ver: rpa2
title: The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation
  and Safety in LLMs
arxiv_id: '2510.07775'
source_url: https://arxiv.org/abs/2510.07775
tags:
- refusal
- safety
- heads
- arxiv
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates a previously overlooked trade-off in large
  language model alignment: enhancing truthfulness to reduce hallucinations can unintentionally
  weaken safety alignment, making models more susceptible to harmful prompts. The
  root cause is that hallucination and refusal behaviors share overlapping internal
  representations, particularly in certain attention heads.'
---

# The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs

## Quick Facts
- arXiv ID: 2510.07775
- Source URL: https://arxiv.org/abs/2510.07775
- Reference count: 33
- Primary result: Proposed method reduces harmful outputs (attack success rates from ~9% to ~0%) while maintaining or improving task utility

## Executive Summary
This paper investigates a previously overlooked trade-off in large language model alignment: enhancing truthfulness to reduce hallucinations can unintentionally weaken safety alignment, making models more susceptible to harmful prompts. The root cause is that hallucination and refusal behaviors share overlapping internal representations, particularly in certain attention heads. To address this, the authors propose a method using sparse autoencoders to disentangle refusal and hallucination features, then preserve the refusal subspace during fine-tuning via orthogonalization. Evaluated on commonsense reasoning and harmful benchmarks, their approach significantly reduces harmful outputs while maintaining or improving task utility.

## Method Summary
The authors propose a method to disentangle refusal and hallucination features using sparse autoencoders (SAEs) trained on attention head activations. They identify overlapping hallucination/refusal heads via contrastive influence analysis, then use gradient attribution to extract refusal-specific features. During fine-tuning, they orthogonalize gradient updates against the refusal subspace to preserve safety while allowing task-specific learning. The approach is evaluated on LLaMA3-8B-Instruct and Qwen2.5-7B-Instruct models using CommonsenseQA for fine-tuning and various safety/utility benchmarks for evaluation.

## Key Results
- Attack success rates reduced from ~9% to ~0% on harmful prompts
- Task utility maintained or improved on commonsense reasoning benchmarks
- Method successfully preserves refusal behavior while reducing hallucination

## Why This Works (Mechanism)

### Mechanism 1: Representational Overlap of Refusal and Hallucination Features
- Claim: Hallucination and refusal behaviors share overlapping internal representations in specific attention heads, creating a trade-off where enhancing truthfulness inadvertently weakens safety alignment.
- Mechanism: Interventions that suppress hallucination-related features simultaneously suppress refusal-related features due to their co-localization in the same attention heads.
- Evidence: Head overlap analysis shows significant overlap between hallucination and refusal heads, and patching these heads degrades refusal behavior.

### Mechanism 2: Gradient Orthogonalization Preserves Refusal Subspace
- Claim: Orthogonalizing gradient updates against a refusal subspace during fine-tuning preserves safety alignment while allowing task-specific learning.
- Mechanism: By projecting gradients to be orthogonal to the refusal subspace, fine-tuning updates improve task utility without degrading refusal behavior.
- Evidence: Implementation details show successful preservation of refusal features during LoRA fine-tuning.

### Mechanism 3: Sparse Autoencoder Disentanglement Enables Feature-Level Control
- Claim: Sparse autoencoders can disentangle polysemantic representations in attention heads, isolating refusal and hallucination features for targeted intervention.
- Mechanism: SAEs decompose head activations into sparse latent features, allowing identification and preservation of refusal-specific features.
- Evidence: SAE training on attention outputs successfully identifies distinct refusal and hallucination features.

## Foundational Learning

- **Concept: Polysemanticity in Neural Networks**
  - Why needed here: Understanding why attention heads can encode multiple features simultaneously is crucial to grasping why disentanglement is necessary.
  - Quick check question: Can a single neuron or attention head in an LLM encode multiple distinct behaviors? If so, how does this complicate targeted interventions?

- **Concept: Sparse Autoencoders (SAEs)**
  - Why needed here: SAEs are the tool used to disentangle features, so understanding their architecture and purpose is essential.
  - Quick check question: How does a sparsity constraint in an autoencoder help isolate individual features in model activations?

- **Concept: Gradient Orthogonalization**
  - Why needed here: The proposed fine-tuning method projects gradients orthogonal to a refusal subspace.
  - Quick check question: If you project a gradient vector to be orthogonal to a subspace, what happens to the component of the gradient that lies within that subspace?

## Architecture Onboarding

- **Component map**:
  - Data (CommonsenseQA, harmful prompts) -> Model (LLaMA3-8B-Instruct/Qwen2.5-7B) -> Head Analysis (identify overlap) -> SAE Training (1B tokens) -> Feature Extraction (gradient attribution) -> Orthogonalization (LoRA fine-tuning)

- **Critical path**:
  1. Identify overlapping hallucination/refusal heads via contrastive influence analysis
  2. Train SAE on these heads' activations
  3. Extract refusal features from SAE latents using gradient attribution
  4. During fine-tuning, project gradients orthogonal to refusal subspace

- **Design tradeoffs**:
  - SAE training cost vs. feature quality
  - Orthogonalization strength vs. learning capacity
  - Head overlap set size vs. intervention precision

- **Failure signatures**:
  - Task utility drops significantly after fine-tuning
  - Attack success rate on harmful benchmarks remains high
  - SAE features fail to correlate with refusal behavior

- **First 3 experiments**:
  1. Replicate head overlap analysis using contrastive influence scoring
  2. Validate SAE feature extraction with gradient attribution
  3. Implement orthogonalized fine-tuning and evaluate safety/utility trade-off

## Open Questions the Paper Calls Out

- How does the representational overlap manifest across different model architectures and layers beyond the specific attention heads identified?
- Can the refusal subspace be preserved effectively during fine-tuning without relying on Sparse Autoencoders?
- Do other alignment methods induce similar knowledge suppression effects, and can the gradient orthogonalization approach generalize to them?

## Limitations

- SAE training adds computational overhead and complexity
- Method relies on specific head overlap analysis that may not generalize to all architectures
- Limited external validation beyond the studied model architectures

## Confidence

- **High Confidence**: The existence of a safety-utility trade-off in LLM alignment
- **Medium Confidence**: The mechanism of overlapping hallucination/refusal features as the root cause
- **Low Confidence**: The effectiveness of SAE-based disentanglement and gradient orthogonalization

## Next Checks

1. Conduct ablation studies by selectively patching identified overlapping heads to confirm direct causation of safety degradation.

2. Validate that SAE-extracted refusal features are truly distinct from hallucination features by ablating them and measuring impact on refusal behavior.

3. Test the orthogonalization method on diverse LLM architectures to assess robustness and identify model-specific limitations.