---
ver: rpa2
title: Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches
arxiv_id: '2509.25555'
source_url: https://arxiv.org/abs/2509.25555
tags:
- learning
- server
- training
- clients
- bsfl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses scalability and security challenges in collaborative
  learning frameworks like Federated Learning (FL) and Split Learning (SL). SL suffers
  from slow training times, while FL imposes heavy computational burdens on clients.
---

# Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches

## Quick Facts
- **arXiv ID:** 2509.25555
- **Source URL:** https://arxiv.org/abs/2509.25555
- **Reference count:** 31
- **Primary result:** Proposed BSFL improves resilience to data poisoning attacks by 62.7% while maintaining superior performance over standard SplitFed Learning

## Executive Summary
This paper addresses scalability and security limitations in collaborative learning frameworks by proposing two novel architectures: Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning (BSFL). SSFL improves scalability by distributing the SplitFed Learning server workload across multiple parallel shards, achieving 31.2% better performance and 85.2% faster round completion times. BSFL further enhances security by replacing the centralized server with a blockchain-based architecture and committee-driven consensus mechanism, creating the first end-to-end decentralized SplitFed Learning system that resists data poisoning attacks.

## Method Summary
The authors combine Split Learning (SL) and Federated Learning (FL) into SplitFed Learning (SFL), then enhance it through two approaches. SSFL distributes clients across parallel shard servers to reduce computational bottlenecks and improve scalability. BSFL replaces the central FL server with a blockchain ledger and smart contracts, using committee consensus to validate and aggregate model updates. Both approaches are implemented using PyTorch for ML operations, Flask for communication, and Hyperledger Fabric for blockchain functionality, tested on Fashion MNIST with non-IID data distribution across nodes.

## Key Results
- SSFL achieves 31.2% performance improvement and 85.2% reduction in round completion time compared to standard SFL
- BSFL increases resilience to data poisoning attacks by 62.7% while maintaining superior overall performance
- BSFL successfully implements the first end-to-end decentralized SplitFed Learning system using blockchain technology

## Why This Works (Mechanism)

### Mechanism 1: Workload Parallelization via Sharding
Distributing clients across parallel "shard" servers significantly reduces round completion time compared to monolithic SplitFed Learning. The dataset and model allow for parallel processing without dependency conflicts between shards during a training round.

### Mechanism 2: Learning Rate Balancing via Hierarchical Aggregation
Aggregating shard-level models using a Federated server (FedAvg) mitigates the learning rate imbalance inherent in Split Learning, stabilizing convergence by effectively "smoothing out" each SL server's model updates.

### Mechanism 3: Decentralized Filtering via Committee Consensus
Replacing a central validation server with a blockchain committee removes single points of failure and filters malicious updates. Committee members evaluate each other's model updates using validation metrics, selecting only top-performing models for aggregation.

## Foundational Learning

- **Model Splitting (Split Learning):** Understanding where the model is "cut" is essential to configuring the shards. *Quick check:* Can you identify which layers forward pass on the client and which on the server in the architecture table?

- **Federated Averaging (FedAvg):** This is the mathematical glue for both SSFL (aggregating shards) and BSFL (aggregating committee winners). *Quick check:* If one shard updates 1000 times and another only 10, how does FedAvg handle this imbalance?

- **Blockchain Consensus & Smart Contracts:** BSFL relies on deterministic execution of code (smart contracts) to replace the human/operator element of the central server. *Quick check:* What happens to the training cycle if the smart contract gas fees exceed the budget or if the network clogs?

## Architecture Onboarding

- **Component map:** Clients -> Shard Servers (SFL Servers) -> Aggregation Layer (Central FL Server for SSFL, Blockchain Ledger + Smart Contracts for BSFL)

- **Critical path:**
  1. Handshake: Client assigned to Shard (SSFL) or via Smart Contract (BSFL)
  2. Local Round: Forward pass -> Smashed data to Shard Server -> Gradients back to Client
  3. Global Update: Shard servers push weights to Central FL Server (SSFL) or Ledger (BSFL) -> Aggregation -> Broadcast

- **Design tradeoffs:**
  - **Latency vs. Security:** SSFL offers lowest latency (85.2% faster) but trusts central server. BSFL adds ~6x latency overhead but removes central trust assumption and resists poisoning.
  - **Scalability constant:** Increasing shards reduces per-server load (SSFL) but increases blockchain transaction volume (BSFL).

- **Failure signatures:**
  - **SSFL Stall:** If Central FL server crashes, entire system halts (Single Point of Failure)
  - **BSFL Drift:** If validation data on committee nodes is biased, "best" model selected will be biased, causing global model drift
  - **Gradient Mismatch:** If client and server learning rates are not balanced by aggregation layer, validation loss will oscillate violently

- **First 3 experiments:**
  1. Baseline Reproduction: Implement standard SFL with 9 nodes to establish vanishing accuracy and high round-time baseline
  2. SSFL Scalability Test: Increase clients to 36 and shard across 6 servers. Measure if round completion time drops proportionally
  3. BSFL Poison Injection: Implement BSFL with 47% malicious nodes submitting random weights. Verify committee scoring excludes them (Test Loss should remain ~0.325)

## Open Questions the Paper Calls Out

### Open Question 1
Can SSFL and BSFL be extended to support multi-part model splits (three or more segments) to enhance client-side privacy without imposing prohibitive computational costs on resource-constrained devices? The current study restricts implementation to a two-part split, leaving scalability and efficiency of multi-part splits unverified.

### Open Question 2
How can the BSFL evaluation mechanism be effectively adapted for generative applications where traditional validation loss is inapplicable? The current BSFL framework relies on validation loss for committee consensus; feasibility of integrating unsupervised metrics like FID into blockchain evaluation smart contracts remains unexplored.

### Open Question 3
How does a hardware-aware committee selection algorithm compare to the proposed score-based selection in terms of fairness and convergence speed? The current implementation selects committee members based on model performance scores, potentially forcing resource-constrained nodes to perform heavy server-side duties.

## Limitations

- Performance improvements rely on controlled parallel processing without inter-shard dependencies
- Learning rate balancing claim lacks explicit validation in presence of highly Non-IID data distributions
- Security guarantees against coordinated malicious attacks assume honest majority without quantifying exact threshold required

## Confidence

- **High:** Architectural description of SSFL and BSFL is internally consistent and clearly articulated
- **Medium:** Performance improvements supported by experimental results but lack detailed hyperparameter specifications
- **Low:** Security guarantees against coordinated malicious attacks assume honest majority without quantifying exact threshold required

## Next Checks

1. **Parameter Sensitivity:** Test SSFL performance across different learning rates and batch sizes to identify optimal configurations and verify robustness of scalability claims

2. **Non-IID Stress Test:** Evaluate SSFL and BSFL under extreme data heterogeneity (Dirichlet concentration approaching 0) to assess model convergence and performance degradation

3. **Committee Attack Surface:** Simulate Byzantine behavior in BSFL where malicious nodes collude to manipulate validation scores, measuring exact threshold at which committee consensus fails