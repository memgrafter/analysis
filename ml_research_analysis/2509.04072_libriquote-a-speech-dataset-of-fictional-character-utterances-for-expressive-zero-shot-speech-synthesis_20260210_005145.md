---
ver: rpa2
title: 'LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive
  Zero-Shot Speech Synthesis'
arxiv_id: '2509.04072'
source_url: https://arxiv.org/abs/2509.04072
tags:
- speech
- expressive
- audio
- information
- quotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LibriQuote is a large-scale speech dataset derived from audiobooks,
  designed for training and benchmarking expressive zero-shot text-to-speech (TTS)
  systems. It contains 18K hours of speech: 12.7K hours of neutral narration and 5.3K
  hours of expressive character quotations.'
---

# LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis

## Quick Facts
- arXiv ID: 2509.04072
- Source URL: https://arxiv.org/abs/2509.04072
- Reference count: 37
- Primary result: LibriQuote is a large-scale speech dataset derived from audiobooks, designed for training and benchmarking expressive zero-shot text-to-speech (TTS) systems.

## Executive Summary
LibriQuote is a large-scale speech dataset derived from audiobooks, designed for training and benchmarking expressive zero-shot text-to-speech (TTS) systems. It contains 18K hours of speech: 12.7K hours of neutral narration and 5.3K hours of expressive character quotations. Each quotation is supplemented with contextual information and pseudo-labels of speech verbs and adverbs. A challenging 7.5-hour test set is provided for evaluating expressive synthesis from neutral reference speech. Objective and subjective evaluations show that modern TTS systems fail to synthesize speech as expressive and natural as ground truth, and fine-tuning on LibriQuote significantly improves speech intelligibility.

## Method Summary
The dataset was created by extracting and aligning character quotations from audiobooks using automatic speech recognition and text alignment techniques. Each quotation was labeled with its surrounding narrative context, along with automatically generated speech verb and adverb pseudo-labels to capture expressive cues. The dataset is split into training, validation, and a challenging test set specifically designed to evaluate expressive zero-shot synthesis from neutral speech.

## Key Results
- LibriQuote contains 18K hours of speech: 12.7K hours of neutral narration and 5.3K hours of expressive character quotations
- Modern TTS systems fail to synthesize speech as expressive and natural as ground truth when evaluated on the challenging test set
- Fine-tuning on LibriQuote significantly improves speech intelligibility in downstream TTS models

## Why This Works (Mechanism)
The dataset leverages the rich expressive content found in audiobooks, where character quotations naturally exhibit a wide range of emotions and speaking styles. By pairing these expressive utterances with their neutral narrative context and automatically extracted expressive labels, the dataset provides a structured way for TTS models to learn the mapping between neutral and expressive speech. The contextual information helps models understand the situational and emotional cues that drive expressiveness.

## Foundational Learning
- **Speech alignment and segmentation**: Needed to accurately extract character quotations from continuous audiobook recordings. Quick check: verify alignment accuracy using manual spot-checks on sample utterances.
- **Automatic speech recognition (ASR)**: Used to generate pseudo-labels for speech verbs and adverbs. Quick check: measure ASR error rate on the dataset's domain-specific vocabulary.
- **Expressive speech modeling**: Core concept for capturing the emotional and stylistic variations in character speech. Quick check: compare expressiveness metrics before and after fine-tuning on LibriQuote.

## Architecture Onboarding

**Component Map**: Audiobook recordings -> Speech alignment & segmentation -> ASR-based labeling -> Dataset construction -> TTS training & evaluation

**Critical Path**: The most critical components are the speech alignment and ASR-based labeling, as errors in these stages propagate to the quality of the final dataset and downstream TTS performance.

**Design Tradeoffs**: The use of automatic labeling enables large-scale dataset creation but introduces potential noise in the pseudo-labels. Manual annotation would improve label quality but is prohibitively expensive at this scale.

**Failure Signatures**: Poor speech alignment leads to mismatched text-speech pairs, while ASR errors in labeling can misguide the model's understanding of expressive cues. These manifest as unnatural prosody or incorrect emotional tone in synthesized speech.

**First Experiments**:
1. Train a baseline TTS model on LibriQuote and evaluate expressiveness on the test set.
2. Fine-tune an existing TTS model on LibriQuote and measure improvements in speech intelligibility.
3. Perform ablation studies by removing contextual information or pseudo-labels to assess their impact on synthesis quality.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on automatically extracted pseudo-labels for speech verbs and adverbs, which may contain errors affecting TTS training
- Potential biases introduced by audiobook narration styles and character representations that may not generalize to other domains
- Subjective evaluations limited in scale and demographic diversity of raters, affecting generalizability of expressiveness and naturalness claims

## Confidence
- High confidence: The dataset creation methodology and basic statistics (18K hours, split between neutral narration and expressive quotations) are well-documented and reproducible.
- Medium confidence: Claims about the expressiveness and naturalness of synthesized speech compared to ground truth are supported by evaluations, but the subjective nature of these assessments introduces some uncertainty.
- Medium confidence: The improvement in speech intelligibility through fine-tuning is demonstrated, but the evaluation may not capture all aspects of intelligibility across diverse linguistic contexts.

## Next Checks
1. Conduct a detailed error analysis of the automatically generated speech verb and adverb pseudo-labels to quantify their accuracy and impact on TTS performance.
2. Expand subjective evaluations to include a more diverse set of raters across different demographics and linguistic backgrounds to assess the generalizability of the expressiveness and naturalness claims.
3. Test the robustness of the claimed intelligibility improvements by evaluating fine-tuned models on out-of-domain speech data and across multiple TTS architectures.