---
ver: rpa2
title: Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex
  Constraints
arxiv_id: '2504.15243'
source_url: https://arxiv.org/abs/2504.15243
tags:
- constraint
- penalty
- learning
- optimization
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses stochastic non-convex optimization with weakly-convex
  constraints, a problem prevalent in machine learning applications. Existing methods
  often suffer from slow convergence rates or rely on double-loop designs.
---

# Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints

## Quick Facts
- arXiv ID: 2504.15243
- Source URL: https://arxiv.org/abs/2504.15243
- Reference count: 40
- Single-loop penalty-based algorithm achieves O(ε⁻⁶) complexity for weakly-convex constrained stochastic non-convex optimization

## Executive Summary
This paper tackles stochastic non-convex optimization problems with weakly-convex constraints, which are prevalent in machine learning applications like fair learning and continual learning. Existing methods typically suffer from slow convergence rates or require computationally expensive double-loop designs. The authors propose a novel single-loop penalty-based stochastic algorithm that uses a hinge-based penalty function with a constant penalty parameter. This approach achieves a state-of-the-art complexity of O(ε⁻⁶) for finding approximate KKT solutions, significantly improving upon existing methods that often require O(ε⁻¹⁰) or double-loop structures.

## Method Summary
The authors develop a single-loop penalty-based stochastic algorithm for optimizing non-convex objectives subject to weakly-convex constraints. The key innovation is the use of a non-smooth hinge-based penalty function, which preserves weak convexity while allowing for efficient sampling and a constant penalty parameter. This contrasts with traditional smooth penalty functions that require diminishing penalty parameters and often lead to slower convergence. The algorithm performs gradient-based updates on both the objective and the penalty term in a single loop, eliminating the need for expensive inner optimization loops. The method is further extended to handle finite-sum coupled compositional objectives through a separable coupling structure, establishing improved complexity bounds over existing approaches.

## Key Results
- Achieves O(ε⁻⁶) complexity for finding approximate KKT solutions, improving upon existing O(ε⁻¹⁰) methods
- Single-loop design eliminates computational overhead of double-loop methods
- Experimental results on fair learning with ROC fairness constraints and continual learning with non-forgetting constraints demonstrate superior performance compared to baselines
- The hinge-based penalty function preserves weak convexity while enabling efficient sampling

## Why This Works (Mechanism)
The method works by leveraging a non-smooth hinge penalty function that maintains weak convexity of the constrained problem while allowing for a constant penalty parameter. This is crucial because traditional smooth penalty functions typically require diminishing penalty parameters, which can slow convergence. The hinge function's structure enables efficient stochastic gradient computation while preserving the geometric properties needed for convergence analysis. By avoiding the need for inner optimization loops, the single-loop design significantly reduces computational overhead while still achieving favorable complexity bounds.

## Foundational Learning

**Weak convexity**: Functions that become convex after adding a quadratic term - needed because many ML constraints (like Wasserstein distances) are weakly but not strongly convex. Quick check: verify constraint Hessians have eigenvalues bounded below by -μ.

**KKT conditions for non-convex problems**: First-order optimality conditions that extend Lagrange multipliers to non-convex settings - needed because standard KKT don't apply to non-convex objectives. Quick check: confirm constraint qualifications hold at solution.

**Hinge-based penalty functions**: Non-smooth penalty terms that can maintain weak convexity - needed because smooth penalties typically destroy weak convexity structure. Quick check: verify the hinge parameter preserves weak convexity modulus.

## Architecture Onboarding

**Component map**: Stochastic gradient estimator -> Hinge penalty computation -> Single-loop update rule -> Approximate KKT solution

**Critical path**: The algorithm follows a single loop where at each iteration: (1) sample stochastic gradients of objective and constraints, (2) compute hinge penalty terms, (3) update iterates using combined gradient information. The critical path is the gradient computation and update step, as all other components depend on these.

**Design tradeoffs**: Single-loop vs double-loop (computational efficiency vs potential accuracy), hinge penalty vs smooth penalty (complexity vs smoothness properties), constant vs diminishing penalty parameter (simplicity vs potential tuning requirements).

**Failure signatures**: Slow convergence may indicate poor choice of penalty parameter, constraint violations suggest weak convexity assumptions are violated, numerical instability could arise from non-smooth penalty computations in high noise regimes.

**First experiments**: 1) Test on synthetic problems with known weakly-convex constraints to verify convergence rates, 2) Compare with double-loop methods on fair learning benchmarks, 3) Evaluate sensitivity to stochastic noise levels and batch sizes.

## Open Questions the Paper Calls Out

None

## Limitations
- The O(ε⁻⁶) complexity depends on problem-specific constants that are not explicitly characterized
- The finite-sum compositional extension assumes separable coupling structures that may not hold in many real-world applications
- Experimental comparisons focus on specific fairness and continual learning constraints, limiting generalizability to other weakly-convex constraint types

## Confidence

**Single-loop penalty method with constant parameter**: High
**O(ε⁻⁶) complexity guarantee**: Medium (depends on unspecified constants)
**Finite-sum compositional extension**: Medium (assumes separable structure)
**Empirical superiority over baselines**: Medium (limited experimental scope)

## Next Checks

1. Characterize explicitly how problem constants (Lipschitz parameters, weak convexity moduli) affect the O(ε⁻⁶) bound in practice
2. Test the method on weakly-convex constraints beyond fairness and forgetting (e.g., Wasserstein distance, group sparsity)
3. Evaluate sensitivity to stochastic noise levels and batch sizes in the hinge penalty computation