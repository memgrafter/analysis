---
ver: rpa2
title: Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline
  Reinforcement Learning in Care Coordination and Population Health Management
arxiv_id: '2509.16291'
source_url: https://arxiv.org/abs/2509.16291
tags:
- cost
- calibration
- value
- local
- deliberation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a lightweight offline reinforcement learning
  approach, TTL+ITD, for optimizing care coordination and population health management
  decisions in large Medicaid populations. The method augments trained policies with
  local neighborhood calibration (TTL) and inference-time deliberation (ITD) via a
  Q-ensemble that incorporates predictive uncertainty and time/effort cost.
---

# Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management

## Quick Facts
- arXiv ID: 2509.16291
- Source URL: https://arxiv.org/abs/2509.16291
- Reference count: 25
- Key outcome: Lightweight offline RL (TTL+ITD) achieves near-zero harm with 17.1 vs 0.05 normalized cost units for Medicaid care coordination

## Executive Summary
This study introduces TTL+ITD, a lightweight offline reinforcement learning framework for optimizing care coordination in large Medicaid populations. The approach combines test-time learning (TTL) via local conformal calibration with inference-time deliberation (ITD) using a Q-ensemble that incorporates predictive uncertainty and time/effort costs. TTL+ITD achieves stable value estimates while enabling predictable efficiency trade-offs, achieving near-zero harm with substantially lower expected effort costs compared to baselines. The method exposes transparent governance dials for neighborhood size and uncertainty/cost penalties without requiring retraining, supporting auditable and equitable deployment in strictly offline settings.

## Method Summary
TTL+ITD augments a trained policy with two inference-time components: local neighborhood calibration (TTL) that computes action-specific safety thresholds via kNN conformal prediction, and inference-time deliberation (ITD) that refines action selection using a Q-ensemble incorporating predictive uncertainty and effort costs. The framework trains a risk model to predict harm probabilities, uses global conformal calibration to set safety thresholds, then applies local kNN calibration for context-specific thresholds. A preference model is trained on the "safe" subset and blended with neighborhood priors. During inference, the system computes deliberation scores combining Q-values, uncertainty estimates, harm predictions, and action costs to select actions while masking unsafe options. Evaluation uses FQE and doubly robust estimators with bootstrap confidence intervals.

## Key Results
- TTL+ITD achieves near-zero harm with substantially lower expected effort costs (17.1 vs 0.05 normalized cost units) compared to baselines
- The approach provides stable value estimates while enabling predictable efficiency trade-offs through transparent governance dials
- Local conformal calibration (TTL) enables context-specific safety thresholds that maintain coverage while improving efficiency

## Why This Works (Mechanism)
TTL+ITD works by combining local neighborhood calibration with inference-time deliberation to balance safety and efficiency in offline RL settings. The local kNN calibration (TTL) provides context-specific safety thresholds that adapt to local data density, while the Q-ensemble deliberation (ITD) incorporates predictive uncertainty and effort costs to make more informed decisions. This two-stage approach allows the system to maintain safety guarantees while optimizing for efficiency, with governance parameters that can be adjusted without retraining.

## Foundational Learning
- **Conformal Prediction**: Provides finite-sample marginal coverage guarantees by calibrating prediction thresholds on held-out data; needed for safety-critical decisions where false positives have high cost
- **Offline RL Evaluation**: Uses FQE and doubly robust estimators to estimate policy value from historical data without online interaction; needed for healthcare settings where experimentation is ethically constrained
- **kNN Local Calibration**: Adapts global thresholds to local data density via neighborhood statistics; needed to handle varying data support across different patient contexts
- **Ensemble Uncertainty Quantification**: Uses bootstrap Q-ensembles to estimate predictive uncertainty; needed to identify high-variance regions where safety constraints should be stricter
- **Efficiency-Cost Tradeoffs**: Incorporates explicit effort cost terms into deliberation scores; needed to optimize resource allocation in large-scale population health management

## Architecture Onboarding

**Component Map**: JSON state -> Feature extraction -> kNN query -> Local τ_s(a) computation -> Preference model blending -> Deliberation score computation -> Action selection

**Critical Path**: During inference, the system processes JSON state snapshots through feature extraction, performs kNN queries to obtain local safety thresholds and neighborhood priors, blends these with the preference model output, computes deliberation scores using the Q-ensemble, and selects the optimal action while masking unsafe options.

**Design Tradeoffs**: The framework trades computational overhead at inference time (kNN queries, ensemble evaluations) for safety guarantees and efficiency optimization without requiring expensive retraining. The local calibration approach balances between conservative global thresholds and potentially unreliable local estimates in sparse regions.

**Failure Signatures**: 
- kNN locality issues when feature preprocessing doesn't match training (causing poor neighbor matches)
- Conformal coverage failure when neighborhoods are too small or sparse (local thresholds become unreliable)
- OPE instability when linear FQE models underfit high-dimensional features or limited data

**3 First Experiments**:
1. Verify feature consistency and kNN locality using synthetic data with matching schema to the original
2. Reproduce calibration coverage plots and local τ_s(a) stability as neighborhood size K varies
3. Confirm OPE estimates (FQE vs DR) and bootstrap CI widths for baseline and TTL+ITD policies

## Open Questions the Paper Calls Out
- **Prospective Evaluation**: The authors explicitly state that prospective evaluation remains the decisive next step, planning to run matched-control pilots to monitor visit rates, harm events, and staff satisfaction in live clinical deployment.
- **Conditional Coverage Guarantees**: While local kNN calibration yields empirical benefits, the paper notes it does not provide formal conditional coverage guarantees, with theoretical notes and open questions outlined in Appendix C.
- **Extension to High-Risk Interventions**: The framework is currently tuned for low-risk modalities, with future directions suggesting extension to high-risk interventions via model-based lookahead, though safety assurances for such settings remain unproven.

## Limitations
- Local kNN calibration lacks formal conditional coverage guarantees despite empirical benefits
- The method is validated only on retrospective data, with prospective clinical deployment still pending
- Current framework is optimized for low-risk interventions and may require modifications for high-stakes clinical decisions

## Confidence
- **Method Conceptual Soundness**: High - Uses standard conformal prediction, OPE estimators, and explicit efficiency/delay trade-offs
- **Reported Comparisons**: Medium - Comparisons appear sound but exact reproduction requires original data
- **Near-Zero Harm Claims**: Medium - Theoretical framework supports claims but requires exact reproduction for verification
- **Cost Reduction Claims**: Medium - Methodology appears sound but depends on accurate feature extraction and policy implementation

## Next Checks
1. Verify feature consistency and kNN locality with synthetic data matching the original schema
2. Reproduce calibration coverage plots and local τ_s(a) stability as neighborhood size K varies
3. Confirm OPE estimates (FQE vs DR) and bootstrap CI widths for baseline and TTL+ITD policies