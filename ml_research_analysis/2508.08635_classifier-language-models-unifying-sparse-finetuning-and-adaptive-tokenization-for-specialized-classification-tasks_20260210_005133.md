---
ver: rpa2
title: 'Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization
  for Specialized Classification Tasks'
arxiv_id: '2508.08635'
source_url: https://arxiv.org/abs/2508.08635
tags:
- finetuning
- tokens
- each
- token
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting small language models
  for specialized semantic classification tasks that require domain expertise and
  high throughput inference. The authors propose a novel approach combining task-specific
  tokenization with sparse parameter tuning, called AdaPT.
---

# Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks

## Quick Facts
- **arXiv ID**: 2508.08635
- **Source URL**: https://arxiv.org/abs/2508.08635
- **Reference count**: 33
- **Key outcome**: AdaPT achieves consistent outperformance across five semantic classification tasks by combining task-specific tokenization with sparse parameter tuning, outperforming LoRA, prefix tuning, and end-to-end finetuning while using half the training costs

## Executive Summary
This paper addresses the challenge of adapting small language models for specialized semantic classification tasks that require domain expertise and high throughput inference. The authors propose AdaPT, a novel approach combining task-specific tokenization with sparse parameter tuning that identifies and finetunes only the most sensitive parameters while leaving most pretrained weights unchanged. AdaPT introduces new task-specific tokens and selectively finetunes parameters measured by index of dispersion and gradient norm. The method consistently outperforms established baselines across five diverse semantic classification tasks, achieving greater stability and reduced training costs compared to end-to-end finetuning.

## Method Summary
AdaPT combines task-specific tokenization with sparse parameter tuning to adapt small language models for specialized classification tasks. The method first mines closed sequences from tokenized training data using the BIDE algorithm, filtering by frequency and unigram-normalized perplexity to select task-specific tokens (targeting â‰¤10% vocabulary expansion). For parameter selection, AdaPT computes sensitivity per module as the sum of index of dispersion and gradient norm, then finetunes only the top 10-20% most sensitive parameters plus new token embeddings. This approach is evaluated against end-to-end finetuning, LoRA, layer selection, and prefix tuning on five semantic classification tasks, demonstrating superior performance with reduced computational costs.

## Key Results
- AdaPT consistently outperforms end-to-end finetuning, LoRA, layer selection, and prefix tuning across five diverse semantic classification tasks
- Optimal performance achieved by tuning only 10-20% of parameters, with degradation observed when tuning 50% or 100%
- AdaPT achieves greater stability across multiple runs with different initializations compared to end-to-end finetuning
- Training costs reduced by approximately 50% compared to end-to-end finetuning while maintaining superior performance

## Why This Works (Mechanism)
AdaPT works by identifying the most task-relevant parameters and token representations while preserving the general knowledge in pretrained models. The sensitivity-based parameter selection ensures that only parameters most responsive to the target task are updated, preventing catastrophic forgetting of general knowledge. Task-specific tokenization allows the model to capture domain-specific constructs and patterns that generic tokenizers miss, particularly important for specialized classification tasks like biomedical text or scientific relation extraction. By combining these approaches, AdaPT achieves a balance between adaptation capability and parameter efficiency that neither pure tokenization nor pure parameter tuning can achieve alone.

## Foundational Learning
**Index of dispersion**: A measure of statistical dispersion calculated as variance divided by mean. In AdaPT, it quantifies how much a module's output varies relative to its average magnitude across training inputs. Needed to identify parameters that show high variability in response to task-specific patterns. Quick check: Compute I_M(D) for a module and verify it increases when the module processes diverse task-specific patterns.

**Gradient norm**: The magnitude of gradients computed during backpropagation. Used in AdaPT to measure how much each parameter contributes to task-specific learning signals. Needed to identify parameters that have strong gradients and thus are most influential for the task. Quick check: Calculate average gradient norm per parameter and confirm higher values indicate more important parameters for the task.

**BIDE algorithm**: A closed sequential pattern mining algorithm that finds all frequent subsequences in a dataset. In AdaPT, it identifies task-specific token sequences that appear frequently in training data. Needed to discover domain-specific multi-token constructs that generic tokenizers fail to capture. Quick check: Run BIDE on sample tokenized text and verify it finds contiguous and non-contiguous frequent patterns.

**Module granularity**: The level at which model components are defined for sensitivity analysis (e.g., individual layers, attention heads, or parameter groups). In AdaPT, this determines how sensitivity is computed and which parameters are selected for tuning. Needed to balance computational efficiency of sensitivity computation with granularity of parameter selection. Quick check: Compute sensitivity at different granularities and compare the resulting parameter sets and performance.

## Architecture Onboarding

**Component map**: Data -> BIDE Mining -> Token Expansion -> Parameter Sensitivity Computation -> Sparse Fine-tuning -> Classification Output

**Critical path**: Token mining and parameter sensitivity computation form the critical path, as they determine which parameters and tokens will be tuned. These steps must complete before the actual fine-tuning begins.

**Design tradeoffs**: The main tradeoff is between adaptation capability and parameter efficiency. Tuning too few parameters limits adaptation, while tuning too many risks overfitting and increased computational cost. The 10-20% tuning ratio represents the optimal balance observed across tasks.

**Failure signatures**: Performance degradation occurs when tuning >20% of parameters (overfitting), when task-specific tokens are not properly integrated into the tokenizer, or when sensitivity computation fails to identify truly important parameters due to poor module granularity definition.

**Three first experiments**:
1. Implement BIDE mining on BERT-tokenized training data and verify that selected sequences capture domain-specific patterns not present in the original vocabulary
2. Compute parameter sensitivity using both index of dispersion and gradient norm, then compare the top-k% parameters selected using different sensitivity metrics
3. Evaluate the impact of task-specific tokens alone (without parameter tuning) to isolate their contribution to performance improvement

## Open Questions the Paper Calls Out
None

## Limitations
- The BIDE sequence mining phase lacks precise frequency and perplexity threshold values, requiring experimental determination
- Module definition for sensitivity computation is ambiguous, potentially affecting which parameters are selected
- Handling of non-contiguous token sequences during insertion is underspecified, which could impact tokenization consistency

## Confidence
**Method performance claims (High confidence)**: Strong empirical support from comparative results across five diverse tasks with stability measurements and parameter count validation
**Tokenization methodology claims (Medium confidence)**: Clear methodology but lacks specific thresholds and has underspecified non-contiguous sequence handling
**Training efficiency claims (Medium confidence)**: Parameter count comparisons support 50% cost reduction, but actual computational savings depend on implementation details

## Next Checks
1. Implement the complete AdaPT pipeline on one task (e.g., Movie Reviews) with multiple threshold combinations for sequence mining to verify the 10% vocabulary expansion constraint and assess sensitivity to threshold choices
2. Compare parameter sensitivity rankings across different module granularities (individual layers vs. attention heads vs. parameter groups) to determine whether the choice of module definition significantly impacts performance
3. Measure actual training wall-clock time and memory usage for AdaPT versus end-to-end finetuning across different batch sizes to validate the claimed computational efficiency improvements in practice