---
ver: rpa2
title: 'LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities'
arxiv_id: '2504.16078'
source_url: https://arxiv.org/abs/2504.16078
tags:
- action
- arxiv
- reward
- actions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically examines why LLMs perform suboptimally
  in decision-making scenarios. It identifies three prevalent failure modes: greediness
  (overly favoring high-reward actions), frequency bias (repeatedly selecting the
  most frequent action regardless of reward), and the knowing-doing gap (models know
  how to solve a task but fail to act on that knowledge).'
---

# LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities

## Quick Facts
- arXiv ID: 2504.16078
- Source URL: https://arxiv.org/abs/2504.16078
- Reference count: 40
- This work systematically examines why LLMs perform suboptimally in decision-making scenarios and proposes RL Fine-Tuning as a solution

## Executive Summary
This paper investigates why large language models struggle with decision-making tasks, identifying three key failure modes: greediness, frequency bias, and the knowing-doing gap. The authors propose Reinforcement Learning Fine-Tuning (RLFT) using self-generated Chain-of-Thought rationales as an effective solution. Through experiments across multi-armed bandits, contextual bandits, and Tic-tac-toe, RLFT demonstrates significant improvements in exploration and decision quality, reducing regret by 30-50% compared to in-context learning.

## Method Summary
The paper systematically examines LLM decision-making limitations through controlled experiments across three synthetic environments. The authors first identify failure modes through behavioral analysis, then apply RL Fine-Tuning using self-generated Chain-of-Thought rationales as the training signal. The RLFT process involves generating COT reasoning for decisions, then fine-tuning the model using reinforcement learning to optimize these rationales for better decision outcomes. Various exploration mechanisms including ε-greedy, self-consistency, and exploration bonuses are evaluated to determine their effectiveness in improving LLM decision-making.

## Key Results
- RLFT reduces regret by 30-50% compared to in-context learning across tested environments
- RLFT improves action coverage by 12-20%, indicating better exploration
- Exploration bonuses prove particularly effective for fine-tuning LLMs in decision-making scenarios
- RLFT successfully narrows the knowing-doing gap by training models to act on their reasoning

## Why This Works (Mechanism)
The RLFT approach works by directly optimizing the model's decision-making process through reinforcement learning on self-generated rationales. By fine-tuning on the quality of Chain-of-Thought reasoning rather than just final answers, the model learns to generate more balanced exploration strategies and overcome its inherent biases toward greedy or frequency-based decisions. The use of self-generated rationales provides rich supervision signals that capture the model's reasoning process, allowing RL to shape not just outcomes but the underlying decision strategies.

## Foundational Learning
- **Reinforcement Learning Fine-Tuning**: Why needed - to optimize decision-making strategies beyond supervised learning; Quick check - compare reward signals and learning curves across different RL algorithms
- **Chain-of-Thought Reasoning**: Why needed - provides interpretable decision rationales for RL supervision; Quick check - evaluate COT quality before and after RLFT using human judgment
- **Multi-armed Bandit Theory**: Why needed - provides theoretical framework for evaluating exploration-exploitation tradeoffs; Quick check - measure regret and action coverage against theoretical bounds
- **Contextual Bandits**: Why needed - extends bandit problems to include state information; Quick check - verify policy improvement across varying context distributions
- **Exploration Mechanisms**: Why needed - different approaches to balancing exploration and exploitation; Quick check - compare ε-greedy, self-consistency, and bonus-based exploration in identical settings
- **Knowing-Doing Gap**: Why needed - identifies discrepancy between reasoning and action; Quick check - measure alignment between COT rationales and actual decisions before/after RLFT

## Architecture Onboarding
- **Component Map**: LLMs (base model) -> COT Generator -> RL Fine-Tuner -> Decision-Maker -> Environment (Bandit/Game) -> Reward Signal
- **Critical Path**: COT generation → reward evaluation → RL optimization → decision improvement → environment interaction
- **Design Tradeoffs**: Self-generated COT vs. human-annotated rationales (cost vs. quality), exploration mechanism selection (simplicity vs. effectiveness), RL algorithm choice (stability vs. sample efficiency)
- **Failure Signatures**: Over-reliance on greedy actions (high regret, low coverage), repetitive action selection (frequency bias), rational COT but suboptimal actions (knowing-doing gap)
- **Three First Experiments**:
  1. Compare regret curves for RLFT vs. in-context learning across increasing numbers of training episodes
  2. Measure action coverage distribution before and after RLFT to quantify exploration improvement
  3. Evaluate COT-rationale alignment scores to quantify knowing-doing gap reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation across three synthetic environments may not generalize to complex real-world decision-making scenarios
- RLFT approach requires reliable self-generated Chain-of-Thought rationales, which may not be feasible in all domains
- 30-50% regret reduction comes from controlled environments that may not capture practical application complexity

## Confidence
- High confidence: Three distinct failure modes are well-supported by empirical evidence across multiple experiments
- Medium confidence: Exploration bonuses being "particularly effective" requires additional comparative context
- Low confidence: Broad claim that RLFT "enhances LLMs' decision-making abilities" is overstated without evidence of generalization

## Next Checks
1. Test RLFT with self-consistency and other LLM-specific exploration mechanisms across a wider range of environments, including partially observable Markov decision processes and real-world recommendation systems
2. Evaluate whether the identified failure modes persist in larger models (beyond those tested) and whether RLFT effectiveness scales with model size
3. Conduct ablation studies to determine whether benefits come from RL fine-tuning itself or from specific choices of reward signal and COT generation method