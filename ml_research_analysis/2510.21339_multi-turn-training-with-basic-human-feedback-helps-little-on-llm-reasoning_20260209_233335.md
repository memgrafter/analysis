---
ver: rpa2
title: Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning
arxiv_id: '2510.21339'
source_url: https://arxiv.org/abs/2510.21339
tags:
- training
- multi-turn
- reasoning
- single-turn
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether multi-turn training with human
  feedback is necessary for enhancing reasoning capabilities in large language models
  (LLMs). While single-turn reinforcement learning is standard for training reasoning
  abilities, real-world applications often involve multi-turn interactions where users
  provide feedback to refine model outputs.
---

# Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning

## Quick Facts
- arXiv ID: 2510.21339
- Source URL: https://arxiv.org/abs/2510.21339
- Reference count: 7
- Multi-turn training with basic human feedback provides limited benefits and can impair reasoning capabilities for complete-information tasks

## Executive Summary
This study investigates whether multi-turn training with human feedback is necessary for enhancing reasoning capabilities in large language models (LLMs). While single-turn reinforcement learning is standard for training reasoning abilities, real-world applications often involve multi-turn interactions where users provide feedback to refine model outputs. The research compares single-turn training with three multi-turn strategies (UACR, ULCR, UADR) on GSM8K and finds that models trained in a single-turn setting generalize effectively to both single- and multi-turn inference scenarios. In contrast, multi-turn trained models exhibit significant degradation in single-turn reasoning performance with no clear advantage in multi-turn inference.

## Method Summary
The study compares single-turn versus multi-turn GRPO training on GSM8K using Qwen2.5-3B-Instruct as the base model. Three multi-turn strategies are evaluated: UACR (Update at All responses with Consistent Reward), ULCR (Update at Last response with Consistent Reward), and UADR (Update at All responses with Decay Reward). The training uses a simple human feedback prompt when answers are incorrect and evaluates performance using Pass@8 accuracy (8 independent samples) and 8-turn accuracy (sequential with feedback). All models are trained for 5 epochs with validation on 1319 samples.

## Key Results
- Single-turn trained models generalize effectively to both single- and multi-turn inference scenarios
- Multi-turn trained models show significant degradation in single-turn reasoning performance
- No clear advantage for multi-turn trained models in multi-turn inference settings
- UACR and ULCR show larger performance degradation than UADR

## Why This Works (Mechanism)
Not specified in the paper

## Foundational Learning
- **Single-turn GRPO**: Standard reinforcement learning with binary rewards based on final answer correctness
  - Why needed: Baseline comparison for evaluating multi-turn approaches
  - Quick check: Verify binary reward assignment and temperature=0 sampling
- **Multi-turn training strategies**: Three approaches for incorporating feedback across multiple turns
  - Why needed: To test whether sequential feedback improves reasoning over single-turn corrections
  - Quick check: Confirm feedback prompt injection and history concatenation
- **Pass@K vs K-turn evaluation**: Independent sampling versus sequential feedback scenarios
  - Why needed: To distinguish between generalization capabilities and turn-specific performance
  - Quick check: Ensure proper implementation of both evaluation metrics
- **Reward decay mechanisms**: Logarithmic decay in UADR versus binary rewards in UACR/ULCR
- **Model update granularity**: All-tokens versus last-tokens update strategies

## Architecture Onboarding
- **Component map**: GSM8K dataset -> VeRL framework (with interaction module) -> Qwen2.5-3B-Instruct -> GRPO training -> evaluation
- **Critical path**: Dataset → Multi-turn rollouts → Reward assignment → Model updates → Validation
- **Design tradeoffs**: Simple binary feedback versus rich constructive feedback; all-tokens versus last-tokens updates
- **Failure signatures**: Multi-turn trained models underperforming on Pass@K; no performance gap between training paradigms
- **First experiments**:
  1. Implement single-turn GRPO baseline with GSM8K
  2. Implement UACR multi-turn training with feedback injection
  3. Compare Pass@8 and 8-turn performance between single and multi-turn models

## Open Questions the Paper Calls Out
- Does multi-turn training benefit "incomplete information" tasks? The study focuses on complete-information tasks like math, but doesn't verify if conclusions apply to tasks requiring iterative information gathering.
- Would richer, constructive feedback alter the effectiveness of multi-turn training? The study uses basic binary feedback, which may not provide actionable guidance for reasoning correction.
- Do these findings scale to significantly larger model sizes? The study is limited to a 3B parameter model, and larger models may integrate historical context differently.

## Limitations
- Results are specific to mathematical problem-solving with complete information
- Evaluation focuses on binary correctness rewards rather than nuanced feedback
- Human feedback prompt is quite basic ("Your response is incorrect...")
- Limited to Qwen2.5-3B-Instruct model size

## Confidence
- **High confidence**: Single-turn trained models generalize effectively to multi-turn inference; multi-turn training degrades single-turn reasoning
- **Medium confidence**: No advantage for multi-turn trained models in multi-turn inference; varying degradation across strategies
- **Low confidence**: Findings extend to non-mathematical or incomplete-information tasks

## Next Checks
1. Test single-turn trained models on non-mathematical reasoning tasks (commonsense reasoning, code generation) to assess domain generalizability
2. Implement and compare more sophisticated human feedback mechanisms beyond binary correctness
3. Evaluate performance across different reward structures, including dense rewards and partial credit systems