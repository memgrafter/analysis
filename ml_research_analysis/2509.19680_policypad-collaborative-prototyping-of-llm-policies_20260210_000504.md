---
ver: rpa2
title: 'PolicyPad: Collaborative Prototyping of LLM Policies'
arxiv_id: '2509.19680'
source_url: https://arxiv.org/abs/2509.19680
tags:
- policy
- prototyping
- experts
- policies
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PolicyPad enables domain experts to collaboratively prototype\
  \ LLM policies by integrating UX prototyping methods like rapid iteration, scenario-based\
  \ testing, and real-time collaboration. In workshops with 22 mental health and legal\
  \ experts, PolicyPad led to 4\xD7 more novel policy statements than a baseline system,\
  \ with 51.9% novelty versus 18.2%."
---

# PolicyPad: Collaborative Prototyping of LLM Policies

## Quick Facts
- arXiv ID: 2509.19680
- Source URL: https://arxiv.org/abs/2509.19680
- Reference count: 40
- Primary result: PolicyPad led to 4× more novel policy statements than a baseline system (51.9% vs 18.2%)

## Executive Summary
PolicyPad is a system designed to enable domain experts to collaboratively prototype LLM policies through rapid iteration, scenario-based testing, and real-time collaboration. In workshops with 22 mental health and legal experts, PolicyPad facilitated the creation of novel policy statements that offered specific guidance on deferring to human experts, crisis procedures, and proactive information elicitation. Experts valued the system's heuristics for alignment, spotlight scenarios for collaboration, and direct model experimentation for informing edits.

## Method Summary
PolicyPad was evaluated through within-subjects studies with 22 domain experts (10 mental health, 12 legal) organized into 8 groups of 2-4 participants. Participants used PolicyPad and a baseline system in counterbalanced sessions to draft policies for two tasks: Tone and Guardrails. The study measured novelty of policy statements using human-AI joint evaluation against existing policies, and collected expert feedback through interviews. PolicyPad featured interactive scenario widgets, private testing sidebars, spotlight cards for group discussion, and automated suggestion generation based on edited responses.

## Key Results
- PolicyPad produced 51.9% novel policy statements versus 18.2% for the baseline system
- 100% of automated policy suggestions were accepted by experts
- Experts reported enhanced collaborative dynamics and valued the tight feedback loops between policy authoring and model testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tight feedback loops between policy authoring and model testing surface misalignments that asynchronous workflows miss.
- Mechanism: Experts draft statements → immediately regenerate model outputs on realistic scenarios → observe behavior gaps → iterate within seconds rather than days.
- Core assumption: Experts can translate observed behavior gaps into actionable policy language when given live signals.
- Evidence anchors: [abstract] enhanced collaborative dynamics and tight feedback loops; [section 3.2.1] experts unable to obtain signals without policy-informed model; [corpus] no direct corpus evidence.
- Break condition: If policy-to-behavior latency exceeds ~30 seconds, experts lose ability to connect edits to outcomes.

### Mechanism 2
- Claim: Embedding scenarios as interactive widgets in the shared editor creates a shared perceptual ground that replaces screen-sharing improvisation.
- Mechanism: Scenario widgets are inline in collaborative document; clicking opens private sidebar for testing; spotlight expands widget for group discussion and response editing.
- Core assumption: Visual anchoring of specific examples reduces cognitive load of context-switching between policy text and test results.
- Evidence anchors: [section 5.2.3-5.2.6] describes @-mention widgets and spotlight cards; P22 found this helped brainstorming; [section 7.1.2] baseline users improvised screen-sharing.
- Break condition: If widget rendering or sidebar synchronization lags under collaborative load, shared grounding degrades.

### Mechanism 3
- Claim: Converting edited model responses into suggested policy statements reduces the barrier between "I know what good behavior looks like" and "I can encode it."
- Mechanism: When users collaboratively edit a spotlighted response, reasoning model analyzes edits against current policy/heuristics and generates candidate statement.
- Core assumption: Experts can articulate behavioral corrections more easily than authoring abstract policy language from scratch.
- Evidence anchors: [section 5.2.6] automatic analysis generates policy statement; [section 7.1.2] 100% suggestions accepted; P17 found helpful when difficult to put thoughts into words.
- Break condition: If suggestion quality varies by domain or contradicts existing policy structure, users may reject or mistrust feature.

## Foundational Learning

- Concept: **Low-fidelity prototyping principles (from UX)**
  - Why needed here: Paper explicitly frames LLM policy prototyping as analogous to low-fidelity UX prototyping—focus on high-level intent, defer detailed wording, iterate fast.
  - Quick check question: Can you explain why a policy prototype that emphasizes "defer to human expert in X situations" is low-fidelity, while a production policy with exact jurisdictional citations is high-fidelity?

- Concept: **LLM system prompts as policy injection**
  - Why needed here: PolicyPad feeds drafted policy into model as system prompt; understanding this helps debug why policy changes may not immediately change behavior.
  - Quick check question: If you append new policy statement at end of document but model ignores it, what are three possible causes related to how system prompts are processed?

- Concept: **Real-time collaborative editing (CRDTs/OT)**
  - Why needed here: PolicyPad uses TipTap Cloud for real-time collaboration; understanding conflict resolution helps anticipate where concurrent edits may diverge.
  - Quick check question: When two users simultaneously edit a spotlighted scenario response, what determines which edit "wins," and how might this affect the policy suggestion that follows?

## Architecture Onboarding

- Component map: React + TypeScript + TipTap (collaborative editor) → TipTap Cloud (real-time sync) → Llama 3.3 70B Instruct (policy-informed responses) → o1-mini (suggestion generation) → GPT-4o (auxiliary tasks)

- Critical path: User opens scenario → sidebar loads conversation → user regenerates response with current policy → user embeds scenario widget → group clicks → opens sidebar for everyone → group edits response → saves → suggestion engine generates policy statement → user accepts/integrates → user snapshots policy → heuristic evaluator flags unmet heuristics → group iterates

- Design tradeoffs:
  - Private sidebar vs. shared testing: Chose private to allow independent exploration, but means insights must be manually surfaced via widgets/spotlights
  - Low-fidelity policy focus: Defers production-level wording, which speeds iteration but requires post-hoc refinement before deployment
  - Synchronous collaboration only: Does not support async contribution; scaling requires hybrid sync-then-async approach

- Failure signatures:
  - Model behavior does not change after policy edit → check prompt injection, model instruction-following quirks, or policy conflicts
  - Scenario widgets fail to sync across users → likely TipTap Cloud connection issue or widget state deserialization error
  - Suggested policy statements are too generic or contradictory → inspect o1-mini prompt, check if heuristics are being passed, verify policy context length

- First 3 experiments:
  1. Latency tolerance test: Measure how policy-regenerate-response latency affects expert iteration speed; aim for <10s for single-turn regeneration
  2. Widget adoption tracking: Log how often scenarios are embedded vs. discussed verbally; correlate with policy novelty scores
  3. Suggestion acceptance analysis: For each accepted suggestion, compare pre/post policy semantic similarity; identify domains where suggestions are least aligned and refine suggestion prompt

## Open Questions the Paper Calls Out

- Question: How do the content and priorities of LLM policies differ when prototyped by laypeople compared to domain experts?
  - Basis in paper: [explicit] Section 9 states future work can run policy prototyping sessions with general public to pinpoint key similarities and differences between LLM policies desired by laypeople versus experts.
  - Why unresolved: Evaluation study exclusively recruited domain experts in mental health and law, excluding general public from policy creation process.
  - What evidence would resolve it: Comparative study analyzing policies generated by layperson groups against expert-generated policies for same scenarios.

- Question: To what extent do LLM policy prototypes generalize across cultural and legal systems outside of the United States?
  - Basis in paper: [explicit] Section 9 notes all participants except two legal experts were based in the U.S., and resulting perspectives are heavily influenced by American mental health and legal systems.
  - Why unresolved: Specific regulations, professional norms, and cultural values embedded in drafted policies may not transfer to global user bases.
  - What evidence would resolve it: Replicating policy prototyping workshops with experts from non-Western or non-U.S. contexts and analyzing cross-cultural divergences.

- Question: Can the PolicyPad prototyping methodology be successfully adapted to design traditional (non-AI) public policies using simulated populations?
  - Basis in paper: [explicit] Section 8.4 proposes systems like PolicyPad can facilitate collaborative drafting, testing, and deliberation of wide variety of policies by integrating policy-informed simulated population.
  - Why unresolved: Current system relies on tight feedback loops from policy-informed LLM; efficacy and latency of using agent-based simulations for policy feedback remain untested.
  - What evidence would resolve it: Modified system evaluation where users prototype public policies and receive feedback via agent simulations, measuring impact on policy iteration speed and quality.

## Limitations
- Study relied on convenience samples of domain experts rather than systematic recruitment, limiting generalizability to other expert populations
- Novelty was measured against existing policies but may not capture downstream effectiveness of resulting guidelines in real deployment
- Synchronous collaboration model excludes asynchronous workflows that are common in professional policy development

## Confidence
- **High confidence** in collaborative prototyping mechanisms (feedback loops, scenario widgets, suggestion generation) as described, supported by direct workshop observations and participant feedback
- **Medium confidence** in novelty metric (51.9% vs 18.2%) due to reliance on human-AI joint evaluation and limited comparison set of existing policies
- **Low confidence** in generalizability beyond mental health and legal domains without additional validation studies

## Next Checks
1. **Domain transfer test:** Run PolicyPad with domain experts in high-stakes areas like finance or healthcare operations to assess whether novelty gains persist across fields
2. **Behavior impact validation:** Deploy generated policies with real models and measure safety/performance outcomes in live scenarios, not just policy novelty
3. **Asynchronicity extension:** Implement an async collaboration mode and compare policy quality and novelty against synchronous-only baseline to understand tradeoffs