---
ver: rpa2
title: Many-Turn Jailbreaking
arxiv_id: '2508.06755'
source_url: https://arxiv.org/abs/2508.06755
tags:
- follow-up
- arxiv
- questions
- jailbreaking
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces multi-turn jailbreaking, a new vulnerability
  where large language models (LLMs) can be progressively exploited through multiple
  conversation turns after an initial successful jailbreak. The authors construct
  MTJ-Bench, a dataset based on HarmBench, to evaluate this threat in two scenarios:
  irrelevant follow-up questions and relevant follow-up questions.'
---

# Many-Turn Jailbreaking

## Quick Facts
- arXiv ID: 2508.06755
- Source URL: https://arxiv.org/abs/2508.06755
- Reference count: 33
- Primary result: Multi-turn jailbreaking enables "free" harmful responses with 30-70% success rates across models

## Executive Summary
This paper introduces multi-turn jailbreaking, a vulnerability where large language models can be progressively exploited through multiple conversation turns after an initial successful jailbreak. The authors construct MTJ-Bench, a dataset based on HarmBench, to evaluate this threat in two scenarios: irrelevant follow-up questions and relevant follow-up questions. Experiments on 14 open-source models and one closed-source model reveal that once jailbroken, LLMs consistently respond to additional harmful queries, with attack success rates ranging from 30-70% across models. The study shows that multi-turn jailbreaking is a universal vulnerability, amplifying potential misuse by enabling continuous exploitation of aligned LLMs. This work highlights the need for enhanced safety measures to address long-context jailbreaking threats.

## Method Summary
The study constructs MTJ-Bench from HarmBench, containing 320 harmful queries with follow-up questions in two scenarios: irrelevant (10 random harmful questions per query) and relevant (11 style-specific questions per query). Four attack methods (GCG, PAIR, TAP, AutoDAN) are used to generate adversarial prompts for the first turn. For successful first-turn attacks, follow-up responses are generated in full context without further optimization. Evaluation uses HarmBench classifier for irrelevant follow-ups and Claude-3-Sonnet as judge for relevant follow-ups. The study tests 14 open-source models plus Claude 3 Sonnet across various sizes and architectures.

## Key Results
- Multi-turn jailbreaking shows ASRGain of 5-20% on irrelevant follow-ups, indicating "free" additional attacks
- Relevant follow-up attacks achieve 30-40% success rates across methods, substantially higher than irrelevant follow-ups
- Attack transfer from smaller to larger models shows non-trivial success rates, suggesting architectural vulnerabilities
- The vulnerability persists across different model families and sizes, indicating a universal safety alignment gap

## Why This Works (Mechanism)

### Mechanism 1: Context Persistence Exploit
Once an LLM generates a harmful response in turn 1, safety alignment appears to be locally suppressed within that conversation context. The adversarial attack creates a token sequence that bypasses safety filters, and subsequent prompts benefit from the established "cooperation mode" and normalization of harmful content. Safety alignment in current LLMs is context-dependent rather than globally persistent across conversation turns.

### Mechanism 2: Conversational Momentum & Consistency Bias
LLMs trained for multi-turn conversation exhibit a consistency bias that makes them more likely to comply with requests consistent with prior context. When a model has already provided harmful content, refusing a follow-up would create conversational inconsistency. The model's training on helpful, coherent dialogue creates pressure to continue the established interaction pattern.

### Mechanism 3: Attack Transfer & Contextual Amplification
Jailbreak prompts optimized on smaller/other models transfer to larger models within conversation contexts, and the multi-turn setting amplifies transfer success. Shared architectural features and training data create transfer vulnerability, while multi-turn context provides additional "softening" that makes transfer attacks more effective than single-turn transfer.

## Foundational Learning

- **Attack Success Rate (ASR) & ASRGain**: Core metrics for quantifying jailbreak vulnerability. ASR₁ measures first-turn success; ASR₂ measures follow-up success; ASRGain quantifies "free" additional attacks from multi-turn context. *Quick check*: If ASR₁ = 60% and ASRir₂ = 15% on a set of questions, is it possible for ASRGain to be 0%? (Yes—ASRGain only counts follow-up successes where the first turn failed.)

- **Adversarial Prompt Optimization (GCG, PAIR, TAP, AutoDAN)**: These are the attack methods tested. Understanding their mechanisms (gradient-based optimization vs. LLM-driven search vs. genetic algorithms) is essential for analyzing why they transfer and persist. *Quick check*: Which of these methods requires white-box (gradient) access to the target model? (GCG uses gradient-based token optimization; PAIR, TAP, and AutoDAN are black-box methods.)

- **Context Window & Long-Context Safety**: The paper positions many-turn jailbreaking as a consequence of expanding context windows (now up to 1M tokens). Larger contexts enable more conversation history to influence current behavior. *Quick check*: Does a larger context window inherently make a model more vulnerable to many-turn jailbreaking? (Not necessarily—but it enables more turns and more context accumulation, which the paper shows amplifies vulnerability.)

## Architecture Onboarding

- **Component map**: Target LLM (M) -> Attack function (f) -> Judge models (Jir, Jre) -> Benchmark (MTJ-Bench)
- **Critical path**: 1) Run first-turn attack: o₁ = M(f(q)); 2) Evaluate first-turn: Jir(q, o₁); 3) Generate follow-up queries; 4) Run second-turn with full context: o₂ = M([f(q); o₁; q_ir/re]); 5) Evaluate second-turn: Jir or Jre
- **Design tradeoffs**: Direct vs. transfer attacks (effectiveness vs. scalability); irrelevant vs. relevant follow-ups (universal vs. amplified harm); judge model selection (nuance vs. cost)
- **Failure signatures**: Low ASR₁ but high ASRir₂ (context-persistence vulnerability); high ASR₁ but low ASRre₂ (resists conversational escalation); transfer attack succeeds better than direct (over-optimized direct attacks)
- **First 3 experiments**: 1) Baseline test on 2-3 diverse models with all 4 attack methods; 2) Scaling follow-up count (10 → 50 → 100 → 200) and plotting ASRGain; 3) Turn-depth test extending to 5 turns on subset

## Open Questions the Paper Calls Out

- **Open Question 1**: Can many-turn jailbreaking vulnerabilities be effectively transferred to Large Vision-Language Models (LVLMs) through multi-modal inputs? The paper currently only includes text-only attacks on LLMs but suggests applying many-turn jailbreaking to LVLMs with image-based adversarial contexts.

- **Open Question 2**: What are the internal mechanistic causes that drive aligned neural networks to fail safety checks during many-turn attacks? The paper empirically demonstrates the vulnerability but does not provide theoretical or interpretability-based explanations for the loss of alignment.

- **Open Question 3**: What defense mechanisms can effectively mitigate many-turn jailbreaking without compromising the model's ability to maintain long-context conversations? The paper notes effective defense would be useful but is out of current scope.

## Limitations

- Experimental scope limited to single conversation sessions, leaving open whether vulnerabilities persist across session resets
- Transfer attack assumptions may oversimplify the mechanism—whether success is due to shared architecture or coincidental prompt effectiveness
- Judge model potential bias from using Claude-3-Sonnet as context-aware judge for relevant follow-ups

## Confidence

- **High confidence**: The core empirical finding of ASRGain 5-20% on irrelevant follow-ups is well-supported across 15 models and multiple attack methods
- **Medium confidence**: The mechanism explanation (context persistence and conversational momentum) is plausible but not definitively proven
- **Low confidence**: The claim of "universal vulnerability" across all LLM architectures may be overstated given limited model diversity tested

## Next Checks

1. **Cross-session persistence test**: Evaluate whether many-turn jailbreaking vulnerabilities persist when conversations are reset between turns, isolating context-dependent effects from model-level safety failures

2. **Architecture-specific defense evaluation**: Test the vulnerability on models with fundamentally different safety training approaches to determine if certain architectures are inherently resistant

3. **Judge model ablation study**: Replace Claude-3-Sonnet with multiple alternative judges to quantify the impact of judge selection on reported success rates