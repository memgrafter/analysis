---
ver: rpa2
title: On The Variability of Concept Activation Vectors
arxiv_id: '2509.24058'
source_url: https://arxiv.org/abs/2509.24058
tags:
- random
- number
- examples
- variance
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the variability of Concept Activation Vectors
  (CAVs) in the TCAV method, focusing on how the number of random examples affects
  their stability. The authors provide a theoretical analysis showing that the variance
  of CAVs decreases as 1/N, where N is the number of random examples, under certain
  assumptions.
---

# On The Variability of Concept Activation Vectors
## Quick Facts
- arXiv ID: 2509.24058
- Source URL: https://arxiv.org/abs/2509.24058
- Reference count: 40
- Primary result: CAV variance decreases as 1/N with random examples, but TCAV score variance may not decrease due to borderline evaluation points

## Executive Summary
This paper investigates the variability of Concept Activation Vectors (CAVs) in the TCAV method, focusing on how the number of random examples affects their stability. The authors provide theoretical analysis showing that CAV variance decreases at a rate of 1/N with sample size, while TCAV score variance may not follow the same pattern due to borderline evaluation points. Through experiments on image, tabular, and text data, they demonstrate that using multiple runs with modest sample sizes yields stable TCAV scores, while larger per-run sample sizes are needed for stable CAV directions.

## Method Summary
The paper analyzes the variability of CAVs and TCAV scores by examining how the number of random examples affects their stability. The authors derive theoretical bounds showing that CAV variance decreases as 1/N, where N is the number of random examples, under certain assumptions about class-conditional distributions. They also show that sensitivity score variance follows the same rate. However, TCAV score variance does not necessarily decrease with more samples due to the presence of borderline evaluation points. The study uses linear probes trained on image, tabular, and text data to empirically validate these theoretical findings.

## Key Results
- CAV variance decreases as 1/N with the number of random examples
- TCAV score variance may not decrease with more samples due to borderline evaluation points
- Multiple runs with modest sample sizes produce stable TCAV scores
- Larger per-run sample sizes are needed for stable CAV directions

## Why This Works (Mechanism)
The mechanism behind CAV variability is rooted in the statistical properties of random sampling and linear decision boundaries. As more random examples are used to construct CAVs, the estimation error decreases, following the central limit theorem. The linear nature of CAVs means that their variance is directly related to the variance of the underlying data distribution. However, TCAV scores depend on the interaction between CAVs and specific evaluation points, which can be sensitive to borderline cases where small perturbations in CAV direction lead to significant changes in the score.

## Foundational Learning
- Concept Activation Vectors (CAVs): Linear directions in model activation space representing concepts, derived from examples of the concept
  * Why needed: Provides interpretable directions for understanding model behavior
  * Quick check: Verify CAVs capture meaningful semantic directions in activation space

- TCAV (Testing with CAVs): Method for quantifying concept importance using CAVs
  * Why needed: Enables interpretable evaluation of concept influence on model predictions
  * Quick check: Confirm TCAV scores correlate with expected concept importance

- Linear probes: Simple models trained to classify concepts in activation space
  * Why needed: Provides a computationally efficient way to extract CAVs
  * Quick check: Ensure probe accuracy is sufficient for reliable CAV estimation

## Architecture Onboarding
Component map: Random examples -> Linear probe -> CAV -> TCAV score calculation
Critical path: Data sampling → Probe training → CAV estimation → Score evaluation
Design tradeoffs: Multiple runs with modest samples vs. single run with large samples
Failure signatures: High variance in CAV directions across runs, unstable TCAV scores
First experiments:
1. Measure CAV variance decay rate across different sample sizes
2. Compare TCAV stability using multiple runs vs. single large sample
3. Test sensitivity of TCAV scores to borderline evaluation points

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalizability of their findings to non-linear probe architectures and complex model architectures. They note that the theoretical analysis relies on assumptions about class-conditional distributions and linear decision boundaries, which may not hold in more complex scenarios. The impact of dataset characteristics and model architecture on CAV variability remains an open area for investigation.

## Limitations
- Theoretical analysis relies on assumptions about class-conditional distributions and linear decision boundaries
- Experiments primarily focus on image data with simple linear probes
- The impact of complex model architectures on CAV variability is not fully explored
- Generalization to non-linear probe architectures requires further investigation

## Confidence
High: The theoretical derivation showing CAV variance decreases as 1/N under stated assumptions is mathematically sound
Medium: The recommendation to use multiple runs with modest sample sizes for stable TCAV scores is well-supported but may depend on specific use cases
Low: The generalizability of findings to non-linear probes and complex model architectures requires further investigation

## Next Checks
1. Test the 1/N variance decay rate for CAVs with non-linear probe architectures (e.g., multi-layer perceptrons) across different model families
2. Evaluate TCAV stability across diverse evaluation points beyond borderline cases to quantify the impact on overall variance
3. Extend experiments to additional data modalities (e.g., audio, video) and compare variance patterns across domains