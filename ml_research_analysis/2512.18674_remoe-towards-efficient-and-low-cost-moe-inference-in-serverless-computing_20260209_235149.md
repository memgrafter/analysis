---
ver: rpa2
title: 'Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing'
arxiv_id: '2512.18674'
source_url: https://arxiv.org/abs/2512.18674
tags:
- expert
- experts
- inference
- remote
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Remoe, a system designed to minimize the
  cost of serverless inference for Mixture-of-Experts (MoE) models. MoE models have
  a large number of experts, most of which are not activated during inference, leading
  to high memory usage and costs in serverless environments where billing is based
  on allocated resources.
---

# Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing

## Quick Facts
- arXiv ID: 2512.18674
- Source URL: https://arxiv.org/abs/2512.18674
- Authors: Wentao Liu; Yuhao Hu; Ruiting Zhou; Baochun Li; Ne Wang
- Reference count: 39
- Key result: Reduces MoE inference cost by up to 57% and cold start latency by 47% in serverless environments

## Executive Summary
This paper introduces Remoe, a system designed to minimize the cost of serverless inference for Mixture-of-Experts (MoE) models. MoE models have a large number of experts, most of which are not activated during inference, leading to high memory usage and costs in serverless environments where billing is based on allocated resources. Existing methods to address this, such as expert offloading, still require significant memory allocation. Remoe proposes a heterogeneous architecture that assigns non-expert modules to GPUs and expert modules to CPUs. It further offloads infrequently activated experts to separate serverless functions, enabling parallel execution and reducing memory overhead.

## Method Summary
Remoe addresses the challenge of high memory and cost overhead in MoE inference within serverless environments through a three-pronged approach. First, it employs a heterogeneous architecture that assigns non-expert modules to GPUs and expert modules to CPUs, optimizing resource utilization. Second, it introduces a Similar Prompts Searching (SPS) algorithm that predicts expert activation patterns based on semantic similarity of inputs, allowing for intelligent pre-allocation of resources. Third, a joint memory and replica optimization framework leverages Lagrangian duality and the Longest Processing Time (LPT) algorithm to balance resource allocation and service-level objectives (SLOs). These techniques are implemented on Kubernetes and evaluated across multiple LLM benchmarks, demonstrating significant improvements in inference cost and latency.

## Key Results
- Reduces MoE inference cost by up to 57% compared to state-of-the-art baselines
- Achieves 47% reduction in cold start latency
- Successfully implements a heterogeneous architecture that assigns non-expert modules to GPUs and expert modules to CPUs
- Demonstrates effective expert offloading to separate serverless functions for parallel execution

## Why This Works (Mechanism)
Remoe works by addressing the fundamental inefficiency in MoE inference within serverless environments: the high memory usage caused by the need to allocate resources for all experts, even though only a subset is activated per inference. By employing a heterogeneous architecture, Remoe optimizes resource utilization by leveraging the strengths of both GPUs and CPUs. The SPS algorithm predicts expert activation patterns, enabling intelligent pre-allocation of resources and reducing the need for excessive memory allocation. The joint memory and replica optimization framework ensures that SLOs are met while minimizing resource usage, further reducing costs.

## Foundational Learning
- Mixture-of-Experts (MoE) models: Why needed - to understand the core problem of high memory usage in MoE inference. Quick check - verify that the paper clearly explains the MoE architecture and its activation patterns.
- Serverless computing: Why needed - to understand the context of the problem and the billing model that drives the need for cost reduction. Quick check - ensure the paper explains the serverless computing model and its implications for MoE inference.
- Heterogeneous architecture: Why needed - to understand how Remoe optimizes resource utilization. Quick check - verify that the paper clearly explains the heterogeneous architecture and its benefits.
- Similar Prompts Searching (SPS) algorithm: Why needed - to understand how Remoe predicts expert activation patterns. Quick check - ensure the paper explains the SPS algorithm and its accuracy in predicting expert activation.
- Lagrangian duality and LPT algorithm: Why needed - to understand the optimization framework used in Remoe. Quick check - verify that the paper clearly explains these algorithms and their role in the joint memory and replica optimization.

## Architecture Onboarding
- Component map: User request -> SPS algorithm -> Memory allocation -> Expert execution -> Result aggregation
- Critical path: SPS algorithm -> Memory allocation -> Expert execution
- Design tradeoffs: Heterogeneous architecture vs. homogeneous architecture; expert offloading vs. in-memory execution
- Failure signatures: SPS algorithm inaccuracies leading to suboptimal memory allocation; expert offloading failures causing increased latency
- First experiments:
  1. Evaluate SPS algorithm accuracy on diverse input distributions
  2. Test heterogeneous architecture performance under varying workloads
  3. Assess expert offloading effectiveness with different expert activation patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The heterogeneous architecture's effectiveness depends heavily on the assumption that GPU-CPU memory division provides optimal resource utilization across diverse workloads
- The SPS algorithm's accuracy in predicting expert activation patterns may degrade with more diverse or domain-specific inputs not represented in the evaluation benchmarks
- The memory optimization framework's reliance on Lagrangian duality and LPT algorithms assumes certain resource allocation patterns that may not hold in real-world, dynamic serverless environments

## Confidence
- High confidence in the core observation that expert offloading reduces memory overhead
- Medium confidence in the claimed cost and latency improvements across diverse scenarios
- Medium confidence in the SPS algorithm's prediction accuracy for unseen input distributions
- Low confidence in how well the framework generalizes to non-LLM MoE architectures

## Next Checks
1. Test the system with MoE models from different domains (e.g., vision, multimodal) to assess generalizability beyond LLMs
2. Evaluate performance under varying serverless cluster sizes and configurations to understand sensitivity to infrastructure changes
3. Conduct stress tests with adversarial or highly diverse input sequences to measure SPS algorithm degradation under non-ideal conditions