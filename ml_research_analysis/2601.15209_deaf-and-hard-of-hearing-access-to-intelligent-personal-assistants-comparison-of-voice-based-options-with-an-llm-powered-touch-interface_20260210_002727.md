---
ver: rpa2
title: 'Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison
  of Voice-Based Options with an LLM-Powered Touch Interface'
arxiv_id: '2601.15209'
source_url: https://arxiv.org/abs/2601.15209
tags:
- participants
- speech
- deaf
- alexa
- touch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compared usability of LLM-assisted touch interface versus
  natural and re-spoken deaf speech for IPAs. Using Alexa, it tested three input methods:
  natural deaf speech, re-spoken speech via human wizard, and LLM-generated touch
  commands.'
---

# Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface

## Quick Facts
- **arXiv ID:** 2601.15209
- **Source URL:** https://arxiv.org/abs/2601.15209
- **Reference count:** 40
- **One-line primary result:** SUS scores showed no significant differences across natural deaf speech (59.6), re-spoken speech (62.5), and LLM touch interface (63.5) for IPA interactions.

## Executive Summary
This study evaluates three input methods for Deaf and Hard of Hearing (DHH) users interacting with Intelligent Personal Assistants: natural deaf speech, re-spoken speech via human facilitator, and an LLM-powered touch interface. Using Amazon Alexa on an Echo Show, the research found no significant usability differences between methods based on SUS scores. While word error rates were low for deaf speech in command-and-control contexts, participants expressed mixed preferences, citing latency issues with the touch interface and a desire for hands-free interaction. The findings suggest both speech and touch modalities show promise for IPA accessibility, though each has limitations that could be addressed through multimodal approaches.

## Method Summary
The study used a within-subjects design with 12 DHH participants testing three input methods: natural deaf speech, re-spoken speech via human wizard-of-oz facilitator, and an LLM-powered touch interface. The touch interface used GPT-4o to generate context-aware command suggestions based on user history and environment. System Usability Scale (SUS) scores, word error rates, and qualitative feedback were collected. Testing occurred in a lab with participants' own smart home devices connected to an Echo Show. The LLM touch interface employed a multi-step prompt chain to generate UI buttons, with latency measured at 6-10 seconds per interaction.

## Key Results
- SUS scores were not significantly different across input methods (touch: 63.5, re-spoken: 62.5, natural speech: 59.6)
- Word error rates were low, with half of participants experiencing zero errors in deaf speech recognition
- Qualitative feedback showed mixed preferences, with latency and limited options cited as touch interface issues

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Prompt Engineering for Touch Input
- **Claim:** LLM-assisted touch interfaces improve usability by dynamically narrowing command space based on user history and environmental context.
- **Core assumption:** Users prefer selecting from a narrow set of high-probability predictions rather than navigating static menus.
- **Evidence anchors:** Touch SUS (63.5) higher than prior non-LLM touch studies; prompt includes user_history, task_list, and home_environment variables.
- **Break condition:** If LLM temperature is too high or history is sparse, option unpredictability increases, causing user disorientation.

### Mechanism 2: Constrained Grammar Recognition for Deaf Speech
- **Claim:** ASR systems perform better with deaf-accented speech in command-and-control contexts than open-domain transcription.
- **Core assumption:** Limited domain (smart home commands) allows ASR to correct for phonetic variability more effectively.
- **Evidence anchors:** Low WER in study; participants surprised by Alexa's understanding of natural speech; command-and-control uses well-defined patterns.
- **Break condition:** If IPA moves to open-domain chatbot interactions, constraint advantage may disappear.

### Mechanism 3: Multimodal Redundancy for Access Resilience
- **Claim:** Usability is maintained through fallback modalities when primary mode fails.
- **Core assumption:** Users will tolerate higher latency in touch interfaces for accuracy unavailable in voice modes.
- **Evidence anchors:** Study underscores importance of supporting natural deaf speech and flexible multimodal options; participants preferred touch when voice failed.
- **Break condition:** If touch latency exceeds 8-10 seconds, users reject it despite accuracy benefit.

## Foundational Learning

- **Concept:** **Wizard-of-Oz (WoZ) Prototyping**
  - **Why needed here:** The study uses a human facilitator to simulate ideal speech-to-text translation. You must understand this to interpret the "Facilitated English" condition not as working product but as theoretical ceiling.
  - **Quick check question:** Does the "Facilitated English" condition test existing software or human capability to simulate future software? (Answer: Human capability simulation).

- **Concept:** **System Usability Scale (SUS) & "Marginal" Usability**
  - **Why needed here:** Paper reports mean SUS scores around 60. You need to know that 68 is generally considered "average," meaning these interfaces currently offer "marginal" usability.
  - **Quick check question:** Did any input method reach the industry standard average SUS score of 68? (Answer: No).

- **Concept:** **LLM Temperature Parameter**
  - **Why needed here:** Authors adjusted LLM temperature from 0.7 to 0.2. You need to understand that lower temperature reduces randomness, crucial for generating predictable UI buttons.
  - **Quick check question:** Why would higher temperature be detrimental to UI that generates fixed button options? (Answer: It would change button labels/order unpredictably, confusing user).

## Architecture Onboarding

- **Component map:** User Action -> Flask Server -> Prompt Construction (History + Env) -> GPT-4o -> Response Parsing -> Tablet UI -> Echo Show
- **Critical path:** The prompt construction chain with 2-second round-trip time per step creating 6-10 second cumulative delay.
- **Design tradeoffs:** Accuracy vs. Latency (LLM queries per selection step ensure context-awareness but create delay); Flexibility vs. Consistency (LLM handles infinite variations but causes UI instability).
- **Failure signatures:** Voice Mode (high WER correlated with lower SUS scores, r=-0.51); Touch Mode (Option Drift and Confirmation Fatigue).
- **First 3 experiments:** 1) Implement pre-fetching to mask latency and measure impact on perceived speed; 2) Test hybrid input with type-ahead filtering of LLM suggestions; 3) Run targeted test isolating users with >20% WER to validate fallback mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does usability and latency of commercially available re-speaking technologies (e.g., VoiceITT, Project Relate) compare to the WoZ scenario?
- **Basis:** Section 6.2 states testing re-speaking approaches with commercial technology is an important avenue.
- **Why unresolved:** Study used human facilitator to simulate best-case scenario, leaving real-world performance of current tools unverified.
- **Evidence:** Comparative user study measuring SUS scores and interaction delays between WoZ method and actual re-speaking apps.

### Open Question 2
- **Question:** To what extent does required training time for personalized ASR models negatively impact usability and user acceptance of re-speaking IPA interfaces?
- **Basis:** Section 6.2 notes isolating training time effect from interaction is important for meaningful usability results.
- **Why unresolved:** Researchers excluded commercial tools to avoid training burden confounding results, leaving this cost unquantified.
- **Evidence:** Longitudinal study separating onboarding from usage phase to correlate training duration with subsequent SUS scores.

### Open Question 3
- **Question:** Can pre-fetching LLM API responses significantly improve usability by mitigating latency issues?
- **Basis:** Section 6.2 suggests pre-fetching as potential way to improve responsiveness.
- **Why unresolved:** Participants criticized touch interface for being slow, and while pre-fetching proposed as solution, it wasn't implemented or tested.
- **Evidence:** Controlled experiment comparing current sequential loading against pre-fetched architecture using task completion times and qualitative feedback.

## Limitations

- Small sample size of 12 participants limits generalizability and statistical power
- WoZ human facilitator doesn't represent real-world ASR system performance
- Touch interface latency of 6-10 seconds significantly impacted user experience
- Study only tested command-and-control interactions, not open-domain conversations
- All testing occurred in controlled lab environment with participants' own devices

## Confidence

**High Confidence:**
- SUS scores were not significantly different across input methods
- Low word error rates for deaf speech in command-and-control contexts

**Medium Confidence:**
- LLM-assisted touch interfaces improve usability over static touch menus
- Latency significantly impacts touch interface acceptance

**Low Confidence:**
- Predictions about long-term user preferences between modalities
- Claims about exact contribution of each prompt engineering variable to system performance

## Next Checks

1. **Longitudinal Field Study**: Deploy system in participants' homes for 4+ weeks to assess whether SUS scores and preference patterns change with extended use and varied environmental conditions.

2. **ASR Integration Validation**: Replace wizard-of-oz human facilitator with actual ASR systems (both standard and deaf-specific models) to measure gap between theoretical and practical speech recognition performance.

3. **Latency Reduction Experiment**: Implement pre-fetching of LLM responses as suggested and measure impact on SUS scores and task completion times to determine if latency is primary barrier to touch interface adoption.