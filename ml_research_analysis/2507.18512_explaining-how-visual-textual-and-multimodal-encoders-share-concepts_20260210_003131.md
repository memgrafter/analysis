---
ver: rpa2
title: Explaining How Visual, Textual and Multimodal Encoders Share Concepts
arxiv_id: '2507.18512'
source_url: https://arxiv.org/abs/2507.18512
tags:
- features
- text
- clip
- image
- encoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new method to compare visual, textual,
  and multimodal encoders by analyzing the features learned by sparse autoencoders
  (SAEs) on their activations. The authors propose two tools: wMPPC, a weighted similarity
  metric that emphasizes important features, and Comparative Sharedness, which identifies
  features shared with one class of models but not another.'
---

# Explaining How Visual, Textual and Multimodal Encoders Share Concepts

## Quick Facts
- arXiv ID: 2507.18512
- Source URL: https://arxiv.org/abs/2507.18512
- Reference count: 40
- This paper introduces wMPPC and Comparative Sharedness tools to analyze cross-modal feature sharing between visual, textual, and multimodal encoders using sparse autoencoders.

## Executive Summary
This paper presents a novel framework for analyzing how visual, textual, and multimodal encoders share conceptual representations. The authors introduce wMPPC, a weighted similarity metric that emphasizes important features, and Comparative Sharedness, which identifies features unique to specific model classes. Using these tools to analyze 21 encoders across three datasets, they find that cross-modal alignment is strongest in the last layers of models and that multimodal pretraining significantly influences visual feature representation. The work provides insights into how text pretraining affects visual understanding in multimodal models.

## Method Summary
The authors analyze feature sharing between encoders using sparse autoencoders (SAEs) to decompose model activations into interpretable features. They propose wMPPC as a weighted similarity metric that emphasizes important features when comparing encoders, and Comparative Sharedness to identify features shared with one class of models but not another. The analysis is applied to 21 encoders (CLIP, BLIP, and text-only models) across three datasets, examining activation patterns layer-by-layer to understand how cross-modal alignment develops through the network. The methodology combines feature extraction via SAEs with quantitative metrics to provide a systematic comparison of concept sharing across modalities.

## Key Results
- Cross-modal alignment is strongest in the last layers of models, with wMPPC values ranging from 0.042 to 0.578 across different encoder pairs
- Visual features in VLMs that are shared with text encoders but not visual-only models reveal the impact of text pretraining on visual understanding
- High-level semantic concepts (geographic regions, verbs like "to ride") show better cross-modal sharing, emphasizing the importance of multimodal training

## Why This Works (Mechanism)
The method works by decomposing complex model activations into interpretable features using sparse autoencoders, then comparing these feature representations across different model architectures. The wMPPC metric provides a weighted similarity measure that accounts for feature importance, while Comparative Sharedness enables fine-grained analysis of which features are shared between specific model classes. This approach reveals how multimodal pretraining creates shared conceptual representations that bridge visual and textual modalities.

## Foundational Learning
- Sparse autoencoders (SAEs): Decompose model activations into sparse, interpretable features; needed to make high-dimensional activations analyzable, check by verifying feature sparsity and interpretability
- Cross-modal alignment: The degree to which different modality encoders represent similar concepts; needed to understand multimodal model effectiveness, check by comparing wMPPC values across model pairs
- Feature weighting: Assigning importance scores to individual features when computing similarity; needed to avoid treating all features equally, check by analyzing feature activation magnitudes

## Architecture Onboarding

**Component Map:** SAE -> Feature Extraction -> wMPPC Calculation -> Comparative Sharedness Analysis -> Cross-Modal Alignment Assessment

**Critical Path:** Data → SAE Feature Extraction → Feature Weighting → wMPPC/Comparative Sharedness → Cross-Modal Insights

**Design Tradeoffs:** The use of SAEs provides interpretability but may miss subtle feature interactions; weighted metrics emphasize important features but require careful threshold selection; cross-modal analysis is comprehensive but computationally intensive.

**Failure Signatures:** Low wMPPC values indicate poor cross-modal alignment; inconsistent Comparative Sharedness results suggest feature extraction issues; layer-wise analysis showing no improvement in later layers indicates architectural limitations.

**3 First Experiments:**
1. Apply SAE feature extraction to a single layer of two different encoder types to verify basic functionality
2. Compute wMPPC between matched layers of a VLM and a text encoder to establish baseline cross-modal similarity
3. Perform Comparative Sharedness analysis to identify features unique to multimodal versus unimodal models

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can wMPPC characterize concept sharing in generative models (decoders) across modalities?
- Basis in paper: The authors suggest conducting a systematic analysis of wMPPC on generative models as a future perspective.
- Why unresolved: The current study is restricted to encoders; decoders have different architectural objectives (generation vs. representation) and activation distributions.
- What evidence would resolve it: Applying the wMPPC pipeline to text or image decoders to validate if cross-modal shared information patterns persist.

### Open Question 2
- Question: Is wMPPC a reliable metric for automated dataset curation and filtering?
- Basis in paper: The authors propose that comparative studies using wMPPC could be used to select or filter datasets for training new models.
- Why unresolved: While the study observes a correlation between low wMPPC and poor image-text alignment (e.g., Laion vs. COCO), the causal efficacy of using this metric for active data selection is unproven.
- What evidence would resolve it: Experiments training new models on datasets filtered via wMPPC scores, demonstrating improved downstream performance or alignment.

### Open Question 3
- Question: How can automated feature naming incorporate multimodal inputs to scale Comparative Sharedness analysis?
- Basis in paper: The authors note that techniques for automatically naming SAE features considering both images and captions could allow for large-scale analysis.
- Why unresolved: The current typology relies on manual inspection of top activations; a robust, scalable method for assigning multimodal labels to features does not yet exist.
- What evidence would resolve it: An automated pipeline that labels features using cross-modal data (images and text) with high consistency compared to human-derived interpretations.

## Limitations
- The study relies on sparse autoencoder analysis which may not capture all aspects of feature sharing between modalities
- The analysis focuses on specific model families (CLIP, BLIP, and text encoders) which may not generalize to other architectures
- The cross-modal alignment findings are based on a limited set of datasets, potentially limiting external validity

## Confidence
- High confidence in the methodological framework (wMPPC and Comparative Sharedness) and the finding that cross-modal alignment is strongest in last layers
- Medium confidence in the quantitative wMPPC values (0.042-0.578 range) due to potential SAE hyperparameter sensitivity
- Medium confidence in the typology of visual features shared between VLMs and text encoders, as this requires careful interpretation of feature semantics

## Next Checks
1. Validate wMPPC sensitivity by testing across different SAE sparsity levels and comparing with alternative feature attribution methods like integrated gradients or attention-based approaches
2. Replicate the cross-modal alignment findings on additional datasets including non-natural image domains (medical imaging, satellite imagery) and multilingual text encoders to test generalizability
3. Conduct ablation studies removing text pretraining components in VLMs to quantify their specific contribution to the shared visual features identified in the typology analysis