---
ver: rpa2
title: Large Language Models Achieve Gold Medal Performance at the International Olympiad
  on Astronomy & Astrophysics (IOAA)
arxiv_id: '2510.05016'
source_url: https://arxiv.org/abs/2510.05016
tags:
- ioaa
- problems
- data
- llms
- exams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks five state-of-the-art large language models
  on International Olympiad on Astronomy and Astrophysics (IOAA) exams to evaluate
  their astronomical problem-solving capabilities. The IOAA exams are used as an ecologically
  valid benchmark because they test deep conceptual understanding, multi-step derivations,
  and multimodal analysis rather than simple knowledge recall.
---

# Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)

## Quick Facts
- **arXiv ID**: 2510.05016
- **Source URL**: https://arxiv.org/abs/2510.05016
- **Reference count**: 40
- **Primary result**: GPT-5 and Gemini 2.5 Pro achieve gold medal performance on IOAA theory exams, with GPT-5 also ranking top 10 in data analysis

## Executive Summary
This paper benchmarks five state-of-the-art large language models on International Olympiad on Astronomy and Astrophysics (IOAA) exams, using these as an ecologically valid benchmark for deep astronomical problem-solving. The results show GPT-5 and Gemini 2.5 Pro achieving gold medal performance in theory exams with average scores of 84.2% and 85.6%, ranking among the top two performers compared to human participants. However, error analysis reveals consistent weaknesses in geometric reasoning and spatial visualization (52-79% accuracy), indicating LLMs still struggle with fundamental aspects of astronomical problem-solving despite approaching peak human performance in theory exams.

## Method Summary
The study evaluates five LLMs (GPT-5, Gemini 2.5 Pro, OpenAI o3, Claude Opus 4.1, and Claude Sonnet 4) on 57 IOAA exam problems from 2022-2025 using zero-shot prompting with a standardized constants sheet. Problems are processed from LaTeX source with embedded figures, and models generate LaTeX solutions with TikZ plots. Responses are independently graded by IOAA experts using official rubrics. The evaluation focuses on both theory (49 problems) and data analysis (8 problems) exams, with scores compared to human participants' medal thresholds.

## Key Results
- GPT-5 and Gemini 2.5 Pro achieve gold medal performance in theory exams (84.2% and 85.6% average scores)
- GPT-5 excels in data analysis exams with 88.5% average score, ranking top 10 among human participants
- All models show significant performance drops in data analysis (48-76%) compared to theory performance
- Geometric reasoning and spatial visualization problems show consistent weaknesses (52-79% accuracy) across all models

## Why This Works (Mechanism)

### Mechanism 1: Domain Knowledge Integration with Multi-Step Reasoning
- **Claim**: Top-performing models succeed by integrating astronomical knowledge with sustained multi-step derivations.
- **Mechanism**: Pre-trained astronomical and physical knowledge gets retrieved and chained through reasoning steps; models maintain derivation coherence across 5-75 point problems requiring physical reasoning, mathematical analysis, and domain-specific approximations.
- **Evidence**: GPT-5 and Gemini 2.5 Pro achieve 84.2% and 85.6% on theory exams, outperforming other models by 7-25 percentage points.

### Mechanism 2: Multimodal Processing Quality Differentiates Data Analysis Performance
- **Claim**: Superior plot interpretation and figure generation capabilities separate top performers in data analysis tasks.
- **Mechanism**: Models with stronger visual encoding can extract quantitative information from light curves, spectra, and plots; better visual decoding enables correct downstream calculations.
- **Evidence**: GPT-5's 88.5% data analysis score vs. 48-76% for other models, with fewer plot reading and plotting errors.

### Mechanism 3: Spatial Reasoning as Architectural Bottleneck
- **Claim**: Text-based architectures cannot perform genuine spatial visualization; they approximate geometric reasoning through symbolic pattern matching.
- **Mechanism**: Models process geometric descriptions as text sequences without internal 3D representations; spherical trigonometry and coordinate transformations fail when problems require mental rotation or perspective shifts not explicit in text.
- **Evidence**: 15-26 percentage point degradation on Category I geometric problems (49-78%) vs. Category II physics problems (67-91%).

## Foundational Learning

- **Concept**: Spherical Trigonometry and Celestial Coordinate Systems
  - **Why needed here**: 37% of IOAA theory problems (Category I) require spherical geometry; models consistently fail on great circle relationships and coordinate transformations.
  - **Quick check**: Given a right ascension and declination, can you compute the angular distance between two celestial objects?

- **Concept**: Astronomical Time Systems (Tropical vs. Sidereal vs. Calendar Years)
  - **Why needed here**: Models show "confusion with timekeeping systems and choose between tropical and sidereal years incorrectly" (Section 3.3.1); temporal reference frame errors invalidate downstream calculations.
  - **Quick check**: Why does the tropical year differ from the sidereal year, and which governs seasonal cycles?

- **Concept**: Order-of-Magnitude Astrophysical Reasoning
  - **Why needed here**: "Models often misjudge astronomical distances by orders of magnitude or fail to recognize when approximations are invalid under problem constraints" (Section 3.3.1).
  - **Quick check**: Estimate the luminosity ratio between a K-dwarf and the Sun given only their approximate temperatures and radii.

## Architecture Onboarding

- **Component map**: Input preprocessing (LaTeX extraction → figure encoding) → Model inference (single-pass generation) → Output pipeline (LaTeX extraction → PDF compilation) → Human grading
- **Critical path**: Multimodal encoding quality → geometric problem detection (15-26 percentage point degradation expected) → derivation completeness (partial credit requires shown steps)
- **Design tradeoffs**: Single-attempt evaluation sacrifices potential improvement from self-correction for ecological validity; LaTeX output enables automatic extraction but introduces compilation failures unrelated to reasoning quality
- **Failure signatures**: Spherical trigonometry violations (claiming distinct angles equal), temporal frame confusion (equating calendar and tropical years), incomplete derivations (stating "after integration" without showing work), visual measurement errors (20-50% distance measurement errors)
- **First 3 experiments**:
  1. Spatial reasoning augmentation: Implement visual sketchpad tool allowing models to generate intermediate diagrams; measure Category I performance change
  2. Multimodal training data synthesis: Create synthetic visual Q&A pairs from astronomical plots; evaluate data analysis score improvement
  3. Error type stratification: Run detailed categorization on held-out set to quantify conceptual vs. geometric vs. calculation error proportions

## Open Questions the Paper Calls Out

- **Open Question 1**: Would visual sketchpad tools meaningfully reduce the 15-26 percentage point performance gap between geometric/spatial and physics/mathematics problems?
  - **Basis**: "future work can implement visual sketchpad [35] so that the models can imitate humans to visualize or draw spatial representations"
  - **Status**: Proposed intervention has not been tested

- **Open Question 2**: What specific capabilities enable GPT-5 to maintain performance from theory to data analysis while all other models drop 10-15 percentage points?
  - **Basis**: GPT-5's 88.5% data analysis score vs. 48-76% for other models
  - **Status**: Paper attributes this to "superior multimodal capabilities" but does not isolate components

- **Open Question 3**: Can LLMs achieve comparable performance on observation-type problems through simulation interfaces?
  - **Basis**: Observation exams excluded "due to LLMs' digital nature" without testing virtual telescope interfaces
  - **Status**: No evaluation attempted for simulated observational astronomy tasks

## Limitations

- Model access constraints: GPT-5 and OpenAI o3 are not publicly available, making exact replication impossible
- Geometric reasoning limitations appear to be architectural rather than fixable through training, representing a fundamental barrier
- Human grading introduces subjective interpretation, particularly for distinguishing conceptual from geometric errors

## Confidence

- **High Confidence**: GPT-5 and Gemini 2.5 Pro achieving gold medal performance in theory exams (84.2-85.6%); spatial reasoning identified as systematic weakness (52-79% accuracy)
- **Medium Confidence**: Multimodal processing quality as differentiator for data analysis performance; architectural bottleneck for spatial visualization
- **Low Confidence**: Extrapolation of findings to other scientific domains; generalizability beyond astronomical problem-solving

## Next Checks

1. Implement visual sketchpad tool allowing models to generate intermediate diagrams and measure Category I performance improvement
2. Create synthetic visual Q&A pairs from astronomical plots following Yang et al. recommendations and evaluate data analysis score improvement
3. Run detailed error categorization on held-out set to quantify conceptual vs. geometric vs. calculation error proportions for targeted fine-tuning