---
ver: rpa2
title: 'Explainable Artificial Intelligence techniques for interpretation of food
  datasets: a review'
arxiv_id: '2504.10527'
source_url: https://arxiv.org/abs/2504.10527
tags:
- food
- data
- issn
- techniques
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews eXplainable Artificial Intelligence (XAI) techniques
  applied to food quality datasets, addressing the lack of transparency in AI models
  used in food engineering. The authors present a taxonomy classifying research by
  data types (tabular, pictorial, spectral, time series) and explanation methods (numerical,
  rule-based, visual, mixed).
---

# Explainable Artificial Intelligence techniques for interpretation of food datasets: a review

## Quick Facts
- arXiv ID: 2504.10527
- Source URL: https://arxiv.org/abs/2504.10527
- Reference count: 40
- This survey reviews XAI techniques for food quality datasets, classifying research by data types and explanation methods, and identifying trends and gaps.

## Executive Summary
This paper reviews eXplainable Artificial Intelligence (XAI) techniques applied to food quality datasets, addressing the lack of transparency in AI models used in food engineering. The authors present a taxonomy classifying research by data types (tabular, pictorial, spectral, time series) and explanation methods (numerical, rule-based, visual, mixed). They analyze over 100 papers, identifying trends such as the dominance of visual explanations for pictorial data and the need for better XAI methods for spectral and time series data. The study highlights challenges like interpretability and suggests future directions, including rule-based and concept-based explanations. This work bridges the gap between XAI and food quality, providing insights to improve model transparency and trust.

## Method Summary
The authors conducted a systematic literature review using Google Scholar and Scopus, searching for papers combining keywords related to XAI and food quality. They included papers that explicitly applied XAI techniques to food datasets, then classified each by food task, data type, AI method, and explanation type. The survey covers over 100 studies from the past decade, using snowball sampling from foundational XAI works and reference sections. The goal was to develop a taxonomy and identify trends and gaps in XAI applications for food quality.

## Key Results
- Visual explanations (e.g., Grad-CAM) dominate for pictorial data, helping inspectors verify model focus on relevant features.
- SHAP is widely used for numerical explanations in tabular data, revealing global model behavior through local feature attributions.
- Spectral and time series data remain underexplored by XAI, with few tailored methods for these complex data types.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual explanations (CAM-based methods) reduce friction in model adoption because they produce human-interpretable heatmaps that do not require deep domain expertise to decode.
- **Mechanism:** Grad-CAM, Score-CAM, and related techniques compute gradient-weighted activation maps that highlight spatial regions influencing predictions. By overlaying heatmaps on input images, inspectors can verify that the model focuses on task-relevant features (e.g., diseased leaf regions rather than background artifacts).
- **Core assumption:** Users can visually assess whether highlighted regions align with domain knowledge (e.g., color patterns, texture, shape).
- **Evidence anchors:**
  - [abstract]: "Grad-CAM... can pinpoint which... image regions contribute most to a prediction, enhancing transparency and aiding quality control inspectors."
  - [Section 4.1]: "CAM-based technology contributed to model verification and highlighted regions with particular texture and color patterns."
  - [corpus]: Limited direct corpus evidence on cognitive validation of visual XAI in food; primarily inferred from food-domain case studies.
- **Break condition:** If the model fixates on spurious features (e.g., background, watermarks) but still achieves high accuracy, visual explanations may give false confidence without true causal alignment.

### Mechanism 2
- **Claim:** SHAP provides numerical feature attributions that, when aggregated, reveal global model behavior despite being computed locally.
- **Mechanism:** SHAP approximates Shapley values for each feature per sample, quantifying each input's additive contribution. Although computed per instance, systematic patterns across samples enable identification of dominant features (e.g., spectral wavelengths, environmental variables).
- **Core assumption:** Feature importance correlates with causal influence within the model's learned representation; real-world causality may differ.
- **Evidence anchors:**
  - [Section 5.2]: "Several studies have applied advanced ML techniques using tabular data... integrating multiple data sources and employing SHAP for interpretability."
  - [Section 9]: "Local explanations can be used to extrapolate global information about the models."
  - [corpus]: Nearby surveys (XAI in finance, software lifecycle) similarly report SHAP for global insight via local aggregation, but no controlled validation in food.
- **Break condition:** If features are highly collinear or interactions are complex, SHAP attributions may mislead; importance scores do not guarantee causal pathways.

### Mechanism 3
- **Claim:** Spectral and time-series XAI remain underexplored because most available explainers were designed for tabular or image data, limiting transferability.
- **Mechanism:** Spectral data (NIR, Raman, NMR) represent high-dimensional, sequential wavelength responses. Standard CAM methods lack natural spatial structure, and SHAP's computational cost scales poorly with spectral dimensionality. Time-series methods are similarly underserved.
- **Core assumption:** Existing XAI tools can be adapted or extended without fundamental methodological innovation.
- **Evidence anchors:**
  - [Section 10]: "Spectral data... has yet to be adequately addressed by current XAI techniques."
  - [Section 9]: "Time series and spectral data... have not been extensively explored with XAI techniques."
  - [corpus]: Corpus provides no strong counterexamples of mature spectral/time-series XAI in food.
- **Break condition:** If tailored XAI methods for spectral data emerge (e.g., wavelength-aware attention), this gap may close quickly.

## Foundational Learning

- **Interpretability vs. Explainability**
  - Why needed here: The paper distinguishes *interpretability* (intrinsic model transparency, e.g., Linear Regression coefficients) from *explainability* (post-hoc approximate insights, e.g., SHAP, Grad-CAM). Confusing these leads to mismatched expectations.
  - Quick check question: "Does the model's structure directly expose decision logic, or is an external method needed to approximate it?"

- **Local vs. Global Explanations**
  - Why needed here: 90.3% of surveyed methods are local. Understanding scope determines whether you explain individual predictions or model-wide behavior.
  - Quick check question: "Is the explanation for one sample, or does it characterize the model across the dataset?"

- **Model-Agnostic vs. Model-Specific XAI**
  - Why needed here: Choice constrains flexibility. Grad-CAM requires CNN architectures; SHAP/LIME apply broadly but may be less precise for specific architectures.
  - Quick check question: "Can the explainer be swapped if the model changes, or is it tightly coupled?"

## Architecture Onboarding

- **Component map:**
  [Data Source] → [Preprocessing] → [AI Model (ML/DL)] → [Prediction]
                                             ↓
                                      [XAI Module]
                                             ↓
                                    [Explanation Output]
                                   (visual/numerical/mixed)

- **Critical path:**
  1. Identify data type and quality task (safety, authenticity, nutrition, sensory, sustainability).
  2. Select appropriate AI model (CNNs for images; tree ensembles for tabular; specialized architectures for spectral/time-series).
  3. Choose XAI method compatible with model and desired explanation format (Grad-CAM for visual; SHAP/LIME for numerical/mixed).
  4. Validate explanation alignment with domain knowledge (e.g., do highlighted regions match known defect patterns?).

- **Design tradeoffs:**
  - Accuracy vs. interpretability: More expressive models (Transformers, deep CNNs) are harder to explain.
  - Local vs. global: Local explanations are easier to generate but require aggregation for system-level insight.
  - Single vs. multi-method: 21/177 papers use mixed explanations; single methods risk partial views.

- **Failure signatures:**
  - Heatmaps consistently highlight background/irrelevant regions → model may be learning artifacts.
  - SHAP importance concentrated on a single feature across all samples → potential data leakage or overfitting.
  - Explanations require domain experts to interpret → practical deployment barrier.

- **First 3 experiments:**
  1. **Baseline visual XAI:** Train a CNN on a food image dataset (e.g., fruit defect classification). Apply Grad-CAM and verify highlighted regions correspond to known defect patterns using held-out expert annotations.
  2. **Tabular feature attribution:** Train XGBoost on a food composition dataset. Apply SHAP and check whether top features align with established physicochemical knowledge (e.g., moisture content, protein levels).
  3. **Multi-method consistency:** For the same image model, compare Grad-CAM, LIME, and SHAP explanations on identical samples. Assess agreement/disagreement to surface method-specific biases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can XAI techniques be adapted to effectively explain spectral data in food quality applications?
- Basis in paper: [explicit] The authors state "Spectral data, commonly used in physicochemical analysis, has yet to be adequately addressed by current XAI techniques. Closing this gap will require the development of tailored approaches to interpret this complex data type."
- Why unresolved: Spectral data is inherently high-dimensional and requires domain-specific wavelength interpretation; current visual explanation methods (e.g., Grad-CAM on spectrograms) do not directly map spectral features to chemical or physical food properties.
- What evidence would resolve it: Development of XAI methods that produce explanations linking specific spectral bands to known physicochemical food characteristics, validated by food scientists on tasks like contaminant detection or authenticity verification.

### Open Question 2
- Question: What XAI methods can provide meaningful explanations for time series data in food quality monitoring?
- Basis in paper: [explicit] The authors note "there are relatively few XAI methods specifically designed to explain AI models used in time series analysis" and that time series data remains underexplored despite its use in continuous monitoring.
- Why unresolved: Time series explanations must capture temporal dependencies and sequential patterns, which standard feature-attribution methods do not address; the food domain adds complexity with sensor noise and environmental variability.
- What evidence would resolve it: New or adapted XAI techniques that highlight critical time windows and temporal patterns in food-related time series, demonstrated on tasks like freshness monitoring or fermentation tracking.

### Open Question 3
- Question: Can hierarchical or multi-level explainability approaches decompose the contributions of fused data modalities in food quality predictions?
- Basis in paper: [explicit] The authors state that data fusion "makes understanding the outcomes more challenging" and propose "hierarchical-based explainability approaches could be proposed to break down the contribution of each data type to the final decision. Although not yet well-defined, this type of XAI could offer a viable solution."
- Why unresolved: Fused datasets combine heterogeneous modalities (e.g., chemical, sensory, imaging) with different scales and semantics; no established XAI framework attributes predictions across these levels.
- What evidence would resolve it: A hierarchical XAI framework that provides both modality-level and feature-level attribution for fused-food-dataset models, validated on tasks integrating imaging with spectral or tabular data.

## Limitations
- **Generalizability across domains**: Findings on XAI for food quality may not transfer directly to other domains without validation.
- **Empirical validation gaps**: The paper identifies trends and gaps but does not provide controlled experiments comparing XAI methods' effectiveness in improving model adoption or trust in food engineering contexts.
- **Methodological assumptions**: Classification of XAI techniques as local vs. global, or model-agnostic vs. model-specific, relies on the authors' interpretation of literature rather than systematic validation.

## Confidence
- **High confidence**: The taxonomy classification and trend identification (e.g., dominance of visual explanations for images, need for spectral/time-series XAI) are well-supported by the surveyed literature.
- **Medium confidence**: The mechanisms proposed (visual explanations for model verification, SHAP for global insight) are plausible but lack direct experimental validation in food quality contexts.
- **Low confidence**: Claims about the scalability or transferability of XAI methods to spectral/time-series data are speculative, as the paper identifies this as an open challenge without concrete solutions.

## Next Checks
1. **Controlled experiment**: Compare the effectiveness of Grad-CAM vs. SHAP vs. LIME in improving trust and adoption of AI models for food quality tasks (e.g., defect detection in fruit images). Measure user confidence and decision-making accuracy.
2. **Spectral XAI prototype**: Develop and test a wavelength-aware attention mechanism for spectral data (e.g., NIR spectroscopy in food authentication) and compare its explanations to standard SHAP or LIME.
3. **Cross-domain validation**: Apply the proposed XAI mechanisms (visual, numerical, mixed) to a non-food domain (e.g., medical imaging) and assess whether the same patterns of interpretability and adoption barriers hold.