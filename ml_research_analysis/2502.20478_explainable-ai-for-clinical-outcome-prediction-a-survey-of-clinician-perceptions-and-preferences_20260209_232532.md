---
ver: rpa2
title: 'Explainable AI for Clinical Outcome Prediction: A Survey of Clinician Perceptions
  and Preferences'
arxiv_id: '2502.20478'
source_url: https://arxiv.org/abs/2502.20478
tags:
- clinical
- methods
- participants
- patient
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study surveyed 32 clinical practitioners to understand their
  preferences for explainable AI (XAI) techniques when interpreting model predictions
  over text-based EHR data. Four XAI methods (LIME, attention-based span highlights,
  exemplar patient retrieval, and free-text rationales from LLMs) were implemented
  on an ICU admission note-based mortality prediction model.
---

# Explainable AI for Clinical Outcome Prediction: A Survey of Clinician Perceptions and Preferences

## Quick Facts
- arXiv ID: 2502.20478
- Source URL: https://arxiv.org/abs/2502.20478
- Authors: Jun Hou; Lucy Lu Wang
- Reference count: 36
- Primary result: Clinicians preferred free-text LLM rationales over attention, LIME, and similar patient retrieval for interpreting ICU mortality predictions

## Executive Summary
This study evaluated clinician preferences for four explainable AI (XAI) methods when interpreting model predictions over clinical text data. The research surveyed 32 clinical practitioners using a semi-structured interview format with think-aloud protocols to assess four XAI approaches: LIME word-level highlights, attention-based span highlights, exemplar patient retrieval, and free-text rationales from GPT-4. Participants ranked free-text rationales as most understandable and reasonable, while attention-based explanations were least preferred. The findings suggest that while no single XAI method satisfies all clinical needs, free-text rationales show promise for enhancing provider-patient communication despite lacking grounded evidence.

## Method Summary
The study implemented four XAI methods on an ICU admission note-based mortality prediction model using MIMIC-III data. A UmlsBERT model was fine-tuned for binary mortality classification, achieving micro-F1 of 87.86 and macro-F1 of 66.43. The four XAI approaches included LIME word-level explanations, attention-based highlights from the model's [CLS] token, similar patient retrieval using contrastive fine-tuning, and GPT-4-generated free-text rationales. Thirty-two clinicians participated in surveys where they evaluated these explanations across three clinical scenarios, rating understandability, reasonableness, and usefulness on Likert scales while providing preference rankings and qualitative feedback through thematic analysis.

## Key Results
- Free-text rationales from GPT-4 were most preferred by clinicians for understandability and reasonableness
- Attention-based explanations were least preferred across all evaluation metrics
- Similar patient retrieval was valued for its evidence-based approach but ranked lowest overall
- Participants highlighted the need for both efficient generalized tools and specialist-sensitive options tailored to varying clinical expertise levels

## Why This Works (Mechanism)
The study demonstrates that clinical practitioners value XAI explanations that align with their existing decision-making processes and communication needs. Free-text rationales succeeded because they provided narrative explanations that clinicians could easily interpret and potentially use in patient discussions. The preference hierarchy reflects the tension between evidence-based reasoning (similar patient retrieval) and communicative efficiency (free-text rationales). The mixed-methods approach combining quantitative ratings with qualitative thematic analysis captured nuanced preferences that purely numerical assessments would miss.

## Foundational Learning

### Clinical Text Processing
Why needed: EHR data contains semi-structured text requiring specialized handling for NLP tasks
Quick check: Verify section extraction preserves clinical context and temporal relationships

### XAI Method Diversity
Why needed: Different clinical scenarios require different explanation types (diagnostic vs. prognostic)
Quick check: Test each method across multiple prediction tasks and clinical contexts

### Clinician Survey Design
Why needed: Valid clinical evaluation requires methods that capture both quantitative preferences and qualitative reasoning
Quick check: Pilot survey with smaller group to refine questions and reduce cognitive load

## Architecture Onboarding

### Component Map
MIMIC-III admission notes -> UmlsBERT fine-tuning -> Binary mortality classifier -> Four XAI methods -> Survey platform -> Clinician evaluation

### Critical Path
Model training (UmlsBERT) -> XAI implementation (LIME, attention, retrieval, GPT-4) -> Survey design and recruitment -> Data collection and thematic analysis

### Design Tradeoffs
Evidence grounding vs. interpretability: Similar patient retrieval provides evidence but is complex; free-text rationales are interpretable but lack grounding
Technical complexity vs. clinical utility: Simpler methods may be more widely adopted despite lower accuracy
Generalization vs. specialization: Tools must balance broad applicability with domain-specific requirements

### Failure Signatures
Poor retrieval relevance indicates embedding quality issues or inadequate contrastive training
LIME highlighting uninformative terms suggests tokenization problems or need for preprocessing
GPT-4 hallucinations reveal prompt engineering deficiencies or insufficient in-context examples

### First Experiments
1. Validate UmlsBERT performance across multiple clinical prediction tasks
2. Test XAI methods on synthetic clinical scenarios with known ground truth
3. Conduct pilot interviews with 5-10 clinicians to refine survey methodology

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Small sample size (N=32) limits generalizability across clinical specialties and experience levels
- Survey methodology relies on self-reported preferences rather than actual clinical decision-making
- GPT-4 free-text rationales lack grounded evidence and may introduce hallucinations

## Confidence

| Major Claim Cluster | Confidence Level |
|---------------------|------------------|
| XAI method preference rankings | High |
| Clinical utility assessment | Medium |
| Free-text rationale effectiveness | Medium |

## Next Checks
1. Replicate UmlsBERT fine-tuning with explicit hyperparameter documentation and validate class-wise performance across both mortality and survival predictions
2. Conduct larger-scale survey with diverse clinical specialties and experience levels to validate preference rankings across different clinical contexts
3. Implement pilot study where clinicians use XAI explanations in actual clinical scenarios rather than hypothetical survey questions to measure real-world utility and decision impact