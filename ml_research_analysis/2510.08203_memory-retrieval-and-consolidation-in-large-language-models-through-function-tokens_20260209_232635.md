---
ver: rpa2
title: Memory Retrieval and Consolidation in Large Language Models through Function
  Tokens
arxiv_id: '2510.08203'
source_url: https://arxiv.org/abs/2510.08203
tags:
- tokens
- function
- token
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how memory retrieval and consolidation\
  \ work in large language models by proposing the function token hypothesis. Function\
  \ tokens\u2014high-frequency tokens like punctuation, articles, and prepositions\u2014\
  are shown to dynamically activate the most predictive features from context during\
  \ inference, while the prediction of content tokens following function tokens drives\
  \ feature learning during pre-training."
---

# Memory Retrieval and Consolidation in Large Language Models through Function Tokens

## Quick Facts
- arXiv ID: 2510.08203
- Source URL: https://arxiv.org/abs/2510.08203
- Reference count: 40
- Primary result: Function tokens dynamically activate predictive features from context during inference while driving feature learning during pre-training

## Executive Summary
This paper proposes the function token hypothesis, demonstrating that high-frequency tokens like punctuation, articles, and prepositions play a crucial role in memory retrieval and consolidation in large language models. Through bipartite graph analysis and sparse autoencoders, the authors show that a small set of function tokens activates the majority of the model's features, especially in middle layers. Pre-training experiments reveal that models first learn to predict function tokens, with optimization dominated by predicting content tokens that follow them. The findings are supported by extensive empirical evidence including case studies showing that steering activations on function tokens can directly control model outputs.

## Method Summary
The study analyzes token-feature relationships using bipartite graphs constructed from SAE-decomposed activations at specific transformer layers. Function tokens are identified as the top 122 tokens covering ~40% of corpus occurrences. Pre-training experiments train 1.5B and 8B LLaMA-style models on SlimPajama for one epoch, tracking loss across four token-transition categories. SAEs with dictionary width 2^20 are applied to layer 2 activations at multiple checkpoints. The steering experiments use binary search to identify k features whose activation steers model outputs.

## Key Results
- Top 10 function tokens activate 76.46% of features at layer 20 in Gemma2-9B
- Function→content token prediction has the highest loss throughout training (4.88 for 1.5B, 4.27 for 8B)
- Feature count grows from early to late pre-training, with function tokens consistently activating more features than content tokens
- Steering function token activations can directly control model outputs (e.g., changing 'Japan' to '日本')

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Function tokens dynamically activate predictive features from context during inference to govern next-token prediction.
- Mechanism: When a function token is processed, it reactivates features previously activated by content tokens in the context, creating context-dependent feature combinations that direct generation. The same function token activates different features depending on preceding content.
- Core assumption: Features encode semantic concepts and can be reliably steered to control outputs.
- Evidence anchors:
  - [abstract]: "function tokens activate the most predictive features from context and govern next token prediction (memory retrieval)"
  - [Section 3.2]: Case studies show function tokens ':', 'the', '\n' propagate activations; steering final function token changes responses (e.g., 'Japan' → '日本')
  - [corpus]: Weak direct corpus support; "Lost in the Middle" paper suggests position affects retrieval, which may interact with token placement
- Break condition: If features cannot be steered to control outputs in new contexts, the hypothesis may not generalize beyond observed cases.

### Mechanism 2
- Claim: Predicting content tokens after function tokens drives feature growth and parameter updates during pre-training.
- Mechanism: The function→content prediction task has the highest loss throughout training (4.88 for 1.5B, 4.27 for 8B), making it the optimization bottleneck. This high challenge forces the model to develop features that can retrieve relevant context.
- Core assumption: High-loss token transitions are the primary driver of feature learning.
- Evidence anchors:
  - [abstract]: "predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features"
  - [Section 4.2]: Feature count grows from early to late pre-training; function tokens consistently activate more features than content tokens
  - [corpus]: No direct corpus confirmation of this specific training dynamic
- Break condition: If scaling studies show feature growth is driven by other factors (e.g., overall data volume), the mechanism would need revision.

### Mechanism 3
- Claim: A small set of high-frequency function tokens provides universal access to the feature space via a scale-free bipartite graph structure.
- Mechanism: ~122 function tokens (top 40% by occurrence) activate >70% of features in middle layers. The token-feature degree distribution follows a power law that persists across training checkpoints.
- Core assumption: Document coverage (uniform vs. bursty distribution) explains why function tokens generalize across contexts.
- Evidence anchors:
  - [Section 3.1]: Top 10 tokens activate 76.46% of features at layer 20 in Gemma2-9B
  - [Section 2.2]: Function tokens like 'of' appear uniformly across documents; content tokens like 'Tokyo' are bursty
  - [corpus]: No corpus papers confirm this scale-free property
- Break condition: If layer-specific analyses show different patterns, the universality claim would need qualification.

## Foundational Learning
- Concept: Sparse Autoencoders (SAE)
  - Why needed here: Decompose polysemantic neuron activations into interpretable monosemantic features for analysis
  - Quick check question: Can you explain how SAE reconstruction loss trades off against sparsity (L0 penalty)?
- Concept: Key-Value Memory interpretation of FFN
  - Why needed here: Provides the theoretical basis for how knowledge is stored and retrieved in Transformer layers
  - Quick check question: How does the ReLU activation function serve as an unnormalized weighting in this interpretation?
- Concept: Zipf's Law in token distributions
  - Why needed here: Explains why function/content token classification based on frequency approximates linguistic categories
  - Quick check question: Why does high document coverage matter for function tokens' role in memory retrieval?

## Architecture Onboarding
- Component map: Residual stream → LayerNorm → Self-Attention (context composition) → Add&Norm → FFN (feature activation) → Add&Norm → Output
- Critical path: 1. Identify target layer (middle layers like layer 20 in Gemma2-9B show highest interpretability) 2. Apply SAE decomposition to activations 3. Construct token-feature bipartite graph from activation pairs 4. Analyze degree distributions and cumulative coverage
- Design tradeoffs: Dictionary width (2^20 used) vs. computational cost; Reconstruction fidelity vs. sparsity (λ tuning varies by checkpoint); Function token threshold (40% cumulative frequency) vs. linguistic precision
- Failure signatures: SAE reconstruction score < 0.8 indicates poor feature extraction; Feature activation rate < 85% suggests insufficient coverage; Power law distribution breaking down indicates potential data issues
- First 3 experiments: 1. Replicate bipartite graph analysis on LLaMA-3.1-8B at layers 9, 20, 31 to verify universality 2. Track function→content vs. content→content loss curves during small-scale pre-training (1B tokens) to confirm optimization dynamics 3. Test steering effectiveness across multiple function tokens ('.', ',', '\n') to verify which tokens have strongest predictive feature activation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do function tokens acquire the ability to dynamically activate predictive features during training, while content tokens do not?
- Basis in paper: [explicit] Authors state this capability "likely emerges from the interplay of model architecture, data nature, training loss, and learning algorithm during training" but the specific interaction mechanism remains unknown.
- Why unresolved: The hypothesis explains what function tokens do but not how the training process selectively endows function tokens (not content tokens) with this capability.
- What evidence would resolve it: Ablation studies isolating each factor (architecture, data distribution, loss function, learning algorithm) combined with analysis of when feature-activation capability emerges during pre-training.

### Open Question 2
- Question: How does post-training (SFT, RL) modify the activation patterns of function tokens to enable new capabilities like instruction-following and chain-of-thought reasoning?
- Basis in paper: [explicit] Authors observe that post-training requires few steps yet yields substantial improvements, and that training only on function tokens via RL enhances reasoning, but how post-training changes activation patterns "remains an open question."
- Why unresolved: It is unclear whether post-training creates new activation patterns or merely repurposes existing pre-trained ones.
- What evidence would resolve it: Comparative analysis of function token activation patterns before and after post-training, combined with intervention studies that selectively disable specific post-training changes.

### Open Question 3
- Question: Why does the token-feature degree distribution exhibit a scale-free (power law) property throughout training, and what principles govern this phenomenon?
- Basis in paper: [explicit] Authors observe that "function tokens consistently activate most features, exhibiting a scale-free property" but state that "the dynamics of feature formation and the underlying reason of this scale-free property remain unclear."
- Why unresolved: The paper documents the phenomenon but does not explain the mechanism producing it or whether it follows from fundamental principles of learning or language structure.
- What evidence would resolve it: Mathematical analysis of whether the power law emerges from Zipfian token distributions, combined with controlled experiments varying corpus statistics and model scale.

### Open Question 4
- Question: Why is feature steerability concentrated in middle layers rather than shallow or deep layers of the transformer?
- Basis in paper: [explicit] Authors confirm that middle layers offer superior interpretability and steerability but state "the mechanistic explanation for why this steerability is concentrated in middle layers, rather than shallow or deep layers, remains elusive."
- Why unresolved: The observation is consistent with prior work but lacks a theoretical account of why middle layers specialize in this way.
- What evidence would resolve it: Layer-wise analysis of feature semantic granularity and circuit connectivity patterns to test whether middle layers occupy a "sweet spot" between low-level and abstract representations.

## Limitations
- SAE interpretation uncertainty: The function token hypothesis relies on sparse autoencoder interpretation, which may miss polysemantic features or create artifacts
- Limited steering validation: Steering experiments demonstrate proof-of-concept control but lack systematic evaluation of robustness across diverse contexts and prompts
- Single epoch analysis: Pre-training analysis focuses on one epoch rather than full convergence, potentially missing important dynamics

## Confidence
- **High confidence**: The empirical observation that function tokens activate more features than content tokens, and that a small set of function tokens achieves high cumulative coverage
- **Medium confidence**: The mechanism that function→content prediction drives feature learning during pre-training
- **Low confidence**: The steering experiments as evidence for feature semantics due to underspecified binary search approach and lack of robustness testing

## Next Checks
- Check 1: Apply bipartite graph analysis to LLaMA-3.1-8B at layers 9, 20, 31 to verify universality of top-10 function tokens consistently achieving >70% feature coverage
- Check 2: Train two 1.5B models (one with standard function→content loss distribution, another with equalized loss) to determine whether function→content loss is necessary for optimal feature development
- Check 3: Design systematic test suite with 100+ diverse prompts to measure steering success rate, activation stability, and robustness across function token types