---
ver: rpa2
title: Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based
  Attribution Fingerprinting
arxiv_id: '2511.06197'
source_url: https://arxiv.org/abs/2511.06197
tags:
- adversarial
- detection
- clean
- attacks
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel SHAP-based attribution fingerprinting
  method to enhance the robustness of IoT intrusion detection systems against adversarial
  attacks. The approach extracts SHAP-based attribution fingerprints from network
  traffic features using DeepExplainer, enabling the IDS to reliably distinguish between
  clean and adversarially perturbed inputs.
---

# Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting

## Quick Facts
- **arXiv ID**: 2511.06197
- **Source URL**: https://arxiv.org/abs/2511.06197
- **Reference count**: 40
- **Primary result**: SHAP-based attribution fingerprinting significantly improves IoT IDS robustness against adversarial attacks while enhancing model transparency.

## Executive Summary
This paper presents a novel SHAP-based attribution fingerprinting method to enhance the robustness of IoT intrusion detection systems against adversarial attacks. The approach extracts SHAP-based attribution fingerprints from network traffic features using DeepExplainer, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. The model leverages a deep autoencoder trained on clean SHAP attribution vectors to detect adversarial examples based on reconstruction error, without requiring labeled attack data. Experimental results on the CIC-IoT2023 dataset demonstrate significant improvements over state-of-the-art adversarial defense methods, achieving accuracy above 0.99 and F1-scores above 0.99 across FGSM, PGD, and DeepFool attacks.

## Method Summary
The method trains a baseline FFNN classifier on IoT network traffic, then uses SHAP DeepExplainer to generate attribution vectors for each input. These SHAP vectors capture feature importance patterns. An autoencoder is trained exclusively on clean SHAP vectors to learn normal attribution patterns. During inference, the autoencoder reconstructs incoming SHAP vectors and calculates reconstruction error. Inputs with error exceeding threshold τ=0.02 are flagged as adversarial. The approach works unsupervised - no attack labels needed during training.

## Key Results
- Achieved accuracy >0.99 and F1-scores >0.99 against FGSM, PGD, and DeepFool attacks
- Outperformed standard adversarial training baselines on CIC-IoT2023 dataset
- Demonstrated robust detection across diverse attack scenarios without labeled attack data
- Successfully identified characteristic rank shifts in feature importance under adversarial conditions

## Why This Works (Mechanism)

### Mechanism 1: Attribution Drift Detection
Adversarial perturbations distort the model's internal reasoning patterns, creating SHAP attribution fingerprints that deviate from clean data distributions. The autoencoder learns to compress and reconstruct normal attribution patterns, with high reconstruction error indicating adversarial inputs.

### Mechanism 2: Unsupervised Reconstruction Anomaly Scoring
The autoencoder trained only on clean SHAP vectors serves as an anomaly detector. Since adversarial SHAP vectors lie outside the learned clean manifold, they produce higher reconstruction errors that can be thresholded for detection.

### Mechanism 3: Feature Rank Shift Sensitivity
Adversarial attacks induce characteristic shifts in feature importance rankings that differ from natural traffic variations. The method leverages these disrupted correlation structures as detectable anomalies.

## Foundational Learning

- **SHAP (SHapley Additive exPlanations) & DeepExplainer**: The core "sensor" that assigns feature importance values using Shapley values from game theory. DeepExplainer approximates this for deep networks via gradient information. *Quick check*: Does a high SHAP value mean the feature value is high, or that the feature strongly influenced the prediction?

- **Autoencoders for Anomaly Detection**: The detection engine that learns to reconstruct normal data. Trained only on clean SHAP vectors, it flags inputs with high reconstruction error as anomalies. *Quick check*: Why train the autoencoder only on clean SHAP vectors rather than mixing clean and attack samples?

- **White-box Adversarial Attacks (FGSM, PGD)**: The threat models that define the attack space. FGSM uses single-step gradient ascent, while PGD iteratively applies perturbations. *Quick check*: Could an attacker craft perturbations that fool both the classifier and mimic clean SHAP vectors?

## Architecture Onboarding

- **Component map**: Data In -> Reference IDS (Inference) -> Extract SHAP Vector -> Detector (Reconstruct) -> Calculate Error -> (Error > τ?) -> Flag Adversarial

- **Critical path**: Network traffic flows through the FFNN classifier, SHAP values are extracted, the autoencoder attempts reconstruction, and the reconstruction error determines if the input is adversarial.

- **Design tradeoffs**: 
  - Latency vs. Robustness: SHAP computation adds significant overhead
  - Threshold Sensitivity: τ setting balances false positives vs security
  - Static vs. Adaptive: Current model doesn't handle concept drift

- **Failure signatures**:
  - High False Positives: Legitimate traffic pattern changes trigger detection
  - Undetected Attacks: Zero-day attacks mimicking clean SHAP patterns slip through
  - Gradient Masking: Non-smooth models produce uninformative SHAP values

- **First 3 experiments**:
  1. Threshold Calibration: Establish baseline reconstruction error on clean validation data
  2. Attack Vector Injection: Test detection rates across FGSM and PGD variants
  3. Baseline Comparison: Compare ROC-AUC against adversarially trained models

## Open Questions the Paper Calls Out

- **Multi-class extension**: How does the approach perform when extended from binary to multi-class intrusion detection scenarios? The current binary evaluation leaves multi-class generalization unexplored.

- **Adaptive learning integration**: Can the model integrate adaptive online or continual learning to detect evolving or zero-day adversarial threats? The static training approach needs validation for dynamic threat landscapes.

- **Temporal architecture improvements**: Do temporal or transformer-based architectures improve detection robustness compared to the standard feedforward autoencoder? The current model may miss temporal dependencies in network traffic.

## Limitations

- Key architectural parameters (FFNN depth, autoencoder latent dimension, SHAP background size) are unspecified, making exact reproduction difficult
- Effectiveness depends on assumption that adversarial attacks consistently produce detectable attribution pattern deviations
- No evaluation of attacks specifically designed to minimize attribution shifts or gradient-masked models
- Static training approach may not handle concept drift in evolving network environments

## Confidence

- **High Confidence**: Conceptual framework linking SHAP attribution to adversarial detection is well-established; experimental methodology is clearly specified
- **Medium Confidence**: Performance metrics are impressive but depend heavily on threshold calibration and CIC-IoT2023 dataset characteristics
- **Low Confidence**: No analysis of adaptive attacks that could learn to mimic clean SHAP patterns or handle gradient-masked models

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary τ from 0.01 to 0.05 and plot ROC curves to verify separability isn't threshold-dependent
2. **Attack-Specific Robustness**: Test against adaptive attacks minimizing SHAP reconstruction error to assess defense under concerted effort
3. **Dataset Generalization**: Apply trained detector to TON_IoT dataset to verify approach isn't overfitting to CIC-IoT2023's specific characteristics