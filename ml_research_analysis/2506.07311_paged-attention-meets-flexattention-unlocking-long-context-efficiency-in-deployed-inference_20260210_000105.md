---
ver: rpa2
title: 'Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in
  Deployed Inference'
arxiv_id: '2506.07311'
source_url: https://arxiv.org/abs/2506.07311
tags:
- memory
- latency
- inference
- attention
- pagedattention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PagedAttention, an OS-inspired paging mechanism\
  \ for KV caches, integrated into IBM\u2019s Foundation Model Stack (FMS) via PyTorch\u2019\
  s FlexAttention. The method addresses severe memory inefficiencies in long-context\
  \ LLM inference by partitioning KV caches into fixed-size pages, enabling dynamic,\
  \ fine-grained memory management with near-zero fragmentation."
---

# Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference

## Quick Facts
- **arXiv ID:** 2506.07311
- **Source URL:** https://arxiv.org/abs/2506.07311
- **Reference count:** 17
- **Primary result:** Near-FlashAttention performance with significantly reduced inference latency and near-zero memory fragmentation for long-context LLM inference.

## Executive Summary
This paper introduces PagedAttention, an OS-inspired paging mechanism for KV caches, integrated into IBM's Foundation Model Stack (FMS) via PyTorch's FlexAttention. The method addresses severe memory inefficiencies in long-context LLM inference by partitioning KV caches into fixed-size pages, enabling dynamic, fine-grained memory management with near-zero fragmentation. The core contribution is a lock-free page manager and a fused FlexAttention kernel that executes attention over non-contiguous memory layouts with near-FlashAttention performance. Experiments on NVIDIA L4 GPU (24GB) show significantly reduced inference latency, scaling linearly (~2x) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching.

## Method Summary
PagedAttention partitions KV caches into fixed-size pages, enabling dynamic memory management and near-zero fragmentation. The system uses a lock-free page manager to allocate and deallocate pages efficiently, and a fused FlexAttention kernel to execute attention over non-contiguous memory layouts. This approach is integrated into IBM's Foundation Model Stack (FMS) via PyTorch's FlexAttention, providing a seamless upgrade path for deployed inference systems. The implementation achieves numerical equivalence with standard attention while delivering near-FlashAttention performance.

## Key Results
- Near-FlashAttention performance with significantly reduced inference latency for long-context inference.
- Linear scaling of latency (~2x) with sequence length from 128 to 2048 tokens when using global KV cache.
- Minimal incremental memory usage from paged attention, becoming noticeable only at sequence lengths exceeding 2048 tokens.

## Why This Works (Mechanism)
PagedAttention works by partitioning KV caches into fixed-size pages, which allows for dynamic allocation and deallocation of memory without fragmentation. The lock-free page manager ensures efficient memory operations, while the fused FlexAttention kernel can handle non-contiguous memory layouts, maintaining high performance. This OS-inspired paging mechanism enables fine-grained memory management, reducing overhead and improving scalability for long-context inference.

## Foundational Learning
- **KV Cache Partitioning:** Dividing KV caches into fixed-size pages to enable dynamic memory management and reduce fragmentation.
- **Lock-free Page Management:** Using lock-free data structures to efficiently allocate and deallocate memory pages without contention.
- **Non-contiguous Memory Layouts:** Handling attention computations over memory that is not stored contiguously, requiring specialized kernels.
- **Fused FlexAttention:** Combining attention operations into a single kernel to reduce overhead and improve performance.
- **Global KV Cache:** Storing KV cache across multiple tokens to avoid recomputation and reduce latency.
- **Numerical Equivalence:** Ensuring that the PagedAttention implementation produces the same results as standard attention.

## Architecture Onboarding

**Component Map:**
PagedAttention -> Lock-free Page Manager -> FlexAttention Kernel -> FMS (via PyTorch)

**Critical Path:**
1. KV cache partitioning into fixed-size pages
2. Lock-free page allocation/deallocation
3. Fused FlexAttention kernel execution over non-contiguous memory
4. Integration with FMS for inference

**Design Tradeoffs:**
- **Memory vs. Performance:** Fixed-size pages reduce fragmentation but may lead to internal fragmentation if page size is not optimal.
- **Complexity vs. Flexibility:** Lock-free page manager adds complexity but enables efficient memory operations.
- **Kernel Fusion vs. Modularity:** Fused FlexAttention improves performance but may reduce modularity and ease of debugging.

**Failure Signatures:**
- Memory leaks or fragmentation if page size is not well-tuned
- Performance degradation if lock-free page manager experiences contention
- Numerical inaccuracies if fused kernel introduces rounding errors

**3 First Experiments:**
1. Benchmark PagedAttention vs. standard attention on NVIDIA L4 GPU with sequence lengths from 128 to 2048 tokens.
2. Measure memory fragmentation with and without PagedAttention at various sequence lengths.
3. Evaluate the impact of different page sizes on memory usage and performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section suggests areas for further investigation, such as scalability to longer sequence lengths and multi-GPU deployments.

## Limitations
- Experimental scope limited to a single GPU model (NVIDIA L4) and sequence lengths up to 2048 tokens.
- Memory savings attributed to KV cache management without fully disentangling contributions from other FMS optimizations.
- "Near-zero fragmentation" claim not empirically validated with fragmentation metrics.
- Focus on inference-time efficiency without addressing training implications or multi-GPU/multi-node deployments.

## Confidence
- **High confidence** in the technical feasibility and correctness of the PagedAttention implementation, given the open-sourced code and numerical equivalence with standard attention.
- **Medium confidence** in the latency and memory usage claims, as results are based on a limited experimental setup and do not explore edge cases or alternative hardware.
- **Low confidence** in the generality of the approach for extreme sequence lengths (>2048 tokens) and for multi-GPU or distributed inference scenarios, as these are not addressed in the paper.

## Next Checks
1. Conduct experiments on multiple GPU architectures (e.g., A100, H100) and sequence lengths beyond 2048 tokens to assess scalability and generalizability.
2. Measure and report actual memory fragmentation metrics with and without PagedAttention to substantiate the "near-zero fragmentation" claim.
3. Evaluate the impact of PagedAttention on multi-GPU and distributed inference settings, including cross-node KV cache management and communication overhead.