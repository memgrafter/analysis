---
ver: rpa2
title: What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency
  Maps in Explainable AI (XAI)
arxiv_id: '2504.17023'
source_url: https://arxiv.org/abs/2504.17023
tags:
- saliency
- metrics
- maps
- user
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared three popular saliency map methods (Grad-CAM,
  Guided Backpropagation, and LIME) for evaluating AI image classifiers across subjective
  measures (trust and satisfaction), objective user abilities (accuracy prediction
  and error detection), and mathematical metrics (fidelity, robustness, complexity,
  and localization). Results showed no differences in trust or satisfaction across
  methods.
---

# What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)

## Quick Facts
- **arXiv ID:** 2504.17023
- **Source URL:** https://arxiv.org/abs/2504.17023
- **Reference count:** 40
- **Key outcome:** Mathematical metrics poorly correlate with user abilities in evaluating saliency maps; Grad-CAM best improves user accuracy predictions and error detection

## Executive Summary
This study evaluates three popular saliency map methods (Grad-CAM, Guided Backpropagation, and LIME) for explainable AI image classifiers across subjective measures (trust and satisfaction), objective user abilities (accuracy prediction and error detection), and mathematical metrics (fidelity, robustness, complexity, and localization). Results show no differences in trust or satisfaction across methods, but Grad-CAM significantly outperforms others in improving user accuracy predictions and classification error detection. Mathematical metrics achieved the most favorable scores for Guided Backpropagation. Notably, mathematical metrics were largely uncorrelated with user abilities, except for deletion and energy pointing game metrics, suggesting that mathematical metrics and user-centered evaluations often diverge.

## Method Summary
The study compared three saliency methods (Grad-CAM, Guided Backpropagation, and LIME) through three evaluation approaches: subjective user measures (trust and satisfaction), objective user abilities (accuracy prediction and error detection), and mathematical metrics (fidelity, robustness, complexity, and localization). Participants were presented with image classifications alongside different saliency maps and asked to assess trust, satisfaction, predict accuracy, and detect errors. Mathematical metrics were computed independently. The study controlled for task difficulty and ensured balanced presentation across methods.

## Key Results
- No differences in trust or satisfaction across saliency methods
- Grad-CAM significantly outperformed other methods in improving user accuracy predictions and classification error detection
- Mathematical metrics showed Guided Backpropagation achieved the most favorable scores
- Mathematical metrics were largely uncorrelated with user abilities, except for deletion and energy pointing game metrics

## Why This Works (Mechanism)
Assumption: Grad-CAM likely performs better for user abilities because its heatmap visualization more clearly highlights relevant image regions for classification decisions, making it easier for users to understand model reasoning and detect potential errors compared to the more abstract visualizations of Guided Backpropagation or LIME.

## Foundational Learning
Unknown: The paper does not explicitly discuss what foundational learning or theoretical insights this work builds upon or contributes to the field of explainable AI.

## Architecture Onboarding
Unknown: The paper does not provide specific guidance on how to integrate or onboard these saliency map evaluation methods into existing AI architecture development or deployment workflows.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions for future research in the provided content.

## Limitations
- Findings may not generalize beyond the three tested saliency methods or image classification tasks
- Results may not extend to other AI architectures, domains like NLP or tabular data, or different explanation types
- Small sample sizes (n=30-36 per condition) and controlled experimental settings limit confidence in real-world applicability

## Confidence
- **Confidence in mathematical metrics correlation findings: Medium** - Limited by small sample sizes and specific task contexts
- **Confidence in Grad-CAM superiority for user abilities: High** - Robust statistical significance and effect sizes within tested conditions, though no advantage in subjective measures
- **Confidence in generalizability: Low** - Single task, three methods tested, no NLP or tabular data evaluation

## Next Checks
1. Replicate the study across diverse AI tasks (e.g., object detection, segmentation, NLP) and model architectures to test generalizability
2. Conduct longitudinal field studies measuring actual user decision-making and trust calibration in real-world deployment contexts
3. Test additional saliency methods and hybrid approaches combining multiple explanation types to determine if better overall performance is achievable