---
ver: rpa2
title: 'Concept-SAE: Active Causal Probing of Visual Model Behavior'
arxiv_id: '2509.22015'
source_url: https://arxiv.org/abs/2509.22015
tags:
- concept
- layer
- concepts
- resnet-18
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Concept-SAE, a framework that transforms
  sparse autoencoders (SAEs) from passive feature extractors into active instruments
  for causal probing of neural network behavior. The key innovation is a dual-supervision
  strategy that grounds concept tokens to human-defined concepts through both existence
  scores and spatial localization, while retaining free tokens for open-ended discovery.
---

# Concept-SAE: Active Causal Probing of Visual Model Behavior

## Quick Facts
- **arXiv ID**: 2509.22015
- **Source URL**: https://arxiv.org/abs/2509.22015
- **Authors**: Jianrong Ding; Muxi Chen; Chenchen Zhao; Qiang Xu
- **Reference count**: 22
- **Primary result**: Up to 40.77% accuracy improvement on adversarial samples via causal concept intervention

## Executive Summary
Concept-SAE transforms sparse autoencoders into active instruments for causal probing of neural network behavior through a dual-supervision strategy. The framework grounds concept tokens to human-defined concepts using both existence scores and spatial localization while retaining free tokens for open-ended discovery. This produces concept representations that are faithful and spatially localized, significantly outperforming baseline methods in disentanglement. The validated fidelity enables direct causal intervention on predictions to correct errors and systematic localization of adversarial vulnerabilities to specific layers.

## Method Summary
Concept-SAE is a hybrid sparse autoencoder framework that decomposes neural network features into supervised "concept tokens" and unsupervised "free tokens." The method uses a three-stage training process: first training a Concept Tokenizer to predict concept existence scores and spatial masks from features, then training a Concept Aggregator to reconstruct features from tokens (with the tokenizer frozen), and finally training Free Tokenizer/Aggregator modules jointly to capture residual information. Dual supervision from vision-language models and segmentation models anchors concept tokens to human-defined concepts, while the free pathway maintains reconstruction fidelity. The framework enables causal probing by modifying concept token scores and passing them through the frozen aggregator to generate counterfactual features.

## Key Results
- Concept-SAE achieves significantly higher Localization Ratio (LocR > 1.0) than baseline methods, demonstrating superior disentanglement
- Direct causal intervention on concept tokens improves adversarial sample accuracy by up to 40.77%
- Layers with higher Jensen-Shannon distances between clean and adversarial concept score distributions yield stronger robustness gains after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Semantic Anchoring via Dual-Supervision
The framework forces disentanglement of specific concepts from background features by explicitly supervising both concept existence scores and spatial location masks. This prevents semantic drift seen in standard SAEs by binding specific neurons to human-defined concepts through ground-truth labels generated by external vision-language and segmentation models.

### Mechanism 2: Hybrid Decomposition (Concept + Residual)
Separating the latent space into supervised concept tokens and unsupervised free tokens allows the model to explain known concepts while retaining full reconstruction capability. The parallel structure ensures known attributes are handled by concept modules while free modules capture residual information not covered by predefined concepts.

### Mechanism 3: Invertible Intervention for Causal Probing
Because the Aggregator is trained to invert token representations back into feature space, modifying concept token scores allows synthesis of counterfactual features to test causal influence. Adjusting a concept score (e.g., setting "beard" to 0) and passing through the frozen Aggregator generates synthetic features that, when substituted into the vision model, alter predictions to establish causal links.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAE)**
  - **Why needed here**: Concept-SAE modifies standard SAE objectives to address ungrounded features
  - **Quick check question**: How does an L1 penalty on latent activations encourage monosemanticity in a standard autoencoder?

- **Concept: Feature Superposition**
  - **Why needed here**: The paper addresses ambiguous, ungrounded features resulting from superposition
  - **Quick check question**: Why can't we simply look at a single neuron in a dense layer to understand a complex visual concept?

- **Concept: Jensen-Shannon (JS) Divergence**
  - **Why needed here**: Used to quantify vulnerability by measuring distance between concept score distributions
  - **Quick check question**: Why is JS divergence preferred over simple mean difference when comparing concept activation distributions?

## Architecture Onboarding

- **Component map**: Vision Model -> Feature Extractor -> Concept Tokenizer + Free Tokenizer -> Concept Aggregator + Free Aggregator -> Reconstruction
- **Critical path**: Concept Label Generation is the most brittle dependency; Tokenizer -> Aggregator flow must be staged (freeze tokenizer before training aggregator)
- **Design tradeoffs**: Supervision vs. Discovery (strict concept supervision guarantees interpretability but limits novel feature discovery); Layer Selection (shallow layers preserve spatial details but may lack semantic abstraction)
- **Failure signatures**: High Entropy in Free Tokens (concept set insufficient); Intervention Artifacts (synthetic features out-of-distribution)
- **First 3 experiments**: 
  1. Localization Ratio Test comparing Concept-SAE vs. CEM on reconstructing features within concept mask vs. background
  2. Causal Correction: For misclassified gender examples, manually set concept scores (e.g., beard=1.0 for males) and measure prediction flip rate
  3. Robustness Probing: Generate adversarial samples, calculate JS divergence per layer, verify if high-divergence layers correlate with highest accuracy drops

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the framework be adapted to effectively handle abstract, textural, or globally distributed concepts that lack distinct spatial localization?
- **Basis**: Section 5 explicitly states extending to abstract concepts where spatial segmentation is difficult remains an important direction
- **Why unresolved**: Current methodology depends on spatial segmentation masks, presupposing concepts are localized visual objects
- **What evidence would resolve it**: Modified supervision mechanism successfully recovering non-local attributes without relying on spatial masks

### Open Question 2
- **Question**: To what extent does noise or hallucination in external supervision models degrade causal probing fidelity?
- **Basis**: Section 5 highlights that noise from VLM or segmentation models may lead to imprecise concept identification
- **Why unresolved**: Framework treats generated concept labels as ground truth without quantifying robustness against supervisor errors
- **What evidence would resolve it**: Sensitivity analysis measuring degradation of concept disentanglement and intervention success rates under varying synthetic noise levels

### Open Question 3
- **Question**: Does correlation between concept score distribution shifts (JS distance) and layer-wise vulnerability hold for sophisticated adversarial attacks beyond FGSM?
- **Basis**: RQ3 methodology relies exclusively on FGSM for generating adversarial samples
- **Why unresolved**: FGSM is simple; uncertain if multi-step or semantic attacks disrupt concept score distributions similarly
- **What evidence would resolve it**: Repeating vulnerability localization experiments using stronger attacks (e.g., AutoAttack) to verify if same layers exhibit high JS distance

## Limitations
- Dependency on high-quality external supervision introduces brittleness, with unspecified VLM architecture creating uncertainty
- Staging requirement between tokenizer and aggregator training creates fragility without empirical validation of failure modes
- No validation that synthetic features from interventions remain within natural activation manifolds

## Confidence
- **High Confidence**: Concept disentanglement (LocR > 1.0), causal intervention effectiveness on adversarial samples
- **Medium Confidence**: Adversarial vulnerability localization via JS divergence, layer-wise intervention analysis
- **Low Confidence**: Quality of external supervision, assumption that free tokens remain complementary rather than entangled

## Next Checks
1. **External Supervision Quality Audit**: Generate validation set with human verification of VLM existence scores and segmentation masks to quantify supervision noise propagation
2. **Out-of-Distribution Intervention Test**: Generate deliberately extreme concept scores (e.g., "beard" = 10.0) and measure whether aggregator produces features within natural activation distribution
3. **Free Token Monosemanticity Analysis**: Compute correlation between free token activations and individual concept scores to detect concept leakage into free pathway