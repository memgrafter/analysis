---
ver: rpa2
title: 'How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study'
arxiv_id: '2505.15404'
source_url: https://arxiv.org/abs/2505.15404
tags:
- safety
- reasoning
- data
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to enhance the safety of Large Reasoning
  Models (LRMs) through Supervised Fine-Tuning (SFT). The authors observe that directly
  distilling safe responses from DeepSeek-R1 fails to significantly improve safety.
---

# How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study
## Quick Facts
- arXiv ID: 2505.15404
- Source URL: https://arxiv.org/abs/2505.15404
- Reference count: 25
- Directly distilling safe responses from DeepSeek-R1 fails to significantly improve safety; explicit addressing of identified failure patterns substantially reduces attack success rate from 77.0% to 7.0%

## Executive Summary
This paper investigates safety enhancement for Large Reasoning Models (LRMs) through Supervised Fine-Tuning (SFT), challenging the assumption that simply distilling safe responses from existing models is sufficient. The authors conduct a comprehensive empirical study identifying three critical failure patterns in reasoning models' safety behaviors: lack of safety awareness, overthinking, and inconsistency between reasoning and final answers. Through systematic experimentation across four different LRM architectures, they demonstrate that explicitly addressing these failure patterns during distillation can dramatically reduce attack success rates while maintaining task performance. The study also reveals that shorter reasoning processes or template-based approaches can achieve comparable safety outcomes with reduced complexity.

## Method Summary
The study employs Supervised Fine-Tuning on Large Reasoning Models using a curated dataset of safe responses and adversarial prompts. The researchers first distill from DeepSeek-R1 as a baseline safety approach, then systematically identify failure patterns through manual analysis of model outputs. They implement targeted fine-tuning strategies to address each failure pattern: incorporating explicit safety awareness training, preventing overthinking through balanced reasoning length, and ensuring consistency between intermediate reasoning and final outputs. The methodology includes mixing benign math reasoning data to balance safety with over-refusal, and comparing performance across different reasoning chain lengths and formats.

## Key Results
- Directly distilling safe responses from DeepSeek-R1 achieves only modest safety improvements
- Explicitly addressing identified failure patterns reduces attack success rate from 77.0% to 7.0% across four models
- Shorter or template-based reasoning processes achieve comparable safety performance while being easier to learn than long reasoning chains
- Mixing benign math reasoning data during safety fine-tuning helps balance safety and over-refusal behaviors

## Why This Works (Mechanism)
The approach works by targeting specific failure modes in LRM safety reasoning rather than applying generic fine-tuning. By identifying and correcting systematic flaws in how models process safety-related prompts—including failures to recognize harmful content, excessive deliberation on dangerous requests, and contradictions between reasoning and conclusions—the method creates more robust safety mechanisms. The incorporation of benign math data prevents the model from becoming overly cautious while maintaining safety standards, addressing the common trade-off between safety and utility in model fine-tuning.

## Foundational Learning
**Large Reasoning Models (LRMs)**: Advanced AI models that explicitly generate intermediate reasoning steps before producing final answers, enabling better interpretability and task performance on complex problems. Needed to understand the target of safety enhancement; quick check: models produce visible chain-of-thought reasoning.

**Supervised Fine-Tuning (SFT)**: A training approach where models learn from labeled examples of desired behaviors, contrasting with unsupervised or reinforcement learning methods. Needed to understand the core methodology; quick check: models are trained on curated datasets of safe responses.

**Jailbreaking**: Adversarial techniques that manipulate model prompts to bypass safety constraints and elicit harmful outputs. Needed to contextualize the attack success rate metric; quick check: specialized prompts designed to circumvent safety filters.

**Attack Success Rate (ASR)**: The percentage of adversarial attempts that successfully elicit unsafe responses from a model, serving as the primary safety evaluation metric. Needed to quantify safety improvements; quick check: ratio of successful jailbreaks to total attempts.

## Architecture Onboarding
**Component map**: Base LRM architecture -> Safety Fine-Tuning Dataset -> Distillation from DeepSeek-R1 -> Failure Pattern Analysis -> Targeted SFT -> Safety Evaluation
**Critical path**: Data curation and distillation from DeepSeek-R1 -> Manual failure pattern identification -> Targeted fine-tuning addressing specific failures -> Evaluation with adversarial prompts
**Design tradeoffs**: Long reasoning chains provide thorough safety consideration but increase computational cost and overfitting risk; shorter chains are more efficient but may miss critical safety considerations; template-based approaches balance consistency with flexibility
**Failure signatures**: Models fail when they lack explicit safety awareness in reasoning chains, overthink harmful requests leading to eventual compliance, or produce reasoning that contradicts final outputs; these manifest as successful jailbreak attempts in evaluation
**First experiments to run**:
1. Baseline distillation from DeepSeek-R1 safety responses without addressing failure patterns
2. Fine-tuning targeting only the safety awareness failure pattern
3. Evaluation comparing long reasoning chains versus template-based approaches for safety performance

## Open Questions the Paper Calls Out
None

## Limitations
- The safety evaluation relies on curated adversarial examples that may not represent real-world attack distributions
- All safety improvements are achieved through distillation from a single source (DeepSeek-R1), limiting generalizability
- The trade-off between reasoning length and task accuracy is not thoroughly examined across diverse reasoning tasks

## Confidence
- High: The identification of three failure patterns and their systematic addressing is well-supported by empirical evidence
- Medium: The finding that mixing benign math data improves safety-over-refusal balance requires further validation across task distributions
- Low: The claim about template-based reasoning achieving comparable safety performance lacks comprehensive comparative analysis

## Next Checks
1. Evaluate fine-tuned models against dynamically generated adversarial prompts collected from actual deployment scenarios over a 3-month period
2. Apply the same safety fine-tuning methodology to distill from multiple base models (e.g., OpenAI o1, Claude 3) to test generalizability
3. Monitor safety performance over extended inference sessions (1000+ consecutive prompts) to detect potential degradation or catastrophic forgetting