---
ver: rpa2
title: 'Semantic-Condition Tuning: Fusing Graph Context with Large Language Models
  for Knowledge Graph Completion'
arxiv_id: '2510.08966'
source_url: https://arxiv.org/abs/2510.08966
tags:
- knowledge
- graph
- semantic
- language
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively fusing structured
  knowledge from Knowledge Graphs (KGs) with the parametric knowledge of Large Language
  Models (LLMs) for Knowledge Graph Completion (KGC). The proposed Semantic-Condition
  Tuning (SCT) framework moves beyond simple prefix-tuning by introducing a deep,
  feature-level integration of KG context.
---

# Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion

## Quick Facts
- **arXiv ID:** 2510.08966
- **Source URL:** https://arxiv.org/abs/2510.08966
- **Authors:** Ruitong Liu; Yan Wen; Te Sun; Yunjia Wu; Pingyang Huang; Zihang Yu; Siyuan Li
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on KG completion benchmarks by integrating structured knowledge with LLM parametric knowledge through semantic-condition tuning

## Executive Summary
This paper introduces Semantic-Condition Tuning (SCT), a framework that fuses structured knowledge from Knowledge Graphs (KGs) with the parametric knowledge of Large Language Models (LLMs) for Knowledge Graph Completion (KGC). The key innovation is moving beyond simple prefix-tuning by introducing a deep, feature-level integration of KG context through a two-stage process: first pre-training a Semantic Graph Module using relation-centric message passing, then adaptively modulating LLM embeddings via a Condition-Adaptive Fusion Module. Extensive experiments demonstrate that SCT achieves state-of-the-art performance on link prediction and triple classification benchmarks, with significant gains over strong baselines like SSQR-LLaMA2.

## Method Summary
SCT comprises two core modules: a Semantic Graph Module (SGM) that extracts a context-aware semantic condition vector from the KG using relation-centric message passing guided by LLM-enhanced semantic descriptions, and a Condition-Adaptive Fusion Module that performs a feature-wise affine transformation on the LLM's input embeddings based on this condition. The framework operates in two stages: Stage 1 pre-trains the SGM using RotatE-style scoring for 20 epochs, while Stage 2 fine-tunes the entire system (with frozen SGM) using LoRA adapters on the LLM backbone. The semantic condition vector is generated through Top-K neighbor selection based on cosine similarity of relation definitions produced by GPT-4O and encoded by Sentence-BERT.

## Key Results
- Achieves state-of-the-art MRR and Hits@1/3/10 scores on WN18RR and FB15k-237 link prediction benchmarks
- Outperforms strong baselines like SSQR-LLaMA2 with significant performance gains in accuracy and F1 metrics
- Ablation studies confirm the critical contributions of both the Semantic Graph Module and Condition-Adaptive Fusion Module
- Optimal Top-K neighbor selection identified at K=10, balancing context richness with noise reduction

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Guided Context Distillation
The Semantic Graph Module uses relation-centric message passing guided by explicit semantic definitions to filter noise more effectively than structural adjacency alone. By calculating cosine similarity between semantic vectors of central relations and neighbors, it creates a context vector reflecting functional similarity rather than just graph proximity. This approach assumes relation names are ambiguous and require explicit definitions for accurate neighbor selection. Evidence shows that knowledge-enhanced definitions significantly outperform lexical matching in neighbor selection quality.

### Mechanism 2: Pre-Fusion Feature Modulation
The Condition-Adaptive Fusion Module applies a feature-wise linear modulation (FiLM) to text embeddings rather than concatenating prefix tokens. The semantic condition vector generates scaling and shifting vectors via MLPs, transforming the input representation before it enters the LLM. This assumes the graph-derived semantic condition can be mapped to the LLM's embedding space via linear transformation to meaningfully alter semantic direction. Ablation studies demonstrate that removing this fusion causes larger performance drops than removing the graph module entirely.

### Mechanism 3: Structural Pre-Alignment via RotatE
Pre-training the graph module using RotatE scoring functions provides robust initialization for the downstream generative task. The self-supervised pre-training forces the module to learn complex relational patterns before interfacing with the LLM, ensuring the semantic condition vector is structurally meaningful. This assumes geometric properties required for link prediction correlate with semantic features needed for LLM reasoning. Ablation studies show RotatE outperforms simpler scoring functions due to better modeling of complex relations.

## Foundational Learning

- **Concept: Feature-wise Linear Modulation (FiLM)**
  - *Why needed here:* The core fusion mechanism is not an attention layer but a FiLM layer. Understanding how $\gamma$ (scaling) and $\beta$ (shifting) vectors condition a network is necessary to debug fusion failures.
  - *Quick check question:* If the MLP projecting $c_S$ to $\gamma$ outputs a vector of all 1s, what happens to the text embedding $X$? (Answer: It remains unchanged except for the bias $\beta$)

- **Concept: Relation-Centric Message Passing**
  - *Why needed here:* Unlike standard GNNs that pass messages between nodes, this architecture passes messages between edges (relations). This is crucial for handling dynamic contexts where the relation defines the entity's role.
  - *Quick check question:* In a standard GCN, node A updates based on neighbors. In this paper, how does the "Treats" relation update? (Answer: It updates based on semantic similarity and aggregation of neighboring relations in the local subgraph)

- **Concept: Negative Sampling in Self-Supervision**
  - *Why needed here:* The paper uses specific Stage 1 training involving "corrupted triples." Understanding that the model learns by distinguishing true facts $(h, r, t)$ from false ones $(h', r, t)$ is necessary to interpret pre-training.
  - *Quick check question:* Why is the loss function minimized? (Answer: To maximize the score of true triples while minimizing the score of generated negative triples)

## Architecture Onboarding

- **Component map:** GPT-4o (generates definitions) -> Sentence-BERT (encodes definitions) -> Semantic Graph Module (Top-K selection + message passing) -> Condition-Adaptive Fusion Module (MLPs generate $\gamma, \beta$) -> Alpaca-7B with LoRA (takes modulated embeddings)

- **Critical path:** The quality of the Semantic Condition Vector ($c_S$) is the linchpin. If Top-K selection retrieves irrelevant neighbors due to poor definition encoding, the CAFM will modulate text embeddings with garbage signals, causing the LLM to hallucinate or fail to reason.

- **Design tradeoffs:**
  - *Top-K Selection:* K=10 is optimal. Low K risks missing multi-hop context; high K risks over-smoothing and introducing noise.
  - *Scoring Function:* RotatE chosen over TransE. RotatE is more expressive but computationally heavier than simple distance-based models.

- **Failure signatures:**
  - *Prefix-like behavior:* If LLM ignores graph context, check gradient flow through CAFM. If $\gamma$ near 1.0 and $\beta$ near 0.0, MLPs are failing to project the condition vector meaningfully.
  - *Semantic Drift:* If model predicts "USA" for "Barack Obama" but relation was "profession," knowledge enhancement may be failing to distinguish fine-grained semantics.

- **First 3 experiments:**
  1. *Ablation on Modulation:* Run framework using concatenation instead of affine transformation on FB15k-237 to reproduce "w/o Fusion" result and validate fusion architecture contribution.
  2. *Sensitivity Analysis (K):* Vary K in {4, 10, 16, 32} to observe trade-off between context richness and noise, confirming the "Goldilocks" zone.
  3. *Qualitative Neighbor Check:* Inspect Top-5 neighbors for sample queries to ensure "Knowledge Enhanced" definitions are superior to "Lexical Matching." If not, LLM-based definition generation needs prompt engineering.

## Open Questions the Paper Calls Out

### Open Question 1
- *Question:* Can the Semantic Graph Module be extended to a hierarchical generation mechanism to better capture global graph context and improve reasoning depth?
- *Basis in paper:* The authors explicitly state they "plan to introduce a hierarchical semantic condition generation mechanism to enhance reasoning depth" to overcome the limitation of "limited reasoning depth."
- *Why unresolved:* Current implementation aggregates a "local graph neighborhood" which may fail to capture complex, multi-hop dependencies required for deeper reasoning.
- *What evidence would resolve it:* Experiments comparing current local aggregation against hierarchical approaches on multi-hop reasoning datasets, measuring MRR improvements as path length increases.

### Open Question 2
- *Question:* How can the SCT framework be adapted to incorporate temporal awareness and handle dynamic knowledge graphs where facts evolve over time?
- *Basis in paper:* The Conclusion identifies "insufficient adaptability to dynamic knowledge graphs" as a key challenge and proposes "incorporating temporal awareness" as a primary direction.
- *Why unresolved:* Current framework treats KG as static without encoding timestamps or temporal validity, limiting utility in real-world scenarios where knowledge changes rapidly.
- *What evidence would resolve it:* Application of modified SCT framework (incorporating time embeddings or temporal GNN layers) to temporal KGC benchmarks like ICEWS to assess performance on time-sensitive predictions.

### Open Question 3
- *Question:* How robust is the Semantic Graph Module to the quality and ambiguity of LLM-generated relation descriptions used for neighbor selection?
- *Basis in paper:* Paper relies on GPT-4o to generate "canonical textual descriptions" for relations. While ablation shows descriptions help, method assumes external LLM provides accurate definitions, leaving potential vulnerability to hallucination or vague descriptions.
- *Why unresolved:* Paper does not analyze performance when relation descriptions are noisy, ambiguous, or sparse, which is likely in open-domain or noisy KGs.
- *What evidence would resolve it:* Sensitivity analysis where generated relation descriptions are systematically perturbed (adding noise, removing keywords, or replacing with generic definitions) to observe degradation rate of link prediction metrics.

## Limitations
- Reliance on GPT-4O for relation definition generation introduces potential variability in semantic representations, particularly for domain-specific or technical relations
- Top-K neighbor selection mechanism may struggle with relations having sparse neighborhoods or those requiring multi-hop reasoning
- Affine transformation assumes simple linear relationship between graph context and language embeddings, which may not capture complex semantic alignments

## Confidence
- **High Confidence:** Architectural design and implementation details of Semantic Graph Module and Condition-Adaptive Fusion Module
- **Medium Confidence:** Claims regarding RotatE pre-training providing superior initialization (supported by ablation but mechanism remains somewhat theoretical)
- **Medium Confidence:** Superiority over baseline methods demonstrated on benchmark datasets, but specific conditions require further investigation

## Next Checks
1. **Semantic Robustness Test:** Evaluate SCT performance when GPT-4O-generated relation definitions are replaced with human-curated definitions to isolate impact of LLM quality on semantic representations
2. **Scalability Assessment:** Test framework on larger, more complex KGs (e.g., Wikidata) to evaluate whether Top-K neighbor selection remains effective at scale
3. **Cross-Domain Generalization:** Apply SCT to biomedical knowledge graphs where relation semantics are highly specialized to assess whether semantic enhancement mechanism generalizes beyond standard benchmarks