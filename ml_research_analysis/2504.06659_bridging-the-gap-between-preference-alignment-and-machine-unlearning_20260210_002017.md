---
ver: rpa2
title: Bridging the Gap Between Preference Alignment and Machine Unlearning
arxiv_id: '2504.06659'
source_url: https://arxiv.org/abs/2504.06659
tags:
- unlearning
- performance
- arxiv
- preference
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges the gap between preference alignment (PA) and
  machine unlearning (MU) for large language models (LLMs). While RLHF is effective
  for PA, it is resource-intensive and unstable, whereas MU offers a more efficient
  alternative by removing negative examples.
---

# Bridging the Gap Between Preference Alignment and Machine Unlearning

## Quick Facts
- **arXiv ID:** 2504.06659
- **Source URL:** https://arxiv.org/abs/2504.06659
- **Reference count:** 34
- **Primary result:** Bi-level optimization framework (U2A) that bridges preference alignment and machine unlearning, achieving better PA performance with 90% less training time than RLHF.

## Executive Summary
This paper addresses the computational inefficiency of preference alignment (PA) in large language models (LLMs) by leveraging machine unlearning (MU). While reinforcement learning from human feedback (RLHF) is effective for PA, it is resource-intensive and unstable. The authors propose a novel framework called Unlearning to Align (U2A) that uses bi-level optimization to selectively unlearn negative examples, thereby improving PA performance. Their theoretical analysis reveals that not all negative examples contribute equally to alignment improvement, and the impact varies significantly across examples. Extensive experiments demonstrate that U2A significantly enhances PA performance while maintaining comparable unlearning effectiveness and reducing computational costs.

## Method Summary
The paper proposes Unlearning to Align (U2A), a bi-level optimization framework that bridges preference alignment and machine unlearning. The method works by first partitioning data into fine-tuning, PA evaluation, and negative sample selection sets. A reward model analyzes the selection set to identify low-reward token combinations. The bi-level optimization then iteratively selects and weights negative samples: the inner loop performs unlearning to find optimal model parameters, while the outer loop updates sample weights using implicit gradients to maximize PA performance. The framework converges by adding samples with maximum marginal gain to the unlearning set. The final model is trained using the selected samples with optimized weights, achieving PA improvements while maintaining unlearning effectiveness and reducing computational costs compared to traditional RLHF approaches.

## Key Results
- U2A achieves 90% reduction in training time compared to RLHF while maintaining or improving PA performance.
- Selective unlearning of negative examples (12.5% of total) can achieve comparable PA performance to full-set unlearning.
- The gradient alignment between unlearning and PA objectives is theoretically proven, showing that unlearning low-reward token combinations improves PA when gradients oppose.
- Extensive experiments on Llama-2-7B-Chat and Llama-3.1-8B-Instruct across multiple datasets (PKU SafeRLHF, UltraFeedback, HaluEval) confirm the effectiveness of the approach.

## Why This Works (Mechanism)

### Mechanism 1: Gradient Alignment Between Unlearning and Preference Alignment
The paper proves that PA performance change depends on the inner product between PA objective gradient and unlearning objective gradient. When unlearning low-reward token combinations, these gradients tend to oppose (cos(φ) < 0), yielding positive PA performance improvement. This means unlearning acts as a proxy for positive preference optimization without needing positive examples. The core assumption is that the reward model accurately distinguishes high-reward from low-reward behaviors, and the proportion of low-reward tokens determines the sign of the gradient inner product.

### Mechanism 2: Bi-level Optimization for Sample Selection and Weighting
Bi-level optimization enables efficient identification of optimal negative samples and unlearning weights. The inner loop performs unlearning to find optimal parameters for given weights, while the outer loop computes gradients to update weights via projected gradient descent. This iteratively selects samples with high marginal gain, adding them to the unlearning set. Convergence is guaranteed with error O(1/t). The core assumption is that the implicit function theorem applies and the conjugate gradient approximation of the Hessian-inverse-vector product is sufficient for convergence.

### Mechanism 3: Computational Efficiency via Sparse Unlearning Sets
Selectively unlearning a small subset of negative samples achieves comparable or better PA performance with significantly lower computational cost. The size of the unlearning set required to achieve error ε is O((L+ε₁)ε⁻¹), meaning only a fraction of negative samples need processing. This sparsity, combined with the efficiency of fine-tuning versus RLHF's full training loop, yields ~90% reduction in training time. The core assumption is that the selected sparse set is representative of harmful behaviors and general model utility is preserved by regularization.

## Foundational Learning

- **Concept: Bi-level Optimization**
  - Why needed: The U2A framework relies entirely on BLO to jointly optimize sample selection and model parameters.
  - Quick check: If the inner loop's loss landscape changes with ω, does the outer loop's gradient still provide a valid direction for optimization? (Answer: Yes, via the implicit function theorem and Hessian-vector products.)

- **Concept: Gradient Ascent for Unlearning**
  - Why needed: The paper uses gradient ascent on the forget set to increase loss, directly opposing the training objective.
  - Quick check: Why does maximizing the loss on the forget set remove knowledge rather than just making the model random? (Answer: Ideally, it pushes model parameters away from the manifold that generates the forgotten data, but regularization is critical to prevent collapse.)

- **Concept: Reward Models and the Bradley-Terry Model**
  - Why needed: The analysis decomposes samples into token combinations and uses a reward model to identify low-reward vs. high-reward elements.
  - Quick check: If a sample has a low average reward, does that guarantee unlearning it will improve PA? (Answer: No. It depends on the proportion of low-reward tokens; a mix can still harm PA.)

## Architecture Onboarding

- **Component map:** Data Partitioner -> Reward Analyzer -> Bi-level Optimizer (Inner Loop: parameter update, Outer Loop: weight update, Sample Selector: add best sample) -> Unlearning Engine

- **Critical path:** Data Partitioning -> Reward Analysis (to pre-filter potential samples) -> Bi-level Optimization (Inner Loop: parameter update, Outer Loop: weight update, Selector: add best sample) -> Repeat until convergence -> Final weight re-optimization

- **Design tradeoffs:**
  - Reward model choice vs. accuracy: Simpler models speed up analysis but may misclassify token rewards.
  - Sparsity coefficient (β) vs. unlearning set size: Higher β forces smaller sets, which is faster but may miss important samples.
  - Conjugate gradient iterations vs. precision: More iterations yield more accurate Hessian approximations but increase computational cost.

- **Failure signatures:**
  - Unlearning harms PA: Selected samples have low proportion of low-reward tokens. Check reward distribution.
  - Model utility collapse: Perplexity spikes. Regularization weight is too low.
  - Slow convergence: Marginal gain is small for all samples. Check if reward model produces useful signals.

- **First 3 experiments:**
  1. Reproduce Figure 1/2: Unlearn individual sample groups and plot ΔPA vs. proportion of low-reward tokens to verify the correlation.
  2. Ablate Sample Selection: Compare U2A vs. random selection of negative samples, measuring PA performance and convergence speed.
  3. Vary Unlearning Method: Run U2A with GA, GradDiff, and NPO as base unlearning engine, comparing final PA scores and model utility.

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical connection between machine unlearning and preference alignment be extended to decoding-based alignment methods that do not update model parameters? The authors explicitly limit their scope to "learning-based methods" because "the former [MU] requires parameter updates," excluding decoding-based approaches like rejection sampling or MCTS. This remains unresolved because the U2A framework fundamentally relies on bi-level optimization over parameter gradients, which are inapplicable to fixed-parameter inference strategies.

### Open Question 2
Is the selection of the L₁/₂-norm for the sparsity regularization term universally optimal, or is it merely a heuristic compromise? The paper sets p=1/2 "considering these factors" to balance convexity and compression, acknowledging p=0 is better for sparsity but harder to optimize. This is unresolved because the paper does not provide an ablation study on the exponent p, leaving the sensitivity of alignment performance to this specific norm choice undetermined.

### Open Question 3
How robust is the U2A convergence guarantee when Assumption 4.1 (positive semi-definite Hessian) is violated in the non-convex landscapes of large LLMs? The derivation of Proposition 4.2 and the convergence analysis rely on this assumption, but LLM loss landscapes are highly non-convex. If this assumption fails during training, the estimated impact of unlearning on PA performance may become inaccurate.

## Limitations

- The theoretical connection between unlearning and PA relies heavily on the quality of a single reward model, with no ablation studies using alternative reward models.
- The specific hyperparameters (β=0.5, λ=1.0) are not justified by ablation studies, potentially limiting generalizability.
- The computational efficiency claim is based on comparison to RLHF but lacks direct comparison to other efficient alignment methods like DPO.

## Confidence

- **High Confidence:** The overall framework of using bi-level optimization to select and weight negative examples for preference alignment is well-defined and the experimental results on multiple models and datasets are robust.
- **Medium Confidence:** The theoretical analysis of gradient alignment between unlearning and PA is mathematically correct, but its practical significance depends heavily on the quality of the reward model.
- **Low Confidence:** The claim that unlearning low-reward token combinations always improves PA is the weakest, as it relies on a single reward model and assumes perfect reward signal quality.

## Next Checks

1. **Reward Model Ablation:** Repeat the main experiments using a different, independently-trained reward model to verify that gradient alignment and sample selection results are not artifacts of a specific model. Measure the correlation between the two reward models' low-reward token classifications.

2. **Hyperparameter Sensitivity:** Conduct an ablation study varying the sparsity coefficient β (e.g., 0.3, 0.5, 0.7) and the regularization weight λ (e.g., 0.1, 1.0, 10.0). Plot the Pareto frontier of PA performance vs. PPL to identify the optimal tradeoff.

3. **Gradient Alignment Robustness:** For a held-out test set, compute the actual gradient inner product after unlearning a sample and compare this to the predicted sign from the reward model's low-reward token proportion to quantify how often the theoretical mechanism matches the empirical outcome.