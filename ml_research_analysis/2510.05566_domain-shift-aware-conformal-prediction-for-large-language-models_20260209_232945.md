---
ver: rpa2
title: Domain-Shift-Aware Conformal Prediction for Large Language Models
arxiv_id: '2510.05566'
source_url: https://arxiv.org/abs/2510.05566
tags:
- prediction
- coverage
- domain
- ds-cp
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Domain-Shift-Aware Conformal Prediction (DS-CP)
  for large language models (LLMs) to address the problem of unreliable uncertainty
  quantification under domain shift. Standard conformal prediction often under-covers
  when calibration and test data distributions differ, leading to overly narrow prediction
  sets.
---

# Domain-Shift-Aware Conformal Prediction for Large Language Models

## Quick Facts
- arXiv ID: 2510.05566
- Source URL: https://arxiv.org/abs/2510.05566
- Reference count: 35
- This paper proposes Domain-Shift-Aware Conformal Prediction (DS-CP) for large language models (LLMs) to address the problem of unreliable uncertainty quantification under domain shift.

## Executive Summary
This paper addresses the challenge of unreliable uncertainty quantification in large language models when calibration and test data distributions differ. Standard conformal prediction often under-covers in such scenarios, producing prediction sets that are too narrow. The proposed Domain-Shift-Aware Conformal Prediction (DS-CP) method reweights calibration samples based on their semantic proximity to test prompts, using embeddings and density ratio estimation to preserve coverage validity while improving adaptivity. Experiments on the MMLU benchmark across 16 LLM models demonstrate consistent improvements in empirical coverage compared to standard conformal prediction, particularly in cases of severe under-coverage.

## Method Summary
DS-CP embeds prompts into a semantic space using a pre-trained model and reweights calibration samples based on their proximity to the test prompt. The method applies weighted conformal prediction with a Label-Agnostic Conformal (LAC) score S(X,Y) = 1 - f(X)_Y and regularization λ=1. For each test instance, density ratios are estimated between calibration and test domains using an XGBoost classifier trained on semantic embeddings. These ratios weight the calibration data in the quantile calculation that determines prediction set size. The approach theoretically extends nonexchangeable conformal prediction to data-dependent weights, with coverage bounds that remain valid under domain shift.

## Key Results
- DS-CP consistently improves empirical coverage compared to standard CP across 16 LLM models on MMLU benchmark
- Method effectively addresses 10-20% under-coverage typical of standard CP under domain shift
- Modest increases in prediction set size (typically 1-2 additional answers) maintain practical usability while achieving target coverage
- Performance improvements are most pronounced in subjects with severe domain shift between training and test domains

## Why This Works (Mechanism)
Standard conformal prediction assumes exchangeability between calibration and test data, which fails when domain shift occurs. DS-CP addresses this by recognizing that samples closer in semantic space to the test prompt are more likely to share the same conditional distribution. By reweighting calibration samples based on density ratios estimated through semantic embeddings, the method effectively adapts the conformal prediction set size to account for distributional differences. The regularization parameter λ=1 ensures the prediction set is never empty while allowing inflation when needed for coverage.

## Foundational Learning
- **Conformal Prediction**: A framework for uncertainty quantification that produces prediction sets with guaranteed coverage probability. Needed to understand the baseline method being improved.
- **Domain Shift**: The phenomenon where training and test data come from different distributions, violating exchangeability assumptions. Critical for understanding why standard CP fails in practice.
- **Density Ratio Estimation**: The process of estimating P(test|Z=z)/P(calibration|Z=z) from embeddings to weight calibration samples. Forms the core mechanism for adapting to domain shift.
- **Label-Agnostic Conformal (LAC) Score**: A nonconformity score that depends only on the model's confidence for a given label, not on other labels. Enables tractable weighted conformal prediction.
- **Nonexchangeable Conformal Prediction**: Extensions of CP theory that handle data where calibration and test samples come from different distributions. Provides the theoretical foundation for DS-CP.
- **Semantic Embeddings**: Vector representations that capture meaning and allow distance-based similarity computation. Enable the method to identify calibration samples relevant to the test domain.

## Architecture Onboarding

**Component Map:** 
Prompts -> Semantic Embeddings (all-MiniLM-L6-v2) -> Density Ratio Estimation (XGBoost) -> Weighted Quantile Calculation -> Prediction Set

**Critical Path:**
The core pipeline processes test prompts through embedding generation, followed by density ratio estimation against calibration data, and finally weighted conformal prediction. The embedding step is critical as it determines how well the method can identify relevant calibration samples.

**Design Tradeoffs:**
The method trades computational overhead (embedding generation and density ratio estimation) for improved coverage under domain shift. Using a simple LAC score with regularization enables tractable weighted conformal prediction, but may not capture all aspects of model uncertainty. The choice of embedding model and classifier impacts performance but requires no labeled test data.

**Failure Signatures:**
Prediction sets degenerate to the full output space (all 6 options) under severe domain shift when density ratios become extreme. Standard CP showing 10-20% under-coverage (coverage ~70-80% instead of 90%) indicates domain shift that DS-CP should address. Poor embedding quality manifests as DS-CP failing to improve coverage over standard CP.

**Three First Experiments:**
1. Verify that density ratios increase for calibration samples from domains closer to the test prompt by computing average ratios across different subject pairs.
2. Test coverage improvement on a single subject pair with known domain shift by comparing DS-CP to standard CP with the same calibration data.
3. Examine prediction set sizes for calibration vs. test instances to confirm that weighting appropriately inflates sets for shifted domains.

## Open Questions the Paper Calls Out
- How can DS-CP be effectively extended to open-ended generation tasks such as summarization or code generation? The current framework is validated on multiple-choice QA where the output space is finite. Open-ended tasks lack a finite output space Y, making standard set construction and coverage calculation infeasible without defining semantic equivalence classes.
- What is the optimal, principled strategy for tuning the regularization parameter λ? The paper defaults to λ=1 for fairness and simplicity, but this value may not optimally balance the inflation of prediction sets against the risk of under-coverage in diverse shift scenarios.
- To what extent does the choice of embedding model and density ratio estimator impact the robustness of DS-CP? The experiments utilize a single embedding model and classifier. It is unclear if the method fails when the embedding space poorly aligns with the domain shift or if higher-dimensional embeddings are necessary.

## Limitations
- Reliance on pre-computed logits and embeddings without specifying exact prompt templates affects reproducibility
- Assumes semantic embeddings adequately capture domain similarity, which may fail for stylistic or format changes without semantic content changes
- Requires careful calibration of regularization parameter λ, though paper uses fixed value of 1 without systematic exploration
- Computational overhead from density ratio estimation and embedding generation may be prohibitive for real-time applications

## Confidence
- **High Confidence**: The theoretical framework for weighted conformal prediction under domain shift is sound and builds on established results. The empirical improvements in coverage over standard CP are well-documented across multiple models and subjects.
- **Medium Confidence**: The choice of MiniLM-L6-v2 for embedding generation and XGBoost for density ratio estimation appears reasonable but may not be optimal for all domain shift scenarios. The fixed regularization parameter λ=1 was not systematically validated across different settings.
- **Low Confidence**: The generalizability of results to domain shifts beyond the MMLU benchmark and the sensitivity of the method to embedding quality for non-semantic domain shifts (e.g., formatting changes) remain uncertain.

## Next Checks
1. **Prompt Template Validation**: Reproduce the results using different prompt templates for the same base LLM models to assess sensitivity to prompt formatting and ensure robustness to minor variations in how questions are presented to the model.
2. **Regularization Parameter Sweep**: Systematically vary the regularization parameter λ across a wider range (e.g., {0.1, 0.5, 1, 2, 5}) to evaluate its impact on the coverage-variance tradeoff and determine if the fixed choice of λ=1 is near-optimal across all settings.
3. **Non-Semantic Domain Shift Test**: Apply the method to a dataset where domain shift is primarily non-semantic (e.g., same questions in different formats or styles) to validate whether the embedding-based approach remains effective when semantic content is unchanged but surface form differs substantially.