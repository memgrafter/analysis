---
ver: rpa2
title: Towards Experience Replay for Class-Incremental Learning in Fully-Binary Networks
arxiv_id: '2503.07107'
source_url: https://arxiv.org/abs/2503.07107
tags:
- learning
- replay
- training
- latent
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a novel approach for enabling class-incremental\
  \ learning (CIL) in fully-binarized neural networks (FBNNs), addressing the challenge\
  \ of continual learning in ultra-low power edge devices. The authors introduce four\
  \ key contributions: (1) revisiting FBNN design and training for CIL, (2) exploring\
  \ loss balancing to optimize performance across tasks, (3) proposing a semi-supervised\
  \ method for pre-training feature extractors to enhance transferability, and (4)\
  \ comparing two CIL methods\u2014Latent and Native replay\u2014at iso-memory."
---

# Towards Experience Replay for Class-Incremental Learning in Fully-Binary Networks

## Quick Facts
- **arXiv ID:** 2503.07107
- **Source URL:** https://arxiv.org/abs/2503.07107
- **Authors:** Yanis Basso-Bert; Anca Molnos; Romain Lemaire; William Guicquero; Antoine Dupret
- **Reference count:** 40
- **Primary result:** 3Mb FBNN achieves final accuracy on par with or better than larger real-valued neural network models in class-incremental learning settings.

## Executive Summary
This paper presents a novel approach for enabling class-incremental learning (CIL) in fully-binarized neural networks (FBNNs), addressing the challenge of continual learning in ultra-low power edge devices. The authors introduce four key contributions: (1) revisiting FBNN design and training for CIL, (2) exploring loss balancing to optimize performance across tasks, (3) proposing a semi-supervised method for pre-training feature extractors to enhance transferability, and (4) comparing two CIL methods—Latent and Native replay—at iso-memory. The approach is evaluated on CIFAR100 and scaled to the CORE50 dataset, demonstrating that a 3Mb FBNN achieves performance on par with or better than larger real-valued neural network models. The study highlights the trade-offs between adaptation and retention in CIL and provides insights into optimizing FBNNs for dynamic, resource-constrained environments.

## Method Summary
The paper introduces a framework for class-incremental learning in fully-binarized neural networks, focusing on enabling continual learning on ultra-low power edge devices. The method involves revisiting FBNN design and training specifically for CIL scenarios, implementing loss balancing techniques to optimize performance across multiple tasks, and introducing a semi-supervised pre-training approach for feature extractors to improve transferability. Two CIL methods—Latent and Native replay—are compared under equal memory constraints. The approach is evaluated on CIFAR100 (with 50 classes for pre-training and 5 tasks of 10 classes each) and scaled to the CORE50 dataset, demonstrating that a 3Mb FBNN can achieve competitive performance compared to larger real-valued networks.

## Key Results
- A 3Mb fully-binarized neural network achieves final accuracy comparable to or better than larger real-valued neural network models in class-incremental learning scenarios.
- The proposed semi-supervised pre-training method enhances feature extractor transferability across tasks.
- Native and Latent replay methods show different trade-offs in terms of adaptation and retention, with the choice depending on specific application requirements.
- The framework successfully scales from CIFAR100 to the more challenging CORE50 dataset while maintaining competitive performance.

## Why This Works (Mechanism)
The approach works by addressing the fundamental challenges of continual learning in binary networks: catastrophic forgetting and limited representational capacity. The semi-supervised pre-training creates robust feature representations that transfer well across tasks, while the experience replay mechanisms (both Native and Latent) maintain a buffer of past experiences to prevent forgetting. The loss balancing ensures that the network doesn't bias too heavily toward new tasks at the expense of old ones. The fully-binarized nature of the network, combined with efficient scaling factors and thermometer encoding, enables ultra-low power operation while maintaining sufficient representational power for incremental learning tasks.

## Foundational Learning
- **Class-Incremental Learning (CIL):** Learning new classes sequentially without forgetting previous ones. Needed to handle real-world scenarios where data arrives continuously. Quick check: Verify that model performance degrades on old classes when trained on new ones without replay.
- **Fully-Binarized Neural Networks (FBNNs):** Networks where both weights and activations are binary (+1/-1). Needed for ultra-low power edge deployment. Quick check: Confirm that memory footprint is reduced by 32x compared to 32-bit networks.
- **Experience Replay:** Maintaining a buffer of past examples to prevent catastrophic forgetting. Needed because neural networks typically overwrite old knowledge when learning new tasks. Quick check: Measure accuracy drop when replay buffer is disabled.
- **Semi-supervised Pre-training:** Using both labeled and unlabeled data to create robust feature extractors. Needed to bootstrap learning when labeled data is scarce for new tasks. Quick check: Compare transfer performance with and without pre-training.
- **Loss Balancing:** Weighting different task losses to prevent bias toward new classes. Needed because unweighted losses naturally favor recently learned information. Quick check: Monitor class-specific accuracy across tasks to ensure balanced performance.
- **Thermometer Encoding:** Converting continuous pixel values to binary representations. Needed to maintain information while enabling binary operations. Quick check: Verify that encoding preserves sufficient discriminative information.

## Architecture Onboarding

**Component Map:** Input -> Thermometer Encoder -> FBNN Backbone -> LGAP -> Classifier -> Output

**Critical Path:** The critical path for inference is: Thermometer Encoder → FBNN Backbone → LGAP → Classifier → Output. For training, the critical path extends to include the loss computation and backpropagation through all components.

**Design Tradeoffs:** The architecture trades representational capacity for extreme memory efficiency. Binarization reduces memory by 32x but can limit expressivity. The semi-supervised pre-training partially compensates by creating better initial representations. The choice between Native and Latent replay involves a tradeoff between computational overhead (Native requires more memory bandwidth) and model complexity (Latent requires additional network capacity).

**Failure Signatures:** Training instability manifests as immediate divergence when learning rates exceed 10^-3. Poor retention shows as rapid forgetting of old classes in early tasks. Stagnation appears as accuracy capping well below expected levels (e.g., <50% on CIFAR100), often indicating improper scaling factor initialization that prevents gradient propagation in deep binary layers.

**First Experiments:**
1. Implement and validate the 3Mb-BNN architecture with proper scaling factor initialization and test basic forward pass functionality.
2. Test the semi-supervised pre-training on the first 50 CIFAR100 classes, combining supervised CCE with Barlow Twins SSL and activation regularization losses.
3. Execute the complete CIL loop with reservoir buffer sampling, verifying that interleaved training on new data and buffer data maintains reasonable performance across all tasks.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Architectural details of the Learnable Global Average Pooling (LGAP) block are underspecified, particularly regarding binary weight initialization and integration with standard GAP.
- The "Masking" block in the classifier architecture is mentioned but not described in sufficient detail to reproduce.
- Early stopping parameters for the Reduce-On-Plateau scheduler (patience, delta thresholds) are not explicitly stated.

## Confidence
- **High confidence** in the core methodology (CIL framework with Native/Latent replay, loss balancing, and feature extractor pre-training).
- **Medium confidence** in the architectural specifications, particularly the exact implementation of LGAP and Masking blocks.
- **Medium confidence** in hyperparameter choices (LR scheduling, loss coefficients) based on typical values in binarized networks.

## Next Checks
1. Implement a baseline LGAP block with configurable binary weight initialization and verify gradient flow.
2. Validate the impact of different masking strategies on classifier performance using CIFAR10.
3. Test reservoir buffer sampling with varying buffer sizes (10-20% of task data) to confirm retention-accuracy trade-offs.