---
ver: rpa2
title: 'Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for
  Digital Inclusion'
arxiv_id: '2509.14249'
source_url: https://arxiv.org/abs/2509.14249
tags:
- intent
- dataset
- shona
- language
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the underrepresentation of African languages\
  \ in NLP by creating a novel Shona\u2013English slang dataset annotated for intent,\
  \ sentiment, dialogue acts, code-mixing, and tone. A fine-tuned multilingual DistilBERT\
  \ classifier achieves 96.4% accuracy and 96.3% F1-score on intent recognition, integrated\
  \ into a hybrid chatbot combining rule-based responses with retrieval-augmented\
  \ generation (RAG)."
---

# Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion

## Quick Facts
- **arXiv ID:** 2509.14249
- **Source URL:** https://arxiv.org/abs/2509.14249
- **Reference count:** 4
- **Primary result:** Fine-tuned multilingual DistilBERT achieves 96.4% accuracy and 96.3% F1-score on intent classification for Shona-English code-mixed slang.

## Executive Summary
This study addresses the underrepresentation of African languages in NLP by creating a novel Shona–English slang dataset annotated for intent, sentiment, dialogue acts, code-mixing, and tone. A fine-tuned multilingual DistilBERT classifier achieves 96.4% accuracy and 96.3% F1-score on intent recognition, integrated into a hybrid chatbot combining rule-based responses with retrieval-augmented generation (RAG). The system demonstrates cultural relevance and improved user engagement compared to RAG-only baselines in a use case assisting prospective students with graduate program information at Pace University. The dataset, model, and methodology are publicly available, advancing NLP resources for low-resource African languages and promoting digital inclusion.

## Method Summary
The approach involves fine-tuning a multilingual DistilBERT model on a manually annotated dataset of ~34,000 Shona-English code-mixed utterances for intent classification. The hybrid chatbot architecture routes intents to either rule-based Shona responses (for predictable queries like greetings) or a RAG pipeline (for domain-specific queries). The RAG component uses all-MiniLM-L6-v2 embeddings, ChromaDB retrieval (top-5), and Flan-T5-small generation. Training uses learning rate 2×10⁻⁵, batch size 4, 3 epochs, with early stopping and class balancing via oversampling/downsampling.

## Key Results
- Multilingual DistilBERT fine-tuned on code-mixed slang achieves 96.4% accuracy and 96.3% F1-score on intent classification.
- Hybrid chatbot (rule-based + RAG) shows superior cultural relevance and user engagement compared to RAG-only baseline in Pace University graduate program use case.
- The dataset and model are publicly available at https://github.com/HappymoreMasoka/Working_with_shona-slang.

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning multilingual DistilBERT on code-mixed slang data enables robust intent classification in low-resource African language contexts. The multilingual pre-training provides cross-lingual representations that transfer to code-mixed Shona-English inputs; fine-tuning on slang-annotated data adapts these representations to informal registers and intent-specific patterns. Core assumption: The multilingual model's learned representations generalize sufficiently to Shona-English code-mixing despite limited Shona exposure during pre-training.

### Mechanism 2
Hybrid rule-based + RAG architecture improves cultural relevance over RAG-only systems for predictable query types. Rule-based responses provide culturally grounded, predefined Shona greetings/farewells that ensure appropriate tone; RAG handles domain-specific open-ended queries where cultural adaptation is less critical. Core assumption: Predictable intents (greetings, farewells) can be reliably detected and mapped to culturally appropriate responses without generation variability.

### Mechanism 3
Multi-dimensional annotation (intent, sentiment, dialogue acts, code-mixing, tone) enables richer contextual understanding but requires substantial annotation effort. Multiple annotation dimensions allow the system to disambiguate utterances (e.g., distinguishing a friendly greeting from a formal query) and select response strategies accordingly. Core assumption: Annotators can consistently label these dimensions in code-mixed slang, and labels transfer to downstream utility.

## Foundational Learning

- **Transfer Learning with Multilingual Transformers** - Understanding how DistilBERT's multilingual pre-training enables adaptation to code-mixed Shona-English without training from scratch. Quick check: Can you explain why fine-tuning requires far less data than pre-training, and what happens if the target language was poorly represented in pre-training data?

- **Retrieval-Augmented Generation (RAG)** - The chatbot uses RAG for domain-specific queries; understanding embedding-based retrieval and generation pipeline is essential. Quick check: Given a query, can you trace how embeddings are computed, how ChromaDB retrieves top-k documents, and how the generator conditions on retrieved context?

- **Code-Mixing and Slang in NLP** - The dataset focuses on Shona-English code-mixing; standard NLP assumptions about monolingual input do not apply. Quick check: How would tokenization differ for "Hie swit mom" vs. formal text, and what challenges does word-level vs. phrase-level code-mixing present?

## Architecture Onboarding

- **Component map:** User input → preprocessing → DistilBERT intent classifier → router → (rule-based module OR RAG pipeline) → response output
- **Critical path:** 1. User input received and normalized 2. DistilBERT classifies intent with confidence score 3. Router selects: rule-based response (high confidence, predictable intent) OR RAG pipeline (domain-specific query) 4. If RAG: query embedded → ChromaDB retrieves relevant documents → Flan-T5 generates response 5. Response delivered; exit commands trigger farewell
- **Design tradeoffs:** Rule-based vs. generative: Rule-based ensures cultural accuracy but lacks flexibility; RAG provides flexibility but may lack cultural grounding. DistilBERT vs. larger models: DistilBERT chosen for efficiency (Colab constraints) but may sacrifice accuracy on complex inputs. Top-5 retrieval: Balances context richness vs. noise; no evidence provided for optimal k
- **Failure signatures:** Low confidence scores on intent classification → may route to incorrect module. Finance queries return fallback replies → insufficient training examples for that intent. Generic or off-topic responses from RAG-only mode → cultural context not incorporated. Out-of-scope inputs not handled gracefully
- **First 3 experiments:** 1. Intent coverage analysis: Measure per-class F1 scores across all intent categories; identify underperforming intents and quantify training data distribution. 2. Hybrid vs. RAG-only A/B test: Conduct structured user evaluation with metrics for cultural relevance, response accuracy, and engagement; compare statistical significance. 3. Retrieval ablation: Vary top-k (3, 5, 10) and embedding model; measure retrieval relevance and downstream generation quality.

## Open Questions the Paper Calls Out
- **Human-in-the-loop evaluation comparison:** How does human-in-the-loop evaluation compare to automated metrics for assessing cultural appropriateness and user engagement? Current evaluation relies on automated classification metrics without systematic human judgment data.
- **Generalization to other domains:** To what extent does the hybrid architecture generalize to domains beyond university admissions (e.g., healthcare, finance)? Single-domain evaluation cannot establish whether performance is domain-specific.
- **Minimum dataset size:** What is the minimum dataset size required for robust intent classification on code-mixed Shona-English slang across diverse intent categories? High overall accuracy may mask poor performance on underrepresented intent classes.

## Limitations
- **Limited evidence on generalization:** High accuracy reported on same dataset used for training; no cross-dataset validation or hold-out testing on external slang data.
- **Evaluation methodology gaps:** Superiority claims based on qualitative assessment rather than structured quantitative comparison with statistical significance testing.
- **Reproducibility constraints:** Missing critical implementation details including random seeds, exact rule-based mappings, confidence thresholds, and knowledge base structure.

## Confidence
- **High Confidence:** Technical implementation of hybrid architecture and reported metrics on provided dataset.
- **Medium Confidence:** Fine-tuning multilingual DistilBERT on code-mixed slang enables robust intent classification, but limited by lack of external validation.
- **Low Confidence:** Superiority of hybrid system over RAG-only baselines in cultural relevance and user engagement based solely on qualitative evaluation without quantitative metrics.

## Next Checks
1. **Per-class performance and data distribution analysis:** Compute and report F1-scores for each intent category individually, with confusion matrices showing misclassification patterns. Analyze training data distribution across intents to identify class imbalance issues.
2. **Systematic hybrid vs. RAG-only comparison:** Conduct structured A/B testing with multiple human raters scoring responses on cultural relevance, factual accuracy, response completeness, and user engagement. Include statistical significance testing and error analysis.
3. **External validation and robustness testing:** Evaluate intent classifier on external dataset of Shona-English code-mixed text from different domains or time periods. Test robustness to out-of-distribution slang patterns, misspellings, and code-switching density variations.