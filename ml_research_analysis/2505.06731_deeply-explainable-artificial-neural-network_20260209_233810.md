---
ver: rpa2
title: Deeply Explainable Artificial Neural Network
arxiv_id: '2505.06731'
source_url: https://arxiv.org/abs/2505.06731
tags:
- dxann
- explainability
- deep
- learning
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DxANN addresses the lack of transparency in deep learning models
  by embedding explainability directly into the training process, rather than relying
  on post-hoc interpretation methods. The core innovation is the Explainability Contribution
  Score (ECS), which measures the influence of individual features on predictions
  by comparing embedded feature values to class-specific distribution means in a latent
  space.
---

# Deeply Explainable Artificial Neural Network

## Quick Facts
- arXiv ID: 2505.06731
- Source URL: https://arxiv.org/abs/2505.06731
- Authors: David Zucker
- Reference count: 0
- Primary result: DxANN achieves 97.1% accuracy on diabetic macular edema diagnosis with per-sample feature explanations

## Executive Summary
DxANN addresses the lack of transparency in deep learning models by embedding explainability directly into the training process, rather than relying on post-hoc interpretation methods. The core innovation is the Explainability Contribution Score (ECS), which measures the influence of individual features on predictions by comparing embedded feature values to class-specific distribution means in a latent space. This ante-hoc approach provides per-sample, per-feature explanations as part of the forward pass, eliminating the need for external interpretation tools.

## Method Summary
DxANN is a binary classifier built on Real-NVP flow architecture with CNN backbone. The model transforms inputs through two Affine Transformation Blocks (ATBs), each with 8 convolutional layers, to a latent space containing two class-conditional Gaussian distributions. Classification is performed by computing likelihood under each distribution, while ECS quantifies feature importance as the distance from class means. The loss function combines classification likelihood with flow-based log-determinant terms. DxANN was trained on retinal OCT images for diabetic macular edema diagnosis and knee X-rays for osteoarthritis detection, achieving high accuracy with clinically interpretable heatmaps.

## Key Results
- DxANN achieved 97.1% accuracy on diabetic macular edema diagnosis from OCT images
- DxANN achieved 97.2% accuracy on osteoarthritis diagnosis from knee X-ray images
- Performance matches or exceeds fine-tuned ResNet-50 and VGG-16 baselines

## Why This Works (Mechanism)

### Mechanism 1
DxANN achieves intrinsic explainability by computing feature importance directly during the forward pass using latent space distances. The Explainability Contribution Score (ECS) measures each embedded feature's deviation from its class-conditional distribution mean: `ECS_nm ~ |z_nm - μ_i|`. Features requiring greater "effort" to align with the expected class distribution receive higher scores, indicating stronger influence on the classification decision. The core assumption is that distance from class mean in the learned latent space meaningfully correlates with a feature's contribution to the model's decision.

### Mechanism 2
Classification emerges from likelihood comparison in latent space rather than a separate classification head. DxANN assigns two Gaussian distributions with different means (μ₀, μ₁) to the two classes. Classification follows from: `prediction = argmax(q_z0(z_n), q_z1(z_n))`—selecting the distribution that maximizes the embedded sample's likelihood. The core assumption is that class separability can be achieved through affine transformations that map inputs to distinguishable Gaussian clusters.

### Mechanism 3
Flow-based bijective transformations preserve traceability from input features to latent representations. Building on Real-NVP, DxANN uses affine transformations `y = s ⊙ x + t` where scale (s) and translation (t) are learned. The bijective property ensures each input dimension can be traced through the transformation chain to its latent representation. The core assumption is that the log-determinant term in the loss (`-Σlog|s_k|`) preserves invertibility without collapsing feature contributions.

## Foundational Learning

- **Normalizing Flows / Flow-based Generative Models**: DxANN's architecture is explicitly built on Real-NVP; understanding bijective transformations, change of variables formula, and log-determinant computation is essential. *Quick check: Can you explain why the log-determinant of the Jacobian is required in the loss function for flow-based models?*

- **Gaussian Likelihood and Distribution Distance**: Classification uses likelihood under class-conditional Gaussians; ECS uses distance from distribution mean. Understanding how these relate is critical. *Quick check: Given a sample z and two Gaussian distributions N(μ₀, I) and N(μ₁, I), how would you compute which class is more likely?*

- **Spatial Context in CNN Feature Maps**: The paper notes that pixel importance "depends only on the spatial context and relation between the pixels"—this is how ECS becomes meaningful for images. *Quick check: Why would a pixel's ECS value differ from its raw intensity value when processed through convolutional layers?*

## Architecture Onboarding

- **Component map**: Input -> CNN backbone -> ATB 1 -> ATB 2 -> Latent space (N(μ₀,I), N(μ₁,I)) -> Classification + ECS heatmap

- **Critical path**: 
  1. Input → CNN backbone extracts features
  2. Features pass through ATB 1 → scaled/translated representation
  3. Representation passes through ATB 2 → final latent embedding z
  4. Compute likelihood under both class distributions → prediction
  5. Compute ECS for each feature/pixel → explainability output

- **Design tradeoffs**: 
  - Accuracy vs. Explainability: DxANN achieves 97.1-97.2% accuracy, comparable to ResNet-50/VGG-16, but requires training from scratch rather than fine-tuning pre-trained weights
  - ATB Depth: Only 2 ATBs used—fewer than typical flow models; this may limit expressiveness but reduces computational cost
  - Binary Constraint: Current formulation supports only binary classification; multi-class requires architectural extension (not addressed in paper)

- **Failure signatures**:
  - ECS heatmap shows uniform coloring: May indicate class distributions have converged to similar means (poor separation)
  - Accuracy drops significantly below baseline CNN: May indicate loss function weighting imbalance between classification and flow terms
  - Training instability with log-determinant: Check for near-zero scaling factors `s_k` causing numerical issues

- **First 3 experiments**:
  1. Baseline reproduction: Train CNN-based DxANN on a simple binary image dataset (e.g., MNIST digits 0 vs. 1) with 1 ATB, verify ECS highlights discriminative regions
  2. Ablation on ATB count: Compare 1 ATB vs. 2 ATBs on the same dataset—measure accuracy and ECS heatmap sharpness
  3. Distribution separation check: After training, visualize latent space embeddings colored by class; verify that μ₀ and μ₁ are well-separated and clusters are distinguishable

## Open Questions the Paper Calls Out
- Can the DxANN architecture and the Explainability Contribution Score (ECS) be effectively adapted for sequential data and Transformer-based models?
- How does the binary formulation of DxANN scale to multi-class classification problems?
- Does the Explainability Contribution Score (ECS) provide quantitatively faithful explanations compared to ground truth or perturbation-based metrics?

## Limitations
- The binary classification constraint significantly limits real-world applicability, as most medical diagnosis tasks involve multi-class decisions
- The paper lacks quantitative validation against established XAI benchmarks, relying on qualitative heatmap examples
- The ECS mechanism's assumption that distance from class mean reliably indicates feature importance is not empirically validated through ablation studies

## Confidence
- DxANN achieves state-of-the-art accuracy for binary medical image classification: **High**
- ECS provides meaningful, clinically interpretable explanations: **Medium**
- Flow-based architecture guarantees intrinsic explainability: **Medium**

## Next Checks
1. Conduct quantitative comparison between DxANN's ECS heatmaps and Grad-CAM using established metrics like Pointing Game accuracy and deletion/insertion curves on the same medical datasets
2. Implement multi-class extension using mixture of Gaussians or one-vs-rest strategy, and validate performance degradation compared to binary case
3. Perform ablation study varying the number of ATBs (1, 2, 3) to identify optimal trade-off between computational cost and explanation quality