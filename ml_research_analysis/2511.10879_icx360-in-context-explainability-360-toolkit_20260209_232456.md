---
ver: rpa2
title: 'ICX360: In-Context eXplainability 360 Toolkit'
arxiv_id: '2511.10879'
source_url: https://arxiv.org/abs/2511.10879
tags:
- icx360
- explanations
- methods
- input
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ICX360 provides a unified toolkit for explaining LLM outputs in
  terms of their inputs, focusing on in-context explainability. It includes three
  methods: MExGen (black-box/white-box input attribution via perturbations and gradients),
  CELL (black-box contrastive explanations), and Token Highlighter (white-box token-level
  importance via gradients).'
---

# ICX360: In-Context eXplainability 360 Toolkit

## Quick Facts
- arXiv ID: 2511.10879
- Source URL: https://arxiv.org/abs/2511.10879
- Reference count: 16
- Primary result: Unified toolkit for explaining LLM outputs in terms of inputs via MExGen, CELL, and Token Highlighter methods

## Executive Summary
ICX360 provides a unified toolkit for explaining LLM outputs in terms of their inputs, focusing on in-context explainability. It includes three methods: MExGen (black-box/white-box input attribution via perturbations and gradients), CELL (black-box contrastive explanations), and Token Highlighter (white-box token-level importance via gradients). The toolkit supports multi-level input granularity, output-sequence explanations, and integrates with Hugging Face and OpenAI APIs. It addresses the lack of coherent, interpretable explanations for LLMs compared to existing toolkits like SHAP or Captum, which operate at token level or lack context attribution.

## Method Summary
ICX360 provides a unified toolkit for explaining LLM outputs in terms of their inputs, focusing on in-context explainability. It includes three methods: MExGen (black-box/white-box input attribution via perturbations and gradients), CELL (black-box contrastive explanations), and Token Highlighter (white-box token-level importance via gradients). The toolkit supports multi-level input granularity, output-sequence explanations, and integrates with Hugging Face and OpenAI APIs. It addresses the lack of coherent, interpretable explanations for LLMs compared to existing toolkits like SHAP or Captum, which operate at token level or lack context attribution. ICX360 improves explanation quality and computational efficiency, offering utilities for infilling, scalarizing, segmenting, and evaluation. It enables users to understand why LLMs generate specific outputs, supporting high-stakes applications like medical advice or business forecasting. The toolkit is open-source and extensible, welcoming future contributions such as attention-based explanations or amortized methods.

## Key Results
- Provides unified toolkit for in-context explainability of LLM outputs
- Integrates three methods: MExGen, CELL, and Token Highlighter for different explainability needs
- Supports multi-level input granularity and output-sequence explanations
- Open-source and extensible with API integration for Hugging Face and OpenAI

## Why This Works (Mechanism)
ICX360 works by providing a comprehensive framework that addresses the limitations of existing explainability tools for LLMs. The toolkit's modular design allows users to choose between black-box methods (CELL, MExGen) for scenarios where model internals are inaccessible, and white-box methods (Token Highlighter) when gradient information is available. By supporting multi-level input granularity, ICX360 enables explanations at different semantic levels (tokens, spans, or entire contexts), which is crucial for understanding complex reasoning processes in LLMs. The integration of multiple explanation methods within a single toolkit allows users to cross-validate explanations and choose the most appropriate approach for their specific use case.

## Foundational Learning
- **In-context explainability**: Understanding why LLMs generate specific outputs based on input context; needed because traditional explainability methods operate at token level and miss contextual reasoning; quick check: verify explanations align with human understanding of context importance
- **Black-box vs white-box methods**: Different approaches for when model internals are accessible or not; needed to handle different deployment scenarios and API access levels; quick check: confirm method selection matches available model access
- **Multi-level input granularity**: Ability to explain at token, span, or context level; needed because different tasks require different levels of explanation detail; quick check: test explanations at multiple granularity levels for same input

## Architecture Onboarding

Component Map: User Interface -> ICX360 Core -> MExGen/CELL/Token Highlighter -> Hugging Face/OpenAI APIs -> LLM Model

Critical Path: User requests explanation → ICX360 selects appropriate method → Method processes input through LLM API → Explanation generated → Results returned to user

Design Tradeoffs: Black-box methods (CELL, MExGen) offer broader compatibility but may be less precise than white-box methods; Token Highlighter requires gradient access but provides more detailed token-level attribution; multi-level granularity increases flexibility but adds computational overhead

Failure Signatures: Poor explanations may result from insufficient context in input, inappropriate method selection for task type, or API limitations with certain LLM models; computational bottlenecks may occur with large inputs or complex multi-level explanations

First Experiments:
1. Test basic token-level attribution using Token Highlighter on simple classification task
2. Compare CELL explanations for positive vs negative contrastive examples
3. Evaluate MExGen performance with different perturbation strategies on the same input

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation across diverse use cases and real-world scenarios
- Brief evaluation section lacks comprehensive comparative benchmarks against existing tools
- Effectiveness in high-stakes applications not demonstrated with domain-specific case studies

## Confidence

High confidence:
- Modular design and API integration are well-defined and implementable
- Core methods (MExGen, CELL, Token Highlighter) are clearly described with methodological foundations

Medium confidence:
- Claims of improved explanation quality and efficiency compared to SHAP/Captum lack rigorous validation
- Limited comparative benchmarks or user studies to support performance assertions

Low confidence:
- Effectiveness in high-stakes applications asserted but not demonstrated with case studies
- Potential risks in critical decision-making contexts not addressed

## Next Checks

1. Conduct user studies or expert evaluations to assess interpretability and usability in real-world scenarios, particularly for high-stakes applications

2. Perform comparative benchmarks of ICX360 against existing explainability toolkits (SHAP, Captum) in terms of explanation quality, computational efficiency, and scalability

3. Extend evaluation to diverse use cases (code generation, medical diagnosis, financial forecasting) to validate generalizability and robustness across domains