---
ver: rpa2
title: 'Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual
  Steganography'
arxiv_id: '2512.20168'
source_url: https://arxiv.org/abs/2512.20168
tags:
- image
- systems
- jailbreak
- content
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a critical vulnerability in commercial multimodal
  large language model (MLLM)-integrated systems that rely on safety filters to detect
  and block harmful content. These systems assume malicious content is explicitly
  visible in inputs or outputs, an assumption that breaks down in MLLM-integrated
  systems due to their ability to process multiple modalities.
---

# Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography

## Quick Facts
- **arXiv ID**: 2512.20168
- **Source URL**: https://arxiv.org/abs/2512.20168
- **Reference count**: 40
- **Primary result**: Achieves up to 99% attack success rate against commercial MLLM systems using dual steganography to embed malicious queries and responses in images

## Executive Summary
This paper exposes a critical blind spot in commercial multimodal large language model (MLLM)-integrated systems that rely on safety filters to detect harmful content. The core vulnerability stems from the assumption that malicious content is explicitly visible in inputs or outputs, which breaks down when MLLMs can process multiple modalities. The authors propose Odysseus, a novel jailbreak paradigm that uses dual steganography to covertly embed malicious queries and responses into benign-looking images, bypassing safety filters that cannot detect hidden content. Evaluations on four commercial systems (GPT-4o, Gemini-2.0-pro, Gemini-2.0-flash, and Grok-3) demonstrate that Odysseus achieves up to 99% attack success rate while maintaining high stealthiness (SSIM up to 0.8361, PSNR up to 68.6 dB).

## Method Summary
Odysseus exploits the multimodal processing capabilities of commercial MLLM-integrated systems through a four-stage pipeline. First, malicious queries are encoded into binary matrices using Base64 encoding and Hamming code error correction. Second, these binary payloads are embedded into images using a GAN-based steganography model (encoder E, decoder D, noise layer N, discriminator A) trained to minimize reconstruction loss while maximizing stealth. Third, the MLLM's function-calling capability extracts the hidden query, generates a response, and re-embeds it via the encoder. Finally, the attacker decodes the response locally. The method uses 128×128 patches embedding 32 bits each, with q=3 redundant embeddings and τ=0.5 binarization threshold. The steganography model is trained on COCO images using a loss function combining binary reconstruction (L_B), GAN loss (L_G), and image fidelity loss (L_I).

## Key Results
- Achieves up to 99% attack success rate across four commercial MLLM systems (GPT-4o, Gemini-2.0-pro, Gemini-2.0-flash, Grok-3)
- Maintains high stealthiness with SSIM up to 0.8361 and PSNR up to 68.6 dB
- Demonstrates robustness against common defenses including input filtering, steganalysis, image transformations, and adaptive detection
- Outperforms existing jailbreak baselines significantly while operating covertly through image-based steganography

## Why This Works (Mechanism)
Odysseus exploits the fundamental assumption in MLLM-integrated systems that malicious content must be explicitly visible. By embedding harmful queries and responses within images using steganography, the method bypasses safety filters that cannot detect hidden payloads. The dual-steganography approach—hiding both the malicious query and the generated response—creates a complete covert communication channel that traditional content filtering cannot intercept. The MLLM's function-calling capability serves as the extraction mechanism, allowing the model to process hidden content without triggering safety mechanisms designed for explicit inputs.

## Foundational Learning

**Dual steganography**: Hiding both malicious queries and generated responses in images to create covert communication channels. *Why needed*: Single-layer steganography only hides the input, but responses remain visible and detectable. *Quick check*: Verify that both query and response are successfully extracted and decoded from the same image.

**Hamming code error correction**: Error-correcting codes used to protect binary payloads against image transformations. *Why needed*: Image transformations (cropping, resizing, compression) corrupt hidden data; error correction ensures reliable extraction. *Quick check*: Test extraction accuracy under various image transformations with and without error correction.

**Function calling as extraction mechanism**: Using MLLM's structured output capabilities to decode hidden steganographic content. *Why needed*: Direct pixel manipulation by the model is unreliable; function calling provides a structured interface for content extraction. *Quick check*: Verify that the MLLM correctly calls extraction functions and returns expected payloads.

## Architecture Onboarding

**Component map**: Query → Hamming encoding → GAN encoder → Image → MLLM function calling → GAN decoder → Extracted query → Response generation → GAN encoder → Image → MLLM function calling → GAN decoder → Decoded response

**Critical path**: The steganography model (E/D/N/A) and function-calling interface are the critical components. The steganography model must achieve >95% extraction accuracy under transformations, while the function-calling interface must reliably extract and re-embed payloads without triggering safety filters.

**Design tradeoffs**: Higher payload capacity vs. stealth (larger embeddings reduce SSIM/PSNR), stronger error correction vs. payload size (Hamming codes add redundancy), and extraction accuracy vs. robustness (q=3 redundancy balances performance).

**Failure signatures**: Low extraction accuracy (<90%) indicates steganography model failure; MLLM refusing function calls suggests safety filter activation; high ASR but benign responses indicate false positives in HarmBench classifier.

**First experiments**:
1. Test steganography model extraction accuracy on synthetic transforms (crop, resize, compress) before full jailbreak evaluation
2. Validate function-calling interface with benign prompts to ensure proper schema implementation
3. Verify HarmBench classifier calibration using known safe/harmful prompts

## Open Questions the Paper Calls Out

**Open Question 1**: How can developers design lightweight, format-agnostic auditing mechanisms for function-calling interfaces to detect steganographic payloads without introducing prohibitive latency? The paper notes this is important for future research, as current auditing is difficult due to heterogeneous function outputs and high computational overhead.

**Open Question 2**: Does the Odysseus paradigm maintain efficacy and stealthiness when applied to non-visual modalities like audio or video in MLLM-integrated systems? The methodology is theoretically modality-agnostic but remains unevaluated beyond images.

**Open Question 3**: Can advanced redundancy mechanisms or complex error-correction coding significantly improve character-level extraction accuracy under aggressive image transformations? The current Hamming code implementation struggles with multi-bit sensitivity during sequential transformations.

## Limitations

- Key hyperparameters for steganography training (learning rates, optimizer, architecture specifics) are unspecified, making exact replication challenging
- Evaluation depends on commercial API access with potential pricing, rate limits, and model version variability
- Robustness claims against adaptive detection lack sufficient technical detail to verify specific defense mechanisms tested
- Character-level extraction accuracy degrades under aggressive multi-transformation attacks, limiting reliability

## Confidence

- **High confidence**: Core jailbreak methodology is technically sound; evaluation metrics (SSIM, PSNR, ACC) are standard and well-defined
- **Medium confidence**: Reported ASR results (up to 99%) are plausible but depend heavily on implementation details and commercial system versions
- **Low confidence**: Robustness against adaptive detection and advanced steganalysis claims lack sufficient technical verification

## Next Checks

1. **Extraction accuracy validation**: Test the trained steganography model's extraction accuracy under various image transformations (cropping, resizing, compression) using the published code. Verify that the model achieves >95% accuracy on synthetic test cases before attempting full jailbreak evaluation.

2. **Function calling interface verification**: Validate that the `extract` and `hide` function schemas work correctly with the target MLLM APIs. Test with benign prompts first to ensure the MLLM properly calls functions and returns structured responses before attempting jailbreak scenarios.

3. **HarmBench classifier calibration**: Verify the HarmBench classifier implementation and threshold settings used for ASR calculation. Test the classifier on known safe/harmful prompts to ensure it correctly identifies harmful content, and consider manual verification for a subset of high-scoring responses.