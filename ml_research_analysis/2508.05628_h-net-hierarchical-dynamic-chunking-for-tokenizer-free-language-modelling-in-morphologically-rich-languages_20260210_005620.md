---
ver: rpa2
title: 'H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling
  in Morphologically-Rich Languages'
arxiv_id: '2508.05628'
source_url: https://arxiv.org/abs/2508.05628
tags:
- h-net
- persian
- zwnj
- morphological
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes H-Net++, a tokenizer-free language model for
  morphologically-rich languages like Persian. The key innovation is a hierarchical
  dynamic chunking architecture that learns segmentation boundaries jointly with language
  modeling, using a lightweight transformer mixer to propagate cross-chunk context.
---

# H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages

## Quick Facts
- **arXiv ID**: 2508.05628
- **Source URL**: https://arxiv.org/abs/2508.05628
- **Reference count**: 8
- **Primary result**: Achieves 0.159 BPB reduction versus BPE-based GPT-2-fa with 12% better compression

## Executive Summary
H-Net++ introduces a hierarchical dynamic chunking architecture for tokenizer-free language modeling, specifically designed for morphologically-rich languages like Persian. The model learns segmentation boundaries jointly with language modeling through a lightweight transformer mixer that propagates cross-chunk context. By employing a two-level latent hyper-prior for document-level consistency and specialized handling of orthographic artifacts like Persian ZWNJ, H-Net++ demonstrates state-of-the-art performance on a 1.4B-token Persian corpus. The learned chunks naturally align with Persian morphology without explicit supervision, offering an effective tokenizer-free solution while maintaining computational efficiency.

## Method Summary
The method employs hierarchical dynamic chunking where the model learns to segment text into chunks while simultaneously performing language modeling. A lightweight transformer mixer propagates context across chunk boundaries, enabling long-range dependencies without fixed tokenization. The architecture uses a two-level latent hyper-prior to maintain document-level consistency, and includes specialized handling for Persian orthographic features like ZWNJ. Curriculum-based training with staged sequence lengths gradually increases complexity during training. The model operates without predefined tokenization, instead learning optimal segmentation patterns that align with morphological structure.

## Key Results
- 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better compression)
- 5.4 percentage point gain on ParsGLUE benchmark
- 53% improved robustness to ZWNJ corruption
- 73.8% F1 score on gold morphological boundaries

## Why This Works (Mechanism)
The hierarchical dynamic chunking approach works by learning segmentation boundaries jointly with language modeling, allowing the model to adapt chunk boundaries to the underlying morphological structure of the language. The lightweight transformer mixer efficiently propagates context across chunks without the computational overhead of processing fixed-length tokens. The two-level latent hyper-prior captures document-level dependencies that would otherwise be lost with independent chunk processing. This architecture eliminates the need for predefined tokenization while maintaining the ability to model long-range dependencies crucial for morphologically-rich languages.

## Foundational Learning

**Dynamic Chunking**: Why needed - Traditional fixed tokenization fails for morphologically-rich languages with complex word formation. Quick check - Model can segment text into variable-length chunks based on linguistic structure.

**Latent Hyper-Prior**: Why needed - Captures higher-level document structure and consistency across chunks. Quick check - Document-level coherence improves when processing across chunk boundaries.

**Transformer Mixer**: Why needed - Efficiently propagates context across dynamically determined chunk boundaries. Quick check - Cross-chunk dependencies are modeled without excessive computational cost.

**Curriculum-Based Training**: Why needed - Gradually increases sequence complexity to improve convergence. Quick check - Model performance improves monotonically with staged training progression.

**Orthographic Artifact Handling**: Why needed - Persian ZWNJ and similar features require specialized treatment for accurate modeling. Quick check - Model maintains performance under controlled corruption of orthographic markers.

## Architecture Onboarding

**Component Map**: Input Text -> Dynamic Chunker -> Transformer Mixer -> Language Model Head -> Output Probabilities

**Critical Path**: The most critical sequence is Input Text → Dynamic Chunker → Transformer Mixer, as these components jointly determine the segmentation strategy and cross-chunk context propagation that enables effective language modeling without fixed tokenization.

**Design Tradeoffs**: The architecture trades computational efficiency for flexibility by using a lightweight transformer mixer instead of full cross-chunk attention. This allows modeling long-range dependencies while keeping computational costs manageable. The dynamic chunking approach sacrifices the speed benefits of fixed tokenization but gains the ability to adapt to morphological structure.

**Failure Signatures**: Model may produce overly short or long chunks if the segmentation learning becomes unstable. Cross-chunk context propagation may fail for very long-range dependencies beyond the mixer's capacity. The curriculum-based training may not converge properly if sequence length progression is too aggressive.

**First Experiments**: 
1. Test basic segmentation capability on known morphological boundaries in Persian text
2. Evaluate cross-chunk context propagation with controlled ablation of the transformer mixer
3. Assess curriculum-based training effectiveness by comparing staged vs. uniform sequence length training

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation scope limited to Persian language, raising questions about generalizability to other morphologically-rich languages
- Computational efficiency claims need independent verification for larger models and longer sequences
- Learned chunk boundaries may not align with linguistic intuitions or downstream tasks requiring explicit morphological analysis

## Confidence

- BPB and compression results: High
- ParsGLUE performance claims: Medium (language-specific)
- Morphological boundary F1: Medium (depends on annotation quality)
- Computational efficiency claims: Medium (limited scaling data)

## Next Checks

1. Evaluate H-Net++ on morphologically-rich languages from different families (e.g., Turkish, Arabic, Hebrew) to assess cross-linguistic generalizability of the hierarchical dynamic chunking approach.

2. Conduct ablation studies isolating the contributions of: (a) two-level latent hyper-prior, (b) transformer mixer architecture, and (c) curriculum-based training to quantify their individual impact on performance.

3. Test model robustness beyond ZWNJ corruption by evaluating on systematically corrupted Persian text with other orthographic variations (diacritics, spelling variations) and compare against both BPE and other tokenizer-free baselines.