---
ver: rpa2
title: Compressing Deep Neural Networks Using Explainable AI
arxiv_id: '2507.05286'
source_url: https://arxiv.org/abs/2507.05286
tags:
- pruning
- compression
- accuracy
- importance
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of compressing deep neural networks
  (DNNs) to reduce memory footprint while maintaining accuracy for deployment on resource-constrained
  edge devices. The proposed approach uses a gradient-based explainable AI (XAI) method
  called Layer-wise Relevance Propagation (LRP) to compute importance scores for DNN
  parameters (weights).
---

# Compressing Deep Neural Networks Using Explainable AI

## Quick Facts
- arXiv ID: 2507.05286
- Source URL: https://arxiv.org/abs/2507.05286
- Authors: Kimia Soroush; Mohsen Raji; Behnam Ghavami
- Reference count: 26
- Primary result: Proposes XAI-based DNN compression using LRP importance scoring, achieving 64% size reduction and 42% accuracy improvement

## Executive Summary
This paper presents a novel approach to compressing deep neural networks using explainable AI techniques, specifically Layer-wise Relevance Propagation (LRP). The method addresses the challenge of deploying large DNN models on resource-constrained edge devices by reducing memory footprint while maintaining or improving accuracy. The approach uses LRP to compute importance scores for network parameters, then applies a two-step compression process: pruning unimportant weights and applying mixed-precision quantization based on importance scores. The experimental results demonstrate significant compression ratios while achieving notable accuracy improvements over existing XAI-based compression methods.

## Method Summary
The proposed compression framework uses LRP to compute importance scores for DNN parameters, which guide a two-stage compression process. First, weights with zero or negative importance scores are pruned from the network. Second, remaining weights undergo mixed-precision quantization where higher precision is assigned to more important weights based on their LRP scores. This approach leverages the interpretability provided by XAI methods to make informed decisions about which parameters can be aggressively compressed versus which require higher precision preservation. The method aims to balance compression efficiency with model accuracy by prioritizing the retention of critical network parameters.

## Key Results
- Achieved 64% reduction in model size compared to uncompressed baseline
- Improved accuracy by 42% relative to state-of-the-art XAI-based compression methods
- Demonstrated effectiveness of LRP-based importance scoring for guided model compression
- Showed potential for combining pruning and mixed-precision quantization in unified framework

## Why This Works (Mechanism)
The mechanism leverages the interpretability provided by LRP to identify which network parameters contribute most to model predictions. By computing relevance scores that indicate the contribution of each weight to final outputs, the method can distinguish between critical and redundant parameters. This information enables intelligent compression decisions - aggressively pruning parameters that have minimal impact on model behavior while preserving higher precision for weights that are crucial for accurate predictions. The mixed-precision approach allows for fine-grained control over the compression-accuracy tradeoff, storing important weights with higher precision while aggressively compressing less important ones.

## Foundational Learning
- Layer-wise Relevance Propagation (LRP): A gradient-based XAI method for computing feature importance; needed for identifying which network parameters contribute most to predictions; quick check: verify LRP correctly propagates relevance from output to input layers
- Mixed-precision quantization: Storing different parameters with varying bit-widths; needed to balance compression ratio with accuracy preservation; quick check: confirm precision assignments align with importance scores
- Network pruning: Removing redundant or unimportant parameters; needed for initial model size reduction; quick check: validate pruned model maintains baseline accuracy
- Weight importance scoring: Quantifying parameter significance for model predictions; needed to guide compression decisions; quick check: test correlation between scores and actual impact on outputs

## Architecture Onboarding

**Component Map:**
Input -> LRP Importance Scoring -> Pruning Module -> Mixed-Precision Quantization -> Compressed Model

**Critical Path:**
The critical path involves computing LRP scores for all parameters, then using these scores to guide both pruning and quantization decisions. The quality of importance scoring directly impacts compression effectiveness and accuracy preservation.

**Design Tradeoffs:**
- Aggressive pruning vs. accuracy retention: More aggressive pruning increases compression but risks accuracy degradation
- Precision levels in quantization: More precision levels provide finer control but increase implementation complexity
- LRP configuration parameters: Affect quality of importance scores and computational overhead

**Failure Signatures:**
- Accuracy degradation beyond acceptable thresholds indicates importance scoring failure or overly aggressive compression
- Minimal size reduction suggests ineffective pruning or inappropriate quantization thresholds
- Inconsistent results across different architectures may indicate method sensitivity to model structure

**3 First Experiments:**
1. Apply LRP scoring to a simple feedforward network and visualize importance distribution
2. Test single-stage compression (either pruning or quantization alone) to establish baseline effectiveness
3. Evaluate sensitivity of results to LRP hyperparameters on a small network

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The unusually high 42% accuracy improvement claim compared to state-of-the-art methods requires independent verification
- Limited architectural details and dataset specifications make generalizability assessment difficult
- Evaluation focuses on memory footprint without actual measurements of inference latency and energy consumption on edge hardware

## Confidence

**High confidence:** The basic methodology of using LRP for importance scoring and the two-step compression process (pruning + quantization)

**Medium confidence:** The reported compression ratio of 64% and relative accuracy improvements, pending independent verification

**Low confidence:** Claims about deployment efficiency on edge devices without actual hardware measurements

## Next Checks
1. Replicate the experiments on a different dataset with varying DNN architectures to test generalizability
2. Conduct measurements of actual inference latency and energy consumption on target edge hardware devices
3. Compare the approach against non-XAI-based state-of-the-art compression methods to contextualize the claimed improvements