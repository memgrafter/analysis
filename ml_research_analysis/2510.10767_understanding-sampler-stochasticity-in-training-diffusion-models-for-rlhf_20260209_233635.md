---
ver: rpa2
title: Understanding Sampler Stochasticity in Training Diffusion Models for RLHF
arxiv_id: '2510.10767'
source_url: https://arxiv.org/abs/2510.10767
tags:
- reward
- diffusion
- preprint
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reward gap in diffusion models
  fine-tuned with RLHF, where stochastic samplers are used for training and deterministic
  samplers for inference. The authors theoretically bound the reward gap between SDE
  and ODE sampling and show that higher stochasticity during training improves ODE
  inference quality.
---

# Understanding Sampler Stochasticity in Training Diffusion Models for RLHF

## Quick Facts
- arXiv ID: 2510.10767
- Source URL: https://arxiv.org/abs/2510.10767
- Authors: Jiayuan Sheng; Hanyang Zhao; Haoxian Chen; David D. Yao; Wenpin Tang
- Reference count: 40
- This paper addresses the challenge of reward gap in diffusion models fine-tuned with RLHF, where stochastic samplers are used for training and deterministic samplers for inference

## Executive Summary
This paper investigates the theoretical and empirical aspects of reward gaps in diffusion models trained with Reinforcement Learning from Human Feedback (RLHF). The authors focus on the discrepancy between stochastic sampling methods used during training and deterministic methods used during inference, which can lead to performance degradation. Through theoretical analysis and large-scale experiments on text-to-image models, they demonstrate that higher stochasticity during training leads to better quality ODE inference and that reward gaps naturally narrow as training progresses.

## Method Summary
The authors develop a theoretical framework to bound the reward gap between stochastic differential equation (SDE) sampling used in training and ordinary differential equation (ODE) sampling used in inference. They introduce measures of stochasticity and prove that higher stochasticity during training results in better ODE sampling quality. The empirical validation involves training large-scale diffusion models on text-to-image tasks, comparing different levels of stochasticity and measuring reward gaps across training epochs. The methodology combines theoretical analysis with controlled experiments across multiple diffusion model architectures and scales.

## Key Results
- Theoretical bounds on reward gap between SDE and ODE sampling are established and validated
- Higher stochasticity during training consistently improves ODE inference quality
- Reward gaps between training and inference methods naturally narrow over training epochs
- Large-scale text-to-image model experiments confirm theoretical predictions across different model scales

## Why This Works (Mechanism)
The effectiveness stems from the alignment between training and inference distributions. When models are trained with higher stochasticity, they learn to handle noise more robustly, which translates to better performance when deterministic sampling is used at inference. The theoretical bounds show that the difference between stochastic and deterministic sampling can be controlled through the level of stochasticity in training. As training progresses, the model parameters converge, naturally reducing the reward gap regardless of the initial disparity.

## Foundational Learning

### Stochastic Differential Equations (SDEs) in Diffusion Models
**Why needed**: SDEs provide the mathematical foundation for continuous-time diffusion models and enable the theoretical analysis of stochastic sampling behaviors.
**Quick check**: Verify understanding of the relationship between SDEs and discrete diffusion steps in standard DDPM models.

### Reinforcement Learning from Human Feedback (RLHF)
**Why needed**: RLHF is the core training paradigm being analyzed, where human preference data guides model improvement through reward modeling.
**Quick check**: Confirm knowledge of how reward models are trained and used to fine-tune base diffusion models.

### ODE Solvers for Deterministic Sampling
**Why needed**: Deterministic ODE sampling is commonly used in practice for faster inference, creating the gap that this paper addresses.
**Quick check**: Understand the difference between DDPM sampling, DDIM sampling, and ODE-based deterministic sampling.

## Architecture Onboarding

### Component Map
Diffusion Model Base -> Reward Model -> Stochastic Sampler (Training) -> Deterministic Sampler (Inference) -> Performance Gap

### Critical Path
The critical path flows from the base diffusion model through the reward model training, where the choice of stochastic sampler during RLHF directly impacts the quality of deterministic inference. The reward gap emerges at this junction and is the primary focus of the theoretical and empirical analysis.

### Design Tradeoffs
The main tradeoff involves training stability versus inference quality. Higher stochasticity improves ODE sampling quality but may slow convergence and increase variance during training. The authors show this tradeoff is worthwhile as the reward gap narrows significantly with appropriate stochasticity levels.

### Failure Signatures
Reward gaps that don't narrow over training epochs indicate misalignment between training and inference distributions. Excessive stochasticity can lead to training instability and poor convergence. Insufficient stochasticity results in poor ODE sampling quality at inference time.

### 3 First Experiments
1. Measure reward gap between SDE and ODE sampling at different training epochs
2. Compare ODE sampling quality across different stochasticity levels in training
3. Test theoretical bounds on reward gap with varying model scales and architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds rely on assumptions about Lipschitz continuity and gradient boundedness that may not hold precisely with neural network discriminators
- Empirical validation is limited to text-to-image domains and may not generalize to other modalities like language or audio
- Analysis assumes specific reward model architecture and training regime that may not transfer to alternative implementations

## Confidence

**High confidence**: Empirical observation that reward gaps narrow over training epochs, supported by clear convergence patterns
**Medium confidence**: Theoretical bounds provide useful directional guidance but may overestimate practical effects due to conservative assumptions
**Medium confidence**: Claim about higher-stochasticity training improving ODE inference quality, though relationship appears consistent across experiments

## Next Checks
1. Test theoretical bounds with different reward model architectures and loss functions to assess robustness
2. Validate findings across multiple data modalities including text and audio to establish broader applicability
3. Investigate interaction between stochasticity levels and different diffusion model variants (e.g., DDIM, DDPM) to understand universality of effects