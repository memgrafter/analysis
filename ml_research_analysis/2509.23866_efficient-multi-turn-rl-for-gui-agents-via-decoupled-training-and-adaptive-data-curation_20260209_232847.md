---
ver: rpa2
title: Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive
  Data Curation
arxiv_id: '2509.23866'
source_url: https://arxiv.org/abs/2509.23866
tags:
- training
- arxiv
- task
- tasks
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of applying reinforcement
  learning to GUI agents, which struggle with slow multi-turn interactions and insufficient
  high-quality training data. To solve this, the authors propose DART, a decoupled
  training framework that separates RL into four asynchronous modules (environment
  cluster, rollout service, data manager, and trainer) to maximize resource utilization
  and throughput.
---

# Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation

## Quick Facts
- arXiv ID: 2509.23866
- Source URL: https://arxiv.org/abs/2509.23866
- Reference count: 21
- Primary result: Achieves 42.13% task success rate on OSWorld, 14.61% absolute improvement over baseline

## Executive Summary
This paper addresses the inefficiency of applying reinforcement learning to GUI agents, which struggle with slow multi-turn interactions and insufficient high-quality training data. To solve this, the authors propose DART, a decoupled training framework that separates RL into four asynchronous modules (environment cluster, rollout service, data manager, and trainer) to maximize resource utilization and throughput. They also introduce an adaptive data curation strategy that dynamically adjusts sampling frequency, trajectory length, and training focus based on task difficulty and entropy. Evaluated on the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute improvement over the baseline, and outperforms the previous open-source state-of-the-art by 7.34%, demonstrating both efficiency gains and superior performance.

## Method Summary
The DART framework decouples reinforcement learning into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. Environments execute tasks and generate rollouts continuously, while rollout workers serve inference requests via pooled GPUs with per-worker model synchronization. The data manager stores trajectories and implements adaptive curation strategies, including high-entropy step selection and truncated importance sampling. The trainer performs step-wise GRPO updates asynchronously. This architecture enables rollout-wise sampling where individual trajectories, not batches, are the scheduling unit, maximizing resource utilization. Adaptive data curation dynamically adjusts sampling frequency, trajectory length, and training focus based on task difficulty and entropy to improve sample efficiency.

## Key Results
- Achieves 42.13% task success rate on OSWorld benchmark, 14.61% absolute improvement over baseline
- Demonstrates 1.6× GPU utilization for rollout, 1.9× training throughput, and 5.5× environment utilization
- Outperforms previous open-source state-of-the-art by 7.34% absolute success rate

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Asynchronous Architecture
Separating RL into four non-blocking modules (environment cluster, rollout service, data manager, trainer) maximizes resource utilization by eliminating sequential dependencies. Environments generate rollouts continuously; rollout workers serve inference requests via pooled GPUs; trainer updates policy asynchronously; per-worker model sync avoids global halts. This pipeline parallelism ensures no single component becomes a blocking bottleneck. Core assumption: GUI tasks have heterogeneous trajectory lengths (tens of minutes, dozens of steps), so batch-wise or task-wise sampling causes significant idle gaps that decoupling can exploit.

### Mechanism 2: Rollout-wise Sampling with Per-Worker Model Synchronization
Treating individual trajectories—not batches or tasks—as the scheduling unit, combined with staggered model updates, maximizes environment and GPU throughput. Once an environment finishes a rollout, it immediately requests the next task without waiting for peers. Model weights update per-worker (not globally), so some workers serve inference while others refresh, avoiding system-wide downtime. Core assumption: Task difficulty and environmental randomness cause trajectory lengths to vary substantially even for identical tasks, creating idle slots under coarser scheduling.

### Mechanism 3: Multi-Level Adaptive Data Curation
Adaptive curation across task, trajectory, step, and token levels improves sample efficiency by focusing computation on informative signals and stabilizing off-policy updates. Experience Pool supplements rare successes for hard tasks; dynamic rollout/length reduces over-sampling easy tasks; high-entropy step selection prioritizes critical decisions; truncated importance sampling aligns rollout-to-trainer distribution gaps. Core assumption: GUI tasks have sparse rewards and heavy-tailed difficulty; uniform sampling wastes compute on easy tasks while providing few positive signals for hard ones.

## Foundational Learning

### Concept: Group Relative Policy Optimization (GRPO)
- Why needed here: The paper uses step-wise GRPO to compute advantages by normalizing rewards across steps within a task group, avoiding separate value function learning.
- Quick check question: Can you explain how GRPO differs from PPO in its advantage normalization and why that matters for multi-step GUI tasks?

### Concept: Importance Sampling for Off-Policy Correction
- Why needed here: Decoupled rollout and training cause distribution shift; truncated importance sampling reweights gradients to correct for policy mismatch.
- Quick check question: What happens to training stability if the rollout policy diverges significantly from the trainer policy without importance sampling?

### Concept: Entropy as a Proxy for Decision Criticality
- Why needed here: The method selects high-entropy steps under the assumption they correspond to critical forks in reasoning.
- Quick check question: In a GUI navigation task, what might cause a high-entropy prediction that is NOT a critical decision (e.g., ambiguous but inconsequential UI elements)?

## Architecture Onboarding

### Component map:
Env Cluster -> Rollout Service -> Data Manager -> Trainer

### Critical path:
Task dispatch → Env execution → Rollout inference → Data storage → Filtering (high-entropy, reward-based) → Trainer update → Checkpoint sync to Rollout workers

### Design tradeoffs:
- Rollout-wise sampling vs. consistency: Finer granularity improves throughput but may increase staleness across workers.
- High-entropy step selection vs. coverage: Focuses learning but risks missing important low-entropy transitions.
- Experience Pool vs. on-policy purity: Improves signal for hard tasks but introduces off-policy bias (mitigated by importance sampling).

### Failure signatures:
- Catastrophic accuracy drop mid-training (likely missing importance sampling or KL regularization).
- Environment utilization stuck low (check rollout-wise scheduling or model sync bottleneck).
- No improvement on hard tasks (Experience Pool may be empty or entropy threshold misconfigured).

### First 3 experiments:
1. Run a small-scale coupling test: Compare batch-wise vs. rollout-wise sampling on 8 environments with 4 tasks; measure env utilization gap.
2. Ablate high-entropy step selection: Train with 100% vs. top 80% steps on a 45-task subset; track success rate divergence.
3. Stress-test distribution alignment: Disable importance sampling; monitor when accuracy collapses (expect near step 60 as in Figure 6(d)).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DART framework effectively support more granular or composite action spaces (e.g., simultaneous keyboard modifiers) to resolve the execution failures observed in complex desktop interactions?
- Basis in paper: [explicit] The authors acknowledge "action space limitations" in Figure 14(b), where the model fails to hold "Ctrl" while clicking, executing actions sequentially instead.
- Why unresolved: The current action space is restricted to discrete primitives like `click` and `hotkey`, which cannot represent the simultaneous inputs required for specific multi-file selections.
- What evidence would resolve it: An evaluation of DART agents trained with a composite action space on benchmarks requiring chorded inputs (e.g., Ctrl+Shift+Click).

### Open Question 2
- Question: Is the static threshold of selecting the top 80% high-entropy steps universally optimal, or does it discard critical deterministic decisions in specific GUI workflows?
- Basis in paper: [inferred] Section 4.3 states the model selects "steps whose entropy is at least larger than 20%," but does not analyze if this fixed cutoff negatively impacts tasks with many low-entropy but crucial operational steps.
- Why unresolved: While the method prioritizes "critical forks," it assumes low-entropy steps are non-critical, potentially filtering out repetitive but essential grounding actions.
- What evidence would resolve it: A sensitivity analysis comparing performance when varying the entropy percentile cutoff (e.g., 50% vs. 80% vs. 100%) across diverse task types.

### Open Question 3
- Question: Does the "Experience Pool" mechanism induce overfitting to pre-collected trajectories rather than fostering genuine exploration and generalization for novel tasks?
- Basis in paper: [inferred] Section 4.2 describes pre-populating the pool with successful trajectories to aid difficult tasks, but the training is restricted to a specific subset of OSWorld tasks where such data is available.
- Why unresolved: It is unclear if the agent learns to replicate trajectories rather than learning transferable policies, which could fail in environments where no pre-collected successes exist.
- What evidence would resolve it: Testing the trained agent on out-of-distribution software or tasks where the Experience Pool is strictly empty or unavailable.

## Limitations
- Infrastructure complexity: The decoupling architecture relies on precise orchestration across four asynchronous modules; implementation bugs or misconfigurations could mask or inflate reported efficiency gains.
- Adaptive curation sensitivity: The effectiveness of high-entropy step selection and dynamic rollout frequency is sensitive to task-specific hyperparameters not fully explored in ablation studies.
- Generalizability: Performance gains are benchmarked on OSWorld; real-world GUI environments may differ in complexity, reward sparsity, and interaction patterns.

## Confidence

### Major Uncertainties and Limitations
- **High confidence** in decoupled architecture improving GPU and environment utilization (supported by measurable throughput metrics and aligned with prior work on asynchronous RL training).
- **Medium confidence** in adaptive data curation boosting task success (significant gains shown in controlled experiments, but heavily dependent on hyperparameter tuning and entropy-quality correlation assumptions).
- **Low confidence** in absolute task success rate improvements without access to full task lists and Experience Pool details (critical components for reproducibility and fair comparison).

## Next Checks

### Exactly 3 Concrete Next Validation Checks
1. **Task diversity validation**: Reconstruct or obtain the exact list of 203 OSWorld tasks used and verify train/test splits to ensure reported 42.13% success rate is reproducible and not benchmark-specific.
2. **Experience Pool ablation**: Systematically ablate the pre-collected Experience Pool across different difficulty tiers; measure impact on hard-task success rates to quantify its necessity versus online-only learning.
3. **Cross-environment robustness**: Evaluate DART on a held-out set of real-world GUI tasks or an alternative benchmark (e.g., mobile GUIs) to test generalizability beyond OSWorld.