---
ver: rpa2
title: Stable On-Policy Distillation through Adaptive Target Reformulation
arxiv_id: '2601.07155'
source_url: https://arxiv.org/abs/2601.07155
tags:
- student
- teacher
- on-policy
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Veto addresses training instability in on-policy knowledge distillation\
  \ by reformulating the target distribution in logit space, effectively suppressing\
  \ harmful gradients for low-confidence student predictions. The method constructs\
  \ a geometric bridge between teacher and student distributions using a parameter\
  \ \u03B2 that controls both gradient vetoing and mode-seeking behavior."
---

# Stable On-Policy Distillation through Adaptive Target Reformulation

## Quick Facts
- arXiv ID: 2601.07155
- Source URL: https://arxiv.org/abs/2601.07155
- Reference count: 12
- Veto achieves 39.9% accuracy on GSM8K, 16.4% Pass@10 on HumanEval, and 56.5% win-rate on DialogSum

## Executive Summary
Veto addresses training instability in on-policy knowledge distillation by reformulating the target distribution in logit space. The method constructs a geometric bridge between teacher and student distributions using a parameter β that controls both gradient vetoing and mode-seeking behavior. Extensive experiments across GSM8K, HumanEval, and DialogSum demonstrate consistent improvements over supervised fine-tuning and existing on-policy baselines.

## Method Summary
Veto reformulates the distillation target by modifying the teacher's probability distribution in logit space. The method introduces a parameter β that determines how aggressively to suppress gradients when the student's confidence is low. This creates an adaptive mechanism that vetoes harmful gradients while maintaining effective knowledge transfer. The geometric bridge interpretation connects the modified target to both forward and reverse KL divergences, providing theoretical grounding for the approach.

## Key Results
- GSM8K accuracy: 39.9% (Veto) vs 35.1% (baseline)
- HumanEval Pass@10: 16.4% (Veto) vs 10.6% (baseline)
- DialogSum win-rate: 56.5% (Veto) vs 54.3% (baseline)

## Why This Works (Mechanism)
The geometric bridge between teacher and student distributions is controlled by β, which modulates the trade-off between gradient suppression and mode-seeking behavior. When student confidence is low, Veto vetoes gradients that would push the student toward overconfident but incorrect predictions. This adaptive mechanism aligns with forward KL's preference for mode-seeking while maintaining stability through entropy regularization. The logit-space reformulation provides a natural interface for implementing this veto mechanism without requiring explicit probability computation.

## Foundational Learning
- Forward KL divergence: why needed - measures asymmetric divergence favoring mode-seeking; quick check - verify KL(P||Q) behavior with mismatched distributions
- Reverse KL divergence: why needed - complements forward KL with mass-covering properties; quick check - compare mode-seeking vs mass-covering behavior
- Entropy regularization: why needed - stabilizes training by preventing overconfidence; quick check - monitor entropy values during training
- Logit-space operations: why needed - enables efficient gradient manipulation; quick check - verify logit transformations preserve probability constraints
- Teacher-student capacity gap: why needed - affects distillation effectiveness; quick check - measure KL divergence between teacher and student outputs
- Adaptive gradient suppression: why needed - prevents harmful updates from low-confidence predictions; quick check - track gradient magnitudes across confidence levels

## Architecture Onboarding

Component map: Teacher model -> Veto transformation -> Student model training

Critical path: Teacher forward pass → Veto logit reformulation → Student backward pass

Design tradeoffs: β controls the balance between stability and expressiveness; higher β increases stability but may reduce knowledge transfer effectiveness

Failure signatures: Training divergence with low β; plateaued performance with high β; gradient explosion with inappropriate β scaling

First experiments:
1. Baseline on-policy distillation without Veto modification
2. Veto with extreme β values (very low and very high) to establish bounds
3. Ablation study comparing Veto to supervised fine-tuning on the same tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the results and methodology.

## Limitations
The practical effectiveness of Veto depends heavily on the choice of β and may vary across tasks. The optimal β selection strategy requires further exploration, and the method's behavior when the teacher and student have vastly different capacities remains unclear. The claim of "stable" training is supported by the absence of reported divergence issues, but explicit stability metrics or comparisons to instability rates in baseline methods would strengthen this assertion.

## Confidence
High confidence: The empirical improvements across GSM8K, HumanEval, and DialogSum are well-documented and statistically significant. The connection between Veto and adaptive gradient suppression in forward KL is theoretically sound.

Medium confidence: The effectiveness of Veto may depend on task-specific characteristics and the quality of the teacher model. The optimal β selection strategy requires further exploration.

Low confidence: The long-term stability of Veto in extreme generation lengths or highly diverse domains has not been tested. The method's behavior when the teacher and student have vastly different capacities remains unclear.

## Next Checks
1. Conduct an ablation study varying β across multiple orders of magnitude to identify sensitivity and optimal ranges for different task types.
2. Test Veto with teacher-student pairs having size ratios beyond the 7B-to-7B setup to assess scalability and performance degradation patterns.
3. Implement explicit stability metrics comparing training curves, gradient norms, and divergence rates between Veto and baseline on-policy distillation across extended training runs.