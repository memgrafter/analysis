---
ver: rpa2
title: 'ARC-Encoder: learning compressed text representations for large language models'
arxiv_id: '2510.20535'
source_url: https://arxiv.org/abs/2510.20535
tags:
- tokens
- decoder
- encoder
- compressed
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARC-Encoder compresses text contexts into fewer continuous representations
  without modifying the decoder, reducing computational cost while maintaining performance.
  It uses a pooling mechanism to merge hidden states in the encoder's final layer,
  followed by an MLP projector to align outputs with the decoder's embedding space.
---

# ARC-Encoder: learning compressed text representations for large language models

## Quick Facts
- **arXiv ID:** 2510.20535
- **Source URL:** https://arxiv.org/abs/2510.20535
- **Reference count:** 40
- **Primary result:** Compresses text contexts into fewer continuous representations without modifying the decoder, reducing computational cost while maintaining performance.

## Executive Summary
ARC-Encoder introduces a method to compress text contexts into continuous representations that can replace raw token embeddings in frozen decoder LLMs. The approach uses query pooling in the encoder's final attention layer followed by an MLP projector to align outputs with the decoder's embedding space. By pretraining on alternating reconstruction and continuation tasks, ARC-Encoder achieves near open-book baseline performance with 4× compression on QA tasks and extends context windows up to 8× for long documents. The method preserves the decoder architecture while reducing computational cost and enabling flexible deployment across multiple decoders with minimal additional parameters.

## Method Summary
ARC-Encoder compresses text contexts by removing the output head and last two layers from a decoder LLM (Llama3.2 3B backbone), disabling the causal mask, and applying query pooling in the final self-attention layer. The pooling mechanism averages consecutive queries to reduce sequence length by a specified factor while keeping keys and values uncompressed. A 2-layer MLP projector (without activation) maps the pooled representations to the decoder's embedding dimension through a dimensional bottleneck. The model is pretrained on Common Crawl data using alternating reconstruction (regenerating original text) and continuation (generating next tokens) tasks, signaled by special tokens <Rec> and <Cont>. During fine-tuning, few-shot examples are compressed and interleaved with queries, with loss applied only to final answers. The approach enables a single encoder to adapt to multiple decoders through decoder-specific MLPs, adding only 15M parameters per decoder.

## Key Results
- Achieves near open-book baseline performance with 4× compression on QA tasks
- Extends context windows up to 8× for long documents, outperforming context extension baselines
- Enables flexible deployment with a single encoder adapted to multiple decoders with minimal additional parameters (<1% of encoder parameters per decoder)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pooling queries in the final attention layer preserves more semantic information than earlier pooling.
- **Mechanism:** Consecutive queries in the last self-attention module are averaged to produce fewer output vectors; keys and values remain uncompressed. The pooled queries attend to the full key-value pairs, producing compressed representations where each vector implicitly summarizes a fixed-width token window. Late-stage pooling allows the encoder to fully process information before merging.
- **Core assumption:** Information integration completes before the final layer; averaging adjacent hidden states retains task-relevant semantics.
- **Evidence anchors:**
  - [Section 3.2]: "pooling is performed in the self-attention module. We average consecutive queries to reach the targeted pooling factor, while keys and values remain unchanged."
  - [Section 4.4]: "We explored inserting the pooling mechanism earlier in the encoder, but this lead to poorer performance. This follows the intuition that the information should be as processed as possible before pooling."
  - [corpus]: Weak direct evidence; related work (Tang et al. 2025, GMSA) also suggests late-stage merging is beneficial but does not conclusively prove causality.
- **Break condition:** If the encoder is too shallow or pooling factor too aggressive (e.g., 32×), performance degrades sharply (Section A.2: "pooling factor of 32 where the model has an averaged score of 33.1").

### Mechanism 2
- **Claim:** Alternating reconstruction and continuation pretraining tasks produces representations that are both decompressible and useful for downstream generation.
- **Mechanism:** Reconstruction forces the encoder to retain enough information to reproduce the original tokens via the decoder, preventing trivial compression. Continuation trains the encoder-decoder to generate plausible next tokens directly from compressed segments, aligning representations with inference behavior. The special tokens <Rec> and <Cont> signal which task to perform, conditioning the decoder appropriately.
- **Core assumption:** Task-specific tokens can reliably switch decoder behavior; both reconstruction and continuation objectives are necessary for general-purpose compression.
- **Evidence anchors:**
  - [Section 3.3]: "these compressed representations cannot be well exploited by the decoder on downstream tasks, as the model tends to regurgitate the entire context, instead of extracting pertinent information from it."
  - [Section 4.4, Table 4]: 0% reconstruction yields 39.8 avg score; 100% reconstruction drops to 37.5; 20% reconstruction achieves best 41.6.
  - [corpus]: Related work (ICAE, Ge et al. 2024) also uses reconstruction but does not establish the alternating task necessity; corpus evidence is indirect.
- **Break condition:** If reconstruction ratio is too high (>50%) or omitted entirely, downstream task performance degrades; if continuation is omitted, models fail to generalize beyond rote reproduction.

### Mechanism 3
- **Claim:** A lightweight MLP projector enables a single shared encoder to adapt to multiple decoders with minimal per-decoder parameters.
- **Mechanism:** The encoder produces a general compressed representation; a 2-layer MLP (no activation, bottlenecked dimension) maps this to each decoder's specific embedding space. Only the MLP and special tokens are decoder-specific (<1% of encoder parameters), allowing the main encoder to remain shared and portable.
- **Core assumption:** Different decoders' embedding spaces are sufficiently aligned that a simple linear projection can bridge them; the encoder can learn representations that are not overfit to one decoder's hidden state geometry.
- **Evidence anchors:**
  - [Section 3.1]: "The MLP projector is a 2-layer MLP without activation, mapping the encoder output to the embedding dimension of the decoder through a dimensional bottleneck."
  - [Section 4.2]: "a single encoder can be trained to be used by multiple decoders... we introduce a separate MLP projector for each decoder. This allows lightweight specialization... adding only 15M parameters per decoder."
  - [corpus]: No direct corpus evidence for multi-decoder projector mechanism; related methods (xRAG, PISCO) require decoder fine-tuning or single-decoder assumptions.
- **Break condition:** If decoders have radically different architectures or embedding dimensions without sufficient MLP capacity, representations may not transfer; the paper shows OLMo-7B adaptation works but with a larger performance gap than same-family decoders (Table 2).

## Foundational Learning

- **Concept: Soft vs. Hard Context Compression**
  - Why needed here: ARC-Encoder performs soft compression (continuous vectors); understanding the distinction clarifies why it achieves higher compression ratios than token pruning methods but requires encoder training.
  - Quick check question: Does the method operate on discrete tokens or continuous embeddings? If you remove the encoder entirely, can the decoder still process the input?

- **Concept: Teacher Forcing in Autoencoder Training**
  - Why needed here: The reconstruction task uses teacher forcing to train the decoder to reproduce original tokens from compressed representations; continuation also uses it for next-token prediction.
  - Quick check question: During pretraining, is the decoder conditioned on ground-truth tokens or its own prior predictions when computing the loss?

- **Concept: Frozen Decoder with Trainable Encoder**
  - Why needed here: ARC-Encoder's core constraint is not modifying the decoder; this preserves the decoder's general abilities and enables plug-and-play deployment.
  - Quick check question: If you observe degraded performance on tasks unrelated to compression, what component likely needs adjustment? (Answer: check if decoder was accidentally modified.)

## Architecture Onboarding

- **Component map:**
  - Input tokens -> Encoder backbone (Llama3.2 3B with last 2 layers removed, no causal mask) -> Query pooling in final attention layer -> MLP projector (2-layer, no activation) -> Compressed representations -> Frozen decoder

- **Critical path:**
  1. Tokenize input text with encoder tokenizer.
  2. Pass through encoder transformer (non-causal).
  3. Apply query pooling in final attention layer to reduce sequence length by pooling factor.
  4. Project pooled representations via MLP to decoder embedding dimension.
  5. Concatenate with any task-specific tokens or few-shot examples.
  6. Feed to frozen decoder for generation.

- **Design tradeoffs:**
  - Pooling factor: Higher (8×) improves speed but degrades accuracy; 4× is near-optimal for most tasks.
  - Encoder size: Truncating layers reduces parameters but hurts long-context tasks; full encoder best for performance.
  - Reconstruction ratio during pretraining: 20% is optimal; 0% or 100% both underperform.
  - Pretraining pooling factor: Pretraining at 8× and fine-tuning at 4× outperforms matching factors.

- **Failure signatures:**
  - Model regurgitates context verbatim instead of answering: Likely over-reliance on reconstruction task; increase continuation ratio.
  - Performance collapses on translation tasks: Pooling may be merging tokens across sentence boundaries; check pooling factor and whether contiguous pooling is appropriate.
  - Multi-decoder adapter fails: MLP bottleneck may be too small; increase hidden dimension or verify decoder embedding space compatibility.
  - Long-context task fails: Ensure chunks are processed in parallel and concatenated correctly; verify chunk size and number match training setup.

- **First 3 experiments:**
  1. Validate pooling mechanism: Train ARC-Encoder with pooling factor 4 on short sequences (≤256 tokens) using only reconstruction; verify near-perfect reconstruction on held-out samples.
  2. Test alternating pretraining: Compare three models—reconstruction-only, continuation-only, and alternating (20% reconstruction)—on a QA benchmark; confirm alternating performs best.
  3. Multi-decoder sanity check: Train shared encoder with two decoders (e.g., Mistral 7B and Llama3.1 8B) using separate MLPs; verify both achieve within 1 point of specialized encoders.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single encoder be trained to produce truly universal compressed representations that work seamlessly across decoder architectures with fundamentally different hidden state spaces (e.g., Transformer vs. Mamba, or models from different training paradigms)?
- Basis in paper: [explicit] Conclusion states: "This opens the way towards universal compressed representations."
- Why unresolved: The paper demonstrates multi-decoder training only for Llama3.1 8B and Mistral 7B, which share similar architectures and training pipelines. Adaptation to OLMo-7B required new projector training and showed larger gaps from open-book performance.
- What evidence would resolve it: Successful zero-shot or minimal-adaptation transfer of compressed representations to decoders with substantially different architectures (e.g., state-space models) without significant performance degradation.

### Open Question 2
- Question: What is the upper bound on compression ratio before task-relevant information is irrecoverably lost, and does this bound vary systematically across task types?
- Basis in paper: [explicit] Section 4.4 notes: "When pooling too much performance degrades sharply as with a pooling factor of 32