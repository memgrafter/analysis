---
ver: rpa2
title: 'Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in
  Trace-Based Knowledge Distillation'
arxiv_id: '2505.13792'
source_url: https://arxiv.org/abs/2505.13792
tags:
- traces
- intermediate
- final
- correct
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the disconnect between intermediate reasoning
  traces and final solution correctness in knowledge distillation for small language
  models. The authors propose a rule-based problem decomposition method for Open Book
  QA, breaking problems into classification and information retrieval steps to generate
  verifiable intermediate traces.
---

# Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2505.13792
- **Source URL**: https://arxiv.org/abs/2505.13792
- **Reference count**: 31
- **Primary result**: Correct reasoning traces do not guarantee correct final solutions in knowledge distillation for small language models, challenging assumptions about using intermediate reasoning traces for training

## Executive Summary
This paper investigates the fundamental assumption in knowledge distillation that correct intermediate reasoning traces lead to correct final solutions when training small language models. The authors propose a rule-based problem decomposition method for Open Book QA that breaks complex problems into verifiable classification and information retrieval steps. Through systematic experiments with Llama-3.2-1B-Instruct and Qwen3-1.7B models on three datasets, they find that while intermediate traces can be evaluated for correctness, there is little correlation between trace correctness and final solution accuracy. The study reveals that models can arrive at correct answers through reasoning that doesn't match the provided traces, suggesting that intermediate reasoning traces may not faithfully represent the actual reasoning process used by models.

## Method Summary
The authors employ a rule-based decomposition approach to transform Open Book QA problems into structured sub-problems consisting of classification and information retrieval steps. They create controlled training datasets with either correct or intentionally incorrect traces paired with correct final answers, then perform Supervised Fine-Tuning on small language models using these Input-Trace-Output tuples. The evaluation framework independently measures trace correctness (against sub-problem ground truth) and final solution correctness, using confusion matrices to reveal the disconnect between process and outcome. Experiments are conducted on Llama-3.2-1B-Instruct and Qwen3-1.7B models with QLoRA fine-tuning across three QA datasets.

## Key Results
- Correct traces do not guarantee correct final solutions: On CoTemp QA, models achieved only 52.88% final solution accuracy despite 47.06% classification step accuracy when trained with correct traces
- Models trained on incorrect traces can achieve comparable final solution accuracy to those trained on correct traces
- High rate of "False Positives" (correct answer with incorrect trace) reveals that models are not using provided traces as their actual reasoning process
- The disconnect between trace correctness and solution correctness challenges core assumptions about knowledge distillation using reasoning traces

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Problem Decomposition
The paper employs a rule-based decomposition method to transform complex, open-ended QA problems into structured, verifiable sub-problems, enabling objective evaluation of intermediate reasoning steps. A complex Open Book QA problem is decomposed into two discrete, verifiable steps: 1) a Classification step to identify the problem's type (e.g., a temporal relation like 'during'), and 2) an Information Retrieval (IR) step to isolate the specific facts from the context needed to answer the query. This generates a structured "intermediate trace" whose components can be automatically checked against ground truth, making trace correctness objectively measurable. The mechanism is brittle for problems that are ambiguous, require multi-hop reasoning blending categories, or rely on external knowledge not contained in the provided text.

### Mechanism 2: Supervised Fine-Tuning with Input-Trace-Output Tuples
The central mechanism for knowledge distillation is standard Supervised Fine-Tuning (SFT) where the training signal is expanded from just (input, output) pairs to (input, trace, output) tuples. A small language model (student) is trained to maximize the likelihood of the entire target sequence, which includes both the intermediate trace and the final answer. The key experimental manipulation is training separate models on Correct Traces (ground truth sub-problem steps) and Incorrect Traces (intentionally wrong steps), while always keeping the final answer correct. This tests the core assumption of knowledge distillation: that training a model to produce a correct reasoning trace will causally improve its ability to produce the correct final answer. The intended effect (improving final answer accuracy via correct traces) is shown to fail in the paper's results.

### Mechanism 3: Decoupled Evaluation of Process and Outcome
The paper introduces an evaluation framework that independently measures the correctness of the intermediate reasoning trace and the final solution, allowing for a granular analysis of their relationship. At inference, the model's output is parsed to separate the generated trace from the final answer. The trace is evaluated by checking its components (Classification Step, IR Step) against ground truth sub-problem solutions. The final answer is evaluated against the ground truth answer. This creates four distinct outcome categories: True Positives (correct trace & answer), True Negatives (wrong trace & answer), and crucially, False Positives and False Negatives, which reveal the disconnect. The evaluation relies on the assumption that the structured trace format is reliably generated and parsable.

## Foundational Learning

- **Knowledge Distillation (KD)**: The core technique being investigated. The paper situates itself within the KD paradigm, specifically focusing on distilling reasoning capabilities from a teacher's traces to a student model. Quick check: What is the fundamental goal of knowledge distillation when applied to language models?

- **Faithfulness of Reasoning Traces**: The entire paper is predicated on the challenge of evaluating whether a model's stated reasoning ("intermediate trace") is a truthful account of how it reached its conclusion. Understanding this concept is key to grasping the significance of the "disconnect" the authors find. Quick check: Why is it problematic if a model produces a correct answer but its provided "reasoning trace" is flawed or unrelated?

- **Supervised Fine-Tuning (SFT)**: This is the concrete training method used in the paper's experiments. A reader needs to understand SFT to follow the experimental setup ("SFT w/ Correct Traces," "SFT w/ Incorrect Traces") and the interpretation of the results. Quick check: In the context of this paper, what are the two key components of the target sequence used for SFT?

## Architecture Onboarding

- **Component map**: Problem Decomposer -> SFT Data Generator -> SLM Student -> Decoupled Evaluator

- **Critical path**: 1. Decomposition & Data Creation: A QA problem is broken down, and ground truth traces are generated. This is used to create the training datasets with controlled trace correctness. 2. Fine-Tuning: The SLM is trained on the (Input, Trace, Output) data. The loss function teaches it to produce both a reasoning trace and an answer. 3. Inference & Evaluation: The trained model generates outputs on a test set. The output is parsed, and trace/solution correctness is evaluated separately to populate a confusion matrix.

- **Design tradeoffs**: Interpretability vs. Generality (the rule-based decomposition creates highly interpretable and verifiable traces but is only applicable to a specific class of Open Book QA problems), Control vs. Realism (the paper generates synthetic "incorrect" traces to test causality, but this may not reflect real error distributions)

- **Failure signatures**: False Positives (High Rate): The model outputs the correct final answer but its generated intermediate trace is factually or logically wrong, indicating the model is not using the trace to derive the answer, breaking the core KD assumption. Trace-Length Discrepancy: Models trained on incorrect traces sometimes produced shorter traces, which could indicate the model is learning to shortcut the reasoning process.

- **First 3 experiments**: 1. Baseline Comparison: Run direct prompting and "SFT-Vanilla" (no traces) to establish baseline performance for the chosen SLMs on the three QA datasets. 2. SFT with Correct Traces: Fine-tune the SLMs using the dataset where all intermediate traces are derived from correct sub-problem solutions. Evaluate both trace and solution accuracy. 3. SFT with Incorrect Traces: Fine-tune a separate set of SLMs using the dataset where all intermediate traces are intentionally incorrect, but final answers are correct. Compare the final solution accuracy to the model trained on correct traces to test for a causal link.

## Open Questions the Paper Calls Out
None

## Limitations
- The rule-based decomposition approach is highly specialized to specific Open Book QA formats and may not extend to more complex or open-ended reasoning tasks
- The use of synthetic incorrect traces, though methodologically sound for establishing causality, may not accurately reflect the distribution of errors produced by actual reasoning models
- The evaluation framework assumes that the structured trace format is reliably generated and parsable, which may not hold for more flexible or less constrained model outputs

## Confidence
- **High Confidence**: The empirical finding that correct traces do not guarantee correct final solutions (as demonstrated through controlled SFT experiments across three datasets)
- **Medium Confidence**: The generalizability of the disconnect finding to other reasoning domains beyond the specific QA formats tested
- **Medium Confidence**: The proposed rule-based decomposition method as a solution for generating verifiable traces

## Next Checks
1. Test the disconnect finding with naturally occurring incorrect traces from a large reasoning model (like DeepSeek R1) rather than synthetically generated ones, to validate whether the disconnect persists with real error distributions.

2. Evaluate the framework on multi-hop reasoning tasks that require blending multiple reasoning steps or categories, to assess whether the rule-based decomposition can scale beyond the current dataset constraints.

3. Implement a robustness check on trace parsing by intentionally introducing format variations in model outputs and measuring the impact on the decoupled evaluation framework's reliability.