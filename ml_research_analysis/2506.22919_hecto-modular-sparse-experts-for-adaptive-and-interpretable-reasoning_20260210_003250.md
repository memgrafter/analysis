---
ver: rpa2
title: 'Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning'
arxiv_id: '2506.22919'
source_url: https://arxiv.org/abs/2506.22919
tags:
- expert
- routing
- ffnn
- hecto
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hecto introduces a novel MoE architecture that combines a GRU expert
  for temporal reasoning and an FFNN expert for static abstraction under a sparse
  Top-1 gating mechanism. The model is evaluated on three reasoning benchmarks (AG
  News, SST-2, HotpotQA) and a regression task (STS-B), achieving competitive performance
  while demonstrating clear expert specialization.
---

# Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning

## Quick Facts
- arXiv ID: 2506.22919
- Source URL: https://arxiv.org/abs/2506.22919
- Authors: Sanskar Pandey; Ruhaan Chopra; Saad Murtaza Bhat; Ark Abhyudaya
- Reference count: 40
- One-line primary result: Hecto achieves competitive performance on reasoning benchmarks while demonstrating clear expert specialization through architectural heterogeneity.

## Executive Summary
Hecto introduces a novel Mixture-of-Experts (MoE) architecture that combines a GRU expert for temporal reasoning with an FFNN expert for static abstraction under sparse Top-1 gating. The model achieves competitive performance across three reasoning benchmarks (AG News, SST-2, HotpotQA) and a regression task (STS-B) while maintaining strong interpretability through clear expert specialization. When trained with larger batch sizes, Hecto outperforms homogeneous baselines, demonstrating its scalability. The model maintains robust performance even with a frozen encoder, achieving 87.76% accuracy on AG News.

## Method Summary
Hecto uses a DistilBERT encoder (frozen or fine-tuned) with dual projection layers (768→256) that feed both a Top-1 gating network and two distinct experts. The gating network (MLP: 256→128→2) selects between a GRU expert (processing full token sequences) and an FFNN expert (operating on the [CLS] vector). Training employs entropy (λent=0.05) and diversity (λdiv=0.08) regularization to prevent routing collapse. The model uses straight-through sampling for Top-1 routing during training and argmax at inference. Training runs for 5 epochs with AdamW optimizer (lr=2e-5) and batch size 16, with performance improving at batch size 64.

## Key Results
- Hecto achieves 90.82% accuracy on AG News and 90.71% F1 on SST-2 when trained with batch size 64, outperforming homogeneous baselines
- The GRU expert is predominantly selected across tasks (79.9% on AG News, 73.3% on SST-2), indicating effective alignment with sequential reasoning demands
- With a frozen encoder, Hecto maintains 87.76% accuracy on AG News, showing only 2.26pp drop while exhibiting stronger expert skew (92.5% GRU usage)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Architectural heterogeneity between experts enables more meaningful specialization than homogeneous expert pools.
- Mechanism: A GRU expert processes full token sequences (inductive bias toward temporal/sequential reasoning), while an FFNN expert operates on the [CLS] vector (inductive bias toward static abstraction). The gating network learns to route inputs based on which computational structure better matches the input's reasoning demands.
- Core assumption: Different reasoning types (temporal vs. static) benefit from structurally distinct processing pathways, and the gating network can learn this alignment.
- Evidence anchors: Abstract states combining GRU for temporal reasoning and FFNN for static abstraction with experts aligning to distinct reasoning types. Section 3 explains FFNN operates on [CLS] for static reasoning while GRU processes full sequences for temporal dependencies. Related work (arXiv:2512.19765) examines optimal semantic specialization but focuses on homogeneous expert configurations.

### Mechanism 2
- Claim: Entropy and diversity regularization on the gating network prevents routing collapse while maintaining confident expert selection.
- Mechanism: The entropy penalty (λent = 0.05) encourages low-entropy (confident) gating decisions per sample. The diversity penalty (λdiv = 0.08) promotes balanced expert usage across batches by penalizing skewed gate mass distribution. Together, these produce interpretable, task-aligned routing without degenerating to single-expert collapse.
- Core assumption: Without explicit regularization, sparse Top-1 routing tends toward degenerate solutions where one expert receives nearly all inputs.
- Evidence anchors: Abstract mentions Top-1 gating with entropy and load-balancing penalties. Section 3.2/B.2 shows without regularization, entropy collapses from 0.40 bits to 0.02 bits with 99.7% routing to one expert. Limited direct corroboration in related papers (arXiv:2510.01185, arXiv:2512.20291) which address expert specialization through different mechanisms.

### Mechanism 3
- Claim: Larger batch sizes improve MoE optimization by providing more stable gradient estimates per expert and better routing diversity signals.
- Mechanism: With sparse Top-1 routing, each expert receives gradients only from the subset of samples routed to it. Larger batches increase the effective sample size per expert per update, reducing gradient variance and enabling more coherent specialization. The gating network also observes a broader input distribution per step.
- Core assumption: The benefits of architectural heterogeneity require sufficient gradient flow to both experts, which sparse routing inherently limits.
- Evidence anchors: Abstract states Hecto exhibits improved performance at larger batch sizes. Section 5.3/Appendix C shows at batch size 64, Hecto outperforms homogeneous baselines with 90.82% accuracy on AG News and 90.71% F1 on SST-2. Limited direct corroboration in corpus signals.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) with Sparse Gating**
  - Why needed here: Hecto's core contribution is a sparse MoE with heterogeneous experts. Understanding how gating networks select experts and how Top-k routing works is prerequisite to understanding why architectural diversity matters.
  - Quick check question: Can you explain why Top-1 routing is differentiable with straight-through estimation but not with naive argmax?

- Concept: **Inductive Biases in Neural Architectures**
  - Why needed here: The paper's central claim hinges on GRUs having temporal/sequential inductive bias while FFNNs have static/pattern-matching bias. Without understanding why these architectures prefer different input structures, the routing behavior appears arbitrary.
  - Quick check question: Why would a GRU potentially outperform an FFNN on a task requiring order-sensitive reasoning?

- Concept: **Regularization for Routing Networks**
  - Why needed here: The entropy and diversity penalties are non-negotiable for stable Hecto training. Ablation B.2 shows that removing them causes immediate routing collapse.
  - Quick check question: If your gating network routes 99.9% of samples to one expert, which regularizer (entropy or diversity) would most directly address this?

## Architecture Onboarding

- Component map:
  Input Text → DistilBERT Encoder (768-dim) → Dual Projection Layer → Gate Network (MLP: 256→128→2) → Expert selection → FFNN Expert (256→128→C) OR GRU Expert (256→128→C) → Classification Logits

- Critical path:
  1. Encoder produces token embeddings → CLS vector and full sequence
  2. Gate network computes softmax probabilities over experts
  3. Straight-through sampling selects one expert during training; argmax at inference
  4. Selected expert processes its input representation → classification logits
  5. Loss = Cross-entropy + λent×entropy_penalty + λdiv×diversity_penalty

- Design tradeoffs:
  - Top-1 vs Top-2 routing: Top-1 preserves single-expert interpretability; Top-2 adds +0.58pp accuracy but "effectively degenerates into 'always run the GRU'" (Appendix B.1)
  - Frozen vs fine-tuned encoder: Frozen maintains 87.76% accuracy (2.26pp drop) with stronger expert skew (92.5% GRU); fine-tuned achieves 90.02% with more balanced routing
  - 2 experts vs 4 experts: Doubling experts decreases accuracy by ~2.5pp and causes complete routing collapse (Appendix B.3)

- Failure signatures:
  - Routing collapse: One expert receives >99% of inputs → check diversity penalty weight
  - Low-confidence gating: High gate entropy (>0.5 bits) → entropy penalty may be insufficient
  - Underperforming homogeneous baselines: Verify batch size ≥64; Hecto underperforms at batch=16 but exceeds baselines at batch=64

- First 3 experiments:
  1. Baseline replication: Train Hecto (FFNN+GRU) on AG News with batch=16, λent=0.05, λdiv=0.08. Expect ~90% accuracy, ~80% GRU usage. Verify routing doesn't collapse.
  2. Ablation: Remove regularization: Same setup with λent=0, λdiv=0. Confirm routing collapses to single expert (99.7% to one expert per Appendix B.2). This validates the regularization mechanism.
  3. Scaling test: Increase batch size to 64. Expect accuracy improvement to ~90.8%, surpassing homogeneous baselines. This validates the batch-size scaling mechanism.

## Open Questions the Paper Calls Out

- Question: Does the Hecto framework generalize to generative tasks, multilingual settings, and domain-shifted scenarios?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section (Section 7) that the "current scope remains limited to supervised, English-language datasets" and that "Future work should expand toward generative tasks, multilingual settings, and domain-shifted scenarios."
- Why unresolved: The current study only evaluates the heterogeneous MoE on classification (AG News, SST-2), QA (HotpotQA), and regression (STS-B), leaving its efficacy on generation or cross-lingual transfer untested.
- What evidence would resolve it: Evaluation of Hecto-based architectures on generative benchmarks (e.g., summarization) or multilingual datasets to observe if the static/temporal expert dichotomy holds.

- Question: Can finer-grained interpretability tools (e.g., token saliency) quantify why specific inputs are routed to a particular expert?
- Basis in paper: [explicit] Section 7 notes "Limited Granular Interpretability," stating that while high-level usage trends are visible, "quantifying why a specific expert was chosen for a given input remains challenging."
- Why unresolved: The current analysis relies on aggregate usage distributions (e.g., 79.9% GRU usage), but lacks mechanisms to explain the causal features driving individual routing decisions.
- What evidence would resolve it: The integration of token-level attribution methods or gate introspection techniques that correlate specific input features with the gating network's probability output.

- Question: Is the Hecto architecture robust and efficient when deployed on CPU or mobile hardware?
- Basis in paper: [explicit] The authors acknowledge in Section 7 that "Latency results are reported only on a single T4 GPU" and that "further profiling on CPUs and mobile hardware is necessary to assess real-world deployability."
- Why unresolved: While the model is designed for "low-resource regimes," the empirical evidence for efficiency is currently restricted to a specific GPU environment (Google Colab T4).
- What evidence would resolve it: Benchmarking inference latency and energy consumption on edge devices or CPU-only servers to validate the claim of modular, efficient inference outside of GPU-accelerated environments.

## Limitations
- The framework is currently limited to supervised English-language datasets, with generalization to generative tasks, multilingual settings, and domain-shifted scenarios untested.
- Interpretability is limited to aggregate expert usage patterns, lacking mechanisms to explain individual routing decisions at the token or feature level.
- Empirical efficiency claims are based solely on T4 GPU benchmarks, with no validation on CPU or mobile hardware for real-world deployment assessment.

## Confidence

- **High Confidence**: The architectural framework (GRU + FFNN experts with Top-1 gating) is correctly implemented and achieves stated benchmark results. The routing collapse without regularization is reproducible and well-documented.
- **Medium Confidence**: The interpretability claims about expert specialization (GRU for temporal reasoning, FFNN for static abstraction) are supported by usage patterns but rely on post-hoc analysis. The generalization to diverse tasks is demonstrated but not systematically explored across different reasoning types.
- **Low Confidence**: The claim that architectural heterogeneity is the primary factor enabling specialization versus other factors like optimization dynamics, regularization schedules, or dataset characteristics.

## Next Checks

1. **Ablation on Expert Architectural Choices**: Replace the GRU expert with a Transformer block or another sequential architecture while keeping the FFNN expert constant. This would isolate whether the temporal reasoning capability specifically requires GRU architecture versus any architecture with sequential processing capability.

2. **Task Diversity and Reasoning Type Analysis**: Apply Hecto to a benchmark specifically designed to separate temporal vs static reasoning demands (e.g., synthetic tasks where input order manipulation affects performance). This would provide stronger evidence that expert routing aligns with the claimed reasoning specializations.

3. **Robustness to Regularization Weights**: Conduct a systematic grid search over λent and λdiv values across different batch sizes and tasks. This would establish whether the current regularization settings are optimal or task-specific, and quantify the sensitivity of routing behavior to these hyperparameters.