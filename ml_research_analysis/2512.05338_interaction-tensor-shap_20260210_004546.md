---
ver: rpa2
title: Interaction Tensor SHAP
arxiv_id: '2512.05338'
source_url: https://arxiv.org/abs/2512.05338
tags:
- tensor
- representation
- interaction
- structure
- stii
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IT-SHAP, a tensor-algebraic formulation of
  the Shapley-Taylor Interaction Index (STII) that enables exact computation of higher-order
  interactions in machine learning models. The core idea is to represent STII as a
  linear transformation on a value tensor and derive an explicit algebraic representation
  of its weight tensor using discrete finite difference operators.
---

# Interaction Tensor SHAP

## Quick Facts
- arXiv ID: 2512.05338
- Source URL: https://arxiv.org/abs/2512.05338
- Reference count: 20
- This paper proposes IT-SHAP, a tensor-algebraic formulation of the Shapley-Taylor Interaction Index (STII) that enables exact computation of higher-order interactions in machine learning models.

## Executive Summary
This paper reformulates the Shapley-Taylor Interaction Index (STII) as a linear transformation on tensor representations, enabling efficient computation of higher-order feature interactions in machine learning models. By expressing STII as a contraction between a value tensor and a weight tensor with explicit Tensor Train (TT) structure, the authors demonstrate that interaction indices can be computed in parallel complexity class NC² when the value function admits a TT representation. The key insight is that the combinatorial weights of STII are induced by discrete finite difference operators that preserve TT structure, separating the computational difficulty from the interaction index definition itself. This work establishes both tractability (under TT assumptions) and intractability (#P-hardness) for general cases, showing that computational difficulty is determined by tensor representation rather than the interaction index.

## Method Summary
The method reformulates STII as a tensor contraction between a value tensor V(M,P) and a weight tensor fW^(k), where the weight tensor is constructed using discrete finite difference operators δ_i that preserve Tensor Train structure. The value tensor represents the model M and background P on the Boolean hypercube {0,1}^n_in. The key computational step involves contracting these TT-structured tensors using a parallel algorithm with O(log² n_in) depth. The approach assumes the value tensor is available in TT form and proves that the weight tensor construction maintains this structure through rank-1 operator applications.

## Key Results
- STII weight tensor admits exact TT representation due to structure-preserving discrete difference operators
- Under TT representation, STII computation achieves parallel complexity class NC²
- Without TT structure assumptions, STII computation is proven #P-hard
- The computational difficulty of higher-order interactions is determined by tensor representation, not the index definition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The computational difficulty of higher-order interaction indices (STII) is determined by the underlying tensor representation rather than the index definition itself.
- **Mechanism:** The paper reformulates the Shapley-Taylor Interaction Index (STII) as a linear transformation (contraction) between a value tensor and a weight tensor. By expressing the value function as a tensor over {0,1}^n_in, the interaction computation reduces to a linear algebra operation, separating the combinatorial weights from the model evaluation.
- **Core assumption:** The value function can be embedded into a tensor space where operations are well-defined linear maps.
- **Evidence anchors:**
  - [abstract] "reformulate STII as a linear transformation acting on a value function and derive an exact algebraic representation"
  - [section 4.2] "STII can be expressed as a contraction between a signed weight tensor and a value tensor"
  - [corpus] "SHAP Meets Tensor Networks" validates that SHAP computation reduces to tensor contraction.

### Mechanism 2
- **Claim:** The STII weight tensor admits an exact Tensor Train (TT) representation because discrete difference operators preserve TT structure.
- **Mechanism:** The STII relies on higher-order discrete difference operators δ_S. The paper shows these operators are products of rank-1, mode-wise linear maps (δ_i). When applied to a base tensor in TT form, these operators update only local TT cores without altering the chain connectivity, ensuring the weight tensor remains in TT format.
- **Core assumption:** Discrete difference operators effectively act as local linear maps that do not couple non-adjacent tensor modes.
- **Evidence anchors:**
  - [abstract] "weight tensor is shown to possess a multilinear structure induced by discrete finite difference operators"
  - [section 4.4.2] "The action of δ_i affects only the TT core corresponding to mode i and preserves the TT structure."
  - [corpus] "TT-FSI" supports the applicability of Tensor-Train to interaction indices.

### Mechanism 3
- **Claim:** Under the TT representation, STII computation moves from intractable (#P-hard) to the parallel complexity class NC².
- **Mechanism:** Because both the value tensor (by assumption) and the weight tensor (by Mechanism 2) are in TT form, their contraction reduces to a sequence of matrix multiplications along the chain. This allows evaluation by parallel circuits of polylogarithmic depth (O(log² n_in)).
- **Core assumption:** The value tensor V(M,P) is available in TT form with manageable ranks.
- **Evidence anchors:**
  - [abstract] "higher order interaction indices can be computed in the parallel complexity class NC squared"
  - [section 4.3.2] "computation of IT-SHAP can be implemented as a contraction of TT tensors and thus admits a parallel algorithm"
  - [corpus] Corpus neighbor "Tractable Shapley Values..." supports replacing coalition enumeration with tensor network evaluation.

## Foundational Learning

- **Concept: Shapley-Taylor Interaction Index (STII)**
  - **Why needed here:** This is the target functional the paper aims to compute. It extends Shapley values (1st order) to quantify interactions among subsets of features (higher-order).
  - **Quick check question:** How does STII differ from standard Shapley values when analyzing a feature pair {i, j}?

- **Concept: Tensor Train (TT) Representation**
  - **Why needed here:** The core enabler of tractability. It decomposes a high-order tensor into a chain of 3rd-order cores, avoiding exponential memory storage.
  - **Quick check question:** If a tensor has order d and TT-ranks r, what is the storage complexity?

- **Concept: Discrete Finite Difference Operators**
  - **Why needed here:** These operators define the interaction logic (marginal contributions). The paper proves they are structurally compatible with TT decomposition.
  - **Quick check question:** How does the rank-1 property of the operator δ_i ensure the TT structure is preserved during contraction?

## Architecture Onboarding

- **Component map:** Value Tensor (V) -> Weight Tensor (fW) -> TT-Engine -> Interaction Components
- **Critical path:** Constructing the Value Tensor in TT format. While the paper proves the Weight Tensor is TT-structured, the efficiency gains only materialize if the input model M admits a low-rank TT approximation.
- **Design tradeoffs:**
  - **Exactness vs. Scalability:** The theory is exact for TT inputs. For general models, you must approximate M as TT, introducing error but gaining NC² complexity.
  - **Processor count vs. Interaction order:** While depth is polylogarithmic, the number of processors scales with the output size of interaction terms (Θ(n^k_in)).
- **Failure signatures:**
  - **Rank Explosion:** If the value tensor requires exponential TT-ranks to represent accurately, the "tractable" claim collapses.
  - **Structural Mismatch:** If using Tucker or CP decompositions instead of TT, the preservation of discrete difference operators fails (as noted in Section 4.4.3).
- **First 3 experiments:**
  1. **Validation of Weight Tensor Structure:** Implement the construction of fW^(k) for small n and verify its TT-ranks match the theoretical bounds derived from discrete difference operators.
  2. **Synthetic Tractability Test:** Compare the runtime of IT-SHAP (using TT contraction) vs. naive STII enumeration on a known low-rank TT model (e.g., a polynomial function). Plot scaling against n_in.
  3. **Hardness Verification:** Attempt to compute IT-SHAP on a generic (non-TT) tensor network or dense tensor representation and confirm the computational bottleneck matches the #P-hard prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can randomized or sketching-based tensor decomposition techniques be integrated with IT-SHAP to maintain computational tractability when exact Tensor Train (TT) ranks grow prohibitively large?
- **Basis in paper:** [explicit] The Discussion section states that integrating approximate techniques like TensorSketch or randomized SVD "remains an important avenue for future work" to address the computational bottleneck of rank growth.
- **Why unresolved:** The paper establishes theoretical tractability (NC^2) assuming an exact TT representation but acknowledges that TT ranks may grow exponentially in worst-case scenarios, necessitating approximate methods not covered by the current proofs.
- **What evidence would resolve it:** A modified algorithm or theoretical bound demonstrating that an approximate TT construction yields a polynomial-time approximation scheme for IT-SHAP with rigorously bounded error rates.

### Open Question 2
- **Question:** Which specific classes of machine learning models or value functions guarantee that the Modified Weighted Coalitional Tensor admits a TT representation with ranks bounded independently of the input dimension?
- **Basis in paper:** [inferred] The authors prove the TT structure exists for all models but note in the Discussion that "TT ranks may grow exponentially... in worst-case scenarios," implying the need to identify when this does not happen.
- **Why unresolved:** While the existence of the TT representation is proven generally, the paper does not characterize the specific structural properties of a model (e.g., additive vs. multiplicative interactions) that result in low-rank tensors suitable for efficient computation.
- **What evidence would resolve it:** A theoretical characterization linking specific model architectures (such as ensembles of low-depth trees) to constant or logarithmic bounds on the TT ranks of the STII weight tensor.

### Open Question 3
- **Question:** Does the computational overhead of converting a general high-dimensional model into the required TT input format negate the parallel efficiency gains achieved during the IT-SHAP evaluation phase?
- **Basis in paper:** [inferred] The Discussion notes that practical application requires "careful consideration of the preprocessing stage," and the complexity results (NC^2) rely strictly on the input already being in TT form.
- **Why unresolved:** The paper decouples the representation problem from the evaluation problem; it proves the evaluation is fast but does not quantify the computational cost of the necessary "tensorization" or TT-decomposition step for arbitrary models.
- **What evidence would resolve it:** An end-to-end complexity analysis or empirical benchmark comparing the total time (preprocessing + parallel evaluation) against sampling-based baselines like SHAP-IQ for standard benchmarks.

## Limitations

- The paper assumes the value tensor V(M,P) is available in TT form without providing conversion methods for arbitrary ML models
- TT rank growth may be prohibitive for complex models, limiting practical applicability
- The polynomial processor count requirement may still be substantial for high-order interactions

## Confidence

- **High Confidence:** The algebraic reformulation of STII as tensor contraction and the proof of TT structure for the weight tensor
- **Medium Confidence:** The NC² parallel complexity claim for TT-represented value tensors, pending empirical validation on real models
- **Low Confidence:** The practical applicability without additional structure, as the paper does not address TT-rank bounds or conversion procedures for arbitrary ML models

## Next Checks

1. **TT Conversion Protocol:** Develop and test a systematic procedure for converting a given ML model M into its TT-represented value tensor V(M,P), measuring rank growth as a function of model complexity
2. **Rank Bound Characterization:** Establish empirical bounds on TT-ranks for fW^(k) and V(M,P) across different model families (e.g., polynomials, neural networks, decision trees)
3. **Parallel Implementation Benchmark:** Implement the NC² algorithm and measure actual parallel depth and processor utilization against theoretical bounds, comparing with baseline enumeration methods on problems of increasing scale