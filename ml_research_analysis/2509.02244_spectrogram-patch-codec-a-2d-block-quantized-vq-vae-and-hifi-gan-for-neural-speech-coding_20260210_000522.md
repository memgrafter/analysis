---
ver: rpa2
title: 'Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural
  Speech Coding'
arxiv_id: '2509.02244'
source_url: https://arxiv.org/abs/2509.02244
tags:
- codec
- speech
- audio
- quality
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neural speech codec that replaces complex
  residual vector quantization with a single-stage 2D block quantization approach.
  Instead of RVQ stacks, it operates directly on mel-spectrograms, quantizing non-overlapping
  4x4 patches into a shared codebook to produce a discrete token grid.
---

# Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding

## Quick Facts
- **arXiv ID**: 2509.02244
- **Source URL**: https://arxiv.org/abs/2509.02244
- **Authors**: Luis Felipe Chary; Miguel Arjona Ramirez
- **Reference count**: 19
- **Primary result**: Simplified neural speech codec using 2D block quantization achieves STOI 0.844 and PESQ 2.70 at 7.5 kbits/s

## Executive Summary
This paper introduces a neural speech codec that replaces complex residual vector quantization with a single-stage 2D block quantization approach. Instead of RVQ stacks, it operates directly on mel-spectrograms, quantizing non-overlapping 4x4 patches into a shared codebook to produce a discrete token grid. A late adversarial fine-tuning stage is combined with a HiFi-GAN vocoder trained from scratch on the codec's reconstructions. At approximately 7.5 kbits/s for 16 kHz speech, the system is evaluated against state-of-the-art codecs using objective metrics. Results show competitive perceptual quality and intelligibility, with STOI of 0.844 and PESQ of 2.70, validating this simplified architecture as an effective and open foundation for future low-latency codec designs.

## Method Summary
The Spectrogram Patch Codec replaces traditional residual vector quantization with a single-stage 2D block quantization system that directly processes mel-spectrograms. The encoder generates non-overlapping 4x4 patches from the mel-spectrogram, which are quantized into a shared codebook to produce a discrete token grid. This approach simplifies the quantization process while maintaining representational capacity. A late adversarial fine-tuning stage is applied to improve reconstruction quality, followed by a HiFi-GAN vocoder that is trained from scratch on the codec's outputs. The entire system operates at approximately 7.5 kbits/s for 16 kHz speech, using objective metrics for evaluation against existing state-of-the-art codecs.

## Key Results
- Achieves STOI score of 0.844 at 7.5 kbits/s for 16 kHz speech
- Achieves PESQ score of 2.70 at 7.5 kbits/s for 16 kHz speech
- Demonstrates competitive perceptual quality and intelligibility compared to state-of-the-art codecs

## Why This Works (Mechanism)
The simplified architecture replaces complex residual vector quantization stacks with direct 2D block quantization of mel-spectrogram patches. By quantizing non-overlapping 4x4 patches into a shared codebook, the system reduces computational complexity while maintaining representational capacity. The late adversarial fine-tuning stage improves reconstruction quality by refining the quantized representations. Training the HiFi-GAN vocoder from scratch on the codec's reconstructions ensures optimal compatibility between the encoder and decoder components, resulting in high-quality speech synthesis at low bitrates.

## Foundational Learning

**VQ-VAE (Vector Quantized Variational Autoencoder)**: Why needed - provides discrete latent representations for compression; Quick check - verify codebook learning stability and codebook usage distribution.

**Mel-spectrogram processing**: Why needed - efficient time-frequency representation of speech signals; Quick check - validate mel filter bank design and normalization parameters.

**HiFi-GAN vocoder**: Why needed - high-quality neural waveform synthesis from spectrograms; Quick check - confirm training convergence and generation latency on target hardware.

**Adversarial fine-tuning**: Why needed - improves reconstruction quality by optimizing perceptual metrics; Quick check - monitor training stability and mode collapse risks.

**2D block quantization**: Why needed - simplifies quantization while maintaining spatial structure; Quick check - evaluate patch size sensitivity and codebook capacity requirements.

## Architecture Onboarding

**Component map**: Mel-spectrogram -> 2D Patch Encoder -> Shared Codebook -> Discrete Token Grid -> HiFi-GAN Vocoder -> Speech waveform

**Critical path**: The most critical path runs from the mel-spectrogram input through the 2D patch encoder to the shared codebook, as quantization quality directly determines reconstruction fidelity. The HiFi-GAN vocoder is also critical since it generates the final speech output.

**Design tradeoffs**: Single-stage quantization vs. residual quantization complexity, shared codebook size vs. representational capacity, late adversarial fine-tuning vs. training stability, and HiFi-GAN optimization for codec-specific characteristics.

**Failure signatures**: Poor codebook usage indicating insufficient codebook capacity, high reconstruction error suggesting quantization artifacts, vocoder instability indicating mismatched training data, and adversarial training divergence suggesting instability in fine-tuning stage.

**3 first experiments**:
1. Test quantization performance with different patch sizes (2x2, 4x4, 8x8) to find optimal balance between compression and quality
2. Evaluate codebook capacity requirements by varying codebook size and measuring reconstruction quality
3. Compare late adversarial fine-tuning against standard VQ-VAE training to quantify perceptual improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks subjective listening tests, which are critical for assessing speech codec quality given the well-documented gap between objective metrics and perceived quality
- The late adversarial fine-tuning stage introduces potential instability risks that are not thoroughly characterized
- Computational complexity and real-time processing requirements are not reported, making it difficult to assess practical deployment feasibility

## Confidence

**Objective metric performance claims**: Medium confidence (results appear consistent with reported methodology but lack subjective validation)

**Architectural simplification benefits**: Medium confidence (theoretical advantages demonstrated, but practical trade-offs not fully explored)

**Competitive quality claims**: Low confidence (objective metrics only, no perceptual validation)

## Next Checks

1. Conduct MUSHRA or ITU-T P.800 subjective listening tests across multiple languages and acoustic conditions to validate objective metric findings

2. Characterize real-time processing latency and computational complexity, including model size and inference speed on edge devices

3. Perform ablation studies on the adversarial fine-tuning stage to quantify its contribution and stability across different training runs