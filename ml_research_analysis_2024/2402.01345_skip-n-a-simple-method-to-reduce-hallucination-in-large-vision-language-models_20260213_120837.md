---
ver: rpa2
title: 'Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language
  Models'
arxiv_id: '2402.01345'
source_url: https://arxiv.org/abs/2402.01345
tags:
- arxiv
- lvlms
- hallucinations
- preprint
- greedy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a semantic shift bias in large vision-language
  models (LVLMs) that occurs after paragraph breaks (\"\n\n\"), leading to increased
  hallucinations. The authors propose a simple method called Skip \n that prevents
  LVLMs from generating paragraph breaks, thereby mitigating this bias.
---

# Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2402.01345
- **Source URL**: https://arxiv.org/abs/2402.01345
- **Reference count**: 7
- **Primary result**: Skip \n reduces hallucinations by preventing paragraph breaks that trigger semantic shift bias

## Executive Summary
This paper identifies a semantic shift bias in large vision-language models (LVLMs) that occurs after paragraph breaks (`\n\n`), leading to increased hallucinations. The authors propose a simple method called Skip \n that prevents LVLMs from generating paragraph breaks, thereby mitigating this bias. The method modifies the prompt to encourage single-paragraph descriptions (MiHI) and adjusts the decoding strategy to avoid outputting `\n` tokens (MiHO). Experiments on multiple LVLMs show that Skip \n significantly reduces hallucinations compared to original outputs, with hallucination severity (Cs) decreasing by up to 26% and hallucination rate (Ci) decreasing by up to 29% across different models and decoding strategies.

## Method Summary
The Skip \n method consists of two orthogonal components: MiHI (Modify Input Hint) and MiHO (Modify Inference Hyperparameters). MiHI modifies the input prompt by adding instructions like "in one paragraph" to encourage single-paragraph output. MiHO adjusts the decoding process by applying an infinite penalty to `\n` tokens, effectively preventing their generation. These methods work by preventing the model from encountering paragraph breaks that trigger a learned semantic shift bias from training data, where content before and after `\n\n` frequently exhibits significant semantic changes.

## Key Results
- Hallucination severity (Cs) decreased by up to 26% compared to original outputs
- Hallucination rate (Ci) decreased by up to 29% across different models
- The method works with both greedy and sampling decoding strategies
- MiHI and MiHO show orthogonal effects and can be combined for additive benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic shift bias in LVLMs causes hallucinations after paragraph breaks
- Mechanism: The training data shows that content before and after `\n\n` tokens frequently has significant semantic changes. LVLMs learn this pattern and infer that content after `\n\n` should be "obviously different" from preceding content, leading to less hallucinatory descriptions in the first part and more hallucinatory descriptions in the second part.
- Core assumption: The model's training data contains systematic semantic shifts at paragraph breaks that the model learns as a pattern
- Evidence anchors:
  - [abstract] "Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('\n\n'), where the content before and after '\n\n' in the training data frequently exhibit significant semantic changes."
  - [section 2] "We identify a special semantic shift bias triggered by paragraph breaks, where training data often show significant semantic changes before and after '\n\n'. This leads to a tendency for LVLMs to deviate from the previous non-hallucinatory description after '\n\n', resulting in hallucinations."
  - [corpus] Weak - related papers focus on object hallucination but don't explicitly discuss semantic shift bias at paragraph breaks
- Break condition: When the model encounters `\n\n` during generation, it activates the learned pattern to shift semantic content

### Mechanism 2
- Claim: Deliberately inserting `\n\n` tokens induces more hallucinations
- Mechanism: When `\n\n` is manually inserted at specific positions (like after the k-th period), it triggers the semantic shift bias, causing the model to generate more hallucinatory content after the break.
- Core assumption: The model's learned association between `\n\n` and semantic shifts is strong enough to override visual grounding when explicitly triggered
- Evidence anchors:
  - [section 3] "we explore the use of '\n\n' as a method to induce hallucinations in existing LVLMs. We find that inserting '\n\n' in generated sentences significantly increases the probability of hallucinations"
  - [section 3] "when the sentence outputs the period ('\n') token for the k-th time, we manually insert the '\n\n' to initiate the attack"
  - [corpus] Weak - related papers discuss hallucination mitigation but not induced hallucination via paragraph breaks
- Break condition: When `\n\n` is inserted during generation, it triggers the semantic shift pattern regardless of visual input

### Mechanism 3
- Claim: Skipping `\n` tokens prevents semantic shift bias activation
- Mechanism: By modifying the prompt to encourage single-paragraph descriptions (MiHI) or adjusting decoding to avoid `\n` tokens (MiHO), the model never encounters the semantic shift trigger, maintaining continuity and coherence in the generated text.
- Core assumption: Preventing `\n` output is sufficient to maintain the model's focus on visual content without triggering learned semantic shift patterns
- Evidence anchors:
  - [section 2] "Our proposed method aims to reduce hallucinations by preventing the model from generating paragraph breaks ('\n\n'). Therefore, we can mitigate the semantic shift bias in LVLMs"
  - [section 2] "This modification emphasizes the generation of a single, continuous paragraph, thereby avoiding the output of paragraph breaks '\n\n'"
  - [section 2] "we can avoid the output of '\n' by reducing the logits corresponding to the '\n' token"
  - [corpus] Weak - related papers discuss hallucination mitigation but not specifically through paragraph break prevention
- Break condition: When the model completes generation without producing `\n` tokens, the semantic shift bias is never triggered

## Foundational Learning

- Concept: Semantic shift bias in language models
  - Why needed here: Understanding how models learn and apply patterns from training data to generation is crucial for grasping why paragraph breaks trigger hallucinations
  - Quick check question: How does a model's exposure to paragraph breaks in training data create a bias that affects generation behavior?

- Concept: Multimodal hallucination in vision-language models
  - Why needed here: This work specifically addresses object hallucinations where models describe non-existent objects, requiring understanding of the hallucination problem in LVLMs
  - Quick check question: What distinguishes object hallucination from other types of hallucinations in vision-language models?

- Concept: Decoding strategies and their impact on generation
  - Why needed here: The MiHO method modifies decoding logits to prevent `\n` output, requiring understanding of how decoding affects token generation
  - Quick check question: How does adjusting token logits during decoding influence the probability of specific tokens being generated?

## Architecture Onboarding

- Component map:
  - Vision encoder: Processes input images
  - Language model: Generates text descriptions
  - Semantic shift bias detector: Implicit component that identifies paragraph break patterns
  - MiHI module: Modifies input prompts
  - MiHO module: Adjusts decoding logits
  - Hallucination evaluator: Uses CHAIR framework to measure Cs and Ci metrics

- Critical path: Image → Vision encoder → Language model → Text output
  - MiHI modifies input before vision encoder
  - MiHO modifies output during decoding
  - Both aim to prevent semantic shift bias activation

- Design tradeoffs:
  - MiHI requires prompt engineering that may affect model performance on other tasks
  - MiHO requires hyperparameter tuning (λ value) and may affect natural text flow
  - Both methods add minimal computational overhead compared to retraining approaches
  - Neither method addresses the root cause of semantic shift bias in training data

- Failure signatures:
  - MiHI fails if the model doesn't understand instruction modifications (e.g., Fuyu-8B)
  - MiHO fails if λ is set too low (allowing `\n` generation) or too high (distorting other token probabilities)
  - Both methods may fail if the semantic shift bias is deeply embedded in model weights

- First 3 experiments:
  1. Verify semantic shift bias by comparing hallucination rates before and after `\n\n` in model outputs
  2. Test MiHO effectiveness by adjusting λ values and measuring hallucination reduction
  3. Compare MiHI and MiHO separately and combined to identify which provides better performance across different models

## Open Questions the Paper Calls Out
- What specific characteristics of the training data cause the semantic shift bias related to paragraph breaks in LVLMs?
- Does the scale of the LVLM model impact the severity of the `\n\n`-induced hallucination bias?
- Are there other special tokens or formatting elements in training data that could induce similar semantic shift biases in LVLMs?
- How does the position of `\n\n` insertion in generated text affect the severity of hallucinations?
- Can the Skip `\n` method be extended to other types of multimodal hallucinations beyond object hallucinations?

## Limitations
- The paper relies on indirect evidence for the semantic shift bias hypothesis rather than direct analysis of training corpora
- Effectiveness varies significantly across different LVLMs, with some models showing minimal improvement
- The method doesn't address the root cause of semantic shift bias in training data
- Results are limited to object hallucinations and may not generalize to other hallucination types

## Confidence
- **High Confidence**: The empirical observation that hallucinations increase after paragraph breaks (`\n\n`) is well-supported by the experimental results with consistent metrics across multiple models.
- **Medium Confidence**: The explanation that semantic shift bias in training data causes the `\n\n`-triggered hallucinations is plausible but not definitively proven.
- **Low Confidence**: The claim that MiHI and MiHO are "orthogonal" components that can be combined for additive benefits needs more rigorous validation.

## Next Checks
1. Conduct systematic analysis of training corpora to verify whether semantic shift patterns actually exist at paragraph breaks
2. Perform ablation studies on prompt modifications to determine if specific phrasing is necessary for MiHI's effectiveness
3. Evaluate Skip `\n` on datasets beyond MSCOCO to test cross-domain generalization of the method