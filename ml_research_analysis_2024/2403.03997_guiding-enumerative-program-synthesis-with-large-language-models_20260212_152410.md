---
ver: rpa2
title: Guiding Enumerative Program Synthesis with Large Language Models
arxiv_id: '2403.03997'
source_url: https://arxiv.org/abs/2403.03997
tags:
- synthesis
- prog
- program
- grammar
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the ability of large language models (LLMs)
  to solve formal program synthesis problems with precise logical specifications.
  The authors find that standalone LLMs like GPT-3.5 perform poorly on these benchmarks,
  easily outperformed by state-of-the-art formal synthesis algorithms.
---

# Guiding Enumerative Program Synthesis with Large Language Models

## Quick Facts
- arXiv ID: 2403.03997
- Source URL: https://arxiv.org/abs/2403.03997
- Authors: Yixuan Li; Julian Parsert; Elizabeth Polgreen
- Reference count: 40
- Primary result: LLM-guided enumerative synthesis significantly outperforms standalone LLMs and baseline enumerators, achieving performance on par with state-of-the-art solver cvc5.

## Executive Summary
This paper evaluates large language models (LLMs) for formal program synthesis with precise logical specifications, finding that standalone LLMs like GPT-3.5 perform poorly compared to formal synthesis algorithms. To address this limitation, the authors propose two novel methods that integrate LLMs into enumerative synthesis algorithms. The first method constructs a probabilistic context-free grammar (pCFG) from LLM-generated solutions to guide the search, while the second method, iLLM-synth, integrates the LLM into the enumerator to provide real-time syntactic feedback and helper functions. Both approaches significantly outperform standalone LLMs and baseline enumerative synthesizers, with iLLM-synth achieving performance comparable to the state-of-the-art solver cvc5.

## Method Summary
The authors propose two approaches to integrate LLMs into enumerative program synthesis. The first approach, pCFG-synth, prompts an LLM with the synthesis problem and constructs a weighted probabilistic context-free grammar (pCFG) from the generated solutions, using this grammar to guide an enumerative search. The second approach, iLLM-synth, integrates the LLM directly into the enumerative synthesizer, allowing it to provide syntactic feedback and helper functions during the search process. Both methods are evaluated on SyGuS competition benchmarks using GPT-3.5-turbo, with results showing significant performance improvements over standalone LLMs and baseline enumerative synthesizers.

## Key Results
- Standalone LLMs like GPT-3.5 perform poorly on formal synthesis benchmarks compared to state-of-the-art solvers
- pCFG-synth significantly outperforms standalone LLMs by using LLM-generated solutions to guide the search
- iLLM-synth achieves performance on par with the state-of-the-art solver cvc5 by integrating the LLM into the enumerator
- Both LLM-guided approaches significantly outperform baseline enumerative synthesizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLM-generated programs to infer a weighted probabilistic context-free grammar (pCFG) guides the enumerator toward the solution space more efficiently.
- Mechanism: The LLM is prompted to solve the synthesis problem. Even incorrect solutions are used to build a weighted grammar where rule weights are proportional to their frequency in the derivations of LLM-generated programs. The enumerator then uses this pCFG to bias its search toward likely correct programs.
- Core assumption: The correct solution is often in the "neighborhood" of incorrect LLM-generated solutions, meaning it shares similar syntactic structure.
- Evidence anchors:
  - [abstract]: "we construct a probabilistic Context-Free Grammar (pCFG) based on the incorrect solutions proposed by the LLM, and use this to guide an enumerative synthesizer"
  - [section]: "we calculate a weight for each rule ri âˆˆ R as the number of times that rule appears in the left-most derivations of the programs" (Section 5.1)
  - [corpus]: Weak - no direct citation, but related work on pCFG-guided synthesis exists (e.g., [27])
- Break condition: If the LLM consistently generates solutions far from the correct one in structure, the pCFG will not effectively guide the search.

### Mechanism 2
- Claim: Integrating the LLM into the enumerator allows it to provide real-time syntactic feedback and helper functions, improving search efficiency.
- Mechanism: During enumeration, the LLM is prompted with the current partial program and constraints. It responds with helper functions that augment the grammar and update rule weights. This allows the LLM to guide the search dynamically based on progress.
- Core assumption: The LLM can provide useful helper functions and syntactic guidance when given partial programs and counterexamples.
- Evidence anchors:
  - [abstract]: "integrating an LLM into an enumerative synthesizer, allowing it to provide syntactic feedback and helper functions during the search"
  - [section]: "we ask it to provide helper functions to help 'a student' complete the partially enumerated program" (Section 6.1)
  - [corpus]: Weak - no direct citation, but related work on LLM integration in synthesis exists (e.g., [23, 40])
- Break condition: If the LLM fails to provide useful helper functions or syntactic feedback, the integration will not improve performance.

### Mechanism 3
- Claim: Combining LLM-generated pCFGs with A* search significantly outperforms both standalone LLMs and traditional enumerative synthesis.
- Mechanism: The pCFG inferred from LLM solutions is used to guide an A* search, where the heuristic is based on the probability of non-terminal symbols being converted to terminals. This focuses the search on high-probability areas of the solution space.
- Core assumption: A* search with a probability-based heuristic will efficiently find solutions in the high-probability regions of the grammar space.
- Evidence anchors:
  - [abstract]: "our approach integrating the LLM into an enumerative synthesis algorithm shows significant performance gains over both the LLM and the enumerative synthesizer alone"
  - [section]: "We implement a second variation of pCFG-synth using the A* weighted search algorithm as the underlying enumerator" (Section 5.3)
  - [corpus]: Weak - no direct citation, but related work on A* in synthesis exists (e.g., [27])
- Break condition: If the probability estimates in the pCFG are inaccurate, the A* heuristic will not effectively guide the search.

## Foundational Learning

- Concept: Context-Free Grammars (CFGs) and their weighted extensions (wCFGs and pCFGs)
  - Why needed here: The synthesis problem is guided by a grammar, and the LLM-generated solutions are used to build a weighted grammar that biases the search.
  - Quick check question: Given a set of programs, how would you calculate the weight of a production rule in a wCFG?

- Concept: CounterExample Guided Inductive Synthesis (CEGIS)
  - Why needed here: The enumerative synthesis algorithm uses CEGIS, alternating between generating candidate solutions and verifying them against constraints.
  - Quick check question: In a CEGIS loop, what information is passed back to the synthesis phase when verification fails?

- Concept: A* Search Algorithm
  - Why needed here: The A* variant of pCFG-synth uses a probability-based heuristic to guide the search efficiently.
  - Quick check question: How is the heuristic function g(x) calculated in the A* implementation for pCFG-synth?

## Architecture Onboarding

- Component map:
  - LLM -> Prompt Generator -> wCFG/pCFG Builder -> Enumerator (or A*) -> Verifier -> Solution
  - LLM also provides helper functions and syntactic feedback to Enumerator

- Critical path:
  1. Prompt the LLM with the synthesis problem
  2. Extract programs from LLM responses
  3. Build wCFG/pCFG from the programs
  4. Run the enumerator (or A* search) guided by the pCFG
  5. Verify candidate solutions
  6. If verification fails, update the pCFG with new information and repeat

- Design tradeoffs:
  - Using LLM-generated pCFGs vs. pre-trained probabilistic grammars: LLM-generated pCFGs require no training data but may be less accurate; pre-trained grammars are more accurate but require a dataset of known solutions.
  - Probabilistic enumerator vs. A* search: The probabilistic enumerator is simpler but may be less efficient; A* search is more complex but can be more efficient with a good heuristic.
  - Frequency of LLM calls in iLLM-synth: More frequent calls provide better guidance but are more expensive; less frequent calls are cheaper but may provide less useful feedback.

- Failure signatures:
  - LLM consistently generates solutions far from the correct one in structure
  - LLM fails to provide useful helper functions or syntactic feedback
  - Probability estimates in the pCFG are inaccurate, leading to inefficient search
  - Verifier fails to find counterexamples even when the candidate solution is incorrect

- First 3 experiments:
  1. Implement the standalone LLM approach and evaluate its performance on a small set of benchmarks
  2. Implement pCFG-synth with the probabilistic enumerator and compare its performance to the standalone LLM
  3. Implement iLLM-synth and compare its performance to pCFG-synth and the standalone LLM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in formal program synthesis scale with model size and fine-tuning on relevant datasets?
- Basis in paper: [inferred] The paper mentions that GPT-3.5 is not fine-tuned to the problem setting and that obtaining training datasets is challenging.
- Why unresolved: The authors did not experiment with larger or fine-tuned models, so the impact of model size and training data on performance remains unclear.
- What evidence would resolve it: Empirical results comparing the performance of different sized LLMs (e.g., GPT-3.5 vs GPT-4) and LLMs fine-tuned on formal synthesis datasets.

### Open Question 2
- Question: Can the proposed LLM-guided synthesis methods be effectively extended to handle programming-by-example (PBE) specifications?
- Basis in paper: [explicit] The authors state they omitted PBE benchmarks because their approaches are less effective with PBE specifications.
- Why unresolved: The paper does not explore modifications to the prompting strategy or search algorithm that might make them more suitable for PBE.
- What evidence would resolve it: Successful application of the LLM-guided methods to PBE benchmarks with appropriate modifications.

### Open Question 3
- Question: How sensitive are the LLM-guided synthesis results to the choice of hyperparameters, such as temperature, enumeration depth, and heuristic functions?
- Basis in paper: [explicit] The authors mention that they did not invest time in parameter tuning and that different results may be obtained by changing these parameters.
- Why unresolved: The paper does not report on the sensitivity of results to hyperparameter changes.
- What evidence would resolve it: Systematic experiments varying the hyperparameters and reporting on the impact on synthesis performance.

## Limitations

- The effectiveness of the approach depends on the accuracy of probability estimates in the pCFG, which may be inaccurate if LLM solutions are poor
- The integration of the LLM into the enumerator introduces additional computational overhead and complexity
- The generalizability of the approach to other domains and synthesis problems beyond the SyGuS benchmarks used in the evaluation is unclear

## Confidence

- High Confidence: The overall methodology and approach are well-founded, and the results demonstrate significant improvements over standalone LLMs and baseline enumerative synthesizers.
- Medium Confidence: The specific implementation details and hyperparameters used in the experiments may impact the performance of the approach.
- Low Confidence: The generalizability of the approach to other domains and synthesis problems beyond the SyGuS benchmarks used in the evaluation.

## Next Checks

1. Evaluate the approach on a diverse set of synthesis benchmarks beyond the SyGuS competition to assess its generalizability.
2. Investigate the impact of different LLM models and prompt formulations on the performance of the approach.
3. Analyze the computational overhead introduced by the LLM integration and explore techniques to optimize the interaction between the LLM and the enumerator.