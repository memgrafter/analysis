---
ver: rpa2
title: 'Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against
  Adversarial Attacks: A Dataset-Centric analysis'
arxiv_id: '2402.02154'
source_url: https://arxiv.org/abs/2402.02154
tags:
- adversarial
- training
- dataset
- attacks
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates adversarial robustness in off-road semantic\
  \ segmentation. Existing work focuses on architecture-level defenses or adversarial\
  \ training but does not examine the dataset\u2019s role in robustness."
---

# Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against Adversarial Attacks: A Dataset-Centric analysis

## Quick Facts
- arXiv ID: 2402.02154
- Source URL: https://arxiv.org/abs/2402.02154
- Authors: Pankaj Deoli; Rohit Kumar; Axel Vierling; Karsten Berns
- Reference count: 34
- Primary result: Dataset-centric approach to adversarial robustness in off-road segmentation shows quantitative improvements but qualitative limitations in ambiguous scenes

## Executive Summary
This study investigates adversarial robustness in off-road semantic segmentation by focusing on dataset composition rather than architecture-level defenses. The authors propose creating a "robustified" dataset containing only robust features and evaluate its impact on U-Net and LinkNet segmentation models under adversarial attacks. Through a four-stage process involving standard training, adversarial training, dataset robustification, and retraining, the study finds that while quantitative metrics show comparable performance, models still struggle with ambiguous off-road scenes. The research highlights that adversarial robustness is not solely a function of dataset composition and calls for further investigation into model-agnostic defenses for complex off-road environments.

## Method Summary
The authors employ a dataset-centric approach to adversarial robustness in off-road semantic segmentation. They begin by merging two off-road datasets (Freiburg Forest and YCOR) into 1442 images with 10 classes, then train baseline models (U-Net and LinkNet) for 100 epochs. Next, they perform adversarial training using PGD attacks with L2 and L∞ bounds. They then create a robustified dataset by iteratively adjusting random samples to match the representation learned by the adversarially trained model. Finally, they train models on this robustified dataset and evaluate performance using IoU and loss metrics on both clean and adversarial test sets.

## Key Results
- Quantitative metrics (IoU, loss) show comparable results between standard and robustified datasets
- Qualitative analysis reveals persistent challenges in accurately segmenting ambiguous off-road scenes
- Models struggle to distinguish surface types despite good quantitative performance
- Adversarial robustness is not solely determined by dataset composition
- Implications for improving surface-type detection on autonomous vehicles like the Unimog U5023

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial training improves segmentation robustness by exposing the model to perturbed inputs during training, forcing it to learn features that are invariant to small changes.
- **Mechanism:** During adversarial training, the model is trained on adversarial examples generated via PGD attacks, which iteratively adjust input pixels to maximize the loss. This process causes the network to prioritize learning "robust features" (e.g., object edges, texture) over "non-robust features" (e.g., background noise, lighting variations).
- **Core assumption:** The PGD attack can approximate the worst-case perturbation within a bounded norm (L2 or L∞), ensuring that the model is exposed to realistic adversarial scenarios.
- **Evidence anchors:**
  - The authors use PGD attacks with L2 and L∞ bounds for adversarial training and evaluate robustness by comparing IoU and loss on perturbed test data.
  - The study evaluates effects of adversarial attacks on U-Net and LinkNet models trained with PGD.
- **Break condition:** If the perturbation budget is too small, the adversarial examples may not be diverse enough to improve generalization; if too large, the model may overfit to unrealistic perturbations and degrade clean-data performance.

### Mechanism 2
- **Claim:** Robustifying the dataset by removing non-robust features helps the model focus on invariant patterns, reducing overfitting to spurious correlations.
- **Mechanism:** The robustification process iteratively modifies training samples to minimize the difference between the representation of a perturbed sample and the original. This is done by training a robust classifier first, then using its penultimate layer output as a target representation. Random samples are adjusted via gradient descent to match this representation, yielding a dataset that retains only features useful for classification under adversarial conditions.
- **Core assumption:** The penultimate layer of an adversarially trained model captures a stable representation of the input that is invariant to small perturbations.
- **Evidence anchors:**
  - The authors describe a gradient-based iterative adjustment of random samples to match the representation learned by the adversarially trained model.
  - The study creates a robust dataset consisting of only robust features and trains models on it.
- **Break condition:** If the robust model is not sufficiently accurate or overfits to the adversarial training distribution, the robustified dataset may lose important discriminative features, leading to poor performance on clean data.

### Mechanism 3
- **Claim:** Model architecture influences robustness: architectures that better preserve spatial information (e.g., LinkNet) may be less sensitive to adversarial perturbations than those that lose spatial detail early (e.g., U-Net).
- **Mechanism:** LinkNet's skip connections propagate encoder features to the decoder at every layer, allowing the model to recover spatial information that might otherwise be corrupted by adversarial noise. U-Net, while also using skip connections, may be more sensitive to noise due to its encoder-decoder bottleneck.
- **Core assumption:** The preservation of spatial detail throughout the network reduces the impact of pixel-level perturbations on the final segmentation output.
- **Evidence anchors:**
  - The authors compare U-Net and LinkNet, noting that LinkNet uses encoder-decoder connections to recover spatial information.
  - The study evaluates two SOTA segmentation networks (U-Net and LinkNet) on the robustified dataset.
- **Break condition:** If the adversarial perturbation is large enough to corrupt the skip connections or the decoder cannot recover the lost information, both architectures may fail similarly.

## Foundational Learning

- **Concept:** Adversarial examples and gradient-based attacks
  - Why needed here: Understanding how PGD attacks generate adversarial examples is critical to grasping why adversarial training improves robustness.
  - Quick check question: What is the key difference between FGSM and PGD attacks, and why is PGD considered stronger?

- **Concept:** Semantic segmentation and IoU metric
  - Why needed here: The paper evaluates segmentation performance using IoU, so understanding what IoU measures and how it reflects model accuracy is essential.
  - Quick check question: How is IoU calculated, and what does a high IoU indicate about a segmentation model's performance?

- **Concept:** Dataset robustification and feature decomposition
  - Why needed here: The paper's main contribution is creating a robust dataset by removing non-robust features; understanding this process is key to interpreting the results.
  - Quick check question: What is the difference between robust and non-robust features, and why might removing non-robust features improve adversarial robustness?

## Architecture Onboarding

- **Component map:** Data pipeline (Off-road datasets → preprocessing → merged dataset) → Models (U-Net, LinkNet) → Training loop (standard → adversarial → robustification → standard) → Evaluation (IoU, loss) → Adversarial attack module (PGD)

- **Critical path:**
  1. Load and preprocess the merged off-road dataset
  2. Train baseline models (U-Net, LinkNet) on clean data
  3. Perform adversarial training using PGD attacks
  4. Create robustified dataset by iteratively adjusting random samples to match the robust model's representation
  5. Train models on the robustified dataset
  6. Evaluate performance on clean and adversarial test sets

- **Design tradeoffs:**
  - Using a smaller, domain-specific dataset (off-road) vs. a larger, more general dataset (urban driving) for training
  - Training on adversarial examples vs. standard examples (tradeoff between robustness and clean-data accuracy)
  - Removing non-robust features vs. retaining all features (tradeoff between robustness and segmentation detail)

- **Failure signatures:**
  - Large drop in IoU when evaluated on adversarial examples
  - High loss values on robustified dataset despite good IoU on clean data
  - Poor qualitative segmentation (e.g., failure to distinguish surface types) despite good quantitative metrics
  - Overfitting to adversarial examples, resulting in poor performance on clean data

- **First 3 experiments:**
  1. Train U-Net and LinkNet on the merged off-road dataset for 100 epochs; evaluate IoU and loss on clean and adversarial test sets.
  2. Perform adversarial training on U-Net and LinkNet using PGD attacks (L2 and L∞); evaluate IoU and loss on adversarial test sets.
  3. Create robustified datasets using the adversarially trained models; train U-Net and LinkNet on these datasets and evaluate IoU and loss on clean and adversarial test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-class ambiguities in off-road environments fundamentally limit classifier robustness against adversarial perturbations?
- Basis in paper: "Presence of Multi-class in input images" section notes that ambiguous (non-rich features) multi-classes present fundamental barriers to classifier's robustness, with the observation that classifiers cannot be resistant against tiny perturbations due to decision boundaries being near significant portions of inputs.
- Why unresolved: The paper identifies this as a barrier but doesn't quantify how different types or levels of ambiguity affect robustness, or what threshold of ambiguity makes robustness impossible.
- What evidence would resolve it: Systematic experiments varying class ambiguity levels and measuring corresponding robustness thresholds, or theoretical analysis of decision boundary proximity in high-ambiguity datasets.

### Open Question 2
- Question: What is the minimum dataset size required for robust off-road semantic segmentation under adversarial attacks?
- Basis in paper: "Insufficient data" section references [21] showing robust classifiers require O(sqrt(d)) samples, and notes that adversarial training reduces effective information requiring more samples.
- Why unresolved: The paper acknowledges this as a problem but doesn't empirically determine the sample size needed for their specific task and dataset.
- What evidence would resolve it: Empirical studies varying training dataset sizes while measuring adversarial robustness metrics, or theoretical bounds on required samples for their specific off-road segmentation task.

### Open Question 3
- Question: Why do qualitative and quantitative performance metrics diverge in off-road adversarial robustness evaluation?
- Basis in paper: "FINDINGS" section explicitly states "the direct unreliable relation between qualitative and quantitative performance" and notes this was discovered despite metrics exceeding industry standards.
- Why unresolved: The paper observes this discrepancy but doesn't investigate the underlying causes or propose metrics that would better align with human perception.
- What evidence would resolve it: Analysis of which specific metrics fail to correlate with qualitative assessment, development of new evaluation metrics that better capture perceptual quality, or ablation studies identifying which aspects of the evaluation framework cause misalignment.

## Limitations
- Limited evaluation on diverse attack types beyond PGD attacks
- Small dataset size (1442 images) may not capture full complexity of off-road environments
- Lack of downstream task evaluation to assess real-world impact of segmentation improvements

## Confidence
- Medium: The methodology is well-structured but lacks critical implementation details for the robustification algorithm, particularly the exact gradient descent procedure and iteration parameters.

## Next Checks
1. Implement the exact robustification algorithm with specified hyperparameters to verify reproducibility of the robust dataset creation process
2. Evaluate model performance on a larger and more diverse set of adversarial attack types beyond PGD (e.g., CW attacks, physical-world perturbations)
3. Test the robustified models on downstream navigation tasks to assess real-world impact of the segmentation improvements