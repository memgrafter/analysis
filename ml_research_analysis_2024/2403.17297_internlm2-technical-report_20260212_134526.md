---
ver: rpa2
title: InternLM2 Technical Report
arxiv_id: '2403.17297'
source_url: https://arxiv.org/abs/2403.17297
tags:
- data
- training
- language
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InternLM2 is an open-source large language model that achieves
  state-of-the-art performance across multiple benchmarks, including comprehensive
  examinations, language and knowledge tasks, reasoning and mathematics, coding, long-context
  modeling, and tool utilization. The model is trained on diverse data types and employs
  innovative techniques like Grouped-Query Attention and Conditional Online Reinforcement
  Learning from Human Feedback (COOL RLHF) to enhance its capabilities.
---

# InternLM2 Technical Report

## Quick Facts
- arXiv ID: 2403.17297
- Source URL: https://arxiv.org/abs/2403.17297
- Authors: 85 researchers from multiple institutions
- Reference count: 40
- Key outcome: State-of-the-art performance across 30 benchmarks including comprehensive exams, reasoning, coding, long-context modeling, and tool utilization

## Executive Summary
InternLM2 is an open-source large language model achieving state-of-the-art performance through innovative pre-training and alignment techniques. The model leverages diverse high-quality data spanning text, code, and long-context sequences, combined with a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy. With 200k context window capability and strong performance across 6 evaluation dimensions, InternLM2 demonstrates exceptional capabilities in comprehensive exams, reasoning, mathematics, coding, and tool utilization while maintaining efficient inference through Grouped-Query Attention.

## Method Summary
InternLM2 is trained through a multi-stage process beginning with diverse pre-training data preparation across text, code, and long-context domains. The base model uses Grouped-Query Attention for efficient long-context processing and is trained initially on 4k tokens before advancing to 32k tokens. A capability-specific enhancement phase targets reasoning, mathematics, and coding domains using curated datasets. The model is aligned through Supervised Fine-Tuning followed by the novel COOL RLHF strategy that employs conditional reward models to reconcile conflicting human preferences while mitigating reward hacking.

## Key Results
- Achieves state-of-the-art performance on comprehensive examinations including C-Eval, Gaokao, and College English Test
- Demonstrates exceptional long-context modeling with 200k "Needle-in-a-Haystack" test performance
- Shows strong tool utilization capabilities across 30+ benchmarks spanning language, reasoning, mathematics, and coding tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** InternLM2 achieves state-of-the-art performance by combining high-quality diverse pre-training data with innovative alignment techniques.
- **Mechanism:** The model leverages a meticulously prepared corpus spanning text, code, and long-context data, followed by a novel Conditional Online RLHF (COOL RLHF) strategy to reconcile conflicting human preferences and mitigate reward hacking.
- **Core assumption:** Diverse, high-quality data combined with conditional reward modeling and multi-round online RLHF leads to superior performance across multiple benchmarks.
- **Evidence anchors:**
  - [abstract] "InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques."
  - [section 3.1] "InternLM2 extensively details how it prepares text, code, and long-context data for pre-training."
  - [section 4.2] "We propose COnditional OnLine RLHF (COOL RLHF). COOL RLHF first introduces a Conditional Reward mechanism to reconcile diverse preferences, which allows the reward model to dynamically allocate its attention to various preferences based on specific conditions, thereby optimally integrating multiple preferences."
- **Break condition:** If the data quality is poor or the COOL RLHF strategy fails to effectively reconcile conflicting preferences, the model's performance will degrade significantly.

### Mechanism 2
- **Claim:** InternLM2's long-context modeling capability is enhanced by Grouped-Query Attention (GQA) and extended context training.
- **Mechanism:** GQA reduces memory footprint during inference, while training on up to 32k tokens with positional encoding extrapolation enables strong performance on the 200k "Needle-in-a-Haystack" test.
- **Core assumption:** GQA and extended context training effectively improve the model's ability to handle long sequences without significant performance degradation.
- **Evidence anchors:**
  - [abstract] "InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k 'Needle-in-a-Haystack' test."
  - [section 2.2] "InternLM2 series models all choose Grouped-Query Attention (GQA) (Ainslie et al., 2023), so that it can infer both in high speed and low GPU memory with very long contexts."
  - [section 3.3.2] "In the second phase, we included 50% of pre-training corpora with lengths not exceeding 32k. In the third phase, we utilized capability-specific enhancement data."
- **Break condition:** If GQA implementation is flawed or the extended context training data is insufficient or noisy, the model's long-context performance will suffer.

### Mechanism 3
- **Claim:** InternLM2's capability-specific enhancement training significantly improves performance on reasoning, mathematics, and coding tasks.
- **Mechanism:** A curated dataset with high-quality retrieved data and domain-specific enhancement data is used in a separate training phase with a smaller learning rate and batch size.
- **Core assumption:** Targeted training on capability-specific data leads to substantial improvements in those domains.
- **Evidence anchors:**
  - [abstract] "InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking."
  - [section 3.3.3] "Capabilities such as reasoning, mathematical problem-solving, and knowledge memorizing are key abilities expected from large language models. However, in the pre-training process, high-quality capability-related data is sparsely distributed in the entire corpus, which makes it hard for models to be proficient at these mentioned capabilities."
  - [section 5.2.5] "By comparing the performance of the base models before and after undergoing enhancement training when subjected to Supervised Fine-Tuning (SFT) in Table 14. The capability dimensions are aligned with the categorization adopted in OpenCompass."
- **Break condition:** If the capability-specific data is not truly high-quality or the training process is not carefully controlled, the enhancement phase could lead to overfitting or degradation in other areas.

## Foundational Learning

- **Concept:** Pre-training data preparation
  - **Why needed here:** The quality and diversity of pre-training data directly impact the model's foundational knowledge and capabilities.
  - **Quick check question:** How does the data deduplication process using MinHash contribute to the overall data quality?

- **Concept:** Long-context modeling techniques
  - **Why needed here:** Handling long sequences is crucial for many real-world applications, and specific architectural choices are needed to do this efficiently.
  - **Quick check question:** What is the role of positional encoding extrapolation in enabling 200k context window performance?

- **Concept:** Reinforcement learning from human feedback (RLHF)
  - **Why needed here:** RLHF aligns the model's behavior with human preferences, improving its helpfulness and safety.
  - **Quick check question:** How does the conditional reward model in COOL RLHF address preference conflicts compared to traditional RLHF methods?

## Architecture Onboarding

- **Component map:** Pre-training data pipeline -> Base Transformer with GQA -> Capability-specific enhancement training -> Supervised Fine-Tuning -> COOL RLHF alignment
- **Critical path:** 1) Prepare high-quality diverse pre-training data, 2) Train base model with long-context capability, 3) Enhance specific capabilities with targeted data, 4) Align using SFT and COOL RLHF
- **Design tradeoffs:** GQA prioritizes inference speed and memory efficiency over potentially higher accuracy from standard multi-head attention. Conditional reward model in COOL RLHF adds complexity but aims to better handle conflicting preferences.
- **Failure signatures:** Reward hacking (e.g., generating overly long or repetitive responses) suggests RLHF issues. Poor long-context performance despite GQA indicates training data or positional encoding problems.
- **First 3 experiments:**
  1. **Data quality assessment:** Analyze deduplication and filtering rates at each stage of the pre-training data pipeline to ensure high-quality data is being used.
  2. **Long-context performance baseline:** Evaluate model performance on "Needle-in-a-Haystack" test at different context lengths to identify effective context window.
  3. **COOL RLHF ablation study:** Compare performance of models trained with and without conditional reward model and online RLHF strategy to isolate impact of these innovations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Conditional Online RLHF (COOL RLHF) strategy handle conflicting human preferences in complex scenarios?
- Basis in paper: [explicit]
- Why unresolved: While the paper introduces COOL RLHF and its conditional reward model, it doesn't provide specific examples or detailed analysis of how it handles conflicting preferences in practice. The effectiveness of this approach in real-world, complex scenarios remains to be seen.
- What evidence would resolve it: Empirical results demonstrating COOL RLHF's performance in tasks with known conflicting preferences, or a detailed case study analyzing how the model resolves specific preference conflicts.

### Open Question 2
- Question: What is the impact of different tokenization approaches on the performance of InternLM2, especially for Chinese text?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions the use of GPT-4's tokenizer with modifications for Chinese text, but doesn't provide a detailed analysis of how different tokenization strategies might affect the model's performance. The choice of tokenizer could significantly impact the model's ability to understand and generate Chinese text.
- What evidence would resolve it: A comparison of InternLM2's performance using different tokenization approaches, or an ablation study isolating the impact of tokenization on Chinese language tasks.

### Open Question 3
- Question: How does the long-context capability of InternLM2 scale with increasing context length, and what are the limitations of the current approach?
- Basis in paper: [explicit]
- Why unresolved: The paper demonstrates strong performance on long-context benchmarks up to 200k tokens, but doesn't explore the scalability limits or potential degradation in performance for even longer contexts. Understanding these limitations is crucial for real-world applications requiring extremely long contexts.
- What evidence would resolve it: Evaluation of InternLM2 on benchmarks with context lengths exceeding 200k tokens, or an analysis of performance degradation as context length increases.

### Open Question 4
- Question: How does the performance of InternLM2 compare to other state-of-the-art models when fine-tuned on specific downstream tasks?
- Basis in paper: [inferred]
- Why unresolved: While the paper provides a comprehensive evaluation of InternLM2's general capabilities, it doesn't delve into its performance when fine-tuned on specific tasks. This information would be valuable for users looking to adapt the model for particular applications.
- What evidence would resolve it: A comparison of InternLM2's fine-tuned performance on specific downstream tasks against other state-of-the-art models, or a case study showcasing its effectiveness in a particular domain.

## Limitations

- Data quality verification remains uncertain - specific quality thresholds and their impact on final model performance are not quantified
- COOL RLHF mechanism validation is incomplete - lacks ablation studies isolating contributions of conditional reward model versus online RLHF components
- Long-context performance verification gap - 200k context window achievement needs more rigorous validation across context lengths

## Confidence

**High confidence** in general training methodology and architectural choices (GQA, Transformer-based design, multi-stage training approach)
**Medium confidence** in claimed state-of-the-art performance across all 30 benchmarks - some evaluations lack comparisons with recent competing models
**Low confidence** in isolated contribution of COOL RLHF to overall performance improvements - without proper ablation studies or comparison to baseline RLHF implementations

## Next Checks

1. **Independent replication of capability-specific enhancement** - Train a baseline model without the capability-specific enhancement phase and compare performance on reasoning, mathematics, and coding benchmarks to isolate the contribution of this training stage.

2. **COOL RLHF ablation study** - Implement and evaluate InternLM2 with: (a) standard SFT only, (b) SFT + traditional RLHF, and (c) full COOL RLHF pipeline to quantify the specific performance impact of the conditional reward mechanism.

3. **Long-context performance degradation analysis** - Systematically evaluate the model's performance on "Needle-in-a-Haystack" style tasks across context lengths from 4k to 200k tokens to identify the effective context window and characterize performance degradation patterns.