---
ver: rpa2
title: Learning Run-time Safety Monitors for Machine Learning Components
arxiv_id: '2406.16220'
source_url: https://arxiv.org/abs/2406.16220
tags:
- safety
- monitor
- each
- data
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to create safety monitors for machine
  learning (ML) components in autonomous systems by training on degraded datasets.
  The core idea is to simulate environmental changes (e.g., fog, blur) as transformations
  applied to input data, then measure how these affect the ML component's performance.
---

# Learning Run-time Safety Monitors for Machine Learning Components

## Quick Facts
- arXiv ID: 2406.16220
- Source URL: https://arxiv.org/abs/2406.16220
- Reference count: 25
- Key outcome: Method to create safety monitors for ML components using degraded datasets achieves 92% accuracy on road sign classification

## Executive Summary
This paper introduces a novel approach to create safety monitors for machine learning components in autonomous systems by training on synthetically degraded datasets. The method simulates environmental changes (fog, blur) as transformations applied to input data, measures their impact on ML component performance, and trains a new model to classify inputs into safety risk levels. This addresses the critical challenge of runtime safety monitoring when ground truth is unavailable. The approach was validated on a road sign classifier using 25 degraded datasets from 4,110 original images, achieving 92% accuracy in predicting safety risk levels through 5-fold cross-validation.

## Method Summary
The approach creates safety monitors by first applying transformations (e.g., haze, blur) at varying intensities to original datasets to simulate environmental degradation. The ML component's performance is measured on each degraded dataset, which is then labeled according to safety thresholds reflecting normal, limited, and emergency stop modes. A new ML model (the safety monitor) is trained to classify input conditions into these risk levels based on the labeled degraded datasets. During deployment, this safety monitor operates in parallel with the primary ML component to predict safety risks without requiring ground truth, enabling dynamic assurance in changing environments.

## Key Results
- Safety monitor achieved 92% accuracy in predicting safety risk levels using 5-fold cross-validation
- 25 degraded datasets created from 4,110 original road sign images covering combinations of haze and blur perturbations
- Method successfully labels datasets based on performance thresholds linked to safety states (normal, limited, emergency stop)
- Enables runtime monitoring of ML safety without ground truth availability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety monitor accurately predicts unsafe ML component performance by training on synthetically degraded datasets
- Mechanism: Synthetic transformations applied at multiple intensity levels create datasets simulating real-world degradation. ML component performance on each dataset is measured and datasets are labeled by safety thresholds. Safety monitor trained to classify inputs into safety risk levels.
- Core assumption: Degradation patterns applied during training are representative of real-world perturbations the ML component will encounter
- Evidence anchors:
  - [abstract] "simulate environmental changes (e.g., fog, blur) as transformations applied to input data, then measure how these affect the ML component's performance"
  - [section] "These degraded data sets are presented to the ML component and labelled to reflect their impact on model performance"
  - [corpus] Weak evidence - related papers discuss runtime monitoring but not specific degraded dataset training approach
- Break condition: Real-world degradation patterns significantly different from synthetic transformations used during training

### Mechanism 2
- Claim: Safety monitor generalizes to unseen degraded inputs by learning relationship between input degradation and ML component performance
- Mechanism: Safety monitor learns mapping from input features (e.g., image blur, haze intensity) to performance categories during training. Uses learned mapping to predict safety risk levels of new inputs without ground truth labels.
- Core assumption: Relationship between input degradation and ML component performance is stable and learnable from synthetic training data
- Evidence anchors:
  - [abstract] "safety monitor that is created is deployed to the AS in parallel to the ML component to provide a prediction of the safety risk"
  - [section] "This classification allows the safety monitor to estimate the operational safety of the ML component used in an AS in real-world conditions"
  - [corpus] Weak evidence - related papers discuss runtime monitoring but not specific generalization mechanism
- Break condition: ML component performance degrades due to factors not captured by input degradation features

### Mechanism 3
- Claim: Safety monitor provides timely warnings by detecting performance degradation before hazardous system states
- Mechanism: Safety monitor continuously evaluates incoming inputs and predicts associated safety risk levels. High-risk levels trigger system responses (e.g., fall-back to alternate systems or restricted operation).
- Core assumption: Performance thresholds accurately reflect boundaries between safe and unsafe system states
- Evidence anchors:
  - [abstract] "This is a particularly difficult challenge when ground truth is unavailable at runtime"
  - [section] "These thresholds are derived through an assessment of the impact of the performance of the ML component on the safety of the AS"
  - [corpus] Weak evidence - related papers discuss runtime monitoring but not specific warning mechanism
- Break condition: Performance thresholds incorrectly defined or safety monitor predictions delayed

## Foundational Learning

- Concept: Machine learning performance metrics and their relationship to system safety
  - Why needed here: Understanding how ML performance metrics (e.g., accuracy) relate to system safety is crucial for defining appropriate performance thresholds and labeling training data
  - Quick check question: What are some common ML performance metrics, and how might they be used to assess the safety of an autonomous system?

- Concept: Data augmentation and synthetic data generation techniques
  - Why needed here: Creating degraded datasets through synthetic transformations requires knowledge of data augmentation techniques and their impact on ML model performance
  - Quick check question: What are some common data augmentation techniques, and how can they be used to simulate real-world degradation in input data?

- Concept: Machine learning model training and evaluation processes
  - Why needed here: Training and evaluating the safety monitor requires understanding of ML model training processes, including data splitting, model selection, and performance assessment
  - Quick check question: What are the key steps in training and evaluating an ML model, and how can cross-validation be used to assess model performance?

## Architecture Onboarding

- Component map: Original dataset → Transformation functions → Degraded datasets → Performance measurement → Labeled datasets → Safety monitor training → Risk level prediction → System response
- Critical path: Input data → Safety monitor → Risk level prediction → System response (if high risk)
- Design tradeoffs:
  - Tradeoff between safety monitor accuracy and computational efficiency: More accurate monitors may require complex models and computational resources
  - Tradeoff between false positives and false negatives: Too sensitive monitors generate false positives; less sensitive monitors may miss high-risk situations
- Failure signatures:
  - High false positive rate: Monitor frequently predicts high-risk for safe inputs, causing unnecessary system responses
  - High false negative rate: Monitor fails to predict high-risk for unsafe inputs, potentially leading to hazardous states
  - Slow response time: Monitor takes too long to predict risk levels, delaying system responses
- First 3 experiments:
  1. Evaluate safety monitor performance on held-out test set to assess generalization to unseen degraded inputs
  2. Test safety monitor's ability to detect performance degradation under different real-world conditions (varying weather, lighting)
  3. Measure impact of safety monitor on overall system performance, including delays or computational overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we identify relevant influencing factors and appropriate perturbations for non-image datasets, such as tabular patient data?
- Basis in paper: [inferred] Paper acknowledges this as key challenge for future work, noting medical applications with multi-attribute tabular data require different perturbations than image-based examples
- Why unresolved: Methodology focuses on image classification with mathematical transformations. Approach for identifying and modeling influencing factors for other data types is not established
- What evidence would resolve it: Case studies demonstrating identification of influencing factors and perturbations for specific non-image ML components, with validation of resulting safety monitors' effectiveness

### Open Question 2
- Question: How do labeling decisions affect safety monitor performance, and what guidance can optimize this process?
- Basis in paper: [explicit] Authors state labeling datasets is crucial and acknowledge different risk region definitions could lead to missed high-risk outputs or excessive false warnings, but further work is needed
- Why unresolved: Paper uses simple three-level threshold system but doesn't investigate how different labeling schemes impact monitor performance or provide systematic guidance for choosing thresholds
- What evidence would resolve it: Empirical studies comparing safety monitor performance across different labeling schemes and threshold configurations, leading to evidence-based recommendations

### Open Question 3
- Question: How can safety monitors be designed to handle continuous data streams where single erroneous outputs may not affect system behavior?
- Basis in paper: [explicit] Authors mention this as future work, noting many ML applications involve continuous data streams and monitors requiring multiple high-risk inputs before warning could help mitigate operational issues
- Why unresolved: Current methodology focuses on single-shot inputs. Approach for aggregating and analyzing multiple sequential inputs to determine safety risk is not addressed
- What evidence would resolve it: Demonstrations of safety monitors operating on continuous data streams, showing how they aggregate evidence across multiple inputs and how this affects both detection accuracy and system operational behavior

## Limitations
- Synthetic transformations may not fully capture real-world degradation complexity
- Results validated only on single dataset with controlled transformations
- Performance thresholds for labeling not explicitly specified
- No validation on held-out test data or real-world deployment scenarios

## Confidence
- Mechanism 1 (Synthetic dataset training): Medium - validated on single dataset with controlled transformations
- Mechanism 2 (Generalization capability): Low - limited testing on held-out data or real-world conditions
- Mechanism 3 (Timely warnings): Low - no evaluation of actual warning latency or system response effectiveness

## Next Checks
1. Evaluate safety monitor performance on a completely held-out test set with real-world degraded images to assess generalization beyond synthetic transformations
2. Test the approach across multiple ML component types (e.g., object detection, semantic segmentation) and datasets to verify robustness
3. Conduct controlled experiments measuring time delay between degradation detection and system response to validate warning mechanism's effectiveness