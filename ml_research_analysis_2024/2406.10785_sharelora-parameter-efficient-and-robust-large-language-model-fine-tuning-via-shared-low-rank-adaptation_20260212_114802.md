---
ver: rpa2
title: 'ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning
  via Shared Low-Rank Adaptation'
arxiv_id: '2406.10785'
source_url: https://arxiv.org/abs/2406.10785
tags:
- lora
- sharea
- llama
- across
- sharelora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShareLoRA is a parameter-efficient fine-tuning method for large
  language models that shares low-rank weight matrices across layers to reduce trainable
  parameters by 44% to 96% compared to standard LoRA. The method maintains model adaptability
  and robustness across diverse tasks and model scales by selectively sharing either
  matrix A or B while keeping the other trainable.
---

# ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2406.10785
- Source URL: https://arxiv.org/abs/2406.10785
- Authors: Yurun Song; Junchen Zhao; Ian G. Harris; Sangeetha Abdu Jyothi
- Reference count: 16
- Primary result: 44-96% reduction in trainable parameters while maintaining or improving performance across diverse tasks

## Executive Summary
ShareLoRA introduces a parameter-efficient fine-tuning method for large language models that strategically shares low-rank weight matrices across layers. By sharing either matrix A or B while keeping the other trainable, the method achieves significant parameter reductions while maintaining model adaptability and robustness. Experimental results demonstrate consistent improvements over standard LoRA across zero-shot, few-shot, and continual fine-tuning scenarios, with up to 1.2% higher accuracy on key benchmarks and up to 6.3GB memory savings for large models.

## Method Summary
ShareLoRA builds on LoRA's foundation by sharing low-rank matrices across layers rather than having unique matrices per layer. The method offers three configurations: ShareA (sharing matrix A across all layers with unique B matrices), ShareB (sharing matrix B with unique A matrices), and ShareAB (sharing both matrices). This sharing strategy reduces the number of trainable parameters by 44% to 96% compared to standard LoRA while maintaining performance through selective preservation of layer-specific adaptation capacity. The approach is evaluated across multiple model scales (from RoBERTa-base to LLaMA 13B) and diverse tasks including GLUE, E2E NLG Challenge, and continual learning scenarios.

## Key Results
- Achieves 44% to 96% reduction in trainable parameters compared to standard LoRA
- Consistently outperforms LoRA with up to 1.2% higher accuracy on tasks like GSM8K and HumanEval
- Demonstrates strong cross-domain generalization and knowledge retention during multi-task continual learning
- Reduces memory footprint by up to 6.3GB for large models without compromising performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing the A matrix across layers reduces trainable parameters while preserving model adaptability.
- Mechanism: By uniformly sharing the low-rank matrix A across all layers, ShareLoRA reduces the number of unique parameters that need to be learned. Each layer retains a unique B matrix, allowing for task-specific transformations while leveraging a shared projection-down structure.
- Core assumption: The shared A matrix captures sufficient representational capacity to support diverse layer-wise transformations when combined with unique B matrices.
- Evidence anchors:
  - [abstract] "By strategically sharing the low-rank weight matrices across different layers, ShareLoRA achieves 44% to 96% reduction in trainable parameters compared to standard LoRA"
  - [section 3] "In the ShareA configuration, the low-rank matrix A is uniformly shared across all layers, with each layer employing its own unique matrix Bi"
  - [corpus] Weak evidence - neighboring papers discuss parameter sharing but don't directly validate the specific A-sharing mechanism described here
- Break condition: If the shared A matrix becomes too restrictive, preventing layers from learning task-specific features effectively, performance would degrade compared to full LoRA.

### Mechanism 2
- Claim: ShareLoRA maintains or improves performance across diverse tasks by balancing parameter efficiency with model expressiveness.
- Mechanism: ShareLoRA achieves this balance by sharing either A or B while keeping the other matrix unique per layer. This allows the model to retain sufficient capacity for task adaptation while reducing redundancy across layers.
- Core assumption: The choice of which matrix to share (A or B) can be optimized based on the specific characteristics of the task and model architecture.
- Evidence anchors:
  - [abstract] "It consistently outperforms LoRA in zero-shot, few-shot, and continual fine-tuning scenarios, achieving up to 1.2% average accuracy improvement"
  - [section 5] "On the E2E NLG Challenge in Table 2, ShareA demonstrates markedly greater efficiency on GPT-2 models: it reduces LoRA's parameter count by 43% on the Medium model, yet still achieves performance gains"
  - [corpus] Moderate evidence - some neighboring papers discuss parameter-efficient fine-tuning but don't directly address the specific performance maintenance mechanisms of ShareLoRA
- Break condition: If the shared matrix becomes too dominant, it could prevent the model from learning task-specific nuances, leading to performance degradation.

### Mechanism 3
- Claim: ShareLoRA demonstrates robustness across domains by maintaining knowledge retention during continual learning.
- Mechanism: By sharing matrices across layers, ShareLoRA creates a more stable parameter space that resists catastrophic forgetting when adapting to new tasks or domains.
- Core assumption: The shared structure provides a consistent foundation that helps preserve previously learned knowledge while allowing for new task adaptation.
- Evidence anchors:
  - [abstract] "In continual learning settings, ShareLoRA achieves 1.2% higher accuracy on GSM8K, 0.6% on HumanEval, and 0.5% on both MMLU and MMLU-Pro"
  - [section 4] "We also investigate continual fine-tuning across multiple tasksâ€”starting from Alpaca, followed by GSM8K, then CodeAlpaca, and finally returning to Alpaca"
  - [corpus] Weak evidence - neighboring papers discuss continual learning but don't specifically address the knowledge retention mechanisms of ShareLoRA
- Break condition: If the shared structure becomes too rigid, it could prevent the model from adapting to significantly different domains, leading to performance degradation.

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: ShareLoRA builds on LoRA's foundation of decomposing weight updates into low-rank matrices A and B
  - Quick check question: How does decomposing a matrix into A and B reduce the number of parameters compared to full fine-tuning?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: ShareLoRA is a PEFT method that aims to achieve performance comparable to full fine-tuning with fewer trainable parameters
  - Quick check question: What are the key differences between PEFT methods like LoRA, prefix-tuning, and adapter-based approaches?

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: ShareLoRA's robustness across domains relies on its ability to retain knowledge from previous tasks while adapting to new ones
  - Quick check question: What mechanisms do continual learning methods typically employ to prevent catastrophic forgetting?

## Architecture Onboarding

- Component map: Pre-trained model weights (frozen) -> Shared matrix (A or B) -> Unique per-layer matrix (B or A) -> Integration point in transformer layers

- Critical path:
  1. Load pre-trained model with frozen weights
  2. Initialize shared matrix (A or B)
  3. Initialize unique per-layer matrices
  4. Integrate into transformer layers at appropriate positions
  5. Fine-tune only the shared and unique matrices

- Design tradeoffs:
  - Sharing A vs. B: Sharing A provides more parameter reduction but may limit expressiveness; sharing B preserves more capacity but with less reduction
  - Scope of sharing: Sharing across all layers vs. selective sharing (e.g., only QKV matrices) affects both efficiency and performance
  - Task specificity: More aggressive sharing may work better for similar tasks, while less sharing may be needed for diverse tasks

- Failure signatures:
  - Slow convergence: May indicate that the shared structure is too restrictive
  - Performance degradation: Could suggest that the wrong matrix was chosen for sharing or that sharing scope is too broad
  - Overfitting on small datasets: Might indicate that the model still has too many unique parameters relative to data size

- First 3 experiments:
  1. Compare convergence speed and final performance of ShareA vs. standard LoRA on a small dataset
  2. Test different sharing configurations (ShareA, ShareB, ShareAB) on a medium-sized dataset to identify optimal sharing strategy
  3. Evaluate cross-domain generalization by fine-tuning on one task and testing on a related but distinct task, comparing ShareLoRA to LoRA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice between sharing matrix A versus matrix B affect the long-term generalization capabilities of models across vastly different domain distributions beyond the tested benchmarks?
- Basis in paper: [explicit] The paper states that "sharing the up-projection matrix B plays a pivotal role by significantly enhancing the dimensionality of the low-rank representation" and suggests that "sharing the less critical module, LoRA A, while retaining the integrity of B" is justified, yet experiments show ShareA consistently outperforms ShareB across various tasks.
- Why unresolved: The paper demonstrates empirical superiority of ShareA over ShareB on specific tasks and datasets but does not provide a theoretical explanation for why A-sharing leads to better generalization. The underlying mechanisms driving this performance difference across domains remain unexplored.
- What evidence would resolve it: Controlled experiments comparing ShareA and ShareB on deliberately constructed domain shift scenarios, combined with ablation studies isolating the contribution of each matrix to task adaptation, would clarify the causal relationship between sharing strategy and generalization.

### Open Question 2
- Question: What is the precise relationship between the number of singular values retained in the low-rank decomposition and the model's ability to maintain performance during continual learning across multiple sequential tasks?
- Basis in paper: [inferred] The paper shows ShareLoRA outperforms LoRA in continual learning scenarios and includes SVD analysis revealing that ShareA weights have a "smoother, more gradual decrease in singular values" compared to LoRA's "sharp decrease," but does not directly connect singular value distribution to continual learning performance.
- Why unresolved: While the SVD analysis provides insight into weight distributions, the paper does not establish whether the broader singular value distribution in ShareA is the causal mechanism for improved continual learning, or merely correlated with it. The optimal singular value distribution for balancing specialization and generalization remains unknown.
- What evidence would resolve it: Systematic experiments varying the rank of the decomposition while measuring performance degradation across sequential tasks, coupled with analysis of how singular value spectra evolve during each fine-tuning phase, would reveal the causal relationship.

### Open Question 3
- Question: How does ShareLoRA's parameter efficiency scale when applied to models significantly larger than LLaMA 13B, particularly in the context of emergent capabilities that appear at scale?
- Basis in paper: [explicit] The paper demonstrates ShareLoRA's effectiveness on models ranging from RoBERTa-base to LLaMA 13B, showing parameter reductions of 44% to 96% compared to LoRA, but does not test on models beyond this scale.
- Why unresolved: The scaling behavior of ShareLoRA at model sizes approaching or exceeding 70B parameters remains untested, particularly regarding whether the relative parameter efficiency gains persist or whether emergent capabilities at scale might interact differently with the sharing strategy.
- What evidence would resolve it: Benchmarking ShareLoRA on frontier models (e.g., LLaMA 70B, GPT-4 class models) across diverse tasks, measuring both absolute performance and parameter efficiency ratios, would determine whether the scaling advantages hold at extreme model sizes.

## Limitations
- Cross-domain generalization claims are based only on related NLP tasks, not on significantly different domains
- Continual learning improvements are modest (1.2% on GSM8K) and may be influenced by task ordering
- Scaling properties beyond LLaMA 13B are untested, leaving uncertainty about performance at extreme model sizes

## Confidence

High confidence in parameter efficiency claims (44-96% reduction), as these are directly measurable from the number of trainable parameters. However, the robustness and performance claims warrant medium confidence due to several factors:

- Cross-domain generalization: Medium - based on related NLP tasks only, lacks theoretical justification
- Continual learning benefits: Medium - modest improvements (1.2% on GSM8K) without systematic ablation studies
- Scaling properties: Low - only tested up to LLaMA 13B, no analysis of behavior at larger scales

## Next Checks

1. **Ablation study on sharing strategy**: Systematically test ShareA, ShareB, and ShareAB configurations on the same model-task pairs to quantify the exact contribution of each sharing strategy to both efficiency and performance.

2. **Domain transfer robustness**: Fine-tune on a base task (e.g., Alpaca) and evaluate on significantly different domains (e.g., code generation, mathematical reasoning) to test whether the shared structure provides genuine robustness or merely memorizes task-specific patterns.

3. **Continual learning ablation**: Compare ShareLoRA against LoRA in a controlled continual learning setup where task order is varied systematically, measuring both performance retention and catastrophic forgetting across all tasks in the sequence.