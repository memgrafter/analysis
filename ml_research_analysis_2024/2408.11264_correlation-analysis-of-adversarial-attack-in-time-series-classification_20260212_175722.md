---
ver: rpa2
title: Correlation Analysis of Adversarial Attack in Time Series Classification
arxiv_id: '2408.11264'
source_url: https://arxiv.org/abs/2408.11264
tags:
- adversarial
- attack
- time
- series
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how adversarial attacks affect time series
  classification models, with a focus on local versus global information processing.
  It introduces frequency-based regularization using FFT to enhance attack effectiveness
  and employs noise and Gaussian filtering as defense strategies.
---

# Correlation Analysis of Adversarial Attack in Time Series Classification

## Quick Facts
- arXiv ID: 2408.11264
- Source URL: https://arxiv.org/abs/2408.11264
- Reference count: 25
- Key outcome: This paper examines how adversarial attacks affect time series classification models, with a focus on local versus global information processing. It introduces frequency-based regularization using FFT to enhance attack effectiveness and employs noise and Gaussian filtering as defense strategies. The study shows that models focusing on global information exhibit stronger resistance to adversarial perturbations. Regularization targeting frequency components significantly increases attack success rates, while noise introduction and Gaussian filtering reduce the Attack Success Rate (ASR) and enhance robustness. The results underscore the importance of frequency domain analysis in developing more resilient time series classification models.

## Executive Summary
This study investigates how adversarial attacks affect time series classification models, with particular attention to the role of local versus global information processing. The research introduces frequency-based regularization techniques, including Fast Fourier Transform (FFT), to enhance attack effectiveness, and explores noise introduction and Gaussian filtering as defensive strategies. The findings reveal that models prioritizing global information demonstrate greater resilience to adversarial perturbations, while frequency-focused regularization significantly increases attack success rates. The study underscores the importance of frequency domain analysis in developing robust time series classification models and highlights the potential for improving both attack and defense mechanisms through targeted frequency manipulation.

## Method Summary
The study employs five different models (Inception Time v4, LSTM-FCN, MACNN, ResNet18, TS2V) and five adversarial attack methods (PGD, SWAP, SWAP(L2), COS, FFT) on the UCR2018 datasets. Experiments are implemented using PyTorch 2.0 on a server with Nvidia RTX 4090 GPUs, 128 GB RAM, and Dual Xeon E5-2667 v4 processors. The evaluation metrics are Attack Success Rate (ASR) and Mean Success Distance (MSD) based on L2 Distance. The study explores how frequency-based regularization techniques and defensive strategies like noise introduction and Gaussian filtering affect model robustness to adversarial attacks.

## Key Results
- Models focusing on global information exhibit stronger resistance to adversarial perturbations
- Frequency-based regularization using FFT significantly increases attack success rates
- Noise introduction and Gaussian filtering effectively reduce ASR and enhance model robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks are more effective when perturbations emphasize high-frequency components.
- Mechanism: Regularization techniques like FFT and COS explicitly boost high-frequency components in the perturbation vector, increasing ASR and lowering MSD.
- Core assumption: Neural networks trained on time series are more sensitive to local (high-frequency) features than global (low-frequency) ones.
- Evidence anchors:
  - [abstract] "regularization techniques, particularly those employing Fast Fourier Transform (FFT) methods and targeting frequency components of perturbations, markedly enhance the effectiveness of attacks."
  - [section] "We begin our discussion with Table 1, where we evaluate five adversarial attack methods: PGD, SWAP, SWAP(L2), COS, and FFT... FFT approach... consistently achieves higher ASR and lower MSD in the majority of cases."
  - [corpus] "Leveraging Information Consistency in Frequency and Spatial Domain for Adversarial Attacks" (frequency domain focus supports premise).
- Break condition: If the target model is explicitly designed to prioritize global information (e.g., TS2V with augmentation), the perturbation's high-frequency emphasis loses effectiveness.

### Mechanism 2
- Claim: Defense via Gaussian filtering and noise addition reduces ASR by removing or masking high-frequency information.
- Mechanism: Gaussian filtering acts as a low-pass filter, removing high-frequency perturbations; noise addition masks high-frequency components, making them harder for the model to interpret as signal.
- Core assumption: Adversarial perturbations rely on high-frequency components to achieve misclassification.
- Evidence anchors:
  - [abstract] "defense strategies, like noise introduction and Gaussian filtering, are shown to significantly lower the Attack Success Rate (ASR), with approaches based on noise introducing notably effective in countering high-frequency distortions."
  - [section] "Our observations indicate that models fortified with noise as a defense mechanism exhibited a more pronounced reduction in ASR... models fortified with Gaussian smoothing... slightly outperform their counterparts employing noise in terms of test set performance."
  - [corpus] Weak/no direct match; evidence is primarily from paper's own experiments.
- Break condition: If the adversarial attack method itself emphasizes low-frequency perturbations (not tested in paper), filtering may be less effective.

### Mechanism 3
- Claim: Models that prioritize global information are inherently more robust to adversarial attacks.
- Mechanism: TS2V and similar architectures designed for global feature extraction learn to ignore local noise, reducing susceptibility to high-frequency adversarial perturbations.
- Core assumption: Local features are more vulnerable to adversarial manipulation than global features.
- Evidence anchors:
  - [abstract] "models designed to prioritize global information are revealed to possess greater resistance to adversarial manipulations."
  - [section] "TS2V model demonstrates a significant augmentation in test accuracy when training incorporates augmentation... Remarkably, employing Gaussian noise as defense slightly increased the ASR for FFT attacks compared to SWAP, indicating that while noise affects high-frequency data, it doesn't completely remove relevant high-frequency information..."
  - [corpus] "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain" (frequency domain defenses align with global vs. local information focus).
- Break condition: If the attack strategy is adapted to exploit global features (e.g., via frequency-domain manipulation targeting low-frequency components), the defense advantage disappears.

## Foundational Learning

- Concept: Normalized Auto Correlation Function (NACF)
  - Why needed here: NACF is used to measure how neural networks prioritize local vs. global information under adversarial conditions.
  - Quick check question: What does a steep decline in NACF at small τ values indicate about a model's feature preferences?

- Concept: Fast Fourier Transform (FFT) and frequency domain analysis
  - Why needed here: FFT is used both to enhance attack effectiveness (by amplifying high-frequency components in perturbations) and to understand model vulnerabilities.
  - Quick check question: Why would multiplying a perturbation's FFT by a sigmoid function increase its high-frequency content?

- Concept: Adversarial training and regularization
  - Why needed here: Regularization terms (COS, FFT) are added to the loss function to shape the perturbation's frequency profile.
  - Quick check question: How does adding a regularization term to the loss function influence the gradient descent update of the perturbation?

## Architecture Onboarding

- Component map:
  Input x -> Defense layer -> Neural Network Encoder -> Fully Connected Layer -> Softmax Layer -> Probabilities p

- Critical path:
  1. Input x → Defense layer → Model f → Logits z → Softmax → Probabilities p
  2. For attacks: p compared to target distribution q → Loss L = H(p,q) + α·reg → Backprop → Update r
  3. For defenses: x → Defense layer → Model f → Standard training → Update θ

- Design tradeoffs:
  - High-frequency perturbations (FFT/COS) → higher ASR, lower MSD, but more detectable
  - Noise augmentation → better robustness but may hurt test accuracy on clean data
  - Gaussian filtering → preserves global features, better accuracy retention, but less effective than noise at reducing ASR

- Failure signatures:
  - ASR plateaus despite increasing perturbation magnitude → defense effective
  - MSD increases sharply → attack too aggressive, perturbation becomes detectable
  - Test accuracy drops significantly with defense → over-regularization or noise corruption

- First 3 experiments:
  1. Apply FFT-based regularization to SWAP attack on Inception Time; measure ASR and MSD against undefended baseline
  2. Add Gaussian filtering defense to TS2V; evaluate ASR reduction for FFT attack
  3. Train TS2V with noise augmentation; compare clean test accuracy and ASR under PGD attack to undefended TS2V

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different neural network architectures' capacities to learn global versus local information influence their susceptibility to adversarial attacks?
- Basis in paper: [explicit] The study found that models focusing on global information exhibit stronger resistance to adversarial perturbations.
- Why unresolved: The paper does not deeply analyze the architectural differences that contribute to this resilience, leaving a gap in understanding which specific model features or configurations enhance global information processing.
- What evidence would resolve it: Comparative studies examining the performance of various architectures under adversarial conditions, focusing on their design aspects that prioritize global versus local information processing.

### Open Question 2
- Question: What is the optimal balance between attack sophistication and model robustness that can be achieved through frequency domain analysis?
- Basis in paper: [inferred] The paper highlights the importance of frequency domain analysis in designing robust defense mechanisms and effective attacks.
- Why unresolved: While the paper demonstrates the effectiveness of frequency-focused regularization, it does not provide a comprehensive framework for balancing attack sophistication with model robustness.
- What evidence would resolve it: Experimental results showing the performance of models trained with varying degrees of frequency-focused regularization against different attack strategies, identifying the optimal balance for robust defense.

### Open Question 3
- Question: How do different defense strategies, such as noise introduction and Gaussian filtering, affect the model's ability to generalize to new, unseen data?
- Basis in paper: [explicit] The study shows that noise introduction and Gaussian filtering reduce the Attack Success Rate (ASR) and enhance robustness.
- Why unresolved: The paper focuses on the effectiveness of these defenses against attacks but does not explore their impact on the model's generalization performance on new data.
- What evidence would resolve it: Empirical studies comparing the generalization performance of models with and without these defense strategies on a variety of datasets, assessing their ability to maintain accuracy on unseen data.

## Limitations

- The study's findings are limited by its focus on the UCR2018 dataset collection, which may not fully represent the diversity of real-world time series applications.
- The effectiveness of frequency-based attacks and defenses could vary significantly across different domains and data characteristics.
- The paper does not explore adaptive attacks that could potentially circumvent the proposed defenses, leaving open questions about the long-term robustness of these methods.

## Confidence

- **High Confidence**: The core finding that models emphasizing global information show greater robustness to adversarial attacks is well-supported by experimental results across multiple architectures and attack methods.
- **Medium Confidence**: The effectiveness of frequency-based regularization in enhancing attack success rates is demonstrated, but the generalizability to other attack frameworks remains uncertain.
- **Low Confidence**: The comparative advantage of noise-based defenses over Gaussian filtering is based on limited experimental conditions and may not hold across different model architectures or attack types.

## Next Checks

1. Test the frequency-based attack and defense mechanisms on non-UCR datasets, particularly those with different characteristics (e.g., financial time series, sensor data) to assess generalizability.
2. Evaluate adaptive attacks that specifically target global features to determine if the observed robustness advantage of global-information models can be overcome.
3. Investigate the impact of different noise distributions and filtering parameters on attack success rates to optimize defense strategies.