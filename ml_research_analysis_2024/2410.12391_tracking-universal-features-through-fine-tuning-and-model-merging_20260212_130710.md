---
ver: rpa2
title: Tracking Universal Features Through Fine-Tuning and Model Merging
arxiv_id: '2410.12391'
source_url: https://arxiv.org/abs/2410.12391
tags:
- features
- feature
- language
- sparse
- tinystories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how features in Transformer language models
  evolve during transfer learning tasks like fine-tuning and model merging. The authors
  start with a base model trained on BabyLM and Python code, then fine-tune two copies
  on Lua programming and English text (TinyStories) respectively.
---

# Tracking Universal Features Through Fine-Tuning and Model Merging

## Quick Facts
- arXiv ID: 2410.12391
- Source URL: https://arxiv.org/abs/2410.12391
- Authors: Niels Horn; Desmond Elliott
- Reference count: 12
- Key outcome: Most features do not persist through fine-tuning and merging, but the few that do are interpretable and correspond to generic properties like punctuation and formatting.

## Executive Summary
This paper studies how features in Transformer language models evolve during transfer learning tasks like fine-tuning and model merging. The authors start with a base model trained on BabyLM and Python code, then fine-tune two copies on Lua programming and English text (TinyStories) respectively. These are merged using spherical linear interpolation to create a fourth model. Using sparse autoencoders, they extract and track features across all four models. Their key finding is that most features do not persist through fine-tuning and merging, but the few that do are interpretable and correspond to generic properties like punctuation and formatting. They present two case studies: a persistent feature tracking variable assignments across all models, and a disappearing feature that handled Python exceptions but was lost when fine-tuning to Lua.

## Method Summary
The authors train a base model on BabyLM and Python code, then create two fine-tuned variants on Lua programming and children's text (TinyStories). They merge these fine-tuned models using spherical linear interpolation (SLERP) at t=0.58 to find an equilibrium point. Sparse autoencoders are trained on MLP activations from each model using 15M tokens sampled from their respective training distributions. Features are then correlated across models using an 80% threshold to identify emerging, disappearing, and persisting features. The methodology tracks how neural features evolve during typical transfer learning scenarios.

## Key Results
- Most features from the base model disappear through fine-tuning on new domains
- A small set of features persist across all models, corresponding to generic properties like punctuation and formatting
- One persistent feature tracked variable assignments across all models, while a disappearing feature handled Python exceptions that became irrelevant in Lua

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on different domains causes most features to disappear rather than persist.
- Mechanism: When fine-tuning on a new domain (Lua or TinyStories), the training data distribution changes significantly from the base BabyPython model. Features specialized for Python syntax and BabyLM text become less relevant and their activation patterns diverge from the parent model, causing correlation below the 80% persistence threshold.
- Core assumption: Feature persistence requires similar activation patterns across domains, and domain shift breaks these patterns.
- Evidence anchors:
  - [abstract] "most features do not persist through fine-tuning and merging"
  - [section 4.1] "we note that most of the features from the base model BabyPython disappear through fine-tuning"
  - [corpus] Weak - no direct citations on domain shift effects on feature persistence

### Mechanism 2
- Claim: Spherical linear interpolation (SLERP) can merge models while preserving some domain-specific features.
- Mechanism: SLERP interpolates model parameters along a spherical path between fine-tuned models, finding an equilibrium point (t=58%) where the merged model retains useful features from both domains. The interpolation creates a new parameter space that balances the two fine-tuned models.
- Core assumption: Model parameters from different fine-tuned models exist on a connected manifold that allows smooth interpolation.
- Evidence anchors:
  - [section 3.3] "we can use generalised linear mode connectivity to enable model merging via spherical linear interpolation"
  - [section 3.3] "We pick the model corresponding to the parameters at equilibrium of the merged model accuracy on Lua and TinyStories at t = 58%"
  - [corpus] Weak - no direct citations on SLERP effectiveness for language models

### Mechanism 3
- Claim: Sparse autoencoders can extract universal features across models trained on different domains.
- Mechanism: By training sparse autoencoders on MLP activations from each model using the same data distribution as the original training, the method can identify feature activation patterns that correspond to similar concepts (like punctuation or variable assignments) across different models.
- Core assumption: Despite domain differences, certain fundamental text properties have similar neural representations across models.
- Evidence anchors:
  - [abstract] "their key finding is that most features do not persist through fine-tuning and merging, but the few that do are interpretable"
  - [section 4.2.1] "we find that our tracked set of features are qualitatively universal, by analysing top-activating tokens and contexts"
  - [corpus] Weak - no direct citations on sparse autoencoder universality claims

## Foundational Learning

- Concept: Feature universality in neural networks
  - Why needed here: Understanding that features can be extracted across different models requires knowledge of how neural networks learn similar representations for similar concepts
  - Quick check question: Why would two models trained on different data still have some similar features?

- Concept: Transfer learning and domain adaptation
  - Why needed here: The paper studies how features evolve when models are adapted to new domains, which requires understanding the mechanisms of transfer learning
  - Quick check question: What happens to model parameters when fine-tuning on a new domain?

- Concept: Sparse autoencoders and dictionary learning
  - Why needed here: The paper uses sparse autoencoders to extract and track features, requiring understanding of how they decompose neural activations into interpretable components
  - Quick check question: How do sparse autoencoders identify meaningful features from neural network activations?

## Architecture Onboarding

- Component map: Base model → Fine-tuning (Lua/TinyStories) → Model merging → Sparse autoencoders for feature extraction → Correlation analysis
- Critical path: Train base model → Fine-tune on two domains → Merge models → Extract features from all models → Correlate features across models
- Design tradeoffs: Small models for faster training vs. potentially missing complex feature interactions; sparse autoencoders vs. other feature extraction methods
- Failure signatures: Low correlation between features (below 80% threshold); merged model performs worse than base model; sparse autoencoder fails to extract meaningful features
- First 3 experiments:
  1. Train the base BabyPython model and verify it can predict both Python and English text
  2. Fine-tune one copy on Lua and one on TinyStories, checking that each specializes in its domain
  3. Use sparse autoencoders to extract features from the base model and verify they can be interpreted

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on a small-scale 125M parameter model which may not generalize to larger language models
- Domain shift analysis limited to two specific domains (Lua programming and children's text)
- Correlation threshold of 80% for feature persistence is somewhat arbitrary

## Confidence

**High Confidence Claims:**
- Technical methodology for feature extraction using sparse autoencoders is well-established and correctly implemented
- Observation that most features disappear during fine-tuning is empirically supported by the data
- Case study of the variable assignment feature is clearly demonstrated with interpretable examples

**Medium Confidence Claims:**
- Claim that SLERP finds an optimal equilibrium point at t=58% is supported but could benefit from additional interpolation points
- Interpretation of universal features as "generic properties" is reasonable but somewhat subjective
- Automated interpretability pipeline appears functional but details are limited

**Low Confidence Claims:**
- Assertion that the specific set of persisting features is universal across all language models
- Claims about feature evolution mechanisms without deeper analysis of why specific features persist or disappear
- Generalizability of findings to larger models and different fine-tuning scenarios

## Next Checks
1. **Replication with Larger Models**: Test the feature tracking methodology on a 1-2B parameter model to assess scalability and whether the pattern of feature persistence/disappearance holds at larger scales.

2. **Multiple Fine-tuning Runs**: Perform multiple independent fine-tuning runs with different random seeds to determine whether the identified universal features are consistent across different training instances.

3. **Broader Domain Coverage**: Expand the analysis to include fine-tuning on additional domains (e.g., legal text, scientific writing) to test whether the identified universal features remain consistent across a wider range of domain shifts.