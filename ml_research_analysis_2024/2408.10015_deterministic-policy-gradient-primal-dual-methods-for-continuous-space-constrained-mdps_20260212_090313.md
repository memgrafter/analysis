---
ver: rpa2
title: Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space Constrained
  MDPs
arxiv_id: '2408.10015'
source_url: https://arxiv.org/abs/2408.10015
tags:
- function
- policy
- value
- deterministic
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first deterministic policy gradient primal-dual
  (D-PGPD) method for continuous-space constrained Markov decision processes (MDPs).
  The key innovation is leveraging regularization of the Lagrangian to enable deterministic
  policy search in continuous state-action spaces, overcoming the computational intractability
  of finding deterministic optimal policies.
---

# Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space Constrained MDPs

## Quick Facts
- arXiv ID: 2408.10015
- Source URL: https://arxiv.org/abs/2408.10015
- Authors: Sergio Rozada; Dongsheng Ding; Antonio G. Marques; Alejandro Ribeiro
- Reference count: 40
- This paper proposes the first deterministic policy gradient primal-dual (D-PGPD) method for continuous-space constrained Markov decision processes (MDPs).

## Executive Summary
This paper introduces D-PGPD, the first deterministic policy gradient primal-dual method for continuous-space constrained MDPs. The key innovation is using Lagrangian regularization to enable deterministic policy search in continuous spaces, overcoming the computational intractability of finding deterministic optimal policies. The method updates the deterministic policy via quadratic-regularized gradient ascent and the dual variable via quadratic-regularized gradient descent, achieving sub-linear convergence rates under standard assumptions. Experiments on robot navigation and fluid control demonstrate improved constraint satisfaction compared to existing methods.

## Method Summary
The method reformulates constrained MDPs as finding saddle points of a regularized Lagrangian, where deterministic policies are sufficient under non-atomicity assumptions. The primal update uses proximal optimization to maximize the augmented action-value function, while the dual update performs gradient descent on constraint violations. Function approximation with compatible bases enables practical implementation while preserving convergence guarantees. The algorithm maintains strong convexity/concavity through regularization, allowing for tractable deterministic policy gradient computation.

## Key Results
- D-PGPD successfully converges to feasible solutions with low variance in continuous control tasks
- The method achieves sub-linear convergence rates up to function approximation error
- D-PGPD outperforms PGDual baseline in constraint satisfaction while maintaining comparable reward performance
- First sample complexity result for constrained MDPs with continuous spaces established

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic policies are sufficient for constrained MDPs with continuous state-action spaces under non-atomicity assumption.
- Mechanism: In non-atomic MDPs, the deterministic value image VD equals the general value image VT, ensuring optimal deterministic policies exist within VD.
- Core assumption: The MDP is non-atomic (ρ(s) = 0 and p(s'|s,a) = 0 for all s, s', a).
- Evidence anchors:
  - [abstract] "deterministic policies are sub-optimal in finite state-action spaces, but sufficient for constrained MDPs with continuous state-action spaces"
  - [section] "Deterministic policies can be sub-optimal in constrained MDPs with finite state-action spaces... However, there is a rich body of constrained control literature that studies problems where optimal policies are deterministic"
  - [corpus] Weak - neighboring papers don't directly address this sufficiency condition.
- Break condition: If the MDP becomes atomic (has atoms in transition or initial distributions), deterministic policies may no longer be sufficient.

### Mechanism 2
- Claim: Regularization of the Lagrangian enables tractable deterministic policy gradient computation.
- Mechanism: Adding quadratic regularization h(λ) = λ² and ha(a) = -||a||² creates strong convexity/concavity, enabling proximal-point-type updates that converge to global saddle points.
- Core assumption: The regularized state-action value function Qλ,τ(s,a) - τ₀||π₀(s) - a||² is concave in action a for some τ₀ ∈ [0,τ).
- Evidence anchors:
  - [abstract] "leverage regularization of the Lagrangian to enable deterministic policy search in continuous state-action spaces"
  - [section] "we employ Theorem 1 to interpret Problem (1) as a saddle point problem" and "we resort to the regularization method"
  - [corpus] Weak - neighboring papers don't discuss this specific regularization approach for deterministic policies.
- Break condition: If the concavity assumption fails (e.g., non-concave reward/utility functions), the regularization may not create sufficient structure for convergence.

### Mechanism 3
- Claim: Function approximation with compatible bases enables practical implementation while preserving convergence.
- Mechanism: Approximating the augmented action-value function Jπ(s,a) = Qλ,τ(s,a) + 1/ηπ(s)⊤a with linear estimators ˜Jθ(s,a) = ϕ(s,a)⊤θ maintains concavity and bounded approximation error.
- Core assumption: The feature covariance matrix Σν = E[ϕ(s,a)ϕ(s,a)⊤] is positive definite and the approximation error is bounded.
- Evidence anchors:
  - [abstract] "We instantiate D-PGPD with function approximation and prove that the primal-dual iterates of D-PGPD converge at a sub-linear rate, up to a function approximation error"
  - [section] "To instantiate D-PGPD (6) with function approximation we begin by expanding the objective in (6a) and dropping the terms that do not depend on the action a"
  - [corpus] Weak - neighboring papers don't discuss this specific function approximation strategy for primal-dual methods.
- Break condition: If the basis functions cannot represent the true augmented value function or approximation error grows unbounded, convergence guarantees may fail.

## Foundational Learning

- Concept: Non-atomic probability measures
  - Why needed here: The sufficiency of deterministic policies relies on the non-atomicity assumption, which ensures no single state has positive probability mass.
  - Quick check question: What does it mean for a probability measure to be non-atomic, and why does this property enable deterministic policies in continuous MDPs?

- Concept: Convexity of value images
  - Why needed here: The zero duality gap proof depends on the convexity of the deterministic value image VD, even though individual value functions are non-convex in policy.
  - Quick check question: How can the set of value functions VD be convex while each V(π) is non-convex in π?

- Concept: Saddle point problems and regularization
  - Why needed here: The method reformulates the constrained problem as finding saddle points of a regularized Lagrangian, requiring understanding of primal-dual optimization.
  - Quick check question: How does adding regularization terms h(λ) = λ² and ha(a) = -||a||² help find saddle points in non-convex problems?

## Architecture Onboarding

- Component map:
  - D-PGPD algorithm with primal (policy) and dual (Lagrangian multiplier) updates
  - Function approximation module: Linear estimator with basis functions for augmented value functions
  - Sample-based implementation: Rollout-based estimation of Q and V functions
  - Projection operators: PΠ⋆τ and PΛ⋆τ for mapping to optimal policy and multiplier sets

- Critical path:
  1. Initialize policy π₀ and multiplier λ₀
  2. At each iteration t: estimate Jθt(s,a) via function approximation
  3. Update policy via proximal-point-type ascent on advantage function
  4. Update multiplier via gradient descent on constraint violation
  5. Project to optimal sets and repeat

- Design tradeoffs:
  - Regularization parameter τ: Larger τ improves convergence but reduces solution quality
  - Step-size η: Larger η speeds convergence but increases oscillation amplitude
  - Basis function selection: Must maintain concavity while approximating value functions well

- Failure signatures:
  - High variance in reward/utility values indicates constraint violations
  - Persistent oscillations suggest step-size is too large or regularization insufficient
  - Divergence indicates poor function approximation or incompatible basis functions

- First 3 experiments:
  1. Test convergence on known-dynamics navigation problem with quadratic rewards
  2. Evaluate constraint satisfaction on absolute-value penalty version of same problem
  3. Benchmark against PGDual on non-linear fluid control problem with unknown dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the deterministic policy gradient primal-dual method be extended to handle non-stationary environments where the reward and constraint functions change over time?
- Basis in paper: [inferred] The paper focuses on stationary MDPs, but the method could potentially be adapted for non-stationary settings.
- Why unresolved: The paper does not explore or analyze the performance of the method in non-stationary environments.
- What evidence would resolve it: Experiments and theoretical analysis demonstrating the effectiveness and convergence properties of the method in non-stationary environments.

### Open Question 2
- Question: How does the choice of regularization parameter τ affect the convergence rate and final solution quality in practice?
- Basis in paper: [explicit] The paper discusses the impact of τ on convergence but does not provide a comprehensive empirical study on its effect.
- Why unresolved: The paper provides theoretical insights but lacks extensive empirical evaluation of τ's impact on convergence and solution quality.
- What evidence would resolve it: Empirical studies comparing the performance of the method with different τ values on various benchmark problems.

### Open Question 3
- Question: Can the deterministic policy gradient primal-dual method be combined with other reinforcement learning techniques, such as actor-critic methods, to improve performance?
- Basis in paper: [inferred] The paper introduces a novel method but does not explore its integration with other RL techniques.
- Why unresolved: The paper focuses on the standalone performance of the method and does not investigate potential synergies with other RL approaches.
- What evidence would resolve it: Experiments and theoretical analysis demonstrating the benefits of combining the method with other RL techniques.

## Limitations
- The non-atomicity assumption for deterministic policy sufficiency may be restrictive in practical continuous MDPs
- Theoretical guarantees rely on strong concavity conditions that may not hold for all reward/utility functions
- Experimental validation lacks comparison with modern constrained RL baselines beyond PGDual

## Confidence

- **High confidence**: The algorithmic framework and its relationship to existing primal-dual methods is well-established. The regularization approach for enabling deterministic policy search is technically sound.
- **Medium confidence**: The non-atomicity sufficiency condition for deterministic policies in continuous MDPs is theoretically justified but may be restrictive in practice. The sub-linear convergence rates under the stated assumptions appear correct.
- **Low confidence**: The practical impact of function approximation errors on convergence is not fully characterized, and the experimental comparisons lack breadth against modern constrained RL baselines.

## Next Checks

1. **Test atomicity sensitivity**: Evaluate D-PGPD performance on MDPs with small atomic components to assess robustness when the non-atomicity assumption is violated.

2. **Benchmark against alternative methods**: Compare D-PGPD with recent constrained RL approaches like CPO or Lagrangian methods with stochastic policies on identical continuous control tasks.

3. **Analyze approximation error scaling**: Systematically vary the basis function dimension and measure the resulting impact on convergence rates and constraint satisfaction to validate the theoretical error bounds.