---
ver: rpa2
title: On Initializing Transformers with Pre-trained Embeddings
arxiv_id: '2407.12514'
source_url: https://arxiv.org/abs/2407.12514
tags:
- embeddings
- pre-trained
- xavier
- embedding
- glove
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates why pre-trained embeddings sometimes underperform
  random initialization in transformer models. The authors identify two key factors:
  (1) distributional mismatch between pre-trained embeddings and optimal parameter
  ranges, and (2) interactions between embeddings and position encodings.'
---

# On Initializing Transformers with Pre-trained Embeddings
## Quick Facts
- **arXiv ID:** 2407.12514
- **Source URL:** https://arxiv.org/abs/2407.12514
- **Reference count:** 18
- **Primary result:** Pre-trained embeddings with high variance underperform random initialization due to distributional mismatch; standardization improves performance

## Executive Summary
This paper investigates why pre-trained word embeddings sometimes perform worse than random initialization in transformer models. The authors identify two key factors: distributional mismatch between pre-trained embeddings and optimal parameter ranges, and interactions between embeddings and position encodings. They demonstrate that embeddings with larger variance (like GloVe, T5, mT5) perform worse than Xavier-initialized embeddings, while those with similar variance (like BERT, mBERT) perform comparably or better. Standardizing pre-trained embeddings to match Xavier's distribution significantly improves performance for embeddings with high variance.

## Method Summary
The authors conducted controlled experiments across multiple transformer architectures and datasets, comparing pre-trained embeddings (GloVe, BERT, T5 variants) against Xavier-initialized embeddings. They analyzed variance distributions, conducted synthetic experiments with shuffled position encodings, and tested standardized pre-trained embeddings. The study primarily focused on multilingual settings using mT5, mBERT, and XLM-R models. They measured performance across different initialization strategies and examined the interaction between pre-trained embeddings and position encodings through ablation and synthetic experiments.

## Key Results
- Pre-trained embeddings with larger variance (GloVe, T5, mT5) perform worse than Xavier-initialized embeddings
- Embeddings with similar variance to Xavier (BERT, mBERT) perform comparably or better than random initialization
- Standardizing pre-trained embeddings to match Xavier's distribution significantly improves performance for high-variance embeddings
- Positional encodings can reduce the effectiveness of pre-trained embeddings by "absorbing" position information
- Pre-trained embeddings retain some semantic benefits even when position encodings are shuffled

## Why This Works (Mechanism)
The effectiveness of pre-trained embeddings depends on their alignment with the optimal parameter distribution for transformer models. Xavier initialization provides embeddings with a specific variance range that works well with transformers' architecture and training dynamics. When pre-trained embeddings have significantly different variance distributions, they can destabilize training or prevent the model from learning effectively. Position encodings can interfere with pre-trained embeddings by capturing position information that the embeddings themselves might provide, creating redundancy or conflicting signals during training.

## Foundational Learning
- **Xavier/Glorot initialization**: A weight initialization method that sets variance based on layer size to maintain signal propagation; needed to understand the baseline comparison and optimal embedding distribution
- **Positional encoding in transformers**: Learnable or fixed patterns added to embeddings to provide sequence position information; needed to understand the interaction between embeddings and positional information
- **Variance distribution analysis**: Statistical comparison of embedding parameter distributions; needed to identify the mismatch between pre-trained and optimal initialization ranges
- **Synthetic experiment design**: Using shuffled position encodings to isolate effects; needed to test hypotheses about positional encoding interactions
- **Multilingual model architectures**: Understanding mT5, mBERT, XLM-R differences; needed to contextualize results across different model types
- **Embedding standardization**: Adjusting pre-trained embeddings to match target distributions; needed to evaluate the proposed solution

## Architecture Onboarding
**Component Map**: Pre-trained Embeddings -> Transformer Encoder -> Position Encodings -> Output Layer
**Critical Path**: Embedding initialization -> Layer normalization -> Self-attention computation -> Feed-forward network
**Design Tradeoffs**: 
- High-variance embeddings provide rich semantic information but may destabilize training
- Xavier initialization provides stable training but may lack semantic richness
- Position encodings can compensate for semantic information in embeddings but may create redundancy
**Failure Signatures**: 
- Poor early training convergence with high-variance embeddings
- Reduced performance when position encodings and embeddings conflict
- Inconsistent results across different initialization strategies
**First Experiments**:
1. Compare GloVe embeddings with and without standardization across multiple transformer architectures
2. Test shuffled position encodings with standardized vs. non-standardized pre-trained embeddings
3. Evaluate the interaction between embedding variance and model depth on training stability

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily focuses on multilingual settings, leaving uncertainty about monolingual generalization
- Positional encoding interaction analysis relies on synthetic experiments that may not capture real-world degradation patterns
- Standardization solution requires per-model calibration, raising practical scalability concerns
- Limited testing across diverse embedding types beyond the current scope

## Confidence
- **High confidence**: The core finding that distributional mismatch explains pre-trained embedding underperformance, supported by controlled experiments comparing Xavier initialization with standardized pre-trained embeddings
- **Medium confidence**: The claim that positional encodings reduce pre-trained embedding effectiveness, as this relies on indirect evidence from shuffled position experiments rather than direct ablation studies
- **Medium confidence**: The practical recommendation to standardize pre-trained embeddings, which works well in tested cases but may not generalize to all embedding types or model architectures

## Next Checks
1. Test standardization approaches across diverse embedding types (Word2Vec, FastText, ELMo) beyond the current scope of GloVe, BERT, and T5 variants
2. Conduct ablation studies isolating positional encoding effects by comparing models with and without position encodings across multiple datasets
3. Evaluate the long-term training dynamics of standardized pre-trained embeddings to determine if initial performance gains persist throughout training