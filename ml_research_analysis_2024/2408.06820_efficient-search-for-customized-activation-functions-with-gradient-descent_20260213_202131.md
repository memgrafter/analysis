---
ver: rpa2
title: Efficient Search for Customized Activation Functions with Gradient Descent
arxiv_id: '2408.06820'
source_url: https://arxiv.org/abs/2408.06820
tags:
- search
- activation
- functions
- activations
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for automatically searching for task-specific
  activation functions by adapting gradient-based neural architecture search techniques
  to the space of activation functions. The key idea is to model activation functions
  as computational cells composed of basic mathematical operations, and then optimize
  these cells using a bi-level optimization framework similar to DARTS.
---

# Efficient Search for Customized Activation Functions with Gradient Descent

## Quick Facts
- arXiv ID: 2408.06820
- Source URL: https://arxiv.org/abs/2408.06820
- Reference count: 40
- Primary result: Discovered activation functions consistently outperform standard baselines across ResNet, ViT, and GPT model families with minimal search cost

## Executive Summary
This paper introduces a gradient-based method for automatically discovering task-specific activation functions by adapting neural architecture search techniques to the space of activation functions. The approach models activations as computational cells composed of basic mathematical operations and optimizes them using a bi-level optimization framework. The method demonstrates strong performance across multiple model families (ResNet, Vision Transformers, GPT) and shows excellent transferability to larger models and new datasets, while requiring only a few function evaluations compared to thousands needed by previous methods.

## Method Summary
The method searches for activation functions by treating them as computational cells with continuous relaxation, allowing gradient descent to optimize both activation parameters and network weights through a bi-level optimization framework. Key innovations include warm-starting with standard activations, constraining unbounded operations through clipping to prevent divergence, progressive shrinking of the search space to improve efficiency, and using variance reduction sampling. The search space consists of combinations of unary and binary operations that are continuously relaxed through weighted sums, enabling efficient gradient-based optimization.

## Key Results
- ResNet20 on CIFAR10: Discovered activations achieve 91.87-92.148% accuracy vs 91.81% for ReLU
- ViT-tiny on CIFAR10: Discovered activations achieve 91.634-92.228% accuracy vs 91.474% for GELU
- miniGPT on TinyStories: Discovered activations achieve lower loss than GELU across three model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based search can efficiently explore the space of activation functions by treating them as computational cells with continuous relaxation.
- Mechanism: The search space is modeled as combinations of basic mathematical operations (unary and binary) that are continuously relaxed through weighted sums, allowing gradient descent to optimize activation parameters.
- Core assumption: The space of activation functions can be effectively represented as a DAG of basic operations that is rich enough to capture useful functions while remaining small enough for efficient search.
- Evidence anchors:
  - [abstract]: "leverage recent advancements in gradient-based search techniques for neural architectures to efficiently identify high-performing activation functions"
  - [section]: "the space of activation functions is defined as a combination of unary and binary operations, which form a scalar function f"
  - [corpus]: Weak evidence - no directly comparable papers found in corpus
- Break condition: If the search space is too constrained to represent useful activation functions, or too large for efficient gradient-based optimization.

### Mechanism 2
- Claim: Warm-starting and progressive shrinking address the instability issues in gradient-based activation function search.
- Mechanism: Initial warm-starting with standard activations ensures reasonable initialization, while progressive shrinking prunes less important operations over time, improving both stability and search efficiency.
- Core assumption: The activation function search space has a hierarchical structure where simpler functions emerge from more complex ones through pruning.
- Evidence anchors:
  - [section]: "Warmstarting the search... ensures initializing the search with reasonable settings for both the network weights and the activation function parameters"
  - [section]: "Progressive shrinking... improves efficacy of the approach but further expedites the search process"
  - [corpus]: No direct evidence in corpus for progressive shrinking in activation function search
- Break condition: If warm-starting is insufficient to prevent divergence, or if progressive shrinking removes operations that are actually important for the optimal activation.

### Mechanism 3
- Claim: Constraining unbounded operations prevents divergence during gradient-based search.
- Mechanism: Unbounded operations that could cause exploding gradients are clipped to a threshold value, maintaining numerical stability during optimization.
- Core assumption: Unbounded operations are the primary source of divergence in activation function search, and simple clipping is sufficient to maintain stability.
- Evidence anchors:
  - [section]: "To address this issue, we regularize the search space by constraining the unbounded operations in the search space"
  - [section]: "operation outputs y with magnitude beyond a threshold |y| > ℓ will be set to y = ℓ sign(y)"
  - [corpus]: No direct evidence in corpus for this specific constraint mechanism
- Break condition: If clipping causes loss of expressiveness needed for optimal activation functions, or if other sources of instability emerge.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) techniques
  - Why needed here: The paper adapts NAS methods for activation function search, requiring understanding of DARTS, DrNAS, and bi-level optimization
  - Quick check question: What is the key difference between DARTS and DrNAS in terms of how they handle architecture parameters?

- Concept: Computational cells and search spaces
  - Why needed here: The search space is defined as a computational cell composed of basic operations, requiring understanding of DAG-based search spaces
  - Quick check question: How does continuous relaxation of discrete operations enable gradient-based optimization in search spaces?

- Concept: Bi-level optimization framework
  - Why needed here: The search optimizes activation parameters at the upper level while training network weights at the lower level
  - Quick check question: What are the two nested optimization problems in bi-level optimization for activation function search?

## Architecture Onboarding

- Component map:
  - Search cell: Computational graph of unary and binary operations
  - Optimization framework: Bi-level optimization with warm-starting and progressive shrinking
  - Constraint mechanism: Clipping for unbounded operations
  - Distribution sampling: Dirichlet distribution for architecture parameters
  - Transfer evaluation: Testing discovered activations on different models and datasets

- Critical path:
  1. Initialize search cell with standard activation
  2. Warm-start by optimizing network weights while keeping activation fixed
  3. Perform bi-level optimization with progressive shrinking
  4. Discretize final activation function
  5. Evaluate and transfer to other models/datasets

- Design tradeoffs:
  - Search space expressiveness vs. optimization efficiency
  - Warm-starting duration vs. exploration capability
  - Constraint threshold vs. activation function expressiveness
  - Search cell size vs. computational cost

- Failure signatures:
  - Divergence during search (often due to unbounded operations)
  - Poor performance on evaluation datasets (possibly due to over-fitting to search dataset)
  - Inability to find better activations than baselines (possibly due to inadequate search space)

- First 3 experiments:
  1. Run basic search on ResNet20/CIFAR10 with default parameters to verify the pipeline works
  2. Test the effect of warm-starting duration by comparing searches with different warm-start epochs
  3. Evaluate the impact of constraint threshold by running searches with different clipping values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of discovered activation functions vary with different search space designs?
- Basis in paper: [inferred] The paper mentions using a search space of basic mathematical operations and existing activation functions, but does not explore alternative search space designs or their impact on performance.
- Why unresolved: The paper focuses on demonstrating the effectiveness of gradient-based search for activation functions, rather than comparing different search space designs. The choice of search space could significantly impact the quality and efficiency of discovered activations.
- What evidence would resolve it: Conducting experiments with alternative search space designs (e.g., different sets of operations, different cell structures) and comparing their performance on the same tasks would provide insights into the importance of search space design.

### Open Question 2
- Question: How do the discovered activation functions generalize to other architectures beyond ResNet, ViT, and GPT?
- Basis in paper: [explicit] The paper demonstrates strong transferability of discovered activations to larger models of the same type and new datasets, but does not explore their performance on completely different architectures.
- Why unresolved: While the paper shows promising results for transferability within the same architecture family, it remains unclear how well these activations would perform on other types of neural networks, such as recurrent networks or graph neural networks.
- What evidence would resolve it: Applying the discovered activations to a diverse set of architectures and evaluating their performance would reveal the generalizability of the approach across different network types.

### Open Question 3
- Question: What is the impact of the search algorithm's hyperparameters on the quality of discovered activation functions?
- Basis in paper: [inferred] The paper mentions using specific hyperparameters for the gradient-based search algorithm, but does not investigate the sensitivity of the results to these choices.
- Why unresolved: The performance of the discovered activations could be influenced by the choice of hyperparameters, such as learning rates, batch sizes, or the number of search epochs. Understanding this sensitivity would help in optimizing the search process.
- What evidence would resolve it: Conducting ablation studies or systematic hyperparameter searches to assess the impact of different hyperparameter settings on the quality of discovered activations would provide insights into the robustness of the approach.

## Limitations

- The search space is constrained to combinations of predefined unary and binary operations, potentially missing more complex activation functions
- Progressive shrinking might prematurely remove operations that are important for optimal activation functions
- The clipping mechanism for unbounded operations may restrict the expressiveness of discovered activations

## Confidence

- **High confidence**: The efficiency claims are well-supported by empirical results showing few function evaluations compared to prior methods
- **Medium confidence**: Transferability claims are supported but limited to specific model families (ResNets, ViTs, GPT variants)
- **Low confidence**: The assertion that discovered activations will generalize to arbitrary new architectures and tasks is largely speculative

## Next Checks

1. **Search Space Coverage**: Systematically test whether the current search space can represent known effective activation functions (Swish, Mish, etc.) when initialized appropriately, to validate the space's expressiveness.

2. **Transferability Boundaries**: Evaluate discovered activations on architectures significantly different from the search models (e.g., MobileNets, EfficientNets, or RNNs) to test generalization beyond the tested model families.

3. **Scaling Analysis**: Investigate how search performance scales with model size by conducting searches on larger models (ResNet50, ViT-base) and comparing results to baseline activations, addressing the gap in current evaluation.