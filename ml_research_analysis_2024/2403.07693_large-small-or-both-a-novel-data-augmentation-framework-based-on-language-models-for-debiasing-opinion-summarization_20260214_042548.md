---
ver: rpa2
title: 'Large, Small or Both: A Novel Data Augmentation Framework Based on Language
  Models for Debiasing Opinion Summarization'
arxiv_id: '2403.07693'
source_url: https://arxiv.org/abs/2403.07693
tags:
- data
- sentiment
- text
- language
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the issue of sentiment bias in opinion summarization,
  where models struggle to generate negative summaries for negative reviews due to
  the dataset's inherent bias toward positive reviews. The authors propose LASS, a
  novel data augmentation framework that combines large and small language models
  to alleviate this bias.
---

# Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization

## Quick Facts
- arXiv ID: 2403.07693
- Source URL: https://arxiv.org/abs/2403.07693
- Authors: Yanyue Zhang; Pengfei Li; Yilong Lai; Deyu Zhou; Yulan He
- Reference count: 25
- Primary result: LASS improves negative sentiment accuracy by up to 51% without harming ROUGE scores

## Executive Summary
This paper addresses sentiment bias in opinion summarization, where models struggle to generate negative summaries due to dataset bias toward positive reviews. The authors propose LASS, a hybrid data augmentation framework combining large and small language models. The approach uses large models for generating high-quality counterfactual examples through optimized prompts, then trains a disentangled autoencoder on this data to generate large volumes of synthetic data. The generated data is filtered using perplexity and sentiment classification to ensure quality. Experiments on Amazon and Yelp datasets show significant improvements in negative sentiment accuracy (up to 51%) while maintaining ROUGE scores, achieving comparable results to large models only but at lower cost.

## Method Summary
LASS is a novel data augmentation framework that combines large and small language models to debias opinion summarization. The method uses a large language model to generate counterfactual data by rewriting positive reviews into negative ones using optimized prompts. A disentangled reconstruction autoencoder (Dis-AE) is then trained on this generated data to learn sentiment and content representations. The Dis-AE generates large volumes of synthetic data by recombining different sentiment and content representations. Quality control is implemented through filtering based on perplexity and sentiment classification. The framework aims to address the limitations of using only large language models (cost and potential toxicity) while maintaining high-quality augmentation.

## Key Results
- LASS achieves up to 51% increase in negative sentiment accuracy
- Maintains ROUGE-1, ROUGE-2, and ROUGE-L scores comparable to baseline models
- Outperforms traditional data augmentation methods and matches the performance of using only large language models
- Demonstrates effectiveness on both Amazon and Yelp datasets with imbalanced sentiment distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The disentangled autoencoder can generate high-quality synthetic data by separating sentiment and content representations.
- Mechanism: The model learns to disentangle sentiment and content features from paired reviews (positive and negative versions of the same text). By combining different sentiment representations with content representations, it can generate new text that maintains coherent content while expressing a different sentiment.
- Core assumption: Sentiment and content can be effectively separated and recombined without loss of semantic meaning.
- Evidence anchors: [abstract] "a disentangle reconstruction model is trained based on the generated data", [section] "a disentangled autoencoder is proposed to obtain the sentiment and content representation through reconstruction"
- Break condition: If sentiment and content representations cannot be properly disentangled, the generated text will be incoherent or express unintended sentiments.

### Mechanism 2
- Claim: Combining large and small language models provides better data quality at lower cost than using only large models.
- Mechanism: Large models generate a small number of high-quality counterfactual examples through carefully designed prompts. These examples train a smaller disentanglement model that can then generate large volumes of synthetic data. This reduces the computational cost while maintaining quality.
- Core assumption: A small number of high-quality examples is sufficient to train a model that can generate diverse, quality synthetic data.
- Evidence anchors: [abstract] "data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs", [section] "we propose LASS, a novel framework based on both LArge and Small language models for debiaSing opinion summarization"
- Break condition: If the small model cannot learn effective disentanglement from limited examples, or if the quality control filters are too restrictive, the approach may not scale effectively.

### Mechanism 3
- Claim: Sentiment classification and perplexity filtering ensure the quality of generated synthetic data.
- Mechanism: After generation, the synthetic data is filtered based on two criteria: perplexity (to ensure readability and fluency) and sentiment classification (to ensure correct sentiment polarity). This prevents low-quality or incorrectly labeled examples from entering the training data.
- Core assumption: Perplexity and sentiment classification are reliable indicators of data quality for opinion summarization.
- Evidence anchors: [abstract] "filtering based on confusion degree and sentiment classification to ensure quality", [section] "Due to the limitation of small model generation ability, the generated text may be unreadable, or with incorrect sentiment polarity. Therefore, we add a data filtering process based on perplexity and sentiment classification"
- Break condition: If the sentiment classifier is not accurate or if the perplexity threshold is set incorrectly, the filtering process may remove too many valid examples or retain low-quality ones.

## Foundational Learning

- Concept: Data augmentation for debiasing
  - Why needed here: The dataset has significant sentiment bias (70%+ positive reviews), which causes models to struggle with generating negative summaries. Data augmentation helps balance the sentiment distribution.
  - Quick check question: What is the primary reason for using data augmentation in this context?

- Concept: Counterfactual data generation
  - Why needed here: To create synthetic negative reviews from positive ones while maintaining the same content, enabling balanced training data.
  - Quick check question: How does counterfactual data generation help address sentiment bias?

- Concept: Disentangled representation learning
  - Why needed here: To separate sentiment and content features so they can be recombined to generate new synthetic data with controlled sentiment.
  - Quick check question: Why is it important to disentangle sentiment from content in this framework?

## Architecture Onboarding

- Component map: Large Model → Prompt Optimization → Counterfactual Generation → Dis-AE Training → Data Reproduction → Filtering → Augmented Dataset
- Critical path: Large Language Model for generating initial counterfactual pairs → Disentangled Autoencoder (Dis-AE) for learning representation separation and recombination → Filtering system using perplexity and sentiment classification
- Design tradeoffs: Using a hybrid approach balances quality and cost, but requires careful prompt design and quality control. Pure LLM generation would be higher quality but more expensive; pure small model generation would be cheaper but potentially lower quality.
- Failure signatures: Poor negative sentiment accuracy despite data augmentation suggests issues with either the generation quality, disentanglement learning, or filtering thresholds. Degradation in positive sentiment accuracy may indicate the model is being confused by synthetic data.
- First 3 experiments:
  1. Test prompt optimization effectiveness by comparing success rates of counterfactual generation with basic vs. optimized prompts
  2. Validate Dis-AE training by measuring counterfactual reconstruction ROUGE scores and perplexity on validation set
  3. Evaluate filtering thresholds by analyzing the percentage of generated samples that pass filtering and their quality characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of synthetic data required for training the Dis-AE model to achieve the best balance between perplexity and counterfactual reconstruction ROUGE score?
- Basis in paper: [explicit] The paper discusses the impact of different sizes of training data on the quality of generated text, measured by perplexity and counterfactual reconstruction ROUGE score, in Section 4.5.
- Why unresolved: The paper does not provide a definitive answer on the optimal amount of synthetic data, only indicating that quality improves steadily with increased data volume up to 200k, after which instabilities are observed.
- What evidence would resolve it: A systematic study varying the amount of synthetic data beyond 200k and evaluating the impact on downstream task performance would provide a clearer answer.

### Open Question 2
- Question: How does the performance of the LASS framework compare to using only large language models (LLMs) in terms of sentiment accuracy and ROUGE scores across different summarization models and datasets?
- Basis in paper: [explicit] The paper compares the performance of LASS with GPT (LLMs only) in terms of sentiment accuracy and ROUGE scores, showing that LASS achieves comparable results but more economically.
- Why unresolved: While the paper shows that LASS is effective, it does not provide a comprehensive comparison across all summarization models and datasets to determine the generalizability of the approach.
- What evidence would resolve it: Conducting experiments with a wider range of summarization models and datasets, and comparing their performance with and without LASS, would provide a more complete picture of its effectiveness.

### Open Question 3
- Question: What are the potential risks and limitations of using large language models for data augmentation in opinion summarization, and how can they be mitigated?
- Basis in paper: [explicit] The paper mentions two disadvantages of using LLMs for data augmentation: potential issues or toxicity in the augmented data and expensive costs. It also discusses the limitations of the LASS framework in Section 5.
- Why unresolved: The paper does not provide a detailed analysis of the risks and limitations of using LLMs for data augmentation, nor does it offer comprehensive strategies for mitigating these issues.
- What evidence would resolve it: A thorough investigation of the types of issues that can arise from LLM-generated data, along with strategies for detecting and mitigating these issues, would provide a more complete understanding of the risks and limitations.

## Limitations
- Limited ablation of key components, making it difficult to assess which component drives the most improvement
- Missing critical implementation details (prompt parameters, Dis-AE architecture) limiting reproducibility
- Dataset bias generalization not thoroughly tested beyond Amazon and Yelp domains

## Confidence
- **High confidence**: The core mechanism of combining large and small language models for cost-effective data augmentation is well-supported by the experimental results showing 51% improvement in negative sentiment accuracy
- **Medium confidence**: The effectiveness of the disentangled reconstruction autoencoder is demonstrated, but the lack of architectural details and ablation studies limits full confidence in this component
- **Low confidence**: The filtering mechanism's robustness is assumed based on perplexity and sentiment classification, but the paper doesn't validate these thresholds or show sensitivity analysis

## Next Checks
1. **Component ablation study**: Run controlled experiments removing each key component (prompt optimization, Dis-AE generation, filtering) to quantify their individual contributions to the overall performance improvement
2. **Threshold sensitivity analysis**: Systematically vary perplexity and sentiment classification thresholds to determine their optimal values and assess the robustness of the filtering mechanism
3. **Cross-domain validation**: Test the framework on datasets from different domains (e.g., product reviews, movie reviews, restaurant reviews) to evaluate generalization beyond Amazon and Yelp