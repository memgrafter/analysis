---
ver: rpa2
title: Training-free Graph Neural Networks and the Power of Labels as Features
arxiv_id: '2404.19288'
source_url: https://arxiv.org/abs/2404.19288
tags:
- gnns
- tfgnns
- graph
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes training-free graph neural networks (TFGNNs)
  that can be used without training or improved with optional training for transductive
  node classification. The core idea is to use labels as features (LaF), which enhances
  the expressive power of GNNs by allowing them to represent label propagation.
---

# Training-free Graph Neural Networks and the Power of Labels as Features

## Quick Facts
- arXiv ID: 2404.19288
- Source URL: https://arxiv.org/abs/2404.19288
- Reference count: 40
- One-line primary result: TFGNNs achieve non-trivial accuracy without training and converge faster with optional training by using labels as features

## Executive Summary
This paper introduces Training-free Graph Neural Networks (TFGNNs) that can be deployed instantly without training while achieving competitive performance on node classification tasks. The key innovation is using Labels as Features (LaF), which enhances the expressive power of GNNs by enabling them to represent label propagation algorithms. The authors show that standard GNNs cannot approximate label propagation, while GNNs with LaF can achieve arbitrary precision. TFGNNs leverage this insight through special parameter initialization that embeds label propagation directly into the model, allowing for both training-free operation and faster convergence when optional training is applied.

## Method Summary
TFGNNs extend standard message passing GNNs by concatenating label vectors to node features during the transductive learning phase. The method uses special initialization patterns (zero/one patterns) that embed label propagation into the model parameters, enabling the network to compute label propagation scores through standard message passing operations without requiring optimization. This initialization can be achieved through analytical solutions or simple linear regression, making the model deployable instantly. The architecture maintains standard GNN layer structures but modifies the input layer to include label information and applies the special initialization to parameter matrices.

## Key Results
- TFGNNs achieve non-trivial accuracy on standard benchmarks without any training
- With optional training, TFGNNs converge much faster than standard GCNs
- The method provides improved robustness to feature noise compared to standard GNNs
- TFGNNs outperform existing GNNs in training-free settings while maintaining competitive performance when trained

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Labels as Features (LaF) provably enhances GNN expressive power by enabling representation of label propagation.
- Mechanism: By concatenating label vectors to node embeddings, GNNs gain access to class distribution information from neighboring nodes, allowing them to approximate the random walk-based label propagation algorithm.
- Core assumption: The transductive setting provides ground truth labels for training nodes that can be used as input features without leaking test information.
- Evidence anchors:
  - [abstract] "We show that LaF provably enhances the expressive power of graph neural networks."
  - [section 4] "Theorem 4.1. GNNs with LaF can approximate label propagation with any precision."
  - [corpus] Weak evidence - no directly relevant corpus papers found
- Break condition: If test nodes cannot be isolated from training labels during training (minibatch handling), or if graph structure prevents effective label propagation.

### Mechanism 2
- Claim: Training-free initialization embeds label propagation algorithm into GNN parameters.
- Mechanism: Special parameter initialization (zero/one patterns) ensures that untrained TFGNNs directly compute label propagation scores through message passing without requiring optimization.
- Core assumption: The recursive computation of random walk probabilities can be mapped to standard GNN message passing operations.
- Evidence anchors:
  - [section 5] "The key to TFGNNs lies in initialization... The following proposition shows that the initialized TFGNNs approximate label propagation."
  - [section 6.2] "TFGNNs achieve non-trivial accuracy without training and can be deployed instantly"
  - [corpus] Weak evidence - no directly relevant corpus papers found
- Break condition: If initialization pattern is disturbed during training, or if graph has poor connectivity preventing effective label propagation.

### Mechanism 3
- Claim: TFGNNs converge faster in optional training mode due to better initialization.
- Mechanism: Starting from a point close to label propagation solution, TFGNNs require fewer training iterations to reach optimal performance compared to randomly initialized GNNs.
- Core assumption: The label propagation solution is a good approximation of the optimal solution for many graph learning tasks.
- Evidence anchors:
  - [section 6.4] "TFGNNs converge much faster than GCNs... We hypothesize that TFGNNs converge faster because the initialized TFGNNs are in a good starting point"
  - [section 6.2] "TFGNNs can also be improved with optional training... Users can use TFGNNs with the training-free mode or train TFGNNs for few iterations"
  - [corpus] Weak evidence - no directly relevant corpus papers found
- Break condition: If the label propagation solution is far from optimal for the specific task, or if overfitting occurs during training.

## Foundational Learning

- Concept: Transductive vs Inductive learning settings
  - Why needed here: TFGNNs specifically leverage transductive property of having access to training labels as features
  - Quick check question: In transductive learning, are the same graph and node labels used for both training and testing?

- Concept: Message passing GNN architecture
  - Why needed here: TFGNNs build upon standard message passing framework with special initialization
  - Quick check question: What are the three main components of a message passing GNN layer?

- Concept: Expressive power and 1-WL test equivalence
  - Why needed here: Understanding that TFGNNs extend expressive power beyond standard GNNs that are limited by 1-WL test
  - Quick check question: What is the maximum expressive power of standard message passing GNNs in terms of graph isomorphism testing?

## Architecture Onboarding

- Component map:
  Input layer: Node features + LaF (label vectors for training nodes) -> Hidden layers: Standard GNN layers with special initialization patterns -> Output layer: Classification head

- Critical path:
  1. Initialize model with special patterns (zero/one initialization for last rows)
  2. Use model immediately without training (training-free mode)
  3. Optionally train with standard optimization for few iterations

- Design tradeoffs:
  - Depth vs. training-free performance: Deeper models perform better without training
  - Homophily requirement: Performance degrades on heterophilious graphs
  - Feature noise robustness: Better than standard GNNs due to dual information source

- Failure signatures:
  - Poor performance on heterophilious graphs
  - Overfitting if trained too long (breaking initialization structure)
  - Degradation with extreme feature noise

- First 3 experiments:
  1. Verify training-free performance on Cora dataset compared to standard GCN
  2. Test convergence speed with optional training vs. standard GCN
  3. Evaluate robustness to feature noise by adding Gaussian noise to node features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do TFGNNs perform on heterophilic graphs compared to homophilic graphs?
- Basis in paper: [inferred] from the limitations section stating that TFGNNs cannot be applied to heterophilic graphs or their performance degrades.
- Why unresolved: The paper does not provide experimental results on heterophilic graphs, and it is unclear how much the performance degrades.
- What evidence would resolve it: Experiments comparing TFGNN performance on heterophilic and homophilic graphs, with quantitative metrics such as accuracy and convergence speed.

### Open Question 2
- Question: Can the LaF technique be effectively combined with other training-free GNN approaches, such as graph echo state networks?
- Basis in paper: [explicit] from the related work section mentioning that combining LaF with graph echo state networks is an interesting future direction.
- Why unresolved: The paper does not explore this combination, and it is unclear how LaF would interact with the fixed weights of graph echo state networks.
- What evidence would resolve it: Experiments implementing a combined approach and comparing its performance to TFGNNs and graph echo state networks separately.

### Open Question 3
- Question: How does the performance of TFGNNs scale with increasing graph size and node feature dimensionality?
- Basis in paper: [inferred] from the discussion of TFGNNs' ability to handle large graphs without training, but no specific scaling analysis is provided.
- Why unresolved: The paper focuses on datasets of moderate size and does not investigate how TFGNNs perform on very large graphs or with high-dimensional features.
- What evidence would resolve it: Experiments scaling up graph size and feature dimensionality, measuring performance and computational requirements.

### Open Question 4
- Question: What is the theoretical limit of the approximation error when using LaF with GNNs for label propagation?
- Basis in paper: [explicit] from Theorem 4.1, which shows that GNNs with LaF can approximate label propagation with any precision, but does not provide a bound on the error.
- Why unresolved: The theorem proves the existence of a GNN that can approximate label propagation, but does not provide a quantitative measure of the approximation error.
- What evidence would resolve it: A theoretical analysis deriving bounds on the approximation error as a function of the number of layers and the graph structure.

## Limitations
- Method is restricted to transductive learning settings and cannot be directly applied to inductive scenarios
- Performance degrades on heterophilious graphs where label propagation is less effective
- Special initialization pattern may be fragile and can be broken during optional training
- Requires access to training labels as features, which may not be available in all settings

## Confidence
**High Confidence (8-10/10)**: The theoretical claims about expressive power enhancement through LaF are well-supported by the formal proofs in Theorem 4.1. The core mechanism of using labels as features to approximate label propagation is mathematically sound and clearly articulated.

**Medium Confidence (5-7/10)**: The empirical results showing training-free performance and faster convergence with optional training are promising but limited to a small set of benchmark datasets. The claims about robustness to feature noise and performance on heterophilious graphs require further validation across diverse graph types.

**Low Confidence (1-4/10)**: The paper lacks extensive comparison with recent GNN variants and does not thoroughly investigate the method's behavior under different graph characteristics beyond homophily/heterophily. The claims about instant deployment readiness need more rigorous testing.

## Next Checks
1. **Inductive Learning Extension**: Design an experiment testing whether the LaF mechanism can be adapted for inductive settings by using label distributions from similar nodes or through meta-learning approaches.

2. **Robustness Benchmarking**: Systematically evaluate TFGNN performance across graphs with varying levels of homophily, heterophily, and feature noise to quantify the claimed robustness benefits under controlled conditions.

3. **Initialization Stability Analysis**: Conduct ablation studies where different portions of the initialization pattern are perturbed during training to determine the minimum requirements for maintaining training-free benefits while allowing optimization.