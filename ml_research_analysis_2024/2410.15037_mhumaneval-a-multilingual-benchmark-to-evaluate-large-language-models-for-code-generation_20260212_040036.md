---
ver: rpa2
title: mHumanEval -- A Multilingual Benchmark to Evaluate Large Language Models for
  Code Generation
arxiv_id: '2410.15037'
source_url: https://arxiv.org/abs/2410.15037
tags:
- latn
- arab
- language
- code
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mHumanEval introduces the first multilingual code generation benchmark
  spanning 204 natural languages and 25 programming languages, addressing limitations
  in existing benchmarks regarding linguistic diversity and test coverage. The benchmark
  leverages machine translation methods combined with quality assurance via BERTScore
  and CometKiwi to generate high-quality prompts, with expert human translations for
  15 languages across all resource levels.
---

# mHumanEval -- A Multilingual Benchmark to Evaluate Large Language Models for Code Generation

## Quick Facts
- arXiv ID: 2410.15037
- Source URL: https://arxiv.org/abs/2410.15037
- Reference count: 20
- First multilingual code generation benchmark spanning 204 natural languages and 25 programming languages

## Executive Summary
mHumanEval introduces a comprehensive multilingual benchmark for evaluating code generation capabilities of large language models across 204 natural languages and 25 programming languages. The benchmark addresses critical gaps in existing evaluation frameworks by incorporating diverse linguistic representations and testing coverage across different resource levels. Using a combination of machine translation with quality assurance via BERTScore and CometKiwi, the benchmark generates high-quality prompts with expert human translations for 15 languages. The evaluation reveals significant performance variations across models and languages, demonstrating that multilingual pretraining and fine-tuning are essential for effective cross-lingual code generation.

## Method Summary
The benchmark leverages machine translation methods combined with quality assurance via BERTScore and CometKiwi to generate high-quality prompts. Expert human translations were performed for 15 languages across all resource levels to ensure accuracy. The evaluation framework tests state-of-the-art models across diverse language pairs, measuring performance variations in code generation tasks. The methodology focuses on single-language prompts but establishes a foundation for understanding multilingual code generation capabilities and limitations of current language models.

## Key Results
- Proprietary models like GPT-4o and Claude-3.5 maintain consistent performance even in low-resource languages
- Specialized code models like WizardCoder and DeepSeek-Coder show steep performance declines in multilingual settings
- Multilingual pretraining and fine-tuning are crucial for effective cross-lingual code generation, with base multilingual models generally outperforming specialized code models in non-English contexts

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of linguistic diversity combined with rigorous quality assurance mechanisms. By incorporating 204 natural languages and 25 programming languages, it captures the full spectrum of multilingual code generation challenges. The use of BERTScore and CometKiwi for quality validation ensures that translation artifacts don't confound model performance measurements. The expert human translations for 15 languages provide ground truth for quality assessment, while the large-scale machine translation approach enables broad coverage. This combination allows for accurate measurement of genuine model capabilities rather than translation-induced artifacts.

## Foundational Learning
- **BERTScore**: Machine learning-based metric for evaluating text generation quality by comparing contextual embeddings between candidate and reference texts; needed to validate translation quality at scale without manual review of all prompts
- **CometKiwi**: Translation quality estimation tool that predicts human assessment scores; required to identify and filter low-quality translations that could bias model performance results
- **Multilingual pretraining**: Training language models on diverse language corpora; essential for developing models that can effectively generate code across different natural languages
- **Code generation evaluation**: Systematic assessment of model ability to produce functional code from natural language prompts; fundamental for measuring practical utility of language models in software development
- **Resource-level categorization**: Classification of languages by available training data; important for understanding performance variations across high, medium, and low-resource language settings

## Architecture Onboarding

**Component Map**
Translation Pipeline (Machine Translation -> BERTScore Quality Check -> CometKiwi Validation) -> Benchmark Dataset -> Model Evaluation Framework -> Performance Analysis

**Critical Path**
Translation generation and quality validation → Benchmark construction → Model evaluation → Performance analysis and comparison

**Design Tradeoffs**
- Machine translation with automated quality checks vs. full manual translation (coverage vs. guaranteed quality)
- Single-language prompts vs. mixed-language scenarios (simpler evaluation vs. real-world applicability)
- Broad language coverage (204 languages) vs. deep expert validation (15 languages)

**Failure Signatures**
- Performance drops correlated with translation quality scores indicate benchmark validity issues
- Consistent performance gaps between models across multiple languages suggest genuine capability differences
- Large variations within language resource levels point to model architecture limitations

**First 3 Experiments**
1. Validate translation quality by comparing machine-translated prompts against expert human translations for 10+ diverse language pairs
2. Test benchmark applicability by evaluating models on additional code generation tasks beyond the current scope
3. Assess real-world relevance by evaluating models using mixed-language prompts (code with comments/documentation in different languages)

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark construction relies heavily on machine translation with post-hoc quality checks rather than comprehensive human validation across all language pairs
- Reported performance gaps may partly reflect dataset-specific characteristics rather than inherent model capabilities
- Focus on single-language prompts may not capture real-world multilingual software development scenarios

## Confidence

**High confidence**: Benchmark construction methodology and the basic finding that multilingual pretraining improves cross-lingual code generation performance

**Medium confidence**: Relative performance rankings between specific models (GPT-4o, Claude-3.5, specialized code models) require broader validation across different code generation tasks

**Medium confidence**: Conclusion about specialized code models underperforming in multilingual settings may be influenced by specific evaluation criteria and translation quality

## Next Checks
1. Conduct human expert validation of translated prompts across 10+ diverse language pairs to verify that translation quality issues aren't driving observed performance differences

2. Test the benchmark with additional code generation tasks beyond current scope to assess whether performance patterns generalize across different programming domains

3. Evaluate models using mixed-language prompts (code with comments/documentation in different languages) to better reflect real-world multilingual software development scenarios