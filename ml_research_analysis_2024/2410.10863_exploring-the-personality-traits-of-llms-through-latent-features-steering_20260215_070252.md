---
ver: rpa2
title: Exploring the Personality Traits of LLMs through Latent Features Steering
arxiv_id: '2410.10863'
source_url: https://arxiv.org/abs/2410.10863
tags:
- personality
- llms
- features
- factors
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores how personality traits emerge in large language\
  \ models (LLMs) by leveraging the framework of social determinism, which distinguishes\
  \ between long-term background factors and short-term pressures. The authors propose\
  \ a training-free method to steer LLM behavior by extracting interpretable features\
  \ from internal model activations\u2014using sparse autoencoders (SAEs) for long-term\
  \ background factors and representation-based methods for short-term pressures."
---

# Exploring the Personality Traits of LLMs through Latent Features Steering

## Quick Facts
- arXiv ID: 2410.10863
- Source URL: https://arxiv.org/abs/2410.10863
- Reference count: 39
- This paper proposes a training-free method to steer LLM behavior by extracting interpretable features from internal model activations to modify personality traits.

## Executive Summary
This paper investigates how personality traits emerge in large language models by leveraging the framework of social determinism, which distinguishes between long-term background factors and short-term pressures. The authors propose a training-free method to steer LLM behavior by extracting interpretable features from internal model activations—using sparse autoencoders (SAEs) for long-term background factors and representation-based methods for short-term pressures. These features are then used to modify the model's outputs without fine-tuning. The study applies this approach to personality tests like the Big Five Inventory and Short Dark Triad, revealing how specific factors influence traits such as agreeableness, neuroticism, and dark triad tendencies.

## Method Summary
The paper proposes a training-free approach to modify LLM behavior by extracting and steering latent features using sparse autoencoders (SAEs) for long-term background factors and representation-based methods for short-term pressures. For background factors, SAEs learn a sparse dictionary where each basis vector corresponds to a human-interpretable concept. For short-term pressures, the method contrasts activations from positive and negative stimuli to identify linear directions in activation space. These extracted features are then added to the model's residual stream or activations to steer personality traits, with coefficients tuned for each model size (Gemma-2B-Instruct and Gemma-2-9B-Instruct).

## Key Results
- SAEs successfully extracted monosemantic features for background factors like socioeconomic status and competence
- Steering coefficients significantly affect personality trait modification, with higher values producing stronger effects but risking instability
- Safety evaluations showed that enhanced background features can reduce model security, particularly for offensive content handling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-term background factors are encoded as stable, monosemantic features that can be extracted via SAEs.
- Mechanism: SAEs learn a sparse dictionary where each basis vector corresponds to a human-interpretable concept through reconstruction loss and sparsity penalty.
- Core assumption: Features in LLM activations are monosemantic and correspond to discrete concepts rather than mixed or polysemantic representations.
- Evidence anchors:
  - [abstract]: "we propose a training-free approach to modify the model's behavior by extracting and steering latent features"
  - [section]: "SAEs are well-suited for capturing long-term factors because of their ability to disentangle stable, deeply embedded features"
- Break condition: If features extracted via SAEs are found to be polysemantic (mixed concepts), the steering will produce unintended behavioral changes.

### Mechanism 2
- Claim: Short-term pressures are encoded as linear directions in activation space that can be identified via representation-based methods.
- Mechanism: By contrasting activations from positive and negative stimuli (e.g., "competent" vs "inadequate" prompts), the difference vector captures the concept direction for that pressure.
- Core assumption: Activation differences between opposing stimuli isolate the neural subspace representing the target concept.
- Evidence anchors:
  - [abstract]: "using sparse autoencoders (SAEs) for long-term background factors and representation-based methods for short-term pressures"
  - [section]: "For short-term pressure features, we adopted a representation-based method... We input this dataset through LLM and compute the normalized difference between their average l-th layer activations"
- Break condition: If the model's activation space is not linear for the concept or if the stimulus pairs are not well-matched, the extracted direction will not reliably steer behavior.

### Mechanism 3
- Claim: Adding the extracted feature vector to the model's residual stream or activations steers the generation toward the corresponding personality trait.
- Mechanism: The steering hook adds a scaled feature vector to the model's hidden states, biasing subsequent token predictions toward outputs consistent with the target trait.
- Core assumption: The model's generation process is influenced by the direction of its internal activations, and small perturbations can reliably shift behavior.
- Evidence anchors:
  - [abstract]: "We present techniques for fine-grained personality control in LLMs using interpretable features extracted through Sparse Autoencoder and representation-based methods"
  - [section]: "we employ these features to steer the LLM's output... background features are integrated into the LLM's residual stream, and pressure features are added into the corresponding activation"
- Break condition: If the steering coefficient is too large, the model may produce repetitive or nonsensical outputs; if too small, the effect is negligible.

## Foundational Learning

- Concept: Linear representation hypothesis
  - Why needed here: Understanding that personality traits can be represented as directions in activation space is fundamental to both SAE and representation-based feature extraction methods.
  - Quick check question: If the vector "man" - "woman" + "aunt" ≈ "uncle" in word embeddings, what does this suggest about how abstract concepts are represented in neural models?

- Concept: Sparse autoencoders (SAEs)
  - Why needed here: SAEs are the primary tool for extracting monosemantic features from LLM activations, which is essential for long-term background factor analysis.
  - Quick check question: In the SAE loss function L(z) = ||z - SAE(z)||²₂ + α||ReLU(zWenc + benc)||₁, what is the role of the α term?

- Concept: Feature steering via activation addition
  - Why needed here: The mechanism for modifying model behavior by adding extracted features to internal activations is central to the proposed approach.
  - Quick check question: What is the difference between steering via residual stream (for SAE features) versus steering via activation addition (for representation-based features)?

## Architecture Onboarding

- Component map: Input -> Feature extraction (SAE/representation-based) -> Steering layer (residual stream/activation) -> Modified token generation -> Evaluation (TRAIT/SafetyBench)
- Critical path:
  1. Extract features (SAE or representation-based)
  2. Select appropriate steering layer and coefficient
  3. Add feature to model's internal states
  4. Generate output and evaluate personality shift
  5. Measure safety implications

- Design tradeoffs:
  - SAE vs representation-based: SAE provides monosemantic features but requires training; representation-based is faster but may capture polysemantic directions
  - Steering coefficient: Higher coefficients produce stronger effects but risk instability; lower coefficients are safer but may be ineffective
  - Layer selection: Deeper layers capture more abstract features but may be less stable; earlier layers are more stable but capture simpler concepts

- Failure signatures:
  - Oversteering: Repetitive or nonsensical outputs when coefficient is too high
  - Understeering: No observable personality change when coefficient is too low
  - Polysemantic features: Unintended personality traits emerge when SAE features are not truly monosemantic
  - Linear assumption violation: Steering fails when the activation space does not have the expected linear structure

- First 3 experiments:
  1. Extract and verify monosemanticity of SAE features for a simple concept (e.g., socioeconomic status) using human evaluation
  2. Test steering with varying coefficients on a held-out prompt to find the optimal balance between effect and stability
  3. Compare personality test scores before and after steering for a single background factor to validate the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different SAE sizes and layer depths affect the interpretability and monotonicity of extracted personality-related features across various LLM architectures?
- Basis in paper: [explicit] The paper discusses the impact of SAE size and layer depth on feature extraction, particularly in Table 9 where different configurations are compared.
- Why unresolved: The paper focuses on specific models (Gemma-2B and Gemma-2-9B) and their SAE configurations, but does not explore a broader range of architectures or systematically vary SAE parameters to understand their general impact.
- What evidence would resolve it: Systematic experiments varying SAE sizes (e.g., 16k, 131k) and layer depths across multiple LLM architectures, with quantitative metrics for feature interpretability and monotonicity.

### Open Question 2
- Question: What is the relationship between model size, training data diversity, and the stability of personality traits when applying long-term background feature steering?
- Basis in paper: [explicit] The paper observes that Gemma-2-9B-Instruct shows more stable personality traits compared to Gemma-2B-Instruct when altering background facts, suggesting a correlation with model size and training data.
- Why unresolved: While the paper demonstrates this relationship for two specific models, it does not provide a comprehensive analysis across different model scales or quantify the exact contribution of training data diversity versus parameter count.
- What evidence would resolve it: Comparative studies across multiple model sizes with controlled training data diversity, measuring personality trait stability under identical feature steering conditions.

### Open Question 3
- Question: How do different steering coefficient values affect the balance between personality trait modification and generation quality across various personality dimensions?
- Basis in paper: [explicit] The paper discusses coefficient selection and its impact on generation quality, noting that coefficients that are too high can lead to repetitive or over-steered outputs.
- Why unresolved: The paper provides optimal coefficients for specific models but does not explore how different personality dimensions (e.g., agreeableness vs. neuroticism) might require different coefficient ranges for effective steering without degradation.
- What evidence would resolve it: Experiments varying coefficients for different personality dimensions, measuring both the magnitude of trait modification and generation quality metrics like coherence and diversity.

## Limitations
- The monosemanticity of SAE-extracted features is uncertain, with limited evidence that these features are truly discrete rather than polysemantic
- The linear representation assumption for short-term pressures may not hold for all personality concepts
- Steering effectiveness is highly sensitive to coefficient tuning with no systematic approach provided

## Confidence
- High confidence: The general framework connecting social determinism theory to LLM behavior steering is theoretically sound
- Medium confidence: The SAE feature extraction process is technically sound but specific implementation details for personality features are not fully specified
- Low confidence: The claim that steering produces reliable personality trait changes without fine-tuning is the most uncertain due to hyperparameter sensitivity

## Next Checks
1. **Feature Monosemanticity Validation**: Conduct human evaluation studies where multiple annotators rate the interpretability and uniqueness of SAE-extracted features for each background factor.
2. **Linear Representation Verification**: Systematically test the representation-based method by varying the stimulus pairs and measuring the consistency of extracted directions.
3. **Coefficient Sensitivity Analysis**: Perform a systematic grid search over steering coefficients for each factor and model scale, documenting the relationship between coefficient magnitude and both personality effect strength and output quality.