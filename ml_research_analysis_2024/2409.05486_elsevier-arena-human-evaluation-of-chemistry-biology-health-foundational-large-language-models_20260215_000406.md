---
ver: rpa2
title: 'Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational
  Large Language Models'
arxiv_id: '2409.05486'
source_url: https://arxiv.org/abs/2409.05486
tags:
- elsevier
- evaluation
- human
- answer
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of evaluating large language
  models (LLMs) in domain-specific contexts, such as biomedical applications, where
  traditional automated benchmarks fall short. The authors conducted a human evaluation
  experiment comparing a custom 8.8B parameter Elsevier LLM trained on a curated 135B
  token biomedical corpus against OpenAI's GPT-3.5-turbo (175B parameters) and Meta's
  Llama 2 (7B parameters).
---

# Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models

## Quick Facts
- arXiv ID: 2409.05486
- Source URL: https://arxiv.org/abs/2409.05486
- Authors: Camilo Thorne; Christian Druckenbrodt; Kinga Szarkowska; Deepika Goyal; Pranita Marajan; Vijay Somanath; Corey Harper; Mao Yan; Tony Scerri
- Reference count: 7
- Primary result: GPT-3.5-turbo outperformed domain-specific models in biomedical applications, but smaller curated models showed viability

## Executive Summary
This study addresses the challenge of evaluating large language models (LLMs) in domain-specific contexts, such as biomedical applications, where traditional automated benchmarks fall short. The authors conducted a human evaluation experiment comparing a custom 8.8B parameter Elsevier LLM trained on a curated 135B token biomedical corpus against OpenAI's GPT-3.5-turbo (175B parameters) and Meta's Llama 2 (7B parameters). Using an A/B testing framework with 7 human annotators, they assessed 47 prompts across biology, chemistry, and health domains based on factuality, coherence, relevance, and overall quality. Results showed a clear preference for GPT-3.5-turbo across all criteria, with moderate agreement scores (0.3-0.4 Krippendorff's alpha) indicating limited consensus among evaluators.

## Method Summary
The study employed an A/B testing framework where human annotators evaluated paired model outputs on 47 biomedical prompts across biology, chemistry, and health domains. Seven annotators assessed each pair based on factuality, coherence, relevance, and overall quality, voting for either model or declaring a tie. The evaluation used temperature t = 0.7 sampling for all models, with the custom Elsevier LLM (8.8B parameters) trained on a 135B token biomedical corpus compared against GPT-3.5-turbo (175B parameters) and Llama 2 (7B parameters).

## Key Results
- GPT-3.5-turbo was clearly preferred over both domain-specific models across all evaluation criteria
- The Elsevier model performed competitively against Llama 2 but lagged behind GPT-3.5-turbo
- Inter-rater reliability scores were moderate (0.3-0.4 Krippendorff's alpha), indicating limited consensus among evaluators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Human evaluation reveals quality dimensions that automated benchmarks miss, particularly in domain-specific contexts like biomedical applications.
- **Mechanism:** Automated benchmarks often use general metrics that don't capture domain-specific nuances, while human evaluators can assess factuality, coherence, relevance, and overall quality in context.
- **Core assumption:** Human evaluators can reliably assess complex domain-specific criteria better than automated metrics.
- **Evidence anchors:**
  - [abstract] "The quality and capabilities of large language models cannot be currently fully assessed with automated, benchmark evaluations. Instead, human evaluations that expand on traditional qualitative techniques from natural language generation literature are required."
  - [section 1] "In some domains such as the life sciences, due to regulation, there traditionally has been a preference in grounding the outputs of predictive models text in well-documented, highly curated, or copyrighted data."
- **Break condition:** If evaluators lack domain expertise or if evaluation criteria are not well-defined, the mechanism fails.

### Mechanism 2
- **Claim:** Domain-specific training on curated datasets can produce viable alternatives to general-purpose models, even with smaller parameter counts.
- **Mechanism:** Focused training on high-quality, domain-specific data allows smaller models to capture specialized knowledge that larger, general models may not prioritize.
- **Core assumption:** Curated biomedical data provides sufficient signal for effective model training, despite smaller scale.
- **Evidence anchors:**
  - [abstract] "Results showed a clear preference for GPT-3.5-turbo across all criteria... But at the same time, indicate that for less massive models training on smaller but well-curated training sets can potentially give rise to viable alternatives in the biomedical domain."
  - [section 2] "The Elsevier corpus (see Table 1) was built with scholarly documents, and in particular journal papers and patents (37% biomedical journals and 64% biomedical patents)."
- **Break condition:** If curated data lacks breadth or if domain knowledge requires scale beyond what curated datasets provide.

### Mechanism 3
- **Claim:** A/B testing frameworks with blinded model comparisons reduce evaluator bias and provide reliable preference signals.
- **Mechanism:** By comparing model outputs side-by-side without identifying information, evaluators make decisions based on content quality rather than brand recognition or preconceptions.
- **Core assumption:** Blinding evaluators prevents bias and allows fair comparison of model outputs.
- **Evidence anchors:**
  - [section 2] "Judges (annotators) were thereafter asked to cast a vote along the following 4 criteria... in favor of either generation, or else declare a tie. Prompts were moreover grouped by domain... All A/B tests had the model trained on Elsevier data as root comparison. LLM completions were generated via vanilla temperature-based sampling with temperature t = 0.7."
  - [section 2] "As expected, judges expressed a clear preference for gold set prompts, but also for powerful LLMs such as GPT-3.5-turbo, and this across all criteria."
- **Break condition:** If evaluators can infer model identity from output characteristics, or if comparison criteria are unclear.

## Foundational Learning

- **Concept:** Krippendorff's alpha for inter-rater reliability
  - **Why needed here:** To measure agreement among human evaluators and assess the reliability of human evaluation results
  - **Quick check question:** What does a Krippendorff's alpha score of 0.3-0.4 indicate about evaluator agreement?

- **Concept:** A/B testing methodology for model comparison
  - **Why needed here:** To systematically compare model outputs based on specific quality criteria
  - **Quick check question:** Why is it important to pair model outputs with gold standard answers in A/B testing?

- **Concept:** Domain-specific prompt engineering
  - **Why needed here:** To create evaluation prompts that accurately represent real-world use cases in biomedical applications
  - **Quick check question:** What types of prompts were used in this evaluation (open vs. closed book question answering)?

## Architecture Onboarding

- **Component map:** Elsevier Arena (Flask-based A/B testing platform) -> Evaluation prompts (141 total) -> Three models (Elsevier LLM, Llama 2 7B, GPT-3.5-turbo) -> Human evaluators (7 annotators) -> Scoring criteria (factuality, coherence, relevance, overview)
- **Critical path:** Prompt generation -> Model output generation -> A/B test pairing -> Human evaluation -> Score aggregation -> Analysis
- **Design tradeoffs:** Smaller, curated training corpus vs. larger general corpus; local model deployment vs. API access; custom evaluation platform vs. existing solutions
- **Failure signatures:** Low inter-rater reliability scores (Krippendorff's alpha < 0.7), evaluator fatigue due to long prompts, bias from unblinded comparisons
- **First 3 experiments:**
  1. Replicate the comparison using a subset of prompts to verify model preferences before full evaluation
  2. Test different temperature settings (e.g., t=0.7 vs. t=1.0) to see impact on evaluator preferences
  3. Run a pilot with domain experts to validate prompt quality and evaluation criteria before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the quality of biomedical LLMs trained on domain-specific data compare to those trained on larger, more diverse datasets?
- **Basis in paper:** [explicit] The paper compares a custom 8.8B parameter Elsevier LLM trained on a curated 135B token biomedical corpus against GPT-3.5-turbo and Llama 2, showing GPT-3.5-turbo's preference across all criteria.
- **Why unresolved:** While the study provides insights into the performance of domain-specific models, it doesn't explore the long-term impact of training on smaller, curated datasets versus larger, more diverse datasets on model quality and applicability.
- **What evidence would resolve it:** Comparative studies over extended periods, assessing model performance across various biomedical tasks and real-world applications, would help determine the sustainability and scalability of domain-specific training approaches.

### Open Question 2
- **Question:** What are the implications of low inter-rater reliability (IRR) scores on the validity of human evaluation results for LLMs in biomedical domains?
- **Basis in paper:** [explicit] The paper reports low Krippendorff's alpha scores (0.3-0.4 range) indicating limited consensus among evaluators.
- **Why unresolved:** The study acknowledges the low IRR scores but doesn't fully explore their impact on the reliability and generalizability of the evaluation results.
- **What evidence would resolve it:** Further research with larger evaluator groups and more diverse evaluation prompts could provide insights into whether the low IRR scores are due to the complexity of the domain or other factors, thus clarifying their impact on evaluation validity.

### Open Question 3
- **Question:** How can human evaluation methods be improved to better capture quality dimensions that automated benchmarks miss in biomedical LLMs?
- **Basis in paper:** [explicit] The paper highlights the need for human evaluation to measure quality dimensions like fluency, accuracy, and relevance, which are often poorly captured by current benchmarks.
- **Why unresolved:** While the paper demonstrates the use of human evaluation, it doesn't propose specific improvements or alternative methods to enhance the capture of these quality dimensions.
- **What evidence would resolve it:** Developing and testing new evaluation frameworks or criteria that incorporate domain-specific expertise and diverse evaluation metrics could provide a more comprehensive assessment of LLM quality in biomedical contexts.

## Limitations

- The study's inter-rater reliability scores (Krippendorff's alpha 0.3-0.4) indicate only moderate agreement among evaluators, suggesting potential inconsistencies in how quality criteria were applied
- The evaluation focused on a relatively small sample of 47 prompts, which may not fully represent the breadth of biomedical applications
- The custom Elsevier LLM was trained on a proprietary corpus with unclear curation methodology, making direct replication challenging

## Confidence

- **High Confidence:** The finding that GPT-3.5-turbo outperformed domain-specific models across all evaluation criteria is well-supported by the experimental data and consistent across all 47 prompts
- **Medium Confidence:** The observation that the Elsevier model performed competitively against Llama 2 suggests domain-focused training provides value, but the proprietary nature of the training data limits broader validation
- **Low Confidence:** The generalizability of these results to other biomedical domains or model architectures is uncertain due to the specific evaluation setup and limited prompt diversity

## Next Checks

1. **Replicate with expanded prompt set:** Conduct the A/B testing with a larger, more diverse set of biomedical prompts (target 100+) to better assess model performance across the full spectrum of potential use cases
2. **Expert evaluator validation:** Run the same evaluation with domain experts (PhD-level biomedical researchers) to determine if professional knowledge affects model preferences and inter-rater reliability
3. **Fine-tuning impact analysis:** Compare the custom Elsevier LLM against the same base model fine-tuned on the Elsevier corpus using standardized evaluation metrics to isolate the effect of domain-specific training