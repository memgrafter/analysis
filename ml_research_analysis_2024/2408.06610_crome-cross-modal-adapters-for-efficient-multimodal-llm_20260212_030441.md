---
ver: rpa2
title: 'CROME: Cross-Modal Adapters for Efficient Multimodal LLM'
arxiv_id: '2408.06610'
source_url: https://arxiv.org/abs/2408.06610
tags:
- crome
- arxiv
- training
- image
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CROME introduces a cross-modal adapter that fuses visual and textual
  representations before LLM input, avoiding costly LLM retraining while maintaining
  performance. The adapter uses gated linear units for modality-specific down-projection
  and weight-sharing for up-projection, with only ~5M parameters trained during task-specific
  fine-tuning.
---

# CROME: Cross-Modal Adapters for Efficient Multimodal LLM

## Quick Facts
- **arXiv ID**: 2408.06610
- **Source URL**: https://arxiv.org/abs/2408.06610
- **Reference count**: 40
- **Primary result**: CROME achieves 41.2% average accuracy on 8 MLLM benchmarks while training only ~5M parameters during task-specific fine-tuning

## Executive Summary
CROME introduces a cross-modal adapter that fuses visual and textual representations before LLM input, avoiding costly LLM retraining while maintaining performance. The adapter uses gated linear units for modality-specific down-projection and weight-sharing for up-projection, with only ~5M parameters trained during task-specific fine-tuning. On 8 MLLM benchmarks, CROME-Vicuna13B achieves 41.2% average accuracy, outperforming open-source baselines and approaching commercial models like GPT-4V and Gemini Pro Vision. Fine-tuning CROME on ScienceQA-Image and AI2D datasets yields 93.2% and 75.3% accuracy respectively, surpassing task-specific specialist methods while training only 2.5% of their parameters.

## Method Summary
CROME is a cross-modal adapter that sits between a frozen vision encoder (ViT-G/14) and a frozen LLM (Vicuna or Flan-T5). It uses a gated linear unit mechanism for down-projection and weight-sharing for up-projection, with skip connections. The model is trained in three stages: pretraining on image-caption pairs (5 epochs), instruction tuning on image-instruction pairs (2M iterations, 4 epochs), and optional task-specific fine-tuning where only the adapter parameters are updated. The architecture leverages a pretrained Q-Former from InstructBLIP and keeps the LLM frozen throughout to preserve reasoning capabilities while adding multimodal understanding through the adapter.

## Key Results
- CROME-Vicuna13B achieves 41.2% average accuracy on 8 MLLM benchmarks, outperforming open-source baselines
- Task-specific fine-tuning on ScienceQA-Image yields 93.2% accuracy, surpassing specialist methods while training only 2.5% of their parameters
- Zero-shot performance approaches commercial models like GPT-4V and Gemini Pro Vision on certain benchmarks
- The adapter trains only ~5M parameters during task-specific fine-tuning while keeping the 7B/13B LLM frozen

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal adapter alignment before LLM input avoids catastrophic forgetting of LLM reasoning capabilities.
- Mechanism: By fusing visual and textual representations in a lightweight adapter layer prior to LLM ingestion, CROME preserves the frozen LLM's pretrained language understanding and reasoning skills while still enabling effective multimodal comprehension.
- Core assumption: The pretrained LLM has sufficient flexibility in its input processing to accommodate cross-modal representations fused by the adapter.
- Evidence anchors:
  - [abstract] "This lightweight adapter, trained with minimal parameters, enables efficient cross-modal understanding. Notably, CROME demonstrates superior zero-shot performance... while maintaining generalization on text understanding and reasoning tasks."
  - [section 3.1] "Crucially, unlike typical adapter placement after feed-forward and self-attention layers in Transformers, this module facilitates the fusion of textual and visual representations before they enter into the LLM."
  - [corpus] Weak evidence - related works focus on parameter-efficient fine-tuning but not specifically on pre-LLM alignment to prevent catastrophic forgetting.

### Mechanism 2
- Claim: Gated linear units in the adapter's down-projection control information flow and emphasize relevant multimodal relationships.
- Mechanism: The component-wise product of sigmoid-activated linear transformations (Wg) with another linear transformation (Wd) creates a gating mechanism that dynamically weights which features pass through, allowing the adapter to learn which cross-modal interactions are most useful.
- Core assumption: The gating mechanism can effectively learn to prioritize relevant multimodal feature combinations during training.
- Evidence anchors:
  - [section 3.1] "in the down-projection unit we use the component-wise product of two linear transformations named as Wd ∈ Rd×m and Wg ∈ Rd×m where the input one of which is sigmoid-activated... This gating mechanism helps the adapter control the flow of information, potentially emphasizing the most useful and relevant multimodal relationships."
  - [corpus] Weak evidence - Gated linear units are used in transformer feed-forward layers but their effectiveness specifically for cross-modal adapter gating is not well-established in related literature.

### Mechanism 3
- Claim: Weight-sharing in the up-projection unit encourages learning of cross-modal relations between modalities.
- Mechanism: By using the same weight matrix (Wu) to project both the image and text branch outputs back to the original dimension, the adapter is forced to learn representations that are compatible with each other, promoting cross-modal alignment.
- Core assumption: Weight-sharing creates sufficient regularization to encourage meaningful cross-modal feature alignment without overly constraining the model.
- Evidence anchors:
  - [section 3.1] "the up-projection unit uses a weight-sharing mechanism between the two modalities where the m-dimensional vector z ∈ Rm is projected back to d input dimensions via Wu ∈ Rm×d, in order to better encourage learning of cross-modal relations."
  - [corpus] Weak evidence - While weight-sharing is used in adapter literature, its specific application to encourage cross-modal relations is not well-documented.

## Foundational Learning

- Concept: Cross-modal representation fusion
  - Why needed here: CROME's core innovation is fusing visual and textual representations before LLM input, which requires understanding how different modalities can be meaningfully combined.
  - Quick check question: How does the gated linear unit in CROME's down-projection differ from a standard linear projection in terms of information flow control?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: CROME trains only ~5M parameters (the adapter) while keeping the LLM and vision encoder frozen, making understanding PEFT methods essential for grasping the efficiency gains.
  - Quick check question: What is the key difference between CROME's adapter approach and LoRA when applied to multimodal models?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: By keeping the LLM frozen, CROME aims to prevent the degradation of language understanding and reasoning capabilities that can occur when LLMs are retrained on multimodal data.
  - Quick check question: Why might retraining an LLM on multimodal data lead to degradation in text-only tasks?

## Architecture Onboarding

- Component map: Input → Vision Encoder (frozen) → Projection Layer (trainable) → Q-Former (trainable during IT) → CROME-Adapter (trainable) → LLM (frozen) → Output
- Critical path: Image/Text → Vision Encoder + Q-Former → CROME-Adapter → LLM
- Design tradeoffs: Freezing LLM provides efficiency and prevents forgetting but limits model adaptation; weight-sharing in up-projection encourages cross-modal alignment but may constrain modality-specific learning
- Failure signatures: Poor cross-modal understanding (fails when visual and text information must be integrated); catastrophic forgetting of LLM capabilities (degrades on pure text tasks)
- First 3 experiments:
  1. Ablation: Remove CROME-Adapter entirely to establish baseline performance (should match BLIP-2/InstructBLIP)
  2. Ablation: Remove gating mechanism from down-projection to test its contribution
  3. Ablation: Use modality-specific weights in up-projection instead of weight-sharing to test cross-modal alignment benefits

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations identified in the reproduction notes and analysis, several key open questions emerge:

- What is the optimal bottleneck dimension (m) for the CROME-Adapter across different LLM architectures?
- How does CROME's pre-LLM cross-modal fusion compare to post-LLM fusion approaches in terms of parameter efficiency and performance?
- What is the impact of pretraining dataset size and composition on CROME's zero-shot performance across diverse tasks?
- How does CROME's performance scale with larger vision encoders (e.g., ViT-G/28) and higher-resolution inputs?
- What is the optimal ratio between pretraining and instruction-tuning data for maximizing CROME's performance?

## Limitations

- The paper doesn't empirically validate that freezing the LLM prevents catastrophic forgetting through systematic ablation studies
- Weight-sharing mechanism's contribution to cross-modal alignment lacks quantitative evidence through ablation studies
- Parameter efficiency claims focus only on task-specific fine-tuning parameters, ignoring the 188M Q-Former parameters trained during instruction tuning
- Commercial model comparisons (GPT-4V, Gemini Pro Vision) are not directly comparable due to different architectures and access methods

## Confidence

**High Confidence:** The basic architectural approach of using a cross-modal adapter before LLM input is sound and the zero-shot benchmark results are credible given the methodology described.

**Medium Confidence:** The claim that CROME achieves "superior" zero-shot performance is supported but should be interpreted cautiously given the relatively small number of benchmarks and the fact that commercial models are not directly comparable.

**Low Confidence:** The mechanism-level claims about gated linear units and weight-sharing specifically for cross-modal learning lack strong empirical validation through ablation studies.

## Next Checks

1. Conduct controlled ablation experiments removing the gating mechanism and weight-sharing components to quantify their individual contributions to performance.

2. Design experiments comparing CROME's frozen LLM approach against fine-tuned LLM approaches on pure text tasks to empirically verify that catastrophic forgetting is prevented.

3. Perform a comprehensive accounting of all trainable parameters across the full pipeline (including Q-Former during instruction tuning) to provide accurate parameter efficiency metrics.