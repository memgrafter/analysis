---
ver: rpa2
title: 'Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing'
arxiv_id: '2404.14618'
source_url: https://arxiv.org/abs/2404.14618
tags:
- quality
- queries
- cost
- performance
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid inference approach for large language
  models (LLMs) that routes queries to either a small or large model based on predicted
  query difficulty and desired quality level. The router is trained to estimate the
  quality gap between the small and large model responses, and routes queries with
  high estimated quality gap to the small model.
---

# Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing

## Quick Facts
- **arXiv ID**: 2404.14618
- **Source URL**: https://arxiv.org/abs/2404.14618
- **Reference count**: 29
- **Primary result**: Router can reduce large model calls by up to 40% with no quality drop

## Executive Summary
This paper proposes a hybrid inference approach for large language models that routes queries to either a small or large model based on predicted query difficulty and desired quality level. The router is trained to estimate the quality gap between small and large model responses, routing queries with high estimated quality gap to the small model. Experiments on a dataset of real-world natural language queries show this approach can reduce calls to the large model by up to 40% with no drop in response quality. The router design incorporates uncertainty due to randomness in LLM responses and introduces a novel data transformation to handle cases where the small model is much weaker than the large model.

## Method Summary
The approach uses a router (DeBERTa-v3-large encoder) that predicts routing scores based on quality gap estimation between small and large LLM responses. The router is trained on a dataset of queries with multiple responses from both models, computing quality gaps using BART score. Three router variants are proposed: deterministic (using single sampled responses), probabilistic (incorporating response uncertainty through sampling), and probabilistic with data transformation (shifting quality gap threshold to address class imbalance). The router assigns queries to the small model when the estimated quality gap suggests the small model's response quality is comparable to the large model's.

## Key Results
- Hybrid approach reduces large model calls by up to 40% with no quality drop
- Probabilistic router (rprob) consistently outperforms deterministic router (rdet)
- Data transformation approach (rtrans) effectively handles cases where large model significantly outperforms small model
- Router inference time is negligible compared to LLM inference time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Router leverages quality gap estimation to identify easy queries where small model's response quality is close to large model's
- Mechanism: Router computes probability that small model's response quality is at least as good as large model's and routes high-probability queries to small model
- Core assumption: Quality gap between models is reliable signal for query difficulty
- Evidence anchors: Quality gap defined as H(x) := q(S(x)) - q(L(x)); approach uses predicted query difficulty and desired quality level

### Mechanism 2
- Claim: Incorporating probabilistic estimates of quality gap improves router performance over deterministic labels
- Mechanism: Router uses expected value of indicator function Pr[q(S(x)) ≥ q(L(x))] computed from multiple samples rather than single sampled response
- Core assumption: LLM responses are non-deterministic and capturing uncertainty through sampling leads to better quality gap estimates
- Evidence anchors: rprob consistently improves rdet in performance comparison; uncertainty incorporated into router training loss

### Mechanism 3
- Claim: Data transformation with shifted quality gap threshold addresses class imbalance when small model is much weaker than large model
- Mechanism: Router trains on Pr[H(x) ≥ -t] for optimized t > 0 rather than Pr[H(x) ≥ 0], relaxing definition of "easy" queries
- Core assumption: Original quality gap signal is too sparse when small model is much weaker, requiring threshold relaxation
- Evidence anchors: Empirical estimates of Pr[H(xi) ≥ 0] tend to be extremely small when large model significantly outperforms small model; rtrans achieves better performance in large quality gap scenarios

## Foundational Learning

- Concept: BART score as response quality metric
  - Why needed here: Router needs way to quantify and compare response quality between models; BART score provides efficient, task-agnostic evaluation metric correlating with human judgment
  - Quick check question: What are key advantages of using BART score over other evaluation metrics like BLEU or ROUGE for this routing task?

- Concept: Quality gap distribution and relationship to query difficulty
  - Why needed here: Understanding quality gap distribution between small and large models across queries is fundamental to grasping how approach works
  - Quick check question: Why is it important that quality gap H(x) = q(S(x)) - q(L(x)) is treated as random variable rather than deterministic value?

- Concept: Data transformation for handling class imbalance
  - Why needed here: When small model is much weaker, original labels (Pr[H(x) ≥ 0]) are too imbalanced for effective learning, requiring techniques like shifted threshold approach
  - Quick check question: How does shifting threshold from 0 to -t in quality gap distribution help address class imbalance problem during router training?

## Architecture Onboarding

- Component map: Query → Router (DeBERTa-v3-large) → (Small or Large LLM) → Response
- Critical path: Query → Router → (Small or Large LLM) → Response
  Router inference must be negligible compared to LLM inference time to maintain cost advantages
- Design tradeoffs:
  - Router model size vs. inference cost: Larger routers may be more accurate but add overhead
  - Sampling strategy for probabilistic training: More samples improve quality gap estimation but increase training cost
  - Threshold selection: Conservative thresholds maintain quality but reduce cost savings; aggressive thresholds increase savings but risk quality drops
- Failure signatures:
  - Router consistently routes to one model regardless of query: Indicates poor training or extreme quality gap imbalance
  - Router latency comparable to LLM inference: Router model is too large or inefficient
  - Quality drops significantly exceed expectations: Threshold is set too aggressively or router is mis-calibrated
- First 3 experiments:
  1. Validate router latency is << LLM inference latency across all model pairs
  2. Test router performance on held-out validation set with varying thresholds to find optimal tradeoff
  3. Compare router-assigned query difficulty vs. actual quality gap to verify correct routing behavior

## Open Questions the Paper Calls Out
- How well does the approach generalize to new domains or distributions?
- How does router performance vary across different types of natural language tasks?
- What is the impact of using more sophisticated methods for choosing the relaxation parameter t?
- How does the approach perform when routing to more than two models?
- How well does the router generalize to out-of-distribution queries or new data distributions?

## Limitations
- Effectiveness depends critically on existence of meaningful quality gap distribution between models
- Assumes BART score is appropriate proxy for response quality across diverse query types
- Approach relies on generating multiple responses from both models during training, which could be computationally expensive at scale
- Generalizability to real-world deployment scenarios with continuously evolving model capabilities is not thoroughly examined

## Confidence
- **High Confidence**: Core mechanism of using quality gap estimation for routing; experimental results showing up to 40% reduction in large model calls
- **Medium Confidence**: Benefits of incorporating probabilistic estimates over deterministic labels; data transformation approach effectiveness
- **Low Confidence**: Generalizability to real-world deployment scenarios; scalability to production environments

## Next Checks
1. Measure router inference latency across different router model sizes and verify it remains << LLM inference latency at scale (e.g., 100+ QPS)
2. Evaluate router's performance on queries from domains not represented in the MixInstruct dataset to assess BART score reliability and routing effectiveness in new domains
3. Simulate scenarios where quality gap between small and large models changes over time to test router's ability to adapt to shifting quality distributions without complete retraining