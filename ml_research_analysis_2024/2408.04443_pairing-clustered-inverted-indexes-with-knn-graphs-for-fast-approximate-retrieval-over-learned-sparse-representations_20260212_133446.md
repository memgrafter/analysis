---
ver: rpa2
title: Pairing Clustered Inverted Indexes with kNN Graphs for Fast Approximate Retrieval
  over Learned Sparse Representations
arxiv_id: '2408.04443'
source_url: https://arxiv.org/abs/2408.04443
tags:
- seismic
- retrieval
- inverted
- sparse
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper improves upon Seismic, a fast approximate MIPS algorithm
  for learned sparse representations, by introducing two key innovations. First, it
  processes blocks in order of importance rather than arbitrarily, prioritizing promising
  blocks for evaluation.
---

# Pairing Clustered Inverted Indexes with kNN Graphs for Fast Approximate Retrieval over Learned Sparse Representations

## Quick Facts
- arXiv ID: 2408.04443
- Source URL: https://arxiv.org/abs/2408.04443
- Reference count: 36
- Key result: SeismicWave achieves up to 2.2× speedup over Seismic while maintaining almost-exact accuracy

## Executive Summary
This paper introduces SeismicWave, an improvement to the Seismic algorithm for fast approximate retrieval over learned sparse representations. The key innovations are ordered block traversal, which processes inverted list blocks by importance rather than arbitrary order, and k-NN graph expansion, which leverages global similarity information to improve accuracy. The method achieves significant speedups while maintaining high accuracy on public datasets.

## Method Summary
The paper proposes SeismicWave, which improves upon the Seismic algorithm for approximate MIPS over learned sparse representations. The method combines ordered block traversal (OBT) with k-NN graph expansion. OBT sorts blocks within inverted lists by their summary scores before processing, prioritizing promising blocks. The k-NN graph expansion incorporates the clustering hypothesis by adding neighbors of retrieved documents to the candidate set. Together, these innovations achieve up to 2.2× speedup over Seismic while maintaining near-exact accuracy.

## Key Results
- Achieves up to 2.2× speedup over Seismic algorithm
- Maintains 99% accuracy on Ms Marco dataset with 1.6× faster latency (409μs vs 95,818μs for exact)
- Outperforms original Seismic across multiple accuracy thresholds

## Why This Works (Mechanism)

### Mechanism 1
Ordered block traversal improves latency by prioritizing high-scoring blocks. Instead of processing blocks in arbitrary order, blocks are sorted by their inner product with the query, so the most promising blocks are evaluated first. The success relies on the heap threshold increasing monotonically during traversal, making early high-scoring blocks more likely to qualify for evaluation.

### Mechanism 2
k-NN graph expansion improves accuracy by incorporating global similarity information. After initial retrieval, documents are expanded by adding their k nearest neighbors from a pre-computed graph, then re-ranked. This leverages the clustering hypothesis, which suggests that closely-related documents tend to be relevant to the same queries.

### Mechanism 3
Combining ordered traversal with k-NN expansion provides synergistic improvements. Ordered traversal improves efficiency for lower accuracy thresholds while k-NN expansion compensates for potential accuracy loss by adding globally relevant documents. The two mechanisms address different limitations - local ordering vs global context.

## Foundational Learning

- Concept: Maximum Inner Product Search (MIPS)
  - Why needed here: The paper's core problem is efficient MIPS over learned sparse representations
  - Quick check question: What distinguishes MIPS from standard nearest neighbor search, and why is it particularly challenging for sparse representations?

- Concept: Inverted index block clustering
  - Why needed here: Seismic organizes inverted lists into geometrically-cohesive blocks with summary vectors
  - Quick check question: How does block clustering reduce computation compared to evaluating all documents in an inverted list?

- Concept: Clustering hypothesis in information retrieval
  - Why needed here: The k-NN graph expansion relies on the assumption that related documents share relevance
  - Quick check question: What empirical evidence supports the clustering hypothesis, and what are its limitations in sparse retrieval?

## Architecture Onboarding

- Component map:
  - Query processor -> Inverted index (clustered blocks with summary vectors) -> Forward index (document vectors) -> k-NN graph (pre-computed neighbors) -> Re-ranker

- Critical path:
  1. For each query term, traverse associated inverted list
  2. Compute inner products between query and block summaries
  3. Sort blocks by score and evaluate in order
  4. Maintain top-k heap with scaled threshold
  5. Expand retrieved documents using k-NN graph
  6. Re-rank expanded set and return top-k results

- Design tradeoffs:
  - Memory vs accuracy: Higher λ (inverted list length) and larger k improve accuracy but increase memory
  - Latency vs completeness: Ordered traversal may skip relevant blocks, but k-NN expansion compensates
  - Pre-processing cost: k-NN graph construction is expensive but amortized over many queries

- Failure signatures:
  - Low accuracy despite high memory budget: k-NN graph may be inaccurate or block clustering ineffective
  - High latency with low accuracy: Ordered traversal may be skipping too many relevant blocks
  - Memory usage much higher than expected: Inverted lists or k-NN graph may be larger than anticipated

- First 3 experiments:
  1. Measure accuracy-latency tradeoff with varying heap_factor and cut parameters on Splade embeddings
  2. Compare accuracy with and without ordered block traversal at different memory budgets
  3. Test k-NN graph effectiveness by measuring accuracy improvement from expansion at different k values

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Effectiveness depends on heap threshold monotonicity, which may vary across datasets
- k-NN graph construction method and accuracy are not fully specified
- Trade-off between block-skipping and k-NN expansion effectiveness needs more systematic analysis

## Confidence
- High confidence: The 2.2× speedup claim with 99% accuracy on Ms Marco is well-supported by experimental results
- Medium confidence: The effectiveness of ordered block traversal, as the paper provides theoretical justification but limited empirical comparison with unordered traversal
- Medium confidence: The k-NN graph contribution, as the clustering hypothesis is cited but not directly validated for this specific application

## Next Checks
1. **Empirical validation of ordered traversal**: Compare accuracy-latency curves with and without ordered block traversal across multiple datasets to quantify its individual contribution
2. **k-NN graph sensitivity analysis**: Systematically vary k and measure accuracy improvements, particularly for cases where the graph may introduce noise or irrelevant documents
3. **Robustness testing**: Evaluate performance on datasets with different clustering characteristics to assess generalizability beyond Ms Marco and Natural Questions