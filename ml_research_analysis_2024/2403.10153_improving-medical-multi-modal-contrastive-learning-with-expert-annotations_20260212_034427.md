---
ver: rpa2
title: Improving Medical Multi-modal Contrastive Learning with Expert Annotations
arxiv_id: '2403.10153'
source_url: https://arxiv.org/abs/2403.10153
tags:
- image
- eclip
- expert
- learning
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents eCLIP, a method to improve medical image-text
  contrastive learning by incorporating radiologist eye-gaze heatmaps. The key innovation
  is a heatmap processor that uses multi-headed attention to integrate expert attention
  maps into the image embeddings, combined with mixup augmentation and curriculum
  learning to address data scarcity.
---

# Improving Medical Multi-modal Contrastive Learning with Expert Annotations

## Quick Facts
- arXiv ID: 2403.10153
- Source URL: https://arxiv.org/abs/2403.10153
- Authors: Yogesh Kumar; Pekka Marttinen
- Reference count: 40
- Key outcome: eCLIP achieves up to 5.8% higher zero-shot classification accuracy and better embedding quality by incorporating radiologist eye-gaze heatmaps into medical image-text contrastive learning.

## Executive Summary
This paper presents eCLIP, a method to improve medical image-text contrastive learning by incorporating radiologist eye-gaze heatmaps. The key innovation is a heatmap processor that uses multi-headed attention to integrate expert attention maps into the image embeddings, combined with mixup augmentation and curriculum learning to address data scarcity. The method improves zero-shot classification, sample efficiency, and cross-modal retrieval performance on chest X-ray datasets compared to CLIP baselines. Specifically, eCLIP achieves up to 5.8% higher zero-shot classification accuracy and better uniformity and alignment in the embedding space. It also demonstrates enhanced performance in generating radiology reports using a frozen LLM without fine-tuning. The approach is model-agnostic and can be applied to any CLIP variant.

## Method Summary
eCLIP integrates a heatmap processor with multi-headed attention into the standard CLIP architecture to incorporate radiologist eye-gaze information into image embeddings. The method uses mixup augmentation to create additional training samples from limited expert annotations and employs curriculum learning to gradually introduce expert data. The training process involves pretraining on MIMIC-CXR dataset with expert-annotated heatmaps, followed by evaluation on zero-shot classification, sample efficiency, cross-modal retrieval, and report generation tasks. The approach is designed to be model-agnostic and can be applied to any variant of CLIP without requiring modifications to the core architecture.

## Key Results
- Achieves up to 5.8% higher zero-shot classification accuracy compared to CLIP baselines on chest X-ray datasets
- Improves uniformity and alignment metrics in the embedding space, indicating better cross-modal alignment
- Demonstrates enhanced performance in generating radiology reports using a frozen LLM without fine-tuning
- Shows better sample efficiency, learning effectively from limited expert annotations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-headed attention (MHA) in the heatmap processor improves embedding quality by focusing the model on expert-attended regions of the image.
- **Mechanism**: The MHA layer takes the original image patches as keys and values and the heatmap-processed patches as queries. This allows the model to integrate expert attention information into the image representation before it is passed to the standard CLIP image encoder.
- **Core assumption**: Radiologists' eye-gaze heatmaps accurately reflect the clinically important regions of the X-ray images, and these regions contain information that is semantically relevant to the accompanying radiology reports.
- **Evidence anchors**:
  - [abstract] "eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness."
  - [section 4.5] "Our evaluation of uniformity and alignment reveals that eCLIP surpasses both the MIMIC-pretrained and internet-pretrained models in these key metrics"
  - [corpus] Weak. No direct evidence in the corpus about the effectiveness of MHA in integrating expert attention. The corpus mentions "RegionMed-CLIP" which uses region-aware learning, but not specifically MHA with eye-gaze heatmaps.
- **Break condition**: If the radiologist's eye-gaze does not correlate with clinically relevant regions, or if the MHA layer fails to properly integrate the heatmap information into the image representation.

### Mechanism 2
- **Claim**: Mixup augmentation creates additional positive and negative pairs, improving the model's ability to learn from limited expert annotations.
- **Mechanism**: By interpolating between the original image and the heatmap-processed image, mixup generates new image embeddings (vλ_i) that form new positive pairs with the text embedding (ti) and corresponding negative pairs. This increases the number of training samples without requiring more expert annotations.
- **Core assumption**: The interpolated images (Iλ_i) generated by mixup still contain meaningful information that can contribute to the contrastive learning objective.
- **Evidence anchors**:
  - [abstract] "eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations"
  - [section 4.3] "Sample efficiency measures how well a model learns from limited amount of training data. eCLIP improves this efficiency by using expert annotated images to form new positive and negative pairs"
  - [corpus] Weak. The corpus mentions "MedCLIP-SAMv2" which uses text-driven segmentation, but not specifically mixup for handling limited expert annotations.
- **Break condition**: If the interpolated images are too noisy or do not contribute meaningful information to the learning process, or if the mixup ratio (lambda) is not properly tuned.

### Mechanism 3
- **Claim**: Curriculum learning gradually introduces expert annotations, allowing the model to first establish a robust baseline before incorporating expert information.
- **Mechanism**: The training process starts without expert annotations (cold start), then gradually increases the probability of using expert annotations (warmup), and finally reduces the probability again (cooldown). This phased approach ensures a smooth transition and prevents the model from being overwhelmed by expert information too early.
- **Core assumption**: A gradual introduction of expert annotations allows the model to better integrate this information into its existing knowledge base, leading to improved performance.
- **Evidence anchors**:
  - [abstract] "eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture"
  - [section 3.2] "To seamlessly integrate expert annotations without disrupting the foundational training of the eCLIP model, we employ a phased curriculum learning strategy"
  - [corpus] Weak. The corpus does not mention curriculum learning in the context of integrating expert annotations into CLIP models.
- **Break condition**: If the curriculum learning schedule is not properly tuned, or if the model fails to benefit from the gradual introduction of expert information.

## Foundational Learning

- **Concept**: Contrastive Learning
  - **Why needed here**: Contrastive learning is the core training objective used to align image and text embeddings in CLIP models. Understanding how contrastive loss works is crucial for understanding how eCLIP improves upon it.
  - **Quick check question**: What is the main goal of the InfoNCE loss used in contrastive learning, and how does it achieve this goal?

- **Concept**: Multi-headed Attention (MHA)
  - **Why needed here**: MHA is a key component of the heatmap processor in eCLIP. Understanding how MHA works is essential for understanding how eCLIP integrates expert attention information into the image representation.
  - **Quick check question**: How does MHA differ from standard self-attention, and what advantages does it offer in the context of processing expert heatmaps?

- **Concept**: Mixup Augmentation
  - **Why needed here**: Mixup is used in eCLIP to create additional training samples from limited expert annotations. Understanding how mixup works is crucial for understanding how eCLIP improves sample efficiency.
  - **Quick check question**: What is the mathematical formulation of mixup, and how does it create new training samples?

## Architecture Onboarding

- **Component map**: Image Encoder -> Heatmap Processor -> Mixup -> Image Encoder -> Text Encoder -> Loss Function
- **Critical path**: Image -> Heatmap Processor -> Mixup -> Image Encoder -> Text Encoder -> Loss Function
- **Design tradeoffs**:
  1. Using MHA vs. simpler methods (e.g., direct mask multiplication) for heatmap processing.
  2. Using mixup vs. other data augmentation techniques for handling limited expert annotations.
  3. Using curriculum learning vs. other strategies for gradually introducing expert annotations.
- **Failure signatures**:
  1. If the model fails to improve performance, it could indicate that the heatmap processor is not properly integrating expert information, or that the mixup augmentation is not generating meaningful samples.
  2. If the model overfits to the expert annotations, it could indicate that the curriculum learning schedule is not properly tuned, or that the mixup ratio is too high.
- **First 3 experiments**:
  1. Train eCLIP with and without the heatmap processor to evaluate the impact of MHA on performance.
  2. Train eCLIP with different mixup ratios (lambda) to find the optimal balance between original and heatmap-processed images.
  3. Train eCLIP with different curriculum learning schedules (e.g., different cold start, warmup, and cooldown phases) to find the optimal gradual introduction of expert annotations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and distribution of expert annotations needed to maximize the performance of eCLIP across different abnormality subgroups?
- Basis in paper: [inferred] The paper acknowledges limitations due to the small size of expert annotated data and the lack of comprehensive analysis on the impact of size or distribution of expert annotations across different abnormalities.
- Why unresolved: The study did not systematically vary the amount or distribution of expert annotations to determine the point of diminishing returns or the impact of class imbalance on the effectiveness of eCLIP.
- What evidence would resolve it: A series of experiments varying the number of expert-annotated samples per abnormality class, analyzing the trade-off between annotation cost and performance improvement, and identifying the optimal distribution of annotations across different classes.

### Open Question 2
- Question: How do the temporal dynamics of eye-tracking data contribute to the quality of medical image-text embeddings when integrated into contrastive learning?
- Basis in paper: [explicit] The paper suggests extending the approach to leverage the temporal dynamics of eye-tracking data by aligning the sequential frames with corresponding report snippets as a future research direction.
- Why unresolved: The current implementation uses static heatmaps derived from eye-tracking data, not exploiting the temporal sequence of fixations that might capture the radiologist's reasoning process.
- What evidence would resolve it: Experiments comparing eCLIP performance using static heatmaps versus dynamic sequences of eye-tracking data, potentially using sequence modeling techniques like recurrent networks or transformers to process the temporal information.

### Open Question 3
- Question: What is the clinical relevance and diagnostic accuracy of radiology reports generated by eCLIP-augmented LLMs compared to those written by human radiologists?
- Basis in paper: [explicit] The paper acknowledges that the clinical relevance of generated radiology reports has not been validated by medical experts, relying instead on standard metrics known for potential biases and inaccuracies.
- Why unresolved: The study used automated metrics like BLEU, BERT score, and embedding similarity rather than direct evaluation by radiologists, and these metrics may not correlate well with clinical utility.
- What evidence would resolve it: A clinician study where radiologists evaluate the generated reports for diagnostic accuracy, completeness, and clinical utility compared to ground truth reports, potentially using blinded comparison studies or clinical decision-making scenarios.

## Limitations
- Effectiveness relies heavily on quality and representativeness of radiologist eye-gaze heatmaps, which are subjective
- Focus on chest X-ray imaging limits generalizability to other medical imaging modalities
- Computational overhead of additional heatmap processor and curriculum learning strategy not thoroughly analyzed

## Confidence

- **High confidence**: The overall framework design and implementation details are well-specified, and the reported improvements in zero-shot classification and sample efficiency are statistically significant
- **Medium confidence**: The effectiveness of the multi-headed attention mechanism in integrating expert attention, as the paper provides limited ablation studies on this specific component
- **Medium confidence**: The curriculum learning strategy's impact on model performance, as the paper does not explore alternative scheduling strategies or provide extensive sensitivity analysis

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the heatmap processor, mixup augmentation, and curriculum learning components to overall performance
2. Test the method on additional medical imaging modalities (e.g., CT, MRI) to assess generalizability beyond chest X-rays
3. Evaluate the computational efficiency and memory requirements of the eCLIP approach compared to standard CLIP training, particularly for larger-scale deployments