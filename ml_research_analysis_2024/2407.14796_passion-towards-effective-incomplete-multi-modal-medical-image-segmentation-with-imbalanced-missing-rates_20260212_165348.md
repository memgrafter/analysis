---
ver: rpa2
title: 'PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation
  with Imbalanced Missing Rates'
arxiv_id: '2407.14796'
source_url: https://arxiv.org/abs/2407.14796
tags:
- modalities
- modality
- segmentation
- multi-modal
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses incomplete multi-modal medical image segmentation
  where modalities have imbalanced missing rates during both training and deployment.
  The proposed PASSION method introduces pixel-wise and semantic-wise self-distillation
  to balance modality-specific optimization objectives, along with task-wise and gradient-wise
  regularization based on relative preference to align convergence rates across modalities.
---

# PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation with Imbalanced Missing Rates

## Quick Facts
- arXiv ID: 2407.14796
- Source URL: https://arxiv.org/abs/2407.14796
- Reference count: 40
- Primary result: PASSION improves segmentation Dice scores by 1-3% across modalities with imbalanced missing rates

## Executive Summary
This paper addresses the challenge of incomplete multi-modal medical image segmentation where modalities have different missing rates during both training and deployment. The proposed PASSION method introduces pixel-wise and semantic-wise self-distillation to balance modality-specific optimization objectives, along with task-wise and gradient-wise regularization based on relative preference to align convergence rates across modalities. Experimental results on BraTS2020 and MyoPS2020 datasets demonstrate consistent performance improvements over state-of-the-art approaches.

## Method Summary
PASSION is a plug-and-play module that enhances incomplete multi-modal medical image segmentation by addressing imbalanced missing rates across modalities. It employs pixel-wise self-distillation using KL divergence between multi-modal and uni-modal predictions, and semantic-wise self-distillation by aligning class prototypes. The method introduces relative preference metrics to quantify modality neglect, enabling adaptive task-wise and gradient-wise regularization to balance learning paces. The approach is validated across multiple backbone architectures including mmFormer, RFNet, and M2FTrans.

## Key Results
- PASSION improves average Dice scores by 1-3% across different modality combinations on BraTS2020 and MyoPS2020
- The method functions as a plug-and-play module, consistently improving performance across various backbone architectures
- PASSION effectively handles modality imbalance during both training and deployment phases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel-wise and semantic-wise self-distillation transfers multi-modal knowledge to uni-modal networks, balancing modality-specific optimization objectives under imbalanced missing rates.
- Mechanism: Multi-modal predictions act as "soft labels" that guide each available uni-modal encoder. The pixel-wise distillation uses KL divergence between softmax outputs across modalities, while semantic-wise distillation aligns class prototype representations. This balances optimization by penalizing each modality based on its distance to the multi-modal teacher, preventing strong modalities from dominating.
- Core assumption: Multi-modal predictions capture richer and more robust representations than any single uni-modal prediction, and this knowledge gap can be effectively transferred through distillation.
- Evidence anchors: [abstract] "We, for the first time, formulate such a challenging setting and propose Preference-Aware Self-diStillatION (PASSION) for incomplete multi-modal medical image segmentation under imbalanced missing rates."
- Break condition: If the multi-modal model itself is poorly optimized due to missing modalities, the transferred knowledge could be noisy or misleading.

### Mechanism 2
- Claim: Relative preference metric quantifies how much each modality is neglected compared to others, enabling adaptive regularization to balance learning paces.
- Mechanism: Relative preference is computed as the normalized distance between uni-modal and multi-modal semantic prototypes. Modalities with negative relative preference (more neglected) are accelerated via task-wise regularization (ð›¿ mask), while those with positive preference are slowed via gradient-wise regularization (ð›½ coefficient). This dynamically adjusts learning rates based on modality dominance.
- Core assumption: The semantic gap between uni-modal and multi-modal predictions accurately reflects the relative learning difficulty and optimization imbalance of each modality.
- Evidence anchors: [abstract] "we define relative preference to evaluate the dominance of each modality during training, based on which to design task-wise and gradient-wise regularization to balance the convergence rates of different modalities."
- Break condition: If the semantic gap does not correlate with actual learning difficulty (e.g., due to noise or dataset bias), the regularization could misalign learning paces.

### Mechanism 3
- Claim: PASSION functions as a plug-and-play module that improves segmentation performance across various backbone architectures.
- Mechanism: The method integrates seamlessly by replacing or augmenting the standard segmentation loss with self-distillation and preference-aware regularization terms. It does not require architectural changes to existing backbones like mmFormer, RFNet, or M2FTrans, only modification of the training objective.
- Core assumption: The core learning dynamics of multi-modal segmentation are similar enough across architectures that the same balancing principles apply.
- Evidence anchors: [abstract] "More importantly, PASSION is validated to work as a plug-and-play module for consistent performance improvement across different backbones."
- Break condition: If a backbone has fundamentally different fusion or modality handling mechanisms, the plug-and-play assumption may fail.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: PASSION uses multi-modal predictions as soft targets to guide uni-modal encoders, requiring understanding of how distillation transfers knowledge between networks.
  - Quick check question: What is the difference between hard label supervision and soft label distillation in training objectives?

- Concept: Prototype Learning
  - Why needed here: Semantic-wise self-distillation uses class prototypes to measure semantic richness and guide knowledge transfer between modalities.
  - Quick check question: How do you compute a prototype for a class given a set of feature vectors and their labels?

- Concept: Imbalanced Learning
  - Why needed here: The core challenge is handling modalities with different missing rates during training, which requires techniques to prevent dominant modalities from overwhelming weaker ones.
  - Quick check question: What is the key difference between re-weighting samples and re-weighting modalities in an imbalanced learning scenario?

## Architecture Onboarding

- Component map: Multi-modal input -> modality-specific encoders -> shared fusion decoder -> segmentation output. PASSION module sits outside, computing pixel-wise and semantic-wise distillation losses between multi-modal and uni-modal predictions, and preference-aware regularization based on relative preference metrics.

- Critical path: During each training iteration, features flow through modality-specific encoders, are fused in the shared decoder, and predictions are compared to ground truth. Simultaneously, PASSION computes distillation losses and preference metrics to update the regularization coefficients ð›½ and ð›¿ masks, which influence the next forward pass.

- Design tradeoffs: Using a unified network for both teacher and student roles simplifies training but may propagate errors if the multi-modal model is poorly optimized. The method trades off architectural complexity for training objective complexity.

- Failure signatures: If performance degrades when integrating PASSION, check whether the distillation temperature ðœ is too high/low, whether the balancing hyper-parameters ðœ†1 and ðœ†2 are properly tuned, or whether the relative preference computation is unstable due to small batch sizes.

- First 3 experiments:
  1. Validate that PASSION improves segmentation when added to mmFormer on BraTS2020 with imbalanced missing rates.
  2. Test whether removing the preference-aware regularization (keeping only self-distillation) degrades performance, confirming its necessity.
  3. Evaluate PASSION on a backbone not used in the paper (e.g., TransUNet) to confirm plug-and-play functionality.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The method's effectiveness at missing rates exceeding 80% remains untested, leaving uncertainty about theoretical limits
- Performance comparison with traditional knowledge distillation from separately trained teacher models is not provided
- The approach doesn't address spatially-varying or population-specific missing rate patterns that occur in clinical practice

## Confidence
- Generalization to extreme missing rates (>80%): Medium
- Plug-and-play functionality across all backbone architectures: Medium
- Self-distillation superiority over traditional KD: Low

## Next Checks
1. Test PASSION on a fourth, structurally different backbone (e.g., TransUNet) to confirm architecture independence
2. Evaluate performance when modality quality is heterogeneous rather than uniformly missing
3. Measure whether the method maintains benefits when training with highly imbalanced datasets (missing rates >90%)