---
ver: rpa2
title: 'Themis: A Reference-free NLG Evaluation Language Model with Flexibility and
  Interpretability'
arxiv_id: '2406.18365'
source_url: https://arxiv.org/abs/2406.18365
tags:
- evaluation
- text
- summary
- linguistics
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Themis, a reference-free LLM specifically
  designed for NLG evaluation with flexibility and interpretability. It addresses
  the limitations of existing methods by training on a large-scale NLG evaluation
  corpus (NLG-Eval) constructed with human and GPT-4 annotations.
---

# Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability

## Quick Facts
- arXiv ID: 2406.18365
- Source URL: https://arxiv.org/abs/2406.18365
- Reference count: 40
- Key outcome: Themis outperforms GPT-4 and previous models on NLG evaluation benchmarks while being reference-free

## Executive Summary
Themis is a reference-free language model specifically designed for natural language generation evaluation. It addresses the limitations of existing reference-free methods by training on a large-scale NLG evaluation corpus (NLG-Eval) constructed with human and GPT-4 annotations. The model employs innovative multi-perspective consistency verification and rating-guided preference alignment methods to achieve superior evaluation performance across various NLG tasks. Themis demonstrates strong generalization to unseen tasks and robustness against perturbations while providing interpretable evaluation analyses without requiring references.

## Method Summary
Themis uses Llama-3-8B as the base model and undergoes a two-step training process. First, supervised fine-tuning is performed on approximately 67K filtered samples from the NLG-Eval corpus, which contains ~0.5 million samples across 9 NLG tasks. The filtering uses multi-perspective consistency verification to select reliable data. Second, preference alignment is applied using a rating-guided modification of Direct Preference Optimization (DPO), which compensates for rating differences when constructing preference pairs. The model outputs both a rating (1-5 Likert scale) and an optional analysis text for evaluation aspects.

## Key Results
- Outperforms GPT-4 and previous models on standard NLG evaluation benchmarks (SummEval, Topical-Chat, SFRES&SFHOT, QAGS, MANS, WMT23)
- Demonstrates superior generalization to unseen tasks and robustness against perturbations
- Achieves strong correlation with human judgments across multiple aspects without requiring reference texts
- Provides interpretable evaluation analyses alongside numerical ratings

## Why This Works (Mechanism)

### Mechanism 1: Multi-perspective consistency verification
The approach combines three filters: self-consistency (keeping samples where GPT-4's rating is consistent across multiple samplings), cross-validation (prioritizing samples where human and GPT-4 ratings are close), and evaluation inspection (rechecking that analysis matches rating and aspect). This layered filtering yields higher-quality training data than raw human or LLM-only data. Core assumption: GPT-4's self-consistency and agreement with human ratings are valid proxies for annotation reliability.

### Mechanism 2: Rating-guided preference alignment
Standard DPO assumes uniform preference differences, but Themis modifies the BT model by subtracting a value proportional to the rating difference (-α|R(y1)-R(y2)|). This compensates for prior preference differences caused by rating differences, balancing preference pairs more evenly during fine-tuning. Core assumption: Rating differences are valid proxies for preference strength between responses.

### Mechanism 3: Large-scale multi-task training
Training on the NLG-Eval corpus covering 9 NLG tasks and 58 datasets with ~0.5M samples enables strong generalization. The broad training distribution helps the model learn evaluation criteria that transfer beyond specific tasks seen during training. Core assumption: Coverage of diverse tasks and aspects in training data correlates with generalization ability.

## Foundational Learning

- **Supervised fine-tuning of LLMs on task-specific data**: Adapts a general-purpose LLM to the specialized task of NLG evaluation with correct rating outputs and analysis generation. Quick check: What loss function is used during supervised fine-tuning of Themis? Answer: Standard cross-entropy loss on training data pairs.

- **Preference optimization (DPO-style) for aligning model outputs to human preferences**: Refines the fine-tuned model so evaluation ratings and analyses better match human judgments on preference pairs. Quick check: How are preference pairs constructed in Themis? Answer: By comparing evaluations with consistent ratings (chosen) vs those that do not (rejected), with preference strength guided by rating differences.

- **Correlation metrics (Pearson, Kendall, Spearman) for evaluating metric quality**: Quantifies how well Themis's ratings align with human judgments on benchmark datasets. Quick check: Which correlation coefficient is typically reported as the primary metric for summarization evaluation in the paper? Answer: Pearson correlation (ρ) is often used, but all three are reported depending on dataset conventions.

## Architecture Onboarding

- **Component map**: Input layer (task description, generated text, optional reference, optional aspect/criteria) -> Core model (Llama-3-8B fine-tuned with supervised learning + preference alignment) -> Output layer (Rating 1-5 + optional analysis text)

- **Critical path**: Data → Consistency verification → Supervised fine-tuning → Preference alignment → Inference

- **Design tradeoffs**: Llama-3-8B balances performance and inference cost; reference-free design chosen for flexibility but may sacrifice some precision; multi-perspective filtering yields higher quality but smaller dataset (67K optimal).

- **Failure signatures**: Low correlation on benchmarks (overfitting or poor alignment), inconsistent ratings across runs (sampling instability), misaligned analysis vs rating (aspect confusion).

- **First 3 experiments**: 1) Sanity check: Run Themis on small summarization sample with known human ratings; 2) Ablation: Train baseline without consistency filtering; compare correlation on SummEval; 3) Perturbation test: Apply fluency perturbation to reference; check if rating changes minimally for unaffected aspects.

## Open Questions the Paper Calls Out

### Open Question 1
How does Themis perform on extremely long-form generation tasks beyond the instruction-following and long-form question-answering evaluations tested in the paper? The paper only covered moderate-length text evaluation, leaving uncertainty about performance on much longer documents where coherence and consistency become more challenging. What evidence would resolve it: Evaluating Themis on benchmark datasets containing texts of 10,000+ words and measuring correlation with human judgments across multiple aspects.

### Open Question 2
What is the impact of using different foundational LLMs (beyond Llama-3-8B) on Themis's evaluation performance? The paper mentions experimenting with Mistral-7B, Llama-2-7B, and Llama-2-13B, but doesn't provide detailed results or explore more powerful models. What evidence would resolve it: Systematic evaluation of Themis fine-tuned on various foundational models ranging from 7B to 70B parameters, with performance comparisons across all tested NLG tasks.

### Open Question 3
How does Themis handle evaluation of multimodal NLG tasks involving text combined with images, audio, or other non-text modalities? All experiments and the NLG-Eval corpus focus exclusively on text-based generation tasks, with no mention of multimodal evaluation scenarios. What evidence would resolve it: Testing Themis on multimodal generation benchmarks and comparing its performance to specialized multimodal evaluation methods.

## Limitations

- Data construction reliability depends heavily on GPT-4's self-consistency as a proxy for data quality, without quantitative validation of filtering effectiveness
- Rating-guided preference alignment assumes rating differences accurately reflect preference strength, which may not hold across all NLG tasks
- Generalization claims are based on limited testing and would benefit from more extensive validation across truly novel evaluation scenarios

## Confidence

**High Confidence**: The core methodology of using LLMs for reference-free NLG evaluation is well-established, and empirical results showing Themis outperforming previous models on standard benchmarks are reproducible.

**Medium Confidence**: The effectiveness of rating-guided preference alignment and multi-perspective consistency verification methods, while plausible, require more rigorous ablation studies to confirm individual contributions.

**Low Confidence**: Claims about generalization to unseen tasks and robustness to perturbations are based on limited testing and would benefit from more extensive validation.

## Next Checks

1. **Consistency Filtering Ablation**: Train two versions of Themis - one with full multi-perspective consistency verification and one without any filtering. Compare correlation performance on SummEval and Topical-Chat to quantify the actual benefit of the filtering approach.

2. **Rating Difference Correlation Study**: For a subset of preference pairs, collect human judgments on which response is preferred. Calculate correlation between actual human preference strength and rating differences used in preference alignment to validate whether the compensation term is justified.

3. **Novel Task Generalization Test**: Select 2-3 NLG tasks not represented in NLG-Eval corpus (e.g., dialogue response generation, headline generation) and evaluate Themis's performance on established human-annotated datasets for these tasks. Compare against reference-based metrics to assess practical utility of reference-free evaluation in truly novel domains.