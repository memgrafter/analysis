---
ver: rpa2
title: Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation
  Budget Constraints
arxiv_id: '2401.09168'
source_url: https://arxiv.org/abs/2401.09168
tags:
- fine-tuning
- budget
- dataset
- squad
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates fine-tuning strategies for domain-specific
  question answering under low annotation budget constraints. The authors compare
  18 different fine-tuning combinations across 4 datasets with varying budget sizes
  (100 to 1600 annotations).
---

# Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints

## Quick Facts
- arXiv ID: 2401.09168
- Source URL: https://arxiv.org/abs/2401.09168
- Reference count: 38
- The paper investigates 18 fine-tuning strategies across 4 datasets, finding that merging SQuAD with target data outperforms sequential fine-tuning by 2.28-6.48% under low annotation budgets.

## Executive Summary
This paper investigates optimal fine-tuning strategies for domain-specific question answering when annotation budgets are limited. The authors compare 18 different fine-tuning approaches across four datasets (COVID-QA, CUAD-QA, MOVIE-QA, KG-QA) with varying annotation budgets from 100 to 1,600 examples. The key finding is that the standard sequential fine-tuning approach is suboptimal, with the best strategy (RoBERTa-Base-MWO) consistently outperforming it. The research provides practical guidance for practitioners working with limited annotation resources in domain-specific QA scenarios.

## Method Summary
The study evaluates 18 fine-tuning strategies that combine knowledge-alignment (MLM fine-tuning on domain corpus), task-alignment (SQuAD fine-tuning), and target data fine-tuning with various merging approaches. The strategies are tested on four domain-specific QA datasets using 5-fold cross-validation. All experiments use RoBERTa base as the pre-trained model with a fixed annotation budget ranging from 100 to 1,600 samples. The MWO (Merging with Oversampling) strategy merges SQuAD and target data in a single training step, which the authors find to be the most effective approach for low-budget scenarios.

## Key Results
- The MWO fine-tuning strategy outperforms standard sequential fine-tuning by 2.28% to 6.48% in low-budget settings.
- A small annotation budget of 200 examples provides substantial performance improvements over zero-shot baselines.
- Knowledge-alignment fine-tuning via MLM on small domain corpora does not yield significant improvements.
- Doubling the annotation effort from 200 to 1,600 only results in 1-2% improvement on average.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging target dataset with SQuAD in a single training step (MWO) improves performance under low annotation budgets.
- Mechanism: Combining SQuAD's diverse QA examples with target domain examples in each training batch provides regularization and prevents catastrophic forgetting, leading to better generalization.
- Core assumption: The merged training signal from SQuAD and target data is more effective than sequential fine-tuning.
- Evidence anchors: [abstract], [section IV-B], [corpus]

### Mechanism 2
- Claim: A small annotation budget (200 examples) provides substantial performance improvements over zero-shot baselines.
- Mechanism: Even limited domain-specific examples provide strong learning signals that significantly improve model performance compared to relying solely on pre-training.
- Core assumption: Domain-specific examples, even in small numbers, contain valuable information that pre-training cannot capture.
- Evidence anchors: [abstract], [section IV-B], [corpus]

### Mechanism 3
- Claim: Knowledge-alignment fine-tuning via MLM on small domain corpora does not yield significant improvements.
- Mechanism: The small size of domain-specific corpora (several orders of magnitude smaller than pre-training corpora) is insufficient to meaningfully align the language model's knowledge.
- Core assumption: MLM requires large corpora to be effective.
- Evidence anchors: [abstract], [section IV-B], [corpus]

## Foundational Learning

- Concept: Transfer learning effectiveness depends on similarity between source and target tasks.
  - Why needed here: Understanding why SQuAD works well as a general QA source for domain-specific fine-tuning.
  - Quick check question: What metrics would you use to quantify the similarity between SQuAD and a domain-specific dataset?

- Concept: Catastrophic forgetting occurs when sequentially fine-tuning models on different tasks.
  - Why needed here: Explaining why the MWO strategy (merging SQuAD and target data) outperforms sequential fine-tuning.
  - Quick check question: How would you detect catastrophic forgetting in a fine-tuned model?

- Concept: Regularization through data diversity improves generalization.
  - Why needed here: Understanding why combining diverse SQuAD examples with domain-specific examples in each batch improves performance.
  - Quick check question: What other regularization techniques could be applied to domain-specific QA fine-tuning?

## Architecture Onboarding

- Component map: RoBERTa base model → SQuAD fine-tuning (optional) → MWO fine-tuning (SQuAD + target data)
- Critical path: Data preparation (merge SQuAD and target data) → Model fine-tuning (single step with merged data) → Evaluation
- Design tradeoffs: MWO strategy trades potential for more specialized knowledge (sequential fine-tuning) for better generalization and reduced risk of overfitting on small target datasets.
- Failure signatures: If the target domain is extremely dissimilar from SQuAD, MWO strategy may underperform sequential fine-tuning despite theoretical advantages.
- First 3 experiments:
  1. Compare MWO strategy with sequential fine-tuning on a small dataset (100-200 examples) to observe the performance gap.
  2. Test the effect of different merge ratios (e.g., 1:1 vs. 1:3 target:SQuAD) on model performance.
  3. Evaluate the impact of removing SQuAD entirely (target-only fine-tuning) to quantify the regularization benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal merge ratio between SQuAD and target datasets for maximizing QA performance under different budget constraints?
- Basis in paper: [inferred] The paper explores various merging strategies but does not systematically investigate different merge ratios between SQuAD and target datasets.
- Why unresolved: The paper only examines specific merging strategies with fixed ratios, leaving the optimal ratio unexplored for different domains and budget sizes.
- What evidence would resolve it: Systematic experiments varying the SQuAD-to-target dataset ratio across multiple domains and budget sizes, measuring performance to identify optimal ratios.

### Open Question 2
- Question: How do different fine-tuning strategies perform on other QA tasks beyond extractive QA, such as generative QA or multi-choice QA?
- Basis in paper: [explicit] The paper focuses specifically on extractive QA and acknowledges that other QA formats exist but are not explored.
- Why unresolved: The experiments are limited to extractive QA, and the paper does not investigate whether the findings generalize to other QA task formats.
- What evidence would resolve it: Comparative studies applying the same fine-tuning strategies to generative QA and multi-choice QA tasks across multiple datasets.

### Open Question 3
- Question: What is the impact of different corpus sizes for knowledge-alignment fine-tuning (MLM) on domain-specific QA performance?
- Basis in paper: [explicit] The paper concludes that MLM fine-tuning on small corpora does not significantly improve performance but does not systematically explore the relationship between corpus size and effectiveness.
- Why unresolved: The paper only tests MLM with the available domain corpora without varying corpus sizes to determine the threshold where MLM becomes beneficial.
- What evidence would resolve it: Experiments systematically varying the size of the domain corpus used for MLM fine-tuning while measuring QA performance across different domains.

### Open Question 4
- Question: How do the identified optimal fine-tuning strategies perform on larger language models beyond RoBERTa-Base, such as RoBERTa-Large or other transformer architectures?
- Basis in paper: [inferred] The paper uses RoBERTa-Base as the base model but does not explore whether the findings generalize to larger or different architectures.
- Why unresolved: The experiments are limited to a single model architecture and size, leaving questions about scalability and generalization to other models.
- What evidence would resolve it: Replicating the experiments with RoBERTa-Large, BERT variants, and other transformer architectures to compare performance across model sizes and types.

## Limitations

- Limited domain diversity: Only four domain-specific datasets tested, which may not generalize to all domain-specific QA scenarios.
- Fixed pre-trained model choice: All experiments use RoBERTa base model, optimal strategy might vary for different architectures.
- Annotation budget granularity: Budget sizes tested at 100-sample intervals, optimal budgets for specific domains may fall between these intervals.
- Implementation specifics: Exact implementation details of merging strategies and MLM hyperparameters not fully specified.

## Confidence

**High confidence**: The MWO fine-tuning strategy consistently outperforms standard sequential fine-tuning; small annotation budgets provide substantial improvements; MLM on small corpora does not improve performance.

**Medium confidence**: Doubling annotation effort from 200 to 1600 only results in 1-2% improvement; recommendation for 200 annotations with MWO or ≥1,600 for substantial gains.

## Next Checks

1. **Cross-model validation**: Test MWO strategy on different pre-trained models (BERT, T5, DeBERTa) to verify performance gains generalize beyond RoBERTa base.

2. **Domain similarity analysis**: Quantify similarity between SQuAD and each target domain using metrics like vocabulary overlap, embedding distance, and question type distribution to understand when MWO strategy breaks down.

3. **Budget granularity study**: Conduct experiments with 50-sample intervals between 100-400 annotations to identify precise inflection point where additional annotations yield diminishing returns.