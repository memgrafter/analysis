---
ver: rpa2
title: 'RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented
  Question Answering'
arxiv_id: '2407.13998'
source_url: https://arxiv.org/abs/2407.13998
tags:
- answer
- answers
- lfrqa
- rag-qa
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LFRQA, a new dataset for long-form retrieval-augmented
  question answering that addresses limitations in existing datasets by providing
  human-written long-form answers that integrate multiple short extractive answers
  into coherent narratives across seven different domains. The authors propose RAG-QA
  Arena, a model-based evaluation framework that directly compares model-generated
  answers against LFRQA's answers using LLMs as evaluators, demonstrating high correlation
  with human judgments.
---

# RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering

## Quick Facts
- arXiv ID: 2407.13998
- Source URL: https://arxiv.org/abs/2407.13998
- Authors: Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, Vittorio Castelli
- Reference count: 21
- Only 41.3% of GPT-4's answers are preferred over LFRQA's answers in pairwise comparison

## Executive Summary
This paper introduces LFRQA, a new dataset for long-form retrieval-augmented question answering that addresses limitations in existing datasets by providing human-written long-form answers that integrate multiple short extractive answers into coherent narratives across seven different domains. The authors propose RAG-QA Arena, a model-based evaluation framework that directly compares model-generated answers against LFRQA's answers using LLMs as evaluators, demonstrating high correlation with human judgments. The evaluation reveals that only 41.3% of GPT-4's answers are preferred over LFRQA's answers, establishing RAG-QA Arena as a challenging benchmark for future research. The study covers 26K queries across seven domains and shows that LFRQA's answers, which combine information from multiple documents, provide a more suitable evaluation target for modern LLMs that generate long-form responses.

## Method Summary
The paper presents a two-part contribution: LFRQA, a dataset of 26K queries across seven domains with human-written long-form answers that integrate multiple short answers into coherent narratives, and RAG-QA Arena, a model-based evaluation framework that uses LLM evaluators to perform pairwise comparisons between model-generated answers and LFRQA's answers. The evaluation uses ColBERTv2 to retrieve top 5 passages per query, generates answers using multiple LLMs, and evaluates using GPT-4-0125-PREVIEW as the evaluator. The framework employs an Elo rating system to rank models across domains, showing high correlation with human judgments while avoiding the limitations of token overlap metrics that penalize long-form answers.

## Key Results
- Only 41.3% of GPT-4's answers are preferred over LFRQA's answers in pairwise comparison
- LFRQA's long-form answer format shows Pearson correlation > 0.52 and Cohen's Kappa > 0.43 with human judgments
- Doubling retrieved passages from 5 to 10 significantly improves RAG-QA performance
- Elo rating system perfectly aligns with win ratio rankings across 43.6% more pairs

## Why This Works (Mechanism)

### Mechanism 1
LFRQA provides better evaluation ground truth than ROBUSTQA because it integrates multiple short answers into coherent long-form narratives. Modern LLMs generate long-form responses that combine information from multiple documents into coherent narratives, and LFRQA's format matches this output style while ROBUSTQA's short extractive answers don't, leading to unfair token overlap penalties.

### Mechanism 2
RAG-QA Arena's pairwise comparison approach provides more reliable evaluation than reference-based metrics. Instead of comparing LLM outputs to reference answers using token overlap, RAG-QA Arena directly compares LLM outputs against LFRQA answers using LLM evaluators. This avoids the need to examine potentially noisy retrieved passages.

### Mechanism 3
The Elo rating system provides reliable rankings across multiple domains. By accumulating pairwise comparisons across all models and domains, the Elo rating system can differentiate model performance while accounting for domain-specific variations.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) pipeline
  - Why needed here: Understanding how RAG works is crucial for grasping why LFRQA was created and how RAG-QA Arena evaluates systems
  - Quick check question: What are the two main components of a RAG system and what does each do?

- Concept: Cross-domain generalization
  - Why needed here: The paper focuses on evaluating domain robustness, so understanding what this means is essential
  - Quick check question: Why is it important to evaluate RAG systems on out-of-domain data rather than just in-domain data?

- Concept: Pairwise preference evaluation
  - Why needed here: RAG-QA Arena uses this approach, so understanding how it works is key to understanding the evaluation framework
  - Quick check question: How does pairwise preference evaluation differ from reference-based evaluation in terms of what it measures?

## Architecture Onboarding

- Component map: Retriever (ColBERTv2) → Passage selection (top K passages) → LLM answer generation → LLM-based pairwise evaluation → Elo rating computation
- Critical path: Retriever → Answer generation → Pairwise evaluation (this is where the system's value is realized)
- Design tradeoffs: Using LFRQA only for pairwise comparison vs. including retrieved passages reduces computational complexity from O(K²) to O(K) but may miss some context
- Failure signatures: Low correlation between model-based and human evaluations, significant performance drops in specific domains, or models consistently producing "no answer found" responses
- First 3 experiments:
  1. Run pairwise evaluation with a small subset of queries to verify correlation with human judgments
  2. Test the impact of varying the number of retrieved passages (K) on evaluation results
  3. Compare Elo ratings using different LLM evaluators to ensure consistency

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of RAG-QA answers vary when using different retriever models (beyond ColBERT-v2) on LFRQA? The paper acknowledges that studying different retrievers could be valuable but doesn't explore this dimension.

### Open Question 2
What is the impact of increasing the number of retrieved passages beyond 10 on RAG-QA performance, particularly for different model sizes? The paper only compares 5 vs 10 passages and doesn't explore whether there's a point of diminishing returns.

### Open Question 3
How does the "no answer found" ratio vary across different domains and query types in LFRQA, and what are the characteristics of queries that lead to this outcome? While the paper identifies this as an issue with one model, it doesn't provide systematic analysis across domains.

## Limitations
- Dataset construction focused on Wikipedia as the primary source, limiting domain coverage
- Evaluation framework relies heavily on LLM-based pairwise comparisons which may introduce evaluator biases
- Computational requirements for full evaluation (43.6% increase in pairs for Elo rating) may limit accessibility
- Study doesn't extensively explore impact of different retrieval strategies or passage selection methods

## Confidence

**High Confidence** (Pearson correlation > 0.52, Cohen's Kappa > 0.43 with human judgments):
- Effectiveness of LLM-based pairwise evaluation matching human preferences
- Superiority of LFRQA's long-form answer format for evaluating modern LLMs
- Correlation between Elo ratings and win ratio rankings

**Medium Confidence** (supported by experimental results but with potential confounding factors):
- Only 41.3% of GPT-4 answers are preferred over LFRQA answers
- RAG-QA Arena represents a challenging benchmark
- Effectiveness of the Elo rating system for cross-domain ranking

**Low Confidence** (requires additional validation):
- Generalizability of results to non-Wikipedia domains
- Scalability of evaluation framework to larger datasets
- Impact of different retrieval configurations on final rankings

## Next Checks

1. **Cross-domain generalization test**: Evaluate the RAG-QA Arena framework on a non-Wikipedia domain (e.g., biomedical or legal) to verify that the Elo rating system and pairwise comparisons maintain their effectiveness and correlation with human judgments.

2. **Retrieval strategy ablation**: Systematically vary the number of retrieved passages (K) and retrieval models to quantify their impact on final answer quality and Elo ratings, particularly for models showing significant performance drops.

3. **Evaluator bias analysis**: Test the pairwise evaluation framework with multiple evaluator models (including smaller models) to identify potential systematic biases and establish robustness thresholds for reliable evaluation.