---
ver: rpa2
title: Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback
  without Reward Inference
arxiv_id: '2409.17401'
source_url: https://arxiv.org/abs/2409.17401
tags:
- policy
- function
- reward
- human
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes two reinforcement learning from human feedback
  (RLHF) algorithms that avoid reward inference: Zeroth-Order Policy Gradient (ZPG)
  and Zeroth-Order Block-Coordinate Policy Gradient (ZBCPG). The key insight is using
  human preference feedback to estimate local value function differences between policies,
  then approximating policy gradients using zeroth-order optimization techniques.'
---

# Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference

## Quick Facts
- arXiv ID: 2409.17401
- Source URL: https://arxiv.org/abs/2409.17401
- Authors: Qining Zhang; Lei Ying
- Reference count: 40
- One-line primary result: Polynomial convergence rates for RLHF without reward inference using zeroth-order policy gradients

## Executive Summary
This paper introduces two reinforcement learning from human feedback (RLHF) algorithms that bypass reward inference by directly optimizing policies using human preference feedback. The proposed Zeroth-Order Policy Gradient (ZPG) and Zeroth-Order Block-Coordinate Policy Gradient (ZBCPG) algorithms estimate local value function differences between policies from human preferences and approximate policy gradients using zeroth-order optimization techniques. Both algorithms achieve polynomial convergence rates to stationary policies without requiring explicit reward modeling or inference.

## Method Summary
The method estimates policy gradients by perturbing current policy parameters along random directions, collecting trajectory pairs from both original and perturbed policies, and using human preference comparisons to estimate value function differences. This value difference serves as a zeroth-order gradient estimator that approximates the true policy gradient. ZBCPG extends this by using block-coordinate perturbation to reduce computational complexity while maintaining convergence guarantees. The algorithms query multiple human evaluators for each trajectory pair and apply a trimming operator to preference probability estimates for robustness.

## Key Results
- Polynomial convergence rates to ε-stationary policies with complexity O(d⁸H³/(ε⁵ log(d/ε)))
- Outperforms baseline RLHF methods (PPO with reward model, DPO, online DPO) on stochastic GridWorld
- Achieves better final policies under both Bradley-Terry and Weibull human feedback models
- ZBCPG provides computational advantages through parallel optimization while preserving convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local policy gradient estimation from human preference feedback enables reward-free RLHF optimization
- Mechanism: The algorithm perturbs current policy parameters along a random direction, collects trajectory pairs from both original and perturbed policies, and uses human preference comparisons to estimate the value function difference. This value difference serves as a zeroth-order gradient estimator that approximates the true policy gradient without requiring reward inference.
- Core assumption: Human preference feedback provides sufficient information about relative policy quality to enable local gradient estimation
- Evidence anchors: [abstract] states the key idea is estimating local value function differences from human preferences and approximating policy gradients with zeroth-order gradient approximators

### Mechanism 2
- Claim: Block-coordinate perturbation reduces computational complexity while maintaining convergence guarantees
- Mechanism: Instead of perturbing all policy parameters simultaneously, the algorithm samples a subset of coordinates and applies zeroth-order gradient estimation to these blocks. This enables parallel implementation and reduces memory requirements while preserving convergence properties.
- Core assumption: Partial parameter updates can approximate full gradient descent effectively for policy optimization
- Evidence anchors: [abstract] notes ZBCPG differs from ZPG in its policy perturbation rule, which has lower computational complexity and allows parallel optimization

### Mechanism 3
- Claim: Trimmed preference probability estimation provides robustness to human feedback noise
- Mechanism: The algorithm queries multiple human evaluators for each trajectory pair and applies a trimming operator to the empirical preference probability before passing it through the inverse link function. This prevents extreme values from amplifying estimation errors.
- Core assumption: Human feedback contains stochastic noise that can be mitigated through aggregation and clipping
- Evidence anchors: [section 3.1] explains trimming pt,n with a small constant to ensure finite variance after applying inverse link function

## Foundational Learning

- Concept: Zeroth-order optimization (gradient-free optimization using function evaluations)
  - Why needed here: Direct policy optimization from human preferences requires estimating gradients without access to explicit reward functions, making zeroth-order methods the natural approach
  - Quick check question: Can you explain why zeroth-order optimization is necessary when rewards are only available through human preferences rather than explicit values?

- Concept: Preference models and link functions (Bradley-Terry, Weibull models)
  - Why needed here: The algorithm relies on mapping human preference probabilities to value function differences through inverse link functions, requiring understanding of different preference model formulations
  - Quick check question: What are the key differences between Bradley-Terry and Weibull preference models in terms of their link functions?

- Concept: Lyapunov drift analysis for convergence proofs
  - Why needed here: The theoretical analysis establishes convergence rates by bounding the drift of a Lyapunov function, requiring familiarity with this optimization analysis framework
  - Quick check question: Can you explain how Lyapunov drift analysis differs from standard gradient descent convergence proofs?

## Architecture Onboarding

- Component map: Policy network (θ) -> Perturbation mechanism (v/block selection) -> Trajectory sampling -> Human preference query interface -> Value difference estimator -> Policy update module

- Critical path:
  1. Sample perturbation direction/block coordinates
  2. Generate trajectory pairs under original and perturbed policies
  3. Query human evaluators and aggregate preferences
  4. Compute value difference estimate via inverse link function
  5. Apply zeroth-order gradient approximation
  6. Update policy parameters

- Design tradeoffs:
  - Perturbation distance vs. estimation accuracy (larger perturbations increase variance, smaller ones increase bias)
  - Number of human queries per pair vs. convergence speed (more queries reduce noise but increase cost)
  - Block size vs. parallelization efficiency (larger blocks reduce parallelism, smaller blocks increase communication overhead)
  - Trim constant vs. robustness (larger trimming prevents outliers but may clip legitimate extreme preferences)

- Failure signatures:
  - Policy updates show high variance or oscillate → check perturbation magnitude and human query aggregation
  - Convergence stalls after initial progress → verify link function invertibility and gradient scaling
  - Performance degrades with increased parallelization → examine block size selection and communication patterns
  - Human query efficiency drops → investigate preference model mismatch or noisy feedback patterns

- First 3 experiments:
  1. Implement basic ZPG with synthetic preferences (deterministic Bradley-Terry model) to verify gradient estimation correctness
  2. Test block-coordinate implementation with varying block sizes to measure parallelization efficiency vs. convergence rate
  3. Compare trimmed vs. untrimmed preference aggregation on noisy synthetic feedback to validate robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would KL regularization affect the convergence rates and sample complexity of ZPG and ZBCPG?
- Basis in paper: The authors discuss KL regularization in RLHF applications and mention it could be incorporated into their algorithms with modifications to gradient estimation.
- Why unresolved: The paper does not provide theoretical analysis or experimental results for KL-regularized versions of ZPG and ZBCPG.
- What evidence would resolve it: Convergence rate theorems and experimental results comparing KL-regularized vs unregularized versions of both algorithms.

### Open Question 2
- Question: Can ZPG and ZBCPG achieve global convergence under practical assumptions on the value function landscape?
- Basis in paper: The authors explicitly state they only establish local convergence and conjecture global convergence might be possible under additional assumptions like convexity or PL condition.
- Why unresolved: The paper does not provide global convergence guarantees or identify which minimal assumptions would suffice.
- What evidence would resolve it: Global convergence proofs under specific, practically-relevant assumptions about the value function.

### Open Question 3
- Question: What is the minimal number of human queries required for practical performance of ZPG and ZBCPG?
- Basis in paper: The authors note both algorithms require many online human queries for accurate gradient estimation, which may limit practicality.
- Why unresolved: While the paper provides theoretical sample complexity bounds, it doesn't empirically determine the minimal query requirements for acceptable performance.
- What evidence would resolve it: Empirical studies showing performance degradation as human queries decrease, identifying practical query thresholds.

### Open Question 4
- Question: How do ZPG and ZBCPG perform with AI feedback instead of human feedback?
- Basis in paper: The authors mention AI feedback as a potential alternative to human feedback to reduce query requirements.
- Why unresolved: The paper does not explore or evaluate AI feedback implementations.
- What evidence would resolve it: Experimental results comparing human vs AI feedback performance on both algorithms.

## Limitations

- The polynomial sample complexity O(d⁸H³/(ε⁵ log(d/ε))) suggests poor scaling with dimension, raising questions about practical applicability to high-dimensional control tasks
- Theoretical guarantees rely heavily on smoothness assumptions for both policy parameterization and value function that may not hold for complex neural network policies
- Convergence proofs assume perfect preference feedback models, but real human feedback exhibits systematic biases and inconsistencies not captured in the analysis

## Confidence

- **High confidence**: The zeroth-order gradient estimation mechanism and its relationship to policy gradients - this follows established optimization theory with clear mathematical derivation
- **Medium confidence**: The block-coordinate optimization benefits and computational complexity reduction - theoretical analysis appears sound but practical gains depend heavily on implementation details
- **Medium confidence**: The robustness of trimmed preference estimation to noise - while the trimming approach is principled, the choice of trimming constant and its impact on convergence requires empirical validation

## Next Checks

1. **Scalability validation**: Test the algorithms on GridWorld environments with varying dimensions (e.g., 10x10, 20x20) to empirically verify the d⁸ dependence and identify practical limitations

2. **Noise sensitivity analysis**: Systematically vary the amount of noise in synthetic human feedback to quantify how the trimmed estimator compares to untrimmed approaches under realistic conditions

3. **Block size optimization**: Conduct ablation studies on block-coordinate performance across different block sizes and state-action partitionings to identify optimal configurations for various problem scales