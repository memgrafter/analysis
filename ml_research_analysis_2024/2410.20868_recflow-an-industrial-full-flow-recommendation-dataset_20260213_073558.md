---
ver: rpa2
title: 'RecFlow: An Industrial Full Flow Recommendation Dataset'
arxiv_id: '2410.20868'
source_url: https://arxiv.org/abs/2410.20868
tags:
- stage
- ranking
- samples
- user
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RecFlow is the first industrial full-flow recommendation dataset
  containing stage samples across six stages: retrieval, pre-ranking, coarse ranking,
  ranking, re-ranking, and edge ranking. Collected from 9.3 million online requests
  over 37 days, it includes 38 million exposure samples and 1.9 billion stage samples
  from 42K users across 9M items.'
---

# RecFlow: An Industrial Full Flow Recommendation Dataset

## Quick Facts
- arXiv ID: 2410.20868
- Source URL: https://arxiv.org/abs/2410.20868
- Reference count: 27
- First industrial full-flow recommendation dataset containing stage samples across six stages: retrieval, pre-ranking, coarse ranking, ranking, re-ranking, and edge ranking

## Executive Summary
RecFlow is the first industrial full-flow recommendation dataset that includes stage samples across all six stages of a typical recommendation pipeline. Collected from 9.3 million online requests over 37 days, it contains 38 million exposure samples and 1.9 billion stage samples from 42K users across 9M items. Unlike existing datasets that only capture exposed items, RecFlow includes unexposed items filtered at each stage, enabling study of distribution shift between training and serving spaces and multi-stage interplay.

## Method Summary
RecFlow was collected from a large-scale industrial recommendation system over 37 days, capturing request logs and stage samples across six recommendation stages. The dataset includes user behavior sequences, item features, context features, and stage-specific samples. For experimental validation, baseline models (SASRec for retrieval, DSSM for coarse ranking, DIN for ranking) were trained using day-by-day data from the first 36 days and evaluated on day 37. The key innovation involves using unexposed stage samples as hard negatives during training and incorporating stage-specific auxiliary ranking tasks.

## Key Results
- Stage samples as hard negatives improve retrieval model accuracy by providing challenging negative examples
- FS-LTR (Full Flow Learning to Rank) improves performance by learning both user preferences and subsequent stage preferences simultaneously
- Including stage samples as extra negatives during training reduces data distribution shift between training and serving spaces

## Why This Works (Mechanism)

### Mechanism 1: Hard Negative Mining
- **Claim:** Using unexposed stage samples as hard negatives improves retrieval model accuracy by providing challenging negative examples
- **Mechanism:** Stage samples filtered by later stages are more similar to positive items but were deemed less attractive, creating harder negative examples that better train the retrieval model to distinguish user preferences
- **Core assumption:** Items filtered at later stages are similar to exposed items but less preferred by users, making them valid hard negatives
- **Evidence anchors:** [abstract] "Unlike existing datasets, RecFlow includes samples not only from the exposure space but also unexposed items filtered at each stage"

### Mechanism 2: FS-LTR Multi-Task Learning
- **Claim:** FS-LTR improves performance by learning both user preferences and subsequent stage preferences simultaneously
- **Mechanism:** The model learns to score items that satisfy both user preferences (from positive feedback) and stage preferences (by ranking stage samples according to their stage priority)
- **Core assumption:** The preferences of subsequent stages correlate with user preferences, so modeling both jointly improves overall system performance
- **Evidence anchors:** [section 3.1.2] "we introduce additional ranking loss which forces the logits of samples from high-priority stages to be bigger than the logits of samples from low-priority stages"

### Mechanism 3: Distribution Shift Reduction
- **Claim:** Including stage samples as extra negatives during training reduces data distribution shift between training and serving spaces
- **Mechanism:** Training with samples from the serving space (stage samples) makes the training distribution more similar to the testing distribution, improving generalization
- **Core assumption:** The serving space (stage samples) represents the true distribution that the model encounters during inference
- **Evidence anchors:** [section 3.2.1] "We try to directly supplement the stage samples as extra negative samples into the training data"

## Foundational Learning

- **Concept: Multi-stage recommendation pipeline**
  - Why needed here: Understanding how items flow through retrieval → pre-ranking → ranking → re-ranking stages is crucial for interpreting RecFlow's structure and the stage sample relationships
  - Quick check question: What is the typical output size at each stage of a multi-stage recommendation pipeline?

- **Concept: Distribution shift**
  - Why needed here: The paper addresses how training on exposed samples creates a mismatch with the serving space; understanding this concept is essential for grasping the dataset's value
  - Quick check question: Why does training on exposed samples create a distribution shift problem?

- **Concept: Hard negative mining**
  - Why needed here: The paper uses unexposed stage samples as hard negatives; understanding this technique is crucial for interpreting the retrieval experiments
  - Quick check question: What makes a negative sample "hard" in recommendation systems?

## Architecture Onboarding

- **Component map:** Request logs → Stage sample processor → Feature engineering → Model training framework → Evaluation system
- **Critical path:** 1. Request arrives → multiple stages process items 2. Collect exposed items and representative unexposed items from each stage 3. Store with stage labels and user feedback 4. Engineer features (behavior sequences, context, etc.) 5. Train models with stage samples as hard negatives/auxiliary tasks 6. Evaluate using stage-aware metrics
- **Design tradeoffs:** Stage sample quantity vs storage (sampling strategy needed); Privacy vs utility (anonymization protects user data but may reduce signal quality); Stage completeness vs timeliness (more complete stage information takes longer to collect)
- **Failure signatures:** Models trained only on exposed samples show poor generalization to serving environment; Stage samples don't improve performance when they're too easy or too hard as negatives; Privacy protection overly aggressive, removing useful features
- **First 3 experiments:** 1. Baseline retrieval model using only exposed samples vs same model with stage samples as hard negatives 2. FS-LTR vs baseline with random negatives to test stage preference learning 3. Coarse ranking with extra stage negatives vs baseline to measure distribution shift reduction

## Open Questions the Paper Calls Out
1. How does the optimal ratio between easy and hard negative samples vary across different recommendation stages?
2. What is the impact of incorporating stage samples from all stages simultaneously versus a subset of stages in FS-LTR?
3. How does the effectiveness of the auxiliary ranking task and user behavior sequence modeling vary across different recommendation stages?

## Limitations
- The 37-day collection window might not represent long-term user behavior patterns
- Privacy protection measures could have removed subtle but important signals from the data
- Experimental improvements (1-4% relative gains) suggest benefits may be context-dependent

## Confidence
- **High Confidence:** The dataset's basic structure and composition (38M samples from 42K users across 9M items)
- **Medium Confidence:** The effectiveness of using stage samples as hard negatives and for auxiliary tasks
- **Medium Confidence:** The claim that stage samples reduce distribution shift

## Next Checks
1. **Longitudinal Stability Test:** Validate whether the distribution shift reduction and performance improvements persist across different time periods and seasonal variations in user behavior
2. **Privacy Impact Analysis:** Systematically measure the information loss from privacy protection measures by comparing model performance with and without these anonymization steps
3. **Cross-Stage Consistency Check:** Verify that stage samples from different stages (pre-rank neg, coarse neg, etc.) consistently behave as expected across different recommendation domains and item types