---
ver: rpa2
title: 'PairDistill: Pairwise Relevance Distillation for Dense Retrieval'
arxiv_id: '2410.01383'
source_url: https://arxiv.org/abs/2410.01383
tags:
- retrieval
- dense
- pairwise
- distillation
- reranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pairwise Relevance Distillation (PairDistill)
  to improve dense retrieval by leveraging the fine-grained training signals from
  pairwise rerankers. The method addresses the challenge of inconsistent comparisons
  in existing pointwise distillation approaches.
---

# PairDistill: Pairwise Relevance Distillation for Dense Retrieval

## Quick Facts
- arXiv ID: 2410.01383
- Source URL: https://arxiv.org/abs/2410.01383
- Reference count: 14
- This paper proposes Pairwise Relevance Distillation (PairDistill) to improve dense retrieval by leveraging fine-grained training signals from pairwise rerankers, achieving state-of-the-art performance on multiple benchmarks.

## Executive Summary
This paper addresses the challenge of improving dense retrieval models by leveraging the fine-grained training signals from pairwise rerankers. The proposed Pairwise Relevance Distillation (PairDistill) method combines pointwise and pairwise distillation losses with contrastive learning to train dense retrievers more effectively. By capturing relative relevance comparisons between documents, the method achieves significant performance improvements across multiple benchmarks including MS MARCO, BEIR, and LoTTE. The approach is shown to be effective across different dense retrieval architectures and demonstrates strong out-of-domain generalization capabilities.

## Method Summary
PairDistill is a knowledge distillation framework that leverages both pointwise and pairwise rerankers to improve dense retrieval training. The method uses KL divergence to match the retriever's output distribution with both absolute relevance scores from a pointwise reranker and relative comparisons from a pairwise reranker. The training combines contrastive learning with two distillation losses, sampling passage pairs based on rank proximity. An iterative training strategy refreshes the top-k document set in each iteration to prevent overfitting to static documents. The approach is architecture-agnostic and can be applied to various dense retrieval models.

## Key Results
- Achieves state-of-the-art performance on MS MARCO, BEIR, and LoTTE benchmarks
- Outperforms contrastive learning alone by significant margins across all evaluation datasets
- Demonstrates strong zero-shot domain adaptation capabilities using LLM-based reranking
- Shows effectiveness across different dense retrieval architectures including ColBERT and DPR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise relevance distillation provides finer-grained training signals than pointwise distillation by focusing on relative comparisons between documents.
- Mechanism: By modeling the probability that one document is more relevant than another (P(di ≻ dj | q)), the method captures subtle distinctions between similarly relevant documents that absolute relevance scores miss.
- Core assumption: Relative relevance comparisons between documents are more informative for training dense retrievers than absolute relevance scores.
- Evidence anchors:
  - [abstract] "pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models"
  - [section 4.1] "pairwise reranking produces better reranking results by comparing two passages simultaneously"
  - [corpus] Weak evidence - related work focuses on binary relevance or absolute scores rather than pairwise comparisons
- Break condition: If the pairwise reranker fails to provide meaningful relative comparisons, the additional complexity offers no benefit over pointwise distillation.

### Mechanism 2
- Claim: The proposed method improves retriever performance by iteratively refreshing the top-k document set during training.
- Mechanism: The iterative training strategy updates the retrieved document pool in each iteration, preventing overfitting to a static set and allowing the retriever to improve progressively.
- Core assumption: Static top-k documents limit the retriever's ability to learn diverse relevance patterns.
- Evidence anchors:
  - [section 4.3] "The iterative training allows for refreshing the retrieved documents in each iteration, avoiding training on the fixed set of documents"
  - [section 5.3.1] "The improvement converges after 2 iteration"
  - [corpus] Moderate evidence - iterative training is common in retrieval but specific to pairwise distillation is unclear
- Break condition: If the document pool doesn't change significantly between iterations, the computational overhead of iterative training provides no benefit.

### Mechanism 3
- Claim: The combination of contrastive learning with pointwise and pairwise distillation losses creates a more robust training signal.
- Mechanism: The three-loss framework (contrastive, pointwise distillation, pairwise distillation) addresses different aspects of the retrieval task - initial relevance matching, absolute score calibration, and relative ranking refinement.
- Core assumption: Different training signals complement each other and jointly improve retrieval performance more than any single approach.
- Evidence anchors:
  - [section 4.2] "Our proposed method can also be applied to scenarios where no labeled training data is available. In such cases, the contrastive loss LCL is discarded"
  - [section 5.1] "We use all 800K queries for knowledge distillation, while the 500K labeled queries are used for contrastive learning"
  - [corpus] Weak evidence - similar multi-loss approaches exist but specific combination is novel
- Break condition: If one loss type dominates training or provides conflicting signals, the combined approach may degrade performance.

## Foundational Learning

- Concept: KL divergence as a loss function for probability distribution matching
  - Why needed here: The method uses KL divergence to measure the difference between the retriever's predicted relevance distribution and the reranker's distribution
  - Quick check question: What property of KL divergence makes it suitable for measuring distributional similarity in distillation tasks?

- Concept: Contrastive learning objectives (InfoNCE loss)
  - Why needed here: The retriever is initially trained with contrastive learning before distillation, establishing a baseline relevance matching capability
  - Quick check question: How does the InfoNCE loss encourage the model to distinguish between relevant and irrelevant document pairs?

- Concept: Dual encoder architecture for efficient retrieval
  - Why needed here: The method builds on the dual encoder framework where queries and documents are encoded separately for efficient similarity computation
  - Quick check question: What is the computational advantage of using dual encoders compared to cross-encoder architectures for large-scale retrieval?

## Architecture Onboarding

- Component map:
  Dense Retriever (ColBERTv2/DPR) -> Pointwise Reranker (MiniLM) -> Pairwise Reranker (duoT5) -> PLAID Engine -> Training pipeline with three losses (contrastive, pointwise distillation, pairwise distillation)

- Critical path: Query → Dense Retriever → Top-k retrieval → Pointwise Reranking → Pairwise Reranking → Distillation losses → Updated Dense Retriever

- Design tradeoffs: The method trades increased computational cost during training (reranking top-k documents with both pointwise and pairwise models) for improved final retrieval performance. The use of ColBERT's late interaction mechanism provides better accuracy at the cost of more complex similarity computation compared to standard dot product.

- Failure signatures:
  - Training instability when pairwise and pointwise rerankers provide conflicting signals
  - Degraded performance if the pairwise reranker is poorly calibrated
  - Memory issues when sampling too many document pairs from large top-k sets

- First 3 experiments:
  1. Train ColBERTv2 with only contrastive loss, then with contrastive + pointwise distillation to verify the baseline improvement
  2. Add pairwise distillation loss to the previous setup to measure the isolated contribution of pairwise signals
  3. Test different values of the pairwise distillation weight λpair to find the optimal balance between loss components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PairDistill vary when using different architectures for the dense retriever beyond ColBERT and DPR?
- Basis in paper: [explicit] The paper mentions that PairDistill is effective across different architectures, specifically testing ColBERT and DPR, and states that the method is "agnostic to the architecture used for dense retrieval as long as it produces a relevance score for each query-passage pair."
- Why unresolved: The paper only explicitly tests PairDistill on ColBERT and DPR architectures, leaving uncertainty about its effectiveness on other dense retrieval architectures.
- What evidence would resolve it: Experimental results showing PairDistill performance on additional dense retrieval architectures such as ANCE, SPLADE, or late-interaction models with different interaction mechanisms.

### Open Question 2
- Question: What is the optimal strategy for selecting pairs of passages for pairwise distillation to maximize performance while minimizing computational cost?
- Basis in paper: [inferred] The paper uses a heuristic for pair sampling based on rank proximity (|i-j| < δ), but acknowledges this is a "simple heuristic" and mentions that the method "may pose challenges in terms of computational resources required for training" due to the need for more training pairs.
- Why unresolved: The current approach uses a basic heuristic without exploring more sophisticated pair selection strategies that could balance performance gains with computational efficiency.
- What evidence would resolve it: Comparative experiments testing various pair selection strategies (e.g., based on similarity scores, uncertainty, diversity, or active learning principles) and their impact on retrieval performance and computational requirements.

### Open Question 3
- Question: How does PairDistill perform when the pairwise reranker is trained on data from a different domain than the retrieval task?
- Basis in paper: [explicit] The paper demonstrates zero-shot domain adaptation using LLMs for instruction-based reranking and mentions that "our method holds potential for integration with other methods to achieve further improvements," but doesn't specifically test cross-domain pairwise reranker training.
- Why unresolved: While the paper shows effectiveness with in-domain pairwise rerankers and zero-shot LLM-based reranking, it doesn't investigate the scenario where the pairwise reranker is trained on one domain but used to distill knowledge for retrieval in another domain.
- What evidence would resolve it: Experiments where PairDistill is trained using pairwise rerankers trained on source domain data (e.g., MS MARCO) and then evaluated on target domains (e.g., BEIR datasets), comparing performance against using target-domain pairwise rerankers or other adaptation methods.

## Limitations
- The method's effectiveness depends critically on the quality of the pairwise reranker, with no clear guidance on minimum quality thresholds.
- The pair sampling heuristic (|i-j| < δ) may miss important training signals from more distant but relevant passages.
- The iterative training approach increases computational cost significantly without clear guidance on when benefits plateau.

## Confidence
- High confidence: The core mechanism of combining pointwise and pairwise distillation losses is well-supported by experimental results across multiple benchmarks.
- Medium confidence: The claim that pairwise signals provide "fine-grained distinctions" is supported by relative performance gains but lacks detailed ablation studies.
- Low confidence: The zero-shot domain adaptation results using LLMs for instruction-based reranking are promising but based on limited evaluation.

## Next Checks
1. Conduct controlled experiments varying the pairwise reranker quality (using smaller models or adding noise) to quantify its impact on final retrieval performance and establish failure thresholds.

2. Perform detailed ablation studies on the pair sampling heuristic parameters (δ and k) across different query difficulty levels to identify optimal settings and understand when pairwise distillation adds value versus noise.

3. Evaluate the method's robustness to noisy or incomplete relevance judgments by systematically degrading the quality of pairwise training signals and measuring performance degradation patterns.