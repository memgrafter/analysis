---
ver: rpa2
title: Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise
  Adaptivity
arxiv_id: '2410.08198'
source_url: https://arxiv.org/abs/2410.08198
tags:
- adam
- norm
- loss
- convergence
- smoothness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The key outcome of this work is the introduction of a novel convergence\
  \ analysis for Adam under \u2113\u221E-geometry, which explains its empirical advantage\
  \ over SGD in training large language models. By leveraging the coordinate-wise\
  \ adaptivity of Adam, the authors show that the (1,1)-norm of the Hessian\u2014\
  a measure of \u2113\u221E-smoothness\u2014serves as a tighter complexity metric\
  \ than the standard \u21132-based smoothness, particularly for deep learning models\
  \ like GPT-2 and ResNet."
---

# Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity

## Quick Facts
- arXiv ID: 2410.08198
- Source URL: https://arxiv.org/abs/2410.08198
- Authors: Shuo Xie; Mohamad Amin Mohamadi; Zhiyuan Li
- Reference count: 40
- Key outcome: Adam exploits ℓ∞-geometry of loss landscape via coordinate-wise adaptivity, achieving better convergence than rotation-equivariant methods like SGD on deep learning models

## Executive Summary
This paper provides a novel convergence analysis for Adam under ℓ∞-geometry, explaining its empirical advantage over SGD in training large language models. The authors show that Adam's coordinate-wise adaptivity allows it to exploit the (1,1)-norm of the Hessian as a tighter complexity metric than standard ℓ2-based smoothness measures. This geometric exploitation is rotation-sensitive, meaning Adam's advantage disappears when the loss function is rotated, while SGD remains unaffected due to its rotation-equivariance. The analysis is extended to blockwise Adam, which generalizes the adaptivity to structured parameter blocks.

## Method Summary
The paper introduces a convergence analysis framework for Adam that leverages the (1,1)-norm of the Hessian as a measure of ℓ∞-smoothness. The method involves measuring Hessian smoothness metrics, implementing Adam variants with β1=0 and β2∈[0,1], creating rotated loss functions via orthogonal transformations, and training models with various optimizer configurations. The blockwise Adam extension partitions parameters into blocks and normalizes gradients within each block. Hessian-vector products with Cauchy vectors are used for (1,1)-norm computation, and the RandPerm algorithm provides efficient parameter rotation.

## Key Results
- Adam achieves O(1/√T) convergence in deterministic settings and O((log T/T)^(1/4)) in stochastic settings when exploiting favorable ℓ∞-geometry
- Adam's performance degrades significantly when the loss function is rotated, while SGD remains stable due to rotation-equivariance
- The (1,1)-norm/ dimension exceeds spectral norm for GPT-2 and ResNet, explaining Adam's empirical advantage
- Blockwise Adam generalizes coordinate-wise adaptivity to structured parameter blocks with novel smoothness measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam exploits ℓ∞-geometry of the loss landscape via coordinate-wise adaptivity
- Mechanism: Adam's update direction aligns with the normalized steepest descent in ℓ∞-norm, making it sensitive to the (1,1)-norm of the Hessian, which captures coordinate-wise smoothness better than spectral norm for deep learning models
- Core assumption: The loss function exhibits favorable (1,1)-norm smoothness properties that correlate with optimization speed
- Evidence anchors:
  - [abstract] "we argue that the exploitation of nice ℓ∞-geometry is the key advantage of Adam over SGD"
  - [section 1] "SGD is rotation-equivariant, while Adam is only permutation equivariant" - rotation breaks Adam's advantage
  - [corpus] AdamW paper explicitly studies ℓ∞ norm constrained optimization, supporting this geometry-based explanation
- Break condition: If rotation changes the (1,1)-norm of Hessian without affecting spectral norm, Adam's advantage disappears (confirmed in experiments)

### Mechanism 2
- Claim: Blockwise Adam generalizes Adam's adaptivity to structured parameter blocks
- Mechanism: By partitioning parameters into blocks and normalizing gradients within each block, Adam can exploit block-wise smoothness properties while maintaining adaptivity
- Core assumption: The loss function has block-wise smoothness that differs across parameter partitions
- Evidence anchors:
  - [section 3.3] "blockwise Adam (Algorithm 3) whose convergence rate can be characterized by a novel smoothness measure"
  - [section 3.9] "H(L, Φ) is the smallest constant H such that L is H-smooth under partition Φ"
  - [corpus] Why Transformers Need Adam paper discusses block heterogeneity of Hessian matrix
- Break condition: If block-wise smoothness becomes uniform across all partitions, Adam's advantage over rotation-equivariant methods vanishes

### Mechanism 3
- Claim: Adam maintains stable training at larger learning rates than rotation-equivariant methods
- Mechanism: Coordinate-wise normalization provides implicit gradient clipping that prevents divergence at high learning rates
- Core assumption: The learning rate stability advantage is independent of rotation-equivariance properties
- Evidence anchors:
  - [section 4.2] "Another advantage of Adam over AdaSGD: it can maintain stable training at a larger learning rate"
  - [section 4.3] "When a larger learning rate is used, the performance of Adam is improved but the performance of rotated Adam becomes worse"
  - [corpus] AYLA paper discusses gradient sensitivity and non-convex optimization challenges
- Break condition: If the implicit clipping effect is disabled (e.g., by setting ϵ very large), Adam's learning rate advantage disappears

## Foundational Learning

- Concept: Rotation equivariance vs permutation equivariance
  - Why needed here: Understanding why Adam outperforms SGD requires distinguishing between these two types of equivariance
  - Quick check question: If you rotate the coordinate system, which optimizer's trajectory remains unchanged - SGD or Adam?

- Concept: ℓp-norms and their duals
  - Why needed here: The smoothness assumptions use different norms (ℓ∞ for Adam, ℓ2 for SGD) which affect convergence rates
  - Quick check question: What is the dual norm of ℓ∞-norm, and why is it relevant for Adam's convergence analysis?

- Concept: Block-wise smoothness and partitioned optimization
  - Why needed here: Blockwise Adam extends coordinate-wise adaptivity to structured parameter blocks
  - Quick check question: How does the smoothness constant H(L, Φ) change when you refine the partition Φ?

## Architecture Onboarding

- Component map:
  - Loss landscape characterization: (1,1)-norm vs spectral norm of Hessian
  - Optimizer variants: Adam (coordinate-wise), AdaSGD (block-wise with single block), SGD (rotation-equivariant)
  - Rotation mechanisms: RandPerm for efficient parameter rotation
  - Hessian estimation: Hessian-vector products with Cauchy vectors for (1,1)-norm computation

- Critical path:
  1. Measure Hessian smoothness metrics for target model
  2. Determine if rotation breaks Adam's advantage
  3. Select appropriate optimizer based on smoothness properties
  4. Tune learning rate considering implicit gradient clipping effects

- Design tradeoffs:
  - Memory vs adaptivity: Adam stores per-parameter second moments
  - Rotation sensitivity vs stability: Adam excels on favorable geometry but fails under rotation
  - Block size selection: Finer partitions increase adaptivity but computational cost

- Failure signatures:
  - Adam performance degrades significantly after parameter rotation
  - Training instability occurs at higher learning rates with rotation-equivariant methods
  - Convergence slows when (1,1)-norm/ dimension exceeds spectral norm

- First 3 experiments:
  1. Measure (1,1)-norm vs spectral norm of Hessian for your model
  2. Compare Adam vs rotated Adam on a small validation task
  3. Test learning rate stability across Adam, AdaSGD, and SGD variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the exploitation of ℓ∞-geometry by Adam specifically relate to the convergence speed advantage over SGD in large language models?
- Basis in paper: [explicit] The authors propose that Adam's coordinate-wise adaptivity exploits non-rotation-invariant properties of the loss landscape, particularly the ℓ∞-smoothness, which is not captured by standard ℓ2-based smoothness assumptions.
- Why unresolved: The paper provides theoretical and empirical evidence that Adam performs worse when the favorable ℓ∞-geometry is altered via rotation, while SGD remains unaffected. However, the exact mechanism by which Adam exploits ℓ∞-geometry to achieve faster convergence is not fully elucidated.
- What evidence would resolve it: Detailed analysis of the gradient updates and loss landscape geometry during training with Adam and SGD, comparing the impact of ℓ∞-smoothness on convergence rates.

### Open Question 2
- Question: What are the implications of using (1, 1)-norm of the Hessian as a smoothness measure for other optimization algorithms beyond Adam?
- Basis in paper: [explicit] The authors introduce the (1, 1)-norm of the Hessian as a novel complexity metric for the optimization problem, which yields a much better empirical smoothness constant for GPT-2 and ResNet models.
- Why unresolved: While the paper demonstrates the effectiveness of this measure for Adam and blockwise Adam, its applicability and benefits for other optimization algorithms are not explored.
- What evidence would resolve it: Empirical studies comparing the performance of various optimization algorithms using the (1, 1)-norm of the Hessian as a smoothness measure, and theoretical analysis of its impact on convergence rates.

### Open Question 3
- Question: How does the choice of learning rate affect the performance of Adam and its variants in exploiting ℓ∞-geometry?
- Basis in paper: [explicit] The authors find that Adam can maintain stable training at a larger learning rate, which is often beneficial to faster and more efficient convergence, while rotated Adam and AdaSGD perform worse with larger learning rates.
- Why unresolved: The paper does not provide a detailed analysis of how the learning rate interacts with the ℓ∞-geometry exploitation by Adam, and how this affects the convergence speed and stability.
- What evidence would resolve it: Systematic experiments varying the learning rate for Adam, rotated Adam, and AdaSGD, and analyzing the impact on convergence rates and loss landscape geometry.

### Open Question 4
- Question: What are the limitations of the proposed blockwise Adam algorithm, and how can it be further improved?
- Basis in paper: [explicit] The authors extend the convergence analysis to blockwise Adam, which contains both Adam and AdaSGD as special cases, and introduce novel smoothness measures corresponding to the set of blocks used in blockwise Adam.
- Why unresolved: The paper does not discuss the potential limitations of the blockwise Adam algorithm, such as the choice of block partition and the impact on convergence rates, or how it can be further improved to exploit the ℓ∞-geometry more effectively.
- What evidence would resolve it: Comparative studies of different block partition strategies for blockwise Adam, and theoretical analysis of the impact of block size and shape on convergence rates and loss landscape geometry.

## Limitations
- The connection between (1,1)-norm smoothness and optimization performance requires further validation across diverse model architectures
- Theoretical results assume deterministic gradients, while practical deep learning uses stochastic gradients
- Blockwise Adam's practical benefits depend heavily on problem-specific partition choices that aren't fully characterized
- The claim about (1,1)-norm vs spectral norm being predictive of optimization speed is based on limited experimental evidence (GPT-2 and ResNet only)

## Confidence
**High Confidence**: The core claim that Adam exploits ℓ∞-geometry via coordinate-wise adaptivity is well-supported by both theoretical analysis and empirical verification (rotation experiments clearly show Adam's performance degradation).

**Medium Confidence**: The extension to blockwise Adam is theoretically sound but the practical benefits depend heavily on problem-specific partition choices that aren't fully characterized.

**Low Confidence**: The claim about (1,1)-norm vs spectral norm being predictive of optimization speed across all deep learning models is based on limited experimental evidence.

## Next Checks
1. **Cross-architecture validation**: Measure (1,1)-norm and spectral norm smoothness across diverse architectures (vision transformers, diffusion models, recommendation systems) to verify the claimed correlation with Adam's performance advantage.

2. **Partition sensitivity analysis**: Systematically evaluate blockwise Adam performance across different partition strategies (random, structured, learned) to identify patterns in optimal partition selection for specific model types.

3. **Learning rate stability experiment**: Directly test the implicit gradient clipping hypothesis by comparing Adam's behavior at high learning rates with and without the β2 parameter, and against explicit gradient clipping methods.