---
ver: rpa2
title: 'DeformTime: Capturing Variable Dependencies with Deformable Attention for
  Time Series Forecasting'
arxiv_id: '2406.07438'
source_url: https://arxiv.org/abs/2406.07438
tags:
- forecasting
- time
- series
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeformTime improves multi-variable time series forecasting by using
  deformable attention blocks (DABs) to learn variable and temporal dependencies.
  It introduces variable DAB (V-DAB) to capture inter-variable dependencies across
  time and temporal DAB (T-DAB) to capture intra-variable dependencies.
---

# DeformTime: Capturing Variable Dependencies with Deformable Attention for Time Series Forecasting

## Quick Facts
- arXiv ID: 2406.07438
- Source URL: https://arxiv.org/abs/2406.07438
- Reference count: 40
- Key outcome: DeformTime improves multi-variable time series forecasting by using deformable attention blocks (DABs) to learn variable and temporal dependencies, reducing MAE by 7.2% on average across six benchmark and disease forecasting tasks.

## Executive Summary
DeformTime introduces a novel architecture for multi-variable time series forecasting that leverages deformable attention blocks to capture both inter-variable and intra-variable dependencies. The method employs variable DAB (V-DAB) to learn cross-variable relationships across time steps and temporal DAB (T-DAB) to preserve temporal dependencies within each variable. By incorporating neighborhood-aware input embedding (NAE) and positional embeddings, DeformTime demonstrates state-of-the-art performance, achieving an average 7.2% reduction in MAE compared to competitive models across diverse forecasting tasks.

## Method Summary
DeformTime is a transformer-based architecture for multi-variable time series forecasting that uses deformable attention blocks to capture variable and temporal dependencies. The method includes a neighborhood-aware input embedding (NAE) that reorders variables by correlation with the target and embeds them in groups, followed by positional embeddings. The encoder consists of two layers, each containing V-DAB for cross-variable attention across time and T-DAB for intra-variable attention at multiple temporal resolutions. A 2-layer GRU decoder processes the encoder output, followed by an MLP for forecasting. The model is trained using MSE loss with Adam optimizer and evaluated using MAE and sMAPE metrics on benchmark and disease forecasting tasks.

## Key Results
- DeformTime reduces MAE by 7.2% on average across six benchmark and disease forecasting tasks
- The method outperforms competitive models including vanilla transformers, Informer, FEDformer, Autoformer, and Pyraformer
- Performance gains are consistent across longer forecasting horizons and different types of time series data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variable Deformable Attention Block (V-DAB) learns inter-variable dependencies by adaptively deforming the receptive field across both time and variables.
- Mechanism: V-DAB segments input along the temporal dimension, then uses a 2D CNN to compute position offsets for each element. Bilinear interpolation samples key/value embeddings from deformed positions, allowing cross-variable attention across neighboring time steps.
- Core assumption: The learned offsets capture meaningful cross-variable relationships that are not aligned in time.
- Evidence anchors:
  - [abstract]: "learning dependencies across variables from different time steps (variable DAB)"
  - [section 3.3]: "V-DAB aims to capture inter-variable dependencies across time. This is achieved by performing cross-attention over embeddings of both endo- and exogenous variables within proximal time steps."
  - [corpus]: Weak—no direct mentions of deformable cross-variable attention in neighbors.
- Break condition: If learned offsets do not correlate with actual cross-variable dependencies, V-DAB adds no value over standard self-attention.

### Mechanism 2
- Claim: Temporal Deformable Attention Block (T-DAB) preserves intra-variable temporal dependencies at multiple resolutions by segmenting input along the variable dimension and deforming along time.
- Mechanism: T-DAB transforms each variable's time series into a 2D matrix of different temporal resolutions (controlled by r), partitions variables into correlated groups, and applies deformable sampling to capture dependencies across time within each group.
- Core assumption: Grouping variables by correlation with the target variable creates natural clusters whose temporal dependencies can be modeled jointly.
- Evidence anchors:
  - [abstract]: "preserving temporal dependencies in data from previous time steps (temporal DAB)"
  - [section 3.4]: "T-DAB module... to capture intra-variable dependencies across different time periods... we set the time window r to different values across encoder layers, enabling the model to attend to information at multiple temporal granularities."
  - [corpus]: Weak—no neighbor mentions of variable-grouped temporal deformable attention.
- Break condition: If variable grouping does not align with actual temporal dependency structures, T-DAB performance degrades.

### Mechanism 3
- Claim: Neighborhood-aware input embedding (NAE) improves variable dependency learning by embedding correlated variable groups separately, reducing interference.
- Mechanism: Variables are reordered by correlation with target, then embedded in G groups of size d/G. Each group's embedding is concatenated, allowing the model to preserve local correlation structures before deformation.
- Core assumption: Variables with high correlation to the target share latent dependencies that benefit from joint embedding.
- Evidence anchors:
  - [section 3.2]: "We first re-arrange the order of variables in the input Z, ranking them based on their linear correlation with the target variable... embed G neighbouring groups of variables, each one to a shortened embedding of size d/G, using a fully connected layer."
  - [section 4.3 ablation]: "The inclusion of the NAE component is equally important as it improves MAE by 6.4% on average."
  - [corpus]: Weak—no neighbor mentions of neighborhood-aware embedding for MTS forecasting.
- Break condition: If correlation-based grouping does not reflect true dependency structure, NAE adds no value.

## Foundational Learning

- Concept: Attention mechanisms and multi-head attention
  - Why needed here: DeformTime builds on transformer attention but modifies it with deformable offsets; understanding attention is prerequisite to grasping V-DAB/T-DAB.
  - Quick check question: In multi-head attention, how are query, key, and value projections computed from the input, and how is the final output formed?

- Concept: Positional encodings and embeddings
  - Why needed here: DeformTime uses both fixed sinusoidal and learned relative positional embeddings to maintain sequence order after deformation.
  - Quick check question: What is the difference between absolute positional encodings and relative positional biases in transformer models?

- Concept: Bilinear and linear interpolation for sampling
  - Why needed here: Deformation in V-DAB uses bilinear interpolation over 2x2 grids; T-DAB uses linear interpolation along time. Understanding these is essential for implementing deformable sampling.
  - Quick check question: How does bilinear interpolation determine the value at a non-integer coordinate from four surrounding integer coordinates?

## Architecture Onboarding

- Component map: Input → NAE → Positional embedding → Encoder (2 layers with V-DAB + T-DAB) → Decoder (2-layer GRU) → MLP → Forecast

- Critical path: NAE → Encoder (V-DAB + T-DAB) → Decoder (GRU) → Output projection

- Design tradeoffs:
  - Segmentation length ℓ vs. receptive field: shorter ℓ reduces complexity but may miss long-range dependencies
  - Number of groups G vs. embedding granularity: larger G preserves finer correlation structure but increases parameter count
  - Time window r per encoder layer vs. temporal resolution: larger r captures longer patterns but may lose short-term detail

- Failure signatures:
  - Degraded performance with very large or very small G values
  - Overfitting when number of input variables is large relative to embedding dimension
  - Memory blow-up when segmentation length ℓ is too small relative to look-back window L

- First 3 experiments:
  1. Ablation: Remove V-DAB and evaluate on ILI tasks to confirm inter-variable dependency contribution
  2. Sensitivity: Vary G (grouping) and r (temporal window) on ETTh1 to find optimal configurations
  3. Stress test: Increase exogenous variables C to 512 on ILI tasks to measure scalability and memory impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal value of the grouping parameter G in DeformTime's NAE module vary across different types of time series forecasting tasks (e.g., univariate vs. multivariate, short-term vs. long-term forecasting)?
- Basis in paper: [explicit] The paper mentions that "more groups tend to benefit performance for shorter forecasting horizons, whereas a more moderate level of grouping is required for the optimal performance as the forecasting horizon increases."
- Why unresolved: The paper only explores a limited range of G values (2, 4, 8, 16) and does not provide a comprehensive analysis of how G should be selected based on the specific characteristics of the forecasting task.
- What evidence would resolve it: Experiments that systematically vary G across a wider range of values and different types of time series forecasting tasks, potentially including an automated method for selecting G based on task characteristics.

### Open Question 2
- Question: What is the impact of different interpolation methods (e.g., bilinear vs. linear) on the performance of DeformTime's deformable attention blocks?
- Basis in paper: [explicit] The paper mentions that "bilinear interpolation" is used in V-DAB and "linear interpolation" is used in T-DAB, but does not provide a comparison of different interpolation methods.
- Why unresolved: The choice of interpolation method could significantly impact the performance of the deformable attention blocks, and it is unclear whether the chosen methods are optimal for the tasks considered.
- What evidence would resolve it: Experiments that compare the performance of DeformTime using different interpolation methods in its deformable attention blocks across various time series forecasting tasks.

### Open Question 3
- Question: How does the performance of DeformTime compare to other state-of-the-art models when using longer look-back windows (e.g., several years of data) for forecasting tasks?
- Basis in paper: [inferred] The paper mentions that "increasing the look-back window does not overly affect memory consumption" for DeformTime, suggesting that it may be well-suited for tasks with longer look-back windows. However, the experiments only use look-back windows of up to 336 time steps.
- Why unresolved: The performance of time series forecasting models can vary significantly depending on the length of the look-back window, and it is unclear how DeformTime performs compared to other models when using much longer look-back windows.
- What evidence would resolve it: Experiments that compare the performance of DeformTime and other state-of-the-art models using look-back windows of several years of data for various time series forecasting tasks.

## Limitations

- The method's memory and computational complexity scale with the number of input variables and look-back window length, potentially limiting scalability to very large datasets.
- Performance generalizability to other domains or data characteristics beyond the six evaluated benchmark tasks remains unclear.
- Several implementation specifics are underspecified, including exact hyperparameter configurations and handling of missing data in ILI forecasting.

## Confidence

**High Confidence**:
- DeformTime achieves state-of-the-art performance on the evaluated benchmark datasets, with average 7.2% MAE reduction compared to competitive models
- The V-DAB and T-DAB modules function as described and contribute to improved forecasting accuracy based on ablation results
- The NAE component provides measurable performance improvements (6.4% MAE reduction)

**Medium Confidence**:
- The mechanism by which V-DAB captures inter-variable dependencies through deformable sampling is theoretically sound and empirically validated
- The variable grouping strategy based on correlation with target variables effectively captures meaningful dependency structures
- The combination of fixed and learned positional embeddings adequately preserves sequence order after deformation

**Low Confidence**:
- The specific choice of segmentation length ℓ = 2 across all experiments is optimal for all datasets and tasks
- The learned deformation offsets consistently capture true cross-variable relationships rather than spurious correlations
- The method's performance advantage will generalize to datasets with significantly different characteristics

## Next Checks

1. **Ablation Study Extension**: Conduct comprehensive ablation experiments removing individual components (V-DAB, T-DAB, NAE) on all six benchmark tasks to quantify each component's contribution and test for interaction effects between components.

2. **Robustness Testing**: Evaluate DeformTime on synthetic MTS datasets with controlled variable dependencies and temporal patterns to verify that the method captures intended relationships rather than dataset-specific artifacts.

3. **Scalability Analysis**: Systematically vary the number of input variables and look-back window length to characterize the method's computational and memory scaling behavior, identifying practical limits and potential mitigation strategies.