---
ver: rpa2
title: 'Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared,
  and Transferred Knowledge'
arxiv_id: '2403.05189'
source_url: https://arxiv.org/abs/2403.05189
tags:
- languages
- knowledge
- facts
- cross-lingual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how multilingual language models acquire
  and represent factual knowledge across different languages. Using the mLAMA probing
  dataset, the authors examine two models (mBERT and XLM-R) and identify three distinct
  patterns of fact representation: language-independent, cross-lingual shared, and
  cross-lingual transferred.'
---

# Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge

## Quick Facts
- arXiv ID: 2403.05189
- Source URL: https://arxiv.org/abs/2403.05189
- Reference count: 16
- This study identifies three patterns of fact representation in multilingual language models: language-independent, cross-lingual shared, and cross-lingual transferred.

## Executive Summary
This paper investigates how multilingual language models (ML-LMs) acquire and represent factual knowledge across different languages. Using the mLAMA probing dataset, the authors examine two models (mBERT and XLM-R) and identify three distinct patterns of fact representation: language-independent, cross-lingual shared, and cross-lingual transferred. They develop methods to distinguish between these patterns by analyzing active neurons and tracing facts back to their sources in Wikipedia. The findings reveal that while cross-lingual transfer of factual knowledge does occur, it is limited, with most facts being either language-independent or shared through simple inference mechanisms. This highlights the challenges in achieving effective cross-lingual factual knowledge transfer and underscores the need for better fact representation learning in multilingual language models.

## Method Summary
The authors conduct factual probing experiments using mLAMA on mBERT and XLM-R, analyzing neuron activity patterns with PROBELESS to identify active neurons for correctly predicted facts. They trace facts back to Wikipedia by checking subject-object co-occurrence in the pretraining corpus, classifying facts into three types: language-independent (distinct neuron patterns across languages), cross-lingual shared (similar neuron patterns), and cross-lingual transferred (absent from training data but correctly predicted). The analysis reveals that cross-lingual transfer of factual knowledge is limited, with most facts being either language-independent or shared through simple inference mechanisms like shared entity tokens and naming cues.

## Key Results
- Cross-lingual transfer of factual knowledge in ML-LMs is limited, with only a small fraction of facts being truly transferred across languages
- Most facts are either language-independent (represented differently across languages) or shared through simple inference mechanisms
- The study develops a knowledge tracing method to verify whether facts predicted by ML-LMs in one language exist in the training data for that language
- Shared entity tokens and naming cues account for many predictions of facts absent from the training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ML-LMs acquire and represent factual knowledge through three distinct patterns: language-independent, cross-lingual shared, and cross-lingual transferred.
- Mechanism: The model uses different neural representations for the same fact across languages, with some facts being encoded independently in each language, some using shared neurons, and some being transferred from one language to another during pretraining.
- Core assumption: The same fact can be represented differently across languages within the same model, and these representations can be distinguished through neuron-level analysis and tracing facts back to training data.
- Evidence anchors:
  - [abstract] "We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them."
  - [section 5] "Our findings indicate that while some languages exhibit similar neuron activity patterns for a given fact, others exhibit distinct distributions, as depicted in Figure 5. This indicates the presence of both language-independent and cross-lingual fact representations in ML-LMs, even for the same fact."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.471, average citations=0.0. Top related titles: Tracing Multilingual Factual Knowledge Acquisition in Pretraining, Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation, Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations.

### Mechanism 2
- Claim: The effectiveness of cross-lingual factual knowledge transfer is limited and varies based on the presence of shared entity tokens and naming cues in the facts.
- Mechanism: ML-LMs can predict facts absent in the training data for a language by using simple inference based on shared subwords between subject and object entities or by recognizing patterns in naming conventions (e.g., person names and nationalities).
- Core assumption: Some facts can be predicted without explicit training data through pattern recognition and inference rather than true knowledge transfer.
- Evidence anchors:
  - [section 6.2] "Analysis revealed that many of the facts that were absent in the knowledge source but correctly predicted were relatively easy to predict. We categorized these easy-to-predict facts into two types: shared entity tokens and naming cues."
  - [section 6.2] "Figure 8 shows the proportions of facts correctly predicted without knowledge sources by mBERT for the three types."
  - [corpus] Weak evidence - no direct citations found for this specific mechanism of using shared tokens and naming cues for inference.

### Mechanism 3
- Claim: Cross-lingual fact representations are formed through a combination of individual learning from distinct language corpora and subsequent alignment into a common semantic space, with some facts being acquired through cross-lingual transfer.
- Mechanism: During pretraining, ML-LMs first learn facts from each language's corpus independently, then align these representations across languages. Some facts are directly transferred from one language to another when the model encounters them in one language and then applies them to another language with similar context.
- Core assumption: The pretraining process involves both independent learning and cross-lingual alignment, and some facts can be transferred across languages during this process.
- Evidence anchors:
  - [section 6] "To identify the reason behind the formation of a cross-lingual representation, it is crucial to verify if the fact originates from the training data. We used a simple yet effective method to check the presence of a fact in text: for a fact triplet (subject, relation, object), we examined the occurrences of the subject and object in mBERT's training data, Wikipedia."
  - [section 6.2] "Our statistics show that while cross-lingual transfer of factual knowledge in ML-LMs does occur, it is limited, highlighting the challenges in achieving effective cross-lingual factual knowledge transfer."
  - [corpus] Weak evidence - while related papers exist, none directly support this specific mechanism of cross-lingual transfer formation.

## Foundational Learning

- Concept: Neuron-level analysis for fact representation
  - Why needed here: To distinguish between language-independent and cross-lingual fact representations within the model's parameter space
  - Quick check question: Can you explain how PROBELESS identifies active neurons for specific facts and how this differs from analyzing entire layers or attention heads?

- Concept: Cross-lingual transfer in multilingual language models
  - Why needed here: To understand how ML-LMs acquire factual knowledge in low-resource languages through transfer from high-resource languages
  - Quick check question: What are the key differences between the mechanisms of cross-lingual transfer for linguistic knowledge versus factual knowledge in ML-LMs?

- Concept: Knowledge tracing in pretraining data
  - Why needed here: To verify whether facts predicted by ML-LMs in one language actually exist in the training data for that language
  - Quick check question: How does subject-object co-occurrence in Wikipedia text serve as an approximation for fact presence, and what are its limitations?

## Architecture Onboarding

- Component map:
  ML-LMs (mBERT, XLM-R) -> mLAMA dataset -> Wikipedia (pretraining data) -> PROBELESS (neuron activity measurement) -> Knowledge tracing system

- Critical path:
  1. Load ML-LM and mLAMA dataset
  2. Perform factual probing using full-match method
  3. Analyze neuron activity for correctly predicted facts
  4. Trace roots of facts back to Wikipedia
  5. Classify facts into three types (language-independent, shared, transferred)
  6. Analyze patterns and limitations

- Design tradeoffs:
  - Using full-match vs. partial-match probing methods: Full-match provides cleaner data but may miss some valid predictions; partial-match captures more predictions but introduces noise
  - Wikipedia-based knowledge tracing: Simple and effective but may miss facts expressed in different ways than subject-object pairs
  - Neuron analysis focus: Concentrating on mask token neurons provides specific fact signatures but may miss broader context

- Failure signatures:
  - Low P@1 scores across all languages indicate general probing difficulties
  - Inconsistent neuron patterns across languages for the same fact suggest language-independent representation
  - High rates of predictable absent facts indicate reliance on inference rather than true knowledge transfer
  - Strong correlation between training data volume and P@1 suggests limited cross-lingual transfer

- First 3 experiments:
  1. Reproduce factual probing results with mBERT using full-match method on mLAMA to establish baseline performance
  2. Implement PROBELESS neuron analysis on correctly predicted facts to identify active neurons and compare across languages
  3. Develop Wikipedia knowledge tracing system to check subject-object co-occurrence for predicted facts and classify into three types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact contribution of cross-lingual transfer to factual knowledge acquisition in low-resource languages?
- Basis in paper: explicit
- Why unresolved: The study found that cross-lingual transfer does occur but is limited, and the authors call for a more precise evaluation of its contribution.
- What evidence would resolve it: A controlled experiment comparing factual knowledge acquisition in low-resource languages with and without cross-lingual transfer capabilities.

### Open Question 2
- Question: How can factual probing datasets be improved to better evaluate model proficiency in fact representation?
- Basis in paper: explicit
- Why unresolved: The study identified limitations in the current mLAMA dataset, such as the restriction to a single standard format for answers, which does not reflect the diversity in entity expressions in text.
- What evidence would resolve it: Development and evaluation of a new factual probing dataset that incorporates more diverse entity expressions and fact types.

### Open Question 3
- Question: What are the specific mechanisms by which multilingual language models learn and represent cross-lingual shared facts?
- Basis in paper: inferred
- Why unresolved: The study identified the existence of cross-lingual shared facts but did not fully explore the underlying mechanisms of how these facts are learned and represented.
- What evidence would resolve it: A detailed analysis of the learning dynamics and internal representations of multilingual language models during the acquisition of cross-lingual shared facts.

## Limitations

- The Wikipedia-based knowledge tracing method relies on simple subject-object co-occurrence, which may miss facts expressed in different syntactic constructions
- Neuron activity analysis focuses only on mask token neurons, potentially overlooking broader contextual representations
- The distinction between "transferred" and "shared" facts may be somewhat arbitrary, as both involve cross-lingual mechanisms
- The study uses only two ML-LM architectures (mBERT and XLM-R), limiting generalizability to other models

## Confidence

**High Confidence:** The finding that cross-lingual transfer of factual knowledge is limited, based on the relatively low rates of facts correctly predicted without training data presence and the categorization of easily predictable absent facts into shared tokens and naming cues. The methodology for knowledge tracing and the statistical analysis of transfer rates are straightforward and reproducible.

**Medium Confidence:** The three-pattern classification of fact representations (language-independent, shared, transferred) is well-supported by the neuron activity analysis and knowledge tracing, but the boundaries between these categories may be less clear than presented. The analysis provides compelling evidence but relies on several assumptions about how to interpret neuron patterns and training data absence.

**Low Confidence:** The specific mechanisms proposed for how cross-lingual fact representations form (independent learning followed by alignment, with some facts being transferred) are somewhat speculative given the available evidence. While the study identifies patterns, the causal mechanisms behind these patterns require further investigation with more targeted experiments.

## Next Checks

1. **Validate the knowledge tracing method** by manually checking a sample of facts classified as "transferred" to verify whether they truly exist in the training data but in different surface forms than subject-object pairs. This would test whether the co-occurrence method is too conservative and missing valid facts.

2. **Conduct ablation studies on neuron analysis** by comparing mask token neuron activity with full-layer attention head analysis for the same facts across languages. This would determine whether focusing on mask token neurons provides a complete picture of fact representation or misses important cross-lingual alignment mechanisms.

3. **Test the inference mechanism hypothesis** by creating controlled mLAMA instances that vary the presence of shared entity tokens and naming cues while keeping the core fact constant. This would provide stronger evidence for whether the model is truly transferring knowledge or simply applying pattern-based inference.