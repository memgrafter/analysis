---
ver: rpa2
title: Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language
  Models
arxiv_id: '2402.19449'
source_url: https://arxiv.org/abs/2402.19449
tags: []
core_contribution: This paper investigates why the Adam optimizer outperforms gradient
  descent (GD) on large language models, focusing on the role of heavy-tailed class
  imbalance in language tasks. The authors find that GD makes slower progress on infrequent
  classes, leading to slow overall training, while Adam and sign-based methods are
  less sensitive to this issue.
---

# Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models

## Quick Facts
- arXiv ID: 2402.19449
- Source URL: https://arxiv.org/abs/2402.19449
- Authors: Frederik Kunstner; Robin Yadav; Alan Milligan; Mark Schmidt; Alberto Bietti
- Reference count: 40
- Primary result: Adam outperforms GD on language models due to heavy-tailed class imbalance affecting infrequent classes

## Executive Summary
This paper investigates why Adam optimizer consistently outperforms gradient descent (GD) on large language models. The authors identify heavy-tailed class imbalance as the key factor - language tasks exhibit highly imbalanced word frequencies where infrequent words dominate total loss. While GD makes slow progress on these rare classes, Adam and sign-based methods are less sensitive to this imbalance, leading to faster overall training. The authors demonstrate this behavior across various architectures and data types, providing both empirical evidence and theoretical analysis.

## Method Summary
The authors conduct comprehensive experiments comparing Adam and GD across different architectures including language transformers, vision CNNs, and linear models. They analyze gradient statistics, train linear models with cross-entropy loss on imbalanced datasets, and perform continuous-time convergence analysis. The study systematically examines how class imbalance affects gradient and Hessian structures, comparing optimization dynamics between different methods. Experiments span both synthetic and real-world datasets to validate findings across multiple scenarios.

## Key Results
- GD makes slower progress on infrequent classes, leading to slow overall training
- Adam and sign-based methods are less sensitive to class imbalance issues
- Class imbalance leads to imbalanced, correlated gradients and Hessians in linear models
- In continuous time, GD converges slowly on low-frequency classes while sign descent does not

## Why This Works (Mechanism)
The heavy-tailed class imbalance in language data creates a scenario where rare words contribute disproportionately to the total loss. GD, which relies on magnitude-based updates, struggles to make efficient progress on these infrequent but important classes. Adam, by using adaptive learning rates and sign-based updates, can better handle this imbalance. The correlation structure between gradients and Hessians for different classes further compounds GD's difficulties, while Adam's normalization mechanism mitigates these effects.

## Foundational Learning
1. **Class Imbalance Effects** - Understanding how frequency distributions affect optimization dynamics; quick check: verify power-law distribution in language data
2. **Gradient Correlation Structure** - How class correlations impact optimization; quick check: compute gradient cosine similarities across classes
3. **Continuous-Time Optimization Analysis** - Theoretical framework for comparing optimization methods; quick check: verify convergence rates in simple quadratic problems
4. **Adaptive Learning Rate Mechanisms** - How Adam's normalization helps with imbalanced data; quick check: compare Adam with and without normalization
5. **Sign-Based Optimization** - Benefits of sign-based updates over magnitude-based; quick check: implement sign gradient descent
6. **Hessian-Vector Products** - Understanding second-order effects in optimization; quick check: compute Hessian spectra for different classes

## Architecture Onboarding
**Component Map:** Data -> Model -> Loss -> Gradients -> Optimizer -> Parameters
**Critical Path:** Data distribution → Class imbalance → Gradient structure → Optimizer behavior → Convergence rate
**Design Tradeoffs:** GD offers simplicity but struggles with imbalance; Adam adds complexity but handles imbalance better
**Failure Signatures:** Slow convergence on rare classes, high variance in gradient updates, poor performance on long-tail distributions
**First Experiments:**
1. Train linear model with synthetic power-law distributed classes
2. Compare gradient norms across frequent vs. infrequent classes
3. Measure correlation between gradients for different class pairs

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Theoretical analysis limited to linear models with cross-entropy loss
- Extension to deep networks relies on empirical observations rather than rigorous theory
- Focus primarily on language tasks, though some vision experiments included
- Role of language-specific characteristics (tokenization, vocabulary size) in creating imbalance not fully explored

## Confidence
**High confidence** in empirical observations of Adam's superior performance on language models
**Medium confidence** in theoretical analysis connecting class imbalance to gradient correlations
**Medium confidence** in generalization to deep language models

## Next Checks
1. Extend theoretical analysis to multi-layer networks with non-linear activations
2. Systematically vary class imbalance in synthetic language datasets
3. Conduct ablation studies comparing different Adam variants (momentum, epsilon values)