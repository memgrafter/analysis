---
ver: rpa2
title: 'TIC: Translate-Infer-Compile for accurate "text to plan" using LLMs and Logical
  Representations'
arxiv_id: '2402.06608'
source_url: https://arxiv.org/abs/2402.06608
tags:
- object
- init
- task
- description
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of accurately generating plans
  from natural language requests by combining LLMs with classical planning tools.
  The core method, called TIC (Translate-Infer-Compile), uses an LLM to generate a
  logically interpretable intermediate representation of the task, a logic reasoner
  (ASP) to infer missing information, and then compiles the result into a PDDL task.
---

# TIC: Translate-Infer-Compile for accurate "text to plan" using LLMs and Logical Representations

## Quick Facts
- arXiv ID: 2402.06608
- Source URL: https://arxiv.org/abs/2402.06608
- Authors: Sudhir Agarwal; Anu Sreepathy
- Reference count: 40
- Primary result: Achieved 100% accuracy on PDDL generation for seven planning domains using a three-step approach combining LLMs with ASP reasoning.

## Executive Summary
This paper addresses the challenge of generating accurate planning domain definition language (PDDL) files from natural language task descriptions. The proposed TIC (Translate-Infer-Compile) approach uses large language models (LLMs) to generate intermediate logical representations, logic reasoning to infer missing information, and compilation to produce PDDL files. Experiments across seven benchmark domains show TIC achieves high accuracy, particularly when using in-context examples, and generalizes well to paraphrased inputs.

## Method Summary
TIC is a three-step approach for generating PDDL tasks from natural language descriptions. First, an LLM translates the description into an intermediate ASP representation. Second, a logic reasoner (Clingo) infers missing information using domain knowledge rules. Third, code compiles the materialized representation into the final PDDL task. The approach is evaluated on seven planning domains with both original and paraphrased descriptions, comparing in-context example (IC) and generic prompt (G) approaches across different LLM models.

## Key Results
- TIC-IC achieved 100% accuracy on 6 of 7 domains with GPT-4
- TIC-G3 achieved 95-100% accuracy across domains with GPT-4
- TIC generalizes well to language variations, maintaining high accuracy on paraphrased inputs
- Direct LLM+PDDL generation without TIC steps performed significantly worse

## Why This Works (Mechanism)

### Mechanism 1
Breaking task PDDL generation into three sub-steps (Translate, Infer, Compile) reduces LLM errors compared to direct generation. LLMs are better at translating natural language into a logically interpretable intermediate representation than directly generating structured PDDL, because the intermediate representation is closer to natural language and shorter.

### Mechanism 2
Using a logic reasoner (ASP) to infer missing information from the intermediate representation eliminates LLM errors in logical dependencies. Logical dependencies that are deterministic and can be computed in an error-free manner with appropriate logic reasoners augment the information extracted by LLMs.

### Mechanism 3
The TIC approach generalizes to language variations in natural language task descriptions. The generic domain-independent prompt allows the LLM to extract cardinalities, named objects, and rules from task descriptions with varied language.

## Foundational Learning

- Concept: Answer Set Programming (ASP)
  - Why needed here: ASP is used as the logical language for the intermediate representation and inference step.
  - Quick check question: What are the key features of ASP that make it suitable for modeling planning tasks?

- Concept: Planning Domain Definition Language (PDDL)
  - Why needed here: PDDL is the target representation for planning tasks, and TIC generates task PDDLs from natural language descriptions.
  - Quick check question: What are the main components of a PDDL task description?

- Concept: In-context learning
  - Why needed here: In-context learning is used in the TIC-IC approach to generate the intermediate representation from task descriptions.
  - Quick check question: How does in-context learning differ from fine-tuning in terms of adapting LLMs to new tasks?

## Architecture Onboarding

- Component map:
  Natural Language Task Description -> LLM (Translate step) -> Intermediate Representation (ASP) -> Logic Reasoner (Infer step) -> Materialized Representation (ASP) -> Code (Compile step) -> Task PDDL

- Critical path:
  1. LLM generates intermediate representation from task description.
  2. Logic reasoner infers missing information from intermediate representation and domain rules.
  3. Code compiles materialized representation into task PDDL.

- Design tradeoffs:
  - Using ASP vs other logical languages for intermediate representation.
  - Using in-context learning vs generic prompts for LLM.
  - Using code vs LLM for compilation step.

- Failure signatures:
  - LLM generates incorrect intermediate representation.
  - Logic reasoner fails to infer all required information.
  - Code fails to compile materialized representation into valid task PDDL.

- First 3 experiments:
  1. Test TIC-IC approach on a simple planning domain with a known in-context example.
  2. Test TIC-G3 approach on a simple planning domain with a generic prompt.
  3. Test TIC approach on a complex planning domain with varied language in task descriptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the TIC approach scale to domains with significantly larger state spaces or more complex object hierarchies compared to the seven benchmark domains tested?
- Basis in paper: [explicit] The paper notes TIC achieves high accuracy on seven planning domains but does not evaluate scalability to larger or more complex domains.
- Why unresolved: The experimental results focus on a fixed set of seven domains without varying domain complexity or state space size to test scalability limits.
- What evidence would resolve it: Experiments showing TIC's accuracy and performance (wall-clock time) on domains with 2x-10x more objects, predicates, or state space size compared to the largest tested domain.

### Open Question 2
- Question: Can the generic prompt-based TIC-G1 approach maintain high accuracy without any in-context examples if provided with only domain PDDL and object/predicate definitions?
- Basis in paper: [explicit] The paper states TIC-G1 uses a domain-independent prompt but does not test the extreme case of zero in-context examples.
- Why unresolved: The current evaluation uses in-context examples derived from the test data itself, which may provide implicit domain knowledge that helps accuracy.
- What evidence would resolve it: Experiments comparing TIC-G1 accuracy with zero in-context examples versus with in-context examples across all seven domains.

### Open Question 3
- Question: How does the TIC approach handle ambiguous or underspecified natural language descriptions where multiple valid interpretations exist?
- Basis in paper: [inferred] The paper mentions TIC handles language variations but doesn't address cases where descriptions are genuinely ambiguous or underspecified.
- Why unresolved: The language variation dataset still provides clear, complete descriptions - there's no test of how TIC handles truly ambiguous inputs.
- What evidence would resolve it: Testing TIC on descriptions with intentional ambiguities (e.g., missing object counts, unclear relationships) and measuring whether it makes reasonable default assumptions or fails.

## Limitations
- TIC relies on the assumption that all deterministically inferable information can be captured through domain knowledge rules, which may not hold for all planning domains.
- Accuracy evaluation is limited to seven benchmark domains, and the approach's generalizability to more complex or diverse domains remains untested.
- The paper does not provide a direct comparison between TIC and direct LLM+PDDL generation approaches, making it difficult to quantify the performance gains.

## Confidence

- High confidence: The TIC approach's effectiveness in reducing LLM errors by breaking task PDDL generation into three sub-steps (Translate, Infer, Compile).
- Medium confidence: The TIC approach's ability to generalize to language variations in natural language task descriptions.
- Low confidence: The TIC approach's performance on more complex or diverse planning domains beyond the seven benchmark domains tested.

## Next Checks

1. Conduct a direct comparison between TIC and direct LLM+PDDL generation approaches on the same set of planning domains to quantify the performance gains of the proposed method.
2. Evaluate the TIC approach on a broader range of planning domains, including more complex or diverse domains, to assess its generalizability and scalability.
3. Investigate the development and integration of domain-specific knowledge rules for new or complex planning domains to understand the challenges and potential solutions for applying the TIC approach in practice.