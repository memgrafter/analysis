---
ver: rpa2
title: 'Attention Meets Post-hoc Interpretability: A Mathematical Perspective'
arxiv_id: '2402.03485'
source_url: https://arxiv.org/abs/2402.03485
tags:
- attention
- explanations
- weights
- post-hoc
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the interpretability of attention-based models
  by comparing attention-based and post-hoc explanation methods. It introduces a simplified
  single-layer multi-head attention network and derives explicit expressions for both
  gradient-based and LIME explanations.
---

# Attention Meets Post-hoc Interpretability: A Mathematical Perspective

## Quick Facts
- **arXiv ID:** 2402.03485
- **Source URL:** https://arxiv.org/abs/2402.03485
- **Reference count:** 40
- **Key outcome:** Post-hoc explanation methods (gradient-based and LIME) provide more informative explanations than attention weights alone, as they incorporate information about the final linear layer and can capture negative contributions.

## Executive Summary
This paper provides a mathematical analysis of attention-based interpretability by comparing attention-based explanations with post-hoc methods (gradient-based and LIME) in a simplified single-layer multi-head attention network. The authors derive explicit expressions for both types of explanations, demonstrating that post-hoc methods are more informative because they account for the final linear layer's influence and can capture negative contributions. Experimental validation on sentiment analysis confirms that gradient-based and LIME explanations capture more useful insights about the model's decision-making process compared to attention-based explanations.

## Method Summary
The paper analyzes a single-layer multi-head attention network followed by a linear layer for binary classification. The model uses shared embeddings for queries, keys, and values across all heads. The authors derive explicit expressions for attention-based explanations (average and max of attention weights), gradient-based explanations (including G-avg, G-l1, G-l2, and G^I variants), and LIME explanations. Theoretical results are validated through experiments on the IMDB sentiment analysis dataset using a trained model with specified architecture (Tmax=256, de=128, datt=64, dout=64) and training procedure (AdamW optimizer, lr=0.0001, 10 epochs).

## Key Results
- Post-hoc methods capture the influence of the final linear layer, which attention weights ignore
- Gradient-based explanations show better correlation with model predictions than attention-based explanations
- LIME explanations approximate an affine transformation of attention weights weighted by the final linear layer
- Attention-based explanations can be misleading as they cannot capture negative contributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention-based explanations can be misleading because they ignore the final linear layer's influence on the prediction.
- **Mechanism:** The attention weights α are multiplied by the value vectors vt and then transformed by the final linear layer Wℓ. This means that even if αt is large, if Wℓvt is small or negative, the contribution to the prediction can be minimal or opposite in sign.
- **Core assumption:** The final linear layer Wℓ significantly transforms the attention-weighted values.
- **Evidence anchors:**
  - [abstract]: "post-hoc methods are capable of capturing more useful insights than merely examining the attention weights."
  - [section 4.2]: "the gradient captures the influence of the linear layers Wℓ: we believe that this is a useful insight, disregarded by attention-based explanations."
  - [corpus]: Weak - no direct mention of linear layer influence in neighbors.
- **Break condition:** If Wℓ is the identity matrix or close to it, attention weights would directly reflect model importance.

### Mechanism 2
- **Claim:** Gradient-based explanations incorporate both attention weights and the final linear layer, providing more accurate feature importance.
- **Mechanism:** The gradient ∇et f(pxq includes terms proportional to αtWℓ and αt(Wℓ)vt - (∑αsvs), capturing both the attention-weighted values and their deviation from the average.
- **Core assumption:** The model's prediction is a linear combination of head outputs, making gradients computable.
- **Evidence anchors:**
  - [section 4.2]: "the gradient of f, at the first order approximation, is linear in α, which can explain the correlation with the attention weights to some extent."
  - [section 4.2]: "the gradient captures the influence of the linear layers Wℓ: we believe that this is a useful insight, disregarded by attention-based explanations."
  - [corpus]: Weak - neighbors discuss attention mechanisms but not gradient-based interpretations.
- **Break condition:** If the model includes non-linearities beyond attention (like ReLU), gradients may not capture true feature importance.

### Mechanism 3
- **Claim:** LIME explanations approximate an affine transformation of attention weights, weighted by the final linear layer.
- **Mechanism:** Theorem 5.1 shows β∞j ≈ (3/2K)∑i∑t Wℓ(αtvt - αh,tvh,t)1ξt=j, where αh,t is attention to the [UNK] token. This means LIME weights are proportional to attention weights adjusted by value vectors and the final layer.
- **Core assumption:** Large document size (d ≈ T) allows LIME to approximate conditional expectations accurately.
- **Evidence anchors:**
  - [section 5.2]: "LIME explanations are quite different from gradient-based explanations... there is a major difference with plain attention-based explanations: the last layer comes into account in the explanation."
  - [section 5.2]: "LIME explanations will be near zero whenever αtvt ≈ αh,tvh,t, a scenario in which the attention × value of head i given to token t is comparable to that of the attention given to the [UNK] token."
  - [corpus]: Weak - neighbors discuss LIME but not in the context of attention mechanisms.
- **Break condition:** If the [UNK] token embedding is not representative, LIME weights may be inaccurate.

## Foundational Learning

- **Concept:** Scaled dot-product attention mechanism
  - Why needed here: Understanding how attention weights are computed is crucial for interpreting their limitations
  - Quick check question: How does the scaling factor 1/√datt prevent softmax saturation?

- **Concept:** Gradient-based feature attribution methods
  - Why needed here: Comparing gradient-based methods with attention-based explanations reveals why gradients are more informative
  - Quick check question: Why do G-l1 and G-l2 methods produce non-negative weights while G^I can produce negative weights?

- **Concept:** LIME perturbation sampling for text
  - Why needed here: Understanding LIME's sampling mechanism explains why it can capture interactions between attention and value vectors
  - Quick check question: How does removing words from the document affect the attention weights and value vectors?

## Architecture Onboarding

- **Component map:** Input tokens → shared embeddings → keys/queries/values → scaled dot-product attention → weighted values → head aggregation → final linear layer → prediction

- **Critical path:** Token → embedding → keys/queries/values → attention weights → weighted values → final linear layer → prediction
  - For explanations: attention weights → value vectors → final layer → importance scores

- **Design tradeoffs:**
  - Single layer vs. deep architecture: Single layer enables theoretical analysis but may limit expressiveness
  - Shared embeddings vs. head-specific: Shared embeddings simplify analysis but may reduce model capacity
  - [CLS] token vs. sequence: Using [CLS] token simplifies classification but may lose token-level information

- **Failure signatures:**
  - Attention weights correlate poorly with gradient-based explanations: Final layer is distorting attention signals
  - LIME weights are near zero for high-attention tokens: Value vectors for those tokens are similar to [UNK] token
  - Gradient explanations are inconsistent across heads: Heads are focusing on different aspects of the input

- **First 3 experiments:**
  1. Compute correlation between α-avg and G-avg for a trained model to verify theoretical predictions
  2. Compare LIME weights with the approximation from Theorem 5.1 to validate the theoretical analysis
  3. Remove the final linear layer and observe if attention weights become better explanations of predictions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the findings of this paper generalize to multi-layer attention models with skip connections and non-linear activation functions?
- **Basis in paper:** [explicit] The paper acknowledges limitations including single-layer architecture, no skip connections, and no non-linearities, noting that analysis could be adapted to these settings.
- **Why unresolved:** The current theoretical analysis is specifically designed for a single-layer model without skip connections or non-linearities. While the authors suggest adaptation is possible, they don't provide concrete results or proofs for more complex architectures.
- **What evidence would resolve it:** Theoretical proofs extending Theorem 4.1 and Theorem 5.1 to multi-layer architectures, or experimental validation showing similar interpretability differences persist in deeper models with skip connections.

### Open Question 2
- **Question:** Under what conditions (if any) do attention weights provide reliable explanations, and can we formally characterize these conditions?
- **Basis in paper:** [inferred] The paper shows attention weights are less informative than post-hoc methods but doesn't fully resolve the debate between Jain & Wallace (2019) and Wiegrefe & Pinter (2019). The authors note that Bibal et al. (2022) suggest attention might be useful for certain NLP tasks like syntax-related ones.
- **Why unresolved:** The paper provides theoretical evidence that attention weights are limited for interpretability but doesn't establish precise conditions under which they might be reliable. The ongoing debate in the literature suggests this is still an open question.
- **What evidence would resolve it:** Formal characterization of model/task properties where attention weights align with true feature importance, or identification of specific architectural patterns where attention-based explanations become trustworthy.

### Open Question 3
- **Question:** How does the sampling mechanism in perturbation-based methods (particularly LIME) interact with the tokenizer used by the model, and what are the implications for interpretability quality?
- **Basis in paper:** [explicit] The authors note in the conclusion that there is "some interplay between the sampling mechanism of perturbation-based methods (often replacing at the word level) and the tokenizer used by the model (tokens are often subwords)" which they would like to understand better.
- **Why unresolved:** The paper uses a simplified setting where tokens and words coincide, but real-world models use subword tokenization. This creates potential mismatches between how LIME samples perturbations and how the model processes input.
- **What evidence would resolve it:** Analysis showing how subword tokenization affects LIME's sampling distribution and resulting explanations, or development of modified sampling strategies that better align with the model's tokenization scheme.

## Limitations

- Theoretical analysis assumes a simplified single-layer architecture that may not capture deep attention models
- LIME approximation requires large document sizes (d ≈ T) to be accurate
- Experimental validation uses only the IMDB dataset and a simple model architecture

## Confidence

**High Confidence:** The claim that post-hoc methods incorporate information about the final linear layer is well-supported by the mathematical derivations in Theorems 4.1 and 5.1, and confirmed by experimental observations showing better correlation between gradient-based explanations and model predictions.

**Medium Confidence:** The assertion that attention weights alone can be misleading due to their inability to capture negative contributions has theoretical backing but requires further validation across diverse model architectures and tasks.

**Medium Confidence:** The comparison between LIME and gradient-based explanations showing their complementary nature is supported by theoretical analysis but needs more extensive empirical validation to confirm the claimed differences in their explanatory power.

## Next Checks

1. **Architecture Generalization Test:** Evaluate whether the theoretical insights hold for deeper transformer architectures (2+ layers) with residual connections and layer normalization. Compare attention-based vs. post-hoc explanations across different depths.

2. **Cross-Dataset Validation:** Test the theoretical predictions on multiple NLP datasets beyond IMDB, including question answering and natural language inference tasks, to assess the generalizability of the findings about explanation quality.

3. **Model Complexity Impact:** Analyze how the addition of non-linearities (ReLU, GeLU) after attention layers affects the relationship between attention weights and gradient-based explanations, particularly the correlation patterns observed in the simplified linear case.