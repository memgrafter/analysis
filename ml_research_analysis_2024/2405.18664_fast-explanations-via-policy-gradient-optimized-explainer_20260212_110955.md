---
ver: rpa2
title: Fast Explanations via Policy Gradient-Optimized Explainer
arxiv_id: '2405.18664'
source_url: https://arxiv.org/abs/2405.18664
tags:
- methods
- explanations
- policy
- model-agnostic
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of model-agnostic explanations
  in XAI, which require extensive model queries for sample-level explanations, making
  them impractical for real-time, large-scale deployment. The proposed FEX framework
  learns an attribution-based explainer directly from data and prediction models using
  policy gradient optimization, eliminating reliance on proxy methods.
---

# Fast Explanations via Policy Gradient-Optimized Explainer

## Quick Facts
- arXiv ID: 2405.18664
- Source URL: https://arxiv.org/abs/2405.18664
- Reference count: 12
- The proposed FEX framework learns an attribution-based explainer directly from data and prediction models using policy gradient optimization, eliminating reliance on proxy methods. Experiments show FEX reduces inference time by over 97% and memory usage by 70% compared to traditional model-agnostic approaches.

## Executive Summary
This paper addresses the inefficiency of model-agnostic explanations in XAI, which require extensive model queries for sample-level explanations, making them impractical for real-time, large-scale deployment. The authors propose FEX, a framework that learns an attribution-based explainer directly from data and prediction models using policy gradient optimization. By eliminating reliance on proxy methods, FEX achieves significant improvements in inference time and memory usage while maintaining high-quality explanations and broad applicability.

## Method Summary
FEX uses a policy gradient approach to learn a distribution-based explainer that approximates empirical attributions. The framework optimizes a multivariate Bernoulli distribution q to approximate the empirical attribution ϕ(x) by maximizing the expected score function c(m, x). Proximal Policy Optimization (PPO) with entropy regularization and KL-divergence regularization is used to update the parameters of q. The explainer is trained on datasets such as ImageNet, SST2, and Movies Reviews, and evaluated using metrics like AUC, Pixel Accuracy, mAP, mIoU for image classification, and F1 score for text classification.

## Key Results
- FEX reduces inference time by over 97% compared to traditional model-agnostic approaches
- FEX achieves 70% reduction in memory usage
- Maintains high-quality explanations while being broadly applicable across tasks

## Why This Works (Mechanism)
FEX works by learning an explainer that directly approximates the attribution function from data, rather than querying the black-box model repeatedly. The policy gradient optimization framework treats explanation generation as a sequential decision problem, where the explainer learns to select important features through a learned distribution. By training on a diverse set of examples, the explainer captures general attribution patterns that can be applied to new samples with minimal computational overhead.

## Foundational Learning
1. **Policy Gradient Methods** - Why needed: Enables learning of continuous action spaces for explanation generation. Quick check: Verify the gradient estimator is unbiased and has low variance.
2. **Proximal Policy Optimization (PPO)** - Why needed: Stabilizes policy updates and prevents performance collapse. Quick check: Monitor KL divergence between old and new policies during training.
3. **Multivariate Bernoulli Distributions** - Why needed: Models the probability of selecting each feature for explanation. Quick check: Ensure the distribution parameters remain in valid range [0,1].
4. **Entropy Regularization** - Why needed: Prevents premature convergence to suboptimal deterministic policies. Quick check: Track entropy values during training to ensure exploration.
5. **KL-Divergence Regularization** - Why needed: Maintains consistency between explainer and model predictions. Quick check: Monitor KL divergence between predicted and actual attributions.
6. **Proximal Policy Optimization (PPO)** - Why needed: Stabilizes policy updates and prevents performance collapse. Quick check: Monitor KL divergence between old and new policies during training.

## Architecture Onboarding

**Component Map**: Data -> FEX Explainer (g(x)) -> Bernoulli Distribution q -> Policy Gradient Optimizer -> Updated q

**Critical Path**: Input sample → Explainer network → Bernoulli sampling → Attribution generation → Policy update → Next iteration

**Design Tradeoffs**: 
- Explainer complexity vs. inference speed: More complex explainers may capture better patterns but increase inference time
- KL-divergence regularization coefficient: Higher values ensure better alignment with model predictions but may reduce explainer flexibility
- Entropy regularization coefficient: Balances exploration vs. exploitation in explanation generation

**Failure Signatures**:
- Performance collapse: Policy changes too drastically between updates
- Poor generalization: Explainer fails on classes not well-represented in training data
- Mode collapse: Explainer consistently misses important features

**First 3 Experiments**:
1. Verify basic functionality by training on a small subset of ImageNet and checking explanation quality on validation samples
2. Compare inference time and memory usage against FastSHAP on the same hardware
3. Test explainer performance across different classes to identify potential bias in learned attributions

## Open Questions the Paper Calls Out
1. How does FEX performance scale when trained on increasingly larger datasets (e.g., 10M+ samples) compared to its current 1.3M training set?
2. Can the policy gradient approach be extended to handle structured outputs beyond binary classification (e.g., multi-label, ordinal, or regression tasks)?
3. How does FEX perform on models with different architectures (e.g., small CNNs, large language models, graph neural networks) compared to task-specific explanation methods?

## Limitations
- The exact architecture of the neural network g(x) is not specified, which could impact reproducibility
- Hyperparameter settings for policy gradient optimization are not provided
- Evaluation metrics are not fully described in terms of their implementation and calculation

## Confidence
- High: The paper's claim of reducing inference time by over 97% and memory usage by 70% is supported by experimental results
- Medium: The claim of maintaining high-quality explanations is based on experimental results, but specific evaluation metrics and their implementation are not fully described
- Low: The claim of eliminating reliance on proxy methods is supported by the proposed framework, but specific implementation details are not provided

## Next Checks
1. Implement the FEX framework with different neural network architectures for g(x) and evaluate their impact on the performance of the explainer
2. Conduct a sensitivity analysis on the hyperparameters used in the policy gradient optimization to determine their effect on the explainer's performance
3. Evaluate the FEX explainer on additional datasets and tasks to assess its generalizability and applicability across different domains