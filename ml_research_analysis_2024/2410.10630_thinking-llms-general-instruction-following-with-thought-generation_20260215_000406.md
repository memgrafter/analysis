---
ver: rpa2
title: 'Thinking LLMs: General Instruction Following with Thought Generation'
arxiv_id: '2410.10630'
source_url: https://arxiv.org/abs/2410.10630
tags:
- thought
- training
- response
- prompt
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thought Preference Optimization (TPO), a
  method to equip language models with the ability to think in natural language before
  responding, improving general instruction-following performance without human thought
  data. The approach uses iterative preference optimization, where a judge model evaluates
  only the final responses while the model learns to generate better thoughts through
  self-improvement.
---

# Thinking LLMs: General Instruction Following with Thought Generation

## Quick Facts
- arXiv ID: 2410.10630
- Source URL: https://arxiv.org/abs/2410.10630
- Authors: Tianhao Wu; Janice Lan; Weizhe Yuan; Jiantao Jiao; Jason Weston; Sainbayar Sukhbaatar
- Reference count: 40
- Primary result: TPO-trained models achieve 52.5% win rate on AlpacaEval and 37.3% on Arena-Hard

## Executive Summary
This paper introduces Thought Preference Optimization (TPO), a method to equip language models with the ability to think in natural language before responding, improving general instruction-following performance without human thought data. The approach uses iterative preference optimization, where a judge model evaluates only the final responses while the model learns to generate better thoughts through self-improvement. Experiments show that TPO-trained models achieve a 52.5% win rate on AlpacaEval and 37.3% on Arena-Hard, outperforming baselines. Surprisingly, thinking benefits not only reasoning tasks but also non-reasoning domains like marketing, health, and general knowledge. The method demonstrates that thinking enhances model performance across diverse instruction types.

## Method Summary
TPO trains LLMs to generate thoughts in natural language before responding to instructions, then optimizes these thoughts to improve final responses without human supervision. The method uses iterative preference optimization: for each instruction, the model samples multiple outputs containing thought-response pairs, a judge evaluates only the response parts, and preference optimization trains the model to produce thoughts that lead to better responses. The process iterates, with the model sampling k≤K outputs per instruction, selecting best/worst response pairs based on judge scores, and training via DPO or IRPO loss with length control to prevent response inflation.

## Key Results
- TPO-trained models achieve 52.5% win rate on AlpacaEval and 37.3% on Arena-Hard benchmarks
- Thinking improves performance across both reasoning and non-reasoning domains (marketing, health, general knowledge)
- The method maintains computational efficiency by leveraging natural language thoughts that take advantage of pre-training knowledge
- Length control effectively prevents response inflation, with ρ=0.02 reducing response length by 6.5%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative preference optimization with judge-only-response evaluation creates a reinforcement signal that implicitly optimizes thought quality
- Mechanism: The model generates thought-response pairs, the judge evaluates only responses, and preference optimization trains the model to produce thoughts that lead to better responses
- Core assumption: Better responses imply better thoughts, creating a differentiable learning signal through the response-only judge
- Evidence anchors: [abstract] "For each instruction, the thought candidates are scored using a judge model to evaluate their responses only" [section 2.2] "the judge can only see the response part of the outputs, so the thought part cannot influence its judgement"

### Mechanism 2
- Claim: Sampling multiple thought candidates and selecting best/worst pairs creates rich preference signal driving rapid learning
- Mechanism: For each instruction, model samples K outputs, judge ranks responses, and highest/lowest scoring pairs create preference examples for DPO training
- Core assumption: Space of possible thoughts contains sufficient diversity to find better strategies through sampling
- Evidence anchors: [abstract] "We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations" [section 2.2] "For each input, we sample k ≤ K outputs, each containing thought zk i and response yk i parts"

### Mechanism 3
- Claim: Natural language thoughts leverage pre-training knowledge and allow interpretability while maintaining computational efficiency
- Mechanism: Thoughts in natural language take advantage of model's pre-training on human-written text containing thoughts, enabling better understanding and generation while allowing human inspection
- Core assumption: Pre-training corpus contains sufficient thought-related information to bootstrap thinking capabilities
- Evidence anchors: [abstract] "LLMs are pre-trained on text containing human-written thoughts, which are hence encoded into the model" [section 2.1] "thoughts in natural language have several benefits such as taking advantage of human-written LLM pre-training data"

## Foundational Learning

- Concept: Preference Optimization (DPO/IRPO)
  - Why needed here: Provides way to optimize thought generation without direct supervision by using relative quality judgments
  - Quick check question: How does DPO differ from standard RLHF in terms of feedback signal and optimization objective?

- Concept: Judge-as-a-Service (LLM-as-Judge)
  - Why needed here: Enables automated evaluation of responses without human annotation, making iterative training feasible
  - Quick check question: What are the key failure modes of judge models that could break the training signal?

- Concept: Sampling and Selection Strategies
  - Why needed here: Multiple candidate generation allows exploration of thought space to find effective thinking patterns
  - Quick check question: How does the number of samples (K) affect quality and diversity of preference signal?

## Architecture Onboarding

- Component map: Seed model → Thought prompt → Sampling (K outputs) → Judge evaluation → Preference pair selection → DPO training → New model
- Critical path: Instruction → Thought generation → Response generation → Judge evaluation → Preference optimization → Improved model
- Design tradeoffs: Natural language thoughts vs computational efficiency, sampling diversity vs training speed, judge quality vs automation
- Failure signatures: Degraded math performance, increasing response length, parsing errors, overthinking behaviors
- First 3 experiments:
  1. Compare seed model performance with and without thought prompting on small validation set
  2. Test different sampling temperatures and K values to find optimal exploration-exploitation balance
  3. Evaluate judge bias by measuring response length correlation with scores across multiple iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would Thinking LLMs perform on larger model sizes (e.g., 70B parameters or more)?
- Basis in paper: [inferred] from limitation section noting all experiments based on 8B parameter models
- Why unresolved: Paper only tested TPO on 8B parameter models and explicitly states it's worth investigating effect on larger scale models
- What evidence would resolve it: Training and evaluating TPO on 70B+ parameter models and comparing performance gains against non-thinking counterparts

### Open Question 2
- Question: Can the model be trained to dynamically adjust thought length based on task complexity?
- Basis in paper: [inferred] from limitation section stating thought lengths are purely determined by model itself with no steerability
- Why unresolved: Current TPO implementation generates fixed-length thoughts without ability to adapt thought length based on instruction complexity
- What evidence would resolve it: Developing and evaluating steerable TPO model that can adjust thought token count based on task complexity indicators

### Open Question 3
- Question: How does TPO performance vary across different categories of tasks when using different thought prompt types?
- Basis in paper: [explicit] from analysis section comparing generic vs specific thought prompts
- Why unresolved: Paper didn't conduct detailed category-by-category analysis of which prompt type works best for which task types
- What evidence would resolve it: Conducting fine-grained evaluation comparing generic vs specific thought prompts across all 20 task categories

## Limitations

- Judge quality and bias significantly impact results, with different judges (ArmoRM vs STE) producing different win rates (52.5% vs 37.3%)
- Domain generalization claims lack rigorous ablation studies to isolate thinking effect from general prompting benefits
- Computational efficiency concerns due to iterative procedure requiring multiple sampling rounds and judge evaluations per instruction

## Confidence

**High Confidence**: Core mechanism of iterative preference optimization with judge-only response evaluation works as described. Experimental setup clearly specified and win rates reproducible.

**Medium Confidence**: Claim that thinking improves performance across reasoning and non-reasoning domains is supported by benchmark results but lacks ablation studies to isolate thinking effect from general prompting benefits.

**Low Confidence**: Assertion that natural language thoughts leverage pre-training knowledge is plausible but not empirically validated. Paper doesn't test whether same improvements could be achieved with structured or symbolic thoughts.

## Next Checks

1. **Judge Ablation Study**: Train TPO models using different judge models (ArmoRM, STE, GPT-4) and measure both win rates and judge-consistency scores to quantify how much performance depends on judge selection.

2. **Prompt Ablation**: Create minimal thought prompts (e.g., "think" vs full multi-sentence prompts) and compare performance across reasoning and non-reasoning tasks to isolate whether thinking content or prompting act matters more.

3. **Computational Efficiency Benchmark**: Compare TPO against single-pass baseline that samples K outputs and selects highest-scoring response without iterative training, measuring win rates, training time, and inference latency.