---
ver: rpa2
title: Protecting against simultaneous data poisoning attacks
arxiv_id: '2408.13221'
source_url: https://arxiv.org/abs/2408.13221
tags:
- attacks
- attack
- backdoor
- examples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the problem of defending against multiple
  simultaneous backdoor attacks, demonstrating that poisoned datasets can install
  multiple backdoors without substantial clean accuracy degradation. Existing defenses
  fail in this setting.
---

# Protecting against simultaneous data poisoning attacks

## Quick Facts
- arXiv ID: 2408.13221
- Source URL: https://arxiv.org/abs/2408.13221
- Authors: Neel Alex; Shoaib Ahmed Siddiqui; Amartya Sanyal; David Krueger
- Reference count: 40
- Primary result: BaDLoss achieves 7.98% ASR on CIFAR-10 and 10.29% on GTSRB in multi-attack setting vs 64.48% and 84.28% for other defenses

## Executive Summary
This work addresses the problem of defending against multiple simultaneous backdoor attacks in machine learning models. The authors demonstrate that poisoned datasets can successfully install multiple backdoors without substantially degrading clean accuracy, and show that existing backdoor defense methods fail in this setting. They propose BaDLoss, a defense that identifies poisoned examples by tracking and comparing loss trajectories during training to those of known clean examples.

## Method Summary
BaDLoss tracks per-example loss trajectories during training and compares them to trajectories of known clean examples to identify poisoned data. The method trains for 30 epochs while recording loss for each example, then calculates anomaly scores by comparing each example's trajectory to 50 nearest clean trajectories among 250 clean probes. The 40% of examples with highest anomaly scores are removed, and the model is retrained from scratch on the filtered dataset. This approach is evaluated against seven simultaneous backdoor attacks on CIFAR-10 and GTSRB datasets.

## Key Results
- BaDLoss achieves 7.98% attack success rate on CIFAR-10 vs 64.48% for other defenses
- BaDLoss achieves 10.29% attack success rate on GTSRB vs 84.28% for other defenses
- Minimal clean accuracy degradation compared to existing defenses

## Why This Works (Mechanism)

### Mechanism 1
- Backdoor triggers create anomalous loss trajectories compared to clean examples during training
- The BaDLoss defense tracks per-example loss across multiple short training runs and compares these trajectories to those of known clean examples
- Core assumption: The attacker cannot control the training dynamics of poisoned examples to perfectly mimic clean ones
- Break condition: If attacker can design triggers that produce identical loss trajectories to clean examples, this defense would fail

### Mechanism 2
- Multiple simultaneous backdoor attacks can be learned without substantial clean accuracy degradation
- The model can simultaneously learn multiple backdoor patterns because each trigger represents a distinct feature association that doesn't interfere with the core classification task when poisoning ratios are low
- Core assumption: Different backdoor triggers target different classes and don't conflict in feature space
- Break condition: If triggers conflict in feature space or target the same classes, clean accuracy degradation would occur

### Mechanism 3
- Existing defenses fail in multi-attack setting because their assumptions break down when multiple attacks occur simultaneously
- Defenses like Neural Cleanse assume one class will have anomalously low mask magnitude, but with multiple attacked classes this assumption fails
- Core assumption: Defenses are tuned for single-attack scenarios and don't generalize to multiple simultaneous attacks
- Break condition: If defenses can be retuned or generalized to handle multiple attacks, this mechanism would not hold

## Foundational Learning

- Concept: Loss trajectory analysis
  - Why needed here: BaDLoss relies on tracking per-example loss across training epochs to identify anomalies
  - Quick check question: How would you track and store loss values for each training example across multiple epochs?

- Concept: Backdoor attack mechanics
  - Why needed here: Understanding how triggers work and how they affect model behavior is crucial for designing defenses
  - Quick check question: What makes a backdoor trigger different from normal input features in terms of model learning?

- Concept: Multi-attack interference
  - Why needed here: The paper demonstrates that multiple attacks can coexist, so understanding how they interact is important
  - Quick check question: Under what conditions would multiple backdoor attacks interfere with each other during training?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Model training module (with loss tracking) -> Clean example management (probe dataset) -> Loss trajectory comparison engine -> Example filtering mechanism -> Retraining pipeline

- Critical path: Clean examples → training with loss tracking → trajectory comparison → anomaly detection → example filtering → retraining

- Design tradeoffs:
  - Number of clean examples vs. detection accuracy
  - Number of epochs for pretraining vs. computational cost
  - Rejection fraction vs. clean accuracy preservation
  - Choice of distance metric for trajectory comparison

- Failure signatures:
  - High false positive rate (clean examples flagged as backdoored)
  - Low detection rate (backdoored examples not identified)
  - Excessive clean accuracy degradation after filtering
  - Inconsistent results across runs

- First 3 experiments:
  1. Test BaDLoss on a single known backdoor attack to verify basic functionality
  2. Vary the rejection fraction to find optimal balance between attack removal and clean accuracy
  3. Test with different numbers of clean examples to determine minimum required for effective detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BaDLoss perform when the number of bona fide clean examples (nclean) is significantly reduced or increased beyond the tested range?
- Basis in paper: The paper mentions that BaDLoss uses 250 bona fide clean examples but doesn't explore the impact of varying this number on detection performance
- Why unresolved: The paper focuses on demonstrating effectiveness with a fixed parameter setting but doesn't investigate sensitivity to the size of the clean probe set
- What evidence would resolve it: Experiments varying nclean across a wide range (e.g., 50-1000) while measuring detection accuracy and clean accuracy degradation would show the robustness of BaDLoss to probe set size

### Open Question 2
- Question: Can BaDLoss effectively defend against backdoor attacks in self-supervised learning settings where loss trajectories of clean and backdoored examples become similar?
- Basis in paper: The paper explicitly states that Li et al. [29] observed that loss dynamics grow more similar in self-supervised contrastive learning, and acknowledges this as a limitation
- Why unresolved: The paper doesn't evaluate BaDLoss on self-supervised learning frameworks or explore modifications needed for such settings
- What evidence would resolve it: Testing BaDLoss on self-supervised learning models (e.g., SimCLR, MoCo) with backdoor attacks and comparing detection rates to supervised settings would demonstrate effectiveness or need for adaptation

### Open Question 3
- Question: How does BaDLoss handle backdoor attacks that specifically target minority classes, given its potential to exacerbate existing biases?
- Basis in paper: The paper acknowledges that BaDLoss may more likely mark minority-class data as anomalous due to unstable trajectories from reduced training data
- Why unresolved: The paper doesn't provide empirical evidence of this effect or propose mitigation strategies for minority class vulnerability
- What evidence would resolve it: Experiments on datasets with known class imbalance, measuring both attack success rate and clean accuracy specifically for minority classes, would quantify the impact and potential bias issues

## Limitations
- The paper lacks ablation studies on key hyper-parameters (number of clean probes, rejection fraction, number of training epochs for loss trajectory tracking)
- No statistical significance tests across multiple runs are provided
- The paper doesn't address potential false positive rates or how BaDLoss performs when clean datasets are noisy
- The probe set may contain some poisoned examples in real-world scenarios, which isn't evaluated

## Confidence
- Confidence Level: Medium for the core claim that BaDLoss effectively defends against multiple simultaneous backdoor attacks
- Confidence Level: Low for claims about why existing defenses fail in the multi-attack setting
- Major Limitation: The paper doesn't address potential false positive rates or how BaDLoss performs when clean datasets are noisy or when the probe set contains some poisoned examples

## Next Checks
1. **Ablation Study**: Run experiments varying the number of clean probes (currently 250), rejection fraction (currently 40%), and pretraining epochs (currently 30) to understand sensitivity to these hyper-parameters and identify optimal settings

2. **Statistical Validation**: Perform multiple independent runs of all experiments to establish confidence intervals and statistical significance of the performance differences between BaDLoss and baseline defenses

3. **Robustness Testing**: Evaluate BaDLoss performance when the probe dataset contains increasing proportions of poisoned examples (e.g., 5%, 10%, 20%) to assess real-world applicability where clean data may not be perfectly verified