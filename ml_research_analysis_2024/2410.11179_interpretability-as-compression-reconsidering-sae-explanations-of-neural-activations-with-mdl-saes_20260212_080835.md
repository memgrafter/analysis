---
ver: rpa2
title: 'Interpretability as Compression: Reconsidering SAE Explanations of Neural
  Activations with MDL-SAEs'
arxiv_id: '2410.11179'
source_url: https://arxiv.org/abs/2410.11179
tags:
- features
- feature
- description
- activations
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper frames sparse autoencoders (SAEs) as communication\
  \ protocols for transmitting neural activations. The authors argue that interpretability\
  \ requires SAE features to have the property of \"independent additivity\"\u2014\
  each feature can be understood separately."
---

# Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs

## Quick Facts
- arXiv ID: 2410.11179
- Source URL: https://arxiv.org/abs/2410.11179
- Reference count: 12
- Key outcome: MDL-SAEs discover more interpretable features than sparsity-based SAEs, finding stroke-like components in MNIST that are composable and have a principled decision boundary for feature splitting.

## Executive Summary
This paper reconsiders sparse autoencoders (SAEs) through the lens of information theory and communication protocols, arguing that interpretability requires features to have the property of "independent additivity" - each feature can be understood separately. The authors introduce the Minimal Description Length (MDL) principle as an alternative to sparsity-based training, demonstrating that MDL-optimal SAEs discover more interpretable features (e.g., line segments in MNIST) compared to features learned via sparsity alone. The MDL framework also provides a principled way to identify and mitigate undesirable feature splitting.

## Method Summary
The authors propose MDL-SAEs as an alternative to traditional sparsity-based SAE training. Rather than using L1 regularization to enforce sparsity, they optimize SAEs to minimize the total description length required to communicate neural activations. This approach treats SAEs as communication protocols where the encoder and decoder represent a codebook, and the goal is to find the most efficient representation. The MDL framework naturally balances reconstruction accuracy against the cost of encoding features, leading to features that are both accurate and concise to describe.

## Key Results
- MDL-SAEs discover stroke-like features in MNIST that are more interpretable and composable than features from standard sparsity-based SAEs
- The MDL framework provides a principled decision boundary for identifying when features should be split versus combined
- MDL-optimal SAEs achieve better reconstruction accuracy with fewer active features compared to extremely sparse alternatives

## Why This Works (Mechanism)
MDL-SAEs work because they optimize for communication efficiency rather than arbitrary sparsity constraints. By minimizing total description length, the model naturally discovers features that are both informative and concise to encode. This aligns with the authors' argument that interpretability requires features to be "independently additive" - each feature can be understood and communicated separately without reference to others.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural networks that learn compressed representations by reconstructing inputs through a bottleneck layer. Why needed: Understanding the baseline approach being improved upon. Quick check: Can you explain how SAEs differ from standard autoencoders?
- **Minimal Description Length (MDL)**: Information-theoretic principle stating that the best model is the one that compresses the data most effectively. Why needed: This is the core theoretical framework replacing sparsity regularization. Quick check: Can you state the trade-off MDL optimizes between model complexity and accuracy?
- **Independent Additivity**: The property that features can be understood and combined separately. Why needed: The paper argues this is a key requirement for interpretability. Quick check: Can you give an example of a feature that lacks this property?
- **Feature Splitting**: The phenomenon where a single semantic concept is represented by multiple redundant features. Why needed: MDL helps identify and mitigate this interpretability problem. Quick check: Can you explain why feature splitting reduces interpretability?

## Architecture Onboarding
**Component Map**: Input Activations -> Encoder -> Codebook (Features) -> Decoder -> Reconstructed Activations
**Critical Path**: The critical optimization loop involves finding feature representations that minimize total description length while maintaining reconstruction accuracy
**Design Tradeoffs**: MDL-SAEs may produce non-sparse feature sets, trading sparsity for more meaningful feature discovery and better handling of feature splitting
**Failure Signatures**: Features that are too specialized (overfitting) or too general (underfitting) can be identified through their description length costs
**First Experiments**: 
1. Train MDL-SAE and standard SAE on MNIST with identical architectures and compare feature interpretability
2. Test MDL-SAE on a simple dataset with known ground-truth features to verify feature discovery
3. Analyze feature splitting behavior by gradually increasing model capacity in both MDL and sparsity-based approaches

## Open Questions the Paper Calls Out
The paper acknowledges that MDL may produce non-sparse feature sets and doesn't thoroughly address how this impacts interpretability in high-dimensional spaces or whether communication efficiency gains translate to more complex datasets.

## Limitations
- Results are limited to MNIST, a simple dataset that may not generalize to more complex domains like language models
- The theoretical framework assumes features should be "independently additive" without exploring alternative interaction patterns
- Computational trade-offs of MDL-based training versus sparsity-based approaches are not thoroughly investigated

## Confidence
**High confidence** in the theoretical framework linking interpretability to communication efficiency and independent additivity. The mathematical foundations connecting MDL to feature discovery are sound.

**Medium confidence** in the empirical demonstration, as results are limited to MNIST where the "ground truth" features (strokes) are relatively simple and well-defined. The improvements over standard sparsity-based SAEs are clear but may not scale to more complex domains.

**Low confidence** in the claim that MDL-SAEs will universally discover more interpretable features, as this depends heavily on the underlying data structure and what constitutes "interpretable" features varies by domain.

## Next Checks
1. Test MDL-SAEs on a more complex dataset like CIFAR-10 or natural language text to verify whether stroke-like features generalize beyond MNIST's simple geometry.
2. Compare feature compositionality and interpretability metrics between MDL-SAEs and standard SAEs using quantitative measures beyond qualitative feature visualization.
3. Investigate the computational trade-offs of MDL-based training versus sparsity-based approaches, particularly for large-scale applications with millions of features.