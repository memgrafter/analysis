---
ver: rpa2
title: 'Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in
  Emotion Attribution'
arxiv_id: '2403.03121'
source_url: https://arxiv.org/abs/2403.03121
tags:
- gender
- emotions
- emotion
- women
- stereotypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) reflect
  gendered emotional stereotypes in their responses. The authors prompt five state-of-the-art
  LLMs to attribute emotions to events while adopting a gendered persona.
---

# Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution

## Quick Facts
- arXiv ID: 2403.03121
- Source URL: https://arxiv.org/abs/2403.03121
- Reference count: 27
- Primary result: LLMs overwhelmingly associate sadness with women and anger with men, regardless of actual emotion distributions in source data

## Executive Summary
This paper investigates whether large language models (LLMs) reflect gendered emotional stereotypes in their responses. The authors prompt five state-of-the-art LLMs to attribute emotions to events while adopting a gendered persona. They analyze over 200,000 model completions for 7,000 events and two personas. The results show that all models consistently exhibit gendered emotions influenced by stereotypes, with women overwhelmingly associated with sadness and men with anger, regardless of the actual emotions reported in the source dataset.

## Method Summary
The study uses zero-shot prompting to investigate gendered emotion attribution in five LLMs (Llama2-7b/13b/70b, Mistral-7b, GPT-4). Researchers adopt persona templates from Gupta et al. (2023) and prompt models to attribute emotions to 7,000 events from the ISEAR dataset. They analyze frequency distributions, statistical significance (χ² test), confusion matrices, and model explanations. The study compares model predictions to gold labels in ISEAR and assesses whether differences reflect stereotypes or lived experiences.

## Key Results
- All five LLMs consistently predict SADNESS for women and ANGER for men, diverging from actual emotion distributions in ISEAR data
- Models show significant statistical differences in emotion attribution by gender (p < 0.01)
- Model explanations reveal stereotype-based reasoning rather than factual emotional response patterns
- The misalignment between model predictions and real-world data suggests emotion attribution is based on societal stereotypes rather than lived experiences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM emotion attribution shifts based on assigned persona, producing different emotion distributions for men and women
- Mechanism: Persona prompts alter the model's internal conditioning state, biasing subsequent generation toward culturally stereotyped associations between gender and emotion
- Core assumption: The LLM has internalized gendered emotional associations during pretraining from corpus statistics
- Evidence anchors: [abstract] "The authors prompt five state-of-the-art LLMs to attribute emotions to events while adopting a gendered persona"

### Mechanism 2
- Claim: LLMs associate SADNESS with women and ANGER with men, reflecting societal stereotypes rather than actual emotion distributions in source data
- Mechanism: During pretraining, text co-occurrence patterns between gendered pronouns and emotion words reinforce stereotypical pairings, which persist in zero-shot generation
- Core assumption: Pretraining corpus contains disproportionate gendered emotion associations that models memorize
- Evidence anchors: [abstract] "Women are overwhelmingly associated with sadness and men with anger, regardless of the actual emotions reported in the source dataset"

### Mechanism 3
- Claim: Model explanations reveal stereotype-based reasoning rather than factual emotional response patterns
- Mechanism: The model's internal knowledge representation conflates societal expectations with plausible reasoning, leading explanations to justify emotion attributions with stereotypical logic
- Core assumption: The model's reasoning chain generation is influenced by learned stereotypes, not just the final prediction
- Evidence anchors: [abstract] "women are often thought of as more empathetic, while men's anger is more socially accepted"

## Foundational Learning

- Concept: Zero-shot learning (ZSL) in LLMs
  - Why needed here: The study uses ZSL prompting to elicit emotion attributions without task-specific fine-tuning, isolating bias from supervised learning effects
  - Quick check question: What distinguishes zero-shot from few-shot prompting in LLM experiments?

- Concept: Confusion matrix analysis for bias detection
  - Why needed here: Comparing predicted vs. actual gender-emotion pairs reveals systematic over/under-prediction of specific emotions per gender
  - Quick check question: How does a confusion matrix help distinguish random error from systematic bias in emotion attribution?

- Concept: Statistical significance testing (χ² test)
  - Why needed here: Confirms that observed differences in emotion distributions between genders are not due to sampling variability
  - Quick check question: What does a p-value < 0.01 in a χ² test indicate about the relationship between gender and predicted emotion?

## Architecture Onboarding

- Component map: Persona template -> Event text -> LLM encoder -> Decoder with persona-conditioned generation -> Output emotion + optional explanation
- Critical path: Persona prompt construction -> Event pairing -> Model generation (greedy decoding) -> Emotion extraction -> Statistical analysis
- Design tradeoffs: Persona template richness vs. prompt brevity; emotion constraint specificity vs. generation flexibility
- Failure signatures: Model refusal for certain personas; inconsistent emotion attribution across persona templates; emotion output not matching prompt constraints
- First 3 experiments:
  1. Test baseline emotion attribution without persona prompts to measure baseline bias
  2. Vary decoding temperature to see if randomness affects gendered patterns
  3. Apply emotion constraint prompts to different model families to compare bias strength

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do emotion attribution patterns vary across different cultural contexts and languages when using LLMs?
- Basis in paper: [inferred] The paper acknowledges that gender stereotypes and expectations likely vary between languages and cultures, but the study focuses only on English due to data constraints
- Why unresolved: The study's reliance on a single language (English) and a specific emotion dataset (ISEAR) limits the generalizability of the findings to other cultural contexts
- What evidence would resolve it: Conducting similar studies with multilingual datasets and diverse cultural contexts to compare emotion attribution patterns across languages and cultures

### Open Question 2
- Question: What are the long-term effects of using LLMs that perpetuate gendered emotional stereotypes in sensitive applications like mental health and human-computer interaction?
- Basis in paper: [explicit] The paper raises concerns about the potential risks of using LLMs that reflect societal biases in sensitive areas like mental health and human-computer interaction, highlighting the spread of representational and allocational harms
- Why unresolved: The study identifies the presence of gendered emotional stereotypes in LLMs but does not explore the long-term consequences of using these models in sensitive applications
- What evidence would resolve it: Longitudinal studies examining the impact of LLM-generated gendered emotional stereotypes on user perceptions, behaviors, and mental health outcomes in various applications

### Open Question 3
- Question: How can interdisciplinary collaboration between NLP researchers and experts in psychology and gender studies help mitigate gendered emotional stereotypes in LLMs?
- Basis in paper: [explicit] The paper advocates for interdisciplinary work, embracing disciplines such as psychology and philosophy to inform and mitigate gendered emotions based on social stereotypes within NLP systems
- Why unresolved: While the paper emphasizes the need for interdisciplinary collaboration, it does not provide specific strategies or frameworks for integrating insights from psychology and gender studies into LLM development and evaluation
- What evidence would resolve it: Collaborative research projects that bring together NLP researchers, psychologists, and gender studies experts to develop and test interventions for reducing gendered emotional stereotypes in LLMs, with measurable outcomes and best practices for implementation

## Limitations
- The study uses only binary gender categories (man/woman), excluding non-binary or gender-fluid identities
- Exact persona templates are not fully specified in the paper, making exact replication challenging
- Analysis is limited to English language and ISEAR dataset, which may not capture full emotional experience spectrum across cultures

## Confidence
- High Confidence: Core finding that LLMs exhibit gendered emotion attribution patterns aligned with stereotypes
- Medium Confidence: Interpretation that models amplify rather than merely reflect stereotypes
- Low Confidence: Qualitative analysis of model explanations based on limited examples

## Next Checks
1. Replicate the experiment using a different emotion dataset (e.g., EmoBank or GoEmotions) to verify whether gendered attribution patterns persist across diverse data sources
2. Expand persona prompts to include non-binary gender identities to assess whether models exhibit additional bias patterns
3. Run the same prompts with varying decoding temperatures (0.2, 0.5, 1.0) to determine whether the observed gendered patterns are robust to stochastic variation