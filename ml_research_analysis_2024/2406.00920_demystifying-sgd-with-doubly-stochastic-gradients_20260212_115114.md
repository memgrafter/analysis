---
ver: rpa2
title: Demystifying SGD with Doubly Stochastic Gradients
arxiv_id: '2406.00920'
source_url: https://arxiv.org/abs/2406.00920
tags:
- u1d456
- u1d499
- u1d45b
- u1d453
- uni2016
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence properties of stochastic gradient
  descent (SGD) with doubly stochastic gradients (doubly SGD) for optimization problems
  with intractable expectations. The authors establish convergence guarantees under
  general conditions, encompassing dependent component gradient estimators and various
  subsampling strategies.
---

# Demystifying SGD with Doubly Stochastic Gradients

## Quick Facts
- arXiv ID: 2406.00920
- Source URL: https://arxiv.org/abs/2406.00920
- Reference count: 40
- This paper analyzes the convergence properties of stochastic gradient descent (SGD) with doubly stochastic gradients (doubly SGD) for optimization problems with intractable expectations.

## Executive Summary
This paper establishes convergence guarantees for SGD with doubly stochastic gradients, where both component gradient estimators and the sum are approximated through sampling. The authors prove that if individual component gradient estimators satisfy expected residual (ER) and bounded variance (BV) conditions, the resulting doubly stochastic estimator also satisfies these conditions, guaranteeing convergence of doubly SGD. The analysis encompasses dependent component gradient estimators and various subsampling strategies, providing practical insights for computational budget allocation between minibatch size and Monte Carlo samples.

## Method Summary
The core method involves analyzing doubly SGD where the gradient estimator of each component is used while estimating the sum by subsampling over these estimators. The authors establish a general variance bound that accounts for correlation between component estimators and demonstrate that doubly SGD with random reshuffling improves complexity dependence on subsampling noise compared to independent subsampling. The approach applies to various objective function classes including convex, quasar convex, and non-convex smooth objectives, with specific focus on strongly convex functions.

## Key Results
- A general variance bound showing how component estimator variance, correlation between estimators, and subsampling variance contribute to the overall gradient variance
- Proof that doubly SGD with random reshuffling improves complexity dependence on subsampling noise from O(1/∛n) to O(1/n)
- Practical insights suggesting that when dependent gradient estimators are used, increasing minibatch size should be preferred over increasing Monte Carlo samples for a fixed computational budget

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Doubly SGD with correlated component estimators reduces variance when increasing minibatch size λ compared to increasing Monte Carlo samples m.
- **Mechanism**: When component gradient estimators share Monte Carlo samples across the batch, increasing λ reduces both the subsampling variance and the variance of individual component estimators simultaneously, due to correlation effects.
- **Core assumption**: Component gradient estimators are dependent (share Monte Carlo samples) and have heterogeneous variances across components.
- **Evidence anchors**:
  - [abstract]: "our analysis provides insight into this effect. In particular, we reveal that reducing subsampling variance also reduces Monte Carlo variances."
  - [section]: "Corollary 1 (i), the term with 1/∑/u1D45B/u1D456=1/u1D70E2/u1D456 is reduced in a rate of )u1D4AA(1∕/u1D45A/u1D44F)"
- **Break condition**: When component gradient estimators are independent (ρ=0) or have similar variance across components, increasing λ becomes more beneficial.

### Mechanism 2
- **Claim**: Random reshuffling (doubly SGD-RR) improves the dependence on subsampling noise from O(1/∛n) to O(1/n) compared to independent subsampling.
- **Mechanism**: Random reshuffling uses conditionally biased gradient estimates that minimize a biased Lyapunov function, reducing the impact of subsampling noise while maintaining convergence guarantees.
- **Core assumption**: Objective function is strongly convex with strongly convex components, and component estimators satisfy the expected residual (ER) condition.
- **Evidence anchors**:
  - [abstract]: "we prove that random reshuffling (RR) improves the complexity dependence on the subsampling noise."
  - [section]: "Compared to doubly SGD, doubly SGD-RR improves the dependence on the subsampling noise σ2 from )u1D4AA(1∕/u1D716) to )u1D4AA(1∕/u1D716)"
- **Break condition**: When component estimation noise dominates subsampling noise, or when the objective is not strongly convex.

### Mechanism 3
- **Claim**: Doubly SGD with doubly stochastic gradients converges under general conditions encompassing dependent component estimators.
- **Mechanism**: If individual component gradient estimators satisfy expected residual (ER) and bounded variance (BV) conditions, the doubly stochastic estimator also satisfies these conditions, guaranteeing convergence of doubly SGD.
- **Core assumption**: Component gradient estimators are unbiased and satisfy ER and BV conditions individually.
- **Evidence anchors**:
  - [abstract]: "we establish the convergence of doubly SGD with independent minibatching and random reshuffling under general conditions, which encompasses dependent component gradient estimators."
  - [section]: "Theorems 2 and 3: Using the general variance bound, we show that a doubly stochastic estimator subsampling over correlated estimators satisfying the ER condition and the bounded variance (BV) condition equally satisfies the ER and BV conditions as well."
- **Break condition**: When component estimators violate the ER or BV conditions, or when strong assumptions like bounded variance uniformly are required.

## Foundational Learning

- **Concept**: Expected Residual (ER) condition
  - Why needed here: ER condition allows bounding gradient variance on arbitrary points using variance on the solution set, which is crucial for establishing convergence guarantees for doubly stochastic gradients.
  - Quick check question: How does the ER condition differ from expected smoothness (ES), and why is it preferred for doubly stochastic estimators?

- **Concept**: Bounded Variance (BV) condition
  - Why needed here: BV condition bounds the gradient variance on the solution set, complementing the ER condition to establish convergence of SGD with doubly stochastic gradients.
  - Quick check question: What is the relationship between BV condition and the variance of the subsampling estimator?

- **Concept**: Random reshuffling vs independent subsampling
  - Why needed here: Understanding the difference between these sampling strategies is crucial for analyzing doubly SGD-RR and its improved complexity dependence on subsampling noise.
  - Quick check question: Why does random reshuffling use a biased Lyapunov function, and how does this affect convergence rates?

## Architecture Onboarding

- **Component map**: Objective function -> Component gradient estimators -> Doubly stochastic estimator -> Convergence analysis
- **Critical path**:
  1. Define component gradient estimators satisfying ER and BV conditions
  2. Establish variance bound for doubly stochastic estimator
  3. Prove doubly stochastic estimator satisfies ER and BV conditions
  4. Apply existing SGD convergence results
- **Design tradeoffs**:
  - Dependent vs independent component estimators: Dependent estimators share Monte Carlo samples but introduce correlation; independent estimators require more samples but simplify analysis
  - Minibatch size vs Monte Carlo samples: For dependent estimators, increasing minibatch size is generally more effective
  - Random reshuffling vs independent subsampling: Random reshuffling improves dependence on subsampling noise but requires strongly convex objectives
- **Failure signatures**:
  - Divergence or slow convergence: Component estimators may not satisfy ER or BV conditions
  - High variance: May need to increase minibatch size or Monte Carlo samples
  - Poor performance of random reshuffling: Objective may not be strongly convex or components may not be strongly convex
- **First 3 experiments**:
  1. Verify ER and BV conditions for component estimators on a simple test problem
  2. Compare variance of doubly stochastic estimator with theoretical bounds
  3. Test convergence of doubly SGD with different minibatch sizes and Monte Carlo samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlation parameter ρ between component gradient estimators affect the optimal allocation between Monte Carlo samples m and minibatch size λ under a fixed computational budget?
- Basis in paper: Explicit - The paper states "our analysis suggests where one should invest most of the budget in general" and provides theoretical insights in Remark 4 and Remark 5 about the different effects of m and λ under varying correlation regimes.
- Why unresolved: The analysis provides general insights but doesn't give specific quantitative guidelines for different correlation levels or problem types. The optimal allocation likely depends on the specific problem structure and correlation characteristics.
- What evidence would resolve it: Empirical studies across different problem classes showing the relationship between correlation levels, budget allocation, and convergence rates, or analytical expressions relating correlation strength to optimal m vs λ trade-offs.

### Open Question 2
- Question: Does random reshuffling (RR) provide any asymptotic complexity improvement over independent sampling for doubly stochastic gradients when component estimators are independent (ρ = 0)?
- Basis in paper: Explicit - Remark 11 states that for independent estimators, "the asymptotic complexity of the two is equal" between doubly SGD-RR and full-batch SGD.
- Why unresolved: The paper proves that doubly SGD-RR improves the dependence on subsampling noise from O(1/n^(1/3)) to O(1/n), but doesn't establish whether there are other complexity improvements for independent estimators beyond this subsampling effect.
- What evidence would resolve it: Rigorous proof of the asymptotic complexity for doubly SGD-RR with independent estimators, or counterexamples showing no additional asymptotic benefits beyond the subsampling improvement.

### Open Question 3
- Question: How do the variance reduction properties of doubly SGD-RR translate to non-convex objectives beyond strongly convex cases?
- Basis in paper: Explicit - The analysis focuses on "strongly convex functions where the effect of stochasticity is most detrimental" and Theorem 4 specifically addresses strongly convex objectives.
- Why unresolved: The paper's theoretical analysis is limited to strongly convex and convex settings, but many practical applications involve non-convex objectives where the behavior of doubly SGD-RR might differ significantly.
- What evidence would resolve it: Convergence analysis for doubly SGD-RR on non-convex smooth objectives, or empirical studies demonstrating its effectiveness in practical non-convex applications like deep learning.

## Limitations
- The analysis assumes unbiased component estimators satisfying ER and BV conditions, which may not hold in practice
- Random reshuffling benefits require strongly convex objectives with strongly convex components, limiting applicability to broader problem classes
- The paper focuses on theoretical guarantees without providing extensive empirical validation or implementation guidelines for practitioners

## Confidence
- **High confidence**: General variance bound (Theorem 1) and proof that doubly SGD converges under ER and BV conditions
- **Medium confidence**: Specific claims about random reshuffling improvements and computational budget allocation insights
- **Low confidence**: Practical applicability of theoretical bounds without empirical validation

## Next Checks
1. **Empirical validation**: Test doubly SGD with dependent component estimators on benchmark optimization problems to verify theoretical predictions about variance reduction and convergence behavior.

2. **Assumption relaxation**: Investigate convergence guarantees when component estimators don't satisfy ER or BV conditions, or when the objective function doesn't meet the strong convexity requirements for random reshuffling.

3. **Hyperparameter sensitivity**: Study the practical impact of different minibatch sizes and Monte Carlo sample allocations on convergence speed and final accuracy for various problem classes.