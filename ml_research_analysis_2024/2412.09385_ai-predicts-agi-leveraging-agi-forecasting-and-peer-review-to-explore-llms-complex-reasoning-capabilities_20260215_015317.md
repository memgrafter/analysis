---
ver: rpa2
title: 'AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs''
  Complex Reasoning Capabilities'
arxiv_id: '2412.09385'
source_url: https://arxiv.org/abs/2412.09385
tags:
- llms
- criterion
- forecasting
- scores
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces an automated peer review process for evaluating
  LLM-generated AGI forecasts, finding high consistency among LLM raters (ICC = 0.79).
  Sixteen state-of-the-art LLMs estimated AGI likelihood by 2030, ranging from 3%
  to 47.6%, with a median of 12.5%, closely aligning with human expert surveys.
---

# AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities

## Quick Facts
- **arXiv ID**: 2412.09385
- **Source URL**: https://arxiv.org/abs/2412.09385
- **Reference count**: 0
- **Primary result**: LLM-generated AGI forecasts (median 12.5% by 2030) closely align with human expert surveys, validated through automated peer review with high consistency (ICC = 0.79)

## Executive Summary
This study introduces an automated peer review process for evaluating large language models' (LLMs) AGI forecasts, finding that LLM-generated predictions align closely with human expert estimates. The methodology involves having 16 state-of-the-art LLMs generate AGI likelihood predictions and then using automated peer review to assess forecast quality. The LLM peer review demonstrated strong reliability with an Intraclass Correlation Coefficient of 0.79. Notably, the study reveals that standard LLM evaluation benchmarks fail to predict AGI forecasting ability, leading to the development of specialized "AGI benchmarks" that better capture relevant evaluation skills.

## Method Summary
The study evaluates 16 state-of-the-art LLMs on their ability to forecast AGI likelihood by 2030. Each LLM generates a forecast using a structured prompt, then participates in an automated peer review (LLM-PR) process where it evaluates every other LLM's forecast using 9 criteria on a 1-5 Likert scale. The peer review consistency is measured using ICC, and benchmark performance is compared against expert rankings. The authors develop specialized AGI benchmarks by optimizing confidence weights to align with expert evaluations, testing whether specialized evaluation panels better capture AGI forecasting skill.

## Key Results
- LLM-generated AGI forecasts showed median likelihood of 12.5% by 2030, closely matching human expert surveys (10% by 2027)
- Automated peer review achieved high consistency (ICC = 0.79), demonstrating LLMs can reliably evaluate each other's AGI forecasts
- Standard benchmarks (Arena, MixEval, AlpacaEval) failed to predict AGI forecasting performance, leading to development of specialized AGI benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Peer review consistency (ICC = 0.79) demonstrates LLMs can evaluate AGI forecasts reliably across models
- Mechanism: Each LLM rates every other LLM's AGI forecast on 9 criteria, creating a matrix of scores. High ICC indicates raters agree on relative forecast quality despite different individual scoring tendencies
- Core assumption: LLMs can meaningfully assess reasoning quality and forecast rigor when given structured criteria
- Evidence anchors:
  - [abstract] "The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79)"
  - [section 6.1] "The ICC(C,16) value of 0.79 indicates a high level of consistency in the LLM evaluations"
  - [corpus] Missing evidence on whether this consistency transfers to human-like evaluation quality
- Break condition: If criteria don't capture what makes a good AGI forecast, consistency could be high but evaluations could be systematically wrong

### Mechanism 2
- Claim: Standard benchmarks fail to predict AGI forecasting skill
- Mechanism: LLM rankings based on Arena, MixEval, and AlpacaEval scores remain consistent with uniform-weighted rankings and don't align with expert rankings, suggesting benchmarks measure different capabilities than AGI forecasting
- Core assumption: Different evaluation tasks require different cognitive capabilities that may not overlap
- Evidence anchors:
  - [section 6.2] "The main conclusion from these findings is that the choice of benchmark used to evaluate the LLMs does not significantly impact the final rankings"
  - [section 6.3] "A significant negative correlation exists between SES score and Arena value, as well as between SEI score and Arena value"
  - [corpus] Weak evidence on what specific capabilities AGI forecasting requires versus what benchmarks measure
- Break condition: If benchmarks were designed to capture AGI-relevant skills, they might show correlation with AGI forecasting performance

### Mechanism 3
- Claim: Specialized "AGI benchmarks" can better identify AGI forecasting skill
- Mechanism: Confidence weights optimized to align with expert rankings create AGI Bench16 and AGI Bench14, which when used for evaluation produce rankings closest to expert judgments
- Core assumption: Expert evaluations represent the ground truth for AGI forecasting quality
- Evidence anchors:
  - [section 7.4] "AGI Bench16 and AGI Bench14 share a crucial characteristic: they produce rankings of forecasters that most closely match expert rankings"
  - [section 6.3] "scores weighted by Arena or given by the AGI 14 and AGI 16 panels exhibit a negative significant correlation with the Expert scores"
  - [corpus] No external validation of expert rankings as ground truth
- Break condition: If expert evaluations are themselves inconsistent or biased, optimized benchmarks may just replicate those biases

## Foundational Learning

- Intraclass Correlation Coefficient (ICC)
  - Why needed here: To quantify agreement among LLM raters evaluating each other's forecasts
  - Quick check question: If ICC = 0.79, what percentage of total variance is due to between-rater differences versus within-rater differences?

- Kendall distance for ranking comparison
  - Why needed here: To measure how similar different ranking methods are (e.g., uniform vs benchmark-weighted vs expert)
  - Quick check question: If two rankings have Kendall distance of 0.1 on 16 items, approximately how many pairwise disagreements exist between them?

- Confidence weighting optimization
  - Why needed here: To find evaluation panels whose judgments align with expert preferences
  - Quick check question: In the optimization, what happens to LLMs with zero confidence weights?

## Architecture Onboarding

- Component map:
  Forecast generation -> Peer review evaluation -> Benchmark weighting -> Optimization -> Analysis

- Critical path:
  1. Generate AGI forecasts from all 16 LLMs
  2. Have each LLM evaluate all forecasts using 9 criteria
  3. Compute ICC to assess consistency
  4. Apply benchmark weighting schemes
  5. Optimize confidence weights to align with expert rankings
  6. Analyze results and develop AGI benchmarks

- Design tradeoffs:
  - Consistency vs accuracy: High ICC doesn't guarantee expert-aligned judgments
  - Complexity vs interpretability: 9 criteria provide granularity but make analysis harder
  - Benchmark diversity vs specialization: Multiple benchmarks provide breadth but may miss AGI-specific skills

- Failure signatures:
  - Low ICC (< 0.5): Raters disagree fundamentally on what makes a good forecast
  - Benchmark rankings far from uniform: Benchmarks measure very different capabilities
  - Optimization produces extreme weights: Panel selection is unstable or overfit to expert rankings

- First 3 experiments:
  1. Generate forecasts and compute ICC - verify consistency mechanism works
  2. Apply uniform weighting vs single benchmark weighting - test if benchmarks change rankings
  3. Run optimization with different alpha/beta values - explore sensitivity to expert alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-generated AGI forecasts compare in accuracy to human expert forecasts when evaluated against actual AGI development outcomes?
- Basis in paper: [explicit] The study compares LLM predictions (median 12.5% by 2030) with human expert estimates (10% by 2027) and notes alignment, but does not validate against actual outcomes
- Why unresolved: AGI has not yet occurred, making empirical validation impossible with current data
- What evidence would resolve it: Future tracking of AGI development timelines against LLM and human expert predictions, with statistical comparison of prediction accuracy

### Open Question 2
- Question: What specific cognitive capabilities or reasoning processes enable certain LLMs to generate more accurate AGI forecasts than others?
- Basis in paper: [inferred] The study notes significant variation in LLM forecasts (3% to 47.6%) but doesn't identify underlying factors driving this variation
- Why unresolved: The paper analyzes forecast outcomes but doesn't investigate the internal reasoning mechanisms or model architectures that produce better predictions
- What evidence would resolve it: Comparative analysis of successful vs. unsuccessful forecasting LLMs, examining their training data, architecture differences, and reasoning patterns

### Open Question 3
- Question: Can the AGI-specific benchmarks (AGI Bench16 and AGI Bench14) developed in this study be generalized to evaluate LLM performance on other speculative, interdisciplinary forecasting tasks?
- Basis in paper: [explicit] The authors developed these benchmarks specifically for AGI forecasting and note their effectiveness in aligning LLM evaluations with human experts
- Why unresolved: The benchmarks were tailored for AGI prediction and their applicability to other complex forecasting domains remains untested
- What evidence would resolve it: Application of AGI Bench16 and AGI Bench14 to different speculative domains (e.g., climate change, technological singularity) and comparison with expert evaluations in those fields

## Limitations

- External validity concerns about whether LLM evaluators can truly replicate human expert judgment quality
- Benchmark inadequacy - uncertainty about what specific cognitive capabilities AGI forecasting requires versus what benchmarks assess
- Self-evaluation bias in LLM self-assessments, with higher-performing models systematically underestimating their outputs

## Confidence

**High confidence**: Automated peer review achieves strong consistency (ICC = 0.79) across LLM raters
**Medium confidence**: LLM-generated AGI forecasts align with human expert surveys (median 12.5% vs expert range of 10-25%)
**Low confidence**: Standard benchmarks fail to predict AGI forecasting skill and specialized AGI benchmarks can reliably identify forecasting ability

## Next Checks

1. Cross-validation with human reviewers: Have human experts independently evaluate the same LLM-generated forecasts to compare against automated peer review results

2. Benchmark capability mapping: Analyze what specific capabilities each benchmark measures and systematically map these to skills required for AGI forecasting

3. External benchmark correlation: Test proposed AGI benchmarks against LLM performance on tasks requiring long-term forecasting, scenario planning, and complex reasoning