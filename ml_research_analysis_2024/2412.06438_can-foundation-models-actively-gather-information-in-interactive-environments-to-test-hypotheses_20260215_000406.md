---
ver: rpa2
title: Can foundation models actively gather information in interactive environments
  to test hypotheses?
arxiv_id: '2412.06438'
source_url: https://arxiv.org/abs/2412.06438
tags:
- exploration
- gemini
- environment
- object
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Foundation models can strategically explore interactive environments
  to test hypotheses about hidden reward functions, performing near-optimally in simple
  single-feature tasks but showing degraded performance in more complex multi-feature
  scenarios. Self-correction prompting significantly improves their ability to learn
  causal rules across trials, enabling adaptive re-learning when environment rules
  change.
---

# Can foundation models actively gather information in interactive environments to test hypotheses?

## Quick Facts
- arXiv ID: 2412.06438
- Source URL: https://arxiv.org/abs/2412.06438
- Authors: Danny P. Sawyer; Nan Rosemary Ke; Hubert Soyer; Martin Engelcke; David P Reichert; Drew A. Hudson; John Reid; Alexander Lerchner; Danilo Jimenez Rezende; Timothy P Lillicrap; Michael Mozer; Jane X Wang
- Reference count: 39
- Primary result: Foundation models can strategically explore interactive environments to test hypotheses about hidden reward functions, performing near-optimally in simple single-feature tasks but showing degraded performance in more complex multi-feature scenarios.

## Executive Summary
Foundation models demonstrate the ability to actively gather information in interactive environments to test hypotheses about hidden reward functions, showing near-optimal performance on simple single-feature tasks while struggling with more complex multi-feature scenarios. The study introduces a framework for evaluating exploratory reasoning in both text-based and 3D embodied environments, revealing that current models possess latent information-gathering capabilities that can be enhanced through appropriate prompting strategies. Self-correction prompting enables emergent meta-learning across trials, allowing models to adaptively re-learn when environment rules change.

## Method Summary
The paper evaluates foundation models' active information gathering capabilities using text-based Feature World environments and 3D embodied Construction Lab environments. Models are tested on tasks where they must identify hidden reward functions through iterative exploration without fine-tuning, using zero-shot prompting. The study compares different foundation models (Gemini 1.5 Pro, Gemini 1.5 Flash, Claude 3.7, ChatGPT-4o, o4-mini) against optimal and random baselines, measuring exploration efficiency, property identification accuracy, and visual error rates. Self-correction prompting is introduced to enable meta-learning across trials.

## Key Results
- Foundation models perform near-optimally on single-feature tasks but show suboptimal performance on conjunction tasks requiring multiple feature identification
- Self-correction prompting enables emergent meta-learning, allowing models to adaptively re-learn when environment rules change
- Gemini 2.5 demonstrates the strongest robustness across tasks, while smaller models like Gemini Flash excel at simpler tasks and larger models with self-correction perform better on complex conjunction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Foundation models can actively explore environments to test hypotheses about hidden reward functions
- Mechanism: Models iteratively gather information through action-observation cycles, updating beliefs based on rewards received from chosen objects
- Core assumption: Models can reason about cause-effect relationships between object properties and rewards
- Evidence anchors:
  - [abstract] "Foundation models excel at single-turn reasoning but struggle with multi-turn exploration in dynamic environments"
  - [section] "Our experiments with Gemini 1.5 reveal significant exploratory capabilities, effective navigation of complex abstract problem spaces"
  - [corpus] Found 25 related papers; PhysGym and World-Model Learning benchmarks show similar exploration themes
- Break condition: When tasks require integrating knowledge over time rather than selecting informative actions in the moment

### Mechanism 2
- Claim: Self-correction prompting enables emergent meta-learning across trials
- Mechanism: Regular summarization of observations allows models to form hypotheses about environment rules and adaptively re-learn when rules change
- Core assumption: Models can maintain coherent in-context memory of past trials and extract patterns
- Evidence anchors:
  - [abstract] "Crucially, we found that prompting the models to summarize their observations at regular intervals enabled an emergent meta-learning process"
  - [section] "we found that prompting the models to summarize their observations at regular intervals enabled an emergent meta-learning process"
  - [corpus] Information Seeking for Robust Decision Making under Partial Observability supports iterative hypothesis testing
- Break condition: When in-context memory load becomes too high or environmental complexity exceeds model's reasoning capacity

### Mechanism 3
- Claim: Model size affects exploration efficiency differently based on reward function complexity
- Mechanism: Smaller models perform better on simple single-feature tasks while larger models with self-correction excel at complex conjunction tasks
- Core assumption: There's a tradeoff between model size/reasoning complexity and reward function complexity
- Evidence anchors:
  - [section] "Statistical analysis reveals that Gemini Flash excels with simpler reward functions, while Gemini Pro with self-correction performs better on those with multiple factors"
  - [abstract] "models struggle most with integrating knowledge over time rather than selecting informative actions in the moment"
  - [corpus] Weak - only 25 related papers found, suggesting this specific relationship isn't well-established in literature
- Break condition: When task complexity exceeds the optimal model size threshold

## Foundational Learning

- Concept: Active information gathering vs passive exploration
  - Why needed here: Understanding the difference between random exploration and strategic hypothesis testing
  - Quick check question: What distinguishes "active exploration" from "random exploration" in reinforcement learning?

- Concept: Multi-turn reasoning in sequential decision-making
  - Why needed here: Models must maintain coherent reasoning across multiple exploration steps
  - Quick check question: How does single-turn reasoning differ from multi-turn reasoning in interactive environments?

- Concept: In-context learning and memory management
  - Why needed here: Models must efficiently use limited context to store and retrieve past observations
  - Quick check question: What happens to model performance as in-context memory load increases?

## Architecture Onboarding

- Component map: Foundation model -> Exploration policy -> Action execution -> Observation -> Belief update -> Next action
- Critical path: Prompt formulation -> Model inference -> Action selection -> Environment response -> Memory update
- Design tradeoffs: Model size vs inference time vs exploration efficiency; simple vs complex reward functions
- Failure signatures: Degraded performance with multi-feature rewards; increased errors with higher cognitive load
- First 3 experiments:
  1. Run single-feature task with varying numbers of unique colors to test exploration efficiency
  2. Compare Gemini Pro vs Flash performance on conjunction tasks with self-correction
  3. Test 3D embodied environment translation from text-based performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do foundation models perform on more complex multi-feature tasks beyond the simple conjunction tasks tested in this paper?
- Basis in paper: [explicit] The paper states that "when the model must identify a conjunction of rewarding features, performance is suboptimal" and suggests this is due to limitations in policy translation and in-context memory use.
- Why unresolved: The paper only tests conjunction tasks with up to two features. It's unclear how models would perform with three or more features, or with more complex logical relationships between features.
- What evidence would resolve it: Testing foundation models on tasks with three or more features, or with different logical relationships (e.g., disjunctions, exclusive-or) between features, and comparing their performance to optimal baselines.

### Open Question 2
- Question: How would fine-tuning the vision system of foundation models affect their performance in 3D embodied environments?
- Basis in paper: [explicit] The paper mentions that "accurate interpretation of visual observations remains a challenge due to limitations in the vision system" and suggests that "Improving visual accuracy, potentially through fine-tuning, is important for achieving comparable performance in 3D embodied environments."
- Why unresolved: The paper uses off-the-shelf vision models without any fine-tuning. It's unclear how much improvement in performance could be achieved through fine-tuning specifically for the task at hand.
- What evidence would resolve it: Comparing the performance of foundation models with and without fine-tuned vision systems on the same 3D embodied tasks, measuring both exploration efficiency and accuracy in identifying rewarding features.

### Open Question 3
- Question: How does the size of the foundation model impact its ability to perform exploratory reasoning in complex environments?
- Basis in paper: [explicit] The paper mentions that "Gemini Flash was found to outperform the Pro model in the single-factor condition" and suggests a potential trade-off between model size and reward function complexity.
- Why unresolved: The paper only compares two sizes of Gemini models. It's unclear how other model sizes or architectures would perform, and whether there's an optimal size for exploratory reasoning tasks.
- What evidence would resolve it: Testing a range of foundation model sizes and architectures on the same exploratory reasoning tasks, measuring both exploration efficiency and accuracy across different task complexities.

## Limitations

- Performance degradation on multi-feature tasks suggests current models struggle with complex hypothesis integration
- Translation from text-based to 3D embodied environments reveals significant capability gaps
- Self-correction prompting mechanism may not scale to more complex environments where in-context memory becomes overwhelmed

## Confidence

**High confidence**: Single-feature task performance and the superiority of Gemini 2.5 across tasks are well-supported by quantitative comparisons with optimal and random baselines.

**Medium confidence**: The emergent meta-learning effect from self-correction prompting is observed but may be partially attributable to increased inference steps rather than genuine causal rule learning.

**Low confidence**: The claim that models "struggle most with integrating knowledge over time rather than selecting informative actions in the moment" lacks direct comparative evidence between these two specific failure modes.

## Next Checks

1. **Ablation study on self-correction frequency**: Systematically vary the intervals at which models summarize observations to determine the optimal balance between meta-learning benefits and context window constraints.

2. **Cross-task generalization test**: Evaluate whether models trained on single-feature tasks can successfully transfer their exploration strategies to conjunction tasks without additional prompting modifications.

3. **Memory efficiency analysis**: Quantify the relationship between context window usage, exploration performance, and inference time across different model sizes to identify scalability limits of the active exploration approach.