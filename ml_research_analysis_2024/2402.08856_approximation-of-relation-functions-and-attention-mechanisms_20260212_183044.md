---
ver: rpa2
title: Approximation of relation functions and attention mechanisms
arxiv_id: '2402.08856'
source_url: https://arxiv.org/abs/2402.08856
tags:
- neural
- function
- inner
- relation
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the approximation properties of inner products
  of neural networks for modeling relations between inputs. It is shown that the inner
  product of a multi-layer perceptron with itself is a universal approximator for
  symmetric positive-definite relation functions, while the inner product of two different
  multi-layer perceptrons can approximate asymmetric relation functions.
---

# Approximation of relation functions and attention mechanisms

## Quick Facts
- arXiv ID: 2402.08856
- Source URL: https://arxiv.org/abs/2402.08856
- Authors: Awni Altabaa; John Lafferty
- Reference count: 8
- Key outcome: Inner products of neural networks can universally approximate symmetric positive-definite relation functions (via single MLP) and asymmetric relation functions (via two different MLPs), with applications to attention mechanisms in Transformers

## Executive Summary
This paper establishes theoretical foundations for approximating relation functions using inner products of neural networks. It demonstrates that symmetric positive-definite relations can be modeled by the self-inner-product of a single MLP, while asymmetric relations require the inner product of two different MLPs. The work provides bounds on the number of neurons needed for accurate approximation and applies these results to analyze attention mechanisms in Transformers, showing that any retrieval mechanism defined by an abstract preorder can be approximated through attention's inner product relations.

## Method Summary
The paper uses Mercer's theorem to decompose symmetric positive-definite kernels into eigenfunctions and eigenvalues, then approximates the square roots of these components using a single MLP. For asymmetric relations, it quantizes the space using one neural network and represents the relation as a matrix decomposed into the product of two matrices, each approximated by different MLPs. The attention mechanism analysis converts preorders into utility functions via Debreu's representation theorem, which are then approximated by neural networks whose inner products drive attention weights.

## Key Results
- Symmetric positive-definite relation functions can be universally approximated by self-inner-products of MLPs
- Asymmetric relation functions can be universally approximated by inner-products of two different MLPs
- Attention mechanisms can approximate any retrieval function defined by an abstract preorder
- Exponential dependence on input dimension is generally necessary for asymmetric relations without additional structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetric positive-definite relation functions can be approximated by inner products of a single MLP.
- Mechanism: A symmetric positive-definite kernel has a Mercer decomposition into eigenfunctions and eigenvalues. The neural network ϕ approximates the square roots of the first d eigenvalues times their eigenfunctions. The inner product of ϕ with itself then approximates the original kernel.
- Core assumption: The kernel is symmetric positive-definite and continuous on a compact Euclidean space.
- Evidence anchors:
  - [abstract]: "It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions"
  - [section 2]: "In this section, we characterize the class of relation functions that can be modeled by symmetric inner products of neural networks. We show that any continuous symmetric positive definite kernel (i.e., Mercer kernel) can be approximated by a symmetric inner product of neural networks"
  - [corpus]: Weak corpus support; related work focuses on low-rank approximation and matrix analysis, not kernel approximation theory.
- Break condition: If the kernel is not symmetric positive-definite or the space is not compact, Mercer's theorem doesn't apply and the approximation guarantee fails.

### Mechanism 2
- Claim: Asymmetric relation functions can be approximated by inner products of two different MLPs.
- Mechanism: First quantize the space into Voronoi cells with a neural network η. Then represent the relation on the quantized space as a matrix R and decompose R = P⊤Q. The MLPs ϕ = P∘η and ψ = Q∘η approximate the relation via inner product.
- Core assumption: The relation function is continuous and the space can be sufficiently quantized.
- Evidence anchors:
  - [abstract]: "In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator"
  - [section 3]: "In this section, we consider modeling a general asymmetric relation r : X × X → R as the inner product of two different neural network encodings"
  - [corpus]: Weak corpus support; related work focuses on function approximation and matrix decomposition, not asymmetric relation modeling.
- Break condition: If the relation is discontinuous or the quantization diameter cannot be made small enough, the approximation error cannot be controlled.

### Mechanism 3
- Claim: Attention mechanisms can approximate any retrieval function defined by an abstract preorder.
- Mechanism: Use Debreu's representation theorem to convert the preorder into a continuous utility function. Then approximate this utility function with neural networks and use the resulting inner products in attention to retrieve the most relevant element.
- Core assumption: The preorder family satisfies key-continuity and query-continuity assumptions.
- Evidence anchors:
  - [abstract]: "Finally, these approximation results are applied to analyzing the attention mechanism underlying Transformers, showing that any retrieval mechanism defined by an abstract preorder can be approximated by attention through its inner product relations"
  - [section 5]: "In this section, we will use the results developed above to analyze the attention mechanism underlying the Transformer architecture"
  - [corpus]: Weak corpus support; related work focuses on attention mechanisms and transformers, not preorder representation theory.
- Break condition: If the preorder is not continuous or doesn't satisfy the regularity assumptions, Debreu's theorem doesn't apply and the approximation fails.

## Foundational Learning

- Concept: Mercer's theorem and reproducing kernel Hilbert spaces
  - Why needed here: The symmetric case relies on Mercer's theorem to decompose kernels into eigenfunctions and eigenvalues, which are then approximated by neural networks.
  - Quick check question: What does Mercer's theorem guarantee about symmetric positive-definite kernels on compact spaces?

- Concept: Banach space duality and reproducing kernel Banach spaces
  - Why needed here: The asymmetric case uses the theory of reproducing kernel Banach spaces, where the dual space plays a crucial role in representing the relation function.
  - Quick check question: How does a reproducing kernel Banach space differ from a reproducing kernel Hilbert space?

- Concept: Debreu's representation theorem in utility theory
  - Why needed here: The attention application uses Debreu's theorem to convert a preorder relation into a utility function that can be approximated by neural networks.
  - Quick check question: What are the key assumptions required for Debreu's representation theorem to hold?

## Architecture Onboarding

- Component map: Input → MLP → Feature vector → Inner product → Output (symmetric); Input1 → MLP1 → Feature vector1, Input2 → MLP2 → Feature vector2 → Inner product → Output (asymmetric); Query/Context → MLPs → Inner products → Softmax → Weighted sum (attention)

- Critical path: For symmetric relations: input → MLP → feature vector → inner product with itself → output. For asymmetric relations: input1 → MLP1 → feature vector1, input2 → MLP2 → feature vector2 → inner product → output. For attention: query/context → MLPs → inner products → softmax → weighted sum.

- Design tradeoffs: The main tradeoff is between approximation accuracy and network size. More neurons allow better approximation but increase computational cost. For symmetric relations, the tradeoff is characterized by the kernel's spectral decay. For asymmetric relations, the curse of dimensionality means exponential growth in neurons with input dimension unless additional structure is present.

- Failure signatures: Poor approximation when the target relation doesn't satisfy the required mathematical properties (positive-definiteness for symmetric case, continuity for asymmetric case). Attention failures when the preorder doesn't satisfy continuity assumptions or when the data distribution doesn't allow clear separation of relevance scores.

- First 3 experiments:
  1. Verify symmetric case: Implement a simple Mercer kernel (e.g., RBF) and show that a single MLP can approximate it via self-inner-product.
  2. Verify asymmetric case: Create a synthetic asymmetric relation (e.g., a preference ordering) and show that two different MLPs can approximate it via inner product.
  3. Verify attention case: Implement a simple preorder (e.g., lexicographic ordering) and show that attention with appropriate MLPs can retrieve the most relevant element from a context.

## Open Questions the Paper Calls Out

- Question: Can the exponential dependence on the input dimension in the bounds for approximating asymmetric relation functions be avoided under more structured assumptions on the relation function?
- Basis in paper: [explicit] The paper states that in general, an exponential dependence on the input dimension is necessary for any method of approximation, but also mentions that this can be avoided with additional structure on the relation function.
- Why unresolved: While the paper provides a specific example of a compositional structure that avoids the curse of dimensionality, it is unclear whether this is the most general case or if there are other types of structure that can also lead to improved bounds.
- What evidence would resolve it: A comprehensive analysis of different types of structure on the relation function and their impact on the approximation bounds, including both positive and negative results.

- Question: How can the full distribution of attention values computed by the softmax function in Transformers be approximated more tightly?
- Basis in paper: [explicit] The paper focuses on approximating the most relevant key to a given query using attention, but mentions that it would be of interest to derive approximation bounds for the full distribution of attention values.
- Why unresolved: The softmax function introduces non-linearities that may make it challenging to obtain tight bounds on the attention distribution. Additionally, the interaction between the query, context, and attention mechanism may need to be considered more carefully.
- What evidence would resolve it: A theoretical analysis of the softmax function and its interaction with the query and context in attention mechanisms, leading to improved approximation bounds for the attention distribution.

- Question: What are the approximation properties of higher-order, recursive relations in the context of relational learning?
- Basis in paper: [explicit] The paper mentions that higher-order, recursive relations naturally arise in the context of relational learning and suggests that it may be interesting to study function spaces of hierarchical relations in such settings.
- Why unresolved: The paper does not provide any specific results or insights into the approximation properties of higher-order, recursive relations. Understanding these properties would be important for developing more expressive and powerful relational learning models.
- What evidence would resolve it: A theoretical analysis of higher-order, recursive relations and their approximation properties, potentially drawing on tools from functional analysis, harmonic analysis, or other relevant fields.

## Limitations
- Theoretical bounds rely on strong mathematical assumptions (compactness, continuity) that may not hold in practical applications
- Exponential dependence on input dimension for asymmetric relations without additional structure
- Computational complexity of implementing these approximations at scale is not addressed

## Confidence
- High Confidence: The basic mathematical framework (Mercer's theorem, universal approximation theorems) is well-established and correctly applied
- Medium Confidence: The specific approximation bounds and their derivation, as these involve novel combinations of existing results
- Medium Confidence: The application to attention mechanisms, as it depends on the practical applicability of abstract preorder assumptions

## Next Checks
1. Implement numerical experiments to verify that the theoretical neuron bounds are tight in practice, testing both symmetric (RBF kernel) and asymmetric (preference orderings) cases.
2. Test the attention mechanism approximation on realistic datasets where ground truth relevance orderings are known, measuring both approximation quality and computational efficiency.
3. Investigate the sensitivity of the approximation results to violations of the mathematical assumptions (e.g., non-compact domains, discontinuous relations) through systematic ablation studies.