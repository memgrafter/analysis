---
ver: rpa2
title: 'EmoBench: Evaluating the Emotional Intelligence of Large Language Models'
arxiv_id: '2402.12071'
source_url: https://arxiv.org/abs/2402.12071
tags:
- emotional
- emotions
- llms
- emotion
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmoBench, a comprehensive benchmark for evaluating
  emotional intelligence (EI) in large language models (LLMs). The benchmark addresses
  limitations in existing EI evaluations, which focus primarily on emotion recognition
  and rely on pattern-based datasets.
---

# EmoBench: Evaluating the Emotional Intelligence of Large Language Models

## Quick Facts
- arXiv ID: 2402.12071
- Source URL: https://arxiv.org/abs/2402.12071
- Reference count: 36
- Primary result: EmoBench reveals significant gap between LLM and human emotional intelligence performance

## Executive Summary
This paper introduces EmoBench, a comprehensive benchmark for evaluating emotional intelligence (EI) in large language models (LLMs). Unlike existing EI evaluations that focus on pattern-based emotion recognition, EmoBench proposes a more holistic definition of machine EI, encompassing Emotional Understanding (EU) and Emotional Application (EA). The benchmark consists of 400 hand-crafted multiple-choice questions in English and Chinese, designed to require deep reasoning and understanding. Experiments with various LLMs reveal a significant gap between their EI capabilities and average human performance, with the best-performing LLM (GPT-4) falling short of human benchmarks. The findings highlight the need for further research in developing emotionally intelligent LLMs.

## Method Summary
EmoBench evaluates LLM emotional intelligence through a two-dimensional framework: Emotional Understanding (EU) and Emotional Application (EA). The benchmark includes 400 hand-crafted multiple-choice questions in English and Chinese, created by workers following detailed taxonomy guidelines. Questions are designed to require inference of implicit emotional causes rather than relying on explicit patterns. Evaluation uses zero-shot prompting with task instructions and chain-of-thought reasoning. The dataset is publicly available on GitHub for replication.

## Key Results
- GPT-4 achieves the highest performance among tested LLMs but still falls significantly short of human benchmarks
- Chain-of-thought reasoning shows little improvement for emotional intelligence tasks compared to direct answering
- Models struggle particularly with complex emotions and perspective-taking scenarios in the EU dimension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EmoBench creates more challenging scenarios by embedding implicit emotional causes and transcending common patterns.
- Mechanism: Traditional emotion datasets rely on explicit cause statements and pattern-based emotion labels (e.g., "losing" → "sadness"). EmoBench removes this explicit mapping, requiring models to infer the cause from context and reason about the individual's mental state.
- Core assumption: The model can perform multi-step reasoning about human mental states and cultural/personal factors to deduce emotional causes.
- Evidence anchors:
  - [abstract] "they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation"
  - [section] "we create more challenging scenarios in which merely relying on common patterns would not lead to the correct response, and understanding emotional implications and thorough reasoning is necessitated"
- Break condition: If the model defaults to pattern matching or relies solely on surface-level textual similarity without engaging in reasoning about mental states.

### Mechanism 2
- Claim: EmoBench's taxonomy-based categorization ensures comprehensive coverage of emotional intelligence dimensions.
- Mechanism: By synthesizing established psychological theories, EmoBench defines two core dimensions: Emotional Understanding (EU) and Emotional Application (EA). Each dimension has a detailed taxonomy that guides scenario creation and annotation, ensuring systematic assessment of different EI capabilities.
- Core assumption: The psychological theories accurately capture the essential components of machine emotional intelligence and map cleanly to assessable scenarios.
- Evidence anchors:
  - [abstract] "proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application"
  - [section] "we identified and taxonomized essential capabilities for the established dimensions: Emotional Understanding (EU) and Emotional Application (EA)"
- Break condition: If the taxonomies miss critical EI dimensions or if scenario generation doesn't faithfully represent the intended categories.

### Mechanism 3
- Claim: EmoBench's hand-crafted scenarios with multiple-choice format provide reliable, scalable evaluation while maintaining difficulty.
- Mechanism: Workers manually create scenarios based on taxonomy categories, then annotate emotion labels and causes. Multiple annotators review and translate questions to ensure quality. The multiple-choice format with plausible distractors forces models to distinguish between subtle emotional nuances rather than guessing.
- Core assumption: Human annotators can reliably identify "objectively correct" answers for subjective emotional scenarios through consensus.
- Evidence anchors:
  - [abstract] "400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding"
  - [section] "one worker was assigned to translate the MCQ... and, with the addition of two other workers, meticulously review its content to ensure data quality and overall agreement"
- Break condition: If annotator agreement is low or if the "objectively correct" answers are actually ambiguous, leading to unreliable evaluation.

## Foundational Learning

- Concept: **Theory of Mind (ToM)**
  - Why needed here: EmoBench's perspective-taking categories (False Belief, Faux Pas, Strange Story) require understanding that others have different beliefs, knowledge, and emotions than oneself - the core of ToM.
  - Quick check question: Can you explain why a person who didn't witness an event might have a different emotional reaction than someone who did?

- Concept: **Emotion Recognition vs. Emotion Understanding**
  - Why needed here: Traditional benchmarks focus on pattern-based emotion recognition (mapping words to emotions). EmoBench requires understanding the *cause* and *context* of emotions, which involves deeper reasoning.
  - Quick check question: What's the difference between recognizing that "losing a gift" might cause sadness versus understanding *why* losing a gift from a deceased relative causes more intense sadness?

- Concept: **Implicit vs. Explicit Information**
  - Why needed here: EmoBench scenarios hide emotional causes in context rather than stating them directly, testing whether models can infer unstated information.
  - Quick check question: How would you determine someone's emotional state if they never explicitly say how they feel, but their actions and context suggest strong emotions?

## Architecture Onboarding

- Component map: Psychological theory → Taxonomy definition → Scenario generation → Annotation & review → MCQ creation → LLM evaluation → Analysis
- Critical path: Taxonomy → Scenario Generation → Annotation & Review → MCQ Creation → LLM Evaluation → Analysis
- Design tradeoffs: Hand-crafted scenarios ensure quality and difficulty but limit scale compared to automated generation; multiple-choice format enables reliable evaluation but may constrain the richness of emotional scenarios.
- Failure signatures: Models perform near chance on EU tasks but better on EA tasks (suggesting pattern matching without understanding); models rely heavily on choice ordering bias; inter-annotator agreement is low.
- First 3 experiments:
  1. Run baseline models (Random, Majority) on EmoBench to establish performance floor
  2. Test few-shot prompting vs. zero-shot to see if examples improve performance
  3. Analyze error patterns by category (e.g., which EU subcategories models struggle with most)

## Open Questions the Paper Calls Out

- **Open Question 1**: How do fine-grained personal traits like detailed experiences, characteristics, and language expression affect the experienced emotions in emotionally intelligent systems?
  - Basis in paper: Explicit
  - Why unresolved: The paper acknowledges this as a limitation, noting that they only studied two types of relationships (personal and social) and didn't explore more fine-grained personal traits that could impact emotional responses.
  - What evidence would resolve it: Experimental results comparing LLM performance on emotion recognition and application tasks with varying levels of personal trait detail in scenarios.

- **Open Question 2**: Does the performance of large language models on emotional intelligence tasks improve significantly when chain-of-thought reasoning is augmented with specialized emotional reasoning techniques?
  - Basis in paper: Explicit
  - Why unresolved: The paper found that chain-of-thought reasoning had little to no improvement on emotional intelligence tasks, but only tested a basic CoT approach without specialized emotional reasoning techniques.
  - What evidence would resolve it: Comparative experiments testing LLMs with and without specialized emotional reasoning techniques in CoT prompting on the EmoBench benchmark.

- **Open Question 3**: How does the emotional intelligence performance of large language models vary across different languages and cultures beyond English and Chinese?
  - Basis in paper: Inferred
  - Why unresolved: The paper only evaluated LLMs on English and Chinese subsets of the benchmark due to resource limitations, leaving open the question of cross-linguistic and cross-cultural performance differences.
  - What evidence would resolve it: Comprehensive multilingual and multicultural evaluation of LLMs on the EmoBench benchmark, testing performance across diverse languages and cultural contexts.

## Limitations

- Reliance on human annotator consensus for determining "objectively correct" answers to inherently subjective emotional scenarios
- Hand-crafted nature of only 400 questions creates a relatively small evaluation set
- Benchmark's focus on English and Chinese limits cross-cultural generalizability

## Confidence

**High Confidence**: The claim that EmoBench addresses limitations in existing EI evaluations by moving beyond pattern-based datasets is well-supported by clear distinctions between traditional emotion recognition and the proposed EU/EA framework.

**Medium Confidence**: The assertion that LLMs show a significant gap compared to human performance is supported by experimental results, but the human benchmark methodology isn't fully detailed.

**Low Confidence**: The claim that EmoBench's taxonomy provides comprehensive coverage of EI dimensions relies heavily on the assumption that the selected psychological theories fully capture machine EI.

## Next Checks

1. **Inter-annotator Reliability Analysis**: Request or calculate Cohen's kappa or similar statistics for scenario annotation to verify that human consensus on "correct" emotional responses is sufficiently reliable.

2. **Cross-cultural Validation**: Test EmoBench with native speakers from additional cultures beyond English and Chinese to verify that the scenarios and their "correct" answers generalize across cultural contexts.

3. **Adversarial Testing with Pattern-Matching Models**: Evaluate simple pattern-matching baselines on EmoBench to quantify how much performance improvement over random guessing actually requires the claimed deep reasoning versus surface-level textual features.