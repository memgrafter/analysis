---
ver: rpa2
title: 'AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation'
arxiv_id: '2411.13152'
source_url: https://arxiv.org/abs/2411.13152
tags:
- domain
- adaptation
- data
- target
- ssda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGLP, a novel approach for semi-supervised
  domain adaptation (SSDA) that incorporates graph learning to model structural information
  in data. Unlike existing SSDA methods that focus primarily on domain labels and
  class labels, AGLP leverages graph convolutional networks (GCN) to capture and propagate
  structural relationships within the data.
---

# AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation

## Quick Facts
- arXiv ID: 2411.13152
- Source URL: https://arxiv.org/abs/2411.13152
- Authors: Houcheng Su; Mengzhu Wang; Jiao Li; Nan Yin; Liang Yang; Li Shen
- Reference count: 30
- Primary result: Outperforms state-of-the-art SSDA methods with 77.6% accuracy on Office-Home (3-shot) and 77.3% on DomainNet

## Executive Summary
AGLP introduces a novel graph learning perspective for semi-supervised domain adaptation by leveraging graph convolutional networks to capture structural relationships in data. Unlike existing SSDA methods that focus primarily on domain labels and class labels, AGLP constructs a densely connected instance graph using CNN features and structural scores, allowing structural information to be effectively learned and utilized. The method achieves state-of-the-art performance on Office-Home and DomainNet benchmarks while demonstrating the effectiveness of structural alignment in improving domain adaptation.

## Method Summary
AGLP is a semi-supervised domain adaptation method that incorporates graph learning to model structural information in data. The method uses a backbone CNN (ResNet34) to extract features, a Data Structure Analyzer (DSA) to generate structural scores, and a Graph Convolutional Network (GCN) to process a densely connected instance graph constructed from these features and scores. The model introduces a class alignment loss to ensure domain-invariant and semantic representations, along with a moving centroid strategy to mitigate the impact of incorrect pseudo-labels. Multiple loss functions including cross-entropy, adaptive clustering, pseudo-labeling, consistency, and class centroid alignment are combined for training.

## Key Results
- Achieves 77.6% accuracy on Office-Home (3-shot) and 74.7% (1-shot)
- Achieves 77.3% accuracy on DomainNet (3-shot) and 75.3% (1-shot)
- Outperforms state-of-the-art SSDA methods on both benchmarks
- Ablation study validates effectiveness of structural alignment and class centroid alignment components
- Visualization analysis confirms improved clustering of features after applying AGLP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph convolutional network propagates structural information through weighted edges to capture relationships between samples.
- Mechanism: The method constructs a densely connected instance graph using CNN features and structural scores. Each node corresponds to a sample's CNN features, and edges are weighted by structural similarity. The GCN then performs hierarchical propagation operations on this graph, allowing structural information to flow along the edges and be learned by the network.
- Core assumption: The structural relationships between samples contain discriminative information that can improve domain adaptation performance.
- Evidence anchors:
  - [abstract]: "We apply the graph convolutional network to the instance graph which allows structural information to propagate along the weighted graph edges."
  - [section 2.2]: "Our approach begins by utilizing a Data Structure Analyzer (DSA) network to generate structural scores for mini-batch samples. These scores, together with the learned CNN features of the samples, are used to construct a densely connected instance graph."
- Break condition: If the structural relationships between samples do not correlate with class labels or domain differences, the GCN propagation would not improve adaptation performance.

### Mechanism 2
- Claim: Class centroid alignment ensures features of the same class from different domains are mapped nearby, improving discriminability.
- Mechanism: After pseudo-labeling the target domain data, centroids are computed for each class in both source and target domains. The method then minimizes the squared Euclidean distance between corresponding class centroids across domains. This forces the model to map features of the same class from different domains close together in the feature space.
- Core assumption: The centroids of each class in the source and target domains should be aligned to ensure semantic consistency across domains.
- Evidence anchors:
  - [section 2.3]: "The centroid alignment objective is defined as: LCA(XS, YS, XT , YT) = ∑K k=1 ϕ(C k S, Ck T), where C k S and C k T are the centroids of class k in the source and target domains, respectively."
  - [section 2.4]: "By minimizing the distance between centroids across domains, we ensure that features of the same class are mapped nearby."
- Break condition: If pseudo-labels are highly inaccurate, centroid alignment could force incorrect class mappings and degrade performance.

### Mechanism 3
- Claim: The moving centroid strategy mitigates the impact of incorrect pseudo-labels during class centroid alignment.
- Mechanism: When computing pseudo-labels for unlabeled target data, the method uses a confidence threshold to retain only high-confidence pseudo-labels. This reduces the influence of potentially incorrect pseudo-labels on centroid computation, preventing the model from being misled by noisy pseudo-labels during alignment.
- Core assumption: High-confidence pseudo-labels are more likely to be correct and can be safely used for centroid computation.
- Evidence anchors:
  - [section 2.1.2]: "The final loss for pseudo-labeling is defined as: LP L = -∑M j=1 1{max(Pj) ≥ τ }˜yu j log(p′′ j) where P ′′ j = p(x′′ j) = σ(F (G(x′′ j))) denotes the model prediction of the transformed image x′′ j, and τ is a scalar confidence threshold that determines the subset of pseudo labels that should be retained for model training."
- Break condition: If the confidence threshold is set too high, too few pseudo-labels will be used, limiting the effectiveness of the class centroid alignment.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: GCNs are used to propagate structural information through the instance graph, allowing the model to capture relationships between samples that traditional CNNs cannot.
  - Quick check question: How does a GCN layer update node features using the adjacency matrix and node features?

- Concept: Semi-Supervised Domain Adaptation (SSDA)
  - Why needed here: The paper addresses SSDA, which aims to leverage partially labeled target domain data along with labeled source domain data to improve generalization on the target domain.
  - Quick check question: What are the three main categories of existing SSDA methods mentioned in the introduction?

- Concept: Cross-Entropy Loss
  - Why needed here: Cross-entropy loss is used to train the classifier on labeled data from both source and target domains, encouraging correct predictions.
  - Quick check question: How is the cross-entropy loss formulated for labeled data in the paper?

## Architecture Onboarding

- Component map: Input -> CNN Backbone (ResNet34) -> DSA -> Instance Graph Construction -> GCN -> Feature Concatenation -> Classifier -> Predictions
- Critical path: Input → CNN Backbone → DSA → Instance Graph Construction → GCN → Feature Concatenation → Classifier → Predictions
- Design tradeoffs: The paper trades increased model complexity (adding GCN and DSA) for improved adaptation performance. The structural alignment mechanism adds computational overhead but captures important relationships between samples.
- Failure signatures: Poor performance on domain adaptation tasks, slow convergence during training, or failure to improve over baseline methods would indicate issues with the structural alignment or class centroid alignment mechanisms.
- First 3 experiments:
  1. Reproduce baseline CDAC SLA results on Office-Home dataset to establish performance baseline
  2. Test ablation study by removing either structure-aware alignment (SAA) or class centroid alignment (CA) to measure individual contributions
  3. Visualize t-SNE embeddings to qualitatively assess clustering of features before and after applying AGLP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AGLP change when using different graph construction methods, such as varying the number of neighbors or the similarity metric?
- Basis in paper: [explicit] The paper mentions that the adjacency matrix is constructed using structure scores, but does not explore alternative graph construction methods.
- Why unresolved: The paper focuses on a specific graph construction approach and does not provide a comprehensive analysis of how different methods might impact performance.
- What evidence would resolve it: Conducting experiments with different graph construction methods and comparing their performance to the current approach would provide insights into the impact of graph structure on AGLP's effectiveness.

### Open Question 2
- Question: Can AGLP be effectively extended to handle more complex domain adaptation scenarios, such as open-set domain adaptation or partial domain adaptation?
- Basis in paper: [inferred] The paper focuses on semi-supervised domain adaptation and does not explore other domain adaptation scenarios. The graph learning perspective and alignment mechanisms proposed in AGLP could potentially be adapted to handle these more complex scenarios.
- Why unresolved: The paper does not provide any experiments or analysis on extending AGLP to other domain adaptation settings, leaving the potential for adaptation to these scenarios unexplored.
- What evidence would resolve it: Implementing and evaluating AGLP on open-set and partial domain adaptation tasks would demonstrate its ability to handle more complex scenarios and provide insights into necessary modifications.

### Open Question 3
- Question: How does the performance of AGLP compare to other graph-based methods for semi-supervised domain adaptation?
- Basis in paper: [inferred] The paper introduces a novel graph learning perspective for semi-supervised domain adaptation but does not compare its performance to other existing graph-based methods in the same domain.
- Why unresolved: While AGLP outperforms several state-of-the-art methods, a direct comparison with other graph-based approaches would provide a clearer understanding of its relative effectiveness and the advantages of its specific graph learning strategy.
- What evidence would resolve it: Conducting experiments comparing AGLP with other graph-based methods for semi-supervised domain adaptation, such as those using graph attention networks or graph convolutional networks with different architectures, would provide a comprehensive comparison of their performance.

## Limitations

- Method's effectiveness depends heavily on quality of pseudo-labels, though moving centroid strategy attempts to mitigate this limitation
- Graph construction and structural scoring mechanisms add computational complexity that may limit scalability to very large datasets
- Performance may degrade when pseudo-label confidence threshold is set too high, limiting class centroid alignment effectiveness

## Confidence

- High confidence in empirical results showing AGLP outperforming state-of-the-art SSDA methods on both Office-Home and DomainNet benchmarks
- Medium confidence in theoretical claims about structural alignment mechanisms, as the paper provides empirical evidence but limited theoretical analysis of why graph learning specifically helps domain adaptation
- Medium confidence in robustness to label noise, given ablation study but limited analysis of performance degradation with increasing pseudo-label errors

## Next Checks

1. Conduct sensitivity analysis varying the confidence threshold τ for pseudo-label selection to quantify robustness to label noise across different threshold settings
2. Perform ablation study on Office-Home with varying numbers of target domain labels (e.g., 1, 3, 5, 10 shots) to identify the sweet spot where structural alignment provides maximum benefit
3. Implement visualization of learned structural features before and after GCN propagation using t-SNE or UMAP to qualitatively assess whether structural information is being effectively captured and propagated