---
ver: rpa2
title: Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from
  Indirect Evidence
arxiv_id: '2405.15750'
source_url: https://arxiv.org/abs/2405.15750
tags:
- noun
- agreement
- language
- corpus
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Filtered Corpus Training (FiCT), a method
  that trains language models (LMs) on corpora with certain linguistic constructions
  filtered out, to test whether LMs can generalize from indirect evidence. FiCT is
  applied to both LSTM and Transformer LMs, training them on filtered corpora targeting
  a wide range of linguistic phenomena.
---

# Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence

## Quick Facts
- arXiv ID: 2405.15750
- Source URL: https://arxiv.org/abs/2405.15750
- Reference count: 20
- Language models can generalize from indirect evidence when trained on filtered corpora

## Executive Summary
This paper introduces Filtered Corpus Training (FiCT), a method that trains language models on corpora with specific linguistic constructions filtered out, to test whether models can learn grammatical rules from indirect evidence. The approach is applied to both LSTM and Transformer architectures, training them on filtered corpora targeting various linguistic phenomena. Results show that while Transformers achieve lower perplexity than LSTMs, both architectures perform equally well on linguistic generalization measures, indicating that models can form sophisticated generalizations without direct exposure to target constructions.

## Method Summary
The study filters the Gulordava et al. (2018) English Wikipedia corpus to create 15 filtered corpora targeting 67 BLiMP benchmarks, plus one full corpus. Each corpus is down-sampled to 2.4M lines. LSTM and Transformer models (68M and 67M parameters respectively) are trained on each of the 16 corpora for 40 epochs using AdamW optimizer with linear scheduler and batch size 32. Models are evaluated on both perplexity and BLiMP benchmarks, with additional metrics including accuracy delta (acc∆) comparing filtered vs. full models and probability delta (P∆) measuring confidence in grammaticality judgments.

## Key Results
- Transformers achieve lower perplexity than LSTMs on all training corpora
- Both architectures show similar linguistic generalization abilities on BLiMP benchmarks
- LSTMs demonstrate greater item-level robustness to corpus filtering than Transformers
- Models maintain non-trivial performance on filtered constructions despite never seeing them during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language models can learn syntactic rules from indirect evidence without direct exposure to the target construction.
- **Mechanism**: By filtering out specific constructions from the training corpus, the model must infer grammaticality from other related syntactic patterns. For example, a model never seeing PP-modified subjects must still generalize subject-verb agreement by observing agreement in other NP configurations.
- **Core assumption**: The model has learned abstract syntactic rules (like subject-verb agreement) that can be applied to novel constructions, rather than memorizing specific sentence patterns.
- **Evidence anchors**:
  - [abstract] "our results show that while transformers are better qua language models (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures"
  - [section 3.2.1] "A model that has not seen PP-modified subjects could still make the correct judgments by forming the following generalizations: verbs agree with the head noun of the subject, and NPs can be modified by PPs"
  - [corpus] "filtered corpora that target a wide range of linguistic phenomena" - Weak evidence, the corpus filtering method is described but no direct evidence of model learning from indirect evidence is provided in the corpus description itself.
- **Break condition**: If the model has only memorized specific constructions without forming abstract syntactic rules, it would fail to generalize to filtered constructions, resulting in low accuracy on the BLiMP benchmarks.

### Mechanism 2
- **Claim**: Perplexity and linguistic generalization ability are dissociated in language models.
- **Mechanism**: While Transformers achieve lower perplexity than LSTMs, this does not translate to better performance on linguistic generalization tasks. This suggests that the language modeling objective (minimizing perplexity) does not directly optimize for the ability to generalize linguistic rules.
- **Core assumption**: Lower perplexity indicates better memorization of training data patterns, but not necessarily better abstract linguistic understanding.
- **Evidence anchors**:
  - [abstract] "while transformers are better qua language models (as measured by perplexity), their linguistic generalization abilities are not better than that of the LSTMs"
  - [section 5.1] "We found that Transformers uniformly achieve lower perplexities on the test corpus than the LSTMs for all training corpora"
  - [corpus] No direct evidence about perplexity-linguistics dissociation in the corpus description.
- **Break condition**: If perplexity was directly correlated with linguistic generalization ability, Transformers would show superior performance on both metrics, contradicting the observed dissociation.

### Mechanism 3
- **Claim**: LSTMs are more robust to corpus filtering than Transformers on an item-level.
- **Mechanism**: While both architectures show similar average accuracy deltas, LSTMs maintain higher Pearson correlations between P∆ scores before and after filtering. This indicates that LSTMs are less impacted by the removal of specific constructions on individual items.
- **Core assumption**: The robustness of grammaticality judgments to corpus perturbations is a distinct property from average accuracy, and LSTMs exhibit this property more strongly than Transformers.
- **Evidence anchors**:
  - [section 5.4] "the LSTM correlations are systematically larger than those of the Transformer. This shows that LSTMs are less impacted by filtering on an item-level than Transformers"
  - [section 5.3] "for the cases where the absolute value of the deltas was appreciably larger than zero, it is not the case that one architecture is uniformly better than the other"
  - [corpus] No direct evidence about item-level robustness in the corpus description.
- **Break condition**: If Transformers were equally or more robust to corpus filtering on an item-level, their Pearson correlations would be equal to or higher than those of LSTMs, contradicting the observed difference.

## Foundational Learning

- **Concept**: Linguistic generalization from indirect evidence
  - **Why needed here**: Understanding how models can infer grammatical rules without direct exposure to target constructions is central to interpreting the FiCT methodology and results.
  - **Quick check question**: If a model never sees PP-modified subjects during training, how can it still correctly judge subject-verb agreement in such constructions?

- **Concept**: Perplexity as a language modeling metric
  - **Why needed here**: Perplexity is used to measure language model quality in this study, but the results show it's dissociated from linguistic generalization ability, requiring understanding of what perplexity actually measures.
  - **Quick check question**: What does a lower perplexity score indicate about a model's knowledge of its training corpus?

- **Concept**: Targeted syntactic evaluations and minimal pairs
  - **Why needed here**: The BLiMP benchmark uses minimal pairs to test grammaticality judgments, which is the primary evaluation method for linguistic generalization in this study.
  - **Quick check question**: How does the "full-sentence" method in BLiMP determine whether a model correctly judges grammaticality?

## Architecture Onboarding

- **Component map**: Filtered corpus -> Train LSTM and Transformer -> Evaluate on BLiMP using accuracy, accuracy delta, probability delta -> Compare architectures and filtered corpora
- **Critical path**: 1) Apply filters to remove specific linguistic constructions from training corpus 2) Train LSTM and Transformer models on both filtered and full corpora 3) Evaluate models on BLiMP using accuracy, accuracy delta, and probability delta metrics 4) Compare performance between architectures and across different filtered corpora
- **Design tradeoffs**: LSTMs vs Transformers - LSTMs show lower perplexity but equal linguistic generalization ability, while Transformers achieve better perplexity but are less robust to corpus filtering on an item-level. The tradeoff appears to be between raw language modeling performance and robustness of linguistic generalization.
- **Failure signatures**: 
  - If accuracy delta is large and negative for a filtered corpus, the model failed to generalize from indirect evidence
  - If Pearson correlation between P∆ scores drops significantly after filtering, the model's item-level judgments became unreliable
  - If perplexity improves but BLiMP accuracy doesn't, the model is memorizing rather than generalizing
- **First 3 experiments**:
  1. Train an LSTM and Transformer on the full corpus, verify they achieve different perplexities but similar BLiMP accuracy
  2. Apply the AGR-PP-MOD filter to create a filtered corpus, train models, and measure accuracy delta on PP-modified subject benchmarks
  3. Compare P∆ scores before and after filtering for the NPI-ONLY filter to assess item-level robustness differences between architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Transformers achieve lower perplexity on the test corpus while maintaining comparable linguistic generalization abilities to LSTMs?
- Basis in paper: [explicit] The paper observes that Transformers uniformly achieve lower perplexities on the test corpus than LSTMs for all training corpora, despite similar linguistic generalization performance.
- Why unresolved: The paper suggests this is due to Transformers being better at memorizing the structure of their training data, but the specific mechanisms remain unclear.
- What evidence would resolve it: Further analysis of Transformer and LSTM architectures, focusing on their differences in handling frequency, long-distance dependencies, and low-frequency constructions.

### Open Question 2
- Question: What explains the differences in robustness to corpus filtering between LSTMs and Transformers, as observed in the correlation of P∆ scores?
- Basis in paper: [explicit] The paper notes that LSTMs are less impacted by filtering on an item-level than Transformers, as evidenced by higher Pearson correlations between P∆ scores before and after filtering.
- Why unresolved: The paper does not provide a detailed explanation for this architectural difference in robustness.
- What evidence would resolve it: Detailed analysis of the internal representations and decision-making processes of LSTMs and Transformers, particularly in how they handle grammaticality judgments.

### Open Question 3
- Question: Can the Filtered Corpus Training (FiCT) methodology be extended to models of different sizes and pretraining corpora to generalize the findings?
- Basis in paper: [explicit] The paper suggests future work will extend this approach to models of different sizes and pretraining corpora.
- Why unresolved: The current study only examines LSTMs and Transformers of comparable size trained on a single corpus.
- What evidence would resolve it: Application of the FiCT methodology to a wider range of model architectures, sizes, and pretraining datasets, followed by comparison of linguistic generalization abilities.

## Limitations

- The filter implementations are not fully specified in the paper, requiring reconstruction from source code
- The study only examines two specific architectures (LSTM and Transformer) of similar size
- The practical significance of item-level robustness differences between architectures is not explored

## Confidence

- **High**: Language models can maintain non-trivial performance on linguistic benchmarks after filtering specific constructions
- **Medium**: The dissociation between perplexity and linguistic generalization ability is robust across architectures
- **Medium**: LSTMs show greater item-level robustness to corpus filtering than Transformers

## Next Checks

1. **Filter Fidelity Check**: Implement a manual verification protocol where 100 random sentences are sampled from each filtered corpus to confirm that target constructions are completely absent. This addresses the uncertainty about whether filters are correctly removing all instances of the targeted linguistic phenomena.

2. **Cross-Architectural Generalization Test**: Train an additional architecture (e.g., RoBERTa or GPT-style model) on the same filtered corpora and evaluate on BLiMP. This would test whether the observed generalization patterns are architecture-specific or represent a more general property of language modeling.

3. **Intervention Study on Model Behavior**: Use attention visualization or probing classifiers to directly examine whether models are relying on the hypothesized indirect evidence mechanisms (e.g., agreement patterns in other NP configurations) when making grammaticality judgments on filtered constructions.