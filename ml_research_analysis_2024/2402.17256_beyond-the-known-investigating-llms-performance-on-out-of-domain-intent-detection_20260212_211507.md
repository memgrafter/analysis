---
ver: rpa2
title: 'Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection'
arxiv_id: '2402.17256'
source_url: https://arxiv.org/abs/2402.17256
tags:
- chatgpt
- intent
- detection
- intents
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) like ChatGPT
  for out-of-domain (OOD) intent detection in task-oriented dialogue systems. It proposes
  two frameworks, ZSD-LLM for zero-shot and FSD-LLM for few-shot detection, using
  prompt engineering.
---

# Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection

## Quick Facts
- arXiv ID: 2402.17256
- Source URL: https://arxiv.org/abs/2402.17256
- Reference count: 0
- Primary result: ChatGPT shows strong zero-shot and few-shot intent detection but underperforms fine-tuned models as intent count increases

## Executive Summary
This paper evaluates large language models (LLMs) like ChatGPT for out-of-domain (OOD) intent detection in task-oriented dialogue systems. The authors propose two frameworks, ZSD-LLM for zero-shot and FSD-LLM for few-shot detection, using prompt engineering approaches. Experiments on CLINC and Banking datasets reveal that while LLMs demonstrate strong zero-shot and few-shot capabilities, their performance significantly degrades compared to fine-tuned models, especially as the number of intents increases. The study identifies key challenges including conflicts between general and domain-specific knowledge, difficulty in knowledge transfer, and sensitivity to input length.

## Method Summary
The paper proposes two LLM-based frameworks for OOD intent detection: ZSD-LLM (zero-shot) and FSD-LLM (few-shot). Both frameworks use prompt engineering to instruct ChatGPT to classify user queries into predefined intents or mark them as OOD. The ZSD-LLM framework operates without prior knowledge of in-distribution (IND) intents, while FSD-LLM provides demonstration examples. The authors compare these approaches against fine-tuned discriminative models (SCL, KNN-CL, UniNL) on CLINC and Banking datasets under various settings including different numbers of IND intents and few-shot scenarios.

## Key Results
- ChatGPT achieves strong zero-shot performance without IND intent priors, demonstrating powerful NLU capabilities
- FSD-LLM with demonstrations shows better performance than ZSD-LLM but still underperforms fine-tuned models
- LLM performance significantly degrades as the number of intents increases, particularly for OOD detection
- ChatGPT struggles with instruction understanding when prompts become lengthy due to many intents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit strong zero-shot and few-shot intent detection capability due to their broad pretraining on diverse text corpora.
- Mechanism: The LLMs' general language understanding allows them to perform intent detection without task-specific fine-tuning, relying on prompt-based instruction following.
- Core assumption: The model's internal knowledge covers sufficient domain-agnostic patterns to generalize to unseen intent tasks.
- Evidence anchors:
  - [abstract] "We find that LLMs exhibit strong zero-shot and few-shot capabilities"
  - [section] "ChatGPT can achieve good zero-shot performance without providing any IND intent priors, demonstrating his powerful NLU capabilities"
  - [corpus] Weak corpus evidence: related papers focus on OOD detection but not on zero-shot performance from general pretraining.
- Break condition: If domain-specific knowledge gaps are too large, or if prompt instructions are too complex for the model to parse correctly.

### Mechanism 2
- Claim: Few-shot performance improves with more demonstrations, but excessive examples introduce noise that harms OOD detection.
- Mechanism: Demonstrations provide prior knowledge about intent labels, but too many examples cause the model to overfit to IND patterns, reducing its ability to detect OOD samples.
- Core assumption: The model can extract useful features from examples but may also learn spurious correlations from too many samples.
- Evidence anchors:
  - [abstract] "ChatGPT performs significantly worse than baselines with a large number of IND intents"
  - [section] "Too many demonstrations may introduce noise into OOD detection"
  - [corpus] Weak corpus evidence: no direct evidence on the trade-off between demonstration quantity and OOD performance.
- Break condition: When the number of examples exceeds the model's context window or when IND and OOD distributions are too dissimilar.

### Mechanism 3
- Claim: LLMs are sensitive to instruction length and prompt complexity, leading to task failure when prompts become too long.
- Mechanism: As the number of intents increases, the prompt length grows, exceeding the model's processing capacity and causing it to overlook key information.
- Core assumption: The model's attention mechanism cannot effectively parse very long instructions, especially when distinguishing between many intent labels.
- Evidence anchors:
  - [abstract] "Particularly when the increase in intents leads to longer instructions, ChatGPT may overlook key information in the prompts, resulting in task failure"
  - [section] "When the prompt becomes longer, it is easy to exceed the processing capacity of LLMs, leading to erroneous outputs"
  - [corpus] Weak corpus evidence: no direct evidence on instruction length sensitivity in OOD detection tasks.
- Break condition: When the number of intents exceeds a threshold where the prompt length becomes unmanageable for the model.

## Foundational Learning

- Concept: Prompt engineering and template design
  - Why needed here: The effectiveness of LLMs for OOD detection heavily depends on how prompts are structured to convey task instructions and prior knowledge.
  - Quick check question: Can you design a prompt that clearly instructs the model to classify intents and return "unknown" for OOD samples?

- Concept: Zero-shot vs few-shot learning paradigms
  - Why needed here: Understanding the difference between providing no examples (zero-shot) and a few labeled examples (few-shot) is crucial for evaluating LLM performance.
  - Quick check question: What are the key differences in how an LLM processes zero-shot vs few-shot prompts for intent detection?

- Concept: Out-of-domain detection metrics and evaluation
  - Why needed here: Properly measuring the performance of OOD detection requires understanding metrics like recall, F1-score, and accuracy for both IND and OOD samples.
  - Quick check question: How would you calculate the OOD recall given the number of true OOD samples and the number of OOD samples correctly identified?

## Architecture Onboarding

- Component map: Prompt generation -> LLM inference -> Intent classification -> OOD detection
- Critical path: Prompt construction -> LLM response parsing -> Intent label extraction
- Design tradeoffs: Trade-off between prompt length and model performance; trade-off between demonstration quantity and OOD detection accuracy
- Failure signatures: High misclassification of OOD as IND; failure to output according to instructions; inconsistent performance across different intent splits
- First 3 experiments:
  1. Test zero-shot performance on a small set of intents to establish baseline capability
  2. Gradually increase the number of intents in the prompt to observe performance degradation
  3. Vary the number of demonstration examples to find the optimal balance between IND accuracy and OOD detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively inject domain-specific knowledge into LLMs to improve OOD detection performance?
- Basis in paper: [explicit] The paper identifies "injecting domain knowledge" as a future improvement direction and discusses the challenges of conflicts between general knowledge and domain-specific knowledge in LLMs.
- Why unresolved: The paper suggests that simply providing demonstrations or examples may not be sufficient due to the potential for noise and the influence of factors like the number of intents and demonstrations. The effectiveness of different methods for injecting domain knowledge remains unclear.
- What evidence would resolve it: Experiments comparing the performance of various techniques for injecting domain knowledge (e.g., fine-tuning, prompt engineering, retrieval-augmented generation) on OOD detection tasks would provide insights into the most effective approaches.

### Open Question 2
- Question: How can we improve knowledge transfer from in-distribution (IND) to out-of-distribution (OOD) data in LLMs for OOD detection?
- Basis in paper: [explicit] The paper highlights the difficulty of knowledge transfer from IND to OOD as a challenge for LLMs and suggests that learning transfer knowledge from IND's prior knowledge to OOD detection is a potential research direction.
- Why unresolved: The paper notes that FSD-LLM, which incorporates IND examples, achieves limited improvements in OOD detection. The reasons for this limited improvement and potential solutions for better knowledge transfer are not fully explored.
- What evidence would resolve it: Studies investigating different strategies for knowledge transfer (e.g., meta-learning, contrastive learning, domain adaptation) and their impact on OOD detection performance would shed light on effective approaches.

### Open Question 3
- Question: How can we improve LLMs' understanding of long instructions for OOD detection tasks with a large number of intents?
- Basis in paper: [explicit] The paper identifies sensitivity to input length as a challenge for LLMs in OOD detection and suggests that understanding long instructions is a future improvement direction.
- Why unresolved: The paper mentions that when the prompt becomes longer due to a large number of intents, LLMs may exceed their processing capacity, leading to errors. However, the specific mechanisms behind this limitation and potential solutions are not discussed in detail.
- What evidence would resolve it: Research exploring techniques to improve LLMs' ability to handle long instructions (e.g., hierarchical prompting, instruction decomposition) and their impact on OOD detection performance with a large number of intents would provide valuable insights.

## Limitations
- Performance degrades significantly as the number of intents increases, limiting practical applicability
- Results are based on ChatGPT (GPT-3.5) and may not generalize to other LLM variants
- Specific prompt templates and optimal configurations are not fully detailed, affecting reproducibility
- Limited to two specific datasets (CLINC and Banking), potentially limiting generalizability

## Confidence
- High Confidence: Claims about LLMs showing strong zero-shot and few-shot capabilities on smaller intent sets, and performance degradation with increasing intent count
- Medium Confidence: Claims about mechanisms behind performance degradation (prompt length sensitivity, knowledge conflicts)
- Low Confidence: Claims about specific prompt engineering optimizations and exact thresholds for performance breakdown

## Next Checks
1. Test the ZSD-LLM and FSD-LLM frameworks with different LLM variants (GPT-4, Claude, Llama) to assess whether the observed performance patterns are consistent across models or specific to ChatGPT.

2. Systematically vary prompt templates and instruction formats to identify which components are most critical for OOD detection performance and where exactly instruction length becomes problematic.

3. Implement a hybrid system that combines LLM zero-shot/few-shot capabilities with task-specific fine-tuning to determine if this addresses the identified weaknesses while maintaining the benefits of LLM flexibility.