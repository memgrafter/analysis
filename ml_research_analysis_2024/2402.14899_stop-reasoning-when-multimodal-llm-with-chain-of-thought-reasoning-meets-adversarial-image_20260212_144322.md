---
ver: rpa2
title: Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial
  Image
arxiv_id: '2402.14899'
source_url: https://arxiv.org/abs/2402.14899
tags:
- answer
- attack
- rationale
- reasoning
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of Chain-of-Thought (CoT) reasoning
  on the adversarial robustness of multimodal large language models (MLLMs). The authors
  first generalize existing adversarial attack methods to CoT-based inference by attacking
  the rationale and answer components.
---

# Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image

## Quick Facts
- **arXiv ID**: 2402.14899
- **Source URL**: https://arxiv.org/abs/2402.14899
- **Reference count**: 20
- **Primary result**: CoT reasoning marginally improves adversarial robustness, but a novel "stop-reasoning attack" can effectively bypass this protection by forcing direct answers without reasoning.

## Executive Summary
This paper investigates how Chain-of-Thought (CoT) reasoning affects adversarial robustness in multimodal large language models (MLLMs). The authors first generalize existing adversarial attacks to CoT-based inference by targeting both the rationale and answer components. While CoT provides only marginal robustness against these baseline attacks, the researchers develop a novel "stop-reasoning attack" that forces models to skip the reasoning process entirely and output answers directly. Experiments across three MLLMs (MiniGPT4, OpenFlamingo, LLaVA) and two datasets (A-OKVQA, ScienceQA) demonstrate that this attack is highly effective, significantly outperforming baseline methods and achieving accuracy close to models without CoT reasoning.

## Method Summary
The authors evaluate three adversarial attack methods on MLLMs with and without CoT reasoning. The answer attack targets only the final answer choice using PGD optimization, while the rationale attack jointly targets both rationale and answer using KL divergence. The novel stop-reasoning attack aims to interrupt the reasoning process by aligning initial output tokens with the answer format, forcing direct answer generation. Experiments use a perturbation magnitude of 系=16 and 200 maximum iterations, measuring accuracy drops across three models and two VQA datasets.

## Key Results
- CoT reasoning provides only marginal robustness against existing answer and rationale attacks
- Stop-reasoning attack significantly outperforms baseline attacks, reducing CoT model accuracy to near non-CoT levels
- The attack successfully forces CoT models to bypass reasoning and output direct answers
- Rationale changes under adversarial attacks reveal model reasoning errors and misconceptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT reasoning introduces intermediate rationale steps that provide model interpretability and marginally improve adversarial robustness by requiring perturbations to affect both rationale and answer.
- Mechanism: CoT forces the model to generate multi-step reasoning, so adversarial attacks must manipulate both the reasoning steps (rationale) and the final answer. Perturbing only the answer is insufficient because the model can still rely on the rationale to infer the correct answer.
- Core assumption: The rationale and answer are coupled such that altering one affects the other, and rationale changes expose the reasoning flaws caused by adversarial images.
- Evidence anchors:
  - [abstract] "CoT marginally improves adversarial robustness against these existing attacks."
  - [section 4.2.1] "CoT brings a marginal robustness boost against the two existing attacks."
  - [corpus] "Average neighbor FMR=0.408, average citations=0.0" (weak evidence from related work)

### Mechanism 2
- Claim: Stop-reasoning attack exploits the fact that MLLMs with CoT can be forced to skip the reasoning process entirely, bypassing the robustness benefit of CoT.
- Mechanism: By crafting adversarial perturbations that align the model's initial tokens with a predefined answer format, the model outputs the answer directly without generating rationale, even if prompted for CoT. This eliminates the intermediate reasoning that provided robustness.
- Core assumption: The model's output format can be controlled to bypass the CoT process, and the rationale is not strictly required if the answer format is enforced.
- Evidence anchors:
  - [abstract] "stop-reasoning attack that aims to interrupt the reasoning process and force the model to directly answer the question, bypassing the CoT."
  - [section 3.4] "By increasing the loss, MLLMs directly output the answer by aligning the initial tokens with the specified answer format and alter the answer into a wrong one."
  - [corpus] No direct evidence; assumption based on experimental results.

### Mechanism 3
- Claim: Rationale changes under adversarial attacks reveal the model's reasoning errors, providing interpretability for why incorrect answers are generated.
- Mechanism: When adversarial images are presented, CoT-generated rationales often change to reflect misconceptions (e.g., misidentifying objects or attributes), exposing the flawed reasoning path that leads to wrong answers.
- Core assumption: The rationale is an honest reflection of the model's reasoning process and changes in a way that can be interpreted by humans.
- Evidence anchors:
  - [abstract] "CoT reasoning generates intermediate reasoning steps... This approach not only improves models' inference power but also introduces explainability."
  - [section 4.3] "The intermediate reasoning steps (rationale part) generated by the CoT reasoning process provide insights and potentially reveal the reasoning process of the MLLMs."
  - [corpus] No direct evidence; based on qualitative examples in the paper.

## Foundational Learning

- Concept: Adversarial attacks on neural networks
  - Why needed here: The paper builds on standard adversarial attack methods (e.g., FGSM, PGD) and extends them to multimodal models with CoT reasoning.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Chain-of-Thought (CoT) reasoning in large language models
  - Why needed here: CoT is central to the paper's investigation of how intermediate reasoning steps affect adversarial robustness.
  - Quick check question: How does CoT improve model performance on complex reasoning tasks?

- Concept: Multimodal large language models (MLLMs)
  - Why needed here: The paper evaluates adversarial robustness specifically on MLLMs that process both text and images.
  - Quick check question: What are the key architectural differences between MLLMs and single-modality LLMs?

## Architecture Onboarding

- Component map: Image + Question + Answer Choices -> MLLM with CoT capability -> Rationale + Answer -> Adversarial attack module -> Perturbed image -> Evaluation accuracy

- Critical path:
  1. Prepare clean image and question
  2. Generate clean prediction (with/without CoT)
  3. Apply adversarial perturbation using attack loss
  4. Generate adversarial prediction
  5. Evaluate accuracy drop

- Design tradeoffs:
  - Perturbation magnitude (系) vs. imperceptibility
  - Attack success rate vs. attack generalizability across models
  - CoT robustness vs. computational overhead of generating rationale

- Failure signatures:
  - High accuracy on adversarial images (attack failed)
  - Rationale changes without answer changes (attack partially succeeded)
  - Stop-reasoning attack succeeds but answer attack fails (CoT bypassed)

- First 3 experiments:
  1. Compare accuracy of MLLMs with/without CoT under answer attack
  2. Apply rationale attack and analyze changes in rationale text
  3. Implement stop-reasoning attack and measure success rate on CoT models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of stop-reasoning attack generalize to other types of reasoning tasks beyond visual question answering, such as mathematical reasoning or logical inference?
- Basis in paper: [inferred] The paper focuses on VQA tasks and demonstrates the effectiveness of stop-reasoning attack on MLLMs with CoT. However, it does not explore the attack's effectiveness on other reasoning domains.
- Why unresolved: The paper's experiments are limited to VQA datasets (A-OKVQA and ScienceQA). It remains unclear whether the attack's success is specific to visual reasoning or if it generalizes to other reasoning tasks that require CoT.
- What evidence would resolve it: Testing stop-reasoning attack on MLLMs with CoT across diverse reasoning tasks, such as mathematical problem-solving, logical reasoning, and commonsense reasoning, would provide insights into its generalizability.

### Open Question 2
- Question: How does the performance of stop-reasoning attack vary with different CoT prompting strategies, such as different levels of granularity or explicitness in the rationale?
- Basis in paper: [explicit] The paper uses a specific CoT prompt requiring "at least three sentences" in the rationale. However, it does not explore the impact of varying the prompt's structure or requirements on the attack's effectiveness.
- Why unresolved: The paper's experiments use a fixed CoT prompt. It is unclear whether the attack's success is dependent on the specific prompt used or if it remains effective with different CoT prompting strategies.
- What evidence would resolve it: Conducting experiments with different CoT prompts, varying in granularity, explicitness, and structure, would reveal the attack's sensitivity to prompting strategies and its potential robustness.

### Open Question 3
- Question: Can MLLMs be trained to be more robust against stop-reasoning attacks, and if so, what training strategies would be most effective?
- Basis in paper: [explicit] The paper demonstrates that stop-reasoning attack can significantly reduce the adversarial robustness of MLLMs with CoT. However, it does not explore potential defenses or training strategies to mitigate this vulnerability.
- Why unresolved: The paper focuses on attacking MLLMs with CoT but does not investigate methods to improve their robustness against such attacks. It remains an open question whether MLLMs can be made more resilient to stop-reasoning attacks.
- What evidence would resolve it: Experimenting with different training techniques, such as adversarial training with stop-reasoning attacks or incorporating robustness objectives into the training process, would provide insights into potential defenses against this attack.

## Limitations

- The effectiveness of stop-reasoning attack may be overstated due to the relatively high perturbation magnitude (系=16) used in experiments
- Results are primarily demonstrated on multiple-choice question answering tasks, limiting generalizability to open-ended reasoning tasks
- The study focuses on three specific MLLM architectures, and results may not extend to other CoT-capable models or different reasoning paradigms

## Confidence

- **High Confidence**: The marginal robustness benefit of CoT against existing adversarial attacks (answer and rationale attacks) is well-supported by experimental evidence across multiple models and datasets.
- **Medium Confidence**: The effectiveness of the stop-reasoning attack in bypassing CoT robustness is demonstrated, but relies on specific implementation details and attack parameters that may not generalize.
- **Low Confidence**: The interpretability claims about rationale changes revealing reasoning errors are primarily qualitative and lack systematic analysis of what these changes actually indicate about model reasoning.

## Next Checks

1. Test the stop-reasoning attack on open-ended reasoning tasks where models must generate free-form answers rather than select from multiple choices, to verify if the attack generalizes beyond classification settings.

2. Conduct ablation studies varying the perturbation magnitude (系) below 16 to assess whether the reported attack effectiveness persists at more realistic imperceptibility thresholds.

3. Implement the same attack methodology on a different class of CoT-capable models (such as those using different architectural approaches to multimodal reasoning) to test the generalizability of the stop-reasoning attack mechanism.