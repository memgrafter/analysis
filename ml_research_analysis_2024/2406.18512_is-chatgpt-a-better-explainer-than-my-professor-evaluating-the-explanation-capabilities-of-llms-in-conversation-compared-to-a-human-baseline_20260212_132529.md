---
ver: rpa2
title: '"Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation
  Capabilities of LLMs in Conversation Compared to a Human Baseline'
arxiv_id: '2406.18512'
source_url: https://arxiv.org/abs/2406.18512
tags:
- explainer
- explanation
- responses
- conversation
- explainee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compares three methods for generating explainer responses
  in conversational science communication: human expert responses (baseline), GPT-4
  standard generation, and GPT-4 guided by annotated explanatory acts. Using a dataset
  of staged explanation dialogues, human annotators evaluated responses across 8 dimensions
  and ranked them.'
---

# "Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline

## Quick Facts
- arXiv ID: 2406.18512
- Source URL: https://arxiv.org/abs/2406.18512
- Authors: Grace Li; Milad Alshomary; Smaranda Muresan
- Reference count: 3
- Primary result: GPT-4 standard responses preferred 49% of time over human baseline (18%) and EA-guided approach (33%)

## Executive Summary
This study evaluates whether large language models can serve as effective conversational explainers in science communication by comparing GPT-4 responses against human expert explanations. Using a dataset of staged explanation dialogues from WIRED's "5 Levels" series, researchers tested three generation strategies: human baseline, GPT-4 standard, and GPT-4 guided by annotated explanatory acts. Human annotators evaluated responses across eight dimensions and ranked them. The results show GPT-4 standard generation is preferred for its conciseness, while EA-guided responses perform better when they include engaging questions. However, the low inter-annotator agreement indicates significant variability in personal preferences for explanation styles.

## Method Summary
The study used the Booshehri et al. (2023) annotated 5-Levels dataset containing 20 explanatory acts to generate responses for 11 scientific topics. Researchers created three response strategies: human expert baseline, GPT-4 standard generation, and GPT-4 guided by specific explanatory acts. Human annotators evaluated responses using Label Studio across eight dimensions (coherence, conciseness, conversationality, acknowledgment, appropriateness, depth, guidance, engagement) and provided pairwise rankings. The study calculated inter-annotator agreement scores to assess consistency in evaluation.

## Key Results
- GPT-4 standard responses were preferred 49% of the time over human baseline (18%) and EA-guided approaches (33%)
- GPT-4 standard responses averaged 10 words shorter than EA-guided approaches
- EA-guided responses performed better when they included engaging questions or thought-provoking elements
- Inter-annotator agreement was low (0.167), indicating high variability in personal preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 standard generation outperforms human expert explanations in conversational science communication due to conciseness.
- Mechanism: The model generates shorter, more focused responses that avoid over-explaining while maintaining clarity, reducing cognitive load on explainees.
- Core assumption: Conciseness is a primary driver of explanation quality in science communication contexts.
- Evidence anchors:
  - [abstract] "The main advantage of GPT-4 standard was conciseness, with responses averaging 10 words shorter than the EA-guided approach."
  - [section] "The most common rationale for this ranking was because the S2 strategy was 'a little too long,' 'overly wordy,' 'long winded' and 'over-explains in several areas and is longer than necessary' according to annotators."
  - [corpus] Weak evidence - no corpus data directly supports this mechanism.
- Break condition: When topics require extensive background context, conciseness may compromise completeness and understanding.

### Mechanism 2
- Claim: Guided GPT-4 with explanatory acts performs better when it includes engaging questions or thought-provoking elements.
- Mechanism: Explicit prompting for specific explanatory acts like concept completion questions or test understanding questions creates more interactive dialogue that actively engages explainees.
- Core assumption: Explanation quality is enhanced when models are explicitly instructed to use specific conversational strategies.
- Evidence anchors:
  - [abstract] "EA-guided responses performed better when they included engaging questions or thought-provoking elements."
  - [section] "The rational that many annotators wrote included responses such as 'actively guides the conversation,' 'engaged the explainee with a followup question,' and 'asks a thought provoking question prompting deeper conversation.'"
  - [corpus] Weak evidence - no corpus data directly supports this mechanism.
- Break condition: When explanatory acts are poorly selected or mismatched to the conversation context, they may feel forced or unnatural.

### Mechanism 3
- Claim: Low inter-annotator agreement (0.167) indicates high variability in personal preferences for explanation styles, suggesting need for personalized explanation systems.
- Mechanism: Different explainees have varying preferences for explanation style (concise vs. detailed, formal vs. conversational), and no single approach dominates across all contexts.
- Core assumption: Personal preference variability is significant enough to warrant adaptive explanation systems.
- Evidence anchors:
  - [abstract] "The low inter-annotator agreement (0.167) indicates high variability in personal preferences for explanation styles."
  - [section] "Demonstrates how this annotation task is highly variable due to each annotator's own specific preferences for engaging in explanation conversations."
  - [corpus] Weak evidence - no corpus data directly supports this mechanism.
- Break condition: When explanation quality can be objectively measured through learning outcomes rather than subjective preferences.

## Foundational Learning

- Concept: Explanatory acts framework
  - Why needed here: The study uses Booshehri et al.'s inventory of 20 explanatory acts to understand and guide explanation strategies in conversational settings.
  - Quick check question: What are the two main categories of explanatory acts in the framework, and how do they differ in purpose?

- Concept: Conversational explanation dynamics
  - Why needed here: The research focuses on how explanations adapt to explainee background and proficiency, requiring understanding of interactive dialogue patterns.
  - Quick check question: Why might an explanation strategy for a 5-year-old differ significantly from one for a college student, even for the same topic?

- Concept: Evaluation methodology for explanation quality
  - Why needed here: The study uses 8 dimensions and ranking systems to assess explanation quality, requiring understanding of evaluation metrics.
  - Quick check question: What are the three most critical dimensions for evaluating explanation quality in conversational settings according to the study?

## Architecture Onboarding

- Component map: Data processing pipeline -> GPT-4 generation modules (standard + EA-guided) -> Human evaluation interface
- Critical path: For each explainee utterance: construct context -> generate three responses -> present to annotators -> collect ratings/rankings -> calculate agreement scores
- Design tradeoffs: GPT-4 standard (faster, more concise) vs. GPT-4 with EAs (potentially more engaging but longer) represents tradeoff between efficiency and depth of engagement
- Failure signatures: Low inter-annotator agreement (0.167) indicates high subjectivity; GPT-4 standard consistently underperforming suggests model limitations with complex concepts
- First 3 experiments:
  1. Test GPT-4 standard generation for new scientific topics not in training data
  2. Evaluate specific explanatory acts (e.g., Analogy, Elaboration) for consistent engagement improvement
  3. Measure whether combining conciseness with selective engaging questions outperforms baseline approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of explanatory acts affect the quality and effectiveness of LLM-generated explanations?
- Basis in paper: [explicit] The paper mentions that GPT w/ EA responses were preferred when they included engaging questions or thought-provoking elements, but the study didn't systematically analyze which specific explanatory acts led to better performance.
- Why unresolved: The current study only evaluated overall performance of GPT with and without explanatory acts, but didn't analyze which specific acts or combinations of acts were most effective in different contexts.
- What evidence would resolve it: A controlled study testing individual explanatory acts and their combinations, measuring which specific types consistently improve explanation quality across different topics and audience levels.

### Open Question 2
- Question: What is the optimal balance between conciseness and engagement in LLM-generated explanations?
- Basis in paper: [explicit] The study found GPT Standard responses were preferred due to conciseness (10 words shorter on average), but GPT w/ EA performed better when including engaging questions, suggesting a tension between brevity and engagement.
- Why unresolved: The study didn't determine if there's an optimal middle ground between overly concise explanations and those that maintain engagement through questions or elaboration.
- What evidence would resolve it: Experiments varying the length and engagement level of responses systematically to identify the sweet spot that maximizes both comprehension and engagement.

### Open Question 3
- Question: How can LLMs be designed to adapt explanations to individual explainee preferences and backgrounds?
- Basis in paper: [inferred] The low inter-annotator agreement (0.167) suggests high variability in personal preferences for explanation styles, indicating a need for personalized approaches.
- Why unresolved: The current study used a one-size-fits-all approach without accounting for individual differences in learning styles, background knowledge, or preferences for explanation depth and style.
- What evidence would resolve it: Research developing adaptive explanation systems that can detect and respond to individual explainee characteristics, measuring improvements in comprehension and engagement.

## Limitations

- Reliance on subjective human evaluation without objective learning outcome measures
- Dataset limited to 11 topics from WIRED's "5 Levels" series, limiting generalizability
- Low inter-annotator agreement (0.167) suggests high variability may overshadow genuine quality differences

## Confidence

- **High confidence**: GPT-4 standard responses are preferred over human baseline (49% vs 18%) and EA-guided approaches due to conciseness
- **Medium confidence**: EA-guided responses perform better when they include engaging questions, though this requires specific prompt engineering
- **Low confidence**: The practical significance of preference differences given the high variability in annotator agreement

## Next Checks

1. **Objective Learning Outcomes**: Conduct a controlled study measuring actual knowledge retention and comprehension after receiving explanations from each approach, rather than relying solely on conversational preference ratings.

2. **Demographic Preference Analysis**: Analyze whether specific explainee demographics (age, education level, prior knowledge) show consistent preference patterns for different explanation styles, which would justify personalized explanation systems.

3. **Real-world Deployment Test**: Deploy the best-performing explanation approach in a live conversational setting with actual science explainees to assess whether controlled study preferences hold in natural interaction contexts.