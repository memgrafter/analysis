---
ver: rpa2
title: 'Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic
  Music and Audio Generation'
arxiv_id: '2408.01180'
source_url: https://arxiv.org/abs/2408.01180
tags:
- music
- tokens
- compound
- token
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Nested Music Transformer (NMT), an architecture
  designed to decode compound tokens in symbolic music and audio generation. The NMT
  consists of two transformers: a main decoder that models a sequence of compound
  tokens and a sub-decoder that models sub-tokens of each compound token.'
---

# Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation

## Quick Facts
- arXiv ID: 2408.01180
- Source URL: https://arxiv.org/abs/2408.01180
- Authors: Jiwoo Ryu; Hao-Wen Dong; Jongmin Jung; Dasaem Jeong
- Reference count: 0
- Primary result: NMT improves perplexity on symbolic music and discrete audio token generation through compound token decoding

## Executive Summary
This paper introduces the Nested Music Transformer (NMT), an architecture designed to efficiently decode compound tokens in symbolic music and audio generation. The NMT employs two transformers: a main decoder that processes compound token sequences and a sub-decoder that models individual sub-tokens within each compound token. The key innovation is the use of cross-attention architectures within the sub-decoder, including an intra-token decoder and an Embedding Enricher, which help capture interdependencies between musical features while maintaining computational efficiency.

The NMT is evaluated on multiple symbolic music datasets (Pop1k7, Pop909, SOD, LMD clean) and the MAESTRO dataset for audio tokens. Experiments demonstrate that the NMT outperforms baseline approaches, particularly when using pitch-first compound token grouping and the Embedding Enricher component. The architecture shows particular promise for reducing sequence length while maintaining or improving generation quality, though further validation is needed for its effectiveness on audio tokens specifically.

## Method Summary
The Nested Music Transformer addresses the challenge of decoding compound tokens by using a nested transformer architecture. The main decoder processes compound tokens as single units, while the sub-decoder sequentially decodes individual sub-tokens within each compound token using cross-attention mechanisms. The sub-decoder uses the main decoder's output as query and concatenates embeddings of previously sampled sub-tokens as key/value. The Embedding Enricher enriches shallow sub-token embeddings with contextual information from recent compound tokens through additional cross-attention. The model is trained with AdamW optimizer, cosine learning rate schedule with warm-up, and dropout, using average negative log-likelihood as the primary metric for evaluation.

## Key Results
- NMT improves negative log-likelihood on all tested symbolic music datasets compared to baseline models
- Pitch-first compound token grouping (NB-PF) outperforms metric-first grouping (NB-MF) in predicting pitch features
- The Embedding Enricher component contributes to performance improvements, though its individual contribution requires further ablation analysis
- NMT shows promise for audio token generation, though optimal architecture for fully-sequential compound feature decoding remains an open question

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention in the sub-decoder better captures interdependencies between sub-tokens than self-attention or feed-forward architectures. The sub-decoder uses the output of the main decoder (hi) as the query and concatenates embeddings of previously sampled sub-tokens as the key/value. This allows each sub-token prediction to condition on both the broader context from the main decoder and the previously predicted sub-tokens within the same compound token. The main decoder's hidden states contain sufficient contextual information to inform sub-token predictions, and using them as keys allows direct gradient flow for more stable learning.

### Mechanism 2
The Embedding Enricher improves sub-token prediction by enriching shallow sub-token embeddings with deeper contextual information from recent compound tokens. The Embedding Enricher performs cross-attention between the embedding of a sub-token and a context sequence derived from the outputs of the main decoder for the current and previous w compound tokens. This updated embedding is then used in the sub-decoder's key/value sequence. Since sub-token embeddings are too shallow to capture necessary context on their own, incorporating information from the main decoder's hidden states provides a richer representation.

### Mechanism 3
Pitch-first token grouping (NB-PF) is more effective than metric-first grouping (NB-MF) because it allows the main decoder to leverage note position information when predicting pitch. In NB-PF, the pitch sub-token is predicted first within each compound token, followed by duration and velocity. This ordering allows the main decoder to use information about the note's position (beat) from the previous compound token when predicting the current pitch. The position of a note (beat) is a strong predictor of its pitch, and predicting pitch first allows the model to condition on this information more effectively.

## Foundational Learning

- **Transformer architecture and attention mechanisms**: The NMT is built upon transformer layers for both the main decoder and sub-decoder. Understanding how self-attention and cross-attention work is crucial for grasping the model's design and behavior. Quick check: How does cross-attention differ from self-attention, and why is cross-attention preferred in the sub-decoder?

- **Tokenization and representation learning in music**: The NMT operates on compound tokens, which group multiple musical features into a single token. Understanding how music is represented as tokens and how different encoding schemes (REMI, CP, NB) work is essential for understanding the model's input and output. Quick check: What are the advantages and disadvantages of using compound tokens compared to flattened tokens in music generation?

- **Autoregressive modeling and sequence generation**: The NMT generates music by autoregressively predicting tokens one at a time. Understanding how autoregressive models work and how they handle long-term dependencies is important for understanding the model's generation process. Quick check: How does the NMT handle long-term dependencies in music, and what are the potential challenges in generating coherent musical pieces?

## Architecture Onboarding

- **Component map**: Token Embedding -> Main Decoder -> Sub-decoder (with or without Embedding Enricher) -> Music Generation

- **Critical path**: Token Embedding → Main Decoder → Sub-decoder (with or without Embedding Enricher) → Music Generation

- **Design tradeoffs**: Compound tokens reduce sequence length but introduce interdependencies between sub-tokens that need to be modeled. Cross-attention in the sub-decoder captures interdependencies but adds complexity compared to simpler architectures. The Embedding Enricher enriches sub-token embeddings but requires additional computation and memory.

- **Failure signatures**: Poor perplexity or generation quality could indicate issues with the token embedding, main decoder, or sub-decoder. High memory usage or slow training could indicate inefficiencies in the architecture or hyperparameter choices. Lack of coherence in generated music could indicate issues with handling long-term dependencies or capturing musical structure.

- **First 3 experiments**: 1) Compare the NMT with and without the Embedding Enricher on a small dataset to assess its impact on performance and efficiency. 2) Test different token orderings (pitch-first vs. metric-first) within the NB encoding to determine the optimal grouping strategy. 3) Evaluate the NMT's performance on a diverse set of music genres to assess its generalizability and robustness.

## Open Questions the Paper Calls Out

- **Integration of symbolic music semantics with audio tokens**: How can the semantic information of symbolic music be effectively integrated with discrete audio tokens? The paper notes that current models, including the NMT, do not fully exploit semantic information present in symbolic music when applied to audio tokens, suggesting this could be a valuable direction for future research.

- **Optimal architecture for audio token decoding**: What is the optimal architecture for decoding compound features in a fully-sequential manner for audio tokens? The paper mentions that while NMT shows promise, the optimal architecture for audio tokens is still being explored, particularly in comparison to other architectures like self-attention-based sub-decoders.

- **Impact of compound token grouping strategies**: How does the choice of compound token grouping strategy affect the performance of music generation models? The paper discusses the impact of different grouping strategies like pitch-first versus metric-first, noting that effectiveness can vary depending on the specific task and interdependencies between musical features.

## Limitations

- The superiority of cross-attention over self-attention in the sub-decoder is demonstrated only through comparison with a simpler baseline without thorough ablation studies.
- The Embedding Enricher component's individual contribution is not quantified through separate ablation analysis.
- The pitch-first vs metric-first token ordering advantage is observed but not rigorously explained with theoretical justification.
- Computational overhead of the nested architecture, particularly for the Embedding Enricher, is not thoroughly analyzed.
- Empirical validation is limited to perplexity metrics without qualitative analysis of generation quality or musical coherence.

## Confidence

- **High Confidence**: The general approach of using nested transformers for compound token decoding is technically sound and the experimental methodology is rigorous.
- **Medium Confidence**: The claim that cross-attention outperforms self-attention in the sub-decoder has reasonable theoretical support but limited empirical validation.
- **Low Confidence**: The specific claims about pitch-first token ordering being universally superior lack sufficient theoretical justification and cross-dataset validation.

## Next Checks

1. **Ablation study on Embedding Enricher**: Run experiments comparing the NMT with and without the Embedding Enricher across all datasets to quantify its individual contribution to performance gains and computational overhead.

2. **Cross-attention vs self-attention analysis**: Conduct detailed analysis of when and why cross-attention outperforms self-attention in the sub-decoder by testing on datasets with varying levels of sub-token interdependencies and measuring attention weight distributions.

3. **Musical coherence evaluation**: Implement qualitative evaluation of generated music samples from different token ordering schemes (pitch-first vs metric-first) to assess whether perplexity improvements translate to musically coherent outputs, including expert musician evaluations.