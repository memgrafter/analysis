---
ver: rpa2
title: Preserving Diversity in Supervised Fine-Tuning of Large Language Models
arxiv_id: '2408.16673'
source_url: https://arxiv.org/abs/2408.16673
tags:
- diversity
- distribution
- arxiv
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reduced output diversity in
  supervised fine-tuning (SFT) of large language models, which can hinder subsequent
  development stages requiring diverse sampling for better response exploration. The
  authors propose a game-theoretic framework that introduces an auxiliary variable
  to regulate the learning process, connecting to reverse KL minimization with entropy
  regularization.
---

# Preserving Diversity in Supervised Fine-Tuning of Large Language Models

## Quick Facts
- arXiv ID: 2408.16673
- Source URL: https://arxiv.org/abs/2408.16673
- Reference count: 40
- Primary result: Introduces GEM framework that preserves output diversity in SFT while maintaining downstream performance, improving test-time scaling by 5-8 points and reducing alignment tax by 83%

## Executive Summary
This paper addresses the critical problem of reduced output diversity in supervised fine-tuning (SFT) of large language models, which can hinder downstream development stages requiring diverse sampling for better response exploration. The authors propose a game-theoretic framework with entropy regularization that maintains output diversity while achieving comparable performance to standard cross-entropy training. Their proposed GEM algorithm demonstrates significant improvements in test-time scaling performance and knowledge retention across chat and code generation tasks, while requiring only half the sampling budget.

## Method Summary
The paper introduces a game-theoretic framework for SFT that addresses the diversity collapse problem inherent in standard cross-entropy loss. The framework introduces an auxiliary variable to regulate the learning process, which connects to reverse KL minimization with entropy regularization. The proposed GEM algorithm efficiently implements this framework through alternating optimization between the model parameters and the auxiliary variable. The key insight is that entropy regularization prevents the model from collapsing to deterministic outputs during training, preserving the ability to generate diverse responses at test time. The method requires tuning two hyperparameters (λ and β) that control the strength of entropy regularization and the auxiliary variable update, respectively.

## Key Results
- GEM improves test-time scaling performance by 5-8 points on chat and code generation tasks compared to standard CE loss
- Reduces required sampling budget by 50% while maintaining performance levels
- Reduces alignment tax by 83% through better retention of pre-training knowledge
- Maintains comparable downstream task performance to standard CE training while preserving output diversity

## Why This Works (Mechanism)
The proposed method works by addressing the fundamental limitation of cross-entropy loss in SFT: it encourages the model to assign high probability to the single reference token in the training data, leading to deterministic and less diverse outputs. By introducing entropy regularization through a game-theoretic framework with an auxiliary variable, the method maintains a balance between fitting the training data and preserving the model's ability to generate diverse responses. This approach effectively implements reverse KL minimization, which naturally encourages the model to cover the support of the target distribution rather than just focusing on high-probability modes. The entropy term prevents the model from becoming overconfident in its predictions during training, preserving diversity that can be exploited during test-time sampling.

## Foundational Learning

**Cross-Entropy Loss**: Standard loss function for classification tasks that maximizes the probability of the correct class; needed to understand why SFT leads to diversity collapse; quick check: compute CE loss for a simple classification example.

**KL Divergence**: Measures the difference between two probability distributions; needed to understand the theoretical connection to reverse KL minimization; quick check: calculate KL divergence between two simple distributions.

**Entropy Regularization**: Technique that adds entropy terms to loss functions to encourage exploration; needed to understand how diversity is preserved during training; quick check: verify that entropy term encourages uniform distributions.

**Game-Theoretic Frameworks**: Mathematical approach for modeling interactions between competing objectives; needed to understand how the auxiliary variable regulates the learning process; quick check: implement a simple two-player game optimization.

**Test-Time Scaling**: Phenomenon where model performance improves with increased sampling budget; needed to understand the evaluation metric for diversity preservation; quick check: measure performance improvement with increased sampling.

## Architecture Onboarding

**Component Map**: Training data -> Cross-entropy loss + Entropy regularization -> Model parameters (θ) <-> Auxiliary variable (α) -> Output distribution -> Performance metrics

**Critical Path**: Data loading → Loss computation (CE + entropy) → Parameter updates → Auxiliary variable updates → Model evaluation

**Design Tradeoffs**: The method trades some training stability for diversity preservation; higher entropy regularization may slow convergence but improves test-time performance; computational overhead is minimal but hyperparameter tuning is critical.

**Failure Signatures**: Model collapse to deterministic outputs; poor test-time scaling performance; overfitting to training data; sensitivity to hyperparameter choices (λ and β).

**First Experiments**:
1. Compare CE loss vs. GEM on a simple synthetic dataset with known diversity characteristics
2. Measure test-time scaling curves for both methods across different sampling budgets
3. Ablation study removing entropy regularization to confirm its importance

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The method's effectiveness depends heavily on hyperparameter choices (λ and β) that may not generalize across different model architectures
- Computational efficiency claims need validation across different model sizes and dataset complexities
- Evaluation focuses primarily on chat and code generation tasks, with unclear performance on other language modeling tasks
- The trade-off between diversity preservation and task-specific accuracy requires deeper investigation for practical deployment

## Confidence
- High confidence: The empirical improvements in test-time scaling performance (5-8 points) are well-supported by the experimental results presented
- Medium confidence: The computational efficiency claims and alignment tax reduction require additional validation across diverse model families and task types
- Medium confidence: The theoretical connection to reverse KL minimization is sound, but practical implications for model behavior need further exploration

## Next Checks
1. Test GEM across different model sizes (1B to 70B parameters) and architectures to verify computational efficiency claims scale appropriately
2. Evaluate performance on additional task types including summarization, translation, and reasoning tasks to assess generalizability beyond chat and code generation
3. Conduct ablation studies on hyperparameter sensitivity (λ and β) to establish robust default settings for different use cases