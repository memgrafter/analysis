---
ver: rpa2
title: 'Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection
  with Vision-Language Models'
arxiv_id: '2411.03888'
source_url: https://arxiv.org/abs/2411.03888
tags:
- speech
- hate
- cultural
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi3Hate, the first parallel multimodal
  and multilingual hate speech dataset annotated by a multicultural set of annotators.
  The dataset contains 300 parallel meme samples across five languages (English, German,
  Spanish, Hindi, and Mandarin) annotated by native speakers from the USA, Germany,
  Mexico, India, and China.
---

# Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models

## Quick Facts
- arXiv ID: 2411.03888
- Source URL: https://arxiv.org/abs/2411.03888
- Reference count: 21
- Primary result: Cultural background significantly influences multimodal hate speech annotations, with average pairwise agreement among countries at only 74%

## Executive Summary
This paper introduces Multi3Hate, the first parallel multimodal and multilingual hate speech dataset annotated by a multicultural set of annotators. The dataset contains 300 parallel meme samples across five languages (English, German, Spanish, Hindi, and Mandarin) annotated by native speakers from five countries (USA, Germany, Mexico, India, and China). The study finds that cultural background significantly influences hate speech annotations, with average pairwise agreement among countries at only 74% and the lowest agreement of 67% between the USA and India. Experiments with five large vision-language models in zero-shot settings reveal that these models align more closely with US annotations than with other cultures, even when prompts are presented in the dominant language of the target culture. This bias persists even when country information is added to prompts, indicating a strong cultural bias towards US perspectives in current vision-language models.

## Method Summary
The Multi3Hate dataset was created through web crawling of meme content, followed by caption filtering, translation, and multimodal recreation. Five native speakers from each of five countries (USA, Germany, Mexico, India, China) annotated 300 parallel meme samples across five languages. Zero-shot evaluation was conducted using five large vision-language models (GPT-4o, Gemini 1.5 Pro, Qwen2-VL 72B, LLaVA OneVision 73B, and InternVL2 76B) with multiple prompt variations including image-only and image+caption inputs. The evaluation measured cross-cultural agreement rates and model performance alignment with different cultural annotations.

## Key Results
- Average pairwise agreement among cultures is only 74%, significantly lower than within-culture agreement
- Models align more closely with US annotations than with other cultures, regardless of prompt language
- Adding country information to prompts does not improve model alignment with target culture annotations
- Zero-shot performance peaks for English prompts despite testing on non-English content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural background significantly influences multimodal hate speech annotations, causing lower agreement across cultures than within cultures.
- Mechanism: Annotators from different cultural contexts interpret the same meme differently due to varying social norms, historical context, and existing stereotypes. This results in inconsistent labeling when aggregating across countries.
- Core assumption: Cultural perceptions of hate speech are not universal; they vary based on national and linguistic context.
- Evidence anchors:
  - [abstract] "cultural background significantly influences hate speech annotations, with average pairwise agreement among countries at only 74%"
  - [section 4.2] "The average pairwise agreement among countries is only 74%, significantly lower than that of randomly selected annotator groups"
  - [corpus] Weak: Corpus contains no peer-reviewed evaluations of this cultural variability claim; relies on internal dataset statistics.
- Break condition: If annotations show no variance across cultures, or if pairwise agreement is high (e.g., >90%), the mechanism collapses.

### Mechanism 2
- Claim: Large VLMs exhibit a cultural bias toward US annotations, regardless of prompt language.
- Mechanism: Models are pre-trained on predominantly English/US-centric data, causing them to align more closely with US-based hate speech perceptions even when tested on non-English content or prompted in other languages.
- Core assumption: Pretraining data and cultural representation in training corpora are heavily skewed toward US norms.
- Evidence anchors:
  - [abstract] "these models align more closely with US annotations than with those from other cultures, even when prompts are presented in the dominant language of the target culture"
  - [section 5.3] "out of 50 combinations of models, languages, and input variations, 42 demonstrate the highest alignment with US labels"
  - [corpus] Weak: No cross-validation against other benchmarks; cultural bias claim is inferred from internal agreement scores only.
- Break condition: If models achieve highest accuracy on non-US cultures or if accuracy is uniform across cultures, the mechanism fails.

### Mechanism 3
- Claim: Adding country information to prompts does not improve model alignment with the target culture's annotations.
- Mechanism: The models either ignore the injected country information or interpret it through their US-centric lens, rendering the cultural cue ineffective.
- Core assumption: Model conditioning on cultural context is not learned during pretraining; embeddings do not dynamically adjust to injected cultural metadata.
- Evidence anchors:
  - [section 5.5] "Injecting country information generally decreases performance across target cultures... we conclude that adding country information does not positively impact performance"
  - [corpus] Weak: No ablation studies showing why models ignore metadata; mechanism is observational.
- Break condition: If adding country information increases accuracy on the target culture, the mechanism is invalid.

## Foundational Learning

- Concept: Multimodal hate speech detection
  - Why needed here: The task involves interpreting both image and text modalities; understanding multimodal embeddings and cross-modal alignment is critical.
  - Quick check question: How does a VLM fuse visual and textual features before classification?

- Concept: Cross-cultural dataset construction
  - Why needed here: Creating balanced, parallel datasets across languages and cultures requires understanding sampling bias, annotation reliability, and cultural representation.
  - Quick check question: What criteria ensure that parallel translations preserve the original semantic and cultural context?

- Concept: Statistical significance testing in ML evaluation
  - Why needed here: Claims about model bias and cultural alignment rely on comparing distributions; tests like Wilcoxon rank-sum are used to validate differences.
  - Quick check question: When should a non-parametric test be preferred over a t-test in model evaluation?

## Architecture Onboarding

- Component map: Web crawling -> Caption filtering -> Translation -> Multimodal recreation -> Cross-cultural annotation -> Zero-shot prompting -> Binary classification -> Statistical analysis
- Critical path: Annotate dataset -> Run zero-shot evaluations -> Analyze alignment -> Debug model bias
- Design tradeoffs:
  - Dataset size vs. cultural coverage: 300 samples per language balances cost and representativeness
  - Prompt language vs. model performance: English prompts perform better, but do not reflect true cross-cultural capability
  - Zero-shot vs. fine-tuning: Zero-shot reveals inherent bias; fine-tuning can mitigate but requires more data
- Failure signatures:
  - Low agreement within a culture -> Annotation guidelines unclear or inconsistent
  - Uniform performance across cultures -> Model lacks cultural sensitivity or dataset lacks cultural variance
  - High variance across prompt variations -> Model sensitivity to prompt phrasing or lack of robustness
- First 3 experiments:
  1. Run zero-shot classification with English prompts and compare agreement to random annotator groups to verify cultural bias exists.
  2. Test multilingual prompting by translating prompts to target language and measuring if alignment with that culture increases.
  3. Inject country information into prompts and compare accuracy to baseline to test if cultural conditioning is effective.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cultural bias in VLMs persist when using supervised fine-tuning on culturally diverse datasets?
- Basis in paper: [inferred] The paper shows VLMs align more with US annotations in zero-shot settings and notes that supervised fine-tuning could improve alignment with each culture.
- Why unresolved: The paper only provides a small supervised baseline with 3-fold cross-validation but does not explore comprehensive fine-tuning approaches or measure bias reduction.
- What evidence would resolve it: Experiments showing bias metrics before and after fine-tuning VLMs on datasets balanced across multiple cultures.

### Open Question 2
- Question: How does the cultural background of annotators within the same country affect hate speech annotations?
- Basis in paper: [inferred] The paper equates culture with country but acknowledges countries are multicultural and multiethnic.
- Why unresolved: The study only recruited monocultural annotators per country and did not capture intra-country cultural diversity.
- What evidence would resolve it: Comparative analysis of hate speech annotations from annotators with different ethnic, regional, or religious backgrounds within the same country.

### Open Question 3
- Question: Would creating multimodal datasets specifically targeting culturally diverse content sources reduce cultural bias in VLMs?
- Basis in paper: [inferred] The paper notes its dataset was sourced from a single English-language website and does not target culturally diverse content.
- Why unresolved: The study does not experiment with culturally targeted content creation or compare bias across datasets from different cultural sources.
- What evidence would resolve it: VLMs evaluated on datasets containing memes and captions specifically created by and for different cultural communities, measuring changes in cultural bias.

## Limitations

- Dataset size limitation: Only 300 samples per language may not fully capture cultural diversity in hate speech manifestations
- Binary classification oversimplification: The study assumes binary classification is appropriate for hate speech detection, potentially oversimplifying nuanced cultural interpretations
- Limited bias mitigation exploration: Zero-shot evaluation reveals inherent bias but does not explore whether fine-tuning on culturally diverse data could mitigate these biases

## Confidence

- High Confidence: The empirical finding that average pairwise agreement among cultures is only 74% demonstrates measurable cultural variation in hate speech annotations.
- Medium Confidence: The claim that VLMs exhibit cultural bias toward US annotations is supported by the data but could be influenced by prompt formulation effects.
- Low Confidence: The mechanism explanation for why country information injection fails to improve performance is largely observational without sufficient evidence.

## Next Checks

1. **Dataset Expansion Validation**: Test whether the 74% agreement rate persists when expanding the dataset to 1000+ samples per language, particularly focusing on edge cases where cultural interpretations are most divergent.

2. **Fine-tuning Experiment**: Train VLMs on subsets of Multi3Hate data from non-US cultures and measure whether performance on those cultures improves relative to zero-shot baselines, controlling for dataset size effects.

3. **Annotation Reliability Study**: Conduct inter-annotator agreement analysis within each culture to determine if low cross-cultural agreement stems from genuine cultural differences or inconsistent annotation standards across countries.