---
ver: rpa2
title: Large Vocabulary Size Improves Large Language Models
arxiv_id: '2406.16508'
source_url: https://arxiv.org/abs/2406.16508
tags:
- vocabulary
- size
- training
- language
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically investigates how subword vocabulary size
  affects the performance of large language models (LLMs). Experiments in English
  and Japanese show that larger vocabulary sizes (e.g., 100k or 500k) consistently
  lead to better performance on commonsense reasoning tasks compared to smaller sizes
  (e.g., 5k), especially for generation tasks.
---

# Large Vocabulary Size Improves Large Language Models

## Quick Facts
- arXiv ID: 2406.16508
- Source URL: https://arxiv.org/abs/2406.16508
- Reference count: 40
- Larger subword vocabularies consistently improve LLM performance on commonsense reasoning tasks, especially for generation tasks and Japanese language

## Executive Summary
This paper presents empirical evidence that increasing subword vocabulary size in large language models leads to consistent performance improvements, particularly for generation tasks and Japanese language processing. Through systematic experiments across multiple vocabulary sizes (5k, 50k, 100k, 500k) and languages, the authors demonstrate that larger vocabularies outperform smaller ones on commonsense reasoning benchmarks. The study also introduces an effective method for continual training that constructs new vocabularies and initializes embeddings through random projection, outperforming existing vocabulary expansion techniques.

## Method Summary
The authors conducted controlled experiments training LLMs with different subword vocabulary sizes (5k, 50k, 100k, 500k) on English and Japanese datasets. They evaluated performance on commonsense reasoning tasks using multiple benchmarks, comparing both zero-shot and continual training scenarios. For continual training, they compared three methods: reusing the original vocabulary, vocabulary expansion via random projection, and vocabulary expansion via parameter insertion. Training data sizes were varied to assess the impact of vocabulary size across different data regimes.

## Key Results
- Larger vocabularies (100k, 500k) consistently outperform smaller ones (5k, 50k) on commonsense reasoning tasks
- Performance improvements are more pronounced for generation tasks compared to classification tasks
- Japanese language shows larger benefits from increased vocabulary size compared to English
- Vocabulary expansion via random projection outperforms existing methods in continual training scenarios

## Why This Works (Mechanism)
Assumption: Larger vocabularies provide finer-grained tokenization that better captures linguistic nuances and reduces ambiguity in token representations. The increased vocabulary size allows the model to distinguish between semantically different but surface-similar tokens, leading to more precise contextual representations. For Japanese, which has complex character sets and compound words, larger vocabularies can better handle morphological variations and reduce the need for splitting words into unnatural subword units.

## Foundational Learning

Tokenization and vocabulary design
- Why needed: Determines how text is represented as discrete units for model processing
- Quick check: Can you explain the difference between character-level, word-level, and subword tokenization?

Subword vocabularies and Byte-Pair Encoding
- Why needed: Enables handling of large vocabularies while maintaining computational efficiency
- Quick check: What is the purpose of merging frequent character pairs in BPE?

Language model pretraining and adaptation
- Why needed: Understanding how models learn representations and adapt to new languages
- Quick check: How does continual training differ from initial pretraining in terms of vocabulary requirements?

## Architecture Onboarding

Component map: Tokenizer -> Embedding Layer -> Transformer Blocks -> Output Layer

Critical path: Tokenization → Embedding initialization → Model training → Evaluation

Design tradeoffs: Larger vocabularies improve representation quality but increase computational costs and memory requirements

Failure signatures: Performance degradation when vocabulary is too small for language complexity; increased training instability with extremely large vocabularies

First experiments:
1. Compare model performance across different vocabulary sizes on a held-out validation set
2. Measure training convergence speed for each vocabulary size configuration
3. Analyze tokenization quality by examining out-of-vocabulary rates for each setting

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions, but the findings suggest several areas for future research including the optimal vocabulary size for different language families, the impact of vocabulary size on model robustness to noise, and the relationship between vocabulary size and sample efficiency during training.

## Limitations
- Evaluation focused primarily on commonsense reasoning tasks, limiting generalizability to other NLP applications
- Japanese results may reflect specific linguistic properties that don't generalize to other languages
- Computational cost analysis is limited to training time without addressing inference efficiency or memory requirements
- The study uses controlled experimental conditions that may not reflect real-world deployment scenarios

## Confidence

**High confidence** in the empirical observation that larger vocabularies improve performance on commonsense reasoning tasks in controlled settings

**Medium confidence** in the generalizability of these findings to other task types and languages beyond the specific English and Japanese benchmarks tested

**Medium confidence** in the technical claims about vocabulary expansion methods, given the comparison to only one previous method

## Next Checks

1. Evaluate vocabulary size effects across a broader range of NLP tasks including machine translation, summarization, and domain-specific benchmarks to test generalizability

2. Conduct cross-linguistic validation with additional languages, particularly morphologically rich languages and low-resource languages, to understand the scope of the vocabulary size benefits

3. Perform comprehensive computational cost analysis including inference time, memory usage, and training efficiency across different hardware configurations to assess practical trade-offs

Required section headers to keep/add exactly:
- ## Method Summary
- ## Key Results
- ## Why This Works (Mechanism)
- ## Foundational Learning
- ## Architecture Onboarding
- ## Open Questions the Paper Calls Out
- ## Limitations
- ## Confidence
- ## Next Checks