---
ver: rpa2
title: 'Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models:
  An Empirical Analysis'
arxiv_id: '2407.18251'
source_url: https://arxiv.org/abs/2407.18251
tags:
- attack
- attacks
- pixels
- patch
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies adversarial pixel perturbations on multimodal
  models, focusing on black-box attacks using L0-norm constraints. The authors extend
  sparse attacks to include five contiguous shapes (row, column, diagonal, anti-diagonal,
  patch) and evaluate them against four multimodal models (ALIGN, AltCLIP, CLIP-B/32,
  GroupViT) and two unimodal DNNs (ResNet-50, VAN-base).
---

# Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models: An Empirical Analysis

## Quick Facts
- arXiv ID: 2407.18251
- Source URL: https://arxiv.org/abs/2407.18251
- Reference count: 40
- Key outcome: Sparse attacks are most effective on multimodal models with ViT encoders, while patch attacks excel against CNN-based ALIGN model; multimodal models show higher vulnerability than unimodal DNNs.

## Executive Summary
This paper investigates adversarial pixel perturbations on multimodal models using black-box attacks with L0-norm constraints. The authors extend sparse attacks to include five contiguous shapes (row, column, diagonal, anti-diagonal, patch) and evaluate them against four multimodal models (ALIGN, AltCLIP, CLIP-B/32, GroupViT) and two unimodal DNNs (ResNet-50, VAN-base). Using differential evolution optimization, the study reveals that multimodal models are significantly more vulnerable than unimodal DNNs, with sparse attacks being most effective on ViT-based encoders and patch attacks on CNN-based models. Notably, attacks affecting only 0.02% of image area achieve 99% success rate on ALIGN.

## Method Summary
The study employs differential evolution (DE) as a black-box attack method to generate adversarial pixel perturbations with L0-norm constraints. Five attack types are evaluated: sparse (individual pixels), row, column, diagonal, anti-diagonal, and patch perturbations. The DE parameters include mutation rate 0.55, crossover rate 0.8, population 300, and maximum 30,000 queries. Attacks target 4, 9, and 16 pixels (0.02-0.03% of image area) on 100 preprocessed ImageNet test images per model. Success rate (SR) measures the proportion of images successfully misclassified, with targeted attacks aiming for specific labels and untargeted attacks aiming for any incorrect label.

## Key Results
- Multimodal models are significantly more vulnerable to adversarial pixel perturbations than unimodal DNNs
- Sparse attacks achieve highest success rates on multimodal models with ViT encoders (CLIP-B/32, AltCLIP, GroupViT)
- Patch attacks are most effective against the CNN-based ALIGN model
- With only 16 pixels (0.02% of image area), untargeted attacks achieve 99% success rate on ALIGN
- The effectiveness of attack types varies significantly based on the underlying model architecture

## Why This Works (Mechanism)

### Mechanism 1: ViT Token Disruption
ViT models split images into visual patches treated as tokens. Sparse attacks modify pixels across different patches, corrupting multiple tokens simultaneously. This disrupts self-attention mechanisms that rely on inter-patch relationships, causing significant information loss in semantic meaning.

### Mechanism 2: CNN Kernel Vulnerability
CNN-based models like ALIGN use convolutional kernels that process local image regions. Contiguous patch attacks corrupt input regions matching kernel sizes, breaking feature extraction in early convolution layers and propagating incorrect activations through pooling layers.

### Mechanism 3: Contrastive Learning Flexibility
Multimodal models trained with contrastive learning on large diverse datasets without explicit class labels have smoother decision boundaries compared to supervised unimodal models. This flexibility increases susceptibility to adversarial examples as the model lacks precise feature boundaries.

## Foundational Learning

- **Differential Evolution (DE)**: Black-box optimization method using mutation, crossover, and selection to evolve pixel perturbations without gradient information. Needed for attacking models without access to gradients; quick check: what are DE's key components and how do they find adversarial perturbations?

- **L0-norm constraints**: Limits the number of pixels changed in an attack, controlling perturbation size and imperceptibility. Needed to create sparse, stealthy attacks; quick check: how does L0-norm affect the trade-off between success rate and stealth?

- **ViT tokenization and self-attention**: ViT processes images as visual tokens with self-attention mechanisms. Needed to understand why sparse attacks are effective on ViT models; quick check: how does perturbing pixels in different visual patches affect ViT's self-attention computation?

## Architecture Onboarding

- **Component map**: Image preprocessing → perturbation application → model inference → fitness evaluation → DE population update
- **Critical path**: 1) Initialize DE population with random perturbations, 2) Apply perturbations to preprocessed images, 3) Query model for predictions, 4) Compute fitness using hinge loss, 5) Evolve population via mutation, crossover, selection, 6) Repeat until max queries or success
- **Design tradeoffs**: Sparse vs contiguous attacks (more tokens vs perceptibility), number of pixels vs success rate (more pixels increase success but reduce stealth), query budget vs attack quality (more queries allow better optimization)
- **Failure signatures**: Low success rates despite many queries (preprocessing or DE parameter issues), high variance in results (suboptimal population size or mutation rate), contiguous attacks underperforming (encoding or mutation strategy misalignment)
- **First 3 experiments**: 1) Test DE with different mutation rates (0.5, 0.7, 0.9) on small image set, 2) Compare sparse vs row attack success rates on ViT model with 4 pixels, 3) Measure success rate vs number of queries to identify convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
How does the vulnerability of multimodal models to sparse attacks relate to their zero-shot learning capability? The paper speculates this connection but provides no concrete evidence or analysis to confirm the relationship.

### Open Question 2
How does the depth and width configuration of multimodal models influence their vulnerability to contiguous attacks? The paper observes trends for AltCLIP but lacks detailed analysis of architectural impact on contiguous attack vulnerability.

### Open Question 3
How does GroupViT's grouping mechanism contribute to its robustness against sparse attacks? The paper suggests the grouping mechanism may help but provides only speculative explanation without empirical validation.

## Limitations

- Limited model and attack configuration scope (4 multimodal, 2 unimodal models; 5 shapes; 3 pixel counts)
- Unclear multimodal model preprocessing pipeline details affecting reproducibility
- Small evaluation dataset (100 images per model may not capture full vulnerability distribution)
- No investigation of defense mechanisms or attack transferability across models

## Confidence

**High Confidence**: Multimodal models are more vulnerable than unimodal DNNs - well-supported empirical results with consistent findings across attack types.

**Medium Confidence**: Sparse attacks most effective on ViT-based models - reasonable support but mechanistic explanation needs additional ablation studies.

**Medium Confidence**: Patch attacks most effective against CNN-based models - supported by data but kernel vulnerability mechanism not empirically validated.

**Low Confidence**: Trade-off between model adaptability and robustness - mentioned but not empirically quantified or causally established.

## Next Checks

1. **Ablation Study on Token Disruption**: Run sparse attacks on ViT models while measuring token-level attention scores and semantic coherence. Compare against baseline ViT models with attention masking or token denoising to isolate whether token disruption is the primary vulnerability mechanism.

2. **Convolutional Architecture Analysis**: For the ALIGN model, conduct patch attacks with varying patch sizes relative to kernel sizes. Measure the correlation between patch size, feature map corruption, and attack success rate to validate the kernel vulnerability hypothesis.

3. **Training Data Diversity Experiment**: Fine-tune a multimodal model on a smaller, more curated dataset with explicit class boundaries. Compare vulnerability to sparse attacks against the original model to test whether the lack of class-specific supervision directly contributes to increased vulnerability.