---
ver: rpa2
title: Still Not Quite There! Evaluating Large Language Models for Comorbid Mental
  Health Diagnosis
arxiv_id: '2410.03908'
source_url: https://arxiv.org/abs/2410.03908
tags:
- depression
- anxiety
- mental
- health
- post
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ANGST is a novel benchmark dataset for depression-anxiety comorbidity
  classification from social media posts, addressing limitations of existing datasets
  that oversimplify mental health disorders. The dataset comprises 2,876 expert-annotated
  posts and 7,667 silver-labeled posts, enabling multi-label classification for depression,
  anxiety, both, or neither.
---

# Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis

## Quick Facts
- arXiv ID: 2410.03908
- Source URL: https://arxiv.org/abs/2410.03908
- Reference count: 40
- Current state-of-the-art LLMs struggle with mental health comorbidity detection, with best models achieving <72% F1 score

## Executive Summary
This paper introduces ANGST, a novel benchmark dataset for depression-anxiety comorbidity classification from social media posts, addressing limitations of existing datasets that oversimplify mental health disorders. The dataset comprises 2,876 expert-annotated posts and 7,667 silver-labeled posts, enabling multi-label classification for depression, anxiety, both, or neither. Benchmarking with state-of-the-art models including Mental-BERT, Mental-RoBERTa, Mental-XLNet, Mental-LongFormer, LLama-2, GPT-3.5-turbo, and GPT-4 revealed that while GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72% in multi-class comorbid classification. The study highlights ongoing challenges in applying language models to mental health diagnostics and demonstrates the complexity of accurately identifying comorbid conditions from text.

## Method Summary
The study evaluates large language models for comorbid mental health diagnosis using the ANGST dataset, which includes 2,876 expert-annotated Reddit posts and 7,667 silver-labeled posts. Mental health-focused PLMs (Mental-BERT, Mental-RoBERTa, Mental-XLNet, Mental-LongFormer) are fine-tuned on silver-labeled data, while generative LLMs (LLama-2, GPT-3.5-turbo, GPT-4) are evaluated using zero-shot and few-shot prompting. Models are assessed on both multi-label classification (depression, anxiety, both, or neither) and binary classification tasks using standard metrics including weighted precision, recall, F1 scores, macro-F1, and Hamming loss. Hyperparameter tuning involves learning rates of {2e-5, 2e-6} and batch sizes of {4, 8, 16, 32} with 30 epochs and early stopping after 10 epochs of no improvement.

## Key Results
- GPT-4 outperforms other models but none exceed 72% F1 score in multi-class comorbid classification
- Mental-LongFormer shows strong performance among PLMs but still falls short of GPT-4
- Binary classification tasks yield better results than multi-label tasks across all models
- Zero-shot prompting often outperforms few-shot prompting for LLMs in this domain

## Why This Works (Mechanism)

### Mechanism 1
Neutral seeding of control posts improves inter-class similarity, making the dataset harder to classify and thus more realistic. By sampling control posts from the same distribution as mental health posts, semantic divergence between classes is reduced, leading to higher JSD/MMD values. Higher inter-class similarity better reflects real-world difficulty in distinguishing mental health symptoms from general language. Break condition: If neutral seeding inadvertently includes posts semantically closer to mental health language than expected, the intended challenge may be reduced.

### Mechanism 2
Silver-labeled data via GPT-3.5-turbo with explanation generation improves label quality compared to naive keyword-based silver labels. The prompt instructs the LLM to refer to DSM-5 criteria and provide rationales, leading to more clinically grounded annotations. LLM-generated explanations correlate with human judgment better than simple keyword matching. Break condition: If the LLM's DSM-5 references are inaccurate or hallucinated, silver labels could mislead model training.

### Mechanism 3
Multi-label framing captures comorbidity better than binary classification, aligning with clinical reality. Allowing simultaneous depression and anxiety labels reflects known high comorbidity rates in mental health, enabling more nuanced model training. Social media posts often contain mixed symptoms that cannot be cleanly separated into single labels. Break condition: If multi-label setup leads to ambiguous or noisy annotations, model performance may suffer.

## Foundational Learning

- **Jensen-Shannon Divergence (JSD)**: Used to quantify inter-class similarity between mental health and control posts. Quick check: What does a lower JSD value between two classes imply about their distinguishability?

- **Multi-label classification**: Required to model comorbidity (depression + anxiety) instead of treating disorders as mutually exclusive. Quick check: How does Hamming loss differ from standard binary classification loss in multi-label settings?

- **Prompt engineering for LLMs**: Critical for generating high-quality silver labels and zero/few-shot classifications. Quick check: What is the effect of including DSM-5 criteria in the prompt on label accuracy?

## Architecture Onboarding

- **Component map**: Data pipeline → Annotation (gold/silver) → Feature extraction (BERT embeddings) → Model training (PLMs/LLMs) → Evaluation (F1, Hamming loss)
- **Critical path**: Data collection → Filtering → Annotation → Train/validation split → Model fine-tuning → Benchmark
- **Design tradeoffs**: Gold annotation ensures quality but is costly; silver annotation scales but risks noise; multi-label increases realism but complicates training
- **Failure signatures**: High JSD/MMD despite neutral seeding → control posts too similar to mental health; low recall in depression → model misses subtle cues; high Hamming loss → over-conservative predictions
- **First 3 experiments**:
  1. Replicate JSD/MMD analysis on a subset to verify inter-class similarity
  2. Compare silver vs. gold label consistency on a validation set
  3. Train a simple logistic regression on TF-IDF features to establish a baseline for classification difficulty

## Open Questions the Paper Calls Out

### Open Question 1
How do ANGST results compare to other mental health datasets when evaluated with the same models? The paper mentions cross-dataset analysis but lacks direct model performance comparisons on the same models across datasets. A comprehensive table showing model performance (F1 scores, precision, recall) for each dataset using the same evaluation framework would resolve this.

### Open Question 2
What is the optimal balance between zero-shot and few-shot prompting for different types of mental health classification tasks? The paper discusses relative performance but doesn't provide systematic analysis of when each approach works best. A detailed ablation study varying the number of few-shot examples and measuring performance across different task types and model families would resolve this.

### Open Question 3
How does the temporal aspect of mental health symptoms affect model performance in detecting depression and anxiety? The paper identifies this limitation but doesn't explore methods to address it or quantify its impact. An analysis showing how performance changes when models are given explicit temporal context or when posts are categorized by temporal indicators would resolve this.

### Open Question 4
What is the impact of multimodal information (e.g., post timing, subreddit context) on model performance for mental health classification? The paper mentions timing could indicate insomnia or social relationship issues but doesn't explore how additional modalities might improve performance. An experimental comparison showing performance gains when incorporating timing data, subreddit information, or other contextual features alongside text would resolve this.

## Limitations

- Dataset generalizability issues due to Reddit sampling bias toward younger, more technologically literate users
- Silver label reliability concerns without robust human validation mechanisms
- Model evaluation constraints that don't fully address temporal aspects of depression

## Confidence

- **High Confidence**: Dataset construction methodology and fundamental finding that current LLMs struggle with comorbid mental health classification
- **Medium Confidence**: Claims about ANGST's superior benchmark quality due to neutral seeding and its representation of real-world classification difficulty
- **Low Confidence**: Assertions about silver label quality improvements through explanation generation and DSM-5 grounding

## Next Checks

1. Conduct inter-annotator agreement analysis between gold and silver labels on a stratified sample of 500 posts to quantify label consistency and identify systematic discrepancies in the silver labeling process.

2. Perform demographic analysis comparing Reddit users in the ANGST dataset to clinical mental health populations to assess external validity and identify potential sampling biases that could affect model generalizability.

3. Implement temporal analysis framework to evaluate models' ability to distinguish current from past mental health states, including time-aware features and longitudinal evaluation metrics to address the temporal limitations in current classification approaches.