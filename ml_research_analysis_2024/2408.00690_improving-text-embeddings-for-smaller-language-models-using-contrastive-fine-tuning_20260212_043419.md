---
ver: rpa2
title: Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning
arxiv_id: '2408.00690'
source_url: https://arxiv.org/abs/2408.00690
tags:
- minicpm
- fine-tuning
- text
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving text embedding
  quality for smaller language models, which are more resource-efficient but typically
  underperform larger models without specialized optimization. The authors propose
  a contrastive fine-tuning approach using the NLI dataset to enhance text embeddings
  for three small models: MiniCPM, Phi-2, and Gemma.'
---

# Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning

## Quick Facts
- arXiv ID: 2408.00690
- Source URL: https://arxiv.org/abs/2408.00690
- Authors: Trapoom Ukarapol; Zhicheng Lee; Amy Xin
- Reference count: 5
- Primary result: MiniCPM achieves 83.84% ± 4.27 Spearman correlation on STS tasks after contrastive fine-tuning, outperforming other small models

## Executive Summary
This paper addresses the challenge of improving text embedding quality for smaller language models, which are more resource-efficient but typically underperform larger models without specialized optimization. The authors propose a contrastive fine-tuning approach using the NLI dataset to enhance text embeddings for three small models: MiniCPM, Phi-2, and Gemma. By employing InfoNCE loss with low-rank adaptation (LoRA) for parameter-efficient fine-tuning, they achieve significant performance gains across nine STS benchmarks. MiniCPM showed the most substantial improvement with an average 56.33% performance gain, achieving an overall Spearman correlation of 83.84% ± 4.27.

## Method Summary
The authors employ contrastive fine-tuning on the NLI dataset using InfoNCE loss with LoRA (Low-Rank Adaptation) for parameter-efficient adaptation. The method involves fine-tuning three small language models (MiniCPM, Phi-2, and Gemma) for one epoch using a batch size of 60, learning rate of 5e-5, and bf16 mixed precision. Embeddings are extracted from the <EOS> token position, and the models are evaluated on nine STS benchmark datasets using Spearman correlation as the primary metric.

## Key Results
- MiniCPM achieves the highest average Spearman correlation of 83.84% ± 4.27 across all benchmarks
- MiniCPM shows a 56.33% average performance gain compared to its baseline
- MiniCPM achieves most of its performance gains within the first 200 training steps

## Why This Works (Mechanism)

### Mechanism 1
Contrastive fine-tuning with InfoNCE loss improves text embedding quality by pulling similar text pairs closer and pushing dissimilar pairs apart in embedding space. The model learns to distinguish between entailment pairs (positive examples) and contradiction pairs (negative examples) by optimizing the InfoNCE objective. This contrastive learning approach forces the embeddings to capture semantic similarity more effectively than standard pretraining. Core assumption: The NLI dataset provides appropriate positive and negative pairs for learning semantic similarity relationships.

### Mechanism 2
LoRA (Low-Rank Adaptation) enables parameter-efficient fine-tuning by adapting model weights using low-rank matrix factorization instead of updating all parameters. LoRA introduces small trainable low-rank matrices that modify the original weight matrices during fine-tuning, allowing significant performance improvements with minimal parameter changes and computational resources. Core assumption: The low-rank approximation captures the essential changes needed for the embedding task without requiring full model adaptation.

### Mechanism 3
MiniCPM's architecture and training regimen enable superior performance on text embedding tasks compared to other small models. The combination of MiniCPM's model architecture, training strategies, and the effectiveness of contrastive fine-tuning allows it to achieve better semantic alignment with human judgment across diverse datasets. Core assumption: MiniCPM's architectural design and pre-training provide a foundation that responds particularly well to contrastive fine-tuning for embedding tasks.

## Foundational Learning

- **Concept: Contrastive learning**
  - Why needed here: Understanding how contrastive objectives like InfoNCE work is essential for grasping why this approach improves embeddings by learning semantic relationships
  - Quick check question: What is the fundamental difference between contrastive learning and traditional supervised learning in the context of text embeddings?

- **Concept: Low-rank adaptation (LoRA)**
  - Why needed here: LoRA is the parameter-efficient fine-tuning technique used, so understanding how low-rank matrix factorization reduces computational requirements while maintaining performance is crucial
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning, and what are the tradeoffs?

- **Concept: Spearman correlation**
  - Why needed here: The evaluation metric used to measure embedding quality, so understanding how it measures rank correlation between predicted and ground truth similarities is important
  - Quick check question: Why might Spearman correlation be preferred over Pearson correlation for evaluating embedding quality in STS tasks?

## Architecture Onboarding

- **Component map**: Pretrained small language model -> LoRA adapter -> InfoNCE loss computation -> Parameter updates -> Evaluation on STS benchmarks
- **Critical path**: Data loading → LoRA adapter application → InfoNCE loss computation → Parameter updates → Evaluation on STS benchmarks
- **Design tradeoffs**: Parameter efficiency vs. potential performance ceiling (LoRA), computational resources vs. batch size (affects contrastive learning quality), model size vs. embedding quality
- **Failure signatures**: Poor performance on STS benchmarks despite successful training, high variance in results across different datasets, slow convergence or instability during training
- **First 3 experiments**:
  1. Run contrastive fine-tuning on MiniCPM with default parameters and evaluate on STS benchmarks to establish baseline performance
  2. Test different learning rates (e.g., 5e-4, 5e-5, 5e-3) to identify optimal training stability and performance
  3. Compare performance with and without hard negative penalty to validate the effectiveness of this contrastive learning component

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The study relies heavily on the NLI dataset for contrastive fine-tuning, which may not capture all semantic relationships relevant to STS tasks
- The specific reasons for MiniCPM's superior performance are not fully explained, lacking detailed architectural analysis
- Limited hyperparameter exploration with fixed settings (LoRA rank 8, learning rate 5e-5) without extensive ablation studies

## Confidence

- **High Confidence**: The overall methodology of using contrastive fine-tuning with InfoNCE loss and LoRA for parameter-efficient adaptation is well-established and the implementation details are clearly specified
- **Medium Confidence**: The specific claim that MiniCPM outperforms other models due to its architecture and training regimen is supported by results but lacks detailed architectural analysis
- **Low Confidence**: The generalization of these improvements to other tasks beyond STS (semantic textual similarity) remains untested

## Next Checks

1. **Cross-domain generalization test**: Evaluate the fine-tuned models on text embedding tasks from different domains (e.g., medical text similarity, legal document clustering) to verify that improvements extend beyond the tested STS benchmarks

2. **Ablation study on contrastive components**: Systematically test the impact of removing hard negative penalties, varying the InfoNCE temperature parameter, and comparing in-batch vs. external negatives to isolate which contrastive learning components contribute most to performance gains

3. **Architecture sensitivity analysis**: Conduct controlled experiments comparing different small language model architectures with identical fine-tuning procedures to determine whether MiniCPM's superior performance is due to its architecture or other factors like training data quality