---
ver: rpa2
title: Improving Generated and Retrieved Knowledge Combination Through Zero-shot Generation
arxiv_id: '2412.18800'
source_url: https://arxiv.org/abs/2412.18800
tags:
- knowledge
- generated
- retrieved
- passages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BRMGR, an unsupervised method for combining
  retrieved and LLM-generated knowledge in open-domain question answering. The core
  idea is to use zero-shot generation to rerank both retrieved and generated passages
  separately, then combine them using greedy matching.
---

# Improving Generated and Retrieved Knowledge Combination Through Zero-shot Generation

## Quick Facts
- arXiv ID: 2412.18800
- Source URL: https://arxiv.org/abs/2412.18800
- Authors: Xinkai Du; Quanjie Han; Chao Lv; Yan Liu; Yalin Sun; Hao Shu; Hongbo Shan; Maosong Sun
- Reference count: 34
- One-line primary result: BRMGR improves performance by +1.7 and +1.6 on NQ and WebQ datasets respectively compared to competitive baselines

## Executive Summary
This paper proposes BRMGR, an unsupervised method for combining retrieved and LLM-generated knowledge in open-domain question answering. The core idea is to use zero-shot generation to rerank both retrieved and generated passages separately, then combine them using greedy matching. The method addresses the challenge of pairing retrieved and generated knowledge sources without definitive labels. Experimental results on three datasets show that BRMGR improves performance by +1.7 and +1.6 on NQ and WebQ datasets respectively compared to competitive baselines, while achieving comparable results on TriviaQA.

## Method Summary
BRMGR (Bi-Reranking for Merging Generated and Retrieved knowledge) combines retrieved passages from DPR with LLM-generated passages using zero-shot reranking. The method factorizes combination relevance scores into separate probabilities for generated and retrieved passages, assuming conditional independence. Both passage types are reranked using language model likelihood scores, then combined through greedy matching. When the number of generated and retrieved passages is equal, greedy matching is theoretically equivalent to optimal bipartite matching. The final answers are generated using a Fusion-in-Decoder reader model.

## Key Results
- BRMGR improves performance by +1.7 and +1.6 on NQ and WebQ datasets respectively compared to competitive baselines
- The method achieves comparable results to strong baselines on TriviaQA dataset
- Reranking scores computed by log-likelihood of generating passage tokens conditioned on the query consistently enhance the original outcome
- The factorization of combination relevance score into two separate probabilities enables effective unsupervised reranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The factorization of combination relevance score into two separate probabilities enables unsupervised reranking of both retrieved and generated passages.
- Mechanism: The model assumes conditional independence between p(lpi|q) and p(q|rpj), allowing separate computation of relevance scores for generated and retrieved passages before combining them.
- Core assumption: The combination relevance score can be factorized as p(lpi,rpj|q) = p(lpi|q)p(rpj|q).
- Evidence anchors:
  - [abstract] "We pair the two types of passages using two separate re-ranking methods and then combine them through greedy matching."
  - [section II] "Given the query q, assume the combination relevance score between lpi and rpj is conditional independent, then it can be factorized into two probabilities: p(lpi,rpj|q) = p(lpi|q)p(rpj|q)"
  - [corpus] Weak - no direct evidence in neighboring papers about this specific factorization approach.
- Break condition: If the conditional independence assumption is violated (e.g., generated passages strongly depend on specific retrieved passages), the factorization becomes invalid.

### Mechanism 2
- Claim: Greedy matching is equivalent to bipartite matching when the number of generated and retrieved passages is equal.
- Mechanism: The model proves that when combination relevance scores are factorized products, the optimal matching found by Hungarian algorithm equals the greedy matching solution.
- Core assumption: The number of generated passages equals the number of retrieved passages (M=N).
- Evidence anchors:
  - [section II] "Theorem 1. If we assume that the combination relevance score of p(lpi, rpj|q) can be factorized into the relevance scores of the generated knowledge and the retrieved knowledge, and further, if the number of both types of knowledge is the same, then we can conclude that the optimal match obtained through bipartite matching loss is equivalent to the one obtained through the greedy matching."
  - [section II] "Proof. We will prove this statement using mathematical induction... By induction, we can solve the remaining N-1 pairs using a greedy matching."
  - [corpus] No direct evidence in neighboring papers about this mathematical equivalence.
- Break condition: When M≠N (unequal numbers of generated and retrieved passages), the greedy matching no longer guarantees optimal solution.

### Mechanism 3
- Claim: Zero-shot generation likelihood scores effectively rerank generated passages by capturing their relevance to the query.
- Mechanism: The model uses the conditional probability p(lpi|q) computed via language model log-likelihood to rerank generated passages, addressing hallucination issues.
- Core assumption: Language models can effectively score the relevance of generated passages to queries using zero-shot generation.
- Evidence anchors:
  - [section II] "Due to the Hallucination of large language model [22], the generated passages may contain some unhelpful content. Given the large language model's strong ability, the generated information is highly related to the query. We propose to utilize the conditional probability of the generated document conditioned on query p(lpi|q) to rerank the generated passages."
  - [section III-C2] "From the observed results in figure 3, it is evident that the reranking scores, computed by the log-likelihood of generating passage tokens conditioned on the query, consistently enhance the original outcome."
  - [corpus] Weak - neighboring papers focus on different reranking approaches, not zero-shot generation likelihood.
- Break condition: If generated passages contain substantial hallucinated content unrelated to the query, the reranking scores may not effectively filter out irrelevant information.

## Foundational Learning

- Concept: Conditional independence assumption in probability factorization
  - Why needed here: The core mechanism relies on factorizing combination relevance scores, which requires understanding when variables can be assumed independent.
  - Quick check question: Can you explain under what conditions p(A,B|C) = p(A|C)p(B|C) holds true, and provide an example where this would be violated?

- Concept: Greedy vs. optimal matching algorithms
  - Why needed here: The paper proves that greedy matching equals optimal matching under specific conditions, requiring understanding of both algorithms and their properties.
  - Quick check question: Given a 3×3 matrix of relevance scores, can you manually compute both the greedy matching and Hungarian algorithm solution to verify they produce the same result?

- Concept: Zero-shot learning and generation likelihood
  - Why needed here: The reranking approach uses zero-shot generation likelihood scores, requiring understanding of how language models can score text generation without task-specific training.
  - Quick check question: How would you compute the log-likelihood of a sentence given a prompt using a language model, and what does this score represent in terms of relevance?

## Architecture Onboarding

- Component map:
  Query input → DPR retriever → Top-10 retrieved passages
  Query input → LLM generator → Top-10 generated passages
  Reranker component (uses language model) for both passage types
  Greedy matching combiner
  FiD reader for final answer generation

- Critical path:
  Query → DPR retrieval → Reranking of retrieved passages → Reranking of generated passages → Greedy matching → FiD reader → Answer

- Design tradeoffs:
  - Using greedy matching instead of Hungarian algorithm provides O(N) complexity instead of O(N³), but only works when M=N
  - Zero-shot generation for reranking avoids need for labeled data but may be less precise than supervised methods
  - Combining two knowledge sources adds complexity but provides complementary information

- Failure signatures:
  - Performance degrades when generated passages contain significant hallucination
  - System fails when DPR retrieves irrelevant passages (reranking cannot fix fundamentally wrong retrievals)
  - Greedy matching produces suboptimal results when M≠N

- First 3 experiments:
  1. Verify factorization assumption by testing if p(lpi,rpj|q) ≈ p(lpi|q)p(rpj|q) holds on a small dataset
  2. Compare greedy matching vs Hungarian algorithm results when M=N to confirm equivalence
  3. Test reranking effectiveness by measuring performance improvement on NQ dev set with different PLM sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BRMGR vary when using different zero-shot generation models for reranking?
- Basis in paper: [explicit] The paper mentions using Flan-T5 models and T0-3B for reranking, but does not explore other potential models.
- Why unresolved: The paper focuses on specific models but does not provide a comprehensive comparison with other potential zero-shot generation models.
- What evidence would resolve it: Experimental results comparing BRMGR's performance using various zero-shot generation models like GPT-3.5, GPT-4, or other large language models.

### Open Question 2
- Question: What is the impact of varying the number of retrieved and generated passages on the performance of BRMGR?
- Basis in paper: [explicit] The paper uses 10 retrieved and/or generated passages but does not explore the effects of using different numbers of passages.
- Why unresolved: The optimal number of passages for balancing performance and computational efficiency is not explored.
- What evidence would resolve it: Experimental results showing BRMGR's performance with different numbers of retrieved and generated passages, such as 5, 15, or 20 passages.

### Open Question 3
- Question: How does BRMGR handle cases where the retrieved and generated passages provide conflicting information?
- Basis in paper: [inferred] The paper discusses combining retrieved and generated knowledge but does not address how conflicts between these sources are resolved.
- Why unresolved: The method for resolving conflicts between retrieved and generated passages is not explicitly discussed or tested.
- What evidence would resolve it: Analysis of BRMGR's performance on datasets or scenarios where retrieved and generated passages contain conflicting information, and a proposed method for conflict resolution.

## Limitations

- The conditional independence assumption may not hold in practice when generated passages depend on retrieved content, potentially degrading performance.
- The theoretical equivalence between greedy and optimal matching only holds when M=N, limiting practical applicability to scenarios with equal numbers of passages.
- The reranking method using zero-shot generation likelihood may not effectively filter hallucinated content if generated passages contain substantial irrelevant information mixed with relevant content.

## Confidence

**High Confidence**: The experimental results showing BRMGR outperforms competitive baselines on NQ and WebQ datasets are well-supported with clear metrics and comparison points. The methodology for combining reranked passages through greedy matching is clearly specified.

**Medium Confidence**: The theoretical proof of greedy matching equivalence to bipartite matching under factorization assumptions is mathematically sound, but its practical applicability is limited by the equal-number constraint.

**Low Confidence**: The effectiveness of zero-shot generation likelihood for reranking in the presence of significant hallucination remains questionable, as the paper provides limited evidence about how well this approach handles generated content with mixed relevance.

## Next Checks

1. **Conditional Independence Validation**: Test the factorization assumption on a small dataset by measuring the correlation between p(lpi,rpj|q) and p(lpi|q)p(rpj|q). If correlation is low, the core mechanism may fail.

2. **Greedy vs Optimal Matching Comparison**: When M≠N, systematically compare greedy matching performance against Hungarian algorithm on the same datasets to quantify the performance gap from violating the theoretical constraint.

3. **Hallucination Robustness Test**: Generate passages with varying levels of hallucination (controlled by adjusting LLM temperature or using different prompts) and measure how reranking performance degrades as hallucinated content increases.